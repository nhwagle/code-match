{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Equation Focused Primary Method.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cbd9f66494e74e619caeedb662d45d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3d1a901151344ec9e45ddc3204eabfc",
              "IPY_MODEL_62221b88393e442da20abca70c330190",
              "IPY_MODEL_6bbc13ddfd5e4f7eaa04d52ecf6f8331"
            ],
            "layout": "IPY_MODEL_fdd104276d654cd4ae6a75e6c2fa15ea"
          }
        },
        "a3d1a901151344ec9e45ddc3204eabfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6b6204739874340a54578ad9707483f",
            "placeholder": "​",
            "style": "IPY_MODEL_524bcc0ae68146cdbe426d8afac80584",
            "value": "Downloading: 100%"
          }
        },
        "62221b88393e442da20abca70c330190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b755bdff504e4dd0b4cef9b108c98db8",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78318016df8f403f9f3215a8b6048512",
            "value": 213450
          }
        },
        "6bbc13ddfd5e4f7eaa04d52ecf6f8331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bacc1b1b92fe45b39c5cb8d2db3a8388",
            "placeholder": "​",
            "style": "IPY_MODEL_fe613d70045f40a18c2020f37695d25c",
            "value": " 208k/208k [00:00&lt;00:00, 292kB/s]"
          }
        },
        "fdd104276d654cd4ae6a75e6c2fa15ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6b6204739874340a54578ad9707483f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "524bcc0ae68146cdbe426d8afac80584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b755bdff504e4dd0b4cef9b108c98db8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78318016df8f403f9f3215a8b6048512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bacc1b1b92fe45b39c5cb8d2db3a8388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe613d70045f40a18c2020f37695d25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee86ab59c7ca4fd6af04ac1e46d5253a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_000eadd214694fc492b3549cff75bcf5",
              "IPY_MODEL_f26a3a9226234725872eae236e430c4d",
              "IPY_MODEL_c38cb9e9f2e945328567a074b70c761a"
            ],
            "layout": "IPY_MODEL_d6b90ec7a5a6444a836a84f09ef5b6ed"
          }
        },
        "000eadd214694fc492b3549cff75bcf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3acfd9aa07fa4a2c8ef89ac4aa268893",
            "placeholder": "​",
            "style": "IPY_MODEL_93baea1861ca400791546952beba54de",
            "value": "Downloading: 100%"
          }
        },
        "f26a3a9226234725872eae236e430c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6766332df1641d7ba625f8cd8107340",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e4f13b343dc4840a2524f054037efff",
            "value": 29
          }
        },
        "c38cb9e9f2e945328567a074b70c761a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cc315be84a94f28a885754bf1f1b7a9",
            "placeholder": "​",
            "style": "IPY_MODEL_38c4e970147044f497f7fbc02f16567c",
            "value": " 29.0/29.0 [00:00&lt;00:00, 1.05kB/s]"
          }
        },
        "d6b90ec7a5a6444a836a84f09ef5b6ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3acfd9aa07fa4a2c8ef89ac4aa268893": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93baea1861ca400791546952beba54de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6766332df1641d7ba625f8cd8107340": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e4f13b343dc4840a2524f054037efff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4cc315be84a94f28a885754bf1f1b7a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38c4e970147044f497f7fbc02f16567c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44e1f71f00644438925860f930652820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc4cf24e549e4e0f9a5b188df34309c8",
              "IPY_MODEL_990c71fba1b4499385f31e753deaec0f",
              "IPY_MODEL_803dcd037b874c47834a5a8ccb15ad31"
            ],
            "layout": "IPY_MODEL_a6692258a61c4fe6b57d290bdb3b46a2"
          }
        },
        "cc4cf24e549e4e0f9a5b188df34309c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_438eaf30587146b6a1ec6f3cea991fcd",
            "placeholder": "​",
            "style": "IPY_MODEL_44a3b346c7084c0a85387481079fa3d3",
            "value": "Downloading: 100%"
          }
        },
        "990c71fba1b4499385f31e753deaec0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_811b7d8618a244f59dcc7413cbce0982",
            "max": 411,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_762da9a7c4ab4fc49e9d2d87000a9960",
            "value": 411
          }
        },
        "803dcd037b874c47834a5a8ccb15ad31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a03d2d5c5bb48c5949c691bb83b0660",
            "placeholder": "​",
            "style": "IPY_MODEL_aecafd7246ec4e6fb281926ad1925b6c",
            "value": " 411/411 [00:00&lt;00:00, 18.1kB/s]"
          }
        },
        "a6692258a61c4fe6b57d290bdb3b46a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "438eaf30587146b6a1ec6f3cea991fcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44a3b346c7084c0a85387481079fa3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "811b7d8618a244f59dcc7413cbce0982": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "762da9a7c4ab4fc49e9d2d87000a9960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a03d2d5c5bb48c5949c691bb83b0660": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aecafd7246ec4e6fb281926ad1925b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fefb5e101b14584ac8ec7b8b3d56b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbee662035ff4e9d81e53ce578c62887",
              "IPY_MODEL_44cfb725e5cf47aeabfe21f045fffc53",
              "IPY_MODEL_8df83a5348584916b75376a8eada88f0"
            ],
            "layout": "IPY_MODEL_d52bbdc8fec342cea46b3f0a28fb51e7"
          }
        },
        "dbee662035ff4e9d81e53ce578c62887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41280685adca42178c2e3e9a8bd681d0",
            "placeholder": "​",
            "style": "IPY_MODEL_72ce80d1fc7e4f5a9fa8b41ca697920e",
            "value": "Downloading: 100%"
          }
        },
        "44cfb725e5cf47aeabfe21f045fffc53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_471fb7328b3a42dc87c09f13c311de6f",
            "max": 263273408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f67f219d481d46958b23933f519bc3e5",
            "value": 263273408
          }
        },
        "8df83a5348584916b75376a8eada88f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dabd9627f8ec450c829a807f4b4105ac",
            "placeholder": "​",
            "style": "IPY_MODEL_b3a5ddf3327145e49ee581d0fd44af47",
            "value": " 251M/251M [00:03&lt;00:00, 66.0MB/s]"
          }
        },
        "d52bbdc8fec342cea46b3f0a28fb51e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41280685adca42178c2e3e9a8bd681d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72ce80d1fc7e4f5a9fa8b41ca697920e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "471fb7328b3a42dc87c09f13c311de6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f67f219d481d46958b23933f519bc3e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dabd9627f8ec450c829a807f4b4105ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3a5ddf3327145e49ee581d0fd44af47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhwagle/code-match/blob/main/Equation_Focused_Primary_Method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82i_Cc08zWK3"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing codegen. Their install_env.sh won't work for colab so modifying such that all conda imports are done with pip"
      ],
      "metadata": {
        "id": "Lq8FKDVXwgxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install transformers pylatexenc"
      ],
      "metadata": {
        "id": "YzjVFdEo5rwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from pylatexenc.latex2text import LatexNodes2Text"
      ],
      "metadata": {
        "id": "ZXwX4tnk4xlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUL_IFxeA3L7",
        "outputId": "8a084743-64a8-4dd2-bd19-558876635e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/MyDrive/RSS/Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-dGMK1NA3R8",
        "outputId": "a831bed2-6025-44f7-dd0b-931ed9d7cca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "algorithms.csv\tcode.csv\t  code-small.csv  func-rep-small.csv\n",
            "code-all.csv\tcode-func-rep.pt  equations.csv   func-rep-small.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eqns = pd.read_csv(\"/content/gdrive/MyDrive/RSS/Data/equations.csv\")\n",
        "algs = pd.read_csv(\"/content/gdrive/MyDrive/RSS/Data/algorithms.csv\")\n",
        "code = pd.read_csv(\"/content/gdrive/MyDrive/RSS/Data/code.csv\")"
      ],
      "metadata": {
        "id": "UQe_7U9zmPKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gkM5fw6ASbCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing methods for reading in latex"
      ],
      "metadata": {
        "id": "a6WSS85agxc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_class = transformers.DistilBertModel \n",
        "tokenizer_class = transformers.DistilBertTokenizer\n",
        "weights = 'distilbert-base-cased'\n",
        "\n",
        "tokenizer = tokenizer_class.from_pretrained(weights)\n",
        "model = model_class.from_pretrained(weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218,
          "referenced_widgets": [
            "cbd9f66494e74e619caeedb662d45d2d",
            "a3d1a901151344ec9e45ddc3204eabfc",
            "62221b88393e442da20abca70c330190",
            "6bbc13ddfd5e4f7eaa04d52ecf6f8331",
            "fdd104276d654cd4ae6a75e6c2fa15ea",
            "c6b6204739874340a54578ad9707483f",
            "524bcc0ae68146cdbe426d8afac80584",
            "b755bdff504e4dd0b4cef9b108c98db8",
            "78318016df8f403f9f3215a8b6048512",
            "bacc1b1b92fe45b39c5cb8d2db3a8388",
            "fe613d70045f40a18c2020f37695d25c",
            "ee86ab59c7ca4fd6af04ac1e46d5253a",
            "000eadd214694fc492b3549cff75bcf5",
            "f26a3a9226234725872eae236e430c4d",
            "c38cb9e9f2e945328567a074b70c761a",
            "d6b90ec7a5a6444a836a84f09ef5b6ed",
            "3acfd9aa07fa4a2c8ef89ac4aa268893",
            "93baea1861ca400791546952beba54de",
            "f6766332df1641d7ba625f8cd8107340",
            "7e4f13b343dc4840a2524f054037efff",
            "4cc315be84a94f28a885754bf1f1b7a9",
            "38c4e970147044f497f7fbc02f16567c",
            "44e1f71f00644438925860f930652820",
            "cc4cf24e549e4e0f9a5b188df34309c8",
            "990c71fba1b4499385f31e753deaec0f",
            "803dcd037b874c47834a5a8ccb15ad31",
            "a6692258a61c4fe6b57d290bdb3b46a2",
            "438eaf30587146b6a1ec6f3cea991fcd",
            "44a3b346c7084c0a85387481079fa3d3",
            "811b7d8618a244f59dcc7413cbce0982",
            "762da9a7c4ab4fc49e9d2d87000a9960",
            "5a03d2d5c5bb48c5949c691bb83b0660",
            "aecafd7246ec4e6fb281926ad1925b6c",
            "9fefb5e101b14584ac8ec7b8b3d56b66",
            "dbee662035ff4e9d81e53ce578c62887",
            "44cfb725e5cf47aeabfe21f045fffc53",
            "8df83a5348584916b75376a8eada88f0",
            "d52bbdc8fec342cea46b3f0a28fb51e7",
            "41280685adca42178c2e3e9a8bd681d0",
            "72ce80d1fc7e4f5a9fa8b41ca697920e",
            "471fb7328b3a42dc87c09f13c311de6f",
            "f67f219d481d46958b23933f519bc3e5",
            "dabd9627f8ec450c829a807f4b4105ac",
            "b3a5ddf3327145e49ee581d0fd44af47"
          ]
        },
        "id": "9RN3BbnwhWfK",
        "outputId": "f3d29117-7365-48c5-c32f-93e139569a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbd9f66494e74e619caeedb662d45d2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee86ab59c7ca4fd6af04ac1e46d5253a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44e1f71f00644438925860f930652820"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/251M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fefb5e101b14584ac8ec7b8b3d56b66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eqn_index = random.randint(0,10000)\n",
        "#bellman:\n",
        "# eqn_index = 3210\n",
        "\n",
        "\n",
        "print(f\"Example of equation index number {eqn_index}\")\n",
        "\n",
        "latex = eqns.iloc[eqn_index].eqn\n",
        "print(\"\")\n",
        "print(\"======\")\n",
        "print(\"Multistep\")\n",
        "print(\"======\")\n",
        "print(\"Raw latex below:\")\n",
        "print(\"\")\n",
        "print(latex)\n",
        "print(\"Converted Unicode below:\")\n",
        "uni_rep = LatexNodes2Text().latex_to_text(latex)\n",
        "print(uni_rep)\n",
        "print(\"Tokenization below:\")\n",
        "print(\"\")\n",
        "print(tokenizer(uni_rep,padding=\"max_length\",truncation=True))\n",
        "print(\"\")\n",
        "print(\"Inverted Tokenization\")\n",
        "print(\"\")\n",
        "print(tokenizer.decode(tokenizer(uni_rep)['input_ids']))\n",
        "\n",
        "print(\"======\")\n",
        "print(\"Single step\")\n",
        "print(\"======\")\n",
        "print(\"Inverted Tokenization\")\n",
        "print(\"\")\n",
        "print(tokenizer.decode(tokenizer(latex)['input_ids']))"
      ],
      "metadata": {
        "id": "vGiDynIxAh9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d286a6-5b6c-4578-b1da-d47b0d98fc60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of equation index number 4775\n",
            "\n",
            "======\n",
            "Multistep\n",
            "======\n",
            "Raw latex below:\n",
            "\n",
            "\\label{loss1}\n",
            "        \\mathcal{L}\n",
            "        =\n",
            "        \\frac{1}{p}\n",
            "        \\sum_{\\alpha=1}^p\n",
            "        \\left( \n",
            "        \\frac{s_\\alpha-\\theta_\\alpha}{\\theta_\\alpha}\n",
            "        \\right)^2\n",
            "        +\n",
            "        \\frac{1}{N}\n",
            "        \\sum_{i=1}^N\n",
            "        \\left(\n",
            "        \\frac{\\hat x_i-x_i}{x_i}\n",
            "        \\right)^2\n",
            "        \\,.\n",
            "    \n",
            "Converted Unicode below:\n",
            "\n",
            "        ℒ\n",
            "        =\n",
            "        1/p\n",
            "        ∑_α=1^p\n",
            "        ( \n",
            "        s_α-θ_α/θ_α\n",
            "        )^2\n",
            "        +\n",
            "        1/N\n",
            "        ∑_i=1^N\n",
            "        (\n",
            "        x̂_i-x_i/x_i\n",
            "        )^2\n",
            "         .\n",
            "    \n",
            "Tokenization below:\n",
            "\n",
            "{'input_ids': [101, 100, 134, 122, 120, 185, 100, 168, 418, 134, 122, 167, 185, 113, 188, 168, 418, 118, 425, 168, 418, 120, 425, 168, 418, 114, 167, 123, 116, 122, 120, 151, 100, 168, 178, 134, 122, 167, 151, 113, 100, 168, 178, 118, 193, 168, 178, 120, 193, 168, 178, 114, 167, 123, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
            "\n",
            "Inverted Tokenization\n",
            "\n",
            "[CLS] [UNK] = 1 / p [UNK] _ α = 1 ^ p ( s _ α - θ _ α / θ _ α ) ^ 2 + 1 / N [UNK] _ i = 1 ^ N ( [UNK] _ i - x _ i / x _ i ) ^ 2. [SEP]\n",
            "======\n",
            "Single step\n",
            "======\n",
            "Inverted Tokenization\n",
            "\n",
            "[CLS] \\ label { loss1 } \\ mathcal { L } = \\ frac { 1 } { p } \\ sum _ { \\ alpha = 1 } ^ p \\ left ( \\ frac { s _ \\ alpha - \\ theta _ \\ alpha } { \\ theta _ \\ alpha } \\ right ) ^ 2 + \\ frac { 1 } { N } \\ sum _ { i = 1 } ^ N \\ left ( \\ frac { \\ hat x _ i - x _ i } { x _ i } \\ right ) ^ 2 \\,. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appears to be how to work with tokenized latex..."
      ],
      "metadata": {
        "id": "NRWqBHfNsUL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model(**tokenizer(latex,return_tensors='pt',padding='max_length',truncation=True))[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JQtdoWxk8JN",
        "outputId": "c56995e4-da8e-42b4-88e7-ce6f146f64d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = tokenizer(latex,return_tensors='pt',padding='max_length',truncation=True)\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzfqXM8IoHr9",
        "outputId": "0c432079-d3a4-4d8f-b6b1-a9b40bf1f6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,   165,  3107,   196,  2445,  1475,   198,   165, 12523,  7867,\n",
              "           196,   149,   198,   134,   165,   175, 19366,   196,   122,   198,\n",
              "           196,   185,   198,   165,  7584,   168,   196,   165, 11164,   134,\n",
              "           122,   198,   167,   185,   165,  1286,   113,   165,   175, 19366,\n",
              "           196,   188,   168,   165, 11164,   118,   165,  1103,  1777,   168,\n",
              "           165, 11164,   198,   196,   165,  1103,  1777,   168,   165, 11164,\n",
              "           198,   165,  1268,   114,   167,   123,   116,   165,   175, 19366,\n",
              "           196,   122,   198,   196,   151,   198,   165,  7584,   168,   196,\n",
              "           178,   134,   122,   198,   167,   151,   165,  1286,   113,   165,\n",
              "           175, 19366,   196,   165,  6131,   193,   168,   178,   118,   193,\n",
              "           168,   178,   198,   196,   193,   168,   178,   198,   165,  1268,\n",
              "           114,   167,   123,   165,   117,   119,   102,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gdrive/MyDrive/RSS/Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzLHThU1Ci3n",
        "outputId": "a00a5a0d-af4e-4c1a-aae5-3ce2b4aa85df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "algorithms.csv\tcode.csv\t  code-small.csv  func-rep-small.csv\n",
            "code-all.csv\tcode-func-rep.pt  equations.csv   func-rep-small.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "func_rep_df = pd.read_pickle('gdrive/MyDrive/RSS/Data/code-func-rep.pt')"
      ],
      "metadata": {
        "id": "GFgpn_f9C1-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_rep_df = func_rep_df[func_rep_df.functions.apply(len) != 0]\n",
        "func_rep_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "MmYp0Fo8Jgh6",
        "outputId": "8853d0ce-73f4-4c64-f40a-cc8ecd3ac202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               id                                          functions  \\\n",
              "0      1811.02182  [def main ( config ) : NEW_LINE INDENT from da...   \n",
              "1      1811.02182  [def check_config_used ( config , target_sourc...   \n",
              "2      1811.02182  [def __init__ ( self , batch_size , paired = F...   \n",
              "3      1811.02182  [def weights_init ( m ) : NEW_LINE INDENT clas...   \n",
              "4      1811.02182  [def weights_init ( m ) : NEW_LINE INDENT clas...   \n",
              "...           ...                                                ...   \n",
              "29193  2112.01073  [def my_lcs ( string , sub ) : NEW_LINE INDENT...   \n",
              "29194  2112.01073  [def __init__ ( self , n = 4 ) : NEW_LINE INDE...   \n",
              "29195  2112.01073  [def precook ( s , n = 4 , out = False ) : NEW...   \n",
              "29197  2112.01073  [def __init__ ( self ) : NEW_LINE INDENT self ...   \n",
              "29199  2112.01073  [def tokenize ( self , captions_for_image ) : ...   \n",
              "\n",
              "                                         representations  \n",
              "0      [[[tensor(0.6232), tensor(-0.2805), tensor(0.3...  \n",
              "1      [[[tensor(0.6108), tensor(-0.2560), tensor(0.1...  \n",
              "2      [[[tensor(0.8430), tensor(0.1034), tensor(0.30...  \n",
              "3      [[[tensor(0.6790), tensor(-0.0221), tensor(0.1...  \n",
              "4      [[[tensor(0.6790), tensor(-0.0221), tensor(0.1...  \n",
              "...                                                  ...  \n",
              "29193  [[[tensor(0.6292), tensor(0.0059), tensor(0.09...  \n",
              "29194  [[[tensor(0.4932), tensor(-0.1197), tensor(-0....  \n",
              "29195  [[[tensor(0.7558), tensor(-0.0280), tensor(0.0...  \n",
              "29197  [[[tensor(0.7251), tensor(-0.0360), tensor(0.0...  \n",
              "29199  [[[tensor(0.8905), tensor(-0.4485), tensor(0.2...  \n",
              "\n",
              "[23814 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ecf09ea1-d604-480b-aa97-b70fbfb7531d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>functions</th>\n",
              "      <th>representations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def main ( config ) : NEW_LINE INDENT from da...</td>\n",
              "      <td>[[[tensor(0.6232), tensor(-0.2805), tensor(0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def check_config_used ( config , target_sourc...</td>\n",
              "      <td>[[[tensor(0.6108), tensor(-0.2560), tensor(0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def __init__ ( self , batch_size , paired = F...</td>\n",
              "      <td>[[[tensor(0.8430), tensor(0.1034), tensor(0.30...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def weights_init ( m ) : NEW_LINE INDENT clas...</td>\n",
              "      <td>[[[tensor(0.6790), tensor(-0.0221), tensor(0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def weights_init ( m ) : NEW_LINE INDENT clas...</td>\n",
              "      <td>[[[tensor(0.6790), tensor(-0.0221), tensor(0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29193</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def my_lcs ( string , sub ) : NEW_LINE INDENT...</td>\n",
              "      <td>[[[tensor(0.6292), tensor(0.0059), tensor(0.09...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29194</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def __init__ ( self , n = 4 ) : NEW_LINE INDE...</td>\n",
              "      <td>[[[tensor(0.4932), tensor(-0.1197), tensor(-0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29195</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def precook ( s , n = 4 , out = False ) : NEW...</td>\n",
              "      <td>[[[tensor(0.7558), tensor(-0.0280), tensor(0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29197</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def __init__ ( self ) : NEW_LINE INDENT self ...</td>\n",
              "      <td>[[[tensor(0.7251), tensor(-0.0360), tensor(0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29199</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def tokenize ( self , captions_for_image ) : ...</td>\n",
              "      <td>[[[tensor(0.8905), tensor(-0.4485), tensor(0.2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23814 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecf09ea1-d604-480b-aa97-b70fbfb7531d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ecf09ea1-d604-480b-aa97-b70fbfb7531d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ecf09ea1-d604-480b-aa97-b70fbfb7531d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eqns['id'] = eqns.id.astype(str)\n",
        "func_rep_df['id'] = func_rep_df['id'].astype(str)\n",
        "collected_func_reps = func_rep_df.groupby(by='id').sum()"
      ],
      "metadata": {
        "id": "ujFld01NTNx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gbWd5VBeT74V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demofuncs = collected_func_reps[collected_func_reps.index=='1811.02182']\n",
        "demofuncs.functions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P73AE7lXZpK",
        "outputId": "400ffb2b-2769-45cd-8e48-ff2166d10de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "1811.02182    [def main ( config ) : NEW_LINE INDENT from da...\n",
              "Name: functions, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demofuncs.functions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woNBKrDPYgGl",
        "outputId": "a335ef50-f2a3-4f88-d660-495162545c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"def main ( config ) : NEW_LINE INDENT from data_loader import DataLoader NEW_LINE if ( config . trainer == ' minimize _ DCE ' ) : NEW_LINE INDENT from trainer_DCE import Trainer NEW_LINE paired = True NEW_LINE DEDENT elif ( config . trainer == ' acoustic _ supervision ' ) : NEW_LINE INDENT from trainer_acoustic import Trainer NEW_LINE paired = False NEW_LINE DEDENT elif ( config . trainer == ' AAS ' ) : NEW_LINE INDENT from trainer_AAS import Trainer NEW_LINE paired = False NEW_LINE DEDENT elif ( config . trainer == ' FSEGAN ' ) : NEW_LINE INDENT from trainer_FSEGAN import Trainer NEW_LINE paired = True NEW_LINE DEDENT if config . gpu >= 0 : NEW_LINE INDENT torch . cuda . manual_seed ( config . random_seed ) NEW_LINE torch . cuda . set_device ( config . gpu ) NEW_LINE DEDENT if ( config . DB_name == ' librispeech ' ) : NEW_LINE INDENT if ( paired ) : NEW_LINE INDENT config . tr_ny_manifest = ' data / libri _ tr _ ny _ paired . csv ' NEW_LINE config . trsub_manifest = ' data / libri _ trsub _ ny _ paired . csv ' NEW_LINE config . val_manifest = ' data / libri _ val _ paired . csv ' NEW_LINE DEDENT else : NEW_LINE INDENT config . tr_ny_manifest = ' data / libri _ tr _ ny . csv ' NEW_LINE config . trsub_manifest = ' data / libri _ trsub _ ny . csv ' NEW_LINE config . val_manifest = ' data / libri _ val . csv ' NEW_LINE DEDENT config . tr_cl_manifest = ' data / libri _ tr _ cl . csv ' NEW_LINE DEDENT elif ( config . DB_name == ' chime ' ) : NEW_LINE INDENT if ( paired ) : NEW_LINE INDENT config . tr_ny_manifest = ' data / chime _ ' + config . simul_real + ' _ tr _ ny _ paired . csv ' NEW_LINE config . trsub_manifest = ' data / chime _ ' + config . simul_real + ' _ trsub _ ny _ paired . csv ' NEW_LINE config . val_manifest = ' data / chime _ real _ val _ paired . csv ' NEW_LINE confnig . val2_manifest = ' data / chime _ simul _ val _ paired . csv ' NEW_LINE DEDENT else : NEW_LINE INDENT config . tr_ny_manifest = ' data / chime _ ' + config . simul_real + ' _ tr _ ny . csv ' NEW_LINE config . trsub_manifest = ' data / chime _ ' + config . simul_real + ' _ trsub _ ny . csv ' NEW_LINE config . val_manifest = ' data / chime _ real _ val . csv ' NEW_LINE confnig . val2_manifest = ' data / chime _ simul _ val . csv ' NEW_LINE DEDENT config . tr_cl_manifest = ' data / chime _ tr _ org . csv ' NEW_LINE DEDENT with open ( config . labels_path ) as label_file : NEW_LINE INDENT labels = str ( ' ' . join ( json . load ( label_file ) ) ) NEW_LINE DEDENT data_loader = DataLoader ( batch_size = config . batch_size , paired = paired , tr_cl_manifest = config . tr_cl_manifest , tr_ny_manifest = config . tr_ny_manifest , trsub_manifest = config . trsub_manifest , val_manifest = config . val_manifest , val2_manifest = config . val2_manifest , labels = labels ) NEW_LINE if not os . path . exists ( ' logs / ' + str ( config . expnum ) ) : NEW_LINE INDENT os . makedirs ( ' logs / ' + str ( config . expnum ) ) NEW_LINE DEDENT trainer = Trainer ( config , data_loader ) NEW_LINE torch . manual_seed ( config . random_seed ) NEW_LINE if ( config . mode == ' train ' ) : NEW_LINE INDENT trainer . train ( ) NEW_LINE DEDENT elif ( config . mode == ' test ' ) : NEW_LINE INDENT trainer . test ( ) NEW_LINE DEDENT elif ( config . mode == ' visualize ' ) : NEW_LINE INDENT trainer . visualize ( ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def check_config_used ( config , target_source ) : NEW_LINE INDENT config_count = dict ( vars ( config ) ) NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT config_count [ k ] = 0 NEW_LINE DEDENT for source in target_source : NEW_LINE INDENT fp = open ( source , ' r ' ) NEW_LINE text = fp . read ( ) NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT if ( text . find ( k ) >= 0 ) : NEW_LINE INDENT config_count [ k ] = 1 NEW_LINE DEDENT DEDENT fp . close ( ) NEW_LINE DEDENT config_unused = [ ] NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT if ( config_count [ k ] == 0 ) : NEW_LINE INDENT config_unused . append ( k ) NEW_LINE DEDENT DEDENT print ( ' unused ▁ config ▁ = ▁ ' ) NEW_LINE print ( config_unused ) NEW_LINE assert ( len ( config_unused ) == 0 ) , ' unused ▁ config ▁ exists , ▁ please ▁ properly ▁ use ▁ it ▁ or ▁ comment ▁ it ' NEW_LINE DEDENT\",\n",
              " 'def to_np ( x ) : NEW_LINE INDENT return x . data . cpu ( ) . numpy ( ) NEW_LINE DEDENT',\n",
              " \"def get_weight_statistic ( M ) : NEW_LINE INDENT print ( ' Model ▁ parameter ▁ statistic ' ) NEW_LINE modules = list ( M . modules ( ) ) [ 0 ] . _modules NEW_LINE for k , v in modules . items ( ) : NEW_LINE INDENT if ( len ( v . state_dict ( ) ) > 2 ) : NEW_LINE INDENT for l in range ( len ( v ) ) : NEW_LINE INDENT layer = v [ l ] NEW_LINE if ( hasattr ( layer , ' module ' ) ) : NEW_LINE INDENT layer_m = layer . module NEW_LINE for j in range ( len ( layer_m ) ) : NEW_LINE INDENT sublayer = layer_m [ j ] NEW_LINE if ( hasattr ( sublayer , ' bias ' ) ) : NEW_LINE INDENT if ( sublayer . bias is not None ) : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( sublayer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT else : NEW_LINE INDENT if ( hasattr ( layer , ' weight ' ) ) : NEW_LINE INDENT if ( hasattr ( layer , ' bias ' ) ) : NEW_LINE INDENT if ( hasattr ( layer . bias , ' data ' ) ) : NEW_LINE INDENT print ( str ( layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( layer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT print ( str ( layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT if ( hasattr ( layer , ' rnn ' ) ) : NEW_LINE INDENT rnn_layer = layer . rnn NEW_LINE print ( str ( rnn_layer ) ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0 . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0 . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0 . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 _ reverse ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0_reverse . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ ih _ l0 ▁ = ▁ ' + str ( rnn_layer . weight_ih_l0 . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_ih_l0 . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_ih_l0 . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 _ reverse ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0_reverse . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT if ( hasattr ( layer , ' batch _ norm ' ) ) : NEW_LINE INDENT if ( layer . batch_norm ) : NEW_LINE INDENT bn_layer = layer . batch_norm . module NEW_LINE print ( str ( bn_layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( bn_layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( bn_layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( bn_layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) , ▁ bias ▁ = ▁ ' + str ( bn_layer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( bn_layer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( bn_layer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT DEDENT DEDENT else : NEW_LINE INDENT if ( hasattr ( v , ' weight ' ) ) : NEW_LINE INDENT if ( hasattr ( v , ' bias ' ) ) : NEW_LINE INDENT print ( k + ' ▁ : ▁ weight ▁ = ▁ ' + str ( v . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( v . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( k + ' ▁ : ▁ weight ▁ = ▁ ' + str ( v . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT print ( ' ▁ ' ) NEW_LINE print ( ' ▁ ' ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 and classname . find ( ' ConvResidualBlock ' ) == - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.1 ) NEW_LINE if hasattr ( m , ' bias ' ) : NEW_LINE INDENT if ( hasattr ( m . bias , ' data ' ) ) : NEW_LINE INDENT m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT DEDENT DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def _get_variable ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def _get_variable_volatile ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) , volatile = True ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs , volatile = True ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def _get_variable_nograd ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) , requires_grad = False ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs , requires_grad = False ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def __init__ ( self ) : NEW_LINE INDENT self . reset ( ) NEW_LINE DEDENT',\n",
              " 'def reset ( self ) : NEW_LINE INDENT self . val = 0 NEW_LINE self . avg = 0 NEW_LINE self . sum = 0 NEW_LINE self . count = 0 NEW_LINE DEDENT',\n",
              " 'def update ( self , val , n = 1 ) : NEW_LINE INDENT self . val = val NEW_LINE self . sum += val * n NEW_LINE self . count += n NEW_LINE self . avg = self . sum / self . count NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , batch_size , paired = False , tr_cl_manifest = \" \" , tr_ny_manifest = \" \" , trsub_manifest = \" \" , val_manifest = \" \" , val2_manifest = \" \" , labels = None ) : NEW_LINE INDENT self . batch_size = batch_size NEW_LINE self . labels = labels NEW_LINE if ( paired ) : NEW_LINE INDENT self . Loader = FeatLoader_paired NEW_LINE DEDENT else : NEW_LINE INDENT self . Loader = FeatLoader NEW_LINE DEDENT if ( len ( tr_cl_manifest ) > 0 ) : NEW_LINE INDENT self . tr_cl_ds = FeatDataset ( manifest = tr_cl_manifest , labels = labels ) NEW_LINE self . tr_cl_sp = FeatSampler ( self . tr_cl_ds , batch_size = batch_size ) NEW_LINE self . tr_cl_dl = iter ( self . Loader ( self . tr_cl_ds , num_workers = 1 , batch_sampler = self . tr_cl_sp ) ) NEW_LINE DEDENT if ( len ( tr_ny_manifest ) > 0 ) : NEW_LINE INDENT self . tr_ny_ds = FeatDataset ( manifest = tr_ny_manifest , labels = labels ) NEW_LINE self . tr_ny_sp = FeatSampler ( self . tr_ny_ds , batch_size = batch_size ) NEW_LINE self . tr_ny_dl = iter ( self . Loader ( self . tr_ny_ds , num_workers = 1 , batch_sampler = self . tr_ny_sp ) ) NEW_LINE DEDENT if ( len ( trsub_manifest ) > 0 ) : NEW_LINE INDENT self . trsub_ds = FeatDataset ( manifest = trsub_manifest , labels = labels ) NEW_LINE self . trsub_dl = iter ( self . Loader ( self . trsub_ds , batch_size = batch_size ) ) NEW_LINE DEDENT if ( len ( val_manifest ) > 0 ) : NEW_LINE INDENT self . val_ds = FeatDataset ( manifest = val_manifest , labels = labels ) NEW_LINE self . val_dl = iter ( self . Loader ( self . val_ds , batch_size = batch_size ) ) NEW_LINE DEDENT if ( len ( val2_manifest ) > 0 ) : NEW_LINE INDENT self . val2_ds = FeatDataset ( manifest = val2_manifest , labels = labels ) NEW_LINE self . val2_dl = iter ( self . Loader ( self . val2_ds , batch_size = batch_size ) ) NEW_LINE DEDENT DEDENT',\n",
              " \"def next ( self , cl_ny = ' ' , type = ' ' ) : NEW_LINE INDENT if ( cl_ny == ' ny ' ) : NEW_LINE INDENT if ( type == ' train ' ) : NEW_LINE INDENT loader = self . tr_ny_dl NEW_LINE DEDENT elif ( type == ' trsub ' ) : NEW_LINE INDENT loader = self . trsub_dl NEW_LINE DEDENT elif ( type == ' val ' ) : NEW_LINE INDENT loader = self . val_dl NEW_LINE DEDENT elif ( type == ' val2' ) : NEW_LINE INDENT loader = self . val2_dl NEW_LINE DEDENT DEDENT elif ( cl_ny == ' cl ' ) : NEW_LINE INDENT if ( type == ' train ' ) : NEW_LINE INDENT loader = self . tr_cl_dl NEW_LINE DEDENT DEDENT try : NEW_LINE INDENT data_list = loader . next ( ) NEW_LINE DEDENT except StopIteration : NEW_LINE INDENT if ( cl_ny == ' ny ' ) : NEW_LINE INDENT if ( type == ' train ' ) : NEW_LINE INDENT self . tr_ny_sp . shuffle ( ) NEW_LINE self . tr_ny_dl = iter ( self . Loader ( self . tr_ny_ds , num_workers = 1 , batch_sampler = self . tr_ny_sp ) ) NEW_LINE loader = self . tr_ny_dl NEW_LINE DEDENT elif ( type == ' trsub ' ) : NEW_LINE INDENT self . trsub_dl = iter ( self . Loader ( self . trsub_ds , batch_size = self . batch_size , num_workers = 1 ) ) NEW_LINE loader = self . trsub_dl NEW_LINE DEDENT elif ( type == ' val ' ) : NEW_LINE INDENT self . val_dl = iter ( self . Loader ( self . val_ds , batch_size = self . batch_size , num_workers = 1 ) ) NEW_LINE loader = self . val_dl NEW_LINE DEDENT elif ( type == ' val2' ) : NEW_LINE INDENT self . val2_dl = iter ( self . Loader ( self . val2_ds , batch_size = self . batch_size , num_workers = 1 ) ) NEW_LINE loader = self . val2_dl NEW_LINE loader = self . te_dl NEW_LINE DEDENT DEDENT elif ( cl_ny == ' cl ' ) : NEW_LINE INDENT if ( type == ' train ' ) : NEW_LINE INDENT self . tr_cl_sp . shuffle ( ) NEW_LINE self . tr_cl_dl = iter ( self . Loader ( self . tr_cl_ds , num_workers = 1 , batch_sampler = self . tr_cl_sp ) ) NEW_LINE loader = self . tr_cl_dl NEW_LINE DEDENT DEDENT data_list = loader . next ( ) NEW_LINE DEDENT return data_list NEW_LINE DEDENT\",\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def __init__ ( self , config , data_loader = None ) : NEW_LINE INDENT self . config = config NEW_LINE self . data_loader = data_loader NEW_LINE self . lr = config . lr NEW_LINE self . beta1 = config . beta1 NEW_LINE self . beta2 = config . beta2 NEW_LINE self . optimizer = config . optimizer NEW_LINE self . batch_size = config . batch_size NEW_LINE self . diffLoss = L1Loss_mask ( ) NEW_LINE self . valmin_iter = 0 NEW_LINE self . model_dir = ' logs / ' + str ( config . expnum ) NEW_LINE self . savename_G = ' ' NEW_LINE self . decoder = GreedyDecoder ( data_loader . labels ) NEW_LINE self . kt = 0 NEW_LINE self . lb = 0.001 NEW_LINE self . conv_measure = 0 NEW_LINE self . dce_tr = AverageMeter ( ) NEW_LINE self . dce_val = AverageMeter ( ) NEW_LINE self . wer_tr = AverageMeter ( ) NEW_LINE self . cer_tr = AverageMeter ( ) NEW_LINE self . wer_val = AverageMeter ( ) NEW_LINE self . cer_val = AverageMeter ( ) NEW_LINE self . build_model ( ) NEW_LINE self . G . loss_stop = 100000 NEW_LINE if self . config . gpu >= 0 : NEW_LINE INDENT self . G . cuda ( ) NEW_LINE self . ASR . cuda ( ) NEW_LINE DEDENT if len ( self . config . load_path ) > 0 : NEW_LINE INDENT self . load_model ( ) NEW_LINE DEDENT if config . mode == ' train ' : NEW_LINE INDENT self . logFile = open ( self . model_dir + ' / log . txt ' , ' w ' ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def zero_grad_all ( self ) : NEW_LINE INDENT self . G . zero_grad ( ) NEW_LINE DEDENT',\n",
              " \"def build_model ( self ) : NEW_LINE INDENT print ( ' initialize ▁ enhancement ▁ model ' ) NEW_LINE self . G = stackedBRNN ( I = self . config . nFeat , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE print ( ' load ▁ pre - trained ▁ ASR ▁ model ' ) NEW_LINE package_ASR = torch . load ( self . config . ASR_path , map_location = lambda storage , loc : storage ) NEW_LINE self . ASR = DeepSpeech . load_model_package ( package_ASR ) NEW_LINE DEDENT\",\n",
              " 'def load_model ( self ) : NEW_LINE INDENT print ( \" [ * ] ▁ Load ▁ models ▁ from ▁ { } . . . \" . format ( self . load_path ) ) NEW_LINE postfix = \\' _ valmin \\' NEW_LINE paths = glob ( os . path . join ( self . load_path , \\' G { } * . pth \\' . format ( postfix ) ) ) NEW_LINE paths . sort ( ) NEW_LINE if len ( paths ) == 0 : NEW_LINE INDENT print ( \" [ ! ] ▁ No ▁ checkpoint ▁ found ▁ in ▁ { } . . . \" . format ( self . load_path ) ) NEW_LINE assert ( 0 ) , \\' checkpoint ▁ not ▁ avilable \\' NEW_LINE DEDENT idxes = [ int ( os . path . basename ( path . split ( \\' . \\' ) [ 0 ] . split ( \\' _ \\' ) [ - 1 ] ) ) for path in paths ] NEW_LINE if self . config . start_iter < 0 : NEW_LINE INDENT self . config . start_iter = max ( idxes ) NEW_LINE if ( self . config . start_iter < 0 ) : NEW_LINE INDENT raise Exception ( \" start ▁ iter ▁ is ▁ still ▁ less ▁ than ▁ 0 ▁ - - > ▁ probably ▁ try ▁ to ▁ load ▁ initial ▁ random ▁ model \" ) NEW_LINE DEDENT DEDENT if self . config . gpu < 0 : NEW_LINE INDENT map_location = lambda storage , loc : storage NEW_LINE DEDENT else : NEW_LINE INDENT map_location = None NEW_LINE DEDENT print ( \\' Load ▁ models ▁ from ▁ \\' + self . load_path + \\' , ▁ ITERATION ▁ = ▁ \\' + str ( self . config . start_iter ) ) NEW_LINE self . G . load_state_dict ( torch . load ( \\' { } / G { } _ { } . pth \\' . format ( self . load_path [ : - 1 ] , postfix , self . config . start_iter ) , map_location = map_location ) ) NEW_LINE print ( \" [ * ] ▁ Model ▁ loaded \" ) NEW_LINE DEDENT',\n",
              " 'def train ( self ) : NEW_LINE INDENT optimizer_g = torch . optim . Adam ( self . G . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE for iter in trange ( self . config . start_iter , self . config . max_iter ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' train \\' ) NEW_LINE inputs , cleans , mask = _get_variable_nograd ( data_list [ 0 ] ) , _get_variable_nograd ( data_list [ 1 ] ) , _get_variable_nograd ( data_list [ 2 ] ) NEW_LINE outputs = self . G ( inputs ) NEW_LINE dce , nElement = self . diffLoss ( outputs , cleans , mask ) NEW_LINE self . zero_grad_all ( ) NEW_LINE dce . backward ( ) NEW_LINE optimizer_g . step ( ) NEW_LINE if ( iter + 1 ) % self . config . log_iter == 0 : NEW_LINE INDENT str_loss = \" [ { } / { } ] ▁ ( train ) ▁ DCE : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , dce . data [ 0 ] ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE DEDENT if ( iter + 1 ) % self . config . save_iter == 0 : NEW_LINE INDENT self . G . eval ( ) NEW_LINE self . dce_tr . reset ( ) NEW_LINE self . wer_tr . reset ( ) NEW_LINE self . cer_tr . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . trsub_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' trsub \\' ) NEW_LINE inputs , cleans , mask , targets , input_percentages , target_sizes = _get_variable_volatile ( data_list [ 0 ] ) , _get_variable_volatile ( data_list [ 1 ] ) , _get_variable_volatile ( data_list [ 2 ] ) , data_list [ 3 ] , data_list [ 4 ] , data_list [ 5 ] NEW_LINE outputs = self . G ( inputs ) NEW_LINE dce , nElement = self . diffLoss ( outputs , cleans , mask ) NEW_LINE self . dce_tr . update ( dce . data [ 0 ] , nElement ) NEW_LINE wer , cer , nWord , nChar = self . greedy_decoding ( inputs , targets , input_percentages , target_sizes ) NEW_LINE self . wer_tr . update ( wer , nWord ) NEW_LINE self . cer_tr . update ( cer , nChar ) NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( training ▁ subset ) ▁ DCE : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . dce_tr . avg ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE str_loss = \" [ { } / { } ] ▁ ( training ▁ subset ) ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . wer_tr . avg * 100 , self . cer_tr . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . dce_val . reset ( ) NEW_LINE self . wer_val . reset ( ) NEW_LINE self . cer_val . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . val_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' val \\' ) NEW_LINE inputs , cleans , mask , targets , input_percentages , target_sizes = _get_variable_volatile ( data_list [ 0 ] ) , _get_variable_volatile ( data_list [ 1 ] ) , _get_variable_volatile ( data_list [ 2 ] ) , data_list [ 3 ] , data_list [ 4 ] , data_list [ 5 ] NEW_LINE outputs = self . G ( inputs ) NEW_LINE dce , nElement = self . diffLoss ( outputs , cleans , mask ) NEW_LINE self . dce_val . update ( dce . data [ 0 ] , nElement ) NEW_LINE wer , cer , nWord , nChar = self . greedy_decoding ( inputs , targets , input_percentages , target_sizes ) NEW_LINE self . wer_val . update ( wer , nWord ) NEW_LINE self . cer_val . update ( cer , nChar ) NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( validation ) ▁ DCE : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . dce_val . avg ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE str_loss = \" [ { } / { } ] ▁ ( validation ) ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . wer_val . avg * 100 , self . cer_val . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . G . train ( ) NEW_LINE self . logFile . flush ( ) NEW_LINE if ( len ( self . savename_G ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_G ) : NEW_LINE INDENT os . remove ( self . savename_G ) NEW_LINE DEDENT DEDENT self . savename_G = \\' { } / G _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . G . state_dict ( ) , self . savename_G ) NEW_LINE if ( self . G . loss_stop > self . wer_val . avg ) : NEW_LINE INDENT self . G . loss_stop = self . wer_val . avg NEW_LINE savename_G_valmin_prev = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_G_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_G_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_G_valmin = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_G , savename_G_valmin ) NEW_LINE self . valmin_iter = iter NEW_LINE DEDENT DEDENT DEDENT DEDENT',\n",
              " \"def greedy_decoding ( self , inputs , targets , input_percentages , target_sizes , transcript_prob = 0.001 ) : NEW_LINE INDENT split_targets = [ ] NEW_LINE offset = 0 NEW_LINE for size in target_sizes : NEW_LINE INDENT split_targets . append ( targets [ offset : offset + size ] ) NEW_LINE offset += size NEW_LINE DEDENT enhanced = self . G ( inputs ) NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = input_percentages . mul_ ( int ( T ) ) . int ( ) NEW_LINE decoded_output , _ = self . decoder . decode ( prob . data , sizes ) NEW_LINE target_strings = self . decoder . convert_to_strings ( split_targets ) NEW_LINE we , ce , total_word , total_char = 0 , 0 , 0 , 0 NEW_LINE for x in range ( len ( target_strings ) ) : NEW_LINE INDENT decoding , reference = decoded_output [ x ] [ 0 ] , target_strings [ x ] [ 0 ] NEW_LINE nChar = len ( reference ) NEW_LINE nWord = len ( reference . split ( ) ) NEW_LINE we_i = self . decoder . wer ( decoding , reference ) NEW_LINE ce_i = self . decoder . cer ( decoding , reference ) NEW_LINE we += we_i NEW_LINE ce += ce_i NEW_LINE total_word += nWord NEW_LINE total_char += nChar NEW_LINE if ( random . uniform ( 0 , 1 ) < transcript_prob ) : NEW_LINE INDENT print ( ' reference ▁ = ▁ ' + reference ) NEW_LINE print ( ' decoding ▁ = ▁ ' + decoding ) NEW_LINE print ( ' wer ▁ = ▁ ' + str ( we_i / float ( nWord ) ) + ' , ▁ cer ▁ = ▁ ' + str ( ce_i / float ( nChar ) ) ) NEW_LINE DEDENT DEDENT wer = we / total_word NEW_LINE cer = ce / total_word NEW_LINE return wer , cer , total_word , total_char NEW_LINE DEDENT\",\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def __init__ ( self , config , data_loader = None ) : NEW_LINE INDENT self . config = config NEW_LINE self . data_loader = data_loader NEW_LINE self . lr = config . lr NEW_LINE self . beta1 = config . beta1 NEW_LINE self . beta2 = config . beta2 NEW_LINE self . optimizer = config . optimizer NEW_LINE self . batch_size = config . batch_size NEW_LINE self . diffLoss = L1Loss_mask ( ) NEW_LINE self . valmin_iter = 0 NEW_LINE self . model_dir = ' logs / ' + str ( config . expnum ) NEW_LINE self . savename_G = ' ' NEW_LINE self . savename_D = ' ' NEW_LINE self . savename_ASR = ' ' NEW_LINE self . kt = 0 NEW_LINE self . lb = self . config . lambda_k NEW_LINE self . gamma = self . config . gamma NEW_LINE self . conv_measure = 0 NEW_LINE self . ctc_tr = AverageMeter ( ) NEW_LINE self . ctc_tr_local = AverageMeter ( ) NEW_LINE self . ctc_val = AverageMeter ( ) NEW_LINE self . adv_ny_tr = AverageMeter ( ) NEW_LINE self . adv_ny_val = AverageMeter ( ) NEW_LINE self . wer_tr = AverageMeter ( ) NEW_LINE self . wer_val = AverageMeter ( ) NEW_LINE self . cer_tr = AverageMeter ( ) NEW_LINE self . cer_val = AverageMeter ( ) NEW_LINE self . CTCLoss = CTCLoss ( ) NEW_LINE self . decoder = GreedyDecoder ( data_loader . labels ) NEW_LINE self . build_model ( ) NEW_LINE self . G . loss_stop = 100000 NEW_LINE if self . config . gpu >= 0 : NEW_LINE INDENT self . G . cuda ( ) NEW_LINE self . D . cuda ( ) NEW_LINE self . diffLoss . cuda ( ) NEW_LINE self . ASR . cuda ( ) NEW_LINE DEDENT if len ( self . config . load_path ) > 0 : NEW_LINE INDENT self . load_model ( ) NEW_LINE DEDENT if config . mode == ' train ' : NEW_LINE INDENT self . logFile = open ( self . model_dir + ' / log . txt ' , ' w ' ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def zero_grad_all ( self ) : NEW_LINE INDENT self . G . zero_grad ( ) NEW_LINE self . D . zero_grad ( ) NEW_LINE self . ASR . zero_grad ( ) NEW_LINE DEDENT',\n",
              " \"def build_model ( self ) : NEW_LINE INDENT print ( ' initialize ▁ enhancement ▁ & ▁ discriminator ▁ model ' ) NEW_LINE self . G = stackedBRNN ( I = self . config . nFeat , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE self . D = stackedBRNN ( I = self . config . nFeat , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE print ( ' load ▁ pre - trained ▁ ASR ▁ model ' ) NEW_LINE package_ASR = torch . load ( self . config . ASR_path , map_location = lambda storage , loc : storage ) NEW_LINE self . ASR = DeepSpeech . load_model_package ( package_ASR ) NEW_LINE DEDENT\",\n",
              " 'def load_model ( self ) : NEW_LINE INDENT print ( \" [ * ] ▁ Load ▁ models ▁ from ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE postfix = \\' _ valmin \\' NEW_LINE paths = glob ( os . path . join ( self . config . load_path , \\' G { } * . pth \\' . format ( postfix ) ) ) NEW_LINE paths . sort ( ) NEW_LINE if len ( paths ) == 0 : NEW_LINE INDENT print ( \" [ ! ] ▁ No ▁ checkpoint ▁ found ▁ in ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE assert ( 0 ) , \\' checkpoint ▁ not ▁ avilable \\' NEW_LINE DEDENT idxes = [ int ( os . path . basename ( path . split ( \\' . \\' ) [ 0 ] . split ( \\' _ \\' ) [ - 1 ] ) ) for path in paths ] NEW_LINE if self . config . start_iter <= 0 : NEW_LINE INDENT self . config . start_iter = max ( idxes ) NEW_LINE if ( self . config . start_iter < 0 ) : NEW_LINE INDENT raise Exception ( \" start ▁ iter ▁ is ▁ still ▁ less ▁ than ▁ 0 ▁ - - > ▁ probably ▁ try ▁ to ▁ load ▁ initial ▁ random ▁ model \" ) NEW_LINE DEDENT DEDENT if self . config . gpu < 0 : NEW_LINE INDENT map_location = lambda storage , loc : storage NEW_LINE DEDENT else : NEW_LINE INDENT map_location = None NEW_LINE DEDENT print ( \\' Load ▁ models ▁ from ▁ \\' + self . config . load_path + \\' , ▁ ITERATION ▁ = ▁ \\' + str ( self . config . start_iter ) ) NEW_LINE self . G . load_state_dict ( torch . load ( \\' { } / G { } _ { } . pth \\' . format ( self . config . load_path , postfix , self . config . start_iter ) , map_location = map_location ) ) NEW_LINE print ( \" [ * ] ▁ Model ▁ loaded \" ) NEW_LINE DEDENT',\n",
              " 'def train ( self ) : NEW_LINE INDENT optimizer_g = torch . optim . Adam ( self . G . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE optimizer_asr = torch . optim . Adam ( self . ASR . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE optimizer_d = torch . optim . Adam ( self . D . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE for iter in trange ( self . config . start_iter , self . config . max_iter ) : NEW_LINE INDENT self . zero_grad_all ( ) NEW_LINE data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' train \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes , mask = _get_variable_nograd ( data_list [ 0 ] ) , _get_variable_nograd ( data_list [ 1 ] , cuda = False ) , data_list [ 2 ] , _get_variable_nograd ( data_list [ 3 ] , cuda = False ) , _get_variable_nograd ( data_list [ 4 ] ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE enhanced = self . G ( inputs ) NEW_LINE enhanced_D = enhanced . detach ( ) NEW_LINE ae_ny_G = self . D ( enhanced ) NEW_LINE l_adv_ny_G , _ = self . diffLoss ( ae_ny_G , enhanced , mask ) NEW_LINE l_adv_ny_G = l_adv_ny_G * self . config . w_adversarial NEW_LINE l_adv_ny_G_data = l_adv_ny_G . data [ 0 ] NEW_LINE l_adv_ny_G . backward ( retain_graph = True ) NEW_LINE g_adv = self . get_gradient_norm ( self . G ) NEW_LINE self . D . zero_grad ( ) NEW_LINE del l_adv_ny_G NEW_LINE ae_ny_D = self . D ( enhanced_D ) NEW_LINE l_adv_ny_D , _ = self . diffLoss ( ae_ny_D , enhanced_D , mask ) NEW_LINE l_adv_ny_D = l_adv_ny_D * ( - self . kt ) * self . config . w_adversarial NEW_LINE l_adv_ny_D . backward ( ) NEW_LINE del l_adv_ny_D NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = _get_variable_nograd ( input_percentages . mul_ ( int ( T ) ) . int ( ) , cuda = False ) NEW_LINE l_CTC = self . config . w_acoustic * self . CTCLoss ( prob , targets , sizes , target_sizes ) / N NEW_LINE self . ctc_tr_local . update ( l_CTC . data [ 0 ] , N ) NEW_LINE l_CTC . backward ( ) NEW_LINE g_ctc_adv = self . get_gradient_norm ( self . G ) NEW_LINE del l_CTC NEW_LINE data_list_cl = self . data_loader . next ( cl_ny = \\' cl \\' , type = \\' train \\' ) NEW_LINE inputs , mask = _get_variable_nograd ( data_list_cl [ 0 ] ) , _get_variable_nograd ( data_list_cl [ 4 ] ) NEW_LINE ae_cl = self . D ( inputs ) NEW_LINE l_adv_cl , _ = self . diffLoss ( ae_cl , inputs , mask ) NEW_LINE l_adv_cl = self . config . w_adversarial * l_adv_cl NEW_LINE l_adv_cl . backward ( ) NEW_LINE l_adv_cl_data = l_adv_cl . data [ 0 ] NEW_LINE del l_adv_cl NEW_LINE optimizer_g . step ( ) NEW_LINE optimizer_d . step ( ) NEW_LINE if ( iter > self . config . allow_ASR_update_iter ) : NEW_LINE INDENT optimizer_asr . step ( ) NEW_LINE DEDENT g_d_balance = self . gamma * l_adv_cl_data - l_adv_ny_G_data NEW_LINE self . kt += self . lb * g_d_balance NEW_LINE self . kt = max ( min ( 1 , self . kt ) , 0 ) NEW_LINE conv_measure = l_adv_cl_data + abs ( g_d_balance ) NEW_LINE if ( iter + 1 ) % self . config . log_iter == 0 : NEW_LINE INDENT str_loss = \" [ { } / { } ] ▁ ( train ) ▁ CTC : ▁ { : . 7f } , ▁ ADV _ cl : ▁ { : . 7f } , ▁ ADV _ ny : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_tr_local . avg , l_adv_cl_data , l_adv_ny_G_data ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE str_loss = \" [ { } / { } ] ▁ ( train ) ▁ conv _ measure : ▁ { : . 4f } , ▁ kt : ▁ { : . 4f } ▁ \" . format ( iter , self . config . max_iter , conv_measure , self . kt ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE str_loss = \" [ { } / { } ] ▁ ( train ) ▁ gradient ▁ norm , ▁ adv : ▁ { : . 4f } , ▁ adv ▁ + ▁ ctc ▁ : ▁ { : . 4f } \" . format ( iter , self . config . max_iter , g_adv . data [ 0 ] , g_ctc_adv . data [ 0 ] ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . ctc_tr_local . reset ( ) NEW_LINE DEDENT if ( iter + 1 ) % self . config . save_iter == 0 : NEW_LINE INDENT self . G . eval ( ) NEW_LINE self . ctc_tr . reset ( ) NEW_LINE self . adv_ny_tr . reset ( ) NEW_LINE self . wer_tr . reset ( ) NEW_LINE self . cer_tr . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . trsub_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' trsub \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes , mask = data_list [ 0 ] , data_list [ 1 ] , data_list [ 2 ] , data_list [ 3 ] , _get_variable_volatile ( data_list [ 4 ] ) NEW_LINE ctc , adv_ny , nElement , wer , cer , nWord , nChar = self . greedy_decoding_and_AAS ( inputs , targets , input_percentages , target_sizes , mask ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE self . ctc_tr . update ( ctc . data [ 0 ] , N ) NEW_LINE self . adv_ny_tr . update ( adv_ny . data [ 0 ] , nElement ) NEW_LINE self . wer_tr . update ( wer , nWord ) NEW_LINE self . cer_tr . update ( cer , nChar ) NEW_LINE del ctc , adv_ny NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( training ▁ subset ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_tr . avg , self . wer_tr . avg * 100 , self . cer_tr . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . ctc_val . reset ( ) NEW_LINE self . adv_ny_val . reset ( ) NEW_LINE self . wer_val . reset ( ) NEW_LINE self . cer_val . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . val_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' val \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes , mask = data_list [ 0 ] , data_list [ 1 ] , data_list [ 2 ] , data_list [ 3 ] , _get_variable_volatile ( data_list [ 4 ] ) NEW_LINE ctc , adv_ny , nElement , wer , cer , nWord , nChar = self . greedy_decoding_and_AAS ( inputs , targets , input_percentages , target_sizes , mask ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE self . ctc_val . update ( ctc . data [ 0 ] , N ) NEW_LINE self . adv_ny_val . update ( adv_ny . data [ 0 ] , nElement ) NEW_LINE self . wer_val . update ( wer , nWord ) NEW_LINE self . cer_val . update ( cer , nChar ) NEW_LINE del ctc , adv_ny NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( validation ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_val . avg , self . wer_val . avg * 100 , self . cer_val . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . G . train ( ) NEW_LINE if ( len ( self . savename_G ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_G ) : NEW_LINE INDENT os . remove ( self . savename_G ) NEW_LINE DEDENT DEDENT self . savename_G = \\' { } / G _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . G . state_dict ( ) , self . savename_G ) NEW_LINE if ( len ( self . savename_ASR ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_ASR ) : NEW_LINE INDENT os . remove ( self . savename_ASR ) NEW_LINE DEDENT DEDENT self . savename_ASR = \\' { } / ASR _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . ASR . state_dict ( ) , self . savename_ASR ) NEW_LINE if ( self . G . loss_stop > self . wer_val . avg ) : NEW_LINE INDENT self . G . loss_stop = self . wer_val . avg NEW_LINE savename_G_valmin_prev = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_G_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_G_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_G_valmin = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_G , savename_G_valmin ) NEW_LINE savename_ASR_valmin_prev = \\' { } / ASR _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_ASR_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_ASR_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_ASR_valmin = \\' { } / ASR _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_ASR , savename_ASR_valmin ) NEW_LINE self . valmin_iter = iter NEW_LINE DEDENT DEDENT DEDENT DEDENT',\n",
              " \"def greedy_decoding_and_AAS ( self , inputs , targets , input_percentages , target_sizes , mask , transcript_prob = 0.001 ) : NEW_LINE INDENT inputs = _get_variable_volatile ( inputs ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE split_targets = [ ] NEW_LINE offset = 0 NEW_LINE for size in target_sizes : NEW_LINE INDENT split_targets . append ( targets [ offset : offset + size ] ) NEW_LINE offset += size NEW_LINE DEDENT enhanced = self . G ( inputs ) NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = input_percentages . mul_ ( int ( T ) ) . int ( ) NEW_LINE decoded_output , _ = self . decoder . decode ( prob . data , sizes ) NEW_LINE target_strings = self . decoder . convert_to_strings ( split_targets ) NEW_LINE we , ce , total_word , total_char = 0 , 0 , 0 , 0 NEW_LINE for x in range ( len ( target_strings ) ) : NEW_LINE INDENT decoding , reference = decoded_output [ x ] [ 0 ] , target_strings [ x ] [ 0 ] NEW_LINE nChar = len ( reference ) NEW_LINE nWord = len ( reference . split ( ) ) NEW_LINE we_i = self . decoder . wer ( decoding , reference ) NEW_LINE ce_i = self . decoder . cer ( decoding , reference ) NEW_LINE we += we_i NEW_LINE ce += ce_i NEW_LINE total_word += nWord NEW_LINE total_char += nChar NEW_LINE if ( random . uniform ( 0 , 1 ) < transcript_prob ) : NEW_LINE INDENT print ( ' reference ▁ = ▁ ' + reference ) NEW_LINE print ( ' decoding ▁ = ▁ ' + decoding ) NEW_LINE print ( ' wer ▁ = ▁ ' + str ( we_i / float ( nWord ) ) + ' , ▁ cer ▁ = ▁ ' + str ( ce_i / float ( nChar ) ) ) NEW_LINE DEDENT DEDENT wer = we / total_word NEW_LINE cer = ce / total_word NEW_LINE ae_ny = self . D ( enhanced ) NEW_LINE l_adv_ny , nElement = self . diffLoss ( ae_ny , enhanced , mask ) NEW_LINE l_adv_ny = l_adv_ny * self . config . w_adversarial NEW_LINE targets = _get_variable_volatile ( targets , cuda = False ) NEW_LINE sizes = _get_variable_volatile ( sizes , cuda = False ) NEW_LINE target_sizes = _get_variable_volatile ( target_sizes , cuda = False ) NEW_LINE l_CTC = self . config . w_acoustic * self . CTCLoss ( prob , targets , sizes , target_sizes ) / N NEW_LINE return l_CTC , l_adv_ny , nElement , wer , cer , total_word , total_char NEW_LINE DEDENT\",\n",
              " 'def get_gradient_norm ( self , model ) : NEW_LINE INDENT params = list ( model . parameters ( ) ) NEW_LINE grad_norm = 0 NEW_LINE for param in params : NEW_LINE INDENT grad_norm += torch . pow ( param . grad , 2 ) . sum ( ) NEW_LINE DEDENT grad_norm = torch . pow ( grad_norm , 0.5 ) NEW_LINE return grad_norm NEW_LINE DEDENT',\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def __init__ ( self , config , data_loader = None ) : NEW_LINE INDENT self . config = config NEW_LINE self . data_loader = data_loader NEW_LINE self . lr = config . lr NEW_LINE self . beta1 = config . beta1 NEW_LINE self . beta2 = config . beta2 NEW_LINE self . optimizer = config . optimizer NEW_LINE self . batch_size = config . batch_size NEW_LINE self . valmin_iter = 0 NEW_LINE self . model_dir = ' logs / ' + str ( config . expnum ) NEW_LINE self . savename_G = ' ' NEW_LINE self . savename_ASR = ' ' NEW_LINE self . kt = 0 NEW_LINE self . lb = 0.001 NEW_LINE self . conv_measure = 0 NEW_LINE self . ctc_tr = AverageMeter ( ) NEW_LINE self . ctc_tr_local = AverageMeter ( ) NEW_LINE self . ctc_val = AverageMeter ( ) NEW_LINE self . wer_tr = AverageMeter ( ) NEW_LINE self . wer_val = AverageMeter ( ) NEW_LINE self . cer_tr = AverageMeter ( ) NEW_LINE self . cer_val = AverageMeter ( ) NEW_LINE self . CTCLoss = CTCLoss ( ) NEW_LINE self . decoder = GreedyDecoder ( data_loader . labels ) NEW_LINE self . build_model ( ) NEW_LINE self . G . loss_stop = 100000 NEW_LINE if self . config . gpu >= 0 : NEW_LINE INDENT self . G . cuda ( ) NEW_LINE self . ASR . cuda ( ) NEW_LINE DEDENT if len ( self . config . load_path ) > 0 : NEW_LINE INDENT self . load_model ( ) NEW_LINE DEDENT if config . mode == ' train ' : NEW_LINE INDENT self . logFile = open ( self . model_dir + ' / log . txt ' , ' w ' ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def zero_grad_all ( self ) : NEW_LINE INDENT self . G . zero_grad ( ) NEW_LINE self . ASR . zero_grad ( ) NEW_LINE DEDENT',\n",
              " \"def build_model ( self ) : NEW_LINE INDENT print ( ' initialize ▁ enhancement ▁ model ' ) NEW_LINE self . G = stackedBRNN ( I = self . config . nFeat , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE print ( ' load ▁ pre - trained ▁ ASR ▁ model ' ) NEW_LINE package_ASR = torch . load ( self . config . ASR_path , map_location = lambda storage , loc : storage ) NEW_LINE self . ASR = DeepSpeech . load_model_package ( package_ASR ) NEW_LINE DEDENT\",\n",
              " 'def load_model ( self ) : NEW_LINE INDENT print ( \" [ * ] ▁ Load ▁ models ▁ from ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE postfix = \\' _ valmin \\' NEW_LINE paths = glob ( os . path . join ( self . config . load_path , \\' G { } * . pth \\' . format ( postfix ) ) ) NEW_LINE paths . sort ( ) NEW_LINE if len ( paths ) == 0 : NEW_LINE INDENT print ( \" [ ! ] ▁ No ▁ checkpoint ▁ found ▁ in ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE assert ( 0 ) , \\' checkpoint ▁ not ▁ avilable \\' NEW_LINE DEDENT idxes = [ int ( os . path . basename ( path . split ( \\' . \\' ) [ 0 ] . split ( \\' _ \\' ) [ - 1 ] ) ) for path in paths ] NEW_LINE if self . config . start_iter <= 0 : NEW_LINE INDENT self . config . start_iter = max ( idxes ) NEW_LINE if ( self . config . start_iter < 0 ) : NEW_LINE INDENT raise Exception ( \" start ▁ iter ▁ is ▁ still ▁ less ▁ than ▁ 0 ▁ - - > ▁ probably ▁ try ▁ to ▁ load ▁ initial ▁ random ▁ model \" ) NEW_LINE DEDENT DEDENT if self . config . gpu < 0 : NEW_LINE INDENT map_location = lambda storage , loc : storage NEW_LINE DEDENT else : NEW_LINE INDENT map_location = None NEW_LINE DEDENT print ( \\' Load ▁ models ▁ from ▁ \\' + self . config . load_path + \\' , ▁ ITERATION ▁ = ▁ \\' + str ( self . config . start_iter ) ) NEW_LINE self . G . load_state_dict ( torch . load ( \\' { } / G { } _ { } . pth \\' . format ( self . config . load_path , postfix , self . config . start_iter ) , map_location = map_location ) ) NEW_LINE print ( \" [ * ] ▁ Model ▁ loaded \" ) NEW_LINE DEDENT',\n",
              " 'def train ( self ) : NEW_LINE INDENT optimizer_g = torch . optim . Adam ( self . G . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE optimizer_asr = torch . optim . Adam ( self . ASR . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE for iter in trange ( self . config . start_iter , self . config . max_iter ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' train \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes = _get_variable_nograd ( data_list [ 0 ] ) , _get_variable_nograd ( data_list [ 1 ] , cuda = False ) , data_list [ 2 ] , _get_variable_nograd ( data_list [ 3 ] , cuda = False ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE enhanced = self . G ( inputs ) NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = _get_variable_nograd ( input_percentages . mul_ ( int ( T ) ) . int ( ) , cuda = False ) NEW_LINE loss = self . CTCLoss ( prob , targets , sizes , target_sizes ) NEW_LINE loss = loss / N NEW_LINE self . zero_grad_all ( ) NEW_LINE loss . backward ( ) NEW_LINE optimizer_g . step ( ) NEW_LINE if ( iter > self . config . allow_ASR_update_iter ) : NEW_LINE INDENT optimizer_asr . step ( ) NEW_LINE DEDENT self . ctc_tr_local . update ( loss . data [ 0 ] , N ) NEW_LINE del loss NEW_LINE if ( iter + 1 ) % self . config . log_iter == 0 : NEW_LINE INDENT str_loss = \" [ { } / { } ] ▁ ( train ) ▁ CTC : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_tr_local . avg ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . ctc_tr_local . reset ( ) NEW_LINE DEDENT if ( iter + 1 ) % self . config . save_iter == 0 : NEW_LINE INDENT self . G . eval ( ) NEW_LINE self . ctc_tr . reset ( ) NEW_LINE self . wer_tr . reset ( ) NEW_LINE self . cer_tr . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . trsub_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' trsub \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes = data_list [ 0 ] , data_list [ 1 ] , data_list [ 2 ] , data_list [ 3 ] NEW_LINE ctc , wer , cer , nWord , nChar = self . greedy_decoding_and_CTCLoss ( inputs , targets , input_percentages , target_sizes ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE self . ctc_tr . update ( ctc . data [ 0 ] , N ) NEW_LINE self . wer_tr . update ( wer , nWord ) NEW_LINE self . cer_tr . update ( cer , nChar ) NEW_LINE del ctc NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( training ▁ subset ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_tr . avg , self . wer_tr . avg * 100 , self . cer_tr . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . ctc_val . reset ( ) NEW_LINE self . wer_val . reset ( ) NEW_LINE self . cer_val . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . val_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' val \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes = data_list [ 0 ] , data_list [ 1 ] , data_list [ 2 ] , data_list [ 3 ] NEW_LINE ctc , wer , cer , nWord , nChar = self . greedy_decoding_and_CTCLoss ( inputs , targets , input_percentages , target_sizes ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE self . ctc_val . update ( ctc . data [ 0 ] , N ) NEW_LINE self . wer_val . update ( wer , nWord ) NEW_LINE self . cer_val . update ( cer , nChar ) NEW_LINE del ctc NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( validation ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_val . avg , self . wer_val . avg * 100 , self . cer_val . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . G . train ( ) NEW_LINE if ( len ( self . savename_G ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_G ) : NEW_LINE INDENT os . remove ( self . savename_G ) NEW_LINE DEDENT DEDENT self . savename_G = \\' { } / G _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . G . state_dict ( ) , self . savename_G ) NEW_LINE if ( len ( self . savename_ASR ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_ASR ) : NEW_LINE INDENT os . remove ( self . savename_ASR ) NEW_LINE DEDENT DEDENT self . savename_ASR = \\' { } / ASR _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . ASR . state_dict ( ) , self . savename_ASR ) NEW_LINE if ( self . G . loss_stop > self . wer_val . avg ) : NEW_LINE INDENT self . G . loss_stop = self . wer_val . avg NEW_LINE savename_G_valmin_prev = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_G_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_G_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_G_valmin = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_G , savename_G_valmin ) NEW_LINE savename_ASR_valmin_prev = \\' { } / ASR _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_ASR_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_ASR_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_ASR_valmin = \\' { } / ASR _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_ASR , savename_ASR_valmin ) NEW_LINE self . valmin_iter = iter NEW_LINE DEDENT DEDENT DEDENT DEDENT',\n",
              " \"def greedy_decoding_and_CTCLoss ( self , inputs , targets , input_percentages , target_sizes , transcript_prob = 0.001 ) : NEW_LINE INDENT inputs = _get_variable_volatile ( inputs ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE split_targets = [ ] NEW_LINE offset = 0 NEW_LINE for size in target_sizes : NEW_LINE INDENT split_targets . append ( targets [ offset : offset + size ] ) NEW_LINE offset += size NEW_LINE DEDENT enhanced = self . G ( inputs ) NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = input_percentages . mul_ ( int ( T ) ) . int ( ) NEW_LINE decoded_output , _ = self . decoder . decode ( prob . data , sizes ) NEW_LINE target_strings = self . decoder . convert_to_strings ( split_targets ) NEW_LINE we , ce , total_word , total_char = 0 , 0 , 0 , 0 NEW_LINE for x in range ( len ( target_strings ) ) : NEW_LINE INDENT decoding , reference = decoded_output [ x ] [ 0 ] , target_strings [ x ] [ 0 ] NEW_LINE nChar = len ( reference ) NEW_LINE nWord = len ( reference . split ( ) ) NEW_LINE we_i = self . decoder . wer ( decoding , reference ) NEW_LINE ce_i = self . decoder . cer ( decoding , reference ) NEW_LINE we += we_i NEW_LINE ce += ce_i NEW_LINE total_word += nWord NEW_LINE total_char += nChar NEW_LINE if ( random . uniform ( 0 , 1 ) < transcript_prob ) : NEW_LINE INDENT print ( ' reference ▁ = ▁ ' + reference ) NEW_LINE print ( ' decoding ▁ = ▁ ' + decoding ) NEW_LINE print ( ' wer ▁ = ▁ ' + str ( we_i / float ( nWord ) ) + ' , ▁ cer ▁ = ▁ ' + str ( ce_i / float ( nChar ) ) ) NEW_LINE DEDENT DEDENT wer = we / total_word NEW_LINE cer = ce / total_word NEW_LINE targets = _get_variable_volatile ( targets , cuda = False ) NEW_LINE sizes = _get_variable_volatile ( sizes , cuda = False ) NEW_LINE target_sizes = _get_variable_volatile ( target_sizes , cuda = False ) NEW_LINE loss = self . CTCLoss ( prob , targets , sizes , target_sizes ) NEW_LINE loss = loss / N NEW_LINE return loss , wer , cer , total_word , total_char NEW_LINE DEDENT\",\n",
              " 'def _collate_fn ( batch ) : NEW_LINE INDENT def func ( p ) : NEW_LINE INDENT return p [ 0 ] . size ( 1 ) NEW_LINE DEDENT batch = sorted ( batch , key = lambda sample : sample [ 0 ] . size ( 1 ) , reverse = True ) NEW_LINE longest_sample = max ( batch , key = func ) [ 0 ] NEW_LINE freq_size = longest_sample . size ( 0 ) NEW_LINE minibatch_size = len ( batch ) NEW_LINE max_seqlength = longest_sample . size ( 1 ) NEW_LINE inputs = torch . zeros ( minibatch_size , freq_size , max_seqlength ) NEW_LINE input_percentages = torch . FloatTensor ( minibatch_size ) NEW_LINE target_sizes = torch . IntTensor ( minibatch_size ) NEW_LINE targets = [ ] NEW_LINE mask = torch . ByteTensor ( minibatch_size , 1 , max_seqlength ) . zero_ ( ) NEW_LINE for x in range ( minibatch_size ) : NEW_LINE INDENT sample = batch [ x ] NEW_LINE tensor = sample [ 0 ] NEW_LINE target = sample [ 1 ] NEW_LINE seq_length = tensor . size ( 1 ) NEW_LINE inputs [ x ] . narrow ( 1 , 0 , seq_length ) . copy_ ( tensor ) NEW_LINE input_percentages [ x ] = seq_length / float ( max_seqlength ) NEW_LINE target_sizes [ x ] = len ( target ) NEW_LINE targets . extend ( target ) NEW_LINE if ( seq_length < max_seqlength ) : NEW_LINE INDENT mask [ x ] [ : , seq_length : ] . fill_ ( 1 ) NEW_LINE DEDENT DEDENT targets = torch . IntTensor ( targets ) NEW_LINE return inputs , targets , input_percentages , target_sizes , mask NEW_LINE DEDENT',\n",
              " 'def _collate_fn_paired ( batch ) : NEW_LINE INDENT def func ( p ) : NEW_LINE INDENT return p [ 0 ] . size ( 1 ) NEW_LINE DEDENT batch = sorted ( batch , key = lambda sample : sample [ 0 ] . size ( 1 ) , reverse = True ) NEW_LINE longest_sample = max ( batch , key = func ) [ 0 ] NEW_LINE freq_size = longest_sample . size ( 0 ) NEW_LINE minibatch_size = len ( batch ) NEW_LINE max_seqlength = longest_sample . size ( 1 ) NEW_LINE inputs = torch . zeros ( minibatch_size , freq_size , max_seqlength ) NEW_LINE outputs = torch . zeros ( minibatch_size , freq_size , max_seqlength ) NEW_LINE mask = torch . ByteTensor ( minibatch_size , 1 , max_seqlength ) . zero_ ( ) NEW_LINE input_percentages = torch . FloatTensor ( minibatch_size ) NEW_LINE target_sizes = torch . IntTensor ( minibatch_size ) NEW_LINE targets = [ ] NEW_LINE for x in range ( minibatch_size ) : NEW_LINE INDENT sample = batch [ x ] NEW_LINE tensor = sample [ 0 ] NEW_LINE txt = sample [ 1 ] NEW_LINE target = sample [ 2 ] NEW_LINE seq_length = tensor . size ( 1 ) NEW_LINE inputs [ x ] . narrow ( 1 , 0 , seq_length ) . copy_ ( tensor ) NEW_LINE outputs [ x ] . narrow ( 1 , 0 , seq_length ) . copy_ ( target ) NEW_LINE if ( seq_length < max_seqlength ) : NEW_LINE INDENT mask [ x ] [ : , seq_length : ] . fill_ ( 1 ) NEW_LINE DEDENT input_percentages [ x ] = seq_length / float ( max_seqlength ) NEW_LINE target_sizes [ x ] = len ( txt ) NEW_LINE targets . extend ( txt ) NEW_LINE DEDENT targets = torch . IntTensor ( targets ) NEW_LINE return inputs , outputs , mask , targets , input_percentages , target_sizes NEW_LINE DEDENT',\n",
              " \"def __init__ ( self , manifest , labels ) : NEW_LINE INDENT with open ( manifest ) as f : NEW_LINE INDENT ids = f . readlines ( ) NEW_LINE DEDENT ids = [ x . strip ( ) . split ( ' , ' ) for x in ids ] NEW_LINE self . ids = ids NEW_LINE self . size = len ( ids ) NEW_LINE self . labels_map = dict ( [ ( labels [ i ] , i ) for i in range ( len ( labels ) ) ] ) NEW_LINE super ( FeatDataset , self ) . __init__ ( ) NEW_LINE DEDENT\",\n",
              " 'def __getitem__ ( self , index ) : NEW_LINE INDENT sample = self . ids [ index ] NEW_LINE if ( len ( sample ) == 2 ) : NEW_LINE INDENT feat_path , transcript_path = sample [ 0 ] , sample [ 1 ] NEW_LINE DEDENT else : NEW_LINE INDENT feat_path , transcript_path , feat_paired_path = sample [ 0 ] , sample [ 1 ] , sample [ 2 ] NEW_LINE DEDENT feat = torch . load ( feat_path ) NEW_LINE transcript = self . parse_transcript ( transcript_path ) NEW_LINE if ( len ( sample ) == 2 ) : NEW_LINE INDENT return feat , transcript NEW_LINE DEDENT else : NEW_LINE INDENT feat_paired = torch . load ( feat_paired_path ) NEW_LINE return feat , transcript , feat_paired NEW_LINE DEDENT DEDENT',\n",
              " \"def parse_transcript ( self , transcript_path ) : NEW_LINE INDENT with open ( transcript_path , ' r ' , encoding = ' utf8' ) as transcript_file : NEW_LINE INDENT transcript = transcript_file . read ( ) . replace ( ' \\\\n ' , ' ' ) NEW_LINE DEDENT transcript = list ( filter ( None , [ self . labels_map . get ( x ) for x in list ( transcript ) ] ) ) NEW_LINE return transcript NEW_LINE DEDENT\",\n",
              " 'def __len__ ( self ) : NEW_LINE INDENT return self . size NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , * args , ** kwargs ) : NEW_LINE INDENT super ( FeatLoader , self ) . __init__ ( * args , ** kwargs ) NEW_LINE self . collate_fn = _collate_fn NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , * args , ** kwargs ) : NEW_LINE INDENT super ( FeatLoader_paired , self ) . __init__ ( * args , ** kwargs ) NEW_LINE self . collate_fn = _collate_fn_paired NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , data_source , batch_size = 1 ) : NEW_LINE INDENT super ( FeatSampler , self ) . __init__ ( data_source ) NEW_LINE self . data_source = data_source NEW_LINE ids = list ( range ( 0 , len ( data_source ) ) ) NEW_LINE self . bins = [ ids [ i : i + batch_size ] for i in range ( 0 , len ( ids ) , batch_size ) ] NEW_LINE DEDENT',\n",
              " 'def __iter__ ( self ) : NEW_LINE INDENT for ids in self . bins : NEW_LINE INDENT np . random . shuffle ( ids ) NEW_LINE yield ids NEW_LINE DEDENT DEDENT',\n",
              " 'def __len__ ( self ) : NEW_LINE INDENT return len ( self . bins ) NEW_LINE DEDENT',\n",
              " 'def shuffle ( self ) : NEW_LINE INDENT np . random . shuffle ( self . bins ) NEW_LINE DEDENT',\n",
              " \"def load_model ( cls , path , gpu = - 1 ) : NEW_LINE INDENT package = torch . load ( path , map_location = lambda storage , loc : storage ) NEW_LINE model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] ) NEW_LINE blacklist = [ ' rnns . 0 . batch _ norm . module . weight ' , ' rnns . 0 . batch _ norm . module . bias ' , ' rnns . 0 . batch _ norm . module . running _ mean ' , ' rnns . 0 . batch _ norm . module . running _ var ' ] NEW_LINE for x in blacklist : NEW_LINE INDENT if x in package [ ' state _ dict ' ] : NEW_LINE INDENT del package [ ' state _ dict ' ] [ x ] NEW_LINE DEDENT DEDENT model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE for x in model . rnns : NEW_LINE INDENT x . flatten_parameters ( ) NEW_LINE DEDENT if gpu >= 0 : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def load_model_package ( cls , package , gpu = - 1 ) : NEW_LINE INDENT model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] , ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE if ( gpu >= 0 ) : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def serialize ( model , optimizer = None , epoch = None , iteration = None , loss_results = None , cer_results = None , wer_results = None , avg_loss = None , meta = None ) : NEW_LINE INDENT package = { ' version ' : model . _version , ' rnn _ size ' : model . rnn_size , ' rnn _ layers ' : model . rnn_layers , ' cnn _ map ' : model . cnn_map , ' cnn _ kernel ' : model . cnn_kernel , ' cnn _ stride ' : model . cnn_stride , ' cnn _ layers ' : model . cnn_layers , ' rnn _ type ' : supported_rnns_inv . get ( model . rnn_type , model . rnn_type . __name__ . lower ( ) ) , ' labels ' : model . _labels , ' state _ dict ' : model . state_dict ( ) } NEW_LINE if optimizer is not None : NEW_LINE INDENT package [ ' optim _ dict ' ] = optimizer . state_dict ( ) NEW_LINE DEDENT if avg_loss is not None : NEW_LINE INDENT package [ ' avg _ loss ' ] = avg_loss NEW_LINE DEDENT if epoch is not None : NEW_LINE INDENT package [ ' epoch ' ] = epoch + 1 NEW_LINE DEDENT if iteration is not None : NEW_LINE INDENT package [ ' iteration ' ] = iteration NEW_LINE DEDENT if loss_results is not None : NEW_LINE INDENT package [ ' loss _ results ' ] = loss_results NEW_LINE package [ ' cer _ results ' ] = cer_results NEW_LINE package [ ' wer _ results ' ] = wer_results NEW_LINE DEDENT if meta is not None : NEW_LINE INDENT package [ ' meta ' ] = meta NEW_LINE DEDENT return package NEW_LINE DEDENT\",\n",
              " 'def get_labels ( model ) : NEW_LINE INDENT return model . _labels NEW_LINE DEDENT',\n",
              " 'def get_param_size ( model ) : NEW_LINE INDENT params = 0 NEW_LINE for p in model . parameters ( ) : NEW_LINE INDENT tmp = 1 NEW_LINE for x in p . size ( ) : NEW_LINE INDENT tmp *= x NEW_LINE DEDENT params += tmp NEW_LINE DEDENT return params NEW_LINE DEDENT',\n",
              " 'def get_audio_conf ( model ) : NEW_LINE INDENT return model . _audio_conf NEW_LINE DEDENT',\n",
              " 'def get_meta ( model ) : NEW_LINE INDENT model_is_cuda = next ( model . parameters ( ) ) . is_cuda NEW_LINE m = model . module if model_is_cuda else model NEW_LINE meta = { \" version \" : m . _version , \" rnn _ size \" : m . rnn_size , \" rnn _ layers \" : m . rnn_layers , \" cnn _ map \" : m . cnn_map , \" cnn _ kernel \" : m . cnn_kernel , \" cnn _ stride \" : m . cnn_stride , \" cnn _ layers \" : m . cnn_layers , \" rnn _ type \" : supported_rnns_inv [ m . rnn_type ] } NEW_LINE return meta NEW_LINE DEDENT',\n",
              " 'def __init__ ( self ) : NEW_LINE INDENT super ( L1Loss_mask , self ) . __init__ ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , input , target , mask ) : NEW_LINE INDENT mask_sum = mask . data . sum ( ) NEW_LINE if ( mask . data [ 0 ] [ 0 ] [ 0 ] == 0 ) : NEW_LINE INDENT nElement = mask . data . nelement ( ) - mask_sum NEW_LINE DEDENT err = torch . abs ( input - target ) NEW_LINE err . masked_fill ( mask , 0 ) NEW_LINE loss = err . sum ( ) / nElement NEW_LINE return loss , nElement NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , module ) : NEW_LINE INDENT super ( SequenceWise , self ) . __init__ ( ) NEW_LINE self . module = module NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT t , n = x . size ( 0 ) , x . size ( 1 ) NEW_LINE x = x . view ( t * n , - 1 ) NEW_LINE x = self . module ( x ) NEW_LINE x = x . view ( t , n , - 1 ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " \"def __repr__ ( self ) : NEW_LINE INDENT tmpstr = self . __class__ . __name__ + ' ▁ ( \\\\n ' NEW_LINE tmpstr += self . module . __repr__ ( ) NEW_LINE tmpstr += ' ) ' NEW_LINE return tmpstr NEW_LINE DEDENT\",\n",
              " 'def forward ( self , input_ ) : NEW_LINE INDENT if not self . training : NEW_LINE INDENT batch_size = input_ . size ( ) [ 0 ] NEW_LINE return torch . stack ( [ F . softmax ( input_ [ i ] , dim = 1 ) for i in range ( batch_size ) ] , 0 ) NEW_LINE DEDENT else : NEW_LINE INDENT return input_ NEW_LINE DEDENT DEDENT',\n",
              " 'def __init__ ( self , input_size , hidden_size , rnn_type = nn . LSTM , bidirectional = False , batch_norm = True ) : NEW_LINE INDENT super ( BatchRNN , self ) . __init__ ( ) NEW_LINE self . input_size = input_size NEW_LINE self . hidden_size = hidden_size NEW_LINE self . bidirectional = bidirectional NEW_LINE self . batch_norm = SequenceWise ( nn . BatchNorm1d ( input_size ) ) if batch_norm else None NEW_LINE self . rnn = rnn_type ( input_size = input_size , hidden_size = hidden_size , bidirectional = bidirectional , bias = False ) NEW_LINE self . num_directions = 2 if bidirectional else 1 NEW_LINE DEDENT',\n",
              " 'def flatten_parameters ( self ) : NEW_LINE INDENT self . rnn . flatten_parameters ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if self . batch_norm is not None : NEW_LINE INDENT x = self . batch_norm ( x ) NEW_LINE DEDENT x , _ = self . rnn ( x ) NEW_LINE if self . bidirectional : NEW_LINE INDENT x = x . view ( x . size ( 0 ) , x . size ( 1 ) , 2 , - 1 ) . sum ( 2 ) . view ( x . size ( 0 ) , x . size ( 1 ) , - 1 ) NEW_LINE DEDENT return x NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , input_size , hidden_size , rnn_type = nn . LSTM , bidirectional = False ) : NEW_LINE INDENT super ( BRNN , self ) . __init__ ( ) NEW_LINE self . input_size = input_size NEW_LINE self . hidden_size = hidden_size NEW_LINE self . bidirectional = bidirectional NEW_LINE self . rnn = rnn_type ( input_size = input_size , hidden_size = hidden_size , bidirectional = bidirectional , bias = False ) NEW_LINE self . num_directions = 2 if bidirectional else 1 NEW_LINE DEDENT',\n",
              " 'def flatten_parameters ( self ) : NEW_LINE INDENT self . rnn . flatten_parameters ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT x , _ = self . rnn ( x ) NEW_LINE if self . bidirectional : NEW_LINE INDENT x = x . view ( x . size ( 0 ) , x . size ( 1 ) , 2 , - 1 ) . sum ( 2 ) . view ( x . size ( 0 ) , x . size ( 1 ) , - 1 ) NEW_LINE DEDENT return x NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , I , O , H , L = 3 , rnn_type = nn . LSTM ) : NEW_LINE INDENT super ( SpeechClassifierRNN , self ) . __init__ ( ) NEW_LINE self . I = I NEW_LINE self . H = H NEW_LINE self . L = L NEW_LINE self . rnn1 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn2 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn3 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE if ( I != H ) : NEW_LINE INDENT self . first_linear = nn . Conv1d ( I , H , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE DEDENT else : NEW_LINE INDENT self . first_linear = None NEW_LINE DEDENT self . final_linear = nn . Linear ( H , O ) NEW_LINE self . criterion = nn . CrossEntropyLoss ( size_average = False ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , input , target ) : NEW_LINE INDENT if ( self . first_linear ) : NEW_LINE INDENT input = self . first_linear ( input ) NEW_LINE DEDENT input = input . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) NEW_LINE h1 = self . rnn1 ( input ) + input NEW_LINE h2 = self . rnn2 ( h1 ) + h1 NEW_LINE h3 = self . rnn3 ( h2 ) + h2 NEW_LINE h3 = h3 . sum ( 0 ) NEW_LINE output = self . final_linear ( h3 ) NEW_LINE loss = self . criterion ( output , target ) NEW_LINE return loss NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , I , H , L , nCH , mel_basis , rnn_type = nn . GRU ) : NEW_LINE INDENT super ( BRNNmultiCH , self ) . __init__ ( ) NEW_LINE self . I = I NEW_LINE self . H = H NEW_LINE self . nCH = nCH NEW_LINE self . rnn_type = rnn_type NEW_LINE self . L = L NEW_LINE self . rnn1 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn2 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE if ( self . L == 3 ) : NEW_LINE INDENT self . rnn3 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE DEDENT self . first_linear = nn . Conv1d ( I , H , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE self . final_linear_real = nn . Conv1d ( H , int ( I / 2 ) , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE self . final_linear_imag = nn . Conv1d ( H , int ( I / 2 ) , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE self . mel_basis = Variable ( torch . unsqueeze ( torch . FloatTensor ( mel_basis ) . repeat ( 1 , self . nCH ) , - 1 ) . cuda ( ) ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , input ) : NEW_LINE INDENT input_linear = self . first_linear ( input ) NEW_LINE input_linear = input_linear . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) NEW_LINE h1 = self . rnn1 ( input_linear ) + input_linear NEW_LINE h2 = self . rnn2 ( h1 ) + h1 NEW_LINE if ( self . L == 3 ) : NEW_LINE INDENT h3 = self . rnn3 ( h2 ) + h2 NEW_LINE h = h3 . transpose ( 0 , 1 ) . transpose ( 1 , 2 ) NEW_LINE DEDENT else : NEW_LINE INDENT h = h2 . transpose ( 0 , 1 ) . transpose ( 1 , 2 ) NEW_LINE DEDENT mask_real = self . final_linear_real ( h ) NEW_LINE mask_imag = self . final_linear_imag ( h ) NEW_LINE stft = input . view ( input . size ( 0 ) , 2 , - 1 , input . size ( - 1 ) ) NEW_LINE stft_real = stft [ : , 0 ] NEW_LINE stft_imag = stft [ : , 1 ] NEW_LINE enh_real = torch . mul ( stft_real , mask_real ) NEW_LINE enh_imag = torch . mul ( stft_imag , mask_imag ) NEW_LINE enh_power = torch . pow ( enh_real , 2 ) + torch . pow ( enh_imag , 2 ) NEW_LINE enh_mel = F . conv1d ( enh_power , self . mel_basis ) NEW_LINE output = torch . log1p ( enh_mel ) NEW_LINE return output NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , I , O , H , L , rnn_type = nn . LSTM ) : NEW_LINE INDENT super ( stackedBRNN , self ) . __init__ ( ) NEW_LINE self . I = I NEW_LINE self . H = H NEW_LINE self . L = L NEW_LINE self . rnn_type = rnn_type NEW_LINE self . rnn1 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn2 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn3 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn4 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . first_linear = nn . Conv1d ( I , H , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE self . final_linear = nn . Conv1d ( H , O , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , input ) : NEW_LINE INDENT input_linear = self . first_linear ( input ) NEW_LINE input_linear = input_linear . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) NEW_LINE h1 = self . rnn1 ( input_linear ) + input_linear NEW_LINE h2 = self . rnn2 ( h1 ) + h1 NEW_LINE h3 = self . rnn3 ( h2 ) + h2 NEW_LINE h4 = self . rnn4 ( h3 ) + h3 NEW_LINE h4 = h4 . transpose ( 0 , 1 ) . transpose ( 1 , 2 ) NEW_LINE output = self . final_linear ( h4 ) NEW_LINE return output NEW_LINE DEDENT',\n",
              " 'def forward_paired ( self , input , paired ) : NEW_LINE INDENT input = torch . cat ( ( input , paired ) , dim = 1 ) NEW_LINE output = self . forward ( input ) NEW_LINE return output NEW_LINE DEDENT',\n",
              " 'def forward_with_intermediate_output ( self , input ) : NEW_LINE INDENT input_linear = self . first_linear ( input ) NEW_LINE input_linear = input_linear . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) NEW_LINE h1 = self . rnn1 ( input_linear ) + input_linear NEW_LINE h2 = self . rnn2 ( h1 ) + h1 NEW_LINE h3 = self . rnn3 ( h2 ) + h2 NEW_LINE h4 = self . rnn4 ( h3 ) + h3 NEW_LINE h4 = h4 . transpose ( 0 , 1 ) . transpose ( 1 , 2 ) NEW_LINE output = self . final_linear ( h4 ) NEW_LINE return [ output , h4 ] NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , rnn_type = nn . LSTM , labels = \" abc \" , rnn_hidden_size = 512 , rnn_layers = 2 , bidirectional = True , kernel_sz = 11 , stride = 2 , map = 256 , cnn_layers = 2 , nFreq = 40 , nDownsample = 1 , audio_conf = None ) : NEW_LINE INDENT super ( DeepSpeech , self ) . __init__ ( ) NEW_LINE self . nFreq = nFreq NEW_LINE self . _version = \\'0.0.1\\' NEW_LINE self . _audio_conf = audio_conf NEW_LINE self . rnn_size = rnn_hidden_size NEW_LINE self . rnn_layers = rnn_layers NEW_LINE self . rnn_type = rnn_type NEW_LINE self . bidirectional = bidirectional NEW_LINE self . cnn_stride = stride NEW_LINE self . cnn_map = map NEW_LINE self . cnn_kernel = kernel_sz NEW_LINE self . nDownsample = nDownsample NEW_LINE self . cnn_layers = cnn_layers NEW_LINE self . _labels = labels NEW_LINE num_classes = len ( self . _labels ) NEW_LINE conv_list = [ ] NEW_LINE conv_list . append ( nn . Conv1d ( nFreq , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE if ( self . nDownsample == 1 ) : NEW_LINE INDENT stride = 1 NEW_LINE DEDENT for x in range ( self . cnn_layers - 1 ) : NEW_LINE INDENT conv_list . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE DEDENT self . conv = nn . Sequential ( * conv_list ) NEW_LINE rnn_input_size = map NEW_LINE rnns = [ ] NEW_LINE rnn = BatchRNN ( input_size = rnn_input_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional , batch_norm = False ) NEW_LINE rnns . append ( ( \\'0\\' , rnn ) ) NEW_LINE for x in range ( self . rnn_layers - 1 ) : NEW_LINE INDENT rnn = BatchRNN ( input_size = rnn_hidden_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional ) NEW_LINE rnns . append ( ( \\' % d \\' % ( x + 1 ) , rnn ) ) NEW_LINE DEDENT self . rnns = nn . Sequential ( OrderedDict ( rnns ) ) NEW_LINE fully_connected = nn . Sequential ( nn . BatchNorm1d ( rnn_hidden_size ) , nn . Linear ( rnn_hidden_size , num_classes , bias = False ) ) NEW_LINE self . fc = nn . Sequential ( SequenceWise ( fully_connected ) , ) NEW_LINE self . inference_softmax = InferenceBatchSoftmax ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT x = self . conv ( x ) NEW_LINE x = x . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) NEW_LINE x = self . rnns ( x ) NEW_LINE x = self . fc ( x ) NEW_LINE x = x . transpose ( 0 , 1 ) NEW_LINE x = self . inference_softmax ( x ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def __init__ ( self , config , data_loader = None ) : NEW_LINE INDENT self . config = config NEW_LINE self . data_loader = data_loader NEW_LINE self . lr = config . lr NEW_LINE self . beta1 = config . beta1 NEW_LINE self . beta2 = config . beta2 NEW_LINE self . optimizer = config . optimizer NEW_LINE self . batch_size = config . batch_size NEW_LINE self . diffLoss = L1Loss_mask ( ) NEW_LINE self . valmin_iter = 0 NEW_LINE self . model_dir = ' logs / ' + str ( config . expnum ) NEW_LINE self . savename_G = ' ' NEW_LINE self . savename_D = ' ' NEW_LINE self . savename_ASR = ' ' NEW_LINE self . kt = 0 NEW_LINE self . lb = self . config . lambda_k NEW_LINE self . gamma = self . config . gamma NEW_LINE self . conv_measure = 0 NEW_LINE self . dce_tr = AverageMeter ( ) NEW_LINE self . dce_tr_local = AverageMeter ( ) NEW_LINE self . dce_val = AverageMeter ( ) NEW_LINE self . adv_ny_tr = AverageMeter ( ) NEW_LINE self . adv_ny_val = AverageMeter ( ) NEW_LINE self . wer_tr = AverageMeter ( ) NEW_LINE self . wer_val = AverageMeter ( ) NEW_LINE self . cer_tr = AverageMeter ( ) NEW_LINE self . cer_val = AverageMeter ( ) NEW_LINE self . decoder = GreedyDecoder ( data_loader . labels ) NEW_LINE self . build_model ( ) NEW_LINE self . G . loss_stop = 100000 NEW_LINE if self . config . gpu >= 0 : NEW_LINE INDENT self . G . cuda ( ) NEW_LINE self . D . cuda ( ) NEW_LINE self . diffLoss . cuda ( ) NEW_LINE self . ASR . cuda ( ) NEW_LINE DEDENT if len ( self . config . load_path ) > 0 : NEW_LINE INDENT self . load_model ( ) NEW_LINE DEDENT if config . mode == ' train ' : NEW_LINE INDENT self . logFile = open ( self . model_dir + ' / log . txt ' , ' w ' ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def zero_grad_all ( self ) : NEW_LINE INDENT self . G . zero_grad ( ) NEW_LINE self . D . zero_grad ( ) NEW_LINE self . ASR . zero_grad ( ) NEW_LINE DEDENT',\n",
              " \"def build_model ( self ) : NEW_LINE INDENT print ( ' initialize ▁ enhancement ▁ & ▁ discriminator ▁ model ' ) NEW_LINE self . G = stackedBRNN ( I = self . config . nFeat_in , O = self . config . nFeat_out , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE self . D = stackedBRNN ( I = self . config . nFeat_D , O = self . config . nFeat_out , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE print ( ' load ▁ pre - trained ▁ ASR ▁ model ' ) NEW_LINE package_ASR = torch . load ( self . config . ASR_path , map_location = lambda storage , loc : storage ) NEW_LINE self . ASR = DeepSpeech . load_model_package ( package_ASR ) NEW_LINE DEDENT\",\n",
              " 'def load_model ( self ) : NEW_LINE INDENT print ( \" [ * ] ▁ Load ▁ models ▁ from ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE postfix = \\' _ valmin \\' NEW_LINE paths = glob ( os . path . join ( self . config . load_path , \\' G { } * . pth \\' . format ( postfix ) ) ) NEW_LINE paths . sort ( ) NEW_LINE if len ( paths ) == 0 : NEW_LINE INDENT print ( \" [ ! ] ▁ No ▁ checkpoint ▁ found ▁ in ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE assert ( 0 ) , \\' checkpoint ▁ not ▁ avilable \\' NEW_LINE DEDENT idxes = [ int ( os . path . basename ( path . split ( \\' . \\' ) [ 0 ] . split ( \\' _ \\' ) [ - 1 ] ) ) for path in paths ] NEW_LINE if self . config . start_iter <= 0 : NEW_LINE INDENT self . config . start_iter = max ( idxes ) NEW_LINE if ( self . config . start_iter < 0 ) : NEW_LINE INDENT raise Exception ( \" start ▁ iter ▁ is ▁ still ▁ less ▁ than ▁ 0 ▁ - - > ▁ probably ▁ try ▁ to ▁ load ▁ initial ▁ random ▁ model \" ) NEW_LINE DEDENT DEDENT if self . config . gpu < 0 : NEW_LINE INDENT map_location = lambda storage , loc : storage NEW_LINE DEDENT else : NEW_LINE INDENT map_location = None NEW_LINE DEDENT print ( \\' Load ▁ models ▁ from ▁ \\' + self . config . load_path + \\' , ▁ ITERATION ▁ = ▁ \\' + str ( self . config . start_iter ) ) NEW_LINE self . G . load_state_dict ( torch . load ( \\' { } / G { } _ { } . pth \\' . format ( self . config . load_path , postfix , self . config . start_iter ) , map_location = map_location ) ) NEW_LINE print ( \" [ * ] ▁ Model ▁ loaded \" ) NEW_LINE DEDENT',\n",
              " 'def train ( self ) : NEW_LINE INDENT optimizer_g = torch . optim . Adam ( self . G . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE optimizer_d = torch . optim . Adam ( self . D . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE for iter in trange ( self . config . start_iter , self . config . max_iter ) : NEW_LINE INDENT self . zero_grad_all ( ) NEW_LINE data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' train \\' ) NEW_LINE mixture , cleans , mask = _get_variable_nograd ( data_list [ 0 ] ) , _get_variable_nograd ( data_list [ 1 ] ) , _get_variable_nograd ( data_list [ 2 ] ) NEW_LINE enhanced = self . G ( mixture ) NEW_LINE enhanced_D = enhanced . detach ( ) NEW_LINE ae_ny_G = self . D . forward_paired ( enhanced , mixture ) NEW_LINE l_adv_ny_G , _ = self . diffLoss ( ae_ny_G , enhanced , mask ) NEW_LINE l_adv_ny_G = l_adv_ny_G * self . config . w_adversarial NEW_LINE l_adv_ny_G_data = l_adv_ny_G . data [ 0 ] NEW_LINE l_adv_ny_G . backward ( retain_graph = True ) NEW_LINE g_adv = self . get_gradient_norm ( self . G ) NEW_LINE self . D . zero_grad ( ) NEW_LINE del l_adv_ny_G NEW_LINE ae_ny_D = self . D . forward_paired ( enhanced_D , mixture ) NEW_LINE l_adv_ny_D , _ = self . diffLoss ( ae_ny_D , enhanced_D , mask ) NEW_LINE l_adv_ny_D = l_adv_ny_D * ( - self . kt ) * self . config . w_adversarial NEW_LINE l_adv_ny_D . backward ( ) NEW_LINE del l_adv_ny_D NEW_LINE dce , nElement = self . diffLoss ( enhanced , cleans , mask ) NEW_LINE dce_loss = dce . data [ 0 ] NEW_LINE dce_tr_local . update ( dce_loss , nElement ) NEW_LINE ae_cl = self . D ( cleans , mixture ) NEW_LINE l_adv_cl , _ = self . diffLoss ( ae_cl , cleans , mask ) NEW_LINE l_adv_cl = self . config . w_adversarial * l_adv_cl NEW_LINE l_adv_cl . backward ( ) NEW_LINE l_adv_cl_data = l_adv_cl . data [ 0 ] NEW_LINE del l_adv_cl NEW_LINE optimizer_g . step ( ) NEW_LINE optimizer_d . step ( ) NEW_LINE g_d_balance = self . gamma * l_adv_cl_data - l_adv_ny_G_data NEW_LINE self . kt += self . lb * g_d_balance NEW_LINE self . kt = max ( min ( 1 , self . kt ) , 0 ) NEW_LINE conv_measure = l_adv_cl_data + abs ( g_d_balance ) NEW_LINE if ( iter + 1 ) % self . config . log_iter == 0 : NEW_LINE INDENT str_loss = \" [ { } / { } ] ▁ ( train ) ▁ DCE : ▁ { : . 7f } , ▁ ADV _ cl : ▁ { : . 7f } , ▁ ADV _ ny : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . dce_tr_local . avg , l_adv_cl_data , l_adv_ny_G_data ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE str_loss = \" [ { } / { } ] ▁ ( train ) ▁ conv _ measure : ▁ { : . 4f } , ▁ kt : ▁ { : . 4f } ▁ \" . format ( iter , self . config . max_iter , conv_measure , self . kt ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . dce_tr_local . reset ( ) NEW_LINE DEDENT if ( iter + 1 ) % self . config . save_iter == 0 : NEW_LINE INDENT self . G . eval ( ) NEW_LINE self . dce_tr . reset ( ) NEW_LINE self . adv_ny_tr . reset ( ) NEW_LINE self . wer_tr . reset ( ) NEW_LINE self . cer_tr . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . trsub_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' trsub \\' ) NEW_LINE mixture , cleans , mask , targets , input_percentages , target_sizes = data_list [ 0 ] , data_list [ 1 ] , _get_variable_volatile ( data_list [ 2 ] ) , data_list [ 3 ] , data_list [ 4 ] , data_list [ 5 ] NEW_LINE dce , adv_ny , nElement , wer , cer , nWord , nChar = self . greedy_decoding_and_FSEGAN ( mixture , cleans , targets , input_percentages , target_sizes , mask ) NEW_LINE self . dce_tr . update ( dce . data [ 0 ] , nElement ) NEW_LINE self . adv_ny_tr . update ( adv_ny . data [ 0 ] , nElement ) NEW_LINE self . wer_tr . update ( wer , nWord ) NEW_LINE self . cer_tr . update ( cer , nChar ) NEW_LINE del dce , adv_ny NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( training ▁ subset ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . dce_tr . avg , self . wer_tr . avg * 100 , self . cer_tr . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . dce_val . reset ( ) NEW_LINE self . adv_ny_val . reset ( ) NEW_LINE self . wer_val . reset ( ) NEW_LINE self . cer_val . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . val_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' val \\' ) NEW_LINE mixture , cleans , mask , targets , input_percentages , target_sizes = data_list [ 0 ] , data_list [ 1 ] , _get_variable_volatile ( data_list [ 2 ] ) , data_list [ 3 ] , data_list [ 4 ] , data_list [ 5 ] NEW_LINE dce , adv_ny , nElement , wer , cer , nWord , nChar = self . greedy_decoding_and_FSEGAN ( mixture , cleans , targets , input_percentages , target_sizes , mask ) NEW_LINE self . dce_val . update ( dce . data [ 0 ] , nElement ) NEW_LINE self . adv_ny_val . update ( adv_ny . data [ 0 ] , nElement ) NEW_LINE self . wer_val . update ( wer , nWord ) NEW_LINE self . cer_val . update ( cer , nChar ) NEW_LINE del ctc , adv_ny NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( validation ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . dce_val . avg , self . wer_val . avg * 100 , self . cer_val . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . G . train ( ) NEW_LINE if ( len ( self . savename_G ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_G ) : NEW_LINE INDENT os . remove ( self . savename_G ) NEW_LINE DEDENT DEDENT self . savename_G = \\' { } / G _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . G . state_dict ( ) , self . savename_G ) NEW_LINE if ( self . G . loss_stop > self . wer_val . avg ) : NEW_LINE INDENT self . G . loss_stop = self . wer_val . avg NEW_LINE savename_G_valmin_prev = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_G_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_G_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_G_valmin = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_G , savename_G_valmin ) NEW_LINE self . valmin_iter = iter NEW_LINE DEDENT DEDENT DEDENT DEDENT',\n",
              " \"def greedy_decoding_and_FSEGAN ( self , mixture , cleans , targets , input_percentages , target_sizes , mask , transcript_prob = 0.001 ) : NEW_LINE INDENT mixture = _get_variable_volatile ( mixture ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE split_targets = [ ] NEW_LINE offset = 0 NEW_LINE for size in target_sizes : NEW_LINE INDENT split_targets . append ( targets [ offset : offset + size ] ) NEW_LINE offset += size NEW_LINE DEDENT enhanced = self . G ( mixture ) NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = input_percentages . mul_ ( int ( T ) ) . int ( ) NEW_LINE decoded_output , _ = self . decoder . decode ( prob . data , sizes ) NEW_LINE target_strings = self . decoder . convert_to_strings ( split_targets ) NEW_LINE we , ce , total_word , total_char = 0 , 0 , 0 , 0 NEW_LINE for x in range ( len ( target_strings ) ) : NEW_LINE INDENT decoding , reference = decoded_output [ x ] [ 0 ] , target_strings [ x ] [ 0 ] NEW_LINE nChar = len ( reference ) NEW_LINE nWord = len ( reference . split ( ) ) NEW_LINE we_i = self . decoder . wer ( decoding , reference ) NEW_LINE ce_i = self . decoder . cer ( decoding , reference ) NEW_LINE we += we_i NEW_LINE ce += ce_i NEW_LINE total_word += nWord NEW_LINE total_char += nChar NEW_LINE if ( random . uniform ( 0 , 1 ) < transcript_prob ) : NEW_LINE INDENT print ( ' reference ▁ = ▁ ' + reference ) NEW_LINE print ( ' decoding ▁ = ▁ ' + decoding ) NEW_LINE print ( ' wer ▁ = ▁ ' + str ( we_i / float ( nWord ) ) + ' , ▁ cer ▁ = ▁ ' + str ( ce_i / float ( nChar ) ) ) NEW_LINE DEDENT DEDENT wer = we / total_word NEW_LINE cer = ce / total_word NEW_LINE ae_ny = self . D . forward_paired ( enhanced , mixture ) NEW_LINE l_adv_ny , nElement = self . diffLoss ( ae_ny , enhanced , mask ) NEW_LINE l_adv_ny = l_adv_ny * self . config . w_adversarial NEW_LINE dce , nElement_ = self . diffLoss ( enhanced , cleans , mask ) NEW_LINE assert ( nElement == nElement_ ) NEW_LINE return dce , l_adv_ny , nElement , wer , cer , total_word , total_char NEW_LINE DEDENT\",\n",
              " \"def str2bool ( v ) : NEW_LINE INDENT return v . lower ( ) in ( ' true ' , '1' ) NEW_LINE DEDENT\",\n",
              " 'def add_argument_group ( name ) : NEW_LINE INDENT arg = parser . add_argument_group ( name ) NEW_LINE arg_lists . append ( arg ) NEW_LINE return arg NEW_LINE DEDENT',\n",
              " \"def get_config ( ) : NEW_LINE INDENT config , unparsed = parser . parse_known_args ( ) NEW_LINE if ( len ( unparsed ) > 0 ) : NEW_LINE INDENT print ( unparsed ) NEW_LINE assert ( len ( unparsed ) == 0 ) , ' length ▁ of ▁ unparsed ▁ option ▁ should ▁ be ▁ 0' NEW_LINE DEDENT return config , unparsed NEW_LINE DEDENT\",\n",
              " 'def random_combination ( iterable , r ) : NEW_LINE INDENT pool = tuple ( iterable ) NEW_LINE n = len ( pool ) NEW_LINE indices = sorted ( random . sample ( range ( n ) , r ) ) NEW_LINE return tuple ( pool [ i ] for i in indices ) NEW_LINE DEDENT',\n",
              " \"def check_config_used ( config , target_source ) : NEW_LINE INDENT config_count = dict ( vars ( config ) ) NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT config_count [ k ] = 0 NEW_LINE DEDENT for source in target_source : NEW_LINE INDENT fp = open ( source , ' r ' ) NEW_LINE text = fp . read ( ) NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT if ( text . find ( k ) >= 0 ) : NEW_LINE INDENT config_count [ k ] = 1 NEW_LINE DEDENT DEDENT fp . close ( ) NEW_LINE DEDENT config_unused = [ ] NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT if ( config_count [ k ] == 0 ) : NEW_LINE INDENT config_unused . append ( k ) NEW_LINE DEDENT DEDENT print ( ' unused ▁ config ▁ = ▁ ' ) NEW_LINE print ( config_unused ) NEW_LINE assert ( len ( config_unused ) == 0 ) , ' unused ▁ config ▁ exists , ▁ please ▁ properly ▁ use ▁ it ▁ or ▁ comment ▁ it ' NEW_LINE DEDENT\",\n",
              " 'def to_np ( x ) : NEW_LINE INDENT return x . data . cpu ( ) . numpy ( ) NEW_LINE DEDENT',\n",
              " \"def get_weight_statistic ( M ) : NEW_LINE INDENT print ( ' Model ▁ parameter ▁ statistic ' ) NEW_LINE modules = list ( M . modules ( ) ) [ 0 ] . _modules NEW_LINE for k , v in modules . items ( ) : NEW_LINE INDENT if ( len ( v . state_dict ( ) ) > 2 ) : NEW_LINE INDENT for l in range ( len ( v ) ) : NEW_LINE INDENT layer = v [ l ] NEW_LINE if ( hasattr ( layer , ' module ' ) ) : NEW_LINE INDENT layer_m = layer . module NEW_LINE for j in range ( len ( layer_m ) ) : NEW_LINE INDENT sublayer = layer_m [ j ] NEW_LINE if ( hasattr ( sublayer , ' bias ' ) ) : NEW_LINE INDENT if ( sublayer . bias is not None ) : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( sublayer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT else : NEW_LINE INDENT if ( hasattr ( layer , ' weight ' ) ) : NEW_LINE INDENT if ( hasattr ( layer , ' bias ' ) ) : NEW_LINE INDENT if ( hasattr ( layer . bias , ' data ' ) ) : NEW_LINE INDENT print ( str ( layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( layer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT print ( str ( layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT if ( hasattr ( layer , ' rnn ' ) ) : NEW_LINE INDENT rnn_layer = layer . rnn NEW_LINE print ( str ( rnn_layer ) ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0 . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0 . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0 . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 _ reverse ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0_reverse . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ ih _ l0 ▁ = ▁ ' + str ( rnn_layer . weight_ih_l0 . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_ih_l0 . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_ih_l0 . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 _ reverse ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0_reverse . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT if ( hasattr ( layer , ' batch _ norm ' ) ) : NEW_LINE INDENT if ( layer . batch_norm ) : NEW_LINE INDENT bn_layer = layer . batch_norm . module NEW_LINE print ( str ( bn_layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( bn_layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( bn_layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( bn_layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) , ▁ bias ▁ = ▁ ' + str ( bn_layer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( bn_layer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( bn_layer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT DEDENT DEDENT else : NEW_LINE INDENT if ( hasattr ( v , ' weight ' ) ) : NEW_LINE INDENT if ( hasattr ( v , ' bias ' ) ) : NEW_LINE INDENT print ( k + ' ▁ : ▁ weight ▁ = ▁ ' + str ( v . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( v . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( k + ' ▁ : ▁ weight ▁ = ▁ ' + str ( v . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT print ( ' ▁ ' ) NEW_LINE print ( ' ▁ ' ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 and classname . find ( ' ConvResidualBlock ' ) == - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.1 ) NEW_LINE if hasattr ( m , ' bias ' ) : NEW_LINE INDENT if ( hasattr ( m . bias , ' data ' ) ) : NEW_LINE INDENT m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT DEDENT DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def _get_variable ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def _get_variable_volatile ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) , volatile = True ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs , volatile = True ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def _get_variable_nograd ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) , requires_grad = False ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs , requires_grad = False ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def __init__ ( self ) : NEW_LINE INDENT self . reset ( ) NEW_LINE DEDENT',\n",
              " 'def reset ( self ) : NEW_LINE INDENT self . val = 0 NEW_LINE self . avg = 0 NEW_LINE self . sum = 0 NEW_LINE self . count = 0 NEW_LINE DEDENT',\n",
              " 'def update ( self , val , n = 1 ) : NEW_LINE INDENT self . val = val NEW_LINE self . sum += val * n NEW_LINE self . count += n NEW_LINE self . avg = self . sum / self . count NEW_LINE DEDENT',\n",
              " 'def decode_dataset ( logits , test_dataset , batch_size , lm_alpha , lm_beta , mesh_x , mesh_y , labels ) : NEW_LINE INDENT print ( \" Beginning ▁ decode ▁ for ▁ { } , ▁ { } \" . format ( lm_alpha , lm_beta ) ) NEW_LINE test_loader = FeatLoader ( test_dataset , batch_size = batch_size , num_workers = 0 ) NEW_LINE target_decoder = GreedyDecoder ( labels , blank_index = labels . index ( \\' _ \\' ) ) NEW_LINE decoder = BeamCTCDecoder ( labels , beam_width = args . beam_width , cutoff_top_n = args . cutoff_top_n , blank_index = labels . index ( \\' _ \\' ) , lm_path = args . lm_path , alpha = lm_alpha , beta = lm_beta , num_processes = 1 ) NEW_LINE total_cer , total_wer = 0 , 0 NEW_LINE for i , ( data ) in enumerate ( test_loader ) : NEW_LINE INDENT inputs , targets , input_percentages , target_sizes = data NEW_LINE split_targets = [ ] NEW_LINE offset = 0 NEW_LINE for size in target_sizes : NEW_LINE INDENT split_targets . append ( targets [ offset : offset + size ] ) NEW_LINE offset += size NEW_LINE DEDENT out = torch . from_numpy ( logits [ i ] [ 0 ] ) NEW_LINE sizes = torch . from_numpy ( logits [ i ] [ 1 ] ) NEW_LINE decoded_output , _ = decoder . decode ( out , sizes ) NEW_LINE target_strings = target_decoder . convert_to_strings ( split_targets ) NEW_LINE wer , cer = 0 , 0 NEW_LINE for x in range ( len ( target_strings ) ) : NEW_LINE INDENT transcript , reference = decoded_output [ x ] [ 0 ] , target_strings [ x ] [ 0 ] NEW_LINE wer_inst = decoder . wer ( transcript , reference ) / float ( len ( reference . split ( ) ) ) NEW_LINE cer_inst = decoder . cer ( transcript , reference ) / float ( len ( reference ) ) NEW_LINE wer += wer_inst NEW_LINE cer += cer_inst NEW_LINE if ( random . uniform ( 0 , 1 ) < float ( args . detail_log_print_prob ) ) : NEW_LINE INDENT print ( \\' decoding ▁ : ▁ \\' + transcript ) NEW_LINE print ( \\' reference ▁ : ▁ \\' + reference ) NEW_LINE print ( \\' WER ▁ = ▁ \\' + str ( wer_inst ) + \\' , ▁ CER ▁ = ▁ \\' + str ( cer_inst ) ) NEW_LINE print ( \\' ▁ \\' ) NEW_LINE logger . error ( \\' decoding ▁ : ▁ \\' + transcript ) NEW_LINE logger . error ( \\' reference ▁ : ▁ \\' + reference ) NEW_LINE logger . error ( \\' WER ▁ = ▁ \\' + str ( wer_inst ) + \\' , ▁ CER ▁ = ▁ \\' + str ( cer_inst ) ) NEW_LINE logger . error ( \\' ▁ \\' ) NEW_LINE DEDENT DEDENT total_cer += cer NEW_LINE total_wer += wer NEW_LINE DEDENT wer = total_wer / len ( test_loader . dataset ) NEW_LINE cer = total_cer / len ( test_loader . dataset ) NEW_LINE return [ mesh_x , mesh_y , lm_alpha , lm_beta , wer , cer ] NEW_LINE DEDENT',\n",
              " 'def getWER ( item ) : NEW_LINE INDENT return item [ 5 ] NEW_LINE DEDENT',\n",
              " 'def result_callback ( result ) : NEW_LINE INDENT results . append ( result ) NEW_LINE DEDENT',\n",
              " \"def __init__ ( self , labels , blank_index = 0 ) : NEW_LINE INDENT self . labels = labels NEW_LINE self . int_to_char = dict ( [ ( i , c ) for ( i , c ) in enumerate ( labels ) ] ) NEW_LINE self . blank_index = blank_index NEW_LINE space_index = len ( labels ) NEW_LINE if ' ▁ ' in labels : NEW_LINE INDENT space_index = labels . index ( ' ▁ ' ) NEW_LINE DEDENT self . space_index = space_index NEW_LINE DEDENT\",\n",
              " \"def wer ( self , s1 , s2 ) : NEW_LINE INDENT b = set ( s1 . split ( ) + s2 . split ( ) ) NEW_LINE word2char = dict ( zip ( b , range ( len ( b ) ) ) ) NEW_LINE w1 = [ chr ( word2char [ w ] ) for w in s1 . split ( ) ] NEW_LINE w2 = [ chr ( word2char [ w ] ) for w in s2 . split ( ) ] NEW_LINE return Lev . distance ( ' ' . join ( w1 ) , ' ' . join ( w2 ) ) NEW_LINE DEDENT\",\n",
              " \"def cer ( self , s1 , s2 ) : NEW_LINE INDENT s1 , s2 , = s1 . replace ( ' ▁ ' , ' ' ) , s2 . replace ( ' ▁ ' , ' ' ) NEW_LINE return Lev . distance ( s1 , s2 ) NEW_LINE DEDENT\",\n",
              " 'def decode ( self , probs , sizes = None ) : NEW_LINE INDENT raise NotImplementedError NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , labels , lm_path = None , alpha = 0 , beta = 0 , cutoff_top_n = 40 , cutoff_prob = 1.0 , beam_width = 100 , num_processes = 4 , blank_index = 0 ) : NEW_LINE INDENT super ( BeamCTCDecoder , self ) . __init__ ( labels ) NEW_LINE try : NEW_LINE INDENT from ctcdecode import CTCBeamDecoder NEW_LINE DEDENT except ImportError : NEW_LINE INDENT raise ImportError ( \" BeamCTCDecoder ▁ requires ▁ paddledecoder ▁ package . \" ) NEW_LINE DEDENT self . _decoder = CTCBeamDecoder ( labels , lm_path , alpha , beta , cutoff_top_n , cutoff_prob , beam_width , num_processes , blank_index ) NEW_LINE DEDENT',\n",
              " \"def convert_to_strings ( self , out , seq_len ) : NEW_LINE INDENT results = [ ] NEW_LINE for b , batch in enumerate ( out ) : NEW_LINE INDENT utterances = [ ] NEW_LINE for p , utt in enumerate ( batch ) : NEW_LINE INDENT size = seq_len [ b ] [ p ] NEW_LINE if size > 0 : NEW_LINE INDENT transcript = ' ' . join ( map ( lambda x : self . int_to_char [ x ] , utt [ 0 : size ] ) ) NEW_LINE DEDENT else : NEW_LINE INDENT transcript = ' ' NEW_LINE DEDENT utterances . append ( transcript ) NEW_LINE DEDENT results . append ( utterances ) NEW_LINE DEDENT return results NEW_LINE DEDENT\",\n",
              " 'def convert_tensor ( self , offsets , sizes ) : NEW_LINE INDENT results = [ ] NEW_LINE for b , batch in enumerate ( offsets ) : NEW_LINE INDENT utterances = [ ] NEW_LINE for p , utt in enumerate ( batch ) : NEW_LINE INDENT size = sizes [ b ] [ p ] NEW_LINE if sizes [ b ] [ p ] > 0 : NEW_LINE INDENT utterances . append ( utt [ 0 : size ] ) NEW_LINE DEDENT else : NEW_LINE INDENT utterances . append ( torch . IntTensor ( ) ) NEW_LINE DEDENT DEDENT results . append ( utterances ) NEW_LINE DEDENT return results NEW_LINE DEDENT',\n",
              " 'def decode ( self , probs , sizes = None ) : NEW_LINE INDENT probs = probs . cpu ( ) . transpose ( 0 , 1 ) . contiguous ( ) NEW_LINE out , scores , offsets , seq_lens = self . _decoder . decode ( probs ) NEW_LINE strings = self . convert_to_strings ( out , seq_lens ) NEW_LINE offsets = self . convert_tensor ( offsets , seq_lens ) NEW_LINE return strings , offsets NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , labels , blank_index = 0 ) : NEW_LINE INDENT super ( GreedyDecoder , self ) . __init__ ( labels , blank_index ) NEW_LINE DEDENT',\n",
              " \"def process_string ( self , sequence , size , remove_repetitions = False ) : NEW_LINE INDENT string = ' ' NEW_LINE offsets = [ ] NEW_LINE for i in range ( size ) : NEW_LINE INDENT if ( sequence [ i ] != self . blank_index ) : NEW_LINE INDENT char = self . int_to_char [ sequence [ i ] ] NEW_LINE if remove_repetitions and i != 0 and char == self . int_to_char [ sequence [ i - 1 ] ] : NEW_LINE INDENT pass NEW_LINE DEDENT elif char == self . labels [ self . space_index ] : NEW_LINE INDENT string += ' ▁ ' NEW_LINE offsets . append ( i ) NEW_LINE DEDENT else : NEW_LINE INDENT string = string + char NEW_LINE offsets . append ( i ) NEW_LINE DEDENT DEDENT DEDENT return string , torch . IntTensor ( offsets ) NEW_LINE DEDENT\",\n",
              " 'def decode ( self , probs , sizes = None ) : NEW_LINE INDENT _ , max_probs = torch . max ( probs . transpose ( 0 , 1 ) , 2 ) NEW_LINE strings , offsets = self . convert_to_strings ( max_probs . view ( max_probs . size ( 0 ) , max_probs . size ( 1 ) ) , sizes , remove_repetitions = True , return_offsets = True ) NEW_LINE return strings , offsets NEW_LINE DEDENT',\n",
              " \"def str2bool ( v ) : NEW_LINE INDENT return v . lower ( ) in ( ' true ' , '1' ) NEW_LINE DEDENT\",\n",
              " \"def str2bool ( v ) : NEW_LINE INDENT return v . lower ( ) in ( ' true ' , '1' ) NEW_LINE DEDENT\",\n",
              " \"def load_model ( cls , path , gpu = - 1 ) : NEW_LINE INDENT package = torch . load ( path , map_location = lambda storage , loc : storage ) NEW_LINE model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] ) NEW_LINE blacklist = [ ' rnns . 0 . batch _ norm . module . weight ' , ' rnns . 0 . batch _ norm . module . bias ' , ' rnns . 0 . batch _ norm . module . running _ mean ' , ' rnns . 0 . batch _ norm . module . running _ var ' ] NEW_LINE for x in blacklist : NEW_LINE INDENT if x in package [ ' state _ dict ' ] : NEW_LINE INDENT del package [ ' state _ dict ' ] [ x ] NEW_LINE DEDENT DEDENT model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE for x in model . rnns : NEW_LINE INDENT x . flatten_parameters ( ) NEW_LINE DEDENT if gpu >= 0 : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def load_model_package ( cls , package , gpu = - 1 ) : NEW_LINE INDENT model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] , ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE if ( gpu >= 0 ) : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def serialize ( model , optimizer = None , epoch = None , iteration = None , loss_results = None , cer_results = None , wer_results = None , avg_loss = None , meta = None ) : NEW_LINE INDENT package = { ' version ' : model . _version , ' rnn _ size ' : model . rnn_size , ' rnn _ layers ' : model . rnn_layers , ' cnn _ map ' : model . cnn_map , ' cnn _ kernel ' : model . cnn_kernel , ' cnn _ stride ' : model . cnn_stride , ' cnn _ layers ' : model . cnn_layers , ' rnn _ type ' : supported_rnns_inv . get ( model . rnn_type , model . rnn_type . __name__ . lower ( ) ) , ' labels ' : model . _labels , ' state _ dict ' : model . state_dict ( ) } NEW_LINE if optimizer is not None : NEW_LINE INDENT package [ ' optim _ dict ' ] = optimizer . state_dict ( ) NEW_LINE DEDENT if avg_loss is not None : NEW_LINE INDENT package [ ' avg _ loss ' ] = avg_loss NEW_LINE DEDENT if epoch is not None : NEW_LINE INDENT package [ ' epoch ' ] = epoch + 1 NEW_LINE DEDENT if iteration is not None : NEW_LINE INDENT package [ ' iteration ' ] = iteration NEW_LINE DEDENT if loss_results is not None : NEW_LINE INDENT package [ ' loss _ results ' ] = loss_results NEW_LINE package [ ' cer _ results ' ] = cer_results NEW_LINE package [ ' wer _ results ' ] = wer_results NEW_LINE DEDENT if meta is not None : NEW_LINE INDENT package [ ' meta ' ] = meta NEW_LINE DEDENT return package NEW_LINE DEDENT\",\n",
              " 'def get_labels ( model ) : NEW_LINE INDENT return model . _labels NEW_LINE DEDENT',\n",
              " 'def get_param_size ( model ) : NEW_LINE INDENT params = 0 NEW_LINE for p in model . parameters ( ) : NEW_LINE INDENT tmp = 1 NEW_LINE for x in p . size ( ) : NEW_LINE INDENT tmp *= x NEW_LINE DEDENT params += tmp NEW_LINE DEDENT return params NEW_LINE DEDENT',\n",
              " 'def get_audio_conf ( model ) : NEW_LINE INDENT return model . _audio_conf NEW_LINE DEDENT',\n",
              " 'def get_meta ( model ) : NEW_LINE INDENT model_is_cuda = next ( model . parameters ( ) ) . is_cuda NEW_LINE m = model . module if model_is_cuda else model NEW_LINE meta = { \" version \" : m . _version , \" rnn _ size \" : m . rnn_size , \" rnn _ layers \" : m . rnn_layers , \" cnn _ map \" : m . cnn_map , \" cnn _ kernel \" : m . cnn_kernel , \" cnn _ stride \" : m . cnn_stride , \" cnn _ layers \" : m . cnn_layers , \" rnn _ type \" : supported_rnns_inv [ m . rnn_type ] } NEW_LINE return meta NEW_LINE DEDENT',\n",
              " \"def load_model ( cls , path , gpu = - 1 ) : NEW_LINE INDENT package = torch . load ( path , map_location = lambda storage , loc : storage ) NEW_LINE model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] ) NEW_LINE blacklist = [ ' rnns . 0 . batch _ norm . module . weight ' , ' rnns . 0 . batch _ norm . module . bias ' , ' rnns . 0 . batch _ norm . module . running _ mean ' , ' rnns . 0 . batch _ norm . module . running _ var ' ] NEW_LINE for x in blacklist : NEW_LINE INDENT if x in package [ ' state _ dict ' ] : NEW_LINE INDENT del package [ ' state _ dict ' ] [ x ] NEW_LINE DEDENT DEDENT model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE for x in model . rnns : NEW_LINE INDENT x . flatten_parameters ( ) NEW_LINE DEDENT if gpu >= 0 : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def load_model_package ( cls , package , gpu = - 1 ) : NEW_LINE INDENT model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] , ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE if ( gpu >= 0 ) : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def serialize ( model , optimizer = None , epoch = None , iteration = None , loss_results = None , cer_results = None , wer_results = None , avg_loss = None , param_norm = None , param_max = None , grad_norm = None , grad_max = None , meta = None ) : NEW_LINE INDENT package = { ' version ' : model . _version , ' rnn _ size ' : model . rnn_size , ' rnn _ layers ' : model . rnn_layers , ' cnn _ map ' : model . cnn_map , ' cnn _ kernel ' : model . cnn_kernel , ' cnn _ stride ' : model . cnn_stride , ' cnn _ layers ' : model . cnn_layers , ' rnn _ type ' : supported_rnns_inv . get ( model . rnn_type , model . rnn_type . __name__ . lower ( ) ) , ' labels ' : model . _labels , ' state _ dict ' : model . state_dict ( ) } NEW_LINE if optimizer is not None : NEW_LINE INDENT package [ ' optim _ dict ' ] = optimizer . state_dict ( ) NEW_LINE DEDENT if avg_loss is not None : NEW_LINE INDENT package [ ' avg _ loss ' ] = avg_loss NEW_LINE DEDENT if epoch is not None : NEW_LINE INDENT package [ ' epoch ' ] = epoch + 1 NEW_LINE DEDENT if iteration is not None : NEW_LINE INDENT package [ ' iteration ' ] = iteration NEW_LINE DEDENT if loss_results is not None : NEW_LINE INDENT package [ ' loss _ results ' ] = loss_results NEW_LINE package [ ' cer _ results ' ] = cer_results NEW_LINE package [ ' wer _ results ' ] = wer_results NEW_LINE package [ ' param _ norm ' ] = param_norm NEW_LINE package [ ' param _ max ' ] = param_max NEW_LINE package [ ' grad _ norm ' ] = grad_norm NEW_LINE package [ ' grad _ max ' ] = grad_max NEW_LINE DEDENT if meta is not None : NEW_LINE INDENT package [ ' meta ' ] = meta NEW_LINE DEDENT return package NEW_LINE DEDENT\",\n",
              " 'def get_labels ( model ) : NEW_LINE INDENT return model . _labels NEW_LINE DEDENT',\n",
              " 'def get_param_size ( model ) : NEW_LINE INDENT params = 0 NEW_LINE for p in model . parameters ( ) : NEW_LINE INDENT tmp = 1 NEW_LINE for x in p . size ( ) : NEW_LINE INDENT tmp *= x NEW_LINE DEDENT params += tmp NEW_LINE DEDENT return params NEW_LINE DEDENT',\n",
              " 'def get_audio_conf ( model ) : NEW_LINE INDENT return model . _audio_conf NEW_LINE DEDENT',\n",
              " 'def get_meta ( model ) : NEW_LINE INDENT model_is_cuda = next ( model . parameters ( ) ) . is_cuda NEW_LINE m = model . module if model_is_cuda else model NEW_LINE meta = { \" version \" : m . _version , \" rnn _ size \" : m . rnn_size , \" rnn _ layers \" : m . rnn_layers , \" cnn _ map \" : m . cnn_map , \" cnn _ kernel \" : m . cnn_kernel , \" cnn _ stride \" : m . cnn_stride , \" cnn _ layers \" : m . cnn_layers , \" rnn _ type \" : supported_rnns_inv [ m . rnn_type ] } NEW_LINE return meta NEW_LINE DEDENT',\n",
              " \"def load_model ( cls , path , gpu = - 1 ) : NEW_LINE INDENT package = torch . load ( path , map_location = lambda storage , loc : storage ) NEW_LINE model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_residual_blocks = package [ ' cnn _ residual _ blocks ' ] , labels = package [ ' labels ' ] ) NEW_LINE blacklist = [ ' rnns . 0 . batch _ norm . module . weight ' , ' rnns . 0 . batch _ norm . module . bias ' , ' rnns . 0 . batch _ norm . module . running _ mean ' , ' rnns . 0 . batch _ norm . module . running _ var ' ] NEW_LINE for x in blacklist : NEW_LINE INDENT if x in package [ ' state _ dict ' ] : NEW_LINE INDENT del package [ ' state _ dict ' ] [ x ] NEW_LINE DEDENT DEDENT model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE for x in model . rnns : NEW_LINE INDENT x . flatten_parameters ( ) NEW_LINE DEDENT if gpu >= 0 : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def load_model_package ( cls , package , gpu = - 1 ) : NEW_LINE INDENT model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_residual_blocks = package [ ' cnn _ residual _ blocks ' ] , labels = package [ ' labels ' ] , ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE if ( gpu >= 0 ) : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def serialize ( model , optimizer = None , epoch = None , iteration = None , loss_results = None , cer_results = None , wer_results = None , avg_loss = None , meta = None ) : NEW_LINE INDENT package = { ' version ' : model . _version , ' rnn _ size ' : model . rnn_size , ' rnn _ layers ' : model . rnn_layers , ' cnn _ map ' : model . cnn_map , ' cnn _ kernel ' : model . cnn_kernel , ' cnn _ stride ' : model . cnn_stride , ' cnn _ residual _ blocks ' : model . cnn_residual_blocks , ' rnn _ type ' : supported_rnns_inv . get ( model . rnn_type , model . rnn_type . __name__ . lower ( ) ) , ' labels ' : model . _labels , ' state _ dict ' : model . state_dict ( ) } NEW_LINE if optimizer is not None : NEW_LINE INDENT package [ ' optim _ dict ' ] = optimizer . state_dict ( ) NEW_LINE DEDENT if avg_loss is not None : NEW_LINE INDENT package [ ' avg _ loss ' ] = avg_loss NEW_LINE DEDENT if epoch is not None : NEW_LINE INDENT package [ ' epoch ' ] = epoch + 1 NEW_LINE DEDENT if iteration is not None : NEW_LINE INDENT package [ ' iteration ' ] = iteration NEW_LINE DEDENT if loss_results is not None : NEW_LINE INDENT package [ ' loss _ results ' ] = loss_results NEW_LINE package [ ' cer _ results ' ] = cer_results NEW_LINE package [ ' wer _ results ' ] = wer_results NEW_LINE DEDENT if meta is not None : NEW_LINE INDENT package [ ' meta ' ] = meta NEW_LINE DEDENT return package NEW_LINE DEDENT\",\n",
              " 'def get_labels ( model ) : NEW_LINE INDENT return model . _labels NEW_LINE DEDENT',\n",
              " 'def get_param_size ( model ) : NEW_LINE INDENT params = 0 NEW_LINE for p in model . parameters ( ) : NEW_LINE INDENT tmp = 1 NEW_LINE for x in p . size ( ) : NEW_LINE INDENT tmp *= x NEW_LINE DEDENT params += tmp NEW_LINE DEDENT return params NEW_LINE DEDENT',\n",
              " 'def get_audio_conf ( model ) : NEW_LINE INDENT return model . _audio_conf NEW_LINE DEDENT',\n",
              " 'def get_meta ( model ) : NEW_LINE INDENT model_is_cuda = next ( model . parameters ( ) ) . is_cuda NEW_LINE m = model . module if model_is_cuda else model NEW_LINE meta = { \" version \" : m . _version , \" rnn _ size \" : m . rnn_size , \" rnn _ layers \" : m . rnn_layers , \" cnn _ map \" : m . cnn_map , \" cnn _ kernel \" : m . cnn_kernel , \" cnn _ stride \" : m . cnn_stride , \" cnn _ layers \" : m . cnn_layers , \" rnn _ type \" : supported_rnns_inv [ m . rnn_type ] } NEW_LINE return meta NEW_LINE DEDENT',\n",
              " \"def load_model ( cls , path , gpu = - 1 ) : NEW_LINE INDENT package = torch . load ( path , map_location = lambda storage , loc : storage ) NEW_LINE model = cls ( map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , labels = package [ ' labels ' ] ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE for x in model . rnns : NEW_LINE INDENT x . flatten_parameters ( ) NEW_LINE DEDENT if gpu >= 0 : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def load_model_package ( cls , package , gpu = - 1 ) : NEW_LINE INDENT model = cls ( map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , labels = package [ ' labels ' ] , ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE if ( gpu >= 0 ) : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def serialize ( model , optimizer = None , epoch = None , iteration = None , loss_results = None , cer_results = None , wer_results = None , avg_loss = None , meta = None ) : NEW_LINE INDENT package = { ' version ' : model . _version , ' cnn _ map ' : model . cnn_map , ' cnn _ kernel ' : model . cnn_kernel , ' cnn _ stride ' : model . cnn_stride , ' labels ' : model . _labels , ' state _ dict ' : model . state_dict ( ) } NEW_LINE if optimizer is not None : NEW_LINE INDENT package [ ' optim _ dict ' ] = optimizer . state_dict ( ) NEW_LINE DEDENT if avg_loss is not None : NEW_LINE INDENT package [ ' avg _ loss ' ] = avg_loss NEW_LINE DEDENT if epoch is not None : NEW_LINE INDENT package [ ' epoch ' ] = epoch + 1 NEW_LINE DEDENT if iteration is not None : NEW_LINE INDENT package [ ' iteration ' ] = iteration NEW_LINE DEDENT if loss_results is not None : NEW_LINE INDENT package [ ' loss _ results ' ] = loss_results NEW_LINE package [ ' cer _ results ' ] = cer_results NEW_LINE package [ ' wer _ results ' ] = wer_results NEW_LINE DEDENT if meta is not None : NEW_LINE INDENT package [ ' meta ' ] = meta NEW_LINE DEDENT return package NEW_LINE DEDENT\",\n",
              " 'def get_labels ( model ) : NEW_LINE INDENT return model . _labels NEW_LINE DEDENT',\n",
              " 'def get_param_size ( model ) : NEW_LINE INDENT params = 0 NEW_LINE for p in model . parameters ( ) : NEW_LINE INDENT tmp = 1 NEW_LINE for x in p . size ( ) : NEW_LINE INDENT tmp *= x NEW_LINE DEDENT params += tmp NEW_LINE DEDENT return params NEW_LINE DEDENT',\n",
              " 'def get_audio_conf ( model ) : NEW_LINE INDENT return model . _audio_conf NEW_LINE DEDENT',\n",
              " 'def get_meta ( model ) : NEW_LINE INDENT model_is_cuda = next ( model . parameters ( ) ) . is_cuda NEW_LINE m = model . module if model_is_cuda else model NEW_LINE meta = { \" version \" : m . _version , \" cnn _ map \" : m . cnn_map , \" cnn _ kernel \" : m . cnn_kernel , \" cnn _ stride \" : m . cnn_stride , \" cnn _ layers \" : m . cnn_layers , } NEW_LINE return meta NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , module ) : NEW_LINE INDENT super ( SequenceWise , self ) . __init__ ( ) NEW_LINE self . module = module NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT t , n = x . size ( 0 ) , x . size ( 1 ) NEW_LINE x = x . view ( t * n , - 1 ) NEW_LINE x = self . module ( x ) NEW_LINE x = x . view ( t , n , - 1 ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " \"def __repr__ ( self ) : NEW_LINE INDENT tmpstr = self . __class__ . __name__ + ' ▁ ( \\\\n ' NEW_LINE tmpstr += self . module . __repr__ ( ) NEW_LINE tmpstr += ' ) ' NEW_LINE return tmpstr NEW_LINE DEDENT\",\n",
              " 'def forward ( self , input_ ) : NEW_LINE INDENT if not self . training : NEW_LINE INDENT return F . softmax ( input_ , dim = - 1 ) NEW_LINE DEDENT else : NEW_LINE INDENT return input_ NEW_LINE DEDENT DEDENT',\n",
              " 'def __init__ ( self , input_size , hidden_size , rnn_type = nn . LSTM , bidirectional = False , batch_norm = True ) : NEW_LINE INDENT super ( BatchRNN , self ) . __init__ ( ) NEW_LINE self . input_size = input_size NEW_LINE self . hidden_size = hidden_size NEW_LINE self . bidirectional = bidirectional NEW_LINE self . batch_norm = SequenceWise ( nn . BatchNorm1d ( input_size ) ) if batch_norm else None NEW_LINE self . rnn = rnn_type ( input_size = input_size , hidden_size = hidden_size , bidirectional = bidirectional , bias = False ) NEW_LINE self . num_directions = 2 if bidirectional else 1 NEW_LINE DEDENT',\n",
              " 'def flatten_parameters ( self ) : NEW_LINE INDENT self . rnn . flatten_parameters ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if self . batch_norm is not None : NEW_LINE INDENT x = self . batch_norm ( x ) NEW_LINE DEDENT x , _ = self . rnn ( x ) NEW_LINE if self . bidirectional : NEW_LINE INDENT x = x . view ( x . size ( 0 ) , x . size ( 1 ) , 2 , - 1 ) . sum ( 2 ) . view ( x . size ( 0 ) , x . size ( 1 ) , - 1 ) NEW_LINE DEDENT return x NEW_LINE DEDENT',\n",
              " \"def __init__ ( self , n_features , context ) : NEW_LINE INDENT super ( Lookahead , self ) . __init__ ( ) NEW_LINE self . n_features = n_features NEW_LINE self . weight = Parameter ( torch . Tensor ( n_features , context + 1 ) ) NEW_LINE assert context > 0 NEW_LINE self . context = context NEW_LINE self . register_parameter ( ' bias ' , None ) NEW_LINE self . init_parameters ( ) NEW_LINE DEDENT\",\n",
              " 'def init_parameters ( self ) : NEW_LINE INDENT stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) NEW_LINE self . weight . data . uniform_ ( - stdv , stdv ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , input ) : NEW_LINE INDENT seq_len = input . size ( 0 ) NEW_LINE padding = torch . zeros ( self . context , * ( input . size ( ) [ 1 : ] ) ) . type_as ( input . data ) NEW_LINE x = torch . cat ( ( input , Variable ( padding ) ) , 0 ) NEW_LINE x = [ x [ i : i + self . context + 1 ] for i in range ( seq_len ) ] NEW_LINE x = torch . stack ( x ) NEW_LINE x = x . permute ( 0 , 2 , 3 , 1 ) NEW_LINE x = torch . mul ( x , self . weight ) . sum ( dim = 3 ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " \"def __repr__ ( self ) : NEW_LINE INDENT return self . __class__ . __name__ + ' ( ' + ' n _ features = ' + str ( self . n_features ) + ' , ▁ context = ' + str ( self . context ) + ' ) ' NEW_LINE DEDENT\",\n",
              " 'def __init__ ( self , rnn_type = nn . LSTM , labels = \" abc \" , rnn_hidden_size = 512 , rnn_layers = 2 , bidirectional = True , kernel_sz = 11 , stride = 2 , map = 256 , cnn_layers = 2 , nFreq = 40 , nDownsample = 1 , audio_conf = None ) : NEW_LINE INDENT super ( DeepSpeech_ken , self ) . __init__ ( ) NEW_LINE self . nFreq = nFreq NEW_LINE self . _version = \\'0.0.1\\' NEW_LINE self . _audio_conf = audio_conf NEW_LINE self . rnn_size = rnn_hidden_size NEW_LINE self . rnn_layers = rnn_layers NEW_LINE self . rnn_type = rnn_type NEW_LINE self . bidirectional = bidirectional NEW_LINE self . cnn_stride = stride NEW_LINE self . cnn_map = map NEW_LINE self . cnn_kernel = kernel_sz NEW_LINE self . nDownsample = nDownsample NEW_LINE self . cnn_layers = cnn_layers NEW_LINE self . _labels = labels NEW_LINE num_classes = len ( self . _labels ) NEW_LINE conv_list = [ ] NEW_LINE conv_list . append ( nn . Conv1d ( nFreq , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE if ( self . nDownsample == 1 ) : NEW_LINE INDENT stride = 1 NEW_LINE DEDENT for x in range ( self . cnn_layers - 1 ) : NEW_LINE INDENT conv_list . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE DEDENT self . conv = nn . Sequential ( * conv_list ) NEW_LINE rnn_input_size = map NEW_LINE print ( \\' rnn ▁ input ▁ size ▁ = ▁ \\' + str ( rnn_input_size ) ) NEW_LINE rnns = [ ] NEW_LINE rnn = BatchRNN ( input_size = rnn_input_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional , batch_norm = False ) NEW_LINE rnns . append ( ( \\'0\\' , rnn ) ) NEW_LINE for x in range ( self . rnn_layers - 1 ) : NEW_LINE INDENT rnn = BatchRNN ( input_size = rnn_hidden_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional ) NEW_LINE rnns . append ( ( \\' % d \\' % ( x + 1 ) , rnn ) ) NEW_LINE DEDENT self . rnns = nn . Sequential ( OrderedDict ( rnns ) ) NEW_LINE fully_connected = nn . Sequential ( nn . BatchNorm1d ( rnn_hidden_size ) , nn . Linear ( rnn_hidden_size , num_classes , bias = False ) ) NEW_LINE self . fc = nn . Sequential ( SequenceWise ( fully_connected ) , ) NEW_LINE self . inference_softmax = InferenceBatchSoftmax ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if ( x . dim ( ) == 4 ) : NEW_LINE INDENT x = x . squeeze ( ) NEW_LINE DEDENT x = self . conv ( x ) NEW_LINE x = x . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) . contiguous ( ) NEW_LINE x = self . rnns ( x ) NEW_LINE x = self . fc ( x ) NEW_LINE x = x . transpose ( 0 , 1 ) NEW_LINE x = self . inference_softmax ( x ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , rnn_type = nn . LSTM , labels = \" abc \" , rnn_hidden_size = 512 , rnn_layers = 2 , bidirectional = True , kernel_sz = 11 , stride = 2 , map = 256 , cnn_layers = 2 , nFreq = 40 , nDownsample = 1 , audio_conf = None , include_first_BN = True ) : NEW_LINE INDENT super ( DeepSpeech_ken , self ) . __init__ ( ) NEW_LINE self . nFreq = nFreq NEW_LINE self . _version = \\'0.0.1\\' NEW_LINE self . _audio_conf = audio_conf NEW_LINE self . rnn_size = rnn_hidden_size NEW_LINE self . rnn_layers = rnn_layers NEW_LINE self . rnn_type = rnn_type NEW_LINE self . bidirectional = bidirectional NEW_LINE self . cnn_stride = stride NEW_LINE self . cnn_map = map NEW_LINE self . cnn_kernel = kernel_sz NEW_LINE self . nDownsample = nDownsample NEW_LINE self . cnn_layers = cnn_layers NEW_LINE self . _labels = labels NEW_LINE num_classes = len ( self . _labels ) NEW_LINE conv_list = [ ] NEW_LINE conv_list . append ( nn . Conv1d ( nFreq , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE if ( include_first_BN ) : NEW_LINE INDENT conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE DEDENT conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE if ( self . nDownsample == 1 ) : NEW_LINE INDENT stride = 1 NEW_LINE DEDENT for x in range ( self . cnn_layers - 1 ) : NEW_LINE INDENT conv_list . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE DEDENT self . conv = nn . Sequential ( * conv_list ) NEW_LINE rnn_input_size = map NEW_LINE print ( \\' rnn ▁ input ▁ size ▁ = ▁ \\' + str ( rnn_input_size ) ) NEW_LINE rnns = [ ] NEW_LINE rnn = BatchRNN ( input_size = rnn_input_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional , batch_norm = False ) NEW_LINE rnns . append ( ( \\'0\\' , rnn ) ) NEW_LINE for x in range ( self . rnn_layers - 1 ) : NEW_LINE INDENT rnn = BatchRNN ( input_size = rnn_hidden_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional ) NEW_LINE rnns . append ( ( \\' % d \\' % ( x + 1 ) , rnn ) ) NEW_LINE DEDENT self . rnns = nn . Sequential ( OrderedDict ( rnns ) ) NEW_LINE fully_connected = nn . Sequential ( nn . BatchNorm1d ( rnn_hidden_size ) , nn . Linear ( rnn_hidden_size , num_classes , bias = False ) ) NEW_LINE self . fc = nn . Sequential ( SequenceWise ( fully_connected ) , ) NEW_LINE self . inference_softmax = InferenceBatchSoftmax ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if ( x . dim ( ) == 4 ) : NEW_LINE INDENT x = x . squeeze ( ) NEW_LINE DEDENT x = self . conv ( x ) NEW_LINE x = x . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) . contiguous ( ) NEW_LINE x = self . rnns ( x ) NEW_LINE x = self . fc ( x ) NEW_LINE x = x . transpose ( 0 , 1 ) NEW_LINE x = self . inference_softmax ( x ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , rnn_type = nn . GRU , labels = \" abc \" , rnn_hidden_size = 512 , rnn_layers = 2 , bidirectional = True , kernel_sz = 11 , stride = 2 , map = 256 , cnn_residual_blocks = 1 , nFreq = 40 , nDownsample = 1 , audio_conf = None ) : NEW_LINE INDENT super ( ResidualDeepSpeech , self ) . __init__ ( ) NEW_LINE self . nFreq = nFreq NEW_LINE self . _version = \\'0.0.1\\' NEW_LINE self . _audio_conf = audio_conf NEW_LINE self . rnn_size = rnn_hidden_size NEW_LINE self . rnn_layers = rnn_layers NEW_LINE self . rnn_type = rnn_type NEW_LINE self . bidirectional = bidirectional NEW_LINE self . cnn_stride = stride NEW_LINE self . cnn_map = map NEW_LINE self . cnn_kernel = kernel_sz NEW_LINE self . nDownsample = nDownsample NEW_LINE self . cnn_residual_blocks = cnn_residual_blocks NEW_LINE self . _labels = labels NEW_LINE num_classes = len ( self . _labels ) NEW_LINE conv1 = [ ] NEW_LINE conv1 . append ( nn . Conv1d ( nFreq , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv1 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE self . conv1 = nn . Sequential ( * conv1 ) NEW_LINE if ( self . nDownsample == 1 ) : NEW_LINE INDENT stride = 1 NEW_LINE DEDENT padding = int ( ( kernel_sz - 1 ) / 2 ) NEW_LINE assert ( stride == 1 ) , \\' padding ▁ is ▁ only ▁ valid ▁ when ▁ stride = 1\\' NEW_LINE residual2 = [ ] NEW_LINE residual2 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding ) ) NEW_LINE residual2 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE residual2 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE residual2 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding ) ) NEW_LINE residual2 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE self . residual2 = nn . Sequential ( * residual2 ) NEW_LINE self . dim_match_layer = nn . Conv1d ( map , rnn_hidden_size , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE rnn_input_size = rnn_hidden_size NEW_LINE print ( \\' rnn ▁ input ▁ size ▁ = ▁ \\' + str ( rnn_input_size ) ) NEW_LINE rnns = [ ] NEW_LINE rnn = BatchRNN ( input_size = rnn_input_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional , batch_norm = False ) NEW_LINE rnns . append ( ( \\'0\\' , rnn ) ) NEW_LINE for x in range ( self . rnn_layers - 1 ) : NEW_LINE INDENT rnn = BatchRNN ( input_size = rnn_hidden_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional ) NEW_LINE rnns . append ( ( \\' % d \\' % ( x + 1 ) , rnn ) ) NEW_LINE DEDENT self . rnns = nn . Sequential ( OrderedDict ( rnns ) ) NEW_LINE fully_connected = nn . Sequential ( nn . BatchNorm1d ( rnn_hidden_size ) , nn . Linear ( rnn_hidden_size , num_classes , bias = False ) ) NEW_LINE self . fc = nn . Sequential ( SequenceWise ( fully_connected ) , ) NEW_LINE self . inference_softmax = InferenceBatchSoftmax ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if ( x . dim ( ) == 4 ) : NEW_LINE INDENT x = x . squeeze ( ) NEW_LINE DEDENT x = self . conv1 ( x ) NEW_LINE x = self . residual2 ( x ) + x NEW_LINE x = self . dim_match_layer ( x ) NEW_LINE x = x . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) . contiguous ( ) NEW_LINE for l in range ( self . rnn_layers ) : NEW_LINE INDENT x = self . rnns [ l ] ( x ) + x NEW_LINE DEDENT x = self . fc ( x ) NEW_LINE x = x . transpose ( 0 , 1 ) NEW_LINE x = self . inference_softmax ( x ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , labels = \" abc \" , kernel_sz = 11 , stride = 2 , map = 512 , nFreq = 40 , nDownsample = 1 , audio_conf = None ) : NEW_LINE INDENT super ( ResidualCNN4block , self ) . __init__ ( ) NEW_LINE self . nFreq = nFreq NEW_LINE self . _version = \\'0.0.1\\' NEW_LINE self . _audio_conf = audio_conf NEW_LINE self . cnn_stride = stride NEW_LINE self . cnn_map = map NEW_LINE self . cnn_kernel = kernel_sz NEW_LINE self . nDownsample = nDownsample NEW_LINE self . _labels = labels NEW_LINE num_classes = len ( self . _labels ) NEW_LINE conv1 = [ ] NEW_LINE conv1 . append ( nn . Conv1d ( nFreq , map , kernel_size = kernel_sz , stride = stride , bias = False ) ) NEW_LINE conv1 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE self . conv1 = nn . Sequential ( * conv1 ) NEW_LINE if ( self . nDownsample == 1 ) : NEW_LINE INDENT stride = 1 NEW_LINE DEDENT padding = int ( ( kernel_sz - 1 ) / 2 ) NEW_LINE assert ( stride == 1 ) , \\' padding ▁ is ▁ only ▁ valid ▁ when ▁ stride = 1\\' NEW_LINE residual2 = [ ] NEW_LINE residual2 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual2 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE residual2 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE residual2 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual2 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE self . residual2 = nn . Sequential ( * residual2 ) NEW_LINE residual3 = [ ] NEW_LINE residual3 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual3 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE residual3 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE residual3 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual3 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE self . residual3 = nn . Sequential ( * residual3 ) NEW_LINE residual4 = [ ] NEW_LINE residual4 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual4 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE residual4 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE residual4 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual4 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE self . residual4 = nn . Sequential ( * residual4 ) NEW_LINE residual5 = [ ] NEW_LINE residual5 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual5 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE residual5 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE residual5 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual5 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE self . residual5 = nn . Sequential ( * residual5 ) NEW_LINE fully_connected = nn . Sequential ( nn . Linear ( map , num_classes ) ) NEW_LINE self . fc = nn . Sequential ( SequenceWise ( fully_connected ) , ) NEW_LINE self . inference_softmax = InferenceBatchSoftmax ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if ( x . dim ( ) == 4 ) : NEW_LINE INDENT x = x . squeeze ( ) NEW_LINE DEDENT x = self . conv1 ( x ) NEW_LINE x = self . residual2 ( x ) + x NEW_LINE x = self . residual3 ( x ) + x NEW_LINE x = self . residual4 ( x ) + x NEW_LINE x = self . residual5 ( x ) + x NEW_LINE x = x . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) . contiguous ( ) NEW_LINE x = self . fc ( x ) NEW_LINE x = x . transpose ( 0 , 1 ) NEW_LINE x = self . inference_softmax ( x ) NEW_LINE return x NEW_LINE DEDENT']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_df = eqns.merge(collected_func_reps , on='id')"
      ],
      "metadata": {
        "id": "SJkjhyPyY6Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in full_df.iterrows():\n",
        "  print(tokenizer(row[1].eqn,return_tensors='pt',truncation=True))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rghVH0GYooer",
        "outputId": "e9e96110-d5a9-462b-ab37-1249d17d7b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,   165,  3295,   196,  3325,   198,   165, 11241,   168,   196,\n",
            "           144,   198,   165, 12477,  1775,   168,   196,   141,   198,   159,\n",
            "           168,   196,   172, 10583,  2249,   198,   113,   144,   117,   141,\n",
            "           114,   134,   165,   142,   168,   196,   113,   165,  3087,  1830,\n",
            "          2087,   196,   193,   198,   168,   196,   188,   198,   117,   165,\n",
            "          3087,  1830,  2087,   196,   193,   198,   168,   196,   189,   198,\n",
            "           114,   165, 27466,  1306,   185,   113,   165,  3087,  1830,  2087,\n",
            "           196,   193,   198,   168,   196,   188,   198,   117,   165,  3087,\n",
            "          1830,  2087,   196,   193,   198,   168,   196,   189,   198,   114,\n",
            "           198,   164,   165,  9366,   141,   113,   165,  3087,  1830,  2087,\n",
            "           196,   193,   198,   168,   188,   117,   165,  3087,  1830,  2087,\n",
            "           196,   193,   198,   168,   196,   189,   198,   114,   166,   165,\n",
            "           165,   116,   165,   142,   168,   196,   165,  3087,  1830,  2087,\n",
            "           196,   193,   198,   168,   196,   188,   198,   165, 27466,  1306,\n",
            "           185,   113,   165,  3087,  1830,  2087,   196,   193,   198,   168,\n",
            "           196,   188,   198,   114,   117,   165,  3087,  1830,  2087,   196,\n",
            "           195,   198,   165, 27466,  1306,   151,   113,   121,   117,   146,\n",
            "           114,   198,   164,   165,  9366,   113,   122,   118,   141,   113,\n",
            "           165,  3087,  1830,  2087,   196,   193,   198,   168,   196,   188,\n",
            "           198,   117,   144,   113,   165,  3087,  1830,  2087,   196,   193,\n",
            "           198,   168,   196,   188,   198,   117,   165,  3087,  1830,  2087,\n",
            "           196,   195,   198,   114,   114,   114,   166,   119,   165,  1322,\n",
            "           196,  3325,   198,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveDataset(Dataset):\n",
        "\n",
        "    def __init__(self, combined_df, all_eqns):\n",
        "        \"\"\"Simple init function\"\"\"\n",
        "        self.df = combined_df\n",
        "        tok_eqns = []\n",
        "        for row in self.df.iterrows():\n",
        "          tok_eqns.append(tokenizer(row[1].eqn,return_tensors='pt',truncation=True))\n",
        "        self.tok_eqns = tok_eqns\n",
        "\n",
        "        self.neg_eqns = all_eqns\n",
        "        neg_eqns = []\n",
        "        for item in all_eqns:\n",
        "          neg_eqns.append(tokenizer(item,return_tensors='pt',truncation=True))\n",
        "\n",
        "        self.neg_toks = neg_eqns\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Get length of dataset\"\"\"\n",
        "        return len(self.df)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.df.iloc[idx]\n",
        "\n",
        "        eqn_paper_id = entry.id\n",
        "        code_id = eqn_paper_id \n",
        "        code_rep = entry.representations\n",
        "        code_funcs = entry.functions\n",
        "\n",
        "        pos_eqn = entry.eqn\n",
        "        pos_eqn_tok = self.tok_eqns[idx]\n",
        "\n",
        "        neg_idx = random.randint(0,len(self.neg_eqns)-1)\n",
        "        neg_eqn = self.neg_eqns.iloc[neg_idx]\n",
        "        neg_eqn_tok = self.neg_toks[neg_idx]\n",
        "\n",
        "        return code_rep, pos_eqn_tok, neg_eqn_tok, pos_eqn, neg_eqn, eqn_paper_id, code_id, code_funcs\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "1jXByPfkDDQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ContrastiveDataset(full_df, eqns.eqn.dropna())"
      ],
      "metadata": {
        "id": "8eGPuAX_eZHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveModel(nn.Module):\n",
        "  def __init__(self, embedding_model):\n",
        "    super(ContrastiveModel,self).__init__()\n",
        "\n",
        "    self.latex_encoder = embedding_model\n",
        "\n",
        "    self.reduce_eqn_fc1 = nn.Linear(768,512)\n",
        "    self.reduce_eqn_fc2 = nn.Linear(512,128)\n",
        "    self.reduce_eqn_fc3 = nn.Linear(128,64)\n",
        "\n",
        "    self.create_att1 = nn.Linear(1024+64,1024+64)\n",
        "    self.create_att2 = nn.Linear(1024+64,512)\n",
        "    self.create_att3 = nn.Linear(512,1024)\n",
        "\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=0)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  def embed_eqn(self, eqn):\n",
        "      return self.latex_encoder(input_ids=eqn[0],attention_mask=eqn[1])[0].mean(-2)\n",
        "\n",
        "  def reduce_eqn(self,eqn_embedding):\n",
        "\n",
        "      h = self.remap_eqn_fc1(eqn_embedding)\n",
        "      h = F.relu(h)\n",
        "      h = self.remap_eqn_fc2(eqn_embedding)\n",
        "      h = F.relu(h)\n",
        "      h = self.remap_eqn_fc3(eqn_embedding)\n",
        "      return h\n",
        "\n",
        "\n",
        "  def forward(self, func_reps,pos_eqn, neg_eqn):\n",
        "      # get embeddings\n",
        "      pos_eqn_embedding = self.embed_eqn(pos_eqn)\n",
        "      neg_eqn_embedding = self.embed_eqn(neg_eqn)\n",
        "\n",
        "\n",
        "      #reduce the embedding size of pos_h and then expand it to match func_reps\n",
        "      #the expansion is concerning, but the hope is we reduce the information           \n",
        "      pos_h = self.reduce_eqn(pos_eqn_embedding)\n",
        "\n",
        "\n",
        "      #norm func reps as well because we want the next operation to correspond\n",
        "      #get an attention map\n",
        "\n",
        "      attn_in = torch.cat(func_reps.squeeze(),pos_h.squeeze())\n",
        "      attn = self.create_att1(attn_in) \n",
        "      attn = F.relu(attn)\n",
        "      attn = self.create_att2(attn)\n",
        "      attn = F.relu(attn)\n",
        "      attn = self.create_attn3(attn)\n",
        "\n",
        "      \n",
        "                            \n",
        "      #zero out so don't have negatives influencing results\n",
        "\n",
        "      #convert to probabilities\n",
        "      attn_map = self.softmax(pre_soft_attn_map)\n",
        "\n",
        "      #recombine function reps based on attention\n",
        "      z = torch.matmul(attn_map,func_reps.squeeze()) if func_reps.shape[0] > 1 else func_reps\n",
        "\n",
        "      z = F.relu(z)\n",
        "      z = self.remix_res_fc1(z)\n",
        "      \n",
        "\n",
        "      return z, pos_eqn_embedding, neg_eqn_embedding, attn_map\n",
        "\n"
      ],
      "metadata": {
        "id": "e_7GNTcLk_2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat([torch.randn((1024)),torch.randn((24))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u--KQfakKJN",
        "outputId": "3b4b1296-7337-40ce-8b65-65eebe0febc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1798, -1.7033,  1.2555,  ...,  0.6673,  1.7393, -1.5411])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,batch_size=1,shuffle=True)"
      ],
      "metadata": {
        "id": "SNFDNx3menro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DistilBertModel.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "combined_model = ContrastiveModel(model)\n",
        "combined_model.train()\n",
        "combined_model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(combined_model.parameters(), lr=5e-5)\n",
        "criterion = nn.TripletMarginLoss()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMBv8zzX05u3",
        "outputId": "02f19e56-e43e-4676-f49f-ab64d94d0f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKy8VDLWtVJ2",
        "outputId": "bcf0a64d-50d1-4df3-f1e2-73966199bab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 27 01:07:22 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W /  70W |   1694MiB / 15109MiB |     12%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30"
      ],
      "metadata": {
        "id": "XXPXvWgoAh3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_model.train()\n",
        "accum_iter = 256\n",
        "for epoch in range(epochs):\n",
        "    print(\"--------\")\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    print(\"--------\")\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_avg = 0\n",
        "    for batch_idx,input in enumerate(train_loader):\n",
        "\n",
        "     \n",
        "\n",
        "      code_rep, pos_eqn_tok, neg_eqn_tok, pos_eqn, neg_eqn, eqn_paper_id, code_id, code_funcs = input  \n",
        "      # if len(code_funcs) < 1:\n",
        "      #     print('continuing')\n",
        "      #     continue    \n",
        "\n",
        "\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      curr_rep = torch.cat(code_rep).cuda()\n",
        "      pos_eqn_prep = pos_eqn_tok['input_ids'].flatten(1).cuda(),pos_eqn_tok['attention_mask'].flatten(1).cuda()\n",
        "      neg_eqn_prep = neg_eqn_tok['input_ids'].flatten(1).cuda(),neg_eqn_tok['attention_mask'].flatten(1).cuda()\n",
        "\n",
        "      # output\n",
        "      z,pos,neg,attn = combined_model(curr_rep,pos_eqn_prep,neg_eqn_prep)\n",
        "      \n",
        "      loss = criterion(z.squeeze(),pos.squeeze(),neg.squeeze()) \n",
        "      \n",
        "      loss = loss / accum_iter\n",
        "      loss.backward()\n",
        "\n",
        "      if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(train_loader)):\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            print(f\"{loss_avg}\")\n",
        "            \n",
        "            loss_avg = 0\n",
        "            print(f\"On batch {batch_idx}\")\n",
        "\n",
        "            print(attn)\n",
        "            print(pos_eqn)\n",
        "            print(code_funcs)\n",
        "\n",
        "\n",
        "      loss_avg += loss.item()\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rRHGr83gjXt5",
        "outputId": "c7ebbc70-7b9c-4ce7-c9e2-688422532b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------\n",
            "Epoch 0\n",
            "--------\n",
            "0.9581608772277832\n",
            "On batch 255\n",
            "tensor([0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
            "        0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
            "        0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
            "        0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
            "        0.0270], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "('\\\\label{all_exchange}\\n        e^{a + \\\\sum_{i = j}^n b_i} + e^{a_j - b_j} > e^{a + \\\\sum_{i = j + 1}^n b_i} + e^{a_j} ,j = 1, \\\\cdots,n.\\n    ',)\n",
            "[(\"def test ( model , test_loader ) : NEW_LINE INDENT start_test = True NEW_LINE model . eval ( ) NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT for batch_idx , test_data in enumerate ( test_loader ) : NEW_LINE INDENT img , labels = test_data [ ' img0' ] , test_data [ ' label ' ] NEW_LINE img = img . cuda ( ) NEW_LINE outputs = model ( img , return_feat = False ) NEW_LINE if start_test : NEW_LINE INDENT all_output = outputs . float ( ) . cpu ( ) NEW_LINE all_label = labels . float ( ) NEW_LINE start_test = False NEW_LINE DEDENT else : NEW_LINE INDENT all_output = torch . cat ( ( all_output , outputs . float ( ) . cpu ( ) ) , 0 ) NEW_LINE all_label = torch . cat ( ( all_label , labels . float ( ) ) , 0 ) NEW_LINE DEDENT DEDENT DEDENT _ , predict = torch . min ( all_output , 1 ) NEW_LINE acc = torch . sum ( torch . squeeze ( predict ) . float ( ) == all_label ) . item ( ) / float ( all_label . size ( ) [ 0 ] ) * 100 NEW_LINE return acc NEW_LINE DEDENT\",), ('def train ( cfg , task ) : NEW_LINE INDENT logger = logging . getLogger ( \" EADA . trainer \" ) NEW_LINE use_cuda = True if torch . cuda . is_available ( ) else False NEW_LINE kwargs = { \\' num _ workers \\' : 2 , \\' pin _ memory \\' : True } if use_cuda else { } NEW_LINE source_transform = build_transform ( cfg , is_train = True , choices = cfg . INPUT . SOURCE_TRANSFORMS ) NEW_LINE target_transform = build_transform ( cfg , is_train = True , choices = cfg . INPUT . TARGET_TRANSFORMS ) NEW_LINE test_transform = build_transform ( cfg , is_train = False , choices = cfg . INPUT . TEST_TRANSFORMS ) NEW_LINE src_train_ds = ImageList ( os . path . join ( cfg . DATASET . ROOT , cfg . DATASET . NAME , cfg . DATASET . SOURCE_TRAIN_DOMAIN ) , transform = source_transform ) NEW_LINE src_train_loader = DataLoader ( src_train_ds , batch_size = cfg . DATALOADER . SOURCE . BATCH_SIZE , shuffle = True , drop_last = True , ** kwargs ) NEW_LINE tgt_unlabeled_ds = ImageList ( os . path . join ( cfg . DATASET . ROOT , cfg . DATASET . NAME , cfg . DATASET . TARGET_TRAIN_DOMAIN ) , transform = target_transform ) NEW_LINE tgt_unlabeled_loader = DataLoader ( tgt_unlabeled_ds , batch_size = cfg . DATALOADER . TARGET . BATCH_SIZE , shuffle = True , drop_last = True , ** kwargs ) NEW_LINE tgt_unlabeled_loader_full = DataLoader ( tgt_unlabeled_ds , batch_size = cfg . DATALOADER . TARGET . BATCH_SIZE , shuffle = True , drop_last = False , ** kwargs ) NEW_LINE tgt_test_ds = ImageList ( os . path . join ( cfg . DATASET . ROOT , cfg . DATASET . NAME , cfg . DATASET . TARGET_VAL_DOMAIN ) , transform = test_transform ) NEW_LINE tgt_test_loader = DataLoader ( tgt_test_ds , batch_size = cfg . DATALOADER . TEST . BATCH_SIZE , shuffle = False , ** kwargs ) NEW_LINE tgt_selected_ds = ImageList ( empty = True , transform = source_transform ) NEW_LINE tgt_selected_loader = DataLoader ( tgt_selected_ds , batch_size = cfg . DATALOADER . SOURCE . BATCH_SIZE , shuffle = True , drop_last = False , ** kwargs ) NEW_LINE model = ResNetFc ( class_num = cfg . DATASET . NUM_CLASS , cfg = cfg ) . cuda ( ) NEW_LINE optimizer = optim . Adadelta ( model . parameters_list ( cfg . OPTIM . LR ) , lr = cfg . OPTIM . LR ) NEW_LINE nll_criterion = NLLLoss ( cfg ) NEW_LINE uns_criterion = FreeEnergyAlignmentLoss ( cfg ) NEW_LINE totality = tgt_unlabeled_ds . __len__ ( ) NEW_LINE logger . info ( \" Start ▁ training \" ) NEW_LINE meters = MetricLogger ( delimiter = \" ▁ ▁ \" ) NEW_LINE start_training_time = time . time ( ) NEW_LINE end = time . time ( ) NEW_LINE final_acc = 0. NEW_LINE final_model = None NEW_LINE all_epoch_result = [ ] NEW_LINE all_selected_images = None NEW_LINE for epoch in range ( 1 , cfg . TRAINER . MAX_EPOCHS + 1 ) : NEW_LINE INDENT model . train ( ) NEW_LINE iter_per_epoch = max ( len ( src_train_loader ) , len ( tgt_unlabeled_loader ) ) NEW_LINE for batch_idx in range ( iter_per_epoch ) : NEW_LINE INDENT data_time = time . time ( ) - end NEW_LINE if batch_idx % len ( src_train_loader ) == 0 : NEW_LINE INDENT src_iter = iter ( src_train_loader ) NEW_LINE DEDENT if batch_idx % len ( tgt_unlabeled_loader ) == 0 : NEW_LINE INDENT tgt_unlabeled_iter = iter ( tgt_unlabeled_loader ) NEW_LINE DEDENT if not tgt_selected_ds . empty : NEW_LINE INDENT if batch_idx % len ( tgt_selected_loader ) == 0 : NEW_LINE INDENT tgt_selected_iter = iter ( tgt_selected_loader ) NEW_LINE DEDENT DEDENT src_data = src_iter . next ( ) NEW_LINE tgt_unlabeled_data = tgt_unlabeled_iter . next ( ) NEW_LINE src_img , src_lbl = src_data [ \\' img0\\' ] , src_data [ \\' label \\' ] NEW_LINE src_img , src_lbl = src_img . cuda ( ) , src_lbl . cuda ( ) NEW_LINE tgt_unlabeled_img = tgt_unlabeled_data [ \\' img \\' ] NEW_LINE tgt_unlabeled_img = tgt_unlabeled_img . cuda ( ) NEW_LINE optimizer . zero_grad ( ) NEW_LINE total_loss = 0 NEW_LINE src_out = model ( src_img , return_feat = False ) NEW_LINE nll_loss = nll_criterion ( src_out , src_lbl ) NEW_LINE total_loss += nll_loss NEW_LINE meters . update ( nll_loss = nll_loss . item ( ) ) NEW_LINE if cfg . TRAINER . ENERGY_ALIGN_WEIGHT > 0 : NEW_LINE INDENT tgt_unlabeled_out = model ( tgt_unlabeled_img , return_feat = False ) NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT output_div_t = - 1.0 * cfg . TRAINER . ENERGY_BETA * src_out NEW_LINE output_logsumexp = torch . logsumexp ( output_div_t , dim = 1 , keepdim = False ) NEW_LINE free_energy = - 1.0 * output_logsumexp / cfg . TRAINER . ENERGY_BETA NEW_LINE src_batch_free_energy = free_energy . mean ( ) . detach ( ) NEW_LINE if epoch == 1 and batch_idx == 0 : NEW_LINE INDENT global_mean = src_batch_free_energy NEW_LINE DEDENT global_mean = momentum_update ( global_mean , src_batch_free_energy ) NEW_LINE DEDENT fea_loss = uns_criterion ( inputs = tgt_unlabeled_out , bound = global_mean ) NEW_LINE total_loss += cfg . TRAINER . ENERGY_ALIGN_WEIGHT * fea_loss NEW_LINE meters . update ( fea_loss = ( cfg . TRAINER . ENERGY_ALIGN_WEIGHT * fea_loss ) . item ( ) ) NEW_LINE DEDENT if not tgt_selected_ds . empty : NEW_LINE INDENT tgt_selected_data = tgt_selected_iter . next ( ) NEW_LINE tgt_selected_img , tgt_selected_lbl = tgt_selected_data [ \\' img0\\' ] , tgt_selected_data [ \\' label \\' ] NEW_LINE tgt_selected_img , tgt_selected_lbl = tgt_selected_img . cuda ( ) , tgt_selected_lbl . cuda ( ) NEW_LINE if tgt_selected_img . size ( 0 ) == 1 : NEW_LINE INDENT tgt_selected_img = torch . cat ( ( tgt_selected_img , tgt_selected_img ) , dim = 0 ) NEW_LINE tgt_selected_lbl = torch . cat ( ( tgt_selected_lbl , tgt_selected_lbl ) , dim = 0 ) NEW_LINE DEDENT tgt_selected_out = model ( tgt_selected_img , return_feat = False ) NEW_LINE selected_nll_loss = nll_criterion ( tgt_selected_out , tgt_selected_lbl ) NEW_LINE total_loss += selected_nll_loss NEW_LINE meters . update ( selected_nll_loss = selected_nll_loss . item ( ) ) NEW_LINE DEDENT total_loss . backward ( ) NEW_LINE optimizer . step ( ) NEW_LINE batch_time = time . time ( ) - end NEW_LINE end = time . time ( ) NEW_LINE meters . update ( time = batch_time , data = data_time ) NEW_LINE eta_seconds = meters . time . global_avg * ( iter_per_epoch * cfg . TRAINER . MAX_EPOCHS - batch_idx * epoch ) NEW_LINE eta_string = str ( datetime . timedelta ( seconds = int ( eta_seconds ) ) ) NEW_LINE if batch_idx % cfg . TRAIN . PRINT_FREQ == 0 : NEW_LINE INDENT logger . info ( meters . delimiter . join ( [ \" eta : ▁ { eta } \" , \" task : ▁ { task } \" , \" epoch : ▁ { epoch } \" , f\" [ iter : ▁ { batch _ idx } / { iter _ per _ epoch } ] \" , \" { meters } \" , \" max ▁ mem : ▁ { memory : .2f } ▁ GB \" , ] ) . format ( task = task , eta = eta_string , epoch = epoch , meters = str ( meters ) , memory = torch . cuda . max_memory_allocated ( ) / 1024.0 / 1024.0 / 1024.0 , ) ) NEW_LINE DEDENT DEDENT if epoch % 5 == 0 : NEW_LINE INDENT testacc = test ( model , tgt_test_loader ) NEW_LINE logger . info ( \\' Task : ▁ { } ▁ Test ▁ Epoch : ▁ { } ▁ testacc : ▁ { : . 2f } \\' . format ( task , epoch , testacc ) ) NEW_LINE all_epoch_result . append ( { \\' epoch \\' : epoch , \\' acc \\' : testacc } ) NEW_LINE if epoch == cfg . TRAINER . MAX_EPOCHS : NEW_LINE INDENT final_model = model . state_dict ( ) NEW_LINE final_acc = testacc NEW_LINE DEDENT DEDENT if epoch in cfg . TRAINER . ACTIVE_ROUND : NEW_LINE INDENT logger . info ( \\' Task : ▁ { } ▁ Active ▁ Epoch : ▁ { } \\' . format ( task , epoch ) ) NEW_LINE if cfg . TRAINER . NAME == \\' RAND \\' : NEW_LINE INDENT active_samples = RAND_active ( tgt_unlabeled_ds = tgt_unlabeled_ds , tgt_selected_ds = tgt_selected_ds , active_ratio = 0.01 , totality = totality ) NEW_LINE DEDENT elif cfg . TRAINER . NAME == \\' EADA \\' : NEW_LINE INDENT active_samples = EADA_active ( tgt_unlabeled_loader_full = tgt_unlabeled_loader_full , tgt_unlabeled_ds = tgt_unlabeled_ds , tgt_selected_ds = tgt_selected_ds , active_ratio = 0.01 , totality = totality , model = model , cfg = cfg ) NEW_LINE DEDENT if all_selected_images is None : NEW_LINE INDENT all_selected_images = active_samples NEW_LINE DEDENT else : NEW_LINE INDENT all_selected_images = np . concatenate ( ( all_selected_images , active_samples ) , axis = 0 ) NEW_LINE DEDENT DEDENT DEDENT ckt_path = os . path . join ( cfg . OUTPUT_DIR , cfg . DATASET . NAME , task ) NEW_LINE mkdir ( ckt_path ) NEW_LINE torch . save ( all_selected_images , os . path . join ( ckt_path , \" all _ selected _ images . pth \" ) ) NEW_LINE torch . save ( final_model , os . path . join ( ckt_path , \" final _ model _ { } . pth \" . format ( task ) ) ) NEW_LINE with open ( os . path . join ( ckt_path , \\' all _ epoch _ result . csv \\' ) , \\' w \\' ) as handle : NEW_LINE INDENT for i , rec in enumerate ( all_epoch_result ) : NEW_LINE INDENT if i == 0 : NEW_LINE INDENT handle . write ( \\' , \\' . join ( list ( rec . keys ( ) ) ) + \\' \\\\n \\' ) NEW_LINE DEDENT line = [ str ( rec [ key ] ) for key in rec . keys ( ) ] NEW_LINE handle . write ( \\' , \\' . join ( line ) + \\' \\\\n \\' ) NEW_LINE DEDENT DEDENT total_training_time = time . time ( ) - start_training_time NEW_LINE total_time_str = str ( datetime . timedelta ( seconds = total_training_time ) ) NEW_LINE logger . info ( \" Total ▁ training ▁ time : ▁ { } ▁ ( { : .4f } ▁ s ▁ / ▁ ep ) \" . format ( total_time_str , total_training_time / cfg . TRAINER . MAX_EPOCHS ) ) NEW_LINE return task , final_acc NEW_LINE DEDENT',), ('def main ( ) : NEW_LINE INDENT parser = argparse . ArgumentParser ( description = \\' PyTorch ▁ Activate ▁ Domain ▁ Adaptation \\' ) NEW_LINE parser . add_argument ( \\' - - cfg \\' , default = \\' \\' , metavar = \\' FILE \\' , help = \\' path ▁ to ▁ config ▁ file \\' , type = str ) NEW_LINE parser . add_argument ( \" opts \" , help = \" Modify ▁ config ▁ options ▁ using ▁ the ▁ command - line \" , default = None , nargs = argparse . REMAINDER , ) NEW_LINE args = parser . parse_args ( ) NEW_LINE cfg . merge_from_file ( args . cfg ) NEW_LINE cfg . merge_from_list ( args . opts ) NEW_LINE output_dir = os . path . join ( cfg . OUTPUT_DIR , cfg . DATASET . NAME ) NEW_LINE if output_dir : NEW_LINE INDENT mkdir ( output_dir ) NEW_LINE DEDENT logger = setup_logger ( \" EADA \" , output_dir , 0 ) NEW_LINE logger . info ( \" Loaded ▁ configuration ▁ file ▁ { } \" . format ( args . cfg ) ) NEW_LINE logger . info ( \" Running ▁ with ▁ config : \\\\n { } \" . format ( cfg ) ) NEW_LINE if cfg . SEED >= 0 : NEW_LINE INDENT print ( \\' Setting ▁ fixed ▁ seed : ▁ { } \\' . format ( cfg . SEED ) ) NEW_LINE set_random_seed ( cfg . SEED ) NEW_LINE DEDENT cudnn . deterministic = True NEW_LINE all_task_result = [ ] NEW_LINE for source in cfg . DATASET . SOURCE_DOMAINS : NEW_LINE INDENT for target in cfg . DATASET . TARGET_DOMAINS : NEW_LINE INDENT if source != target : NEW_LINE INDENT cfg . DATASET . SOURCE_TRAIN_DOMAIN = os . path . join ( source + \\' _ train . txt \\' ) NEW_LINE cfg . DATASET . TARGET_TRAIN_DOMAIN = os . path . join ( target + \\' _ train . txt \\' ) NEW_LINE cfg . DATASET . TARGET_VAL_DOMAIN = os . path . join ( target + \\' _ test . txt \\' ) NEW_LINE cfg . freeze ( ) NEW_LINE task , final_acc = train ( cfg , task = source + \\'2\\' + target ) NEW_LINE all_task_result . append ( { \\' task \\' : task , \\' final _ acc \\' : final_acc } ) NEW_LINE cfg . defrost ( ) NEW_LINE DEDENT DEDENT DEDENT with open ( os . path . join ( output_dir , \\' all _ task _ result . csv \\' ) , \\' w \\' ) as handle : NEW_LINE INDENT for i , rec in enumerate ( all_task_result ) : NEW_LINE INDENT if i == 0 : NEW_LINE INDENT handle . write ( \\' , \\' . join ( list ( rec . keys ( ) ) ) + \\' \\\\n \\' ) NEW_LINE DEDENT line = [ str ( rec [ key ] ) for key in rec . keys ( ) ] NEW_LINE handle . write ( \\' , \\' . join ( line ) + \\' \\\\n \\' ) NEW_LINE DEDENT DEDENT DEDENT',), ('def get_cfg_default ( ) : NEW_LINE INDENT return cfg . clone ( ) NEW_LINE DEDENT',), ('def momentum_update ( ema , current ) : NEW_LINE INDENT lambd = np . random . uniform ( ) NEW_LINE return ema * lambd + current * ( 1 - lambd ) NEW_LINE DEDENT',), ('def mkdir ( path ) : NEW_LINE INDENT try : NEW_LINE INDENT os . makedirs ( path ) NEW_LINE DEDENT except OSError as e : NEW_LINE INDENT if e . errno != errno . EEXIST : NEW_LINE INDENT raise NEW_LINE DEDENT DEDENT DEDENT',), ('def set_random_seed ( seed ) : NEW_LINE INDENT random . seed ( seed ) NEW_LINE np . random . seed ( seed ) NEW_LINE torch . manual_seed ( seed ) NEW_LINE torch . cuda . manual_seed_all ( seed ) NEW_LINE DEDENT',), ('def setup_logger ( name , save_dir , distributed_rank , filename = \" log . txt \" ) : NEW_LINE INDENT logger = logging . getLogger ( name ) NEW_LINE logger . setLevel ( logging . DEBUG ) NEW_LINE if distributed_rank > 0 : NEW_LINE INDENT return logger NEW_LINE DEDENT ch = logging . StreamHandler ( stream = sys . stdout ) NEW_LINE ch . setLevel ( logging . DEBUG ) NEW_LINE formatter = logging . Formatter ( \" % ( asctime ) s ▁ % ( name ) s ▁ % ( levelname ) s : ▁ % ( message ) s \" ) NEW_LINE ch . setFormatter ( formatter ) NEW_LINE logger . addHandler ( ch ) NEW_LINE if save_dir : NEW_LINE INDENT fh = logging . FileHandler ( os . path . join ( save_dir , filename ) ) NEW_LINE fh . setLevel ( logging . DEBUG ) NEW_LINE fh . setFormatter ( formatter ) NEW_LINE logger . addHandler ( fh ) NEW_LINE DEDENT return logger NEW_LINE DEDENT',), ('def __init__ ( self , window_size = 20 ) : NEW_LINE INDENT self . deque = deque ( maxlen = window_size ) NEW_LINE self . series = [ ] NEW_LINE self . total = 0.0 NEW_LINE self . count = 0 NEW_LINE DEDENT',), ('def update ( self , value ) : NEW_LINE INDENT self . deque . append ( value ) NEW_LINE self . series . append ( value ) NEW_LINE self . count += 1 NEW_LINE self . total += value NEW_LINE DEDENT',), ('def median ( self ) : NEW_LINE INDENT d = torch . tensor ( list ( self . deque ) ) NEW_LINE return d . median ( ) . item ( ) NEW_LINE DEDENT',), ('def avg ( self ) : NEW_LINE INDENT d = torch . tensor ( list ( self . deque ) ) NEW_LINE return d . mean ( ) . item ( ) NEW_LINE DEDENT',), ('def global_avg ( self ) : NEW_LINE INDENT return self . total / self . count NEW_LINE DEDENT',), ('def __init__ ( self , delimiter = \" \\\\t \" ) : NEW_LINE INDENT self . meters = defaultdict ( SmoothedValue ) NEW_LINE self . delimiter = delimiter NEW_LINE DEDENT',), ('def update ( self , ** kwargs ) : NEW_LINE INDENT for k , v in kwargs . items ( ) : NEW_LINE INDENT if isinstance ( v , torch . Tensor ) : NEW_LINE INDENT v = v . item ( ) NEW_LINE DEDENT assert isinstance ( v , ( float , int ) ) NEW_LINE self . meters [ k ] . update ( v ) NEW_LINE DEDENT DEDENT',), ('def __getattr__ ( self , attr ) : NEW_LINE INDENT if attr in self . meters : NEW_LINE INDENT return self . meters [ attr ] NEW_LINE DEDENT if attr in self . __dict__ : NEW_LINE INDENT return self . __dict__ [ attr ] NEW_LINE DEDENT raise AttributeError ( \" \\' { } \\' ▁ object ▁ has ▁ no ▁ attribute ▁ \\' { } \\' \" . format ( type ( self ) . __name__ , attr ) ) NEW_LINE DEDENT',), ('def __str__ ( self ) : NEW_LINE INDENT loss_str = [ ] NEW_LINE for name , meter in self . meters . items ( ) : NEW_LINE INDENT loss_str . append ( \" { } : ▁ { : . 4f } ▁ ( { : .4f } ) \" . format ( name , meter . median , meter . global_avg ) ) NEW_LINE DEDENT return self . delimiter . join ( loss_str ) NEW_LINE DEDENT',), ('def RAND_active ( tgt_unlabeled_ds , tgt_selected_ds , active_ratio , totality ) : NEW_LINE INDENT length = len ( tgt_unlabeled_ds . samples ) NEW_LINE index = random . sample ( range ( length ) , round ( totality * active_ratio ) ) NEW_LINE tgt_selected_ds . add_item ( tgt_unlabeled_ds . samples [ index ] ) NEW_LINE tgt_unlabeled_ds . remove_item ( index ) NEW_LINE DEDENT',), (\"def EADA_active ( tgt_unlabeled_loader_full , tgt_unlabeled_ds , tgt_selected_ds , active_ratio , totality , model , cfg ) : NEW_LINE INDENT model . eval ( ) NEW_LINE first_stat = list ( ) NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT for _ , data in enumerate ( tgt_unlabeled_loader_full ) : NEW_LINE INDENT tgt_img , tgt_lbl = data [ ' img ' ] , data [ ' label ' ] NEW_LINE tgt_path , tgt_index = data [ ' path ' ] , data [ ' index ' ] NEW_LINE tgt_img , tgt_lbl = tgt_img . cuda ( ) , tgt_lbl . cuda ( ) NEW_LINE tgt_out = model ( tgt_img , return_feat = False ) NEW_LINE min2 = torch . topk ( tgt_out , k = 2 , dim = 1 , largest = False ) . values NEW_LINE mvsm_uncertainty = min2 [ : , 0 ] - min2 [ : , 1 ] NEW_LINE output_div_t = - 1.0 * tgt_out / cfg . TRAINER . ENERGY_BETA NEW_LINE output_logsumexp = torch . logsumexp ( output_div_t , dim = 1 , keepdim = False ) NEW_LINE free_energy = - 1.0 * cfg . TRAINER . ENERGY_BETA * output_logsumexp NEW_LINE for i in range ( len ( free_energy ) ) : NEW_LINE INDENT first_stat . append ( [ tgt_path [ i ] , tgt_lbl [ i ] . item ( ) , tgt_index [ i ] . item ( ) , mvsm_uncertainty [ i ] . item ( ) , free_energy [ i ] . item ( ) ] ) NEW_LINE DEDENT DEDENT DEDENT first_sample_ratio = cfg . TRAINER . FIRST_SAMPLE_RATIO NEW_LINE first_sample_num = math . ceil ( totality * first_sample_ratio ) NEW_LINE second_sample_ratio = active_ratio / cfg . TRAINER . FIRST_SAMPLE_RATIO NEW_LINE second_sample_num = math . ceil ( first_sample_num * second_sample_ratio ) NEW_LINE first_stat = sorted ( first_stat , key = lambda x : x [ 4 ] , reverse = True ) NEW_LINE second_stat = first_stat [ : first_sample_num ] NEW_LINE second_stat = sorted ( second_stat , key = lambda x : x [ 3 ] , reverse = True ) NEW_LINE second_stat = np . array ( second_stat ) NEW_LINE active_samples = second_stat [ : second_sample_num , 0 : 2 , ... ] NEW_LINE candidate_ds_index = second_stat [ : second_sample_num , 2 , ... ] NEW_LINE candidate_ds_index = np . array ( candidate_ds_index , dtype = np . int ) NEW_LINE tgt_selected_ds . add_item ( active_samples ) NEW_LINE tgt_unlabeled_ds . remove_item ( candidate_ds_index ) NEW_LINE return active_samples NEW_LINE DEDENT\",), ('def bound_max_loss ( energy , bound ) : NEW_LINE INDENT energy_minus_bound = energy - bound NEW_LINE energy_minus_bound = torch . unsqueeze ( energy_minus_bound , dim = 1 ) NEW_LINE zeros = torch . zeros_like ( energy_minus_bound ) NEW_LINE for_select = torch . cat ( ( energy_minus_bound , zeros ) , dim = 1 ) NEW_LINE selected = torch . max ( for_select , dim = 1 ) . values NEW_LINE return selected . mean ( ) NEW_LINE DEDENT',), ('def __init__ ( self , cfg ) : NEW_LINE INDENT super ( FreeEnergyAlignmentLoss , self ) . __init__ ( ) NEW_LINE assert cfg . TRAINER . ENERGY_BETA > 0 , \" beta ▁ for ▁ energy ▁ calculate ▁ must ▁ be ▁ larger ▁ than ▁ 0\" NEW_LINE self . beta = cfg . TRAINER . ENERGY_BETA NEW_LINE self . type = cfg . TRAINER . ENERGY_ALIGN_TYPE NEW_LINE if self . type == \\' l1\\' : NEW_LINE INDENT self . loss = nn . L1Loss ( ) NEW_LINE DEDENT elif self . type == \\' mse \\' : NEW_LINE INDENT self . loss = nn . MSELoss ( ) NEW_LINE DEDENT elif self . type == \\' max \\' : NEW_LINE INDENT self . loss = bound_max_loss NEW_LINE DEDENT DEDENT',), ('def forward ( self , inputs , bound ) : NEW_LINE INDENT mul_neg_beta = - 1.0 * self . beta * inputs NEW_LINE log_sum_exp = torch . logsumexp ( mul_neg_beta , dim = 1 ) NEW_LINE free_energies = - 1.0 * log_sum_exp / self . beta NEW_LINE bound = torch . ones_like ( free_energies ) * bound NEW_LINE loss = self . loss ( free_energies , bound ) NEW_LINE return loss NEW_LINE DEDENT',), ('def __init__ ( self , cfg ) : NEW_LINE INDENT super ( NLLLoss , self ) . __init__ ( ) NEW_LINE assert cfg . TRAINER . ENERGY_BETA > 0 , \" beta ▁ for ▁ energy ▁ calculate ▁ must ▁ be ▁ larger ▁ than ▁ 0\" NEW_LINE self . beta = cfg . TRAINER . ENERGY_BETA NEW_LINE DEDENT',), ('def forward ( self , inputs , targets ) : NEW_LINE INDENT indices = torch . unsqueeze ( targets , dim = 1 ) NEW_LINE energy_c = torch . gather ( inputs , dim = 1 , index = indices ) NEW_LINE all_energy = - 1.0 * self . beta * inputs NEW_LINE free_energy = - 1.0 * torch . logsumexp ( all_energy , dim = 1 , keepdim = True ) / self . beta NEW_LINE nLL = energy_c - free_energy NEW_LINE return nLL . mean ( ) NEW_LINE DEDENT',), ('def __init__ ( self , bottleneck_dim = 256 , class_num = 1000 , cfg = None ) : NEW_LINE INDENT super ( ResNetFc , self ) . __init__ ( ) NEW_LINE self . cfg = cfg NEW_LINE if self . cfg . MODEL . BACKBONE . NAME == \\' resnet18\\' : NEW_LINE INDENT self . model_resnet = models . resnet18 ( pretrained = True ) NEW_LINE DEDENT elif self . cfg . MODEL . BACKBONE . NAME == \\' resnet34\\' : NEW_LINE INDENT self . model_resnet = models . resnet34 ( pretrained = True ) NEW_LINE DEDENT elif self . cfg . MODEL . BACKBONE . NAME == \\' resnet50\\' : NEW_LINE INDENT self . model_resnet = models . resnet50 ( pretrained = True ) NEW_LINE DEDENT elif self . cfg . MODEL . BACKBONE . NAME == \\' resnet101\\' : NEW_LINE INDENT self . model_resnet = models . resnet101 ( pretrained = True ) NEW_LINE DEDENT else : NEW_LINE INDENT raise RuntimeError ( \" Backbone ▁ not ▁ available : ▁ { } \" . format ( self . cfg . MODEL . BACKBONE . NAME ) ) NEW_LINE DEDENT model_resnet = self . model_resnet NEW_LINE self . conv1 = model_resnet . conv1 NEW_LINE self . bn1 = model_resnet . bn1 NEW_LINE self . relu = model_resnet . relu NEW_LINE self . maxpool = model_resnet . maxpool NEW_LINE self . layer1 = model_resnet . layer1 NEW_LINE self . layer2 = model_resnet . layer2 NEW_LINE self . layer3 = model_resnet . layer3 NEW_LINE self . layer4 = model_resnet . layer4 NEW_LINE self . avgpool = model_resnet . avgpool NEW_LINE self . bottleneck = nn . Linear ( model_resnet . fc . in_features , bottleneck_dim ) NEW_LINE self . bn2 = nn . BatchNorm1d ( bottleneck_dim ) NEW_LINE self . classifier = nn . Linear ( bottleneck_dim , class_num ) NEW_LINE DEDENT',), ('def forward ( self , x , return_feat = False ) : NEW_LINE INDENT x = self . conv1 ( x ) NEW_LINE x = self . bn1 ( x ) NEW_LINE x = self . relu ( x ) NEW_LINE x = self . maxpool ( x ) NEW_LINE x = self . layer1 ( x ) NEW_LINE x = self . layer2 ( x ) NEW_LINE x = self . layer3 ( x ) NEW_LINE x = self . layer4 ( x ) NEW_LINE x = self . avgpool ( x ) NEW_LINE x = torch . flatten ( x , 1 ) NEW_LINE x = self . bottleneck ( x ) NEW_LINE x = self . bn2 ( x ) NEW_LINE y = self . classifier ( x ) NEW_LINE if return_feat : NEW_LINE INDENT return x , y NEW_LINE DEDENT else : NEW_LINE INDENT return y NEW_LINE DEDENT DEDENT',), ('def output_num ( self ) : NEW_LINE INDENT return self . __in_features NEW_LINE DEDENT',), (\"def parameters_list ( self , lr ) : NEW_LINE INDENT parameter_list = [ { ' params ' : self . conv1 . parameters ( ) , ' lr ' : lr * self . cfg . OPTIM . BASE_LR_MULT } , { ' params ' : self . bn1 . parameters ( ) , ' lr ' : lr * self . cfg . OPTIM . BASE_LR_MULT } , { ' params ' : self . maxpool . parameters ( ) , ' lr ' : lr * self . cfg . OPTIM . BASE_LR_MULT } , { ' params ' : self . layer1 . parameters ( ) , ' lr ' : lr * self . cfg . OPTIM . BASE_LR_MULT } , { ' params ' : self . layer2 . parameters ( ) , ' lr ' : lr * self . cfg . OPTIM . BASE_LR_MULT } , { ' params ' : self . layer3 . parameters ( ) , ' lr ' : lr * self . cfg . OPTIM . BASE_LR_MULT } , { ' params ' : self . layer4 . parameters ( ) , ' lr ' : lr * self . cfg . OPTIM . BASE_LR_MULT } , { ' params ' : self . avgpool . parameters ( ) , ' lr ' : lr * self . cfg . OPTIM . BASE_LR_MULT } , { ' params ' : self . bottleneck . parameters ( ) } , { ' params ' : self . classifier . parameters ( ) } , ] NEW_LINE return parameter_list NEW_LINE DEDENT\",), (\"def build_transform ( cfg , is_train = True , choices = None ) : NEW_LINE INDENT if choices is None : NEW_LINE INDENT choices = cfg . INPUT . TEST . TRANSFORMS NEW_LINE DEDENT for choice in choices : NEW_LINE INDENT assert choice in AVAI_CHOICES , ' Invalid ▁ transform ▁ choice ▁ ( { } ) , ▁ ' ' expected ▁ to ▁ be ▁ one ▁ of ▁ { } ' . format ( choice , AVAI_CHOICES ) NEW_LINE DEDENT expected_size = ' { } x { } ' . format ( cfg . INPUT . SIZE [ 0 ] , cfg . INPUT . SIZE [ 1 ] ) NEW_LINE normalize = Normalize ( mean = cfg . INPUT . PIXEL_MEAN , std = cfg . INPUT . PIXEL_STD ) NEW_LINE if is_train : NEW_LINE INDENT return _build_transform_train ( cfg , choices , expected_size , normalize ) NEW_LINE DEDENT else : NEW_LINE INDENT return _build_transform_test ( cfg , choices , expected_size , normalize ) NEW_LINE DEDENT DEDENT\",), (\"def _build_transform_train ( cfg , choices , expected_size , normalize ) : NEW_LINE INDENT print ( ' Building ▁ transform _ train ' ) NEW_LINE tfm_train = [ ] NEW_LINE interp_mode = INTERPOLATION_MODES [ cfg . INPUT . INTERPOLATION ] NEW_LINE print ( ' + ▁ resize ▁ to ▁ { } ' . format ( expected_size ) ) NEW_LINE tfm_train += [ Resize ( cfg . INPUT . SIZE , interpolation = interp_mode ) ] NEW_LINE if ' random _ flip ' in choices : NEW_LINE INDENT print ( ' + ▁ random ▁ flip ' ) NEW_LINE tfm_train += [ RandomHorizontalFlip ( p = 0.5 ) ] NEW_LINE DEDENT if ' random _ crop ' in choices : NEW_LINE INDENT crop_padding = cfg . INPUT . CROP_PADDING NEW_LINE print ( ' + ▁ random ▁ crop ▁ ( padding ▁ = ▁ { } ) ' . format ( crop_padding ) ) NEW_LINE tfm_train += [ RandomCrop ( cfg . INPUT . CROP_SIZE , padding = crop_padding ) ] NEW_LINE DEDENT if ' random _ resized _ crop ' in choices : NEW_LINE INDENT print ( ' + ▁ random ▁ resized ▁ crop ' ) NEW_LINE tfm_train += [ RandomResizedCrop ( cfg . INPUT . CROP_SIZE , interpolation = interp_mode ) ] NEW_LINE DEDENT if ' center _ crop ' in choices : NEW_LINE INDENT print ( ' + ▁ center ▁ crop ▁ ( on ▁ 1.125x ▁ enlarged ▁ input ) ' ) NEW_LINE enlarged_size = [ int ( x * 1.125 ) for x in cfg . INPUT . SIZE ] NEW_LINE tfm_train += [ Resize ( enlarged_size , interpolation = interp_mode ) ] NEW_LINE tfm_train += [ CenterCrop ( cfg . INPUT . CROP_SIZE ) ] NEW_LINE DEDENT if ' colorjitter ' in choices : NEW_LINE INDENT print ( ' + ▁ color ▁ jitter ' ) NEW_LINE tfm_train += [ ColorJitter ( brightness = cfg . INPUT . COLORJITTER_SCALAR * 0.8 , contrast = cfg . INPUT . COLORJITTER_SCALAR * 0.8 , saturation = cfg . INPUT . COLORJITTER_SCALAR * 0.8 , hue = cfg . INPUT . COLORJITTER_SCALAR * 0.2 ) ] NEW_LINE DEDENT print ( ' + ▁ to ▁ torch ▁ tensor ▁ of ▁ range ▁ [ 0 , ▁ 1 ] ' ) NEW_LINE tfm_train += [ ToTensor ( ) ] NEW_LINE if ' normalize ' in choices : NEW_LINE INDENT print ( ' std = { } ) ' . format ( cfg . INPUT . PIXEL_MEAN , cfg . INPUT . PIXEL_STD ) ) tfm_train += [ normalize ] NEW_LINE DEDENT tfm_train = Compose ( tfm_train ) NEW_LINE return tfm_train NEW_LINE DEDENT\",), (\"def _build_transform_test ( cfg , choices , expected_size , normalize ) : NEW_LINE INDENT print ( ' Building ▁ transform _ test ' ) NEW_LINE tfm_test = [ ] NEW_LINE interp_mode = INTERPOLATION_MODES [ cfg . INPUT . INTERPOLATION ] NEW_LINE print ( ' + ▁ resize ▁ to ▁ { } ' . format ( expected_size ) ) NEW_LINE tfm_test += [ Resize ( cfg . INPUT . SIZE , interpolation = interp_mode ) ] NEW_LINE if ' center _ crop ' in choices : NEW_LINE INDENT print ( ' + ▁ center ▁ crop ▁ ( on ▁ 1.125x ▁ enlarged ▁ input ) ' ) NEW_LINE enlarged_size = [ int ( x * 1.125 ) for x in cfg . INPUT . SIZE ] NEW_LINE tfm_test += [ Resize ( enlarged_size , interpolation = interp_mode ) ] NEW_LINE tfm_test += [ CenterCrop ( cfg . INPUT . CROP_SIZE ) ] NEW_LINE DEDENT print ( ' + ▁ to ▁ torch ▁ tensor ▁ of ▁ range ▁ [ 0 , ▁ 1 ] ' ) NEW_LINE tfm_test += [ ToTensor ( ) ] NEW_LINE if ' normalize ' in choices : NEW_LINE INDENT print ( ' std = { } ) ' . format ( cfg . INPUT . PIXEL_MEAN , cfg . INPUT . PIXEL_STD ) ) tfm_test += [ normalize ] NEW_LINE DEDENT tfm_test = Compose ( tfm_test ) NEW_LINE return tfm_test NEW_LINE DEDENT\",), (\"def pil_loader ( path ) : NEW_LINE INDENT with open ( path , ' rb ' ) as f : NEW_LINE INDENT img = Image . open ( f ) NEW_LINE return img . convert ( ' RGB ' ) NEW_LINE DEDENT DEDENT\",), (\"def __init__ ( self , root = None , transform = None , target_transform = None , empty = False ) : NEW_LINE INDENT super ( ImageList , self ) . __init__ ( root , transform = transform , target_transform = target_transform ) NEW_LINE self . empty = empty NEW_LINE if empty : NEW_LINE INDENT self . samples = np . empty ( ( 1 , 2 ) , dtype = ' < U1000' ) NEW_LINE DEDENT else : NEW_LINE INDENT self . samples = np . loadtxt ( root , dtype = np . dtype ( ( np . unicode_ , 1000 ) ) , delimiter = ' ▁ ' ) NEW_LINE DEDENT self . loader = pil_loader NEW_LINE self . identity = transforms . Compose ( [ transforms . Resize ( 256 ) , transforms . RandomCrop ( 224 ) , transforms . ToTensor ( ) , transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ] , std = [ 0.229 , 0.224 , 0.225 ] ) ] ) NEW_LINE DEDENT\",), (\"def __getitem__ ( self , index ) : NEW_LINE INDENT path , label = self . samples [ index ] NEW_LINE label = int ( label ) NEW_LINE output = { ' label ' : label , ' path ' : path , ' index ' : index } NEW_LINE img0 = self . loader ( path ) NEW_LINE output [ ' img ' ] = self . identity ( img0 ) NEW_LINE if self . transform is not None : NEW_LINE INDENT output [ ' img0' ] = self . transform ( img0 ) NEW_LINE DEDENT return output NEW_LINE DEDENT\",), ('def __len__ ( self ) : NEW_LINE INDENT return len ( self . samples ) NEW_LINE DEDENT',), ('def add_item ( self , addition ) : NEW_LINE INDENT if self . empty : NEW_LINE INDENT self . samples = addition NEW_LINE self . empty = False NEW_LINE DEDENT else : NEW_LINE INDENT self . samples = np . concatenate ( ( self . samples , addition ) , axis = 0 ) NEW_LINE DEDENT return self . samples NEW_LINE DEDENT',), ('def remove_item ( self , reduced ) : NEW_LINE INDENT self . samples = np . delete ( self . samples , reduced , axis = 0 ) NEW_LINE return self . samples NEW_LINE DEDENT',)]\n",
            "1.257208976894617\n",
            "On batch 511\n",
            "tensor([0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "('\\n\\\\label{eq:find_adv}\\n\\\\max_{\\\\ddelta : \\\\|\\\\ddelta\\\\|\\\\leq \\\\varepsilon}\\n\\\\zeta(y\\\\varphi(\\\\xx+\\\\ddelta))\\\\,.\\n',)\n",
            "[(\"def get_paths ( path , name = ' coco ' , use_restval = False ) : NEW_LINE INDENT roots = { } NEW_LINE ids = { } NEW_LINE if ' coco ' == name : NEW_LINE INDENT imgdir = os . path . join ( path , ' images ' ) NEW_LINE capdir = os . path . join ( path , ' annotations ' ) NEW_LINE roots [ ' train ' ] = { ' img ' : os . path . join ( imgdir , ' train2014' ) , ' cap ' : os . path . join ( capdir , ' captions _ train2014 . json ' ) } NEW_LINE roots [ ' val ' ] = { ' img ' : os . path . join ( imgdir , ' val2014' ) , ' cap ' : os . path . join ( capdir , ' captions _ val2014 . json ' ) } NEW_LINE roots [ ' test ' ] = { ' img ' : os . path . join ( imgdir , ' val2014' ) , ' cap ' : os . path . join ( capdir , ' captions _ val2014 . json ' ) } NEW_LINE roots [ ' trainrestval ' ] = { ' img ' : ( roots [ ' train ' ] [ ' img ' ] , roots [ ' val ' ] [ ' img ' ] ) , ' cap ' : ( roots [ ' train ' ] [ ' cap ' ] , roots [ ' val ' ] [ ' cap ' ] ) } NEW_LINE ids [ ' train ' ] = np . load ( os . path . join ( capdir , ' coco _ train _ ids . npy ' ) ) NEW_LINE ids [ ' val ' ] = np . load ( os . path . join ( capdir , ' coco _ dev _ ids . npy ' ) ) [ : 5000 ] NEW_LINE ids [ ' test ' ] = np . load ( os . path . join ( capdir , ' coco _ test _ ids . npy ' ) ) NEW_LINE ids [ ' trainrestval ' ] = ( ids [ ' train ' ] , np . load ( os . path . join ( capdir , ' coco _ restval _ ids . npy ' ) ) ) NEW_LINE if use_restval : NEW_LINE INDENT roots [ ' train ' ] = roots [ ' trainrestval ' ] NEW_LINE ids [ ' train ' ] = ids [ ' trainrestval ' ] NEW_LINE DEDENT DEDENT elif ' f8k ' == name : NEW_LINE INDENT imgdir = os . path . join ( path , ' images ' ) NEW_LINE cap = os . path . join ( path , ' dataset _ flickr8k . json ' ) NEW_LINE roots [ ' train ' ] = { ' img ' : imgdir , ' cap ' : cap } NEW_LINE roots [ ' val ' ] = { ' img ' : imgdir , ' cap ' : cap } NEW_LINE roots [ ' test ' ] = { ' img ' : imgdir , ' cap ' : cap } NEW_LINE ids = { ' train ' : None , ' val ' : None , ' test ' : None } NEW_LINE DEDENT elif ' f30k ' == name : NEW_LINE INDENT imgdir = os . path . join ( path , ' images ' ) NEW_LINE cap = os . path . join ( path , ' dataset _ flickr30k . json ' ) NEW_LINE roots [ ' train ' ] = { ' img ' : imgdir , ' cap ' : cap } NEW_LINE roots [ ' val ' ] = { ' img ' : imgdir , ' cap ' : cap } NEW_LINE roots [ ' test ' ] = { ' img ' : imgdir , ' cap ' : cap } NEW_LINE ids = { ' train ' : None , ' val ' : None , ' test ' : None } NEW_LINE DEDENT return roots , ids NEW_LINE DEDENT\",), ('def collate_fn ( data ) : NEW_LINE INDENT data . sort ( key = lambda x : len ( x [ 1 ] ) , reverse = True ) NEW_LINE images , captions , ids , img_ids = zip ( * data ) NEW_LINE images = torch . stack ( images , 0 ) NEW_LINE lengths = [ len ( cap ) for cap in captions ] NEW_LINE targets = torch . zeros ( len ( captions ) , max ( lengths ) ) . long ( ) NEW_LINE for i , cap in enumerate ( captions ) : NEW_LINE INDENT end = lengths [ i ] NEW_LINE targets [ i , : end ] = cap [ : end ] NEW_LINE DEDENT return images , targets , lengths , ids NEW_LINE DEDENT',), (\"def get_loader_single ( data_name , split , root , json , vocab , transform , batch_size = 100 , shuffle = True , num_workers = 2 , ids = None , collate_fn = collate_fn ) : NEW_LINE INDENT if ' coco ' in data_name : NEW_LINE INDENT dataset = CocoDataset ( root = root , json = json , vocab = vocab , transform = transform , ids = ids ) NEW_LINE DEDENT elif ' f8k ' in data_name or ' f30k ' in data_name : NEW_LINE INDENT dataset = FlickrDataset ( root = root , split = split , json = json , vocab = vocab , transform = transform ) NEW_LINE DEDENT data_loader = torch . utils . data . DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = shuffle , pin_memory = True , num_workers = num_workers , collate_fn = collate_fn ) NEW_LINE return data_loader NEW_LINE DEDENT\",), ('def get_precomp_loader ( data_path , data_split , vocab , opt , batch_size = 100 , shuffle = True , num_workers = 2 ) : NEW_LINE INDENT dset = PrecompDataset ( data_path , data_split , vocab ) NEW_LINE data_loader = torch . utils . data . DataLoader ( dataset = dset , batch_size = batch_size , shuffle = shuffle , pin_memory = True , collate_fn = collate_fn ) NEW_LINE return data_loader NEW_LINE DEDENT',), (\"def get_transform ( data_name , split_name , opt ) : NEW_LINE INDENT normalizer = transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ] , std = [ 0.229 , 0.224 , 0.225 ] ) NEW_LINE t_list = [ ] NEW_LINE if split_name == ' train ' : NEW_LINE INDENT t_list = [ transforms . RandomResizedCrop ( opt . crop_size ) , transforms . RandomHorizontalFlip ( ) ] NEW_LINE DEDENT elif split_name == ' val ' : NEW_LINE INDENT t_list = [ transforms . Resize ( 256 ) , transforms . CenterCrop ( 224 ) ] NEW_LINE DEDENT elif split_name == ' test ' : NEW_LINE INDENT t_list = [ transforms . Resize ( 256 ) , transforms . CenterCrop ( 224 ) ] NEW_LINE DEDENT t_end = [ transforms . ToTensor ( ) , normalizer ] NEW_LINE transform = transforms . Compose ( t_list + t_end ) NEW_LINE return transform NEW_LINE DEDENT\",), (\"def get_loaders ( data_name , vocab , crop_size , batch_size , workers , opt ) : NEW_LINE INDENT dpath = os . path . join ( opt . data_path , data_name ) NEW_LINE if opt . data_name . endswith ( ' _ precomp ' ) : NEW_LINE INDENT train_loader = get_precomp_loader ( dpath , ' train ' , vocab , opt , batch_size , True , workers ) NEW_LINE val_loader = get_precomp_loader ( dpath , ' dev ' , vocab , opt , batch_size , False , workers ) NEW_LINE DEDENT else : NEW_LINE INDENT roots , ids = get_paths ( dpath , data_name , opt . use_restval ) NEW_LINE transform = get_transform ( data_name , ' train ' , opt ) NEW_LINE train_loader = get_loader_single ( opt . data_name , ' train ' , roots [ ' train ' ] [ ' img ' ] , roots [ ' train ' ] [ ' cap ' ] , vocab , transform , ids = ids [ ' train ' ] , batch_size = batch_size , shuffle = True , num_workers = workers , collate_fn = collate_fn ) NEW_LINE transform = get_transform ( data_name , ' val ' , opt ) NEW_LINE val_loader = get_loader_single ( opt . data_name , ' val ' , roots [ ' val ' ] [ ' img ' ] , roots [ ' val ' ] [ ' cap ' ] , vocab , transform , ids = ids [ ' val ' ] , batch_size = batch_size , shuffle = False , num_workers = workers , collate_fn = collate_fn ) NEW_LINE DEDENT return train_loader , val_loader NEW_LINE DEDENT\",), (\"def get_test_loader ( split_name , data_name , vocab , crop_size , batch_size , workers , opt ) : NEW_LINE INDENT dpath = os . path . join ( opt . data_path , data_name ) NEW_LINE if opt . data_name . endswith ( ' _ precomp ' ) : NEW_LINE INDENT test_loader = get_precomp_loader ( dpath , split_name , vocab , opt , batch_size , False , workers ) NEW_LINE DEDENT else : NEW_LINE INDENT roots , ids = get_paths ( dpath , data_name , opt . use_restval ) NEW_LINE transform = get_transform ( data_name , split_name , opt ) NEW_LINE test_loader = get_loader_single ( opt . data_name , split_name , roots [ split_name ] [ ' img ' ] , roots [ split_name ] [ ' cap ' ] , vocab , transform , ids = ids [ split_name ] , batch_size = batch_size , shuffle = False , num_workers = workers , collate_fn = collate_fn ) NEW_LINE DEDENT return test_loader NEW_LINE DEDENT\",), ('def __init__ ( self , root , json , vocab , transform = None , ids = None ) : NEW_LINE INDENT self . root = root NEW_LINE if isinstance ( json , tuple ) : NEW_LINE INDENT self . coco = ( COCO ( json [ 0 ] ) , COCO ( json [ 1 ] ) ) NEW_LINE DEDENT else : NEW_LINE INDENT self . coco = ( COCO ( json ) , ) NEW_LINE self . root = ( root , ) NEW_LINE DEDENT if ids is None : NEW_LINE INDENT self . ids = list ( self . coco . anns . keys ( ) ) NEW_LINE DEDENT else : NEW_LINE INDENT self . ids = ids NEW_LINE DEDENT if isinstance ( self . ids , tuple ) : NEW_LINE INDENT self . bp = len ( self . ids [ 0 ] ) NEW_LINE self . ids = list ( self . ids [ 0 ] ) + list ( self . ids [ 1 ] ) NEW_LINE DEDENT else : NEW_LINE INDENT self . bp = len ( self . ids ) NEW_LINE DEDENT self . vocab = vocab NEW_LINE self . transform = transform NEW_LINE DEDENT',), (\"def __getitem__ ( self , index ) : NEW_LINE INDENT vocab = self . vocab NEW_LINE root , caption , img_id , path , image = self . get_raw_item ( index ) NEW_LINE if self . transform is not None : NEW_LINE INDENT image = self . transform ( image ) NEW_LINE DEDENT tokens = nltk . tokenize . word_tokenize ( str ( caption ) . lower ( ) . decode ( ' utf - 8' ) ) NEW_LINE caption = [ ] NEW_LINE caption . append ( vocab ( ' < start > ' ) ) NEW_LINE caption . extend ( [ vocab ( token ) for token in tokens ] ) NEW_LINE caption . append ( vocab ( ' < end > ' ) ) NEW_LINE target = torch . Tensor ( caption ) NEW_LINE return image , target , index , img_id NEW_LINE DEDENT\",), (\"def get_raw_item ( self , index ) : NEW_LINE INDENT if index < self . bp : NEW_LINE INDENT coco = self . coco [ 0 ] NEW_LINE root = self . root [ 0 ] NEW_LINE DEDENT else : NEW_LINE INDENT coco = self . coco [ 1 ] NEW_LINE root = self . root [ 1 ] NEW_LINE DEDENT ann_id = self . ids [ index ] NEW_LINE caption = coco . anns [ ann_id ] [ ' caption ' ] NEW_LINE img_id = coco . anns [ ann_id ] [ ' image _ id ' ] NEW_LINE path = coco . loadImgs ( img_id ) [ 0 ] [ ' file _ name ' ] NEW_LINE image = Image . open ( os . path . join ( root , path ) ) . convert ( ' RGB ' ) NEW_LINE return root , caption , img_id , path , image NEW_LINE DEDENT\",), ('def __len__ ( self ) : NEW_LINE INDENT return len ( self . ids ) NEW_LINE DEDENT',), (\"def __init__ ( self , root , json , split , vocab , transform = None ) : NEW_LINE INDENT self . root = root NEW_LINE self . vocab = vocab NEW_LINE self . split = split NEW_LINE self . transform = transform NEW_LINE self . dataset = jsonmod . load ( open ( json , ' r ' ) ) [ ' images ' ] NEW_LINE self . ids = [ ] NEW_LINE for i , d in enumerate ( self . dataset ) : NEW_LINE INDENT if d [ ' split ' ] == split : NEW_LINE INDENT self . ids += [ ( i , x ) for x in range ( len ( d [ ' sentences ' ] ) ) ] NEW_LINE DEDENT DEDENT DEDENT\",), (\"def __getitem__ ( self , index ) : NEW_LINE INDENT vocab = self . vocab NEW_LINE root = self . root NEW_LINE ann_id = self . ids [ index ] NEW_LINE img_id = ann_id [ 0 ] NEW_LINE caption = self . dataset [ img_id ] [ ' sentences ' ] [ ann_id [ 1 ] ] [ ' raw ' ] NEW_LINE path = self . dataset [ img_id ] [ ' filename ' ] NEW_LINE image = Image . open ( os . path . join ( root , path ) ) . convert ( ' RGB ' ) NEW_LINE if self . transform is not None : NEW_LINE INDENT image = self . transform ( image ) NEW_LINE DEDENT tokens = nltk . tokenize . word_tokenize ( str ( caption ) . lower ( ) . decode ( ' utf - 8' ) ) NEW_LINE caption = [ ] NEW_LINE caption . append ( vocab ( ' < start > ' ) ) NEW_LINE caption . extend ( [ vocab ( token ) for token in tokens ] ) NEW_LINE caption . append ( vocab ( ' < end > ' ) ) NEW_LINE target = torch . Tensor ( caption ) NEW_LINE return image , target , index , img_id NEW_LINE DEDENT\",), ('def __len__ ( self ) : NEW_LINE INDENT return len ( self . ids ) NEW_LINE DEDENT',), (\"def __init__ ( self , data_path , data_split , vocab ) : NEW_LINE INDENT self . vocab = vocab NEW_LINE loc = data_path + ' / ' NEW_LINE self . captions = [ ] NEW_LINE with open ( loc + ' % s _ caps . txt ' % data_split , ' rb ' ) as f : NEW_LINE INDENT for line in f : NEW_LINE INDENT self . captions . append ( line . strip ( ) ) NEW_LINE DEDENT DEDENT self . images = np . load ( loc + ' % s _ ims . npy ' % data_split ) NEW_LINE self . length = len ( self . captions ) NEW_LINE if self . images . shape [ 0 ] != self . length : NEW_LINE INDENT self . im_div = 5 NEW_LINE DEDENT else : NEW_LINE INDENT self . im_div = 1 NEW_LINE DEDENT if data_split == ' dev ' : NEW_LINE INDENT self . length = 5000 NEW_LINE DEDENT DEDENT\",), (\"def __getitem__ ( self , index ) : NEW_LINE INDENT img_id = index / self . im_div NEW_LINE image = torch . Tensor ( self . images [ img_id ] ) NEW_LINE caption = self . captions [ index ] NEW_LINE vocab = self . vocab NEW_LINE tokens = nltk . tokenize . word_tokenize ( str ( caption ) . lower ( ) . decode ( ' utf - 8' ) ) NEW_LINE caption = [ ] NEW_LINE caption . append ( vocab ( ' < start > ' ) ) NEW_LINE caption . extend ( [ vocab ( token ) for token in tokens ] ) NEW_LINE caption . append ( vocab ( ' < end > ' ) ) NEW_LINE target = torch . Tensor ( caption ) NEW_LINE return image , target , index , img_id NEW_LINE DEDENT\",), ('def __len__ ( self ) : NEW_LINE INDENT return self . length NEW_LINE DEDENT',), (\"def from_coco_json ( path ) : NEW_LINE INDENT coco = COCO ( path ) NEW_LINE ids = coco . anns . keys ( ) NEW_LINE captions = [ ] NEW_LINE for i , idx in enumerate ( ids ) : NEW_LINE INDENT captions . append ( str ( coco . anns [ idx ] [ ' caption ' ] ) ) NEW_LINE DEDENT return captions NEW_LINE DEDENT\",), (\"def from_flickr_json ( path ) : NEW_LINE INDENT dataset = json . load ( open ( path , ' r ' ) ) [ ' images ' ] NEW_LINE captions = [ ] NEW_LINE for i , d in enumerate ( dataset ) : NEW_LINE INDENT captions += [ str ( x [ ' raw ' ] ) for x in d [ ' sentences ' ] ] NEW_LINE DEDENT return captions NEW_LINE DEDENT\",), (\"def from_txt ( txt ) : NEW_LINE INDENT captions = [ ] NEW_LINE with open ( txt , ' rb ' ) as f : NEW_LINE INDENT for line in f : NEW_LINE INDENT captions . append ( line . strip ( ) ) NEW_LINE DEDENT DEDENT return captions NEW_LINE DEDENT\",), ('def build_vocab ( data_path , data_name , jsons , threshold ) : NEW_LINE INDENT counter = Counter ( ) NEW_LINE for path in jsons [ data_name ] : NEW_LINE INDENT full_path = os . path . join ( os . path . join ( data_path , data_name ) , path ) NEW_LINE if data_name == \\' coco \\' : NEW_LINE INDENT captions = from_coco_json ( full_path ) NEW_LINE DEDENT elif data_name == \\' f8k \\' or data_name == \\' f30k \\' : NEW_LINE INDENT captions = from_flickr_json ( full_path ) NEW_LINE DEDENT else : NEW_LINE INDENT captions = from_txt ( full_path ) NEW_LINE DEDENT for i , caption in enumerate ( captions ) : NEW_LINE INDENT tokens = nltk . tokenize . word_tokenize ( caption . lower ( ) . decode ( \\' utf - 8\\' ) ) NEW_LINE counter . update ( tokens ) NEW_LINE if i % 1000 == 0 : NEW_LINE INDENT print ( \" [ % d / % d ] ▁ tokenized ▁ the ▁ captions . \" % ( i , len ( captions ) ) ) NEW_LINE DEDENT DEDENT DEDENT words = [ word for word , cnt in counter . items ( ) if cnt >= threshold ] NEW_LINE vocab = Vocabulary ( ) NEW_LINE vocab . add_word ( \\' < pad > \\' ) NEW_LINE vocab . add_word ( \\' < start > \\' ) NEW_LINE vocab . add_word ( \\' < end > \\' ) NEW_LINE vocab . add_word ( \\' < unk > \\' ) NEW_LINE for i , word in enumerate ( words ) : NEW_LINE INDENT vocab . add_word ( word ) NEW_LINE DEDENT return vocab NEW_LINE DEDENT',), ('def main ( data_path , data_name ) : NEW_LINE INDENT vocab = build_vocab ( data_path , data_name , jsons = annotations , threshold = 4 ) NEW_LINE with open ( \\' . / vocab / % s _ vocab . pkl \\' % data_name , \\' wb \\' ) as f : NEW_LINE INDENT pickle . dump ( vocab , f , pickle . HIGHEST_PROTOCOL ) NEW_LINE DEDENT print ( \" Saved ▁ vocabulary ▁ file ▁ to ▁ \" , \\' . / vocab / % s _ vocab . pkl \\' % data_name ) NEW_LINE DEDENT',), ('def __init__ ( self ) : NEW_LINE INDENT self . word2idx = { } NEW_LINE self . idx2word = { } NEW_LINE self . idx = 0 NEW_LINE DEDENT',), ('def add_word ( self , word ) : NEW_LINE INDENT if word not in self . word2idx : NEW_LINE INDENT self . word2idx [ word ] = self . idx NEW_LINE self . idx2word [ self . idx ] = word NEW_LINE self . idx += 1 NEW_LINE DEDENT DEDENT',), (\"def __call__ ( self , word ) : NEW_LINE INDENT if word not in self . word2idx : NEW_LINE INDENT return self . word2idx [ ' < unk > ' ] NEW_LINE DEDENT return self . word2idx [ word ] NEW_LINE DEDENT\",), ('def __len__ ( self ) : NEW_LINE INDENT return len ( self . word2idx ) NEW_LINE DEDENT',), (\"def train ( opt , train_loader , model , epoch , val_loader ) : NEW_LINE INDENT batch_time = AverageMeter ( ) NEW_LINE data_time = AverageMeter ( ) NEW_LINE train_logger = LogCollector ( ) NEW_LINE model . train_start ( ) NEW_LINE end = time . time ( ) NEW_LINE for i , train_data in enumerate ( train_loader ) : NEW_LINE INDENT if opt . reset_train : NEW_LINE INDENT model . train_start ( ) NEW_LINE DEDENT data_time . update ( time . time ( ) - end ) NEW_LINE model . logger = train_logger NEW_LINE model . train_emb ( * train_data ) NEW_LINE batch_time . update ( time . time ( ) - end ) NEW_LINE end = time . time ( ) NEW_LINE if model . Eiters % opt . log_step == 0 : NEW_LINE INDENT logging . info ( . format ( epoch , i , len ( train_loader ) , batch_time = batch_time , data_time = data_time , e_log = str ( model . logger ) ) ) DEDENT tb_logger . log_value ( ' epoch ' , epoch , step = model . Eiters ) NEW_LINE tb_logger . log_value ( ' step ' , i , step = model . Eiters ) NEW_LINE tb_logger . log_value ( ' batch _ time ' , batch_time . val , step = model . Eiters ) NEW_LINE tb_logger . log_value ( ' data _ time ' , data_time . val , step = model . Eiters ) NEW_LINE model . logger . tb_log ( tb_logger , step = model . Eiters ) NEW_LINE if model . Eiters % opt . val_step == 0 : NEW_LINE INDENT validate ( opt , val_loader , model ) NEW_LINE DEDENT DEDENT DEDENT\",), ('def validate ( opt , val_loader , model ) : NEW_LINE INDENT img_embs , cap_embs = encode_data ( model , val_loader , opt . log_step , logging . info ) NEW_LINE ( r1 , r5 , r10 , medr , meanr ) = i2t ( img_embs , cap_embs , measure = opt . measure ) NEW_LINE logging . info ( \" Image ▁ to ▁ text : ▁ % .1f , ▁ % .1f , ▁ % .1f , ▁ % .1f , ▁ % .1f \" % ( r1 , r5 , r10 , medr , meanr ) ) NEW_LINE ( r1i , r5i , r10i , medri , meanri ) = t2i ( img_embs , cap_embs , measure = opt . measure ) NEW_LINE logging . info ( \" Text ▁ to ▁ image : ▁ % .1f , ▁ % .1f , ▁ % .1f , ▁ % .1f , ▁ % .1f \" % ( r1i , r5i , r10i , medri , meanri ) ) NEW_LINE currscore = r1 + r5 + r10 + r1i + r5i + r10i NEW_LINE tb_logger . log_value ( \\' r1\\' , r1 , step = model . Eiters ) NEW_LINE tb_logger . log_value ( \\' r5\\' , r5 , step = model . Eiters ) NEW_LINE tb_logger . log_value ( \\' r10\\' , r10 , step = model . Eiters ) NEW_LINE tb_logger . log_value ( \\' medr \\' , medr , step = model . Eiters ) NEW_LINE tb_logger . log_value ( \\' meanr \\' , meanr , step = model . Eiters ) NEW_LINE tb_logger . log_value ( \\' r1i \\' , r1i , step = model . Eiters ) NEW_LINE tb_logger . log_value ( \\' r5i \\' , r5i , step = model . Eiters ) NEW_LINE tb_logger . log_value ( \\' r10i \\' , r10i , step = model . Eiters ) NEW_LINE tb_logger . log_value ( \\' medri \\' , medri , step = model . Eiters ) NEW_LINE tb_logger . log_value ( \\' meanri \\' , meanri , step = model . Eiters ) NEW_LINE tb_logger . log_value ( \\' rsum \\' , currscore , step = model . Eiters ) NEW_LINE return currscore NEW_LINE DEDENT',), (\"def save_checkpoint ( state , is_best , filename = ' checkpoint . pth . tar ' , prefix = ' ' ) : NEW_LINE INDENT torch . save ( state , prefix + filename ) NEW_LINE if is_best : NEW_LINE INDENT shutil . copyfile ( prefix + filename , prefix + ' model _ best . pth . tar ' ) NEW_LINE DEDENT DEDENT\",), (\"def adjust_learning_rate ( opt , optimizer , epoch ) : NEW_LINE INDENT lr = opt . learning_rate * ( 0.1 ** ( epoch // opt . lr_update ) ) NEW_LINE for param_group in optimizer . param_groups : NEW_LINE INDENT param_group [ ' lr ' ] = lr NEW_LINE DEDENT DEDENT\",), ('def accuracy ( output , target , topk = ( 1 , ) ) : NEW_LINE INDENT maxk = max ( topk ) NEW_LINE batch_size = target . size ( 0 ) NEW_LINE _ , pred = output . topk ( maxk , 1 , True , True ) NEW_LINE pred = pred . t ( ) NEW_LINE correct = pred . eq ( target . view ( 1 , - 1 ) . expand_as ( pred ) ) NEW_LINE res = [ ] NEW_LINE for k in topk : NEW_LINE INDENT correct_k = correct [ : k ] . view ( - 1 ) . float ( ) . sum ( 0 ) NEW_LINE res . append ( correct_k . mul_ ( 100.0 / batch_size ) ) NEW_LINE DEDENT return res NEW_LINE DEDENT',), ('def l2norm ( X ) : NEW_LINE INDENT norm = torch . pow ( X , 2 ) . sum ( dim = 1 , keepdim = True ) . sqrt ( ) NEW_LINE X = torch . div ( X , norm ) NEW_LINE return X NEW_LINE DEDENT',), (\"def EncoderImage ( data_name , img_dim , embed_size , finetune = False , cnn_type = ' vgg19' , use_abs = False , no_imgnorm = False ) : NEW_LINE INDENT if data_name . endswith ( ' _ precomp ' ) : NEW_LINE INDENT img_enc = EncoderImagePrecomp ( img_dim , embed_size , use_abs , no_imgnorm ) NEW_LINE DEDENT else : NEW_LINE INDENT img_enc = EncoderImageFull ( embed_size , finetune , cnn_type , use_abs , no_imgnorm ) NEW_LINE DEDENT return img_enc NEW_LINE DEDENT\",), ('def cosine_sim ( im , s ) : NEW_LINE INDENT return im . mm ( s . t ( ) ) NEW_LINE DEDENT',), ('def order_sim ( im , s ) : NEW_LINE INDENT YmX = ( s . unsqueeze ( 1 ) . expand ( s . size ( 0 ) , im . size ( 0 ) , s . size ( 1 ) ) - im . unsqueeze ( 0 ) . expand ( s . size ( 0 ) , im . size ( 0 ) , s . size ( 1 ) ) ) NEW_LINE score = - YmX . clamp ( min = 0 ) . pow ( 2 ) . sum ( 2 ) . sqrt ( ) . t ( ) NEW_LINE return score NEW_LINE DEDENT',), (\"def __init__ ( self , embed_size , finetune = False , cnn_type = ' vgg19' , use_abs = False , no_imgnorm = False ) : NEW_LINE INDENT super ( EncoderImageFull , self ) . __init__ ( ) NEW_LINE self . embed_size = embed_size NEW_LINE self . no_imgnorm = no_imgnorm NEW_LINE self . use_abs = use_abs NEW_LINE self . cnn = self . get_cnn ( cnn_type , True ) NEW_LINE for param in self . cnn . parameters ( ) : NEW_LINE INDENT param . requires_grad = finetune NEW_LINE DEDENT if cnn_type . startswith ( ' vgg ' ) : NEW_LINE INDENT self . fc = nn . Linear ( self . cnn . classifier . _modules [ '6' ] . in_features , embed_size ) NEW_LINE self . cnn . classifier = nn . Sequential ( * list ( self . cnn . classifier . children ( ) ) [ : - 1 ] ) NEW_LINE DEDENT elif cnn_type . startswith ( ' resnet ' ) : NEW_LINE INDENT self . fc = nn . Linear ( self . cnn . module . fc . in_features , embed_size ) NEW_LINE self . cnn . module . fc = nn . Sequential ( ) NEW_LINE DEDENT self . init_weights ( ) NEW_LINE DEDENT\",), ('def get_cnn ( self , arch , pretrained ) : NEW_LINE INDENT if pretrained : NEW_LINE INDENT print ( \" = > ▁ using ▁ pre - trained ▁ model ▁ \\' { } \\' \" . format ( arch ) ) NEW_LINE model = models . __dict__ [ arch ] ( pretrained = True ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \" = > ▁ creating ▁ model ▁ \\' { } \\' \" . format ( arch ) ) NEW_LINE model = models . __dict__ [ arch ] ( ) NEW_LINE DEDENT if arch . startswith ( \\' alexnet \\' ) or arch . startswith ( \\' vgg \\' ) : NEW_LINE INDENT model . features = nn . DataParallel ( model . features ) NEW_LINE model . cuda ( ) NEW_LINE DEDENT else : NEW_LINE INDENT model = nn . DataParallel ( model ) . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT',), (\"def load_state_dict ( self , state_dict ) : NEW_LINE INDENT if ' cnn . classifier . 1 . weight ' in state_dict : NEW_LINE INDENT state_dict [ ' cnn . classifier . 0 . weight ' ] = state_dict [ ' cnn . classifier . 1 . weight ' ] NEW_LINE del state_dict [ ' cnn . classifier . 1 . weight ' ] NEW_LINE state_dict [ ' cnn . classifier . 0 . bias ' ] = state_dict [ ' cnn . classifier . 1 . bias ' ] NEW_LINE del state_dict [ ' cnn . classifier . 1 . bias ' ] NEW_LINE state_dict [ ' cnn . classifier . 3 . weight ' ] = state_dict [ ' cnn . classifier . 4 . weight ' ] NEW_LINE del state_dict [ ' cnn . classifier . 4 . weight ' ] NEW_LINE state_dict [ ' cnn . classifier . 3 . bias ' ] = state_dict [ ' cnn . classifier . 4 . bias ' ] NEW_LINE del state_dict [ ' cnn . classifier . 4 . bias ' ] NEW_LINE DEDENT super ( EncoderImageFull , self ) . load_state_dict ( state_dict ) NEW_LINE DEDENT\",), ('def init_weights ( self ) : NEW_LINE INDENT r = np . sqrt ( 6. ) / np . sqrt ( self . fc . in_features + self . fc . out_features ) NEW_LINE self . fc . weight . data . uniform_ ( - r , r ) NEW_LINE self . fc . bias . data . fill_ ( 0 ) NEW_LINE DEDENT',), ('def forward ( self , images ) : NEW_LINE INDENT features = self . cnn ( images ) NEW_LINE features = l2norm ( features ) NEW_LINE features = self . fc ( features ) NEW_LINE if not self . no_imgnorm : NEW_LINE INDENT features = l2norm ( features ) NEW_LINE DEDENT if self . use_abs : NEW_LINE INDENT features = torch . abs ( features ) NEW_LINE DEDENT return features NEW_LINE DEDENT',), ('def __init__ ( self , img_dim , embed_size , use_abs = False , no_imgnorm = False ) : NEW_LINE INDENT super ( EncoderImagePrecomp , self ) . __init__ ( ) NEW_LINE self . embed_size = embed_size NEW_LINE self . no_imgnorm = no_imgnorm NEW_LINE self . use_abs = use_abs NEW_LINE self . fc = nn . Linear ( img_dim , embed_size ) NEW_LINE self . init_weights ( ) NEW_LINE DEDENT',), ('def init_weights ( self ) : NEW_LINE INDENT r = np . sqrt ( 6. ) / np . sqrt ( self . fc . in_features + self . fc . out_features ) NEW_LINE self . fc . weight . data . uniform_ ( - r , r ) NEW_LINE self . fc . bias . data . fill_ ( 0 ) NEW_LINE DEDENT',), ('def forward ( self , images ) : NEW_LINE INDENT features = self . fc ( images ) NEW_LINE if not self . no_imgnorm : NEW_LINE INDENT features = l2norm ( features ) NEW_LINE DEDENT if self . use_abs : NEW_LINE INDENT features = torch . abs ( features ) NEW_LINE DEDENT return features NEW_LINE DEDENT',), ('def load_state_dict ( self , state_dict ) : NEW_LINE INDENT own_state = self . state_dict ( ) NEW_LINE new_state = OrderedDict ( ) NEW_LINE for name , param in state_dict . items ( ) : NEW_LINE INDENT if name in own_state : NEW_LINE INDENT new_state [ name ] = param NEW_LINE DEDENT DEDENT super ( EncoderImagePrecomp , self ) . load_state_dict ( new_state ) NEW_LINE DEDENT',), ('def __init__ ( self , vocab_size , word_dim , embed_size , num_layers , use_abs = False ) : NEW_LINE INDENT super ( EncoderText , self ) . __init__ ( ) NEW_LINE self . use_abs = use_abs NEW_LINE self . embed_size = embed_size NEW_LINE self . embed = nn . Embedding ( vocab_size , word_dim ) NEW_LINE self . rnn = nn . GRU ( word_dim , embed_size , num_layers , batch_first = True ) NEW_LINE self . init_weights ( ) NEW_LINE DEDENT',), ('def init_weights ( self ) : NEW_LINE INDENT self . embed . weight . data . uniform_ ( - 0.1 , 0.1 ) NEW_LINE DEDENT',), ('def forward ( self , x , lengths ) : NEW_LINE INDENT x = self . embed ( x ) NEW_LINE packed = pack_padded_sequence ( x , lengths , batch_first = True ) NEW_LINE out , _ = self . rnn ( packed ) NEW_LINE padded = pad_packed_sequence ( out , batch_first = True ) NEW_LINE I = torch . LongTensor ( lengths ) . view ( - 1 , 1 , 1 ) NEW_LINE I = Variable ( I . expand ( x . size ( 0 ) , 1 , self . embed_size ) - 1 ) . cuda ( ) NEW_LINE out = torch . gather ( padded [ 0 ] , 1 , I ) . squeeze ( 1 ) NEW_LINE out = l2norm ( out ) NEW_LINE if self . use_abs : NEW_LINE INDENT out = torch . abs ( out ) NEW_LINE DEDENT return out NEW_LINE DEDENT',), (\"def __init__ ( self , margin = 0 , measure = False , max_violation = False ) : NEW_LINE INDENT super ( ContrastiveLoss , self ) . __init__ ( ) NEW_LINE self . margin = margin NEW_LINE if measure == ' order ' : NEW_LINE INDENT self . sim = order_sim NEW_LINE DEDENT else : NEW_LINE INDENT self . sim = cosine_sim NEW_LINE DEDENT self . max_violation = max_violation NEW_LINE DEDENT\",), ('def forward ( self , im , s ) : NEW_LINE INDENT scores = self . sim ( im , s ) NEW_LINE diagonal = scores . diag ( ) . view ( im . size ( 0 ) , 1 ) NEW_LINE d1 = diagonal . expand_as ( scores ) NEW_LINE d2 = diagonal . t ( ) . expand_as ( scores ) NEW_LINE cost_s = ( self . margin + scores - d1 ) . clamp ( min = 0 ) NEW_LINE cost_im = ( self . margin + scores - d2 ) . clamp ( min = 0 ) NEW_LINE mask = torch . eye ( scores . size ( 0 ) ) > .5 NEW_LINE I = Variable ( mask ) NEW_LINE if torch . cuda . is_available ( ) : NEW_LINE INDENT I = I . cuda ( ) NEW_LINE DEDENT cost_s = cost_s . masked_fill_ ( I , 0 ) NEW_LINE cost_im = cost_im . masked_fill_ ( I , 0 ) NEW_LINE if self . max_violation : NEW_LINE INDENT cost_s = cost_s . max ( 1 ) [ 0 ] NEW_LINE cost_im = cost_im . max ( 0 ) [ 0 ] NEW_LINE DEDENT return cost_s . sum ( ) + cost_im . sum ( ) NEW_LINE DEDENT',), ('def __init__ ( self , opt ) : NEW_LINE INDENT self . grad_clip = opt . grad_clip NEW_LINE self . img_enc = EncoderImage ( opt . data_name , opt . img_dim , opt . embed_size , opt . finetune , opt . cnn_type , use_abs = opt . use_abs , no_imgnorm = opt . no_imgnorm ) NEW_LINE self . txt_enc = EncoderText ( opt . vocab_size , opt . word_dim , opt . embed_size , opt . num_layers , use_abs = opt . use_abs ) NEW_LINE if torch . cuda . is_available ( ) : NEW_LINE INDENT self . img_enc . cuda ( ) NEW_LINE self . txt_enc . cuda ( ) NEW_LINE cudnn . benchmark = True NEW_LINE DEDENT self . criterion = ContrastiveLoss ( margin = opt . margin , measure = opt . measure , max_violation = opt . max_violation ) NEW_LINE params = list ( self . txt_enc . parameters ( ) ) NEW_LINE params += list ( self . img_enc . fc . parameters ( ) ) NEW_LINE if opt . finetune : NEW_LINE INDENT params += list ( self . img_enc . cnn . parameters ( ) ) NEW_LINE DEDENT self . params = params NEW_LINE self . optimizer = torch . optim . Adam ( params , lr = opt . learning_rate ) NEW_LINE self . Eiters = 0 NEW_LINE DEDENT',), ('def state_dict ( self ) : NEW_LINE INDENT state_dict = [ self . img_enc . state_dict ( ) , self . txt_enc . state_dict ( ) ] NEW_LINE return state_dict NEW_LINE DEDENT',), ('def load_state_dict ( self , state_dict ) : NEW_LINE INDENT self . img_enc . load_state_dict ( state_dict [ 0 ] ) NEW_LINE self . txt_enc . load_state_dict ( state_dict [ 1 ] ) NEW_LINE DEDENT',), ('def train_start ( self ) : NEW_LINE INDENT self . img_enc . train ( ) NEW_LINE self . txt_enc . train ( ) NEW_LINE DEDENT',), ('def val_start ( self ) : NEW_LINE INDENT self . img_enc . eval ( ) NEW_LINE self . txt_enc . eval ( ) NEW_LINE DEDENT',), ('def forward_emb ( self , images , captions , lengths , volatile = False ) : NEW_LINE INDENT images = Variable ( images , volatile = volatile ) NEW_LINE captions = Variable ( captions , volatile = volatile ) NEW_LINE if torch . cuda . is_available ( ) : NEW_LINE INDENT images = images . cuda ( ) NEW_LINE captions = captions . cuda ( ) NEW_LINE DEDENT img_emb = self . img_enc ( images ) NEW_LINE cap_emb = self . txt_enc ( captions , lengths ) NEW_LINE return img_emb , cap_emb NEW_LINE DEDENT',), (\"def forward_loss ( self , img_emb , cap_emb , ** kwargs ) : NEW_LINE INDENT loss = self . criterion ( img_emb , cap_emb ) NEW_LINE self . logger . update ( ' Le ' , loss . data [ 0 ] , img_emb . size ( 0 ) ) NEW_LINE return loss NEW_LINE DEDENT\",), (\"def train_emb ( self , images , captions , lengths , ids = None , * args ) : NEW_LINE INDENT self . Eiters += 1 NEW_LINE self . logger . update ( ' Eit ' , self . Eiters ) NEW_LINE self . logger . update ( ' lr ' , self . optimizer . param_groups [ 0 ] [ ' lr ' ] ) NEW_LINE img_emb , cap_emb = self . forward_emb ( images , captions , lengths ) NEW_LINE self . optimizer . zero_grad ( ) NEW_LINE loss = self . forward_loss ( img_emb , cap_emb ) NEW_LINE loss . backward ( ) NEW_LINE if self . grad_clip > 0 : NEW_LINE INDENT clip_grad_norm ( self . params , self . grad_clip ) NEW_LINE DEDENT self . optimizer . step ( ) NEW_LINE DEDENT\",), ('def evalrank ( model_path , data_path = None , split = \\' dev \\' , fold5 = False ) : NEW_LINE INDENT checkpoint = torch . load ( model_path ) NEW_LINE opt = checkpoint [ \\' opt \\' ] NEW_LINE if data_path is not None : NEW_LINE INDENT opt . data_path = data_path NEW_LINE DEDENT with open ( os . path . join ( opt . vocab_path , \\' % s _ vocab . pkl \\' % opt . data_name ) , \\' rb \\' ) as f : NEW_LINE INDENT vocab = pickle . load ( f ) NEW_LINE DEDENT opt . vocab_size = len ( vocab ) NEW_LINE model = VSE ( opt ) NEW_LINE model . load_state_dict ( checkpoint [ \\' model \\' ] ) NEW_LINE print ( \\' Loading ▁ dataset \\' ) NEW_LINE data_loader = get_test_loader ( split , opt . data_name , vocab , opt . crop_size , opt . batch_size , opt . workers , opt ) NEW_LINE print ( \\' Computing ▁ results . . . \\' ) NEW_LINE img_embs , cap_embs = encode_data ( model , data_loader ) NEW_LINE print ( \\' Images : ▁ % d , ▁ Captions : ▁ % d \\' % ( img_embs . shape [ 0 ] / 5 , cap_embs . shape [ 0 ] ) ) NEW_LINE if not fold5 : NEW_LINE INDENT r , rt = i2t ( img_embs , cap_embs , measure = opt . measure , return_ranks = True ) NEW_LINE ri , rti = t2i ( img_embs , cap_embs , measure = opt . measure , return_ranks = True ) NEW_LINE ar = ( r [ 0 ] + r [ 1 ] + r [ 2 ] ) / 3 NEW_LINE ari = ( ri [ 0 ] + ri [ 1 ] + ri [ 2 ] ) / 3 NEW_LINE rsum = r [ 0 ] + r [ 1 ] + r [ 2 ] + ri [ 0 ] + ri [ 1 ] + ri [ 2 ] NEW_LINE print ( \" rsum : ▁ % .1f \" % rsum ) NEW_LINE print ( \" Average ▁ i2t ▁ Recall : ▁ % .1f \" % ar ) NEW_LINE print ( \" Image ▁ to ▁ text : ▁ % .1f ▁ % .1f ▁ % .1f ▁ % .1f ▁ % .1f \" % r ) NEW_LINE print ( \" Average ▁ t2i ▁ Recall : ▁ % .1f \" % ari ) NEW_LINE print ( \" Text ▁ to ▁ image : ▁ % .1f ▁ % .1f ▁ % .1f ▁ % .1f ▁ % .1f \" % ri ) NEW_LINE DEDENT else : NEW_LINE INDENT results = [ ] NEW_LINE for i in range ( 5 ) : NEW_LINE INDENT r , rt0 = i2t ( img_embs [ i * 5000 : ( i + 1 ) * 5000 ] , cap_embs [ i * 5000 : ( i + 1 ) * 5000 ] , measure = opt . measure , return_ranks = True ) NEW_LINE print ( \" Image ▁ to ▁ text : ▁ % .1f , ▁ % .1f , ▁ % .1f , ▁ % .1f , ▁ % .1f \" % r ) NEW_LINE ri , rti0 = t2i ( img_embs [ i * 5000 : ( i + 1 ) * 5000 ] , cap_embs [ i * 5000 : ( i + 1 ) * 5000 ] , measure = opt . measure , return_ranks = True ) NEW_LINE if i == 0 : NEW_LINE INDENT rt , rti = rt0 , rti0 NEW_LINE DEDENT print ( \" Text ▁ to ▁ image : ▁ % .1f , ▁ % .1f , ▁ % .1f , ▁ % .1f , ▁ % .1f \" % ri ) NEW_LINE ar = ( r [ 0 ] + r [ 1 ] + r [ 2 ] ) / 3 NEW_LINE ari = ( ri [ 0 ] + ri [ 1 ] + ri [ 2 ] ) / 3 NEW_LINE rsum = r [ 0 ] + r [ 1 ] + r [ 2 ] + ri [ 0 ] + ri [ 1 ] + ri [ 2 ] NEW_LINE print ( \" rsum : ▁ % .1f ▁ ar : ▁ % .1f ▁ ari : ▁ % .1f \" % ( rsum , ar , ari ) ) NEW_LINE results += [ list ( r ) + list ( ri ) + [ rsum , ar , ari ] ] NEW_LINE DEDENT print ( \" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \" ) NEW_LINE print ( \" Mean ▁ metrics : ▁ \" ) NEW_LINE mean_metrics = tuple ( np . array ( results ) . mean ( axis = 0 ) . flatten ( ) ) NEW_LINE print ( \" rsum : ▁ % .1f \" % ( mean_metrics [ 10 ] * 6 ) ) NEW_LINE print ( \" Average ▁ i2t ▁ Recall : ▁ % .1f \" % mean_metrics [ 11 ] ) NEW_LINE print ( \" Image ▁ to ▁ text : ▁ % .1f ▁ % .1f ▁ % .1f ▁ % .1f ▁ % .1f \" % mean_metrics [ : 5 ] ) NEW_LINE print ( \" Average ▁ t2i ▁ Recall : ▁ % .1f \" % mean_metrics [ 12 ] ) NEW_LINE print ( \" Text ▁ to ▁ image : ▁ % .1f ▁ % .1f ▁ % .1f ▁ % .1f ▁ % .1f \" % mean_metrics [ 5 : 10 ] ) NEW_LINE DEDENT torch . save ( { \\' rt \\' : rt , \\' rti \\' : rti } , \\' ranks . pth . tar \\' ) NEW_LINE DEDENT',), (\"def i2t ( images , captions , npts = None , measure = ' cosine ' , return_ranks = False ) : NEW_LINE INDENT if npts is None : NEW_LINE INDENT npts = images . shape [ 0 ] / 5 NEW_LINE DEDENT index_list = [ ] NEW_LINE ranks = numpy . zeros ( npts ) NEW_LINE top1 = numpy . zeros ( npts ) NEW_LINE for index in range ( npts ) : NEW_LINE INDENT im = images [ 5 * index ] . reshape ( 1 , images . shape [ 1 ] ) NEW_LINE if measure == ' order ' : NEW_LINE INDENT bs = 100 NEW_LINE if index % bs == 0 : NEW_LINE INDENT mx = min ( images . shape [ 0 ] , 5 * ( index + bs ) ) NEW_LINE im2 = images [ 5 * index : mx : 5 ] NEW_LINE d2 = order_sim ( torch . Tensor ( im2 ) . cuda ( ) , torch . Tensor ( captions ) . cuda ( ) ) NEW_LINE d2 = d2 . cpu ( ) . numpy ( ) NEW_LINE DEDENT d = d2 [ index % bs ] NEW_LINE DEDENT else : NEW_LINE INDENT d = numpy . dot ( im , captions . T ) . flatten ( ) NEW_LINE DEDENT inds = numpy . argsort ( d ) [ : : - 1 ] NEW_LINE index_list . append ( inds [ 0 ] ) NEW_LINE rank = 1e20 NEW_LINE for i in range ( 5 * index , 5 * index + 5 , 1 ) : NEW_LINE INDENT tmp = numpy . where ( inds == i ) [ 0 ] [ 0 ] NEW_LINE if tmp < rank : NEW_LINE INDENT rank = tmp NEW_LINE DEDENT DEDENT ranks [ index ] = rank NEW_LINE top1 [ index ] = inds [ 0 ] NEW_LINE DEDENT r1 = 100.0 * len ( numpy . where ( ranks < 1 ) [ 0 ] ) / len ( ranks ) NEW_LINE r5 = 100.0 * len ( numpy . where ( ranks < 5 ) [ 0 ] ) / len ( ranks ) NEW_LINE r10 = 100.0 * len ( numpy . where ( ranks < 10 ) [ 0 ] ) / len ( ranks ) NEW_LINE medr = numpy . floor ( numpy . median ( ranks ) ) + 1 NEW_LINE meanr = ranks . mean ( ) + 1 NEW_LINE if return_ranks : NEW_LINE INDENT return ( r1 , r5 , r10 , medr , meanr ) , ( ranks , top1 ) NEW_LINE DEDENT else : NEW_LINE INDENT return ( r1 , r5 , r10 , medr , meanr ) NEW_LINE DEDENT DEDENT\",), (\"def t2i ( images , captions , npts = None , measure = ' cosine ' , return_ranks = False ) : NEW_LINE INDENT if npts is None : NEW_LINE INDENT npts = images . shape [ 0 ] / 5 NEW_LINE DEDENT ims = numpy . array ( [ images [ i ] for i in range ( 0 , len ( images ) , 5 ) ] ) NEW_LINE ranks = numpy . zeros ( 5 * npts ) NEW_LINE top1 = numpy . zeros ( 5 * npts ) NEW_LINE for index in range ( npts ) : NEW_LINE INDENT queries = captions [ 5 * index : 5 * index + 5 ] NEW_LINE if measure == ' order ' : NEW_LINE INDENT bs = 100 NEW_LINE if 5 * index % bs == 0 : NEW_LINE INDENT mx = min ( captions . shape [ 0 ] , 5 * index + bs ) NEW_LINE q2 = captions [ 5 * index : mx ] NEW_LINE d2 = order_sim ( torch . Tensor ( ims ) . cuda ( ) , torch . Tensor ( q2 ) . cuda ( ) ) NEW_LINE d2 = d2 . cpu ( ) . numpy ( ) NEW_LINE DEDENT d = d2 [ : , ( 5 * index ) % bs : ( 5 * index ) % bs + 5 ] . T NEW_LINE DEDENT else : NEW_LINE INDENT d = numpy . dot ( queries , ims . T ) NEW_LINE DEDENT inds = numpy . zeros ( d . shape ) NEW_LINE for i in range ( len ( inds ) ) : NEW_LINE INDENT inds [ i ] = numpy . argsort ( d [ i ] ) [ : : - 1 ] NEW_LINE ranks [ 5 * index + i ] = numpy . where ( inds [ i ] == index ) [ 0 ] [ 0 ] NEW_LINE top1 [ 5 * index + i ] = inds [ i ] [ 0 ] NEW_LINE DEDENT DEDENT r1 = 100.0 * len ( numpy . where ( ranks < 1 ) [ 0 ] ) / len ( ranks ) NEW_LINE r5 = 100.0 * len ( numpy . where ( ranks < 5 ) [ 0 ] ) / len ( ranks ) NEW_LINE r10 = 100.0 * len ( numpy . where ( ranks < 10 ) [ 0 ] ) / len ( ranks ) NEW_LINE medr = numpy . floor ( numpy . median ( ranks ) ) + 1 NEW_LINE meanr = ranks . mean ( ) + 1 NEW_LINE if return_ranks : NEW_LINE INDENT return ( r1 , r5 , r10 , medr , meanr ) , ( ranks , top1 ) NEW_LINE DEDENT else : NEW_LINE INDENT return ( r1 , r5 , r10 , medr , meanr ) NEW_LINE DEDENT DEDENT\",), ('def __init__ ( self ) : NEW_LINE INDENT self . reset ( ) NEW_LINE DEDENT',), ('def reset ( self ) : NEW_LINE INDENT self . val = 0 NEW_LINE self . avg = 0 NEW_LINE self . sum = 0 NEW_LINE self . count = 0 NEW_LINE DEDENT',), ('def update ( self , val , n = 0 ) : NEW_LINE INDENT self . val = val NEW_LINE self . sum += val * n NEW_LINE self . count += n NEW_LINE self . avg = self . sum / ( .0001 + self . count ) NEW_LINE DEDENT',), (\"def __str__ ( self ) : NEW_LINE INDENT if self . count == 0 : NEW_LINE INDENT return str ( self . val ) NEW_LINE DEDENT return ' % .4f ▁ ( % . 4f ) ' % ( self . val , self . avg ) NEW_LINE DEDENT\",), ('def __init__ ( self ) : NEW_LINE INDENT self . meters = OrderedDict ( ) NEW_LINE DEDENT',), ('def update ( self , k , v , n = 0 ) : NEW_LINE INDENT if k not in self . meters : NEW_LINE INDENT self . meters [ k ] = AverageMeter ( ) NEW_LINE DEDENT self . meters [ k ] . update ( v , n ) NEW_LINE DEDENT',)]\n",
            "0.9662282131612301\n",
            "On batch 767\n",
            "tensor([0.0415, 0.0427, 0.0424, 0.0422, 0.0425, 0.0411, 0.0421, 0.0423, 0.0422,\n",
            "        0.0411, 0.0416, 0.0423, 0.0421, 0.0421, 0.0411, 0.0411, 0.0411, 0.0411,\n",
            "        0.0414, 0.0411, 0.0411, 0.0415, 0.0414, 0.0411], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "('\\n\\\\label{eq_mu_post_text_anch}\\n\\\\pmb{\\\\mu}_{post}^{MAP} (\\\\pmb{\\\\theta}_{0}) = (\\\\pmb{\\\\Sigma}^{-1}_{like} +\\\\pmb{\\\\Sigma}^{-1}_{prior})^{-1}(\\\\pmb{\\\\Sigma}^{-1}_{like} \\\\pmb{\\\\mu}_{like} + \\\\pmb{\\\\Sigma}^{-1}_{prior} \\\\pmb{\\\\theta}_{0} ),\\n',)\n",
            "[(\"def main ( ) : NEW_LINE INDENT this_dir = sys . path [ 0 ] NEW_LINE os . chdir ( this_dir ) NEW_LINE path_in = os . path . join ( this_dir , ' test _ data _ private ' ) NEW_LINE path_out = os . path . join ( this_dir , ' models ' , ' MODELS _ OUT ' ) NEW_LINE print ( ' Output ▁ path ▁ is ▁ = ▁ ' , path_out ) NEW_LINE read_mode = ' ' NEW_LINE data_encoded , y_classification , y_regression , scaler = import_epi_data ( path = path_in , process_from_nonhotencoded = True ) NEW_LINE model_wrapper ( data_encoded , y_classification , y_regression , scaler , path_out ) NEW_LINE DEDENT\",), ('def import_epi_data ( path = \\' / home / petteri / Dropbox / manuscriptDrafts / deepPLR / code / RLN _ tabularData / test _ data _ private \\' , test_dataset = \\' TestPredMod . csv \\' , process_from_nonhotencoded = True , file_in = \\' df _ ZCA _ cor . csv \\' , debug_mode = False , verbose = True ) : NEW_LINE INDENT import csv NEW_LINE if process_from_nonhotencoded : NEW_LINE INDENT with open ( os . path . join ( path , test_dataset ) , \" rt \" ) as f : NEW_LINE INDENT reader = csv . reader ( f ) NEW_LINE headers = np . asarray ( next ( reader ) ) NEW_LINE data = [ row for row in reader ] NEW_LINE data = np . asarray ( data ) NEW_LINE DEDENT shape_in = data . shape NEW_LINE no_rows = shape_in [ 0 ] NEW_LINE no_cols = shape_in [ 1 ] NEW_LINE binary_label_col = 2 NEW_LINE continuous_label_col = 3 NEW_LINE data_indices = np . repeat ( True , no_cols ) NEW_LINE data_indices [ binary_label_col ] = False NEW_LINE data_indices [ continuous_label_col ] = False NEW_LINE y_classification = data [ : , binary_label_col ] . astype ( np . float ) NEW_LINE y_regression = data [ : , continuous_label_col ] . astype ( np . float ) NEW_LINE headers_wo_labels = headers [ data_indices ] NEW_LINE data_wo_labels = data [ : , data_indices ] . astype ( np . float ) NEW_LINE categorical_label_names = [ \\' gender \\' , \\' any _ retino \\' , \\' smk _ cat \\' , \\' alc _ cat \\' , \\' edu _ cat \\' , \\' job _ cat2\\' , \\' dm5\\' , \\' hypertension \\' , \\' anti _ ht \\' , \\' anti _ chol \\' , \\' anti _ dm \\' ] NEW_LINE categ_indices_boolean = np . repeat ( False , data_wo_labels . shape [ 1 ] ) NEW_LINE for idx , categ_label in enumerate ( categorical_label_names ) : NEW_LINE INDENT match = np . where ( [ categ_label in i for i in headers_wo_labels ] ) [ 0 ] NEW_LINE categ_indices_boolean [ match ] = True NEW_LINE DEDENT linear_indices = np . where ( categ_indices_boolean ) [ 0 ] NEW_LINE if verbose : NEW_LINE INDENT print ( \\' ▁ We ▁ found ▁ \\' , len ( linear_indices ) , \\' ▁ categorical ▁ features \\' ) NEW_LINE print ( \\' ▁ ▁ ▁ and ▁ continuous ▁ \\' , no_cols - len ( linear_indices ) - 2 , \\' ▁ continuous ▁ indices \\' ) NEW_LINE DEDENT categ_data = data_wo_labels [ : , linear_indices ] NEW_LINE enc = OneHotEncoder ( sparse = False ) NEW_LINE categ_encoded = enc . fit_transform ( categ_data ) NEW_LINE if verbose : NEW_LINE INDENT print ( \\' Categorical ▁ data ▁ | ▁ from ▁ : ▁ \\' , categ_data . shape , \\' ▁ to ▁ \\' , categ_encoded . shape ) NEW_LINE DEDENT categ_names = [ ] NEW_LINE for idx , runner in enumerate ( categorical_label_names ) : NEW_LINE INDENT categ_data_on_header = categ_data [ : , idx ] NEW_LINE unique_labels = np . unique ( categ_data_on_header ) NEW_LINE for idx , label in enumerate ( unique_labels ) : NEW_LINE INDENT categ_names . append ( runner + \\' _ \\' + str ( label ) ) NEW_LINE DEDENT DEDENT contin_data = data_wo_labels [ : , np . logical_not ( categ_indices_boolean ) ] NEW_LINE contin_headers = headers_wo_labels [ np . logical_not ( categ_indices_boolean ) ] NEW_LINE if verbose : NEW_LINE INDENT print ( \\' Continuous ▁ data : ▁ \\' , contin_data . shape ) NEW_LINE DEDENT scaler = StandardScaler ( ) NEW_LINE contin_data_z = scaler . fit_transform ( contin_data ) NEW_LINE data_encoded = np . hstack ( ( contin_data_z , categ_encoded ) ) NEW_LINE data_encoded_unscaled = np . hstack ( ( contin_data , categ_encoded ) ) NEW_LINE header_main = np . hstack ( ( contin_headers , categ_names ) ) NEW_LINE header_main = np . hstack ( ( header_main , \\' class ▁ label \\' , \\' regression ▁ label \\' ) ) NEW_LINE continuous_subheaders = np . tile ( np . array ( \\' continuous \\' ) , contin_data . shape [ 1 ] ) NEW_LINE categorical_subheaders = np . tile ( np . array ( \\' categorical \\' ) , categ_encoded . shape [ 1 ] ) NEW_LINE subheaders = np . hstack ( ( continuous_subheaders , categorical_subheaders ) ) NEW_LINE subheaders = np . hstack ( ( subheaders , \\' classification \\' , \\' regression \\' ) ) NEW_LINE INPUT_DIM = data_encoded . shape [ 1 ] NEW_LINE if verbose : NEW_LINE INDENT print ( \\' Number ▁ of ▁ input ▁ features ▁ = ▁ \\' , INPUT_DIM ) NEW_LINE print ( \\' Train ▁ set ▁ size ▁ = ▁ \\' , data_encoded . shape , \\' , ▁ labels ▁ = ▁ \\' , y_regression . shape ) NEW_LINE print ( \\' \\\\n Saving ▁ all ▁ this ▁ to ▁ disk ▁ back \\' ) NEW_LINE print ( \\' ▁ ▁ ▁ ▁ main ▁ header ▁ length ▁ = ▁ \\' , len ( header_main ) ) NEW_LINE print ( \\' ▁ ▁ ▁ ▁ subheader ▁ length ▁ = ▁ \\' , len ( subheaders ) ) NEW_LINE DEDENT df_scaled = pd . DataFrame ( data = np . hstack ( ( data_encoded , y_classification [ : , np . newaxis ] , y_regression [ : , np . newaxis ] ) ) , columns = [ header_main , subheaders ] ) NEW_LINE df_unscaled = pd . DataFrame ( data = np . hstack ( ( data_encoded_unscaled , y_classification [ : , np . newaxis ] , y_regression [ : , np . newaxis ] ) ) , columns = [ header_main , subheaders ] ) NEW_LINE df_unscaled . to_csv ( os . path . join ( path , \\' df _ unscaled . csv \\' ) , index = False ) NEW_LINE df_scaled . to_csv ( os . path . join ( path , \\' df _ scaled . csv \\' ) , index = False ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \\' \\\\n Load ▁ data ▁ from : ▁ \\' , file_in , \\' \\\\n \\' ) NEW_LINE with open ( os . path . join ( path , file_in ) , \" rt \" ) as f : NEW_LINE INDENT reader = csv . reader ( f ) NEW_LINE headers = np . asarray ( next ( reader ) ) NEW_LINE data = [ row for row in reader ] NEW_LINE data = np . asarray ( data ) NEW_LINE print ( data . shape ) NEW_LINE number_of_cols = data . shape [ 1 ] NEW_LINE data_encoded = data [ : , 0 : number_of_cols - 2 ] . astype ( np . float ) NEW_LINE y_classification = data [ : , number_of_cols - 2 ] . astype ( np . int ) NEW_LINE y_regression = data [ : , number_of_cols - 1 ] . astype ( np . float ) NEW_LINE scaler = [ ] NEW_LINE print ( \\' Train ▁ set ▁ size ▁ = ▁ \\' , data_encoded . shape , \\' , ▁ labels ▁ = ▁ \\' , y_regression . shape ) NEW_LINE DEDENT DEDENT print ( \\' Import ▁ done ! \\' ) NEW_LINE return ( data_encoded , y_classification , y_regression , scaler ) NEW_LINE DEDENT',), ('def compute_regression_metrics ( y_true , y_pred , test_mean , test_std_err , best_aleatoric , best_epistemic , results_cv , path_out , fileout_string , nb_reps ) : NEW_LINE INDENT if y_pred . dtype . type is np . str_ : NEW_LINE INDENT y_pred = y_pred . astype ( np . float ) NEW_LINE DEDENT if y_true . dtype . type is np . str_ : NEW_LINE INDENT y_true = y_true . astype ( np . float ) NEW_LINE DEDENT r2 = r2_score ( y_true , y_pred ) NEW_LINE exp_var = explained_variance_score ( y_true , y_pred ) NEW_LINE mean_abs_err = mean_absolute_error ( y_true , y_pred ) NEW_LINE mse = mean_squared_error ( y_true , y_pred ) NEW_LINE med_abs_err = median_absolute_error ( y_true , y_pred ) NEW_LINE no_of_folds = nb_reps NEW_LINE folds = np . linspace ( 1 , no_of_folds , no_of_folds ) NEW_LINE fold_str = [ ] NEW_LINE for i , item in enumerate ( folds ) : NEW_LINE INDENT fold_str . append ( \\' fold ▁ \\' + str ( int ( item ) ) ) NEW_LINE DEDENT headers2 = [ \\' Test ▁ Mean \\' , \\' Test ▁ Stdev \\' , \\' Aleatoric ▁ Uncertainty \\' , \\' Epistemic ▁ Uncertainty \\' , \\' Explained ▁ variance \\' , \\' Mean ▁ absolute ▁ error \\' , \\' Mean ▁ Squared ▁ Error \\' , \\' Median ▁ Absolute ▁ error \\' , \\' R ^ 2\\' ] NEW_LINE headers_all = headers2 NEW_LINE headers_all = \" , \" . join ( headers_all ) NEW_LINE data_out = np . hstack ( ( test_mean , test_std_err , best_aleatoric , best_epistemic , exp_var , mean_abs_err , mse , med_abs_err , r2 ) ) NEW_LINE np . savetxt ( os . path . join ( path_out , fileout_string ) , np . column_stack ( data_out ) , delimiter = \" , \" , header = headers_all , fmt = \" % f \" , comments = \\' \\' ) NEW_LINE fileout_string = fileout_string . replace ( \\' metrics . \\' , \\' predictions . \\' ) NEW_LINE true_vs_pred = np . hstack ( ( np . row_stack ( y_true ) , np . row_stack ( y_pred ) ) ) NEW_LINE np . savetxt ( os . path . join ( path_out , fileout_string ) , true_vs_pred , delimiter = \" , \" , header = \\' True , Predicted \\' , fmt = \" % f \" , comments = \\' \\' ) NEW_LINE print ( \\' ▁ ▁ ▁ ▁ ▁ R ^ 2 ▁ = ▁ \\' , r2 ) NEW_LINE print ( \\' ▁ ▁ ▁ ▁ ▁ MSE ▁ = ▁ \\' , mse ) NEW_LINE DEDENT',), (\"def __init__ ( self , layer , weight_regularizer = 1e-6 , dropout_regularizer = 1e-5 , init_min = 0.1 , init_max = 0.1 , is_mc_dropout = True , ** kwargs ) : NEW_LINE INDENT assert ' kernel _ regularizer ' not in kwargs NEW_LINE super ( ConcreteDropout , self ) . __init__ ( layer , ** kwargs ) NEW_LINE self . weight_regularizer = weight_regularizer NEW_LINE self . dropout_regularizer = dropout_regularizer NEW_LINE self . is_mc_dropout = is_mc_dropout NEW_LINE self . supports_masking = True NEW_LINE self . p_logit = None NEW_LINE self . p = None NEW_LINE self . init_min = np . log ( init_min ) - np . log ( 1. - init_min ) NEW_LINE self . init_max = np . log ( init_max ) - np . log ( 1. - init_max ) NEW_LINE DEDENT\",), (\"def build ( self , input_shape = None ) : NEW_LINE INDENT self . input_spec = InputSpec ( shape = input_shape ) NEW_LINE if not self . layer . built : NEW_LINE INDENT self . layer . build ( input_shape ) NEW_LINE self . layer . built = True NEW_LINE DEDENT super ( ConcreteDropout , self ) . build ( ) NEW_LINE self . p_logit = self . layer . add_weight ( name = ' p _ logit ' , shape = ( 1 , ) , initializer = initializers . RandomUniform ( self . init_min , self . init_max ) , trainable = True ) NEW_LINE self . p = K . sigmoid ( self . p_logit [ 0 ] ) NEW_LINE input_dim = np . prod ( input_shape [ 1 : ] ) NEW_LINE weight = self . layer . kernel NEW_LINE kernel_regularizer = self . weight_regularizer * K . sum ( K . square ( weight ) ) / ( 1. - self . p ) NEW_LINE dropout_regularizer = self . p * K . log ( self . p ) NEW_LINE dropout_regularizer += ( 1. - self . p ) * K . log ( 1. - self . p ) NEW_LINE dropout_regularizer *= self . dropout_regularizer * input_dim NEW_LINE regularizer = K . sum ( kernel_regularizer + dropout_regularizer ) NEW_LINE self . layer . add_loss ( regularizer ) NEW_LINE DEDENT\",), ('def compute_output_shape ( self , input_shape ) : NEW_LINE INDENT return self . layer . compute_output_shape ( input_shape ) NEW_LINE DEDENT',), ('def concrete_dropout ( self , x ) : NEW_LINE INDENT eps = K . cast_to_floatx ( K . epsilon ( ) ) NEW_LINE temp = 0.1 NEW_LINE unif_noise = K . random_uniform ( shape = K . shape ( x ) ) NEW_LINE drop_prob = ( K . log ( self . p + eps ) - K . log ( 1. - self . p + eps ) + K . log ( unif_noise + eps ) - K . log ( 1. - unif_noise + eps ) ) NEW_LINE drop_prob = K . sigmoid ( drop_prob / temp ) NEW_LINE random_tensor = 1. - drop_prob NEW_LINE retain_prob = 1. - self . p NEW_LINE x *= random_tensor NEW_LINE x /= retain_prob NEW_LINE return x NEW_LINE DEDENT',), ('def call ( self , inputs , training = None ) : NEW_LINE INDENT if self . is_mc_dropout : NEW_LINE INDENT return self . layer . call ( self . concrete_dropout ( inputs ) ) NEW_LINE DEDENT else : NEW_LINE INDENT def relaxed_dropped_inputs ( ) : NEW_LINE INDENT return self . layer . call ( self . concrete_dropout ( inputs ) ) NEW_LINE DEDENT return K . in_train_phase ( relaxed_dropped_inputs , self . layer . call ( inputs ) , training = training ) NEW_LINE DEDENT DEDENT',), ('def model_wrapper ( data_encoded , y_classification , y_regression , scaler , path_out , verbose = True , model = \\' MC _ dropout \\' , do_grid_search = True ) : NEW_LINE INDENT if verbose : NEW_LINE INDENT print ( \\' \\\\n Input ▁ training ▁ data ▁ dimension : \\' , data_encoded . shape ) NEW_LINE print ( \\' ▁ ▁ ▁ continous ▁ regression ▁ target ▁ vector ▁ length ▁ = ▁ \\' , len ( y_classification ) ) NEW_LINE print ( \\' ▁ ▁ ▁ ▁ ▁ discrete ▁ classification ▁ label ▁ vector ▁ length ▁ = ▁ \\' , len ( y_regression ) ) NEW_LINE print ( \\' ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ scaler ▁ object ▁ saved ▁ when ▁ preprocessing ▁ data ▁ = ▁ \" \\' , scaler , \\' \" \\' ) NEW_LINE print ( \\' ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ i . e . ▁ if ▁ you ▁ want ▁ the ▁ data ▁ scaled ▁ back ▁ to ▁ input ▁ range , ▁ or ▁ use ▁ the ▁ model ▁ to ▁ do ▁ predictions ▁ on ▁ new ▁ data \\\\n \\' ) NEW_LINE DEDENT print ( model ) NEW_LINE if model == \\' MC _ dropout \\' : NEW_LINE INDENT print ( \\' Model ▁ = ▁ \\' , model ) NEW_LINE if do_grid_search : NEW_LINE INDENT print ( \\' Grid ▁ Search ▁ for ▁ optimal ▁ hyperparameters \\' ) NEW_LINE nb_feat = [ data_encoded . shape [ 1 ] ] NEW_LINE best_metric , best_network , best_lscale , best_featcount , best_y_preds = MCdpout_grid_search ( data_encoded , y_regression , path_out , nb_feat = nb_feat ) NEW_LINE print ( \\' test ▁ mean ▁ RMSE ▁ = ▁ \\' , best_metric , \\' , ▁ lscale ▁ = ▁ \\' , best_lscale , \\' , ▁ feat _ count ▁ = ▁ \\' , best_featcount ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \\' Skipping ▁ hyperparameters ▁ and ▁ using ▁ optimized ▁ values \\' ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT print ( \\' Used ▁ model ▁ = ▁ \" \\' , model , \\' \" ▁ not ▁ supported \\' ) NEW_LINE DEDENT DEDENT',), ('def logsumexp ( a ) : NEW_LINE INDENT a_max = a . max ( axis = 0 ) NEW_LINE return np . log ( np . sum ( np . exp ( a - a_max ) , axis = 0 ) ) + a_max NEW_LINE DEDENT',), ('def test ( Y_true , MC_samples , D = 1 ) : NEW_LINE INDENT k = MC_samples . shape [ 0 ] NEW_LINE N = Y_true . shape [ 0 ] NEW_LINE mean = MC_samples [ : , : , : D ] NEW_LINE logvar = MC_samples [ : , : , D : ] NEW_LINE test_ll = - 0.5 * np . exp ( - logvar ) * ( mean - Y_true [ None ] ) ** 2. - 0.5 * logvar - 0.5 * np . log ( 2 * np . pi ) NEW_LINE test_ll = np . sum ( np . sum ( test_ll , - 1 ) , - 1 ) NEW_LINE test_ll = logsumexp ( test_ll ) - np . log ( k ) NEW_LINE pppp = test_ll / N NEW_LINE rmse = np . mean ( ( np . mean ( mean , 0 ) - Y_true ) ** 2. ) ** 0.5 NEW_LINE return pppp , rmse NEW_LINE DEDENT',), (\"def dense_layer ( input , nb_feat , n , prev_input , wd , dd , activ = ' relu ' , skip_conn = True , use_attn = True , attn_config = ' oninput ' ) : NEW_LINE INDENT output = ConcreteDropout ( Dense ( nb_feat , activation = activ ) , weight_regularizer = wd , dropout_regularizer = dd ) ( input ) NEW_LINE output_final = output NEW_LINE output = keras . layers . normalization . BatchNormalization ( ) ( output ) NEW_LINE shortcut_y = keras . layers . normalization . BatchNormalization ( ) ( input ) NEW_LINE output_final = output NEW_LINE if skip_conn : NEW_LINE INDENT output_w_skip = keras . layers . add ( [ shortcut_y , output_final ] ) NEW_LINE output_w_skip = keras . layers . Activation ( ' relu ' ) ( output_w_skip ) NEW_LINE output_final = output_w_skip NEW_LINE DEDENT if use_attn : NEW_LINE INDENT if attn_config == ' oninput ' : NEW_LINE INDENT if n == 0 : NEW_LINE INDENT attention_probs = Dense ( nb_feat , activation = ' sigmoid ' , name = ' attention _ vec _ % s ' % ( n ) ) ( output_final ) NEW_LINE attention_mul = Multiply ( name = ' attention _ mul _ % s ' % ( n ) ) ( [ input , attention_probs ] ) NEW_LINE output_gated = Dense ( nb_feat , activation = ' sigmoid ' ) ( attention_mul ) NEW_LINE output_final = output_gated NEW_LINE DEDENT DEDENT elif attn_config == ' onall ' : NEW_LINE INDENT attention_probs = Dense ( nb_feat , activation = ' sigmoid ' , name = ' attention _ vec _ % s ' % ( n ) ) ( output_final ) NEW_LINE attention_mul = Multiply ( name = ' attention _ mul _ % s ' % ( n ) ) ( [ input , attention_probs ] ) NEW_LINE output_gated = Dense ( nb_feat , activation = ' sigmoid ' ) ( attention_mul ) NEW_LINE output_final = output_gated NEW_LINE DEDENT DEDENT return ( output_final ) NEW_LINE DEDENT\",), (\"def fit_model ( nb_epoch , X , Y , X_test , y_test , path_out , l = 1e-4 , nb_features = 58 , nb_dense_layers = 3 , skip_conn = True , use_attn = True , attn_config = ' oninput ' , D = 1 , batch_size = 32 , verbose = False ) : NEW_LINE INDENT N = X . shape [ 0 ] NEW_LINE wd = l ** 2. / N NEW_LINE dd = 2. / N NEW_LINE input_shape = X . shape [ 1 : ] NEW_LINE inp = Input ( input_shape ) NEW_LINE x = inp NEW_LINE if nb_features == ' series ' : NEW_LINE INDENT nb_list = [ 58 , 28 , 14 , 6 ] NEW_LINE DEDENT else : NEW_LINE INDENT nb_list = [ nb_features , nb_features , nb_features , nb_features ] NEW_LINE fixed_nb_feat = nb_features NEW_LINE DEDENT path_out = path_out + ' _ attncfg - ' + attn_config NEW_LINE if os . path . exists ( path_out ) == False : NEW_LINE INDENT os . makedirs ( path_out ) NEW_LINE DEDENT prev_input = x NEW_LINE for n in range ( nb_dense_layers ) : NEW_LINE INDENT x2 = dense_layer ( x , nb_features , n , prev_input , wd , dd , skip_conn = skip_conn , use_attn = use_attn , activ = ' relu ' , attn_config = attn_config ) NEW_LINE prev_input = x NEW_LINE x = x2 NEW_LINE DEDENT mean = ConcreteDropout ( Dense ( D ) , weight_regularizer = wd , dropout_regularizer = dd ) ( x ) NEW_LINE log_var = ConcreteDropout ( Dense ( D ) , weight_regularizer = wd , dropout_regularizer = dd ) ( x ) NEW_LINE out = keras . layers . Concatenate ( name = ' output ' ) ( [ mean , log_var ] ) NEW_LINE model = Model ( inp , out ) NEW_LINE model . summary ( ) NEW_LINE def heteroscedastic_loss ( true , pred ) : NEW_LINE INDENT mean = pred [ : , : D ] NEW_LINE log_var = pred [ : , D : ] NEW_LINE precision = keras . backend . exp ( - log_var ) NEW_LINE return keras . backend . sum ( precision * ( true - mean ) ** 2. + log_var , - 1 ) NEW_LINE DEDENT reduce_lr = keras . callbacks . ReduceLROnPlateau ( monitor = ' loss ' , factor = 0.2 , patience = 20 , min_lr = 0.00005 ) NEW_LINE file_path = os . path . join ( path_out , ' best _ model . hdf5' ) NEW_LINE model_checkpoint = keras . callbacks . ModelCheckpoint ( filepath = file_path , monitor = ' val _ loss ' , save_best_only = True ) NEW_LINE callbacks = [ TrackConcreteDropoutP ( model ) , reduce_lr , model_checkpoint ] NEW_LINE model . compile ( optimizer = keras . optimizers . Adam ( lr = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 ) , loss = heteroscedastic_loss ) NEW_LINE hist = model . fit ( X , Y , nb_epoch = nb_epoch , batch_size = batch_size , verbose = verbose , validation_data = ( X_test , y_test ) , callbacks = callbacks ) NEW_LINE loss = hist . history [ ' loss ' ] [ - 1 ] NEW_LINE hist_df = pd . DataFrame ( hist . history ) NEW_LINE hist_df . to_csv ( os . path . join ( path_out , ' history . csv ' ) , index = False ) NEW_LINE import matplotlib . pyplot as plt NEW_LINE plt . figure ( ) NEW_LINE plt . semilogy ( hist . history [ ' loss ' ] ) NEW_LINE plt . semilogy ( hist . history [ ' val _ ' + ' loss ' ] ) NEW_LINE plt . title ( ' MODEL ▁ TRAINING : ▁ ' + ' log ▁ loss ' ) NEW_LINE plt . ylabel ( ' heteroscedastic ▁ loss ' ) NEW_LINE plt . xlabel ( ' epoch ' ) NEW_LINE plt . legend ( [ ' train ' , ' val ' ] , loc = ' upper ▁ right ' ) NEW_LINE plt . savefig ( os . path . join ( path_out , ' loss . png ' ) ) NEW_LINE plt . close ( ) NEW_LINE return model , - 0.5 * loss , path_out NEW_LINE DEDENT\",), (\"def MCdpout_grid_search ( data_encoded , y_regression , path_out , lscale = [ 1 , 1e-1 , 1e-2 ] , nb_feat = [ 58 ] , no_layers = [ 2 ] , nb_epochs = 1000 , batch_size = 32 , nb_reps = 1 , K_test = 100 , D = 1 , verbose = True ) : NEW_LINE INDENT print ( ' \\\\n GRID ▁ SEARCH ▁ for ▁ best ▁ hyperparameters ' ) NEW_LINE print ( ' ▁ ▁ ▁ . . . ▁ feature ▁ count ▁ from ▁ = ▁ ' , nb_feat ) NEW_LINE print ( ' ▁ ▁ ▁ . . . ▁ length ▁ scale ▁ from ▁ = ▁ ' , lscale ) NEW_LINE data_feats_kept = feature_selector ( data = data_encoded , label = y_regression ) NEW_LINE X_train , y_train , X_test , y_test = split_data_to_sets ( data = data_feats_kept , label = y_regression , test_ratio = 0.33 ) NEW_LINE X_train , y_train , X_validation , y_validation = get_CV_folds ( X_train , y_train ) NEW_LINE best_network = None NEW_LINE best_ll = - float ( ' inf ' ) NEW_LINE best_tau = 0 NEW_LINE best_dropout = 0 NEW_LINE no_comb = len ( lscale ) * len ( nb_feat ) NEW_LINE print ( ' \\\\n Number ▁ of ▁ grid ▁ search ▁ combinations ▁ = ▁ ' , no_comb ) NEW_LINE print ( ' Number ▁ of ▁ epochs ▁ per ▁ iteration ▁ = ▁ ' , nb_epochs , ' \\\\n ' ) NEW_LINE cpu_epoch_time = 0.0554855 NEW_LINE gpu_epoch_time = 0.1605 NEW_LINE print ( ' ▁ ▁ ▁ . . . ▁ rough ▁ estimate ▁ for ▁ waiting ▁ with ▁ CPU ▁ ( i7700 ) ▁ = ▁ ' , ( no_comb * nb_epochs * nb_reps * cpu_epoch_time ) / 60 , ' ▁ minutes ' ) NEW_LINE print ( ' ▁ ▁ ▁ . . . ▁ rough ▁ estimate ▁ for ▁ waiting ▁ withy ▁ GPU ▁ ( 1080Ti ) ▁ = ▁ ' , ( no_comb * nb_epochs * nb_reps * gpu_epoch_time ) / 60 , ' ▁ minutes ▁ \\\\n ' ) NEW_LINE best_network = None NEW_LINE best_metric = float ( ' inf ' ) NEW_LINE best_lscale = 0 NEW_LINE best_dropouts = 0 NEW_LINE best_featcount = 0 NEW_LINE best_y_preds = 0 NEW_LINE path_out_base = path_out NEW_LINE for length in lscale : NEW_LINE INDENT for feat_count in nb_feat : NEW_LINE INDENT for no_lay in no_layers : NEW_LINE INDENT t0 = time . time ( ) NEW_LINE print ( ' - - - - - - - - - ' ) NEW_LINE print ( ' \\\\n Grid ▁ search ▁ step : ▁ length _ scale : ▁ ' + str ( length ) + ' ▁ Feat ▁ count : ▁ ' + str ( feat_count ) + ' ▁ No ▁ of ▁ layers : ▁ ' + str ( feat_count ) ) NEW_LINE print ( ' - - - - - - - - - ' ) NEW_LINE results = [ ] NEW_LINE rep_results = [ ] NEW_LINE for i in range ( nb_reps ) : NEW_LINE INDENT print ( ' ▁ ▁ Repeat ▁ = ▁ ' , i + 1 , ' / ' , nb_reps ) NEW_LINE path_out = os . path . join ( path_out_base , ( ' l - ' + str ( length ) + ' _ f - ' + str ( feat_count ) + ' _ n - ' + str ( no_lay ) + ' _ rep - ' + str ( i ) ) ) NEW_LINE model , ELBO , path_out = fit_model ( nb_epochs , X_train , y_train , X_test , y_test , path_out , l = length , nb_features = feat_count , nb_dense_layers = no_lay , verbose = verbose , batch_size = batch_size ) NEW_LINE MC_samples = np . array ( [ model . predict ( X_test ) for _ in range ( K_test ) ] ) NEW_LINE pppp , rmse = test ( y_test , MC_samples ) NEW_LINE means = MC_samples [ : , : , : D ] NEW_LINE epistemic_uncertainty = np . var ( means , 0 ) . mean ( 0 ) NEW_LINE logvar = np . mean ( MC_samples [ : , : , D : ] , 0 ) NEW_LINE aleatoric_uncertainty = np . exp ( logvar ) . mean ( 0 ) NEW_LINE ps = np . array ( [ keras . backend . eval ( layer . p ) for layer in model . layers if hasattr ( layer , ' p ' ) ] ) NEW_LINE rep_results += [ ( rmse , ps , aleatoric_uncertainty , epistemic_uncertainty ) ] NEW_LINE print ( ' \\\\trmse ▁ = ▁ ' , rmse ) NEW_LINE DEDENT test_mean = np . mean ( [ r [ 0 ] for r in rep_results ] ) NEW_LINE test_std_err = np . std ( [ r [ 0 ] for r in rep_results ] ) / np . sqrt ( nb_reps ) NEW_LINE ps = np . mean ( [ r [ 1 ] for r in rep_results ] , 0 ) NEW_LINE aleatoric_uncertainty = np . mean ( [ r [ 2 ] for r in rep_results ] ) NEW_LINE epistemic_uncertainty = np . mean ( [ r [ 3 ] for r in rep_results ] ) NEW_LINE print ( ' \\\\n \\\\tRMSE ▁ Test ▁ mean : ▁ ' , test_mean ) NEW_LINE print ( ' \\\\tRMSE ▁ Test ▁ stdev : ▁ ' , test_std_err ) NEW_LINE print ( ' \\\\tDropout ▁ probabilities ▁ ( per ▁ dense ▁ layer ) : ▁ ' , ps ) NEW_LINE print ( ' \\\\tAleatoric ▁ uncertainty ▁ ( exp ▁ from ▁ logvar ) : ▁ ' , aleatoric_uncertainty ** 0.5 ) NEW_LINE print ( ' \\\\tEpistemic ▁ uncertainty ▁ ( var ▁ of ▁ means ) : ' , epistemic_uncertainty ** 0.5 , ' \\\\n ' ) NEW_LINE sys . stdout . flush ( ) NEW_LINE results += [ rep_results ] NEW_LINE t1 = time . time ( ) NEW_LINE print ( ' . . . ▁ repeats ▁ took ▁ = ▁ ' , t1 - t0 , ' ▁ seconds \\\\n ' ) NEW_LINE metric = test_mean NEW_LINE if ( metric < best_metric ) : NEW_LINE INDENT best_metric = metric NEW_LINE best_network = model NEW_LINE best_lscale = length NEW_LINE best_featcount = feat_count NEW_LINE best_dropouts = ps NEW_LINE best_y_preds = np . mean ( means , axis = 0 ) NEW_LINE best_aleatoric = aleatoric_uncertainty ** 0.5 NEW_LINE best_epistemic = epistemic_uncertainty ** 0.5 NEW_LINE compute_regression_metrics ( y_test , best_y_preds , test_mean , test_std_err , best_aleatoric , best_epistemic , results , path_out , ' mcdropout _ best _ metrics . csv ' , nb_reps ) NEW_LINE print ( ' Best ▁ test ▁ mean ▁ RMSE : ▁ ' + str ( best_metric ) ) NEW_LINE print ( ' Best ▁ length ▁ changed ▁ to : ▁ ' + str ( best_lscale ) ) NEW_LINE print ( ' Best ▁ feat ▁ count ▁ changed ▁ to : ▁ ' + str ( best_featcount ) ) NEW_LINE print ( ' Best ▁ dropout ▁ rate ▁ changed ▁ to : ▁ ' + str ( best_dropouts ) ) NEW_LINE DEDENT keras . backend . clear_session ( ) NEW_LINE DEDENT DEDENT DEDENT return ( best_metric , best_network , best_lscale , best_featcount , best_y_preds ) NEW_LINE DEDENT\",), ('def __init__ ( self , model : Model ) : NEW_LINE INDENT self . model = model NEW_LINE DEDENT',), ('def on_train_begin ( self , logs = { } ) : NEW_LINE INDENT self . ps = [ ] NEW_LINE self . losses = [ ] NEW_LINE DEDENT',), ('def on_train_end ( self , logs = { } ) : NEW_LINE INDENT return NEW_LINE DEDENT',), ('def on_epoch_begin ( self , epoch , logs = { } ) : NEW_LINE INDENT return NEW_LINE DEDENT',), ('def on_epoch_end ( self , epoch , logs = { } ) : NEW_LINE INDENT from keras . backend import function NEW_LINE self . losses . append ( logs . get ( \\' loss \\' ) ) NEW_LINE ps_tensor = [ x . p for x in self . model . layers if \\' concrete _ dropout \\' in x . name ] NEW_LINE get_ps = function ( [ ] , ps_tensor ) NEW_LINE p = get_ps ( [ ] ) NEW_LINE self . ps . append ( p ) NEW_LINE print ( \" ▁ - ▁ concrete ▁ dropout ▁ probs ▁ ( p ) : ▁ \" , p ) NEW_LINE return NEW_LINE DEDENT',), ('def on_batch_begin ( self , batch , logs = { } ) : NEW_LINE INDENT return NEW_LINE DEDENT',), ('def on_batch_end ( self , batch , logs = { } ) : NEW_LINE INDENT return NEW_LINE DEDENT',), ('def split_data_to_sets ( data = [ ] , label = [ ] , test_ratio = 0.33 , random_state = 42 ) : NEW_LINE INDENT X_train , X_test , y_train , y_test = train_test_split ( data , label , test_size = test_ratio , random_state = random_state ) NEW_LINE return ( X_train , y_train , X_test , y_test ) NEW_LINE DEDENT',), ('def get_CV_folds ( X_train , y_train , k = 10 ) : NEW_LINE INDENT return ( X_train , y_train , [ ] , [ ] ) NEW_LINE DEDENT',), ('def feature_selector ( data = [ ] , label = [ ] ) : NEW_LINE INDENT return ( data ) NEW_LINE DEDENT',)]\n",
            "0.9791999906301498\n",
            "On batch 1023\n",
            "tensor([0.0258, 0.0254, 0.0253, 0.0253, 0.0253, 0.0257, 0.0258, 0.0259, 0.0260,\n",
            "        0.0261, 0.0254, 0.0253, 0.0256, 0.0255, 0.0256, 0.0256, 0.0253, 0.0256,\n",
            "        0.0257, 0.0256, 0.0255, 0.0256, 0.0256, 0.0260, 0.0259, 0.0256, 0.0259,\n",
            "        0.0260, 0.0260, 0.0254, 0.0256, 0.0257, 0.0260, 0.0257, 0.0253, 0.0256,\n",
            "        0.0260, 0.0253, 0.0256], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "('\\\\label{normalized_P}\\n\\\\sum_{\\\\ell=1}^{C+1} p_{\\\\ell,m,n} = 1,  \\\\, {\\\\rm for\\\\, all\\\\,} 1\\\\le m\\\\le M, 1\\\\le n\\\\le N\\n',)\n",
            "[('def log_prb_labels ( x , ls ) : NEW_LINE INDENT if ls == [ ] : NEW_LINE INDENT return torch . sum ( x [ - 1 ] ) NEW_LINE DEDENT P = torch . exp ( x [ ls + [ - 1 ] ] ) NEW_LINE log_p_ls_n = torch . sum ( torch . log ( torch . sum ( P , dim = 0 ) ) ) NEW_LINE sign = ( - 1 ) ** ( len ( ls ) % 2 ) NEW_LINE term_positive = 1 + sign * torch . exp ( torch . sum ( x [ - 1 ] ) - log_p_ls_n ) NEW_LINE for i in range ( 1 , len ( ls ) ) : NEW_LINE INDENT sign = - sign NEW_LINE for comb in combinations ( list ( range ( len ( ls ) ) ) , i ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( torch . sum ( P [ list ( comb ) + [ - 1 ] ] , dim = 0 ) ) ) NEW_LINE term_positive += sign * torch . exp ( log_p - log_p_ls_n ) NEW_LINE DEDENT DEDENT return log_p_ls_n + torch . log ( torch . abs ( term_positive ) + eps4log ) NEW_LINE DEDENT',), ('def log_prb_1l_per_smpl ( x , ls ) : NEW_LINE INDENT B = x . shape [ 0 ] NEW_LINE dims_sum = list ( range ( 1 , len ( x . shape ) - 1 ) ) NEW_LINE log_p_n = torch . sum ( x [ : , - 1 ] , dims_sum ) NEW_LINE log_p_l_n = torch . sum ( torch . log ( torch . exp ( x [ torch . arange ( B ) , ls ] ) + torch . exp ( x [ : , - 1 ] ) ) , dims_sum ) NEW_LINE term_positive = 1 - torch . exp ( log_p_n - log_p_l_n ) NEW_LINE return log_p_l_n + torch . log ( torch . abs ( term_positive ) + eps4log ) NEW_LINE DEDENT',), ('def remove_repetitive_labels ( labels ) : NEW_LINE INDENT y = [ ] NEW_LINE for label in labels : NEW_LINE INDENT if label not in y : NEW_LINE INDENT y . append ( label ) NEW_LINE DEDENT DEDENT return y NEW_LINE DEDENT',), ('def log_prb_0label ( x ) : NEW_LINE INDENT return torch . sum ( x [ - 1 ] ) NEW_LINE DEDENT',), ('def log_prb_1label ( x , l ) : NEW_LINE INDENT log_p_l_n = torch . sum ( torch . log ( torch . exp ( x [ l ] ) + torch . exp ( x [ - 1 ] ) ) ) NEW_LINE log_p = torch . sum ( x [ - 1 ] ) NEW_LINE term_positive = 1 - torch . exp ( log_p - log_p_l_n ) NEW_LINE return log_p_l_n + torch . log ( torch . abs ( term_positive ) + eps4log ) NEW_LINE DEDENT',), ('def log_prb_2label ( x , ls ) : NEW_LINE INDENT P = torch . exp ( x [ ls + [ - 1 ] ] ) NEW_LINE log_p_ls_n = torch . sum ( torch . log ( torch . sum ( P , dim = 0 ) ) ) NEW_LINE log_p = torch . sum ( x [ - 1 ] ) NEW_LINE term_positive = 1 + torch . exp ( log_p - log_p_ls_n ) NEW_LINE for i in range ( 2 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ - 1 ] ) ) NEW_LINE term_positive -= torch . exp ( log_p - log_p_ls_n ) NEW_LINE DEDENT return log_p_ls_n + torch . log ( torch . abs ( term_positive ) + eps4log ) NEW_LINE DEDENT',), ('def log_prb_3label ( x , ls ) : NEW_LINE INDENT P = torch . exp ( x [ ls + [ - 1 ] ] ) NEW_LINE log_p_ls_n = torch . sum ( torch . log ( torch . sum ( P , dim = 0 ) ) ) NEW_LINE log_p = torch . sum ( x [ - 1 ] ) NEW_LINE term_positive = 1 - torch . exp ( log_p - log_p_ls_n ) NEW_LINE for i in range ( 3 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ - 1 ] ) ) NEW_LINE term_positive += torch . exp ( log_p - log_p_ls_n ) NEW_LINE for j in range ( i + 1 , 3 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ j ] + P [ - 1 ] ) ) NEW_LINE term_positive -= torch . exp ( log_p - log_p_ls_n ) NEW_LINE DEDENT DEDENT return log_p_ls_n + torch . log ( torch . abs ( term_positive ) + eps4log ) NEW_LINE DEDENT',), ('def log_prb_4label ( x , ls ) : NEW_LINE INDENT P = torch . exp ( x [ ls + [ - 1 ] ] ) NEW_LINE log_p_ls_n = torch . sum ( torch . log ( torch . sum ( P , dim = 0 ) ) ) NEW_LINE log_p = torch . sum ( x [ - 1 ] ) NEW_LINE term_positive = 1 + torch . exp ( log_p - log_p_ls_n ) NEW_LINE for i in range ( 4 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ - 1 ] ) ) NEW_LINE term_positive -= torch . exp ( log_p - log_p_ls_n ) NEW_LINE for j in range ( i + 1 , 4 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ j ] + P [ - 1 ] ) ) NEW_LINE term_positive += torch . exp ( log_p - log_p_ls_n ) NEW_LINE for k in range ( j + 1 , 4 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ j ] + P [ k ] + P [ - 1 ] ) ) NEW_LINE term_positive -= torch . exp ( log_p - log_p_ls_n ) NEW_LINE DEDENT DEDENT DEDENT return log_p_ls_n + torch . log ( torch . abs ( term_positive ) + eps4log ) NEW_LINE DEDENT',), ('def log_prb_5label ( x , ls ) : NEW_LINE INDENT P = torch . exp ( x [ ls + [ - 1 ] ] ) NEW_LINE log_p_ls_n = torch . sum ( torch . log ( torch . sum ( P , dim = 0 ) ) ) NEW_LINE log_p = torch . sum ( x [ - 1 ] ) NEW_LINE term_positive = 1 - torch . exp ( log_p - log_p_ls_n ) NEW_LINE for i in range ( 5 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ - 1 ] ) ) NEW_LINE term_positive += torch . exp ( log_p - log_p_ls_n ) NEW_LINE for j in range ( i + 1 , 5 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ j ] + P [ - 1 ] ) ) NEW_LINE term_positive -= torch . exp ( log_p - log_p_ls_n ) NEW_LINE for k in range ( j + 1 , 5 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ j ] + P [ k ] + P [ - 1 ] ) ) NEW_LINE term_positive += torch . exp ( log_p - log_p_ls_n ) NEW_LINE for m in range ( k + 1 , 5 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ j ] + P [ k ] + P [ m ] + P [ - 1 ] ) ) NEW_LINE term_positive -= torch . exp ( log_p - log_p_ls_n ) NEW_LINE DEDENT DEDENT DEDENT DEDENT return log_p_ls_n + torch . log ( torch . abs ( term_positive ) + eps4log ) NEW_LINE DEDENT',), ('def log_prb_6label ( x , ls ) : NEW_LINE INDENT P = torch . exp ( x [ ls + [ - 1 ] ] ) NEW_LINE log_p_ls_n = torch . sum ( torch . log ( torch . sum ( P , dim = 0 ) ) ) NEW_LINE log_p = torch . sum ( x [ - 1 ] ) NEW_LINE term_positive = 1 + torch . exp ( log_p - log_p_ls_n ) NEW_LINE for i in range ( 6 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ - 1 ] ) ) NEW_LINE term_positive -= torch . exp ( log_p - log_p_ls_n ) NEW_LINE for j in range ( i + 1 , 6 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ j ] + P [ - 1 ] ) ) NEW_LINE term_positive += torch . exp ( log_p - log_p_ls_n ) NEW_LINE for k in range ( j + 1 , 6 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ j ] + P [ k ] + P [ - 1 ] ) ) NEW_LINE term_positive -= torch . exp ( log_p - log_p_ls_n ) NEW_LINE for m in range ( k + 1 , 6 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ j ] + P [ k ] + P [ m ] + P [ - 1 ] ) ) NEW_LINE term_positive += torch . exp ( log_p - log_p_ls_n ) NEW_LINE for n in range ( m + 1 , 6 ) : NEW_LINE INDENT log_p = torch . sum ( torch . log ( P [ i ] + P [ j ] + P [ k ] + P [ m ] + P [ n ] + P [ - 1 ] ) ) NEW_LINE term_positive -= torch . exp ( log_p - log_p_ls_n ) NEW_LINE DEDENT DEDENT DEDENT DEDENT DEDENT return log_p_ls_n + torch . log ( torch . abs ( term_positive ) + eps4log ) NEW_LINE DEDENT',), ('def nest_images ( images , nestH , nestW ) : NEW_LINE INDENT iH , iW = images . shape [ 2 ] , images . shape [ 3 ] NEW_LINE y = torch . zeros ( images . shape [ 1 ] , nestH , nestW ) NEW_LINE for image in images : NEW_LINE INDENT m = random . randint ( 0 , nestH - iH ) NEW_LINE n = random . randint ( 0 , nestW - iW ) NEW_LINE y [ : , m : m + iH , n : n + iW ] += image NEW_LINE DEDENT y [ y > 1 ] = 1 NEW_LINE return y NEW_LINE DEDENT',), (\"def read_svhn_mat ( file ) : NEW_LINE INDENT data = scipy . io . loadmat ( file ) NEW_LINE images = np . transpose ( data [ ' Images ' ] , [ 3 , 2 , 0 , 1 ] ) NEW_LINE labels = [ ] NEW_LINE for mat_label in data [ ' Labels ' ] : NEW_LINE INDENT label = [ ] NEW_LINE for l in mat_label [ 0 ] [ 0 ] : NEW_LINE INDENT label . append ( int ( l ) ) NEW_LINE DEDENT labels . append ( label ) NEW_LINE DEDENT return images , np . array ( labels ) NEW_LINE DEDENT\",), ('def model ( x ) : NEW_LINE INDENT x = F . leaky_relu ( F . conv2d ( x , W1 [ : - 1 ] . view ( 64 , 1 , 5 , 5 ) , bias = W1 [ - 1 ] , padding = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W2 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W2 [ - 1 ] , padding = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W3 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W3 [ - 1 ] , padding = 2 , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W4 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W4 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W5 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W5 [ - 1 ] ) , negative_slope = 0.1 ) NEW_LINE x = F . conv2d ( x , W6 [ : - 1 ] . view ( 11 , 64 , 5 , 5 ) , bias = W6 [ - 1 ] ) NEW_LINE return x NEW_LINE DEDENT',), ('def train_loss ( images , labels ) : NEW_LINE INDENT y = model ( images ) NEW_LINE y = y - torch . max ( y ) NEW_LINE y = torch . exp ( y ) NEW_LINE y = torch . log ( y / torch . sum ( y ) ) NEW_LINE loss = 0.0 NEW_LINE for i in range ( y . shape [ 0 ] ) : NEW_LINE INDENT for l in labels [ i ] : NEW_LINE INDENT loss -= torch . max ( y [ i , l ] ) NEW_LINE DEDENT DEDENT return loss / y . shape [ 0 ] NEW_LINE DEDENT',), ('def test_loss ( ) : NEW_LINE INDENT num_errs = 0 NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT for data , target in test_loader : NEW_LINE INDENT y = model ( data . to ( device ) ) NEW_LINE y , _ = torch . max ( y , dim = 3 ) NEW_LINE y , _ = torch . max ( y , dim = 2 ) NEW_LINE _ , pred = torch . max ( y , dim = 1 ) NEW_LINE num_errs += torch . sum ( pred != target . to ( device ) ) NEW_LINE DEDENT DEDENT return num_errs . item ( ) / len ( test_loader . dataset ) NEW_LINE DEDENT',), ('def model ( x ) : NEW_LINE INDENT x = F . leaky_relu ( F . conv2d ( x , W1 [ : - 1 ] . view ( 64 , 1 , 5 , 5 ) , bias = W1 [ - 1 ] , padding = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W2 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W2 [ - 1 ] , padding = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W3 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W3 [ - 1 ] , padding = 2 , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W4 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W4 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W5 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W5 [ - 1 ] ) , negative_slope = 0.1 ) NEW_LINE x = F . conv2d ( x , W6 [ : - 1 ] . view ( 11 , 64 , 5 , 5 ) , bias = W6 [ - 1 ] ) NEW_LINE return x NEW_LINE DEDENT',), ('def train_loss ( images , labels ) : NEW_LINE INDENT y = model ( images ) NEW_LINE y = F . log_softmax ( y , 1 ) NEW_LINE loss = - torch . sum ( U . log_prb_1l_per_smpl ( y , labels ) ) NEW_LINE return loss / y . shape [ 0 ] / y . shape [ 2 ] / y . shape [ 3 ] NEW_LINE DEDENT',), ('def test_loss_approx ( ) : NEW_LINE INDENT num_errs = 0 NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT for data , target in test_loader : NEW_LINE INDENT y = model ( data . to ( device ) ) NEW_LINE y = F . log_softmax ( y , 1 ) NEW_LINE y = y [ : , : - 1 ] NEW_LINE y = torch . exp ( y ) NEW_LINE y = torch . sum ( y , dim = [ 2 , 3 ] ) NEW_LINE _ , pred = torch . max ( y , dim = 1 ) NEW_LINE num_errs += torch . sum ( pred != target . to ( device ) ) NEW_LINE DEDENT DEDENT return num_errs . item ( ) / len ( test_loader . dataset ) NEW_LINE DEDENT',), ('def test_loss_exact ( ) : NEW_LINE INDENT num_errs = 0 NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT for data , target in test_loader : NEW_LINE INDENT y = model ( data . to ( device ) ) NEW_LINE y = F . log_softmax ( y , 1 ) NEW_LINE y = torch . exp ( y ) NEW_LINE y = y [ : , : - 1 ] + y [ : , - 1 : ] NEW_LINE y = torch . sum ( torch . log ( y ) , dim = [ 2 , 3 ] ) NEW_LINE _ , pred = torch . max ( y , dim = 1 ) NEW_LINE num_errs += torch . sum ( pred != target . to ( device ) ) NEW_LINE DEDENT DEDENT return num_errs . item ( ) / len ( test_loader . dataset ) NEW_LINE DEDENT',), ('def model ( x ) : NEW_LINE INDENT x = F . leaky_relu ( F . conv2d ( x , W1 [ : - 1 ] . view ( 64 , 1 , 5 , 5 ) , bias = W1 [ - 1 ] , padding = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W2 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W2 [ - 1 ] , padding = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W3 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W3 [ - 1 ] , padding = 2 , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W4 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W4 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W5 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W5 [ - 1 ] ) , negative_slope = 0.1 ) NEW_LINE x = F . conv2d ( x , W6 [ : - 1 ] . view ( 11 , 64 , 5 , 5 ) , bias = W6 [ - 1 ] ) NEW_LINE return x NEW_LINE DEDENT',), ('def train_loss ( images , labels ) : NEW_LINE INDENT y = model ( images ) NEW_LINE y = F . log_softmax ( y , 1 ) NEW_LINE loss = 0.0 NEW_LINE for i in range ( y . shape [ 0 ] ) : NEW_LINE INDENT loss -= U . log_prb_labels ( y [ i ] , labels [ i ] ) NEW_LINE DEDENT return loss / y . shape [ 0 ] / y . shape [ 2 ] / y . shape [ 3 ] NEW_LINE DEDENT',), ('def test_loss ( ) : NEW_LINE INDENT num_errs = 0 NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT for data , target in test_loader : NEW_LINE INDENT y = model ( data . to ( device ) ) NEW_LINE y = F . log_softmax ( y , 1 ) NEW_LINE y = y [ : , : - 1 ] NEW_LINE y = torch . exp ( y ) NEW_LINE y = torch . sum ( y , dim = [ 2 , 3 ] ) NEW_LINE _ , pred = torch . max ( y , dim = 1 ) NEW_LINE num_errs += torch . sum ( pred != target . to ( device ) ) NEW_LINE DEDENT DEDENT return num_errs . item ( ) / len ( test_loader . dataset ) NEW_LINE DEDENT',), ('def model ( x ) : NEW_LINE INDENT x = F . leaky_relu ( F . conv2d ( x , W1 [ : - 1 ] . view ( 64 , 1 , 5 , 5 ) , bias = W1 [ - 1 ] , padding = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W2 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W2 [ - 1 ] , padding = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W3 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W3 [ - 1 ] , padding = 2 , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W4 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W4 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W5 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W5 [ - 1 ] ) , negative_slope = 0.1 ) NEW_LINE x = F . conv2d ( x , W6 [ : - 1 ] . view ( 11 , 64 , 5 , 5 ) , bias = W6 [ - 1 ] ) NEW_LINE return x NEW_LINE DEDENT',), ('def get_batch ( ) : NEW_LINE INDENT choice = random . randint ( 0 , 2 ) NEW_LINE if choice == 0 : NEW_LINE INDENT i = random . randint ( 0 , len_train - batch_size ) NEW_LINE images = train_images [ i : i + batch_size ] NEW_LINE labels = train_labels [ i : i + batch_size ] NEW_LINE DEDENT elif choice == 1 : NEW_LINE INDENT i = random . randint ( 0 , len_extra1 - batch_size ) NEW_LINE images = extra1_images [ i : i + batch_size ] NEW_LINE labels = extra1_labels [ i : i + batch_size ] NEW_LINE DEDENT else : NEW_LINE INDENT i = random . randint ( 0 , len_extra2 - batch_size ) NEW_LINE images = extra2_images [ i : i + batch_size ] NEW_LINE labels = extra2_labels [ i : i + batch_size ] NEW_LINE DEDENT i , j = random . randint ( 0 , 4 ) , random . randint ( 0 , 4 ) NEW_LINE images = images [ : , : , i : i + 60 , j : j + 60 ] NEW_LINE images = images [ : , np . random . permutation ( 3 ) ] NEW_LINE old_height , old_width = images . shape [ 2 ] , images . shape [ 3 ] NEW_LINE new_size = random . randint ( 50 , 70 ) NEW_LINE images = images [ : , : , ( np . arange ( new_size ) * ( old_height - 1 ) / ( new_size - 1 ) ) . astype ( int ) ] NEW_LINE new_size = random . randint ( 50 , 70 ) NEW_LINE images = images [ : , : , : , ( np . arange ( new_size ) * ( old_width - 1 ) / ( new_size - 1 ) ) . astype ( int ) ] NEW_LINE return images , labels NEW_LINE DEDENT',), ('def model ( x ) : NEW_LINE INDENT x = F . leaky_relu ( F . conv2d ( x , W1 [ : - 1 ] . view ( dim1 , dim0 , ks , ks ) , bias = W1 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W2 [ : - 1 ] . view ( dim1 , dim1 , ks , ks ) , bias = W2 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W3 [ : - 1 ] . view ( dim1 , dim1 , ks , ks ) , bias = W3 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE if enable_dropout : NEW_LINE INDENT x = 2 * torch . bernoulli ( torch . rand ( x . shape , device = device ) ) * x NEW_LINE DEDENT x = F . leaky_relu ( F . conv2d ( x , W4 [ : - 1 ] . view ( dim2 , dim1 , ks , ks ) , bias = W4 [ - 1 ] , padding = ks // 2 , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W5 [ : - 1 ] . view ( dim2 , dim2 , ks , ks ) , bias = W5 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W6 [ : - 1 ] . view ( dim2 , dim2 , ks , ks ) , bias = W6 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE if enable_dropout : NEW_LINE INDENT x = 2 * torch . bernoulli ( torch . rand ( x . shape , device = device ) ) * x NEW_LINE DEDENT x = F . leaky_relu ( F . conv2d ( x , W7 [ : - 1 ] . view ( dim3 , dim2 , ks , ks ) , bias = W7 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W8 [ : - 1 ] . view ( dim3 , dim3 , ks , ks ) , bias = W8 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W9 [ : - 1 ] . view ( dim3 , dim3 , ks , ks ) , bias = W9 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE if enable_dropout : NEW_LINE INDENT x = 2 * torch . bernoulli ( torch . rand ( x . shape , device = device ) ) * x NEW_LINE DEDENT x = F . conv2d ( x , W10 [ : - 1 ] . view ( 10 + 1 , dim3 , ks , ks ) , bias = W10 [ - 1 ] ) NEW_LINE return x NEW_LINE DEDENT',), ('def train_loss ( images , labels ) : NEW_LINE INDENT y = model ( images ) NEW_LINE y = F . log_softmax ( y , 1 ) . double ( ) NEW_LINE loss = 0.0 NEW_LINE for i in range ( y . shape [ 0 ] ) : NEW_LINE INDENT loss -= U . log_prb_labels ( y [ i ] , labels [ i ] ) NEW_LINE DEDENT return loss / y . shape [ 0 ] / y . shape [ 2 ] / y . shape [ 3 ] NEW_LINE DEDENT',), ('def test_loss ( ) : NEW_LINE INDENT num_errors = 0 NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT for im_cnt , im in enumerate ( test_images ) : NEW_LINE INDENT image = torch . tensor ( im / 256 , dtype = torch . float , device = device ) NEW_LINE y = model ( image [ None ] ) [ 0 ] NEW_LINE transcribed_result = [ ] NEW_LINE last_detected_label = - 1 NEW_LINE for j in range ( y . shape [ 2 ] ) : NEW_LINE INDENT hist = np . zeros ( 10 ) NEW_LINE for i in range ( y . shape [ 1 ] ) : NEW_LINE INDENT _ , label = torch . max ( y [ : , i , j ] , dim = 0 ) NEW_LINE if label < 10 : NEW_LINE INDENT hist [ label . item ( ) ] += 1 NEW_LINE DEDENT DEDENT if max ( hist ) > 0 : NEW_LINE INDENT label = np . argmax ( hist ) NEW_LINE if label != last_detected_label : NEW_LINE INDENT transcribed_result . append ( label ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT label = - 1 NEW_LINE DEDENT last_detected_label = label NEW_LINE DEDENT if transcribed_result != test_labels [ im_cnt ] : NEW_LINE INDENT num_errors += 1 NEW_LINE DEDENT DEDENT DEDENT return num_errors / len ( test_images ) NEW_LINE DEDENT',), ('def model ( x ) : NEW_LINE INDENT x = F . leaky_relu ( F . conv2d ( x , W1 [ : - 1 ] . view ( dim1 , dim0 , ks , ks ) , bias = W1 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W2 [ : - 1 ] . view ( dim1 , dim1 , ks , ks ) , bias = W2 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W3 [ : - 1 ] . view ( dim1 , dim1 , ks , ks ) , bias = W3 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W4 [ : - 1 ] . view ( dim2 , dim1 , ks , ks ) , bias = W4 [ - 1 ] , padding = ks // 2 , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W5 [ : - 1 ] . view ( dim2 , dim2 , ks , ks ) , bias = W5 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W6 [ : - 1 ] . view ( dim2 , dim2 , ks , ks ) , bias = W6 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W7 [ : - 1 ] . view ( dim3 , dim2 , ks , ks ) , bias = W7 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W8 [ : - 1 ] . view ( dim3 , dim3 , ks , ks ) , bias = W8 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W9 [ : - 1 ] . view ( dim3 , dim3 , ks , ks ) , bias = W9 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . conv2d ( x , W10 [ : - 1 ] . view ( 10 + 1 , dim3 , ks , ks ) , bias = W10 [ - 1 ] ) NEW_LINE return x NEW_LINE DEDENT',), ('def model ( x ) : NEW_LINE INDENT x = F . leaky_relu ( F . conv2d ( x , W1 [ : - 1 ] . view ( dim1 , 3 , 3 , 3 ) , bias = W1 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W2 [ : - 1 ] . view ( dim1 , dim1 , 3 , 3 ) , bias = W2 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W3 [ : - 1 ] . view ( dim1 , dim1 , 3 , 3 ) , bias = W3 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W4 [ : - 1 ] . view ( dim2 , dim1 , 3 , 3 ) , bias = W4 [ - 1 ] , padding = 1 , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W5 [ : - 1 ] . view ( dim2 , dim2 , 3 , 3 ) , bias = W5 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W6 [ : - 1 ] . view ( dim2 , dim2 , 3 , 3 ) , bias = W6 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W7 [ : - 1 ] . view ( dim3 , dim2 , 3 , 3 ) , bias = W7 [ - 1 ] , padding = 1 , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W8 [ : - 1 ] . view ( dim3 , dim3 , 3 , 3 ) , bias = W8 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W9 [ : - 1 ] . view ( dim3 , dim3 , 3 , 3 ) , bias = W9 [ - 1 ] ) , negative_slope = 0.1 ) NEW_LINE x = F . conv2d ( x , W10 [ : - 1 ] . view ( 11 , dim3 , 3 , 3 ) , bias = W10 [ - 1 ] ) NEW_LINE return x NEW_LINE DEDENT',), ('def train_loss ( images , labels ) : NEW_LINE INDENT y = model ( images ) NEW_LINE y = F . log_softmax ( y , 1 ) . double ( ) NEW_LINE loss = - torch . sum ( U . log_prb_1l_per_smpl ( y , labels ) ) NEW_LINE return loss / y . shape [ 0 ] / y . shape [ 2 ] / y . shape [ 3 ] NEW_LINE DEDENT',), ('def test_loss_approx ( ) : NEW_LINE INDENT num_errs = 0 NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT for data , target in test_loader : NEW_LINE INDENT y = model ( data . to ( device ) ) NEW_LINE y = F . log_softmax ( y , 1 ) NEW_LINE y = y [ : , : - 1 ] NEW_LINE y = torch . exp ( y ) NEW_LINE y = torch . sum ( y , dim = [ 2 , 3 ] ) NEW_LINE _ , pred = torch . max ( y , dim = 1 ) NEW_LINE num_errs += torch . sum ( pred != target . to ( device ) ) NEW_LINE DEDENT DEDENT return num_errs . item ( ) / len ( test_loader . dataset ) NEW_LINE DEDENT',), ('def test_loss_exact ( ) : NEW_LINE INDENT num_errs = 0 NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT for data , target in test_loader : NEW_LINE INDENT y = model ( data . to ( device ) ) NEW_LINE y = F . log_softmax ( y , 1 ) NEW_LINE y = torch . exp ( y ) NEW_LINE y = y [ : , : - 1 ] + y [ : , - 1 : ] NEW_LINE y = torch . sum ( torch . log ( y ) , dim = [ 2 , 3 ] ) NEW_LINE _ , pred = torch . max ( y , dim = 1 ) NEW_LINE num_errs += torch . sum ( pred != target . to ( device ) ) NEW_LINE DEDENT DEDENT return num_errs . item ( ) / len ( test_loader . dataset ) NEW_LINE DEDENT',), ('def model ( x ) : NEW_LINE INDENT x = F . leaky_relu ( F . conv2d ( x , W1 [ : - 1 ] . view ( dim1 , dim0 , ks , ks ) , bias = W1 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W2 [ : - 1 ] . view ( dim1 , dim1 , ks , ks ) , bias = W2 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W3 [ : - 1 ] . view ( dim1 , dim1 , ks , ks ) , bias = W3 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W4 [ : - 1 ] . view ( dim2 , dim1 , ks , ks ) , bias = W4 [ - 1 ] , padding = ks // 2 , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W5 [ : - 1 ] . view ( dim2 , dim2 , ks , ks ) , bias = W5 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W6 [ : - 1 ] . view ( dim2 , dim2 , ks , ks ) , bias = W6 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W7 [ : - 1 ] . view ( dim3 , dim2 , ks , ks ) , bias = W7 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W8 [ : - 1 ] . view ( dim3 , dim3 , ks , ks ) , bias = W8 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W9 [ : - 1 ] . view ( dim3 , dim3 , ks , ks ) , bias = W9 [ - 1 ] , padding = ks // 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . conv2d ( x , W10 [ : - 1 ] . view ( 10 + 1 , dim3 , ks , ks ) , bias = W10 [ - 1 ] ) NEW_LINE return x NEW_LINE DEDENT',), ('def model ( x ) : NEW_LINE INDENT x = F . leaky_relu ( F . conv2d ( x , W1 [ : - 1 ] . view ( 64 , 1 , 5 , 5 ) , bias = W1 [ - 1 ] , padding = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W2 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W2 [ - 1 ] , padding = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W3 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W3 [ - 1 ] , padding = 1 , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W4 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W4 [ - 1 ] ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W5 [ : - 1 ] . view ( 64 , 64 , 5 , 5 ) , bias = W5 [ - 1 ] ) , negative_slope = 0.1 ) NEW_LINE x = x . view ( - 1 , 64 * 5 * 5 ) . mm ( W6 [ : - 1 ] ) + W6 [ - 1 ] NEW_LINE return x NEW_LINE DEDENT',), ('def train_loss ( data , target ) : NEW_LINE INDENT y = model ( data ) NEW_LINE y = F . log_softmax ( y , dim = 1 ) NEW_LINE loss = F . nll_loss ( y , target ) NEW_LINE return loss NEW_LINE DEDENT',), ('def test_loss ( ) : NEW_LINE INDENT num_errs = 0 NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT for data , target in test_loader : NEW_LINE INDENT y = model ( data . to ( device ) ) NEW_LINE _ , pred = torch . max ( y , dim = 1 ) NEW_LINE num_errs += torch . sum ( pred != target . to ( device ) ) NEW_LINE DEDENT DEDENT return num_errs . item ( ) / len ( test_loader . dataset ) NEW_LINE DEDENT',), ('def model ( x ) : NEW_LINE INDENT x = F . leaky_relu ( F . conv2d ( x , W1 [ : - 1 ] . view ( 128 , 3 , 3 , 3 ) , bias = W1 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W2 [ : - 1 ] . view ( 128 , 128 , 3 , 3 ) , bias = W2 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W3 [ : - 1 ] . view ( 128 , 128 , 3 , 3 ) , bias = W3 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W4 [ : - 1 ] . view ( 192 , 128 , 3 , 3 ) , bias = W4 [ - 1 ] , padding = 1 , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W5 [ : - 1 ] . view ( 192 , 192 , 3 , 3 ) , bias = W5 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W6 [ : - 1 ] . view ( 192 , 192 , 3 , 3 ) , bias = W6 [ - 1 ] , padding = 1 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W7 [ : - 1 ] . view ( 256 , 192 , 3 , 3 ) , bias = W7 [ - 1 ] , stride = 2 ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W8 [ : - 1 ] . view ( 256 , 256 , 3 , 3 ) , bias = W8 [ - 1 ] ) , negative_slope = 0.1 ) NEW_LINE x = F . leaky_relu ( F . conv2d ( x , W9 [ : - 1 ] . view ( 256 , 256 , 3 , 3 ) , bias = W9 [ - 1 ] ) , negative_slope = 0.1 ) NEW_LINE x = x . view ( - 1 , 256 * 3 * 3 ) . mm ( W10 [ : - 1 ] ) + W10 [ - 1 ] NEW_LINE return x NEW_LINE DEDENT',), ('def train_loss ( data , target ) : NEW_LINE INDENT y = model ( data ) NEW_LINE y = F . log_softmax ( y , dim = 1 ) NEW_LINE loss = F . nll_loss ( y , target ) NEW_LINE return loss NEW_LINE DEDENT',), ('def test_loss ( ) : NEW_LINE INDENT num_errs = 0 NEW_LINE with torch . no_grad ( ) : NEW_LINE INDENT for data , target in test_loader : NEW_LINE INDENT y = model ( data . to ( device ) ) NEW_LINE _ , pred = torch . max ( y , dim = 1 ) NEW_LINE num_errs += torch . sum ( pred != target . to ( device ) ) NEW_LINE DEDENT DEDENT return num_errs . item ( ) / len ( test_loader . dataset ) NEW_LINE DEDENT',)]\n",
            "0.9883650615811348\n",
            "On batch 1279\n",
            "tensor([0.0073, 0.0073, 0.0070, 0.0072, 0.0073, 0.0070, 0.0072, 0.0073, 0.0073,\n",
            "        0.0071, 0.0072, 0.0071, 0.0070, 0.0071, 0.0072, 0.0072, 0.0071, 0.0072,\n",
            "        0.0071, 0.0072, 0.0072, 0.0073, 0.0073, 0.0073, 0.0071, 0.0070, 0.0072,\n",
            "        0.0072, 0.0073, 0.0072, 0.0073, 0.0072, 0.0071, 0.0074, 0.0073, 0.0070,\n",
            "        0.0072, 0.0070, 0.0072, 0.0072, 0.0072, 0.0072, 0.0072, 0.0071, 0.0073,\n",
            "        0.0073, 0.0070, 0.0073, 0.0071, 0.0071, 0.0073, 0.0071, 0.0072, 0.0072,\n",
            "        0.0072, 0.0073, 0.0072, 0.0073, 0.0073, 0.0073, 0.0072, 0.0073, 0.0072,\n",
            "        0.0072, 0.0073, 0.0071, 0.0072, 0.0072, 0.0072, 0.0072, 0.0070, 0.0070,\n",
            "        0.0072, 0.0073, 0.0070, 0.0071, 0.0072, 0.0071, 0.0073, 0.0071, 0.0074,\n",
            "        0.0072, 0.0072, 0.0072, 0.0072, 0.0072, 0.0072, 0.0072, 0.0072, 0.0071,\n",
            "        0.0072, 0.0073, 0.0073, 0.0072, 0.0072, 0.0072, 0.0073, 0.0072, 0.0071,\n",
            "        0.0072, 0.0072, 0.0073, 0.0072, 0.0072, 0.0072, 0.0072, 0.0072, 0.0072,\n",
            "        0.0072, 0.0072, 0.0072, 0.0072, 0.0072, 0.0071, 0.0070, 0.0071, 0.0072,\n",
            "        0.0073, 0.0073, 0.0070, 0.0073, 0.0072, 0.0072, 0.0072, 0.0073, 0.0072,\n",
            "        0.0073, 0.0072, 0.0071, 0.0072, 0.0072, 0.0073, 0.0072, 0.0073, 0.0072,\n",
            "        0.0073, 0.0071, 0.0072, 0.0071], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "(\"\\n{h_k^{s}}'=h^s_k+(h^t_k-h^1_k)\\n\\\\label{eq:mot}\\n\",)\n",
            "[('def read_video ( name , image_shape ) : NEW_LINE INDENT if name . lower ( ) . endswith ( \\' . png \\' ) or name . lower ( ) . endswith ( \\' . jpg \\' ) : NEW_LINE INDENT image = io . imread ( name ) NEW_LINE if len ( image . shape ) == 2 or image . shape [ 2 ] == 1 : NEW_LINE INDENT image = gray2rgb ( image ) NEW_LINE DEDENT if image . shape [ 2 ] == 4 : NEW_LINE INDENT image = image [ ... , : 3 ] NEW_LINE DEDENT image = img_as_float32 ( image ) NEW_LINE video_array = np . moveaxis ( image , 1 , 0 ) NEW_LINE video_array = video_array . reshape ( ( - 1 , ) + image_shape ) NEW_LINE video_array = np . moveaxis ( video_array , 1 , 2 ) NEW_LINE DEDENT elif name . lower ( ) . endswith ( \\' . gif \\' ) or name . lower ( ) . endswith ( \\' . mp4\\' ) or name . lower ( ) . endswith ( \\' . mov \\' ) : NEW_LINE INDENT video = np . array ( mimread ( name ) ) NEW_LINE if len ( video . shape ) == 3 : NEW_LINE INDENT video = np . array ( [ gray2rgb ( frame ) for frame in video ] ) NEW_LINE DEDENT if video . shape [ - 1 ] == 4 : NEW_LINE INDENT video = video [ ... , : 3 ] NEW_LINE DEDENT video_array = img_as_float32 ( video ) NEW_LINE DEDENT else : NEW_LINE INDENT raise Exception ( \" Unknown ▁ file ▁ extensions ▁ ▁ % s \" % name ) NEW_LINE DEDENT return video_array NEW_LINE DEDENT',), ('def __init__ ( self , root_dir , augmentation_params , image_shape = ( 64 , 64 , 3 ) , is_train = True , random_seed = 0 , pairs_list = None , transform = None ) : NEW_LINE INDENT self . root_dir = root_dir NEW_LINE self . images = os . listdir ( root_dir ) NEW_LINE self . image_shape = tuple ( image_shape ) NEW_LINE self . pairs_list = pairs_list NEW_LINE if os . path . exists ( os . path . join ( root_dir , \\' train \\' ) ) : NEW_LINE INDENT assert os . path . exists ( os . path . join ( root_dir , \\' test \\' ) ) NEW_LINE print ( \" Use ▁ predefined ▁ train - test ▁ split . \" ) NEW_LINE train_images = os . listdir ( os . path . join ( root_dir , \\' train \\' ) ) NEW_LINE test_images = os . listdir ( os . path . join ( root_dir , \\' test \\' ) ) NEW_LINE self . root_dir = os . path . join ( self . root_dir , \\' train \\' if is_train else \\' test \\' ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \" Use ▁ random ▁ train - test ▁ split . \" ) NEW_LINE train_images , test_images = train_test_split ( self . images , random_state = random_seed , test_size = 0.2 ) NEW_LINE DEDENT if is_train : NEW_LINE INDENT self . images = train_images NEW_LINE DEDENT else : NEW_LINE INDENT self . images = test_images NEW_LINE DEDENT if transform is None : NEW_LINE INDENT if is_train : NEW_LINE INDENT self . transform = AllAugmentationTransform ( ** augmentation_params ) NEW_LINE DEDENT else : NEW_LINE INDENT self . transform = VideoToTensor ( ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT self . transform = transform NEW_LINE DEDENT DEDENT',), ('def __len__ ( self ) : NEW_LINE INDENT return len ( self . images ) NEW_LINE DEDENT',), (\"def __getitem__ ( self , idx ) : NEW_LINE INDENT img_name = os . path . join ( self . root_dir , self . images [ idx ] ) NEW_LINE video_array = read_video ( img_name , image_shape = self . image_shape ) NEW_LINE out = self . transform ( video_array ) NEW_LINE out [ ' name ' ] = os . path . basename ( img_name ) NEW_LINE return out NEW_LINE DEDENT\",), (\"def __init__ ( self , initial_dataset , number_of_pairs , seed = 0 ) : NEW_LINE INDENT self . initial_dataset = initial_dataset NEW_LINE pairs_list = self . initial_dataset . pairs_list NEW_LINE np . random . seed ( seed ) NEW_LINE if pairs_list is None : NEW_LINE INDENT max_idx = min ( number_of_pairs , len ( initial_dataset ) ) NEW_LINE nx , ny = max_idx , max_idx NEW_LINE xy = np . mgrid [ : nx , : ny ] . reshape ( 2 , - 1 ) . T NEW_LINE number_of_pairs = min ( xy . shape [ 0 ] , number_of_pairs ) NEW_LINE self . pairs = xy . take ( np . random . choice ( xy . shape [ 0 ] , number_of_pairs , replace = False ) , axis = 0 ) NEW_LINE DEDENT else : NEW_LINE INDENT images = self . initial_dataset . images NEW_LINE name_to_index = { name : index for index , name in enumerate ( images ) } NEW_LINE pairs = pd . read_csv ( pairs_list ) NEW_LINE pairs = pairs [ np . logical_and ( pairs [ ' source ' ] . isin ( images ) , pairs [ ' driving ' ] . isin ( images ) ) ] NEW_LINE number_of_pairs = min ( pairs . shape [ 0 ] , number_of_pairs ) NEW_LINE self . pairs = [ ] NEW_LINE self . start_frames = [ ] NEW_LINE for ind in range ( number_of_pairs ) : NEW_LINE INDENT self . pairs . append ( ( name_to_index [ pairs [ ' driving ' ] . iloc [ ind ] ] , name_to_index [ pairs [ ' source ' ] . iloc [ ind ] ] ) ) NEW_LINE DEDENT DEDENT DEDENT\",), ('def __len__ ( self ) : NEW_LINE INDENT return len ( self . pairs ) NEW_LINE DEDENT',), (\"def __getitem__ ( self , idx ) : NEW_LINE INDENT pair = self . pairs [ idx ] NEW_LINE first = self . initial_dataset [ pair [ 0 ] ] NEW_LINE second = self . initial_dataset [ pair [ 1 ] ] NEW_LINE first = { ' driving _ ' + key : value for key , value in first . items ( ) } NEW_LINE second = { ' source _ ' + key : value for key , value in second . items ( ) } NEW_LINE return { ** first , ** second } NEW_LINE DEDENT\",), (\"def load_cpk ( checkpoint_path , generator = None , discriminator = None , kp_detector = None , optimizer_generator = None , optimizer_discriminator = None , optimizer_kp_detector = None ) : NEW_LINE INDENT checkpoint = torch . load ( checkpoint_path ) NEW_LINE if generator is not None : NEW_LINE INDENT generator . load_state_dict ( checkpoint [ ' generator ' ] ) NEW_LINE DEDENT if kp_detector is not None : NEW_LINE INDENT kp_detector . load_state_dict ( checkpoint [ ' kp _ detector ' ] ) NEW_LINE DEDENT if discriminator is not None : NEW_LINE INDENT discriminator . load_state_dict ( checkpoint [ ' discriminator ' ] ) NEW_LINE DEDENT if optimizer_generator is not None : NEW_LINE INDENT optimizer_generator . load_state_dict ( checkpoint [ ' optimizer _ generator ' ] ) NEW_LINE DEDENT if optimizer_discriminator is not None : NEW_LINE INDENT optimizer_discriminator . load_state_dict ( checkpoint [ ' optimizer _ discriminator ' ] ) NEW_LINE DEDENT if optimizer_kp_detector is not None : NEW_LINE INDENT optimizer_kp_detector . load_state_dict ( checkpoint [ ' optimizer _ kp _ detector ' ] ) NEW_LINE DEDENT return checkpoint [ ' epoch ' ] , checkpoint [ ' it ' ] NEW_LINE DEDENT\",), (\"def __init__ ( self , log_dir , log_file_name = ' log . txt ' , log_freq_iter = 100 , cpk_freq_epoch = 100 , zfill_num = 8 , visualizer_params = None ) : NEW_LINE INDENT self . loss_list = [ ] NEW_LINE self . cpk_dir = log_dir NEW_LINE self . visualizations_dir = os . path . join ( log_dir , ' train - vis ' ) NEW_LINE if not os . path . exists ( self . visualizations_dir ) : NEW_LINE INDENT os . makedirs ( self . visualizations_dir ) NEW_LINE DEDENT self . log_file = open ( os . path . join ( log_dir , log_file_name ) , ' a ' ) NEW_LINE self . log_freq = log_freq_iter NEW_LINE self . cpk_freq = cpk_freq_epoch NEW_LINE self . zfill_num = zfill_num NEW_LINE self . visualizer = Visualizer ( ** visualizer_params ) NEW_LINE self . epoch = 0 NEW_LINE self . it = 0 NEW_LINE DEDENT\",), ('def log_scores ( self , loss_names ) : NEW_LINE INDENT loss_mean = np . array ( self . loss_list ) . mean ( axis = 0 ) NEW_LINE loss_string = \" ; ▁ \" . join ( [ \" % s ▁ - ▁ % .5f \" % ( name , value ) for name , value in zip ( loss_names , loss_mean ) ] ) NEW_LINE loss_string = str ( self . it ) . zfill ( self . zfill_num ) + \" ) ▁ \" + loss_string NEW_LINE print ( loss_string , file = self . log_file ) NEW_LINE self . loss_list = [ ] NEW_LINE self . log_file . flush ( ) NEW_LINE DEDENT',), ('def visualize_rec ( self , inp , out ) : NEW_LINE INDENT image = self . visualizer . visualize_reconstruction ( inp , out ) NEW_LINE imageio . mimsave ( os . path . join ( self . visualizations_dir , \" % s - rec . gif \" % str ( self . it ) . zfill ( self . zfill_num ) ) , image ) NEW_LINE DEDENT',), (\"def save_cpk ( self ) : NEW_LINE INDENT cpk = { k : v . state_dict ( ) for k , v in self . models . items ( ) } NEW_LINE cpk [ ' epoch ' ] = self . epoch NEW_LINE cpk [ ' it ' ] = self . it NEW_LINE torch . save ( cpk , os . path . join ( self . cpk_dir , ' % s - checkpoint . pth . tar ' % str ( self . epoch ) . zfill ( self . zfill_num ) ) ) NEW_LINE DEDENT\",), ('def __enter__ ( self ) : NEW_LINE INDENT return self NEW_LINE DEDENT',), (\"def __exit__ ( self , exc_type , exc_val , exc_tb ) : NEW_LINE INDENT if ' models ' in self . __dict__ : NEW_LINE INDENT self . save_cpk ( ) NEW_LINE DEDENT self . log_file . close ( ) NEW_LINE DEDENT\",), ('def log_iter ( self , it , names , values , inp , out ) : NEW_LINE INDENT self . it = it NEW_LINE self . names = names NEW_LINE self . loss_list . append ( values ) NEW_LINE if it % self . log_freq == 0 : NEW_LINE INDENT self . log_scores ( self . names ) NEW_LINE self . visualize_rec ( inp , out ) NEW_LINE DEDENT DEDENT',), ('def log_epoch ( self , epoch , models ) : NEW_LINE INDENT self . epoch = epoch NEW_LINE self . models = models NEW_LINE if epoch % self . cpk_freq == 0 : NEW_LINE INDENT self . save_cpk ( ) NEW_LINE DEDENT DEDENT',), (\"def __init__ ( self , kp_size = 2 , draw_border = False , colormap = ' gist _ rainbow ' ) : NEW_LINE INDENT self . kp_size = kp_size NEW_LINE self . draw_border = draw_border NEW_LINE self . colormap = plt . get_cmap ( colormap ) NEW_LINE DEDENT\",), ('def draw_video_with_kp ( self , video , kp_array ) : NEW_LINE INDENT video_array = np . copy ( video ) NEW_LINE spatial_size = np . array ( video_array . shape [ 2 : 0 : - 1 ] ) [ np . newaxis , np . newaxis ] NEW_LINE kp_array = spatial_size * ( kp_array + 1 ) / 2 NEW_LINE num_kp = kp_array . shape [ 1 ] NEW_LINE for i in range ( len ( video_array ) ) : NEW_LINE INDENT for kp_ind , kp in enumerate ( kp_array [ i ] ) : NEW_LINE INDENT rr , cc = circle ( kp [ 1 ] , kp [ 0 ] , self . kp_size , shape = video_array . shape [ 1 : 3 ] ) NEW_LINE video_array [ i ] [ rr , cc ] = np . array ( self . colormap ( kp_ind / num_kp ) ) [ : 3 ] NEW_LINE DEDENT DEDENT return video_array NEW_LINE DEDENT',), ('def create_video_column_with_kp ( self , video , kp ) : NEW_LINE INDENT video_array = np . array ( [ self . draw_video_with_kp ( v , k ) for v , k in zip ( video , kp ) ] ) NEW_LINE return self . create_video_column ( video_array ) NEW_LINE DEDENT',), ('def create_video_column ( self , videos ) : NEW_LINE INDENT if self . draw_border : NEW_LINE INDENT videos = np . copy ( videos ) NEW_LINE videos [ : , : , [ 0 , - 1 ] ] = ( 1 , 1 , 1 ) NEW_LINE videos [ : , : , : , [ 0 , - 1 ] ] = ( 1 , 1 , 1 ) NEW_LINE DEDENT return np . concatenate ( list ( videos ) , axis = 1 ) NEW_LINE DEDENT',), ('def create_image_grid ( self , * args ) : NEW_LINE INDENT out = [ ] NEW_LINE for arg in args : NEW_LINE INDENT if type ( arg ) == tuple : NEW_LINE INDENT out . append ( self . create_video_column_with_kp ( arg [ 0 ] , arg [ 1 ] ) ) NEW_LINE DEDENT else : NEW_LINE INDENT out . append ( self . create_video_column ( arg ) ) NEW_LINE DEDENT DEDENT return np . concatenate ( out , axis = 2 ) NEW_LINE DEDENT',), (\"def visualize_transfer ( self , driving_video , source_image , out ) : NEW_LINE INDENT out_video_batch = out [ ' video _ prediction ' ] . data . cpu ( ) . numpy ( ) NEW_LINE appearance_deformed_batch = out [ ' video _ deformed ' ] . data . cpu ( ) . numpy ( ) NEW_LINE motion_video_batch = driving_video . data . cpu ( ) . numpy ( ) NEW_LINE appearance_video_batch = source_image [ : , : , 0 : 1 ] . data . cpu ( ) . repeat ( 1 , 1 , out_video_batch . shape [ 2 ] , 1 , 1 ) . numpy ( ) NEW_LINE video_first_frame = driving_video [ : , : , 0 : 1 ] . data . cpu ( ) . repeat ( 1 , 1 , out_video_batch . shape [ 2 ] , 1 , 1 ) . numpy ( ) NEW_LINE kp_video = out [ ' kp _ driving ' ] [ ' mean ' ] . data . cpu ( ) . numpy ( ) NEW_LINE kp_appearance = out [ ' kp _ source ' ] [ ' mean ' ] . data . cpu ( ) . repeat ( 1 , out_video_batch . shape [ 2 ] , 1 , 1 ) . numpy ( ) NEW_LINE kp_norm = out [ ' kp _ norm ' ] [ ' mean ' ] . data . cpu ( ) . numpy ( ) NEW_LINE kp_video_first = out [ ' kp _ driving ' ] [ ' mean ' ] [ : , : 1 ] . data . cpu ( ) . repeat ( 1 , out_video_batch . shape [ 2 ] , 1 , 1 ) . numpy ( ) NEW_LINE video_first_frame = np . transpose ( video_first_frame , [ 0 , 2 , 3 , 4 , 1 ] ) NEW_LINE out_video_batch = np . transpose ( out_video_batch , [ 0 , 2 , 3 , 4 , 1 ] ) NEW_LINE motion_video_batch = np . transpose ( motion_video_batch , [ 0 , 2 , 3 , 4 , 1 ] ) NEW_LINE appearance_video_batch = np . transpose ( appearance_video_batch , [ 0 , 2 , 3 , 4 , 1 ] ) NEW_LINE appearance_deformed_batch = np . transpose ( appearance_deformed_batch , [ 0 , 2 , 3 , 4 , 1 ] ) NEW_LINE image = self . create_image_grid ( ( appearance_video_batch , kp_appearance ) , ( video_first_frame , kp_video_first ) , ( motion_video_batch , kp_video ) , ( out_video_batch , kp_norm ) , out_video_batch , appearance_deformed_batch ) NEW_LINE image = ( 255 * image ) . astype ( np . uint8 ) NEW_LINE return image NEW_LINE DEDENT\",), (\"def visualize_reconstruction ( self , inp , out ) : NEW_LINE INDENT out_video_batch = out [ ' video _ prediction ' ] . data . cpu ( ) . numpy ( ) NEW_LINE if ' driving ' in inp : NEW_LINE INDENT gt_video_batch = inp [ ' driving ' ] . data . cpu ( ) . numpy ( ) NEW_LINE DEDENT else : NEW_LINE INDENT gt_video_batch = inp [ ' video ' ] . data . cpu ( ) . numpy ( ) NEW_LINE DEDENT appearance_deformed_batch = out [ ' video _ deformed ' ] . data . cpu ( ) . numpy ( ) NEW_LINE appearance_video_batch = inp [ ' source ' ] . data . cpu ( ) . repeat ( 1 , 1 , out_video_batch . shape [ 2 ] , 1 , 1 ) . numpy ( ) NEW_LINE kp_video = out [ ' kp _ driving ' ] [ ' mean ' ] . data . cpu ( ) . numpy ( ) NEW_LINE kp_appearance = out [ ' kp _ source ' ] [ ' mean ' ] . data . cpu ( ) . repeat ( 1 , out_video_batch . shape [ 2 ] , 1 , 1 ) . numpy ( ) NEW_LINE out_video_batch = np . transpose ( out_video_batch , [ 0 , 2 , 3 , 4 , 1 ] ) NEW_LINE gt_video_batch = np . transpose ( gt_video_batch , [ 0 , 2 , 3 , 4 , 1 ] ) NEW_LINE appearance_video_batch = np . transpose ( appearance_video_batch , [ 0 , 2 , 3 , 4 , 1 ] ) NEW_LINE appearance_deformed_batch = np . transpose ( appearance_deformed_batch , [ 0 , 2 , 3 , 4 , 1 ] ) NEW_LINE image = self . create_image_grid ( ( appearance_video_batch , kp_appearance ) , ( gt_video_batch , kp_video ) , out_video_batch , appearance_deformed_batch , gt_video_batch ) NEW_LINE image = ( 255 * image ) . astype ( np . uint8 ) NEW_LINE return image NEW_LINE DEDENT\",), ('def prediction ( config , generator , kp_detector , checkpoint , log_dir ) : NEW_LINE INDENT dataset = FramesDataset ( is_train = True , transform = VideoToTensor ( ) , ** config [ \\' dataset _ params \\' ] ) NEW_LINE log_dir = os . path . join ( log_dir , \\' prediction \\' ) NEW_LINE png_dir = os . path . join ( log_dir , \\' png \\' ) NEW_LINE if checkpoint is not None : NEW_LINE INDENT Logger . load_cpk ( checkpoint , generator = generator , kp_detector = kp_detector ) NEW_LINE DEDENT else : NEW_LINE INDENT raise AttributeError ( \" Checkpoint ▁ should ▁ be ▁ specified ▁ for ▁ mode = \\' prediction \\' . \" ) NEW_LINE DEDENT dataloader = DataLoader ( dataset , batch_size = 1 , shuffle = False , num_workers = 1 ) NEW_LINE generator = DataParallelWithCallback ( generator ) NEW_LINE kp_detector = DataParallelWithCallback ( kp_detector ) NEW_LINE if not os . path . exists ( log_dir ) : NEW_LINE INDENT os . makedirs ( log_dir ) NEW_LINE DEDENT if not os . path . exists ( png_dir ) : NEW_LINE INDENT os . makedirs ( png_dir ) NEW_LINE DEDENT print ( \" Extracting ▁ keypoints . . . \" ) NEW_LINE kp_detector . eval ( ) NEW_LINE generator . eval ( ) NEW_LINE keypoints_array = [ ] NEW_LINE prediction_params = config [ \\' prediction _ params \\' ] NEW_LINE for it , x in tqdm ( enumerate ( dataloader ) ) : NEW_LINE INDENT if prediction_params [ \\' train _ size \\' ] is not None : NEW_LINE INDENT if it > prediction_params [ \\' train _ size \\' ] : NEW_LINE INDENT break NEW_LINE DEDENT DEDENT with torch . no_grad ( ) : NEW_LINE INDENT keypoints = [ ] NEW_LINE for i in range ( x [ \\' video \\' ] . shape [ 2 ] ) : NEW_LINE INDENT kp = kp_detector ( x [ \\' video \\' ] [ : , : , i : ( i + 1 ) ] ) NEW_LINE kp = { k : v . data . cpu ( ) . numpy ( ) for k , v in kp . items ( ) } NEW_LINE keypoints . append ( kp ) NEW_LINE DEDENT keypoints_array . append ( keypoints ) NEW_LINE DEDENT DEDENT predictor = PredictionModule ( num_kp = config [ \\' model _ params \\' ] [ \\' common _ params \\' ] [ \\' num _ kp \\' ] , kp_variance = config [ \\' model _ params \\' ] [ \\' common _ params \\' ] [ \\' kp _ variance \\' ] , ** prediction_params [ \\' rnn _ params \\' ] ) . cuda ( ) NEW_LINE num_epochs = prediction_params [ \\' num _ epochs \\' ] NEW_LINE lr = prediction_params [ \\' lr \\' ] NEW_LINE bs = prediction_params [ \\' batch _ size \\' ] NEW_LINE num_frames = prediction_params [ \\' num _ frames \\' ] NEW_LINE init_frames = prediction_params [ \\' init _ frames \\' ] NEW_LINE optimizer = torch . optim . Adam ( predictor . parameters ( ) , lr = lr ) NEW_LINE scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , verbose = True , patience = 50 ) NEW_LINE kp_dataset = KPDataset ( keypoints_array , num_frames = num_frames ) NEW_LINE kp_dataloader = DataLoader ( kp_dataset , batch_size = bs ) NEW_LINE print ( \" Training ▁ prediction . . . \" ) NEW_LINE for _ in trange ( num_epochs ) : NEW_LINE INDENT loss_list = [ ] NEW_LINE for x in kp_dataloader : NEW_LINE INDENT x = { k : v . cuda ( ) for k , v in x . items ( ) } NEW_LINE gt = { k : v . clone ( ) for k , v in x . items ( ) } NEW_LINE for k in x : NEW_LINE INDENT x [ k ] [ : , init_frames : ] = 0 NEW_LINE DEDENT prediction = predictor ( x ) NEW_LINE loss = sum ( [ torch . abs ( gt [ k ] [ : , init_frames : ] - prediction [ k ] [ : , init_frames : ] ) . mean ( ) for k in x ] ) NEW_LINE loss . backward ( ) NEW_LINE optimizer . step ( ) NEW_LINE optimizer . zero_grad ( ) NEW_LINE loss_list . append ( loss . detach ( ) . data . cpu ( ) . numpy ( ) ) NEW_LINE DEDENT loss = np . mean ( loss_list ) NEW_LINE scheduler . step ( loss ) NEW_LINE DEDENT dataset = FramesDataset ( is_train = False , transform = VideoToTensor ( ) , ** config [ \\' dataset _ params \\' ] ) NEW_LINE dataloader = DataLoader ( dataset , batch_size = 1 , shuffle = False , num_workers = 1 ) NEW_LINE print ( \" Make ▁ predictions . . . \" ) NEW_LINE for it , x in tqdm ( enumerate ( dataloader ) ) : NEW_LINE INDENT with torch . no_grad ( ) : NEW_LINE INDENT x [ \\' video \\' ] = x [ \\' video \\' ] [ : , : , : num_frames ] NEW_LINE kp_init = kp_detector ( x [ \\' video \\' ] ) NEW_LINE for k in kp_init : NEW_LINE INDENT kp_init [ k ] [ : , init_frames : ] = 0 NEW_LINE DEDENT kp_source = kp_detector ( x [ \\' video \\' ] [ : , : , : 1 ] ) NEW_LINE kp_video = predictor ( kp_init ) NEW_LINE for k in kp_video : NEW_LINE INDENT kp_video [ k ] [ : , : init_frames ] = kp_init [ k ] [ : , : init_frames ] NEW_LINE DEDENT if \\' var \\' in kp_video and prediction_params [ \\' predict _ variance \\' ] : NEW_LINE INDENT kp_video [ \\' var \\' ] = kp_init [ \\' var \\' ] [ : , ( init_frames - 1 ) : init_frames ] . repeat ( 1 , kp_video [ \\' var \\' ] . shape [ 1 ] , 1 , 1 , 1 ) NEW_LINE DEDENT out = generate ( generator , appearance_image = x [ \\' video \\' ] [ : , : , : 1 ] , kp_appearance = kp_source , kp_video = kp_video ) NEW_LINE x [ \\' source \\' ] = x [ \\' video \\' ] [ : , : , : 1 ] NEW_LINE out_video_batch = out [ \\' video _ prediction \\' ] . data . cpu ( ) . numpy ( ) NEW_LINE out_video_batch = np . concatenate ( np . transpose ( out_video_batch , [ 0 , 2 , 3 , 4 , 1 ] ) [ 0 ] , axis = 1 ) NEW_LINE imageio . imsave ( os . path . join ( png_dir , x [ \\' name \\' ] [ 0 ] + \\' . png \\' ) , ( 255 * out_video_batch ) . astype ( np . uint8 ) ) NEW_LINE image = Visualizer ( ** config [ \\' visualizer _ params \\' ] ) . visualize_reconstruction ( x , out ) NEW_LINE image_name = x [ \\' name \\' ] [ 0 ] + prediction_params [ \\' format \\' ] NEW_LINE imageio . mimsave ( os . path . join ( log_dir , image_name ) , image ) NEW_LINE del x , kp_video , kp_source , out NEW_LINE DEDENT DEDENT DEDENT',), ('def __init__ ( self , keypoints_array , num_frames ) : NEW_LINE INDENT self . keypoints_array = keypoints_array NEW_LINE self . transform = SelectRandomFrames ( consequent = True , number_of_frames = num_frames ) NEW_LINE DEDENT',), ('def __len__ ( self ) : NEW_LINE INDENT return len ( self . keypoints_array ) NEW_LINE DEDENT',), ('def __getitem__ ( self , idx ) : NEW_LINE INDENT keypoints = self . keypoints_array [ idx ] NEW_LINE selected = self . transform ( keypoints ) NEW_LINE selected = { k : np . concatenate ( [ v [ k ] [ 0 ] for v in selected ] , axis = 0 ) for k in selected [ 0 ] . keys ( ) } NEW_LINE return selected NEW_LINE DEDENT',), ('def make_symetric_matrix ( torch_matrix ) : NEW_LINE INDENT a = torch_matrix . cpu ( ) . numpy ( ) NEW_LINE c = ( a + np . transpose ( a , ( 0 , 1 , 2 , 4 , 3 ) ) ) / 2 NEW_LINE d , u = np . linalg . eig ( c ) NEW_LINE d [ d <= 0 ] = 1e-6 NEW_LINE d_matrix = np . zeros_like ( a ) NEW_LINE d_matrix [ ... , 0 , 0 ] = d [ ... , 0 ] NEW_LINE d_matrix [ ... , 1 , 1 ] = d [ ... , 1 ] NEW_LINE res = np . matmul ( np . matmul ( u , d_matrix ) , np . transpose ( u , ( 0 , 1 , 2 , 4 , 3 ) ) ) NEW_LINE res = torch . from_numpy ( res ) . type ( torch_matrix . type ( ) ) NEW_LINE return res NEW_LINE DEDENT',), (\"def normalize_kp ( kp_video , kp_appearance , movement_mult = False , move_location = False , adapt_variance = False , clip_mean = False ) : NEW_LINE INDENT if movement_mult : NEW_LINE INDENT appearance_area = ConvexHull ( kp_appearance [ ' mean ' ] [ 0 , 0 ] . data . cpu ( ) . numpy ( ) ) . volume NEW_LINE video_area = ConvexHull ( kp_video [ ' mean ' ] [ 0 , 0 ] . data . cpu ( ) . numpy ( ) ) . volume NEW_LINE movement_mult = np . sqrt ( appearance_area ) / np . sqrt ( video_area ) NEW_LINE DEDENT else : NEW_LINE INDENT movement_mult = 1 NEW_LINE DEDENT kp_video = { k : v for k , v in kp_video . items ( ) } NEW_LINE if move_location : NEW_LINE INDENT kp_video_diff = ( kp_video [ ' mean ' ] - kp_video [ ' mean ' ] [ : , 0 : 1 ] ) NEW_LINE kp_video_diff *= movement_mult NEW_LINE kp_video [ ' mean ' ] = kp_video_diff + kp_appearance [ ' mean ' ] NEW_LINE DEDENT if clip_mean : NEW_LINE INDENT one = torch . ones ( 1 ) . type ( kp_video_diff . type ( ) ) NEW_LINE kp_video [ ' mean ' ] = torch . max ( kp_video [ ' mean ' ] , - one ) NEW_LINE kp_video [ ' mean ' ] = torch . min ( kp_video [ ' mean ' ] , one ) NEW_LINE DEDENT if ( ' var ' in kp_video ) and adapt_variance : NEW_LINE INDENT var_first = kp_video [ ' var ' ] [ : , 0 : 1 ] . repeat ( 1 , kp_video [ ' var ' ] . shape [ 1 ] , 1 , 1 , 1 ) NEW_LINE kp_var , _ = torch . gesv ( var_first , kp_video [ ' var ' ] ) NEW_LINE kp_var = torch . matmul ( kp_video [ ' var ' ] , matrix_inverse ( kp_video [ ' var ' ] [ : , 0 : 1 ] , eps = 0 ) ) NEW_LINE kp_var = torch . matmul ( kp_var , kp_appearance [ ' var ' ] ) NEW_LINE kp_var = make_symetric_matrix ( kp_var ) NEW_LINE kp_video [ ' var ' ] = kp_var NEW_LINE DEDENT return kp_video NEW_LINE DEDENT\",), (\"def transfer_one ( generator , kp_detector , source_image , driving_video , transfer_params ) : NEW_LINE INDENT cat_dict = lambda l , dim : { k : torch . cat ( [ v [ k ] for v in l ] , dim = dim ) for k in l [ 0 ] } NEW_LINE d = driving_video . shape [ 2 ] NEW_LINE kp_driving = cat_dict ( [ kp_detector ( driving_video [ : , : , i : ( i + 1 ) ] ) for i in range ( d ) ] , dim = 1 ) NEW_LINE kp_source = kp_detector ( source_image ) NEW_LINE kp_driving_norm = normalize_kp ( kp_driving , kp_source , ** transfer_params [ ' normalization _ params ' ] ) NEW_LINE kp_video_list = [ { k : v [ : , i : ( i + 1 ) ] for k , v in kp_driving_norm . items ( ) } for i in range ( d ) ] NEW_LINE out = cat_dict ( [ generator ( source_image = source_image , kp_driving = kp , kp_source = kp_source ) for kp in kp_video_list ] , dim = 2 ) NEW_LINE out [ ' kp _ driving ' ] = kp_driving NEW_LINE out [ ' kp _ source ' ] = kp_source NEW_LINE out [ ' kp _ norm ' ] = kp_driving_norm NEW_LINE return out NEW_LINE DEDENT\",), ('def transfer ( config , generator , kp_detector , checkpoint , log_dir , dataset ) : NEW_LINE INDENT log_dir = os . path . join ( log_dir , \\' transfer \\' ) NEW_LINE png_dir = os . path . join ( log_dir , \\' png \\' ) NEW_LINE transfer_params = config [ \\' transfer _ params \\' ] NEW_LINE dataset = PairedDataset ( initial_dataset = dataset , number_of_pairs = transfer_params [ \\' num _ pairs \\' ] ) NEW_LINE dataloader = DataLoader ( dataset , batch_size = 1 , shuffle = False , num_workers = 1 ) NEW_LINE if checkpoint is not None : NEW_LINE INDENT Logger . load_cpk ( checkpoint , generator = generator , kp_detector = kp_detector ) NEW_LINE DEDENT else : NEW_LINE INDENT raise AttributeError ( \" Checkpoint ▁ should ▁ be ▁ specified ▁ for ▁ mode = \\' transfer \\' . \" ) NEW_LINE DEDENT if not os . path . exists ( log_dir ) : NEW_LINE INDENT os . makedirs ( log_dir ) NEW_LINE DEDENT if not os . path . exists ( png_dir ) : NEW_LINE INDENT os . makedirs ( png_dir ) NEW_LINE DEDENT generator = DataParallelWithCallback ( generator ) NEW_LINE kp_detector = DataParallelWithCallback ( kp_detector ) NEW_LINE generator . eval ( ) NEW_LINE kp_detector . eval ( ) NEW_LINE for it , x in tqdm ( enumerate ( dataloader ) ) : NEW_LINE INDENT with torch . no_grad ( ) : NEW_LINE INDENT x = { key : value if not hasattr ( value , \\' cuda \\' ) else value . cuda ( ) for key , value in x . items ( ) } NEW_LINE driving_video = x [ \\' driving _ video \\' ] NEW_LINE source_image = x [ \\' source _ video \\' ] [ : , : , : 1 , : , : ] NEW_LINE out = transfer_one ( generator , kp_detector , source_image , driving_video , transfer_params ) NEW_LINE img_name = \" - \" . join ( [ x [ \\' driving _ name \\' ] [ 0 ] , x [ \\' source _ name \\' ] [ 0 ] ] ) NEW_LINE out_video_batch = out [ \\' video _ prediction \\' ] . data . cpu ( ) . numpy ( ) NEW_LINE out_video_batch = np . concatenate ( np . transpose ( out_video_batch , [ 0 , 2 , 3 , 4 , 1 ] ) [ 0 ] , axis = 1 ) NEW_LINE imageio . imsave ( os . path . join ( png_dir , img_name + \\' . png \\' ) , ( 255 * out_video_batch ) . astype ( np . uint8 ) ) NEW_LINE image = Visualizer ( ** config [ \\' visualizer _ params \\' ] ) . visualize_transfer ( driving_video = driving_video , source_image = source_image , out = out ) NEW_LINE imageio . mimsave ( os . path . join ( log_dir , img_name + transfer_params [ \\' format \\' ] ) , image ) NEW_LINE DEDENT DEDENT DEDENT',), (\"def crop_clip ( clip , min_h , min_w , h , w ) : NEW_LINE INDENT if isinstance ( clip [ 0 ] , np . ndarray ) : NEW_LINE INDENT cropped = [ img [ min_h : min_h + h , min_w : min_w + w , : ] for img in clip ] NEW_LINE DEDENT elif isinstance ( clip [ 0 ] , PIL . Image . Image ) : NEW_LINE INDENT cropped = [ img . crop ( ( min_w , min_h , min_w + w , min_h + h ) ) for img in clip ] NEW_LINE DEDENT else : NEW_LINE INDENT raise TypeError ( ' Expected ▁ numpy . ndarray ▁ or ▁ PIL . Image ' + ' but ▁ got ▁ list ▁ of ▁ { 0 } ' . format ( type ( clip [ 0 ] ) ) ) NEW_LINE DEDENT return cropped NEW_LINE DEDENT\",), (\"def pad_clip ( clip , h , w ) : NEW_LINE INDENT im_h , im_w = clip [ 0 ] . shape [ : 2 ] NEW_LINE pad_h = ( 0 , 0 ) if h < im_h else ( ( h - im_h ) // 2 , ( h - im_h + 1 ) // 2 ) NEW_LINE pad_w = ( 0 , 0 ) if w < im_w else ( ( w - im_w ) // 2 , ( w - im_w + 1 ) // 2 ) NEW_LINE return pad ( clip , ( ( 0 , 0 ) , pad_h , pad_w , ( 0 , 0 ) ) , mode = ' edge ' ) NEW_LINE DEDENT\",), (\"def resize_clip ( clip , size , interpolation = ' bilinear ' ) : NEW_LINE INDENT if isinstance ( clip [ 0 ] , np . ndarray ) : NEW_LINE INDENT if isinstance ( size , numbers . Number ) : NEW_LINE INDENT im_h , im_w , im_c = clip [ 0 ] . shape NEW_LINE if ( im_w <= im_h and im_w == size ) or ( im_h <= im_w and im_h == size ) : NEW_LINE INDENT return clip NEW_LINE DEDENT new_h , new_w = get_resize_sizes ( im_h , im_w , size ) NEW_LINE size = ( new_w , new_h ) NEW_LINE DEDENT else : NEW_LINE INDENT size = size [ 1 ] , size [ 0 ] NEW_LINE DEDENT scaled = [ resize ( img , size , order = 1 if interpolation == ' bilinear ' else 0 , preserve_range = True , mode = ' constant ' , anti_aliasing = True ) for img in clip ] NEW_LINE DEDENT elif isinstance ( clip [ 0 ] , PIL . Image . Image ) : NEW_LINE INDENT if isinstance ( size , numbers . Number ) : NEW_LINE INDENT im_w , im_h = clip [ 0 ] . size NEW_LINE if ( im_w <= im_h and im_w == size ) or ( im_h <= im_w and im_h == size ) : NEW_LINE INDENT return clip NEW_LINE DEDENT new_h , new_w = get_resize_sizes ( im_h , im_w , size ) NEW_LINE size = ( new_w , new_h ) NEW_LINE DEDENT else : NEW_LINE INDENT size = size [ 1 ] , size [ 0 ] NEW_LINE DEDENT if interpolation == ' bilinear ' : NEW_LINE INDENT pil_inter = PIL . Image . NEAREST NEW_LINE DEDENT else : NEW_LINE INDENT pil_inter = PIL . Image . BILINEAR NEW_LINE DEDENT scaled = [ img . resize ( size , pil_inter ) for img in clip ] NEW_LINE DEDENT else : NEW_LINE INDENT raise TypeError ( ' Expected ▁ numpy . ndarray ▁ or ▁ PIL . Image ' + ' but ▁ got ▁ list ▁ of ▁ { 0 } ' . format ( type ( clip [ 0 ] ) ) ) NEW_LINE DEDENT return scaled NEW_LINE DEDENT\",), ('def get_resize_sizes ( im_h , im_w , size ) : NEW_LINE INDENT if im_w < im_h : NEW_LINE INDENT ow = size NEW_LINE oh = int ( size * im_h / im_w ) NEW_LINE DEDENT else : NEW_LINE INDENT oh = size NEW_LINE ow = int ( size * im_w / im_h ) NEW_LINE DEDENT return oh , ow NEW_LINE DEDENT',), ('def __init__ ( self , time_flip = False , horizontal_flip = False ) : NEW_LINE INDENT self . time_flip = time_flip NEW_LINE self . horizontal_flip = horizontal_flip NEW_LINE DEDENT',), ('def __call__ ( self , clip ) : NEW_LINE INDENT if random . random ( ) < 0.5 and self . time_flip : NEW_LINE INDENT return clip [ : : - 1 ] NEW_LINE DEDENT if random . random ( ) < 0.5 and self . horizontal_flip : NEW_LINE INDENT return [ np . fliplr ( img ) for img in clip ] NEW_LINE DEDENT return clip NEW_LINE DEDENT',), (\"def __init__ ( self , ratio = ( 3. / 4. , 4. / 3. ) , interpolation = ' nearest ' ) : NEW_LINE INDENT self . ratio = ratio NEW_LINE self . interpolation = interpolation NEW_LINE DEDENT\",), ('def __call__ ( self , clip ) : NEW_LINE INDENT scaling_factor = random . uniform ( self . ratio [ 0 ] , self . ratio [ 1 ] ) NEW_LINE if isinstance ( clip [ 0 ] , np . ndarray ) : NEW_LINE INDENT im_h , im_w , im_c = clip [ 0 ] . shape NEW_LINE DEDENT elif isinstance ( clip [ 0 ] , PIL . Image . Image ) : NEW_LINE INDENT im_w , im_h = clip [ 0 ] . size NEW_LINE DEDENT new_w = int ( im_w * scaling_factor ) NEW_LINE new_h = int ( im_h * scaling_factor ) NEW_LINE new_size = ( new_w , new_h ) NEW_LINE resized = resize_clip ( clip , new_size , interpolation = self . interpolation ) NEW_LINE return resized NEW_LINE DEDENT',), ('def __init__ ( self , size ) : NEW_LINE INDENT if isinstance ( size , numbers . Number ) : NEW_LINE INDENT size = ( size , size ) NEW_LINE DEDENT self . size = size NEW_LINE DEDENT',), (\"def __call__ ( self , clip ) : NEW_LINE INDENT h , w = self . size NEW_LINE if isinstance ( clip [ 0 ] , np . ndarray ) : NEW_LINE INDENT im_h , im_w , im_c = clip [ 0 ] . shape NEW_LINE DEDENT elif isinstance ( clip [ 0 ] , PIL . Image . Image ) : NEW_LINE INDENT im_w , im_h = clip [ 0 ] . size NEW_LINE DEDENT else : NEW_LINE INDENT raise TypeError ( ' Expected ▁ numpy . ndarray ▁ or ▁ PIL . Image ' + ' but ▁ got ▁ list ▁ of ▁ { 0 } ' . format ( type ( clip [ 0 ] ) ) ) NEW_LINE DEDENT clip = pad_clip ( clip , h , w ) NEW_LINE im_h , im_w = clip . shape [ 1 : 3 ] NEW_LINE x1 = 0 if h == im_h else random . randint ( 0 , im_w - w ) NEW_LINE y1 = 0 if w == im_w else random . randint ( 0 , im_h - h ) NEW_LINE cropped = crop_clip ( clip , y1 , x1 , h , w ) NEW_LINE return cropped NEW_LINE DEDENT\",), (\"def __init__ ( self , degrees ) : NEW_LINE INDENT if isinstance ( degrees , numbers . Number ) : NEW_LINE INDENT if degrees < 0 : NEW_LINE INDENT raise ValueError ( ' If ▁ degrees ▁ is ▁ a ▁ single ▁ number , ' ' must ▁ be ▁ positive ' ) NEW_LINE DEDENT degrees = ( - degrees , degrees ) NEW_LINE DEDENT else : NEW_LINE INDENT if len ( degrees ) != 2 : NEW_LINE INDENT raise ValueError ( ' If ▁ degrees ▁ is ▁ a ▁ sequence , ' ' it ▁ must ▁ be ▁ of ▁ len ▁ 2 . ' ) NEW_LINE DEDENT DEDENT self . degrees = degrees NEW_LINE DEDENT\",), (\"def __call__ ( self , clip ) : NEW_LINE INDENT angle = random . uniform ( self . degrees [ 0 ] , self . degrees [ 1 ] ) NEW_LINE if isinstance ( clip [ 0 ] , np . ndarray ) : NEW_LINE INDENT rotated = [ rotate ( image = img , angle = angle , preserve_range = True ) for img in clip ] NEW_LINE DEDENT elif isinstance ( clip [ 0 ] , PIL . Image . Image ) : NEW_LINE INDENT rotated = [ img . rotate ( angle ) for img in clip ] NEW_LINE DEDENT else : NEW_LINE INDENT raise TypeError ( ' Expected ▁ numpy . ndarray ▁ or ▁ PIL . Image ' + ' but ▁ got ▁ list ▁ of ▁ { 0 } ' . format ( type ( clip [ 0 ] ) ) ) NEW_LINE DEDENT return rotated NEW_LINE DEDENT\",), ('def __init__ ( self , brightness = 0 , contrast = 0 , saturation = 0 , hue = 0 ) : NEW_LINE INDENT self . brightness = brightness NEW_LINE self . contrast = contrast NEW_LINE self . saturation = saturation NEW_LINE self . hue = hue NEW_LINE DEDENT',), ('def get_params ( self , brightness , contrast , saturation , hue ) : NEW_LINE INDENT if brightness > 0 : NEW_LINE INDENT brightness_factor = random . uniform ( max ( 0 , 1 - brightness ) , 1 + brightness ) NEW_LINE DEDENT else : NEW_LINE INDENT brightness_factor = None NEW_LINE DEDENT if contrast > 0 : NEW_LINE INDENT contrast_factor = random . uniform ( max ( 0 , 1 - contrast ) , 1 + contrast ) NEW_LINE DEDENT else : NEW_LINE INDENT contrast_factor = None NEW_LINE DEDENT if saturation > 0 : NEW_LINE INDENT saturation_factor = random . uniform ( max ( 0 , 1 - saturation ) , 1 + saturation ) NEW_LINE DEDENT else : NEW_LINE INDENT saturation_factor = None NEW_LINE DEDENT if hue > 0 : NEW_LINE INDENT hue_factor = random . uniform ( - hue , hue ) NEW_LINE DEDENT else : NEW_LINE INDENT hue_factor = None NEW_LINE DEDENT return brightness_factor , contrast_factor , saturation_factor , hue_factor NEW_LINE DEDENT',), ('def __call__ ( self , clip ) : NEW_LINE INDENT if isinstance ( clip [ 0 ] , np . ndarray ) : NEW_LINE INDENT brightness , contrast , saturation , hue = self . get_params ( self . brightness , self . contrast , self . saturation , self . hue ) NEW_LINE img_transforms = [ ] NEW_LINE if brightness is not None : NEW_LINE INDENT img_transforms . append ( lambda img : torchvision . transforms . functional . adjust_brightness ( img , brightness ) ) NEW_LINE DEDENT if saturation is not None : NEW_LINE INDENT img_transforms . append ( lambda img : torchvision . transforms . functional . adjust_saturation ( img , saturation ) ) NEW_LINE DEDENT if hue is not None : NEW_LINE INDENT img_transforms . append ( lambda img : torchvision . transforms . functional . adjust_hue ( img , hue ) ) NEW_LINE DEDENT if contrast is not None : NEW_LINE INDENT img_transforms . append ( lambda img : torchvision . transforms . functional . adjust_contrast ( img , contrast ) ) NEW_LINE DEDENT random . shuffle ( img_transforms ) NEW_LINE img_transforms = [ img_as_ubyte , torchvision . transforms . ToPILImage ( ) ] + img_transforms + [ np . array , img_as_float ] NEW_LINE with warnings . catch_warnings ( ) : NEW_LINE INDENT warnings . simplefilter ( \" ignore \" ) NEW_LINE jittered_clip = [ ] NEW_LINE for img in clip : NEW_LINE INDENT jittered_img = img NEW_LINE for func in img_transforms : NEW_LINE INDENT jittered_img = func ( jittered_img ) NEW_LINE DEDENT jittered_clip . append ( jittered_img . astype ( \\' float32\\' ) ) NEW_LINE DEDENT DEDENT DEDENT elif isinstance ( clip [ 0 ] , PIL . Image . Image ) : NEW_LINE INDENT brightness , contrast , saturation , hue = self . get_params ( self . brightness , self . contrast , self . saturation , self . hue ) NEW_LINE img_transforms = [ ] NEW_LINE if brightness is not None : NEW_LINE INDENT img_transforms . append ( lambda img : torchvision . transforms . functional . adjust_brightness ( img , brightness ) ) NEW_LINE DEDENT if saturation is not None : NEW_LINE INDENT img_transforms . append ( lambda img : torchvision . transforms . functional . adjust_saturation ( img , saturation ) ) NEW_LINE DEDENT if hue is not None : NEW_LINE INDENT img_transforms . append ( lambda img : torchvision . transforms . functional . adjust_hue ( img , hue ) ) NEW_LINE DEDENT if contrast is not None : NEW_LINE INDENT img_transforms . append ( lambda img : torchvision . transforms . functional . adjust_contrast ( img , contrast ) ) NEW_LINE DEDENT random . shuffle ( img_transforms ) NEW_LINE jittered_clip = [ ] NEW_LINE for img in clip : NEW_LINE INDENT for func in img_transforms : NEW_LINE INDENT jittered_img = func ( img ) NEW_LINE DEDENT jittered_clip . append ( jittered_img ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT raise TypeError ( \\' Expected ▁ numpy . ndarray ▁ or ▁ PIL . Image \\' + \\' but ▁ got ▁ list ▁ of ▁ { 0 } \\' . format ( type ( clip [ 0 ] ) ) ) NEW_LINE DEDENT return jittered_clip NEW_LINE DEDENT',), ('def __init__ ( self , consequent = False , number_of_frames = 2 ) : NEW_LINE INDENT self . consequent = consequent NEW_LINE self . number_of_frames = number_of_frames NEW_LINE DEDENT',), ('def __call__ ( self , clip ) : NEW_LINE INDENT frame_count = len ( clip ) NEW_LINE num_frames_to_select = self . number_of_frames NEW_LINE if self . consequent : NEW_LINE INDENT first_frame = np . random . choice ( max ( 1 , frame_count - num_frames_to_select + 1 ) , size = 1 ) [ 0 ] NEW_LINE selected = clip [ first_frame : ( first_frame + num_frames_to_select ) ] NEW_LINE DEDENT else : NEW_LINE INDENT selected_index = np . sort ( np . random . choice ( range ( frame_count ) , replace = True , size = num_frames_to_select ) ) NEW_LINE selected = clip [ selected_index ] NEW_LINE DEDENT return selected NEW_LINE DEDENT',), (\"def __call__ ( self , video ) : NEW_LINE INDENT source = np . array ( video [ : 1 ] , dtype = ' float32' ) NEW_LINE video = np . array ( video [ 1 : ] , dtype = ' float32' ) NEW_LINE return { ' video ' : video . transpose ( ( 3 , 0 , 1 , 2 ) ) , ' source ' : source . transpose ( ( 3 , 0 , 1 , 2 ) ) } NEW_LINE DEDENT\",), (\"def __call__ ( self , driving ) : NEW_LINE INDENT driving = np . array ( driving , dtype = ' float32' ) NEW_LINE return { ' video ' : driving . transpose ( ( 3 , 0 , 1 , 2 ) ) } NEW_LINE DEDENT\",), ('def __init__ ( self , resize_param = None , rotation_param = None , flip_param = None , crop_param = None , jitter_param = None ) : NEW_LINE INDENT self . transforms = [ ] NEW_LINE self . select = SelectRandomFrames ( ) NEW_LINE self . transforms . append ( self . select ) NEW_LINE if flip_param is not None : NEW_LINE INDENT self . transforms . append ( RandomFlip ( ** flip_param ) ) NEW_LINE DEDENT if rotation_param is not None : NEW_LINE INDENT self . transforms . append ( RandomRotation ( ** rotation_param ) ) NEW_LINE DEDENT if resize_param is not None : NEW_LINE INDENT self . transforms . append ( RandomResize ( ** resize_param ) ) NEW_LINE DEDENT if crop_param is not None : NEW_LINE INDENT self . transforms . append ( RandomCrop ( ** crop_param ) ) NEW_LINE DEDENT if jitter_param is not None : NEW_LINE INDENT self . transforms . append ( ColorJitter ( ** jitter_param ) ) NEW_LINE DEDENT self . transforms . append ( SplitSourceDriving ( ) ) NEW_LINE DEDENT',), ('def __call__ ( self , clip ) : NEW_LINE INDENT for t in self . transforms : NEW_LINE INDENT clip = t ( clip ) NEW_LINE DEDENT return clip NEW_LINE DEDENT',), (\"def split_kp ( kp_joined , detach = False ) : NEW_LINE INDENT if detach : NEW_LINE INDENT kp_video = { k : v [ : , 1 : ] . detach ( ) for k , v in kp_joined . items ( ) } NEW_LINE kp_appearance = { k : v [ : , : 1 ] . detach ( ) for k , v in kp_joined . items ( ) } NEW_LINE DEDENT else : NEW_LINE INDENT kp_video = { k : v [ : , 1 : ] for k , v in kp_joined . items ( ) } NEW_LINE kp_appearance = { k : v [ : , : 1 ] for k , v in kp_joined . items ( ) } NEW_LINE DEDENT return { ' kp _ driving ' : kp_video , ' kp _ source ' : kp_appearance } NEW_LINE DEDENT\",), (\"def train ( config , generator , discriminator , kp_detector , checkpoint , log_dir , dataset , device_ids ) : NEW_LINE INDENT train_params = config [ ' train _ params ' ] NEW_LINE optimizer_generator = torch . optim . Adam ( generator . parameters ( ) , lr = train_params [ ' lr ' ] , betas = ( 0.5 , 0.999 ) ) NEW_LINE optimizer_discriminator = torch . optim . Adam ( discriminator . parameters ( ) , lr = train_params [ ' lr ' ] , betas = ( 0.5 , 0.999 ) ) NEW_LINE optimizer_kp_detector = torch . optim . Adam ( kp_detector . parameters ( ) , lr = train_params [ ' lr ' ] , betas = ( 0.5 , 0.999 ) ) NEW_LINE if checkpoint is not None : NEW_LINE INDENT start_epoch , it = Logger . load_cpk ( checkpoint , generator , discriminator , kp_detector , optimizer_generator , optimizer_discriminator , optimizer_kp_detector ) NEW_LINE DEDENT else : NEW_LINE INDENT start_epoch = 0 NEW_LINE it = 0 NEW_LINE DEDENT scheduler_generator = MultiStepLR ( optimizer_generator , train_params [ ' epoch _ milestones ' ] , gamma = 0.1 , last_epoch = start_epoch - 1 ) NEW_LINE scheduler_discriminator = MultiStepLR ( optimizer_discriminator , train_params [ ' epoch _ milestones ' ] , gamma = 0.1 , last_epoch = start_epoch - 1 ) NEW_LINE scheduler_kp_detector = MultiStepLR ( optimizer_kp_detector , train_params [ ' epoch _ milestones ' ] , gamma = 0.1 , last_epoch = start_epoch - 1 ) NEW_LINE dataloader = DataLoader ( dataset , batch_size = train_params [ ' batch _ size ' ] , shuffle = True , num_workers = 4 , drop_last = True ) NEW_LINE generator_full = GeneratorFullModel ( kp_detector , generator , discriminator , train_params ) NEW_LINE discriminator_full = DiscriminatorFullModel ( kp_detector , generator , discriminator , train_params ) NEW_LINE generator_full_par = DataParallelWithCallback ( generator_full , device_ids = device_ids ) NEW_LINE discriminator_full_par = DataParallelWithCallback ( discriminator_full , device_ids = device_ids ) NEW_LINE with Logger ( log_dir = log_dir , visualizer_params = config [ ' visualizer _ params ' ] , ** train_params [ ' log _ params ' ] ) as logger : NEW_LINE INDENT for epoch in trange ( start_epoch , train_params [ ' num _ epochs ' ] ) : NEW_LINE INDENT for x in dataloader : NEW_LINE INDENT out = generator_full_par ( x ) NEW_LINE loss_values = out [ : - 2 ] NEW_LINE generated = out [ - 2 ] NEW_LINE kp_joined = out [ - 1 ] NEW_LINE loss_values = [ val . mean ( ) for val in loss_values ] NEW_LINE loss = sum ( loss_values ) NEW_LINE loss . backward ( retain_graph = not train_params [ ' detach _ kp _ discriminator ' ] ) NEW_LINE optimizer_generator . step ( ) NEW_LINE optimizer_generator . zero_grad ( ) NEW_LINE optimizer_discriminator . zero_grad ( ) NEW_LINE if train_params [ ' detach _ kp _ discriminator ' ] : NEW_LINE INDENT optimizer_kp_detector . step ( ) NEW_LINE optimizer_kp_detector . zero_grad ( ) NEW_LINE DEDENT generator_loss_values = [ val . detach ( ) . cpu ( ) . numpy ( ) for val in loss_values ] NEW_LINE loss_values = discriminator_full_par ( x , kp_joined , generated ) NEW_LINE loss_values = [ val . mean ( ) for val in loss_values ] NEW_LINE loss = sum ( loss_values ) NEW_LINE loss . backward ( ) NEW_LINE optimizer_discriminator . step ( ) NEW_LINE optimizer_discriminator . zero_grad ( ) NEW_LINE if not train_params [ ' detach _ kp _ discriminator ' ] : NEW_LINE INDENT optimizer_kp_detector . step ( ) NEW_LINE optimizer_kp_detector . zero_grad ( ) NEW_LINE DEDENT discriminator_loss_values = [ val . detach ( ) . cpu ( ) . numpy ( ) for val in loss_values ] NEW_LINE logger . log_iter ( it , names = generator_loss_names ( train_params [ ' loss _ weights ' ] ) + discriminator_loss_names ( ) , values = generator_loss_values + discriminator_loss_values , inp = x , out = generated ) NEW_LINE it += 1 NEW_LINE DEDENT scheduler_generator . step ( ) NEW_LINE scheduler_discriminator . step ( ) NEW_LINE scheduler_kp_detector . step ( ) NEW_LINE logger . log_epoch ( epoch , { ' generator ' : generator , ' discriminator ' : discriminator , ' kp _ detector ' : kp_detector , ' optimizer _ generator ' : optimizer_generator , ' optimizer _ discriminator ' : optimizer_discriminator , ' optimizer _ kp _ detector ' : optimizer_kp_detector } ) NEW_LINE DEDENT DEDENT DEDENT\",), ('def __init__ ( self , kp_extractor , generator , discriminator , train_params ) : NEW_LINE INDENT super ( GeneratorFullModel , self ) . __init__ ( ) NEW_LINE self . kp_extractor = kp_extractor NEW_LINE self . generator = generator NEW_LINE self . discriminator = discriminator NEW_LINE self . train_params = train_params NEW_LINE DEDENT',), (\"def forward ( self , x ) : NEW_LINE INDENT kp_joined = self . kp_extractor ( torch . cat ( [ x [ ' source ' ] , x [ ' video ' ] ] , dim = 2 ) ) NEW_LINE generated = self . generator ( x [ ' source ' ] , ** split_kp ( kp_joined , self . train_params [ ' detach _ kp _ generator ' ] ) ) NEW_LINE video_prediction = generated [ ' video _ prediction ' ] NEW_LINE video_deformed = generated [ ' video _ deformed ' ] NEW_LINE kp_dict = split_kp ( kp_joined , False ) NEW_LINE discriminator_maps_generated = self . discriminator ( video_prediction , ** kp_dict ) NEW_LINE discriminator_maps_real = self . discriminator ( x [ ' video ' ] , ** kp_dict ) NEW_LINE generated . update ( kp_dict ) NEW_LINE losses = generator_loss ( discriminator_maps_generated = discriminator_maps_generated , discriminator_maps_real = discriminator_maps_real , video_deformed = video_deformed , loss_weights = self . train_params [ ' loss _ weights ' ] ) NEW_LINE return tuple ( losses ) + ( generated , kp_joined ) NEW_LINE DEDENT\",), ('def __init__ ( self , kp_extractor , generator , discriminator , train_params ) : NEW_LINE INDENT super ( DiscriminatorFullModel , self ) . __init__ ( ) NEW_LINE self . kp_extractor = kp_extractor NEW_LINE self . generator = generator NEW_LINE self . discriminator = discriminator NEW_LINE self . train_params = train_params NEW_LINE DEDENT',), (\"def forward ( self , x , kp_joined , generated ) : NEW_LINE INDENT kp_dict = split_kp ( kp_joined , self . train_params [ ' detach _ kp _ discriminator ' ] ) NEW_LINE discriminator_maps_generated = self . discriminator ( generated [ ' video _ prediction ' ] . detach ( ) , ** kp_dict ) NEW_LINE discriminator_maps_real = self . discriminator ( x [ ' video ' ] , ** kp_dict ) NEW_LINE loss = discriminator_loss ( discriminator_maps_generated = discriminator_maps_generated , discriminator_maps_real = discriminator_maps_real , loss_weights = self . train_params [ ' loss _ weights ' ] ) NEW_LINE return loss NEW_LINE DEDENT\",), (\"def generate ( generator , appearance_image , kp_appearance , kp_video ) : NEW_LINE INDENT out = { ' video _ prediction ' : [ ] , ' video _ deformed ' : [ ] } NEW_LINE for i in range ( kp_video [ ' mean ' ] . shape [ 1 ] ) : NEW_LINE INDENT kp_target = { k : v [ : , i : ( i + 1 ) ] for k , v in kp_video . items ( ) } NEW_LINE kp_dict_part = { ' kp _ driving ' : kp_target , ' kp _ source ' : kp_appearance } NEW_LINE out_part = generator ( appearance_image , ** kp_dict_part ) NEW_LINE out [ ' video _ prediction ' ] . append ( out_part [ ' video _ prediction ' ] ) NEW_LINE out [ ' video _ deformed ' ] . append ( out_part [ ' video _ deformed ' ] ) NEW_LINE DEDENT out [ ' video _ prediction ' ] = torch . cat ( out [ ' video _ prediction ' ] , dim = 2 ) NEW_LINE out [ ' video _ deformed ' ] = torch . cat ( out [ ' video _ deformed ' ] , dim = 2 ) NEW_LINE out [ ' kp _ driving ' ] = kp_video NEW_LINE out [ ' kp _ source ' ] = kp_appearance NEW_LINE return out NEW_LINE DEDENT\",), ('def reconstruction ( config , generator , kp_detector , checkpoint , log_dir , dataset ) : NEW_LINE INDENT png_dir = os . path . join ( log_dir , \\' reconstruction / png \\' ) NEW_LINE log_dir = os . path . join ( log_dir , \\' reconstruction \\' ) NEW_LINE if checkpoint is not None : NEW_LINE INDENT Logger . load_cpk ( checkpoint , generator = generator , kp_detector = kp_detector ) NEW_LINE DEDENT else : NEW_LINE INDENT raise AttributeError ( \" Checkpoint ▁ should ▁ be ▁ specified ▁ for ▁ mode = \\' test \\' . \" ) NEW_LINE DEDENT dataloader = DataLoader ( dataset , batch_size = 1 , shuffle = False , num_workers = 1 ) NEW_LINE if not os . path . exists ( log_dir ) : NEW_LINE INDENT os . makedirs ( log_dir ) NEW_LINE DEDENT if not os . path . exists ( png_dir ) : NEW_LINE INDENT os . makedirs ( png_dir ) NEW_LINE DEDENT loss_list = [ ] NEW_LINE generator = DataParallelWithCallback ( generator ) NEW_LINE kp_detector = DataParallelWithCallback ( kp_detector ) NEW_LINE generator . eval ( ) NEW_LINE kp_detector . eval ( ) NEW_LINE cat_dict = lambda l , dim : { k : torch . cat ( [ v [ k ] for v in l ] , dim = dim ) for k in l [ 0 ] } NEW_LINE for it , x in tqdm ( enumerate ( dataloader ) ) : NEW_LINE INDENT if config [ \\' reconstruction _ params \\' ] [ \\' num _ videos \\' ] is not None : NEW_LINE INDENT if it > config [ \\' reconstruction _ params \\' ] [ \\' num _ videos \\' ] : NEW_LINE INDENT break NEW_LINE DEDENT DEDENT with torch . no_grad ( ) : NEW_LINE INDENT kp_appearance = kp_detector ( x [ \\' video \\' ] [ : , : , : 1 ] ) NEW_LINE d = x [ \\' video \\' ] . shape [ 2 ] NEW_LINE kp_video = cat_dict ( [ kp_detector ( x [ \\' video \\' ] [ : , : , i : ( i + 1 ) ] ) for i in range ( d ) ] , dim = 1 ) NEW_LINE out = generate ( generator , appearance_image = x [ \\' video \\' ] [ : , : , : 1 ] , kp_appearance = kp_appearance , kp_video = kp_video ) NEW_LINE x [ \\' source \\' ] = x [ \\' video \\' ] [ : , : , : 1 ] NEW_LINE out_video_batch = out [ \\' video _ prediction \\' ] . data . cpu ( ) . numpy ( ) NEW_LINE out_video_batch = np . concatenate ( np . transpose ( out_video_batch , [ 0 , 2 , 3 , 4 , 1 ] ) [ 0 ] , axis = 1 ) NEW_LINE imageio . imsave ( os . path . join ( png_dir , x [ \\' name \\' ] [ 0 ] + \\' . png \\' ) , ( 255 * out_video_batch ) . astype ( np . uint8 ) ) NEW_LINE image = Visualizer ( ** config [ \\' visualizer _ params \\' ] ) . visualize_reconstruction ( x , out ) NEW_LINE image_name = x [ \\' name \\' ] [ 0 ] + config [ \\' reconstruction _ params \\' ] [ \\' format \\' ] NEW_LINE imageio . mimsave ( os . path . join ( log_dir , image_name ) , image ) NEW_LINE loss = reconstruction_loss ( out [ \\' video _ prediction \\' ] . cpu ( ) , x [ \\' video \\' ] . cpu ( ) , 1 ) NEW_LINE loss_list . append ( loss . data . cpu ( ) . numpy ( ) ) NEW_LINE del x , kp_video , kp_appearance , out , loss NEW_LINE DEDENT DEDENT print ( \" Reconstruction ▁ loss : ▁ % s \" % np . mean ( loss_list ) ) NEW_LINE DEDENT',), ('def get_files_by_file_size ( filepaths , dir , reverse = False ) : NEW_LINE INDENT for i in range ( len ( filepaths ) ) : NEW_LINE INDENT filepaths [ i ] = ( filepaths [ i ] , os . path . getsize ( os . path . join ( dir , filepaths [ i ] ) ) ) NEW_LINE DEDENT filepaths . sort ( key = lambda filename : filename [ 1 ] , reverse = reverse ) NEW_LINE for i in range ( len ( filepaths ) ) : NEW_LINE INDENT filepaths [ i ] = filepaths [ i ] [ 0 ] NEW_LINE DEDENT return filepaths NEW_LINE DEDENT',), ('def onmouse ( event , x , y , flags , param ) : NEW_LINE INDENT global drawing , mask NEW_LINE thickness = thickness_area if value == FILL_AREA else thickness_color NEW_LINE if event == cv . EVENT_LBUTTONDOWN : NEW_LINE INDENT drawing = True NEW_LINE cv . circle ( mask , ( x , y ) , thickness , True , - 1 ) NEW_LINE DEDENT elif event == cv . EVENT_MOUSEMOVE : NEW_LINE INDENT if drawing == True : NEW_LINE INDENT cv . circle ( mask , ( x , y ) , thickness , True , - 1 ) NEW_LINE DEDENT DEDENT elif event == cv . EVENT_LBUTTONUP : NEW_LINE INDENT if drawing == True : NEW_LINE INDENT drawing = False NEW_LINE cv . circle ( mask , ( x , y ) , thickness , True , - 1 ) NEW_LINE DEDENT DEDENT DEDENT',), ('def convert_gif_to_frames ( gif ) : NEW_LINE INDENT frame_num = 0 NEW_LINE frame_list = [ ] NEW_LINE while True : NEW_LINE INDENT try : NEW_LINE INDENT okay , frame = gif . read ( ) NEW_LINE if not okay : NEW_LINE INDENT break NEW_LINE DEDENT frame = cv . resize ( frame , image_shape , interpolation = cv . INTER_NEAREST ) NEW_LINE frame_list . append ( frame ) NEW_LINE frame_num += 1 NEW_LINE DEDENT except KeyboardInterrupt : NEW_LINE INDENT break NEW_LINE DEDENT DEDENT return frame_list NEW_LINE DEDENT',), ('def color_as_num ( val ) : NEW_LINE INDENT val = val . astype ( np . uint64 ) NEW_LINE return val [ ... , 0 ] + 256 * val [ ... , 1 ] + ( 256 * 256 ) * val [ ... , 2 ] NEW_LINE DEDENT',), ('def process ( video , filename , outdir , skipdir ) : NEW_LINE INDENT global mask , value , drawing NEW_LINE video2 = video . copy ( ) NEW_LINE current_it = 0 NEW_LINE cv . namedWindow ( \\' input \\' ) NEW_LINE cv . setMouseCallback ( \\' input \\' , onmouse ) NEW_LINE cv . moveWindow ( \\' input \\' , video . shape [ 2 ] + 10 , 90 ) NEW_LINE video_as_num = color_as_num ( video ) NEW_LINE original_fillmask = video_as_num != ( 256 ** 3 - 1 ) NEW_LINE paused = False NEW_LINE while ( 1 ) : NEW_LINE INDENT current_frame = current_it // 25 NEW_LINE cv . imshow ( \\' input \\' , video [ current_frame % video . shape [ 0 ] ] ) NEW_LINE k = cv . waitKey ( 1 ) NEW_LINE if not paused : NEW_LINE INDENT current_it += 1 NEW_LINE DEDENT if k == 27 : NEW_LINE INDENT break NEW_LINE DEDENT elif k == ord ( \\'0\\' ) : NEW_LINE INDENT print ( \" ▁ Mark ▁ region ▁ to ▁ fill ▁ with ▁ left ▁ mouse ▁ button ▁ \\\\n \" ) NEW_LINE value = FILL_AREA NEW_LINE mask = np . zeros ( video . shape [ 1 : 3 ] , dtype = np . uint8 ) NEW_LINE DEDENT elif k == ord ( \\'1\\' ) : NEW_LINE INDENT print ( \" Mark ▁ colors ▁ to ▁ fill ▁ with ▁ left ▁ mouse ▁ button ▁ \\\\n \" ) NEW_LINE value = FILL_COLOR NEW_LINE mask = np . zeros ( video . shape [ 1 : 3 ] , dtype = np . uint8 ) NEW_LINE DEDENT elif k == ord ( \\'2\\' ) : NEW_LINE INDENT print ( \" Mark ▁ area ▁ to ▁ fill ▁ ( in ▁ ▁ all ▁ frames ) ▁ with ▁ left ▁ mouse ▁ button ▁ \\\\n \" ) NEW_LINE value = FILL_ALL_COLOR NEW_LINE mask = np . zeros ( video . shape [ 1 : 3 ] , dtype = np . uint8 ) NEW_LINE DEDENT elif k == ord ( \\'3\\' ) : NEW_LINE INDENT print ( \" Mark ▁ connected ▁ area ▁ to ▁ fill ▁ with ▁ left ▁ mouse ▁ button ▁ \\\\n \" ) NEW_LINE value = FILL_CONNECTED NEW_LINE mask = np . zeros ( video . shape [ 1 : 3 ] , dtype = np . uint8 ) NEW_LINE DEDENT elif k == ord ( \\' f \\' ) : NEW_LINE INDENT color = np . random . randint ( 255 , size = 3 ) NEW_LINE video_as_num = color_as_num ( video ) NEW_LINE fillmask = video_as_num != ( 256 ** 3 - 1 ) NEW_LINE for i in range ( len ( video ) ) : NEW_LINE INDENT video [ i , scipy . ndimage . morphology . binary_fill_holes ( fillmask [ i ] ) ] = color NEW_LINE DEDENT video [ original_fillmask ] = ( 0 , 0 , 0 ) NEW_LINE DEDENT elif k == ord ( \\' d \\' ) : NEW_LINE INDENT video_as_num = color_as_num ( video ) NEW_LINE fillmask = video_as_num != ( 256 ** 3 - 1 ) NEW_LINE for i in range ( len ( video ) ) : NEW_LINE INDENT video [ i , binary_dilation ( fillmask [ i ] ) ] = ( 0 , 0 , 0 ) NEW_LINE DEDENT DEDENT elif k == ord ( \\' e \\' ) : NEW_LINE INDENT video_as_num = color_as_num ( video ) NEW_LINE fillmask = video_as_num != ( 256 ** 3 - 1 ) NEW_LINE for i in range ( len ( video ) ) : NEW_LINE INDENT video [ i , np . logical_not ( binary_erosion ( fillmask [ i ] ) ) ] = ( 255 , 255 , 255 ) NEW_LINE DEDENT DEDENT elif k == ord ( \\' i \\' ) : NEW_LINE INDENT video = 255 - video NEW_LINE DEDENT elif k == ord ( \\' p \\' ) : NEW_LINE INDENT video = np . array ( [ img_as_ubyte ( np . concatenate ( [ median ( frame [ ... , i ] , disk ( 1 ) ) [ ... , np . newaxis ] for i in range ( 3 ) ] , axis = - 1 ) ) for frame in video ] ) NEW_LINE DEDENT elif k == ord ( \\' l \\' ) : NEW_LINE INDENT paused = not paused NEW_LINE DEDENT elif k == ord ( \\' n \\' ) : NEW_LINE INDENT mimsave ( os . path . join ( outdir , filename ) , video [ ... , : : - 1 ] ) NEW_LINE break NEW_LINE DEDENT elif k == ord ( \\' s \\' ) : NEW_LINE INDENT mimsave ( os . path . join ( skipdir , filename ) , video2 [ ... , : : - 1 ] ) NEW_LINE break NEW_LINE DEDENT elif k == ord ( \\' r \\' ) : NEW_LINE INDENT print ( \" resetting ▁ \\\\n \" ) NEW_LINE drawing = False NEW_LINE video = video2 . copy ( ) NEW_LINE mask = np . zeros ( video . shape [ 1 : 3 ] , dtype = np . uint8 ) NEW_LINE DEDENT if mask . sum ( ) == 0 : NEW_LINE INDENT continue NEW_LINE DEDENT if value == FILL_AREA : NEW_LINE INDENT video [ : , mask . astype ( bool ) ] = ( 255 , 255 , 255 ) NEW_LINE mask = np . zeros ( video . shape [ 1 : 3 ] , dtype = np . uint8 ) NEW_LINE DEDENT elif value == FILL_COLOR : NEW_LINE INDENT colors = video [ current_frame % video . shape [ 0 ] ] [ mask . astype ( bool ) ] NEW_LINE colors = color_as_num ( val = colors ) . reshape ( ( - 1 , ) ) NEW_LINE colors = np . unique ( colors ) NEW_LINE video_as_num = color_as_num ( video ) NEW_LINE for color in colors : NEW_LINE INDENT video [ video_as_num == color ] = ( 255 , 255 , 255 ) NEW_LINE DEDENT mask = np . zeros ( video . shape [ 1 : 3 ] , dtype = np . uint8 ) NEW_LINE DEDENT elif value == FILL_ALL_COLOR : NEW_LINE INDENT colors = video [ : , mask . astype ( bool ) ] NEW_LINE colors = color_as_num ( val = colors ) . reshape ( ( - 1 , ) ) NEW_LINE colors = np . unique ( colors ) NEW_LINE video_as_num = color_as_num ( video ) NEW_LINE for color in colors : NEW_LINE INDENT video [ video_as_num == color ] = ( 255 , 255 , 255 ) NEW_LINE DEDENT mask = np . zeros ( video . shape [ 1 : 3 ] , dtype = np . uint8 ) NEW_LINE DEDENT elif value == FILL_CONNECTED : NEW_LINE INDENT color = np . random . randint ( 255 , size = 3 ) NEW_LINE video_as_num = color_as_num ( video ) NEW_LINE fillmask = video_as_num != ( 256 ** 3 - 1 ) NEW_LINE for i in range ( len ( video ) ) : NEW_LINE INDENT labels = label ( fillmask [ i ] ) NEW_LINE index = labels [ mask ] NEW_LINE video [ i , labels == np . unique ( index ) ] = color NEW_LINE DEDENT video [ original_fillmask ] = ( 0 , 0 , 0 ) NEW_LINE mask = np . zeros ( video . shape [ 1 : 3 ] , dtype = np . uint8 ) NEW_LINE DEDENT DEDENT cv . destroyAllWindows ( ) NEW_LINE DEDENT',), ('def __init__ ( self ) : NEW_LINE INDENT self . _result = None NEW_LINE self . _lock = threading . Lock ( ) NEW_LINE self . _cond = threading . Condition ( self . _lock ) NEW_LINE DEDENT',), (\"def put ( self , result ) : NEW_LINE INDENT with self . _lock : NEW_LINE INDENT assert self . _result is None , ' Previous ▁ result ▁ has\\\\ ' t ▁ been ▁ fetched . ' NEW_LINE self . _result = result NEW_LINE self . _cond . notify ( ) NEW_LINE DEDENT DEDENT\",), ('def get ( self ) : NEW_LINE INDENT with self . _lock : NEW_LINE INDENT if self . _result is None : NEW_LINE INDENT self . _cond . wait ( ) NEW_LINE DEDENT res = self . _result NEW_LINE self . _result = None NEW_LINE return res NEW_LINE DEDENT DEDENT',), ('def run_slave ( self , msg ) : NEW_LINE INDENT self . queue . put ( ( self . identifier , msg ) ) NEW_LINE ret = self . result . get ( ) NEW_LINE self . queue . put ( True ) NEW_LINE return ret NEW_LINE DEDENT',), ('def __init__ ( self , master_callback ) : NEW_LINE INDENT self . _master_callback = master_callback NEW_LINE self . _queue = queue . Queue ( ) NEW_LINE self . _registry = collections . OrderedDict ( ) NEW_LINE self . _activated = False NEW_LINE DEDENT',), (\"def __getstate__ ( self ) : NEW_LINE INDENT return { ' master _ callback ' : self . _master_callback } NEW_LINE DEDENT\",), (\"def __setstate__ ( self , state ) : NEW_LINE INDENT self . __init__ ( state [ ' master _ callback ' ] ) NEW_LINE DEDENT\",), (\"def register_slave ( self , identifier ) : NEW_LINE INDENT if self . _activated : NEW_LINE INDENT assert self . _queue . empty ( ) , ' Queue ▁ is ▁ not ▁ clean ▁ before ▁ next ▁ initialization . ' NEW_LINE self . _activated = False NEW_LINE self . _registry . clear ( ) NEW_LINE DEDENT future = FutureResult ( ) NEW_LINE self . _registry [ identifier ] = _MasterRegistry ( future ) NEW_LINE return SlavePipe ( identifier , self . _queue , future ) NEW_LINE DEDENT\",), (\"def run_master ( self , master_msg ) : NEW_LINE INDENT self . _activated = True NEW_LINE intermediates = [ ( 0 , master_msg ) ] NEW_LINE for i in range ( self . nr_slaves ) : NEW_LINE INDENT intermediates . append ( self . _queue . get ( ) ) NEW_LINE DEDENT results = self . _master_callback ( intermediates ) NEW_LINE assert results [ 0 ] [ 0 ] == 0 , ' The ▁ first ▁ result ▁ should ▁ belongs ▁ to ▁ the ▁ master . ' NEW_LINE for i , res in results : NEW_LINE INDENT if i == 0 : NEW_LINE INDENT continue NEW_LINE DEDENT self . _registry [ i ] . result . put ( res ) NEW_LINE DEDENT for i in range ( self . nr_slaves ) : NEW_LINE INDENT assert self . _queue . get ( ) is True NEW_LINE DEDENT return results [ 0 ] [ 1 ] NEW_LINE DEDENT\",), ('def nr_slaves ( self ) : NEW_LINE INDENT return len ( self . _registry ) NEW_LINE DEDENT',), ('def _sum_ft ( tensor ) : NEW_LINE INDENT return tensor . sum ( dim = 0 ) . sum ( dim = - 1 ) NEW_LINE DEDENT',), ('def _unsqueeze_ft ( tensor ) : NEW_LINE INDENT return tensor . unsqueeze ( 0 ) . unsqueeze ( - 1 ) NEW_LINE DEDENT',), ('def __init__ ( self , num_features , eps = 1e-5 , momentum = 0.1 , affine = True ) : NEW_LINE INDENT super ( _SynchronizedBatchNorm , self ) . __init__ ( num_features , eps = eps , momentum = momentum , affine = affine ) NEW_LINE self . _sync_master = SyncMaster ( self . _data_parallel_master ) NEW_LINE self . _is_parallel = False NEW_LINE self . _parallel_id = None NEW_LINE self . _slave_pipe = None NEW_LINE DEDENT',), ('def forward ( self , input ) : NEW_LINE INDENT if not ( self . _is_parallel and self . training ) : NEW_LINE INDENT return F . batch_norm ( input , self . running_mean , self . running_var , self . weight , self . bias , self . training , self . momentum , self . eps ) NEW_LINE DEDENT input_shape = input . size ( ) NEW_LINE input = input . view ( input . size ( 0 ) , self . num_features , - 1 ) NEW_LINE sum_size = input . size ( 0 ) * input . size ( 2 ) NEW_LINE input_sum = _sum_ft ( input ) NEW_LINE input_ssum = _sum_ft ( input ** 2 ) NEW_LINE if self . _parallel_id == 0 : NEW_LINE INDENT mean , inv_std = self . _sync_master . run_master ( _ChildMessage ( input_sum , input_ssum , sum_size ) ) NEW_LINE DEDENT else : NEW_LINE INDENT mean , inv_std = self . _slave_pipe . run_slave ( _ChildMessage ( input_sum , input_ssum , sum_size ) ) NEW_LINE DEDENT if self . affine : NEW_LINE INDENT output = ( input - _unsqueeze_ft ( mean ) ) * _unsqueeze_ft ( inv_std * self . weight ) + _unsqueeze_ft ( self . bias ) NEW_LINE DEDENT else : NEW_LINE INDENT output = ( input - _unsqueeze_ft ( mean ) ) * _unsqueeze_ft ( inv_std ) NEW_LINE DEDENT return output . view ( input_shape ) NEW_LINE DEDENT',), ('def __data_parallel_replicate__ ( self , ctx , copy_id ) : NEW_LINE INDENT self . _is_parallel = True NEW_LINE self . _parallel_id = copy_id NEW_LINE if self . _parallel_id == 0 : NEW_LINE INDENT ctx . sync_master = self . _sync_master NEW_LINE DEDENT else : NEW_LINE INDENT self . _slave_pipe = ctx . sync_master . register_slave ( copy_id ) NEW_LINE DEDENT DEDENT',), ('def _data_parallel_master ( self , intermediates ) : NEW_LINE INDENT intermediates = sorted ( intermediates , key = lambda i : i [ 1 ] . sum . get_device ( ) ) NEW_LINE to_reduce = [ i [ 1 ] [ : 2 ] for i in intermediates ] NEW_LINE to_reduce = [ j for i in to_reduce for j in i ] NEW_LINE target_gpus = [ i [ 1 ] . sum . get_device ( ) for i in intermediates ] NEW_LINE sum_size = sum ( [ i [ 1 ] . sum_size for i in intermediates ] ) NEW_LINE sum_ , ssum = ReduceAddCoalesced . apply ( target_gpus [ 0 ] , 2 , * to_reduce ) NEW_LINE mean , inv_std = self . _compute_mean_std ( sum_ , ssum , sum_size ) NEW_LINE broadcasted = Broadcast . apply ( target_gpus , mean , inv_std ) NEW_LINE outputs = [ ] NEW_LINE for i , rec in enumerate ( intermediates ) : NEW_LINE INDENT outputs . append ( ( rec [ 0 ] , _MasterMessage ( * broadcasted [ i * 2 : i * 2 + 2 ] ) ) ) NEW_LINE DEDENT return outputs NEW_LINE DEDENT',), (\"def _compute_mean_std ( self , sum_ , ssum , size ) : NEW_LINE INDENT assert size > 1 , ' BatchNorm ▁ computes ▁ unbiased ▁ standard - deviation , ▁ which ▁ requires ▁ size ▁ > ▁ 1 . ' NEW_LINE mean = sum_ / size NEW_LINE sumvar = ssum - sum_ * mean NEW_LINE unbias_var = sumvar / ( size - 1 ) NEW_LINE bias_var = sumvar / size NEW_LINE self . running_mean = ( 1 - self . momentum ) * self . running_mean + self . momentum * mean . data NEW_LINE self . running_var = ( 1 - self . momentum ) * self . running_var + self . momentum * unbias_var . data NEW_LINE return mean , bias_var . clamp ( self . eps ) ** - 0.5 NEW_LINE DEDENT\",), (\"def _check_input_dim ( self , input ) : NEW_LINE INDENT if input . dim ( ) != 2 and input . dim ( ) != 3 : NEW_LINE INDENT raise ValueError ( ' expected ▁ 2D ▁ or ▁ 3D ▁ input ▁ ( got ▁ { } D ▁ input ) ' . format ( input . dim ( ) ) ) NEW_LINE DEDENT super ( SynchronizedBatchNorm1d , self ) . _check_input_dim ( input ) NEW_LINE DEDENT\",), (\"def _check_input_dim ( self , input ) : NEW_LINE INDENT if input . dim ( ) != 4 : NEW_LINE INDENT raise ValueError ( ' expected ▁ 4D ▁ input ▁ ( got ▁ { } D ▁ input ) ' . format ( input . dim ( ) ) ) NEW_LINE DEDENT super ( SynchronizedBatchNorm2d , self ) . _check_input_dim ( input ) NEW_LINE DEDENT\",), (\"def _check_input_dim ( self , input ) : NEW_LINE INDENT if input . dim ( ) != 5 : NEW_LINE INDENT raise ValueError ( ' expected ▁ 5D ▁ input ▁ ( got ▁ { } D ▁ input ) ' . format ( input . dim ( ) ) ) NEW_LINE DEDENT super ( SynchronizedBatchNorm3d , self ) . _check_input_dim ( input ) NEW_LINE DEDENT\",), (\"def execute_replication_callbacks ( modules ) : NEW_LINE INDENT master_copy = modules [ 0 ] NEW_LINE nr_modules = len ( list ( master_copy . modules ( ) ) ) NEW_LINE ctxs = [ CallbackContext ( ) for _ in range ( nr_modules ) ] NEW_LINE for i , module in enumerate ( modules ) : NEW_LINE INDENT for j , m in enumerate ( module . modules ( ) ) : NEW_LINE INDENT if hasattr ( m , ' _ _ data _ parallel _ replicate _ _ ' ) : NEW_LINE INDENT m . __data_parallel_replicate__ ( ctxs [ j ] , i ) NEW_LINE DEDENT DEDENT DEDENT DEDENT\",), ('def patch_replication_callback ( data_parallel ) : NEW_LINE INDENT assert isinstance ( data_parallel , DataParallel ) NEW_LINE old_replicate = data_parallel . replicate NEW_LINE @ functools . wraps ( old_replicate ) NEW_LINE def new_replicate ( module , device_ids ) : NEW_LINE INDENT modules = old_replicate ( module , device_ids ) NEW_LINE execute_replication_callbacks ( modules ) NEW_LINE return modules NEW_LINE DEDENT data_parallel . replicate = new_replicate NEW_LINE DEDENT',), ('def replicate ( self , module , device_ids ) : NEW_LINE INDENT modules = super ( DataParallelWithCallback , self ) . replicate ( module , device_ids ) NEW_LINE execute_replication_callbacks ( modules ) NEW_LINE return modules NEW_LINE DEDENT',), ('def as_numpy ( v ) : NEW_LINE INDENT if isinstance ( v , Variable ) : NEW_LINE INDENT v = v . data NEW_LINE DEDENT return v . cpu ( ) . numpy ( ) NEW_LINE DEDENT',), (\"def assertTensorClose ( self , a , b , atol = 1e-3 , rtol = 1e-3 ) : NEW_LINE INDENT npa , npb = as_numpy ( a ) , as_numpy ( b ) NEW_LINE self . assertTrue ( np . allclose ( npa , npb , atol = atol ) , ' Tensor ▁ close ▁ check ▁ failed \\\\n { } \\\\n { } \\\\n adiff = { } , ▁ rdiff = { } ' . format ( a , b , np . abs ( npa - npb ) . max ( ) , np . abs ( ( npa - npb ) / np . fmax ( npa , 1e-5 ) ) . max ( ) ) ) NEW_LINE DEDENT\",), ('def __init__ ( self , in_features , out_features , norm = False , kernel_size = 4 ) : NEW_LINE INDENT super ( DownBlock3D , self ) . __init__ ( ) NEW_LINE ka = kernel_size // 2 NEW_LINE kb = ka - 1 if kernel_size % 2 == 0 else ka NEW_LINE self . conv = nn . Conv3d ( in_channels = in_features , out_channels = out_features , kernel_size = ( 1 , kernel_size , kernel_size ) ) NEW_LINE if norm : NEW_LINE INDENT self . norm = nn . InstanceNorm3d ( out_features , affine = True ) NEW_LINE DEDENT else : NEW_LINE INDENT self . norm = None NEW_LINE DEDENT DEDENT',), ('def forward ( self , x ) : NEW_LINE INDENT out = x NEW_LINE out = self . conv ( out ) NEW_LINE if self . norm : NEW_LINE INDENT out = self . norm ( out ) NEW_LINE DEDENT out = F . leaky_relu ( out , 0.2 ) NEW_LINE out = F . avg_pool3d ( out , ( 1 , 2 , 2 ) ) NEW_LINE return out NEW_LINE DEDENT',), ('def __init__ ( self , num_channels = 3 , num_kp = 10 , kp_variance = 0.01 , scale_factor = 1 , block_expansion = 64 , num_blocks = 4 , max_features = 512 , kp_embedding_params = None ) : NEW_LINE INDENT super ( Discriminator , self ) . __init__ ( ) NEW_LINE if kp_embedding_params is not None : NEW_LINE INDENT self . kp_embedding = MovementEmbeddingModule ( num_kp = num_kp , kp_variance = kp_variance , num_channels = num_channels , ** kp_embedding_params ) NEW_LINE embedding_channels = self . kp_embedding . out_channels NEW_LINE DEDENT else : NEW_LINE INDENT self . kp_embedding = None NEW_LINE embedding_channels = 0 NEW_LINE DEDENT down_blocks = [ ] NEW_LINE for i in range ( num_blocks ) : NEW_LINE INDENT down_blocks . append ( DownBlock3D ( num_channels + embedding_channels if i == 0 else min ( max_features , block_expansion * ( 2 ** i ) ) , min ( max_features , block_expansion * ( 2 ** ( i + 1 ) ) ) , norm = ( i != 0 ) , kernel_size = 4 ) ) NEW_LINE DEDENT self . down_blocks = nn . ModuleList ( down_blocks ) NEW_LINE self . conv = nn . Conv3d ( self . down_blocks [ - 1 ] . conv . out_channels , out_channels = 1 , kernel_size = 1 ) NEW_LINE self . scale_factor = scale_factor NEW_LINE DEDENT',), ('def forward ( self , x , kp_driving , kp_source ) : NEW_LINE INDENT out_maps = [ x ] NEW_LINE if self . scale_factor != 1 : NEW_LINE INDENT x = F . interpolate ( x , scale_factor = ( 1 , self . scale_factor , self . scale_factor ) ) NEW_LINE DEDENT if self . kp_embedding : NEW_LINE INDENT heatmap = self . kp_embedding ( x , kp_driving , kp_source ) NEW_LINE out = torch . cat ( [ x , heatmap ] , dim = 1 ) NEW_LINE DEDENT else : NEW_LINE INDENT out = x NEW_LINE DEDENT for down_block in self . down_blocks : NEW_LINE INDENT out_maps . append ( down_block ( out ) ) NEW_LINE out = out_maps [ - 1 ] NEW_LINE DEDENT out = self . conv ( out ) NEW_LINE out_maps . append ( out ) NEW_LINE return out_maps NEW_LINE DEDENT',), ('def compute_image_gradient ( image , padding = 0 ) : NEW_LINE INDENT bs , c , h , w = image . shape NEW_LINE sobel_x = torch . from_numpy ( np . array ( [ [ 1 , 0 , - 1 ] , [ 2 , 0 , - 2 ] , [ 1 , 0 , - 1 ] ] ) ) . type ( image . type ( ) ) NEW_LINE filter = sobel_x . unsqueeze ( 0 ) . repeat ( c , 1 , 1 , 1 ) NEW_LINE grad_x = F . conv2d ( image , filter , groups = c , padding = padding ) NEW_LINE grad_x = grad_x NEW_LINE sobel_y = torch . from_numpy ( np . array ( [ [ 1 , 2 , 1 ] , [ 0 , 0 , 0 ] , [ - 1 , - 2 , - 1 ] ] ) ) . type ( image . type ( ) ) NEW_LINE filter = sobel_y . unsqueeze ( 0 ) . repeat ( c , 1 , 1 , 1 ) NEW_LINE grad_y = F . conv2d ( image , filter , groups = c , padding = padding ) NEW_LINE grad_y = grad_y NEW_LINE return torch . cat ( [ grad_x , grad_y ] , dim = 1 ) NEW_LINE DEDENT',), ('def make_coordinate_grid ( spatial_size , type ) : NEW_LINE INDENT h , w = spatial_size NEW_LINE x = torch . arange ( w ) . type ( type ) NEW_LINE y = torch . arange ( h ) . type ( type ) NEW_LINE x = ( 2 * ( x / ( w - 1 ) ) - 1 ) NEW_LINE y = ( 2 * ( y / ( h - 1 ) ) - 1 ) NEW_LINE yy = y . view ( - 1 , 1 ) . repeat ( 1 , w ) NEW_LINE xx = x . view ( 1 , - 1 ) . repeat ( h , 1 ) NEW_LINE meshed = torch . cat ( [ xx . unsqueeze_ ( 2 ) , yy . unsqueeze_ ( 2 ) ] , 2 ) NEW_LINE return meshed NEW_LINE DEDENT',), ('def matrix_inverse ( batch_of_matrix , eps = 0 ) : NEW_LINE INDENT if eps != 0 : NEW_LINE INDENT init_shape = batch_of_matrix . shape NEW_LINE a = batch_of_matrix [ ... , 0 , 0 ] . unsqueeze ( - 1 ) NEW_LINE b = batch_of_matrix [ ... , 0 , 1 ] . unsqueeze ( - 1 ) NEW_LINE c = batch_of_matrix [ ... , 1 , 0 ] . unsqueeze ( - 1 ) NEW_LINE d = batch_of_matrix [ ... , 1 , 1 ] . unsqueeze ( - 1 ) NEW_LINE det = a * d - b * c NEW_LINE out = torch . cat ( [ d , - b , - c , a ] , dim = - 1 ) NEW_LINE eps = torch . tensor ( eps ) . type ( out . type ( ) ) NEW_LINE out /= det . max ( eps ) NEW_LINE return out . view ( init_shape ) NEW_LINE DEDENT else : NEW_LINE INDENT b_mat = batch_of_matrix NEW_LINE eye = b_mat . new_ones ( b_mat . size ( - 1 ) ) . diag ( ) . expand_as ( b_mat ) NEW_LINE b_inv , _ = torch . gesv ( eye , b_mat ) NEW_LINE return b_inv NEW_LINE DEDENT DEDENT',), ('def matrix_det ( batch_of_matrix ) : NEW_LINE INDENT a = batch_of_matrix [ ... , 0 , 0 ] . unsqueeze ( - 1 ) NEW_LINE b = batch_of_matrix [ ... , 0 , 1 ] . unsqueeze ( - 1 ) NEW_LINE c = batch_of_matrix [ ... , 1 , 0 ] . unsqueeze ( - 1 ) NEW_LINE d = batch_of_matrix [ ... , 1 , 1 ] . unsqueeze ( - 1 ) NEW_LINE det = a * d - b * c NEW_LINE return det NEW_LINE DEDENT',), ('def matrix_trace ( batch_of_matrix ) : NEW_LINE INDENT a = batch_of_matrix [ ... , 0 , 0 ] . unsqueeze ( - 1 ) NEW_LINE d = batch_of_matrix [ ... , 1 , 1 ] . unsqueeze ( - 1 ) NEW_LINE return a + d NEW_LINE DEDENT',), ('def smallest_singular ( batch_of_matrix ) : NEW_LINE INDENT a = batch_of_matrix [ ... , 0 , 0 ] . unsqueeze ( - 1 ) NEW_LINE b = batch_of_matrix [ ... , 0 , 1 ] . unsqueeze ( - 1 ) NEW_LINE c = batch_of_matrix [ ... , 1 , 0 ] . unsqueeze ( - 1 ) NEW_LINE d = batch_of_matrix [ ... , 1 , 1 ] . unsqueeze ( - 1 ) NEW_LINE s1 = a ** 2 + b ** 2 + c ** 2 + d ** 2 NEW_LINE s2 = ( a ** 2 + b ** 2 - c ** 2 - d ** 2 ) ** 2 NEW_LINE s2 = torch . sqrt ( s2 + 4 * ( a * c + b * d ) ** 2 ) NEW_LINE norm = torch . sqrt ( ( s1 - s2 ) / 2 ) NEW_LINE return norm NEW_LINE DEDENT',), ('def __init__ ( self , in_features , kernel_size , padding ) : NEW_LINE INDENT super ( ResBlock3D , self ) . __init__ ( ) NEW_LINE self . conv1 = nn . Conv3d ( in_channels = in_features , out_channels = in_features , kernel_size = kernel_size , padding = padding ) NEW_LINE self . conv2 = nn . Conv3d ( in_channels = in_features , out_channels = in_features , kernel_size = kernel_size , padding = padding ) NEW_LINE self . norm1 = BatchNorm3d ( in_features , affine = True ) NEW_LINE self . norm2 = BatchNorm3d ( in_features , affine = True ) NEW_LINE DEDENT',), ('def forward ( self , x ) : NEW_LINE INDENT out = x NEW_LINE out = self . norm1 ( x ) NEW_LINE out = F . relu ( out ) NEW_LINE out = self . conv1 ( out ) NEW_LINE out = self . norm2 ( out ) NEW_LINE out = F . relu ( out ) NEW_LINE out = self . conv2 ( out ) NEW_LINE out += x NEW_LINE return out NEW_LINE DEDENT',), ('def __init__ ( self , in_features , out_features , kernel_size = 3 , padding = 1 ) : NEW_LINE INDENT super ( UpBlock3D , self ) . __init__ ( ) NEW_LINE self . conv = nn . Conv3d ( in_channels = in_features , out_channels = out_features , kernel_size = kernel_size , padding = padding ) NEW_LINE self . norm = BatchNorm3d ( out_features , affine = True ) NEW_LINE DEDENT',), ('def forward ( self , x ) : NEW_LINE INDENT out = F . interpolate ( x , scale_factor = ( 1 , 2 , 2 ) ) NEW_LINE out = self . conv ( out ) NEW_LINE out = self . norm ( out ) NEW_LINE out = F . relu ( out ) NEW_LINE return out NEW_LINE DEDENT',), ('def __init__ ( self , in_features , out_features , kernel_size = 3 , padding = 1 ) : NEW_LINE INDENT super ( DownBlock3D , self ) . __init__ ( ) NEW_LINE self . conv = nn . Conv3d ( in_channels = in_features , out_channels = out_features , kernel_size = kernel_size , padding = padding ) NEW_LINE self . norm = BatchNorm3d ( out_features , affine = True ) NEW_LINE self . pool = nn . AvgPool3d ( kernel_size = ( 1 , 2 , 2 ) ) NEW_LINE DEDENT',), ('def forward ( self , x ) : NEW_LINE INDENT out = self . conv ( x ) NEW_LINE out = self . norm ( out ) NEW_LINE out = F . relu ( out ) NEW_LINE out = self . pool ( out ) NEW_LINE return out NEW_LINE DEDENT',), ('def __init__ ( self , in_features , out_features , groups = None , kernel_size = 3 , padding = 1 ) : NEW_LINE INDENT super ( SameBlock3D , self ) . __init__ ( ) NEW_LINE self . conv = nn . Conv3d ( in_channels = in_features , out_channels = out_features , kernel_size = kernel_size , padding = padding , groups = groups ) NEW_LINE self . norm = BatchNorm3d ( out_features , affine = True ) NEW_LINE DEDENT',), ('def forward ( self , x ) : NEW_LINE INDENT out = self . conv ( x ) NEW_LINE out = self . norm ( out ) NEW_LINE out = F . relu ( out ) NEW_LINE return out NEW_LINE DEDENT',), ('def __init__ ( self , block_expansion , in_features , num_blocks = 3 , max_features = 256 , temporal = False ) : NEW_LINE INDENT super ( Encoder , self ) . __init__ ( ) NEW_LINE down_blocks = [ ] NEW_LINE kernel_size = ( 3 , 3 , 3 ) if temporal else ( 1 , 3 , 3 ) NEW_LINE padding = ( 1 , 1 , 1 ) if temporal else ( 0 , 1 , 1 ) NEW_LINE for i in range ( num_blocks ) : NEW_LINE INDENT down_blocks . append ( DownBlock3D ( in_features if i == 0 else min ( max_features , block_expansion * ( 2 ** i ) ) , min ( max_features , block_expansion * ( 2 ** ( i + 1 ) ) ) , kernel_size = kernel_size , padding = padding ) ) NEW_LINE DEDENT self . down_blocks = nn . ModuleList ( down_blocks ) NEW_LINE DEDENT',), ('def forward ( self , x ) : NEW_LINE INDENT outs = [ x ] NEW_LINE for down_block in self . down_blocks : NEW_LINE INDENT outs . append ( down_block ( outs [ - 1 ] ) ) NEW_LINE DEDENT return outs NEW_LINE DEDENT',), ('def __init__ ( self , block_expansion , in_features , out_features , num_blocks = 3 , max_features = 256 , temporal = False , additional_features_for_block = 0 , use_last_conv = True ) : NEW_LINE INDENT super ( Decoder , self ) . __init__ ( ) NEW_LINE kernel_size = ( 3 , 3 , 3 ) if temporal else ( 1 , 3 , 3 ) NEW_LINE padding = ( 1 , 1 , 1 ) if temporal else ( 0 , 1 , 1 ) NEW_LINE up_blocks = [ ] NEW_LINE for i in range ( num_blocks ) [ : : - 1 ] : NEW_LINE INDENT up_blocks . append ( UpBlock3D ( ( 1 if i == num_blocks - 1 else 2 ) * min ( max_features , block_expansion * ( 2 ** ( i + 1 ) ) ) + additional_features_for_block , min ( max_features , block_expansion * ( 2 ** i ) ) , kernel_size = kernel_size , padding = padding ) ) NEW_LINE DEDENT self . up_blocks = nn . ModuleList ( up_blocks ) NEW_LINE if use_last_conv : NEW_LINE INDENT self . conv = nn . Conv3d ( in_channels = block_expansion + in_features + additional_features_for_block , out_channels = out_features , kernel_size = kernel_size , padding = padding ) NEW_LINE DEDENT else : NEW_LINE INDENT self . conv = None NEW_LINE DEDENT DEDENT',), ('def forward ( self , x ) : NEW_LINE INDENT out = x . pop ( ) NEW_LINE for up_block in self . up_blocks : NEW_LINE INDENT out = up_block ( out ) NEW_LINE out = torch . cat ( [ out , x . pop ( ) ] , dim = 1 ) NEW_LINE DEDENT if self . conv is not None : NEW_LINE INDENT return self . conv ( out ) NEW_LINE DEDENT else : NEW_LINE INDENT return out NEW_LINE DEDENT DEDENT',), ('def __init__ ( self , block_expansion , in_features , out_features , num_blocks = 3 , max_features = 256 , temporal = False , ) : NEW_LINE INDENT super ( Hourglass , self ) . __init__ ( ) NEW_LINE self . encoder = Encoder ( block_expansion , in_features , num_blocks , max_features , temporal = temporal ) NEW_LINE self . decoder = Decoder ( block_expansion , in_features , out_features , num_blocks , max_features , temporal = temporal ) NEW_LINE DEDENT',), ('def forward ( self , x ) : NEW_LINE INDENT return self . decoder ( self . encoder ( x ) ) NEW_LINE DEDENT',), ('def mean_batch ( val ) : NEW_LINE INDENT return val . view ( val . shape [ 0 ] , - 1 ) . mean ( - 1 ) NEW_LINE DEDENT',), ('def reconstruction_loss ( prediction , target , weight ) : NEW_LINE INDENT if weight == 0 : NEW_LINE INDENT return 0 NEW_LINE DEDENT return weight * mean_batch ( torch . abs ( prediction - target ) ) NEW_LINE DEDENT',), ('def generator_gan_loss ( discriminator_maps_generated , weight ) : NEW_LINE INDENT scores_generated = discriminator_maps_generated [ - 1 ] NEW_LINE score = ( 1 - scores_generated ) ** 2 NEW_LINE return weight * mean_batch ( score ) NEW_LINE DEDENT',), ('def discriminator_gan_loss ( discriminator_maps_generated , discriminator_maps_real , weight ) : NEW_LINE INDENT scores_real = discriminator_maps_real [ - 1 ] NEW_LINE scores_generated = discriminator_maps_generated [ - 1 ] NEW_LINE score = ( 1 - scores_real ) ** 2 + scores_generated ** 2 NEW_LINE return weight * mean_batch ( score ) NEW_LINE DEDENT',), ('def generator_loss_names ( loss_weights ) : NEW_LINE INDENT loss_names = [ ] NEW_LINE if loss_weights [ \\' reconstruction _ deformed \\' ] != 0 : NEW_LINE INDENT loss_names . append ( \" rec _ def \" ) NEW_LINE DEDENT if loss_weights [ \\' reconstruction \\' ] is not None : NEW_LINE INDENT for i , _ in enumerate ( loss_weights [ \\' reconstruction \\' ] ) : NEW_LINE INDENT if loss_weights [ \\' reconstruction \\' ] [ i ] == 0 : NEW_LINE INDENT continue NEW_LINE DEDENT loss_names . append ( \" layer - % s _ rec \" % i ) NEW_LINE DEDENT DEDENT loss_names . append ( \" gen _ gan \" ) NEW_LINE return loss_names NEW_LINE DEDENT',), (\"def discriminator_loss_names ( ) : NEW_LINE INDENT return [ ' disc _ gan ' ] NEW_LINE DEDENT\",), (\"def generator_loss ( discriminator_maps_generated , discriminator_maps_real , video_deformed , loss_weights ) : NEW_LINE INDENT loss_values = [ ] NEW_LINE if loss_weights [ ' reconstruction _ deformed ' ] != 0 : NEW_LINE INDENT loss_values . append ( reconstruction_loss ( discriminator_maps_real [ 0 ] , video_deformed , loss_weights [ ' reconstruction _ deformed ' ] ) ) NEW_LINE DEDENT if loss_weights [ ' reconstruction ' ] != 0 : NEW_LINE INDENT for i , ( a , b ) in enumerate ( zip ( discriminator_maps_real [ : - 1 ] , discriminator_maps_generated [ : - 1 ] ) ) : NEW_LINE INDENT if loss_weights [ ' reconstruction ' ] [ i ] == 0 : NEW_LINE INDENT continue NEW_LINE DEDENT loss_values . append ( reconstruction_loss ( b , a , weight = loss_weights [ ' reconstruction ' ] [ i ] ) ) NEW_LINE DEDENT DEDENT loss_values . append ( generator_gan_loss ( discriminator_maps_generated , weight = loss_weights [ ' generator _ gan ' ] ) ) NEW_LINE return loss_values NEW_LINE DEDENT\",), (\"def discriminator_loss ( discriminator_maps_generated , discriminator_maps_real , loss_weights ) : NEW_LINE INDENT loss_values = [ discriminator_gan_loss ( discriminator_maps_generated , discriminator_maps_real , loss_weights [ ' discriminator _ gan ' ] ) ] NEW_LINE return loss_values NEW_LINE DEDENT\",), (\"def __init__ ( self , num_kp , kp_variance , num_channels , use_deformed_source_image = False , use_difference = False , use_heatmap = True , add_bg_feature_map = False , heatmap_type = ' gaussian ' , norm_const = ' sum ' , scale_factor = 1 ) : NEW_LINE INDENT super ( MovementEmbeddingModule , self ) . __init__ ( ) NEW_LINE assert heatmap_type in [ ' gaussian ' , ' difference ' ] NEW_LINE assert ( ( int ( use_heatmap ) + int ( use_deformed_source_image ) + int ( use_difference ) ) >= 1 ) NEW_LINE self . out_channels = ( 1 * use_heatmap + 2 * use_difference + num_channels * use_deformed_source_image ) * ( num_kp + add_bg_feature_map ) NEW_LINE self . kp_variance = kp_variance NEW_LINE self . heatmap_type = heatmap_type NEW_LINE self . use_difference = use_difference NEW_LINE self . use_deformed_source_image = use_deformed_source_image NEW_LINE self . use_heatmap = use_heatmap NEW_LINE self . add_bg_feature_map = add_bg_feature_map NEW_LINE self . norm_const = norm_const NEW_LINE self . scale_factor = scale_factor NEW_LINE DEDENT\",), ('def normalize_heatmap ( self , heatmap ) : NEW_LINE INDENT if self . norm_const == \" sum \" : NEW_LINE INDENT heatmap_shape = heatmap . shape NEW_LINE heatmap = heatmap . view ( heatmap_shape [ 0 ] , heatmap_shape [ 1 ] , heatmap_shape [ 2 ] , - 1 ) NEW_LINE heatmap = heatmap / heatmap . sum ( dim = 3 , keepdim = True ) NEW_LINE return heatmap . view ( * heatmap_shape ) NEW_LINE DEDENT else : NEW_LINE INDENT return heatmap / self . norm_const NEW_LINE DEDENT DEDENT',), (\"def forward ( self , source_image , kp_driving , kp_source ) : NEW_LINE INDENT if self . scale_factor != 1 : NEW_LINE INDENT source_image = F . interpolate ( source_image , scale_factor = ( 1 , self . scale_factor , self . scale_factor ) ) NEW_LINE DEDENT spatial_size = source_image . shape [ 3 : ] NEW_LINE bs , _ , _ , h , w = source_image . shape NEW_LINE _ , d , num_kp , _ = kp_driving [ ' mean ' ] . shape NEW_LINE inputs = [ ] NEW_LINE if self . use_heatmap : NEW_LINE INDENT heatmap = self . normalize_heatmap ( kp2gaussian ( kp_driving , spatial_size = spatial_size , kp_variance = self . kp_variance ) ) NEW_LINE if self . heatmap_type == ' difference ' : NEW_LINE INDENT heatmap_appearance = self . normalize_heatmap ( kp2gaussian ( kp_source , spatial_size = spatial_size , kp_variance = self . kp_variance ) ) NEW_LINE heatmap = heatmap - heatmap_appearance NEW_LINE DEDENT if self . add_bg_feature_map : NEW_LINE INDENT zeros = torch . zeros ( bs , d , 1 , h , w ) . type ( heatmap . type ( ) ) NEW_LINE heatmap = torch . cat ( [ zeros , heatmap ] , dim = 2 ) NEW_LINE DEDENT heatmap = heatmap . unsqueeze ( 3 ) NEW_LINE inputs . append ( heatmap ) NEW_LINE DEDENT num_kp += self . add_bg_feature_map NEW_LINE if self . use_difference or self . use_deformed_source_image : NEW_LINE INDENT kp_video_diff = kp_source [ ' mean ' ] - kp_driving [ ' mean ' ] NEW_LINE if self . add_bg_feature_map : NEW_LINE INDENT zeros = torch . zeros ( bs , d , 1 , 2 ) . type ( kp_video_diff . type ( ) ) NEW_LINE kp_video_diff = torch . cat ( [ zeros , kp_video_diff ] , dim = 2 ) NEW_LINE DEDENT kp_video_diff = kp_video_diff . view ( ( bs , d , num_kp , 2 , 1 , 1 ) ) . repeat ( 1 , 1 , 1 , 1 , h , w ) NEW_LINE DEDENT if self . use_difference : NEW_LINE INDENT inputs . append ( kp_video_diff ) NEW_LINE DEDENT if self . use_deformed_source_image : NEW_LINE INDENT appearance_repeat = source_image . unsqueeze ( 1 ) . unsqueeze ( 1 ) . repeat ( 1 , d , num_kp , 1 , 1 , 1 , 1 ) NEW_LINE appearance_repeat = appearance_repeat . view ( bs * d * num_kp , - 1 , h , w ) NEW_LINE deformation_approx = kp_video_diff . view ( ( bs * d * num_kp , - 1 , h , w ) ) . permute ( 0 , 2 , 3 , 1 ) NEW_LINE coordinate_grid = make_coordinate_grid ( ( h , w ) , type = deformation_approx . type ( ) ) NEW_LINE coordinate_grid = coordinate_grid . view ( 1 , h , w , 2 ) NEW_LINE deformation_approx = coordinate_grid + deformation_approx NEW_LINE appearance_approx_deform = F . grid_sample ( appearance_repeat , deformation_approx ) NEW_LINE appearance_approx_deform = appearance_approx_deform . view ( ( bs , d , num_kp , - 1 , h , w ) ) NEW_LINE inputs . append ( appearance_approx_deform ) NEW_LINE DEDENT movement_encoding = torch . cat ( inputs , dim = 3 ) NEW_LINE movement_encoding = movement_encoding . view ( bs , d , - 1 , h , w ) NEW_LINE return movement_encoding . permute ( 0 , 2 , 1 , 3 , 4 ) NEW_LINE DEDENT\",), ('def __init__ ( self , block_expansion , num_blocks , max_features , mask_embedding_params , num_kp , num_channels , kp_variance , use_correction , use_mask , bg_init = 2 , num_group_blocks = 0 , scale_factor = 1 ) : NEW_LINE INDENT super ( DenseMotionModule , self ) . __init__ ( ) NEW_LINE self . mask_embedding = MovementEmbeddingModule ( num_kp = num_kp , kp_variance = kp_variance , num_channels = num_channels , add_bg_feature_map = True , ** mask_embedding_params ) NEW_LINE self . difference_embedding = MovementEmbeddingModule ( num_kp = num_kp , kp_variance = kp_variance , num_channels = num_channels , add_bg_feature_map = True , use_difference = True , use_heatmap = False , use_deformed_source_image = False ) NEW_LINE group_blocks = [ ] NEW_LINE for i in range ( num_group_blocks ) : NEW_LINE INDENT group_blocks . append ( SameBlock3D ( self . mask_embedding . out_channels , self . mask_embedding . out_channels , groups = num_kp + 1 , kernel_size = ( 1 , 1 , 1 ) , padding = ( 0 , 0 , 0 ) ) ) NEW_LINE DEDENT self . group_blocks = nn . ModuleList ( group_blocks ) NEW_LINE self . hourglass = Hourglass ( block_expansion = block_expansion , in_features = self . mask_embedding . out_channels , out_features = ( num_kp + 1 ) * use_mask + 2 * use_correction , max_features = max_features , num_blocks = num_blocks ) NEW_LINE self . hourglass . decoder . conv . weight . data . zero_ ( ) NEW_LINE bias_init = ( [ bg_init ] + [ 0 ] * num_kp ) * use_mask + [ 0 , 0 ] * use_correction NEW_LINE self . hourglass . decoder . conv . bias . data . copy_ ( torch . tensor ( bias_init , dtype = torch . float ) ) NEW_LINE self . num_kp = num_kp NEW_LINE self . use_correction = use_correction NEW_LINE self . use_mask = use_mask NEW_LINE self . scale_factor = scale_factor NEW_LINE DEDENT',), ('def forward ( self , source_image , kp_driving , kp_source ) : NEW_LINE INDENT if self . scale_factor != 1 : NEW_LINE INDENT source_image = F . interpolate ( source_image , scale_factor = ( 1 , self . scale_factor , self . scale_factor ) ) NEW_LINE DEDENT prediction = self . mask_embedding ( source_image , kp_driving , kp_source ) NEW_LINE for block in self . group_blocks : NEW_LINE INDENT prediction = block ( prediction ) NEW_LINE prediction = F . leaky_relu ( prediction , 0.2 ) NEW_LINE DEDENT prediction = self . hourglass ( prediction ) NEW_LINE bs , _ , d , h , w = prediction . shape NEW_LINE if self . use_mask : NEW_LINE INDENT mask = prediction [ : , : ( self . num_kp + 1 ) ] NEW_LINE mask = F . softmax ( mask , dim = 1 ) NEW_LINE mask = mask . unsqueeze ( 2 ) NEW_LINE difference_embedding = self . difference_embedding ( source_image , kp_driving , kp_source ) NEW_LINE difference_embedding = difference_embedding . view ( bs , self . num_kp + 1 , 2 , d , h , w ) NEW_LINE deformations_relative = ( difference_embedding * mask ) . sum ( dim = 1 ) NEW_LINE DEDENT else : NEW_LINE INDENT deformations_relative = 0 NEW_LINE DEDENT if self . use_correction : NEW_LINE INDENT correction = prediction [ : , - 2 : ] NEW_LINE DEDENT else : NEW_LINE INDENT correction = 0 NEW_LINE DEDENT deformations_relative = deformations_relative + correction NEW_LINE deformations_relative = deformations_relative . permute ( 0 , 2 , 3 , 4 , 1 ) NEW_LINE coordinate_grid = make_coordinate_grid ( ( h , w ) , type = deformations_relative . type ( ) ) NEW_LINE coordinate_grid = coordinate_grid . view ( 1 , 1 , h , w , 2 ) NEW_LINE deformation = deformations_relative + coordinate_grid NEW_LINE z_coordinate = torch . zeros ( deformation . shape [ : - 1 ] + ( 1 , ) ) . type ( deformation . type ( ) ) NEW_LINE return torch . cat ( [ deformation , z_coordinate ] , dim = - 1 ) NEW_LINE DEDENT',), (\"def forward ( self , appearance_frame , kp_video , kp_appearance ) : NEW_LINE INDENT bs , _ , _ , h , w = appearance_frame . shape NEW_LINE _ , d , num_kp , _ = kp_video [ ' mean ' ] . shape NEW_LINE coordinate_grid = make_coordinate_grid ( ( h , w ) , type = appearance_frame . type ( ) ) NEW_LINE coordinate_grid = coordinate_grid . view ( 1 , 1 , h , w , 2 ) . repeat ( bs , d , 1 , 1 , 1 ) NEW_LINE z_coordinate = torch . zeros ( coordinate_grid . shape [ : - 1 ] + ( 1 , ) ) . type ( coordinate_grid . type ( ) ) NEW_LINE return torch . cat ( [ coordinate_grid , z_coordinate ] , dim = - 1 ) NEW_LINE DEDENT\",), (\"def __init__ ( self , num_kp = 10 , kp_variance = 0.01 , num_features = 1024 , num_layers = 1 , dropout = 0.5 ) : NEW_LINE INDENT super ( PredictionModule , self ) . __init__ ( ) NEW_LINE input_size = num_kp * ( 2 + 4 * ( kp_variance == ' matrix ' ) ) NEW_LINE self . rnn = nn . GRU ( input_size = input_size , hidden_size = num_features , num_layers = num_layers , dropout = dropout , batch_first = True ) NEW_LINE self . linear = nn . Linear ( num_features , input_size ) NEW_LINE DEDENT\",), ('def net ( self , input , h = None ) : NEW_LINE INDENT output , h = self . rnn ( input , h ) NEW_LINE init_shape = output . shape NEW_LINE output = output . contiguous ( ) . view ( - 1 , output . shape [ - 1 ] ) NEW_LINE output = self . linear ( output ) NEW_LINE return output . view ( init_shape [ 0 ] , init_shape [ 1 ] , output . shape [ - 1 ] ) , h NEW_LINE DEDENT',), (\"def forward ( self , kp_batch ) : NEW_LINE INDENT bs , d , num_kp , _ = kp_batch [ ' mean ' ] . shape NEW_LINE inputs = [ kp_batch [ ' mean ' ] . contiguous ( ) . view ( bs , d , - 1 ) ] NEW_LINE if ' var ' in kp_batch : NEW_LINE INDENT inputs . append ( kp_batch [ ' var ' ] . contiguous ( ) . view ( bs , d , - 1 ) ) NEW_LINE DEDENT input = torch . cat ( inputs , dim = - 1 ) NEW_LINE output , h = self . net ( input ) NEW_LINE output = output . view ( bs , d , num_kp , - 1 ) NEW_LINE mean = torch . tanh ( output [ : , : , : , : 2 ] ) NEW_LINE kp_array = { ' mean ' : mean } NEW_LINE if ' var ' in kp_batch : NEW_LINE INDENT var = output [ : , : , : , 2 : ] NEW_LINE var = var . view ( bs , d , num_kp , 2 , 2 ) NEW_LINE var = torch . matmul ( var . permute ( 0 , 1 , 2 , 4 , 3 ) , var ) NEW_LINE kp_array [ ' var ' ] = var NEW_LINE DEDENT return kp_array NEW_LINE DEDENT\",), (\"def __init__ ( self , num_channels , num_kp , kp_variance , block_expansion , max_features , num_blocks , num_refinement_blocks , dense_motion_params = None , kp_embedding_params = None , interpolation_mode = ' nearest ' ) : NEW_LINE INDENT super ( MotionTransferGenerator , self ) . __init__ ( ) NEW_LINE self . appearance_encoder = Encoder ( block_expansion , in_features = num_channels , max_features = max_features , num_blocks = num_blocks ) NEW_LINE if kp_embedding_params is not None : NEW_LINE INDENT self . kp_embedding_module = MovementEmbeddingModule ( num_kp = num_kp , kp_variance = kp_variance , num_channels = num_channels , ** kp_embedding_params ) NEW_LINE embedding_features = self . kp_embedding_module . out_channels NEW_LINE DEDENT else : NEW_LINE INDENT self . kp_embedding_module = None NEW_LINE embedding_features = 0 NEW_LINE DEDENT if dense_motion_params is not None : NEW_LINE INDENT self . dense_motion_module = DenseMotionModule ( num_kp = num_kp , kp_variance = kp_variance , num_channels = num_channels , ** dense_motion_params ) NEW_LINE DEDENT else : NEW_LINE INDENT self . dense_motion_module = IdentityDeformation ( ) NEW_LINE DEDENT self . video_decoder = Decoder ( block_expansion = block_expansion , in_features = num_channels , out_features = num_channels , max_features = max_features , num_blocks = num_blocks , additional_features_for_block = embedding_features , use_last_conv = False ) NEW_LINE self . refinement_module = torch . nn . Sequential ( ) NEW_LINE in_features = block_expansion + num_channels + embedding_features NEW_LINE for i in range ( num_refinement_blocks ) : NEW_LINE INDENT self . refinement_module . add_module ( ' r ' + str ( i ) , ResBlock3D ( in_features , kernel_size = ( 1 , 3 , 3 ) , padding = ( 0 , 1 , 1 ) ) ) NEW_LINE DEDENT self . refinement_module . add_module ( ' conv - last ' , nn . Conv3d ( in_features , num_channels , kernel_size = 1 , padding = 0 ) ) NEW_LINE self . interpolation_mode = interpolation_mode NEW_LINE DEDENT\",), ('def deform_input ( self , inp , deformations_absolute ) : NEW_LINE INDENT bs , d , h_old , w_old , _ = deformations_absolute . shape NEW_LINE _ , _ , _ , h , w = inp . shape NEW_LINE deformations_absolute = deformations_absolute . permute ( 0 , 4 , 1 , 2 , 3 ) NEW_LINE deformation = F . interpolate ( deformations_absolute , size = ( d , h , w ) , mode = self . interpolation_mode ) NEW_LINE deformation = deformation . permute ( 0 , 2 , 3 , 4 , 1 ) NEW_LINE deformed_inp = F . grid_sample ( inp , deformation ) NEW_LINE return deformed_inp NEW_LINE DEDENT',), ('def forward ( self , source_image , kp_driving , kp_source ) : NEW_LINE INDENT appearance_skips = self . appearance_encoder ( source_image ) NEW_LINE deformations_absolute = self . dense_motion_module ( source_image = source_image , kp_driving = kp_driving , kp_source = kp_source ) NEW_LINE deformed_skips = [ self . deform_input ( skip , deformations_absolute ) for skip in appearance_skips ] NEW_LINE if self . kp_embedding_module is not None : NEW_LINE INDENT d = kp_driving [ \\' mean \\' ] . shape [ 1 ] NEW_LINE movement_embedding = self . kp_embedding_module ( source_image = source_image , kp_driving = kp_driving , kp_source = kp_source ) NEW_LINE kp_skips = [ F . interpolate ( movement_embedding , size = ( d , ) + skip . shape [ 3 : ] , mode = self . interpolation_mode ) for skip in appearance_skips ] NEW_LINE skips = [ torch . cat ( [ a , b ] , dim = 1 ) for a , b in zip ( deformed_skips , kp_skips ) ] NEW_LINE DEDENT else : NEW_LINE INDENT skips = deformed_skips NEW_LINE DEDENT video_deformed = self . deform_input ( source_image , deformations_absolute ) NEW_LINE video_prediction = self . video_decoder ( skips ) NEW_LINE video_prediction = self . refinement_module ( video_prediction ) NEW_LINE video_prediction = torch . sigmoid ( video_prediction ) NEW_LINE return { \" video _ prediction \" : video_prediction , \" video _ deformed \" : video_deformed } NEW_LINE DEDENT',), (\"def kp2gaussian ( kp , spatial_size , kp_variance = ' matrix ' ) : NEW_LINE INDENT mean = kp [ ' mean ' ] NEW_LINE coordinate_grid = make_coordinate_grid ( spatial_size , mean . type ( ) ) NEW_LINE number_of_leading_dimensions = len ( mean . shape ) - 1 NEW_LINE shape = ( 1 , ) * number_of_leading_dimensions + coordinate_grid . shape NEW_LINE coordinate_grid = coordinate_grid . view ( * shape ) NEW_LINE repeats = mean . shape [ : number_of_leading_dimensions ] + ( 1 , 1 , 1 ) NEW_LINE coordinate_grid = coordinate_grid . repeat ( * repeats ) NEW_LINE shape = mean . shape [ : number_of_leading_dimensions ] + ( 1 , 1 , 2 ) NEW_LINE mean = mean . view ( * shape ) NEW_LINE mean_sub = ( coordinate_grid - mean ) NEW_LINE if kp_variance == ' matrix ' : NEW_LINE INDENT var = kp [ ' var ' ] NEW_LINE inv_var = matrix_inverse ( var ) NEW_LINE shape = inv_var . shape [ : number_of_leading_dimensions ] + ( 1 , 1 , 2 , 2 ) NEW_LINE inv_var = inv_var . view ( * shape ) NEW_LINE under_exp = torch . matmul ( torch . matmul ( mean_sub . unsqueeze ( - 2 ) , inv_var ) , mean_sub . unsqueeze ( - 1 ) ) NEW_LINE under_exp = under_exp . squeeze ( - 1 ) . squeeze ( - 1 ) NEW_LINE out = torch . exp ( - 0.5 * under_exp ) NEW_LINE DEDENT elif kp_variance == ' single ' : NEW_LINE INDENT out = torch . exp ( - 0.5 * ( mean_sub ** 2 ) . sum ( - 1 ) / kp [ ' var ' ] ) NEW_LINE DEDENT else : NEW_LINE INDENT out = torch . exp ( - 0.5 * ( mean_sub ** 2 ) . sum ( - 1 ) / kp_variance ) NEW_LINE DEDENT return out NEW_LINE DEDENT\",), (\"def gaussian2kp ( heatmap , kp_variance = ' matrix ' , clip_variance = None ) : NEW_LINE INDENT shape = heatmap . shape NEW_LINE heatmap = heatmap . unsqueeze ( - 1 ) + 1e-7 NEW_LINE grid = make_coordinate_grid ( shape [ 3 : ] , heatmap . type ( ) ) . unsqueeze_ ( 0 ) . unsqueeze_ ( 0 ) . unsqueeze_ ( 0 ) NEW_LINE mean = ( heatmap * grid ) . sum ( dim = ( 3 , 4 ) ) NEW_LINE kp = { ' mean ' : mean . permute ( 0 , 2 , 1 , 3 ) } NEW_LINE if kp_variance == ' matrix ' : NEW_LINE INDENT mean_sub = grid - mean . unsqueeze ( - 2 ) . unsqueeze ( - 2 ) NEW_LINE var = torch . matmul ( mean_sub . unsqueeze ( - 1 ) , mean_sub . unsqueeze ( - 2 ) ) NEW_LINE var = var * heatmap . unsqueeze ( - 1 ) NEW_LINE var = var . sum ( dim = ( 3 , 4 ) ) NEW_LINE var = var . permute ( 0 , 2 , 1 , 3 , 4 ) NEW_LINE if clip_variance : NEW_LINE INDENT min_norm = torch . tensor ( clip_variance ) . type ( var . type ( ) ) NEW_LINE sg = smallest_singular ( var ) . unsqueeze ( - 1 ) NEW_LINE var = torch . max ( min_norm , sg ) * var / sg NEW_LINE DEDENT kp [ ' var ' ] = var NEW_LINE DEDENT elif kp_variance == ' single ' : NEW_LINE INDENT mean_sub = grid - mean . unsqueeze ( - 2 ) . unsqueeze ( - 2 ) NEW_LINE var = mean_sub ** 2 NEW_LINE var = var * heatmap NEW_LINE var = var . sum ( dim = ( 3 , 4 ) ) NEW_LINE var = var . mean ( dim = - 1 , keepdim = True ) NEW_LINE var = var . unsqueeze ( - 1 ) NEW_LINE var = var . permute ( 0 , 2 , 1 , 3 , 4 ) NEW_LINE kp [ ' var ' ] = var NEW_LINE DEDENT return kp NEW_LINE DEDENT\",), ('def __init__ ( self , block_expansion , num_kp , num_channels , max_features , num_blocks , temperature , kp_variance , scale_factor = 1 , clip_variance = None ) : NEW_LINE INDENT super ( KPDetector , self ) . __init__ ( ) NEW_LINE self . predictor = Hourglass ( block_expansion , in_features = num_channels , out_features = num_kp , max_features = max_features , num_blocks = num_blocks ) NEW_LINE self . temperature = temperature NEW_LINE self . kp_variance = kp_variance NEW_LINE self . scale_factor = scale_factor NEW_LINE self . clip_variance = clip_variance NEW_LINE DEDENT',), ('def forward ( self , x ) : NEW_LINE INDENT if self . scale_factor != 1 : NEW_LINE INDENT x = F . interpolate ( x , scale_factor = ( 1 , self . scale_factor , self . scale_factor ) ) NEW_LINE DEDENT heatmap = self . predictor ( x ) NEW_LINE final_shape = heatmap . shape NEW_LINE heatmap = heatmap . view ( final_shape [ 0 ] , final_shape [ 1 ] , final_shape [ 2 ] , - 1 ) NEW_LINE heatmap = F . softmax ( heatmap / self . temperature , dim = 3 ) NEW_LINE heatmap = heatmap . view ( * final_shape ) NEW_LINE out = gaussian2kp ( heatmap , self . kp_variance , self . clip_variance ) NEW_LINE return out NEW_LINE DEDENT',), ('def mv_all_images ( images , in_folder , out_folder ) : NEW_LINE INDENT for img in images : NEW_LINE INDENT move ( os . path . join ( in_folder , img ) , os . path . join ( out_folder , img ) ) NEW_LINE DEDENT DEDENT',)]\n",
            "0.9831390269100666\n",
            "On batch 1535\n",
            "tensor([0.0921, 0.0923, 0.0921, 0.0897, 0.0908, 0.0901, 0.0890, 0.0901, 0.0904,\n",
            "        0.0905, 0.0931], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "('\\\\label{equiv}\\n\\\\mathbf{S}\\\\in \\\\mathcal{P}^+_N ~\\\\iff~ \\\\mathbf{S} = \\\\mathbf{U}\\\\boldsymbol{\\\\Sigma} \\\\mathbf{U}^\\\\top = \\\\mathbf{U}\\\\boldsymbol{\\\\Lambda} \\\\mathbf{U}^\\\\top\\n',)\n",
            "[('def gen_graph ( M , N ) : NEW_LINE INDENT A = np . zeros ( [ N , N ] ) NEW_LINE for i in range ( N ) : NEW_LINE INDENT for j in range ( i + 1 , N ) : NEW_LINE INDENT if np . random . rand ( ) < M [ i , j ] : NEW_LINE INDENT A [ i , j ] = 1.0 NEW_LINE DEDENT A [ j , i ] = A [ i , j ] NEW_LINE DEDENT DEDENT return A NEW_LINE DEDENT',), ('def get_ideal ( p , q , N ) : NEW_LINE INDENT A = np . zeros ( [ N , N ] ) NEW_LINE for i in range ( N ) : NEW_LINE INDENT for j in range ( i + 1 , N ) : NEW_LINE INDENT if ( i < N // 3 and j < N // 3 ) or ( i in range ( N // 3 , 2 * ( N // 3 ) ) and j in range ( N // 3 , 2 * ( N // 3 ) ) ) or ( i in range ( 2 * ( N // 3 ) , N ) and j in range ( 2 * ( N // 3 ) , N ) ) : NEW_LINE INDENT A [ i , j ] = p NEW_LINE DEDENT else : NEW_LINE INDENT if i < N / 3 and j > 2 * ( N // 3 ) or j < N / 3 and i > 2 * ( N // 3 ) : NEW_LINE INDENT A [ i , j ] = q / 3 NEW_LINE DEDENT else : NEW_LINE INDENT A [ i , j ] = q NEW_LINE DEDENT DEDENT A [ j , i ] = A [ i , j ] NEW_LINE DEDENT DEDENT return A NEW_LINE DEDENT',), ('def get_powers ( A , K ) : NEW_LINE INDENT seq = [ ] NEW_LINE degrees = np . sum ( A , axis = 0 ) NEW_LINE if np . sum ( degrees == 0.0 ) > 0 : NEW_LINE INDENT return seq NEW_LINE DEDENT d_inv = degrees ** ( - 0.5 ) NEW_LINE D = np . diag ( d_inv ) NEW_LINE P = 0.5 * ( np . eye ( A . shape [ 0 ] ) + np . dot ( np . dot ( D , A ) , D ) ) NEW_LINE seq = [ P ] NEW_LINE for k in range ( 1 , K ) : NEW_LINE INDENT seq . append ( np . dot ( seq [ k - 1 ] , P ) ) NEW_LINE DEDENT return seq NEW_LINE DEDENT',), ('def get_adj_powers ( A , K ) : NEW_LINE INDENT seq = [ A ] NEW_LINE for k in range ( 1 , K ) : NEW_LINE INDENT seq . append ( np . dot ( seq [ k - 1 ] , A ) ) NEW_LINE DEDENT return seq NEW_LINE DEDENT',), ('def get_pgr ( A , a ) : NEW_LINE INDENT degrees = np . sum ( A , axis = 0 ) NEW_LINE d_inv = degrees ** ( - 1 ) NEW_LINE D = np . diag ( d_inv ) NEW_LINE A = np . eye ( A . shape [ 0 ] ) - a * np . dot ( A , D ) NEW_LINE A_pgr = np . linalg . inv ( A ) NEW_LINE for i in range ( A . shape [ 0 ] ) : NEW_LINE INDENT A_pgr [ i , i ] = 0.0 NEW_LINE DEDENT return A_pgr NEW_LINE DEDENT',), ('def get_katz ( A , beta ) : NEW_LINE INDENT A_temp = np . eye ( A . shape [ 0 ] ) - beta * A NEW_LINE A_temp2 = np . linalg . inv ( A_temp ) NEW_LINE A_katz = beta * np . dot ( A_temp2 , A ) NEW_LINE for i in range ( A . shape [ 0 ] ) : NEW_LINE INDENT A_katz [ i , i ] = 0.0 NEW_LINE DEDENT return A_katz NEW_LINE DEDENT',), ('def get_neigh ( A ) : NEW_LINE INDENT A_neigh = np . dot ( A , A ) NEW_LINE for i in range ( A . shape [ 0 ] ) : NEW_LINE INDENT A_neigh [ i , i ] = 0.0 NEW_LINE DEDENT return A_neigh NEW_LINE DEDENT',), ('def get_adam ( A , a ) : NEW_LINE INDENT degrees = np . sum ( A , axis = 0 ) NEW_LINE D = np . diag ( degrees ** ( - 1 ) ) NEW_LINE A_adam = np . dot ( np . dot ( A , D ) , A ) NEW_LINE for i in range ( A . shape [ 0 ] ) : NEW_LINE INDENT A_adam [ i , i ] = 0.0 NEW_LINE DEDENT return A_adam NEW_LINE DEDENT',), ('def relative_error ( A , B ) : NEW_LINE INDENT N = A . shape [ 0 ] NEW_LINE A_norm = A - np . sum ( np . sum ( A ) ) / ( N ** 2 - N ) NEW_LINE B_norm = B - np . sum ( np . sum ( B ) ) / ( N ** 2 - N ) NEW_LINE return pearson ( A_norm , B_norm ) NEW_LINE DEDENT',), (\"def pearson ( A , B ) : NEW_LINE INDENT cor = np . sum ( np . sum ( A * B ) ) NEW_LINE cor /= np . linalg . norm ( A , ord = ' fro ' ) * np . linalg . norm ( B , ord = ' fro ' ) NEW_LINE return cor NEW_LINE DEDENT\",), ('def main ( ) : NEW_LINE INDENT parser = argparse . ArgumentParser ( description = \\' Input ▁ SBM ▁ parameters ▁ p ▁ and ▁ q \\' ) NEW_LINE parser . add_argument ( \\' - - p \\' , type = float , default = 0.3 ) NEW_LINE parser . add_argument ( \\' - - q \\' , type = float , default = 0.1 ) NEW_LINE args = parser . parse_args ( ) NEW_LINE iters = 100 NEW_LINE N = 150 NEW_LINE K = 30 NEW_LINE A_ideal = get_ideal ( args . p , args . q , N ) NEW_LINE a = 0.95 NEW_LINE beta = 0.04 NEW_LINE error = np . zeros ( [ K , ] ) NEW_LINE error2 = np . zeros ( [ K , ] ) NEW_LINE error_pgr = 0.0 NEW_LINE error_katz = 0.0 NEW_LINE error_neigh = 0.0 NEW_LINE error_adam = 0.0 NEW_LINE suc_iters = 0 NEW_LINE for _iter in range ( iters ) : NEW_LINE INDENT A = gen_graph ( A_ideal , N ) NEW_LINE seq = get_powers ( A , K ) NEW_LINE seq2 = get_adj_powers ( A , K ) NEW_LINE if seq : NEW_LINE INDENT suc_iters += 1 NEW_LINE for k , P_k , A_k in zip ( range ( K ) , seq , seq2 ) : NEW_LINE INDENT for i in range ( N ) : NEW_LINE INDENT A_k [ i , i ] = 0.0 NEW_LINE P_k [ i , i ] = 0.0 NEW_LINE DEDENT error [ k ] += pearson ( A_ideal , P_k ) NEW_LINE error2 [ k ] += pearson ( A_ideal , A_k ) NEW_LINE DEDENT A_pgr = get_pgr ( A , a ) NEW_LINE A_katz = get_katz ( A , beta ) NEW_LINE A_neigh = get_neigh ( A ) NEW_LINE A_adam = get_adam ( A , a ) NEW_LINE error_pgr += pearson ( A_ideal , A_pgr ) NEW_LINE error_katz += pearson ( A_ideal , A_katz ) NEW_LINE error_neigh += pearson ( A_ideal , A_neigh ) NEW_LINE error_adam += pearson ( A_ideal , A_adam ) NEW_LINE DEDENT DEDENT plt . plot ( error / suc_iters , label = \" $ \\\\mathbf { S } ^ k $ \" ) NEW_LINE plt . plot ( error2 / suc_iters , label = \" $ \\\\mathbf { A } ^ k $ \" , marker = \\' o \\' ) NEW_LINE plt . plot ( ( error_pgr / suc_iters ) * np . ones ( [ K , ] ) , label = \" $ \\\\mathbf { S } _ { PGR } $ \" , marker = \\' + \\' ) NEW_LINE plt . plot ( ( error_katz / suc_iters ) * np . ones ( [ K , ] ) , label = \" $ \\\\mathbf { S } _ { KATZ } $ \" , marker = \\' * \\' ) NEW_LINE plt . plot ( ( error_neigh / suc_iters ) * np . ones ( [ K , ] ) , label = \" $ \\\\mathbf { S } _ { NEIGH } $ \" , marker = \\' > \\' ) NEW_LINE plt . plot ( ( error_adam / suc_iters ) * np . ones ( [ K , ] ) , label = \" $ \\\\mathbf { S } _ { AA } $ \" , marker = \\' ^ \\' ) NEW_LINE plt . xlabel ( \" k \" ) NEW_LINE plt . legend ( ) NEW_LINE tikz_save ( \" . . / figs / p _ \" + str ( int ( 100 * args . p ) ) + \" _ q _ \" + str ( int ( 100 * args . q ) ) + \" . tex \" ) NEW_LINE plt . show ( ) NEW_LINE DEDENT',)]\n",
            "0.967242781072855\n",
            "On batch 1791\n",
            "tensor([0.0165, 0.0167, 0.0171, 0.0169, 0.0163, 0.0169, 0.0165, 0.0166, 0.0166,\n",
            "        0.0167, 0.0168, 0.0163, 0.0163, 0.0163, 0.0166, 0.0167, 0.0166, 0.0164,\n",
            "        0.0163, 0.0166, 0.0169, 0.0163, 0.0168, 0.0166, 0.0163, 0.0166, 0.0166,\n",
            "        0.0163, 0.0170, 0.0170, 0.0168, 0.0168, 0.0169, 0.0169, 0.0169, 0.0169,\n",
            "        0.0168, 0.0169, 0.0168, 0.0167, 0.0164, 0.0163, 0.0165, 0.0165, 0.0168,\n",
            "        0.0167, 0.0168, 0.0167, 0.0165, 0.0166, 0.0168, 0.0167, 0.0169, 0.0168,\n",
            "        0.0166, 0.0167, 0.0170, 0.0167, 0.0167, 0.0169], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "('\\nJ_{\\\\varphi_0}^{(x)}(x,\\\\mu,\\\\gamma)=-J_F^{(v)}(x,\\\\mu,\\\\gamma,\\\\varphi_0(x,\\\\mu,\\\\gamma))^{-1}J_F^{(x)}(x,\\\\mu,\\\\gamma,\\\\varphi_0(x,\\\\mu,\\\\gamma)),\\n',)\n",
            "[('def my_compare_ssim ( x_true , x_est ) : NEW_LINE INDENT return compare_ssim ( x_true [ 6 : 250 , 6 : 250 , : ] , x_est [ 6 : 250 , 6 : 250 , : ] , gaussian_weights = True , data_range = 1 , multichannel = True ) NEW_LINE DEDENT',), ('def switch_test_config ( test_config ) : NEW_LINE INDENT switcher = { \\' GaussianA \\' : [ \\' gaussian _ 1_6\\' , \\' _ std _ 0008\\' , [ 0.008 , 0.008 ] ] , \\' GaussianB \\' : [ \\' gaussian _ 1_6\\' , \\' _ std _ 001_005\\' , [ 0.01 , 0.05 ] ] , \\' GaussianC \\' : [ \\' gaussian _ 3\\' , \\' _ std _ 004\\' , [ 0.04 , 0.04 ] ] , \\' MotionA \\' : [ \\' motion8\\' , \\' _ std _ 001\\' , [ 0.01 , 0.01 ] ] , \\' MotionB \\' : [ \\' motion3\\' , \\' _ std _ 001\\' , [ 0.01 , 0.01 ] ] , \\' Square \\' : [ \\' square _ 7\\' , \\' _ std _ 001\\' , [ 0.01 , 0.01 ] ] } NEW_LINE return switcher . get ( test_config , \" Invalid ▁ test ▁ configuration \" ) NEW_LINE DEDENT',), (\"def compute_ssim_results ( test_config , dataset , path_iRestNet ) : NEW_LINE INDENT name_kernel , name_std , noise_std_range = switch_test_config ( test_config ) NEW_LINE loss_type = ' SSIM ' NEW_LINE path_testset = os . path . join ( ' Datasets ' , ' Testsets ' ) NEW_LINE path_blurred = os . path . join ( path_testset , name_kernel + name_std , dataset ) NEW_LINE path_true = os . path . join ( ' Datasets ' , ' Groundtruth ' , ' cropped ' , dataset ) NEW_LINE file_names = os . listdir ( path_true ) NEW_LINE file_list = [ [ os . path . join ( path_true , i ) , os . path . join ( path_blurred , i ) , os . path . join ( path_iRestNet , i ) ] for i in file_names ] NEW_LINE ssim_blurred , ssim_iRestNet = np . zeros ( len ( file_names ) ) , np . zeros ( len ( file_names ) ) NEW_LINE for i in range ( 0 , len ( file_names ) ) : NEW_LINE INDENT x_true = sio . loadmat ( file_list [ i ] [ 0 ] ) [ ' image ' ] NEW_LINE x_blurred = sio . loadmat ( file_list [ i ] [ 1 ] ) [ ' image ' ] NEW_LINE x_iRestNet = sio . loadmat ( file_list [ i ] [ 2 ] ) [ ' image ' ] NEW_LINE ssim_blurred [ i ] = my_compare_ssim ( x_true , x_blurred ) NEW_LINE ssim_iRestNet [ i ] = my_compare_ssim ( x_true , x_iRestNet ) NEW_LINE DEDENT return np . mean ( ssim_blurred ) , np . mean ( ssim_iRestNet ) NEW_LINE DEDENT\",), (\"def create_testset ( name_set , path_groundtruth , path_testset , name_kernel , name_std , std_range , im_size ) : NEW_LINE INDENT np . random . seed ( 0 ) NEW_LINE torch . manual_seed ( 1 ) NEW_LINE dtype = torch . FloatTensor NEW_LINE chan = 3 NEW_LINE path_kernel = os . path . join ( path_testset , name_kernel + name_std , ' kernel . mat ' ) NEW_LINE path_save = os . path . join ( path_testset , name_kernel + name_std , name_set ) NEW_LINE if not os . path . exists ( path_save ) : NEW_LINE INDENT os . makedirs ( path_save ) NEW_LINE DEDENT kernel = sio . loadmat ( path_kernel ) NEW_LINE kernel = torch . from_numpy ( kernel [ name_kernel ] ) . type ( dtype ) NEW_LINE tens_kernel = torch . zeros ( chan , chan , kernel . size ( 0 ) , kernel . size ( 0 ) ) NEW_LINE for i in range ( chan ) : NEW_LINE INDENT tens_kernel [ i , i , : , : ] = kernel NEW_LINE DEDENT transf2 = transforms . Compose ( [ MyConv2d ( tens_kernel , mode = ' single ' ) . cuda ( ) , add_Gaussian_noise ( std_range ) ] ) NEW_LINE if os . path . exists ( os . path . join ( path_groundtruth , ' cropped ' , name_set ) ) : NEW_LINE INDENT transf1 = OpenMat_transf ( ) NEW_LINE already_cropped = ' yes ' NEW_LINE data = MyDataset ( folder = os . path . join ( path_groundtruth , ' cropped ' , name_set ) , transf1 = transf1 , transf2 = transf2 , need_names = ' yes ' ) NEW_LINE DEDENT else : NEW_LINE INDENT transf1 = transforms . Compose ( [ transforms . CenterCrop ( im_size ) , transforms . ToTensor ( ) ] ) NEW_LINE path_save_true = os . path . join ( path_groundtruth , ' cropped ' , name_set ) NEW_LINE if not os . path . exists ( path_save_true ) : NEW_LINE INDENT os . makedirs ( path_save_true ) NEW_LINE DEDENT already_cropped = ' no ' NEW_LINE data = MyDataset ( folder = os . path . join ( path_groundtruth , ' full ' , name_set ) , transf1 = transf1 , transf2 = transf2 , need_names = ' yes ' ) NEW_LINE DEDENT loader = DataLoader ( data , batch_size = 1 , shuffle = False ) NEW_LINE for minibatch in tqdm ( loader , file = sys . stdout ) : NEW_LINE INDENT [ image_name , x , x_degraded ] = minibatch NEW_LINE file_name_degraded = os . path . join ( path_save , image_name [ 0 ] ) NEW_LINE sio . savemat ( file_name_degraded , { ' image ' : x_degraded [ 0 ] . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE if x [ 0 ] . size ( 0 ) == 3 : NEW_LINE INDENT sio . savemat ( file_name_degraded , { ' image ' : x_degraded [ 0 ] . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE DEDENT elif x [ 0 ] . size ( 0 ) == 1 : NEW_LINE INDENT sio . savemat ( file_name_degraded , { ' image ' : x_degraded [ 0 , 0 ] . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE DEDENT if already_cropped == ' no ' : NEW_LINE INDENT file_name_true = os . path . join ( path_save_true , image_name [ 0 ] ) NEW_LINE if x [ 0 ] . size ( 0 ) == 3 : NEW_LINE INDENT sio . savemat ( file_name_true , { ' image ' : x [ 0 ] . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE DEDENT elif x [ 0 ] . size ( 0 ) == 1 : NEW_LINE INDENT sio . savemat ( file_name_true , { ' image ' : x [ 0 , 0 ] . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE DEDENT DEDENT DEDENT DEDENT\",), ('def compute_PSNR ( x_true , x ) : NEW_LINE INDENT return - 10 * torch . mean ( np . log10 ( torch . mean ( torch . mean ( torch . mean ( ( x_true - x ) ** 2 , 1 ) , 1 ) , 1 ) ) ) NEW_LINE DEDENT',), ('def compute_PSNR_SSIM ( x_true , x_before , x_after , size_set ) : NEW_LINE INDENT size_batch = x_true . data . size ( ) [ 0 ] NEW_LINE snr_before = compute_PSNR ( x_true . data , x_before . data ) * size_batch / size_set NEW_LINE snr_after = compute_PSNR ( x_true . data , x_after . data ) * size_batch / size_set NEW_LINE ssim_before = torch . Tensor . item ( compute_SSIM . ssim ( x_before , x_true ) ) * size_batch / size_set NEW_LINE ssim_after = torch . Tensor . item ( compute_SSIM . ssim ( x_after , x_true ) ) * size_batch / size_set NEW_LINE return np . array ( ( ( ( snr_before ) , ( snr_after ) ) , ( ( ssim_before ) , ( ssim_after ) ) ) ) NEW_LINE DEDENT',), ('def OpenMat ( x ) : NEW_LINE INDENT if len ( x . shape ) == 3 : NEW_LINE INDENT return torch . from_numpy ( x ) . permute ( 2 , 0 , 1 ) . type ( torch . FloatTensor ) NEW_LINE DEDENT elif len ( x . shape ) == 2 : NEW_LINE INDENT return torch . from_numpy ( x ) . type ( torch . FloatTensor ) . unsqueeze ( 0 ) NEW_LINE DEDENT DEDENT',), (\"def RGBToGray ( yes_no , x ) : NEW_LINE INDENT if yes_no == ' yes ' : NEW_LINE INDENT return ( 0.2989 * x [ 0 , : , : ] + 0.5870 * x [ 1 , : , : ] + 0.1140 * x [ 2 , : , : ] ) . unsqueeze ( 0 ) NEW_LINE DEDENT elif yes_no == ' no ' : NEW_LINE INDENT return x NEW_LINE DEDENT DEDENT\",), (\"def GrayToRGB ( x ) : NEW_LINE INDENT if x . size ( 0 ) < 3 : NEW_LINE INDENT return ' yes ' , x . repeat ( 3 , 1 , 1 ) NEW_LINE DEDENT elif x . size ( 0 ) == 3 : NEW_LINE INDENT return ' no ' , x NEW_LINE DEDENT DEDENT\",), ('def TensorFilter ( filt_list , c = 3 , dtype = torch . FloatTensor ) : NEW_LINE INDENT tens_filt_list = [ ] NEW_LINE for filt in filt_list : NEW_LINE INDENT tens_filt = torch . zeros ( c , c , filt . shape [ 0 ] , filt . shape [ 0 ] ) NEW_LINE for i in range ( c ) : NEW_LINE INDENT tens_filt [ i , i , : , : ] = torch . from_numpy ( filt ) . type ( dtype ) NEW_LINE DEDENT tens_filt_list . append ( tens_filt . clone ( ) ) NEW_LINE DEDENT return tens_filt_list NEW_LINE DEDENT',), (\"def TransposeSquareFilter ( filt , im_size , need_norm = ' no ' ) : NEW_LINE INDENT a = int ( ( im_size [ 0 ] - filt . shape [ 0 ] + 1 ) / 2 ) NEW_LINE b = a + filt . shape [ 0 ] NEW_LINE c = int ( np . floor ( filt . shape [ 0 ] / 2 ) ) NEW_LINE fourier_filt = np . zeros ( np . array ( im_size ) ) NEW_LINE fourier_filt [ a : b ] [ : , a : b ] = np . copy ( filt ) NEW_LINE fourier_filt = fft2 ( fftshift ( fourier_filt ) ) NEW_LINE transpose_filter = ifftshift ( ifft2 ( np . conj ( fourier_filt ) ) ) . real [ a : b , a : b ] NEW_LINE square_filter = ifftshift ( ifft2 ( np . absolute ( fourier_filt ) ** 2 ) ) . real [ a - c : b + c , a - c : b + c ] NEW_LINE if need_norm == ' yes ' : NEW_LINE INDENT return transpose_filter , square_filter , np . max ( np . absolute ( fourier_filt ) ** 2 ) NEW_LINE DEDENT else : NEW_LINE INDENT return transpose_filter , square_filter NEW_LINE DEDENT DEDENT\",), ('def __init__ ( self ) : NEW_LINE INDENT super ( OpenMat_transf , self ) . __init__ ( ) NEW_LINE DEDENT',), ('def __call__ ( self , x ) : NEW_LINE INDENT return OpenMat ( x ) NEW_LINE DEDENT',), ('def __init__ ( self , pad_size ) : NEW_LINE INDENT super ( CircularPadding , self ) . __init__ ( ) NEW_LINE self . pad_size = pad_size NEW_LINE DEDENT',), ('def forward ( self , batch ) : NEW_LINE INDENT h = batch . size ( ) [ 2 ] NEW_LINE w = batch . size ( ) [ 3 ] NEW_LINE z = torch . cat ( ( batch [ : , : , : , w - self . pad_size : w ] , batch , batch [ : , : , : , 0 : self . pad_size ] ) , 3 ) NEW_LINE z = torch . cat ( ( z [ : , : , h - self . pad_size : h , : ] , z , z [ : , : , 0 : self . pad_size , : ] ) , 2 ) NEW_LINE return Variable ( z ) NEW_LINE DEDENT',), (\"def __init__ ( self , kernel , mode , pad_type = ' circular ' , padding = 0 , stride = 1 ) : NEW_LINE INDENT super ( MyConv2d , self ) . __init__ ( ) NEW_LINE self . gpu = ' cuda : 0' NEW_LINE self . kernel = nn . Parameter ( kernel , requires_grad = False ) NEW_LINE self . mode = mode NEW_LINE self . stride = stride NEW_LINE if padding == 0 : NEW_LINE INDENT size_padding = int ( ( kernel [ 0 , 0 ] . size ( 0 ) - 1 ) / 2 ) NEW_LINE DEDENT else : NEW_LINE INDENT size_padding = padding NEW_LINE DEDENT if pad_type == ' replicate ' : NEW_LINE INDENT self . padding = nn . ReplicationPad2d ( size_padding ) NEW_LINE DEDENT if pad_type == ' circular ' : NEW_LINE INDENT self . padding = CircularPadding ( size_padding ) NEW_LINE DEDENT DEDENT\",), (\"def forward ( self , x ) : NEW_LINE INDENT if self . mode == ' single ' : NEW_LINE INDENT return F . conv2d ( self . padding ( x . unsqueeze ( 0 ) . cuda ( ) ) , self . kernel , stride = self . stride ) . data [ 0 ] NEW_LINE DEDENT if self . mode == ' batch ' : NEW_LINE INDENT return F . conv2d ( self . padding ( x . data ) , self . kernel , stride = self . stride ) NEW_LINE DEDENT DEDENT\",), ('def __init__ ( self , std_range ) : NEW_LINE INDENT super ( add_Gaussian_noise , self ) . __init__ ( ) NEW_LINE self . std_min = std_range [ 0 ] NEW_LINE self . std_max = std_range [ 1 ] NEW_LINE DEDENT',), ('def __call__ ( self , x ) : NEW_LINE INDENT std = np . random . uniform ( low = self . std_min , high = self . std_max ) NEW_LINE return x + torch . cuda . FloatTensor ( x . size ( ) ) . normal_ ( 0 , std ) NEW_LINE DEDENT',), (\"def __init__ ( self , folder = ' / path / to / folder / ' , transf1 = None , transf2 = None , need_names = ' no ' ) : NEW_LINE INDENT super ( MyDataset , self ) . __init__ ( ) NEW_LINE self . folder = folder NEW_LINE self . file_names = os . listdir ( self . folder ) NEW_LINE self . file_list = [ os . path . join ( self . folder , i ) for i in self . file_names ] NEW_LINE self . transf1 = transf1 NEW_LINE self . transf2 = transf2 NEW_LINE self . need_names = need_names NEW_LINE DEDENT\",), (\"def __getitem__ ( self , index ) : NEW_LINE INDENT if os . path . splitext ( self . file_names [ index ] ) [ 1 ] == ' . mat ' : NEW_LINE INDENT i = self . transf1 ( sio . loadmat ( self . file_list [ index ] ) [ ' image ' ] ) NEW_LINE DEDENT else : NEW_LINE INDENT i = self . transf1 ( Image . open ( self . file_list [ index ] ) ) NEW_LINE DEDENT if i . size ( 0 ) < 3 : NEW_LINE INDENT j = self . transf2 ( i . repeat ( 3 , 1 , 1 ) ) [ 0 ] . unsqueeze ( 0 ) NEW_LINE j = j . repeat ( 3 , 1 , 1 ) NEW_LINE DEDENT else : NEW_LINE INDENT j = self . transf2 ( i ) NEW_LINE DEDENT if self . need_names == ' no ' : NEW_LINE INDENT return i , j NEW_LINE DEDENT elif self . need_names == ' yes ' : NEW_LINE INDENT return os . path . splitext ( self . file_names [ index ] ) [ 0 ] , i , j NEW_LINE DEDENT DEDENT\",), ('def __len__ ( self ) : NEW_LINE INDENT return len ( self . file_list ) NEW_LINE DEDENT',), ('def __init__ ( self , folder_true , folder_previous_block , folder_Htxblurred , folder_std_approx ) : NEW_LINE INDENT super ( MyDataset_OneBlock , self ) . __init__ ( ) NEW_LINE self . file_names = os . listdir ( folder_true ) NEW_LINE self . file_list = [ [ os . path . join ( folder_true , i ) , os . path . join ( folder_previous_block , i ) , os . path . join ( folder_Htxblurred , i ) , os . path . join ( folder_std_approx , os . path . splitext ( i ) [ 0 ] + \" . txt \" ) ] for i in self . file_names ] NEW_LINE self . transf = transforms . ToTensor ( ) NEW_LINE DEDENT',), (\"def __getitem__ ( self , index ) : NEW_LINE INDENT return os . path . splitext ( self . file_names [ index ] ) [ 0 ] , self . transf ( sio . loadmat ( self . file_list [ index ] [ 0 ] ) [ ' image ' ] ) , self . transf ( sio . loadmat ( self . file_list [ index ] [ 1 ] ) [ ' image ' ] ) , self . transf ( sio . loadmat ( self . file_list [ index ] [ 2 ] ) [ ' image ' ] ) , torch . tensor ( np . loadtxt ( self . file_list [ index ] [ 3 ] ) ) NEW_LINE DEDENT\",), ('def __len__ ( self ) : NEW_LINE INDENT return len ( self . file_list ) NEW_LINE DEDENT',), ('def __init__ ( self , folder ) : NEW_LINE INDENT super ( MyTestset , self ) . __init__ ( ) NEW_LINE self . file_names = os . listdir ( folder ) NEW_LINE self . file_list = [ os . path . join ( folder , i ) for i in self . file_names ] NEW_LINE DEDENT',), (\"def __getitem__ ( self , index ) : NEW_LINE INDENT image_test = GrayToRGB ( OpenMat ( sio . loadmat ( self . file_list [ index ] ) [ ' image ' ] ) ) NEW_LINE return self . file_names [ index ] , image_test NEW_LINE DEDENT\",), ('def __len__ ( self ) : NEW_LINE INDENT return len ( self . file_list ) NEW_LINE DEDENT',), ('def forward ( ctx , gamma_mu , xtilde , im_range , mode_training = True ) : NEW_LINE INDENT dtype = torch . cuda . FloatTensor NEW_LINE size = xtilde . size ( ) NEW_LINE x1 , x2 , x3 = torch . zeros ( size ) . type ( dtype ) , torch . zeros ( size ) . type ( dtype ) , torch . zeros ( size ) . type ( dtype ) NEW_LINE crit , crit_compare = torch . zeros ( size ) . type ( dtype ) , torch . zeros ( size ) . type ( dtype ) NEW_LINE sol = torch . zeros ( size ) . type ( dtype ) , NEW_LINE torch_one = torch . ones ( size ) . type ( dtype ) NEW_LINE xmin , xmax = im_range NEW_LINE a = - ( xmin + xmax + xtilde ) NEW_LINE b = xmin * xmax + xtilde * ( xmin + xmax ) - 2 * gamma_mu NEW_LINE c = gamma_mu * ( xmin + xmax ) - xtilde * xmin * xmax NEW_LINE p = b - ( a ** 2 ) / 3 NEW_LINE q = c - a * b / 3 + 2 * ( a ** 3 ) / 27 NEW_LINE delta = ( p / 3 ) ** 3 + ( q / 2 ) ** 2 NEW_LINE ind = delta > 0 NEW_LINE z1 = - q [ ind ] / 2 NEW_LINE z2 = torch . sqrt ( delta [ ind ] ) NEW_LINE u = ( z1 + z2 ) . sign ( ) * torch . pow ( ( z1 + z2 ) . abs ( ) , 1 / 3 ) NEW_LINE v = ( z1 - z2 ) . sign ( ) * torch . pow ( ( z1 - z2 ) . abs ( ) , 1 / 3 ) NEW_LINE x1 [ ind ] = u + v NEW_LINE x2 [ ind ] = - ( u + v ) / 2 ; NEW_LINE x3 [ ind ] = - ( u + v ) / 2 ; NEW_LINE ind = delta == 0 NEW_LINE x1 [ ind ] = 3 * q [ ind ] / p [ ind ] NEW_LINE x2 [ ind ] = - 1.5 * q [ ind ] / p [ ind ] NEW_LINE x3 [ ind ] = - 1.5 * q [ ind ] / p [ ind ] NEW_LINE ind = delta < 0 NEW_LINE cos = ( - q [ ind ] / 2 ) * ( ( 27 / torch . pow ( p [ ind ] , 3 ) ) . abs ( ) ) . sqrt ( ) NEW_LINE cos [ cos < - 1 ] = 0 * cos [ cos < - 1 ] - 1 NEW_LINE cos [ cos > 1 ] = 0 * cos [ cos > 1 ] + 1 NEW_LINE phi = torch . acos ( cos ) NEW_LINE tau = 2 * ( ( p [ ind ] / 3 ) . abs ( ) ) . sqrt ( ) NEW_LINE x1 [ ind ] = tau * torch . cos ( phi / 3 ) NEW_LINE x2 [ ind ] = - tau * torch . cos ( ( phi + np . pi ) / 3 ) NEW_LINE x3 [ ind ] = - tau * torch . cos ( ( phi - np . pi ) / 3 ) NEW_LINE x1 = x1 - a / 3 NEW_LINE x2 = x2 - a / 3 NEW_LINE x3 = x3 - a / 3 NEW_LINE if ( x1 != x1 ) . any ( ) : NEW_LINE INDENT x1 [ x1 != x1 ] = 2 * xmax NEW_LINE DEDENT if ( x2 != x2 ) . any ( ) : NEW_LINE INDENT x2 [ x2 != x2 ] = 2 * xmax NEW_LINE DEDENT if ( x3 != x3 ) . any ( ) : NEW_LINE INDENT x3 [ x3 != x3 ] = 2 * xmax NEW_LINE DEDENT sol = x1 NEW_LINE x_ok = ( x1 > xmin ) & ( x1 < xmax ) NEW_LINE crit [ 1 - x_ok ] = np . inf NEW_LINE crit [ x_ok ] = - ( torch . log ( x1 [ x_ok ] - xmin ) + torch . log ( xmax - x1 [ x_ok ] ) ) NEW_LINE crit = 0.5 * ( x1 - xtilde ) ** 2 + gamma_mu * crit NEW_LINE x_ok = ( x2 > xmin ) & ( x2 < xmax ) NEW_LINE crit_compare [ 1 - x_ok ] = np . inf NEW_LINE crit_compare [ x_ok ] = - ( torch . log ( x2 [ x_ok ] - xmin ) + torch . log ( xmax - x2 [ x_ok ] ) ) NEW_LINE crit_compare = 0.5 * ( x2 - xtilde ) ** 2 + gamma_mu * crit_compare NEW_LINE sol [ crit_compare <= crit ] = x2 [ crit_compare <= crit ] NEW_LINE crit [ crit_compare <= crit ] = crit_compare [ crit_compare <= crit ] NEW_LINE x_ok = ( x3 > xmin ) & ( x3 < xmax ) NEW_LINE crit_compare [ 1 - x_ok ] = np . inf NEW_LINE crit_compare [ x_ok ] = - ( torch . log ( x3 [ x_ok ] - xmin ) + torch . log ( xmax - x3 [ x_ok ] ) ) NEW_LINE crit_compare = 0.5 * ( x3 - xtilde ) ** 2 + gamma_mu * crit_compare NEW_LINE sol [ crit_compare <= crit ] = x3 [ crit_compare <= crit ] NEW_LINE crit [ crit_compare <= crit ] = crit_compare [ crit_compare <= crit ] NEW_LINE crit_compare = ( 0.5 * ( xmin + 1e-10 - xtilde ) ** 2 ) * torch_one - gamma_mu * ( torch . log ( 1e-10 * torch_one ) + torch . log ( ( xmax - xmin - 1e-10 ) * torch_one ) ) NEW_LINE sol [ crit_compare <= crit ] = 0 * sol [ crit_compare <= crit ] + ( xmin + 1e-10 ) NEW_LINE crit [ crit_compare <= crit ] = crit_compare [ crit_compare <= crit ] NEW_LINE crit_compare = ( 0.5 * ( xmax - 1e-10 - xtilde ) ** 2 ) * torch_one - gamma_mu * ( torch . log ( 1e-10 * torch_one ) + torch . log ( ( xmax - xmin - 1e-10 ) * torch_one ) ) NEW_LINE sol [ crit_compare <= crit ] = 0 * sol [ crit_compare <= crit ] + ( xmax - 1e-10 ) NEW_LINE crit [ crit_compare <= crit ] = crit_compare [ crit_compare <= crit ] NEW_LINE xtilde_ok = ( xtilde > xmin ) & ( xtilde < xmax ) NEW_LINE crit_compare [ 1 - xtilde_ok ] = np . inf NEW_LINE crit_compare [ xtilde_ok ] = - ( torch . log ( xmax - xtilde [ xtilde_ok ] ) + torch . log ( xtilde [ xtilde_ok ] - xmin ) ) NEW_LINE crit_compare = gamma_mu * crit_compare NEW_LINE sol [ crit_compare < crit ] = xtilde [ crit_compare < crit ] NEW_LINE if mode_training == True : NEW_LINE INDENT ctx . save_for_backward ( gamma_mu , xtilde , sol ) NEW_LINE DEDENT return sol NEW_LINE DEDENT',), (\"def backward ( ctx , grad_output_var ) : NEW_LINE INDENT xmin = 0 NEW_LINE xmax = 1 NEW_LINE dtype = torch . cuda . FloatTensor NEW_LINE grad_output = grad_output_var . data NEW_LINE gamma_mu , u , x = ctx . saved_tensors NEW_LINE denom = ( x - xmin ) * ( x - xmax ) - 2 * gamma_mu - ( x - u ) * ( xmin + xmax - 2 * x ) NEW_LINE idx = denom . abs ( ) > 1e-7 NEW_LINE denom [ 1 - idx ] = denom [ 1 - idx ] + 1 NEW_LINE grad_input_gamma_mu = ( 2 * x - ( xmin + xmax ) ) / denom NEW_LINE grad_input_u = ( ( x ** 2 - x * ( xmin + xmax ) + xmin * xmax ) ) / denom NEW_LINE grad_input_gamma_mu [ 1 - idx ] = 0 * grad_input_gamma_mu [ 1 - idx ] + 1e5 * torch . sign ( 2 * x [ 1 - idx ] - ( xmin + xmax ) ) NEW_LINE grad_input_u [ 1 - idx ] = 0 * grad_input_u [ 1 - idx ] + 1 NEW_LINE grad_input_gamma_mu = ( grad_input_gamma_mu * grad_output ) . sum ( 1 ) . sum ( 1 ) . sum ( 1 ) . unsqueeze ( 1 ) . unsqueeze ( 2 ) . unsqueeze ( 3 ) NEW_LINE grad_input_u = grad_input_u * grad_output NEW_LINE if ( grad_input_gamma_mu != grad_input_gamma_mu ) . any ( ) : NEW_LINE INDENT print ( ' there ▁ is ▁ a ▁ nan ▁ in ▁ grad _ input _ gamma _ mu ' ) NEW_LINE if ( x != x ) . any ( ) : NEW_LINE INDENT print ( ' there ▁ is ▁ a ▁ nan ▁ in ▁ x ' ) NEW_LINE DEDENT sys . exit ( ) NEW_LINE DEDENT if ( grad_input_u != grad_input_u ) . any ( ) : NEW_LINE INDENT print ( ' there ▁ is ▁ a ▁ nan ▁ in ▁ grad _ input _ u ' ) NEW_LINE sys . exit ( ) NEW_LINE DEDENT grad_input_gamma_mu = Variable ( grad_input_gamma_mu . type ( dtype ) , requires_grad = True ) NEW_LINE grad_input_u = Variable ( grad_input_u . type ( dtype ) , requires_grad = True ) NEW_LINE return grad_input_gamma_mu , grad_input_u , None , None NEW_LINE DEDENT\",), (\"def __init__ ( self , test_conditions , folders , mode = ' first _ layer ' , lr_first_layer = [ 1e-2 , 5 ] , lr_greedy = [ 5e-3 , 5 ] , lr_last_layers_lpp = [ 1e-3 , 50 ] , nb_epochs = [ 40 , 40 , 600 ] , nb_blocks = 40 , nb_greedy_blocks = 30 , batch_size = [ 10 , 10 , 1 ] , loss_type = ' SSIM ' ) : NEW_LINE INDENT super ( iRestNet_class , self ) . __init__ ( ) NEW_LINE self . name_kernel , self . name_std , self . noise_std_range , self . im_size , im_range = test_conditions NEW_LINE self . path_test , self . path_train , self . path_save = folders NEW_LINE self . mode = mode NEW_LINE self . lr_first_layer = lr_first_layer NEW_LINE self . lr_greedy = lr_greedy NEW_LINE self . lr_last_layers_lpp = lr_last_layers_lpp NEW_LINE self . nb_epochs = nb_epochs NEW_LINE self . nb_blocks = nb_blocks NEW_LINE self . nb_greedy_blocks = nb_greedy_blocks NEW_LINE self . batch_size = batch_size NEW_LINE self . loss_type = loss_type NEW_LINE self . kernel = sio . loadmat ( os . path . join ( self . path_test , self . name_kernel + self . name_std , ' kernel . mat ' ) ) NEW_LINE kernel_t , kernel_2 = TransposeSquareFilter ( self . kernel [ self . name_kernel ] , self . im_size ) NEW_LINE self . kernel , kernel_t , self . kernel_2 = TensorFilter ( [ self . kernel [ self . name_kernel ] , kernel_t , kernel_2 ] ) NEW_LINE if self . mode != ' test ' : NEW_LINE INDENT if self . loss_type == ' SSIM ' : NEW_LINE INDENT self . loss_fun = SSIM_loss ( ) NEW_LINE DEDENT elif self . loss_type == ' MSE ' : NEW_LINE INDENT self . loss_fun = torch . nn . MSELoss ( size_average = True ) NEW_LINE DEDENT DEDENT self . dtype = torch . cuda . FloatTensor NEW_LINE self . Ht = MyConv2d ( kernel_t , ' batch ' ) . cuda ( ) NEW_LINE self . model = myModel ( im_range , self . kernel_2 , self . dtype , self . nb_blocks ) . cuda ( ) NEW_LINE self . last_layer = myLastLayer ( ) . cuda ( ) NEW_LINE self . sigmoid = nn . Sigmoid ( ) NEW_LINE DEDENT\",), (\"def CreateLoader ( self ) : NEW_LINE INDENT if self . mode == ' first _ layer ' : NEW_LINE INDENT transf1 = transforms . Compose ( [ transforms . RandomCrop ( self . im_size ) , transforms . ToTensor ( ) ] ) NEW_LINE transf2 = transforms . Compose ( [ MyConv2d ( self . kernel , ' single ' ) . cuda ( ) , add_Gaussian_noise ( self . noise_std_range ) ] ) NEW_LINE train_data = MyDataset ( folder = os . path . join ( self . path_train , ' train ' ) , transf1 = transf1 , transf2 = transf2 , need_names = ' yes ' ) NEW_LINE val_data = MyDataset ( folder = os . path . join ( self . path_train , ' val ' ) , transf1 = transf1 , transf2 = transf2 , need_names = ' yes ' ) NEW_LINE DEDENT elif self . mode == ' greedy ' or self . mode == ' last _ layers _ lpp ' : NEW_LINE INDENT folder_temp = os . path . join ( self . path_save , ' ImagesLastBlock ' ) NEW_LINE train_data = MyDataset_OneBlock ( folder_true = os . path . join ( folder_temp , ' train ' , ' true ' ) , folder_previous_block = os . path . join ( folder_temp , ' train ' , ' previous _ block ' ) , folder_Htxblurred = os . path . join ( folder_temp , ' train ' , ' Htxblurred ' ) , folder_std_approx = os . path . join ( folder_temp , ' train ' , ' std _ approx ' ) ) NEW_LINE val_data = MyDataset_OneBlock ( folder_true = os . path . join ( folder_temp , ' val ' , ' true ' ) , folder_previous_block = os . path . join ( folder_temp , ' val ' , ' previous _ block ' ) , folder_Htxblurred = os . path . join ( folder_temp , ' val ' , ' Htxblurred ' ) , folder_std_approx = os . path . join ( folder_temp , ' val ' , ' std _ approx ' ) ) NEW_LINE DEDENT self . train_loader = DataLoader ( train_data , batch_size = self . batch_size [ 0 ] , shuffle = True ) NEW_LINE self . val_loader = DataLoader ( val_data , batch_size = self . batch_size [ 1 ] , shuffle = False ) NEW_LINE self . size_train = len ( [ n for n in os . listdir ( os . path . join ( self . path_train , ' train ' ) ) ] ) NEW_LINE self . size_val = len ( [ n for n in os . listdir ( os . path . join ( self . path_train , ' val ' ) ) ] ) NEW_LINE DEDENT\",), (\"def CreateFolders ( self , block ) : NEW_LINE INDENT if self . mode == ' first _ layer ' or self . mode == ' greedy ' : NEW_LINE INDENT name = ' block _ ' + str ( block ) NEW_LINE if not os . path . exists ( os . path . join ( self . path_save , name ) ) : NEW_LINE INDENT os . makedirs ( os . path . join ( self . path_save , name , ' training ' ) ) NEW_LINE DEDENT DEDENT elif self . mode == ' last _ layers _ lpp ' : NEW_LINE INDENT name = ' block _ ' + str ( block ) + ' _ ' + str ( self . nb_blocks - 1 ) + ' _ lpp ' NEW_LINE if not os . path . exists ( os . path . join ( self . path_save , name ) ) : NEW_LINE INDENT os . makedirs ( os . path . join ( self . path_save , name , ' training ' ) ) NEW_LINE DEDENT DEDENT if self . mode != ' test ' : NEW_LINE INDENT folder = os . path . join ( self . path_save , ' ImagesLastBlock ' ) NEW_LINE if not os . path . exists ( folder ) : NEW_LINE INDENT subfolders = [ ' train ' , ' val ' ] NEW_LINE subsubfolders = [ ' true ' , ' previous _ block ' , ' Htxblurred ' , ' std _ approx ' ] NEW_LINE paths = [ os . path . join ( folder , sub , subsub ) for sub in subfolders for subsub in subsubfolders ] NEW_LINE for path in paths : NEW_LINE INDENT os . makedirs ( path ) NEW_LINE DEDENT DEDENT DEDENT DEDENT\",), (\"def train ( self , block = 0 ) : NEW_LINE INDENT if self . mode == ' first _ layer ' : NEW_LINE INDENT print ( ' = = = = = = = = = = = = = = = = = = = ▁ Block ▁ number ▁ % d ▁ = = = = = = = = = = = = = = = = = = = ' % ( 0 ) ) NEW_LINE loss_epochs = np . zeros ( self . nb_epochs [ 0 ] ) NEW_LINE psnr_ssim_train = np . zeros ( ( 2 , 2 , self . nb_epochs [ 0 ] ) ) NEW_LINE psnr_ssim_val = np . zeros ( ( 2 , 2 , self . nb_epochs [ 0 ] ) ) NEW_LINE loss_min_val = float ( ' Inf ' ) NEW_LINE self . CreateFolders ( 0 ) NEW_LINE folder = os . path . join ( self . path_save , ' block _ ' + str ( 0 ) ) NEW_LINE self . CreateLoader ( ) NEW_LINE lr = self . lr_first_layer [ 0 ] NEW_LINE optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ( ) ) , lr = lr ) NEW_LINE for epoch in range ( 0 , self . nb_epochs [ 0 ] ) : NEW_LINE INDENT self . model . Layers [ 0 ] . train ( ) NEW_LINE gc . collect ( ) NEW_LINE if epoch % self . lr_first_layer [ 1 ] == 0 and epoch > 0 : NEW_LINE INDENT lr = lr * 0.9 NEW_LINE optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ( ) ) , lr = lr ) NEW_LINE DEDENT for i , minibatch in enumerate ( self . train_loader , 0 ) : NEW_LINE INDENT [ names , x_true , x_blurred ] = minibatch NEW_LINE x_true = Variable ( x_true . type ( self . dtype ) , requires_grad = False ) NEW_LINE x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = self . Ht ( x_blurred ) . detach ( ) NEW_LINE x_pred = self . model ( x_blurred , Ht_x_blurred , self . mode ) NEW_LINE loss = self . loss_fun ( x_pred , x_true ) NEW_LINE loss_epochs [ epoch ] += torch . Tensor . item ( loss ) NEW_LINE sys . stdout . write ( ' ( % d , ▁ % 3d ) ▁ minibatch ▁ loss : ▁ % 5.4f ▁ ' % ( epoch , i , torch . Tensor . item ( loss ) ) ) NEW_LINE optimizer . zero_grad ( ) NEW_LINE loss . backward ( ) NEW_LINE optimizer . step ( ) NEW_LINE psnr_ssim_train [ : , : , epoch ] += compute_PSNR_SSIM ( x_true , x_blurred , x_pred , self . size_train ) NEW_LINE DEDENT if epoch % 20 == 0 : NEW_LINE INDENT utils . save_image ( x_pred . data , os . path . join ( folder , ' training ' , str ( epoch ) + ' _ restored _ images . png ' ) , normalize = True ) NEW_LINE DEDENT torch . save ( self . last_layer . state_dict ( ) , os . path . join ( folder , ' trained _ post - processing . pt ' ) ) NEW_LINE torch . save ( self . model . state_dict ( ) , os . path . join ( folder , ' trained _ model . pt ' ) ) NEW_LINE self . model . eval ( ) NEW_LINE self . last_layer . eval ( ) NEW_LINE psnr_ssim = np . zeros ( ( 2 , 2 ) ) NEW_LINE nb_im = 0 NEW_LINE loss_current_val = 0 NEW_LINE for minibatch in self . val_loader : NEW_LINE INDENT [ names , x_true , x_blurred ] = minibatch NEW_LINE x_true = Variable ( x_true . type ( self . dtype ) , requires_grad = False ) NEW_LINE x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = self . Ht ( x_blurred ) . detach ( ) NEW_LINE x_pred = self . model ( x_blurred , Ht_x_blurred , self . mode ) NEW_LINE loss_current_val += torch . Tensor . item ( self . loss_fun ( x_pred , x_true ) ) NEW_LINE psnr_ssim += compute_PSNR_SSIM ( x_true , x_blurred , x_pred , self . size_val ) NEW_LINE DEDENT if loss_min_val > loss_current_val : NEW_LINE INDENT torch . save ( self . last_layer . state_dict ( ) , os . path . join ( folder , ' trained _ post - processing _ MinLossOnVal . pt ' ) ) NEW_LINE torch . save ( self . model . state_dict ( ) , os . path . join ( folder , ' trained _ model _ MinLossOnVal . pt ' ) ) NEW_LINE loss_min_val = loss_current_val NEW_LINE DEDENT psnr_ssim_val [ : , : , epoch ] = psnr_ssim NEW_LINE self . PrintStatistics ( psnr_ssim_train [ : , : , epoch ] , psnr_ssim_val [ : , : , epoch ] , epoch , loss_epochs [ epoch ] , lr ) NEW_LINE self . SaveLoss_PSNR_SSIM ( epoch , loss_epochs , psnr_ssim_train , psnr_ssim_val , self . mode ) NEW_LINE DEDENT print ( ' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ' ) NEW_LINE print ( ' Training ▁ of ▁ Block ▁ 0 ▁ is ▁ done . ' ) NEW_LINE self . SaveLoss_PSNR_SSIM ( epoch , loss_epochs , psnr_ssim_train , psnr_ssim_val , self . mode ) NEW_LINE self . save_OneBlock ( ) NEW_LINE print ( ' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ' ) NEW_LINE self . mode = ' greedy ' NEW_LINE self . train ( block = 1 ) NEW_LINE DEDENT elif self . mode == ' greedy ' : NEW_LINE INDENT print ( ' = = = = = = = = = = = = = = = = = = = ▁ Block ▁ number ▁ % d ▁ = = = = = = = = = = = = = = = = = = = ' % ( block ) ) NEW_LINE loss_epochs = np . zeros ( self . nb_epochs [ 1 ] ) NEW_LINE psnr_ssim_train = np . zeros ( ( 2 , 2 , self . nb_epochs [ 1 ] ) ) NEW_LINE psnr_ssim_val = np . zeros ( ( 2 , 2 , self . nb_epochs [ 1 ] ) ) NEW_LINE loss_min_val = float ( ' Inf ' ) NEW_LINE self . CreateFolders ( block ) NEW_LINE folder = os . path . join ( self . path_save , ' block _ ' + str ( block ) ) NEW_LINE self . CreateLoader ( ) NEW_LINE self . model . GradFalse ( block , self . mode ) NEW_LINE lr = self . lr_greedy [ 0 ] NEW_LINE optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ( ) ) , lr = lr ) NEW_LINE for epoch in range ( 0 , self . nb_epochs [ 1 ] ) : NEW_LINE INDENT self . model . Layers [ block ] . train ( ) NEW_LINE gc . collect ( ) NEW_LINE if epoch % self . lr_greedy [ 1 ] == 0 and epoch > 0 : NEW_LINE INDENT lr = lr * 0.9 NEW_LINE optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ( ) ) , lr = lr ) NEW_LINE DEDENT for i , minibatch in enumerate ( self . train_loader , 0 ) : NEW_LINE INDENT [ names , x_true , x_blurred , Ht_x_blurred , std_approx ] = minibatch NEW_LINE x_true = Variable ( x_true . type ( self . dtype ) , requires_grad = False ) NEW_LINE x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = Variable ( Ht_x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE std_approx = std_approx . type ( self . dtype ) NEW_LINE x_pred = self . model ( x_blurred , Ht_x_blurred , self . mode , block = block , std_approx = std_approx ) NEW_LINE loss = self . loss_fun ( x_pred , x_true ) NEW_LINE loss_epochs [ epoch ] += torch . Tensor . item ( loss ) NEW_LINE sys . stdout . write ( ' ( % d , ▁ % 3d ) ▁ minibatch ▁ loss : ▁ % 5.4f ▁ ' % ( epoch , i , torch . Tensor . item ( loss ) ) ) NEW_LINE optimizer . zero_grad ( ) NEW_LINE loss . backward ( ) NEW_LINE optimizer . step ( ) NEW_LINE psnr_ssim_train [ : , : , epoch ] += compute_PSNR_SSIM ( x_true , x_blurred , x_pred , self . size_train ) NEW_LINE DEDENT if epoch % 20 == 0 : NEW_LINE INDENT utils . save_image ( x_pred . data , os . path . join ( folder , ' training ' , str ( epoch ) + ' _ restored _ images . png ' ) , normalize = True ) NEW_LINE DEDENT torch . save ( self . last_layer . state_dict ( ) , os . path . join ( folder , ' trained _ post - processing . pt ' ) ) NEW_LINE torch . save ( self . model . state_dict ( ) , os . path . join ( folder , ' trained _ model . pt ' ) ) NEW_LINE self . model . eval ( ) NEW_LINE self . last_layer . eval ( ) NEW_LINE psnr_ssim = np . zeros ( ( 2 , 2 ) ) NEW_LINE nb_im = 0 NEW_LINE loss_current_val = 0 NEW_LINE for minibatch in self . val_loader : NEW_LINE INDENT [ names , x_true , x_blurred , Ht_x_blurred , std_approx ] = minibatch NEW_LINE x_true = Variable ( x_true . type ( self . dtype ) , requires_grad = False ) NEW_LINE x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = Variable ( Ht_x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE std_approx = std_approx . type ( self . dtype ) NEW_LINE x_pred = self . model ( x_blurred , Ht_x_blurred , self . mode , block = block , std_approx = std_approx ) NEW_LINE loss_current_val += torch . Tensor . item ( self . loss_fun ( x_pred , x_true ) ) NEW_LINE psnr_ssim += compute_PSNR_SSIM ( x_true , x_blurred , x_pred , self . size_val ) NEW_LINE DEDENT if loss_min_val > loss_current_val : NEW_LINE INDENT torch . save ( self . last_layer . state_dict ( ) , os . path . join ( folder , ' trained _ post - processing _ MinLossOnVal . pt ' ) ) NEW_LINE torch . save ( self . model . state_dict ( ) , os . path . join ( folder , ' trained _ model _ MinLossOnVal . pt ' ) ) NEW_LINE loss_min_val = loss_current_val NEW_LINE DEDENT psnr_ssim_val [ : , : , epoch ] = psnr_ssim NEW_LINE self . PrintStatistics ( psnr_ssim_train [ : , : , epoch ] , psnr_ssim_val [ : , : , epoch ] , epoch , loss_epochs [ epoch ] , lr ) NEW_LINE self . SaveLoss_PSNR_SSIM ( epoch , loss_epochs , psnr_ssim_train , psnr_ssim_val , self . mode , block = block ) NEW_LINE DEDENT print ( ' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ' ) NEW_LINE print ( ' Training ▁ of ▁ Block ▁ ' + str ( block ) + ' ▁ is ▁ done . ' ) NEW_LINE self . SaveLoss_PSNR_SSIM ( epoch , loss_epochs , psnr_ssim_train , psnr_ssim_val , self . mode , block = block ) NEW_LINE self . save_OneBlock ( block = block ) NEW_LINE print ( ' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ' ) NEW_LINE if block == self . nb_greedy_blocks - 1 : NEW_LINE INDENT self . mode = ' last _ layers _ lpp ' NEW_LINE self . train ( ) NEW_LINE DEDENT else : NEW_LINE INDENT self . train ( block = block + 1 ) NEW_LINE DEDENT DEDENT elif self . mode == ' last _ layers _ lpp ' : NEW_LINE INDENT print ( ' = = = = = = = = = = = = = = = = = = = ▁ Blocks ▁ % d ▁ to ▁ % d ▁ and ▁ lpp ▁ = = = = = = = = = = = = = = = = = = = ' % ( self . nb_greedy_blocks , self . nb_blocks - 1 ) ) NEW_LINE loss_epochs = np . zeros ( self . nb_epochs [ 2 ] ) NEW_LINE psnr_ssim_train = np . zeros ( ( 2 , 2 , self . nb_epochs [ 2 ] ) ) NEW_LINE psnr_ssim_val = np . zeros ( ( 2 , 2 , self . nb_epochs [ 2 ] ) ) NEW_LINE loss_min_val = float ( ' Inf ' ) NEW_LINE self . CreateFolders ( self . nb_greedy_blocks ) NEW_LINE folder = os . path . join ( self . path_save , ' block _ ' + str ( self . nb_greedy_blocks ) + ' _ ' + str ( self . nb_blocks - 1 ) + ' _ lpp ' ) NEW_LINE self . CreateLoader ( ) NEW_LINE self . model . GradFalse ( self . nb_greedy_blocks , self . mode ) NEW_LINE lr = self . lr_last_layers_lpp [ 0 ] NEW_LINE optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ( ) ) , lr = lr ) NEW_LINE for epoch in range ( 0 , self . nb_epochs [ 2 ] ) : NEW_LINE INDENT for k in range ( self . nb_greedy_blocks , self . nb_blocks ) : NEW_LINE INDENT self . model . Layers [ k ] . train ( ) NEW_LINE DEDENT self . last_layer . train ( ) NEW_LINE gc . collect ( ) NEW_LINE if epoch % self . lr_last_layers_lpp [ 1 ] == 0 and epoch > 0 : NEW_LINE INDENT lr = lr * 0.9 NEW_LINE optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ( ) ) , lr = lr ) NEW_LINE DEDENT for i , minibatch in enumerate ( self . train_loader , 0 ) : NEW_LINE INDENT [ names , x_true , x_blurred , Ht_x_blurred , std_approx ] = minibatch NEW_LINE x_true = Variable ( x_true . type ( self . dtype ) , requires_grad = False ) NEW_LINE x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = Variable ( Ht_x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE std_approx = std_approx . type ( self . dtype ) NEW_LINE x_last_block = self . model ( x_blurred , Ht_x_blurred , self . mode , block = self . nb_greedy_blocks , std_approx = std_approx ) NEW_LINE x_pred = self . sigmoid ( x_last_block + self . last_layer ( x_last_block ) ) NEW_LINE loss = self . loss_fun ( x_pred , x_true ) NEW_LINE loss_epochs [ epoch ] += torch . Tensor . item ( loss ) NEW_LINE sys . stdout . write ( ' ( % d , ▁ % 3d ) ▁ minibatch ▁ loss : ▁ % 5.4f ▁ ' % ( epoch , i , torch . Tensor . item ( loss ) ) ) NEW_LINE optimizer . zero_grad ( ) NEW_LINE loss . backward ( ) NEW_LINE optimizer . step ( ) NEW_LINE psnr_ssim_train [ : , : , epoch ] += compute_PSNR_SSIM ( x_true , x_blurred , x_pred , self . size_train ) NEW_LINE DEDENT if epoch % 20 == 0 : NEW_LINE INDENT utils . save_image ( x_pred . data , os . path . join ( folder , ' training ' , str ( epoch ) + ' _ restored _ images . png ' ) , normalize = True ) NEW_LINE DEDENT torch . save ( self . last_layer . state_dict ( ) , os . path . join ( folder , ' trained _ post - processing . pt ' ) ) NEW_LINE torch . save ( self . model . state_dict ( ) , os . path . join ( folder , ' trained _ model . pt ' ) ) NEW_LINE self . model . eval ( ) NEW_LINE self . last_layer . eval ( ) NEW_LINE psnr_ssim = np . zeros ( ( 2 , 2 ) ) NEW_LINE nb_im = 0 NEW_LINE loss_current_val = 0 NEW_LINE for minibatch in self . val_loader : NEW_LINE INDENT [ names , x_true , x_blurred , Ht_x_blurred , std_approx ] = minibatch NEW_LINE x_true = Variable ( x_true . type ( self . dtype ) , requires_grad = False ) NEW_LINE x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = Variable ( Ht_x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE std_approx = std_approx . type ( self . dtype ) NEW_LINE x_last_block = self . model ( x_blurred , Ht_x_blurred , self . mode , block = self . nb_greedy_blocks , std_approx = std_approx ) NEW_LINE x_pred = self . sigmoid ( x_last_block + self . last_layer ( x_last_block ) ) NEW_LINE loss_current_val += torch . Tensor . item ( self . loss_fun ( x_pred , x_true ) ) NEW_LINE psnr_ssim += compute_PSNR_SSIM ( x_true , x_blurred , x_pred , self . size_val ) NEW_LINE DEDENT if loss_min_val > loss_current_val : NEW_LINE INDENT torch . save ( self . last_layer . state_dict ( ) , os . path . join ( folder , ' trained _ post - processing _ MinLossOnVal . pt ' ) ) NEW_LINE torch . save ( self . model . state_dict ( ) , os . path . join ( folder , ' trained _ model _ MinLossOnVal . pt ' ) ) NEW_LINE loss_min_val = loss_current_val NEW_LINE DEDENT psnr_ssim_val [ : , : , epoch ] = psnr_ssim NEW_LINE self . PrintStatistics ( psnr_ssim_train [ : , : , epoch ] , psnr_ssim_val [ : , : , epoch ] , epoch , loss_epochs [ epoch ] , lr ) NEW_LINE self . SaveLoss_PSNR_SSIM ( epoch , loss_epochs , psnr_ssim_train , psnr_ssim_val , self . mode ) NEW_LINE DEDENT print ( ' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ' ) NEW_LINE print ( ' Training ▁ of ▁ last ▁ layers ▁ and ▁ lpp ▁ is ▁ done . ' ) NEW_LINE self . SaveLoss_PSNR_SSIM ( epoch , loss_epochs , psnr_ssim_train , psnr_ssim_val , self . mode ) NEW_LINE print ( ' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ' ) NEW_LINE return NEW_LINE DEDENT DEDENT\",), (\"def save_OneBlock ( self , block = 0 ) : NEW_LINE INDENT self . model . eval ( ) NEW_LINE Haar_filt = TensorFilter ( [ np . array ( ( ( 0.5 , - 0.5 ) , ( - 0.5 , 0.5 ) ) ) ] ) [ 0 ] NEW_LINE Haar = MyConv2d ( Haar_filt , ' batch ' , pad_type = ' circular ' , padding = 1 , stride = 2 ) . cuda ( ) NEW_LINE folder = os . path . join ( self . path_save , ' ImagesLastBlock ' ) NEW_LINE for minibatch in self . train_loader : NEW_LINE INDENT if self . mode == ' first _ layer ' : NEW_LINE INDENT [ names , x_true , x_blurred ] = minibatch NEW_LINE x_true = Variable ( x_true . type ( self . dtype ) , requires_grad = False ) NEW_LINE x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = self . Ht ( x_blurred ) . detach ( ) NEW_LINE x_pred = self . model ( x_blurred , Ht_x_blurred , self . mode ) NEW_LINE y = torch . abs ( Haar ( x_blurred ) ) . view ( x_blurred . data . shape [ 0 ] , - 1 ) . data / 0.6745 NEW_LINE std_approx = torch . topk ( y , ceil ( y . shape [ 1 ] / 2 ) , 1 ) [ 0 ] [ : , - 1 ] NEW_LINE for kk in range ( len ( names ) ) : NEW_LINE INDENT sio . savemat ( os . path . join ( folder , ' train ' , ' true ' , names [ kk ] ) , { ' image ' : x_true . data [ kk ] . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE sio . savemat ( os . path . join ( folder , ' train ' , ' previous _ block ' , names [ kk ] ) , { ' image ' : x_pred . data [ kk ] . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE sio . savemat ( os . path . join ( folder , ' train ' , ' Htxblurred ' , names [ kk ] ) , { ' image ' : Ht_x_blurred . data [ kk ] . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE np . savetxt ( os . path . join ( folder , ' train ' , ' std _ approx ' , names [ kk ] + ' . txt ' ) , np . array ( [ std_approx [ kk ] . cpu ( ) . numpy ( ) ] ) ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT [ names , x_true , x_blurred , Ht_x_blurred , std_approx ] = minibatch NEW_LINE x_true = Variable ( x_true . type ( self . dtype ) , requires_grad = False ) NEW_LINE x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = Variable ( Ht_x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE std_approx = std_approx . type ( self . dtype ) NEW_LINE x_pred = self . model ( x_blurred , Ht_x_blurred , self . mode , block = block , std_approx = std_approx ) NEW_LINE for kk in range ( len ( names ) ) : NEW_LINE INDENT sio . savemat ( os . path . join ( folder , ' train ' , ' previous _ block ' , names [ kk ] ) , { ' image ' : x_pred . data [ kk ] . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE DEDENT DEDENT DEDENT for minibatch in self . val_loader : NEW_LINE INDENT if self . mode == ' first _ layer ' : NEW_LINE INDENT [ names , x_true , x_blurred ] = minibatch NEW_LINE x_true = Variable ( x_true . type ( self . dtype ) , requires_grad = False ) NEW_LINE x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = self . Ht ( x_blurred ) . detach ( ) NEW_LINE x_pred = self . model ( x_blurred , Ht_x_blurred , self . mode ) NEW_LINE y = torch . abs ( Haar ( x_blurred ) ) . view ( x_blurred . data . shape [ 0 ] , - 1 ) . data / 0.6745 NEW_LINE std_approx = torch . topk ( y , ceil ( y . shape [ 1 ] / 2 ) , 1 ) [ 0 ] [ : , - 1 ] NEW_LINE for kk in range ( len ( names ) ) : NEW_LINE INDENT sio . savemat ( os . path . join ( folder , ' val ' , ' true ' , names [ kk ] ) , { ' image ' : x_true . data [ kk ] . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE sio . savemat ( os . path . join ( folder , ' val ' , ' previous _ block ' , names [ kk ] ) , { ' image ' : x_pred . data [ kk ] . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE sio . savemat ( os . path . join ( folder , ' val ' , ' Htxblurred ' , names [ kk ] ) , { ' image ' : Ht_x_blurred . data [ kk ] . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE np . savetxt ( os . path . join ( folder , ' val ' , ' std _ approx ' , names [ kk ] + ' . txt ' ) , np . array ( [ std_approx [ kk ] . cpu ( ) . numpy ( ) ] ) ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT [ names , x_true , x_blurred , Ht_x_blurred , std_approx ] = minibatch NEW_LINE x_true = Variable ( x_true . type ( self . dtype ) , requires_grad = False ) NEW_LINE x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = Variable ( Ht_x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE std_approx = std_approx . type ( self . dtype ) NEW_LINE x_pred = self . model ( x_blurred , Ht_x_blurred , self . mode , block = block , std_approx = std_approx ) NEW_LINE for kk in range ( len ( names ) ) : NEW_LINE INDENT sio . savemat ( os . path . join ( folder , ' val ' , ' previous _ block ' , names [ kk ] ) , { ' image ' : x_pred . data [ kk ] . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE DEDENT DEDENT DEDENT DEDENT\",), (\"def test ( self , dataset , save_gamma_mu_lambda = ' no ' ) : NEW_LINE INDENT path_savetest = os . path . join ( self . path_save , ' Results _ on _ Testsets ' , dataset ) NEW_LINE path_dataset = os . path . join ( self . path_test , self . name_kernel + self . name_std , dataset ) NEW_LINE if save_gamma_mu_lambda == ' no ' : NEW_LINE INDENT print ( ' Saving ▁ restaured ▁ images ▁ in ▁ % s ▁ . . . ' % ( path_savetest ) , flush = True ) NEW_LINE if not os . path . exists ( path_savetest ) : NEW_LINE INDENT os . makedirs ( path_savetest ) NEW_LINE DEDENT data = MyTestset ( folder = path_dataset ) NEW_LINE loader = DataLoader ( data , batch_size = self . batch_size [ 2 ] , shuffle = False ) NEW_LINE self . model . eval ( ) NEW_LINE self . last_layer . eval ( ) NEW_LINE for minibatch in tqdm ( loader , file = sys . stdout ) : NEW_LINE INDENT [ im_names , [ yes_no , x_blurred ] ] = minibatch NEW_LINE x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = self . Ht ( x_blurred ) NEW_LINE x_pred = self . model ( x_blurred , Ht_x_blurred . detach ( ) , self . mode ) NEW_LINE x_pred = x_pred . detach ( ) NEW_LINE x_pred = self . sigmoid ( x_pred + self . last_layer ( x_pred ) ) NEW_LINE for j in range ( len ( im_names ) ) : NEW_LINE INDENT sio . savemat ( os . path . join ( path_savetest , im_names [ j ] ) , { ' image ' : RGBToGray ( yes_no [ j ] , x_pred . data [ j ] ) . permute ( 1 , 2 , 0 ) . cpu ( ) . numpy ( ) . astype ( ' float64' ) } ) NEW_LINE DEDENT DEDENT DEDENT else : NEW_LINE INDENT print ( ' Saving ▁ restaured ▁ images ▁ in ▁ % s ▁ . . . ' % ( path_savetest ) , flush = True ) NEW_LINE print ( ' Saving ▁ stepsize , ▁ barrier ▁ parameter ▁ and ▁ regularization ▁ parameter ▁ in ▁ % s ▁ . . . ' % ( save_gamma_mu_lambda ) , flush = True ) NEW_LINE data = MyTestset ( folder = path_dataset ) NEW_LINE loader = DataLoader ( data , batch_size = 1 , shuffle = False ) NEW_LINE self . model . eval ( ) NEW_LINE self . last_layer . eval ( ) NEW_LINE for minibatch in tqdm ( loader , file = sys . stdout ) : NEW_LINE INDENT [ im_names , [ yes_no , x_blurred ] ] = minibatch NEW_LINE path_gamma_mu_lambda = os . path . join ( save_gamma_mu_lambda , im_names [ 0 ] [ 0 : - 4 ] ) NEW_LINE if not os . path . exists ( path_gamma_mu_lambda ) : NEW_LINE INDENT os . makedirs ( path_gamma_mu_lambda ) NEW_LINE DEDENT x_blurred = Variable ( x_blurred . type ( self . dtype ) , requires_grad = False ) NEW_LINE Ht_x_blurred = self . Ht ( x_blurred ) NEW_LINE x_pred = self . model ( x_blurred , Ht_x_blurred . detach ( ) , self . mode , save_gamma_mu_lambda = path_gamma_mu_lambda ) NEW_LINE DEDENT DEDENT DEDENT\",), (\"def PrintStatistics ( self , train , val , epoch , loss , lr ) : NEW_LINE INDENT print ( ' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ' ) NEW_LINE print ( ' [ % d ] ' % ( epoch ) , ' average ' , self . loss_type , ' : ▁ % 5.5f ' % ( loss ) , ' lr ▁ % .2E ' % ( lr ) ) NEW_LINE print ( ' ▁ ▁ ▁ ▁ ▁ Training ▁ set : ' ) NEW_LINE print ( ' ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ PSNR ▁ blurred ▁ = ▁ % 2.3f , ▁ PSNR ▁ pred ▁ = ▁ % 2.3f ' % ( train [ 0 , 0 ] , train [ 0 , 1 ] ) ) NEW_LINE print ( ' ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ SSIM ▁ blurred ▁ = ▁ % 2.3f , ▁ SSIM ▁ pred ▁ = ▁ % 2.3f ' % ( train [ 1 , 0 ] , train [ 1 , 1 ] ) ) NEW_LINE print ( ' ▁ ▁ ▁ ▁ ▁ Validation ▁ set : ' ) NEW_LINE print ( ' ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ PSNR ▁ blurred ▁ = ▁ % 2.3f , ▁ PSNR ▁ pred ▁ = ▁ % 2.3f ' % ( val [ 0 , 0 ] , val [ 0 , 1 ] ) ) NEW_LINE print ( ' ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ SSIM ▁ blurred ▁ = ▁ % 2.3f , ▁ SSIM ▁ pred ▁ = ▁ % 2.3f ' % ( val [ 1 , 0 ] , val [ 1 , 1 ] ) ) NEW_LINE DEDENT\",), ('def SaveLoss_PSNR_SSIM ( self , epoch , loss_epochs , psnr_ssim_train , psnr_ssim_val , mode , block = 0 ) : NEW_LINE INDENT fig , ( ax_loss ) = plt . subplots ( 1 , 1 , figsize = ( 4 , 4 ) ) NEW_LINE ax_loss . plot ( loss_epochs [ 0 : epoch + 1 ] ) NEW_LINE ax_loss . set_title ( \\' Minimal ▁ loss \\\\n \\' + \" % 5.2f \" % np . min ( loss_epochs ) ) NEW_LINE if mode == \\' first _ layer \\' or mode == \\' greedy \\' : NEW_LINE INDENT name_temp = \\' block _ \\' + str ( block ) NEW_LINE DEDENT elif mode == \\' last _ layers _ lpp \\' : NEW_LINE INDENT name_temp = \\' block _ \\' + str ( self . nb_greedy_blocks ) + \\' _ \\' + str ( self . nb_blocks - 1 ) + \\' _ lpp \\' NEW_LINE DEDENT fig_name = os . path . join ( self . path_save , name_temp , \\' training \\' , \" loss . png \" ) NEW_LINE plt . savefig ( fig_name ) NEW_LINE plt . close ( fig ) NEW_LINE self . MyPlot ( \\' max ▁ PSNR \\' , psnr_ssim_train [ 0 , : , 0 : epoch + 1 ] , \\' psnr _ training _ set . png \\' , block , mode ) NEW_LINE self . MyPlot_JustRestored ( \\' max ▁ PSNR \\' , psnr_ssim_train [ 0 , 1 , 0 : epoch + 1 ] , \\' psnr _ training _ set _ zoom . png \\' , block , mode ) NEW_LINE self . MyPlot ( \\' Max ▁ PSNR \\' , psnr_ssim_val [ 0 , : , 0 : epoch + 1 ] , \\' psnr _ validation _ set . png \\' , block , mode ) NEW_LINE self . MyPlot_JustRestored ( \\' Max ▁ PSNR \\' , psnr_ssim_val [ 0 , 1 , 0 : epoch + 1 ] , \\' psnr _ validation _ set _ zoom . png \\' , block , mode ) NEW_LINE self . MyPlot ( \\' Max ▁ SSIM \\' , psnr_ssim_train [ 1 , : , 0 : epoch + 1 ] , \\' ssim _ training _ set . png \\' , block , mode ) NEW_LINE self . MyPlot_JustRestored ( \\' Max ▁ SSIM \\' , psnr_ssim_train [ 1 , 1 , 0 : epoch + 1 ] , \\' ssim _ training _ set _ zoom . png \\' , block , mode , psnr = 0 ) NEW_LINE self . MyPlot ( \\' Max ▁ SSIM \\' , psnr_ssim_val [ 1 , : , 0 : epoch + 1 ] , \\' ssim _ validation _ set . png \\' , block , mode ) NEW_LINE self . MyPlot_JustRestored ( \\' Max ▁ SSIM \\' , psnr_ssim_val [ 1 , 1 , 0 : epoch + 1 ] , \\' ssim _ validation _ set _ zoom . png \\' , block , mode , psnr = 0 ) NEW_LINE print ( \\' Plots ▁ of ▁ the ▁ training ▁ loss , ▁ PSNR ▁ and ▁ SSIM ▁ during ▁ training ▁ are ▁ saved . \\' ) NEW_LINE DEDENT',), ('def MyPlot ( self , title , vec , name , block , mode ) : NEW_LINE INDENT fig , ax = plt . figure ( ) , plt . subplot ( 111 ) NEW_LINE ax . plot ( vec [ 0 , : ] , \\' b \\' , label = \\' Blurred \\' ) NEW_LINE ax . plot ( vec [ 1 , : ] , \\' g \\' , label = \\' Restored \\' ) NEW_LINE ax . set_title ( title + \" % 3.3f \" % np . max ( vec [ 1 , : ] ) ) NEW_LINE if mode == \\' first _ layer \\' or mode == \\' greedy \\' : NEW_LINE INDENT name_temp = \\' block _ \\' + str ( block ) NEW_LINE DEDENT elif mode == \\' last _ layers _ lpp \\' : NEW_LINE INDENT name_temp = \\' block _ \\' + str ( self . nb_greedy_blocks ) + \\' _ \\' + str ( self . nb_blocks - 1 ) + \\' _ lpp \\' NEW_LINE DEDENT fig_name = os . path . join ( self . path_save , name_temp , \\' training \\' , name ) NEW_LINE handles , labels = ax . get_legend_handles_labels ( ) NEW_LINE lgd = ax . legend ( handles , labels , loc = 2 , bbox_to_anchor = ( 1.05 , 1 ) , borderaxespad = 0. ) NEW_LINE fig . savefig ( fig_name , bbox_extra_artists = ( lgd , ) , bbox_inches = \\' tight \\' ) NEW_LINE plt . close ( fig ) NEW_LINE DEDENT',), ('def MyPlot_JustRestored ( self , title , vec , name , block , mode , psnr = 1 ) : NEW_LINE INDENT fig , ax = plt . figure ( ) , plt . subplot ( 111 ) NEW_LINE ax . plot ( vec , \\' g \\' , label = \\' Restored \\' ) NEW_LINE ax . set_title ( title + \" % 3.3f \" % np . max ( vec ) ) NEW_LINE if mode == \\' first _ layer \\' or mode == \\' greedy \\' : NEW_LINE INDENT name_temp = \\' block _ \\' + str ( block ) NEW_LINE DEDENT elif mode == \\' last _ layers _ lpp \\' : NEW_LINE INDENT if psnr == 1 : NEW_LINE INDENT ax . set_ylim ( [ 30 , 31.65 ] ) NEW_LINE DEDENT else : NEW_LINE INDENT ax . set_ylim ( [ 0.89 , 0.91 ] ) NEW_LINE DEDENT name_temp = \\' block _ \\' + str ( self . nb_greedy_blocks ) + \\' _ \\' + str ( self . nb_blocks - 1 ) + \\' _ lpp \\' NEW_LINE DEDENT fig_name = os . path . join ( self . path_save , name_temp , \\' training \\' , name ) NEW_LINE handles , labels = ax . get_legend_handles_labels ( ) NEW_LINE lgd = ax . legend ( handles , labels , loc = 2 , bbox_to_anchor = ( 1.05 , 1 ) , borderaxespad = 0. ) NEW_LINE fig . savefig ( fig_name , bbox_extra_artists = ( lgd , ) , bbox_inches = \\' tight \\' ) NEW_LINE plt . close ( fig ) NEW_LINE DEDENT',), ('def __init__ ( self ) : NEW_LINE INDENT super ( SSIM_loss , self ) . __init__ ( ) NEW_LINE self . ssim = PyTorch_ssim . SSIM ( ) NEW_LINE DEDENT',), ('def forward ( self , input , target ) : NEW_LINE INDENT return - self . ssim ( input , target ) NEW_LINE DEDENT',), ('def __init__ ( self ) : NEW_LINE INDENT super ( Cnn_bar , self ) . __init__ ( ) NEW_LINE self . conv2 = nn . Conv2d ( 3 , 1 , 5 , padding = 2 ) NEW_LINE self . conv3 = nn . Conv2d ( 1 , 1 , 5 , padding = 2 ) NEW_LINE self . lin = nn . Linear ( 16 * 16 * 1 , 1 ) NEW_LINE self . avg = nn . AvgPool2d ( 4 , 4 ) NEW_LINE self . soft = nn . Softplus ( ) NEW_LINE DEDENT',), ('def forward ( self , x ) : NEW_LINE INDENT x = self . soft ( self . avg ( self . conv2 ( x ) ) ) NEW_LINE x = self . soft ( self . avg ( self . conv3 ( x ) ) ) NEW_LINE x = x . view ( x . size ( 0 ) , - 1 ) NEW_LINE x = self . soft ( self . lin ( x ) ) NEW_LINE x = x . view ( x . size ( 0 ) , - 1 , 1 , 1 ) NEW_LINE return x NEW_LINE DEDENT',), (\"def __init__ ( self , im_range , kernel_2 , Dv , Dh , DvT , DhT , dtype ) : NEW_LINE INDENT super ( IPIter , self ) . __init__ ( ) NEW_LINE self . Dv = MyConv2d ( Dv , ' batch ' , pad_type = ' replicate ' ) NEW_LINE self . Dh = MyConv2d ( Dh , ' batch ' , pad_type = ' replicate ' ) NEW_LINE self . DvT = MyConv2d ( DvT , ' batch ' , pad_type = ' replicate ' ) NEW_LINE self . DhT = MyConv2d ( DhT , ' batch ' , pad_type = ' replicate ' ) NEW_LINE self . HtH = MyConv2d ( kernel_2 , ' batch ' ) NEW_LINE self . im_range = im_range NEW_LINE DEDENT\",), ('def Grad ( self , reg , delta , x , Ht_x_blurred ) : NEW_LINE INDENT Dvx , Dhx = self . Dv ( x ) , self . Dh ( x ) NEW_LINE DtDx = ( ( self . DvT ( Dvx ) + self . DhT ( Dhx ) ) / delta ** 2 ) / torch . sqrt ( ( Dvx ** 2 + Dhx ** 2 ) / delta ** 2 + 1 ) NEW_LINE return self . HtH ( x ) - Ht_x_blurred + reg * DtDx NEW_LINE DEDENT',), ('def forward ( self , gamma , mu , reg_mul , reg_constant , delta , x , Ht_x_blurred , std_approx , mode , save_gamma_mu_lambda ) : NEW_LINE INDENT Dx = torch . cat ( ( self . Dv ( x ) , self . Dh ( x ) ) , 1 ) NEW_LINE avg = Dx . mean ( - 1 ) . mean ( - 1 ) . mean ( - 1 ) . unsqueeze ( 1 ) . unsqueeze ( 2 ) . unsqueeze ( 3 ) NEW_LINE reg = reg_mul * std_approx / ( torch . sqrt ( ( ( Dx - avg ) ** 2 ) . mean ( - 1 ) . mean ( - 1 ) . mean ( - 1 ) ) + reg_constant ) NEW_LINE x_tilde = x - gamma * self . Grad ( reg . unsqueeze ( 1 ) . unsqueeze ( 2 ) . unsqueeze ( 3 ) , delta , x , Ht_x_blurred ) NEW_LINE if save_gamma_mu_lambda != \\' no \\' : NEW_LINE INDENT file = open ( os . path . join ( save_gamma_mu_lambda , \\' gamma . txt \\' ) , \" a \" ) NEW_LINE file . write ( \\' \\\\n \\' + \\' % .3e \\' % gamma . data . cpu ( ) ) NEW_LINE file . close ( ) NEW_LINE file = open ( os . path . join ( save_gamma_mu_lambda , \\' mu . txt \\' ) , \" a \" ) NEW_LINE file . write ( \\' \\\\n \\' + \\' % .3e \\' % mu . data . cpu ( ) ) NEW_LINE file . close ( ) NEW_LINE file = open ( os . path . join ( save_gamma_mu_lambda , \\' lambda . txt \\' ) , \" a \" ) NEW_LINE file . write ( \\' \\\\n \\' + \\' % .3e \\' % reg . data . cpu ( ) ) NEW_LINE file . close ( ) NEW_LINE DEDENT return cardan . apply ( gamma * mu , x_tilde , self . im_range , mode ) NEW_LINE DEDENT',), ('def __init__ ( self , im_range , kernel_2 , Dv , Dh , DvT , DhT , dtype ) : NEW_LINE INDENT super ( Block , self ) . __init__ ( ) NEW_LINE self . cnn_bar = Cnn_bar ( ) NEW_LINE self . soft = nn . Softplus ( ) NEW_LINE self . gamma = nn . Parameter ( torch . FloatTensor ( [ 1 ] ) . cuda ( ) ) NEW_LINE self . reg_mul = nn . Parameter ( torch . FloatTensor ( [ - 7 ] ) . cuda ( ) ) NEW_LINE self . reg_constant = nn . Parameter ( torch . FloatTensor ( [ - 5 ] ) . cuda ( ) ) NEW_LINE self . delta = 0.01 NEW_LINE self . IPIter = IPIter ( im_range , kernel_2 , Dv , Dh , DvT , DhT , dtype ) NEW_LINE DEDENT',), ('def forward ( self , x , Ht_x_blurred , std_approx , save_gamma_mu_lambda ) : NEW_LINE INDENT mu = self . cnn_bar ( x ) NEW_LINE gamma = self . soft ( self . gamma ) NEW_LINE reg_mul = self . soft ( self . reg_mul ) NEW_LINE reg_constant = self . soft ( self . reg_constant ) NEW_LINE return self . IPIter ( gamma , mu , reg_mul , reg_constant , self . delta , x , Ht_x_blurred , std_approx , self . training , save_gamma_mu_lambda ) NEW_LINE DEDENT',), (\"def __init__ ( self , im_range , kernel_2 , dtype , nL ) : NEW_LINE INDENT super ( myModel , self ) . __init__ ( ) NEW_LINE self . Layers = nn . ModuleList ( ) NEW_LINE Dv = np . array ( ( ( 0 , 0 , 0 ) , ( 0 , - 1 , 0 ) , ( 0 , 1 , 0 ) ) ) NEW_LINE DvT = np . array ( ( ( 0 , 1 , 0 ) , ( 0 , - 1 , 0 ) , ( 0 , 0 , 0 ) ) ) NEW_LINE Dh = np . array ( ( ( 0 , 0 , 0 ) , ( 0 , - 1 , 1 ) , ( 0 , 0 , 0 ) ) ) NEW_LINE DhT = np . array ( ( ( 0 , 0 , 0 ) , ( 1 , - 1 , 0 ) , ( 0 , 0 , 0 ) ) ) NEW_LINE Haar_filt = np . array ( ( ( 0.5 , - 0.5 ) , ( - 0.5 , 0.5 ) ) ) NEW_LINE Dv , Dh , DvT , DhT , Haar_filt = TensorFilter ( [ Dv , Dh , DvT , DhT , Haar_filt ] ) NEW_LINE self . Haar = MyConv2d ( Haar_filt , ' batch ' , pad_type = ' circular ' , padding = 1 , stride = 2 ) NEW_LINE for i in range ( nL ) : NEW_LINE INDENT self . Layers . append ( Block ( im_range , kernel_2 , Dv , Dh , DvT , DhT , dtype ) ) NEW_LINE DEDENT DEDENT\",), (\"def GradFalse ( self , block , mode ) : NEW_LINE INDENT if block >= 1 : NEW_LINE INDENT if mode == ' greedy ' : NEW_LINE INDENT self . Layers [ block ] . load_state_dict ( self . Layers [ block - 1 ] . state_dict ( ) ) NEW_LINE DEDENT elif mode == ' last _ layers _ lpp ' : NEW_LINE INDENT for j in range ( block , len ( self . Layers ) ) : NEW_LINE INDENT self . Layers [ j ] . load_state_dict ( self . Layers [ block - 1 ] . state_dict ( ) ) NEW_LINE DEDENT DEDENT for i in range ( 0 , block ) : NEW_LINE INDENT self . Layers [ i ] . eval ( ) NEW_LINE for p in self . Layers [ i ] . parameters ( ) : NEW_LINE INDENT p . requires_grad = False NEW_LINE p . grad = None NEW_LINE DEDENT DEDENT DEDENT DEDENT\",), (\"def forward ( self , x , Ht_x_blurred , mode , block = 0 , std_approx = torch . tensor ( [ - 1 ] ) , save_gamma_mu_lambda = ' no ' ) : NEW_LINE INDENT if ( std_approx < 0 ) . any ( ) : NEW_LINE INDENT y = torch . abs ( self . Haar ( x ) ) . view ( x . data . shape [ 0 ] , - 1 ) . data / 0.6745 NEW_LINE std_approx = torch . topk ( y , ceil ( y . shape [ 1 ] / 2 ) , 1 ) [ 0 ] [ : , - 1 ] NEW_LINE DEDENT if mode == ' first _ layer ' or mode == ' greedy ' : NEW_LINE INDENT x = self . Layers [ block ] ( x , Ht_x_blurred , std_approx , save_gamma_mu_lambda ) NEW_LINE DEDENT elif mode == ' last _ layers _ lpp ' : NEW_LINE INDENT for i in range ( block , len ( self . Layers ) ) : NEW_LINE INDENT x = self . Layers [ i ] ( x , Ht_x_blurred , std_approx , save_gamma_mu_lambda ) NEW_LINE DEDENT DEDENT elif mode == ' test ' : NEW_LINE INDENT for i in range ( 0 , len ( self . Layers ) ) : NEW_LINE INDENT x = self . Layers [ i ] ( x . detach ( ) , Ht_x_blurred , std_approx , save_gamma_mu_lambda ) NEW_LINE DEDENT DEDENT return x NEW_LINE DEDENT\",), ('def __init__ ( self ) : NEW_LINE INDENT super ( myLastLayer , self ) . __init__ ( ) NEW_LINE self . conv1 = nn . Conv2d ( 3 , 64 , 3 , dilation = 1 , padding = 1 ) NEW_LINE self . conv2 = nn . Conv2d ( 64 , 64 , 3 , dilation = 2 , padding = 2 ) NEW_LINE self . conv3 = nn . Conv2d ( 64 , 64 , 3 , dilation = 3 , padding = 3 ) NEW_LINE self . conv4 = nn . Conv2d ( 64 , 64 , 3 , dilation = 4 , padding = 4 ) NEW_LINE self . conv5 = nn . Conv2d ( 64 , 64 , 3 , dilation = 5 , padding = 5 ) NEW_LINE self . conv6 = nn . Conv2d ( 64 , 64 , 3 , dilation = 4 , padding = 4 ) NEW_LINE self . conv7 = nn . Conv2d ( 64 , 64 , 3 , dilation = 3 , padding = 3 ) NEW_LINE self . conv8 = nn . Conv2d ( 64 , 64 , 3 , dilation = 2 , padding = 2 ) NEW_LINE self . conv9 = nn . Conv2d ( 64 , 3 , 3 , dilation = 1 , padding = 1 ) NEW_LINE self . bn1 = nn . BatchNorm2d ( 64 ) NEW_LINE self . bn2 = nn . BatchNorm2d ( 64 ) NEW_LINE self . bn3 = nn . BatchNorm2d ( 64 ) NEW_LINE self . bn4 = nn . BatchNorm2d ( 64 ) NEW_LINE self . bn5 = nn . BatchNorm2d ( 64 ) NEW_LINE self . bn6 = nn . BatchNorm2d ( 64 ) NEW_LINE self . bn7 = nn . BatchNorm2d ( 64 ) NEW_LINE self . relu = nn . ReLU ( ) NEW_LINE DEDENT',), ('def forward ( self , x ) : NEW_LINE INDENT if self . training == True : NEW_LINE INDENT return self . conv9 ( self . relu ( self . bn7 ( self . conv8 ( self . relu ( self . bn6 ( self . conv7 ( self . relu ( self . bn5 ( self . conv6 ( self . relu ( self . bn4 ( self . conv5 ( self . relu ( self . bn3 ( self . conv4 ( self . relu ( self . bn2 ( self . conv3 ( self . relu ( self . bn1 ( self . conv2 ( self . relu ( self . conv1 ( x ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) NEW_LINE DEDENT else : NEW_LINE INDENT r = self . conv1 ( x . detach ( ) ) NEW_LINE r = self . relu ( r . detach ( ) ) NEW_LINE r = self . conv2 ( r . detach ( ) ) NEW_LINE r = self . bn1 ( r . detach ( ) ) NEW_LINE r = self . relu ( r . detach ( ) ) NEW_LINE r = self . conv3 ( r . detach ( ) ) NEW_LINE r = self . bn2 ( r . detach ( ) ) NEW_LINE r = self . relu ( r . detach ( ) ) NEW_LINE r = self . conv4 ( r . detach ( ) ) NEW_LINE r = self . bn3 ( r . detach ( ) ) NEW_LINE r = self . relu ( r . detach ( ) ) NEW_LINE r = self . conv5 ( r . detach ( ) ) NEW_LINE r = self . bn4 ( r . detach ( ) ) NEW_LINE r = self . relu ( r . detach ( ) ) NEW_LINE r = self . conv6 ( r . detach ( ) ) NEW_LINE r = self . bn5 ( r . detach ( ) ) NEW_LINE r = self . relu ( r . detach ( ) ) NEW_LINE r = self . conv7 ( r . detach ( ) ) NEW_LINE r = self . bn6 ( r . detach ( ) ) NEW_LINE r = self . relu ( r . detach ( ) ) NEW_LINE r = self . conv8 ( r . detach ( ) ) NEW_LINE r = self . bn7 ( r . detach ( ) ) NEW_LINE r = self . relu ( r . detach ( ) ) NEW_LINE r = self . conv9 ( r . detach ( ) ) NEW_LINE return r NEW_LINE DEDENT DEDENT',), ('def gaussian ( window_size , sigma ) : NEW_LINE INDENT gauss = torch . Tensor ( [ exp ( - ( x - window_size // 2 ) ** 2 / float ( 2 * sigma ** 2 ) ) for x in range ( window_size ) ] ) NEW_LINE return gauss / gauss . sum ( ) NEW_LINE DEDENT',), ('def create_window ( window_size , channel ) : NEW_LINE INDENT _1D_window = gaussian ( window_size , 1.5 ) . unsqueeze ( 1 ) NEW_LINE _2D_window = _1D_window . mm ( _1D_window . t ( ) ) . float ( ) . unsqueeze ( 0 ) . unsqueeze ( 0 ) NEW_LINE window = Variable ( _2D_window . expand ( channel , 1 , window_size , window_size ) . contiguous ( ) ) NEW_LINE return window NEW_LINE DEDENT',), ('def _ssim ( img1 , img2 , window , window_size , channel , size_average = True ) : NEW_LINE INDENT mu1 = F . conv2d ( img1 , window , padding = window_size // 2 , groups = channel ) NEW_LINE mu2 = F . conv2d ( img2 , window , padding = window_size // 2 , groups = channel ) NEW_LINE mu1_sq = mu1 . pow ( 2 ) NEW_LINE mu2_sq = mu2 . pow ( 2 ) NEW_LINE mu1_mu2 = mu1 * mu2 NEW_LINE sigma1_sq = F . conv2d ( img1 * img1 , window , padding = window_size // 2 , groups = channel ) - mu1_sq NEW_LINE sigma2_sq = F . conv2d ( img2 * img2 , window , padding = window_size // 2 , groups = channel ) - mu2_sq NEW_LINE sigma12 = F . conv2d ( img1 * img2 , window , padding = window_size // 2 , groups = channel ) - mu1_mu2 NEW_LINE C1 = 0.01 ** 2 NEW_LINE C2 = 0.03 ** 2 NEW_LINE ssim_map = ( ( 2 * mu1_mu2 + C1 ) * ( 2 * sigma12 + C2 ) ) / ( ( mu1_sq + mu2_sq + C1 ) * ( sigma1_sq + sigma2_sq + C2 ) ) NEW_LINE if size_average : NEW_LINE INDENT return ssim_map . mean ( ) NEW_LINE DEDENT else : NEW_LINE INDENT return ssim_map . mean ( 1 ) . mean ( 1 ) . mean ( 1 ) NEW_LINE DEDENT DEDENT',), ('def ssim ( img1 , img2 , window_size = 11 , size_average = True ) : NEW_LINE INDENT ( _ , channel , _ , _ ) = img1 . size ( ) NEW_LINE window = create_window ( window_size , channel ) NEW_LINE if img1 . is_cuda : NEW_LINE INDENT window = window . cuda ( img1 . get_device ( ) ) NEW_LINE DEDENT window = window . type_as ( img1 ) NEW_LINE return _ssim ( img1 , img2 , window , window_size , channel , size_average ) NEW_LINE DEDENT',), ('def __init__ ( self , window_size = 11 , size_average = True ) : NEW_LINE INDENT super ( SSIM , self ) . __init__ ( ) NEW_LINE self . window_size = window_size NEW_LINE self . size_average = size_average NEW_LINE self . channel = 1 NEW_LINE self . window = create_window ( window_size , self . channel ) NEW_LINE DEDENT',), ('def forward ( self , img1 , img2 ) : NEW_LINE INDENT ( _ , channel , _ , _ ) = img1 . size ( ) NEW_LINE if channel == self . channel and self . window . data . type ( ) == img1 . data . type ( ) : NEW_LINE INDENT window = self . window NEW_LINE DEDENT else : NEW_LINE INDENT window = create_window ( self . window_size , channel ) NEW_LINE if img1 . is_cuda : NEW_LINE INDENT window = window . cuda ( img1 . get_device ( ) ) NEW_LINE DEDENT window = window . type_as ( img1 ) NEW_LINE self . window = window NEW_LINE self . channel = channel NEW_LINE DEDENT return _ssim ( img1 , img2 , window , self . window_size , channel , self . size_average ) NEW_LINE DEDENT',)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-2eaac9f5331e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_rep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzfkAfweiFKG",
        "outputId": "817c27c1-8480-4e93-bf3c-c2bbd8c3a0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[[ 0.7814, -0.1531,  0.2438,  ...,  0.5276,  0.1782,  0.2289]]]),\n",
              " tensor([[[ 0.4640, -0.0946,  0.2987,  ...,  0.4522,  0.2744,  0.2990]]]),\n",
              " tensor([[[ 0.9164, -0.3907,  0.0960,  ...,  0.5478,  0.3017, -0.0221]]]),\n",
              " tensor([[[ 0.9199, -0.3740,  0.1033,  ...,  0.5090,  0.2973, -0.0067]]]),\n",
              " tensor([[[ 0.9440, -0.3906,  0.1073,  ...,  0.5119,  0.3007, -0.0176]]]),\n",
              " tensor([[[ 0.8080, -0.2522,  0.1362,  ...,  0.4910,  0.2463,  0.0997]]]),\n",
              " tensor([[[ 0.7899, -0.2610,  0.1008,  ...,  0.4811,  0.2407,  0.0743]]]),\n",
              " tensor([[[ 0.8414, -0.2704,  0.1358,  ...,  0.4767,  0.2585,  0.0918]]]),\n",
              " tensor([[[ 0.7523, -0.2260,  0.0746,  ...,  0.4840,  0.2679,  0.1212]]]),\n",
              " tensor([[[ 0.8664, -0.1233,  0.0296,  ...,  0.3527,  0.2741,  0.1871]]]),\n",
              " tensor([[[ 0.7946, -0.1043,  0.0557,  ...,  0.3444,  0.2854,  0.1643]]]),\n",
              " tensor([[[ 0.8321, -0.1260,  0.0382,  ...,  0.3618,  0.2802,  0.1615]]]),\n",
              " tensor([[[ 0.8621, -0.1249,  0.0124,  ...,  0.3173,  0.2748,  0.2579]]]),\n",
              " tensor([[[ 0.7980, -0.0452,  0.0809,  ...,  0.3859,  0.2468,  0.2645]]]),\n",
              " tensor([[[ 0.8297, -0.1134,  0.1162,  ...,  0.2809,  0.2304,  0.2261]]]),\n",
              " tensor([[[ 0.8701, -0.2255,  0.1205,  ...,  0.5024,  0.1990,  0.2280]]]),\n",
              " tensor([[[ 0.4837, -0.0859,  0.1674,  ...,  0.4295,  0.1844,  0.2649]]]),\n",
              " tensor([[[0.5469, 0.0109, 0.1969,  ..., 0.4752, 0.0706, 0.2228]]]),\n",
              " tensor([[[ 0.5228, -0.0429,  0.0735,  ...,  0.3193,  0.2687,  0.2165]]]),\n",
              " tensor([[[ 0.6423, -0.0784, -0.0033,  ...,  0.4051,  0.2856,  0.2489]]]),\n",
              " tensor([[[ 0.5408, -0.2126,  0.3348,  ...,  0.5247,  0.0839,  0.3751]]]),\n",
              " tensor([[[ 0.7719, -0.1331,  0.1098,  ...,  0.4079,  0.3715,  0.0222]]]),\n",
              " tensor([[[ 0.7842, -0.1669,  0.0721,  ...,  0.2581,  0.2087,  0.1804]]]),\n",
              " tensor([[[ 0.6367, -0.1957,  0.0849,  ...,  0.2954,  0.3996,  0.0179]]]),\n",
              " tensor([[[ 0.6221,  0.0567,  0.0631,  ...,  0.4684,  0.2737, -0.0228]]]),\n",
              " tensor([[[ 0.5619, -0.1331,  0.1161,  ...,  0.4885,  0.3910,  0.0136]]]),\n",
              " tensor([[[ 8.4810e-01, -1.4870e-01,  1.3544e-01,  ...,  3.6342e-01,\n",
              "            4.3833e-01,  1.3433e-04]]]),\n",
              " tensor([[[0.7238, 0.0393, 0.1793,  ..., 0.4955, 0.2182, 0.3443]]]),\n",
              " tensor([[[ 0.8718, -0.0829,  0.2484,  ...,  0.4650,  0.4510,  0.1206]]]),\n",
              " tensor([[[0.4398, 0.0435, 0.1635,  ..., 0.5088, 0.2372, 0.0057]]]),\n",
              " tensor([[[ 0.6769, -0.1125, -0.1427,  ...,  0.6662, -0.0316,  0.2369]]]),\n",
              " tensor([[[0.7048, 0.0783, 0.3283,  ..., 0.5428, 0.3210, 0.3558]]]),\n",
              " tensor([[[ 0.6962, -0.1966,  0.0768,  ...,  0.5144,  0.2925,  0.1592]]]),\n",
              " tensor([[[ 0.6897, -0.0279,  0.0689,  ...,  0.5242,  0.3445,  0.1388]]]),\n",
              " tensor([[[ 0.6654, -0.1141,  0.0896,  ...,  0.7175,  0.1809,  0.2496]]]),\n",
              " tensor([[[ 0.7731, -0.2269,  0.2070,  ...,  0.7341,  0.1498,  0.3872]]]),\n",
              " tensor([[[ 0.8124, -0.0954,  0.0887,  ...,  0.6957,  0.2510,  0.1577]]]),\n",
              " tensor([[[ 0.6260, -0.1319,  0.2911,  ...,  0.5407,  0.3226,  0.4711]]]),\n",
              " tensor([[[ 0.8046, -0.2094,  0.1390,  ...,  0.6063,  0.1524,  0.2869]]]),\n",
              " tensor([[[ 0.6660, -0.3841,  0.2874,  ...,  0.7687,  0.2183,  0.0481]]]),\n",
              " tensor([[[ 0.6721, -0.2915,  0.0911,  ...,  0.3297,  0.5388,  0.0362]]]),\n",
              " tensor([[[ 0.7788, -0.2594,  0.2674,  ...,  0.6853,  0.2718,  0.3051]]]),\n",
              " tensor([[[ 0.4777, -0.1662,  0.2018,  ...,  0.6173,  0.1505,  0.3699]]]),\n",
              " tensor([[[ 0.8801, -0.1332,  0.2174,  ...,  0.5846,  0.1887,  0.3818]]]),\n",
              " tensor([[[ 0.5933, -0.2126,  0.2190,  ...,  0.5322,  0.1537,  0.2579]]]),\n",
              " tensor([[[0.8365, 0.0037, 0.2409,  ..., 0.5340, 0.3309, 0.3144]]]),\n",
              " tensor([[[ 0.5012, -0.2055,  0.2122,  ...,  0.5687,  0.1708,  0.3792]]]),\n",
              " tensor([[[ 0.8205, -0.0490,  0.2660,  ...,  0.5630,  0.2370,  0.3910]]]),\n",
              " tensor([[[ 0.4344, -0.1044,  0.2297,  ...,  0.4990,  0.2038,  0.3400]]]),\n",
              " tensor([[[ 0.8556, -0.0595,  0.2309,  ...,  0.5491,  0.2939,  0.2895]]]),\n",
              " tensor([[[ 0.5000, -0.0957,  0.2709,  ...,  0.4685,  0.2379,  0.3407]]])]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "torch.save(combined_model.state_dict(),\"gdrive/MyDrive/RSS/Models/combined_model\"+str(time.strftime('%Y-%m-%d_%H:%M:%S.pt', time.localtime())) )"
      ],
      "metadata": {
        "id": "OUjo25d_TSxV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}