{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code-extract.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqvV6GtOnqT4",
        "outputId": "6c211dc9-0119-4286-af71-8a9278d9937c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=5e487bb5a742a451fc0ad09fe33da169b6a524379b3e84f5131a883e2767e289\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fUZULMknZLDb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wget\n",
        "import zipfile\n",
        "import shutil\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datalinkspath = './text-extractions-100.csv'\n",
        "\n",
        "datalinksdf = pd.read_csv(datalinkspath, index_col=0)\n",
        "datalinksdf.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "Zidk_MLQ_kgA",
        "outputId": "f87dbbc4-2740-4d89-a97d-11d629a109a5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               paper  \\\n",
              "0  Temporal coherence-based self-supervised learn...   \n",
              "1  BinGAN: Learning Compact Binary Descriptors wi...   \n",
              "2  Multiscale Fisher's Independence Test for Mult...   \n",
              "3         POTs: Protective Optimization Technologies   \n",
              "4                                   Surface Networks   \n",
              "\n",
              "                                     link  \\\n",
              "0   http://arxiv.org/pdf/1806.06811v2.pdf   \n",
              "1   http://arxiv.org/pdf/1806.06778v5.pdf   \n",
              "2  https://arxiv.org/pdf/1806.06777v7.pdf   \n",
              "3  https://arxiv.org/pdf/1806.02711v6.pdf   \n",
              "4   http://arxiv.org/pdf/1705.10819v2.pdf   \n",
              "\n",
              "                                               code  \\\n",
              "0     https://gitlab.com/nct_tso_public/pretrain_tc   \n",
              "1             https://github.com/maciejzieba/binGAN   \n",
              "2             https://github.com/MaStatLab/multiFit   \n",
              "3               https://github.com/spring-epfl/pots   \n",
              "4  https://github.com/jiangzhongshi/SurfaceNetworks   \n",
              "\n",
              "                                                text  \n",
              "0  Temporalcoherence-basedself-supervised\\nlearni...  \n",
              "1  BinGAN:LearningCompactBinaryDescriptors\\nwitha...  \n",
              "2  MultiscaleFisher'sIndependenceTestfor\\nMultiva...  \n",
              "3  POTs:ProtectiveOptimizationTechnologies\\nBogda...  \n",
              "4  SurfaceNetworks\\nIlyaKostrikov\\n1\\n,ZhongshiJi...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a81ba20d-2746-4692-a110-bc7ff7b1cb07\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper</th>\n",
              "      <th>link</th>\n",
              "      <th>code</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Temporal coherence-based self-supervised learn...</td>\n",
              "      <td>http://arxiv.org/pdf/1806.06811v2.pdf</td>\n",
              "      <td>https://gitlab.com/nct_tso_public/pretrain_tc</td>\n",
              "      <td>Temporalcoherence-basedself-supervised\\nlearni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BinGAN: Learning Compact Binary Descriptors wi...</td>\n",
              "      <td>http://arxiv.org/pdf/1806.06778v5.pdf</td>\n",
              "      <td>https://github.com/maciejzieba/binGAN</td>\n",
              "      <td>BinGAN:LearningCompactBinaryDescriptors\\nwitha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Multiscale Fisher's Independence Test for Mult...</td>\n",
              "      <td>https://arxiv.org/pdf/1806.06777v7.pdf</td>\n",
              "      <td>https://github.com/MaStatLab/multiFit</td>\n",
              "      <td>MultiscaleFisher'sIndependenceTestfor\\nMultiva...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>POTs: Protective Optimization Technologies</td>\n",
              "      <td>https://arxiv.org/pdf/1806.02711v6.pdf</td>\n",
              "      <td>https://github.com/spring-epfl/pots</td>\n",
              "      <td>POTs:ProtectiveOptimizationTechnologies\\nBogda...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Surface Networks</td>\n",
              "      <td>http://arxiv.org/pdf/1705.10819v2.pdf</td>\n",
              "      <td>https://github.com/jiangzhongshi/SurfaceNetworks</td>\n",
              "      <td>SurfaceNetworks\\nIlyaKostrikov\\n1\\n,ZhongshiJi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a81ba20d-2746-4692-a110-bc7ff7b1cb07')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a81ba20d-2746-4692-a110-bc7ff7b1cb07 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a81ba20d-2746-4692-a110-bc7ff7b1cb07');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datalinksdf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qz-zJ3MY7vL",
        "outputId": "bde3b39b-0bd2-454a-b972-d4e7a1eddea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54055, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://gitlab.com/nct_tso_public/pretrain_tc\n",
        "https://gitlab.com/nct_tso_public/pretrain_tc/-/archive/master/pretrain_tc-master.zip\n",
        "https://github.com/MaStatLab/multiFit\n",
        "https://github.com/MaStatLab/MultiFit/archive/refs/heads/master.zip"
      ],
      "metadata": {
        "id": "3xeq7LIjlwMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contents = []\n",
        "for url in datalinksdf['code']:\n",
        "    print(url)\n",
        "    content = ''\n",
        "    downloadlink = url\n",
        "    reponame = url.split('/')[-1]\n",
        "    if 'gitlab' in url:\n",
        "        downloadlink += '/-/archive/master/' + reponame + '.zip'\n",
        "    elif 'github' in url:\n",
        "        downloadlink += '/archive/refs/heads/master.zip'\n",
        "    else:\n",
        "        content = 'GitHub Not Found'\n",
        "        contents.append(content)\n",
        "        continue\n",
        "    wget.download(downloadlink)\n",
        "    zipfilename = [x for x in os.listdir() if '.zip' in x][0]\n",
        "    unzipped = zipfilename[:-4]\n",
        "    with zipfile.ZipFile(zipfilename,\"r\") as zip_ref:\n",
        "        zip_ref.extractall(unzipped)\n",
        "    os.remove(zipfilename)\n",
        "\n",
        "    for root, dirnames, filenames in os.walk(unzipped):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(('.py')):\n",
        "                fp = os.path.join(root, filename)\n",
        "                text = open(fp, 'r').read()\n",
        "                content += text\n",
        "\n",
        "    shutil.rmtree(unzipped)\n",
        "    contents.append(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjSF1mH1ld17",
        "outputId": "aaf7a514-8206-41c3-8dcb-6e6741b49017"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://gitlab.com/nct_tso_public/pretrain_tc\n",
            "https://github.com/maciejzieba/binGAN\n",
            "https://github.com/MaStatLab/multiFit\n",
            "https://github.com/spring-epfl/pots\n",
            "https://github.com/jiangzhongshi/SurfaceNetworks\n",
            "https://github.com/tech-srl/lstar_extraction\n",
            "https://github.com/keunwoochoi/DrummerNet\n",
            "https://github.com/MaStatLab/WARP\n",
            "https://github.com/bagequan/tencent-transformer-with-disagreement\n",
            "https://github.com/eginhard/cae-utd-utils\n",
            "https://github.com/componavt/wcorpus\n",
            "https://github.com/thunguyenphuoc/RenderNet\n",
            "https://github.com/tomkocmi/SubGram\n",
            "https://github.com/jianzhangcs/ISTA-Net-PyTorch\n",
            "https://github.com/chang-li/SBOR\n",
            "https://github.com/anthony-strock/ijcnn2018\n",
            "https://github.com/bakirillov/capsules\n",
            "https://github.com/tmistele/predicting-citation-counts-net\n",
            "https://github.com/osnandhu/DS5500_PCG_Classification\n",
            "https://github.com/xl-sr/CAL\n",
            "https://github.com/dtak/local-independence-public\n",
            "https://github.com/DNNToolBox/Net-Trim-v1\n",
            "https://github.com/jamesro/cluster-specialists\n",
            "https://github.com/davevanveen/compsensing_dip\n",
            "https://github.com/huckiyang/EyeNet\n",
            "https://github.com/richardaecn/cvpr18-caption-eval\n",
            "https://github.com/lansiz/neuron\n",
            "https://github.com/ExplainableML/UncerGuidedI2I\n",
            "https://github.com/UKPLab/coling18-multimodalSurvey\n",
            "https://github.com/alinadubatovka/information_propagation\n",
            "https://github.com/thunlp/Character-enhanced-Sememe-Prediction\n",
            "https://github.com/wangkenpu/rsrgan\n",
            "https://github.com/wangkenpu/Adaptation-Interspeech18\n",
            "https://github.com/YJHMITWEB/GLoMo-tensorflow\n",
            "https://github.com/BaoWangMath/DNN-DataDependentActivation\n",
            "https://github.com/BaoWangMath/LaplacianSmoothing-GradientDescent\n",
            "https://github.com/miraep8/Minimal_IMAP_MCMC\n",
            "https://github.com/xuyanyu-shh/Personalized-Saliency\n",
            "https://github.com/srxdev0619/Latent_Convolutional_Models\n",
            "https://github.com/xycforgithub/StrategyProof_Conference_Review\n",
            "https://github.com/roaminsight/roamresearch\n",
            "https://github.com/aquemy/HCBR\n",
            "https://github.com/SenticNet/hfusion\n",
            "https://github.com/idiap/gile\n",
            "https://github.com/AskNowQA/EARL\n",
            "https://github.com/richardaecn/cvpr18-inaturalist-transfer\n",
            "https://github.com/IliasZadik/double_orthogonal_ml\n",
            "https://github.com/Kai-Xuan/AidCovDs\n",
            "https://github.com/eustomaqua/EPFD\n",
            "https://github.com/StanfordASL/BaRC\n",
            "https://github.com/fabienbaradel/object_level_visual_reasoning\n",
            "https://github.com/nikhilvravi/DukeSR\n",
            "https://github.com/pschydlo/ActionAnticipation\n",
            "https://github.com/EigenPro/EigenPro2\n",
            "https://github.com/untrix/im2latex\n",
            "https://worksheets.codalab.org/worksheets/0x8ef22fd3cd384029bf1d1cae5b268f2d\n",
            "https://github.com/laiguokun/SWaveNet\n",
            "https://github.com/endgameinc/malware_evasion_competition\n",
            "https://github.com/Garoe/tf_mvg\n",
            "https://github.com/TeaPearce/Deep_Learning_Prediction_Intervals\n",
            "https://github.com/facebookresearch/low-shot-with-diffusion\n",
            "https://github.com/paqs2020/paqs2020\n",
            "https://github.com/fmaglia/BoI\n",
            "https://github.com/jshtok/RepMet\n",
            "https://github.com/andrade-stats/robustBayesClustering\n",
            "https://github.com/AhmedImtiazPrio/heartnet\n",
            "https://github.com/IBM/automation-of-image-data-preprocessing\n",
            "https://github.com/rahafaljundi/Selfless-Sequential-Learning\n",
            "https://github.com/irhete/stability-predictive-monitoring\n",
            "https://github.com/lancopku/SGM\n",
            "https://github.com/jaechanglim/CVAE\n",
            "https://github.com/Vijetha1/WDHT\n",
            "https://github.com/charliememory/Disentangled-Person-Image-Generation\n",
            "https://github.com/koryako/selfcarRaspberryPi\n",
            "https://github.com/xyzacademic/RandomDepthwiseCNN\n",
            "https://github.com/Clumsyndicate/One_layer_analysis_network\n",
            "https://github.com/ahq1993/MPNet\n",
            "https://github.com/irom-lab/PAC-Bayes-Control\n",
            "https://github.com/zhengy09/SysId\n",
            "https://github.com/scottcb/MsANN\n",
            "https://github.com/Kulbear/ivus-segmentation-icsm2018\n",
            "https://github.com/anuragranj/humanflow\n",
            "https://github.com/poloclub/interactive-classification\n",
            "https://github.com/KaiQiangSong/struct_infused_summ\n",
            "https://github.com/claudiogreco/coling18-gte\n",
            "https://github.com/h-aldarmaki/sentence_eval\n",
            "https://github.com/Dam930/rllab\n",
            "https://github.com/benathi/fastswa-semi-sup\n",
            "https://github.com/herrinj/LAP\n",
            "https://github.com/applecrazy/FruitClassifier\n",
            "https://github.com/SSS135/aiqn-vae\n",
            "https://github.com/CAMMA-public/ai4surgery\n",
            "https://github.com/pangshumao/CARN\n",
            "https://github.com/PrincetonUniversity/APPLEpicker\n",
            "https://github.com/emtiyaz/vmp-for-svae\n",
            "https://github.com/ghdi6758/SemAxis\n",
            "https://github.com/amit-sharma/splitdoor-causal-criterion\n",
            "https://github.com/rktamplayo/MCFA\n",
            "https://github.com/rktamplayo/HCSC\n",
            "https://github.com/rktamplayo/Entity2Topic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contents[50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "dFyybTYSwCwR",
        "outputId": "b0e99fc2-2aa6-426c-c058-8efc40d699ab"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import argparse\\nimport model.models as models\\nfrom inference import inference\\nimport ipdb\\n\\nif __name__ == \\'__main__\\':\\n    # Possible models\\n    model_names = sorted(name for name in models.__dict__\\n                         if not name.startswith(\"__\")\\n                         and callable(models.__dict__[name]))\\n\\n    # Parser\\n    parser = argparse.ArgumentParser(description=\\'Pytorch implementation: Object level visual reasoning in videos\\')\\n    parser.add_argument(\\'--arch\\', \\'-a\\', metavar=\\'ARCH\\',\\n                        default=\\'orn_two_heads\\',\\n                        choices=model_names,\\n                        help=\\'model architecture: \\' +\\n                             \\' | \\'.join(model_names) +\\n                             \\' (default: orn_two_heads\\')\\n    parser.add_argument(\\'--depth\\', default=50, type=int,\\n                        metavar=\\'D\\', help=\\'depth of the backbone\\')\\n    parser.add_argument(\\'--dataset\\', metavar=\\'D\\',\\n                        default=\\'vlog\\',\\n                        help=\\'dataset name\\')\\n    parser.add_argument(\\'--train-set\\', metavar=\\'T\\',\\n                        default=\\'train+val\\',\\n                        help=\\'Training set: could be train or train+val when you want to get the final accuracy\\')\\n    parser.add_argument(\\'--root\\', metavar=\\'D\\',\\n                        default=\\'./data/vlog\\',\\n                        help=\\'location of the dataset directory\\')\\n    parser.add_argument(\\'-b\\', \\'--batch-size\\', default=4, type=int,\\n                        metavar=\\'B\\', help=\\'batch size\\')\\n    parser.add_argument(\\'-t\\', \\'--t\\', default=2, type=int,\\n                        metavar=\\'T\\', help=\\'number of timesteps extracted from a video\\')\\n    parser.add_argument(\\'--epochs\\', type=int, default=10, metavar=\\'N\\',\\n                        help=\\'number of epochs to train\\')\\n    parser.add_argument(\\'--nb-crops\\', type=int, default=10, metavar=\\'N\\',\\n                        help=\\'number of crops while testing\\')\\n    parser.add_argument(\\'--lr\\', type=float, default=0.0001, metavar=\\'LR\\',\\n                        help=\\'learning rate\\')\\n    parser.add_argument(\\'-j\\', \\'--workers\\', default=0, type=int, metavar=\\'N\\',\\n                        help=\\'number of data loading workers\\')\\n    parser.add_argument(\\'--print-freq\\', default=1, type=int, metavar=\\'P\\',\\n                        help=\\'frequence of printing in the log\\')\\n    parser.add_argument(\\'--resume\\',\\n                        default=\\'/tmp/my_resume\\',\\n                        # default=\\'./resume/vlog\\',\\n                        type=str, metavar=\\'PATH\\',\\n                        help=\\'path to latest checkpoint\\')\\n    parser.add_argument(\\'-e\\', \\'--evaluate\\', dest=\\'evaluate\\', action=\\'store_true\\',\\n                        help=\\'evaluation mode\\')\\n    parser.add_argument(\\'--cuda\\', dest=\\'cuda\\', action=\\'store_true\\',\\n                        help=\\'cuda mode\\')\\n    parser.add_argument(\\'--add-background\\', dest=\\'add_background\\', action=\\'store_true\\',\\n                        help=\\'add the background as one object\\')\\n    parser.add_argument(\\'--mask-confidence\\', type=float, default=0.50, metavar=\\'LR\\',\\n                        help=\\'mininum confidence for the masks\\')\\n    parser.add_argument(\\'--pooling\\', metavar=\\'POOL\\',\\n                        default=\\'rnn\\',\\n                        help=\\'final pooling methods: avg or rnn\\')\\n    parser.add_argument(\\'--heads\\', metavar=\\'H\\',\\n                        default=\\'object\\',\\n                        # default=\\'object+context\\',\\n                        help=\\'what are the heads of the model: object or context or object+context\\')\\n    parser.add_argument(\\'--blocks\\', metavar=\\'LB\\',\\n                        default=\\'2D_2D_2D_2.5D\\',\\n                        help=\\'Nature of the 4 residual blocks: B1_B2_B3_B4 where Bi can be 2D, 3D or 2.5D\\')\\n    parser.add_argument(\\'--object-head\\', metavar=\\'BH\\',\\n                        default=\\'2D\\',\\n                        help=\\'Nature of teh residual block of the object head: Bi where can be 2D, 3D or 2.5D\\')\\n\\n    # Args\\n    args, _ = parser.parse_known_args()\\n\\n    # Dict\\n    options = vars(args)\\n\\n    inference.main(options)from model.backbone.resnet_based import *\\nfrom model.orn_two_heads.two_heads import *import torch.utils.model_zoo as model_zoo\\nfrom model.backbone.resnet.resnet import model_urls\\nimport ipdb\\nimport torch\\n\\n\\ndef _inflate_weight(w, new_temporal_size, inflation=\\'center\\'):\\n    w_up = w.unsqueeze(2).repeat(1, 1, new_temporal_size, 1, 1)\\n    if inflation == \\'center\\':\\n        w_up = central_inflate_3D_conv(w_up)  # center\\n    elif inflation == \\'mean\\':\\n        w_up /= new_temporal_size  # mean\\n    return w_up\\n\\n\\ndef central_inflate_3D_conv(w):\\n    new_temporal_size = w.size(2)\\n    middle_timestep = int(new_temporal_size / 2.)\\n    before, after = list(range(middle_timestep)), list(range(middle_timestep + 1, new_temporal_size))\\n    if len(before) > 0:\\n        w[:, :, before] = torch.zeros_like(w[:, :, before])\\n    if len(after):\\n        w[:, :, after] = torch.zeros_like(w[:, :, after])\\n    return w\\n\\n\\ndef inflate_temporal_conv(pretrained_W_updated, model_dict, inflation):\\n    for k, v in model_dict.items():\\n        if \\'1t\\' in k:\\n            if \\'conv\\' in k:\\n                if \\'bias\\' in k:\\n                    v_up = torch.zeros_like(v)\\n                elif \\'weight\\' in k:\\n                    v_up = torch.zeros_like(v)\\n\\n                    h, w, T, *_ = v_up.size()\\n                    t_2 = int(T / 2.)\\n                    for i in range(h):\\n                        for j in range(w):\\n                            if i == j:\\n                                if inflation == \\'center\\':\\n                                    v_up[i, j, t_2] = torch.ones_like(v_up[i, j, t_2])\\n                                elif inflation == \\'mean\\':\\n                                    v_up[i, j] = torch.ones_like(v_up[i, j]) / T\\n            # elif \\'bn\\' in k:\\n            #     if \\'running_mean\\' in k:\\n            #         v_up = torch.zeros_like(v)\\n            #     elif \\'running_var\\' in k:\\n            #         v_up = torch.ones_like(v) - 1e-05\\n            #     elif \\'bias\\' in k:\\n            #         v_up = torch.zeros_like(v)\\n            #     elif \\'weight\\' in k:\\n            #         v_up = torch.ones_like(v)\\n\\n                # udpate\\n                pretrained_W_updated.update({k: v_up})\\n    return pretrained_W_updated\\n\\n\\ndef _update_pretrained_weights(model, pretrained_W, inflation=\\'center\\'):\\n    pretrained_W_updated = pretrained_W.copy()\\n    model_dict = model.state_dict()\\n    for k, v in pretrained_W.items():\\n        if \"conv\" in k or (\\'bn\\' in k and \\'1t\\' not in k) or \\'downsample\\' in k:\\n            if k in model_dict.keys():\\n                if len(model_dict[k].shape) == 5:\\n                    new_temporal_size = model_dict[k].size(2)\\n                    v_updated = _inflate_weight(v, new_temporal_size, inflation)\\n                else:\\n                    v_updated = v\\n\\n                if isinstance(v, torch.autograd.Variable):\\n                    pretrained_W_updated.update({k: v_updated.data})\\n                else:\\n                    pretrained_W_updated.update({k: v_updated})\\n        if \"fc.weight\" in k:\\n            pretrained_W_updated.pop(\\'fc.weight\\', None)\\n        if \"fc.bias\" in k:\\n            pretrained_W_updated.pop(\\'fc.bias\\', None)\\n\\n    # update the dict for 1D conv for 2.5D conv\\n    pretrained_W_updated = inflate_temporal_conv(pretrained_W_updated, model_dict, inflation)\\n\\n    # update the state dict\\n    model_dict.update(pretrained_W_updated)\\n\\n    return model_dict\\n\\n\\ndef _keep_only_existing_keys(model, pretrained_weights_inflated):\\n    # Loop over the model_dict and update W\\n    model_dict = model.state_dict()  # Take the initial weights\\n    for k, v in model_dict.items():\\n        if k in pretrained_weights_inflated.keys():\\n            model_dict[k] = pretrained_weights_inflated[k]\\n    return model_dict\\n\\n\\ndef load_pretrained_2D_weights(arch, model, inflation):\\n    pretrained_weights = model_zoo.load_url(model_urls[arch])\\n    pretrained_weights_inflated = _update_pretrained_weights(model, pretrained_weights, inflation)\\n    model.load_state_dict(pretrained_weights_inflated)\\n    print(\"     -> Init: Imagenet - 3D from 2D (inflation = {})\".format(inflation))\\n    return model\\nfrom model.backbone.imagenet_pretraining import load_pretrained_2D_weights\\nfrom model.backbone.resnet.basicblock import BasicBlock2D, BasicBlock3D, BasicBlock2_1D\\nfrom model.backbone.resnet.bottleneck import Bottleneck2D, Bottleneck3D, Bottleneck2_1D\\nfrom model.backbone.resnet.resnet import ResNet\\nimport ipdb\\n\\n__all__ = [\\n    \\'resnet_two_heads\\',\\n]\\n\\n\\ndef resnet_two_heads(options, **kwargs):\\n    \"\"\"Constructs a ResNet-18 model with 2 heads\\n    \"\"\"\\n    # Params\\n    depth, blocks, pooling, object_head = options[\\'depth\\'], \\\\\\n                                          options[\\'blocks\\'], \\\\\\n                                          options[\\'pooling\\'], \\\\\\n                                          options[\\'object_head\\']\\n\\n    # Blocks and layers\\n    list_block, list_layers = get_cnn_features(depth=depth,\\n                                               str_blocks=blocks)\\n    blocks_object_head, _ = get_cnn_features(depth=depth,\\n                                             str_blocks=object_head)\\n\\n    # Model with two heads\\n    model = ResNet(list_block,\\n                   list_layers,\\n                   two_heads=True,\\n                   blocks_2nd_head=blocks_object_head,\\n                   pooling=pooling, **kwargs)\\n\\n    print(\\n        \"*** Backbone: Resnet{} (blocks: {} - pooling: {} - Two heads - blocks 2nd head: {} and fm size 2nd head: {}) ***\".format(\\n            depth,\\n            blocks,\\n            pooling,\\n            object_head,\\n            model.size_fm_2nd_head))\\n\\n    # Pretrained from imagenet weights\\n    model = load_pretrained_2D_weights(\\'resnet{}\\'.format(depth), model, inflation=\\'center\\')\\n\\n    return model\\n\\n\\ndef get_cnn_features(str_blocks=\\'2D_2D_2D_2D\\', depth=18):\\n    # List of blocks\\n    list_block = []\\n\\n    # layers\\n    if depth == 18:\\n        list_layers = [2, 2, 2, 2]\\n        nature_of_block = \\'basic\\'\\n    elif depth == 34:\\n        list_layers = [3, 4, 6, 3]\\n        nature_of_block = \\'basic\\'\\n    elif depth == 50:\\n        list_layers = [3, 4, 6, 3]\\n        nature_of_block = \\'bottleneck\\'\\n    else:\\n        raise NameError\\n\\n    # blocks\\n    if nature_of_block == \\'basic\\':\\n        block_2D, block_3D, block_2_1D = BasicBlock2D, BasicBlock3D, BasicBlock2_1D\\n    elif nature_of_block == \\'bottleneck\\':\\n        block_2D, block_3D, block_2_1D = Bottleneck2D, Bottleneck3D, Bottleneck2_1D\\n    else:\\n        raise NameError\\n\\n    # From string to blocks\\n    list_block_id = str_blocks.split(\\'_\\')\\n\\n    # Catch from the options if exists\\n    for i, str_block in enumerate(list_block_id):\\n        # Block kind\\n        if str_block == \\'2D\\':\\n            list_block.append(block_2D)\\n        elif str_block == \\'2.5D\\':\\n            list_block.append(block_2_1D)\\n        elif str_block == \\'3D\\':\\n            list_block.append(block_3D)\\n        else:\\n            ipdb.set_trace()\\n            raise NameError\\n\\n    return list_block, list_layers\\nimport torch.nn as nn\\nimport ipdb\\n\\n\\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\\n    \"3x3 convolution with padding\"\\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\\n                     padding=1, bias=False, dilation=dilation)\\n\\n\\ndef conv3x3x3(in_planes, out_planes, stride=1, dilation=1):\\n    \"3x3 convolution with padding\"\\n    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride,\\n                     padding=1, bias=False, dilation=(1, dilation, dilation))\\n\\n\\ndef conv1x3x3(in_planes, out_planes, stride=1, dilation=1):\\n    \"3x3 convolution with padding\"\\n    if isinstance(stride, int):\\n        stride_1, stride_2, stride_3 = 1, stride, stride\\n    else:\\n        stride_1, stride_2, stride_3 = 1, stride[1], stride[2]\\n\\n    return nn.Conv3d(in_planes, out_planes, kernel_size=(1, 3, 3),\\n                     stride=(stride_1, stride_2, stride_3),\\n                     padding=(0, 1, 1), bias=False, dilation=(1, dilation, dilation))\\n\\n\\ndef conv1x3x3_conv3x1x1(in_planes, out_planes, stride=1, dilation=1, nb_temporal_conv=3):\\n    \"3x3 convolution with padding\"\\n    if isinstance(stride, int):\\n        stride_2d, stride_1t = (1, stride, stride), (stride, 1, 1)\\n    else:\\n        stride_2d, stride_1t = (1, stride[1], stride[2]), (stride[0], 1, 1)\\n\\n    _2d = nn.Conv3d(in_planes, out_planes, kernel_size=(1, 3, 3), stride=stride_2d,\\n                    padding=(0, 1, 1), bias=False, dilation=dilation)\\n\\n    _1t = nn.Sequential()\\n    for i in range(nb_temporal_conv):\\n        temp_conv = nn.Conv3d(out_planes, out_planes, kernel_size=(3, 1, 1), stride=stride_1t,\\n                              padding=(1, 0, 0), bias=False, dilation=1)\\n        _1t.add_module(\\'temp_conv_{}\\'.format(i), temp_conv)\\n        _1t.add_module((\\'relu_{}\\').format(i), nn.ReLU(inplace=True))\\n\\n    return _2d, _1t\\n\\n\\nclass BasicBlock(nn.Module):\\n    expansion = 1\\n    only_2D = False\\n\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\\n        super(BasicBlock, self).__init__()\\n        self.bn1 = nn.BatchNorm3d(planes)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.bn2 = nn.BatchNorm3d(planes)\\n        self.downsample = downsample\\n        self.stride = stride\\n        self.conv1, self.conv2 = None, None\\n        self.input_dim = 5\\n        self.dilation = dilation\\n\\n    def forward(self, x):\\n        residual = x\\n\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n\\n        if self.downsample is not None:\\n            residual = self.downsample(x)\\n\\n        out += residual\\n        out = self.relu(out)\\n\\n        return out\\n\\n\\nclass BasicBlock3D(BasicBlock):\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, **kwargs):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n        self.conv1 = conv3x3x3(inplanes, planes, stride, dilation)\\n        self.conv2 = conv3x3x3(planes, planes, dilation)\\n\\n\\nclass BasicBlock2D(BasicBlock):\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, **kwargs):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n        # not the same input size here to speed up training\\n        self.bn1 = nn.BatchNorm2d(planes)\\n        self.bn2 = nn.BatchNorm2d(planes)\\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation)\\n        self.conv2 = conv3x3(planes, planes, dilation)\\n        self.input_dim = 4\\n\\n\\nclass BasicBlock2_1D(BasicBlock):\\n    expansion = 1\\n\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, nb_temporal_conv=1):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n        self.conv1, self.conv1_1t = conv1x3x3_conv3x1x1(inplanes, planes, stride, dilation,\\n                                                        nb_temporal_conv=nb_temporal_conv)\\n        self.conv2, self.conv2_1t = conv1x3x3_conv3x1x1(planes, planes, dilation,\\n                                                        nb_temporal_conv=nb_temporal_conv)\\n\\n\\n    def forward(self, x):\\n        residual = x\\n\\n        out = self.conv1(x)  # 2D in space\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        # ipdb.set_trace()\\n        out = self.conv1_1t(out)  # 1D in time + relu after each conv\\n\\n        out = self.conv2(out)  # 2D in space\\n        out = self.bn2(out)\\n\\n        if self.downsample is not None:\\n            residual = self.downsample(x)\\n\\n        out += residual\\n        out = self.relu(out)\\n\\n        out = self.conv2_1t(out)  # 1D in time\\n\\n        return out\\nimport torch.nn as nn\\nimport math\\nfrom model.backbone.resnet.basicblock import BasicBlock2D\\nfrom model.backbone.resnet.bottleneck import Bottleneck2D\\nfrom utils.other import transform_input\\nimport ipdb\\nimport torch\\nfrom utils.meter import *\\n\\nmodel_urls = {\\n    \\'resnet18\\': \\'https://download.pytorch.org/models/resnet18-5c106cde.pth\\',\\n    \\'resnet34\\': \\'https://download.pytorch.org/models/resnet34-333f7ec4.pth\\',\\n    \\'resnet50\\': \\'https://download.pytorch.org/models/resnet50-19c8e357.pth\\',\\n    \\'resnet101\\': \\'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\\',\\n    \\'resnet152\\': \\'https://download.pytorch.org/models/resnet152-b121ed2d.pth\\',\\n}\\n\\nK_1st_CONV = 3\\n\\n\\nclass ResNet(nn.Module):\\n    def __init__(self, blocks, layers, num_classes=1000, str_first_conv=\\'2D\\',\\n                 num_final_fm=4,\\n                 two_heads=False,\\n                 size_fm_2nd_head=7,\\n                 blocks_2nd_head=None,\\n                 pooling=\\'avg\\',\\n                 nb_temporal_conv=1,\\n                 list_stride=[1, 2, 2, 2],\\n                 **kwargs):\\n        self.nb_temporal_conv = nb_temporal_conv\\n        self.size_fm_2nd_head = size_fm_2nd_head\\n        self.two_heads = two_heads\\n        self.inplanes = 64\\n        self.input_dim = 5  # from 5D to 4D tensor if 2D conv\\n        super(ResNet, self).__init__()\\n        self.num_final_fm = num_final_fm\\n        self.time = None\\n        self._first_conv(str_first_conv)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.list_channels = [64, 128, 256, 512]\\n        self.list_inplanes = []\\n        self.list_inplanes.append(self.inplanes)  # store the inplanes after layer1\\n        self.layer1 = self._make_layer(blocks[0], self.list_channels[0], layers[0], stride=list_stride[0])\\n        self.list_inplanes.append(self.inplanes)  # store the inplanes after layer1\\n        self.layer2 = self._make_layer(blocks[1], self.list_channels[1], layers[1], stride=list_stride[1])\\n        self.list_inplanes.append(self.inplanes)  # store the inplanes after layer2\\n        self.layer3 = self._make_layer(blocks[2], self.list_channels[2], layers[2], stride=list_stride[2])\\n        self.list_inplanes.append(self.inplanes)  # store the inplanes after layer3\\n        self.layer4 = self._make_layer(blocks[3], self.list_channels[3], layers[3], stride=list_stride[3])\\n        self.avgpool, self.avgpool_space, self.avgpool_time = None, None, None\\n        self.fc_classifier = nn.Linear(512 * blocks[3].expansion, num_classes)\\n        self.out_dim = 5\\n        self.pooling = pooling\\n\\n        if self.two_heads:\\n            # Stride second head\\n            list_strides_2nd_head = self._get_stride_2nd_head()\\n\\n            # Common block\\n            self.nb_block_common_trunk = 4 - len(blocks_2nd_head)\\n\\n            self.list_layers_bis = []\\n            for i in range(self.nb_block_common_trunk, 4):\\n                # Take the correct inplanes\\n                self.inplanes = self.list_inplanes[i]\\n\\n                # Create the layer\\n                layer = self._make_layer(blocks_2nd_head[i - self.nb_block_common_trunk],\\n                                         self.list_channels[i],\\n                                         layers[i],\\n                                         list_strides_2nd_head[i])\\n                if i == 0:\\n                    self.layer1_bis = layer\\n                    self.list_layers_bis.append(self.layer1_bis)\\n                elif i == 1:\\n                    self.layer2_bis = layer\\n                    self.list_layers_bis.append(self.layer2_bis)\\n                elif i == 2:\\n                    self.layer3_bis = layer\\n                    self.list_layers_bis.append(self.layer3_bis)\\n                elif i == 3:\\n                    self.layer4_bis = layer\\n                    self.list_layers_bis.append(self.layer4_bis)\\n                else:\\n                    raise NameError\\n\\n            # List layers\\n            self.list_layers = [self.layer1, self.layer2, self.layer3, self.layer4]\\n\\n        # Pooling method\\n        if self.pooling == \\'rnn\\':\\n            cnn_features_size = 512 * blocks[3].expansion\\n            hidden_state_size = 256 if cnn_features_size == 512 else 512\\n            self.rnn = nn.GRU(input_size=cnn_features_size,\\n                              hidden_size=hidden_state_size,\\n                              num_layers=1,\\n                              batch_first=True)\\n            self.fc_classifier = nn.Linear(hidden_state_size, num_classes)\\n\\n        # Init of the weights\\n        for m in self.modules():\\n            if isinstance(m, nn.Conv3d) or isinstance(m, nn.Conv2d):\\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\\n                m.weight.data.normal_(0, math.sqrt(2. / n))\\n            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\\n                m.weight.data.fill_(1)\\n                m.bias.data.zero_()\\n\\n    def _get_stride_2nd_head(self):\\n        if self.size_fm_2nd_head == 7:\\n            return [1, 2, 2, 2]\\n        elif self.size_fm_2nd_head == 14:\\n            return [1, 2, 2, 1]\\n        elif self.size_fm_2nd_head == 28:\\n            return [1, 2, 1, 1]\\n\\n    def _first_conv(self, str):\\n        self.conv1_1t = None\\n        self.bn1_1t = None\\n        if str == \\'3D_stabilize\\':\\n            self.conv1 = nn.Conv3d(3, 64, kernel_size=(K_1st_CONV, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3),\\n                                   bias=False)\\n            self.maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\\n            self.bn1 = nn.BatchNorm3d(64)\\n\\n\\n        elif str == \\'2.5D_stabilize\\':\\n            self.conv1 = nn.Conv3d(3, 64, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3),\\n                                   bias=False)\\n            self.conv1_1t = nn.Conv3d(64, 64, kernel_size=(K_1st_CONV, 1, 1), stride=(1, 1, 1),\\n                                      padding=(1, 0, 0),\\n                                      bias=False)\\n            self.bn1_1t = nn.BatchNorm3d(64)\\n            self.maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\\n            self.bn1 = nn.BatchNorm3d(64)\\n\\n        elif str == \\'2D\\':\\n            self.conv1 = nn.Conv2d(3, 64,\\n                                   kernel_size=(7, 7),\\n                                   stride=(2, 2),\\n                                   padding=(3, 3),\\n                                   bias=False)\\n            self.maxpool = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\\n            self.bn1 = nn.BatchNorm2d(64)\\n            self.input_dim = 4\\n\\n        else:\\n            raise NameError\\n\\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\\n        downsample = None\\n\\n        # Upgrade the stride is spatio-temporal kernel\\n        if not (block == BasicBlock2D or block == Bottleneck2D):\\n            stride = (1, stride, stride)\\n\\n        if stride != 1 or self.inplanes != planes * block.expansion:\\n            if block is BasicBlock2D or block is Bottleneck2D:\\n                conv, batchnorm = nn.Conv2d, nn.BatchNorm2d\\n            else:\\n                conv, batchnorm = nn.Conv3d, nn.BatchNorm3d\\n\\n            downsample = nn.Sequential(\\n                conv(self.inplanes, planes * block.expansion,\\n                     kernel_size=1, stride=stride, bias=False, dilation=dilation),\\n                batchnorm(planes * block.expansion),\\n            )\\n\\n        layers = []\\n        layers.append(\\n            block(self.inplanes, planes, stride, downsample, dilation, nb_temporal_conv=self.nb_temporal_conv))\\n        self.inplanes = planes * block.expansion\\n        for i in range(1, blocks):\\n            layers.append(block(self.inplanes, planes, nb_temporal_conv=self.nb_temporal_conv))\\n\\n        return nn.Sequential(*layers)\\n\\n    def get_features_map(self, x, time=None, num=4, out_dim=None):\\n        if out_dim is None:\\n            out_dim = self.out_dim\\n\\n        if self.time is None:\\n            B, C, T, W, H = x.size()\\n            self.time = T\\n\\n        time = self.time\\n\\n        # 5D -> 4D if 2D conv at the beginning\\n        x = transform_input(x, self.input_dim, T=time)\\n\\n        x = self.conv1(x)\\n        x = self.bn1(x)\\n        x = self.relu(x)\\n\\n        if self.conv1_1t is not None:\\n            x = self.conv1_1t(x)\\n            x = self.bn1_1t(x)\\n            x = self.relu(x)\\n\\n        x = self.maxpool(x)\\n\\n        # 1st residual block\\n        if num >= 1:\\n            # ipdb.set_trace()\\n            x = transform_input(x, self.layer1[0].input_dim, T=time)\\n            x = self.layer1(x)\\n\\n        # 2nd residual block\\n        if num >= 2:\\n            x = transform_input(x, self.layer2[0].input_dim, T=time)\\n            x = self.layer2(x)\\n\\n        # 3rd residual block\\n        if num >= 3:\\n            x = transform_input(x, self.layer3[0].input_dim, T=time)\\n            x = self.layer3(x)\\n\\n        # 4th residual block\\n        if num >= 4:\\n            x = transform_input(x, self.layer4[0].input_dim, T=time)\\n            x = self.layer4(x)\\n\\n        return transform_input(x, out_dim, T=time)\\n\\n    def get_two_heads_feature_maps(self, x, T=None, out_dim=None, heads_type=\\'object+context\\'):\\n        x = x[\\'clip\\']  # (B, C, T, W, H)\\n\\n        # Get the before last feature map\\n        x = self.get_features_map(x, T, num=self.nb_block_common_trunk)\\n\\n        # Object head\\n        if \\'object\\' in heads_type:\\n            fm_objects = x\\n            for i in range(len(self.list_layers_bis)):\\n                layer = self.list_layers_bis[i]\\n                fm_objects = transform_input(fm_objects, layer[0].input_dim, T=T)\\n                fm_objects = layer(fm_objects)\\n            fm_objects = transform_input(fm_objects, out_dim, T=T)\\n        else:\\n            fm_objects = None\\n\\n        # Activity head\\n        if \\'context\\' in heads_type:\\n            fm_context = x\\n            for i in range(self.nb_block_common_trunk, 4):\\n                layer = self.list_layers[i]\\n                fm_context = transform_input(fm_context, layer[0].input_dim, T=T)\\n                fm_context = layer(fm_context)\\n            fm_context = transform_input(fm_context, out_dim, T=T)\\n        else:\\n            fm_context = None\\n\\n        return fm_context, fm_objects\\n\\n    def forward(self, x):\\n        x = x[\\'clip\\']\\n\\n        x = self.get_features_map(x, num=self.num_final_fm)\\n\\n        # Global average pooling\\n        if self.pooling == \\'avg\\':\\n            self.avgpool = nn.AvgPool3d((x.size(2), x.size(-1), x.size(-1))) if self.avgpool is None else self.avgpool\\n            x = self.avgpool(x)\\n        elif self.pooling == \\'rnn\\':\\n            self.avgpool_space = nn.AvgPool3d(\\n                (1, x.size(-1), x.size(-1))) if self.avgpool_space is None else self.avgpool_space\\n            x = self.avgpool_space(x)\\n            x = x.view(x.size(0), x.size(1), x.size(2))  # (B,D,T)\\n            x = x.transpose(1, 2)  # (B,T,D)\\n            ipdb.set_trace()\\n            x, _ = self.rnn(x)  # (B,T,D/2)\\n            x = torch.mean(x, 1)  # (B,D/2)\\n\\n        # Final classif\\n        x = x.view(x.size(0), -1)\\n        x = self.fc_classifier(x)\\n\\n        return x\\nimport torch.nn as nn\\nimport ipdb\\n\\n\\nclass Bottleneck(nn.Module):\\n    expansion = 4\\n    only_2D = False\\n\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\\n        super(Bottleneck, self).__init__()\\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\\n        self.bn1 = nn.BatchNorm3d(planes)\\n        self.conv2 = None\\n        self.bn2 = nn.BatchNorm3d(planes)\\n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\\n        self.bn3 = nn.BatchNorm3d(planes * 4)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.downsample = downsample\\n        self.stride = stride\\n        self.input_dim = 5\\n        self.dilation = dilation\\n\\n    def forward(self, x):\\n        residual = x\\n\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n        out = self.relu(out)\\n\\n        out = self.conv3(out)\\n        out = self.bn3(out)\\n\\n        if self.downsample is not None:\\n            residual = self.downsample(x)\\n\\n        out += residual\\n        out = self.relu(out)\\n\\n        return out\\n\\n\\nclass Bottleneck3D(Bottleneck):\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, **kwargs):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=stride,\\n                               padding=1, bias=False, dilation=(1, dilation, dilation))\\n\\n\\nclass Bottleneck2D(Bottleneck):\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, **kwargs):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n        # to speed up the inference process\\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\\n        self.bn1 = nn.BatchNorm2d(planes)\\n        self.bn2 = nn.BatchNorm2d(planes)\\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False, dilation=dilation)\\n        self.bn3 = nn.BatchNorm2d(planes * 4)\\n        self.input_dim = 4\\n\\n        if isinstance(stride, int):\\n            stride_1, stride_2 = stride, stride\\n        else:\\n            stride_1, stride_2 = stride[0], stride[1]\\n\\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=(3, 3), stride=(stride_1, stride_2),\\n                               padding=(1, 1), bias=False)\\n\\n\\nclass Bottleneck2_1D(Bottleneck):\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, nb_temporal_conv=1):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n\\n        if isinstance(stride, int):\\n            stride_2d, stride_1t = (1, stride, stride), (stride, 1, 1)\\n        else:\\n            stride_2d, stride_1t = (1, stride[1], stride[2]), (stride[0], 1, 1)\\n\\n        # CONV2\\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=(1, 3, 3), stride=stride_2d,\\n                               padding=(0, dilation, dilation), bias=False, dilation=dilation)\\n\\n        self.conv2_1t = nn.Sequential()\\n        for i in range(nb_temporal_conv):\\n            temp_conv = nn.Conv3d(planes, planes, kernel_size=(3, 1, 1), stride=stride_1t,\\n                                  padding=(1, 0, 0), bias=False, dilation=1)\\n            self.conv2_1t.add_module(\\'temp_conv_{}\\'.format(i), temp_conv)\\n            self.conv2_1t.add_module((\\'relu_{}\\').format(i), nn.ReLU(inplace=True))\\n\\n\\n    def forward(self, x):\\n        residual = x\\n\\n        ## CONV1 - 3D (1,1,1)\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        ## CONV2\\n        #  Spatial - 2D (1,3,3)\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n        out = self.relu(out)\\n\\n        # Temporal - 3D (3,1,1)\\n        out = self.conv2_1t(out)\\n\\n        ## CONV3 - 3D (1,1,1)\\n        out = self.conv3(out)\\n        out = self.bn3(out)\\n\\n        if self.downsample is not None:\\n            residual = self.downsample(x)\\n\\n        out += residual\\n        out = self.relu(out)\\n\\n        return out\\nfrom __future__ import print_function\\nimport torch.nn as nn\\n\\n\\nclass Classifier(nn.Module):\\n    def __init__(self, size_input, size_output):\\n        super(Classifier, self).__init__()\\n        # Basic settings\\n        self.size_input = size_input\\n        self.size_output= size_output\\n\\n        # FC layer\\n        self.fc = nn.Linear(self.size_input, self.size_output)\\n\\n    def forward(self, x):\\n\\n        preds = self.fc(x)\\n\\n        return predsimport torch\\nimport torch.nn as nn\\nfrom torch.autograd import Variable\\nimport torch.nn.functional as F\\nimport ipdb\\n\\n\\nclass EncoderMLP(nn.Module):\\n    def __init__(self, input_size=149, list_hidden_size=[100, 100], relu_activation=True, p_dropout=0.5):\\n        super(EncoderMLP, self).__init__()\\n        self.input_size = input_size\\n        self.list_hidden_size = list_hidden_size\\n\\n        # Encoder\\n        self.encoder = nn.Sequential()\\n        current_input_size = self.input_size\\n        for i, hidden_size in enumerate(self.list_hidden_size):\\n            # Add the linear layer\\n            self.encoder.add_module(\\'linear_{}\\'.format(i), nn.Linear(current_input_size, hidden_size))\\n            self.encoder.add_module(\\'dropout_{}\\'.format(i), nn.Dropout(p=p_dropout))\\n            current_input_size = hidden_size\\n            # Add ReLu activation\\n            self.encoder.add_module(\\'relu_{}\\'.format(i), nn.ReLU())\\n\\n    def forward(self, x):\\n        size_x = x.size()\\n\\n        # Transform to get the corresponding input vector size\\n        if size_x[-1] == self.input_size:\\n            x_input = x\\n        elif size_x[-1] * size_x[-2] == self.input_size:\\n            if len(size_x) == 5:\\n                B, T, K, W, H = size_x\\n            elif len(size_x) == 4:\\n                B, T, K, W = size_x\\n                H = 1\\n            x = x.contiguous()\\n            x_input = x.view(B, T, K, W * H)\\n        else:\\n            raise Exception\\n\\n        # Encoder\\n        z = self.encoder(x_input)\\n\\n        return zimport torch\\nimport torch.nn as nn\\nfrom torch.autograd import Variable\\nimport torch.nn.functional as F\\nimport random\\nimport ipdb\\n\\nclass AggregationRelations(nn.Module):\\n    def __init__(self):\\n        super(AggregationRelations, self).__init__()\\n\\n    def forward(self, relational_reasoning_vector):\\n        # Basic summation\\n        B, T, K2, _ = relational_reasoning_vector.size()\\n\\n        output = torch.sum(relational_reasoning_vector, 2)\\n\\n\\n        return output  # (B,T-1,512)from __future__ import print_function\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torchvision import models\\nfrom torch.autograd import Variable\\nfrom model.backbone.resnet_based import resnet_two_heads\\nimport numpy as np\\nimport random\\nfrom model.orn_two_heads.encoder import EncoderMLP\\nfrom model.orn_two_heads.classifier import Classifier\\nfrom model.backbone.resnet.bottleneck import Bottleneck2D, Bottleneck3D, Bottleneck2_1D\\nfrom model.backbone.resnet.basicblock import BasicBlock2D, BasicBlock3D, BasicBlock2_1D\\nfrom model.orn_two_heads.aggregation_relations import AggregationRelations\\nimport math\\nfrom model.orn_two_heads.orn import ObjectRelationNetwork\\nimport ipdb\\n\\n__all__ = [\\n    \\'orn_two_heads\\',\\n]\\n\\n\\nclass TwoHeads(nn.Module):\\n    def __init__(self, num_classes=174, cnn=None,\\n                 features_size=512, time=8, mask_size=28, size_RN=256, nb_head=2,\\n                 logits_type=\\'object\\', f_orn=True,\\n                 size_2nd_head=14,\\n                 **kwargs):\\n        super(TwoHeads, self).__init__()\\n        # Basic settings\\n        self.num_classes = num_classes\\n        self.time = time\\n        self.size_cnn_features = features_size\\n        self.size_mask = mask_size\\n        self.size_RN = size_RN\\n        self.size_RNN = size_RN\\n        self.nb_head = nb_head\\n        self.logits_type = logits_type\\n        self.size_fm_2nd_head = size_2nd_head\\n\\n        # CNN: 4 first conv are shared and the 5th blocks is split\\n        self.cnn = cnn\\n        self.cnn.out_dim = 5\\n\\n        # # If object head only then freeze the cnn because it has already trained for the object recognition task\\n        if self.logits_type == \\'object\\':\\n            # freeze the 2 first blocks and the object head\\n            for i, child in enumerate(self.cnn.children()):\\n                if i != 7:\\n                    for param in child.parameters():\\n                        param.requires_grad = False\\n\\n        # Average Pooling\\n        self.avgpool_TxMxM = nn.AvgPool3d((self.time, self.size_fm_2nd_head, self.size_fm_2nd_head))\\n        self.avgpool_1xMxM = nn.AvgPool3d((1, self.size_fm_2nd_head, self.size_fm_2nd_head))\\n        self.avgpool_1_7x7 = nn.AvgPool3d((1, 7, 7))  # for pooling features from the context head\\n        self.avgpool_T_7x7 = nn.AvgPool3d((self.time, 7, 7))  # for pooling features from the context head\\n        self.pixel_pooler = nn.AvgPool3d((1, 2, 2)) if self.size_fm_2nd_head == 14 else  nn.AvgPool3d(\\n            (1, 4, 4))  # from 14x14 to 7x7\\n\\n        # Max pooling for the ORN module\\n        self.pool_orn = nn.MaxPool2d((self.time - 1, 1))\\n\\n        # COCO object features\\n        self.size_COCO_object_features = self.size_cnn_features\\n\\n        # Prediction of the class of each detected COCO objects (MLP from the pooled features)\\n        self.COCO_Object_Class_from_Features = Classifier(\\n            size_input=self.size_cnn_features,\\n            size_output=81)\\n\\n        # Embedding of the binary mask by AutoEncoder\\n        # Goal -> find the latent space of the shape and location of the objects\\n        self.size_mask_embedding = 100\\n        self.Encoder_Binary_Mask = EncoderMLP(input_size=self.size_mask * self.size_mask,\\n                                              list_hidden_size=[self.size_mask_embedding, self.size_mask_embedding])\\n\\n        # Embedding of the object id\\n        # Goal -> find the latent space of object id better than just a one hot vector\\n        self.size_obj_embedding = 100\\n        input_size = 81\\n        self.Encoder_COCO_Obj_Class = EncoderMLP(input_size=input_size,\\n                                                 list_hidden_size=[self.size_obj_embedding, self.size_obj_embedding])\\n\\n        # Object Relational Network (ORN) between coco or pixel objects\\n        size_object = self.size_COCO_object_features + self.size_mask_embedding + self.size_obj_embedding\\n\\n        # ORN\\n        list_size_hidden_layers = [self.size_RN, self.size_RN, self.size_RN]\\n        self.ORN = ObjectRelationNetwork(size_object=size_object,\\n                                         list_hidden_layers_size=list_size_hidden_layers\\n                                         )\\n        # Aggregation over the\\n        self.AggregationRelations = AggregationRelations()\\n\\n        # RNN or AVG\\n        self.f_orn = f_orn\\n        # RNN Object\\n        if self.f_orn == \\'rnn\\':\\n            input_size = self.size_RN\\n            self.size_relation_features = int(self.size_RN / 2.)\\n            self.rnn_objects = nn.GRU(input_size=input_size,\\n                                      hidden_size=self.size_relation_features,\\n                                      num_layers=1,\\n                                      batch_first=True)\\n        else:\\n            self.size_relation_features = self.size_RN\\n\\n        ## Final classification\\n        self.fc_classifier_object = nn.Linear(self.size_relation_features, self.num_classes)\\n        self.fc_classifier_context = nn.Linear(self.size_cnn_features, self.num_classes)\\n\\n    def get_objects_features(self, fm, masks):\\n        # Upsample the features maps to get better precision!\\n        # fm_old = fm\\n        fm = fm.transpose(1, 2)  # (B, T, D, 14, 14)\\n        B, T, D, W, H = fm.size()\\n        fm = fm.view(B * T, D, W, H)\\n        fm_up = F.upsample(fm, size=(self.size_mask, self.size_mask), mode=\\'bilinear\\', align_corners=True)\\n        fm_up = fm_up.view(B, T, D, self.size_mask, self.size_mask)\\n        fm_up = fm_up.transpose(1, 2)  # (B, D, T, 28, 28)\\n\\n        # B and K\\n        B, D, T, W, _ = fm_up.size()\\n        K = masks.size(2)\\n        W_masks = masks.size(-1)\\n\\n        # Make it as the same size and do Hadamard product\\n        fm_plus = fm_up.unsqueeze(1)  # (B,1,D,T,7,7)\\n        masks_plus = masks.transpose(1, 2)\\n        masks_plus = masks_plus.unsqueeze(2)  # (B,K,1,T,7,7)\\n        fm_masked = fm_plus * masks_plus\\n\\n        # Area of the objects in the \"image\"\\n        masks_size = torch.sum(torch.sum(masks_plus, -1), -1)  # (B,K,1,T)\\n\\n        list_object_set = []\\n        for t in range(T):\\n            # Pool the objects features\\n            object_set_avg = torch.sum(torch.sum(fm_masked[:, :, :, t], -1), -1)\\n            object_set_avg /= (masks_size[:, :, :, t] + 1e-4)\\n\\n            # Append\\n            list_object_set.append(object_set_avg)\\n\\n        # Stack\\n        objects_features = torch.stack(list_object_set, 1)  # (B,T,K,D)\\n\\n        return objects_features\\n\\n    def get_pixel_features(self, fm):\\n        *_, W, H = fm.size()\\n        fm = fm.transpose(1, 2)\\n        list_pixel_features = []\\n\\n        if W != 7:\\n            fm = self.pixel_pooler(fm)\\n            *_, W, H = fm.size()\\n\\n        for i in range(W):\\n            for j in range(H):\\n                list_pixel_features.append(fm[:, :, :, i, j])\\n        pixel_features = torch.stack(list_pixel_features, 2)\\n\\n        return pixel_features\\n\\n    def retrieve_relations(self, relational_reasoning_vector_COCO, obj_id):\\n        B, T, K, C = obj_id.size()\\n        B, T_1, K2, D = relational_reasoning_vector_COCO.size()\\n\\n        relations = np.zeros((B, 81, 81))\\n        for b in range(B):\\n            for t in range(T_1):\\n                for k2 in range(K2):\\n                    try:\\n                        # k2-th interaction\\n                        inter = torch.sum(relational_reasoning_vector_COCO[b, t, k2])\\n\\n                        ## Find the corresponding objects\\n                        # k_1 object of previous timestep\\n                        k_1 = math.floor(float(k2) / float(K))\\n                        obj_id_k_1 = self.get_id_object(obj_id[b, t, k_1].data.cpu().numpy())\\n                        # k object of current timestep\\n                        k = k2 - k_1 * K\\n                        obj_id_k = self.get_id_object(obj_id[b, t + 1, k].data.cpu().numpy())\\n\\n                        # Add the relation\\n                        relations[b, obj_id_k_1, obj_id_k] = inter  # matrix but we fill only half of it (the triangle)\\n                    except:\\n                        pass\\n                        # import ipdb\\n                        # ipdb.set_trace()\\n        return relations  # (B, 81, 81)\\n\\n    def retrieve_relations_temporal(self, relational_reasoning_vector_COCO, obj_id):\\n        B, T, K, C = obj_id.size()\\n        B, T_1, K2, D = relational_reasoning_vector_COCO.size()\\n\\n        relations = np.zeros((B, T_1, 81, 81))\\n        for b in range(B):\\n            for t in range(T_1):\\n                for k2 in range(K2):\\n                    try:\\n                        # k2-th interaction\\n                        inter = torch.sum(relational_reasoning_vector_COCO[b, t, k2])\\n\\n                        ## Find the corresponding objects\\n                        # k_1 object of previous timestep\\n                        k_1 = math.floor(float(k2) / float(K))\\n                        obj_id_k_1 = self.get_id_object(obj_id[b, t, k_1].data.cpu().numpy())\\n                        # k object of current timestep\\n                        k = k2 - k_1 * K\\n                        obj_id_k = self.get_id_object(obj_id[b, t + 1, k].data.cpu().numpy())\\n\\n                        # Add the relation\\n                        relations[\\n                            b, t, obj_id_k_1, obj_id_k] = inter  # matrix but we fill only half of it (the triangle)\\n                    except:\\n                        import ipdb\\n                        pass\\n                        # ipdb.set_trace()\\n        return relations  # (B, T-1, 81, 81)\\n\\n    @staticmethod\\n    def get_id_object(one_hot):\\n        if one_hot[0] == 1:\\n            return 0\\n        else:\\n            return np.argmax(one_hot)\\n\\n    def context_head(self, fm_context, B):\\n        # 3D GAP\\n        context_vector = self.avgpool_T_7x7(fm_context)\\n        context_representation = context_vector.view(B, self.size_cnn_features)\\n\\n        return context_representation\\n\\n    def object_head(self, fm_objects, masks, obj_id, B):\\n        # Retrieve the feature vector associated to each detected COCO object\\n        objects_features = self.get_objects_features(fm_objects, masks)\\n\\n        # Classify each detected objects to make sure we extract good object descriptors\\n        preds_class_detected_objects = self.COCO_Object_Class_from_Features(objects_features)\\n\\n        # Reconstruct the binary masks to find the correct embedding (i.e. shape and location of the detected objects)\\n        embedding_objects_location = self.Encoder_Binary_Mask(masks)\\n\\n        # Reconstruct the COCO class id given the one hot vector\\n        embedding_obj_id = self.Encoder_COCO_Obj_Class(obj_id)\\n\\n        # Full objects description\\n        full_objects = torch.cat([objects_features, embedding_objects_location, embedding_obj_id],\\n                                 -1)  # (B,T,K,object_size)\\n\\n        # Run the Relational Reasoning over the different set of COCO objects\\n        D = self.size_cnn_features  # 512\\n        all_e, all_is_obj = self.ORN(full_objects, D)  # [B, T-1, K*K, D]\\n\\n        # Get only interactions where at least one obj is involved (for COCO only)\\n        all_is_obj = all_is_obj.unsqueeze(-1)\\n        all_e *= all_is_obj\\n\\n        # Aggregation of the COCO relations\\n        orn_aggregated = self.AggregationRelations(all_e)\\n\\n        if self.f_orn == \\'rnn\\':\\n            # self.rnn_objects.flatten_parameters() # does not work for multi-GPU ---> bug https://github.com/pytorch/pytorch/issues/7092\\n            object_representation, _ = self.rnn_objects(orn_aggregated)\\n            # self.rnn_objects.flatten_parameters()\\n            object_representation = torch.mean(object_representation,\\n                                               1)  # TODO look at the two lines above for a better pooling over time!\\n            # object_representation = self.pool_orn(object_representation) # TODO interesting point for pooling over the hidden states\\n        elif self.f_orn == \\'avg\\':\\n            object_representation = torch.mean(orn_aggregated, 1)\\n        else:\\n            raise Exception\\n\\n        return object_representation, preds_class_detected_objects\\n\\n    def add(self, logits, logits_head):\\n        if logits is None:\\n            return logits_head\\n        else:\\n            return logits + logits_head\\n\\n    def squeeze_masks(self, max_nb_obj, masks, obj_id, bbox):\\n        # Max number of objects\\n        nb_max_obj_in_B = int(torch.max(max_nb_obj).cpu().numpy())\\n\\n        # Squeeze\\n        masks_squeezed = masks[:, :, :nb_max_obj_in_B]\\n        obj_id_squeezed = obj_id[:, :, :nb_max_obj_in_B]\\n        bbox_squeezed = bbox[:, :, :nb_max_obj_in_B]\\n\\n        return masks_squeezed, obj_id_squeezed, bbox_squeezed\\n\\n    def final_classification(self, context_representation, object_representation):\\n        if context_representation is not None:\\n            return self.fc_classifier_object(object_representation) + self.fc_classifier_context(context_representation)\\n        else:\\n            return self.fc_classifier_object(object_representation)\\n\\n    def forward(self, x):\\n        \"\"\"Forward pass from a tensor of size (B,C,T,W,H)\"\"\"\\n        clip, masks, obj_id, bbox, max_nb_obj = x[\\'clip\\'], x[\\'mask\\'], x[\\'obj_id\\'], x[\\'obj_bbox\\'], x[\\'max_nb_obj\\']\\n\\n        # Get only the real detected objects\\n        masks, obj_id, bbox = self.squeeze_masks(max_nb_obj, masks, obj_id, bbox)\\n\\n        # Get the batch size and the temporal dimension\\n        B = clip.size(0)  # batch size\\n        T = clip.size(2)  # number of timesteps in the sequence\\n        K = masks.size(2)  # number of objects\\n\\n        # Get the two feature maps: context and object reasoning\\n        fm_context, fm_objects = self.cnn.get_two_heads_feature_maps(x, T=T, out_dim=5,\\n                                                                     heads_type=self.logits_type)\\n\\n        # Init returned variable\\n        context_representation, object_representation, preds_class_detected_objects = None, None, None\\n\\n        # HEADS\\n        if \\'object\\' in self.logits_type:\\n            object_representation, preds_class_detected_objects = self.object_head(fm_objects, masks, obj_id, B=B)\\n\\n        if \\'context\\' in self.logits_type:\\n            context_representation = self.context_head(fm_context, B=B)\\n\\n        # Final classification\\n        logits = self.final_classification(context_representation, object_representation)\\n\\n        return logits, preds_class_detected_objects\\n\\n\\ndef orn_two_heads(options, **kwargs):\\n    \"\"\"Constructs a ResNet-18 model.\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n    \"\"\"\\n\\n    # Settings\\n    depth, pooling, heads, mask_size, size_2nd_head, time = options[\\'depth\\'], \\\\\\n                                                            options[\\'pooling\\'], \\\\\\n                                                            options[\\'heads\\'], \\\\\\n                                                            14, \\\\\\n                                                            14, \\\\\\n                                                            options[\\'t\\']\\n\\n    print(\"* TWO-HEADS => Object type: {} , F: {}, Heads: {}\".format(\\'coco\\', pooling, heads))\\n\\n    # CNN\\n    cnn = resnet_two_heads(options, **kwargs)\\n\\n    # Features dim\\n    features_size = 2048 if depth > 34 else 512\\n    size_RN = 512 if depth > 34 else 256\\n\\n    # Model\\n    model = TwoHeads(\\n        cnn=cnn,\\n        features_size=features_size,\\n        size_RN=size_RN,\\n        logits_type=heads,\\n        f_orn=pooling,\\n        mask_size=28,\\n        size_2nd_head=14,\\n        time=time,\\n        **kwargs)\\n\\n    return model\\nimport torch\\nimport torch.nn as nn\\nimport random\\nfrom model.orn_two_heads.encoder import EncoderMLP\\nimport ipdb\\n\\n\\nclass ObjectRelationNetwork(nn.Module):\\n    def __init__(self, size_object, list_hidden_layers_size, relation_type=\\'pairwise-inter\\'):\\n        super(ObjectRelationNetwork, self).__init__()\\n        # Basic Settings\\n        self.size_object = size_object\\n        self.list_hidden_layers_size = list_hidden_layers_size\\n        self.relation_type = relation_type\\n\\n        self.nb_obj = 2\\n\\n        # MLP for inferring spatio-temporal relations\\n        self.mlp_inter = EncoderMLP(input_size=self.nb_obj * self.size_object,\\n                                        list_hidden_size=self.list_hidden_layers_size\\n                                        )\\n\\n    @staticmethod\\n    def create_inter_object_cat(O_1, O_2):\\n        list_input_mlp, input_mlp = [], None\\n        K = O_1.size(1)\\n        for k in range(K):\\n            O_1_k = O_2[:, k].unsqueeze(1).repeat(1, K, 1)\\n            O_1_k_input_relation = torch.cat([O_1_k, O_2], dim=2)\\n            list_input_mlp.append(O_1_k_input_relation)\\n        # Cat\\n        input_mlp = torch.cat(list_input_mlp, 1)  # (B, K^2, 2*|O|)\\n        return input_mlp\\n\\n    @staticmethod\\n    def create_triwise_interactions_input(O_1, O_2):\\n        list_input_mlp, input_mlp = [], None\\n        K = O_1.size(1)\\n        for k1 in range(K):\\n            O_1_k_1 = O_2[:, k1].unsqueeze(1).repeat(1, K, 1)\\n            list_other_k = [x for x in range(K) if x != k1]\\n            for k2 in list_other_k:\\n                O_1_k_2 = O_2[:, k2].unsqueeze(1).repeat(1, K, 1)\\n                O_1_k_input_relation = torch.cat([O_1_k_1, O_1_k_2, O_2], dim=2)\\n                list_input_mlp.append(O_1_k_input_relation)\\n        # Cat\\n        input_mlp = torch.cat(list_input_mlp, 1)  # (B, K^2, 2*|O|)\\n        return input_mlp\\n\\n    def create_input_mlp(self, O_t_1, O_t, D):\\n        K = O_t.size(1)\\n\\n        # Input\\n        input_mlp = self.create_inter_object_cat(O_t_1, O_t)\\n\\n        # Check if at least an object is involved in the input\\n        is_first_obj = torch.clamp(torch.sum(input_mlp[:, :, :D], -1), 0, 1)\\n        is_second_obj = torch.clamp(torch.sum(input_mlp[:, :, D:], -1), 0, 1)\\n        is_objects = is_first_obj * is_second_obj\\n\\n        return input_mlp, is_objects\\n\\n    def compute_O_O_interaction(self, sets_of_objects, t, previous_T, D, sampling=False):\\n\\n        # Object set (the reference one)\\n        O_t = sets_of_objects[:, t]\\n\\n        list_e_inter, list_is_object_inter = [], []\\n        for t_1 in previous_T:\\n            # Get the previous object set\\n            O_t_1 = sets_of_objects[:, t_1]\\n\\n            # Create the input to feed!\\n            input_mlp_inter, is_objects_inter = self.create_input_mlp(O_t_1, O_t, D)\\n\\n            # Infer the relations\\n            e = self.mlp_inter(input_mlp_inter)\\n\\n            # Append\\n            list_e_inter.append(e)\\n            list_is_object_inter.append(is_objects_inter)\\n\\n        if (len(list_e_inter) == 1 and self.training):\\n            # Training so only one interaction computed\\n            return list_e_inter[0], list_is_object_inter[0]\\n        else:\\n            # Stack\\n            all_e_inter = torch.stack(list_e_inter, 1)\\n            pooler = nn.AvgPool3d((all_e_inter.size(1), 1, 1))  # or nn.MaxPool3d((all_e_inter.size(1), 1, 1))\\n            all_e_inter = pooler(all_e_inter)\\n            B, _, T_prim, D = all_e_inter.size()\\n            all_e_inter = all_e_inter.view(B, T_prim, D)\\n            is_objects_inter = torch.stack(list_is_object_inter, 1)\\n            is_objects_inter = torch.clamp(torch.sum(is_objects_inter, 1), 0, 1)\\n            return all_e_inter, is_objects_inter\\n\\n    def forward(self, sets_of_objects, D, sampling=False):\\n\\n        # Number of timesteps\\n        B, T, K, _ = sets_of_objects.size()\\n\\n        list_e, list_is_obj = [], []  # list of the global interaction between two frames\\n        for t in range(1, T):\\n            # Sample during training\\n            previous_T = random.sample(range(t), 1) if self.training else list(range(t))\\n\\n            # Infer the relation between the two sets of objects\\n            e_t, is_obj = self.compute_O_O_interaction(sets_of_objects, t, previous_T, D, sampling)\\n\\n            # Append\\n            list_e.append(e_t)\\n            list_is_obj.append(is_obj)\\n\\n        # Stack\\n        all_e = torch.stack(list_e, 1)\\n        all_is_obj = torch.stack(list_is_obj, 1)\\n\\n        return all_e, all_is_obj\\nfrom __future__ import print_function\\nimport torch\\nimport torch.utils.data as data\\nfrom torchvision import transforms\\nimport os\\nimport random\\nfrom PIL import Image\\nimport numpy as np\\nimport ipdb\\nimport pickle\\nfrom pycocotools import mask as maskUtils\\nimport lintel\\nimport time\\nfrom torch.utils.data.dataloader import default_collate\\nfrom random import shuffle\\nfrom loader.videodataset import VideoDataset\\n\\n\\nclass VLOG(VideoDataset):\\n    \"\"\"\\n    Loader for the VLOG dataset\\n    \"\"\"\\n\\n    def __init__(self, options, **kwargs):\\n        super().__init__(options, **kwargs)\\n\\n        # Dict_video_label pickle\\n        self.video_label_pickle = os.path.join(self.video_dir_full, \\'dict_video_label_{}.pickle\\'.format(self.dataset))\\n\\n        # Videos paths\\n        self.list_video, self.dict_video_length, self.dict_video_label = self.get_videos()\\n\\n    def get_videos(self):\\n        # Open the pickle file\\n        with open(self.dict_video_length_fn, \\'rb\\') as file:\\n            dict_video_length = pickle.load(file)\\n\\n        # Load dict_video_label\\n        dict_video_label = self.load_or_create_dict_video_label()\\n\\n        # Intersect\\n        list_video_from_label = list(dict_video_label.keys())\\n        list_video_from_length = list(dict_video_length.keys())\\n        list_video_from_length = [v[1:].split(\\'clip\\')[0] for v in list_video_from_length]\\n        list_video = list(set(list_video_from_length) & set(list_video_from_label))\\n\\n        return list_video, dict_video_length, dict_video_label\\n\\n    def load_or_create_dict_video_label(self):\\n        if os.path.isfile(self.video_label_pickle):\\n            with open(self.video_label_pickle, \\'rb\\') as file:\\n                dict_video_label = pickle.load(file)\\n        else:\\n            # Load the label matrix\\n            label_npy_path = os.path.join(self.root, \\'meta\\', \\'hand_object\\', \\'hand_object.npy\\')\\n            matrix_label = np.load(label_npy_path)\\n\\n            # split number\\n            if self.dataset == \\'test\\':\\n                split_id = [0]\\n            elif self.dataset == \\'val\\':\\n                split_id = [3]\\n            elif self.dataset == \\'train\\':\\n                split_id = [1, 2]\\n            elif self.dataset == \\'train+val\\':\\n                split_id = [1, 2, 3]\\n            else:\\n                raise NameError\\n\\n            # Get the corresponding files\\n            manifest_file = os.path.join(self.root, \\'meta\\', \\'manifest.txt\\')\\n            split_file = os.path.join(self.root, \\'meta\\', \\'splitId.txt\\')\\n\\n            ## Read each file into a list\\n            # avi file\\n            with open(manifest_file) as f:\\n                list_video_path = f.readlines()\\n            list_video_path = [x.strip() for x in list_video_path]\\n\\n            # split\\n            with open(split_file) as f:\\n                list_split = f.readlines()\\n            list_split = [x.strip() for x in list_split]\\n            dict_video_label = {}\\n            print(\"\\\\n* Creating the dictionnary: video -> label\")\\n            for i, video in enumerate(list_video_path):\\n                if i % 20000 == 0:\\n                    print(\"{}/{} \".format(i, len(list_video_path)))\\n                # Look if it is a good file for the split\\n                if int(list_split[i]) in split_id:\\n                    dict_video_label[video] = matrix_label[i]\\n            print(\"\")\\n\\n            # Store\\n            self.store_dict_video_into_pickle(dict_video_label, self.dataset)\\n\\n        return dict_video_label\\n\\n    def starting_point(self, id):\\n        return 0\\n\\n    def get_mask_file(self, id):\\n        # Get the approriate masks\\n        mask_fn = os.path.join(self.mask_dir_full, id, \\'clip.pkl\\')\\n\\n        return mask_fn\\n\\n    def get_video_fn(self, id):\\n        # Video location\\n        video_location = os.path.join(self.video_dir_full, id, \\'clip\\' + self.video_suffix)\\n        return video_location\\n\\n    def get_length(self, id):\\n        return self.dict_video_length[\\'/{}clip\\'.format(id)]\\n\\n    def get_target(self, id):\\n        label = self.dict_video_label[id]\\n        label = np.clip(label, 0, 1)\\n        return torch.FloatTensor(label)\\nimport argparse\\nfrom loader.vlog import VLOG\\nimport time\\nimport sys\\nimport ipdb\\nimport torch\\nfrom torchvision.utils import save_image\\nfrom PIL import Image\\nimport numpy as np\\nimport os\\nfrom PIL import Image\\nimport matplotlib\\n\\nmatplotlib.use(\\'agg\\')\\nimport matplotlib.patches as patches\\nimport matplotlib.pyplot as plt\\nimport utils.vis as vis_utils\\nfrom utils.other import *\\n\\ndict_dataset = {\\'vlog\\': VLOG}\\n\\n\\ndef get_coco_names():\\n    # read the txt file\\n    fn = \\'./coco_names.txt\\'\\n    with open(fn) as f:\\n        list_coco_obj = f.readlines()\\n    list_coco_obj = [x.strip() for x in list_coco_obj]\\n\\n    # Insert background for the first obj\\n    list_coco_obj.insert(0, \\'background\\')\\n\\n    return list_coco_obj\\n\\n\\ndef main(options):\\n    # Dataset\\n    loader = VLOG if options[\\'dataset\\'] == \\'vlog\\' else None\\n\\n    # Loader\\n    videodataset = loader(options, dataset=\\'test\\', mask_size=100)\\n    print(\"Size of the dataset = {}\".format(len(videodataset)))\\n\\n    # Loop and show\\n    for i, input in enumerate(videodataset):\\n        # Get the data\\n        target = input[\\'target\\']\\n        clip = input[\\'clip\\']\\n        mask = input[\\'mask\\']\\n        obj_id = input[\\'obj_id\\']\\n        obj_bbox = input[\\'obj_bbox\\']\\n        max_nb_obj = input[\\'max_nb_obj\\']\\n        id = input[\"id\"]\\n\\n        # Video id\\n        bytes_id = id.cpu().numpy()  # it has been padded\\n        video_id = decode_videoId(bytes_id)\\n\\n        # Shape\\n        C, T, H, W = clip.shape\\n\\n        # Clip to numpy array\\n        clip_np = input[\\'clip\\'].cpu().numpy().astype(\\'uint8\\')\\n\\n        # Loop over time\\n        for t in range(T):\\n            # Image\\n            img_np = clip_np[:, t]\\n            image = img_np.transpose([1, 2, 0])\\n\\n            # Saved\\n            image_fn = \\'{}_maskRCNN_100X100_50.png\\'.format(t+1)\\n            output_dir = \\'./img\\'\\n            vis_utils.vis_one_image(\\n                image,  # BGR -> RGB for visualization # (W,H,3) np.array\\n                image_fn,  # outfile filename\\n                output_dir,  # output dir\\n                obj_id[t].cpu().numpy(),\\n                obj_bbox[t].cpu().numpy(),\\n                mask[t].cpu().numpy(),\\n                None,\\n                dataset=get_coco_names(),\\n                box_alpha=0.4,\\n                show_class=True,\\n                thresh=0.5,\\n                kp_thresh=10,\\n                show=True,\\n                W=W,\\n                H=H\\n            )\\n\\n        break\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser(description=\\'Testing the loader\\')\\n    parser.add_argument(\\'--dataset\\', metavar=\\'D\\',\\n                        default=\\'vlog\\',\\n                        help=\\'Dataset\\')\\n    parser.add_argument(\\'--root\\', metavar=\\'DIR\\',\\n                        default=\\'../data/vlog\\',\\n                        help=\\'Path to the dataset\\')\\n    parser.add_argument(\\'--t\\', default=2, type=int,\\n                        metavar=\\'H\\', help=\\'Number of timesteps to extract from a super_video\\')\\n\\n    # Args\\n    args, _ = parser.parse_known_args()\\n\\n    # Dict\\n    options = vars(args)\\n\\n    main(options)\\nfrom __future__ import print_function\\nimport torch\\nimport torch.utils.data as data\\nfrom torchvision import transforms\\nimport os\\nimport random\\nfrom PIL import Image\\nimport numpy as np\\nimport ipdb\\nimport pickle\\nfrom pycocotools import mask as maskUtils\\nimport lintel\\nimport time\\nfrom torch.utils.data.dataloader import default_collate\\nfrom random import shuffle\\nfrom  abc import abstractmethod, ABCMeta\\nimport torch.nn.functional as F\\n\\n\\nclass VideoDataset(data.Dataset):\\n    __metaclass__ = ABCMeta\\n    \"\"\"\\n    Generic loader for videos dataset\\n    \"\"\"\\n\\n    def __init__(self, options, nb_classes=30,\\n                 dataset=\\'train\\',\\n                 nb_crops=1,\\n                 #\\n                 usual_transform=False,\\n                 add_background=True,\\n                 #\\n                 video_dir=\\'videos_256x256_30\\', mask_dir=\\'masks/preds_100x100_50\\',\\n                 #\\n                 nb_obj_t_max=10, mask_confidence=0.5,\\n                 video_suffix=\\'.mp4\\',\\n                 mask_size=28,\\n                 w=224, h=224):\\n        # Settings\\n        self.root = options[\\'root\\']\\n        self.w = w\\n        self.h = h\\n        self.t = options[\\'t\\']\\n        self.video_dir = video_dir\\n        self.nb_classes = nb_classes\\n        self.usual_transform = usual_transform\\n        self.nb_obj_max_t = nb_obj_t_max\\n        self.mask_confidence = mask_confidence\\n        self.video_dir_full = os.path.join(self.root, self.video_dir)\\n        self.video_suffix = video_suffix\\n        self.mask_dir = mask_dir\\n        self.mask_dir_full = os.path.join(self.root, self.mask_dir)\\n        self.nb_crops = nb_crops\\n        self.dataset = dataset\\n        self.w_mask, self.h_mask = mask_size, mask_size\\n        self.dict_video_length_fn = os.path.join(self.video_dir_full, \\'dict_id_length.pickle\\')\\n        self.minus_len = 2\\n        self.add_background = add_background\\n        self.video_label_pickle = \\'\\'\\n        self.list_video = []\\n\\n        # Retrieve the real shape of the super_video\\n        self.retrieve_w_and_h_from_dir()\\n\\n        # Max length of a clip\\n        self.max_len_clip = 3 * self.real_fps  # sec by fps -> num of frames - 3 seconds\\n\\n        # Video and length\\n        # self.list_video, self.dict_video_length = self.get_video_and_length()\\n\\n    def store_dict_video_into_pickle(self, dict_video_label, dataset):\\n        with open(self.video_label_pickle, \\'wb\\') as file:\\n            pickle.dump(dict_video_label, file, protocol=pickle.HIGHEST_PROTOCOL)\\n            print(\"Dict_video_label of {} saved! -> {}\\\\n\".format(dataset, self.video_label_pickle))\\n\\n    def get_video_and_length(self):\\n        # Open the pickle file\\n        with open(self.dict_video_length_fn, \\'rb\\') as file:\\n            dict_video_length = pickle.load(file)\\n\\n        # Loop in each super_video dir to get th right super_video file\\n        list_video = []\\n        for video_id, length in dict_video_length.items():\\n            # Video id\\n            real_id = int(video_id.split(\\'/\\')[1])\\n            list_video.append(real_id)\\n\\n        return list_video, dict_video_length\\n\\n    def retrieve_w_and_h_from_dir(self):\\n        _, w_h, fps = self.video_dir.split(\\'_\\')\\n        w, h = w_h.split(\\'x\\')\\n        self.real_w, self.real_h, self.real_fps = int(w), int(h), int(fps)\\n        self.ratio_real_crop_w, self.ratio_real_crop_h = self.real_w / self.w, self.real_h / self.h\\n        self.real_mask_w, self.real_mask_h = int(self.ratio_real_crop_w * self.w_mask), int(\\n            self.ratio_real_crop_h * self.h_mask)\\n\\n    def time_sampling(self, video_len):\\n        # update the video_len on some dataset\\n        video_len = video_len - self.minus_len\\n\\n        # Check that the super_video is not too long\\n        diff = self.max_len_clip - video_len\\n\\n        # Change the start and adapt the length of the super_video\\n        if diff >= 0:\\n            start = 0\\n        else:\\n            start = random.sample(range(abs(diff)), 1)[0]\\n\\n        video_len_up = video_len - start\\n\\n        # Size of the sub-seq\\n        len_subseq = video_len_up / float(self.t)\\n\\n        # Sample over each bin and add the start time\\n        if self.dataset != \\'train\\' and self.nb_crops == 1:\\n            timesteps = [int((len_subseq / 2.0) + t * len_subseq + start) for t in range(self.t)]\\n        else:\\n            timesteps = [int(random.sample(range(int(len_subseq)), 1)[0] + t * len_subseq + start) for t in\\n                         range(self.t)]\\n\\n        return timesteps\\n\\n    def extract_frames(self, video_file, timesteps):\\n\\n        with open(video_file, \\'rb\\') as f:\\n            encoded_video = f.read()\\n\\n            decoded_frames = lintel.loadvid_frame_nums(encoded_video,\\n                                                       frame_nums=timesteps,\\n                                                       width=self.real_w,\\n                                                       height=self.real_h)\\n            try:\\n                np_clip = np.frombuffer(decoded_frames, dtype=np.uint8)\\n                np_clip = np.reshape(np_clip,\\n                                     newshape=(len(timesteps), self.real_h, self.real_w, 3))\\n                np_clip = np_clip.transpose([3, 0, 1, 2])\\n                np_clip = np.float32(np_clip)\\n            except Exception as e:\\n                np_clip = decoded_frames\\n                print(\"cannot decode the stream...\")\\n        return np_clip\\n\\n    @staticmethod\\n    def load_masks(file):\\n        with open(file, \\'rb\\') as f:\\n            masks = pickle.load(f, encoding=\\'latin-1\\')\\n        return (masks[\\'segms\\'], masks[\\'boxes\\'])\\n\\n    def retrieve_associated_masks(self, masks_file, video_len, timesteps, add_background_mask=True, start=0):\\n        T = len(timesteps)\\n\\n        # update the timesteps dpending on the starting point\\n        timesteps = [t + start for t in timesteps]\\n\\n        np_obj_id = np.zeros((T, self.nb_obj_max_t, 81)).astype(np.float32)\\n        np_bbox = np.zeros((T, self.nb_obj_max_t, 4)).astype(np.float32)\\n        np_masks = np.zeros((T, self.nb_obj_max_t, self.real_mask_h, self.real_mask_w)).astype(np.float32)\\n        np_max_nb_obj = np.asarray([self.nb_obj_max_t]).reshape((1,))\\n\\n        try:\\n            # raise Exception\\n            segms, boxes = self.load_masks(masks_file)\\n\\n            # Timestep factor\\n            factor = video_len / len(segms)\\n            timesteps = [int(t / factor) for t in timesteps]\\n\\n            # Retrieve information\\n            list_nb_obj = []\\n            for t_for_clip, t in enumerate(timesteps):\\n                nb_obj_t = 0\\n                # Range of objects\\n                range_objects = list(range(2, 81))\\n                shuffle(range_objects)\\n                range_objects = [1] + range_objects\\n                for c in range_objects:\\n                    for i in range(len(boxes[t][c])):\\n                        if boxes[t][c][i] is not None and len(boxes[t][c]) > 0 and boxes[t][c][i][\\n                            -1] > self.mask_confidence:\\n                            # Obj id\\n                            np_obj_id[t_for_clip, nb_obj_t, c] = 1\\n\\n                            # Bounding box\\n                            H, W = segms[t][c][i][\\'size\\']\\n                            x1, y1, x2, y2, _ = boxes[t][c][i]\\n                            x1, x2 = (x1 / W) * self.real_w, (x2 / W) * self.real_w\\n                            y1, y2 = (y1 / H) * self.real_h, (y2 / H) * self.real_h\\n                            np_bbox[t_for_clip, nb_obj_t] = [x1, y1, x2, y2]\\n\\n                            # Masks\\n                            rle_obj = segms[t][c][i]\\n                            m = maskUtils.decode(rle_obj)  # Python COCO API\\n                            # My resize\\n                            # m = resize(m, (H, W), (self.real_mask_h, self.real_mask_w), thresold=0.1\\n                            # Resize\\n                            m_pil = Image.fromarray(m)\\n                            m_pil = m_pil.resize((self.real_mask_w, self.real_mask_h))\\n                            m = np.array(m_pil, copy=False)\\n                            np_masks[t_for_clip, nb_obj_t] = m\\n\\n                            nb_obj_t += 1\\n\\n                            # Break if too much objects\\n                            if nb_obj_t > (self.nb_obj_max_t - 1):\\n                                break\\n                    # Break if too much objects\\n                    if nb_obj_t > (self.nb_obj_max_t - 1):\\n                        break\\n\\n                # Append\\n                list_nb_obj.append(nb_obj_t)\\n\\n            # And now fill numpy array\\n            np_max_nb_obj[0] = max(list_nb_obj)\\n\\n\\n        except Exception as e:\\n            print(\"mask reading problem: \", e)\\n            ipdb.set_trace()\\n            np_max_nb_obj[0] = 1.\\n\\n            # Add the background mask\\n        if add_background_mask:\\n            # Find the background pixels\\n            sum_masks = np.clip(np.sum(np_masks, 1), 0, 1)\\n            background_mask = 1 - sum_masks\\n\\n            # Add meta data about background\\n            idx_bg_mask = int(np_max_nb_obj[0])\\n            idx_bg_mask -= 1 if self.nb_obj_max_t == idx_bg_mask else 0\\n            np_masks[:, idx_bg_mask] = background_mask\\n            np_obj_id[:, idx_bg_mask, 0] = 1\\n            np_bbox[:, idx_bg_mask] = [0, 0, 1, 1]\\n\\n            # Update the number of mask\\n            np_max_nb_obj[0] = np_max_nb_obj[0] + 1 if np_max_nb_obj < self.nb_obj_max_t else np_max_nb_obj[0]\\n\\n        return (np_obj_id, np_bbox, np_masks, np_max_nb_obj)\\n\\n    def video_transform(self, np_clip, np_masks, np_bbox):\\n\\n        # Random crop\\n        _, _, h, w = np_clip.shape\\n        w_min, h_min = random.sample(range(w - self.w), 1)[0], random.sample(range(h - self.h), 1)[0]\\n        # clip\\n        np_clip = np_clip[:, :, h_min:(self.h + h_min), w_min:(self.w + w_min)]\\n        # mask\\n        h_min_mask, w_min_mask = round((h_min / self.h) * self.h_mask), round((w_min / self.w) * self.w_mask)\\n        np_masks = np_masks[:, :, h_min_mask:(self.h_mask + h_min_mask), w_min_mask:(self.w_mask + w_min_mask)]\\n        # bbox\\n        np_bbox[:, :, [0, 2]] = np.clip(np_bbox[:, :, [0, 2]] - w_min, 0, self.w)\\n        np_bbox[:, :, [1, 3]] = np.clip(np_bbox[:, :, [1, 3]] - h_min, 0, self.h)\\n        # rescale to 0->1\\n        np_bbox[:, :, [0, 2]] /= self.w\\n        np_bbox[:, :, [1, 3]] /= self.h\\n\\n        if self.usual_transform:\\n            # Div by 255\\n            np_clip /= 255.\\n\\n            # Normalization\\n            np_clip -= np.asarray([0.485, 0.456, 0.406]).reshape(3, 1, 1, 1)  # mean\\n            np_clip /= np.asarray([0.229, 0.224, 0.225]).reshape(3, 1, 1, 1)  # std\\n\\n        return np_clip, np_masks, np_bbox\\n\\n    @abstractmethod\\n    def get_mask_file(self, id):\\n        return\\n\\n    @abstractmethod\\n    def starting_point(self, id):\\n        return\\n\\n    @abstractmethod\\n    def get_video_fn(self, id):\\n        return\\n\\n    @abstractmethod\\n    def get_length(self, id):\\n        return\\n\\n    @abstractmethod\\n    def get_target(self, index):\\n        return\\n\\n    def extract_one_clip(self, id):\\n\\n        # Length\\n        length = self.get_length(id)\\n\\n        # Start of the video\\n        start = self.starting_point(id)  # 0 except for EPIC\\n\\n        # Timesteps\\n        timesteps = self.time_sampling(length)\\n\\n        return self.retrieve_clip_and_masks(id, timesteps, length, start)\\n\\n    def retrieve_clip_and_masks(self, id, timesteps, length, start):\\n        # Clip\\n        np_clip = self.extract_frames(self.get_video_fn(id), timesteps)\\n\\n        # Get the masks\\n        (np_obj_id, np_bbox, np_masks, np_max_nb_obj) = self.retrieve_associated_masks(self.get_mask_file(id),\\n                                                                                       length,\\n                                                                                       timesteps,\\n                                                                                       add_background_mask=self.add_background,\\n                                                                                       start=start)\\n\\n        # Data processing on the super_video\\n        np_clip, np_masks, np_bbox = self.video_transform(np_clip, np_masks, np_bbox)\\n\\n        return np_clip, np_masks, np_bbox, np_obj_id, np_max_nb_obj\\n\\n    def extract_multiple_clips(self, id):\\n        # Length\\n        length = self.get_length(id)\\n\\n        # Start of the video\\n        start = self.starting_point(id)  # 0 except for EPIC\\n\\n        # NB_CROPS times\\n        list_timesteps = [self.time_sampling(length) for _ in range(self.nb_crops)]\\n        timesteps_union = list(set().union(*list_timesteps))\\n        timesteps_union.sort(key=int)  # sort numerically\\n\\n        # Get the gathered data\\n        (np_clip_union, np_masks_union, np_bbox_union,\\n         np_obj_id_union, np_max_nb_obj_union) = self.retrieve_clip_and_masks(id,\\n                                                                              timesteps_union,\\n                                                                              length,\\n                                                                              start)\\n\\n        # Loop and retreieve the data per clip\\n        list_np_clip, list_np_masks, list_np_bbox, list_np_obj_id = [], [], [], []\\n        for _, timesteps in enumerate(list_timesteps):\\n            # Get the idx from the timesteps union\\n            idx_union = [timesteps_union.index(time) for time in timesteps]\\n\\n            # retrieve\\n            np_clip = np_clip_union[:, idx_union]\\n            np_masks = np_masks_union[idx_union]\\n            np_bbox = np_bbox_union[idx_union]\\n            np_obj_id = np_obj_id_union[idx_union]\\n\\n            # append\\n            list_np_clip.append(np_clip)\\n            list_np_masks.append(np_masks)\\n            list_np_bbox.append(np_bbox)\\n            list_np_obj_id.append(np_obj_id)\\n\\n        # stack\\n        np_clip = np.stack(list_np_clip)\\n        np_masks = np.stack(list_np_masks)\\n        np_obj_id = np.stack(list_np_obj_id)\\n        np_bbox = np.stack(list_np_bbox)\\n\\n        return np_clip, np_masks, np_bbox, np_obj_id, np_max_nb_obj_union\\n\\n    def __getitem__(self, index):\\n        \"\"\"\\n          Args:\\n              index (int): Index\\n          Returns:\\n              dict: info about the video\\n        \"\"\"\\n\\n        try:\\n            # Get the super_video dir\\n            id = self.list_video[index]\\n\\n            # Target\\n            torch_target = self.get_target(id)\\n\\n            # If nb_crops = 1 it is easy\\n            if self.nb_crops == 1:\\n                np_clip, np_masks, np_bbox, np_obj_id, np_max_nb_obj = self.extract_one_clip(id)\\n            else:\\n                np_clip, np_masks, np_bbox, np_obj_id, np_max_nb_obj = self.extract_multiple_clips(id)\\n\\n            # Video id\\n            np_uint8_id = np.fromstring(str(id), dtype=np.uint8)\\n            torch_id = torch.from_numpy(np_uint8_id)\\n            torch_id = F.pad(torch_id, (0, 300))[:300]\\n\\n            # Torch world\\n            torch_clip = torch.from_numpy(np_clip)\\n            torch_masks = torch.from_numpy(np_masks)\\n            torch_obj_id = torch.from_numpy(np_obj_id)\\n            torch_obj_bboxs = torch.from_numpy(np_bbox)\\n            torch_max_nb_objs = torch.from_numpy(np_max_nb_obj)\\n\\n            return {\"target\": torch_target,\\n                    \"clip\": torch_clip,\\n                    \"mask\": torch_masks,\\n                    \"obj_id\": torch_obj_id,\\n                    \"obj_bbox\": torch_obj_bboxs,\\n                    \"max_nb_obj\": torch_max_nb_objs,\\n                    \"id\": torch_id\\n                    }\\n        except Exception as e:\\n            return None\\n\\n    def __len__(self):\\n        return len(self.list_video)\\n\\n    def __repr__(self):\\n        fmt_str = \\'Dataset \\' + self.__class__.__name__ + \\'\\\\n\\'\\n        fmt_str += \\'    Number of datapoints: {}\\\\n\\'.format(self.__len__())\\n        return fmt_str\\n\\n\\ndef my_collate(batch):\\n    batch = list(filter(lambda x: x is not None, batch))\\n    return default_collate(batch)\\nimport argparse\\nimport os\\nimport subprocess\\nimport time\\nimport sys\\nimport ipdb\\nimport pickle\\nfrom utils.meter import *\\n\\n\\ndef main(args):\\n    # Parameters from the args\\n    dir, h, w, fps, suffix = args.dir, args.height, args.width, args.fps, args.suffix\\n\\n    # Video dir\\n    dir_split = dir.split(\\'/\\')\\n    video_dir = dir_split[-1]\\n    root_dir = \\'/\\'.join(dir_split[:-1])\\n    new_video_dir = \"{}_{}x{}_{}\".format(video_dir, w, h, fps)\\n    new_dir = os.path.join(root_dir, new_video_dir)\\n    os.makedirs(new_dir, exist_ok=True)\\n\\n    # load the existing dict if exist\\n    dict_video_length_fn = os.path.join(new_dir, \\'dict_id_length.pickle\\')\\n    if os.path.isfile(dict_video_length_fn):\\n        with open(dict_video_length_fn, \\'rb\\') as file:\\n            dict_video_length = pickle.load(file)\\n    else:\\n        dict_video_length = {}\\n\\n    # Get the initial video filenames\\n    list_video_fn = get_all_videos(dir, suffix)\\n    print(\"\\\\n### Initial directory: {} ###\".format(dir))\\n    print(\"=> {} videos in total\\\\n\".format(len(list_video_fn)))\\n\\n    # Loop over the super_video and extract\\n    op_time = AverageMeter()\\n    start = time.time()\\n    list_error_fn = []\\n    for i, video_fn in enumerate(list_video_fn):\\n        try:\\n            # Rescale\\n            rescale_video(video_fn, w, h, fps, dir, new_dir, suffix, dict_video_length, ffmpeg=args.ffmpeg,\\n                          crf=args.crf)\\n\\n            # Log\\n            duration = time.time() - start\\n            op_time.update(duration, 1)\\n            time_done = get_time_to_print(op_time.avg * (i + 1))\\n            time_remaining = get_time_to_print(op_time.avg * len(list_video_fn))\\n            print(\\'[{0}/{1}] : Time {batch_time.val:.3f} ({batch_time.avg:.3f}) [{done} => {remaining}]\\\\t\\'.format(\\n                    i + 1, len(list_video_fn), batch_time=op_time,\\n                    done=time_done, remaining=time_remaining))\\n            sys.stdout.flush()\\n            start = time.time()\\n        except:\\n            print(\"Impossible to rescale_videos super_video for {}\".format(video_fn))\\n            list_error_fn.append(video_fn)\\n\\n    print(\"\\\\nDone!\")\\n    print(\"\\\\nImpossible to rescale {} videos: \\\\n {}\".format(len(list_error_fn), list_error_fn))\\n\\n    # Save the dict id -> length\\n    with open(dict_video_length_fn, \\'wb\\') as file:\\n        pickle.dump(dict_video_length, file, protocol=pickle.HIGHEST_PROTOCOL)\\n        print(\"\\\\nLength of each video stored ---> {}\".format(dict_video_length_fn))\\n\\n    # Print\\n    print(\"\\\\n### You can now have access to your videos rescaled => {} ###\\\\n\".format(new_dir))\\n\\n\\ndef get_duration(file):\\n    \"\"\"Get the duration of a super_video using ffprobe. -> https://stackoverflow.com/questions/31024968/using-ffmpeg-to-obtain-super_video-durations-in-python\"\"\"\\n    cmd = \\'ffprobe -i {} -show_entries format=duration -v quiet -of csv=\"p=0\"\\'.format(file)\\n    output = subprocess.check_output(\\n        cmd,\\n        shell=True,  # Let this run in the shell\\n        stderr=subprocess.STDOUT\\n    )\\n    return float(output)\\n\\n\\ndef rescale_video(video_fn, w, h, fps, dir, new_dir, suffix, dict_video_length, ffmpeg, crf=17):\\n    \"\"\" Rescale a video according to its new width, height an fps \"\"\"\\n\\n    # Output video_name\\n    video_id = video_fn.replace(dir, \\'\\').replace(suffix, \\'\\')\\n    video_fn_rescaled = video_fn.replace(dir, new_dir)\\n    video_fn_rescaled = video_fn_rescaled.replace(suffix, suffix.lower())\\n\\n    # Create the dir\\n    video_dir_to_create = \\'/\\'.join(video_fn_rescaled.split(\\'/\\')[:-1])\\n    os.makedirs(video_dir_to_create, exist_ok=True)\\n\\n    # Check if the file already exists\\n    if os.path.isfile(video_fn_rescaled):\\n        print(\"{} already exists\".format(video_fn_rescaled))\\n    else:\\n        subprocess.call(\\n            \\'{ffmpeg} -i {video_input} -vf scale={w}:{h} -crf {crf} -r {fps} -y {video_output} -loglevel panic\\'.format(\\n                ffmpeg=ffmpeg,\\n                video_input=video_fn,\\n                h=h,\\n                w=w,\\n                fps=fps,\\n                video_output=video_fn_rescaled,\\n                crf=crf\\n            ), shell=True)\\n\\n        # Get the duration of the new super_video (in sec)\\n        duration_sec = get_duration(video_fn_rescaled)\\n        duration_frames = int(duration_sec * fps)\\n\\n        # update the dict id -> length\\n        dict_video_length[video_id] = duration_frames\\n\\n    return video_fn_rescaled\\n\\n\\ndef get_all_videos(dir, extension=\\'mp4\\'):\\n    \"\"\" Return a list of the videos filename from a directory and its subdirectories \"\"\"\\n\\n    list_video_fn = []\\n    for dirpath, dirnames, filenames in os.walk(dir):\\n        for filename in [f for f in filenames if f.endswith(extension)]:\\n            # Make sure it is not a hidden file\\n            if filename[0] != \\'.\\':\\n                fn = os.path.join(dirpath, filename)\\n                list_video_fn.append(fn)\\n\\n    return list_video_fn\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser(description=\\'Dataset preprocessing\\')\\n    parser.add_argument(\\'--dir\\', metavar=\\'DIR\\',\\n                        default=\\'../data/vlog/videos\\',\\n                        help=\\'Path to the videos dir\\')\\n    parser.add_argument(\\'--width\\', default=256, type=int,\\n                        metavar=\\'W\\', help=\\'Width of  of the output videos\\')\\n    parser.add_argument(\\'--height\\', default=256, type=int,\\n                        metavar=\\'H\\', help=\\'Height of the output videos\\')\\n    parser.add_argument(\\'--fps\\', default=30, type=int,\\n                        metavar=\\'FPS\\',\\n                        help=\\'Frames per second of the output video\\')\\n    parser.add_argument(\\'--suffix\\', metavar=\\'E\\',\\n                        default=\\'.mp4\\',\\n                        help=\\'Suffix of all the videos files - default version for the VLOG dataset\\')\\n    parser.add_argument(\\'--crf\\', default=17, type=int,\\n                        metavar=\\'CRF\\',\\n                        help=\\'Quality of the compressing - lower is better (default: 17)\\')\\n    parser.add_argument(\\'--ffmpeg\\', metavar=\\'FF\\',\\n                        default=\\'ffmpeg\\',\\n                        help=\\'Path to your ffmpeg to use (default: ffmpeg)\\')\\n\\n    args = parser.parse_args()\\n\\n    main(args)\\n# Copyright (c) 2017-present, Facebook, Inc.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n##############################################################################\\n\\n\"\"\"An awesome colormap for really neat visualizations.\"\"\"\\n\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\nfrom __future__ import unicode_literals\\n\\nimport numpy as np\\n\\n\\ndef colormap(rgb=False):\\n    color_list = np.array(\\n        [\\n            0.000, 0.447, 0.741,\\n            0.850, 0.325, 0.098,\\n            0.929, 0.694, 0.125,\\n            0.494, 0.184, 0.556,\\n            0.466, 0.674, 0.188,\\n            0.301, 0.745, 0.933,\\n            0.635, 0.078, 0.184,\\n            0.300, 0.300, 0.300,\\n            0.600, 0.600, 0.600,\\n            1.000, 0.000, 0.000,\\n            1.000, 0.500, 0.000,\\n            0.749, 0.749, 0.000,\\n            0.000, 1.000, 0.000,\\n            0.000, 0.000, 1.000,\\n            0.667, 0.000, 1.000,\\n            0.333, 0.333, 0.000,\\n            0.333, 0.667, 0.000,\\n            0.333, 1.000, 0.000,\\n            0.667, 0.333, 0.000,\\n            0.667, 0.667, 0.000,\\n            0.667, 1.000, 0.000,\\n            1.000, 0.333, 0.000,\\n            1.000, 0.667, 0.000,\\n            1.000, 1.000, 0.000,\\n            0.000, 0.333, 0.500,\\n            0.000, 0.667, 0.500,\\n            0.000, 1.000, 0.500,\\n            0.333, 0.000, 0.500,\\n            0.333, 0.333, 0.500,\\n            0.333, 0.667, 0.500,\\n            0.333, 1.000, 0.500,\\n            0.667, 0.000, 0.500,\\n            0.667, 0.333, 0.500,\\n            0.667, 0.667, 0.500,\\n            0.667, 1.000, 0.500,\\n            1.000, 0.000, 0.500,\\n            1.000, 0.333, 0.500,\\n            1.000, 0.667, 0.500,\\n            1.000, 1.000, 0.500,\\n            0.000, 0.333, 1.000,\\n            0.000, 0.667, 1.000,\\n            0.000, 1.000, 1.000,\\n            0.333, 0.000, 1.000,\\n            0.333, 0.333, 1.000,\\n            0.333, 0.667, 1.000,\\n            0.333, 1.000, 1.000,\\n            0.667, 0.000, 1.000,\\n            0.667, 0.333, 1.000,\\n            0.667, 0.667, 1.000,\\n            0.667, 1.000, 1.000,\\n            1.000, 0.000, 1.000,\\n            1.000, 0.333, 1.000,\\n            1.000, 0.667, 1.000,\\n            0.167, 0.000, 0.000,\\n            0.333, 0.000, 0.000,\\n            0.500, 0.000, 0.000,\\n            0.667, 0.000, 0.000,\\n            0.833, 0.000, 0.000,\\n            1.000, 0.000, 0.000,\\n            0.000, 0.167, 0.000,\\n            0.000, 0.333, 0.000,\\n            0.000, 0.500, 0.000,\\n            0.000, 0.667, 0.000,\\n            0.000, 0.833, 0.000,\\n            0.000, 1.000, 0.000,\\n            0.000, 0.000, 0.167,\\n            0.000, 0.000, 0.333,\\n            0.000, 0.000, 0.500,\\n            0.000, 0.000, 0.667,\\n            0.000, 0.000, 0.833,\\n            0.000, 0.000, 1.000,\\n            0.000, 0.000, 0.000,\\n            0.143, 0.143, 0.143,\\n            0.286, 0.286, 0.286,\\n            0.429, 0.429, 0.429,\\n            0.571, 0.571, 0.571,\\n            0.714, 0.714, 0.714,\\n            0.857, 0.857, 0.857,\\n            1.000, 1.000, 1.000\\n        ]\\n    ).astype(np.float32)\\n    color_list = color_list.reshape((-1, 3)) * 255\\n    if not rgb:\\n        color_list = color_list[:, ::-1]\\n    return color_list\\nimport os\\nimport math\\nimport torch\\nimport numpy as np\\nimport numbers\\nimport ipdb\\n\\n\\nclass AverageMeter(object):\\n    \"\"\"Computes and stores the average and current value\"\"\"\\n\\n    def __init__(self):\\n        self.reset()\\n\\n    def reset(self):\\n        self.val = 0\\n        self.avg = 0\\n        self.sum = 0\\n        self.count = 0\\n\\n    def update(self, val, n=1):\\n        self.val = val\\n        self.sum += val * n\\n        self.count += n\\n        self.avg = self.sum / self.count\\n\\n\\n\\n\\nclass AveragePrecisionMeter(object):\\n    \"\"\"\\n    The APMeter measures the average precision per class.\\n    The APMeter is designed to operate on `NxK` Tensors `output` and\\n    `target`, and optionally a `Nx1` Tensor weight where (1) the `output`\\n    contains model output scores for `N` examples and `K` classes that ought to\\n    be higher when the model is more convinced that the super_video should be\\n    positively labeled, and smaller when the model believes the super_video should\\n    be negatively labeled (for instance, the output of a sigmoid function); (2)\\n    the `target` contains only values 0 (for negative examples) and 1\\n    (for positive examples); and (3) the `weight` ( > 0) represents weight for\\n    each sample.\\n    \"\"\"\\n\\n    def __init__(self, difficult_examples=False, all_dataset=False):\\n        super(AveragePrecisionMeter, self).__init__()\\n        self.reset()\\n        self.difficult_examples = difficult_examples\\n        self.all_dataset = all_dataset\\n\\n    def reset(self):\\n        \"\"\"Resets the meter with empty member variables\"\"\"\\n        self.scores = torch.FloatTensor(torch.FloatStorage())\\n        self.targets = torch.LongTensor(torch.LongStorage())\\n\\n    def add(self, output, target):\\n        \"\"\"\\n        Args:\\n            output (Tensor): NxK tensor that for each of the N examples\\n                indicates the probability of the super_video belonging to each of\\n                the K classes, according to the model. The probabilities should\\n                sum to one over all classes\\n            target (Tensor): binary NxK tensort that encodes which of the K\\n                classes are associated with the N-th input\\n                    (eg: a row [0, 1, 0, 1] indicates that the super_video is\\n                         associated with classes 2 and 4)\\n            weight (optional, Tensor): Nx1 tensor representing the weight for\\n                each super_video (each weight > 0)\\n        \"\"\"\\n        if not torch.is_tensor(output):\\n            output = torch.from_numpy(output)\\n        if not torch.is_tensor(target):\\n            target = torch.from_numpy(target)\\n\\n        if output.dim() == 1:\\n            output = output.view(-1, 1)\\n        else:\\n            assert output.dim() == 2, \\\\\\n                \\'wrong output size (should be 1D or 2D with one column \\\\\\n                per class)\\'\\n        if target.dim() == 1:\\n            target = target.view(-1, 1)\\n        else:\\n            assert target.dim() == 2, \\\\\\n                \\'wrong target size (should be 1D or 2D with one column \\\\\\n                per class)\\'\\n        if self.scores.numel() > 0:\\n            assert target.size(1) == self.targets.size(1), \\\\\\n                \\'dimensions for output should match previously added examples.\\'\\n\\n        # make sure storage is of sufficient size\\n        if self.scores.storage().size() < self.scores.numel() + output.numel():\\n            new_size = math.ceil(self.scores.storage().size() * 1.5)\\n            self.scores.storage().resize_(int(new_size + output.numel()))\\n            self.targets.storage().resize_(int(new_size + output.numel()))\\n\\n        # store scores and targets\\n        offset = self.scores.size(0) if self.scores.dim() > 0 else 0\\n        self.scores.resize_(offset + output.size(0), output.size(1))\\n        self.targets.resize_(offset + target.size(0), target.size(1))\\n        self.scores.narrow(0, offset, output.size(0)).copy_(output)\\n        self.targets.narrow(0, offset, target.size(0)).copy_(target)\\n\\n        # Idx of correct preds\\n        B, C = target.size()\\n        list_idx_correct_preds = []\\n        for idx in range(B):\\n            correct_preds = True\\n            for j in range(C):\\n                # does not have the same sign so bad preds -> break\\n                target_idx_j = -1 if target[idx, j] == 0 else 1\\n                if target_idx_j * output[idx, j] < 0:\\n                    correct_preds = False\\n                    break\\n            # good preds! if finished the loop\\n            if correct_preds:\\n                list_idx_correct_preds.append(idx)\\n\\n        return list_idx_correct_preds\\n\\n    def value(self):\\n        \"\"\"Returns the model\\'s average precision for each class\\n        Return:\\n            ap (FloatTensor): 1xK tensor, with avg precision for each class k\\n        \"\"\"\\n\\n        if self.scores.numel() == 0:\\n            return 0\\n        ap = torch.zeros(self.scores.size(1))\\n        rg = torch.arange(1, self.scores.size(0)).float()\\n\\n        # compute average precision for each class\\n        for k in range(self.scores.size(1)):\\n            # sort scores\\n            scores = self.scores[:, k]\\n            targets = self.targets[:, k]\\n\\n            # compute average precision\\n            if self.all_dataset:\\n                scores_to_keep, targets_to_keep = scores, targets\\n            else:\\n                scores_to_keep, targets_to_keep = scores[-100:], targets[-100:]\\n            ap[k] = AveragePrecisionMeter.average_precision(scores_to_keep, targets_to_keep,\\n                                                            self.difficult_examples) * 100.\\n            # ap[k] = AveragePrecisionMeter.compute_ap(scores, targets)\\n\\n        return ap.mean(), ap.mean(), ap\\n\\n    @staticmethod\\n    def compute_ap(scores, targets):\\n        # import ipdb\\n        # ipdb.set_trace()\\n        _, sortind = torch.sort(scores, 0, True)\\n        truth = targets[sortind]\\n        rg = torch.range(1, scores.size(0)).float()\\n        tp = truth.float().cumsum(0)\\n\\n        # compute precision curve\\n        precision = tp.div(rg)\\n\\n        # compute average precision\\n        ap = precision[truth.byte()].sum() / max(truth.sum(), 1)\\n\\n        return ap\\n\\n    # not working\\n    @staticmethod\\n    def average_precision(output, target, difficult_examples=True):\\n        # sort examples\\n        sorted, indices = torch.sort(output, dim=0, descending=True)\\n\\n        # Computes prec@i\\n        pos_count = 0.\\n        total_count = 0.\\n        precision_at_i = 0.\\n        for i in indices:\\n            label = target[i]\\n            if difficult_examples and label == 0:\\n                continue\\n            if label == 1:\\n                pos_count += 1\\n            total_count += 1\\n            if label == 1:\\n                precision_at_i += pos_count / total_count\\n        precision_at_i /= pos_count + 1e-5\\n        return precision_at_i\\n\\ndef get_time_to_print(time_sec):\\n    hours = math.trunc(time_sec / 3600)\\n    time_sec = time_sec - hours * 3600\\n    mins = math.trunc(time_sec / 60)\\n    time_sec = time_sec - mins * 60\\n    secs = math.trunc(time_sec % 60)\\n    string = \\'%02d:%02d:%02d\\' % (hours, mins, secs)\\n    return string# Copyright (c) 2017-present, Facebook, Inc.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n##############################################################################\\n\\n\"\"\"Detection output visualization module.\"\"\"\\n\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\nfrom __future__ import unicode_literals\\n\\nfrom PIL import Image\\nimport cv2\\nimport numpy as np\\nimport os\\nimport pycocotools.mask as mask_util\\n\\nfrom utils.colormap import colormap\\n# import utils.keypoints as keypoint_utils\\n\\n# Matplotlib requires certain adjustments in some environments\\n# Must happen before importing matplotlib\\nimport matplotlib\\n\\nmatplotlib.use(\\'Agg\\')  # Use a non-interactive backend\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.patches import Polygon\\nimport ipdb\\n\\nplt.rcParams[\\'pdf.fonttype\\'] = 42  # For editing in Adobe Illustrator\\n\\n_GRAY = (218, 227, 218)\\n_GREEN = (18, 127, 15)\\n_WHITE = (255, 255, 255)\\n\\n\\ndef get_class_string(class_index, score, dataset):\\n    class_text = dataset[class_index] if dataset is not None else \\\\\\n        \\'id{:d}\\'.format(class_index)\\n    return class_text #+ \\' {:0.2f}\\'.format(score).lstrip(\\'0\\')\\n\\n\\ndef vis_bbox(img, bbox, thick=1):\\n    \"\"\"Visualizes a bounding box.\"\"\"\\n    (x0, y0, w, h) = bbox\\n    x1, y1 = int(x0 + w), int(y0 + h)\\n    x0, y0 = int(x0), int(y0)\\n    cv2.rectangle(img, (x0, y0), (x1, y1), _GREEN, thickness=thick)\\n    return img\\n\\ndef vis_one_image(\\n        im, im_name, output_dir, classes, boxes, masks=None, keypoints=None, thresh=0.9,\\n        kp_thresh=2, dpi=200, box_alpha=0.0, dataset=None, show_class=False,\\n        ext=\\'pdf\\', show=False, W=224, H=224):\\n    \"\"\"Visual debugging of detections.\"\"\"\\n    if not os.path.exists(output_dir):\\n        os.makedirs(output_dir)\\n\\n    color_list = colormap(rgb=True) / 255\\n\\n    fig = plt.figure(frameon=False)\\n    fig.set_size_inches(im.shape[1] / dpi, im.shape[0] / dpi)\\n    ax = plt.Axes(fig, [0., 0., 1., 1.])\\n    ax.axis(\\'off\\')\\n    fig.add_axes(ax)\\n    ax.imshow(im)\\n\\n    # Display in largest to smallest order to reduce occlusion\\n    boxes[:,0] *= W\\n    boxes[:, 2] *= W\\n    boxes[:,1] *= H\\n    boxes[:, 3] *= H\\n    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\\n    sorted_inds = np.argsort(-areas)\\n\\n    # torch to numpy\\n    # ipdb.set_trace()\\n    if masks is not None:\\n        # uint8\\n        masks = masks.astype(\\'uint8\\')\\n        # rescale\\n        w_masks, h_masks, _ = masks.shape\\n\\n    mask_color_id = 0\\n    # ipdb.set_trace()\\n    for i in sorted_inds:\\n        bbox = boxes[i, :4]\\n        score = boxes[i, -1]\\n        if score < thresh:\\n            continue\\n\\n        # show box (off by default)\\n        ax.add_patch(\\n            plt.Rectangle((bbox[0], bbox[1]),\\n                          bbox[2] - bbox[0],\\n                          bbox[3] - bbox[1],\\n                          fill=False, edgecolor=\\'g\\',\\n                          linewidth=1, alpha=box_alpha))\\n\\n        if show_class:\\n            # (x, y) = (bbox[0], bbox[3] + 2) if classes[i] == 1 else (bbox[3], bbox[1] - 2)  # below for person or above for the rest\\n            x, y = (bbox[0], bbox[1] - 2)\\n            classes_i = np.argmax(classes[i])\\n            # print(get_class_string(classes_i, score, dataset), classes_i, score)\\n            ax.text(\\n                x, y,\\n                get_class_string(classes_i, score, dataset),\\n                fontsize=4,\\n                family=\\'serif\\',\\n                bbox=dict(\\n                    facecolor=\\'g\\', alpha=0.4, pad=0, edgecolor=\\'none\\'),\\n                color=\\'white\\')\\n\\n        # show mask\\n        if masks is not None:\\n            # ipdb.set_trace()\\n            img = np.ones(im.shape)\\n            color_mask = color_list[mask_color_id % len(color_list), 0:3]\\n            mask_color_id += 1\\n\\n            w_ratio = .4\\n            for c in range(3):\\n                color_mask[c] = color_mask[c] * (1 - w_ratio) + w_ratio\\n            for c in range(3):\\n                img[:, :, c] = color_mask[c]\\n            e_down = masks[i, :, :]\\n\\n            # Rescale mask\\n            e_pil = Image.fromarray(e_down)\\n            e_pil_up = e_pil.resize((H, W),Image.ANTIALIAS)\\n            e = np.array(e_pil_up)\\n\\n            _, contour, hier = cv2.findContours(\\n                e.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\\n\\n            for c in contour:\\n                polygon = Polygon(\\n                    c.reshape((-1, 2)),\\n                    fill=True, facecolor=color_mask,\\n                    edgecolor=\\'w\\', linewidth=1.2,\\n                    alpha=0.5)\\n                ax.add_patch(polygon)\\n\\n    output_name = os.path.basename(im_name) + \\'.\\' + ext\\n    fig.savefig(os.path.join(output_dir, \\'{}\\'.format(output_name)), dpi=dpi)\\n    print(\\'result saved to {}\\'.format(os.path.join(output_dir, \\'{}\\'.format(output_name))))\\n    if show:\\n        plt.show()\\n    plt.close(\\'all\\')\\nimport pickle\\nfrom loader.vlog import VLOG\\nimport torch\\nfrom loader.videodataset import my_collate\\nfrom torch.nn import Module\\nimport torch.nn as nn\\nimport ipdb\\nimport torch\\nfrom utils.meter import *\\nimport shutil\\nimport math\\n\\n\\ndef load_pickle(file):\\n    with open(file, mode=\\'rb\\') as f:\\n        df = pickle.load(f, encoding=\\'latin1\\')\\n    return df\\n\\n\\ndef get_datasets_and_dataloaders(options, cuda=False):\\n    # Choice of Dataset\\n    if options[\\'dataset\\'] == \\'vlog\\':\\n        VideoDataset = VLOG\\n        if options[\\'train_set\\'] == \\'train\\':\\n            train_set_name = \\'train\\'\\n            val_set_name = \\'val\\'\\n            nb_crops = 1\\n        elif options[\\'train_set\\'] == \\'train+val\\':\\n            train_set_name = \\'train+val\\'\\n            val_set_name = \\'test\\'\\n            nb_crops = options[\\'nb_crops\\']\\n        else:\\n            raise NameError\\n    else:\\n        raise NameError\\n\\n    # Dataset\\n    train_dataset = VideoDataset(options,\\n                                 dataset=train_set_name,\\n                                 nb_crops=1,\\n                                 usual_transform=True,\\n                                 add_background=options[\\'add_background\\'])\\n    val_dataset = VideoDataset(options,\\n                               dataset=val_set_name,\\n                               nb_crops=nb_crops,\\n                               usual_transform=True,\\n                               add_background=options[\\'add_background\\'])\\n\\n    # Dataloader\\n    train_loader = torch.utils.data.DataLoader(train_dataset,\\n                                               batch_size=options[\\'batch_size\\'],\\n                                               shuffle=True,\\n                                               num_workers=options[\\'workers\\'],\\n                                               pin_memory=cuda,\\n                                               collate_fn=my_collate)\\n    val_loader = torch.utils.data.DataLoader(val_dataset,\\n                                             batch_size=options[\\'batch_size\\'],\\n                                             shuffle=False,\\n                                             num_workers=options[\\'workers\\'],\\n                                             pin_memory=cuda,\\n                                             collate_fn=my_collate)\\n\\n    return train_dataset, val_dataset, train_loader, val_loader\\n\\n\\ndef get_loss_and_metric(options):\\n    if options[\\'dataset\\'] == \\'vlog\\':\\n        # Metric\\n        metric = AveragePrecisionMeter\\n        # Loss\\n        loss = CriterionLinearCombination([\\'bce\\', \\'ce\\'], [15.0, 1.0])\\n    else:\\n        raise NameError\\n\\n    return loss, metric\\n\\n\\nclass CriterionLinearCombination(Module):\\n    def __init__(self, list_criterion_names, list_weights):\\n        super(CriterionLinearCombination, self).__init__()\\n        assert len(list_criterion_names) == len(list_weights)\\n\\n        self.list_criterion, self.list_weights = [], []\\n        for i, criterion_name in enumerate(list_criterion_names):\\n            # Criterion\\n            if criterion_name == \\'bce\\':\\n                self.list_criterion.append(nn.BCEWithLogitsLoss())\\n            elif criterion_name == \\'ce\\':\\n                self.list_criterion.append(nn.CrossEntropyLoss())\\n            else:\\n                raise Exception\\n            # Weight\\n            self.list_weights.append(list_weights[i])\\n\\n    def forward(self, list_input, list_target, cuda=False):\\n        assert len(list_input) == len(list_target)\\n        # ipdb.set_trace()\\n        loss = 0.0\\n        for i in range(len(self.list_criterion)):\\n            # Cast depending of the criterion\\n            criterion_i, weight_i = self.list_criterion[i], self.list_weights[i]\\n            target_i, input_i = list_target[i], list_input[i]\\n            if input_i is not None:\\n                if isinstance(criterion_i, nn.CrossEntropyLoss):\\n                    target_i = target_i.type(torch.LongTensor)\\n                elif isinstance(criterion_i, nn.BCEWithLogitsLoss):\\n                    target_i = target_i.type(torch.FloatTensor)\\n\\n                target_i = target_i.cuda() if cuda else target_i\\n\\n                # Compute the loss and add\\n                input_i = input_i.view(-1, input_i.size(-1))\\n                loss_i = weight_i * criterion_i(input_i, target_i)\\n                loss = loss + loss_i\\n\\n        return loss\\n\\n\\ndef load_from_dir(model, optimizer, options):\\n    \\'\\'\\' load from resume found in the dir\\'\\'\\'\\n    epoch = 0\\n    if options[\\'resume\\']:\\n        if os.path.isdir(options[\\'resume\\']):\\n            ckpt_resume = os.path.join(options[\\'resume\\'], \\'model_best.pth.tar\\')\\n            if os.path.isfile(ckpt_resume):\\n                print(\"\\\\n=> loading checkpoint \\'{}\\'\".format(ckpt_resume))\\n                checkpoint = torch.load(ckpt_resume, map_location=lambda storage, loc: storage)\\n                epoch = checkpoint[\\'epoch\\']\\n                # Remove the fc_classifier\\'s\\n                updated_params = {}\\n                model_dict = model.state_dict()\\n                for k, v in checkpoint[\\'state_dict\\'].items():\\n                    # Train classifier fom scratch\\n                    if \"fc_classifier\" in k and not options[\\'evaluate\\']:\\n                        pass\\n                    # Look if the size if the same\\n                    if k in list(model_dict.keys()):\\n                        v_new_size, v_old_size = v.size(), model_dict[k].size()\\n                        if v_old_size == v_new_size:\\n                            updated_params[k] = v\\n\\n                # Load\\n                new_params = model.state_dict()\\n                new_params.update(updated_params)\\n                model.load_state_dict(new_params)\\n\\n                # Optim\\n                updated_params = {}\\n                new_params = optimizer.state_dict()\\n                for k, v in checkpoint[\\'state_dict\\'].items():\\n                    if k not in list(new_params.keys()):\\n                        updated_params[k] = v\\n\\n                new_params.update(updated_params)\\n                optimizer.load_state_dict(new_params)\\n\\n                # Epoch\\n                print(\"=> loaded checkpoint \\'{}\\' (epoch {})\"\\n                      .format(ckpt_resume, checkpoint[\\'epoch\\']))\\n            else:\\n                print(\"\\\\n=> no checkpoint found at \\'{}\\'\".format(options[\\'resume\\']))\\n        else:\\n            os.makedirs(options[\\'resume\\'])\\n\\n    return model, optimizer, epoch\\n\\n\\ndef print_number(number):\\n    \"\"\" print a \\' every 3 number starting from the left (e.g 23999 -> 23\\'999)\"\"\"\\n    len_3 = round(len(str(number)) / 3.)\\n\\n    j = 0\\n    number = str(number)\\n    for i in range(1, len_3 + 1):\\n        k = i * 3 + j\\n        number = number[:-k] + \\'\\\\\\'\\' + number[-k:]\\n        j += 1\\n\\n    # remove \\' if it is at the begining\\n    if number[0] == \\'\\\\\\'\\':\\n        return number[1:]\\n    else:\\n        return number\\n\\n\\ndef write_to_log(dataset, resume, epoch, metrics, metrics_per_class):\\n    # Global metric\\n    file_full_name = os.path.join(resume, dataset + \\'_log\\')\\n    with open(file_full_name, \\'a+\\') as f:\\n        f.write(\\'Epoch=%03d, Loss=%.4f, Metric=%.4f\\\\n\\' % (epoch, metrics[0], metrics[1]))\\n\\n    # Per class metric\\n    if metrics_per_class is not None:\\n        file_full_name = os.path.join(resume, dataset + \\'_per_class_metrics_log\\')\\n        np.savetxt(file_full_name, metrics_per_class.numpy(), fmt=\\'%10.4f\\', delimiter=\\',\\')\\n\\n\\ndef transform_input(x, dim, T=8):\\n    diff = len(x.size()) - dim\\n\\n    if diff > 0:\\n        B, C, T, W, H = x.size()\\n        x = x.transpose(1, 2)\\n        x = x.contiguous()\\n        x = x.view(-1, C, W, H)\\n    elif diff < 0:\\n        _, C, W, H = x.size()\\n        x = x.view(-1, T, C, W, H)\\n        x = x.transpose(1, 2)\\n\\n    return x\\n\\n\\ndef count_nb_params(enum_params):\\n    nb_params = 0\\n    for parameter in enum_params:\\n        nb_param_w = 1\\n        for s in parameter.size():\\n            nb_param_w *= s\\n        nb_params += nb_param_w\\n    return nb_params\\n\\n\\ndef save_checkpoint(state, is_best, resume, filename=\\'checkpoint.pth.tar\\'):\\n    full_filename = os.path.join(resume, filename)\\n    torch.save(state, full_filename)\\n    if is_best:\\n        full_filename_best = os.path.join(resume, \\'model_best.pth.tar\\')\\n        shutil.copyfile(full_filename, full_filename_best)\\n\\n\\ndef decode_videoId(bytes_id):\\n    try:\\n        idx_1st_0 = (bytes_id == 0).argmax(axis=0)  # find zero padding\\n    except:\\n        idx_1st_0 = (bytes_id == 0).argmax()  # find zero padding\\n\\n    str_video_id = bytes_id[:idx_1st_0].tobytes().decode(\"utf-8\")  # remove zero padding\\n    return str_video_id\\n\\n\\ndef store_preds(preds, id, list_correct_preds, obj_id, dataset=\\'vlog\\'):\\n    # sigmoid or softmax\\n    # if dataset == \\'vlog\\':\\n    #     f = sigmoid\\n    # else:\\n    #     raise NameError\\n\\n    # cpu - np\\n    id_np = id.cpu().numpy()\\n    preds_cpu = np.round(preds.cpu().numpy().astype(np.float16), 2)\\n    obj_id_cpu = obj_id.cpu().numpy().astype(np.float16)\\n\\n    # Lop\\n    dict_good, dict_failure, dict_obj = {}, {}, {}\\n    for i, p in enumerate(preds_cpu):\\n        # catch id\\n        id_i = decode_videoId(id_np[i])\\n\\n        # obj id\\n        # ipdb.set_trace()\\n        dict_obj[id_i] = np.round(obj_id_cpu[i].sum(0).sum(0), 2)\\n\\n        # good or failure\\n        if i in list_correct_preds:\\n            dict_good[id_i] = p\\n        else:\\n            dict_failure[id_i] = p\\n\\n    return dict_good, dict_failure, dict_obj\\n\\n\\ndef sigmoid(x):\\n    return 1 / (1 + math.exp(-x))\\nfrom model import models\\nimport argparse\\nimport os\\nimport shutil\\nimport time\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.parallel\\nimport torch.optim\\nimport torch.utils.data\\nimport torch.utils.data.distributed\\nfrom utils.meter import *\\nfrom inference.train_val import *\\nimport ipdb\\nfrom model import models\\nfrom utils.other import *\\n\\n\\ndef main(options):\\n    # CUDA\\n    cuda = torch.cuda.is_available()\\n\\n    # Dataset\\n    train_dataset, val_dataset, train_loader, val_loader = get_datasets_and_dataloaders(options, cuda=cuda)\\n    print(\\'\\\\n*** Train set of size {}  -  Val set of size {} ***\\\\n\\'.format(print_number(len(train_dataset)),\\n                                                                           print_number(len(val_dataset))))\\n\\n    # Model\\n    model = models.__dict__[options[\\'arch\\']](num_classes=train_dataset.nb_classes,\\n                                             size_fm_2nd_head=train_dataset.h_mask,\\n                                             options=options)\\n    model = model.cuda() if cuda else model\\n    model = torch.nn.DataParallel(model)\\n\\n    # Trainable params\\n    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\\n\\n    # Print number of parameters\\n    nb_total_params = count_nb_params(model.parameters())\\n    nb_trainable_params = count_nb_params(filter(lambda p: p.requires_grad, model.parameters()))\\n    ratio = float(nb_trainable_params / nb_total_params) * 100.\\n    print(\"\\\\n* Parameter numbers : {} ({}) - {ratio:.2f}% of the weights are trainable\".format(\\n        print_number(nb_total_params),\\n        print_number(nb_trainable_params),\\n        ratio=ratio\\n    ))\\n\\n    # Optimizer\\n    optimizer = torch.optim.Adam(trainable_params, options[\\'lr\\'])\\n\\n    # Loss function and Metric\\n    criterion, metric = get_loss_and_metric(options)\\n\\n    # Load resume from resume if exists\\n    model, optimizer, epoch = load_from_dir(model, optimizer, options)\\n\\n    # My engine\\n    engine = {\\'model\\': model,\\n              \\'optimizer\\': optimizer, \\'criterion\\': criterion, \\'metric\\': metric,\\n              \\'train_loader\\': train_loader, \\'val_loader\\': val_loader}\\n\\n    # Training/Val or Testing #\\n    if options[\\'evaluate\\']:\\n        # Val\\n        loss_val, metric_val, per_class_metric_val, df_good, df_failure, df_objects = validate(epoch, engine, options, cuda=cuda)\\n        # Write into log\\n        write_to_log(val_dataset.dataset, options[\\'resume\\'], epoch, [loss_val, metric_val], per_class_metric_val)\\n        # Save good and failures and object presence\\n        df_good.to_csv(os.path.join(options[\\'resume\\'], \\'df_good_preds.csv\\'), sep=\\',\\', encoding=\\'utf-8\\')\\n        df_failure.to_csv(os.path.join(options[\\'resume\\'], \\'df_failure_preds.csv\\'), sep=\\',\\', encoding=\\'utf-8\\')\\n        df_objects.to_csv(os.path.join(options[\\'resume\\'], \\'df_objects\\'), sep=\\',\\', encoding=\\'utf-8\\')\\n\\n    else:\\n        # Train (and Val if having access tto the val set)\\n        is_best = True\\n        best_metric_val = -0.1\\n        for epoch in range(1, options[\\'epochs\\'] + 1):\\n            # train one epoch\\n            loss_train, metric_train = train(epoch, engine, options, cuda=cuda)\\n\\n            # write into log\\n            write_to_log(train_dataset.dataset, options[\\'resume\\'], epoch, [loss_train, loss_train], None)\\n\\n            # get the val metric\\n            if options[\\'train_set\\'] == \\'train\\':\\n                # Val\\n                loss_val, metric_val, per_class_metric_val, *_ = validate(epoch, engine, options, cuda=cuda)\\n                # Write into log\\n                write_to_log(val_dataset.dataset, options[\\'resume\\'], epoch, [loss_val, metric_val],\\n                             per_class_metric_val)\\n\\n                # Best compared to previous checkpoint ?\\n                is_best = metric_val > best_metric_val\\n                best_metric_val = max(metric_val, best_metric_val)\\n\\n            # save checkpoint\\n            save_checkpoint({\\n                \\'epoch\\': epoch,\\n                \\'arch\\': options[\\'arch\\'],\\n                \\'state_dict\\': model.state_dict(),\\n                \\'best_metric_val\\': best_metric_val,\\n                \\'optimizer\\': optimizer.state_dict(),\\n            }, is_best, options[\\'resume\\'])\\n\\n    return None\\nfrom utils.meter import *\\nimport time\\nimport torch\\nimport ipdb\\nimport sys\\nimport torch.nn as nn\\nfrom utils.meter import *\\nimport matplotlib\\nfrom utils.other import *\\nimport pandas\\n\\n\\ndef make_variable_all_input(dict_input, cuda=False):\\n    dict_input_var = {}\\n    for k, v in dict_input.items():\\n        var = torch.autograd.Variable(v)\\n        dict_input_var[k] = var.cuda() if cuda else var\\n    return dict_input_var\\n\\n\\ndef get_obj_id_for_loss(input_var, is_Variable=True, j=0):\\n    obj_id = input_var[\\'obj_id\\']\\n    if is_Variable:\\n        nb_max_obj = int(torch.max(input_var[\\'max_nb_obj\\']).cpu())\\n    else:\\n        nb_max_obj = int(torch.max(input_var[\\'max_nb_obj\\'].cpu()))\\n\\n    # Catch the useful obj id\\n    obj_id_size = len(obj_id.size())\\n    obj_id = obj_id[:, :, :nb_max_obj] if obj_id_size == 4 else obj_id[:, :, :, :nb_max_obj]\\n    obj_id = torch.max(obj_id, -1)[1]\\n    if obj_id_size == 4:\\n        obj_id = obj_id.view(-1)\\n    else:\\n        obj_id = obj_id[:, j].contiguous()\\n        obj_id = obj_id.view(-1)\\n\\n    return obj_id\\n\\n\\ndef forward_backward(model, input_var, criterion, optimizer=None, cuda=False,\\n                     j=1):\\n    # compute output\\n    output = model(input_var)\\n\\n    # retrieve object id\\n    obj_id = get_obj_id_for_loss(input_var, j=j)\\n\\n    # update output\\n    output = output if isinstance(output, tuple) else (output, None)\\n\\n    # compute loss\\n    loss = criterion(output, [input_var[\\'target\\'], obj_id], cuda)\\n\\n    # backward\\n    if optimizer is not None:\\n        # compute gradient and do SGD step\\n        optimizer.zero_grad()\\n        loss.backward()\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)  # clip grad\\n        optimizer.step()\\n\\n    return output[0], loss  # return the output for the action only - objects preds are just regularizer\\n\\n\\ndef update_metric_loss(input, output, metric, loss, losses):\\n    # loss\\n    losses.update(loss.detach(), input[\\'clip\\'].size(0))\\n\\n    # metrics\\n    target = input[\\'target\\']\\n    target = target.cpu()\\n    preds = output.view(-1, output.size(-1)).data.cpu()\\n    list_idx_correct_preds = metric.add(preds, target)\\n    metric_val, metric_avg, _ = metric.value()\\n\\n    return metric_val, metric_avg, list_idx_correct_preds\\n\\n\\ndef take_clip_j(input_var, j):\\n    input_var_j = {}\\n    for k, v in input_var.items():\\n        if k == \\'video_id\\':\\n            pass\\n        elif k == \\'target\\':\\n            input_var_j[\\'target\\'] = input_var[\\'target\\']\\n        elif k == \\'id\\':\\n            input_var_j[\\'id\\'] = input_var[\\'id\\']\\n        elif k == \\'max_nb_obj\\':\\n            input_var_j[\\'max_nb_obj\\'] = input_var[\\'max_nb_obj\\']\\n        else:\\n            input_var_j[k] = input_var[k][:, j]\\n    return input_var_j\\n\\n\\ndef train(epoch, engine, options, cuda=False):\\n    # Timer\\n    batch_time = AverageMeter()\\n    data_time = AverageMeter()\\n    losses = AverageMeter()\\n\\n    # Engine\\n    model = engine[\\'model\\']\\n    optimizer = engine[\\'optimizer\\']\\n    criterion = engine[\\'criterion\\']\\n    metric = engine[\\'metric\\']()\\n    data_loader = engine[\\'train_loader\\']\\n\\n    # switch to train mode\\n    model.train()\\n\\n    end = time.time()\\n    print(\"\")\\n    for i, input in enumerate(data_loader):\\n        # measure data loading time\\n        data_time.update(time.time() - end)\\n\\n        # Make Variables\\n        input_var = make_variable_all_input(input, cuda=cuda)\\n\\n        # compute output\\n        output, loss = forward_backward(model, input_var, criterion, optimizer, cuda)\\n\\n        # measure elapsed time\\n        batch_time.update(time.time() - end)\\n        end = time.time()\\n\\n        if i % options[\\'print_freq\\'] == 0:\\n            # Do no waste time at computing loss and metric at each iteration of the training process\\n            metric_val, metric_avg, *_ = update_metric_loss(input, output, metric, loss, losses)\\n\\n            time_done = get_time_to_print(batch_time.avg * (i + 1))\\n            time_remaining = get_time_to_print(batch_time.avg * len(data_loader))\\n            print(\\'Epoch: [{0}][{1}/{2}]\\\\t\\'\\n                  \\'Time {batch_time.val:.3f} ({batch_time.avg:.3f}) [{done} => {remaining}]\\\\t\\'\\n                  \\'Data {data_time.val:.3f} ({data_time.avg:.3f})\\\\t\\'\\n                  \\'Loss {loss.val:.4f} ({loss.avg:.4f})\\\\t\\'\\n                  \\'Metric {metric_val:.3f} ({metric_avg:.3f})\\'.format(\\n                epoch, i + 1, len(data_loader), batch_time=batch_time,\\n                data_time=data_time, loss=losses, metric_val=metric_val,\\n                metric_avg=metric_avg,\\n                done=time_done, remaining=time_remaining))\\n            sys.stdout.flush()\\n\\n    return losses.avg, metric_avg\\n\\n\\ndef validate(epoch, engine, options, cuda=False):\\n    # Timer\\n    batch_time = AverageMeter()\\n    losses = AverageMeter()\\n    data_time = AverageMeter()\\n\\n    # Engine\\n    model = engine[\\'model\\']\\n    criterion = engine[\\'criterion\\']\\n    metric = engine[\\'metric\\']()\\n    data_loader = engine[\\'val_loader\\']\\n\\n    # switch to evaluate mode\\n    model.eval()\\n\\n    # create the numpy array for storing the preds and actual target\\n    dict_id_good_preds, dict_id_failures_preds, dict_id_object = {}, {}, {}\\n\\n    end = time.time()\\n    nb_crops = data_loader.dataset.nb_crops\\n    print(\"\")\\n    with torch.no_grad():\\n        for i, input in enumerate(data_loader):\\n            # measure data loading time\\n            data_time.update(time.time() - end)\\n\\n            # Make Variables\\n            input_var = make_variable_all_input(input, cuda=cuda)\\n\\n            output_aggreg, loss_aggreg, obj_id_aggreg = None, None, None\\n            for j in range(nb_crops):\\n                # take the right clip\\n                input_var_j = input_var if nb_crops == 1 else take_clip_j(input_var, j)\\n                obj_id = input_var_j[\\'obj_id\\']\\n\\n                # compute output\\n                output, loss = forward_backward(model, input_var_j, criterion, None, cuda)\\n                # ipdb.set_trace()\\n\\n                # aggreg by summing\\n                if output_aggreg is None:\\n                    output_aggreg = output\\n                    loss_aggreg = loss\\n                    obj_id_aggreg = obj_id\\n                else:\\n                    output_aggreg += output\\n                    loss_aggreg += loss\\n                    obj_id_aggreg += obj_id\\n\\n            # measure accuracy and record loss\\n            output_aggreg /= nb_crops\\n            loss_aggreg /= nb_crops\\n            obj_id_aggreg /= nb_crops\\n            metric_val, metric_avg, list_idx_correct_preds = update_metric_loss(input, output_aggreg, metric,\\n                                                                                loss_aggreg, losses)\\n\\n            # store good and failure cases and detected object\\n            dict_id_good_i, dict_id_failures_i, dict_obj_i = store_preds(\\n                output_aggreg, input[\\'id\\'], list_idx_correct_preds,\\n                obj_id_aggreg,\\n                options[\\'dataset\\'])\\n            dict_id_good_preds.update(dict_id_good_i)\\n            dict_id_failures_preds.update(dict_id_failures_i)\\n            dict_id_object.update(dict_obj_i)\\n\\n\\n            # measure elapsed time\\n            batch_time.update(time.time() - end)\\n            end = time.time()\\n\\n            if (i + 1) % options[\\'print_freq\\'] == 0:\\n                time_done = get_time_to_print(batch_time.avg * (i + 1))\\n                time_remaining = get_time_to_print(batch_time.avg * len(data_loader))\\n                print(\\'Test: [{0}/{1}]\\\\t\\'\\n                      \\'Time {batch_time.val:.3f} ({batch_time.avg:.3f}) [{done} => {remaining}]\\\\t\\'\\n                      \\'Data {data_time.val:.3f} ({data_time.avg:.3f})\\\\t\\'\\n                      \\'Loss {loss.val:.4f} ({loss.avg:.4f})\\\\t\\'\\n                      \\'Metric {metric_val:.3f} ({metric_avg:.3f})\\'.format(\\n                    i + 1, len(data_loader), batch_time=batch_time,\\n                    data_time=data_time, loss=losses, metric_val=metric_val,\\n                    metric_avg=metric_avg,\\n                    done=time_done, remaining=time_remaining))\\n                sys.stdout.flush()\\n\\n    # Finally compute the true mean metric over the overall val set\\n    metric.all_dataset = True\\n    _, metric_avg, per_class_metric_avg = metric.value()\\n\\n    print(\\' * Metric {metric_avg:.3f}\\'.format(metric_avg=metric_avg))\\n    sys.stdout.flush()\\n\\n    # Pandas frame - good & failure\\n    df_good = pandas.DataFrame.from_dict(dict_id_good_preds)\\n    df_failures = pandas.DataFrame.from_dict(dict_id_failures_preds)\\n    df_objects =  pandas.DataFrame.from_dict(dict_id_object)\\n\\n    return losses.avg, metric_avg, per_class_metric_avg, df_good, df_failures, df_objects\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extractionsdf = datalinksdf\n",
        "extractionsdf['code'] = contents\n",
        "extractionsdf.head()"
      ],
      "metadata": {
        "id": "w-IeZWq1OU2R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "5ae8ee9a-3ae3-4589-80cf-5b6dc2054f2c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               paper  \\\n",
              "0  Temporal coherence-based self-supervised learn...   \n",
              "1  BinGAN: Learning Compact Binary Descriptors wi...   \n",
              "2  Multiscale Fisher's Independence Test for Mult...   \n",
              "3         POTs: Protective Optimization Technologies   \n",
              "4                                   Surface Networks   \n",
              "\n",
              "                                     link  \\\n",
              "0   http://arxiv.org/pdf/1806.06811v2.pdf   \n",
              "1   http://arxiv.org/pdf/1806.06778v5.pdf   \n",
              "2  https://arxiv.org/pdf/1806.06777v7.pdf   \n",
              "3  https://arxiv.org/pdf/1806.02711v6.pdf   \n",
              "4   http://arxiv.org/pdf/1705.10819v2.pdf   \n",
              "\n",
              "                                                code  \\\n",
              "0  \"\"\"Self-supervised pretraining based on tempor...   \n",
              "1  import numpy as np\\n\\n\\ndef hamming_dist(test_...   \n",
              "2                                                      \n",
              "3  \"\"\"\\nA hash function for numpy arrays.\\n\"\"\"\\n\\...   \n",
              "4  '''\\nThis file is part of source code for \"Sur...   \n",
              "\n",
              "                                                text  \n",
              "0  Temporalcoherence-basedself-supervised\\nlearni...  \n",
              "1  BinGAN:LearningCompactBinaryDescriptors\\nwitha...  \n",
              "2  MultiscaleFisher'sIndependenceTestfor\\nMultiva...  \n",
              "3  POTs:ProtectiveOptimizationTechnologies\\nBogda...  \n",
              "4  SurfaceNetworks\\nIlyaKostrikov\\n1\\n,ZhongshiJi...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b8be457-0673-4710-98cf-2cfcc0f9161d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper</th>\n",
              "      <th>link</th>\n",
              "      <th>code</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Temporal coherence-based self-supervised learn...</td>\n",
              "      <td>http://arxiv.org/pdf/1806.06811v2.pdf</td>\n",
              "      <td>\"\"\"Self-supervised pretraining based on tempor...</td>\n",
              "      <td>Temporalcoherence-basedself-supervised\\nlearni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BinGAN: Learning Compact Binary Descriptors wi...</td>\n",
              "      <td>http://arxiv.org/pdf/1806.06778v5.pdf</td>\n",
              "      <td>import numpy as np\\n\\n\\ndef hamming_dist(test_...</td>\n",
              "      <td>BinGAN:LearningCompactBinaryDescriptors\\nwitha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Multiscale Fisher's Independence Test for Mult...</td>\n",
              "      <td>https://arxiv.org/pdf/1806.06777v7.pdf</td>\n",
              "      <td></td>\n",
              "      <td>MultiscaleFisher'sIndependenceTestfor\\nMultiva...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>POTs: Protective Optimization Technologies</td>\n",
              "      <td>https://arxiv.org/pdf/1806.02711v6.pdf</td>\n",
              "      <td>\"\"\"\\nA hash function for numpy arrays.\\n\"\"\"\\n\\...</td>\n",
              "      <td>POTs:ProtectiveOptimizationTechnologies\\nBogda...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Surface Networks</td>\n",
              "      <td>http://arxiv.org/pdf/1705.10819v2.pdf</td>\n",
              "      <td>'''\\nThis file is part of source code for \"Sur...</td>\n",
              "      <td>SurfaceNetworks\\nIlyaKostrikov\\n1\\n,ZhongshiJi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b8be457-0673-4710-98cf-2cfcc0f9161d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b8be457-0673-4710-98cf-2cfcc0f9161d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b8be457-0673-4710-98cf-2cfcc0f9161d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extractionsdf.to_csv('code-extractions-100.csv')"
      ],
      "metadata": {
        "id": "2BvYyPR2OmbJ"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}