{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqvV6GtOnqT4",
        "outputId": "d025de07-dde3-4894-902a-1ceb46d7453d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUZULMknZLDb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wget\n",
        "import zipfile\n",
        "import shutil\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zidk_MLQ_kgA"
      },
      "outputs": [],
      "source": [
        "datalinkspath = './paper-code-links.csv'\n",
        "datalinksdf = pd.read_csv(datalinkspath, index_col=0)\n",
        "links = list(datalinksdf['link'])\n",
        "ids = [link.split('/')[-1].split('.pdf')[0].split('v')[0] for link in links]\n",
        "datalinksdf['id'] = ids\n",
        "df = datalinksdf.set_index('id')\n",
        "processed_dir = 'drive/MyDrive/RSS/Processed/'\n",
        "files = os.listdir(processed_dir)\n",
        "processed = df.loc[files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjSF1mH1ld17",
        "outputId": "07fcf118-fe11-4daa-9ab5-b2f561652536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Link 1000 : https://github.com/alvinliu0/visual-sound-localization-in-the-wild\n",
            "Link 1001 : https://github.com/madrylab/editingclassifiers\n",
            "Link 1002 : https://github.com/litinglin/swintrack\n",
            "Link 1003 : https://github.com/french-paragon/libstevi\n",
            "Link 1004 : https://github.com/tacju/partimagenet\n",
            "Link 1005 : https://github.com/akuehlka/xai4b_tlpim\n",
            "Link 1006 : https://github.com/vroulet/tpri\n",
            "Link 1007 : https://github.com/mfinzi/residual-pathway-priors\n",
            "Link 1008 : https://github.com/kunzhan/sdsne\n",
            "Link 1009 : https://github.com/nianlonggu/Local-Citation-Recommendation\n",
            "Link 1010 : https://github.com/hujunxianligong/capgnn\n",
            "Link 1011 : https://github.com/wywywang/shuttlenet\n",
            "Link 1012 : https://github.com/PaddlePaddle/PGL\n",
            "Link 1013 : https://github.com/MLforHealth/predictive_checklists\n",
            "Link 1014 : https://github.com/ncclabsustech/deepseparator\n",
            "Link 1015 : https://github.com/snagcliffs/ee_loss\n",
            "Link 1016 : https://github.com/peymanmorteza/gem\n",
            "Link 1017 : https://github.com/quantumlib/ReCirq\n",
            "Link 1018 : https://github.com/tudkcui/gmfg-learning\n",
            "Link 1019 : https://github.com/speechLabBcCuny/onssen\n",
            "Link 1020 : https://github.com/fredzzhang/upt\n",
            "Link 1021 : https://github.com/L0SG/grouped-ssd-pytorch\n",
            "Link 1022 : https://github.com/YukeWang96/TCGNN-Pytorch\n",
            "Link 1023 : https://github.com/kartta-labs/project\n",
            "Link 1024 : https://github.com/mr28/poly-msc-thesis-trajectory-clustering\n",
            "Link 1025 : https://github.com/shiming-chen/transzero\n",
            "Link 1026 : https://github.com/chiang9/NLP-Chinese_couplet_generation\n",
            "Link 1027 : https://github.com/periakiva/periakiva\n",
            "Link 1028 : https://github.com/vithyayogarajan/Medical-Domain-Specific-Language-Models\n",
            "Link 1029 : https://github.com/eric11eca/inference-information-probing\n",
            "Link 1030 : https://github.com/shuguoj/hsi-classification\n",
            "Link 1031 : https://git5.cs.fau.de/folle/sr-cest\n",
            "Link 1032 : https://github.com/semantic-systems/elenglishwd\n",
            "Link 1033 : https://github.com/aseidelo/wiki_generator\n",
            "Link 1034 : https://github.com/bigshawne/csci_544_final_proj_recommendation_system_based_on_nlp\n",
            "Link 1035 : https://github.com/seznam/dareczech\n",
            "Link 1036 : https://github.com/amiryousefilab/pekit\n",
            "Link 1037 : https://github.com/mrfanuel/denoising-modulo-samples-local-polynomial-estimator\n",
            "Link 1038 : https://github.com/gnobitab/fusedream\n",
            "Link 1039 : https://github.com/adverml/spectraldef_framework\n",
            "Link 1040 : https://github.com/fuy34/dfv\n",
            "Link 1041 : https://github.com/oiplab-dut/mfnet\n",
            "Link 1042 : https://github.com/fengzhang427/hep\n",
            "Link 1043 : https://github.com/blueblue4/object-detection-confidence-bias\n",
            "Link 1044 : https://github.com/zhouzheyuan/sgm3d\n",
            "Link 1045 : https://github.com/mathlee/mccnet\n",
            "Link 1046 : https://github.com/shamandevel/fv-srn\n",
            "Link 1047 : https://github.com/jingyechen/mt-transunet\n",
            "Link 1048 : https://github.com/gortizji/inr_dictionaries\n",
            "Link 1049 : https://github.com/rwickman/sparrl-pytorch\n",
            "Link 1050 : https://github.com/nftqcd/fthmc\n",
            "Link 1051 : https://github.com/kdMoura/label-noise\n",
            "Link 1052 : https://github.com/cogsys-tuebingen/uninas\n",
            "Link 1053 : https://github.com/thaihungle/EPGT\n",
            "Link 1054 : https://github.com/danelee2601/rl-based-automatic-berthing\n",
            "Link 1055 : https://github.com/jxying/mtp2\n",
            "Link 1056 : https://github.com/saforem2/l2hmc-qcd\n",
            "Link 1057 : https://gitlab.com/pj_meyer/TIRA\n",
            "Link 1058 : https://github.com/WangTaoAs/PFD_Net\n",
            "Link 1059 : https://github.com/zixuanke/pycontinual\n",
            "Link 1060 : https://github.com/zixuanke/pycontinual\n",
            "Link 1061 : https://github.com/bupt-ai-cz/BALNMP\n",
            "Link 1062 : https://github.com/yandex-research/ddpm-segmentation\n",
            "Link 1063 : https://github.com/bingrao/ctin\n",
            "Link 1064 : https://github.com/athena-xiourouppa/NLP-The-Simpsons-and-9-11\n",
            "Link 1065 : https://github.com/xavysp/DexiNed\n",
            "Link 1066 : https://github.com/alexlioralexli/learned-fourier-features\n",
            "Link 1067 : https://github.com/microsoft/DEKCOR-CommonsenseQA\n",
            "Link 1068 : https://github.com/interpretml/gam-changer\n",
            "Link 1069 : https://github.com/mees/calvin\n",
            "Link 1070 : https://github.com/nndoubledescent/doubledescent\n",
            "Link 1071 : https://github.com/mklissa/moc\n",
            "Link 1072 : https://github.com/wengong-jin/icml18-jtnn\n",
            "Link 1073 : https://github.com/whatashot/danet\n",
            "Link 1074 : https://github.com/reinholdm/offline-pre-trained-multi-agent-decision-transformer\n",
            "Link 1075 : https://github.com/ed2-source-code/ed2\n",
            "Link 1076 : https://github.com/xylee95/md-pgt\n",
            "Link 1077 : https://github.com/endywon/texture-reformer\n",
            "Link 1078 : https://github.com/ermongroup/bcd-nets\n",
            "Link 1079 : https://github.com/PaddlePaddle/Paddle\n",
            "Link 1080 : https://github.com/GEM-benchmark/NL-Augmenter\n",
            "Link 1081 : https://github.com/mikewangwzhl/eeg-to-text\n",
            "Link 1082 : https://github.com/krr-oxford/bertmap\n",
            "Link 1083 : https://github.com/wqshao126/dtn\n",
            "Link 1084 : https://github.com/abdulmajid-murad/deep_probabilistic_forecast\n",
            "Link 1085 : https://github.com/chen289/Visual-GCN\n",
            "Link 1086 : https://github.com/jctian98/e2e_lfmmi\n",
            "Link 1087 : https://github.com/namkyeong/afgrl\n",
            "Link 1088 : https://github.com/liangzhang1996/efficient_xlight\n",
            "Link 1089 : https://github.com/wangdeze18/DACL\n",
            "Link 1090 : https://github.com/mavridischristos/onlinedeterministicannealing\n",
            "Link 1091 : https://github.com/xharlie/btcdet\n",
            "Link 1092 : https://github.com/threedle/text2mesh\n",
            "Link 1093 : https://github.com/princetonvisualai/HIVE\n",
            "Link 1094 : https://github.com/juliawolleb/diffusion-based-segmentation\n",
            "Link 1095 : https://github.com/FacePerceiver/FaRL\n",
            "Link 1096 : https://github.com/multimodallearning/convexadam\n",
            "Link 1097 : https://github.com/google-research/jax-influence\n",
            "Link 1098 : https://github.com/LukasBommes/PV-Mapper\n",
            "Link 1099 : https://github.com/Shiaoming/ALIKE\n",
            "Link 1100 : https://github.com/philip-mueller/lovt\n",
            "Link 1101 : https://github.com/jasonkks/pttr\n",
            "Link 1102 : https://github.com/jerryx1110/rpcmvos\n",
            "Link 1103 : https://github.com/qi-yangsjtu/it-pcqa\n",
            "Link 1104 : https://github.com/PaddlePaddle/PaddleGAN\n",
            "Link 1105 : https://github.com/pris-cv/relmatch\n",
            "Link 1106 : https://github.com/SeanChenxy/HandMesh\n",
            "Link 1107 : https://github.com/pris-cv/making-a-bird-ai-expert-work-for-you-and-me\n",
            "Link 1108 : https://github.com/jcwang123/separate_cl\n",
            "Link 1109 : https://github.com/harboryuan/polyphonicformer\n",
            "Link 1110 : https://github.com/phiphiphi31/dualtfr\n",
            "Link 1111 : https://github.com/zhengpeng7/glcnet\n",
            "Link 1112 : https://github.com/princeton-computational-imaging/gated2gated\n",
            "Link 1113 : https://github.com/sun-xl/ISC2021\n",
            "Link 1114 : https://github.com/mizhenxing/gbi-net\n",
            "Link 1115 : https://github.com/zhuhao-nju/mofanerf\n",
            "Link 1116 : https://github.com/iliasprc/ssl-vit-cnn\n",
            "Link 1117 : https://github.com/zichengsaber/LAVT-pytorch\n",
            "Link 1118 : https://github.com/jerryxu0129/hhf\n",
            "Link 1119 : https://github.com/tldrafael/facereconstructionwithvaeandfacemasks\n",
            "Link 1120 : https://github.com/niloyb/boundwasserstein\n",
            "Link 1121 : https://github.com/martius-lab/hits\n",
            "Link 1122 : https://github.com/mkuzma96/dta\n",
            "Link 1123 : https://github.com/JonasGeiping/breaching\n",
            "Link 1124 : https://github.com/steremma/curr-meta-learning\n",
            "Link 1125 : https://github.com/elephaint/pedpf\n",
            "Link 1126 : https://github.com/anonmlresearcher/icpa\n",
            "Link 1127 : https://github.com/suphanat123/defi\n",
            "Link 1128 : https://github.com/konpanousis/adversarial-lwta-autoattack\n",
            "Link 1129 : https://github.com/slaweks17/ES-dRNN\n",
            "Link 1130 : https://github.com/squareslab/varclr\n",
            "Link 1131 : https://github.com/zihsyuan1214/rmda\n",
            "Link 1132 : https://github.com/seemoo-lab/myo-keylogging\n",
            "Link 1133 : https://github.com/ruanchaves/hashformers\n",
            "Link 1134 : https://github.com/fe1ixxu/vt-stower\n",
            "Link 1135 : https://github.com/facebookresearch/vocoder-benchmark\n",
            "Link 1136 : https://github.com/shailzajolly/fsdt\n",
            "Link 1137 : https://github.com/xlhex/nlg_api_watermark\n",
            "Link 1138 : https://github.com/LAL-project/linear-arrangement-library\n",
            "Link 1139 : https://github.com/judicaelpoumay/htmot\n",
            "Link 1140 : https://github.com/bahareharandizade/keyetm\n",
            "Link 1141 : https://github.com/xicocaio/its-sentarl\n",
            "Link 1142 : https://github.com/venomnomnom/genderbias\n",
            "Link 1143 : https://github.com/chatnoir-eu/chatnoir-resiliparse\n",
            "Link 1144 : https://github.com/awai54st/logic-shrinkage\n",
            "Link 1145 : https://github.com/imperial-qore/pregan\n",
            "Link 1146 : https://github.com/csteinmetz1/steerable-nafx\n",
            "Link 1147 : https://gitlab.com/ryuan/ieee-cis-data-challenge-fresno\n",
            "Link 1148 : https://github.com/AdriaJ/PolyatomicFW_SPL\n",
            "Link 1149 : https://github.com/lisifra96/improved_rl_algorithm_mmimo_radar\n",
            "Link 1150 : https://github.com/yihengsun/TransBoost\n",
            "Link 1151 : https://github.com/raphaelolivier/smoothingasr\n",
            "Link 1152 : https://github.com/bionlplab/drugprot_bcvii\n",
            "Link 1153 : https://github.com/samasky/algorithmic_fairness_matching\n",
            "Link 1154 : https://github.com/zlh-thu/StealingVerification\n",
            "Link 1155 : https://github.com/smitp415/CSCI_544_Final_Project\n",
            "Link 1156 : https://github.com/jeongwhanchoi/STG-NCDE\n",
            "Link 1157 : https://github.com/zixuanke/pycontinual\n",
            "Link 1158 : https://github.com/parasurama/raceBERT\n",
            "Link 1159 : https://github.com/microsoft/GLIP\n",
            "Link 1160 : https://github.com/Robert-Mar/VFA\n",
            "Link 1161 : https://github.com/k-h-ismail/dilated-convolution-with-learnable-spacings-pytorch\n",
            "Link 1162 : https://github.com/jinhyung-park-info/TP-Net\n",
            "Link 1163 : https://github.com/XH-B/ABM\n",
            "Link 1164 : https://github.com/zhaoyanglyu/point_diffusion_refinement\n",
            "Link 1165 : https://github.com/i-need-sleep/mmcoref_cleaned\n",
            "Link 1166 : https://github.com/viniciusguigo/kairos_minerl_basalt\n",
            "Link 1167 : https://github.com/socom20/facebook-image-similarity-challenge-2021\n",
            "Link 1168 : https://github.com/shehzaadzd/MINERVA\n",
            "Link 1169 : https://github.com/mah533/synthetic-ecg-generation---gan-models-comparison\n",
            "Link 1170 : https://github.com/bupt-ai-cz/HSA-NRL\n",
            "Link 1171 : https://github.com/vita-epfl/cim\n",
            "Link 1172 : https://github.com/vita-epfl/s-attack\n",
            "Link 1173 : https://github.com/edornd/contrastive-distillation\n",
            "Link 1174 : https://github.com/mettyz/ssvc\n",
            "Link 1175 : https://github.com/lelouedec/phd_3dperception\n",
            "Link 1176 : https://github.com/gqding/salfbnet\n",
            "Link 1177 : https://github.com/wenxuecui/nl-csnet-pytorch\n",
            "Link 1178 : https://github.com/moothes/a2s-usod\n",
            "Link 1179 : https://github.com/levigty/aimclr\n",
            "Link 1180 : https://github.com/leminhbinh0209/add\n",
            "Failed downloading zipfile https://github.com/leminhbinh0209/add\n",
            "Link 1181 : https://github.com/shiyuchengtju/par\n",
            "Link 1182 : https://github.com/liwentomng/boxlevelset\n",
            "Link 1183 : https://github.com/petrhruby97/learning_minimal\n",
            "Link 1184 : https://github.com/dmdmello/hc-mgan\n",
            "Link 1185 : https://github.com/vicariousinc/mam\n",
            "Link 1186 : https://github.com/yiren-jian/labelhalluc\n",
            "Link 1187 : https://github.com/andremaz/transformer-pointer-critic\n",
            "Link 1188 : https://github.com/lauromoraes/capsnet-promoter\n",
            "Link 1189 : https://github.com/mryansong/ccasgnn\n",
            "Link 1190 : https://github.com/edbeeching/godot_rl_agents\n",
            "Link 1191 : https://github.com/bond005/yandex-shifts-weather\n",
            "Link 1192 : https://github.com/huawei-noah/trustworthyAI\n",
            "Link 1193 : https://github.com/jeffminlin/vmcnet\n",
            "Link 1194 : https://github.com/MReza89/Emulating-Spatio-Temporal-Realizations-of-Three-Dimensional-Isotropic-Turbulence-via-Deep-Sequence\n",
            "Link 1195 : https://github.com/peymantehrani/fdrl-pc-dyspan\n",
            "Link 1196 : https://gitee.com/ncucjm/GraphPAS\n",
            "Link 1197 : https://github.com/brett-daley/virtual-replay-cache\n",
            "Link 1198 : https://github.com/baratilab/gamd\n",
            "Link 1199 : https://github.com/ncfrey/litmatter\n",
            "Link 1200 : https://github.com/kakarotcq/rna-seq-and-atac-seq-mapping\n",
            "Link 1201 : https://github.com/dvamossy/emtract\n",
            "Link 1202 : https://github.com/wangcongcong123/crisis-mtl\n",
            "Link 1203 : https://bitbucket.org/robvanderg/dataembs2\n",
            "Link 1204 : https://github.com/nguyentthong/CrossSummOptimalTransport\n",
            "Link 1205 : https://github.com/yoyololicon/danna-sep\n",
            "Link 1206 : https://github.com/decodepfl/neural_energy_casimir_control\n",
            "Link 1207 : https://github.com/dougpsg/hif_vegetation_data\n",
            "Link 1208 : https://github.com/Jeongseungwoo/Efficient-Continuous-Manifold-Learning\n",
            "Link 1209 : https://github.com/liyunqianggyn/equal-bits-bnn\n",
            "Link 1210 : https://github.com/colintr/classificationforregression\n",
            "Link 1211 : https://github.com/ratschlab/HIRID-ICU-Benchmark\n",
            "Link 1212 : https://github.com/fergaletto/ReverseFilter_TDA\n",
            "Link 1213 : https://github.com/jgkwak95/AU-GAN\n",
            "Link 1214 : https://github.com/ywa136/TensorGraphicalModels.jl\n",
            "Link 1215 : https://github.com/UsmanMahmood27/BrainGNN\n",
            "Link 1216 : https://github.com/dyc941126/gat-pcm\n",
            "Link 1217 : https://github.com/taharallouche/truth_tracking-via-av\n",
            "Link 1218 : https://github.com/thyrixyang/rid-noise-aaai22\n",
            "Link 1219 : https://github.com/easonnie/mlp-vil\n",
            "Link 1220 : https://github.com/umd-fire-coml/kitti-orientation-learning\n",
            "Link 1221 : https://github.com/lyakaap/isc21-descriptor-track-1st\n",
            "Link 1222 : https://github.com/Dawn-LX/VidSGG-BIG\n",
            "Link 1223 : https://github.com/vita-epfl/looking\n",
            "Link 1224 : https://github.com/MatanCohen1/Transformaly\n",
            "Link 1225 : https://github.com/wbteng9526/mtut_fr\n",
            "Link 1226 : https://github.com/ssundaram21/symmetry\n",
            "Link 1227 : https://github.com/hongfz16/garment4d\n",
            "Link 1228 : https://github.com/liangcici/CITL-VLN\n",
            "Link 1229 : https://github.com/wendongzh/spn\n",
            "Link 1230 : https://github.com/anguyen8/deepface-emd\n",
            "Link 1231 : https://github.com/yookoon/nac\n",
            "Link 1232 : https://github.com/plrbear/auxskd\n",
            "Link 1233 : https://github.com/christophreich1996/3d_baggage_segmentation\n",
            "Link 1234 : https://github.com/nspunn1993/bt-unet\n",
            "Link 1235 : https://github.com/sisl/cnnhyperscanningclassification\n",
            "Link 1236 : https://github.com/ruijiang81/cst\n",
            "Link 1237 : https://github.com/labmlai/annotated_deep_learning_paper_implementations\n",
            "Link 1238 : https://c4science.ch/source/adaptive_rpeak_det_public\n",
            "Link 1239 : https://github.com/hanane-djeddal/complex-answer-generation\n",
            "Link 1240 : https://github.com/onur4229/svsl_lmoe\n",
            "Link 1241 : https://github.com/noabdavid/csale\n",
            "Link 1242 : https://github.com/BIT-DA/ParetoDA\n",
            "Link 1243 : https://github.com/foadsohrabi/dl-activesensing\n",
            "Link 1244 : https://github.com/deepdiagnosis/icse2022\n",
            "Link 1245 : https://github.com/GRAAL-Research/deepparse\n",
            "Link 1246 : https://github.com/rhshi/sparse-rf\n",
            "Link 1247 : https://github.com/huawei-noah/Pretrained-Language-Model\n",
            "Link 1248 : https://github.com/ielab/sdr\n",
            "Link 1249 : https://github.com/xavierohan/AdaClust_DomainBed\n",
            "Link 1250 : https://github.com/mli-lab/regularization_based_continual_learning\n",
            "Link 1251 : https://github.com/shellysheynin/locally-sag-transformer\n",
            "Link 1252 : https://github.com/lschmelzeisen/wikidated\n",
            "Link 1253 : https://github.com/ivandonadello/ml-argument-based-computational-persuasion\n",
            "Link 1254 : https://github.com/fbiying87/explainable_fiqa_with_amva\n",
            "Link 1255 : https://github.com/mail-ecnu/vmagent\n",
            "Link 1256 : https://github.com/awesomericky/multiple-gait-controller-for-quadruped-robot\n",
            "Link 1257 : https://github.com/haoheliu/2021-ismir-mss-challenge-cws-presunet\n",
            "Link 1258 : https://github.com/sail-sg/dualformer\n",
            "Link 1259 : https://github.com/joey-shen/mlph\n",
            "Link 1260 : https://github.com/wolfecameron/i-spasp\n",
            "Link 1261 : https://github.com/wty-ustc/hairclip\n",
            "Link 1262 : https://github.com/wpeebles/gangealing\n",
            "Link 1263 : https://github.com/fundamentalvision/parameterized-ap-loss\n",
            "Link 1264 : https://github.com/andyzoujm/pixmix\n",
            "Link 1265 : https://github.com/vita-epfl/semdisc\n",
            "Link 1266 : https://github.com/sxyu/svox2\n",
            "Link 1267 : https://github.com/fangjinhuawang/itermvs\n",
            "Link 1268 : https://github.com/jamycheung/ISSAFE\n",
            "Link 1269 : https://github.com/padeler/pe-former\n",
            "Link 1270 : https://github.com/dvlab-research/Entity\n",
            "Link 1271 : https://github.com/anindyasdas/selfsupervisedimagetext\n",
            "Link 1272 : https://github.com/xiwuchen/pra-net\n",
            "Link 1273 : https://github.com/HiLab-git/SSL4MIS\n",
            "Link 1274 : https://github.com/cebudding/medneurips_2021\n",
            "Link 1275 : https://github.com/crane-papercode/3dmedpt\n",
            "Link 1276 : https://github.com/axelbarroso/scalenet\n",
            "Link 1277 : https://github.com/vis-opt-group/ruas\n",
            "Link 1278 : https://github.com/lufanma/ifr\n",
            "Link 1279 : https://github.com/zhyever/simipu\n",
            "Link 1280 : https://github.com/w-zx-y/sm-ppm\n",
            "Link 1281 : https://github.com/htyao89/dual-cluster-contrastive\n",
            "Link 1282 : https://github.com/rixez/brats21_kaist_mri_lab\n",
            "Link 1283 : https://github.com/zhechen/deformable-detr-rego\n",
            "Link 1284 : https://github.com/Xianpeng919/MonoCon\n",
            "Link 1285 : https://github.com/ucdvision/cmsf\n",
            "Link 1286 : https://github.com/epfml/uncertainity-estimation\n",
            "Link 1287 : https://github.com/yuanyuan-yuan/manifold-sca\n",
            "Link 1288 : https://github.com/raphaels1/distribution_discrimination\n",
            "Link 1289 : https://github.com/sherzod-hakimov/hasoc-2021---hate-speech-detection\n",
            "Link 1290 : https://github.com/lgondara/dptemporalensemble\n",
            "Link 1291 : https://gitlab.com/lklee/div-est-via-discrete-decomp-models\n",
            "Link 1292 : https://github.com/jumxglhf/akgnn\n",
            "Link 1293 : https://github.com/alexdrk14/usbotdetection\n",
            "Link 1294 : https://github.com/personads/ud-genre\n",
            "Link 1295 : https://github.com/neukg/birte\n",
            "Link 1296 : https://github.com/lcs2-iiitd/multimodal-sarcasm-explanation-muse\n",
            "Link 1297 : https://github.com/alexfrummet/cookversationalsearch\n",
            "Link 1298 : https://github.com/phongnt570/large-scale-csk-extraction\n",
            "Link 1299 : https://github.com/saturnian77/drsan\n",
            "Link 1300 : https://github.com/42jaylonw/rrc2021threewolves\n",
            "Link 1301 : https://github.com/stanfordasl/randup\n",
            "Link 1302 : https://bitbucket.org/RoxanaSz/puzzte\n",
            "Link 1303 : https://github.com/amy-deng/cape\n",
            "Link 1304 : https://github.com/chaochen99/disco\n",
            "Link 1305 : https://github.com/google/neural-tangents\n",
            "Link 1306 : https://github.com/jetsunwhitton/rct-art\n",
            "Link 1307 : https://github.com/magnetar-iiith/pril\n",
            "Link 1308 : https://github.com/jiangjiechen/educat\n",
            "Link 1309 : https://github.com/dotolation/diverse-graph-algo\n",
            "Link 1310 : https://github.com/agoodge/lunar\n",
            "Link 1311 : https://github.com/giseung-park/blockseq\n",
            "Link 1312 : https://github.com/rungjoo/simmc2.0\n",
            "Link 1313 : https://github.com/fdbtrs/masked-face-recognition-kd\n",
            "Link 1314 : https://github.com/snu-mllab/preemptive_robustification\n",
            "Link 1315 : https://github.com/guolan68/attention-set/blob/main/GAM%20attention.py\n",
            "Failed downloading zipfile https://github.com/guolan68/attention-set/blob/main/GAM%20attention.py\n",
            "Link 1316 : https://github.com/fodark/anonygan\n",
            "Link 1317 : https://github.com/8ubpshlr23/central-filter\n",
            "Link 1318 : https://github.com/maikwischow/camera-condition-monitoring\n",
            "Link 1319 : https://github.com/lhf-labs/tm-dataset\n",
            "Link 1320 : https://github.com/kellyiss/situformer\n",
            "Link 1321 : https://github.com/kaiyizhang/AXform\n",
            "Link 1322 : https://github.com/yongleex/sbcc\n",
            "Link 1323 : https://github.com/machinevision-seoultech/evci\n",
            "Link 1324 : https://github.com/rjavadi/social-signal-project\n",
            "Link 1325 : https://github.com/Aleph-Alpha/magma\n",
            "Link 1326 : https://github.com/diagnijmegen/report-guided-annotation\n",
            "Link 1327 : https://github.com/xjtu-cvlab-lowlevel/rnn-mbp\n",
            "Link 1328 : https://github.com/karelzhang/csdnet-csdgan\n",
            "Failed walking repo https://github.com/karelzhang/csdnet-csdgan\n",
            "Link 1329 : https://github.com/mfahes/lpalm\n",
            "Link 1330 : https://github.com/xulabs/aitom\n",
            "Link 1331 : https://github.com/aorvieto/ncpl\n",
            "Link 1332 : https://github.com/pkuzengqi/skeinformer\n",
            "Link 1333 : https://github.com/Chenyang-1995/Online-Steiner-Tree\n",
            "Link 1334 : https://github.com/ebagdasa/propaganda_as_a_service\n",
            "Link 1335 : https://github.com/luchang-cs/chet\n",
            "Link 1336 : https://github.com/sileod/metaeval\n",
            "Link 1337 : https://github.com/bootphon/shennong\n",
            "Link 1338 : https://github.com/tamu-engineering-research/COVID-EMDA\n",
            "Link 1339 : https://github.com/qkrdmsghk/textssl\n",
            "Link 1340 : https://github.com/Spico197/DocEE\n",
            "Link 1341 : https://github.com/chenhaoxing/ASL\n",
            "Link 1342 : https://github.com/nutcrtnk/leaprec\n",
            "Link 1343 : https://github.com/truongkhang/cds-mvsnet\n",
            "Link 1344 : https://github.com/278287847/DEM\n",
            "Link 1345 : https://github.com/ylsung/vl_adapter\n",
            "Link 1346 : https://github.com/cmeo97/maic\n",
            "Link 1347 : https://github.com/astrazeneca/biomedical-kg-topological-imbalance\n",
            "Link 1348 : https://github.com/antoniocarta/ex_model_cl\n",
            "Link 1349 : https://github.com/shichengchen/multiviewdataset\n",
            "Link 1350 : https://github.com/oli-yun/dependency-ljp\n",
            "Link 1351 : https://github.com/sparsefed/sparsefed\n",
            "Link 1352 : https://github.com/zju-vipa/Fast-Datafree\n",
            "Link 1353 : https://github.com/doc-doc/hqga\n",
            "Link 1354 : https://github.com/LiGaoJi/MPLR\n",
            "Link 1355 : https://github.com/canqin001/asda\n",
            "Link 1356 : https://github.com/alexklwong/stereoscopic-universal-perturbations\n",
            "Link 1357 : https://github.com/joyebright/dataselection-nmt\n",
            "Link 1358 : https://github.com/cmavro/tempoqr\n",
            "Link 1359 : https://github.com/michael-camilleri/tim\n",
            "Link 1360 : https://github.com/arminmasoumian/gcndepth\n",
            "Link 1361 : https://github.com/qxgeng/dea-net\n",
            "Link 1362 : https://github.com/sh4174/hypernetensemble\n",
            "Link 1363 : https://github.com/fdbtrs/cr-fiqa\n",
            "Link 1364 : https://github.com/kamilzywanowski/minkloc3d-si\n",
            "Link 1365 : https://github.com/andreabehan/g-acson\n",
            "Link 1366 : https://github.com/xinli-zn/informative-tracking-benchmark\n",
            "Link 1367 : https://github.com/learninginvision/spu\n",
            "Link 1368 : https://github.com/linglix/scl-lle\n",
            "Link 1369 : https://github.com/pdncovid/covid-people-graph\n",
            "Link 1370 : https://github.com/zhwzhong/dagf\n",
            "Link 1371 : https://github.com/63days/PartGlot\n",
            "Link 1372 : https://github.com/tusimple/sst\n",
            "Link 1373 : https://github.com/jaehanlee-mcl/DPICT\n",
            "Link 1374 : https://github.com/paullenwyue/ABSGNet\n",
            "Link 1375 : https://github.com/codyshen0000/itsrn\n",
            "Link 1376 : https://github.com/sunshower76/deep_trainslation_prior\n",
            "Link 1377 : https://github.com/yizhouzhao/tangram\n",
            "Link 1378 : https://github.com/sun-umn/early_stopping_for_dip\n",
            "Link 1379 : https://github.com/RosettaWYzhang/AdaPC\n",
            "Link 1380 : https://github.com/mint-vu/slosh\n",
            "Link 1381 : https://github.com/rucv/channeldiversification\n",
            "Link 1382 : https://github.com/gonyrosenman/tff\n",
            "Link 1383 : https://github.com/k-stacke/ssl-pathology\n",
            "Link 1384 : https://github.com/zudi-lin/pytorch_connectomics\n",
            "Link 1385 : https://github.com/chunmeifeng/fedmri\n",
            "Link 1386 : https://github.com/karthajee/aws-if-interpretability\n",
            "Link 1387 : https://github.com/BScNets/BScNets\n",
            "Link 1388 : https://github.com/ai4finance-foundation/finrl-meta\n",
            "Link 1389 : https://github.com/vvvm23/sundae\n",
            "Link 1390 : https://github.com/keelm/XDCC\n",
            "Link 1391 : https://gitlab.com/dacs-hpi/hiclass\n",
            "Failed downloading zipfile https://gitlab.com/dacs-hpi/hiclass\n",
            "Link 1392 : https://github.com/wyn430/wood\n",
            "Link 1393 : https://github.com/junghunkim7786/scheduling_bilinearrewards\n",
            "Link 1394 : https://github.com/mostafarezapour/hidden-effects-of-covid-19-on-healthcare-workers-a-machine-learning-analysis\n",
            "Link 1395 : https://github.com/sibirbil/less\n",
            "Link 1396 : https://github.com/wentao-xu/shgnn\n",
            "Link 1397 : https://github.com/psg-mit/programming-with-neural-surrogates-of-programs\n",
            "Link 1398 : https://github.com/usccolumbia/tsdnn\n",
            "Link 1399 : https://github.com/paulocmarquesf/rangerror\n",
            "Link 1400 : https://github.com/vittorione94/ostrichrl\n",
            "Link 1401 : https://github.com/eyalrozenberg1/spdcinv\n",
            "Link 1402 : https://github.com/wanganran/HybridBeam\n",
            "Link 1403 : https://github.com/Blue-Giant/SubspaceDNN_tf1\n",
            "Link 1404 : https://github.com/anastasia-zhukova/anea\n",
            "Link 1405 : https://github.com/edhirst/p4cy3ml\n",
            "Link 1406 : https://github.com/zhengrongqin/c2-rec\n",
            "Link 1407 : https://github.com/ielab/apr\n",
            "Link 1408 : https://github.com/rajdeep345/mtlts\n",
            "Link 1409 : https://github.com/andersonenergylab-cornell/nygrid\n",
            "Link 1410 : https://github.com/jorrithimself/gt-china-coal-model\n",
            "Link 1411 : https://github.com/mcc-wh/token\n",
            "Link 1412 : https://github.com/shekoo93/imagery\n",
            "Link 1413 : https://github.com/Stanford-ILIAD/PantheonRL\n",
            "Link 1414 : https://github.com/shangzongjiang/SNAS4MTF\n",
            "Link 1415 : https://github.com/hanzheng98/sn-cqa\n",
            "Link 1416 : https://github.com/jylins/core-text\n",
            "Link 1417 : https://github.com/xmc-aalto/adv-xmtc\n",
            "Link 1418 : https://github.com/sdm-tib/eablock\n",
            "Link 1419 : https://github.com/bjliaa/c2d\n",
            "Link 1420 : https://github.com/flowersteam/architect-builder-abig\n",
            "Link 1421 : https://github.com/ideas-labo/mmo\n",
            "Link 1422 : https://github.com/imperial-qore/COSCO\n",
            "Link 1423 : https://github.com/animadversio/foveated_saccade_simclr\n",
            "Link 1424 : https://github.com/emanuelelm/the-king-is-naked\n",
            "Link 1425 : https://github.com/xinyu1205/robust-loss-mlml\n",
            "Link 1426 : https://github.com/wonbinkweon/calibratedrankingmodels_aaai2022\n",
            "Link 1427 : https://github.com/SRI-CSL/TrinityMultimodalTrojAI\n",
            "Link 1428 : https://github.com/laurarkart/Physical-Anomalous-Trajectory-or-Motion-PHANTOM-Dataset\n",
            "Link 1429 : https://github.com/heidelberg-nlp/valse\n",
            "Link 1430 : https://github.com/VSainteuf/pastis-benchmark\n",
            "Link 1431 : https://github.com/mavillot/funsd-entity-linking\n",
            "Link 1432 : https://github.com/xmed-lab/urn\n",
            "Link 1433 : https://github.com/liyidi/mpt\n",
            "Link 1434 : https://github.com/Karel911/TRACER\n",
            "Link 1435 : https://github.com/mikecheninoulu/cgt\n",
            "Link 1436 : https://github.com/PaddlePaddle/PaddleSeg\n",
            "Link 1437 : https://github.com/bcprescott/msds\n",
            "Link 1438 : https://github.com/simonzhou86/evolutionarycomputing\n",
            "Link 1439 : https://github.com/catlab-team/latentcreative\n",
            "Link 1440 : https://bitbucket.org/gramacylab/tricands\n",
            "Link 1441 : https://github.com/ipieter/biased-rulers\n",
            "Link 1442 : https://github.com/pku-dair/hetu\n",
            "Link 1443 : https://github.com/rithram/flynn\n",
            "Link 1444 : https://github.com/nv-tlabs/CLD-SGM\n",
            "Link 1445 : https://github.com/sharathraparthy/polynomial-mixing-times\n",
            "Link 1446 : https://github.com/ddaedalus/tres\n",
            "Link 1447 : https://github.com/e0397123/mdd-eval\n",
            "Link 1448 : https://github.com/jsavelka/statutory_interpretation\n",
            "Link 1449 : https://github.com/fywu85/mampc\n",
            "Link 1450 : https://github.com/jhuapl/scatterbrained\n",
            "Link 1451 : https://github.com/michealcarac/VSLAM-Mapping\n",
            "Link 1452 : https://github.com/Leminhbinh0209/Asynchronous-in-Frequency-of-GAN\n",
            "Link 1453 : https://github.com/lajlksdf/vtl\n",
            "Link 1454 : https://github.com/wjf5203/SeqFormer\n",
            "Link 1455 : https://github.com/elliothe/n3h_core\n",
            "Link 1456 : https://github.com/Antoine0Girardin/Neural-network-for-separability-problem\n",
            "Link 1457 : https://github.com/prashantksharma/CaCD\n",
            "Link 1458 : https://github.com/piluc/jbrain\n",
            "Link 1459 : https://github.com/mobarakol/ST-MTL\n",
            "Link 1460 : https://github.com/by2299/mese\n",
            "Link 1461 : https://github.com/ssl-codelab/uota\n",
            "Link 1462 : https://github.com/lexrosetta/caselaw_functional_segmentation_multilingual\n",
            "Link 1463 : https://github.com/liu-yushan/tlogic\n",
            "Link 1464 : https://github.com/RetroCirce/Zero_Shot_Audio_Source_Separation\n",
            "Link 1465 : https://github.com/allenai/interscript\n",
            "Link 1466 : https://github.com/IVIPLab/FDIWN\n",
            "Link 1467 : https://github.com/xinleihe/gnnstealing\n",
            "Link 1468 : https://github.com/epfl-dlab/genie\n",
            "Link 1469 : https://github.com/wenyyu/Image-Adaptive-YOLO\n",
            "Link 1470 : https://github.com/xiaohua-chen/risda\n",
            "Link 1471 : https://github.com/dingjiansw101/zegformer\n",
            "Link 1472 : https://github.com/hiraaaad/binarynoveltypsotruss\n",
            "Link 1473 : https://github.com/murali1996/el_tod\n",
            "Link 1474 : https://github.com/veronica320/recursive-nps\n",
            "Link 1475 : https://github.com/Arthur151/ROMP\n",
            "Link 1476 : https://github.com/shuweishao/af-sfmlearner\n",
            "Link 1477 : https://github.com/sambklein/funnels_repo\n",
            "Link 1478 : https://github.com/fwilhelmi/blockchain_enabled_federated_learning\n",
            "Link 1479 : https://github.com/kevinhuang8/cem-gd\n",
            "Link 1480 : https://github.com/redrew/cap\n",
            "Link 1481 : https://github.com/didactsorg/dinn\n",
            "Link 1482 : https://github.com/zhanxinrui/hdn\n",
            "Link 1483 : https://github.com/cjw2021/QAHOI\n",
            "Link 1484 : https://github.com/Coder-Yu/QRec\n",
            "Link 1485 : https://github.com/yuliangxiu/icon\n",
            "Link 1486 : https://github.com/jiangyctarheel/undirected-generation\n",
            "Link 1487 : https://github.com/CMU-SAFARI/BLEND\n",
            "Link 1488 : https://github.com/marvinschmitt/ModelMisspecificationBF\n",
            "Link 1489 : https://github.com/jieqin-ai/amr\n",
            "Link 1490 : https://github.com/dipteshkanojia/challengeCognateFF\n",
            "Link 1491 : https://github.com/esgomezm/microscopy-dl-suite-tf\n",
            "Link 1492 : https://github.com/utiasstars/lfgp\n",
            "Link 1493 : https://github.com/enricogiudice/dualPC\n",
            "Link 1494 : https://github.com/nickgkan/beauty_detr\n",
            "Link 1495 : https://github.com/inm-6/bennch\n",
            "Link 1496 : https://github.com/nupurkmr9/vision-aided-gan\n",
            "Link 1497 : https://github.com/decodepfl/deepdiscoph\n",
            "Link 1498 : https://github.com/sustainlab-group/is-count\n",
            "Link 1499 : https://github.com/JerryX1110/Awesome-3D-Anomaly-Detection\n",
            "Link 1500 : https://github.com/ropas/pytea\n",
            "Link 1501 : https://github.com/valterlej/dvcusi\n",
            "Link 1502 : https://github.com/shiming-chen/transzero_pp\n",
            "Link 1503 : https://github.com/ruiliu-code/fsac\n",
            "Link 1504 : https://github.com/damo-cv/motionrgbd\n",
            "Link 1505 : https://github.com/katou2/cstp\n",
            "Link 1506 : https://github.com/bkhmsi/alchemist\n",
            "Link 1507 : https://github.com/hkust-knowcomp/dualmessagepassing\n",
            "Link 1508 : https://github.com/boschresearch/clin_x\n",
            "Link 1509 : https://github.com/google-research-datasets/mave\n",
            "Link 1510 : https://github.com/dthulke/dstc10-track2\n",
            "Link 1511 : https://github.com/KMnP/nn-revisit\n",
            "Link 1512 : https://github.com/thilinicooray/deepgcfx\n",
            "Link 1513 : https://github.com/google-research/l2p\n",
            "Link 1514 : https://github.com/csebuetnlp/crosssum\n",
            "Link 1515 : https://github.com/naserahmadi/tdmatch\n",
            "Link 1516 : https://github.com/llnl/xnas\n",
            "Link 1517 : https://github.com/dtuzi123/expansion-graph-model\n",
            "Link 1518 : https://github.com/LeiWangR/HDG\n",
            "Link 1519 : https://github.com/huangzongheng/mathm\n",
            "Link 1520 : https://github.com/uralik/oversmoothing_rate\n",
            "Link 1521 : https://github.com/yivan-wyygdsg/agmi\n",
            "Link 1522 : https://github.com/AnTao97/LGM\n",
            "Link 1523 : https://github.com/florentdelgrange/vae_mdp\n",
            "Link 1524 : https://github.com/a-shakouri/Prescribed-time-control\n",
            "Link 1525 : https://github.com/naazs03/cgspan\n",
            "Link 1526 : https://github.com/dipteshkanojia/challengeCognateFF\n",
            "Link 1527 : https://github.com/cguangyan-bit/dit\n",
            "Link 1528 : https://github.com/ictmcg/mtm\n",
            "Link 1529 : https://github.com/genforce/volumegan\n",
            "Link 1530 : https://github.com/GeraldHan/GGD\n",
            "Link 1531 : https://github.com/facebookresearch/Mask2Former\n",
            "Link 1532 : https://github.com/salesforce/alpro\n",
            "Link 1533 : https://github.com/scottemmons/rvs\n",
            "Link 1534 : https://github.com/langnatalie/DeepUME\n",
            "Link 1535 : https://github.com/aesrgan/A-ESRGAN\n",
            "Link 1536 : https://github.com/LiangZhang1996/Advanced_XLight\n",
            "Link 1537 : https://github.com/pytorch/fairseq/tree/main/examples/xglm\n",
            "Failed downloading zipfile https://github.com/pytorch/fairseq/tree/main/examples/xglm\n",
            "Link 1538 : https://github.com/netket/netket\n",
            "Link 1539 : https://github.com/XinMa-AI/CANDE2FC\n",
            "Link 1540 : https://github.com/karnwatcharasupat/latte\n",
            "Link 1541 : https://github.com/fidelity/sim2real-docs\n",
            "Link 1542 : https://github.com/jymChen/Diaformer\n",
            "Link 1543 : https://github.com/coleygroup/pyscreener\n",
            "Link 1544 : https://github.com/timojl/clipseg\n",
            "Link 1545 : https://github.com/crazyofapple/at-bmc\n",
            "Link 1546 : https://github.com/jiayuan6/cssr\n",
            "Link 1547 : https://github.com/githublucheng/effects-of-multi-aspect-online-reviews-with-unobserved-confounders\n",
            "Link 1548 : https://github.com/guijinSON/NNHedge\n",
            "Link 1549 : https://github.com/chenyanglei/sfp-wild\n",
            "Link 1550 : https://github.com/anshulbshah/MMCL\n",
            "Link 1551 : https://github.com/microsoft/StyleSwin\n",
            "Link 1552 : https://github.com/huhaigen/adaptively-customizing-activation-functions\n",
            "Link 1553 : https://github.com/Bhavik-Ardeshna/Question-Answering-for-Low-Resource-Languages\n",
            "Link 1554 : https://github.com/openai/glide-text2im\n",
            "Link 1555 : https://github.com/compvis/latent-diffusion\n",
            "Link 1556 : https://github.com/vbrebion/rt_of_low_high_res_event_cameras\n",
            "Link 1557 : https://github.com/automl/transformerscandobayesianinference\n",
            "Link 1558 : https://github.com/valeoai/radial\n",
            "Link 1559 : https://github.com/cmusatyalab/mega-nerf\n",
            "Link 1560 : https://github.com/bmartacho/UniPose\n",
            "Link 1561 : https://github.com/or-toledano/animation-with-keypoint-mask\n",
            "Link 1562 : https://github.com/carluerjb/3d_sift_cuda\n",
            "Link 1563 : https://github.com/nicolasdonati/qmaps\n",
            "Link 1564 : https://github.com/zhuccly/srn\n",
            "Link 1565 : https://github.com/ragmeh11/qu-brats\n",
            "Link 1566 : https://github.com/greaseuniverse/greaseterminator\n",
            "Link 1567 : https://github.com/xingyuzhou989/privatetabularrl\n",
            "Link 1568 : https://github.com/bibikar/feddst\n",
            "Link 1569 : https://github.com/lshowway/d-han\n",
            "Link 1570 : https://github.com/davidkoleczek/human_marl\n",
            "Link 1571 : https://github.com/yusra-alkendi/ed-kogtl\n",
            "Link 1572 : https://github.com/eduardohenriquearnold/fastreg\n",
            "Link 1573 : https://github.com/gajdosech2/bin-detect\n",
            "Link 1574 : https://github.com/spang-lab/TenSIR\n",
            "Link 1575 : https://github.com/tsigalko18/transferability-testing-sdcs\n",
            "Link 1576 : https://github.com/bychao100/towards-image-compression-and-analysis-with-transformers\n",
            "Link 1577 : https://github.com/fenglinglwb/edt\n",
            "Link 1578 : https://github.com/visionml/pytracking\n",
            "Link 1579 : https://github.com/valterlej/zsarcap\n",
            "Link 1580 : https://github.com/facebookresearch/otter\n",
            "Link 1581 : https://github.com/zerovl/zerovl\n",
            "Link 1582 : https://github.com/tbeucler/CBRAIN-CAM\n",
            "Link 1583 : https://github.com/chenjie04/mpm\n",
            "Link 1584 : https://github.com/agarwalshruti15/wtw_project_page\n",
            "Link 1585 : https://github.com/mvcisback/diss\n",
            "Link 1586 : https://github.com/med-air/harmofl\n",
            "Link 1587 : https://github.com/yurilavinas/MOEADr\n",
            "Link 1588 : https://github.com/hazelhkim/ALP\n",
            "Link 1589 : https://github.com/a-shakouri/Prescribed-time-control\n",
            "Link 1590 : https://github.com/webis-de/arxiv21-stereo-scientific-text-reuse\n",
            "Link 1591 : https://github.com/prosysscience/Symmetry_Breaking_with_ILP\n",
            "Link 1592 : https://github.com/iacobo/continual\n",
            "Link 1593 : https://github.com/gewu-lab/csol_tpami2021\n",
            "Link 1594 : https://github.com/zexinyang/AlignTree\n",
            "Link 1595 : https://github.com/ryderling/adversarial-attacks-and-defenses-for-windows-pe-malware-detection\n",
            "Link 1596 : https://github.com/damo-cv/elsa\n",
            "Link 1597 : https://github.com/jackie840129/fedfr\n",
            "Link 1598 : https://github.com/lm095/curriculum-learning-for-safe-mapless-navigation\n",
            "Link 1599 : https://github.com/oyxhust/HAM\n",
            "Link 1600 : https://github.com/svmgrg/alternate_pg\n",
            "Link 1601 : https://github.com/Picsart-AI-Research/SeMask-Segmentation\n",
            "Link 1602 : https://github.com/mchong6/JoJoGAN\n",
            "Link 1603 : https://github.com/jokeryan/post_training\n",
            "Link 1604 : https://github.com/normaluhr/fast_bat\n",
            "Link 1605 : https://github.com/dongxinshuai/rift-neurips2021\n",
            "Link 1606 : https://github.com/nulabtmn/printedbooklayout\n",
            "Link 1607 : https://github.com/ddrrnn123/omni-seg\n",
            "Link 1608 : https://github.com/racai-ai/romanian-distilbert\n",
            "Link 1609 : https://github.com/njzxj/sdfa\n",
            "Link 1610 : https://github.com/tpopordanoska/calibration_and_bias\n",
            "Link 1611 : https://github.com/ruathudo/tfw2v\n",
            "Link 1612 : https://github.com/looperxx/proslu\n",
            "Link 1613 : https://github.com/facebookresearch/banmo\n",
            "Link 1614 : https://github.com/hongwang01/indudonet_plus\n",
            "Link 1615 : https://github.com/lcartwright94/lpdm-sensitivity-emulation\n",
            "Link 1616 : https://github.com/amsmrc1a/age_struct_malaria\n",
            "Link 1617 : https://github.com/ioanacroi/qb-norm\n",
            "Link 1618 : https://github.com/saferoboticslab/safety_rl\n",
            "Link 1619 : https://github.com/pulkitkatdare/offenveval\n",
            "Link 1620 : https://github.com/wcgcyx/sac-cepo\n",
            "Link 1621 : https://github.com/andreea-glavan/multimodal-audiovisual-scene-recognition\n",
            "Link 1622 : https://github.com/duyichao/e2e-st-tda\n",
            "Link 1623 : https://github.com/hcplab-sysu/hcp-mlr-pl\n",
            "Link 1624 : https://github.com/hellopipu/hqs-net\n",
            "Link 1625 : https://github.com/chenglin-yang/lvt\n",
            "Link 1626 : https://github.com/moabarar/qna\n",
            "Link 1627 : https://github.com/yuxiaodonghri/soit\n",
            "Link 1628 : https://github.com/qinliuliuqin/isegformer\n",
            "Link 1629 : https://github.com/royorel/StyleSDF\n",
            "Link 1630 : https://github.com/baidu/DuReader\n",
            "Link 1631 : https://github.com/g-u-n/pycil\n",
            "Link 1632 : https://github.com/uw-ersl/matruss\n",
            "Link 1633 : https://github.com/nlpersecjtu/ldsgm\n",
            "Link 1634 : https://github.com/SJTU-ViSYS/M2DGR\n",
            "Link 1635 : https://github.com/etetteh/OoD_Gen-Chest_Xray\n",
            "Link 1636 : https://github.com/usc-sail/fed-ser-leakage\n",
            "Link 1637 : https://github.com/hedongxiao-tju/BM-GCN\n",
            "Link 1638 : https://github.com/lhan87/hope\n",
            "Link 1639 : https://github.com/jshin49/pwr\n",
            "Link 1640 : https://github.com/harshraj22/vqa\n",
            "Link 1641 : https://github.com/amodas/PRIME-augmentations\n",
            "Link 1642 : https://github.com/Ferhat94/Random-Transformer-Forest\n",
            "Link 1643 : https://github.com/gsh199449/heteroqa\n",
            "Link 1644 : https://github.com/synxlin/deep-gradient-compression\n",
            "Link 1645 : https://github.com/guotaowang/STANet\n",
            "Link 1646 : https://github.com/ucasligang/simvit\n",
            "Link 1647 : https://github.com/wenchima/miti-detr\n",
            "Failed walking repo https://github.com/wenchima/miti-detr\n",
            "Link 1648 : https://github.com/fnzhan/mise\n",
            "Link 1649 : https://github.com/kfeng123/LSA-Matting\n",
            "Link 1650 : https://github.com/biaslab/reactivemp.jl\n",
            "Link 1651 : https://github.com/LanaSina/evolutionary_illusion_generator\n",
            "Link 1652 : https://github.com/nitkannen/backgprop-aaai-22\n",
            "Link 1653 : https://github.com/haibinzheng/neuronfair\n",
            "Link 1654 : https://github.com/sukiboo/hyperpersonalization\n",
            "Link 1655 : https://github.com/brando90/Does-MAML-Only-Work-via-Feature-Re-use-A-Data-Set-Centric-Perspective\n",
            "Link 1656 : https://github.com/superweisp/dhan2022\n",
            "Link 1657 : https://github.com/kamiLight/CADepth-master\n",
            "Link 1658 : https://github.com/nadavrot/pgo_ml\n",
            "Link 1659 : https://github.com/y2863/MetaGraspNet\n",
            "Link 1660 : https://github.com/smatsumori/lattegan\n",
            "Link 1661 : https://github.com/pulse-ml/momfbo-algorithm\n",
            "Link 1662 : https://github.com/leyadev/vocabulary-transfer\n",
            "Link 1663 : https://github.com/pku-dair/hetu\n",
            "Link 1664 : https://github.com/AleksCipri/DeepAdversaries\n",
            "Link 1665 : https://github.com/mandelbrot99/betavae\n",
            "Link 1666 : https://github.com/leaplabthu/adafocusv2\n",
            "Link 1667 : https://github.com/mccaffary/continual-learning\n",
            "Link 1668 : https://github.com/scify/JedAIToolkit\n",
            "Link 1669 : https://github.com/mendelxu/zsseg.baseline\n",
            "Link 1670 : https://github.com/fpe-vtt/ftt-vpe\n",
            "Link 1671 : https://github.com/BR-IDL/PaddleViT\n",
            "Link 1672 : https://github.com/mutalyzer/algebra\n",
            "Link 1673 : https://github.com/farjadmalik/aigoeswild\n",
            "Link 1674 : https://github.com/zcq15/acdnet\n",
            "Link 1675 : https://github.com/boxinz17/fl-client-sampling\n",
            "Link 1676 : https://github.com/law-ai/lesicin\n",
            "Link 1677 : https://github.com/JonasGeiping/breaching\n",
            "Link 1678 : https://github.com/tagperson/tagperson-blender\n",
            "Link 1679 : https://github.com/dengyang17/tois-page\n",
            "Link 1680 : https://github.com/sarahesl/pubmedclip\n",
            "Link 1681 : https://github.com/zhen-tan-dmml/gfcil\n",
            "Link 1682 : https://github.com/yuezhu95/intelligent-traffic-light-via-reinforcement-learning\n",
            "Link 1683 : https://github.com/martkartasev/btbackchainingrl\n",
            "Link 1684 : https://github.com/acoache/rl-dynamicconvexrisk\n",
            "Link 1685 : https://github.com/yuanmingze/360OpticalFlow-TangentImages\n",
            "Link 1686 : https://github.com/nyu-mll/quality\n",
            "Link 1687 : https://github.com/masqm/Faster-Mean-Shift-Euc\n",
            "Link 1688 : https://github.com/kishanpb/offpolicyevaluation_informationborrowing_contextswitching\n",
            "Link 1689 : https://github.com/fdarmon/neuralwarp\n",
            "Link 1690 : https://github.com/gargrohin/amat\n",
            "Link 1691 : https://github.com/beierzhu/xerm\n",
            "Link 1692 : https://github.com/zhichen902/detarnet\n",
            "Link 1693 : https://github.com/SpaceLearner/SessionRec-pytorch\n",
            "Link 1694 : https://github.com/idea-iitd/neurosed\n",
            "Link 1695 : https://github.com/RunpeiDong/DGMS\n",
            "Link 1696 : https://github.com/ashishsalunkhe/DeepSpamReview-Detection-of-Fake-Reviews-on-Online-Review-Platforms-using-DeepLearning-Architectures\n",
            "Link 1697 : https://github.com/xingjian-zhang/fast-partial-ranking-mnl\n",
            "Link 1698 : https://github.com/theengineroom-unige/owloop\n",
            "Link 1699 : https://github.com/felixbenning/masterthesis\n",
            "Link 1700 : https://github.com/KimGroup/QGasML\n",
            "Link 1701 : https://github.com/arzik1987/prelim\n",
            "Link 1702 : https://github.com/inceptioresearch/iits\n",
            "Link 1703 : https://github.com/bit1029public/simsr\n",
            "Link 1704 : https://github.com/ZHEvent/ZHEvent.github.io\n",
            "Link 1705 : https://github.com/StanfordAI4HI/CSRL\n",
            "Link 1706 : https://github.com/yangalan123/anhp-andtt\n",
            "Link 1707 : https://github.com/kpandey008/DiffuseVAE\n",
            "Link 1708 : https://github.com/reeche/planningamount\n",
            "Link 1709 : https://github.com/raysonlaroca/rodosol-alpr-dataset\n",
            "Link 1710 : https://github.com/wjn922/referformer\n",
            "Link 1711 : https://github.com/Ostrzyciel/food-safety-classif\n",
            "Link 1712 : https://github.com/asfeng/kergnns\n",
            "Link 1713 : https://github.com/sarahpratt/introspective\n",
            "Link 1714 : https://github.com/2groza/action-refined-temporal-difference\n",
            "Link 1715 : https://github.com/jadexin/imea\n",
            "Link 1716 : https://github.com/zju-xmech/phaseguidedcontrol\n",
            "Link 1717 : https://github.com/radi-cho/gatedtabtransformer\n",
            "Link 1718 : https://github.com/numenta/htmpapers\n",
            "Link 1719 : https://github.com/mlpeschl/moral_rl\n",
            "Link 1720 : https://github.com/ericotjo001/explainable_ai\n",
            "Link 1721 : https://github.com/ericotjo001/explainable_ai\n",
            "Link 1722 : https://github.com/guanyaoli/st-tis\n",
            "Link 1723 : https://github.com/ericotjo001/explainable_ai\n",
            "Link 1724 : https://github.com/Rorozhl/CA-MKD\n",
            "Link 1725 : https://github.com/BII-wushuang/Lie-Group-Motion-Prediction\n",
            "Link 1726 : https://github.com/joisino/tiara\n",
            "Link 1727 : https://github.com/nrel/phase-space-sampling\n",
            "Link 1728 : https://github.com/nrel/ganisp\n",
            "Link 1729 : https://github.com/tiehangd/ue-eeg\n",
            "Link 1730 : https://github.com/ita9naiwa/rlrd-aaai2022\n",
            "Link 1731 : https://github.com/adeline-cs/GTR\n",
            "Link 1732 : https://github.com/rutgersdm/dkgr\n",
            "Link 1733 : https://github.com/boluoweifenda/WAGE\n",
            "Link 1734 : https://github.com/shiningsunnyday/mcts-chess\n",
            "Link 1735 : https://github.com/ccsi-toolset/deeperfluids\n",
            "Link 1736 : https://github.com/mzeegers/ADJUST\n",
            "Link 1737 : https://github.com/zohreh-aaa/dnn-testing\n",
            "Link 1738 : https://github.com/yatingtian/hateful-meme\n",
            "Link 1739 : https://github.com/qlan3/quantumexplorer\n",
            "Link 1740 : https://github.com/RongrongMa/GLocalKD\n",
            "Link 1741 : https://github.com/simingyan/iae\n",
            "Link 1742 : https://github.com/uam-biometrics/faceqgen\n",
            "Link 1743 : https://github.com/lustoo/fghv\n",
            "Link 1744 : https://github.com/re-owod/re-owod\n",
            "Link 1745 : https://github.com/um-dsp/DP-UTIL\n",
            "Link 1746 : https://github.com/hanielwang/tvnet\n",
            "Link 1747 : https://github.com/ivrl/ccid\n",
            "Link 1748 : https://github.com/omerbt/Splice\n",
            "Link 1749 : https://github.com/jmoraispk/parkourspotid\n",
            "Link 1750 : https://github.com/bitszwang/dpt\n",
            "Link 1751 : https://github.com/sachith500/MaskedFaceRepresentation\n",
            "Link 1752 : https://github.com/dornik/sporeagent\n",
            "Link 1753 : https://github.com/ICMLA-SAFL/SAFL_pytorch\n",
            "Link 1754 : https://github.com/smalvar/covid-clustering\n",
            "Link 1755 : https://github.com/silviacasac/ranking-cnn-neurons\n",
            "Link 1756 : https://github.com/robert-xiaoqiang/ds-net\n",
            "Link 1757 : https://github.com/yexind/dsne\n",
            "Link 1758 : https://github.com/kazuki-irie/fork--wilds-public\n",
            "Link 1759 : https://github.com/layaars/unsupervised-anomaly-detection-with-a-gan-augmented-autoencoder\n",
            "Link 1760 : https://github.com/mjmjeong/InfoNeRF\n",
            "Link 1761 : https://github.com/yangbincheng/emsrdpn\n",
            "Link 1762 : https://github.com/yzhao062/pyod\n",
            "Link 1763 : https://github.com/liuruijin17/clgo\n",
            "Link 1764 : https://github.com/jianqiangh/deconfounded_vg\n",
            "Link 1765 : https://github.com/linchintung/vmt\n",
            "Link 1766 : https://github.com/gykovacs/maweight\n",
            "Link 1767 : https://github.com/fudanvi/benchmarking-chinese-text-recognition\n",
            "Link 1768 : https://github.com/molnsona/blossom\n",
            "Link 1769 : https://github.com/naver-ai/c3-gan\n",
            "Link 1770 : https://github.com/odegeasslbc/FastGAN-pytorch\n",
            "Link 1771 : https://github.com/big-data-lab-umbc/Reproducible_and_portable_app_in_cloud\n",
            "Link 1772 : https://github.com/shabanian2018/age_mri-classification\n",
            "Link 1773 : https://github.com/dinhhieuhoang/dam-ca-infantbrain\n",
            "Link 1774 : https://github.com/phernst/pd-unet\n",
            "Link 1775 : https://github.com/a11to1n3/alerttrap-dataset\n",
            "Link 1776 : https://github.com/mauvilsa/imgtxtenh\n",
            "Link 1777 : https://github.com/limuhit/pseudocylindrical_convolution\n",
            "Link 1778 : https://github.com/chenzhaiyu/points2poly\n",
            "Link 1779 : https://github.com/lucasfidon/trabit_brats2021\n",
            "Link 1780 : https://github.com/pengbinghui/dynamicl2regression\n",
            "Link 1781 : https://github.com/justc2/worst-case-randomly-collected\n",
            "Link 1782 : https://github.com/n-boehmer/combating-collusion-rings-is-hard-but-possible\n",
            "Link 1783 : https://github.com/aimagelab/mammoth\n",
            "Link 1784 : https://github.com/barbara20901/cluster-link-prediction\n",
            "Link 1785 : https://github.com/eragon10/neural_network_training_with_matrix_inequality_constraints\n",
            "Link 1786 : https://github.com/matthnig/replication_inventors_ethnic_origins\n",
            "Link 1787 : https://github.com/krankile/ensemble_forecasting\n",
            "Link 1788 : https://github.com/joaopedromattos/gnee\n",
            "Link 1789 : https://github.com/huaxiuyao/LISA\n",
            "Link 1790 : https://github.com/xushizhou/fair_data_representation\n",
            "Link 1791 : https://github.com/enyandai/rsgnn\n",
            "Link 1792 : https://github.com/vman049/MultilingualLexicalSemantics\n",
            "Link 1793 : https://github.com/kazuki-irie/dct-fast-weights\n",
            "Link 1794 : https://gitlab.com/mipl/carl\n",
            "Failed downloading zipfile https://gitlab.com/mipl/carl\n",
            "Link 1795 : https://github.com/raulastudillo06/bofn\n",
            "Link 1796 : https://github.com/wjmaddox/benchmarking_iterative_gps\n",
            "Link 1797 : https://github.com/zzwaang/audio2midi\n",
            "Link 1798 : https://gitlab.com/blank.user.autofits/autofits\n",
            "Link 1799 : https://github.com/ooub/peregrine\n",
            "Link 1800 : https://github.com/BNN-UPC/GNNPapersCommNets\n",
            "Link 1801 : https://github.com/mlconference/aet-sgd\n",
            "Link 1802 : https://github.com/seanbenhur/multilingual_aggresive_gender_bias_communal_bias_identifcation\n",
            "Link 1803 : https://github.com/trieuntu/conversation_clustering\n",
            "Link 1804 : https://github.com/sebastian-hofstaetter/tripclick\n",
            "Link 1805 : https://github.com/pluslabnlp/zero_shot_cqa\n",
            "Link 1806 : https://github.com/dhfbk/kind\n",
            "Link 1807 : https://github.com/gonenhila/usage_change\n",
            "Link 1808 : https://github.com/victorvikram/conscience-and-coop\n",
            "Link 1809 : https://github.com/HeathCiff/Multi-modal-Attention-Network-for-Stock-Movements-Prediction\n",
            "Link 1810 : https://github.com/Rongjiehuang/Multi-Singer\n",
            "Link 1811 : https://github.com/ideal-idea/sap\n",
            "Link 1812 : https://github.com/whatissimondoing/cog-bart\n",
            "Link 1813 : https://github.com/sonlam1102/job-prediction-multilabel-vietnamese\n",
            "Link 1814 : https://github.com/rlitschk/EncoderCLIR\n",
            "Link 1815 : https://github.com/ljynlp/w2ner\n",
            "Link 1816 : https://github.com/jinpeng01/wgsum\n",
            "Link 1817 : https://github.com/siddhu001/evaluating-explanations\n",
            "Link 1818 : https://github.com/archanray/approximate_similarities\n",
            "Link 1819 : https://github.com/stevenvdeeckt/cgn_cl_dialect\n",
            "Link 1820 : https://github.com/johnnyjana730/hdae\n",
            "Link 1821 : https://github.com/shihui2010/learn_cfg_with_neural_network\n",
            "Link 1822 : https://github.com/dmis-lab/gener\n",
            "Link 1823 : https://github.com/microsoft/unispeech\n",
            "Link 1824 : https://github.com/alessiomarta/simec-1d-test-code\n",
            "Link 1825 : https://github.com/juliareach/aaai22_re\n",
            "Link 1826 : https://github.com/trusthlt/dp-across-nlp-tasks\n",
            "Link 1827 : https://github.com/thorhildurt/dynamic-human-evaluation\n",
            "Link 1828 : https://github.com/honghanhh/ner-combining-contextual-and-global-features\n",
            "Link 1829 : https://github.com/oriram/spider\n",
            "Link 1830 : https://github.com/jonbarron/squareplus\n",
            "Link 1831 : https://github.com/djwhsdj/vw-sdk\n",
            "Link 1832 : https://github.com/nomuramasahir0/xnes-adaptive-lr\n",
            "Link 1833 : https://github.com/biaslab/aida\n",
            "Link 1834 : https://github.com/keawang/importance-weighting-interpolating-classifiers\n",
            "Link 1835 : https://github.com/akoepke/audio-retrieval-benchmark\n",
            "Link 1836 : https://github.com/jacklishufan/main2021\n",
            "Link 1837 : https://github.com/magenta/midi-ddsp\n",
            "Link 1838 : https://github.com/mkomod/survival.svb\n",
            "Link 1839 : https://github.com/wickstrom/relax\n",
            "Link 1840 : https://github.com/amin-nikanjam/silentbugsintensorflowkeras\n",
            "Link 1841 : https://github.com/evidencebp/e2ese\n",
            "Link 1842 : https://github.com/arise-lab/velvet\n",
            "Link 1843 : https://github.com/kimvc7/robustness\n",
            "Link 1844 : https://github.com/cbg-ethz/subgroupseparation\n",
            "Link 1845 : https://github.com/zulucomputer/mes_lstm\n",
            "Link 1846 : https://github.com/kieranjwood/slow-momentum-fast-reversion\n",
            "Link 1847 : https://github.com/songzhm/arbitraryelasticnet\n",
            "Link 1848 : https://github.com/xuehansheng/RepBin\n",
            "Link 1849 : https://gitlab.com/esteban_vargas_bernal/extending-infomap-to-absorbing-random-walks\n",
            "Link 1850 : https://github.com/ijcruic/multi-modal-networks-reveal-patterns-of-operational-similarity-of-terrorist-organizations\n",
            "Link 1851 : https://github.com/hanfengzhai/PIDOC\n",
            "Link 1852 : https://github.com/athindran/ProBF\n",
            "Link 1853 : https://github.com/unisa-acg/discrete-fpd\n",
            "Link 1854 : https://github.com/bensonren/aem_dim_bench\n",
            "Link 1855 : https://github.com/psu-efd/surrogate_modeling_swes\n",
            "Link 1856 : https://github.com/korobilis/hierarchicalbayes\n",
            "Link 1857 : https://github.com/jyhan03/icassp22-dataset\n",
            "Link 1858 : https://github.com/wngh1187/rawnext\n",
            "Link 1859 : https://github.com/ricbl/etsaliencymaps\n",
            "Link 1860 : https://github.com/vemundfredriksen/lungtumormask\n",
            "Link 1861 : https://github.com/makinyilmaz/lhbdc\n",
            "Link 1862 : https://github.com/phamdanglam1986/An-application-demo-of-audio-visual-crowded-scene-classification-\n",
            "Link 1863 : https://github.com/icecherylxuli/tsan\n",
            "Link 1864 : https://github.com/nosemeocurreapodo/GLSL-Elastic-3D-Wavefield-Simulation\n",
            "Link 1865 : https://github.com/yli131/braingrl\n",
            "Link 1866 : https://github.com/Priesemann-Group/covid19_infoXpand_feedbackloop\n",
            "Link 1867 : https://github.com/thisstillwill/qcb455-fall-2021\n",
            "Link 1868 : https://github.com/weilabmsu/antoencoder-v01\n",
            "Link 1869 : https://github.com/erlichlab/perceptual-gambling\n",
            "Link 1870 : https://github.com/mefeng7/Bird_Outages_MA\n",
            "Link 1871 : https://github.com/oneflyli/yifeinonlieardiffusion2021\n",
            "Link 1872 : https://github.com/antoinezambelli/ensemble-clustering\n",
            "Link 1873 : https://github.com/aarandjel/importance-sampling-with-feedforward-networks\n",
            "Link 1874 : https://github.com/haydenbrown/investing\n",
            "Link 1875 : https://github.com/arnabchakrabarti15/lsd-of-hayashi-yoshida-estimator\n",
            "Link 1876 : https://github.com/mostafarezapour/a-machine-learning-analysis-of-the-relationship-between-some-underlying-medical-conditions-and-covid\n",
            "Link 1877 : https://github.com/Equationliu/GA-Attack\n",
            "Link 1878 : https://github.com/mtics/cement_supplementary\n",
            "Link 1879 : https://github.com/TayfunKaraderi/ICPRAI-2022-Visual-Microfossil-Identification-via-Deep-Metric-Learning\n",
            "Link 1880 : https://github.com/easezyc/deep-transfer-learning\n",
            "Link 1881 : https://github.com/easezyc/deep-transfer-learning\n",
            "Link 1882 : https://github.com/kennqiang/mdfend-weibo21\n",
            "Link 1883 : https://github.com/jiangwenj02/curvenet-v1\n",
            "Link 1884 : https://github.com/gourabkumarpatro/FairRec_www_2020\n",
            "Link 1885 : https://github.com/wgcban/changeformer\n",
            "Link 1886 : https://github.com/Project-MONAI/research-contributions/tree/master/SwinUNETR/BRATS21\n",
            "Failed downloading zipfile https://github.com/Project-MONAI/research-contributions/tree/master/SwinUNETR/BRATS21\n",
            "Link 1887 : https://github.com/xinyiying/mocopnet\n",
            "Link 1888 : https://github.com/huawei-noah/CV-backbones\n",
            "Link 1889 : https://github.com/nd-howardgroup/low-power-in-vivo-imaging\n",
            "Link 1890 : https://github.com/hanhanAnderson/LSF-SAC\n",
            "Link 1891 : https://github.com/devinamhn/radiogalaxies-bbb\n",
            "Link 1892 : https://github.com/aberke/lbs-data\n",
            "Link 1893 : https://github.com/kennthshang/cherry\n",
            "Link 1894 : https://github.com/biomolecularphysicsgroup-uncc/machinelearning\n",
            "Link 1895 : https://github.com/princeton-nlp/xtx\n",
            "Link 1896 : https://github.com/ljyflores/fake-news-explainability\n",
            "Link 1897 : https://github.com/jin530/swalk\n",
            "Link 1898 : https://github.com/li-yun/battery_fr_rl\n",
            "Link 1899 : https://github.com/ruixueqingyang/gpoeo\n",
            "Link 1900 : https://github.com/montrealrobotics/iv_rl\n",
            "Link 1901 : https://github.com/kbunte/lvq_toolbox\n",
            "Link 1902 : https://github.com/prstrive/unimvsnet\n",
            "Link 1903 : https://github.com/tinghua-code/ccfi\n",
            "Link 1904 : https://github.com/jaywadekar/scalingrelations_ml\n",
            "Link 1905 : https://github.com/facebookresearch/av_hubert\n",
            "Link 1906 : https://github.com/ds3lab/tableparser\n",
            "Link 1907 : https://github.com/elitap/classimbalance\n",
            "Link 1908 : https://github.com/uniqzheng/hfr-bvqa\n",
            "Link 1909 : https://github.com/gongxinyao/robust-photon-efficient-imaging-using-prsnet\n",
            "Link 1910 : https://github.com/tensorflow/tpu\n",
            "Link 1911 : https://github.com/Yangzhangcst/RGBD-semantic-segmentation\n",
            "Link 1912 : https://github.com/urmagicsmine/cspr\n",
            "Link 1913 : https://github.com/demolakstate/bumble_bees_detection\n",
            "Link 1914 : https://github.com/armlabstanford/densetact\n",
            "Link 1915 : https://github.com/pfrommerd/variational_state_space_models\n",
            "Link 1916 : https://github.com/bamler-lab/understanding-ans\n",
            "Link 1917 : https://github.com/aday651/embed-reg\n",
            "Link 1918 : https://github.com/daochenzha/simtsc\n",
            "Link 1919 : https://github.com/aminesi/federated\n",
            "Link 1920 : https://github.com/Shen-Lab/GraphCL_Automated\n",
            "Link 1921 : https://github.com/luferrer/DCA-PLDA\n",
            "Link 1922 : https://git.skewed.de/count0/graph-tool\n",
            "Link 1923 : https://github.com/sophiaalthammer/parm\n",
            "Link 1924 : https://github.com/ielab/oltr\n",
            "Link 1925 : https://github.com/tsummerslab/risk_bounded_nonlinear_robot_motion_planning\n",
            "Link 1926 : https://github.com/lendiefollett/hybrid-targeting\n",
            "Link 1927 : https://github.com/peisuke/ConstrainedSpectralClustering\n",
            "Link 1928 : https://github.com/amorim-cleison/asl-datasets-gen\n",
            "Link 1929 : https://github.com/hukkelas/full_body_anonymization\n",
            "Link 1930 : https://github.com/tvhahn/weibull-knowledge-informed-ml\n",
            "Link 1931 : https://github.com/maximus-victor/dvip4wi22\n",
            "Link 1932 : https://github.com/zengyi-qin/bcbf\n",
            "Link 1933 : https://github.com/pgruening/bio_inspired_min_nets_improve_the_performance_and_robustness_of_deep_networks\n",
            "Link 1934 : https://github.com/qubvel/segmentation_models\n",
            "Link 1935 : https://github.com/pierlj/aaf_framework\n",
            "Link 1936 : https://github.com/yuanezhou/cbtrans\n",
            "Link 1937 : https://github.com/blakechen97/sasa\n",
            "Link 1938 : https://github.com/dliu5812/ddf\n",
            "Link 1939 : https://github.com/softserveinc-rnd/symmetry-3d-completion\n",
            "Link 1940 : https://github.com/valeoai/poco\n",
            "Link 1941 : https://github.com/facebookresearch/av_hubert\n",
            "Link 1942 : https://github.com/leicheng-no/cdil-cnn\n",
            "Link 1943 : https://github.com/mariaref/nonlinearshallowae\n",
            "Link 1944 : https://github.com/yaochenzhu/deep-deconf\n",
            "Link 1945 : https://github.com/facusapienza21/optimaladj\n",
            "Link 1946 : https://github.com/yuanpeng16/edcr\n",
            "Link 1947 : https://github.com/louzounlab/pygon\n",
            "Link 1948 : https://github.com/pepebonet/contripscore\n",
            "Link 1949 : https://github.com/dmis-lab/bern2\n",
            "Link 1950 : https://github.com/ibm/tslm-discourse-markers\n",
            "Link 1951 : https://github.com/jctian98/e2e_lfmmi\n",
            "Link 1952 : https://github.com/zoeyliu18/orange_chicken\n",
            "Link 1953 : https://github.com/tlacombe/homogeneousurot\n",
            "Link 1954 : https://github.com/alin256/multi-mode-prediction-with-mtp-loss\n",
            "Link 1955 : https://github.com/oliviatessa/mothpruning\n",
            "Link 1956 : https://github.com/sbelharbi/negev\n",
            "Link 1957 : https://github.com/akaxlh/ST-SHN\n",
            "Link 1958 : https://github.com/rosanajurdi/prior-based-losses-for-medical-image-segmentation\n",
            "Link 1959 : https://github.com/znowu/mirror-learning\n",
            "Link 1960 : https://github.com/akaxlh/gnmr\n",
            "Link 1961 : https://github.com/sgvaze/generalized-category-discovery\n",
            "Link 1962 : https://github.com/facebookresearch/Detic\n",
            "Link 1963 : https://github.com/modeltc/eod\n",
            "Link 1964 : https://github.com/taimurhassan/inc-inst-seg\n",
            "Link 1965 : https://github.com/ewellchen/stin\n",
            "Link 1966 : https://github.com/Shanthika/TerrainAuthoring-Pytorch\n",
            "Link 1967 : https://github.com/tsingqguo/efficientderain\n",
            "Link 1968 : https://github.com/vida-nyu/city-surfaces\n",
            "Link 1969 : https://github.com/computer-vision2022/pama\n",
            "Link 1970 : https://github.com/wecarsoniv/augmented-pca\n",
            "Link 1971 : https://github.com/paolopellizzoni/outliersslidingwindows\n",
            "Link 1972 : https://github.com/hltchkust/cantonese-asr\n",
            "Link 1973 : https://github.com/pearce790/mallowsbinomial\n",
            "Link 1974 : https://github.com/ottosven/dffm\n",
            "Link 1975 : https://github.com/tangshitao/quadtreeattention\n",
            "Link 1976 : https://github.com/huawei-noah/CV-backbones\n",
            "Link 1977 : https://github.com/jaewoosong/pocketnn\n",
            "Link 1978 : https://github.com/aypan17/reward-misspecification\n",
            "Link 1979 : https://github.com/tau-nlp/scrolls\n",
            "Link 1980 : https://github.com/txsun1997/black-box-tuning\n",
            "Link 1981 : https://github.com/seyedhosseinzadeh/sucp\n",
            "Link 1982 : https://github.com/ayanglab/swinmr\n",
            "Link 1983 : https://github.com/akaxlh/cranet\n",
            "Link 1984 : https://github.com/sydney-machine-learning/sentimentanalysis_bhagavadgita\n",
            "Link 1985 : https://github.com/blackfeather-wang/GFNet-Pytorch\n",
            "Link 1986 : https://github.com/linh-gist/gazeestimationtx2\n",
            "Link 1987 : https://github.com/yeweiysh/Human_AI_Team\n",
            "Link 1988 : https://github.com/emjun/tisane\n",
            "Link 1989 : https://github.com/uw-madison-lee-lab/inrep\n",
            "Link 1990 : https://github.com/borealisai/staypositive\n",
            "Link 1991 : https://github.com/rucaibox/wsdm2022-c2crs\n",
            "Link 1992 : https://github.com/isl-org/lang-seg\n",
            "Link 1993 : https://github.com/google-research/head2toe\n",
            "Link 1994 : https://github.com/bmhopkinson/hyslam\n",
            "Link 1995 : https://github.com/firesans/nonlatinphotoocr\n",
            "Link 1996 : https://github.com/orion-ai-lab/prototypeinsar\n",
            "Link 1997 : https://github.com/ritikajha/Attribute-prediction-in-masked-facial-images-with-deep-multitask-learning\n",
            "Link 1998 : https://github.com/mrcgndr/plant_extraction_workflow\n",
            "Link 1999 : https://github.com/The-Learning-And-Vision-Atelier-LAVA/PoSFeat\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "linknum = 1000\n",
        "for id, url in zip(processed.index[1000:2000], processed['code'][1000:2000]):\n",
        "    print('Link', linknum, ':', url)\n",
        "    linknum += 1\n",
        "    downloadlink = url\n",
        "    reponame = url.split('/')[-1]\n",
        "    if 'data' in url:\n",
        "        continue\n",
        "    elif 'gitlab' in url:\n",
        "        downloadlink += '/-/archive/master/' + reponame + '.zip'\n",
        "    elif 'github' in url:\n",
        "        downloadlink += '/archive/refs/heads/master.zip'\n",
        "    else:\n",
        "        continue\n",
        "    try:\n",
        "        try:\n",
        "            wget.download(downloadlink)\n",
        "            zipfilename = [x for x in os.listdir() if '.zip' in x][0]\n",
        "            unzipped = zipfilename[:-4]\n",
        "        except:\n",
        "            print('Failed downloading zipfile', url)\n",
        "            continue\n",
        "        try:\n",
        "            with zipfile.ZipFile(zipfilename,\"r\") as zip_ref:\n",
        "                zip_ref.extractall(unzipped)\n",
        "        except:\n",
        "            print('Unzip failed', url)\n",
        "            os.remove(zipfilename)\n",
        "            continue\n",
        "        os.remove(zipfilename)\n",
        "\n",
        "        try:\n",
        "            for root, dirnames, filenames in os.walk(unzipped):\n",
        "                for filename in filenames:\n",
        "                    if filename.endswith(('.py')):\n",
        "                        fp = os.path.join(root, filename)\n",
        "                        text = open(fp, 'r').read()\n",
        "                        datapt = [id, text]\n",
        "                        data.append(datapt)\n",
        "        except:\n",
        "            print('Failed walking repo', url)\n",
        "            shutil.rmtree(unzipped)\n",
        "            continue\n",
        "        shutil.rmtree(unzipped)\n",
        "    except:\n",
        "        print('Unknown error on', url)\n",
        "    # try:\n",
        "    #     wget.download(downloadlink)\n",
        "    #     zipfilename = [x for x in os.listdir() if '.zip' in x][0]\n",
        "    #     unzipped = zipfilename[:-4]\n",
        "    #     with zipfile.ZipFile(zipfilename,\"r\") as zip_ref:\n",
        "    #         zip_ref.extractall(unzipped)\n",
        "    #     os.remove(zipfilename)\n",
        "\n",
        "    #     for root, dirnames, filenames in os.walk(unzipped):\n",
        "    #         for filename in filenames:\n",
        "    #             if filename.endswith(('.py')):\n",
        "    #                 fp = os.path.join(root, filename)\n",
        "    #                 text = open(fp, 'r').read()\n",
        "    #                 datapt = [id, text]\n",
        "    #                 data.append(datapt)\n",
        "\n",
        "    #     shutil.rmtree(unzipped)\n",
        "    # except:\n",
        "    #     print('Failed download ', url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBzJlgRLEyDj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "36a12763-77f9-4d4a-cc98-56024f9c5eea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           id                                               code\n",
              "0  2112.01008  import warnings, time, timeit, sys, os\\nwarnin...\n",
              "1  2112.01008  import os, io\\nfrom tqdm import tqdm\\nimport t...\n",
              "2  2112.01008  import torch as ch\\nimport matplotlib.pyplot a...\n",
              "3  2112.01008  import os\\nfrom pathlib import Path\\nfrom tqdm...\n",
              "4  2112.01008  import torch\\nimport torch as ch\\nimport numpy..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-439cf10f-969c-4ec1-ade0-615c94ae9e6f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2112.01008</td>\n",
              "      <td>import warnings, time, timeit, sys, os\\nwarnin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2112.01008</td>\n",
              "      <td>import os, io\\nfrom tqdm import tqdm\\nimport t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2112.01008</td>\n",
              "      <td>import torch as ch\\nimport matplotlib.pyplot a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2112.01008</td>\n",
              "      <td>import os\\nfrom pathlib import Path\\nfrom tqdm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2112.01008</td>\n",
              "      <td>import torch\\nimport torch as ch\\nimport numpy...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-439cf10f-969c-4ec1-ade0-615c94ae9e6f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-439cf10f-969c-4ec1-ade0-615c94ae9e6f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-439cf10f-969c-4ec1-ade0-615c94ae9e6f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df = pd.DataFrame(data, columns=['id', 'code'])\n",
        "df.to_csv('code1.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOGMRIWmE4w2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "29f47cc2-6110-45a3-8645-c000bcde8990"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_622c021a-4115-443c-9232-1f10b217eba2\", \"code1.csv\", 353056881)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('code1.csv') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "r9bo_ahitHcb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csvs = [f for f in os.listdir() if '.csv' in f]\n",
        "df = pd.read_csv(csvs[0], index_col=0)\n",
        "for csv in csvs[1:]:\n",
        "    df1 = pd.read_csv(csv, index_col=0)\n",
        "    df = df.append(df1)"
      ],
      "metadata": {
        "id": "DW5UNsRLtKe6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('code0.csv')"
      ],
      "metadata": {
        "id": "fmo0_0fw2iUH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "dFyybTYSwCwR",
        "outputId": "b0e99fc2-2aa6-426c-c058-8efc40d699ab"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'import argparse\\nimport model.models as models\\nfrom inference import inference\\nimport ipdb\\n\\nif __name__ == \\'__main__\\':\\n    # Possible models\\n    model_names = sorted(name for name in models.__dict__\\n                         if not name.startswith(\"__\")\\n                         and callable(models.__dict__[name]))\\n\\n    # Parser\\n    parser = argparse.ArgumentParser(description=\\'Pytorch implementation: Object level visual reasoning in videos\\')\\n    parser.add_argument(\\'--arch\\', \\'-a\\', metavar=\\'ARCH\\',\\n                        default=\\'orn_two_heads\\',\\n                        choices=model_names,\\n                        help=\\'model architecture: \\' +\\n                             \\' | \\'.join(model_names) +\\n                             \\' (default: orn_two_heads\\')\\n    parser.add_argument(\\'--depth\\', default=50, type=int,\\n                        metavar=\\'D\\', help=\\'depth of the backbone\\')\\n    parser.add_argument(\\'--dataset\\', metavar=\\'D\\',\\n                        default=\\'vlog\\',\\n                        help=\\'dataset name\\')\\n    parser.add_argument(\\'--train-set\\', metavar=\\'T\\',\\n                        default=\\'train+val\\',\\n                        help=\\'Training set: could be train or train+val when you want to get the final accuracy\\')\\n    parser.add_argument(\\'--root\\', metavar=\\'D\\',\\n                        default=\\'./data/vlog\\',\\n                        help=\\'location of the dataset directory\\')\\n    parser.add_argument(\\'-b\\', \\'--batch-size\\', default=4, type=int,\\n                        metavar=\\'B\\', help=\\'batch size\\')\\n    parser.add_argument(\\'-t\\', \\'--t\\', default=2, type=int,\\n                        metavar=\\'T\\', help=\\'number of timesteps extracted from a video\\')\\n    parser.add_argument(\\'--epochs\\', type=int, default=10, metavar=\\'N\\',\\n                        help=\\'number of epochs to train\\')\\n    parser.add_argument(\\'--nb-crops\\', type=int, default=10, metavar=\\'N\\',\\n                        help=\\'number of crops while testing\\')\\n    parser.add_argument(\\'--lr\\', type=float, default=0.0001, metavar=\\'LR\\',\\n                        help=\\'learning rate\\')\\n    parser.add_argument(\\'-j\\', \\'--workers\\', default=0, type=int, metavar=\\'N\\',\\n                        help=\\'number of data loading workers\\')\\n    parser.add_argument(\\'--print-freq\\', default=1, type=int, metavar=\\'P\\',\\n                        help=\\'frequence of printing in the log\\')\\n    parser.add_argument(\\'--resume\\',\\n                        default=\\'/tmp/my_resume\\',\\n                        # default=\\'./resume/vlog\\',\\n                        type=str, metavar=\\'PATH\\',\\n                        help=\\'path to latest checkpoint\\')\\n    parser.add_argument(\\'-e\\', \\'--evaluate\\', dest=\\'evaluate\\', action=\\'store_true\\',\\n                        help=\\'evaluation mode\\')\\n    parser.add_argument(\\'--cuda\\', dest=\\'cuda\\', action=\\'store_true\\',\\n                        help=\\'cuda mode\\')\\n    parser.add_argument(\\'--add-background\\', dest=\\'add_background\\', action=\\'store_true\\',\\n                        help=\\'add the background as one object\\')\\n    parser.add_argument(\\'--mask-confidence\\', type=float, default=0.50, metavar=\\'LR\\',\\n                        help=\\'mininum confidence for the masks\\')\\n    parser.add_argument(\\'--pooling\\', metavar=\\'POOL\\',\\n                        default=\\'rnn\\',\\n                        help=\\'final pooling methods: avg or rnn\\')\\n    parser.add_argument(\\'--heads\\', metavar=\\'H\\',\\n                        default=\\'object\\',\\n                        # default=\\'object+context\\',\\n                        help=\\'what are the heads of the model: object or context or object+context\\')\\n    parser.add_argument(\\'--blocks\\', metavar=\\'LB\\',\\n                        default=\\'2D_2D_2D_2.5D\\',\\n                        help=\\'Nature of the 4 residual blocks: B1_B2_B3_B4 where Bi can be 2D, 3D or 2.5D\\')\\n    parser.add_argument(\\'--object-head\\', metavar=\\'BH\\',\\n                        default=\\'2D\\',\\n                        help=\\'Nature of teh residual block of the object head: Bi where can be 2D, 3D or 2.5D\\')\\n\\n    # Args\\n    args, _ = parser.parse_known_args()\\n\\n    # Dict\\n    options = vars(args)\\n\\n    inference.main(options)from model.backbone.resnet_based import *\\nfrom model.orn_two_heads.two_heads import *import torch.utils.model_zoo as model_zoo\\nfrom model.backbone.resnet.resnet import model_urls\\nimport ipdb\\nimport torch\\n\\n\\ndef _inflate_weight(w, new_temporal_size, inflation=\\'center\\'):\\n    w_up = w.unsqueeze(2).repeat(1, 1, new_temporal_size, 1, 1)\\n    if inflation == \\'center\\':\\n        w_up = central_inflate_3D_conv(w_up)  # center\\n    elif inflation == \\'mean\\':\\n        w_up /= new_temporal_size  # mean\\n    return w_up\\n\\n\\ndef central_inflate_3D_conv(w):\\n    new_temporal_size = w.size(2)\\n    middle_timestep = int(new_temporal_size / 2.)\\n    before, after = list(range(middle_timestep)), list(range(middle_timestep + 1, new_temporal_size))\\n    if len(before) > 0:\\n        w[:, :, before] = torch.zeros_like(w[:, :, before])\\n    if len(after):\\n        w[:, :, after] = torch.zeros_like(w[:, :, after])\\n    return w\\n\\n\\ndef inflate_temporal_conv(pretrained_W_updated, model_dict, inflation):\\n    for k, v in model_dict.items():\\n        if \\'1t\\' in k:\\n            if \\'conv\\' in k:\\n                if \\'bias\\' in k:\\n                    v_up = torch.zeros_like(v)\\n                elif \\'weight\\' in k:\\n                    v_up = torch.zeros_like(v)\\n\\n                    h, w, T, *_ = v_up.size()\\n                    t_2 = int(T / 2.)\\n                    for i in range(h):\\n                        for j in range(w):\\n                            if i == j:\\n                                if inflation == \\'center\\':\\n                                    v_up[i, j, t_2] = torch.ones_like(v_up[i, j, t_2])\\n                                elif inflation == \\'mean\\':\\n                                    v_up[i, j] = torch.ones_like(v_up[i, j]) / T\\n            # elif \\'bn\\' in k:\\n            #     if \\'running_mean\\' in k:\\n            #         v_up = torch.zeros_like(v)\\n            #     elif \\'running_var\\' in k:\\n            #         v_up = torch.ones_like(v) - 1e-05\\n            #     elif \\'bias\\' in k:\\n            #         v_up = torch.zeros_like(v)\\n            #     elif \\'weight\\' in k:\\n            #         v_up = torch.ones_like(v)\\n\\n                # udpate\\n                pretrained_W_updated.update({k: v_up})\\n    return pretrained_W_updated\\n\\n\\ndef _update_pretrained_weights(model, pretrained_W, inflation=\\'center\\'):\\n    pretrained_W_updated = pretrained_W.copy()\\n    model_dict = model.state_dict()\\n    for k, v in pretrained_W.items():\\n        if \"conv\" in k or (\\'bn\\' in k and \\'1t\\' not in k) or \\'downsample\\' in k:\\n            if k in model_dict.keys():\\n                if len(model_dict[k].shape) == 5:\\n                    new_temporal_size = model_dict[k].size(2)\\n                    v_updated = _inflate_weight(v, new_temporal_size, inflation)\\n                else:\\n                    v_updated = v\\n\\n                if isinstance(v, torch.autograd.Variable):\\n                    pretrained_W_updated.update({k: v_updated.data})\\n                else:\\n                    pretrained_W_updated.update({k: v_updated})\\n        if \"fc.weight\" in k:\\n            pretrained_W_updated.pop(\\'fc.weight\\', None)\\n        if \"fc.bias\" in k:\\n            pretrained_W_updated.pop(\\'fc.bias\\', None)\\n\\n    # update the dict for 1D conv for 2.5D conv\\n    pretrained_W_updated = inflate_temporal_conv(pretrained_W_updated, model_dict, inflation)\\n\\n    # update the state dict\\n    model_dict.update(pretrained_W_updated)\\n\\n    return model_dict\\n\\n\\ndef _keep_only_existing_keys(model, pretrained_weights_inflated):\\n    # Loop over the model_dict and update W\\n    model_dict = model.state_dict()  # Take the initial weights\\n    for k, v in model_dict.items():\\n        if k in pretrained_weights_inflated.keys():\\n            model_dict[k] = pretrained_weights_inflated[k]\\n    return model_dict\\n\\n\\ndef load_pretrained_2D_weights(arch, model, inflation):\\n    pretrained_weights = model_zoo.load_url(model_urls[arch])\\n    pretrained_weights_inflated = _update_pretrained_weights(model, pretrained_weights, inflation)\\n    model.load_state_dict(pretrained_weights_inflated)\\n    print(\"     -> Init: Imagenet - 3D from 2D (inflation = {})\".format(inflation))\\n    return model\\nfrom model.backbone.imagenet_pretraining import load_pretrained_2D_weights\\nfrom model.backbone.resnet.basicblock import BasicBlock2D, BasicBlock3D, BasicBlock2_1D\\nfrom model.backbone.resnet.bottleneck import Bottleneck2D, Bottleneck3D, Bottleneck2_1D\\nfrom model.backbone.resnet.resnet import ResNet\\nimport ipdb\\n\\n__all__ = [\\n    \\'resnet_two_heads\\',\\n]\\n\\n\\ndef resnet_two_heads(options, **kwargs):\\n    \"\"\"Constructs a ResNet-18 model with 2 heads\\n    \"\"\"\\n    # Params\\n    depth, blocks, pooling, object_head = options[\\'depth\\'], \\\\\\n                                          options[\\'blocks\\'], \\\\\\n                                          options[\\'pooling\\'], \\\\\\n                                          options[\\'object_head\\']\\n\\n    # Blocks and layers\\n    list_block, list_layers = get_cnn_features(depth=depth,\\n                                               str_blocks=blocks)\\n    blocks_object_head, _ = get_cnn_features(depth=depth,\\n                                             str_blocks=object_head)\\n\\n    # Model with two heads\\n    model = ResNet(list_block,\\n                   list_layers,\\n                   two_heads=True,\\n                   blocks_2nd_head=blocks_object_head,\\n                   pooling=pooling, **kwargs)\\n\\n    print(\\n        \"*** Backbone: Resnet{} (blocks: {} - pooling: {} - Two heads - blocks 2nd head: {} and fm size 2nd head: {}) ***\".format(\\n            depth,\\n            blocks,\\n            pooling,\\n            object_head,\\n            model.size_fm_2nd_head))\\n\\n    # Pretrained from imagenet weights\\n    model = load_pretrained_2D_weights(\\'resnet{}\\'.format(depth), model, inflation=\\'center\\')\\n\\n    return model\\n\\n\\ndef get_cnn_features(str_blocks=\\'2D_2D_2D_2D\\', depth=18):\\n    # List of blocks\\n    list_block = []\\n\\n    # layers\\n    if depth == 18:\\n        list_layers = [2, 2, 2, 2]\\n        nature_of_block = \\'basic\\'\\n    elif depth == 34:\\n        list_layers = [3, 4, 6, 3]\\n        nature_of_block = \\'basic\\'\\n    elif depth == 50:\\n        list_layers = [3, 4, 6, 3]\\n        nature_of_block = \\'bottleneck\\'\\n    else:\\n        raise NameError\\n\\n    # blocks\\n    if nature_of_block == \\'basic\\':\\n        block_2D, block_3D, block_2_1D = BasicBlock2D, BasicBlock3D, BasicBlock2_1D\\n    elif nature_of_block == \\'bottleneck\\':\\n        block_2D, block_3D, block_2_1D = Bottleneck2D, Bottleneck3D, Bottleneck2_1D\\n    else:\\n        raise NameError\\n\\n    # From string to blocks\\n    list_block_id = str_blocks.split(\\'_\\')\\n\\n    # Catch from the options if exists\\n    for i, str_block in enumerate(list_block_id):\\n        # Block kind\\n        if str_block == \\'2D\\':\\n            list_block.append(block_2D)\\n        elif str_block == \\'2.5D\\':\\n            list_block.append(block_2_1D)\\n        elif str_block == \\'3D\\':\\n            list_block.append(block_3D)\\n        else:\\n            ipdb.set_trace()\\n            raise NameError\\n\\n    return list_block, list_layers\\nimport torch.nn as nn\\nimport ipdb\\n\\n\\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\\n    \"3x3 convolution with padding\"\\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\\n                     padding=1, bias=False, dilation=dilation)\\n\\n\\ndef conv3x3x3(in_planes, out_planes, stride=1, dilation=1):\\n    \"3x3 convolution with padding\"\\n    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride,\\n                     padding=1, bias=False, dilation=(1, dilation, dilation))\\n\\n\\ndef conv1x3x3(in_planes, out_planes, stride=1, dilation=1):\\n    \"3x3 convolution with padding\"\\n    if isinstance(stride, int):\\n        stride_1, stride_2, stride_3 = 1, stride, stride\\n    else:\\n        stride_1, stride_2, stride_3 = 1, stride[1], stride[2]\\n\\n    return nn.Conv3d(in_planes, out_planes, kernel_size=(1, 3, 3),\\n                     stride=(stride_1, stride_2, stride_3),\\n                     padding=(0, 1, 1), bias=False, dilation=(1, dilation, dilation))\\n\\n\\ndef conv1x3x3_conv3x1x1(in_planes, out_planes, stride=1, dilation=1, nb_temporal_conv=3):\\n    \"3x3 convolution with padding\"\\n    if isinstance(stride, int):\\n        stride_2d, stride_1t = (1, stride, stride), (stride, 1, 1)\\n    else:\\n        stride_2d, stride_1t = (1, stride[1], stride[2]), (stride[0], 1, 1)\\n\\n    _2d = nn.Conv3d(in_planes, out_planes, kernel_size=(1, 3, 3), stride=stride_2d,\\n                    padding=(0, 1, 1), bias=False, dilation=dilation)\\n\\n    _1t = nn.Sequential()\\n    for i in range(nb_temporal_conv):\\n        temp_conv = nn.Conv3d(out_planes, out_planes, kernel_size=(3, 1, 1), stride=stride_1t,\\n                              padding=(1, 0, 0), bias=False, dilation=1)\\n        _1t.add_module(\\'temp_conv_{}\\'.format(i), temp_conv)\\n        _1t.add_module((\\'relu_{}\\').format(i), nn.ReLU(inplace=True))\\n\\n    return _2d, _1t\\n\\n\\nclass BasicBlock(nn.Module):\\n    expansion = 1\\n    only_2D = False\\n\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\\n        super(BasicBlock, self).__init__()\\n        self.bn1 = nn.BatchNorm3d(planes)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.bn2 = nn.BatchNorm3d(planes)\\n        self.downsample = downsample\\n        self.stride = stride\\n        self.conv1, self.conv2 = None, None\\n        self.input_dim = 5\\n        self.dilation = dilation\\n\\n    def forward(self, x):\\n        residual = x\\n\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n\\n        if self.downsample is not None:\\n            residual = self.downsample(x)\\n\\n        out += residual\\n        out = self.relu(out)\\n\\n        return out\\n\\n\\nclass BasicBlock3D(BasicBlock):\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, **kwargs):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n        self.conv1 = conv3x3x3(inplanes, planes, stride, dilation)\\n        self.conv2 = conv3x3x3(planes, planes, dilation)\\n\\n\\nclass BasicBlock2D(BasicBlock):\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, **kwargs):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n        # not the same input size here to speed up training\\n        self.bn1 = nn.BatchNorm2d(planes)\\n        self.bn2 = nn.BatchNorm2d(planes)\\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation)\\n        self.conv2 = conv3x3(planes, planes, dilation)\\n        self.input_dim = 4\\n\\n\\nclass BasicBlock2_1D(BasicBlock):\\n    expansion = 1\\n\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, nb_temporal_conv=1):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n        self.conv1, self.conv1_1t = conv1x3x3_conv3x1x1(inplanes, planes, stride, dilation,\\n                                                        nb_temporal_conv=nb_temporal_conv)\\n        self.conv2, self.conv2_1t = conv1x3x3_conv3x1x1(planes, planes, dilation,\\n                                                        nb_temporal_conv=nb_temporal_conv)\\n\\n\\n    def forward(self, x):\\n        residual = x\\n\\n        out = self.conv1(x)  # 2D in space\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        # ipdb.set_trace()\\n        out = self.conv1_1t(out)  # 1D in time + relu after each conv\\n\\n        out = self.conv2(out)  # 2D in space\\n        out = self.bn2(out)\\n\\n        if self.downsample is not None:\\n            residual = self.downsample(x)\\n\\n        out += residual\\n        out = self.relu(out)\\n\\n        out = self.conv2_1t(out)  # 1D in time\\n\\n        return out\\nimport torch.nn as nn\\nimport math\\nfrom model.backbone.resnet.basicblock import BasicBlock2D\\nfrom model.backbone.resnet.bottleneck import Bottleneck2D\\nfrom utils.other import transform_input\\nimport ipdb\\nimport torch\\nfrom utils.meter import *\\n\\nmodel_urls = {\\n    \\'resnet18\\': \\'https://download.pytorch.org/models/resnet18-5c106cde.pth\\',\\n    \\'resnet34\\': \\'https://download.pytorch.org/models/resnet34-333f7ec4.pth\\',\\n    \\'resnet50\\': \\'https://download.pytorch.org/models/resnet50-19c8e357.pth\\',\\n    \\'resnet101\\': \\'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\\',\\n    \\'resnet152\\': \\'https://download.pytorch.org/models/resnet152-b121ed2d.pth\\',\\n}\\n\\nK_1st_CONV = 3\\n\\n\\nclass ResNet(nn.Module):\\n    def __init__(self, blocks, layers, num_classes=1000, str_first_conv=\\'2D\\',\\n                 num_final_fm=4,\\n                 two_heads=False,\\n                 size_fm_2nd_head=7,\\n                 blocks_2nd_head=None,\\n                 pooling=\\'avg\\',\\n                 nb_temporal_conv=1,\\n                 list_stride=[1, 2, 2, 2],\\n                 **kwargs):\\n        self.nb_temporal_conv = nb_temporal_conv\\n        self.size_fm_2nd_head = size_fm_2nd_head\\n        self.two_heads = two_heads\\n        self.inplanes = 64\\n        self.input_dim = 5  # from 5D to 4D tensor if 2D conv\\n        super(ResNet, self).__init__()\\n        self.num_final_fm = num_final_fm\\n        self.time = None\\n        self._first_conv(str_first_conv)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.list_channels = [64, 128, 256, 512]\\n        self.list_inplanes = []\\n        self.list_inplanes.append(self.inplanes)  # store the inplanes after layer1\\n        self.layer1 = self._make_layer(blocks[0], self.list_channels[0], layers[0], stride=list_stride[0])\\n        self.list_inplanes.append(self.inplanes)  # store the inplanes after layer1\\n        self.layer2 = self._make_layer(blocks[1], self.list_channels[1], layers[1], stride=list_stride[1])\\n        self.list_inplanes.append(self.inplanes)  # store the inplanes after layer2\\n        self.layer3 = self._make_layer(blocks[2], self.list_channels[2], layers[2], stride=list_stride[2])\\n        self.list_inplanes.append(self.inplanes)  # store the inplanes after layer3\\n        self.layer4 = self._make_layer(blocks[3], self.list_channels[3], layers[3], stride=list_stride[3])\\n        self.avgpool, self.avgpool_space, self.avgpool_time = None, None, None\\n        self.fc_classifier = nn.Linear(512 * blocks[3].expansion, num_classes)\\n        self.out_dim = 5\\n        self.pooling = pooling\\n\\n        if self.two_heads:\\n            # Stride second head\\n            list_strides_2nd_head = self._get_stride_2nd_head()\\n\\n            # Common block\\n            self.nb_block_common_trunk = 4 - len(blocks_2nd_head)\\n\\n            self.list_layers_bis = []\\n            for i in range(self.nb_block_common_trunk, 4):\\n                # Take the correct inplanes\\n                self.inplanes = self.list_inplanes[i]\\n\\n                # Create the layer\\n                layer = self._make_layer(blocks_2nd_head[i - self.nb_block_common_trunk],\\n                                         self.list_channels[i],\\n                                         layers[i],\\n                                         list_strides_2nd_head[i])\\n                if i == 0:\\n                    self.layer1_bis = layer\\n                    self.list_layers_bis.append(self.layer1_bis)\\n                elif i == 1:\\n                    self.layer2_bis = layer\\n                    self.list_layers_bis.append(self.layer2_bis)\\n                elif i == 2:\\n                    self.layer3_bis = layer\\n                    self.list_layers_bis.append(self.layer3_bis)\\n                elif i == 3:\\n                    self.layer4_bis = layer\\n                    self.list_layers_bis.append(self.layer4_bis)\\n                else:\\n                    raise NameError\\n\\n            # List layers\\n            self.list_layers = [self.layer1, self.layer2, self.layer3, self.layer4]\\n\\n        # Pooling method\\n        if self.pooling == \\'rnn\\':\\n            cnn_features_size = 512 * blocks[3].expansion\\n            hidden_state_size = 256 if cnn_features_size == 512 else 512\\n            self.rnn = nn.GRU(input_size=cnn_features_size,\\n                              hidden_size=hidden_state_size,\\n                              num_layers=1,\\n                              batch_first=True)\\n            self.fc_classifier = nn.Linear(hidden_state_size, num_classes)\\n\\n        # Init of the weights\\n        for m in self.modules():\\n            if isinstance(m, nn.Conv3d) or isinstance(m, nn.Conv2d):\\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\\n                m.weight.data.normal_(0, math.sqrt(2. / n))\\n            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\\n                m.weight.data.fill_(1)\\n                m.bias.data.zero_()\\n\\n    def _get_stride_2nd_head(self):\\n        if self.size_fm_2nd_head == 7:\\n            return [1, 2, 2, 2]\\n        elif self.size_fm_2nd_head == 14:\\n            return [1, 2, 2, 1]\\n        elif self.size_fm_2nd_head == 28:\\n            return [1, 2, 1, 1]\\n\\n    def _first_conv(self, str):\\n        self.conv1_1t = None\\n        self.bn1_1t = None\\n        if str == \\'3D_stabilize\\':\\n            self.conv1 = nn.Conv3d(3, 64, kernel_size=(K_1st_CONV, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3),\\n                                   bias=False)\\n            self.maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\\n            self.bn1 = nn.BatchNorm3d(64)\\n\\n\\n        elif str == \\'2.5D_stabilize\\':\\n            self.conv1 = nn.Conv3d(3, 64, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3),\\n                                   bias=False)\\n            self.conv1_1t = nn.Conv3d(64, 64, kernel_size=(K_1st_CONV, 1, 1), stride=(1, 1, 1),\\n                                      padding=(1, 0, 0),\\n                                      bias=False)\\n            self.bn1_1t = nn.BatchNorm3d(64)\\n            self.maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\\n            self.bn1 = nn.BatchNorm3d(64)\\n\\n        elif str == \\'2D\\':\\n            self.conv1 = nn.Conv2d(3, 64,\\n                                   kernel_size=(7, 7),\\n                                   stride=(2, 2),\\n                                   padding=(3, 3),\\n                                   bias=False)\\n            self.maxpool = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\\n            self.bn1 = nn.BatchNorm2d(64)\\n            self.input_dim = 4\\n\\n        else:\\n            raise NameError\\n\\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\\n        downsample = None\\n\\n        # Upgrade the stride is spatio-temporal kernel\\n        if not (block == BasicBlock2D or block == Bottleneck2D):\\n            stride = (1, stride, stride)\\n\\n        if stride != 1 or self.inplanes != planes * block.expansion:\\n            if block is BasicBlock2D or block is Bottleneck2D:\\n                conv, batchnorm = nn.Conv2d, nn.BatchNorm2d\\n            else:\\n                conv, batchnorm = nn.Conv3d, nn.BatchNorm3d\\n\\n            downsample = nn.Sequential(\\n                conv(self.inplanes, planes * block.expansion,\\n                     kernel_size=1, stride=stride, bias=False, dilation=dilation),\\n                batchnorm(planes * block.expansion),\\n            )\\n\\n        layers = []\\n        layers.append(\\n            block(self.inplanes, planes, stride, downsample, dilation, nb_temporal_conv=self.nb_temporal_conv))\\n        self.inplanes = planes * block.expansion\\n        for i in range(1, blocks):\\n            layers.append(block(self.inplanes, planes, nb_temporal_conv=self.nb_temporal_conv))\\n\\n        return nn.Sequential(*layers)\\n\\n    def get_features_map(self, x, time=None, num=4, out_dim=None):\\n        if out_dim is None:\\n            out_dim = self.out_dim\\n\\n        if self.time is None:\\n            B, C, T, W, H = x.size()\\n            self.time = T\\n\\n        time = self.time\\n\\n        # 5D -> 4D if 2D conv at the beginning\\n        x = transform_input(x, self.input_dim, T=time)\\n\\n        x = self.conv1(x)\\n        x = self.bn1(x)\\n        x = self.relu(x)\\n\\n        if self.conv1_1t is not None:\\n            x = self.conv1_1t(x)\\n            x = self.bn1_1t(x)\\n            x = self.relu(x)\\n\\n        x = self.maxpool(x)\\n\\n        # 1st residual block\\n        if num >= 1:\\n            # ipdb.set_trace()\\n            x = transform_input(x, self.layer1[0].input_dim, T=time)\\n            x = self.layer1(x)\\n\\n        # 2nd residual block\\n        if num >= 2:\\n            x = transform_input(x, self.layer2[0].input_dim, T=time)\\n            x = self.layer2(x)\\n\\n        # 3rd residual block\\n        if num >= 3:\\n            x = transform_input(x, self.layer3[0].input_dim, T=time)\\n            x = self.layer3(x)\\n\\n        # 4th residual block\\n        if num >= 4:\\n            x = transform_input(x, self.layer4[0].input_dim, T=time)\\n            x = self.layer4(x)\\n\\n        return transform_input(x, out_dim, T=time)\\n\\n    def get_two_heads_feature_maps(self, x, T=None, out_dim=None, heads_type=\\'object+context\\'):\\n        x = x[\\'clip\\']  # (B, C, T, W, H)\\n\\n        # Get the before last feature map\\n        x = self.get_features_map(x, T, num=self.nb_block_common_trunk)\\n\\n        # Object head\\n        if \\'object\\' in heads_type:\\n            fm_objects = x\\n            for i in range(len(self.list_layers_bis)):\\n                layer = self.list_layers_bis[i]\\n                fm_objects = transform_input(fm_objects, layer[0].input_dim, T=T)\\n                fm_objects = layer(fm_objects)\\n            fm_objects = transform_input(fm_objects, out_dim, T=T)\\n        else:\\n            fm_objects = None\\n\\n        # Activity head\\n        if \\'context\\' in heads_type:\\n            fm_context = x\\n            for i in range(self.nb_block_common_trunk, 4):\\n                layer = self.list_layers[i]\\n                fm_context = transform_input(fm_context, layer[0].input_dim, T=T)\\n                fm_context = layer(fm_context)\\n            fm_context = transform_input(fm_context, out_dim, T=T)\\n        else:\\n            fm_context = None\\n\\n        return fm_context, fm_objects\\n\\n    def forward(self, x):\\n        x = x[\\'clip\\']\\n\\n        x = self.get_features_map(x, num=self.num_final_fm)\\n\\n        # Global average pooling\\n        if self.pooling == \\'avg\\':\\n            self.avgpool = nn.AvgPool3d((x.size(2), x.size(-1), x.size(-1))) if self.avgpool is None else self.avgpool\\n            x = self.avgpool(x)\\n        elif self.pooling == \\'rnn\\':\\n            self.avgpool_space = nn.AvgPool3d(\\n                (1, x.size(-1), x.size(-1))) if self.avgpool_space is None else self.avgpool_space\\n            x = self.avgpool_space(x)\\n            x = x.view(x.size(0), x.size(1), x.size(2))  # (B,D,T)\\n            x = x.transpose(1, 2)  # (B,T,D)\\n            ipdb.set_trace()\\n            x, _ = self.rnn(x)  # (B,T,D/2)\\n            x = torch.mean(x, 1)  # (B,D/2)\\n\\n        # Final classif\\n        x = x.view(x.size(0), -1)\\n        x = self.fc_classifier(x)\\n\\n        return x\\nimport torch.nn as nn\\nimport ipdb\\n\\n\\nclass Bottleneck(nn.Module):\\n    expansion = 4\\n    only_2D = False\\n\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\\n        super(Bottleneck, self).__init__()\\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\\n        self.bn1 = nn.BatchNorm3d(planes)\\n        self.conv2 = None\\n        self.bn2 = nn.BatchNorm3d(planes)\\n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\\n        self.bn3 = nn.BatchNorm3d(planes * 4)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.downsample = downsample\\n        self.stride = stride\\n        self.input_dim = 5\\n        self.dilation = dilation\\n\\n    def forward(self, x):\\n        residual = x\\n\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n        out = self.relu(out)\\n\\n        out = self.conv3(out)\\n        out = self.bn3(out)\\n\\n        if self.downsample is not None:\\n            residual = self.downsample(x)\\n\\n        out += residual\\n        out = self.relu(out)\\n\\n        return out\\n\\n\\nclass Bottleneck3D(Bottleneck):\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, **kwargs):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=stride,\\n                               padding=1, bias=False, dilation=(1, dilation, dilation))\\n\\n\\nclass Bottleneck2D(Bottleneck):\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, **kwargs):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n        # to speed up the inference process\\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\\n        self.bn1 = nn.BatchNorm2d(planes)\\n        self.bn2 = nn.BatchNorm2d(planes)\\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False, dilation=dilation)\\n        self.bn3 = nn.BatchNorm2d(planes * 4)\\n        self.input_dim = 4\\n\\n        if isinstance(stride, int):\\n            stride_1, stride_2 = stride, stride\\n        else:\\n            stride_1, stride_2 = stride[0], stride[1]\\n\\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=(3, 3), stride=(stride_1, stride_2),\\n                               padding=(1, 1), bias=False)\\n\\n\\nclass Bottleneck2_1D(Bottleneck):\\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, nb_temporal_conv=1):\\n        super().__init__(inplanes, planes, stride, downsample, dilation)\\n\\n        if isinstance(stride, int):\\n            stride_2d, stride_1t = (1, stride, stride), (stride, 1, 1)\\n        else:\\n            stride_2d, stride_1t = (1, stride[1], stride[2]), (stride[0], 1, 1)\\n\\n        # CONV2\\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=(1, 3, 3), stride=stride_2d,\\n                               padding=(0, dilation, dilation), bias=False, dilation=dilation)\\n\\n        self.conv2_1t = nn.Sequential()\\n        for i in range(nb_temporal_conv):\\n            temp_conv = nn.Conv3d(planes, planes, kernel_size=(3, 1, 1), stride=stride_1t,\\n                                  padding=(1, 0, 0), bias=False, dilation=1)\\n            self.conv2_1t.add_module(\\'temp_conv_{}\\'.format(i), temp_conv)\\n            self.conv2_1t.add_module((\\'relu_{}\\').format(i), nn.ReLU(inplace=True))\\n\\n\\n    def forward(self, x):\\n        residual = x\\n\\n        ## CONV1 - 3D (1,1,1)\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu(out)\\n\\n        ## CONV2\\n        #  Spatial - 2D (1,3,3)\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n        out = self.relu(out)\\n\\n        # Temporal - 3D (3,1,1)\\n        out = self.conv2_1t(out)\\n\\n        ## CONV3 - 3D (1,1,1)\\n        out = self.conv3(out)\\n        out = self.bn3(out)\\n\\n        if self.downsample is not None:\\n            residual = self.downsample(x)\\n\\n        out += residual\\n        out = self.relu(out)\\n\\n        return out\\nfrom __future__ import print_function\\nimport torch.nn as nn\\n\\n\\nclass Classifier(nn.Module):\\n    def __init__(self, size_input, size_output):\\n        super(Classifier, self).__init__()\\n        # Basic settings\\n        self.size_input = size_input\\n        self.size_output= size_output\\n\\n        # FC layer\\n        self.fc = nn.Linear(self.size_input, self.size_output)\\n\\n    def forward(self, x):\\n\\n        preds = self.fc(x)\\n\\n        return predsimport torch\\nimport torch.nn as nn\\nfrom torch.autograd import Variable\\nimport torch.nn.functional as F\\nimport ipdb\\n\\n\\nclass EncoderMLP(nn.Module):\\n    def __init__(self, input_size=149, list_hidden_size=[100, 100], relu_activation=True, p_dropout=0.5):\\n        super(EncoderMLP, self).__init__()\\n        self.input_size = input_size\\n        self.list_hidden_size = list_hidden_size\\n\\n        # Encoder\\n        self.encoder = nn.Sequential()\\n        current_input_size = self.input_size\\n        for i, hidden_size in enumerate(self.list_hidden_size):\\n            # Add the linear layer\\n            self.encoder.add_module(\\'linear_{}\\'.format(i), nn.Linear(current_input_size, hidden_size))\\n            self.encoder.add_module(\\'dropout_{}\\'.format(i), nn.Dropout(p=p_dropout))\\n            current_input_size = hidden_size\\n            # Add ReLu activation\\n            self.encoder.add_module(\\'relu_{}\\'.format(i), nn.ReLU())\\n\\n    def forward(self, x):\\n        size_x = x.size()\\n\\n        # Transform to get the corresponding input vector size\\n        if size_x[-1] == self.input_size:\\n            x_input = x\\n        elif size_x[-1] * size_x[-2] == self.input_size:\\n            if len(size_x) == 5:\\n                B, T, K, W, H = size_x\\n            elif len(size_x) == 4:\\n                B, T, K, W = size_x\\n                H = 1\\n            x = x.contiguous()\\n            x_input = x.view(B, T, K, W * H)\\n        else:\\n            raise Exception\\n\\n        # Encoder\\n        z = self.encoder(x_input)\\n\\n        return zimport torch\\nimport torch.nn as nn\\nfrom torch.autograd import Variable\\nimport torch.nn.functional as F\\nimport random\\nimport ipdb\\n\\nclass AggregationRelations(nn.Module):\\n    def __init__(self):\\n        super(AggregationRelations, self).__init__()\\n\\n    def forward(self, relational_reasoning_vector):\\n        # Basic summation\\n        B, T, K2, _ = relational_reasoning_vector.size()\\n\\n        output = torch.sum(relational_reasoning_vector, 2)\\n\\n\\n        return output  # (B,T-1,512)from __future__ import print_function\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torchvision import models\\nfrom torch.autograd import Variable\\nfrom model.backbone.resnet_based import resnet_two_heads\\nimport numpy as np\\nimport random\\nfrom model.orn_two_heads.encoder import EncoderMLP\\nfrom model.orn_two_heads.classifier import Classifier\\nfrom model.backbone.resnet.bottleneck import Bottleneck2D, Bottleneck3D, Bottleneck2_1D\\nfrom model.backbone.resnet.basicblock import BasicBlock2D, BasicBlock3D, BasicBlock2_1D\\nfrom model.orn_two_heads.aggregation_relations import AggregationRelations\\nimport math\\nfrom model.orn_two_heads.orn import ObjectRelationNetwork\\nimport ipdb\\n\\n__all__ = [\\n    \\'orn_two_heads\\',\\n]\\n\\n\\nclass TwoHeads(nn.Module):\\n    def __init__(self, num_classes=174, cnn=None,\\n                 features_size=512, time=8, mask_size=28, size_RN=256, nb_head=2,\\n                 logits_type=\\'object\\', f_orn=True,\\n                 size_2nd_head=14,\\n                 **kwargs):\\n        super(TwoHeads, self).__init__()\\n        # Basic settings\\n        self.num_classes = num_classes\\n        self.time = time\\n        self.size_cnn_features = features_size\\n        self.size_mask = mask_size\\n        self.size_RN = size_RN\\n        self.size_RNN = size_RN\\n        self.nb_head = nb_head\\n        self.logits_type = logits_type\\n        self.size_fm_2nd_head = size_2nd_head\\n\\n        # CNN: 4 first conv are shared and the 5th blocks is split\\n        self.cnn = cnn\\n        self.cnn.out_dim = 5\\n\\n        # # If object head only then freeze the cnn because it has already trained for the object recognition task\\n        if self.logits_type == \\'object\\':\\n            # freeze the 2 first blocks and the object head\\n            for i, child in enumerate(self.cnn.children()):\\n                if i != 7:\\n                    for param in child.parameters():\\n                        param.requires_grad = False\\n\\n        # Average Pooling\\n        self.avgpool_TxMxM = nn.AvgPool3d((self.time, self.size_fm_2nd_head, self.size_fm_2nd_head))\\n        self.avgpool_1xMxM = nn.AvgPool3d((1, self.size_fm_2nd_head, self.size_fm_2nd_head))\\n        self.avgpool_1_7x7 = nn.AvgPool3d((1, 7, 7))  # for pooling features from the context head\\n        self.avgpool_T_7x7 = nn.AvgPool3d((self.time, 7, 7))  # for pooling features from the context head\\n        self.pixel_pooler = nn.AvgPool3d((1, 2, 2)) if self.size_fm_2nd_head == 14 else  nn.AvgPool3d(\\n            (1, 4, 4))  # from 14x14 to 7x7\\n\\n        # Max pooling for the ORN module\\n        self.pool_orn = nn.MaxPool2d((self.time - 1, 1))\\n\\n        # COCO object features\\n        self.size_COCO_object_features = self.size_cnn_features\\n\\n        # Prediction of the class of each detected COCO objects (MLP from the pooled features)\\n        self.COCO_Object_Class_from_Features = Classifier(\\n            size_input=self.size_cnn_features,\\n            size_output=81)\\n\\n        # Embedding of the binary mask by AutoEncoder\\n        # Goal -> find the latent space of the shape and location of the objects\\n        self.size_mask_embedding = 100\\n        self.Encoder_Binary_Mask = EncoderMLP(input_size=self.size_mask * self.size_mask,\\n                                              list_hidden_size=[self.size_mask_embedding, self.size_mask_embedding])\\n\\n        # Embedding of the object id\\n        # Goal -> find the latent space of object id better than just a one hot vector\\n        self.size_obj_embedding = 100\\n        input_size = 81\\n        self.Encoder_COCO_Obj_Class = EncoderMLP(input_size=input_size,\\n                                                 list_hidden_size=[self.size_obj_embedding, self.size_obj_embedding])\\n\\n        # Object Relational Network (ORN) between coco or pixel objects\\n        size_object = self.size_COCO_object_features + self.size_mask_embedding + self.size_obj_embedding\\n\\n        # ORN\\n        list_size_hidden_layers = [self.size_RN, self.size_RN, self.size_RN]\\n        self.ORN = ObjectRelationNetwork(size_object=size_object,\\n                                         list_hidden_layers_size=list_size_hidden_layers\\n                                         )\\n        # Aggregation over the\\n        self.AggregationRelations = AggregationRelations()\\n\\n        # RNN or AVG\\n        self.f_orn = f_orn\\n        # RNN Object\\n        if self.f_orn == \\'rnn\\':\\n            input_size = self.size_RN\\n            self.size_relation_features = int(self.size_RN / 2.)\\n            self.rnn_objects = nn.GRU(input_size=input_size,\\n                                      hidden_size=self.size_relation_features,\\n                                      num_layers=1,\\n                                      batch_first=True)\\n        else:\\n            self.size_relation_features = self.size_RN\\n\\n        ## Final classification\\n        self.fc_classifier_object = nn.Linear(self.size_relation_features, self.num_classes)\\n        self.fc_classifier_context = nn.Linear(self.size_cnn_features, self.num_classes)\\n\\n    def get_objects_features(self, fm, masks):\\n        # Upsample the features maps to get better precision!\\n        # fm_old = fm\\n        fm = fm.transpose(1, 2)  # (B, T, D, 14, 14)\\n        B, T, D, W, H = fm.size()\\n        fm = fm.view(B * T, D, W, H)\\n        fm_up = F.upsample(fm, size=(self.size_mask, self.size_mask), mode=\\'bilinear\\', align_corners=True)\\n        fm_up = fm_up.view(B, T, D, self.size_mask, self.size_mask)\\n        fm_up = fm_up.transpose(1, 2)  # (B, D, T, 28, 28)\\n\\n        # B and K\\n        B, D, T, W, _ = fm_up.size()\\n        K = masks.size(2)\\n        W_masks = masks.size(-1)\\n\\n        # Make it as the same size and do Hadamard product\\n        fm_plus = fm_up.unsqueeze(1)  # (B,1,D,T,7,7)\\n        masks_plus = masks.transpose(1, 2)\\n        masks_plus = masks_plus.unsqueeze(2)  # (B,K,1,T,7,7)\\n        fm_masked = fm_plus * masks_plus\\n\\n        # Area of the objects in the \"image\"\\n        masks_size = torch.sum(torch.sum(masks_plus, -1), -1)  # (B,K,1,T)\\n\\n        list_object_set = []\\n        for t in range(T):\\n            # Pool the objects features\\n            object_set_avg = torch.sum(torch.sum(fm_masked[:, :, :, t], -1), -1)\\n            object_set_avg /= (masks_size[:, :, :, t] + 1e-4)\\n\\n            # Append\\n            list_object_set.append(object_set_avg)\\n\\n        # Stack\\n        objects_features = torch.stack(list_object_set, 1)  # (B,T,K,D)\\n\\n        return objects_features\\n\\n    def get_pixel_features(self, fm):\\n        *_, W, H = fm.size()\\n        fm = fm.transpose(1, 2)\\n        list_pixel_features = []\\n\\n        if W != 7:\\n            fm = self.pixel_pooler(fm)\\n            *_, W, H = fm.size()\\n\\n        for i in range(W):\\n            for j in range(H):\\n                list_pixel_features.append(fm[:, :, :, i, j])\\n        pixel_features = torch.stack(list_pixel_features, 2)\\n\\n        return pixel_features\\n\\n    def retrieve_relations(self, relational_reasoning_vector_COCO, obj_id):\\n        B, T, K, C = obj_id.size()\\n        B, T_1, K2, D = relational_reasoning_vector_COCO.size()\\n\\n        relations = np.zeros((B, 81, 81))\\n        for b in range(B):\\n            for t in range(T_1):\\n                for k2 in range(K2):\\n                    try:\\n                        # k2-th interaction\\n                        inter = torch.sum(relational_reasoning_vector_COCO[b, t, k2])\\n\\n                        ## Find the corresponding objects\\n                        # k_1 object of previous timestep\\n                        k_1 = math.floor(float(k2) / float(K))\\n                        obj_id_k_1 = self.get_id_object(obj_id[b, t, k_1].data.cpu().numpy())\\n                        # k object of current timestep\\n                        k = k2 - k_1 * K\\n                        obj_id_k = self.get_id_object(obj_id[b, t + 1, k].data.cpu().numpy())\\n\\n                        # Add the relation\\n                        relations[b, obj_id_k_1, obj_id_k] = inter  # matrix but we fill only half of it (the triangle)\\n                    except:\\n                        pass\\n                        # import ipdb\\n                        # ipdb.set_trace()\\n        return relations  # (B, 81, 81)\\n\\n    def retrieve_relations_temporal(self, relational_reasoning_vector_COCO, obj_id):\\n        B, T, K, C = obj_id.size()\\n        B, T_1, K2, D = relational_reasoning_vector_COCO.size()\\n\\n        relations = np.zeros((B, T_1, 81, 81))\\n        for b in range(B):\\n            for t in range(T_1):\\n                for k2 in range(K2):\\n                    try:\\n                        # k2-th interaction\\n                        inter = torch.sum(relational_reasoning_vector_COCO[b, t, k2])\\n\\n                        ## Find the corresponding objects\\n                        # k_1 object of previous timestep\\n                        k_1 = math.floor(float(k2) / float(K))\\n                        obj_id_k_1 = self.get_id_object(obj_id[b, t, k_1].data.cpu().numpy())\\n                        # k object of current timestep\\n                        k = k2 - k_1 * K\\n                        obj_id_k = self.get_id_object(obj_id[b, t + 1, k].data.cpu().numpy())\\n\\n                        # Add the relation\\n                        relations[\\n                            b, t, obj_id_k_1, obj_id_k] = inter  # matrix but we fill only half of it (the triangle)\\n                    except:\\n                        import ipdb\\n                        pass\\n                        # ipdb.set_trace()\\n        return relations  # (B, T-1, 81, 81)\\n\\n    @staticmethod\\n    def get_id_object(one_hot):\\n        if one_hot[0] == 1:\\n            return 0\\n        else:\\n            return np.argmax(one_hot)\\n\\n    def context_head(self, fm_context, B):\\n        # 3D GAP\\n        context_vector = self.avgpool_T_7x7(fm_context)\\n        context_representation = context_vector.view(B, self.size_cnn_features)\\n\\n        return context_representation\\n\\n    def object_head(self, fm_objects, masks, obj_id, B):\\n        # Retrieve the feature vector associated to each detected COCO object\\n        objects_features = self.get_objects_features(fm_objects, masks)\\n\\n        # Classify each detected objects to make sure we extract good object descriptors\\n        preds_class_detected_objects = self.COCO_Object_Class_from_Features(objects_features)\\n\\n        # Reconstruct the binary masks to find the correct embedding (i.e. shape and location of the detected objects)\\n        embedding_objects_location = self.Encoder_Binary_Mask(masks)\\n\\n        # Reconstruct the COCO class id given the one hot vector\\n        embedding_obj_id = self.Encoder_COCO_Obj_Class(obj_id)\\n\\n        # Full objects description\\n        full_objects = torch.cat([objects_features, embedding_objects_location, embedding_obj_id],\\n                                 -1)  # (B,T,K,object_size)\\n\\n        # Run the Relational Reasoning over the different set of COCO objects\\n        D = self.size_cnn_features  # 512\\n        all_e, all_is_obj = self.ORN(full_objects, D)  # [B, T-1, K*K, D]\\n\\n        # Get only interactions where at least one obj is involved (for COCO only)\\n        all_is_obj = all_is_obj.unsqueeze(-1)\\n        all_e *= all_is_obj\\n\\n        # Aggregation of the COCO relations\\n        orn_aggregated = self.AggregationRelations(all_e)\\n\\n        if self.f_orn == \\'rnn\\':\\n            # self.rnn_objects.flatten_parameters() # does not work for multi-GPU ---> bug https://github.com/pytorch/pytorch/issues/7092\\n            object_representation, _ = self.rnn_objects(orn_aggregated)\\n            # self.rnn_objects.flatten_parameters()\\n            object_representation = torch.mean(object_representation,\\n                                               1)  # TODO look at the two lines above for a better pooling over time!\\n            # object_representation = self.pool_orn(object_representation) # TODO interesting point for pooling over the hidden states\\n        elif self.f_orn == \\'avg\\':\\n            object_representation = torch.mean(orn_aggregated, 1)\\n        else:\\n            raise Exception\\n\\n        return object_representation, preds_class_detected_objects\\n\\n    def add(self, logits, logits_head):\\n        if logits is None:\\n            return logits_head\\n        else:\\n            return logits + logits_head\\n\\n    def squeeze_masks(self, max_nb_obj, masks, obj_id, bbox):\\n        # Max number of objects\\n        nb_max_obj_in_B = int(torch.max(max_nb_obj).cpu().numpy())\\n\\n        # Squeeze\\n        masks_squeezed = masks[:, :, :nb_max_obj_in_B]\\n        obj_id_squeezed = obj_id[:, :, :nb_max_obj_in_B]\\n        bbox_squeezed = bbox[:, :, :nb_max_obj_in_B]\\n\\n        return masks_squeezed, obj_id_squeezed, bbox_squeezed\\n\\n    def final_classification(self, context_representation, object_representation):\\n        if context_representation is not None:\\n            return self.fc_classifier_object(object_representation) + self.fc_classifier_context(context_representation)\\n        else:\\n            return self.fc_classifier_object(object_representation)\\n\\n    def forward(self, x):\\n        \"\"\"Forward pass from a tensor of size (B,C,T,W,H)\"\"\"\\n        clip, masks, obj_id, bbox, max_nb_obj = x[\\'clip\\'], x[\\'mask\\'], x[\\'obj_id\\'], x[\\'obj_bbox\\'], x[\\'max_nb_obj\\']\\n\\n        # Get only the real detected objects\\n        masks, obj_id, bbox = self.squeeze_masks(max_nb_obj, masks, obj_id, bbox)\\n\\n        # Get the batch size and the temporal dimension\\n        B = clip.size(0)  # batch size\\n        T = clip.size(2)  # number of timesteps in the sequence\\n        K = masks.size(2)  # number of objects\\n\\n        # Get the two feature maps: context and object reasoning\\n        fm_context, fm_objects = self.cnn.get_two_heads_feature_maps(x, T=T, out_dim=5,\\n                                                                     heads_type=self.logits_type)\\n\\n        # Init returned variable\\n        context_representation, object_representation, preds_class_detected_objects = None, None, None\\n\\n        # HEADS\\n        if \\'object\\' in self.logits_type:\\n            object_representation, preds_class_detected_objects = self.object_head(fm_objects, masks, obj_id, B=B)\\n\\n        if \\'context\\' in self.logits_type:\\n            context_representation = self.context_head(fm_context, B=B)\\n\\n        # Final classification\\n        logits = self.final_classification(context_representation, object_representation)\\n\\n        return logits, preds_class_detected_objects\\n\\n\\ndef orn_two_heads(options, **kwargs):\\n    \"\"\"Constructs a ResNet-18 model.\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n    \"\"\"\\n\\n    # Settings\\n    depth, pooling, heads, mask_size, size_2nd_head, time = options[\\'depth\\'], \\\\\\n                                                            options[\\'pooling\\'], \\\\\\n                                                            options[\\'heads\\'], \\\\\\n                                                            14, \\\\\\n                                                            14, \\\\\\n                                                            options[\\'t\\']\\n\\n    print(\"* TWO-HEADS => Object type: {} , F: {}, Heads: {}\".format(\\'coco\\', pooling, heads))\\n\\n    # CNN\\n    cnn = resnet_two_heads(options, **kwargs)\\n\\n    # Features dim\\n    features_size = 2048 if depth > 34 else 512\\n    size_RN = 512 if depth > 34 else 256\\n\\n    # Model\\n    model = TwoHeads(\\n        cnn=cnn,\\n        features_size=features_size,\\n        size_RN=size_RN,\\n        logits_type=heads,\\n        f_orn=pooling,\\n        mask_size=28,\\n        size_2nd_head=14,\\n        time=time,\\n        **kwargs)\\n\\n    return model\\nimport torch\\nimport torch.nn as nn\\nimport random\\nfrom model.orn_two_heads.encoder import EncoderMLP\\nimport ipdb\\n\\n\\nclass ObjectRelationNetwork(nn.Module):\\n    def __init__(self, size_object, list_hidden_layers_size, relation_type=\\'pairwise-inter\\'):\\n        super(ObjectRelationNetwork, self).__init__()\\n        # Basic Settings\\n        self.size_object = size_object\\n        self.list_hidden_layers_size = list_hidden_layers_size\\n        self.relation_type = relation_type\\n\\n        self.nb_obj = 2\\n\\n        # MLP for inferring spatio-temporal relations\\n        self.mlp_inter = EncoderMLP(input_size=self.nb_obj * self.size_object,\\n                                        list_hidden_size=self.list_hidden_layers_size\\n                                        )\\n\\n    @staticmethod\\n    def create_inter_object_cat(O_1, O_2):\\n        list_input_mlp, input_mlp = [], None\\n        K = O_1.size(1)\\n        for k in range(K):\\n            O_1_k = O_2[:, k].unsqueeze(1).repeat(1, K, 1)\\n            O_1_k_input_relation = torch.cat([O_1_k, O_2], dim=2)\\n            list_input_mlp.append(O_1_k_input_relation)\\n        # Cat\\n        input_mlp = torch.cat(list_input_mlp, 1)  # (B, K^2, 2*|O|)\\n        return input_mlp\\n\\n    @staticmethod\\n    def create_triwise_interactions_input(O_1, O_2):\\n        list_input_mlp, input_mlp = [], None\\n        K = O_1.size(1)\\n        for k1 in range(K):\\n            O_1_k_1 = O_2[:, k1].unsqueeze(1).repeat(1, K, 1)\\n            list_other_k = [x for x in range(K) if x != k1]\\n            for k2 in list_other_k:\\n                O_1_k_2 = O_2[:, k2].unsqueeze(1).repeat(1, K, 1)\\n                O_1_k_input_relation = torch.cat([O_1_k_1, O_1_k_2, O_2], dim=2)\\n                list_input_mlp.append(O_1_k_input_relation)\\n        # Cat\\n        input_mlp = torch.cat(list_input_mlp, 1)  # (B, K^2, 2*|O|)\\n        return input_mlp\\n\\n    def create_input_mlp(self, O_t_1, O_t, D):\\n        K = O_t.size(1)\\n\\n        # Input\\n        input_mlp = self.create_inter_object_cat(O_t_1, O_t)\\n\\n        # Check if at least an object is involved in the input\\n        is_first_obj = torch.clamp(torch.sum(input_mlp[:, :, :D], -1), 0, 1)\\n        is_second_obj = torch.clamp(torch.sum(input_mlp[:, :, D:], -1), 0, 1)\\n        is_objects = is_first_obj * is_second_obj\\n\\n        return input_mlp, is_objects\\n\\n    def compute_O_O_interaction(self, sets_of_objects, t, previous_T, D, sampling=False):\\n\\n        # Object set (the reference one)\\n        O_t = sets_of_objects[:, t]\\n\\n        list_e_inter, list_is_object_inter = [], []\\n        for t_1 in previous_T:\\n            # Get the previous object set\\n            O_t_1 = sets_of_objects[:, t_1]\\n\\n            # Create the input to feed!\\n            input_mlp_inter, is_objects_inter = self.create_input_mlp(O_t_1, O_t, D)\\n\\n            # Infer the relations\\n            e = self.mlp_inter(input_mlp_inter)\\n\\n            # Append\\n            list_e_inter.append(e)\\n            list_is_object_inter.append(is_objects_inter)\\n\\n        if (len(list_e_inter) == 1 and self.training):\\n            # Training so only one interaction computed\\n            return list_e_inter[0], list_is_object_inter[0]\\n        else:\\n            # Stack\\n            all_e_inter = torch.stack(list_e_inter, 1)\\n            pooler = nn.AvgPool3d((all_e_inter.size(1), 1, 1))  # or nn.MaxPool3d((all_e_inter.size(1), 1, 1))\\n            all_e_inter = pooler(all_e_inter)\\n            B, _, T_prim, D = all_e_inter.size()\\n            all_e_inter = all_e_inter.view(B, T_prim, D)\\n            is_objects_inter = torch.stack(list_is_object_inter, 1)\\n            is_objects_inter = torch.clamp(torch.sum(is_objects_inter, 1), 0, 1)\\n            return all_e_inter, is_objects_inter\\n\\n    def forward(self, sets_of_objects, D, sampling=False):\\n\\n        # Number of timesteps\\n        B, T, K, _ = sets_of_objects.size()\\n\\n        list_e, list_is_obj = [], []  # list of the global interaction between two frames\\n        for t in range(1, T):\\n            # Sample during training\\n            previous_T = random.sample(range(t), 1) if self.training else list(range(t))\\n\\n            # Infer the relation between the two sets of objects\\n            e_t, is_obj = self.compute_O_O_interaction(sets_of_objects, t, previous_T, D, sampling)\\n\\n            # Append\\n            list_e.append(e_t)\\n            list_is_obj.append(is_obj)\\n\\n        # Stack\\n        all_e = torch.stack(list_e, 1)\\n        all_is_obj = torch.stack(list_is_obj, 1)\\n\\n        return all_e, all_is_obj\\nfrom __future__ import print_function\\nimport torch\\nimport torch.utils.data as data\\nfrom torchvision import transforms\\nimport os\\nimport random\\nfrom PIL import Image\\nimport numpy as np\\nimport ipdb\\nimport pickle\\nfrom pycocotools import mask as maskUtils\\nimport lintel\\nimport time\\nfrom torch.utils.data.dataloader import default_collate\\nfrom random import shuffle\\nfrom loader.videodataset import VideoDataset\\n\\n\\nclass VLOG(VideoDataset):\\n    \"\"\"\\n    Loader for the VLOG dataset\\n    \"\"\"\\n\\n    def __init__(self, options, **kwargs):\\n        super().__init__(options, **kwargs)\\n\\n        # Dict_video_label pickle\\n        self.video_label_pickle = os.path.join(self.video_dir_full, \\'dict_video_label_{}.pickle\\'.format(self.dataset))\\n\\n        # Videos paths\\n        self.list_video, self.dict_video_length, self.dict_video_label = self.get_videos()\\n\\n    def get_videos(self):\\n        # Open the pickle file\\n        with open(self.dict_video_length_fn, \\'rb\\') as file:\\n            dict_video_length = pickle.load(file)\\n\\n        # Load dict_video_label\\n        dict_video_label = self.load_or_create_dict_video_label()\\n\\n        # Intersect\\n        list_video_from_label = list(dict_video_label.keys())\\n        list_video_from_length = list(dict_video_length.keys())\\n        list_video_from_length = [v[1:].split(\\'clip\\')[0] for v in list_video_from_length]\\n        list_video = list(set(list_video_from_length) & set(list_video_from_label))\\n\\n        return list_video, dict_video_length, dict_video_label\\n\\n    def load_or_create_dict_video_label(self):\\n        if os.path.isfile(self.video_label_pickle):\\n            with open(self.video_label_pickle, \\'rb\\') as file:\\n                dict_video_label = pickle.load(file)\\n        else:\\n            # Load the label matrix\\n            label_npy_path = os.path.join(self.root, \\'meta\\', \\'hand_object\\', \\'hand_object.npy\\')\\n            matrix_label = np.load(label_npy_path)\\n\\n            # split number\\n            if self.dataset == \\'test\\':\\n                split_id = [0]\\n            elif self.dataset == \\'val\\':\\n                split_id = [3]\\n            elif self.dataset == \\'train\\':\\n                split_id = [1, 2]\\n            elif self.dataset == \\'train+val\\':\\n                split_id = [1, 2, 3]\\n            else:\\n                raise NameError\\n\\n            # Get the corresponding files\\n            manifest_file = os.path.join(self.root, \\'meta\\', \\'manifest.txt\\')\\n            split_file = os.path.join(self.root, \\'meta\\', \\'splitId.txt\\')\\n\\n            ## Read each file into a list\\n            # avi file\\n            with open(manifest_file) as f:\\n                list_video_path = f.readlines()\\n            list_video_path = [x.strip() for x in list_video_path]\\n\\n            # split\\n            with open(split_file) as f:\\n                list_split = f.readlines()\\n            list_split = [x.strip() for x in list_split]\\n            dict_video_label = {}\\n            print(\"\\\\n* Creating the dictionnary: video -> label\")\\n            for i, video in enumerate(list_video_path):\\n                if i % 20000 == 0:\\n                    print(\"{}/{} \".format(i, len(list_video_path)))\\n                # Look if it is a good file for the split\\n                if int(list_split[i]) in split_id:\\n                    dict_video_label[video] = matrix_label[i]\\n            print(\"\")\\n\\n            # Store\\n            self.store_dict_video_into_pickle(dict_video_label, self.dataset)\\n\\n        return dict_video_label\\n\\n    def starting_point(self, id):\\n        return 0\\n\\n    def get_mask_file(self, id):\\n        # Get the approriate masks\\n        mask_fn = os.path.join(self.mask_dir_full, id, \\'clip.pkl\\')\\n\\n        return mask_fn\\n\\n    def get_video_fn(self, id):\\n        # Video location\\n        video_location = os.path.join(self.video_dir_full, id, \\'clip\\' + self.video_suffix)\\n        return video_location\\n\\n    def get_length(self, id):\\n        return self.dict_video_length[\\'/{}clip\\'.format(id)]\\n\\n    def get_target(self, id):\\n        label = self.dict_video_label[id]\\n        label = np.clip(label, 0, 1)\\n        return torch.FloatTensor(label)\\nimport argparse\\nfrom loader.vlog import VLOG\\nimport time\\nimport sys\\nimport ipdb\\nimport torch\\nfrom torchvision.utils import save_image\\nfrom PIL import Image\\nimport numpy as np\\nimport os\\nfrom PIL import Image\\nimport matplotlib\\n\\nmatplotlib.use(\\'agg\\')\\nimport matplotlib.patches as patches\\nimport matplotlib.pyplot as plt\\nimport utils.vis as vis_utils\\nfrom utils.other import *\\n\\ndict_dataset = {\\'vlog\\': VLOG}\\n\\n\\ndef get_coco_names():\\n    # read the txt file\\n    fn = \\'./coco_names.txt\\'\\n    with open(fn) as f:\\n        list_coco_obj = f.readlines()\\n    list_coco_obj = [x.strip() for x in list_coco_obj]\\n\\n    # Insert background for the first obj\\n    list_coco_obj.insert(0, \\'background\\')\\n\\n    return list_coco_obj\\n\\n\\ndef main(options):\\n    # Dataset\\n    loader = VLOG if options[\\'dataset\\'] == \\'vlog\\' else None\\n\\n    # Loader\\n    videodataset = loader(options, dataset=\\'test\\', mask_size=100)\\n    print(\"Size of the dataset = {}\".format(len(videodataset)))\\n\\n    # Loop and show\\n    for i, input in enumerate(videodataset):\\n        # Get the data\\n        target = input[\\'target\\']\\n        clip = input[\\'clip\\']\\n        mask = input[\\'mask\\']\\n        obj_id = input[\\'obj_id\\']\\n        obj_bbox = input[\\'obj_bbox\\']\\n        max_nb_obj = input[\\'max_nb_obj\\']\\n        id = input[\"id\"]\\n\\n        # Video id\\n        bytes_id = id.cpu().numpy()  # it has been padded\\n        video_id = decode_videoId(bytes_id)\\n\\n        # Shape\\n        C, T, H, W = clip.shape\\n\\n        # Clip to numpy array\\n        clip_np = input[\\'clip\\'].cpu().numpy().astype(\\'uint8\\')\\n\\n        # Loop over time\\n        for t in range(T):\\n            # Image\\n            img_np = clip_np[:, t]\\n            image = img_np.transpose([1, 2, 0])\\n\\n            # Saved\\n            image_fn = \\'{}_maskRCNN_100X100_50.png\\'.format(t+1)\\n            output_dir = \\'./img\\'\\n            vis_utils.vis_one_image(\\n                image,  # BGR -> RGB for visualization # (W,H,3) np.array\\n                image_fn,  # outfile filename\\n                output_dir,  # output dir\\n                obj_id[t].cpu().numpy(),\\n                obj_bbox[t].cpu().numpy(),\\n                mask[t].cpu().numpy(),\\n                None,\\n                dataset=get_coco_names(),\\n                box_alpha=0.4,\\n                show_class=True,\\n                thresh=0.5,\\n                kp_thresh=10,\\n                show=True,\\n                W=W,\\n                H=H\\n            )\\n\\n        break\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser(description=\\'Testing the loader\\')\\n    parser.add_argument(\\'--dataset\\', metavar=\\'D\\',\\n                        default=\\'vlog\\',\\n                        help=\\'Dataset\\')\\n    parser.add_argument(\\'--root\\', metavar=\\'DIR\\',\\n                        default=\\'../data/vlog\\',\\n                        help=\\'Path to the dataset\\')\\n    parser.add_argument(\\'--t\\', default=2, type=int,\\n                        metavar=\\'H\\', help=\\'Number of timesteps to extract from a super_video\\')\\n\\n    # Args\\n    args, _ = parser.parse_known_args()\\n\\n    # Dict\\n    options = vars(args)\\n\\n    main(options)\\nfrom __future__ import print_function\\nimport torch\\nimport torch.utils.data as data\\nfrom torchvision import transforms\\nimport os\\nimport random\\nfrom PIL import Image\\nimport numpy as np\\nimport ipdb\\nimport pickle\\nfrom pycocotools import mask as maskUtils\\nimport lintel\\nimport time\\nfrom torch.utils.data.dataloader import default_collate\\nfrom random import shuffle\\nfrom  abc import abstractmethod, ABCMeta\\nimport torch.nn.functional as F\\n\\n\\nclass VideoDataset(data.Dataset):\\n    __metaclass__ = ABCMeta\\n    \"\"\"\\n    Generic loader for videos dataset\\n    \"\"\"\\n\\n    def __init__(self, options, nb_classes=30,\\n                 dataset=\\'train\\',\\n                 nb_crops=1,\\n                 #\\n                 usual_transform=False,\\n                 add_background=True,\\n                 #\\n                 video_dir=\\'videos_256x256_30\\', mask_dir=\\'masks/preds_100x100_50\\',\\n                 #\\n                 nb_obj_t_max=10, mask_confidence=0.5,\\n                 video_suffix=\\'.mp4\\',\\n                 mask_size=28,\\n                 w=224, h=224):\\n        # Settings\\n        self.root = options[\\'root\\']\\n        self.w = w\\n        self.h = h\\n        self.t = options[\\'t\\']\\n        self.video_dir = video_dir\\n        self.nb_classes = nb_classes\\n        self.usual_transform = usual_transform\\n        self.nb_obj_max_t = nb_obj_t_max\\n        self.mask_confidence = mask_confidence\\n        self.video_dir_full = os.path.join(self.root, self.video_dir)\\n        self.video_suffix = video_suffix\\n        self.mask_dir = mask_dir\\n        self.mask_dir_full = os.path.join(self.root, self.mask_dir)\\n        self.nb_crops = nb_crops\\n        self.dataset = dataset\\n        self.w_mask, self.h_mask = mask_size, mask_size\\n        self.dict_video_length_fn = os.path.join(self.video_dir_full, \\'dict_id_length.pickle\\')\\n        self.minus_len = 2\\n        self.add_background = add_background\\n        self.video_label_pickle = \\'\\'\\n        self.list_video = []\\n\\n        # Retrieve the real shape of the super_video\\n        self.retrieve_w_and_h_from_dir()\\n\\n        # Max length of a clip\\n        self.max_len_clip = 3 * self.real_fps  # sec by fps -> num of frames - 3 seconds\\n\\n        # Video and length\\n        # self.list_video, self.dict_video_length = self.get_video_and_length()\\n\\n    def store_dict_video_into_pickle(self, dict_video_label, dataset):\\n        with open(self.video_label_pickle, \\'wb\\') as file:\\n            pickle.dump(dict_video_label, file, protocol=pickle.HIGHEST_PROTOCOL)\\n            print(\"Dict_video_label of {} saved! -> {}\\\\n\".format(dataset, self.video_label_pickle))\\n\\n    def get_video_and_length(self):\\n        # Open the pickle file\\n        with open(self.dict_video_length_fn, \\'rb\\') as file:\\n            dict_video_length = pickle.load(file)\\n\\n        # Loop in each super_video dir to get th right super_video file\\n        list_video = []\\n        for video_id, length in dict_video_length.items():\\n            # Video id\\n            real_id = int(video_id.split(\\'/\\')[1])\\n            list_video.append(real_id)\\n\\n        return list_video, dict_video_length\\n\\n    def retrieve_w_and_h_from_dir(self):\\n        _, w_h, fps = self.video_dir.split(\\'_\\')\\n        w, h = w_h.split(\\'x\\')\\n        self.real_w, self.real_h, self.real_fps = int(w), int(h), int(fps)\\n        self.ratio_real_crop_w, self.ratio_real_crop_h = self.real_w / self.w, self.real_h / self.h\\n        self.real_mask_w, self.real_mask_h = int(self.ratio_real_crop_w * self.w_mask), int(\\n            self.ratio_real_crop_h * self.h_mask)\\n\\n    def time_sampling(self, video_len):\\n        # update the video_len on some dataset\\n        video_len = video_len - self.minus_len\\n\\n        # Check that the super_video is not too long\\n        diff = self.max_len_clip - video_len\\n\\n        # Change the start and adapt the length of the super_video\\n        if diff >= 0:\\n            start = 0\\n        else:\\n            start = random.sample(range(abs(diff)), 1)[0]\\n\\n        video_len_up = video_len - start\\n\\n        # Size of the sub-seq\\n        len_subseq = video_len_up / float(self.t)\\n\\n        # Sample over each bin and add the start time\\n        if self.dataset != \\'train\\' and self.nb_crops == 1:\\n            timesteps = [int((len_subseq / 2.0) + t * len_subseq + start) for t in range(self.t)]\\n        else:\\n            timesteps = [int(random.sample(range(int(len_subseq)), 1)[0] + t * len_subseq + start) for t in\\n                         range(self.t)]\\n\\n        return timesteps\\n\\n    def extract_frames(self, video_file, timesteps):\\n\\n        with open(video_file, \\'rb\\') as f:\\n            encoded_video = f.read()\\n\\n            decoded_frames = lintel.loadvid_frame_nums(encoded_video,\\n                                                       frame_nums=timesteps,\\n                                                       width=self.real_w,\\n                                                       height=self.real_h)\\n            try:\\n                np_clip = np.frombuffer(decoded_frames, dtype=np.uint8)\\n                np_clip = np.reshape(np_clip,\\n                                     newshape=(len(timesteps), self.real_h, self.real_w, 3))\\n                np_clip = np_clip.transpose([3, 0, 1, 2])\\n                np_clip = np.float32(np_clip)\\n            except Exception as e:\\n                np_clip = decoded_frames\\n                print(\"cannot decode the stream...\")\\n        return np_clip\\n\\n    @staticmethod\\n    def load_masks(file):\\n        with open(file, \\'rb\\') as f:\\n            masks = pickle.load(f, encoding=\\'latin-1\\')\\n        return (masks[\\'segms\\'], masks[\\'boxes\\'])\\n\\n    def retrieve_associated_masks(self, masks_file, video_len, timesteps, add_background_mask=True, start=0):\\n        T = len(timesteps)\\n\\n        # update the timesteps dpending on the starting point\\n        timesteps = [t + start for t in timesteps]\\n\\n        np_obj_id = np.zeros((T, self.nb_obj_max_t, 81)).astype(np.float32)\\n        np_bbox = np.zeros((T, self.nb_obj_max_t, 4)).astype(np.float32)\\n        np_masks = np.zeros((T, self.nb_obj_max_t, self.real_mask_h, self.real_mask_w)).astype(np.float32)\\n        np_max_nb_obj = np.asarray([self.nb_obj_max_t]).reshape((1,))\\n\\n        try:\\n            # raise Exception\\n            segms, boxes = self.load_masks(masks_file)\\n\\n            # Timestep factor\\n            factor = video_len / len(segms)\\n            timesteps = [int(t / factor) for t in timesteps]\\n\\n            # Retrieve information\\n            list_nb_obj = []\\n            for t_for_clip, t in enumerate(timesteps):\\n                nb_obj_t = 0\\n                # Range of objects\\n                range_objects = list(range(2, 81))\\n                shuffle(range_objects)\\n                range_objects = [1] + range_objects\\n                for c in range_objects:\\n                    for i in range(len(boxes[t][c])):\\n                        if boxes[t][c][i] is not None and len(boxes[t][c]) > 0 and boxes[t][c][i][\\n                            -1] > self.mask_confidence:\\n                            # Obj id\\n                            np_obj_id[t_for_clip, nb_obj_t, c] = 1\\n\\n                            # Bounding box\\n                            H, W = segms[t][c][i][\\'size\\']\\n                            x1, y1, x2, y2, _ = boxes[t][c][i]\\n                            x1, x2 = (x1 / W) * self.real_w, (x2 / W) * self.real_w\\n                            y1, y2 = (y1 / H) * self.real_h, (y2 / H) * self.real_h\\n                            np_bbox[t_for_clip, nb_obj_t] = [x1, y1, x2, y2]\\n\\n                            # Masks\\n                            rle_obj = segms[t][c][i]\\n                            m = maskUtils.decode(rle_obj)  # Python COCO API\\n                            # My resize\\n                            # m = resize(m, (H, W), (self.real_mask_h, self.real_mask_w), thresold=0.1\\n                            # Resize\\n                            m_pil = Image.fromarray(m)\\n                            m_pil = m_pil.resize((self.real_mask_w, self.real_mask_h))\\n                            m = np.array(m_pil, copy=False)\\n                            np_masks[t_for_clip, nb_obj_t] = m\\n\\n                            nb_obj_t += 1\\n\\n                            # Break if too much objects\\n                            if nb_obj_t > (self.nb_obj_max_t - 1):\\n                                break\\n                    # Break if too much objects\\n                    if nb_obj_t > (self.nb_obj_max_t - 1):\\n                        break\\n\\n                # Append\\n                list_nb_obj.append(nb_obj_t)\\n\\n            # And now fill numpy array\\n            np_max_nb_obj[0] = max(list_nb_obj)\\n\\n\\n        except Exception as e:\\n            print(\"mask reading problem: \", e)\\n            ipdb.set_trace()\\n            np_max_nb_obj[0] = 1.\\n\\n            # Add the background mask\\n        if add_background_mask:\\n            # Find the background pixels\\n            sum_masks = np.clip(np.sum(np_masks, 1), 0, 1)\\n            background_mask = 1 - sum_masks\\n\\n            # Add meta data about background\\n            idx_bg_mask = int(np_max_nb_obj[0])\\n            idx_bg_mask -= 1 if self.nb_obj_max_t == idx_bg_mask else 0\\n            np_masks[:, idx_bg_mask] = background_mask\\n            np_obj_id[:, idx_bg_mask, 0] = 1\\n            np_bbox[:, idx_bg_mask] = [0, 0, 1, 1]\\n\\n            # Update the number of mask\\n            np_max_nb_obj[0] = np_max_nb_obj[0] + 1 if np_max_nb_obj < self.nb_obj_max_t else np_max_nb_obj[0]\\n\\n        return (np_obj_id, np_bbox, np_masks, np_max_nb_obj)\\n\\n    def video_transform(self, np_clip, np_masks, np_bbox):\\n\\n        # Random crop\\n        _, _, h, w = np_clip.shape\\n        w_min, h_min = random.sample(range(w - self.w), 1)[0], random.sample(range(h - self.h), 1)[0]\\n        # clip\\n        np_clip = np_clip[:, :, h_min:(self.h + h_min), w_min:(self.w + w_min)]\\n        # mask\\n        h_min_mask, w_min_mask = round((h_min / self.h) * self.h_mask), round((w_min / self.w) * self.w_mask)\\n        np_masks = np_masks[:, :, h_min_mask:(self.h_mask + h_min_mask), w_min_mask:(self.w_mask + w_min_mask)]\\n        # bbox\\n        np_bbox[:, :, [0, 2]] = np.clip(np_bbox[:, :, [0, 2]] - w_min, 0, self.w)\\n        np_bbox[:, :, [1, 3]] = np.clip(np_bbox[:, :, [1, 3]] - h_min, 0, self.h)\\n        # rescale to 0->1\\n        np_bbox[:, :, [0, 2]] /= self.w\\n        np_bbox[:, :, [1, 3]] /= self.h\\n\\n        if self.usual_transform:\\n            # Div by 255\\n            np_clip /= 255.\\n\\n            # Normalization\\n            np_clip -= np.asarray([0.485, 0.456, 0.406]).reshape(3, 1, 1, 1)  # mean\\n            np_clip /= np.asarray([0.229, 0.224, 0.225]).reshape(3, 1, 1, 1)  # std\\n\\n        return np_clip, np_masks, np_bbox\\n\\n    @abstractmethod\\n    def get_mask_file(self, id):\\n        return\\n\\n    @abstractmethod\\n    def starting_point(self, id):\\n        return\\n\\n    @abstractmethod\\n    def get_video_fn(self, id):\\n        return\\n\\n    @abstractmethod\\n    def get_length(self, id):\\n        return\\n\\n    @abstractmethod\\n    def get_target(self, index):\\n        return\\n\\n    def extract_one_clip(self, id):\\n\\n        # Length\\n        length = self.get_length(id)\\n\\n        # Start of the video\\n        start = self.starting_point(id)  # 0 except for EPIC\\n\\n        # Timesteps\\n        timesteps = self.time_sampling(length)\\n\\n        return self.retrieve_clip_and_masks(id, timesteps, length, start)\\n\\n    def retrieve_clip_and_masks(self, id, timesteps, length, start):\\n        # Clip\\n        np_clip = self.extract_frames(self.get_video_fn(id), timesteps)\\n\\n        # Get the masks\\n        (np_obj_id, np_bbox, np_masks, np_max_nb_obj) = self.retrieve_associated_masks(self.get_mask_file(id),\\n                                                                                       length,\\n                                                                                       timesteps,\\n                                                                                       add_background_mask=self.add_background,\\n                                                                                       start=start)\\n\\n        # Data processing on the super_video\\n        np_clip, np_masks, np_bbox = self.video_transform(np_clip, np_masks, np_bbox)\\n\\n        return np_clip, np_masks, np_bbox, np_obj_id, np_max_nb_obj\\n\\n    def extract_multiple_clips(self, id):\\n        # Length\\n        length = self.get_length(id)\\n\\n        # Start of the video\\n        start = self.starting_point(id)  # 0 except for EPIC\\n\\n        # NB_CROPS times\\n        list_timesteps = [self.time_sampling(length) for _ in range(self.nb_crops)]\\n        timesteps_union = list(set().union(*list_timesteps))\\n        timesteps_union.sort(key=int)  # sort numerically\\n\\n        # Get the gathered data\\n        (np_clip_union, np_masks_union, np_bbox_union,\\n         np_obj_id_union, np_max_nb_obj_union) = self.retrieve_clip_and_masks(id,\\n                                                                              timesteps_union,\\n                                                                              length,\\n                                                                              start)\\n\\n        # Loop and retreieve the data per clip\\n        list_np_clip, list_np_masks, list_np_bbox, list_np_obj_id = [], [], [], []\\n        for _, timesteps in enumerate(list_timesteps):\\n            # Get the idx from the timesteps union\\n            idx_union = [timesteps_union.index(time) for time in timesteps]\\n\\n            # retrieve\\n            np_clip = np_clip_union[:, idx_union]\\n            np_masks = np_masks_union[idx_union]\\n            np_bbox = np_bbox_union[idx_union]\\n            np_obj_id = np_obj_id_union[idx_union]\\n\\n            # append\\n            list_np_clip.append(np_clip)\\n            list_np_masks.append(np_masks)\\n            list_np_bbox.append(np_bbox)\\n            list_np_obj_id.append(np_obj_id)\\n\\n        # stack\\n        np_clip = np.stack(list_np_clip)\\n        np_masks = np.stack(list_np_masks)\\n        np_obj_id = np.stack(list_np_obj_id)\\n        np_bbox = np.stack(list_np_bbox)\\n\\n        return np_clip, np_masks, np_bbox, np_obj_id, np_max_nb_obj_union\\n\\n    def __getitem__(self, index):\\n        \"\"\"\\n          Args:\\n              index (int): Index\\n          Returns:\\n              dict: info about the video\\n        \"\"\"\\n\\n        try:\\n            # Get the super_video dir\\n            id = self.list_video[index]\\n\\n            # Target\\n            torch_target = self.get_target(id)\\n\\n            # If nb_crops = 1 it is easy\\n            if self.nb_crops == 1:\\n                np_clip, np_masks, np_bbox, np_obj_id, np_max_nb_obj = self.extract_one_clip(id)\\n            else:\\n                np_clip, np_masks, np_bbox, np_obj_id, np_max_nb_obj = self.extract_multiple_clips(id)\\n\\n            # Video id\\n            np_uint8_id = np.fromstring(str(id), dtype=np.uint8)\\n            torch_id = torch.from_numpy(np_uint8_id)\\n            torch_id = F.pad(torch_id, (0, 300))[:300]\\n\\n            # Torch world\\n            torch_clip = torch.from_numpy(np_clip)\\n            torch_masks = torch.from_numpy(np_masks)\\n            torch_obj_id = torch.from_numpy(np_obj_id)\\n            torch_obj_bboxs = torch.from_numpy(np_bbox)\\n            torch_max_nb_objs = torch.from_numpy(np_max_nb_obj)\\n\\n            return {\"target\": torch_target,\\n                    \"clip\": torch_clip,\\n                    \"mask\": torch_masks,\\n                    \"obj_id\": torch_obj_id,\\n                    \"obj_bbox\": torch_obj_bboxs,\\n                    \"max_nb_obj\": torch_max_nb_objs,\\n                    \"id\": torch_id\\n                    }\\n        except Exception as e:\\n            return None\\n\\n    def __len__(self):\\n        return len(self.list_video)\\n\\n    def __repr__(self):\\n        fmt_str = \\'Dataset \\' + self.__class__.__name__ + \\'\\\\n\\'\\n        fmt_str += \\'    Number of datapoints: {}\\\\n\\'.format(self.__len__())\\n        return fmt_str\\n\\n\\ndef my_collate(batch):\\n    batch = list(filter(lambda x: x is not None, batch))\\n    return default_collate(batch)\\nimport argparse\\nimport os\\nimport subprocess\\nimport time\\nimport sys\\nimport ipdb\\nimport pickle\\nfrom utils.meter import *\\n\\n\\ndef main(args):\\n    # Parameters from the args\\n    dir, h, w, fps, suffix = args.dir, args.height, args.width, args.fps, args.suffix\\n\\n    # Video dir\\n    dir_split = dir.split(\\'/\\')\\n    video_dir = dir_split[-1]\\n    root_dir = \\'/\\'.join(dir_split[:-1])\\n    new_video_dir = \"{}_{}x{}_{}\".format(video_dir, w, h, fps)\\n    new_dir = os.path.join(root_dir, new_video_dir)\\n    os.makedirs(new_dir, exist_ok=True)\\n\\n    # load the existing dict if exist\\n    dict_video_length_fn = os.path.join(new_dir, \\'dict_id_length.pickle\\')\\n    if os.path.isfile(dict_video_length_fn):\\n        with open(dict_video_length_fn, \\'rb\\') as file:\\n            dict_video_length = pickle.load(file)\\n    else:\\n        dict_video_length = {}\\n\\n    # Get the initial video filenames\\n    list_video_fn = get_all_videos(dir, suffix)\\n    print(\"\\\\n### Initial directory: {} ###\".format(dir))\\n    print(\"=> {} videos in total\\\\n\".format(len(list_video_fn)))\\n\\n    # Loop over the super_video and extract\\n    op_time = AverageMeter()\\n    start = time.time()\\n    list_error_fn = []\\n    for i, video_fn in enumerate(list_video_fn):\\n        try:\\n            # Rescale\\n            rescale_video(video_fn, w, h, fps, dir, new_dir, suffix, dict_video_length, ffmpeg=args.ffmpeg,\\n                          crf=args.crf)\\n\\n            # Log\\n            duration = time.time() - start\\n            op_time.update(duration, 1)\\n            time_done = get_time_to_print(op_time.avg * (i + 1))\\n            time_remaining = get_time_to_print(op_time.avg * len(list_video_fn))\\n            print(\\'[{0}/{1}] : Time {batch_time.val:.3f} ({batch_time.avg:.3f}) [{done} => {remaining}]\\\\t\\'.format(\\n                    i + 1, len(list_video_fn), batch_time=op_time,\\n                    done=time_done, remaining=time_remaining))\\n            sys.stdout.flush()\\n            start = time.time()\\n        except:\\n            print(\"Impossible to rescale_videos super_video for {}\".format(video_fn))\\n            list_error_fn.append(video_fn)\\n\\n    print(\"\\\\nDone!\")\\n    print(\"\\\\nImpossible to rescale {} videos: \\\\n {}\".format(len(list_error_fn), list_error_fn))\\n\\n    # Save the dict id -> length\\n    with open(dict_video_length_fn, \\'wb\\') as file:\\n        pickle.dump(dict_video_length, file, protocol=pickle.HIGHEST_PROTOCOL)\\n        print(\"\\\\nLength of each video stored ---> {}\".format(dict_video_length_fn))\\n\\n    # Print\\n    print(\"\\\\n### You can now have access to your videos rescaled => {} ###\\\\n\".format(new_dir))\\n\\n\\ndef get_duration(file):\\n    \"\"\"Get the duration of a super_video using ffprobe. -> https://stackoverflow.com/questions/31024968/using-ffmpeg-to-obtain-super_video-durations-in-python\"\"\"\\n    cmd = \\'ffprobe -i {} -show_entries format=duration -v quiet -of csv=\"p=0\"\\'.format(file)\\n    output = subprocess.check_output(\\n        cmd,\\n        shell=True,  # Let this run in the shell\\n        stderr=subprocess.STDOUT\\n    )\\n    return float(output)\\n\\n\\ndef rescale_video(video_fn, w, h, fps, dir, new_dir, suffix, dict_video_length, ffmpeg, crf=17):\\n    \"\"\" Rescale a video according to its new width, height an fps \"\"\"\\n\\n    # Output video_name\\n    video_id = video_fn.replace(dir, \\'\\').replace(suffix, \\'\\')\\n    video_fn_rescaled = video_fn.replace(dir, new_dir)\\n    video_fn_rescaled = video_fn_rescaled.replace(suffix, suffix.lower())\\n\\n    # Create the dir\\n    video_dir_to_create = \\'/\\'.join(video_fn_rescaled.split(\\'/\\')[:-1])\\n    os.makedirs(video_dir_to_create, exist_ok=True)\\n\\n    # Check if the file already exists\\n    if os.path.isfile(video_fn_rescaled):\\n        print(\"{} already exists\".format(video_fn_rescaled))\\n    else:\\n        subprocess.call(\\n            \\'{ffmpeg} -i {video_input} -vf scale={w}:{h} -crf {crf} -r {fps} -y {video_output} -loglevel panic\\'.format(\\n                ffmpeg=ffmpeg,\\n                video_input=video_fn,\\n                h=h,\\n                w=w,\\n                fps=fps,\\n                video_output=video_fn_rescaled,\\n                crf=crf\\n            ), shell=True)\\n\\n        # Get the duration of the new super_video (in sec)\\n        duration_sec = get_duration(video_fn_rescaled)\\n        duration_frames = int(duration_sec * fps)\\n\\n        # update the dict id -> length\\n        dict_video_length[video_id] = duration_frames\\n\\n    return video_fn_rescaled\\n\\n\\ndef get_all_videos(dir, extension=\\'mp4\\'):\\n    \"\"\" Return a list of the videos filename from a directory and its subdirectories \"\"\"\\n\\n    list_video_fn = []\\n    for dirpath, dirnames, filenames in os.walk(dir):\\n        for filename in [f for f in filenames if f.endswith(extension)]:\\n            # Make sure it is not a hidden file\\n            if filename[0] != \\'.\\':\\n                fn = os.path.join(dirpath, filename)\\n                list_video_fn.append(fn)\\n\\n    return list_video_fn\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser(description=\\'Dataset preprocessing\\')\\n    parser.add_argument(\\'--dir\\', metavar=\\'DIR\\',\\n                        default=\\'../data/vlog/videos\\',\\n                        help=\\'Path to the videos dir\\')\\n    parser.add_argument(\\'--width\\', default=256, type=int,\\n                        metavar=\\'W\\', help=\\'Width of  of the output videos\\')\\n    parser.add_argument(\\'--height\\', default=256, type=int,\\n                        metavar=\\'H\\', help=\\'Height of the output videos\\')\\n    parser.add_argument(\\'--fps\\', default=30, type=int,\\n                        metavar=\\'FPS\\',\\n                        help=\\'Frames per second of the output video\\')\\n    parser.add_argument(\\'--suffix\\', metavar=\\'E\\',\\n                        default=\\'.mp4\\',\\n                        help=\\'Suffix of all the videos files - default version for the VLOG dataset\\')\\n    parser.add_argument(\\'--crf\\', default=17, type=int,\\n                        metavar=\\'CRF\\',\\n                        help=\\'Quality of the compressing - lower is better (default: 17)\\')\\n    parser.add_argument(\\'--ffmpeg\\', metavar=\\'FF\\',\\n                        default=\\'ffmpeg\\',\\n                        help=\\'Path to your ffmpeg to use (default: ffmpeg)\\')\\n\\n    args = parser.parse_args()\\n\\n    main(args)\\n# Copyright (c) 2017-present, Facebook, Inc.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n##############################################################################\\n\\n\"\"\"An awesome colormap for really neat visualizations.\"\"\"\\n\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\nfrom __future__ import unicode_literals\\n\\nimport numpy as np\\n\\n\\ndef colormap(rgb=False):\\n    color_list = np.array(\\n        [\\n            0.000, 0.447, 0.741,\\n            0.850, 0.325, 0.098,\\n            0.929, 0.694, 0.125,\\n            0.494, 0.184, 0.556,\\n            0.466, 0.674, 0.188,\\n            0.301, 0.745, 0.933,\\n            0.635, 0.078, 0.184,\\n            0.300, 0.300, 0.300,\\n            0.600, 0.600, 0.600,\\n            1.000, 0.000, 0.000,\\n            1.000, 0.500, 0.000,\\n            0.749, 0.749, 0.000,\\n            0.000, 1.000, 0.000,\\n            0.000, 0.000, 1.000,\\n            0.667, 0.000, 1.000,\\n            0.333, 0.333, 0.000,\\n            0.333, 0.667, 0.000,\\n            0.333, 1.000, 0.000,\\n            0.667, 0.333, 0.000,\\n            0.667, 0.667, 0.000,\\n            0.667, 1.000, 0.000,\\n            1.000, 0.333, 0.000,\\n            1.000, 0.667, 0.000,\\n            1.000, 1.000, 0.000,\\n            0.000, 0.333, 0.500,\\n            0.000, 0.667, 0.500,\\n            0.000, 1.000, 0.500,\\n            0.333, 0.000, 0.500,\\n            0.333, 0.333, 0.500,\\n            0.333, 0.667, 0.500,\\n            0.333, 1.000, 0.500,\\n            0.667, 0.000, 0.500,\\n            0.667, 0.333, 0.500,\\n            0.667, 0.667, 0.500,\\n            0.667, 1.000, 0.500,\\n            1.000, 0.000, 0.500,\\n            1.000, 0.333, 0.500,\\n            1.000, 0.667, 0.500,\\n            1.000, 1.000, 0.500,\\n            0.000, 0.333, 1.000,\\n            0.000, 0.667, 1.000,\\n            0.000, 1.000, 1.000,\\n            0.333, 0.000, 1.000,\\n            0.333, 0.333, 1.000,\\n            0.333, 0.667, 1.000,\\n            0.333, 1.000, 1.000,\\n            0.667, 0.000, 1.000,\\n            0.667, 0.333, 1.000,\\n            0.667, 0.667, 1.000,\\n            0.667, 1.000, 1.000,\\n            1.000, 0.000, 1.000,\\n            1.000, 0.333, 1.000,\\n            1.000, 0.667, 1.000,\\n            0.167, 0.000, 0.000,\\n            0.333, 0.000, 0.000,\\n            0.500, 0.000, 0.000,\\n            0.667, 0.000, 0.000,\\n            0.833, 0.000, 0.000,\\n            1.000, 0.000, 0.000,\\n            0.000, 0.167, 0.000,\\n            0.000, 0.333, 0.000,\\n            0.000, 0.500, 0.000,\\n            0.000, 0.667, 0.000,\\n            0.000, 0.833, 0.000,\\n            0.000, 1.000, 0.000,\\n            0.000, 0.000, 0.167,\\n            0.000, 0.000, 0.333,\\n            0.000, 0.000, 0.500,\\n            0.000, 0.000, 0.667,\\n            0.000, 0.000, 0.833,\\n            0.000, 0.000, 1.000,\\n            0.000, 0.000, 0.000,\\n            0.143, 0.143, 0.143,\\n            0.286, 0.286, 0.286,\\n            0.429, 0.429, 0.429,\\n            0.571, 0.571, 0.571,\\n            0.714, 0.714, 0.714,\\n            0.857, 0.857, 0.857,\\n            1.000, 1.000, 1.000\\n        ]\\n    ).astype(np.float32)\\n    color_list = color_list.reshape((-1, 3)) * 255\\n    if not rgb:\\n        color_list = color_list[:, ::-1]\\n    return color_list\\nimport os\\nimport math\\nimport torch\\nimport numpy as np\\nimport numbers\\nimport ipdb\\n\\n\\nclass AverageMeter(object):\\n    \"\"\"Computes and stores the average and current value\"\"\"\\n\\n    def __init__(self):\\n        self.reset()\\n\\n    def reset(self):\\n        self.val = 0\\n        self.avg = 0\\n        self.sum = 0\\n        self.count = 0\\n\\n    def update(self, val, n=1):\\n        self.val = val\\n        self.sum += val * n\\n        self.count += n\\n        self.avg = self.sum / self.count\\n\\n\\n\\n\\nclass AveragePrecisionMeter(object):\\n    \"\"\"\\n    The APMeter measures the average precision per class.\\n    The APMeter is designed to operate on `NxK` Tensors `output` and\\n    `target`, and optionally a `Nx1` Tensor weight where (1) the `output`\\n    contains model output scores for `N` examples and `K` classes that ought to\\n    be higher when the model is more convinced that the super_video should be\\n    positively labeled, and smaller when the model believes the super_video should\\n    be negatively labeled (for instance, the output of a sigmoid function); (2)\\n    the `target` contains only values 0 (for negative examples) and 1\\n    (for positive examples); and (3) the `weight` ( > 0) represents weight for\\n    each sample.\\n    \"\"\"\\n\\n    def __init__(self, difficult_examples=False, all_dataset=False):\\n        super(AveragePrecisionMeter, self).__init__()\\n        self.reset()\\n        self.difficult_examples = difficult_examples\\n        self.all_dataset = all_dataset\\n\\n    def reset(self):\\n        \"\"\"Resets the meter with empty member variables\"\"\"\\n        self.scores = torch.FloatTensor(torch.FloatStorage())\\n        self.targets = torch.LongTensor(torch.LongStorage())\\n\\n    def add(self, output, target):\\n        \"\"\"\\n        Args:\\n            output (Tensor): NxK tensor that for each of the N examples\\n                indicates the probability of the super_video belonging to each of\\n                the K classes, according to the model. The probabilities should\\n                sum to one over all classes\\n            target (Tensor): binary NxK tensort that encodes which of the K\\n                classes are associated with the N-th input\\n                    (eg: a row [0, 1, 0, 1] indicates that the super_video is\\n                         associated with classes 2 and 4)\\n            weight (optional, Tensor): Nx1 tensor representing the weight for\\n                each super_video (each weight > 0)\\n        \"\"\"\\n        if not torch.is_tensor(output):\\n            output = torch.from_numpy(output)\\n        if not torch.is_tensor(target):\\n            target = torch.from_numpy(target)\\n\\n        if output.dim() == 1:\\n            output = output.view(-1, 1)\\n        else:\\n            assert output.dim() == 2, \\\\\\n                \\'wrong output size (should be 1D or 2D with one column \\\\\\n                per class)\\'\\n        if target.dim() == 1:\\n            target = target.view(-1, 1)\\n        else:\\n            assert target.dim() == 2, \\\\\\n                \\'wrong target size (should be 1D or 2D with one column \\\\\\n                per class)\\'\\n        if self.scores.numel() > 0:\\n            assert target.size(1) == self.targets.size(1), \\\\\\n                \\'dimensions for output should match previously added examples.\\'\\n\\n        # make sure storage is of sufficient size\\n        if self.scores.storage().size() < self.scores.numel() + output.numel():\\n            new_size = math.ceil(self.scores.storage().size() * 1.5)\\n            self.scores.storage().resize_(int(new_size + output.numel()))\\n            self.targets.storage().resize_(int(new_size + output.numel()))\\n\\n        # store scores and targets\\n        offset = self.scores.size(0) if self.scores.dim() > 0 else 0\\n        self.scores.resize_(offset + output.size(0), output.size(1))\\n        self.targets.resize_(offset + target.size(0), target.size(1))\\n        self.scores.narrow(0, offset, output.size(0)).copy_(output)\\n        self.targets.narrow(0, offset, target.size(0)).copy_(target)\\n\\n        # Idx of correct preds\\n        B, C = target.size()\\n        list_idx_correct_preds = []\\n        for idx in range(B):\\n            correct_preds = True\\n            for j in range(C):\\n                # does not have the same sign so bad preds -> break\\n                target_idx_j = -1 if target[idx, j] == 0 else 1\\n                if target_idx_j * output[idx, j] < 0:\\n                    correct_preds = False\\n                    break\\n            # good preds! if finished the loop\\n            if correct_preds:\\n                list_idx_correct_preds.append(idx)\\n\\n        return list_idx_correct_preds\\n\\n    def value(self):\\n        \"\"\"Returns the model\\'s average precision for each class\\n        Return:\\n            ap (FloatTensor): 1xK tensor, with avg precision for each class k\\n        \"\"\"\\n\\n        if self.scores.numel() == 0:\\n            return 0\\n        ap = torch.zeros(self.scores.size(1))\\n        rg = torch.arange(1, self.scores.size(0)).float()\\n\\n        # compute average precision for each class\\n        for k in range(self.scores.size(1)):\\n            # sort scores\\n            scores = self.scores[:, k]\\n            targets = self.targets[:, k]\\n\\n            # compute average precision\\n            if self.all_dataset:\\n                scores_to_keep, targets_to_keep = scores, targets\\n            else:\\n                scores_to_keep, targets_to_keep = scores[-100:], targets[-100:]\\n            ap[k] = AveragePrecisionMeter.average_precision(scores_to_keep, targets_to_keep,\\n                                                            self.difficult_examples) * 100.\\n            # ap[k] = AveragePrecisionMeter.compute_ap(scores, targets)\\n\\n        return ap.mean(), ap.mean(), ap\\n\\n    @staticmethod\\n    def compute_ap(scores, targets):\\n        # import ipdb\\n        # ipdb.set_trace()\\n        _, sortind = torch.sort(scores, 0, True)\\n        truth = targets[sortind]\\n        rg = torch.range(1, scores.size(0)).float()\\n        tp = truth.float().cumsum(0)\\n\\n        # compute precision curve\\n        precision = tp.div(rg)\\n\\n        # compute average precision\\n        ap = precision[truth.byte()].sum() / max(truth.sum(), 1)\\n\\n        return ap\\n\\n    # not working\\n    @staticmethod\\n    def average_precision(output, target, difficult_examples=True):\\n        # sort examples\\n        sorted, indices = torch.sort(output, dim=0, descending=True)\\n\\n        # Computes prec@i\\n        pos_count = 0.\\n        total_count = 0.\\n        precision_at_i = 0.\\n        for i in indices:\\n            label = target[i]\\n            if difficult_examples and label == 0:\\n                continue\\n            if label == 1:\\n                pos_count += 1\\n            total_count += 1\\n            if label == 1:\\n                precision_at_i += pos_count / total_count\\n        precision_at_i /= pos_count + 1e-5\\n        return precision_at_i\\n\\ndef get_time_to_print(time_sec):\\n    hours = math.trunc(time_sec / 3600)\\n    time_sec = time_sec - hours * 3600\\n    mins = math.trunc(time_sec / 60)\\n    time_sec = time_sec - mins * 60\\n    secs = math.trunc(time_sec % 60)\\n    string = \\'%02d:%02d:%02d\\' % (hours, mins, secs)\\n    return string# Copyright (c) 2017-present, Facebook, Inc.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n##############################################################################\\n\\n\"\"\"Detection output visualization module.\"\"\"\\n\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\nfrom __future__ import unicode_literals\\n\\nfrom PIL import Image\\nimport cv2\\nimport numpy as np\\nimport os\\nimport pycocotools.mask as mask_util\\n\\nfrom utils.colormap import colormap\\n# import utils.keypoints as keypoint_utils\\n\\n# Matplotlib requires certain adjustments in some environments\\n# Must happen before importing matplotlib\\nimport matplotlib\\n\\nmatplotlib.use(\\'Agg\\')  # Use a non-interactive backend\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.patches import Polygon\\nimport ipdb\\n\\nplt.rcParams[\\'pdf.fonttype\\'] = 42  # For editing in Adobe Illustrator\\n\\n_GRAY = (218, 227, 218)\\n_GREEN = (18, 127, 15)\\n_WHITE = (255, 255, 255)\\n\\n\\ndef get_class_string(class_index, score, dataset):\\n    class_text = dataset[class_index] if dataset is not None else \\\\\\n        \\'id{:d}\\'.format(class_index)\\n    return class_text #+ \\' {:0.2f}\\'.format(score).lstrip(\\'0\\')\\n\\n\\ndef vis_bbox(img, bbox, thick=1):\\n    \"\"\"Visualizes a bounding box.\"\"\"\\n    (x0, y0, w, h) = bbox\\n    x1, y1 = int(x0 + w), int(y0 + h)\\n    x0, y0 = int(x0), int(y0)\\n    cv2.rectangle(img, (x0, y0), (x1, y1), _GREEN, thickness=thick)\\n    return img\\n\\ndef vis_one_image(\\n        im, im_name, output_dir, classes, boxes, masks=None, keypoints=None, thresh=0.9,\\n        kp_thresh=2, dpi=200, box_alpha=0.0, dataset=None, show_class=False,\\n        ext=\\'pdf\\', show=False, W=224, H=224):\\n    \"\"\"Visual debugging of detections.\"\"\"\\n    if not os.path.exists(output_dir):\\n        os.makedirs(output_dir)\\n\\n    color_list = colormap(rgb=True) / 255\\n\\n    fig = plt.figure(frameon=False)\\n    fig.set_size_inches(im.shape[1] / dpi, im.shape[0] / dpi)\\n    ax = plt.Axes(fig, [0., 0., 1., 1.])\\n    ax.axis(\\'off\\')\\n    fig.add_axes(ax)\\n    ax.imshow(im)\\n\\n    # Display in largest to smallest order to reduce occlusion\\n    boxes[:,0] *= W\\n    boxes[:, 2] *= W\\n    boxes[:,1] *= H\\n    boxes[:, 3] *= H\\n    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\\n    sorted_inds = np.argsort(-areas)\\n\\n    # torch to numpy\\n    # ipdb.set_trace()\\n    if masks is not None:\\n        # uint8\\n        masks = masks.astype(\\'uint8\\')\\n        # rescale\\n        w_masks, h_masks, _ = masks.shape\\n\\n    mask_color_id = 0\\n    # ipdb.set_trace()\\n    for i in sorted_inds:\\n        bbox = boxes[i, :4]\\n        score = boxes[i, -1]\\n        if score < thresh:\\n            continue\\n\\n        # show box (off by default)\\n        ax.add_patch(\\n            plt.Rectangle((bbox[0], bbox[1]),\\n                          bbox[2] - bbox[0],\\n                          bbox[3] - bbox[1],\\n                          fill=False, edgecolor=\\'g\\',\\n                          linewidth=1, alpha=box_alpha))\\n\\n        if show_class:\\n            # (x, y) = (bbox[0], bbox[3] + 2) if classes[i] == 1 else (bbox[3], bbox[1] - 2)  # below for person or above for the rest\\n            x, y = (bbox[0], bbox[1] - 2)\\n            classes_i = np.argmax(classes[i])\\n            # print(get_class_string(classes_i, score, dataset), classes_i, score)\\n            ax.text(\\n                x, y,\\n                get_class_string(classes_i, score, dataset),\\n                fontsize=4,\\n                family=\\'serif\\',\\n                bbox=dict(\\n                    facecolor=\\'g\\', alpha=0.4, pad=0, edgecolor=\\'none\\'),\\n                color=\\'white\\')\\n\\n        # show mask\\n        if masks is not None:\\n            # ipdb.set_trace()\\n            img = np.ones(im.shape)\\n            color_mask = color_list[mask_color_id % len(color_list), 0:3]\\n            mask_color_id += 1\\n\\n            w_ratio = .4\\n            for c in range(3):\\n                color_mask[c] = color_mask[c] * (1 - w_ratio) + w_ratio\\n            for c in range(3):\\n                img[:, :, c] = color_mask[c]\\n            e_down = masks[i, :, :]\\n\\n            # Rescale mask\\n            e_pil = Image.fromarray(e_down)\\n            e_pil_up = e_pil.resize((H, W),Image.ANTIALIAS)\\n            e = np.array(e_pil_up)\\n\\n            _, contour, hier = cv2.findContours(\\n                e.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\\n\\n            for c in contour:\\n                polygon = Polygon(\\n                    c.reshape((-1, 2)),\\n                    fill=True, facecolor=color_mask,\\n                    edgecolor=\\'w\\', linewidth=1.2,\\n                    alpha=0.5)\\n                ax.add_patch(polygon)\\n\\n    output_name = os.path.basename(im_name) + \\'.\\' + ext\\n    fig.savefig(os.path.join(output_dir, \\'{}\\'.format(output_name)), dpi=dpi)\\n    print(\\'result saved to {}\\'.format(os.path.join(output_dir, \\'{}\\'.format(output_name))))\\n    if show:\\n        plt.show()\\n    plt.close(\\'all\\')\\nimport pickle\\nfrom loader.vlog import VLOG\\nimport torch\\nfrom loader.videodataset import my_collate\\nfrom torch.nn import Module\\nimport torch.nn as nn\\nimport ipdb\\nimport torch\\nfrom utils.meter import *\\nimport shutil\\nimport math\\n\\n\\ndef load_pickle(file):\\n    with open(file, mode=\\'rb\\') as f:\\n        df = pickle.load(f, encoding=\\'latin1\\')\\n    return df\\n\\n\\ndef get_datasets_and_dataloaders(options, cuda=False):\\n    # Choice of Dataset\\n    if options[\\'dataset\\'] == \\'vlog\\':\\n        VideoDataset = VLOG\\n        if options[\\'train_set\\'] == \\'train\\':\\n            train_set_name = \\'train\\'\\n            val_set_name = \\'val\\'\\n            nb_crops = 1\\n        elif options[\\'train_set\\'] == \\'train+val\\':\\n            train_set_name = \\'train+val\\'\\n            val_set_name = \\'test\\'\\n            nb_crops = options[\\'nb_crops\\']\\n        else:\\n            raise NameError\\n    else:\\n        raise NameError\\n\\n    # Dataset\\n    train_dataset = VideoDataset(options,\\n                                 dataset=train_set_name,\\n                                 nb_crops=1,\\n                                 usual_transform=True,\\n                                 add_background=options[\\'add_background\\'])\\n    val_dataset = VideoDataset(options,\\n                               dataset=val_set_name,\\n                               nb_crops=nb_crops,\\n                               usual_transform=True,\\n                               add_background=options[\\'add_background\\'])\\n\\n    # Dataloader\\n    train_loader = torch.utils.data.DataLoader(train_dataset,\\n                                               batch_size=options[\\'batch_size\\'],\\n                                               shuffle=True,\\n                                               num_workers=options[\\'workers\\'],\\n                                               pin_memory=cuda,\\n                                               collate_fn=my_collate)\\n    val_loader = torch.utils.data.DataLoader(val_dataset,\\n                                             batch_size=options[\\'batch_size\\'],\\n                                             shuffle=False,\\n                                             num_workers=options[\\'workers\\'],\\n                                             pin_memory=cuda,\\n                                             collate_fn=my_collate)\\n\\n    return train_dataset, val_dataset, train_loader, val_loader\\n\\n\\ndef get_loss_and_metric(options):\\n    if options[\\'dataset\\'] == \\'vlog\\':\\n        # Metric\\n        metric = AveragePrecisionMeter\\n        # Loss\\n        loss = CriterionLinearCombination([\\'bce\\', \\'ce\\'], [15.0, 1.0])\\n    else:\\n        raise NameError\\n\\n    return loss, metric\\n\\n\\nclass CriterionLinearCombination(Module):\\n    def __init__(self, list_criterion_names, list_weights):\\n        super(CriterionLinearCombination, self).__init__()\\n        assert len(list_criterion_names) == len(list_weights)\\n\\n        self.list_criterion, self.list_weights = [], []\\n        for i, criterion_name in enumerate(list_criterion_names):\\n            # Criterion\\n            if criterion_name == \\'bce\\':\\n                self.list_criterion.append(nn.BCEWithLogitsLoss())\\n            elif criterion_name == \\'ce\\':\\n                self.list_criterion.append(nn.CrossEntropyLoss())\\n            else:\\n                raise Exception\\n            # Weight\\n            self.list_weights.append(list_weights[i])\\n\\n    def forward(self, list_input, list_target, cuda=False):\\n        assert len(list_input) == len(list_target)\\n        # ipdb.set_trace()\\n        loss = 0.0\\n        for i in range(len(self.list_criterion)):\\n            # Cast depending of the criterion\\n            criterion_i, weight_i = self.list_criterion[i], self.list_weights[i]\\n            target_i, input_i = list_target[i], list_input[i]\\n            if input_i is not None:\\n                if isinstance(criterion_i, nn.CrossEntropyLoss):\\n                    target_i = target_i.type(torch.LongTensor)\\n                elif isinstance(criterion_i, nn.BCEWithLogitsLoss):\\n                    target_i = target_i.type(torch.FloatTensor)\\n\\n                target_i = target_i.cuda() if cuda else target_i\\n\\n                # Compute the loss and add\\n                input_i = input_i.view(-1, input_i.size(-1))\\n                loss_i = weight_i * criterion_i(input_i, target_i)\\n                loss = loss + loss_i\\n\\n        return loss\\n\\n\\ndef load_from_dir(model, optimizer, options):\\n    \\'\\'\\' load from resume found in the dir\\'\\'\\'\\n    epoch = 0\\n    if options[\\'resume\\']:\\n        if os.path.isdir(options[\\'resume\\']):\\n            ckpt_resume = os.path.join(options[\\'resume\\'], \\'model_best.pth.tar\\')\\n            if os.path.isfile(ckpt_resume):\\n                print(\"\\\\n=> loading checkpoint \\'{}\\'\".format(ckpt_resume))\\n                checkpoint = torch.load(ckpt_resume, map_location=lambda storage, loc: storage)\\n                epoch = checkpoint[\\'epoch\\']\\n                # Remove the fc_classifier\\'s\\n                updated_params = {}\\n                model_dict = model.state_dict()\\n                for k, v in checkpoint[\\'state_dict\\'].items():\\n                    # Train classifier fom scratch\\n                    if \"fc_classifier\" in k and not options[\\'evaluate\\']:\\n                        pass\\n                    # Look if the size if the same\\n                    if k in list(model_dict.keys()):\\n                        v_new_size, v_old_size = v.size(), model_dict[k].size()\\n                        if v_old_size == v_new_size:\\n                            updated_params[k] = v\\n\\n                # Load\\n                new_params = model.state_dict()\\n                new_params.update(updated_params)\\n                model.load_state_dict(new_params)\\n\\n                # Optim\\n                updated_params = {}\\n                new_params = optimizer.state_dict()\\n                for k, v in checkpoint[\\'state_dict\\'].items():\\n                    if k not in list(new_params.keys()):\\n                        updated_params[k] = v\\n\\n                new_params.update(updated_params)\\n                optimizer.load_state_dict(new_params)\\n\\n                # Epoch\\n                print(\"=> loaded checkpoint \\'{}\\' (epoch {})\"\\n                      .format(ckpt_resume, checkpoint[\\'epoch\\']))\\n            else:\\n                print(\"\\\\n=> no checkpoint found at \\'{}\\'\".format(options[\\'resume\\']))\\n        else:\\n            os.makedirs(options[\\'resume\\'])\\n\\n    return model, optimizer, epoch\\n\\n\\ndef print_number(number):\\n    \"\"\" print a \\' every 3 number starting from the left (e.g 23999 -> 23\\'999)\"\"\"\\n    len_3 = round(len(str(number)) / 3.)\\n\\n    j = 0\\n    number = str(number)\\n    for i in range(1, len_3 + 1):\\n        k = i * 3 + j\\n        number = number[:-k] + \\'\\\\\\'\\' + number[-k:]\\n        j += 1\\n\\n    # remove \\' if it is at the begining\\n    if number[0] == \\'\\\\\\'\\':\\n        return number[1:]\\n    else:\\n        return number\\n\\n\\ndef write_to_log(dataset, resume, epoch, metrics, metrics_per_class):\\n    # Global metric\\n    file_full_name = os.path.join(resume, dataset + \\'_log\\')\\n    with open(file_full_name, \\'a+\\') as f:\\n        f.write(\\'Epoch=%03d, Loss=%.4f, Metric=%.4f\\\\n\\' % (epoch, metrics[0], metrics[1]))\\n\\n    # Per class metric\\n    if metrics_per_class is not None:\\n        file_full_name = os.path.join(resume, dataset + \\'_per_class_metrics_log\\')\\n        np.savetxt(file_full_name, metrics_per_class.numpy(), fmt=\\'%10.4f\\', delimiter=\\',\\')\\n\\n\\ndef transform_input(x, dim, T=8):\\n    diff = len(x.size()) - dim\\n\\n    if diff > 0:\\n        B, C, T, W, H = x.size()\\n        x = x.transpose(1, 2)\\n        x = x.contiguous()\\n        x = x.view(-1, C, W, H)\\n    elif diff < 0:\\n        _, C, W, H = x.size()\\n        x = x.view(-1, T, C, W, H)\\n        x = x.transpose(1, 2)\\n\\n    return x\\n\\n\\ndef count_nb_params(enum_params):\\n    nb_params = 0\\n    for parameter in enum_params:\\n        nb_param_w = 1\\n        for s in parameter.size():\\n            nb_param_w *= s\\n        nb_params += nb_param_w\\n    return nb_params\\n\\n\\ndef save_checkpoint(state, is_best, resume, filename=\\'checkpoint.pth.tar\\'):\\n    full_filename = os.path.join(resume, filename)\\n    torch.save(state, full_filename)\\n    if is_best:\\n        full_filename_best = os.path.join(resume, \\'model_best.pth.tar\\')\\n        shutil.copyfile(full_filename, full_filename_best)\\n\\n\\ndef decode_videoId(bytes_id):\\n    try:\\n        idx_1st_0 = (bytes_id == 0).argmax(axis=0)  # find zero padding\\n    except:\\n        idx_1st_0 = (bytes_id == 0).argmax()  # find zero padding\\n\\n    str_video_id = bytes_id[:idx_1st_0].tobytes().decode(\"utf-8\")  # remove zero padding\\n    return str_video_id\\n\\n\\ndef store_preds(preds, id, list_correct_preds, obj_id, dataset=\\'vlog\\'):\\n    # sigmoid or softmax\\n    # if dataset == \\'vlog\\':\\n    #     f = sigmoid\\n    # else:\\n    #     raise NameError\\n\\n    # cpu - np\\n    id_np = id.cpu().numpy()\\n    preds_cpu = np.round(preds.cpu().numpy().astype(np.float16), 2)\\n    obj_id_cpu = obj_id.cpu().numpy().astype(np.float16)\\n\\n    # Lop\\n    dict_good, dict_failure, dict_obj = {}, {}, {}\\n    for i, p in enumerate(preds_cpu):\\n        # catch id\\n        id_i = decode_videoId(id_np[i])\\n\\n        # obj id\\n        # ipdb.set_trace()\\n        dict_obj[id_i] = np.round(obj_id_cpu[i].sum(0).sum(0), 2)\\n\\n        # good or failure\\n        if i in list_correct_preds:\\n            dict_good[id_i] = p\\n        else:\\n            dict_failure[id_i] = p\\n\\n    return dict_good, dict_failure, dict_obj\\n\\n\\ndef sigmoid(x):\\n    return 1 / (1 + math.exp(-x))\\nfrom model import models\\nimport argparse\\nimport os\\nimport shutil\\nimport time\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.parallel\\nimport torch.optim\\nimport torch.utils.data\\nimport torch.utils.data.distributed\\nfrom utils.meter import *\\nfrom inference.train_val import *\\nimport ipdb\\nfrom model import models\\nfrom utils.other import *\\n\\n\\ndef main(options):\\n    # CUDA\\n    cuda = torch.cuda.is_available()\\n\\n    # Dataset\\n    train_dataset, val_dataset, train_loader, val_loader = get_datasets_and_dataloaders(options, cuda=cuda)\\n    print(\\'\\\\n*** Train set of size {}  -  Val set of size {} ***\\\\n\\'.format(print_number(len(train_dataset)),\\n                                                                           print_number(len(val_dataset))))\\n\\n    # Model\\n    model = models.__dict__[options[\\'arch\\']](num_classes=train_dataset.nb_classes,\\n                                             size_fm_2nd_head=train_dataset.h_mask,\\n                                             options=options)\\n    model = model.cuda() if cuda else model\\n    model = torch.nn.DataParallel(model)\\n\\n    # Trainable params\\n    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\\n\\n    # Print number of parameters\\n    nb_total_params = count_nb_params(model.parameters())\\n    nb_trainable_params = count_nb_params(filter(lambda p: p.requires_grad, model.parameters()))\\n    ratio = float(nb_trainable_params / nb_total_params) * 100.\\n    print(\"\\\\n* Parameter numbers : {} ({}) - {ratio:.2f}% of the weights are trainable\".format(\\n        print_number(nb_total_params),\\n        print_number(nb_trainable_params),\\n        ratio=ratio\\n    ))\\n\\n    # Optimizer\\n    optimizer = torch.optim.Adam(trainable_params, options[\\'lr\\'])\\n\\n    # Loss function and Metric\\n    criterion, metric = get_loss_and_metric(options)\\n\\n    # Load resume from resume if exists\\n    model, optimizer, epoch = load_from_dir(model, optimizer, options)\\n\\n    # My engine\\n    engine = {\\'model\\': model,\\n              \\'optimizer\\': optimizer, \\'criterion\\': criterion, \\'metric\\': metric,\\n              \\'train_loader\\': train_loader, \\'val_loader\\': val_loader}\\n\\n    # Training/Val or Testing #\\n    if options[\\'evaluate\\']:\\n        # Val\\n        loss_val, metric_val, per_class_metric_val, df_good, df_failure, df_objects = validate(epoch, engine, options, cuda=cuda)\\n        # Write into log\\n        write_to_log(val_dataset.dataset, options[\\'resume\\'], epoch, [loss_val, metric_val], per_class_metric_val)\\n        # Save good and failures and object presence\\n        df_good.to_csv(os.path.join(options[\\'resume\\'], \\'df_good_preds.csv\\'), sep=\\',\\', encoding=\\'utf-8\\')\\n        df_failure.to_csv(os.path.join(options[\\'resume\\'], \\'df_failure_preds.csv\\'), sep=\\',\\', encoding=\\'utf-8\\')\\n        df_objects.to_csv(os.path.join(options[\\'resume\\'], \\'df_objects\\'), sep=\\',\\', encoding=\\'utf-8\\')\\n\\n    else:\\n        # Train (and Val if having access tto the val set)\\n        is_best = True\\n        best_metric_val = -0.1\\n        for epoch in range(1, options[\\'epochs\\'] + 1):\\n            # train one epoch\\n            loss_train, metric_train = train(epoch, engine, options, cuda=cuda)\\n\\n            # write into log\\n            write_to_log(train_dataset.dataset, options[\\'resume\\'], epoch, [loss_train, loss_train], None)\\n\\n            # get the val metric\\n            if options[\\'train_set\\'] == \\'train\\':\\n                # Val\\n                loss_val, metric_val, per_class_metric_val, *_ = validate(epoch, engine, options, cuda=cuda)\\n                # Write into log\\n                write_to_log(val_dataset.dataset, options[\\'resume\\'], epoch, [loss_val, metric_val],\\n                             per_class_metric_val)\\n\\n                # Best compared to previous checkpoint ?\\n                is_best = metric_val > best_metric_val\\n                best_metric_val = max(metric_val, best_metric_val)\\n\\n            # save checkpoint\\n            save_checkpoint({\\n                \\'epoch\\': epoch,\\n                \\'arch\\': options[\\'arch\\'],\\n                \\'state_dict\\': model.state_dict(),\\n                \\'best_metric_val\\': best_metric_val,\\n                \\'optimizer\\': optimizer.state_dict(),\\n            }, is_best, options[\\'resume\\'])\\n\\n    return None\\nfrom utils.meter import *\\nimport time\\nimport torch\\nimport ipdb\\nimport sys\\nimport torch.nn as nn\\nfrom utils.meter import *\\nimport matplotlib\\nfrom utils.other import *\\nimport pandas\\n\\n\\ndef make_variable_all_input(dict_input, cuda=False):\\n    dict_input_var = {}\\n    for k, v in dict_input.items():\\n        var = torch.autograd.Variable(v)\\n        dict_input_var[k] = var.cuda() if cuda else var\\n    return dict_input_var\\n\\n\\ndef get_obj_id_for_loss(input_var, is_Variable=True, j=0):\\n    obj_id = input_var[\\'obj_id\\']\\n    if is_Variable:\\n        nb_max_obj = int(torch.max(input_var[\\'max_nb_obj\\']).cpu())\\n    else:\\n        nb_max_obj = int(torch.max(input_var[\\'max_nb_obj\\'].cpu()))\\n\\n    # Catch the useful obj id\\n    obj_id_size = len(obj_id.size())\\n    obj_id = obj_id[:, :, :nb_max_obj] if obj_id_size == 4 else obj_id[:, :, :, :nb_max_obj]\\n    obj_id = torch.max(obj_id, -1)[1]\\n    if obj_id_size == 4:\\n        obj_id = obj_id.view(-1)\\n    else:\\n        obj_id = obj_id[:, j].contiguous()\\n        obj_id = obj_id.view(-1)\\n\\n    return obj_id\\n\\n\\ndef forward_backward(model, input_var, criterion, optimizer=None, cuda=False,\\n                     j=1):\\n    # compute output\\n    output = model(input_var)\\n\\n    # retrieve object id\\n    obj_id = get_obj_id_for_loss(input_var, j=j)\\n\\n    # update output\\n    output = output if isinstance(output, tuple) else (output, None)\\n\\n    # compute loss\\n    loss = criterion(output, [input_var[\\'target\\'], obj_id], cuda)\\n\\n    # backward\\n    if optimizer is not None:\\n        # compute gradient and do SGD step\\n        optimizer.zero_grad()\\n        loss.backward()\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)  # clip grad\\n        optimizer.step()\\n\\n    return output[0], loss  # return the output for the action only - objects preds are just regularizer\\n\\n\\ndef update_metric_loss(input, output, metric, loss, losses):\\n    # loss\\n    losses.update(loss.detach(), input[\\'clip\\'].size(0))\\n\\n    # metrics\\n    target = input[\\'target\\']\\n    target = target.cpu()\\n    preds = output.view(-1, output.size(-1)).data.cpu()\\n    list_idx_correct_preds = metric.add(preds, target)\\n    metric_val, metric_avg, _ = metric.value()\\n\\n    return metric_val, metric_avg, list_idx_correct_preds\\n\\n\\ndef take_clip_j(input_var, j):\\n    input_var_j = {}\\n    for k, v in input_var.items():\\n        if k == \\'video_id\\':\\n            pass\\n        elif k == \\'target\\':\\n            input_var_j[\\'target\\'] = input_var[\\'target\\']\\n        elif k == \\'id\\':\\n            input_var_j[\\'id\\'] = input_var[\\'id\\']\\n        elif k == \\'max_nb_obj\\':\\n            input_var_j[\\'max_nb_obj\\'] = input_var[\\'max_nb_obj\\']\\n        else:\\n            input_var_j[k] = input_var[k][:, j]\\n    return input_var_j\\n\\n\\ndef train(epoch, engine, options, cuda=False):\\n    # Timer\\n    batch_time = AverageMeter()\\n    data_time = AverageMeter()\\n    losses = AverageMeter()\\n\\n    # Engine\\n    model = engine[\\'model\\']\\n    optimizer = engine[\\'optimizer\\']\\n    criterion = engine[\\'criterion\\']\\n    metric = engine[\\'metric\\']()\\n    data_loader = engine[\\'train_loader\\']\\n\\n    # switch to train mode\\n    model.train()\\n\\n    end = time.time()\\n    print(\"\")\\n    for i, input in enumerate(data_loader):\\n        # measure data loading time\\n        data_time.update(time.time() - end)\\n\\n        # Make Variables\\n        input_var = make_variable_all_input(input, cuda=cuda)\\n\\n        # compute output\\n        output, loss = forward_backward(model, input_var, criterion, optimizer, cuda)\\n\\n        # measure elapsed time\\n        batch_time.update(time.time() - end)\\n        end = time.time()\\n\\n        if i % options[\\'print_freq\\'] == 0:\\n            # Do no waste time at computing loss and metric at each iteration of the training process\\n            metric_val, metric_avg, *_ = update_metric_loss(input, output, metric, loss, losses)\\n\\n            time_done = get_time_to_print(batch_time.avg * (i + 1))\\n            time_remaining = get_time_to_print(batch_time.avg * len(data_loader))\\n            print(\\'Epoch: [{0}][{1}/{2}]\\\\t\\'\\n                  \\'Time {batch_time.val:.3f} ({batch_time.avg:.3f}) [{done} => {remaining}]\\\\t\\'\\n                  \\'Data {data_time.val:.3f} ({data_time.avg:.3f})\\\\t\\'\\n                  \\'Loss {loss.val:.4f} ({loss.avg:.4f})\\\\t\\'\\n                  \\'Metric {metric_val:.3f} ({metric_avg:.3f})\\'.format(\\n                epoch, i + 1, len(data_loader), batch_time=batch_time,\\n                data_time=data_time, loss=losses, metric_val=metric_val,\\n                metric_avg=metric_avg,\\n                done=time_done, remaining=time_remaining))\\n            sys.stdout.flush()\\n\\n    return losses.avg, metric_avg\\n\\n\\ndef validate(epoch, engine, options, cuda=False):\\n    # Timer\\n    batch_time = AverageMeter()\\n    losses = AverageMeter()\\n    data_time = AverageMeter()\\n\\n    # Engine\\n    model = engine[\\'model\\']\\n    criterion = engine[\\'criterion\\']\\n    metric = engine[\\'metric\\']()\\n    data_loader = engine[\\'val_loader\\']\\n\\n    # switch to evaluate mode\\n    model.eval()\\n\\n    # create the numpy array for storing the preds and actual target\\n    dict_id_good_preds, dict_id_failures_preds, dict_id_object = {}, {}, {}\\n\\n    end = time.time()\\n    nb_crops = data_loader.dataset.nb_crops\\n    print(\"\")\\n    with torch.no_grad():\\n        for i, input in enumerate(data_loader):\\n            # measure data loading time\\n            data_time.update(time.time() - end)\\n\\n            # Make Variables\\n            input_var = make_variable_all_input(input, cuda=cuda)\\n\\n            output_aggreg, loss_aggreg, obj_id_aggreg = None, None, None\\n            for j in range(nb_crops):\\n                # take the right clip\\n                input_var_j = input_var if nb_crops == 1 else take_clip_j(input_var, j)\\n                obj_id = input_var_j[\\'obj_id\\']\\n\\n                # compute output\\n                output, loss = forward_backward(model, input_var_j, criterion, None, cuda)\\n                # ipdb.set_trace()\\n\\n                # aggreg by summing\\n                if output_aggreg is None:\\n                    output_aggreg = output\\n                    loss_aggreg = loss\\n                    obj_id_aggreg = obj_id\\n                else:\\n                    output_aggreg += output\\n                    loss_aggreg += loss\\n                    obj_id_aggreg += obj_id\\n\\n            # measure accuracy and record loss\\n            output_aggreg /= nb_crops\\n            loss_aggreg /= nb_crops\\n            obj_id_aggreg /= nb_crops\\n            metric_val, metric_avg, list_idx_correct_preds = update_metric_loss(input, output_aggreg, metric,\\n                                                                                loss_aggreg, losses)\\n\\n            # store good and failure cases and detected object\\n            dict_id_good_i, dict_id_failures_i, dict_obj_i = store_preds(\\n                output_aggreg, input[\\'id\\'], list_idx_correct_preds,\\n                obj_id_aggreg,\\n                options[\\'dataset\\'])\\n            dict_id_good_preds.update(dict_id_good_i)\\n            dict_id_failures_preds.update(dict_id_failures_i)\\n            dict_id_object.update(dict_obj_i)\\n\\n\\n            # measure elapsed time\\n            batch_time.update(time.time() - end)\\n            end = time.time()\\n\\n            if (i + 1) % options[\\'print_freq\\'] == 0:\\n                time_done = get_time_to_print(batch_time.avg * (i + 1))\\n                time_remaining = get_time_to_print(batch_time.avg * len(data_loader))\\n                print(\\'Test: [{0}/{1}]\\\\t\\'\\n                      \\'Time {batch_time.val:.3f} ({batch_time.avg:.3f}) [{done} => {remaining}]\\\\t\\'\\n                      \\'Data {data_time.val:.3f} ({data_time.avg:.3f})\\\\t\\'\\n                      \\'Loss {loss.val:.4f} ({loss.avg:.4f})\\\\t\\'\\n                      \\'Metric {metric_val:.3f} ({metric_avg:.3f})\\'.format(\\n                    i + 1, len(data_loader), batch_time=batch_time,\\n                    data_time=data_time, loss=losses, metric_val=metric_val,\\n                    metric_avg=metric_avg,\\n                    done=time_done, remaining=time_remaining))\\n                sys.stdout.flush()\\n\\n    # Finally compute the true mean metric over the overall val set\\n    metric.all_dataset = True\\n    _, metric_avg, per_class_metric_avg = metric.value()\\n\\n    print(\\' * Metric {metric_avg:.3f}\\'.format(metric_avg=metric_avg))\\n    sys.stdout.flush()\\n\\n    # Pandas frame - good & failure\\n    df_good = pandas.DataFrame.from_dict(dict_id_good_preds)\\n    df_failures = pandas.DataFrame.from_dict(dict_id_failures_preds)\\n    df_objects =  pandas.DataFrame.from_dict(dict_id_object)\\n\\n    return losses.avg, metric_avg, per_class_metric_avg, df_good, df_failures, df_objects\\n'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contents[50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "w-IeZWq1OU2R",
        "outputId": "5ae8ee9a-3ae3-4589-80cf-5b6dc2054f2c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8b8be457-0673-4710-98cf-2cfcc0f9161d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper</th>\n",
              "      <th>link</th>\n",
              "      <th>code</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Temporal coherence-based self-supervised learn...</td>\n",
              "      <td>http://arxiv.org/pdf/1806.06811v2.pdf</td>\n",
              "      <td>\"\"\"Self-supervised pretraining based on tempor...</td>\n",
              "      <td>Temporalcoherence-basedself-supervised\\nlearni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BinGAN: Learning Compact Binary Descriptors wi...</td>\n",
              "      <td>http://arxiv.org/pdf/1806.06778v5.pdf</td>\n",
              "      <td>import numpy as np\\n\\n\\ndef hamming_dist(test_...</td>\n",
              "      <td>BinGAN:LearningCompactBinaryDescriptors\\nwitha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Multiscale Fisher's Independence Test for Mult...</td>\n",
              "      <td>https://arxiv.org/pdf/1806.06777v7.pdf</td>\n",
              "      <td></td>\n",
              "      <td>MultiscaleFisher'sIndependenceTestfor\\nMultiva...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>POTs: Protective Optimization Technologies</td>\n",
              "      <td>https://arxiv.org/pdf/1806.02711v6.pdf</td>\n",
              "      <td>\"\"\"\\nA hash function for numpy arrays.\\n\"\"\"\\n\\...</td>\n",
              "      <td>POTs:ProtectiveOptimizationTechnologies\\nBogda...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Surface Networks</td>\n",
              "      <td>http://arxiv.org/pdf/1705.10819v2.pdf</td>\n",
              "      <td>'''\\nThis file is part of source code for \"Sur...</td>\n",
              "      <td>SurfaceNetworks\\nIlyaKostrikov\\n1\\n,ZhongshiJi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b8be457-0673-4710-98cf-2cfcc0f9161d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b8be457-0673-4710-98cf-2cfcc0f9161d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b8be457-0673-4710-98cf-2cfcc0f9161d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               paper  \\\n",
              "0  Temporal coherence-based self-supervised learn...   \n",
              "1  BinGAN: Learning Compact Binary Descriptors wi...   \n",
              "2  Multiscale Fisher's Independence Test for Mult...   \n",
              "3         POTs: Protective Optimization Technologies   \n",
              "4                                   Surface Networks   \n",
              "\n",
              "                                     link  \\\n",
              "0   http://arxiv.org/pdf/1806.06811v2.pdf   \n",
              "1   http://arxiv.org/pdf/1806.06778v5.pdf   \n",
              "2  https://arxiv.org/pdf/1806.06777v7.pdf   \n",
              "3  https://arxiv.org/pdf/1806.02711v6.pdf   \n",
              "4   http://arxiv.org/pdf/1705.10819v2.pdf   \n",
              "\n",
              "                                                code  \\\n",
              "0  \"\"\"Self-supervised pretraining based on tempor...   \n",
              "1  import numpy as np\\n\\n\\ndef hamming_dist(test_...   \n",
              "2                                                      \n",
              "3  \"\"\"\\nA hash function for numpy arrays.\\n\"\"\"\\n\\...   \n",
              "4  '''\\nThis file is part of source code for \"Sur...   \n",
              "\n",
              "                                                text  \n",
              "0  Temporalcoherence-basedself-supervised\\nlearni...  \n",
              "1  BinGAN:LearningCompactBinaryDescriptors\\nwitha...  \n",
              "2  MultiscaleFisher'sIndependenceTestfor\\nMultiva...  \n",
              "3  POTs:ProtectiveOptimizationTechnologies\\nBogda...  \n",
              "4  SurfaceNetworks\\nIlyaKostrikov\\n1\\n,ZhongshiJi...  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extractionsdf = datalinksdf\n",
        "extractionsdf['code'] = contents\n",
        "extractionsdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BvYyPR2OmbJ"
      },
      "outputs": [],
      "source": [
        "extractionsdf.to_csv('code-extractions-100.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "mSuo5xDEzfwU",
        "outputId": "55732f5d-4baf-4147-cbb2-6c1ffb378a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d21b523c2936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'code-extractions-100.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: argument of type 'float' is not iterable"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('code-extractions-100.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "code-extract.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}