,id,code
0,2201.02849,"import argparse
import pickle
import os

import numpy as np
from tqdm import tqdm

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', required=True, choices={'ntu/xsub', 'ntu/xview', 'ntu120/xsub', 'ntu120/xset'}, help='the work folder for storing results')
    parser.add_argument('--alpha', default=1, help='weighted summation', type=float)
    parser.add_argument('--joint_dir', help='Directory containing ""score.pkl"" for joint eval results')
    parser.add_argument('--bone_dir', help='Directory containing ""score.pkl"" for bone eval results')
    parser.add_argument('--joint_motion_dir', default=None)
    parser.add_argument('--bone_motion_dir', default=None)

    arg = parser.parse_args()

    dataset = arg.dataset
    if 'ntu' in arg.dataset:
        if 'xsub' in arg.dataset:
            npz_data = np.load('./gendata/' + 'ntu/' + 'NTU60_XSub.npz')
            label = np.where(npz_data['y_test'] > 0)[1]
        elif 'xview' in arg.dataset:
            npz_data = np.load('./gendata/' + 'ntu/' + 'NTU60_XView.npz')
            label = np.where(npz_data['y_test'] > 0)[1]
    elif 'ntu120' in arg.dataset:
        if 'xsub' in arg.dataset:
            npz_data = np.load('./gendata/' + 'ntu120/' + 'NTU120_XSub.npz')
            label = np.where(npz_data['y_test'] > 0)[1]
        elif 'xset' in arg.dataset:
            npz_data = np.load('./gendata/' + 'ntu120/' + 'NTU120_XSet.npz')
            label = np.where(npz_data['y_test'] > 0)[1]
    else:
        raise NotImplementedError

    with open(os.path.join(arg.joint_dir, 'score.pkl'), 'rb') as r1:
        r1 = list(pickle.load(r1).items())

    with open(os.path.join(arg.bone_dir, 'score.pkl'), 'rb') as r2:
        r2 = list(pickle.load(r2).items())

    if arg.joint_motion_dir is not None:
        with open(os.path.join(arg.joint_motion_dir, 'score.pkl'), 'rb') as r3:
            r3 = list(pickle.load(r3).items())
    if arg.bone_motion_dir is not None:
        with open(os.path.join(arg.bone_motion_dir, 'score.pkl'), 'rb') as r4:
            r4 = list(pickle.load(r4).items())

    right_num = total_num = right_num_5 = 0

    if arg.joint_motion_dir is not None and arg.bone_motion_dir is not None:
        arg.alpha = [0.6, 0.6, 0.4, 0.4]
        for i in tqdm(range(len(label))):
            l = label[i]
            _, r11 = r1[i]
            _, r22 = r2[i]
            _, r33 = r3[i]
            _, r44 = r4[i]
            r = r11 * arg.alpha[0] + r22 * arg.alpha[1] + r33 * arg.alpha[2] + r44 * arg.alpha[3]
            rank_5 = r.argsort()[-5:]
            right_num_5 += int(int(l) in rank_5)
            r = np.argmax(r)
            right_num += int(r == int(l))
            total_num += 1
        acc = right_num / total_num
        acc5 = right_num_5 / total_num
    elif arg.joint_motion_dir is not None and arg.bone_motion_dir is None:
        arg.alpha = [0.6, 0.6, 0.4]
        for i in tqdm(range(len(label))):
            l = label[i]
            _, r11 = r1[i]
            _, r22 = r2[i]
            _, r33 = r3[i]
            r = r11 * arg.alpha[0] + r22 * arg.alpha[1] + r33 * arg.alpha[2]
            rank_5 = r.argsort()[-5:]
            right_num_5 += int(int(l) in rank_5)
            r = np.argmax(r)
            right_num += int(r == int(l))
            total_num += 1
        acc = right_num / total_num
        acc5 = right_num_5 / total_num
    else:
        for i in tqdm(range(len(label))):
            l = label[i]
            _, r11 = r1[i]
            _, r22 = r2[i]
            r = r11 + r22 * arg.alpha
            rank_5 = r.argsort()[-5:]
            right_num_5 += int(int(l) in rank_5)
            r = np.argmax(r)
            right_num += int(r == int(l))
            total_num += 1
        acc = right_num / total_num
        acc5 = right_num_5 / total_num

    print('Top1 Acc: {:.4f}%'.format(acc * 100))
    print('Top5 Acc: {:.4f}%'.format(acc5 * 100))
"
1,2201.02849,"#!/usr/bin/env python
import argparse
import os
import pickle
import random
import shutil
import sys
import time
from collections import OrderedDict
from sklearn.metrics import confusion_matrix
import csv
import numpy as np
import glob
# torch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import yaml
from tensorboardX import SummaryWriter
from tqdm import tqdm


def init_seed(seed):
    torch.cuda.manual_seed_all(1)
    torch.manual_seed(1)
    np.random.seed(1)
    random.seed(1)
    # torch.backends.cudnn.enabled = False
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def import_class(name):
    components = name.split('.')
    mod = __import__(components[0])  # import return model
    for comp in components[1:]:
        mod = getattr(mod, comp)
    return mod

def str2bool(v):
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Unsupported value encountered.')


def get_parser():
    # parameter priority: command line > config > default
    parser = argparse.ArgumentParser(description='Spatial Temporal Tuples Transformer')
    parser.add_argument('--work_dir', default='./work_dir/ntu60/temp', help='the work folder for storing results')
    parser.add_argument('--config', default='./config/ntu60_xsub_joint.yaml', help='path to the configuration file')

    # processor
    parser.add_argument('--run_mode', default='train', help='must be train or test')
    parser.add_argument('--save_score', type=str2bool, default=False, help='if ture, the classification score will be stored')

    # visulize and debug
    parser.add_argument('--save_epoch', type=int, default=80, help='the start epoch to save model (#iteration)')
    parser.add_argument('--eval_interval', type=int, default=5, help='the interval for evaluating models (#iteration)')
    parser.add_argument('--print_log', type=str2bool, default=True, help='print logging or not')
    parser.add_argument('--show_topk', type=int, default=[1, 5], nargs='+', help='which Top K accuracy will be shown')

    # feeder
    parser.add_argument('--feeder', default='feeder.feeder', help='data loader will be used')
    parser.add_argument('--num_worker', type=int, default=8, help='the number of worker for data loader')
    parser.add_argument('--train_feeder_args', default=dict(), help='the arguments of data loader for training')
    parser.add_argument('--test_feeder_args', default=dict(), help='the arguments of data loader for test')

    # model
    parser.add_argument('--model', default=None, help='the model will be used')
    parser.add_argument('--model_args', default=dict(), help='the arguments of model')
    parser.add_argument('--weights', default=None, help='the weights for model testing')
    parser.add_argument('--ignore_weights', type=str, default=[], nargs='+', help='the name of weights which will be ignored in the initialization')

    # optim
    parser.add_argument('--base_lr', type=float, default=0.1, help='initial learning rate')
    parser.add_argument('--step', type=int, default=[60, 80], nargs='+', help='the epoch where optimizer reduce the learning rate')
    parser.add_argument('--cuda_visible_device', default='2,3', help='')
    parser.add_argument('--device', type=int, default=[0,1], nargs='+', help='the indexes of GPUs for training or testing')

    parser.add_argument('--optimizer', default='SGD', help='type of optimizer')
    parser.add_argument('--nesterov', type=str2bool, default=False, help='use nesterov or not')
    parser.add_argument('--batch_size', type=int, default=256, help='training batch size')
    parser.add_argument('--test_batch_size', type=int, default=256, help='test batch size')
    parser.add_argument('--start_epoch', type=int, default=0, help='start training from which epoch')
    parser.add_argument('--num_epoch', type=int, default=80, help='stop training in which epoch')
    parser.add_argument('--weight_decay', type=float, default=0.0005, help='weight decay for optimizer')
    parser.add_argument('--lr_decay_rate', type=float, default=0.1, help='decay rate for learning rate')
    parser.add_argument('--warm_up_epoch', type=int, default=5)

    return parser


def accuracy(output, target, topk=(1,)):
    """"""Computes the precision@k for the specified values of k""""""
    maxk = max(topk)
    batch_size = target.size(0)

    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))

    res = []
    for k in topk:
        correct_k = correct[:k].contiguous().view(-1).float().sum(0)
        res.append(correct_k.mul_(100.0 / batch_size))
    return res


class AverageMeter(object):
    """"""Computes and stores the average and current value""""""
    def __init__(self):
        self.reset()

    def reset(self):
        self.value = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, value, n=1):
        self.value = value
        self.sum += value * n
        self.count += n
        self.avg = self.sum / self.count


class Processor():
    """""" Processor for Skeleton-based Action Recgnition """"""

    def __init__(self, arg):
        self.arg = arg
        self.global_step = 0
        self.lr = self.arg.base_lr
        self.best_acc = 0

        if not os.path.exists(self.arg.work_dir):
            os.makedirs(self.arg.work_dir)
        self.load_model()
        self.load_data() 

        if arg.run_mode == 'train':
            result_visual = os.path.join(arg.work_dir, 'runs')
            if not arg.train_feeder_args['debug']:
                if os.path.isdir(result_visual):
                    print('log_dir: ', result_visual, 'already exist')
                    answer = input('delete it? y/n:')
                    if answer == 'y':
                        shutil.rmtree(result_visual)
                        print('Dir removed: ', result_visual)
                        input('Refresh the website of tensorboard by pressing any keys')
                    else:
                        print('Dir not removed: ', result_visual)
                self.train_writer = SummaryWriter(os.path.join(result_visual, 'train'), 'train')
                self.val_writer = SummaryWriter(os.path.join(result_visual, 'val'), 'val')
                
                self.load_optimizer()
            else:
                self.train_writer = self.val_writer = SummaryWriter(os.path.join(result_visual, 'test'), 'test')


        self.model = self.model.cuda(self.output_device)

        if type(self.arg.device) is list:
            if len(self.arg.device) > 1:
                self.model = nn.DataParallel(self.model, device_ids=self.arg.device, output_device=self.output_device)

    def load_data(self):
        Feeder = import_class(self.arg.feeder)
        self.data_loader = dict()
        if self.arg.run_mode == 'train':
            self.data_loader['train'] = DataLoader(
                dataset=Feeder(**self.arg.train_feeder_args),
                batch_size=self.arg.batch_size,
                shuffle=True,
                num_workers=self.arg.num_worker,
                drop_last=True,
                worker_init_fn=init_seed)
        self.data_loader['test'] = DataLoader(
            dataset=Feeder(**self.arg.test_feeder_args),
            batch_size=self.arg.test_batch_size,
            shuffle=False,
            num_workers=self.arg.num_worker,
            drop_last=False,
            worker_init_fn=init_seed)
        self.print_log('Data load finished')

    def load_model(self):
        output_device = self.arg.device[0] if type(self.arg.device) is list else self.arg.device
        self.output_device = output_device
        Model = import_class(self.arg.model)
        # print(Model)
        self.model = Model(**self.arg.model_args)
        # print(self.model)
        self.loss = nn.CrossEntropyLoss().cuda(output_device)

        if self.arg.weights:
            # self.global_step = int(arg.weights[:-3].split('-')[-1])
            self.print_log('Load weights from {}'.format(self.arg.weights))
            if '.pkl' in self.arg.weights:
                with open(self.arg.weights, 'r') as f:
                    weights = pickle.load(f)
            else:
                weights = torch.load(self.arg.weights)

            weights = OrderedDict([[k.split('module.')[-1], v.cuda(output_device)] for k, v in weights.items()])

            keys = list(weights.keys())
            for w in self.arg.ignore_weights:
                for key in keys:
                    if w in key:
                        if weights.pop(key, None) is not None:
                            self.print_log('Sucessfully Remove Weights: {}.'.format(key))
                        else:
                            self.print_log('Can Not Remove Weights: {}.'.format(key))

            try:
                self.model.load_state_dict(weights)
            except:
                state = self.model.state_dict()
                diff = list(set(state.keys()).difference(set(weights.keys())))
                print('Can not find these weights:')
                for d in diff:
                    print('  ' + d)
                state.update(weights)
                self.model.load_state_dict(state)
        self.print_log('Model load finished: ' + self.arg.model)

    def load_optimizer(self):
        if self.arg.optimizer == 'SGD':
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=self.arg.base_lr,
                momentum=0.9,
                nesterov=self.arg.nesterov,
                weight_decay=self.arg.weight_decay)
        elif self.arg.optimizer == 'Adam':
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=self.arg.base_lr,
                weight_decay=self.arg.weight_decay)
        else:
            raise ValueError()
        self.print_log('Optimizer load finished: ' + self.arg.optimizer)

    def adjust_learning_rate(self, epoch):
        self.print_log('adjust learning rate, using warm up, epoch: {}'.format(self.arg.warm_up_epoch))
        if self.arg.optimizer == 'SGD' or self.arg.optimizer == 'Adam':
            if epoch < self.arg.warm_up_epoch:
                lr = self.arg.base_lr * (epoch + 1) / self.arg.warm_up_epoch
            else:
                lr = self.arg.base_lr * ( self.arg.lr_decay_rate ** np.sum(epoch >= np.array(self.arg.step)))
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
            return lr
        else:
            raise ValueError()

    def print_log(self, str, print_time=True):
        if print_time:
            localtime = time.strftime('%Y-%m-%d %H:%M', time.localtime(time.time()))
            str = ""[ "" + localtime + ' ] ' + str
        print(str)
        if self.arg.print_log:
            with open('{}/log.txt'.format(self.arg.work_dir), 'a') as f:
                print(str, file=f)

    def train(self, epoch, save_model=False):
        losses = AverageMeter()
        top1 = AverageMeter()
        top5 = AverageMeter()

        self.model.train()
        self.adjust_learning_rate(epoch)
 
        self.train_writer.add_scalar('epoch', epoch, self.global_step)

        for batch, (data, label, sample) in enumerate(tqdm(self.data_loader['train'], desc=""Training"", ncols=100)):
            self.global_step += 1
            with torch.no_grad():
                data = data.float().cuda(self.output_device)
                label = label.long().cuda(self.output_device)

            # forward
            output = self.model(data)
            loss = self.loss(output, label)
            # backward
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            prec1, prec5 = accuracy(output.data, label, topk=(1, 5))
            top1.update(prec1.item(), data.size(0))
            top5.update(prec5.item(), data.size(0))
            losses.update(loss.item())

            self.train_writer.add_scalar('top1', top1.avg, self.global_step)
            self.train_writer.add_scalar('loss', losses.avg, self.global_step)

            # statistics
            self.lr = self.optimizer.param_groups[0]['lr']
            self.train_writer.add_scalar('lr', self.lr, self.global_step)

        self.print_log('training: epoch: {}, loss: {:.4f}, top1: {:.2f}%, lr: {:.6f}'.format(
            epoch + 1, losses.avg, top1.avg, self.lr))

        if  epoch + 1 == self.arg.num_epoch:
            state_dict = self.model.state_dict()
            weights = OrderedDict([[k.split('module.')[-1], v.cpu()] for k, v in state_dict.items()])
            torch.save(weights, self.arg.work_dir + '/' + self.arg.work_dir.split('/')[-1] + '.pt')

    def eval(self, epoch, save_score=False, loader_name=['test'], wrong_file=None, result_file=None):
        losses = AverageMeter()
        top1 = AverageMeter()
        top5 = AverageMeter()

        if wrong_file is not None:
            f_w = open(wrong_file, 'w')
        if result_file is not None:
            f_r = open(result_file, 'w')
        self.model.eval()
        for ln in loader_name:
            score_frag = []
            label_list = []
            pred_list = []
            for batch, (data, label, sampie) in enumerate(tqdm(self.data_loader[ln], desc=""Evaluating"", ncols=100)):
                label_list.append(label)
                with torch.no_grad():
                    data = data.float().cuda(self.output_device)
                    label = label.long().cuda(self.output_device)
                    output = self.model(data)
                    loss = self.loss(output, label)
                    
                    score_frag.append(output.data.cpu().numpy())
                    _, predict_label = torch.max(output.data, 1)
                    pred_list.append(predict_label.data.cpu().numpy())

                prec1, prec5 = accuracy(output.data, label, topk=(1, 5))
                top1.update(prec1.item(), data.size(0))
                top5.update(prec5.item(), data.size(0))
                losses.update(loss.item())

                if wrong_file is not None or result_file is not None:
                    predict = list(predict_label.cpu().numpy())
                    true = list(label.data.cpu().numpy())
                    for i, x in enumerate(predict):
                        if result_file is not None:
                            f_r.write(str(x) + ',' + str(true[i]) + '\n')
                        if x != true[i] and wrong_file is not None:
                            f_w.write(str(sampie[i]) + ',' + str(x) + ',' + str(true[i]) + '\n')

            score = np.concatenate(score_frag)
            score_dict = dict(zip(self.data_loader[ln].dataset.sample_name, score))

            if self.arg.run_mode == 'train':
                self.val_writer.add_scalar('loss', top1.avg, self.global_step)
                self.val_writer.add_scalar('acc', losses.avg, self.global_step)

            self.best_acc = top1.avg if top1.avg > self.best_acc else self.best_acc
        
            self.print_log('evaluating: loss: {:.4f}, top1: {:.2f}%, best_acc: {:.2f}%'.format(losses.avg, top1.avg, self.best_acc))

            if save_score:
                with open('{}/score.pkl'.format(self.arg.work_dir), 'wb') as f:
                    pickle.dump(score_dict, f)

    def start(self):

        if self.arg.run_mode == 'train':

            for argument, value in sorted(vars(self.arg).items()):
                self.print_log('{}: {}'.format(argument, value))

            self.global_step = self.arg.start_epoch * len(self.data_loader['train']) / self.arg.batch_size

            def count_parameters(model):
                return sum(p.numel() for p in model.parameters() if p.requires_grad)
            self.print_log(f'# Parameters: {count_parameters(self.model)}')

            self.print_log('###***************start training***************###')

            for epoch in range(self.arg.start_epoch, self.arg.num_epoch):

                save_model = (epoch + 1 == self.arg.num_epoch)
                self.train(epoch, save_model=save_model)

                if ((epoch + 1) % self.arg.eval_interval == 0):
                    self.eval(epoch, save_score=self.arg.save_score, loader_name=['test'])
            self.print_log('Done.\n')

        elif self.arg.run_mode == 'test':
            if not self.arg.test_feeder_args['debug']:
                weights_path = self.arg.work_dir + '.pt'
                wf = self.arg.work_dir + '/wrong.txt'
                rf = self.arg.work_dir + '/right.txt'
            else:
                wf = rf = None

            if self.arg.weights is None:
                raise ValueError('Please appoint --weights.')
            self.arg.print_log = False
            self.print_log('Model:   {}'.format(self.arg.model))
            self.print_log('Weights: {}'.format(self.arg.weights))
            self.eval(epoch=0, save_score=self.arg.save_score, loader_name=['test'], wrong_file=wf, result_file=rf)
            self.print_log('Done.\n')

if __name__ == '__main__':
    parser = get_parser()

    # load arg form config file
    p = parser.parse_args()
    if p.config is not None:
        with open(p.config, 'r') as f:
            default_arg = yaml.load(f, yaml.FullLoader)
        key = vars(p).keys()
        for k in default_arg.keys():
            if k not in key:
                print('WRONG ARG: {}'.format(k))
                assert (k in key)
        parser.set_defaults(**default_arg)

    arg = parser.parse_args()
    os.environ['CUDA_VISIBLE_DEVICES'] = arg.cuda_visible_device
    init_seed(1)
    processor = Processor(arg)
    processor.start()
"
2,2201.02849,"# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
import os.path as osp
import os
import numpy as np
import pickle
import logging


def get_raw_bodies_data(skes_path, ske_name, frames_drop_skes, frames_drop_logger):
    """"""
    Get raw bodies data from a skeleton sequence.

    Each body's data is a dict that contains the following keys:
      - joints: raw 3D joints positions. Shape: (num_frames x 25, 3)
      - colors: raw 2D color locations. Shape: (num_frames, 25, 2)
      - interval: a list which stores the frame indices of this body.
      - motion: motion amount (only for the sequence with 2 or more bodyIDs).

    Return:
      a dict for a skeleton sequence with 3 key-value pairs:
        - name: the skeleton filename.
        - data: a dict which stores raw data of each body.
        - num_frames: the number of valid frames.
    """"""
    if int(ske_name[1:4]) >= 18:
        skes_path = '../nturgbd_raw/nturgb+d_skeletons120/'
    ske_file = osp.join(skes_path, ske_name + '.skeleton')
    assert osp.exists(ske_file), 'Error: Skeleton file %s not found' % ske_file
    # Read all data from .skeleton file into a list (in string format)
    print('Reading data from %s' % ske_file[-29:])
    with open(ske_file, 'r') as fr:
        str_data = fr.readlines()

    num_frames = int(str_data[0].strip('\r\n'))
    frames_drop = []
    bodies_data = dict()
    valid_frames = -1  # 0-based index
    current_line = 1

    for f in range(num_frames):
        num_bodies = int(str_data[current_line].strip('\r\n'))
        current_line += 1

        if num_bodies == 0:  # no data in this frame, drop it
            frames_drop.append(f)  # 0-based index
            continue

        valid_frames += 1
        joints = np.zeros((num_bodies, 25, 3), dtype=np.float32)
        colors = np.zeros((num_bodies, 25, 2), dtype=np.float32)

        for b in range(num_bodies):
            bodyID = str_data[current_line].strip('\r\n').split()[0]
            current_line += 1
            num_joints = int(str_data[current_line].strip('\r\n'))  # 25 joints
            current_line += 1

            for j in range(num_joints):
                temp_str = str_data[current_line].strip('\r\n').split()
                joints[b, j, :] = np.array(temp_str[:3], dtype=np.float32)
                colors[b, j, :] = np.array(temp_str[5:7], dtype=np.float32)
                current_line += 1

            if bodyID not in bodies_data:  # Add a new body's data
                body_data = dict()
                body_data['joints'] = joints[b]  # ndarray: (25, 3)
                body_data['colors'] = colors[b, np.newaxis]  # ndarray: (1, 25, 2)
                body_data['interval'] = [valid_frames]  # the index of the first frame
            else:  # Update an already existed body's data
                body_data = bodies_data[bodyID]
                # Stack each body's data of each frame along the frame order
                body_data['joints'] = np.vstack((body_data['joints'], joints[b]))
                body_data['colors'] = np.vstack((body_data['colors'], colors[b, np.newaxis]))
                pre_frame_idx = body_data['interval'][-1]
                body_data['interval'].append(pre_frame_idx + 1)  # add a new frame index

            bodies_data[bodyID] = body_data  # Update bodies_data

    num_frames_drop = len(frames_drop)
    assert num_frames_drop < num_frames, \
        'Error: All frames data (%d) of %s is missing or lost' % (num_frames, ske_name)
    if num_frames_drop > 0:
        frames_drop_skes[ske_name] = np.array(frames_drop, dtype=np.int)
        frames_drop_logger.info('{}: {} frames missed: {}\n'.format(ske_name, num_frames_drop,
                                                                    frames_drop))

    # Calculate motion (only for the sequence with 2 or more bodyIDs)
    if len(bodies_data) > 1:
        for body_data in bodies_data.values():
            body_data['motion'] = np.sum(np.var(body_data['joints'], axis=0))

    return {'name': ske_name, 'data': bodies_data, 'num_frames': num_frames - num_frames_drop}


def get_raw_skes_data():
    # # save_path = './data'
    # # skes_path = '/data/pengfei/NTU/nturgb+d_skeletons/'
    # stat_path = osp.join(save_path, 'statistics')
    #
    # skes_name_file = osp.join(stat_path, 'skes_available_name.txt')
    # save_data_pkl = osp.join(save_path, 'raw_skes_data.pkl')
    # frames_drop_pkl = osp.join(save_path, 'frames_drop_skes.pkl')
    #
    # frames_drop_logger = logging.getLogger('frames_drop')
    # frames_drop_logger.setLevel(logging.INFO)
    # frames_drop_logger.addHandler(logging.FileHandler(osp.join(save_path, 'frames_drop.log')))
    # frames_drop_skes = dict()

    skes_name = np.loadtxt(skes_name_file, dtype=str)

    num_files = skes_name.size
    print('Found %d available skeleton files.' % num_files)

    raw_skes_data = []
    frames_cnt = np.zeros(num_files, dtype=np.int)

    for (idx, ske_name) in enumerate(skes_name):
        bodies_data = get_raw_bodies_data(skes_path, ske_name, frames_drop_skes, frames_drop_logger)
        raw_skes_data.append(bodies_data)
        frames_cnt[idx] = bodies_data['num_frames']
        if (idx + 1) % 1000 == 0:
            print('Processed: %.2f%% (%d / %d)' % \
                  (100.0 * (idx + 1) / num_files, idx + 1, num_files))

    with open(save_data_pkl, 'wb') as fw:
        pickle.dump(raw_skes_data, fw, pickle.HIGHEST_PROTOCOL)
    np.savetxt(osp.join(save_path, 'raw_data', 'frames_cnt.txt'), frames_cnt, fmt='%d')

    print('Saved raw bodies data into %s' % save_data_pkl)
    print('Total frames: %d' % np.sum(frames_cnt))

    with open(frames_drop_pkl, 'wb') as fw:
        pickle.dump(frames_drop_skes, fw, pickle.HIGHEST_PROTOCOL)

if __name__ == '__main__':
    save_path = './'

    skes_path = '../nturgbd_raw/nturgb+d_skeletons/'
    stat_path = osp.join(save_path, 'statistics')
    if not osp.exists('./raw_data'):
        os.makedirs('./raw_data')

    skes_name_file = osp.join(stat_path, 'skes_available_name.txt')
    save_data_pkl = osp.join(save_path, 'raw_data', 'raw_skes_data.pkl')
    frames_drop_pkl = osp.join(save_path, 'raw_data', 'frames_drop_skes.pkl')

    frames_drop_logger = logging.getLogger('frames_drop')
    frames_drop_logger.setLevel(logging.INFO)
    frames_drop_logger.addHandler(logging.FileHandler(osp.join(save_path, 'raw_data', 'frames_drop.log')))
    frames_drop_skes = dict()

    get_raw_skes_data()

    with open(frames_drop_pkl, 'wb') as fw:
        pickle.dump(frames_drop_skes, fw, pickle.HIGHEST_PROTOCOL)
        
"
3,2201.02849,"# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
import os
import os.path as osp
import numpy as np
import pickle
import logging

root_path = './'
raw_data_file = osp.join(root_path, 'raw_data', 'raw_skes_data.pkl')
save_path = osp.join(root_path, 'denoised_data')

if not osp.exists(save_path):
    os.mkdir(save_path)

rgb_ske_path = osp.join(save_path, 'rgb+ske')
if not osp.exists(rgb_ske_path):
    os.mkdir(rgb_ske_path)

actors_info_dir = osp.join(save_path, 'actors_info')
if not osp.exists(actors_info_dir):
    os.mkdir(actors_info_dir)

missing_count = 0
noise_len_thres = 11
noise_spr_thres1 = 0.8
noise_spr_thres2 = 0.69754
noise_mot_thres_lo = 0.089925
noise_mot_thres_hi = 2

noise_len_logger = logging.getLogger('noise_length')
noise_len_logger.setLevel(logging.INFO)
noise_len_logger.addHandler(logging.FileHandler(osp.join(save_path, 'noise_length.log')))
noise_len_logger.info('{:^20}\t{:^17}\t{:^8}\t{}'.format('Skeleton', 'bodyID', 'Motion', 'Length'))

noise_spr_logger = logging.getLogger('noise_spread')
noise_spr_logger.setLevel(logging.INFO)
noise_spr_logger.addHandler(logging.FileHandler(osp.join(save_path, 'noise_spread.log')))
noise_spr_logger.info('{:^20}\t{:^17}\t{:^8}\t{:^8}'.format('Skeleton', 'bodyID', 'Motion', 'Rate'))

noise_mot_logger = logging.getLogger('noise_motion')
noise_mot_logger.setLevel(logging.INFO)
noise_mot_logger.addHandler(logging.FileHandler(osp.join(save_path, 'noise_motion.log')))
noise_mot_logger.info('{:^20}\t{:^17}\t{:^8}'.format('Skeleton', 'bodyID', 'Motion'))

fail_logger_1 = logging.getLogger('noise_outliers_1')
fail_logger_1.setLevel(logging.INFO)
fail_logger_1.addHandler(logging.FileHandler(osp.join(save_path, 'denoised_failed_1.log')))

fail_logger_2 = logging.getLogger('noise_outliers_2')
fail_logger_2.setLevel(logging.INFO)
fail_logger_2.addHandler(logging.FileHandler(osp.join(save_path, 'denoised_failed_2.log')))

missing_skes_logger = logging.getLogger('missing_frames')
missing_skes_logger.setLevel(logging.INFO)
missing_skes_logger.addHandler(logging.FileHandler(osp.join(save_path, 'missing_skes.log')))
missing_skes_logger.info('{:^20}\t{}\t{}'.format('Skeleton', 'num_frames', 'num_missing'))

missing_skes_logger1 = logging.getLogger('missing_frames_1')
missing_skes_logger1.setLevel(logging.INFO)
missing_skes_logger1.addHandler(logging.FileHandler(osp.join(save_path, 'missing_skes_1.log')))
missing_skes_logger1.info('{:^20}\t{}\t{}\t{}\t{}\t{}'.format('Skeleton', 'num_frames', 'Actor1',
                                                              'Actor2', 'Start', 'End'))

missing_skes_logger2 = logging.getLogger('missing_frames_2')
missing_skes_logger2.setLevel(logging.INFO)
missing_skes_logger2.addHandler(logging.FileHandler(osp.join(save_path, 'missing_skes_2.log')))
missing_skes_logger2.info('{:^20}\t{}\t{}\t{}'.format('Skeleton', 'num_frames', 'Actor1', 'Actor2'))


def denoising_by_length(ske_name, bodies_data):
    """"""
    Denoising data based on the frame length for each bodyID.
    Filter out the bodyID which length is less or equal than the predefined threshold.

    """"""
    noise_info = str()
    new_bodies_data = bodies_data.copy()
    for (bodyID, body_data) in new_bodies_data.items():
        length = len(body_data['interval'])
        if length <= noise_len_thres:
            noise_info += 'Filter out: %s, %d (length).\n' % (bodyID, length)
            noise_len_logger.info('{}\t{}\t{:.6f}\t{:^6d}'.format(ske_name, bodyID,
                                                                  body_data['motion'], length))
            del bodies_data[bodyID]
    if noise_info != '':
        noise_info += '\n'

    return bodies_data, noise_info


def get_valid_frames_by_spread(points):
    """"""
    Find the valid (or reasonable) frames (index) based on the spread of X and Y.

    points: joints or colors
    """"""
    num_frames = points.shape[0]
    valid_frames = []
    for i in range(num_frames):
        x = points[i, :, 0]
        y = points[i, :, 1]
        if (x.max() - x.min()) <= noise_spr_thres1 * (y.max() - y.min()):  # 0.8
            valid_frames.append(i)
    return valid_frames


def denoising_by_spread(ske_name, bodies_data):
    """"""
    Denoising data based on the spread of Y value and X value.
    Filter out the bodyID which the ratio of noisy frames is higher than the predefined
    threshold.

    bodies_data: contains at least 2 bodyIDs
    """"""
    noise_info = str()
    denoised_by_spr = False  # mark if this sequence has been processed by spread.

    new_bodies_data = bodies_data.copy()
    # for (bodyID, body_data) in bodies_data.items():
    for (bodyID, body_data) in new_bodies_data.items():
        if len(bodies_data) == 1:
            break
        valid_frames = get_valid_frames_by_spread(body_data['joints'].reshape(-1, 25, 3))
        num_frames = len(body_data['interval'])
        num_noise = num_frames - len(valid_frames)
        if num_noise == 0:
            continue

        ratio = num_noise / float(num_frames)
        motion = body_data['motion']
        if ratio >= noise_spr_thres2:  # 0.69754
            del bodies_data[bodyID]
            denoised_by_spr = True
            noise_info += 'Filter out: %s (spread rate >= %.2f).\n' % (bodyID, noise_spr_thres2)
            noise_spr_logger.info('%s\t%s\t%.6f\t%.6f' % (ske_name, bodyID, motion, ratio))
        else:  # Update motion
            joints = body_data['joints'].reshape(-1, 25, 3)[valid_frames]
            body_data['motion'] = min(motion, np.sum(np.var(joints.reshape(-1, 3), axis=0)))
            noise_info += '%s: motion %.6f -> %.6f\n' % (bodyID, motion, body_data['motion'])
            # TODO: Consider removing noisy frames for each bodyID

    if noise_info != '':
        noise_info += '\n'

    return bodies_data, noise_info, denoised_by_spr


def denoising_by_motion(ske_name, bodies_data, bodies_motion):
    """"""
    Filter out the bodyID which motion is out of the range of predefined interval

    """"""
    # Sort bodies based on the motion, return a list of tuples
    # bodies_motion = sorted(bodies_motion.items(), key=lambda x, y: cmp(x[1], y[1]), reverse=True)
    bodies_motion = sorted(bodies_motion.items(), key=lambda x: x[1], reverse=True)

    # Reserve the body data with the largest motion
    denoised_bodies_data = [(bodies_motion[0][0], bodies_data[bodies_motion[0][0]])]
    noise_info = str()

    for (bodyID, motion) in bodies_motion[1:]:
        if (motion < noise_mot_thres_lo) or (motion > noise_mot_thres_hi):
            noise_info += 'Filter out: %s, %.6f (motion).\n' % (bodyID, motion)
            noise_mot_logger.info('{}\t{}\t{:.6f}'.format(ske_name, bodyID, motion))
        else:
            denoised_bodies_data.append((bodyID, bodies_data[bodyID]))
    if noise_info != '':
        noise_info += '\n'

    return denoised_bodies_data, noise_info


def denoising_bodies_data(bodies_data):
    """"""
    Denoising data based on some heuristic methods, not necessarily correct for all samples.

    Return:
      denoised_bodies_data (list): tuple: (bodyID, body_data).
    """"""
    ske_name = bodies_data['name']
    bodies_data = bodies_data['data']

    # Step 1: Denoising based on frame length.
    bodies_data, noise_info_len = denoising_by_length(ske_name, bodies_data)

    if len(bodies_data) == 1:  # only has one bodyID left after step 1
        return bodies_data.items(), noise_info_len

    # Step 2: Denoising based on spread.
    bodies_data, noise_info_spr, denoised_by_spr = denoising_by_spread(ske_name, bodies_data)

    if len(bodies_data) == 1:
        return bodies_data.items(), noise_info_len + noise_info_spr

    bodies_motion = dict()  # get body motion
    for (bodyID, body_data) in bodies_data.items():
        bodies_motion[bodyID] = body_data['motion']
    # Sort bodies based on the motion
    # bodies_motion = sorted(bodies_motion.items(), key=lambda x, y: cmp(x[1], y[1]), reverse=True)
    bodies_motion = sorted(bodies_motion.items(), key=lambda x: x[1], reverse=True)
    denoised_bodies_data = list()
    for (bodyID, _) in bodies_motion:
        denoised_bodies_data.append((bodyID, bodies_data[bodyID]))

    return denoised_bodies_data, noise_info_len + noise_info_spr

    # TODO: Consider denoising further by integrating motion method

    # if denoised_by_spr:  # this sequence has been denoised by spread
    #     bodies_motion = sorted(bodies_motion.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)
    #     denoised_bodies_data = list()
    #     for (bodyID, _) in bodies_motion:
    #         denoised_bodies_data.append((bodyID, bodies_data[bodyID]))
    #     return denoised_bodies_data, noise_info

    # Step 3: Denoising based on motion
    # bodies_data, noise_info = denoising_by_motion(ske_name, bodies_data, bodies_motion)

    # return bodies_data, noise_info


def get_one_actor_points(body_data, num_frames):
    """"""
    Get joints and colors for only one actor.
    For joints, each frame contains 75 X-Y-Z coordinates.
    For colors, each frame contains 25 x 2 (X, Y) coordinates.
    """"""
    joints = np.zeros((num_frames, 75), dtype=np.float32)
    colors = np.ones((num_frames, 1, 25, 2), dtype=np.float32) * np.nan
    start, end = body_data['interval'][0], body_data['interval'][-1]
    joints[start:end + 1] = body_data['joints'].reshape(-1, 75)
    colors[start:end + 1, 0] = body_data['colors']

    return joints, colors


def remove_missing_frames(ske_name, joints, colors):
    """"""
    Cut off missing frames which all joints positions are 0s

    For the sequence with 2 actors' data, also record the number of missing frames for
    actor1 and actor2, respectively (for debug).
    """"""
    num_frames = joints.shape[0]
    num_bodies = colors.shape[1]  # 1 or 2

    if num_bodies == 2:  # DEBUG
        missing_indices_1 = np.where(joints[:, :75].sum(axis=1) == 0)[0]
        missing_indices_2 = np.where(joints[:, 75:].sum(axis=1) == 0)[0]
        cnt1 = len(missing_indices_1)
        cnt2 = len(missing_indices_2)

        start = 1 if 0 in missing_indices_1 else 0
        end = 1 if num_frames - 1 in missing_indices_1 else 0
        if max(cnt1, cnt2) > 0:
            if cnt1 > cnt2:
                info = '{}\t{:^10d}\t{:^6d}\t{:^6d}\t{:^5d}\t{:^3d}'.format(ske_name, num_frames,
                                                                            cnt1, cnt2, start, end)
                missing_skes_logger1.info(info)
            else:
                info = '{}\t{:^10d}\t{:^6d}\t{:^6d}'.format(ske_name, num_frames, cnt1, cnt2)
                missing_skes_logger2.info(info)

    # Find valid frame indices that the data is not missing or lost
    # For two-subjects action, this means both data of actor1 and actor2 is missing.
    valid_indices = np.where(joints.sum(axis=1) != 0)[0]  # 0-based index
    missing_indices = np.where(joints.sum(axis=1) == 0)[0]
    num_missing = len(missing_indices)

    if num_missing > 0:  # Update joints and colors
        joints = joints[valid_indices]
        colors[missing_indices] = np.nan
        global missing_count
        missing_count += 1
        missing_skes_logger.info('{}\t{:^10d}\t{:^11d}'.format(ske_name, num_frames, num_missing))

    return joints, colors


def get_bodies_info(bodies_data):
    bodies_info = '{:^17}\t{}\t{:^8}\n'.format('bodyID', 'Interval', 'Motion')
    for (bodyID, body_data) in bodies_data.items():
        start, end = body_data['interval'][0], body_data['interval'][-1]
        bodies_info += '{}\t{:^8}\t{:f}\n'.format(bodyID, str([start, end]), body_data['motion'])

    return bodies_info + '\n'


def get_two_actors_points(bodies_data):
    """"""
    Get the first and second actor's joints positions and colors locations.

    # Arguments:
        bodies_data (dict): 3 key-value pairs: 'name', 'data', 'num_frames'.
        bodies_data['data'] is also a dict, while the key is bodyID, the value is
        the corresponding body_data which is also a dict with 4 keys:
          - joints: raw 3D joints positions. Shape: (num_frames x 25, 3)
          - colors: raw 2D color locations. Shape: (num_frames, 25, 2)
          - interval: a list which records the frame indices.
          - motion: motion amount

    # Return:
        joints, colors.
    """"""
    ske_name = bodies_data['name']
    label = int(ske_name[-2:])
    num_frames = bodies_data['num_frames']
    bodies_info = get_bodies_info(bodies_data['data'])

    bodies_data, noise_info = denoising_bodies_data(bodies_data)  # Denoising data
    bodies_info += noise_info

    bodies_data = list(bodies_data)
    if len(bodies_data) == 1:  # Only left one actor after denoising
        if label >= 50:  # DEBUG: Denoising failed for two-subjects action
            fail_logger_2.info(ske_name)

        bodyID, body_data = bodies_data[0]
        joints, colors = get_one_actor_points(body_data, num_frames)
        bodies_info += 'Main actor: %s' % bodyID
    else:
        if label < 50:  # DEBUG: Denoising failed for one-subject action
            fail_logger_1.info(ske_name)

        joints = np.zeros((num_frames, 150), dtype=np.float32)
        colors = np.ones((num_frames, 2, 25, 2), dtype=np.float32) * np.nan

        bodyID, actor1 = bodies_data[0]  # the 1st actor with largest motion
        start1, end1 = actor1['interval'][0], actor1['interval'][-1]
        joints[start1:end1 + 1, :75] = actor1['joints'].reshape(-1, 75)
        colors[start1:end1 + 1, 0] = actor1['colors']
        actor1_info = '{:^17}\t{}\t{:^8}\n'.format('Actor1', 'Interval', 'Motion') + \
                      '{}\t{:^8}\t{:f}\n'.format(bodyID, str([start1, end1]), actor1['motion'])
        del bodies_data[0]

        actor2_info = '{:^17}\t{}\t{:^8}\n'.format('Actor2', 'Interval', 'Motion')
        start2, end2 = [0, 0]  # initial interval for actor2 (virtual)

        while len(bodies_data) > 0:
            bodyID, actor = bodies_data[0]
            start, end = actor['interval'][0], actor['interval'][-1]
            if min(end1, end) - max(start1, start) <= 0:  # no overlap with actor1
                joints[start:end + 1, :75] = actor['joints'].reshape(-1, 75)
                colors[start:end + 1, 0] = actor['colors']
                actor1_info += '{}\t{:^8}\t{:f}\n'.format(bodyID, str([start, end]), actor['motion'])
                # Update the interval of actor1
                start1 = min(start, start1)
                end1 = max(end, end1)
            elif min(end2, end) - max(start2, start) <= 0:  # no overlap with actor2
                joints[start:end + 1, 75:] = actor['joints'].reshape(-1, 75)
                colors[start:end + 1, 1] = actor['colors']
                actor2_info += '{}\t{:^8}\t{:f}\n'.format(bodyID, str([start, end]), actor['motion'])
                # Update the interval of actor2
                start2 = min(start, start2)
                end2 = max(end, end2)
            del bodies_data[0]

        bodies_info += ('\n' + actor1_info + '\n' + actor2_info)

    with open(osp.join(actors_info_dir, ske_name + '.txt'), 'w') as fw:
        fw.write(bodies_info + '\n')

    return joints, colors


def get_raw_denoised_data():
    """"""
    Get denoised data (joints positions and color locations) from raw skeleton sequences.

    For each frame of a skeleton sequence, an actor's 3D positions of 25 joints represented
    by an 2D array (shape: 25 x 3) is reshaped into a 75-dim vector by concatenating each
    3-dim (x, y, z) coordinates along the row dimension in joint order. Each frame contains
    two actor's joints positions constituting a 150-dim vector. If there is only one actor,
    then the last 75 values are filled with zeros. Otherwise, select the main actor and the
    second actor based on the motion amount. Each 150-dim vector as a row vector is put into
    a 2D numpy array where the number of rows equals the number of valid frames. All such
    2D arrays are put into a list and finally the list is serialized into a cPickle file.

    For the skeleton sequence which contains two or more actors (mostly corresponds to the
    last 11 classes), the filename and actors' information are recorded into log files.
    For better understanding, also generate RGB+skeleton videos for visualization.
    """"""

    with open(raw_data_file, 'rb') as fr:  # load raw skeletons data
        raw_skes_data = pickle.load(fr)

    num_skes = len(raw_skes_data)
    print('Found %d available skeleton sequences.' % num_skes)

    raw_denoised_joints = []
    raw_denoised_colors = []
    frames_cnt = []

    for (idx, bodies_data) in enumerate(raw_skes_data):
        ske_name = bodies_data['name']
        print('Processing %s' % ske_name)
        num_bodies = len(bodies_data['data'])

        if num_bodies == 1:  # only 1 actor
            num_frames = bodies_data['num_frames']
            body_data = list(bodies_data['data'].values())[0]
            joints, colors = get_one_actor_points(body_data, num_frames)
        else:  # more than 1 actor, select two main actors
            joints, colors = get_two_actors_points(bodies_data)
            # Remove missing frames
            joints, colors = remove_missing_frames(ske_name, joints, colors)
            num_frames = joints.shape[0]  # Update
            # Visualize selected actors' skeletons on RGB videos.

        raw_denoised_joints.append(joints)
        raw_denoised_colors.append(colors)
        frames_cnt.append(num_frames)

        if (idx + 1) % 1000 == 0:
            print('Processed: %.2f%% (%d / %d), ' % \
                  (100.0 * (idx + 1) / num_skes, idx + 1, num_skes) + \
                  'Missing count: %d' % missing_count)

    raw_skes_joints_pkl = osp.join(save_path, 'raw_denoised_joints.pkl')
    with open(raw_skes_joints_pkl, 'wb') as f:
        pickle.dump(raw_denoised_joints, f, pickle.HIGHEST_PROTOCOL)

    raw_skes_colors_pkl = osp.join(save_path, 'raw_denoised_colors.pkl')
    with open(raw_skes_colors_pkl, 'wb') as f:
        pickle.dump(raw_denoised_colors, f, pickle.HIGHEST_PROTOCOL)

    frames_cnt = np.array(frames_cnt, dtype=np.int)
    np.savetxt(osp.join(save_path, 'frames_cnt.txt'), frames_cnt, fmt='%d')

    print('Saved raw denoised positions of {} frames into {}'.format(np.sum(frames_cnt),
                                                                     raw_skes_joints_pkl))
    print('Found %d files that have missing data' % missing_count)

if __name__ == '__main__':
    get_raw_denoised_data()
"
4,2201.02849,"# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
import os
import os.path as osp
import numpy as np
import pickle
import logging
import h5py
from sklearn.model_selection import train_test_split

root_path = './'
stat_path = osp.join(root_path, 'statistics')
setup_file = osp.join(stat_path, 'setup.txt')
camera_file = osp.join(stat_path, 'camera.txt')
performer_file = osp.join(stat_path, 'performer.txt')
replication_file = osp.join(stat_path, 'replication.txt')
label_file = osp.join(stat_path, 'label.txt')
skes_name_file = osp.join(stat_path, 'skes_available_name.txt')

denoised_path = osp.join(root_path, 'denoised_data')
raw_skes_joints_pkl = osp.join(denoised_path, 'raw_denoised_joints.pkl')
frames_file = osp.join(denoised_path, 'frames_cnt.txt')

save_path = './'


if not osp.exists(save_path):
    os.mkdir(save_path)


def remove_nan_frames(ske_name, ske_joints, nan_logger):
    num_frames = ske_joints.shape[0]
    valid_frames = []

    for f in range(num_frames):
        if not np.any(np.isnan(ske_joints[f])):
            valid_frames.append(f)
        else:
            nan_indices = np.where(np.isnan(ske_joints[f]))[0]
            nan_logger.info('{}\t{:^5}\t{}'.format(ske_name, f + 1, nan_indices))

    return ske_joints[valid_frames]

def seq_translation(skes_joints):
    for idx, ske_joints in enumerate(skes_joints):
        num_frames = ske_joints.shape[0]
        num_bodies = 1 if ske_joints.shape[1] == 75 else 2
        if num_bodies == 2:
            missing_frames_1 = np.where(ske_joints[:, :75].sum(axis=1) == 0)[0]
            missing_frames_2 = np.where(ske_joints[:, 75:].sum(axis=1) == 0)[0]
            cnt1 = len(missing_frames_1)
            cnt2 = len(missing_frames_2)

        i = 0  # get the ""real"" first frame of actor1
        while i < num_frames:
            if np.any(ske_joints[i, :75] != 0):
                break
            i += 1

        origin = np.copy(ske_joints[i, 3:6])  # new origin: joint-2

        for f in range(num_frames):
            if num_bodies == 1:
                ske_joints[f] -= np.tile(origin, 25)
            else:  # for 2 actors
                ske_joints[f] -= np.tile(origin, 50)

        if (num_bodies == 2) and (cnt1 > 0):
            ske_joints[missing_frames_1, :75] = np.zeros((cnt1, 75), dtype=np.float32)

        if (num_bodies == 2) and (cnt2 > 0):
            ske_joints[missing_frames_2, 75:] = np.zeros((cnt2, 75), dtype=np.float32)

        skes_joints[idx] = ske_joints  # Update

    return skes_joints


def frame_translation(skes_joints, skes_name, frames_cnt):
    nan_logger = logging.getLogger('nan_skes')
    nan_logger.setLevel(logging.INFO)
    nan_logger.addHandler(logging.FileHandler(""./nan_frames.log""))
    nan_logger.info('{}\t{}\t{}'.format('Skeleton', 'Frame', 'Joints'))

    for idx, ske_joints in enumerate(skes_joints):
        num_frames = ske_joints.shape[0]
        # Calculate the distance between spine base (joint-1) and spine (joint-21)
        j1 = ske_joints[:, 0:3]
        j21 = ske_joints[:, 60:63]
        dist = np.sqrt(((j1 - j21) ** 2).sum(axis=1))

        for f in range(num_frames):
            origin = ske_joints[f, 3:6]  # new origin: middle of the spine (joint-2)
            if (ske_joints[f, 75:] == 0).all():
                ske_joints[f, :75] = (ske_joints[f, :75] - np.tile(origin, 25)) / \
                                      dist[f] + np.tile(origin, 25)
            else:
                ske_joints[f] = (ske_joints[f] - np.tile(origin, 50)) / \
                                 dist[f] + np.tile(origin, 50)

        ske_name = skes_name[idx]
        ske_joints = remove_nan_frames(ske_name, ske_joints, nan_logger)
        frames_cnt[idx] = num_frames  # update valid number of frames
        skes_joints[idx] = ske_joints

    return skes_joints, frames_cnt


def align_frames(skes_joints, frames_cnt):
    """"""
    Align all sequences with the same frame length.

    """"""
    num_skes = len(skes_joints)
    max_num_frames = frames_cnt.max()  # 300
    aligned_skes_joints = np.zeros((num_skes, max_num_frames, 150), dtype=np.float32)

    for idx, ske_joints in enumerate(skes_joints):
        num_frames = ske_joints.shape[0]
        num_bodies = 1 if ske_joints.shape[1] == 75 else 2
        if num_bodies == 1:
            aligned_skes_joints[idx, :num_frames] = np.hstack((ske_joints, ske_joints))
            # aligned_skes_joints[idx, :num_frames] = np.hstack((ske_joints, np.zeros_like(ske_joints)))
        else:
            aligned_skes_joints[idx, :num_frames] = ske_joints

    return aligned_skes_joints


def one_hot_vector(labels):
    num_skes = len(labels)
    labels_vector = np.zeros((num_skes, 120))
    for idx, l in enumerate(labels):
        labels_vector[idx, l] = 1

    return labels_vector


def split_train_val(train_indices, method='sklearn', ratio=0.05):
    """"""
    Get validation set by splitting data randomly from training set with two methods.
    In fact, I thought these two methods are equal as they got the same performance.

    """"""
    if method == 'sklearn':
        return train_test_split(train_indices, test_size=ratio, random_state=10000)
    else:
        np.random.seed(10000)
        np.random.shuffle(train_indices)
        val_num_skes = int(np.ceil(0.05 * len(train_indices)))
        val_indices = train_indices[:val_num_skes]
        train_indices = train_indices[val_num_skes:]
        return train_indices, val_indices


def split_dataset(skes_joints, label, performer, setup, evaluation, save_path):
    train_indices, test_indices = get_indices(performer, setup, evaluation)
    # m = 'sklearn'  # 'sklearn' or 'numpy'
    # Select validation set from training set
    # train_indices, val_indices = split_train_val(train_indices, m)

    # Save labels and num_frames for each sequence of each data set
    train_labels = label[train_indices]
    test_labels = label[test_indices]

    train_x = skes_joints[train_indices]
    train_y = one_hot_vector(train_labels)
    test_x = skes_joints[test_indices]
    test_y = one_hot_vector(test_labels)

    save_name = 'NTU120_%s.npz' % evaluation
    np.savez(save_name, x_train=train_x, y_train=train_y, x_test=test_x, y_test=test_y)

    # # Save data into a .h5 file
    # h5file = h5py.File(osp.join(save_path, 'NTU_%s.h5' % (evaluation)), 'w')
    # # Training set
    # h5file.create_dataset('x', data=skes_joints[train_indices])
    # train_one_hot_labels = one_hot_vector(train_labels)
    # h5file.create_dataset('y', data=train_one_hot_labels)
    # # Validation set
    # h5file.create_dataset('valid_x', data=skes_joints[val_indices])
    # val_one_hot_labels = one_hot_vector(val_labels)
    # h5file.create_dataset('valid_y', data=val_one_hot_labels)
    # # Test set
    # h5file.create_dataset('test_x', data=skes_joints[test_indices])
    # test_one_hot_labels = one_hot_vector(test_labels)
    # h5file.create_dataset('test_y', data=test_one_hot_labels)

    # h5file.close()


def get_indices(performer, setup, evaluation='XSub'):
    test_indices = np.empty(0)
    train_indices = np.empty(0)

    if evaluation == 'XSub':  # Cross Subject (Subject IDs)
        train_ids = [1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 27, 28,
                     31, 34, 35, 38, 45, 46, 47, 49, 50, 52, 53, 54, 55, 56, 57,
                     58, 59, 70, 74, 78, 80, 81, 82, 83, 84, 85, 86, 89, 91, 92,
                     93, 94, 95, 97, 98, 100, 103]
        test_ids = [i for i in range(1, 107) if i not in train_ids]

        # Get indices of test data
        for idx in test_ids:
            temp = np.where(performer == idx)[0]  # 0-based index
            test_indices = np.hstack((test_indices, temp)).astype(np.int)

        # Get indices of training data
        for train_id in train_ids:
            temp = np.where(performer == train_id)[0]  # 0-based index
            train_indices = np.hstack((train_indices, temp)).astype(np.int)
    else:  # Cross Setup (Setup IDs)
        train_ids = [i for i in range(1, 33) if i % 2 == 0]  # Even setup
        test_ids = [i for i in range(1, 33) if i % 2 == 1]  # Odd setup

        # Get indices of test data
        for test_id in test_ids:
            temp = np.where(setup == test_id)[0]  # 0-based index
            test_indices = np.hstack((test_indices, temp)).astype(np.int)

        # Get indices of training data
        for train_id in train_ids:
            temp = np.where(setup == train_id)[0]  # 0-based index
            train_indices = np.hstack((train_indices, temp)).astype(np.int)

    return train_indices, test_indices


if __name__ == '__main__':
    setup = np.loadtxt(setup_file, dtype=np.int)  # camera id: 1~32
    performer = np.loadtxt(performer_file, dtype=np.int)  # subject id: 1~106
    label = np.loadtxt(label_file, dtype=np.int) - 1  # action label: 0~119

    frames_cnt = np.loadtxt(frames_file, dtype=np.int)  # frames_cnt
    skes_name = np.loadtxt(skes_name_file, dtype=np.string_)

    with open(raw_skes_joints_pkl, 'rb') as fr:
        skes_joints = pickle.load(fr)  # a list

    skes_joints = seq_translation(skes_joints)

    skes_joints = align_frames(skes_joints, frames_cnt)  # aligned to the same frame length

    evaluations = ['XSet', 'XSub']
    for evaluation in evaluations:
        split_dataset(skes_joints, label, performer, setup, evaluation, save_path)
"
5,2201.02849,"# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
import os.path as osp
import os
import numpy as np
import pickle
import logging


def get_raw_bodies_data(skes_path, ske_name, frames_drop_skes, frames_drop_logger):
    """"""
    Get raw bodies data from a skeleton sequence.

    Each body's data is a dict that contains the following keys:
      - joints: raw 3D joints positions. Shape: (num_frames x 25, 3)
      - colors: raw 2D color locations. Shape: (num_frames, 25, 2)
      - interval: a list which stores the frame indices of this body.
      - motion: motion amount (only for the sequence with 2 or more bodyIDs).

    Return:
      a dict for a skeleton sequence with 3 key-value pairs:
        - name: the skeleton filename.
        - data: a dict which stores raw data of each body.
        - num_frames: the number of valid frames.
    """"""
    ske_file = osp.join(skes_path, ske_name + '.skeleton')
    assert osp.exists(ske_file), 'Error: Skeleton file %s not found' % ske_file
    # Read all data from .skeleton file into a list (in string format)
    print('Reading data from %s' % ske_file[-29:])
    with open(ske_file, 'r') as fr:
        str_data = fr.readlines()

    num_frames = int(str_data[0].strip('\r\n'))
    frames_drop = []
    bodies_data = dict()
    valid_frames = -1  # 0-based index
    current_line = 1

    for f in range(num_frames):
        num_bodies = int(str_data[current_line].strip('\r\n'))
        current_line += 1

        if num_bodies == 0:  # no data in this frame, drop it
            frames_drop.append(f)  # 0-based index
            continue

        valid_frames += 1
        joints = np.zeros((num_bodies, 25, 3), dtype=np.float32)
        colors = np.zeros((num_bodies, 25, 2), dtype=np.float32)

        for b in range(num_bodies):
            bodyID = str_data[current_line].strip('\r\n').split()[0]
            current_line += 1
            num_joints = int(str_data[current_line].strip('\r\n'))  # 25 joints
            current_line += 1

            for j in range(num_joints):
                temp_str = str_data[current_line].strip('\r\n').split()
                joints[b, j, :] = np.array(temp_str[:3], dtype=np.float32)
                colors[b, j, :] = np.array(temp_str[5:7], dtype=np.float32)
                current_line += 1

            if bodyID not in bodies_data:  # Add a new body's data
                body_data = dict()
                body_data['joints'] = joints[b]  # ndarray: (25, 3)
                body_data['colors'] = colors[b, np.newaxis]  # ndarray: (1, 25, 2)
                body_data['interval'] = [valid_frames]  # the index of the first frame
            else:  # Update an already existed body's data
                body_data = bodies_data[bodyID]
                # Stack each body's data of each frame along the frame order
                body_data['joints'] = np.vstack((body_data['joints'], joints[b]))
                body_data['colors'] = np.vstack((body_data['colors'], colors[b, np.newaxis]))
                pre_frame_idx = body_data['interval'][-1]
                body_data['interval'].append(pre_frame_idx + 1)  # add a new frame index

            bodies_data[bodyID] = body_data  # Update bodies_data

    num_frames_drop = len(frames_drop)
    assert num_frames_drop < num_frames, \
        'Error: All frames data (%d) of %s is missing or lost' % (num_frames, ske_name)
    if num_frames_drop > 0:
        frames_drop_skes[ske_name] = np.array(frames_drop, dtype=np.int)
        frames_drop_logger.info('{}: {} frames missed: {}\n'.format(ske_name, num_frames_drop,
                                                                    frames_drop))

    # Calculate motion (only for the sequence with 2 or more bodyIDs)
    if len(bodies_data) > 1:
        for body_data in bodies_data.values():
            body_data['motion'] = np.sum(np.var(body_data['joints'], axis=0))

    return {'name': ske_name, 'data': bodies_data, 'num_frames': num_frames - num_frames_drop}


def get_raw_skes_data():
    # # save_path = './data'
    # # skes_path = '/data/pengfei/NTU/nturgb+d_skeletons/'
    # stat_path = osp.join(save_path, 'statistics')
    #
    # skes_name_file = osp.join(stat_path, 'skes_available_name.txt')
    # save_data_pkl = osp.join(save_path, 'raw_skes_data.pkl')
    # frames_drop_pkl = osp.join(save_path, 'frames_drop_skes.pkl')
    #
    # frames_drop_logger = logging.getLogger('frames_drop')
    # frames_drop_logger.setLevel(logging.INFO)
    # frames_drop_logger.addHandler(logging.FileHandler(osp.join(save_path, 'frames_drop.log')))
    # frames_drop_skes = dict()

    skes_name = np.loadtxt(skes_name_file, dtype=str)

    num_files = skes_name.size
    print('Found %d available skeleton files.' % num_files)

    raw_skes_data = []
    frames_cnt = np.zeros(num_files, dtype=np.int)

    for (idx, ske_name) in enumerate(skes_name):
        bodies_data = get_raw_bodies_data(skes_path, ske_name, frames_drop_skes, frames_drop_logger)
        raw_skes_data.append(bodies_data)
        frames_cnt[idx] = bodies_data['num_frames']
        if (idx + 1) % 1000 == 0:
            print('Processed: %.2f%% (%d / %d)' % \
                  (100.0 * (idx + 1) / num_files, idx + 1, num_files))

    with open(save_data_pkl, 'wb') as fw:
        pickle.dump(raw_skes_data, fw, pickle.HIGHEST_PROTOCOL)
    np.savetxt(osp.join(save_path, 'raw_data', 'frames_cnt.txt'), frames_cnt, fmt='%d')

    print('Saved raw bodies data into %s' % save_data_pkl)
    print('Total frames: %d' % np.sum(frames_cnt))

    with open(frames_drop_pkl, 'wb') as fw:
        pickle.dump(frames_drop_skes, fw, pickle.HIGHEST_PROTOCOL)

if __name__ == '__main__':
    save_path = './'

    skes_path = '../nturgbd_raw/nturgb+d_skeletons/'
    stat_path = osp.join(save_path, 'statistics')
    if not osp.exists('./raw_data'):
        os.makedirs('./raw_data')

    skes_name_file = osp.join(stat_path, 'skes_available_name.txt')
    save_data_pkl = osp.join(save_path, 'raw_data', 'raw_skes_data.pkl')
    frames_drop_pkl = osp.join(save_path, 'raw_data', 'frames_drop_skes.pkl')

    frames_drop_logger = logging.getLogger('frames_drop')
    frames_drop_logger.setLevel(logging.INFO)
    frames_drop_logger.addHandler(logging.FileHandler(osp.join(save_path, 'raw_data', 'frames_drop.log')))
    frames_drop_skes = dict()

    get_raw_skes_data()

    with open(frames_drop_pkl, 'wb') as fw:
        pickle.dump(frames_drop_skes, fw, pickle.HIGHEST_PROTOCOL)
        
"
6,2201.02849,"# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
import os
import os.path as osp
import numpy as np
import pickle
import logging

root_path = './'
raw_data_file = osp.join(root_path, 'raw_data', 'raw_skes_data.pkl')
save_path = osp.join(root_path, 'denoised_data')

if not osp.exists(save_path):
    os.mkdir(save_path)

rgb_ske_path = osp.join(save_path, 'rgb+ske')
if not osp.exists(rgb_ske_path):
    os.mkdir(rgb_ske_path)

actors_info_dir = osp.join(save_path, 'actors_info')
if not osp.exists(actors_info_dir):
    os.mkdir(actors_info_dir)

missing_count = 0
noise_len_thres = 11
noise_spr_thres1 = 0.8
noise_spr_thres2 = 0.69754
noise_mot_thres_lo = 0.089925
noise_mot_thres_hi = 2

noise_len_logger = logging.getLogger('noise_length')
noise_len_logger.setLevel(logging.INFO)
noise_len_logger.addHandler(logging.FileHandler(osp.join(save_path, 'noise_length.log')))
noise_len_logger.info('{:^20}\t{:^17}\t{:^8}\t{}'.format('Skeleton', 'bodyID', 'Motion', 'Length'))

noise_spr_logger = logging.getLogger('noise_spread')
noise_spr_logger.setLevel(logging.INFO)
noise_spr_logger.addHandler(logging.FileHandler(osp.join(save_path, 'noise_spread.log')))
noise_spr_logger.info('{:^20}\t{:^17}\t{:^8}\t{:^8}'.format('Skeleton', 'bodyID', 'Motion', 'Rate'))

noise_mot_logger = logging.getLogger('noise_motion')
noise_mot_logger.setLevel(logging.INFO)
noise_mot_logger.addHandler(logging.FileHandler(osp.join(save_path, 'noise_motion.log')))
noise_mot_logger.info('{:^20}\t{:^17}\t{:^8}'.format('Skeleton', 'bodyID', 'Motion'))

fail_logger_1 = logging.getLogger('noise_outliers_1')
fail_logger_1.setLevel(logging.INFO)
fail_logger_1.addHandler(logging.FileHandler(osp.join(save_path, 'denoised_failed_1.log')))

fail_logger_2 = logging.getLogger('noise_outliers_2')
fail_logger_2.setLevel(logging.INFO)
fail_logger_2.addHandler(logging.FileHandler(osp.join(save_path, 'denoised_failed_2.log')))

missing_skes_logger = logging.getLogger('missing_frames')
missing_skes_logger.setLevel(logging.INFO)
missing_skes_logger.addHandler(logging.FileHandler(osp.join(save_path, 'missing_skes.log')))
missing_skes_logger.info('{:^20}\t{}\t{}'.format('Skeleton', 'num_frames', 'num_missing'))

missing_skes_logger1 = logging.getLogger('missing_frames_1')
missing_skes_logger1.setLevel(logging.INFO)
missing_skes_logger1.addHandler(logging.FileHandler(osp.join(save_path, 'missing_skes_1.log')))
missing_skes_logger1.info('{:^20}\t{}\t{}\t{}\t{}\t{}'.format('Skeleton', 'num_frames', 'Actor1',
                                                              'Actor2', 'Start', 'End'))

missing_skes_logger2 = logging.getLogger('missing_frames_2')
missing_skes_logger2.setLevel(logging.INFO)
missing_skes_logger2.addHandler(logging.FileHandler(osp.join(save_path, 'missing_skes_2.log')))
missing_skes_logger2.info('{:^20}\t{}\t{}\t{}'.format('Skeleton', 'num_frames', 'Actor1', 'Actor2'))


def denoising_by_length(ske_name, bodies_data):
    """"""
    Denoising data based on the frame length for each bodyID.
    Filter out the bodyID which length is less or equal than the predefined threshold.

    """"""
    noise_info = str()
    new_bodies_data = bodies_data.copy()
    for (bodyID, body_data) in new_bodies_data.items():
        length = len(body_data['interval'])
        if length <= noise_len_thres:
            noise_info += 'Filter out: %s, %d (length).\n' % (bodyID, length)
            noise_len_logger.info('{}\t{}\t{:.6f}\t{:^6d}'.format(ske_name, bodyID,
                                                                  body_data['motion'], length))
            del bodies_data[bodyID]
    if noise_info != '':
        noise_info += '\n'

    return bodies_data, noise_info


def get_valid_frames_by_spread(points):
    """"""
    Find the valid (or reasonable) frames (index) based on the spread of X and Y.

    points: joints or colors
    """"""
    num_frames = points.shape[0]
    valid_frames = []
    for i in range(num_frames):
        x = points[i, :, 0]
        y = points[i, :, 1]
        if (x.max() - x.min()) <= noise_spr_thres1 * (y.max() - y.min()):  # 0.8
            valid_frames.append(i)
    return valid_frames


def denoising_by_spread(ske_name, bodies_data):
    """"""
    Denoising data based on the spread of Y value and X value.
    Filter out the bodyID which the ratio of noisy frames is higher than the predefined
    threshold.

    bodies_data: contains at least 2 bodyIDs
    """"""
    noise_info = str()
    denoised_by_spr = False  # mark if this sequence has been processed by spread.

    new_bodies_data = bodies_data.copy()
    # for (bodyID, body_data) in bodies_data.items():
    for (bodyID, body_data) in new_bodies_data.items():
        if len(bodies_data) == 1:
            break
        valid_frames = get_valid_frames_by_spread(body_data['joints'].reshape(-1, 25, 3))
        num_frames = len(body_data['interval'])
        num_noise = num_frames - len(valid_frames)
        if num_noise == 0:
            continue

        ratio = num_noise / float(num_frames)
        motion = body_data['motion']
        if ratio >= noise_spr_thres2:  # 0.69754
            del bodies_data[bodyID]
            denoised_by_spr = True
            noise_info += 'Filter out: %s (spread rate >= %.2f).\n' % (bodyID, noise_spr_thres2)
            noise_spr_logger.info('%s\t%s\t%.6f\t%.6f' % (ske_name, bodyID, motion, ratio))
        else:  # Update motion
            joints = body_data['joints'].reshape(-1, 25, 3)[valid_frames]
            body_data['motion'] = min(motion, np.sum(np.var(joints.reshape(-1, 3), axis=0)))
            noise_info += '%s: motion %.6f -> %.6f\n' % (bodyID, motion, body_data['motion'])
            # TODO: Consider removing noisy frames for each bodyID

    if noise_info != '':
        noise_info += '\n'

    return bodies_data, noise_info, denoised_by_spr


def denoising_by_motion(ske_name, bodies_data, bodies_motion):
    """"""
    Filter out the bodyID which motion is out of the range of predefined interval

    """"""
    # Sort bodies based on the motion, return a list of tuples
    # bodies_motion = sorted(bodies_motion.items(), key=lambda x, y: cmp(x[1], y[1]), reverse=True)
    bodies_motion = sorted(bodies_motion.items(), key=lambda x: x[1], reverse=True)

    # Reserve the body data with the largest motion
    denoised_bodies_data = [(bodies_motion[0][0], bodies_data[bodies_motion[0][0]])]
    noise_info = str()

    for (bodyID, motion) in bodies_motion[1:]:
        if (motion < noise_mot_thres_lo) or (motion > noise_mot_thres_hi):
            noise_info += 'Filter out: %s, %.6f (motion).\n' % (bodyID, motion)
            noise_mot_logger.info('{}\t{}\t{:.6f}'.format(ske_name, bodyID, motion))
        else:
            denoised_bodies_data.append((bodyID, bodies_data[bodyID]))
    if noise_info != '':
        noise_info += '\n'

    return denoised_bodies_data, noise_info


def denoising_bodies_data(bodies_data):
    """"""
    Denoising data based on some heuristic methods, not necessarily correct for all samples.

    Return:
      denoised_bodies_data (list): tuple: (bodyID, body_data).
    """"""
    ske_name = bodies_data['name']
    bodies_data = bodies_data['data']

    # Step 1: Denoising based on frame length.
    bodies_data, noise_info_len = denoising_by_length(ske_name, bodies_data)

    if len(bodies_data) == 1:  # only has one bodyID left after step 1
        return bodies_data.items(), noise_info_len

    # Step 2: Denoising based on spread.
    bodies_data, noise_info_spr, denoised_by_spr = denoising_by_spread(ske_name, bodies_data)

    if len(bodies_data) == 1:
        return bodies_data.items(), noise_info_len + noise_info_spr

    bodies_motion = dict()  # get body motion
    for (bodyID, body_data) in bodies_data.items():
        bodies_motion[bodyID] = body_data['motion']
    # Sort bodies based on the motion
    # bodies_motion = sorted(bodies_motion.items(), key=lambda x, y: cmp(x[1], y[1]), reverse=True)
    bodies_motion = sorted(bodies_motion.items(), key=lambda x: x[1], reverse=True)
    denoised_bodies_data = list()
    for (bodyID, _) in bodies_motion:
        denoised_bodies_data.append((bodyID, bodies_data[bodyID]))

    return denoised_bodies_data, noise_info_len + noise_info_spr

    # TODO: Consider denoising further by integrating motion method

    # if denoised_by_spr:  # this sequence has been denoised by spread
    #     bodies_motion = sorted(bodies_motion.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)
    #     denoised_bodies_data = list()
    #     for (bodyID, _) in bodies_motion:
    #         denoised_bodies_data.append((bodyID, bodies_data[bodyID]))
    #     return denoised_bodies_data, noise_info

    # Step 3: Denoising based on motion
    # bodies_data, noise_info = denoising_by_motion(ske_name, bodies_data, bodies_motion)

    # return bodies_data, noise_info


def get_one_actor_points(body_data, num_frames):
    """"""
    Get joints and colors for only one actor.
    For joints, each frame contains 75 X-Y-Z coordinates.
    For colors, each frame contains 25 x 2 (X, Y) coordinates.
    """"""
    joints = np.zeros((num_frames, 75), dtype=np.float32)
    colors = np.ones((num_frames, 1, 25, 2), dtype=np.float32) * np.nan
    start, end = body_data['interval'][0], body_data['interval'][-1]
    joints[start:end + 1] = body_data['joints'].reshape(-1, 75)
    colors[start:end + 1, 0] = body_data['colors']

    return joints, colors


def remove_missing_frames(ske_name, joints, colors):
    """"""
    Cut off missing frames which all joints positions are 0s

    For the sequence with 2 actors' data, also record the number of missing frames for
    actor1 and actor2, respectively (for debug).
    """"""
    num_frames = joints.shape[0]
    num_bodies = colors.shape[1]  # 1 or 2

    if num_bodies == 2:  # DEBUG
        missing_indices_1 = np.where(joints[:, :75].sum(axis=1) == 0)[0]
        missing_indices_2 = np.where(joints[:, 75:].sum(axis=1) == 0)[0]
        cnt1 = len(missing_indices_1)
        cnt2 = len(missing_indices_2)

        start = 1 if 0 in missing_indices_1 else 0
        end = 1 if num_frames - 1 in missing_indices_1 else 0
        if max(cnt1, cnt2) > 0:
            if cnt1 > cnt2:
                info = '{}\t{:^10d}\t{:^6d}\t{:^6d}\t{:^5d}\t{:^3d}'.format(ske_name, num_frames,
                                                                            cnt1, cnt2, start, end)
                missing_skes_logger1.info(info)
            else:
                info = '{}\t{:^10d}\t{:^6d}\t{:^6d}'.format(ske_name, num_frames, cnt1, cnt2)
                missing_skes_logger2.info(info)

    # Find valid frame indices that the data is not missing or lost
    # For two-subjects action, this means both data of actor1 and actor2 is missing.
    valid_indices = np.where(joints.sum(axis=1) != 0)[0]  # 0-based index
    missing_indices = np.where(joints.sum(axis=1) == 0)[0]
    num_missing = len(missing_indices)

    if num_missing > 0:  # Update joints and colors
        joints = joints[valid_indices]
        colors[missing_indices] = np.nan
        global missing_count
        missing_count += 1
        missing_skes_logger.info('{}\t{:^10d}\t{:^11d}'.format(ske_name, num_frames, num_missing))

    return joints, colors


def get_bodies_info(bodies_data):
    bodies_info = '{:^17}\t{}\t{:^8}\n'.format('bodyID', 'Interval', 'Motion')
    for (bodyID, body_data) in bodies_data.items():
        start, end = body_data['interval'][0], body_data['interval'][-1]
        bodies_info += '{}\t{:^8}\t{:f}\n'.format(bodyID, str([start, end]), body_data['motion'])

    return bodies_info + '\n'


def get_two_actors_points(bodies_data):
    """"""
    Get the first and second actor's joints positions and colors locations.

    # Arguments:
        bodies_data (dict): 3 key-value pairs: 'name', 'data', 'num_frames'.
        bodies_data['data'] is also a dict, while the key is bodyID, the value is
        the corresponding body_data which is also a dict with 4 keys:
          - joints: raw 3D joints positions. Shape: (num_frames x 25, 3)
          - colors: raw 2D color locations. Shape: (num_frames, 25, 2)
          - interval: a list which records the frame indices.
          - motion: motion amount

    # Return:
        joints, colors.
    """"""
    ske_name = bodies_data['name']
    label = int(ske_name[-2:])
    num_frames = bodies_data['num_frames']
    bodies_info = get_bodies_info(bodies_data['data'])

    bodies_data, noise_info = denoising_bodies_data(bodies_data)  # Denoising data
    bodies_info += noise_info

    bodies_data = list(bodies_data)
    if len(bodies_data) == 1:  # Only left one actor after denoising
        if label >= 50:  # DEBUG: Denoising failed for two-subjects action
            fail_logger_2.info(ske_name)

        bodyID, body_data = bodies_data[0]
        joints, colors = get_one_actor_points(body_data, num_frames)
        bodies_info += 'Main actor: %s' % bodyID
    else:
        if label < 50:  # DEBUG: Denoising failed for one-subject action
            fail_logger_1.info(ske_name)

        joints = np.zeros((num_frames, 150), dtype=np.float32)
        colors = np.ones((num_frames, 2, 25, 2), dtype=np.float32) * np.nan

        bodyID, actor1 = bodies_data[0]  # the 1st actor with largest motion
        start1, end1 = actor1['interval'][0], actor1['interval'][-1]
        joints[start1:end1 + 1, :75] = actor1['joints'].reshape(-1, 75)
        colors[start1:end1 + 1, 0] = actor1['colors']
        actor1_info = '{:^17}\t{}\t{:^8}\n'.format('Actor1', 'Interval', 'Motion') + \
                      '{}\t{:^8}\t{:f}\n'.format(bodyID, str([start1, end1]), actor1['motion'])
        del bodies_data[0]

        actor2_info = '{:^17}\t{}\t{:^8}\n'.format('Actor2', 'Interval', 'Motion')
        start2, end2 = [0, 0]  # initial interval for actor2 (virtual)

        while len(bodies_data) > 0:
            bodyID, actor = bodies_data[0]
            start, end = actor['interval'][0], actor['interval'][-1]
            if min(end1, end) - max(start1, start) <= 0:  # no overlap with actor1
                joints[start:end + 1, :75] = actor['joints'].reshape(-1, 75)
                colors[start:end + 1, 0] = actor['colors']
                actor1_info += '{}\t{:^8}\t{:f}\n'.format(bodyID, str([start, end]), actor['motion'])
                # Update the interval of actor1
                start1 = min(start, start1)
                end1 = max(end, end1)
            elif min(end2, end) - max(start2, start) <= 0:  # no overlap with actor2
                joints[start:end + 1, 75:] = actor['joints'].reshape(-1, 75)
                colors[start:end + 1, 1] = actor['colors']
                actor2_info += '{}\t{:^8}\t{:f}\n'.format(bodyID, str([start, end]), actor['motion'])
                # Update the interval of actor2
                start2 = min(start, start2)
                end2 = max(end, end2)
            del bodies_data[0]

        bodies_info += ('\n' + actor1_info + '\n' + actor2_info)

    with open(osp.join(actors_info_dir, ske_name + '.txt'), 'w') as fw:
        fw.write(bodies_info + '\n')

    return joints, colors


def get_raw_denoised_data():
    """"""
    Get denoised data (joints positions and color locations) from raw skeleton sequences.

    For each frame of a skeleton sequence, an actor's 3D positions of 25 joints represented
    by an 2D array (shape: 25 x 3) is reshaped into a 75-dim vector by concatenating each
    3-dim (x, y, z) coordinates along the row dimension in joint order. Each frame contains
    two actor's joints positions constituting a 150-dim vector. If there is only one actor,
    then the last 75 values are filled with zeros. Otherwise, select the main actor and the
    second actor based on the motion amount. Each 150-dim vector as a row vector is put into
    a 2D numpy array where the number of rows equals the number of valid frames. All such
    2D arrays are put into a list and finally the list is serialized into a cPickle file.

    For the skeleton sequence which contains two or more actors (mostly corresponds to the
    last 11 classes), the filename and actors' information are recorded into log files.
    For better understanding, also generate RGB+skeleton videos for visualization.
    """"""

    with open(raw_data_file, 'rb') as fr:  # load raw skeletons data
        raw_skes_data = pickle.load(fr)

    num_skes = len(raw_skes_data)
    print('Found %d available skeleton sequences.' % num_skes)

    raw_denoised_joints = []
    raw_denoised_colors = []
    frames_cnt = []

    for (idx, bodies_data) in enumerate(raw_skes_data):
        ske_name = bodies_data['name']
        print('Processing %s' % ske_name)
        num_bodies = len(bodies_data['data'])

        if num_bodies == 1:  # only 1 actor
            num_frames = bodies_data['num_frames']
            body_data = list(bodies_data['data'].values())[0]
            joints, colors = get_one_actor_points(body_data, num_frames)
        else:  # more than 1 actor, select two main actors
            joints, colors = get_two_actors_points(bodies_data)
            # Remove missing frames
            joints, colors = remove_missing_frames(ske_name, joints, colors)
            num_frames = joints.shape[0]  # Update
            # Visualize selected actors' skeletons on RGB videos.

        raw_denoised_joints.append(joints)
        raw_denoised_colors.append(colors)
        frames_cnt.append(num_frames)

        if (idx + 1) % 1000 == 0:
            print('Processed: %.2f%% (%d / %d), ' % \
                  (100.0 * (idx + 1) / num_skes, idx + 1, num_skes) + \
                  'Missing count: %d' % missing_count)

    raw_skes_joints_pkl = osp.join(save_path, 'raw_denoised_joints.pkl')
    with open(raw_skes_joints_pkl, 'wb') as f:
        pickle.dump(raw_denoised_joints, f, pickle.HIGHEST_PROTOCOL)

    raw_skes_colors_pkl = osp.join(save_path, 'raw_denoised_colors.pkl')
    with open(raw_skes_colors_pkl, 'wb') as f:
        pickle.dump(raw_denoised_colors, f, pickle.HIGHEST_PROTOCOL)

    frames_cnt = np.array(frames_cnt, dtype=np.int)
    np.savetxt(osp.join(save_path, 'frames_cnt.txt'), frames_cnt, fmt='%d')

    print('Saved raw denoised positions of {} frames into {}'.format(np.sum(frames_cnt),
                                                                     raw_skes_joints_pkl))
    print('Found %d files that have missing data' % missing_count)

if __name__ == '__main__':
    get_raw_denoised_data()
"
7,2201.02849,"# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
import os
import os.path as osp
import numpy as np
import pickle
import logging
from sklearn.model_selection import train_test_split

root_path = './'
stat_path = osp.join(root_path, 'statistics')
setup_file = osp.join(stat_path, 'setup.txt')
camera_file = osp.join(stat_path, 'camera.txt')
performer_file = osp.join(stat_path, 'performer.txt')
replication_file = osp.join(stat_path, 'replication.txt')
label_file = osp.join(stat_path, 'label.txt')
skes_name_file = osp.join(stat_path, 'skes_available_name.txt')

denoised_path = osp.join(root_path, 'denoised_data')
raw_skes_joints_pkl = osp.join(denoised_path, 'raw_denoised_joints.pkl')
frames_file = osp.join(denoised_path, 'frames_cnt.txt')

save_path = './'


if not osp.exists(save_path):
    os.mkdir(save_path)


def remove_nan_frames(ske_name, ske_joints, nan_logger):
    num_frames = ske_joints.shape[0]
    valid_frames = []

    for f in range(num_frames):
        if not np.any(np.isnan(ske_joints[f])):
            valid_frames.append(f)
        else:
            nan_indices = np.where(np.isnan(ske_joints[f]))[0]
            nan_logger.info('{}\t{:^5}\t{}'.format(ske_name, f + 1, nan_indices))

    return ske_joints[valid_frames]

def seq_translation(skes_joints):
    for idx, ske_joints in enumerate(skes_joints):
        num_frames = ske_joints.shape[0]
        num_bodies = 1 if ske_joints.shape[1] == 75 else 2
        if num_bodies == 2:
            missing_frames_1 = np.where(ske_joints[:, :75].sum(axis=1) == 0)[0]
            missing_frames_2 = np.where(ske_joints[:, 75:].sum(axis=1) == 0)[0]
            cnt1 = len(missing_frames_1)
            cnt2 = len(missing_frames_2)

        i = 0  # get the ""real"" first frame of actor1
        while i < num_frames:
            if np.any(ske_joints[i, :75] != 0):
                break
            i += 1

        origin = np.copy(ske_joints[i, 3:6])  # new origin: joint-2

        for f in range(num_frames):
            if num_bodies == 1:
                ske_joints[f] -= np.tile(origin, 25)
            else:  # for 2 actors
                ske_joints[f] -= np.tile(origin, 50)

        if (num_bodies == 2) and (cnt1 > 0):
            ske_joints[missing_frames_1, :75] = np.zeros((cnt1, 75), dtype=np.float32)

        if (num_bodies == 2) and (cnt2 > 0):
            ske_joints[missing_frames_2, 75:] = np.zeros((cnt2, 75), dtype=np.float32)

        skes_joints[idx] = ske_joints  # Update

    return skes_joints


def frame_translation(skes_joints, skes_name, frames_cnt):
    nan_logger = logging.getLogger('nan_skes')
    nan_logger.setLevel(logging.INFO)
    nan_logger.addHandler(logging.FileHandler(""./nan_frames.log""))
    nan_logger.info('{}\t{}\t{}'.format('Skeleton', 'Frame', 'Joints'))

    for idx, ske_joints in enumerate(skes_joints):
        num_frames = ske_joints.shape[0]
        # Calculate the distance between spine base (joint-1) and spine (joint-21)
        j1 = ske_joints[:, 0:3]
        j21 = ske_joints[:, 60:63]
        dist = np.sqrt(((j1 - j21) ** 2).sum(axis=1))

        for f in range(num_frames):
            origin = ske_joints[f, 3:6]  # new origin: middle of the spine (joint-2)
            if (ske_joints[f, 75:] == 0).all():
                ske_joints[f, :75] = (ske_joints[f, :75] - np.tile(origin, 25)) / dist[f] + np.tile(origin, 25)
            else:
                ske_joints[f] = (ske_joints[f] - np.tile(origin, 50)) / dist[f] + np.tile(origin, 50)

        ske_name = skes_name[idx]
        ske_joints = remove_nan_frames(ske_name, ske_joints, nan_logger)
        frames_cnt[idx] = num_frames  # update valid number of frames
        skes_joints[idx] = ske_joints

    return skes_joints, frames_cnt


def align_frames(skes_joints, frames_cnt):
    """"""
    Align all sequences with the same frame length.

    """"""
    num_skes = len(skes_joints)
    max_num_frames = frames_cnt.max()  # 300
    aligned_skes_joints = np.zeros((num_skes, max_num_frames, 150), dtype=np.float32)

    for idx, ske_joints in enumerate(skes_joints):
        num_frames = ske_joints.shape[0]
        num_bodies = 1 if ske_joints.shape[1] == 75 else 2
        if num_bodies == 1:
            aligned_skes_joints[idx, :num_frames] = np.hstack((ske_joints, np.zeros_like(ske_joints)))
        else:
            aligned_skes_joints[idx, :num_frames] = ske_joints

    return aligned_skes_joints


def one_hot_vector(labels):
    num_skes = len(labels)
    labels_vector = np.zeros((num_skes, 60))
    for idx, l in enumerate(labels):
        labels_vector[idx, l] = 1

    return labels_vector


def split_train_val(train_indices, method='sklearn', ratio=0.05):
    """"""
    Get validation set by splitting data randomly from training set with two methods.
    In fact, I thought these two methods are equal as they got the same performance.

    """"""
    if method == 'sklearn':
        return train_test_split(train_indices, test_size=ratio, random_state=10000)
    else:
        np.random.seed(10000)
        np.random.shuffle(train_indices)
        val_num_skes = int(np.ceil(0.05 * len(train_indices)))
        val_indices = train_indices[:val_num_skes]
        train_indices = train_indices[val_num_skes:]
        return train_indices, val_indices


def split_dataset(skes_joints, label, performer, camera, evaluation, save_path):
    # Select validation set from training set    
    train_indices, test_indices = get_indices(performer, camera, evaluation)

    # Save labels and num_frames for each sequence of each data set
    train_labels = label[train_indices]
    test_labels = label[test_indices]

    train_x = skes_joints[train_indices]
    train_y = one_hot_vector(train_labels)
    test_x = skes_joints[test_indices]
    test_y = one_hot_vector(test_labels)

    save_name = 'NTU60_%s.npz' % evaluation
    np.savez(save_name, x_train=train_x, y_train=train_y, x_test=test_x, y_test=test_y)

def get_indices(performer, camera, evaluation='XSub'):
    test_indices = np.empty(0)
    train_indices = np.empty(0)

    if evaluation == 'XSub':  # Cross Subject (Subject IDs)
        train_ids = [1,  2,  4,  5,  8,  9,  13, 14, 15, 16,
                     17, 18, 19, 25, 27, 28, 31, 34, 35, 38]
        test_ids = [3,  6,  7,  10, 11, 12, 20, 21, 22, 23,
                    24, 26, 29, 30, 32, 33, 36, 37, 39, 40]

        # Get indices of test data
        for idx in test_ids:
            temp = np.where(performer == idx)[0]  # 0-based index
            test_indices = np.hstack((test_indices, temp)).astype(np.int)

        # Get indices of training data
        for train_id in train_ids:
            temp = np.where(performer == train_id)[0]  # 0-based index
            train_indices = np.hstack((train_indices, temp)).astype(np.int)
    else:  # Cross View (Camera IDs)
        train_ids = [2, 3]
        test_ids = 1
        # Get indices of test data
        temp = np.where(camera == test_ids)[0]  # 0-based index
        test_indices = np.hstack((test_indices, temp)).astype(np.int)

        # Get indices of training data
        for train_id in train_ids:
            temp = np.where(camera == train_id)[0]  # 0-based index
            train_indices = np.hstack((train_indices, temp)).astype(np.int)

    return train_indices, test_indices


if __name__ == '__main__':
    camera = np.loadtxt(camera_file, dtype=np.int)  # camera id: 1, 2, 3
    performer = np.loadtxt(performer_file, dtype=np.int)  # subject id: 1~40
    label = np.loadtxt(label_file, dtype=np.int) - 1  # action label: 0~59

    frames_cnt = np.loadtxt(frames_file, dtype=np.int)  # frames_cnt
    skes_name = np.loadtxt(skes_name_file, dtype=np.string_)

    with open(raw_skes_joints_pkl, 'rb') as fr:
        skes_joints = pickle.load(fr)  # a list

    skes_joints = seq_translation(skes_joints)

    skes_joints = align_frames(skes_joints, frames_cnt)  # aligned to the same frame length

    evaluations = ['XSub', 'XView']
    for evaluation in evaluations:
        split_dataset(skes_joints, label, performer, camera, evaluation, save_path)
"
8,2201.02849,"import random
import matplotlib.pyplot as plt
import numpy as np
import pdb

import torch
import torch.nn.functional as F

def valid_crop_resize(data_numpy,valid_frame_num,p_interval,window):
    # input: C,T,V,M
    C, T, V, M = data_numpy.shape
    begin = 0
    end = valid_frame_num
    valid_size = end - begin

    #crop
    if len(p_interval) == 1:
        p = p_interval[0]
        bias = int((1-p) * valid_size/2)
        data = data_numpy[:, begin+bias:end-bias, :, :]# center_crop
        cropped_length = data.shape[1]
    else:
        p = np.random.rand(1)*(p_interval[1]-p_interval[0])+p_interval[0]
        cropped_length = np.minimum(np.maximum(int(np.floor(valid_size*p)),64), valid_size)# constraint cropped_length lower bound as 64
        bias = np.random.randint(0,valid_size-cropped_length+1)
        data = data_numpy[:, begin+bias:begin+bias+cropped_length, :, :]
        if data.shape[1] == 0:
            print(cropped_length, bias, valid_size)

    # resize
    data = torch.tensor(data,dtype=torch.float)
    data = data.permute(0, 2, 3, 1).contiguous().view(C * V * M, cropped_length)
    data = data[None, None, :, :]
    data = F.interpolate(data, size=(C * V * M, window), mode='bilinear',align_corners=False).squeeze() # could perform both up sample and down sample
    data = data.contiguous().view(C, V, M, window).permute(0, 3, 1, 2).contiguous().numpy()

    return data

def downsample(data_numpy, step, random_sample=True):
    # input: C,T,V,M
    begin = np.random.randint(step) if random_sample else 0
    return data_numpy[:, begin::step, :, :]


def temporal_slice(data_numpy, step):
    # input: C,T,V,M
    C, T, V, M = data_numpy.shape
    return data_numpy.reshape(C, T / step, step, V, M).transpose(
        (0, 1, 3, 2, 4)).reshape(C, T / step, V, step * M)


def mean_subtractor(data_numpy, mean):
    # input: C,T,V,M
    # naive version
    if mean == 0:
        return
    C, T, V, M = data_numpy.shape
    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0
    begin = valid_frame.argmax()
    end = len(valid_frame) - valid_frame[::-1].argmax()
    data_numpy[:, :end, :, :] = data_numpy[:, :end, :, :] - mean
    return data_numpy


def auto_pading(data_numpy, size, random_pad=False):
    C, T, V, M = data_numpy.shape
    if T < size:
        begin = random.randint(0, size - T) if random_pad else 0
        data_numpy_paded = np.zeros((C, size, V, M))
        data_numpy_paded[:, begin:begin + T, :, :] = data_numpy
        return data_numpy_paded
    else:
        return data_numpy


def random_choose(data_numpy, size, auto_pad=True):
    # input: C,T,V,M 随机选择其中一段，不是很合理。因为有0
    C, T, V, M = data_numpy.shape
    if T == size:
        return data_numpy
    elif T < size:
        if auto_pad:
            return auto_pading(data_numpy, size, random_pad=True)
        else:
            return data_numpy
    else:
        begin = random.randint(0, T - size)
        return data_numpy[:, begin:begin + size, :, :]

def random_move(data_numpy,
                angle_candidate=[-10., -5., 0., 5., 10.],
                scale_candidate=[0.9, 1.0, 1.1],
                transform_candidate=[-0.2, -0.1, 0.0, 0.1, 0.2],
                move_time_candidate=[1]):
    # input: C,T,V,M
    C, T, V, M = data_numpy.shape
    move_time = random.choice(move_time_candidate)
    node = np.arange(0, T, T * 1.0 / move_time).round().astype(int)
    node = np.append(node, T)
    num_node = len(node)

    A = np.random.choice(angle_candidate, num_node)
    S = np.random.choice(scale_candidate, num_node)
    T_x = np.random.choice(transform_candidate, num_node)
    T_y = np.random.choice(transform_candidate, num_node)

    a = np.zeros(T)
    s = np.zeros(T)
    t_x = np.zeros(T)
    t_y = np.zeros(T)

    # linspace
    for i in range(num_node - 1):
        a[node[i]:node[i + 1]] = np.linspace(
            A[i], A[i + 1], node[i + 1] - node[i]) * np.pi / 180
        s[node[i]:node[i + 1]] = np.linspace(S[i], S[i + 1],
                                             node[i + 1] - node[i])
        t_x[node[i]:node[i + 1]] = np.linspace(T_x[i], T_x[i + 1],
                                               node[i + 1] - node[i])
        t_y[node[i]:node[i + 1]] = np.linspace(T_y[i], T_y[i + 1],
                                               node[i + 1] - node[i])

    theta = np.array([[np.cos(a) * s, -np.sin(a) * s],
                      [np.sin(a) * s, np.cos(a) * s]])

    # perform transformation
    for i_frame in range(T):
        xy = data_numpy[0:2, i_frame, :, :]
        new_xy = np.dot(theta[:, :, i_frame], xy.reshape(2, -1))
        new_xy[0] += t_x[i_frame]
        new_xy[1] += t_y[i_frame]
        data_numpy[0:2, i_frame, :, :] = new_xy.reshape(2, V, M)

    return data_numpy


def random_shift(data_numpy):
    C, T, V, M = data_numpy.shape
    data_shift = np.zeros(data_numpy.shape)
    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0
    begin = valid_frame.argmax()
    end = len(valid_frame) - valid_frame[::-1].argmax()

    size = end - begin
    bias = random.randint(0, T - size)
    data_shift[:, bias:bias + size, :, :] = data_numpy[:, begin:end, :, :]

    return data_shift


def _rot(rot):
    """"""
    rot: T,3
    """"""
    cos_r, sin_r = rot.cos(), rot.sin()  # T,3
    zeros = torch.zeros(rot.shape[0], 1)  # T,1
    ones = torch.ones(rot.shape[0], 1)  # T,1

    r1 = torch.stack((ones, zeros, zeros),dim=-1)  # T,1,3
    rx2 = torch.stack((zeros, cos_r[:,0:1], sin_r[:,0:1]), dim = -1)  # T,1,3
    rx3 = torch.stack((zeros, -sin_r[:,0:1], cos_r[:,0:1]), dim = -1)  # T,1,3
    rx = torch.cat((r1, rx2, rx3), dim = 1)  # T,3,3

    ry1 = torch.stack((cos_r[:,1:2], zeros, -sin_r[:,1:2]), dim =-1)
    r2 = torch.stack((zeros, ones, zeros),dim=-1)
    ry3 = torch.stack((sin_r[:,1:2], zeros, cos_r[:,1:2]), dim =-1)
    ry = torch.cat((ry1, r2, ry3), dim = 1)

    rz1 = torch.stack((cos_r[:,2:3], sin_r[:,2:3], zeros), dim =-1)
    r3 = torch.stack((zeros, zeros, ones),dim=-1)
    rz2 = torch.stack((-sin_r[:,2:3], cos_r[:,2:3],zeros), dim =-1)
    rz = torch.cat((rz1, rz2, r3), dim = 1)

    rot = rz.matmul(ry).matmul(rx)
    return rot


def random_rot(data_numpy, theta=0.3):
    """"""
    data_numpy: C,T,V,M
    """"""
    data_torch = torch.from_numpy(data_numpy)
    C, T, V, M = data_torch.shape
    data_torch = data_torch.permute(1, 0, 2, 3).contiguous().view(T, C, V*M)  # T,3,V*M
    rot = torch.zeros(3).uniform_(-theta, theta)
    rot = torch.stack([rot, ] * T, dim=0)
    rot = _rot(rot)  # T,3,3
    data_torch = torch.matmul(rot, data_torch)
    data_torch = data_torch.view(T, C, V, M).permute(1, 0, 2, 3).contiguous()

    return data_torch

def openpose_match(data_numpy):
    C, T, V, M = data_numpy.shape
    assert (C == 3)
    score = data_numpy[2, :, :, :].sum(axis=1)
    # the rank of body confidence in each frame (shape: T-1, M)
    rank = (-score[0:T - 1]).argsort(axis=1).reshape(T - 1, M)

    # data of frame 1
    xy1 = data_numpy[0:2, 0:T - 1, :, :].reshape(2, T - 1, V, M, 1)
    # data of frame 2
    xy2 = data_numpy[0:2, 1:T, :, :].reshape(2, T - 1, V, 1, M)
    # square of distance between frame 1&2 (shape: T-1, M, M)
    distance = ((xy2 - xy1) ** 2).sum(axis=2).sum(axis=0)

    # match pose
    forward_map = np.zeros((T, M), dtype=int) - 1
    forward_map[0] = range(M)
    for m in range(M):
        choose = (rank == m)
        forward = distance[choose].argmin(axis=1)
        for t in range(T - 1):
            distance[t, :, forward[t]] = np.inf
        forward_map[1:][choose] = forward
    assert (np.all(forward_map >= 0))

    # string data
    for t in range(T - 1):
        forward_map[t + 1] = forward_map[t + 1][forward_map[t]]

    # generate data
    new_data_numpy = np.zeros(data_numpy.shape)
    for t in range(T):
        new_data_numpy[:, t, :, :] = data_numpy[:, t, :, forward_map[
                                                             t]].transpose(1, 2, 0)
    data_numpy = new_data_numpy

    # score sort
    trace_score = data_numpy[2, :, :, :].sum(axis=1).sum(axis=0)
    rank = (-trace_score).argsort()
    data_numpy = data_numpy[:, :, :, rank]

    return data_numpy
"
9,2201.02849,"import numpy as np
from torch.utils.data import Dataset
from feeders import tools


class Feeder(Dataset):
    def __init__(self, data_path, label_path=None, p_interval=1, split='train', random_choose=False, random_shift=False,
                 random_move=False, random_rot=False, window_size=-1, normalization=False, debug=False, use_mmap=True,
                 bone=False, vel=False):
        """"""
        data_path:
        label_path:
        split: training set or test set
        random_choose: If true, randomly choose a portion of the input sequence
        random_shift: If true, randomly pad zeros at the begining or end of sequence
        random_move:
        random_rot: rotate skeleton around xyz axis
        window_size: The length of the output sequence
        normalization: If true, normalize input sequence
        debug: If true, only use the first 100 samples
        use_mmap: If true, use mmap mode to load data, which can save the running memory
        bone: use bone modality or not
        vel: use motion modality or not
        only_label: only load label for ensemble score compute
        """"""

        self.debug = debug
        self.data_path = data_path
        self.label_path = label_path
        self.split = split
        self.random_choose = random_choose
        self.random_shift = random_shift
        self.random_move = random_move
        self.window_size = window_size
        self.normalization = normalization
        self.use_mmap = use_mmap
        self.p_interval = p_interval
        self.random_rot = random_rot
        self.bone = bone
        self.vel = vel
        self.load_data()
        if normalization:
            self.get_mean_map()

    def load_data(self):
        # data: N C V T M
        if self.use_mmap:
            npz_data = np.load(self.data_path, mmap_mode='r')
        else:
            npz_data = np.load(self.data_path)

        if self.split == 'train':
            self.data = npz_data['x_train']
            self.label = np.where(npz_data['y_train'] > 0)[1]
            self.sample_name = ['train_' + str(i) for i in range(len(self.data))]
        elif self.split == 'test':
            self.data = npz_data['x_test']
            self.label = np.where(npz_data['y_test'] > 0)[1]
            self.sample_name = ['test_' + str(i) for i in range(len(self.data))]
        else:
            raise NotImplementedError('data split only supports train/test')

        N, T, _ = self.data.shape
        self.data = self.data.reshape((N, T, 2, 25, 3)).transpose(0, 4, 1, 3, 2)


    def get_mean_map(self):
        data = self.data
        N, C, T, V, M = data.shape
        self.mean_map = data.mean(axis=2, keepdims=True).mean(axis=4, keepdims=True).mean(axis=0)
        self.std_map = data.transpose((0, 2, 4, 1, 3)).reshape((N * T * M, C * V)).std(axis=0).reshape((C, 1, V, 1))

    def __len__(self):
        return len(self.label)

    def __iter__(self):
        return self

    def __getitem__(self, index):
        data_numpy = self.data[index]
        label = self.label[index]
        data_numpy = np.array(data_numpy)
        valid_frame_num = np.sum(data_numpy.sum(0).sum(-1).sum(-1) != 0)
        # reshape Tx(MVC) to CTVM
        data_numpy = tools.valid_crop_resize(data_numpy, valid_frame_num, self.p_interval, self.window_size)
        if self.random_rot:
            data_numpy = tools.random_rot(data_numpy)
        if self.bone:
            ntu_pairs = ((1, 2), (2, 21), (3, 21), (4, 3), (5, 21), (6, 5),
                (7, 6), (8, 7), (9, 21), (10, 9), (11, 10), (12, 11),
                (13, 1), (14, 13), (15, 14), (16, 15), (17, 1), (18, 17),
                (19, 18), (20, 19), (22, 23), (21, 21), (23, 8), (24, 25),(25, 12))
            bone_data_numpy = np.zeros_like(data_numpy)
            for v1, v2 in ntu_pairs:
                bone_data_numpy[:, :, v1 - 1] = data_numpy[:, :, v1 - 1] - data_numpy[:, :, v2 - 1]
            data_numpy = bone_data_numpy
        if self.vel:
            data_numpy[:, :-1] = data_numpy[:, 1:] - data_numpy[:, :-1]
            data_numpy[:, -1] = 0

        return data_numpy, label, index"
10,2201.02849,"from . import tools
from . import feeder_ntu"
11,2201.02849,"import torch
import torch.nn as nn
import math
import numpy as np

class Pos_Embed(nn.Module):
    def __init__(self, channels, num_frames, num_joints):
        super().__init__()

        pos_list = []
        for tk in range(num_frames):
            for st in range(num_joints):
                pos_list.append(st)

        position = torch.from_numpy(np.array(pos_list)).unsqueeze(1).float()

        pe = torch.zeros(num_frames * num_joints, channels)

        div_term = torch.exp(torch.arange(0, channels, 2).float() * -(math.log(10000.0) / channels)) 
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.view(num_frames, num_joints, channels).permute(2, 0, 1).unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):  # nctv
        x = self.pe[:, :, :x.size(2)]
        return x"
12,2201.02849,"import torch.nn as nn
from .sta_block import STA_Block


def conv_init(conv):
    nn.init.kaiming_normal_(conv.weight, mode='fan_out')
    # nn.init.constant_(conv.bias, 0)

def bn_init(bn, scale):
    nn.init.constant_(bn.weight, scale)
    nn.init.constant_(bn.bias, 0)

def fc_init(fc):
    nn.init.xavier_normal_(fc.weight)
    nn.init.constant_(fc.bias, 0)


class Model(nn.Module):
    def __init__(self, len_parts, num_classes, num_joints, 
                 num_frames, num_heads, num_persons, num_channels, 
                 kernel_size, use_pes=True, config=None, 
                 att_drop=0, dropout=0, dropout2d=0):
        super().__init__()

        self.len_parts = len_parts
        in_channels = config[0][0]
        self.out_channels = config[-1][1]

        num_frames = num_frames // len_parts
        num_joints = num_joints * len_parts
        
        self.input_map = nn.Sequential(
            nn.Conv2d(num_channels, in_channels, 1),
            nn.BatchNorm2d(in_channels),
            nn.LeakyReLU(0.1))

        self.blocks = nn.ModuleList()
        for index, (in_channels, out_channels, qkv_dim) in enumerate(config):
            self.blocks.append(STA_Block(in_channels, out_channels, qkv_dim, 
                                         num_frames=num_frames, 
                                         num_joints=num_joints, 
                                         num_heads=num_heads,
                                         kernel_size=kernel_size,
                                         use_pes=use_pes,
                                         att_drop=att_drop))   

        self.fc = nn.Linear(self.out_channels, num_classes)
        self.drop_out = nn.Dropout(dropout)
        self.drop_out2d = nn.Dropout2d(dropout2d)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                conv_init(m)
            elif isinstance(m, nn.BatchNorm2d):
                bn_init(m, 1)
            elif isinstance(m, nn.Linear):
                fc_init(m)

    def forward(self, x):

        N, C, T, V, M = x.shape

        x = x.permute(0, 4, 1, 2, 3).contiguous().view(N * M, C, T, V)
        x = x.view(x.size(0), x.size(1), T // self.len_parts, V * self.len_parts)
        x = self.input_map(x)

        for i, block in enumerate(self.blocks):
            x = block(x)

        # NM, C, T, V
        x = x.view(N, M, self.out_channels, -1)
        x = x.permute(0, 1, 3, 2).contiguous().view(N, -1, self.out_channels, 1)
        x = self.drop_out2d(x)
        x = x.mean(3).mean(1)

        x = self.drop_out(x)

        return self.fc(x)"
13,2201.02849,from model import sttformer
14,2201.02849,"import torch
import torch.nn as nn
from .pos_embed import Pos_Embed

class STA_Block(nn.Module):
    def __init__(self, in_channels, out_channels, qkv_dim,
                 num_frames, num_joints, num_heads,
                 kernel_size, use_pes=True, att_drop=0):
        super().__init__()
        self.qkv_dim = qkv_dim
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_heads = num_heads
        self.use_pes = use_pes
        pads = int((kernel_size[1] - 1) / 2)
        padt = int((kernel_size[0] - 1) / 2)
        
        # Spatio-Temporal Tuples Attention
        if self.use_pes: self.pes = Pos_Embed(in_channels, num_frames, num_joints)
        self.to_qkvs = nn.Conv2d(in_channels, 2 * num_heads * qkv_dim, 1, bias=True)
        self.alphas = nn.Parameter(torch.ones(1, num_heads, 1, 1), requires_grad=True)
        self.att0s = nn.Parameter(torch.ones(1, num_heads, num_joints, num_joints) / num_joints, requires_grad=True)
        self.out_nets = nn.Sequential(nn.Conv2d(in_channels * num_heads, out_channels, (1, kernel_size[1]), padding=(0, pads)), nn.BatchNorm2d(out_channels))
        self.ff_net = nn.Sequential(nn.Conv2d(out_channels, out_channels, 1), nn.BatchNorm2d(out_channels))

        # Inter-Frame Feature Aggregation
        self.out_nett = nn.Sequential(nn.Conv2d(out_channels, out_channels, (kernel_size[0], 1), padding=(padt, 0)), nn.BatchNorm2d(out_channels))

        if in_channels != out_channels:
            self.ress = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1), nn.BatchNorm2d(out_channels))
            self.rest = nn.Sequential(nn.Conv2d(out_channels, out_channels, 1), nn.BatchNorm2d(out_channels))
        else:
            self.ress = lambda x: x
            self.rest = lambda x: x

        self.tan = nn.Tanh()
        self.relu = nn.LeakyReLU(0.1)
        self.drop = nn.Dropout(att_drop)

    def forward(self, x):

        N, C, T, V = x.size()
        # Spatio-Temporal Tuples Attention
        xs = self.pes(x) + x if self.use_pes else x
        q, k = torch.chunk(self.to_qkvs(xs).view(N, 2 * self.num_heads, self.qkv_dim, T, V), 2, dim=1)
        attention = self.tan(torch.einsum('nhctu,nhctv->nhuv', [q, k]) / (self.qkv_dim * T)) * self.alphas
        attention = attention + self.att0s.repeat(N, 1, 1, 1)
        attention = self.drop(attention)
        xs = torch.einsum('nctu,nhuv->nhctv', [x, attention]).contiguous().view(N, self.num_heads * self.in_channels, T, V)
        x_ress = self.ress(x)
        xs = self.relu(self.out_nets(xs) + x_ress)
        xs = self.relu(self.ff_net(xs) + x_ress)

        # Inter-Frame Feature Aggregation
        xt = self.relu(self.out_nett(xs) + self.rest(xs))

        return xt"
15,2201.02798,"import os
from glob import glob

data_dir = ""data/down_stream""
# data_dir = (
#     ""/Users/kelchtermans/mount/esat/code/contrastive-learning/data/down_stream""
# )
# data_dir = ""/Users/kelchtermans/mount/opal/contrastive_learning/dtd_augment""

TARGETS = [""cone"", ""line"", ""gate""]
LINK_BEST_MODELS = True
WRITE_TABLE = True
TASKS = [""velocities"", ""waypoints""]

output_dir = os.path.join(data_dir, ""overview"")
os.makedirs(output_dir, exist_ok=True)

print(""TARGETS: "", TARGETS)
print(""TASKS: "", TASKS)
print(""OUTPUTDIR: "", output_dir)


def get_results_from_txt(filename) -> dict:
    try:
        with open(filename, ""r"") as f:
            lines = f.readlines()
        return {ln.split("":"")[0]: float(ln.split("":"")[1]) for ln in lines if "":"" in ln}
    except FileNotFoundError:
        return {}


# Get an overview of the quantitative results
# and keep the paths of the best learning rates
overview_results = {t: {c: None for c in TASKS} for t in TARGETS}
winning_lrs = {t: {c: None for c in TASKS} for t in TARGETS}

for target in TARGETS:
    print(f""Parsing target: {target}"")
    for tsk in TASKS:
        print(f""Parsing config: {tsk}"")
        lr_paths = glob(f""{os.path.join(data_dir, tsk, target)}/*"")
        lr_paths = [
            p for p in lr_paths if os.path.exists(os.path.join(p, ""results.txt""))
        ]
        values = {
            lrp: get_results_from_txt(os.path.join(lrp, ""results.txt""))
            for lrp in lr_paths
        }
        validation_losses = {
            lrp: values[lrp][""validation_mse_loss_avg""] for lrp in lr_paths
        }
        best_lrp = [
            k for k, v in sorted(validation_losses.items(), key=lambda item: item[1])
        ][0]
        overview_results[target][tsk] = values[best_lrp]
        winning_lrs[target][tsk] = best_lrp

if WRITE_TABLE:
    print(""WRITE_TABLE"")
    # Print table and store to file:
    overview_file = open(output_dir + ""/overview_table.txt"", ""w"")
    for target in TARGETS:
        msg = f""{target} && \\\\""
        print(msg)
        overview_file.write(""\\hline\n"")
        overview_file.write(msg + ""\n"")
        overview_file.write(""\\hline\n"")
        for conf in TASKS:
            try:
                msg = f'{os.path.basename(conf).replace(""_"", "" "")} '
                msg += (
                    f'&  {overview_results[target][conf][""validation_mse_loss_avg""]} '
                )
                msg += (
                    f'(±{overview_results[target][conf][""validation_mse_loss_std""]}) & '
                )
                msg += f'{overview_results[target][conf][""out-of-distribution_mse_loss_avg""]} '
                msg += f'(±{overview_results[target][conf][""out-of-distribution_mse_loss_std""]})'
                msg += "" \\\\""
                print(msg)
                overview_file.write(msg + ""\n"")
            except KeyError:
                print(f""Failed to parse {conf}/{target}"")
    overview_file.close()

if LINK_BEST_MODELS:
    print(""LINK_BEST_MODELS"")
    for target in TARGETS:
        for tsk in TASKS:
            if os.path.exists(f""{os.path.dirname(winning_lrs[target][tsk])}/best""):
                os.system(f""rm {os.path.dirname(winning_lrs[target][tsk])}/best"")
            # create symbolic link ""best"" pointing to best learning rate
            os.system(
                # f""ln -s {os.path.join(os.getcwd(), winning_lrs[target][tsk])} {os.path.dirname(winning_lrs[target][tsk])}/best""
                f""mv {os.path.join(os.getcwd(), winning_lrs[target][tsk])} {os.path.dirname(winning_lrs[target][tsk])}/best""
            )
            msg = f""{target} - {tsk} - {winning_lrs[target][tsk]}""
            print(msg)

print(""finished"")
"
16,2201.02798,"import os
from argparse import ArgumentParser
import shutil

import json
from pprint import pprint
import torch
from torch.utils.data import DataLoader as TorchDataLoader
from torch.utils.tensorboard import SummaryWriter

import fgbg

parser = ArgumentParser()
parser.add_argument(""--config_file"")
parser.add_argument(""--task"", type=str)
parser.add_argument(""--learning_rate"", type=float)
parser.add_argument(""--output_dir"", type=str)
parser.add_argument(""--target"", type=str)
parser.add_argument(""--encoder_ckpt_dir"", type=str)
parser.add_argument(""--number_of_epochs"", type=int)
parser.add_argument(""--end_to_end"", dest=""end_to_end"", action=""store_true"")
parser.add_argument(""--evaluate"", dest=""evaluate"", action=""store_true"")
parser.add_argument(""--rm"", dest=""rm"", action=""store_true"")
config = vars(parser.parse_args())
if config[""config_file""] is not None:
    with open(config[""config_file""], ""r"") as f:
        json_config = json.load(f)
    for k, v in config.items():
        if v is not None:
            json_config[k] = v
    config = json_config  # update config to json's config
pprint(config)

if __name__ == ""__main__"":
    target = config[""target""]
    output_directory = (
        f'data/{os.path.basename(config[""config_file""][:-5])}/{target}'
        if ""output_dir"" not in config.keys()
        else config[""output_dir""]
    )
    if config[""rm""] and os.path.isdir(output_directory):
        shutil.rmtree(output_directory)
    os.makedirs(output_directory, exist_ok=True)
    with open(os.path.join(output_directory, ""config""), ""w"") as f:
        json.dump(config, f)

    tb_writer = SummaryWriter(log_dir=output_directory)
    checkpoint_file = os.path.join(output_directory, ""checkpoint_model.ckpt"")
    if config[""task""] == ""pretrain"":
        if config[""architecture""] == ""deepsupervision"":
            model = fgbg.DeepSupervisionNet(
                batch_norm=config[""batch_normalisation""],
                no_deep_supervision=not config[""deep_supervision""],
            )
        else:
            raise NotImplementedError
    elif config[""task""] == ""velocities"":
        model = fgbg.DownstreamNet(
            output_size=(4,),
            encoder_ckpt_dir=config[""encoder_ckpt_dir""],
            end_to_end=config[""end_to_end""],
            batch_norm=config[""batch_normalisation""],
            no_deep_supervision=not config[""deep_supervision""],
        )
    elif config[""task""] == ""waypoints"":
        model = fgbg.DownstreamNet(
            output_size=(3,),
            encoder_ckpt_dir=config[""encoder_ckpt_dir""],
            end_to_end=config[""end_to_end""],
            batch_norm=config[""batch_normalisation""],
            no_deep_supervision=not config[""deep_supervision""],
        )

    print(f""{fgbg.get_date_time_tag()} - Generate dataset"")
    if not bool(config[""augment""]):
        dataset = fgbg.CleanDataset(
            hdf5_file=os.path.join(config[""training_directory""], target, ""data.hdf5""),
            json_file=os.path.join(config[""training_directory""], target, ""data.json""),
            fg_augmentation=config[""fg_augmentation""],
            input_size=model.input_size,
            output_size=model.output_size,
        )
    else:
        dataset = fgbg.AugmentedTripletDataset(
            hdf5_file=os.path.join(config[""training_directory""], target, ""data.hdf5""),
            json_file=os.path.join(config[""training_directory""], target, ""data.json""),
            background_images_directory=config[""texture_directory""],
            combined_blur=config[""combined_blur""],
            fg_augmentation=config[""fg_augmentation""],
            input_size=model.input_size,
            output_size=model.output_size,
        )
    train_set, val_set = torch.utils.data.random_split(
        dataset, [int(0.9 * len(dataset)), len(dataset) - int(0.9 * len(dataset))]
    )
    if not config[""evaluate""]:
        train_dataloader = TorchDataLoader(
            dataset=train_set,
            batch_size=config[""batch_size""],
            shuffle=True,
            num_workers=4 if torch.cuda.is_available() else 0,
        )
        val_dataloader = TorchDataLoader(
            dataset=val_set,
            batch_size=config[""batch_size""],
            shuffle=True,
            num_workers=4 if torch.cuda.is_available() else 0,
        )

        print(f""{fgbg.get_date_time_tag()} - Train autoencoder"")
        if config[""task""] == ""pretrain"":
            fgbg.train_autoencoder(
                model,
                train_dataloader,
                val_dataloader,
                checkpoint_file,
                tb_writer,
                triplet_loss_weight=config[""triplet""],
                num_epochs=config[""number_of_epochs""],
                learning_rate=config[""learning_rate""],
                deep_supervision=config[""deep_supervision""],
            )
        else:
            fgbg.train_downstream_task(
                model,
                train_dataloader,
                val_dataloader,
                checkpoint_file,
                tb_writer,
                task=config[""task""],
                num_epochs=config[""number_of_epochs""],
                learning_rate=config[""learning_rate""],
            )
    # set weights to best validation checkpoint
    ckpt = torch.load(checkpoint_file, map_location=torch.device(""cpu""))
    model.load_state_dict(ckpt[""state_dict""])
    model.global_step = ckpt[""global_step""]
    model.eval()

    if config[""task""] == ""pretrain"":
        print(f""{fgbg.get_date_time_tag()} - Evaluate on training images"")
        fgbg.evaluate_qualitatively_on_dataset(""training"", train_set, model, tb_writer)

        print(f""{fgbg.get_date_time_tag()} - Evaluate on validation images"")
        fgbg.evaluate_qualitatively_on_dataset(""validation"", val_set, model, tb_writer)

        print(f""{fgbg.get_date_time_tag()} - Evaluate on out-of-distribution images"")
        ood_set = fgbg.LabelledImagesDataset(
            img_dir_name=config[""ood_directory""] + ""/input"",
            target=target,
            mask_dir_name=config[""ood_directory""] + ""/mask"",
        )
        fgbg.evaluate_qualitatively_on_dataset(
            ""out-of-distribution"", ood_set, model, tb_writer
        )
        fgbg.evaluate_quantitatively_on_dataset(
            ""out-of-distribution"", ood_set, model, tb_writer, config[""task""]
        )

    print(f""{fgbg.get_date_time_tag()} - Finished"")
    os.system(f""touch {output_directory}/FINISHED"")
"
17,2201.02798,"import os
import shutil
from glob import glob

data_dir = ""data/dtd_and_places_augmented""
assert os.path.isdir(data_dir)
# data_dir = (
#     ""/Users/kelchtermans/mount/esat/code/contrastive-learning/data/places_augmented""
# )
# data_dir = ""/Users/kelchtermans/mount/opal/contrastive_learning/dtd_augment""

# TARGETS = [""cone"", ""gate"", ""line""]
TARGETS = [""red_line"", ""line""]
COPY_REAL_IMGS = True
LINK_BEST_MODELS = True
WRITE_TABLE = True

CONFIGS = [
    # ""vanilla"",
    ""default"",
    # ""default_fg"",
    ""deep_supervision"",
    ""deep_supervision_fg"",
    # ""triplet"",
    # ""triplet_fg"",
]
output_dir = os.path.join(data_dir, ""overview"")
os.makedirs(output_dir, exist_ok=True)

print(""TARGETS: "", TARGETS)
print(""CONFIGS: "", CONFIGS)
print(""OUTPUTDIR: "", output_dir)


def get_results_from_txt(filename) -> dict:
    try:
        with open(filename, ""r"") as f:
            lines = f.readlines()
        return {ln.split("":"")[0]: float(ln.split("":"")[1]) for ln in lines if "":"" in ln}
    except FileNotFoundError:
        return {}


# Get an overview of the quantitative results
# and keep the paths of the best learning rates
overview_results = {t: {c: None for c in CONFIGS} for t in TARGETS}
winning_lrs = {t: {c: None for c in CONFIGS} for t in TARGETS}

for target in TARGETS:
    print(f""Parsing target: {target}"")
    for conf in CONFIGS:
        print(f""Parsing config: {conf}"")
        lr_paths = glob(f""{os.path.join(data_dir, 'pretrain',conf, target)}/*"")
        lr_paths = [
            p
            for p in lr_paths
            if os.path.exists(os.path.join(p, ""results.txt""))
            and os.path.exists(os.path.join(p, ""imgs""))
        ]
        values = {
            lrp: get_results_from_txt(os.path.join(lrp, ""results.txt""))
            for lrp in lr_paths
        }
        validation_losses = {
            lrp: values[lrp][""validation_bce_loss_avg""] for lrp in lr_paths
        }
        best_lrp = [
            k for k, v in sorted(validation_losses.items(), key=lambda item: item[1])
        ][0]
        overview_results[target][conf] = values[best_lrp]
        winning_lrs[target][conf] = best_lrp

if WRITE_TABLE:
    print(""WRITE_TABLE"")
    # Print table and store to file:
    overview_file = open(output_dir + ""/overview_table_pretrain.txt"", ""w"")
    overview_file.write(""& IoU (+-std) & Weighted BCELoss (+-std) \\\\ \n"")
    for target in TARGETS:
        overview_file.write(""\\hline \n"")
        msg = ""\\textbf{""+target.replace('_', ' ')+""} & & \\\\""
        # msg = ""\multicolumn{3}{c}{"" + target + ""}\\\\""
        print(msg)
        overview_file.write(msg + ""\n"")
        for conf in CONFIGS:
            try:
                msg = f'{os.path.basename(conf).replace(""_"", "" "")} '
                msg += f'&  {overview_results[target][conf][""validation_iou_avg""]} '
                msg += f'(±{overview_results[target][conf][""validation_iou_std""]}) & '
                msg += f'{overview_results[target][conf][""validation_bce_loss_avg""]} '
                msg += f'(±{overview_results[target][conf][""validation_bce_loss_std""]})'
                msg += "" \\\\""
                print(msg)
                overview_file.write(msg + ""\n"")
            except KeyError:
                print(f""Failed to parse {conf}/{target}"")
    overview_file.close()

# Copy winning real images for quantitative results
if COPY_REAL_IMGS:
    print(""COPY_REAL_IMGS"")
    for target in TARGETS:
        for conf in CONFIGS:
            try:
                shutil.copyfile(
                    winning_lrs[target][conf] + ""/imgs/real_0.jpg"",
                    f""{output_dir}/{target}_{os.path.basename(conf)}.jpg"",
                )
            except FileNotFoundError:
                print(f""Failed to copy from {winning_lrs[target][conf]}"")

if LINK_BEST_MODELS:
    print(""LINK_BEST_MODELS"")
    for target in TARGETS:
        for conf in CONFIGS:
            if not os.path.isdir(f""{os.path.dirname(winning_lrs[target][conf])}/best""):
                # create symbolic link ""best"" pointing to best learning rate
                # f""ln -s {os.path.join(os.getcwd(), winning_lrs[target][conf])} {os.path.dirname(winning_lrs[target][conf])}/best""
                # mv winning lr to 'best'
                os.system(
                    f""mv {os.path.join(os.getcwd(), winning_lrs[target][conf])} {os.path.dirname(winning_lrs[target][conf])}/best""
                )
                msg = f""{target} - {conf} - {winning_lrs[target][conf]}""
                print(msg)
print(""finished"")
"
18,2201.02798,"import os
import time
import shutil
import subprocess
import shlex


INTERPRETER_PATH = ""/esat/opal/kkelchte/conda/envs/venv/bin/python""
PROJECT_PATH = ""/users/visics/kkelchte/code/contrastive-learning""

SPECS = {
    ""Universe"": ""vanilla"",
    ""Requirements"": (
        ""(CUDAGlobalMemoryMb >= 3900) && (CUDACapability < 8.6) ""
        '&& (machine != ""vladimir.esat.kuleuven.be"") '
        '&& (machine != ""kochab.esat.kuleuven.be"") '
        '&& (machine != ""oculus.esat.kuleuven.be"") '
        '&& (machine != ""hematite.esat.kuleuven.be"") '
        '&& (machine != ""bornholm.esat.kuleuven.be"") '
        '&& (machine != ""egholm.esat.kuleuven.be"") '
        '&& (machine != ""estragon.esat.kuleuven.be"") '
    ),
    ""initial_dir"": PROJECT_PATH,
    ""priority"": 1,
    ""RequestCpus"": 4,
    ""Request_GPUs"": 1,
    ""RequestMemory"": ""10 G"",
    ""RequestDisk"": ""50 G"",
    ""Niceuser"": ""True"",
    ""+RequestWalltime"": int(100 * 7 * 60 * 3),
}

LINE_CONFIGS = [
    f""configs/{cf}.json""
    for cf in [
        # ""vanilla"",
        # ""augment_bg_dtd"",
        # ""augment_bg_places"",
        # ""augment_bg_dtd_and_places"",
        # ""augment_fg_brightness"",
        # ""augment_fg_contrast"",
        # ""augment_fg_hue"",
        # ""augment_fg_saturation"",
        ""augment_fg_blur"",
        ""augment_fg_blur_and_brightness"",
        # ""deep_supervision"",
        # ""deep_supervision_triplet""
    ]
]

NUM_EPOCHS = {""line"": 50, ""gate"": 100}
SUBMIT = True
RM_EXIST = True


def create_condor_job_file(trgt, config, lrate):
    config_tag = os.path.basename(config[:-5])
    output_dir = f""data/{trgt}/{config_tag}""
    if RM_EXIST and os.path.isdir(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir)
    with open(os.path.join(output_dir, ""condor.job""), ""w"") as jobfile:
        jobfile.write(f""executable     = {INTERPRETER_PATH} \n"")
        jobfile.write(
            f""arguments = {PROJECT_PATH}/run.py --config_file {PROJECT_PATH}/{config} ""
            f""--learning_rate {lrate} --target {trgt} ""
            f""--output_dir {output_dir} --number_of_epochs {NUM_EPOCHS[trgt]}\n""
        )

        for key, value in SPECS.items():
            jobfile.write(f""{key} \t = {value} \n"")

        jobfile.write(f""error   = {output_dir}/condor.err\n"")
        jobfile.write(f""output  = {output_dir}/condor.out\n"")
        jobfile.write(f""log     = {output_dir}/condor.log\n"")
        jobfile.write(""Queue \n"")

    return os.path.join(output_dir, ""condor.job"")


for conf in LINE_CONFIGS:
    filename = create_condor_job_file(""line"", conf, 0.0001)
    subprocess.call(shlex.split(f""condor_submit {filename}""))


print(""finished"")
"
19,2201.02798,"import os

import torch
import torchvision
from torch.utils.data import DataLoader as TorchDataLoader
import matplotlib.pyplot as plt

import fgbg


def test_model_architecture():
    model = fgbg.AutoEncoder(
        feature_size=512,
        projected_size=512,
        input_channels=3,
        decode_from_projection=True,
    )

    feature = model.encoder(torch.rand(1, 3, 128, 128))
    print(feature.shape)
    print(model.decoder(feature).shape)


def test_data_loading_clean():
    target = ""gate""
    data_dir = f""data/datasets/gate_cone_line/{target}""

    dataset = fgbg.CleanDataset(
        hdf5_file=f""{data_dir}/data.hdf5"", json_file=f""{data_dir}/data.json"",
    )
    dataloader = TorchDataLoader(dataset, 9, shuffle=True)
    for batch in dataloader:
        print(
            f'mean {batch[""observation""].mean()}, '
            f'std {batch[""observation""].std()}, '
            f'min {batch[""observation""].min()}, '
            f'max {batch[""observation""].max()}'
        )
        grid = torchvision.utils.make_grid(batch[""observation""], nrow=3)
        plt.imshow(grid.permute(1, 2, 0).numpy())
        plt.show()
        break


def test_data_loading_real_images():
    target = ""gate""
    data_dir = ""data/datasets/bebop_real""

    dataset = fgbg.ImagesDataset(
        target=target,
        dir_name=data_dir,
        input_size=(3, 200, 200),
        output_size=(200, 200),
    )
    dataloader = TorchDataLoader(dataset, 9, shuffle=True)
    for batch in dataloader:
        print(
            f'mean {batch[""observation""].mean()}, '
            f'std {batch[""observation""].std()}, '
            f'min {batch[""observation""].min()}, '
            f'max {batch[""observation""].max()}'
        )
        grid = torchvision.utils.make_grid(batch[""observation""], nrow=3)
        plt.imshow(grid.permute(1, 2, 0).numpy())
        plt.show()
        break


def test_data_loading_augment():
    target = ""cone""
    data_dir = f""data/datasets/debug_data/{target}""
    dataset = fgbg.AugmentedTripletDataset(
        hdf5_file=f""{data_dir}/data.hdf5"",
        json_file=f""{data_dir}/data.json"",
        background_images_directory=""data/datasets/dtd"",
        blur=True,
        fg_augmentation=True,
    )
    dataloader = TorchDataLoader(dataset, 9, shuffle=True)
    for batch in dataloader:
        print(
            f'mean {batch[""observation""].mean()}, '
            f'std {batch[""observation""].std()}, '
            f'min {batch[""observation""].min()}, '
            f'max {batch[""observation""].max()}'
        )
        grid_observation = torchvision.utils.make_grid(batch[""observation""], nrow=3)
        plt.imshow(grid_observation.permute(1, 2, 0).numpy())
        plt.title(""observation"")
        plt.show()
        grid_positive = torchvision.utils.make_grid(batch[""positive""], nrow=3)
        plt.imshow(grid_positive.permute(1, 2, 0).numpy())
        plt.title(""positive"")
        plt.show()
        grid_negative = torchvision.utils.make_grid(batch[""negative""], nrow=3)
        plt.imshow(grid_negative.permute(1, 2, 0).numpy())
        plt.title(""negative"")
        plt.show()
        break


def test_foreground_map():
    image = fgbg.create_random_gradient_image(size=(200, 200, 3))
    plt.imshow(image)
    plt.show()


def test_data_image_sequence():
    output_dir = ""data/test""
    model = fgbg.DeepSupervisionNet(batch_norm=False)
    checkpoint_file = os.path.join(""data/best_encoders/cone"", ""checkpoint_model.ckpt"")
    ckpt = torch.load(checkpoint_file, map_location=torch.device(""cpu""))
    model.load_state_dict(ckpt[""state_dict""])

    real_dataset = fgbg.ImageSequenceDataset(
        hdf5_file=""data/datasets/bebop_real_movies/cone/pruned_data.hdf5""
    )
    fgbg.evaluate_qualitatively_on_sequences(
        ""eval_real_sequence"", real_dataset, model, output_dir
    )
    print(f""stored in {output_dir}"")
    print(""done"")


if __name__ == ""__main__"":
    test_data_loading_augment()
"
20,2201.02798,"import os
import time
import shutil
import subprocess
import shlex


INTERPRETER_PATH = ""/esat/opal/kkelchte/conda/envs/venv/bin/python""
PROJECT_PATH = ""/users/visics/kkelchte/code/contrastive-learning""

SPECS = {
    ""Universe"": ""vanilla"",
    ""Requirements"": ""(CUDAGlobalMemoryMb >= 3900) && (CUDACapability < 8.6)"",
    ""initial_dir"": PROJECT_PATH,
    ""priority"": 1,
    ""RequestCpus"": 4,
    ""Request_GPUs"": 1,
    ""RequestMemory"": ""10 G"",
    ""RequestDisk"": ""50 G"",
    ""Niceuser"": ""True"",
    ""+RequestWalltime"": int(50 * 3 * 60),
}

# LINE
TARGET = ""line""
CONFIG = ""deep_supervision_triplet""
# CONFIG = ""augment_fg_brightness""
# CONFIG = ""augment_bg_places""

# GATE
# TARGET = 'gate'
# CONFIG = 'deep_supervision_comb_blur_brightness_hue_bn'

ENCODER = f""data/{TARGET}/{CONFIG}""
TASKS = [""velocities""] #, ""waypoints""]
LEARNING_RATE = 0.0001
SUBMIT = True
RM_EXIST = True
NUMEPOCH = 100


def create_condor_job_file(trgt, task, lrate):
    output_dir = f""data/{trgt}/{CONFIG}/{task}""
    if RM_EXIST and os.path.isdir(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir)
    with open(os.path.join(output_dir, ""condor.job""), ""w"") as jobfile:
        jobfile.write(f""executable     = {INTERPRETER_PATH} \n"")
        jobfile.write(
            f""arguments = {PROJECT_PATH}/run.py --config_file ""
            f""{PROJECT_PATH}/configs/{CONFIG}.json ""
            f""--learning_rate {lrate} --target {trgt} ""
            f""--output_dir {output_dir} ""
            f""--encoder_ckpt_dir {ENCODER} ""
            f""--task {task} --number_of_epochs {NUMEPOCH} \n""
        )

        for key, value in SPECS.items():
            jobfile.write(f""{key} \t = {value} \n"")

        jobfile.write(f""error   = {output_dir}/condor.err\n"")
        jobfile.write(f""output  = {output_dir}/condor.out\n"")
        jobfile.write(f""log     = {output_dir}/condor.log\n"")
        jobfile.write(""Queue \n"")

    return os.path.join(output_dir, ""condor.job"")


for task in TASKS:
    filename = create_condor_job_file(TARGET, task, LEARNING_RATE)
    if SUBMIT:
        print(f""submitting {filename}"")
        subprocess.call(shlex.split(f""condor_submit {filename}""))
        time.sleep(0.1)

print(""finished"")
"
21,2201.02798,"from datetime import datetime
import os

import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
import imageio


def create_trajectory_gif(filename: str, data: list):
    imageio.mimsave(filename, data)


def get_IoU(predictions: torch.Tensor, labels: torch.Tensor, threshold: float = 0.5):
    """"""Code inspired by
    https://www.kaggle.com/iezepov/fast-iou-scoring-metric-in-pytorch-and-numpy
    """"""
    eps = 1e-6
    # assert N x H x W
    if len(predictions.shape) != 3:
        predictions.unsqueeze_(0)
    if len(labels.shape) != 3:
        labels.unsqueeze_(0)
    outputs = (predictions > threshold).int()
    labels = labels.int()
    # Will be zero if Truth=0 or Prediction=0
    intersection = (outputs & labels).float().sum((1, 2))
    # Will be zero if both are
    union = (outputs | labels).float().sum((1, 2))
    # We smooth our devision to avoid 0/0
    iou = (intersection + eps) / (union + eps)
    return iou.mean()


def normalize(d: np.ndarray):
    d_min = np.amin(d)
    d_max = np.amax(d)
    return (d - d_min) / (d_max)


def combine_mask_observation(mask: np.array, observation: np.array) -> np.array:
    mask = np.stack([mask + 0.3] * 3, axis=-1)
    mask = np.clip(mask, 0, 1)
    if mask.shape != observation.shape:
        if len(mask.shape) == 4:
            mask = np.stack(
                [cv2.resize(m, observation.shape[1:-1]) for m in mask], axis=0
            )
        else:
            mask = cv2.resize(mask, observation.shape[:-1])
    combination = observation * mask
    return combination


def plot(img):
    plt.imshow(img)
    plt.axis(""off"")
    plt.show()


def plot_data_item(data_item):
    _, axes = plt.subplots(1, 3, figsize=(15, 5))
    axes[0].imshow(data_item[""reference""].permute(1, 2, 0))
    axes[0].set_title(""reference"")
    axes[0].axis(""off"")
    axes[1].imshow(data_item[""positive""].permute(1, 2, 0))
    axes[1].set_title(""positive"")
    axes[1].axis(""off"")
    axes[2].imshow(data_item[""negative""].permute(1, 2, 0))
    axes[2].set_title(""negative"")
    axes[2].axis(""off"")
    plt.show()


def get_binary_mask(image: np.ndarray, gaussian_blur: bool = False) -> np.ndarray:
    mask = cv2.threshold(image.mean(axis=-1), 0.5, 1, cv2.THRESH_BINARY_INV)[1]
    if gaussian_blur:
        # select random odd kernel size
        kernel_size = int(np.random.choice([1, 3, 5, 7, 9]))
        sigma = np.random.uniform(0.1, 3)  # select deviation
        mask = cv2.GaussianBlur(mask, (kernel_size, kernel_size), sigma)
    return np.expand_dims(mask, axis=-1)


def create_random_gradient_image(
    size: tuple, low: float = 0.0, high: float = 1.0
) -> np.ndarray:
    num_channels = size[2]
    # select start and end color gradient location
    locations = (np.random.randint(size[0]), np.random.randint(size[0]))
    while abs(locations[0] - locations[1]) < 10:
        locations = (np.random.randint(size[0]), np.random.randint(size[0]))
    x1, x2 = min(locations), max(locations)
    # for each channel, select an intensity
    channels = []
    for _ in range(num_channels):
        color_a = np.random.uniform(low, high)
        color_b = np.random.uniform(low, high)
        gradient = np.arange(color_a, color_b, ((color_b - color_a) / (x2 - x1)))
        gradient = gradient[: x2 - x1]
        assert len(gradient) == x2 - x1, f""failed: {len(gradient)} vs {x2 - x1}""
        vertical_gradient = np.concatenate(
            [np.ones(x1) * color_a, gradient, np.ones(size[0] - x2) * color_b]
        )
        channels.append(np.stack([vertical_gradient] * size[1], axis=1))
    image = np.stack(channels, axis=-1)
    return image


def get_date_time_tag() -> str:
    return str(datetime.strftime(datetime.now(), format=""%y-%m-%d_%H-%M-%S""))


def draw_trajectory(filename, goal: list, trajectory: list) -> None:
    """"""
    filename: path to jpg file
    goal: list of 3 coordinates of goal
    trajectory: list of lists with drone coordinates
    """"""
    three_d = False
    fig = plt.figure(figsize=(10, 10))
    if three_d:
        ax = fig.add_subplot(111, projection=""3d"")
        ax.scatter(
            trajectory[0][0],
            trajectory[0][1],
            trajectory[0][2],
            s=200,
            color=""green"",
            marker=""v"",
            label=""Start"",
        )
        ax.scatter(
            goal[0], goal[1], goal[2], s=100, color=""red"", marker=""X"", label=""Goal""
        )
        ax.scatter(
            [_[0] for _ in trajectory[1:]],
            [_[1] for _ in trajectory[1:]],
            [_[2] for _ in trajectory[1:]],
            s=20,
            color=""blue"",
            marker=""o"",
            label=""Path"",
        )
        ax.set_xlabel(""x"")
        ax.set_ylabel(""y"")
        ax.set_zlabel(""z"")
        ax.legend()
    else:
        plt.scatter(
            trajectory[0][0],
            trajectory[0][1],
            s=200,
            color=""green"",
            marker=""v"",
            label=""Start"",
        )
        plt.scatter(goal[0], goal[1], s=100, color=""red"", marker=""X"", label=""Goal"")
        plt.scatter(
            [_[0] for _ in trajectory[1:]],
            [_[1] for _ in trajectory[1:]],
            s=20,
            color=""blue"",
            marker=""o"",
            label=""Path"",
        )
        plt.xlabel(""x"")
        plt.ylabel(""y"")
        plt.legend()

    plt.savefig(os.path.join(filename))
    plt.clf()
"
22,2201.02798,"from torch.nn.modules.loss import NLLLoss


class WeightedBinaryCrossEntropyLoss(NLLLoss):
    r""""""The WeightedBinaryCrossEntropyLoss loss. It is useful to train a binary output maps.
    Documentation and implementation based on pytorch BCEWithLogitsLoss.

    If provided, the optional argument :attr:`weight` will balance the 1's
    with respect to the 0's:
    The weight is recommended to be the ratio of 0's in an image.
    Often 90% of the target binary maps is 0 while only 10% is 1.
    Having beta = 0.9 scales the losses on the target-1 pixels with 0.9
    and the losses on target-0 pixels with 0.1.

    The `input` given through a forward call is expected to contain
    probabilities for each pixel. `input` has to be a Tensor of size
    :math:`(minibatch, d_1, d_2, ..., d_K)`
    with :math:`K \geq 1` for the `K`-dimensional case (described later).

    The `target` that this loss expects should be a class index in the range :math:`[1]`
    The loss can be described as:

    .. math::
        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = - \beta * \sum_{(j \in Y^+)} log(Pr(x_j = 1)) -
        (1-\beta) \sum_{(j \in Y^-)} log(Pr(x_j = 0))

    where :math:`x` is the probability input, :math:`y` is the target,
    :math:`\beta` is the balancing weight, and
    :math:`N` is the batch size.

    If :attr:`reduction` is not ``'none'``
    (default ``'mean'``), then

    .. math::
        \ell(x, y) = \begin{cases}
            \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &
            \text{if reduction} = \text{'mean';}\\
            \sum_{n=1}^N l_n,  &
            \text{if reduction} = \text{'sum'.}
        \end{cases}

    Args:
        beta (Tensor, optional): a manual rescaling weight given to each
            class. If given, it has to be a Tensor of size `C`. Otherwise, it is
            treated as if having all ones.
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``:
            no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed.
            Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated,
            and in the meantime,
            specifying either of those two args will override :attr:`reduction`.
            Default: ``'mean'``

    Shape:
        - Input: :math:`(N, C)` where `C = number of classes`, or
          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`
          in the case of `K`-dimensional loss.
        - Target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i]
        \leq C-1`, or
          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of
          K-dimensional loss.
        - Output: scalar.
          If :attr:`reduction` is ``'none'``, then the same size as the target:
          :math:`(N)`, or
          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case
          of K-dimensional loss.
    """"""

    def __init__(self, beta=0.5, reduction=""mean""):
        super(NLLLoss, self).__init__()
        self._beta = beta
        assert 0 <= self._beta <= 1
        self._reduction = reduction

    def forward(self, inputs, targets):
        assert targets.max() <= 1, FloatingPointError(f""got target max {targets.max()}"")
        assert targets.min() >= 0, FloatingPointError(f""got target min {targets.min()}"")
        assert inputs.max() <= 1, FloatingPointError(f""got inputs max {inputs.max()}"")
        assert inputs.min() >= 0, FloatingPointError(f""got inputs min {inputs.min()}"")
        dimension = len(inputs.shape)

        # if an input value == 0, the log value is -inf, where a -1 * -inf == nan.
        epsilon = 1e-3
        unreduced_loss = (
            -self._beta * targets * (inputs + epsilon).log()
            - (1 - self._beta) * (1 - targets) * (1 - inputs + epsilon).log()
        )
        # average over all dimensions except the batch dimension
        unreduced_loss = unreduced_loss.mean(
            dim=tuple([i + 1 for i in range(dimension - 1)])
        )
        if self._reduction == ""none"":
            return unreduced_loss
        elif self._reduction == ""mean"":
            return unreduced_loss.mean()
        elif self._reduction == ""sum"":
            return unreduced_loss.sum()
        else:
            raise NotImplementedError
"
23,2201.02798,"import os

import imageio
import torch
import torchvision
import matplotlib.pyplot as plt
import numpy as np
from torch.nn import Module, MSELoss
from torch.utils.tensorboard.writer import SummaryWriter
from torch.utils.data import Dataset as TorchDataset

from .utils import get_IoU, combine_mask_observation
from .losses import WeightedBinaryCrossEntropyLoss


def evaluate_qualitatively_on_sequences(
    tag: str,
    dataset: TorchDataset,
    model: Module,
    output_directory: str,
    device: str = ""cpu"",
):
    save_dir = os.path.join(output_directory, ""imgs"")
    os.makedirs(save_dir, exist_ok=True)
    for _ in range(len(dataset)):
        data = dataset[_]
        prediction = model(data[""observations""].to(device), intermediate_outputs=False)
        masks = prediction.detach().cpu().squeeze().numpy()
        obs = data[""observations""].detach().cpu().squeeze().permute(0, 2, 3, 1).numpy()
        combined = combine_mask_observation(masks, obs)
        images = list((combined * 255.0).astype(np.uint8))
        imageio.mimsave(os.path.join(save_dir, f""{tag}_{_}.gif""), images)


def evaluate_qualitatively_on_dataset(
    tag: str,
    dataset: TorchDataset,
    model: Module,
    tb_writer: SummaryWriter,
    max_number_of_images: int = 15,
):
    save_dir = os.path.join(tb_writer.get_logdir(), ""imgs"")
    os.makedirs(save_dir, exist_ok=True)
    images = []
    for _ in range(min(len(dataset), max_number_of_images)):
        data = dataset[_]
        prediction = model(data[""observation""].unsqueeze(0), intermediate_outputs=False)
        mask = prediction.detach().cpu().squeeze().numpy()
        obs = data[""observation""].detach().cpu().squeeze().permute(1, 2, 0).numpy()
        combined = combine_mask_observation(mask, obs)
        images.append(torch.from_numpy(combined).permute(2, 0, 1))
        fig, ax = plt.subplots(1, 3, figsize=(9, 3))
        ax[0].imshow(obs)
        ax[0].axis(""off"")
        ax[1].imshow(mask)
        ax[1].axis(""off"")
        ax[2].imshow(combined)
        ax[2].axis(""off"")
        fig.tight_layout()
        plt.savefig(os.path.join(save_dir, f""{tag}_{_}.jpg""))
        plt.close(fig=fig)
    grid = torchvision.utils.make_grid(torch.stack(images), nrow=5)
    tb_writer.add_image(tag, grid, dataformats=""CHW"")


def evaluate_quantitatively_on_dataset(
    tag: str,
    dataset: TorchDataset,
    model: Module,
    tb_writer: SummaryWriter,
    task: str = ""pretrain"",
):
    if task == ""pretrain"":
        return evaluate_mask_prediction(tag, dataset, model, tb_writer)
    else:
        return evaluate_downstream_task(tag, dataset, model, tb_writer, task)


def evaluate_downstream_task(
    tag: str,
    dataset: TorchDataset,
    model: Module,
    tb_writer: SummaryWriter,
    task: str = ""velocities"",
):
    losses = []
    mse_loss = MSELoss()
    for _ in range(min(len(dataset), 100)):
        data = dataset[_]
        prediction = model(data[""observation""].unsqueeze(0))
        loss = mse_loss(prediction, data[task].unsqueeze(0))
        losses.append(loss.detach().cpu())
    tb_writer.add_scalar(
        tag + ""_mse_loss_avg"",
        torch.as_tensor(losses).mean(),
        global_step=model.global_step,
    )
    tb_writer.add_scalar(
        tag + ""_mse_loss_std"",
        torch.as_tensor(losses).std(),
        global_step=model.global_step,
    )
    with open(os.path.join(tb_writer.get_logdir(), ""results.txt""), ""a"") as f:
        f.write(f""{tag}_mse_loss_avg: "" f""{torch.as_tensor(losses).mean():10.3e}\n"",)
        f.write(f""{tag}_mse_loss_std: "" f""{torch.as_tensor(losses).std():10.2e}\n"",)


def evaluate_mask_prediction(
    tag: str, dataset: TorchDataset, model: Module, tb_writer: SummaryWriter,
):
    losses = []
    ious = []
    bce_loss = WeightedBinaryCrossEntropyLoss(beta=0.9)
    for _ in range(min(len(dataset), 100)):
        data = dataset[_]
        prediction = model(data[""observation""].unsqueeze(0), intermediate_outputs=False)
        loss = bce_loss(prediction, data[""mask""])
        losses.append(loss.detach().cpu())
        ious.append(get_IoU(prediction, data[""mask""].unsqueeze(0)).detach().cpu())
    tb_writer.add_scalar(
        tag + ""_bce_loss_avg"",
        torch.as_tensor(losses).mean(),
        global_step=model.global_step,
    )
    tb_writer.add_scalar(
        tag + ""_bce_loss_std"",
        torch.as_tensor(losses).std(),
        global_step=model.global_step,
    )
    tb_writer.add_scalar(
        tag + ""_iou_avg"", torch.as_tensor(ious).mean(), global_step=model.global_step,
    )
    tb_writer.add_scalar(
        tag + ""_iou_std"", torch.as_tensor(ious).std(), global_step=model.global_step,
    )
    with open(os.path.join(tb_writer.get_logdir(), ""results.txt""), ""a"") as f:
        f.write(f""{tag}_bce_loss_avg: "" f""{torch.as_tensor(losses).mean():10.3e}\n"",)
        f.write(f""{tag}_bce_loss_std: "" f""{torch.as_tensor(losses).std():10.2e}\n"",)
        f.write(f""{tag}_ious_avg: "" f""{torch.as_tensor(ious).mean():10.3e}\n"",)
        f.write(f""{tag}_ious_std: "" f""{torch.as_tensor(ious).std():10.2e}\n"",)

"
24,2201.02798,"import os
from typing import Dict

from PIL import Image
import json
import h5py
import numpy as np
import torch
from torch.utils.data import Dataset as TorchDataset
import torchvision.transforms as T


class CleanDataset(TorchDataset):
    def __init__(
        self,
        hdf5_file: str,
        json_file: str,
        fg_augmentation: dict = None,
        input_size: tuple = (3, 200, 200),
        output_size: tuple = (200, 200),
    ):
        self.name = (
            os.path.basename(os.path.dirname(os.path.dirname(hdf5_file)))
            + ""/""
            + os.path.basename(os.path.dirname(hdf5_file))
        )
        self.hdf5_file = h5py.File(hdf5_file, ""r"", libver=""latest"", swmr=True)
        with open(json_file, ""r"") as f:
            self.json_data = json.load(f)

        self.hash_index_tuples = [
            (h, index)
            for h in list(self.json_data.keys())
            for index in range(len(self.json_data[h][""velocities""]))
        ]
        self.input_size = input_size
        self.output_size = output_size
        self.resize = torch.nn.Sequential(T.Resize(self.input_size[1:]))
        if fg_augmentation is not None:
            augmentation_transforms = []
            if fg_augmentation[""fg_color""] != {}:
                augmentation_transforms.append(
                    T.ColorJitter(
                        brightness=fg_augmentation[""fg_color""][""brightness""],
                        hue=fg_augmentation[""fg_color""][""hue""],
                        saturation=fg_augmentation[""fg_color""][""saturation""],
                        contrast=fg_augmentation[""fg_color""][""hue""],
                    )
                )
            if fg_augmentation[""fg_blur""] != {}:
                augmentation_transforms.append(
                    T.GaussianBlur(
                        kernel_size=fg_augmentation[""fg_blur""][""kernel""],
                        sigma=(
                            fg_augmentation[""fg_blur""][""min_sigma""],
                            fg_augmentation[""fg_blur""][""max_sigma""],
                        ),
                    ),
                )
            self.augment = torch.nn.Sequential(*augmentation_transforms)
        else:
            self.augment = None

    def __len__(self) -> int:
        return len(self.hash_index_tuples)

    def load_from_hdf5(self, image) -> torch.Tensor:
        image = torch.as_tensor(np.asarray(image))
        if len(image.shape) == 3:
            image = image.permute(2, 0, 1)
        else:
            image.unsqueeze_(0)
        image = self.resize(image)
        return (
            self.augment(image)
            if self.augment is not None and image.shape[0] == 3
            else image
        )

    def load_from_file(self, img_path: str) -> torch.Tensor:
        image = np.asarray(Image.open(img_path))
        if len(image.shape) == 2 or image.shape[-1] == 1:
            image = np.stack([image.squeeze()] * 3, axis=-1)
        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        image = self.resize(image)
        return self.augment(image) if self.augment is not None else image

    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:
        hsh, sample_index = self.hash_index_tuples[index]
        observation = self.load_from_hdf5(
            self.hdf5_file[hsh][""observation""][sample_index]
        )
        mask = self.load_from_hdf5(self.hdf5_file[hsh][""mask""][sample_index]).squeeze()

        relative_target_location = self.json_data[hsh][""relative_target_location""][
            sample_index
        ]
        relative_target_location = torch.as_tensor(relative_target_location).float()
        velocities = self.json_data[hsh][""velocities""][sample_index]
        velocities = torch.as_tensor(velocities).float()

        return {
            ""observation"": observation,
            ""reference"": observation,
            ""mask"": mask,
            ""velocities"": velocities,
            ""waypoints"": relative_target_location,
        }


class AugmentedTripletDataset(CleanDataset):
    def __init__(
        self,
        hdf5_file: str,
        json_file: str,
        background_images_directory: str,
        combined_blur: dict = {},
        fg_augmentation: dict = {},
        input_size: tuple = (3, 200, 200),
        output_size: tuple = (200, 200),
    ):
        super().__init__(hdf5_file, json_file, fg_augmentation, input_size, output_size)
        self._background_images = (
            [
                os.path.join(background_images_directory, sub_directory, image)
                for sub_directory in os.listdir(background_images_directory)
                if os.path.isdir(
                    os.path.join(background_images_directory, sub_directory)
                )
                for image in os.listdir(
                    os.path.join(background_images_directory, sub_directory)
                )
                if image.endswith("".jpg"")
            ]
            if background_images_directory is not None
            else []
        )
        self._blur = (
            torch.nn.Sequential(
                T.GaussianBlur(
                    kernel_size=combined_blur[""kernel""],
                    sigma=(combined_blur[""min_sigma""], combined_blur[""max_sigma""]),
                )
            )
            if combined_blur != {}
            else None
        )

    def combine_fg_bg(
        self, mask: torch.Tensor, foreground: torch.Tensor, background: torch.Tensor
    ) -> torch.Tensor:
        mask = self.resize(mask.unsqueeze(0))
        mask = torch.stack([mask.squeeze()] * 3, axis=0)
        combination = mask * foreground + (1 - mask) * background
        return self._blur(combination) if self._blur is not None else combination

    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:
        result = super().__getitem__(index)

        foreground = result[""observation""]
        background_img = self.load_from_file(np.random.choice(self._background_images))
        result[""reference""] = self.combine_fg_bg(
            result[""mask""], foreground, background_img
        )
        result[""observation""] = result[""reference""]

        # add different background for positive sample
        new_background_img = self.load_from_file(
            np.random.choice(self._background_images)
        )
        result[""positive""] = self.combine_fg_bg(
            result[""mask""], foreground, new_background_img
        )

        # get different line with different background for negative sample
        random_other_index = index
        # make sure new index is at least 10 frames away
        while abs(random_other_index - index) < 10:
            random_other_index = np.random.randint(0, len(self))

        second_hsh, second_sample_index = self.hash_index_tuples[random_other_index]
        second_foreground = self.load_from_hdf5(
            self.hdf5_file[second_hsh][""observation""][second_sample_index]
        )
        second_mask = self.load_from_hdf5(
            self.hdf5_file[second_hsh][""mask""][second_sample_index]
        )
        result[""negative""] = self.combine_fg_bg(
            second_mask, second_foreground, background_img
        )
        return result


class ImagesDataset(TorchDataset):
    def __init__(
        self,
        dir_name: str,
        target: str,
        input_size: tuple = (3, 200, 200),
        output_size: tuple = (100, 100),
    ) -> None:
        super().__init__()
        self.name = os.path.basename(dir_name)
        self.images = [
            os.path.join(dir_name, f)
            for f in os.listdir(dir_name)
            if (f.endswith("".png"") or f.endswith("".jpg"")) and target in f
        ]
        self.input_size = input_size
        self.output_size = output_size

    def __len__(self):
        return len(self.images)

    def __getitem__(self, index):
        img_file = self.images[index]
        image = Image.open(img_file)
        image = np.array(image.resize(self.input_size[1:]), dtype=np.float32)
        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        return {""observation"": image}


class LabelledImagesDataset(ImagesDataset):
    def __init__(
        self,
        img_dir_name: str,
        target: str,
        mask_dir_name: str,
        input_size: tuple = (3, 200, 200),
        output_size: tuple = (200, 200),
    ) -> None:
        super().__init__(
            dir_name=img_dir_name,
            target=target,
            input_size=input_size,
            output_size=output_size,
        )
        self.mask_dir_name = mask_dir_name

    def __getitem__(self, index):
        data = super().__getitem__(index)
        # get mask
        mask_file = os.path.join(
            self.mask_dir_name,
            os.path.basename(self.images[index]).replace(""jpg"", ""npy""),
        )
        mask = np.load(mask_file)
        mask = torch.from_numpy(mask // 255).float()
        data[""mask""] = mask
        return data


class ImageSequenceDataset(TorchDataset):
    def __init__(
        self,
        hdf5_file: str,
        input_size: tuple = (3, 200, 200),
        output_size: tuple = (100, 100),
    ) -> None:
        super().__init__()
        self.hdf5_file = h5py.File(hdf5_file, ""r"", libver=""latest"", swmr=True)
        self.image_sequences = list(self.hdf5_file.keys())
        self.input_size = input_size
        self.output_size = output_size
        self.transforms = torch.nn.Sequential(T.Resize(self.input_size[1:]))

    def __len__(self):
        return len(self.image_sequences)

    def __getitem__(self, index):
        seq_key = self.image_sequences[index]
        images = np.asarray(self.hdf5_file[seq_key][""observation""])
        images = torch.from_numpy(images).permute(0, 3, 1, 2).float()
        images = self.transforms(images)
        return {""observations"": images}
"
25,2201.02798,"import os

import torch
from torch.nn import TripletMarginLoss, MSELoss
import numpy as np
from tqdm import tqdm

from .utils import get_date_time_tag, get_IoU
from .losses import WeightedBinaryCrossEntropyLoss


def train_downstream_task(
    model,
    train_dataloader,
    val_dataloader,
    checkpoint_file,
    tb_writer,
    task: str = ""velocities"",
    num_epochs: int = 40,
    learning_rate: float = 0.001,
):
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
    model.to(device)
    mse_loss = MSELoss()
    lowest_validation_loss = 100
    optimizer = torch.optim.Adam(
        model.parameters(), lr=learning_rate, weight_decay=0.0001
    )
    if os.path.isfile(checkpoint_file):
        ckpt = torch.load(checkpoint_file, map_location=device)
        model.load_state_dict(ckpt[""state_dict""])
        model.global_step = ckpt[""global_step""]
        optimizer.load_state_dict(ckpt[""optimizer_state_dict""])
        lowest_validation_loss = ckpt[""lowest_val_loss""]
        print(
            f""loaded checkpoint {checkpoint_file}. ""
            f""Starting from {model.global_step}""
        )

    while model.global_step < num_epochs:
        losses = {""train"": [], ""val"": []}
        model.train()
        for _, data in enumerate(tqdm(train_dataloader)):
            optimizer.zero_grad()
            predictions = model(data[""reference""].to(device))
            loss = mse_loss(predictions, data[task].to(device))
            loss.backward()
            optimizer.step()
            losses[""train""].append(loss.cpu().detach().item())
        model.eval()
        for _, data in enumerate(val_dataloader):
            predictions = model(data[""reference""].to(device))
            loss = mse_loss(predictions, data[task].to(device))
            losses[""val""].append(loss.cpu().detach().item())

        print(
            f""{get_date_time_tag()}: epoch {model.global_step} - ""
            f""train {np.mean(losses['train']): 0.3f} [{np.std(losses['train']): 0.2f}]""
            f"" - val {np.mean(losses['val']): 0.3f} [{np.std(losses['val']): 0.2f}]""
        )
        tb_writer.add_scalar(
            ""train/mse_loss"", np.mean(losses[""train""]), global_step=model.global_step,
        )
        tb_writer.add_scalar(
            ""val/mse_loss"", np.mean(losses[""val""]), global_step=model.global_step,
        )
        model.global_step += 1
        if lowest_validation_loss > np.mean(losses[""val""]):
            lowest_validation_loss = np.mean(losses[""val""])
            ckpt = {
                ""state_dict"": model.state_dict(),
                ""global_step"": model.global_step,
                ""optimizer_state_dict"": optimizer.state_dict(),
                ""lowest_val_loss"": lowest_validation_loss,
            }
            torch.save(
                ckpt, checkpoint_file,
            )
            with open(os.path.join(tb_writer.get_logdir(), ""results.txt""), ""w"") as f:
                f.write(
                    f""validation_mse_loss_avg: "" f""{np.mean(losses['val']):10.3e}\n"",
                )
                f.write(
                    f""validation_mse_loss_std: "" f""{np.std(losses['val']):10.2e}\n"",
                )
            print(f""Saved model in {checkpoint_file}."")
    model.to(torch.device(""cpu""))


def train_autoencoder(
    autoencoder,
    train_dataloader,
    val_dataloader,
    checkpoint_file,
    tb_writer,
    triplet_loss_weight: float = 0.0,
    num_epochs: int = 40,
    deep_supervision: bool = False,
    learning_rate: float = 0.01,
):
    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
    autoencoder.to(device)
    bce_loss = WeightedBinaryCrossEntropyLoss(beta=0.9).to(device)
    if triplet_loss_weight != 0.0:
        trplt_loss = TripletMarginLoss(swap=True).to(device)
    lowest_validation_loss = 100
    optimizer = torch.optim.Adam(
        autoencoder.parameters(), lr=learning_rate, weight_decay=0.0001
    )
    if os.path.isfile(checkpoint_file):
        ckpt = torch.load(checkpoint_file, map_location=device)
        autoencoder.load_state_dict(ckpt[""state_dict""])
        autoencoder.global_step = ckpt[""global_step""]
        optimizer.load_state_dict(ckpt[""optimizer_state_dict""])
        lowest_validation_loss = ckpt[""lowest_val_loss""]
        print(
            f""loaded checkpoint {checkpoint_file}. ""
            f""Starting from {autoencoder.global_step}""
        )

    while autoencoder.global_step < num_epochs:
        losses = {""train"": [], ""val"": []}
        ious = {""train"": [], ""val"": []}
        autoencoder.train()
        for batch_idx, data in enumerate(tqdm(train_dataloader)):
            optimizer.zero_grad()
            predictions = autoencoder(
                data[""reference""].to(device), intermediate_outputs=deep_supervision
            )
            if not deep_supervision:
                loss = bce_loss(predictions, data[""mask""].to(device))
            else:
                loss = 0
                for output in predictions:
                    loss += (
                        1 / len(predictions) * bce_loss(output, data[""mask""].to(device))
                    )
            if triplet_loss_weight != 0:
                anchor = autoencoder.project(data[""reference""].to(device))
                positive = autoencoder.project(data[""positive""].to(device))
                negative = autoencoder.project(data[""negative""].to(device))
                triplet = triplet_loss_weight * trplt_loss(anchor, positive, negative)
                # print(
                # f""loss: {loss} vs trplt: {triplet} (weight: {triplet_loss_weight})""
                # )
                loss += triplet
            loss.backward()
            optimizer.step()
            losses[""train""].append(loss.cpu().detach().item())
            ious[""train""].append(
                get_IoU(
                    autoencoder(
                        data[""reference""].to(device), intermediate_outputs=False
                    ),
                    data[""mask""].to(device),
                )
                .detach()
                .cpu()
                .item()
            )
        autoencoder.eval()
        for batch_idx, data in enumerate(val_dataloader):
            predictions = autoencoder(
                data[""reference""].to(device), intermediate_outputs=False
            )
            loss = bce_loss(predictions, data[""mask""].to(device))
            losses[""val""].append(loss.cpu().detach().item())
            ious[""val""].append(
                get_IoU(predictions, data[""mask""].to(device)).detach().cpu().item()
            )

        print(
            f""{get_date_time_tag()}: epoch {autoencoder.global_step} - ""
            f""train {np.mean(losses['train']): 0.3f} [{np.std(losses['train']): 0.2f}]""
            f"" - val {np.mean(losses['val']): 0.3f} [{np.std(losses['val']): 0.2f}]""
        )
        tb_writer.add_scalar(
            ""train/bce_loss/autoencoder""
            + ("""" if not triplet_loss_weight else ""_trplt""),
            np.mean(losses[""train""]),
            global_step=autoencoder.global_step,
        )
        tb_writer.add_scalar(
            ""val/bce_loss/autoencoder"" + ("""" if not triplet_loss_weight else ""_trplt""),
            np.mean(losses[""val""]),
            global_step=autoencoder.global_step,
        )
        tb_writer.add_scalar(
            ""train/iou/autoencoder"",
            np.mean(ious[""train""]),
            global_step=autoencoder.global_step,
        )
        tb_writer.add_scalar(
            ""val/iou/autoencoder"",
            np.mean(ious[""val""]),
            global_step=autoencoder.global_step,
        )
        autoencoder.global_step += 1
        if lowest_validation_loss > np.mean(losses[""val""]):
            lowest_validation_loss = np.mean(losses[""val""])
            ckpt = {
                ""state_dict"": autoencoder.state_dict(),
                ""global_step"": autoencoder.global_step,
                ""optimizer_state_dict"": optimizer.state_dict(),
                ""lowest_val_loss"": lowest_validation_loss,
            }
            torch.save(
                ckpt, checkpoint_file,
            )
            with open(os.path.join(tb_writer.get_logdir(), ""results.txt""), ""w"") as f:
                f.write(
                    f""validation_bce_loss_avg: "" f""{np.mean(losses['val']):10.3e}\n"",
                )
                f.write(
                    f""validation_bce_loss_std: "" f""{np.std(losses['val']):10.2e}\n"",
                )
                f.write(f""validation_iou_avg: "" f""{np.mean(ious['val']):10.3e}\n"",)
                f.write(f""validation_iou_std: "" f""{np.std(ious['val']):10.2e}\n"",)
            print(f""Saved model in {checkpoint_file}."")
    autoencoder.to(torch.device(""cpu""))
"
26,2201.02798,"from typing import Tuple
from collections import OrderedDict

import torch
from torch import nn


class ResidualBlock(nn.Module):
    def __init__(
        self,
        input_channels,
        output_channels,
        batch_norm: bool = True,
        activation: torch.nn.ReLU = torch.nn.ReLU(),
        strides: Tuple[int, int] = (1, 1),
        padding: Tuple[int, int] = (0, 0),
        kernel_sizes: Tuple[int, int] = (3, 3),
    ):
        """"""
        Create a residual block
        :param input_channels: number of input channels at input
        :param output_channels: number of input channels at input
        :param batch_norm: bool specifying to use batch norm 2d (True)
        :param activation: specify torch nn module activation (ReLU)
        :param pool: specify pooling layer applied as first layer
        :param strides: tuple specifying the stride and so the down sampling
        """"""
        super().__init__()
        self._down_sample = torch.nn.Conv2d(
            in_channels=input_channels,
            out_channels=output_channels,
            kernel_size=1,
            stride=strides[0],
            bias=False,
        )
        self._final_activation = activation
        elements = []
        elements.append(
            (
                ""conv_0"",
                torch.nn.Conv2d(
                    in_channels=input_channels,
                    out_channels=output_channels,
                    kernel_size=kernel_sizes[0],
                    padding=padding[0],
                    stride=strides[0],
                    bias=False,
                ),
            )
        )
        if batch_norm:
            elements.append((""bn_0"", torch.nn.BatchNorm2d(output_channels)))
        elements.append((""act_0"", activation))
        elements.append(
            (
                ""conv_1"",
                torch.nn.Conv2d(
                    in_channels=output_channels,
                    out_channels=output_channels,
                    kernel_size=kernel_sizes[1],
                    padding=padding[1],
                    stride=strides[1],
                    bias=False,
                ),
            )
        )
        if batch_norm:
            elements.append((""bn_1"", torch.nn.BatchNorm2d(output_channels)))
        elements.append((""act_1"", activation))
        self.residual_net = torch.nn.Sequential(OrderedDict(elements))

    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        x = self.residual_net(inputs)
        x += self._down_sample(inputs)
        return self._final_activation(x)


class DeepSupervisionNet(nn.Module):
    def __init__(self, batch_norm: bool = False, no_deep_supervision: bool = False):
        super().__init__()
        self.no_deep_supervision = no_deep_supervision
        self.global_step = 0
        self.input_size = (3, 200, 200)
        self.output_size = (200, 200)
        self.sigmoid = nn.Sigmoid()
        self.conv0 = torch.nn.Conv2d(
            in_channels=3, out_channels=32, kernel_size=3, padding=1, stride=1
        )
        self.residual_1 = ResidualBlock(
            input_channels=32,
            output_channels=32,
            batch_norm=batch_norm,
            activation=torch.nn.ReLU(),
            strides=(1, 1),
            padding=(1, 1),
            kernel_sizes=(3, 3),
        )
        self.side_logit_1 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)
        self.weight_1 = nn.Parameter(torch.as_tensor(1 / 4), requires_grad=True)

        self.residual_2 = ResidualBlock(
            input_channels=32,
            output_channels=32,
            batch_norm=batch_norm,
            activation=torch.nn.ReLU(),
            strides=(2, 1),
            padding=(1, 1),
            kernel_sizes=(3, 3),
        )
        self.side_logit_2 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)
        self.weight_2 = nn.Parameter(torch.as_tensor(1 / 4), requires_grad=True)
        self.upsample_2 = nn.Upsample(scale_factor=2, mode=""nearest"")

        self.residual_3 = ResidualBlock(
            input_channels=32,
            output_channels=32,
            batch_norm=batch_norm,
            activation=torch.nn.ReLU(),
            strides=(2, 1),
            padding=(1, 1),
            kernel_sizes=(3, 3),
        )
        self.side_logit_3 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)
        self.weight_3 = nn.Parameter(torch.as_tensor(1 / 4), requires_grad=True)
        self.upsample_3 = nn.Upsample(scale_factor=4, mode=""nearest"")

        self.residual_4 = ResidualBlock(
            input_channels=32,
            output_channels=32,
            batch_norm=batch_norm,
            activation=torch.nn.ReLU(),
            strides=(2, 1),
            padding=(1, 1),
            kernel_sizes=(3, 3),
        )
        self.side_logit_4 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)
        self.weight_4 = nn.Parameter(torch.as_tensor(1 / 4), requires_grad=True)
        self.upsample_4 = nn.Upsample(scale_factor=8, mode=""nearest"")

        self.projection_1 = ResidualBlock(
            input_channels=32,
            output_channels=128,
            batch_norm=batch_norm,
            activation=torch.nn.ReLU(),
            strides=(2, 1),
            padding=(1, 1),
            kernel_sizes=(3, 3),
        )
        self.projection_2 = ResidualBlock(
            input_channels=128,
            output_channels=1024,
            batch_norm=batch_norm,
            activation=torch.nn.ReLU(),
            strides=(2, 1),
            padding=(1, 1),
            kernel_sizes=(3, 3),
        )
        self.avg_pool = nn.MaxPool2d(kernel_size=5, stride=5)

    def forward_with_intermediate_outputs(self, inputs) -> dict:
        results = {""x1"": self.residual_1(self.conv0(inputs))}
        results[""out1""] = self.side_logit_1(results[""x1""])
        results[""prob1""] = self.sigmoid(results[""out1""]).squeeze(dim=1)

        results[""x2""] = self.residual_2(results[""x1""])
        results[""out2""] = self.side_logit_2(results[""x2""])
        results[""prob2""] = self.upsample_2(self.sigmoid(results[""out2""])).squeeze(dim=1)

        results[""x3""] = self.residual_3(results[""x2""])
        results[""out3""] = self.side_logit_3(results[""x3""])
        results[""prob3""] = self.upsample_3(self.sigmoid(results[""out3""])).squeeze(dim=1)

        results[""x4""] = self.residual_4(results[""x3""])
        results[""out4""] = self.side_logit_4(results[""x4""])
        results[""prob4""] = self.upsample_4(self.sigmoid(results[""out4""])).squeeze(dim=1)

        final_logit = (
            self.weight_1 * results[""out1""]
            + self.weight_2 * self.upsample_2(results[""out2""])
            + self.weight_3 * self.upsample_3(results[""out3""])
            + self.weight_4 * self.upsample_4(results[""out4""])
        )
        results[""final_prob""] = self.sigmoid(final_logit).squeeze(dim=1)
        return results

    def forward(self, inputs, intermediate_outputs: bool = False) -> torch.Tensor:
        results = self.forward_with_intermediate_outputs(inputs)
        if intermediate_outputs:
            return [
                results[""prob1""],
                results[""prob2""],
                results[""prob3""],
                results[""prob4""],
                results[""final_prob""],
            ]
        else:
            return results[""prob4"" if self.no_deep_supervision else ""final_prob""]

    def project(self, inputs, frozen: bool = False) -> torch.Tensor:
        if frozen:
            with torch.no_grad():
                results = self.forward_with_intermediate_outputs(inputs)
        else:
            results = self.forward_with_intermediate_outputs(inputs)
        projection = self.projection_1(results[""x4""])
        projection = self.projection_2(projection)
        results[""projection""] = self.avg_pool(projection)

        return results[""projection""]


class DownstreamNet(nn.Module):
    def __init__(
        self,
        output_size: tuple = (6,),
        encoder_ckpt_dir: str = None,
        end_to_end: bool = False,
        batch_norm: bool = False,
        no_deep_supervision: bool = False,
    ):
        super().__init__()
        self.global_step = 0
        self.input_size = (3, 200, 200)
        self.output_size = output_size
        self.encoder = DeepSupervisionNet(
            batch_norm=batch_norm, no_deep_supervision=no_deep_supervision
        )
        self.end_to_end = end_to_end
        self.decoder = nn.Sequential(
            OrderedDict(
                [
                    (""layer_1"", nn.Linear(1024, 512)),
                    (""relu_1"", nn.ReLU()),
                    (""layer_2"", nn.Linear(512, self.output_size[0])),
                ]
            )
        )
        if encoder_ckpt_dir is not None:
            ckpt = torch.load(
                encoder_ckpt_dir + ""/checkpoint_model.ckpt"",
                map_location=torch.device(""cpu""),
            )
            self.encoder.load_state_dict(ckpt[""state_dict""])
            print(f""Loaded encoder from {encoder_ckpt_dir}."")

    def forward(self, inputs) -> torch.Tensor:
        features = (
            self.encoder.project(inputs, frozen=not self.end_to_end)
            .squeeze(-1)
            .squeeze(-1)
        )
        return self.decoder(features)
"
27,2201.02798,"from .data import *  # noqa
from .model import *  # noqa
from .utils import *  # noqa
from .train import *  # noqa
from .evaluate import *  # noqa
"
28,2201.02772,"import torch
import numpy as np
import torch.nn.functional as F
from torch import nn


class dual_softmax_loss(nn.Module):
    def __init__(self, ):
        super(dual_softmax_loss, self).__init__()

    def forward(self, sim_matrix, temp=1000):
        sim_matrix = sim_matrix * F.softmax(sim_matrix / temp, dim=0) * len(sim_matrix)  # With an appropriate temperature parameter, the model achieves higher performance
        logpt = F.log_softmax(sim_matrix, dim=-1)  # row softmax and column softmax
        logpt = torch.diag(logpt)
        loss = -logpt
        return loss


def log_sum_exp(x):
    '''Utility function for computing log-sun-exp while determining
    This will be used to determine unaveraged confidence loss across all examples in a batch.
    '''
    x_max = x.data.max()
    return torch.log(torch.sum(torch.exp(x-x_max), -1, keepdim=True)) + x_max


def l2norm(X, dim, eps=1e-8):
    """"""L2-normalize columns of X
    """"""
    norm = torch.pow(X, 2).sum(dim=dim, keepdim=True).sqrt() + eps
    X = torch.div(X, norm)
    return X


def calcdist(img, txt):
    '''
    Input img = (batch,dim), txt = (batch,dim)
    Output Euclid Distance Matrix = Tensor(batch,batch), and dist[i,j] = d(img_i,txt_j)
    '''
    dist = img.unsqueeze(1) - txt.unsqueeze(0)
    dist = torch.sum(torch.pow(dist, 2), dim=2)
    return torch.sqrt(dist)


def calcmatch(label):
    '''
    Input label = (batch,)
    Output Match Matrix =Tensor(batch,batch) and match[i,j] == 1 iff. label[i]==label[j]
    '''
    match = label.unsqueeze(1) - label.unsqueeze(0)
    match[match != 0] = 1
    return 1 - match


def calcneg(dist, label, anchor, positive):
    '''
    Input dist = (batch,batch), label = (batch,), anchor = index, positive = index
    Output chosen negative sample index
    '''

    standard = dist[anchor, positive]  # positive distance
    dist = dist[anchor] - standard  # distance of other samples
    if max(dist[label != label[anchor]]) >= 0:  # there exists valid negative
        dist[dist < 0] = max(dist) + 2  # delete negative samples below standard
        dist[label == label[anchor]] = max(dist) + 2  # delete positive samples
        return int(torch.argmin(dist).cpu())  # return the closest negative sample
    else:  # choose argmax
        dist[label == label[anchor]] = min(dist) - 2  # delete positive samples
        return int(torch.argmax(dist).cpu())


def calcneg_dot(img, txt, match, anchor, positive):
    '''
    Input img = (batch,dim), txt = (batch,dim), match = (batch,batch), anchor = index, positive = index
    Output chosen negative sample index
    '''
    distdot = torch.sum(torch.mul(img.unsqueeze(1), txt.unsqueeze(0)), 2)
    distdot[match == 1] = -66666
    return int(torch.argmax(distdot[anchor]).cpu())


def Triplet(img, txt, label):
    '''
    Input img = (batch,dim), txt = (batch,dim), label = (batch,)
    Output dist = (batch,batch),match = (batch,batch), triplets = List with shape(pairs,3)
    '''
    batch = img.shape[0]
    dist = calcdist(img, txt)
    match = calcmatch(label)
    match_n = match.cpu().numpy()
    positive_list = np.argwhere(match_n == 1).tolist()   # the index list of all positive samples
    for positive in positive_list:
        negative = calcneg(dist, label, positive[0], positive[1])  # calculate negatives
        # negative = calcneg_dot(img, txt, match, anchor, positive)  # calculate negative with dot  效果很差
        triplet_list.append([positive[0], int(positive[1].cuda()), negative])

    return dist, match, triplet_list


def Positive(img, txt, label):
    '''
    Input img = (batch,dim), txt = (batch,dim), label = (batch,)
    Output dist = (batch,batch),match = (batch,batch), positives = List with shape(pairs,2)
    Remark: return (anchor,positive) without finding triplets
    '''
    batch = img.shape[0]
    dist = calcdist(img, txt)
    match = calcmatch(label)
    sample_list = torch.tensor([x for x in range(batch)]).int().cuda()
    positive_list = [[i, int(j.cpu())] for i in range(batch) for j in sample_list[label == label[i]]]
    return dist, match, positive_list


def Modality_invariant_Loss(img, txt, label, margin=0.2):
    '''
    Input img = (batch,dim), txt = (batch,dim), label = (batch,), margin = parameter
    Calculate invariant loss between images and texts belonging to the same class
    '''
    batch = img.shape[0]
    dist = calcdist(img, txt)
    dist = torch.pow(dist, 2)
    match = calcmatch(label)  # similar is 1, dissimilar is 0
    pos = torch.mul(dist, match)
    loss = torch.sum(pos)

    return loss / batch


def Contrastive_Loss(img, txt, label, margin=0.2):
    '''
    Input img = (batch,dim), txt = (batch,dim), label = (batch,), margin = parameter
    Calculate triplet loss
    '''
    batch = img.shape[0]
    dist = calcdist(img, txt)
    dist = torch.pow(dist, 2)
    match = calcmatch(label)  # similar is 1, dissimilar is 0
    pos = torch.mul(dist, match)
    neg = margin - torch.mul(dist, 1-match)
    neg = torch.clamp(neg, 0)
    loss = torch.sum(pos) + torch.sum(neg)

    return loss / batch


def Triplet_Loss(img, txt, label, margin=0.2, semi_hard=True):
    '''
    Input img = (batch,dim), txt = (batch,dim), label = (batch,), margin = parameter
    Calculate triplet loss
    '''
    loss = 0
    dist = calcdist(img, txt)
    dist = torch.pow(dist, 2)
    match = calcmatch(label)
    match_n = match.cpu().numpy()
    positive = np.argwhere(match_n == 1).tolist()  # the index list of all positive samples
    for x in positive:
        # # Semi-Hard Negative Mining
        if semi_hard:
            neg_index = torch.where(match[x[0]] == 0)  # the index list of all negative samples (shared by image and text)
            neg_dis = dist[x[0]][neg_index]
            tmp = dist[x[0], x[1]] - neg_dis + margin
            tmp = torch.clamp(tmp, 0)
            loss = loss + torch.sum(tmp, dim=-1)
        else:
            # Hard Negative Mining
            negative = calcneg(dist, label, x[0], x[1])  # calculate hard negative
            tmp = dist[x[0], x[1]] - dist[x[0], negative] + margin
            if tmp > 0:
                loss = loss + tmp

    return loss / len(positive)


def Lifted_Loss(img, txt, label, margin=1):  # the margin is set to be 1 as the original paper
    '''
    Input img = (batch,dim), txt = (batch,dim), label = (batch,), margin = parameter
    Calculate lifted structured embedding loss
    '''
    # dist, match, positive = Positive(img, txt, label)
    dist = calcdist(img, txt)
    match = calcmatch(label)
    match_n = match.cpu().numpy()
    positive = np.argwhere(match_n == 1).tolist()  # the index list of all positive samples
    loss = 0
    for x in positive:
        neg_index = torch.where(match[x[0]] == 0)   # the index list of all negative samples (shared by image and text)
        neg_dis_anchor = dist[x[0]][neg_index]
        neg_dis_postive = dist[x[1]][neg_index]
        tmp = dist[x[0], x[1]] + log_sum_exp(margin - neg_dis_postive) + log_sum_exp(margin - neg_dis_anchor)
        loss = loss + tmp

    return loss / (2 * len(positive))


def Npairs(img, txt, label, margin=0.2, alpha=0.1):
    '''
    Input img = (batch,dim), txt = (batch,dim), label = (batch,), margin = parameter, alpha = parameter
    Calculate N-pairs loss
    '''
    # dist, match, positive = Positive(img, txt, label)
    batch = img.shape[0]
    distdot_it = torch.exp(F.linear(img, txt))
    distdot_ti = torch.t(distdot)
    match = calcmatch(label)
    match_n = match.cpu().numpy()
    positive = np.argwhere(match_n == 1).tolist()   # the index list of all positive samples
    loss = 0
    for x in positive:
        neg_index = torch.where(match[x[0]] == 0)  # the index list of all negative samples
        tmp_i2t = distdot_it[x[0], x[1]] - log_sum_exp(distdot_it[x[0]][neg_index])
        tmp_t2i = distdot_ti[x[0], x[1]] - log_sum_exp(distdot_ti[x[0]][neg_index])
        loss = loss + (tmp_i2t + tmp_t2i)/2
    loss = -loss / len(positive)
    for x in range(batch):
        loss = loss + alpha * (torch.norm(img[x]) + torch.norm(txt[x])) / batch

    return loss


def Supervised_Contrastive_Loss(img, txt, label):
    '''
    Input img = (batch,dim), txt = (batch,dim), label = (batch,), margin = parameter
    An unofficial implementation of supervised contrastive loss for multimodal learning
    '''
    loss = 0
    batch = img.shape[0]
    dist = calcdist(img, txt)
    dist = torch.pow(dist, 2)
    dist = dist / (torch.sum(dist) / (batch * batch)) # scale the metric
    match = calcmatch(label)  # 相似为1，不相似为0
    match_n = match.cpu().numpy()
    positive = np.argwhere(match_n == 1).tolist()  # the index list of all positive samples
    loss = 0
    for x in positive:
        neg_index = torch.where(match[x[0]] == 0)  # the index list of all negative samples
        pos_sim = -dist[x[0], x[1]]
        neg_sims = -dist[x[0]][neg_index]
        tmp = pos_sim - log_sum_exp(neg_sims)

        loss = loss + tmp

    loss = -loss / len(positive)

    return loss


def regularization(features, centers, labels):
    # features = l2norm(features, dim=-1)
    distance = (features - centers[labels])
    distance = torch.sum(torch.pow(distance, 2), 1, keepdim=True)
    distance = (torch.sum(distance, 0, keepdim=True)) / features.shape[0]

    return distance


def PAN(features, centers, labels, add_regularization=False):
    """"""The prototype contrastive loss and regularization loss in
    PAN(https://dl.acm.org/doi/abs/10.1145/3404835.3462867)""""""
    batch = features.shape[0]
    features_square = torch.sum(torch.pow(features, 2), 1, keepdim=True)  # 在第一个维度上平方
    centers_square = torch.sum(torch.pow(torch.t(centers), 2), 0, keepdim=True)
    features_into_centers = 2 * torch.matmul(features, torch.t(centers))
    dist = -(features_square + centers_square - features_into_centers)
    output = F.log_softmax(dist, dim=1)
    dce_loss = F.nll_loss(output, labels)

    if add_regularization:
        reg_loss = regularization(features, centers, labels)
        loss = dce_loss + reg_loss

    loss = dce_loss

    return loss / batch


def Label_Regression_Loss(view1_predict, view2_predict, label_onehot):
    loss = ((view1_predict - label_onehot.float()) ** 2).sum(1).sqrt().mean() + (
                (view2_predict - label_onehot.float()) ** 2).sum(1).sqrt().mean()

    return loss


def Proxy_NCA(features, label, proxies, mrg=0.1, alpha=1): 
    """"""
    Input:
    :param feature: [2*batch, dim]  concat image and text features
    :param label: [2*batch]
    :param proxies: [feature_dim, n_classes]
    :return: Proxy Anchor loss
    
    P = torch.t(proxies)  # [feature_dim, n_classes]-->[n_classes, feature_dim] 
    n_classes = P.shape[0]
    # similar to Proxc-NCA and Normlized Softmax
    cos = F.linear(features, P)  # Calcluate cosine similarity [batch, n_classes]

    # Proxy-NCA loss (similar to Normlized Softmax and PAN，while the denominator does not contain positive prototype)
    loss = 0
    for x in range(features.shape[0]):
        pos = torch.exp(cos[x, label[x]])
        neg = torch.exp(cos[x]).sum(dim=-1)-pos
        loss = loss + torch.log(pos / neg)
    loss = -loss / features.shape[0]
    
    return loss


def Proxy_Anchor(features, label, proxies, mrg=0.1, alpha=1): 
    """"""
    Input:
    :param feature: [2*batch, dim]  concat image and text features
    :param label: [2*batch]
    :param proxies: [feature_dim, n_classes]
    :return: Proxy Anchor loss
    """"""
    P = torch.t(proxies)  # [feature_dim, n_classes]-->[n_classes, feature_dim] 
    n_classes = P.shape[0]
    # similar to Proxc-NCA and Normlized Softmax
    cos = F.linear(features, P)  # Calcluate cosine similarity [batch, n_classes]

    P_one_hot = label # [batch, n_classes]
    N_one_hot = 1 - P_one_hot

    pos_exp = torch.exp(-alpha * (cos - mrg)) # [batch, n_class]
    neg_exp = torch.exp(alpha * (cos + mrg))    # 出现了e+30导致nan

    with_pos_proxies = torch.nonzero(P_one_hot.sum(dim=0) != 0).squeeze(dim=1)  # The set of positive proxies of data in the batch
    num_valid_proxies = len(with_pos_proxies)  # The number of positive proxies

    P_sim_sum = torch.where(P_one_hot == 1, pos_exp, torch.zeros_like(pos_exp)).sum(dim=0)
    N_sim_sum = torch.where(N_one_hot == 1, neg_exp, torch.zeros_like(neg_exp)).sum(dim=0)

    pos_term = torch.log(1 + P_sim_sum).sum() / num_valid_proxies
    neg_term = torch.log(1 + N_sim_sum).sum() / n_classes
    loss = pos_term + neg_term

    return loss










"
29,2201.02772,"import torch
import clip
from PIL import Image
import re
import string

device = ""cuda"" if torch.cuda.is_available() else ""cpu""
model, preprocess = clip.load(""RN50"", device=device)
params_to_update = list(model.parameters())   # 返回模型的可训练参数
total = sum([param.nelement() for param in params_to_update])
print(""Number of parameter: %.2fM"" % (total / 1e6))
image = preprocess(Image.open(""CLIP.png"")).unsqueeze(0).to(device)  # torch.Size([1, 3, 224, 224])
text = clip.tokenize([""a diagram"", ""a dog"", ""a cat""]).to(device)

with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)

    logits_per_image, logits_per_text = model(image, text)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()

print(""Label probs:"", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
30,2201.02772,"import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torch.nn.functional as F
import copy
import time
import numpy as np
import sys
import os
import pickle
import matplotlib.pyplot as plt
from model import model
from evaluate import fx_calc_map_label
from metrics import PAN, Triplet_Loss, Contrastive_Loss, Label_Regression_Loss, Modality_invariant_Loss, Proxy_Anchor
from torch.autograd import Function
from torch.utils.data.dataset import Dataset
from torch.utils.data import DataLoader
from extract_clip_feature import CustomDataSet

seed = 1
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
loss_fct = nn.CrossEntropyLoss()


def one_hot(x, num_class):

	return torch.eye(num_class)[x,:]


def train(model, loader, optimizer, num_class, choose_loss='PAN', modality_imbalanced=False):
    model.train()
    running_loss = 0.0
    for img, text, labels, id in loader:  
        optimizer.zero_grad()
        text = text.to(device)
        img = img.to(device)
        label_realvalue = labels.int().type(torch.long).to(device)  # nuswide, wiki, xmedia
        # label_realvalue = (labels - 1).int().type(torch.long).to(device)  # pascal
        centers, img_feature, text_feature, img_predict, text_predict = model(img, text)
        centers = centers[:img_feature.shape[1]]  # multiple GPUs
        if modality_imbalanced:     # i.e. 100%I, 30%T
            bsz = int(img_feature.shape[0]/10)
            text_feature = text_feature[:3*bsz]
            text_label = label_realvalue[:3*bsz]

        if choose_loss == 'CL':   # Contrastive loss
            loss = Contrastive_Loss(img_feature, text_feature, label_realvalue)
        elif choose_loss == 'ML':     # Modality-invariant loss
            loss = Modality_invariant_Loss(img_feature, text_feature, label_realvalue)
        elif choose_loss == 'TL':      # Triplet loss
            loss = Triplet_Loss(img_feature, text_feature, label_realvalue) \
                   + Triplet_Loss(text_feature, img_feature, label_realvalue)
        elif choose_loss == 'LRL':     # Label regression loss
            label_onehot = one_hot(label_realvalue, num_class).to(device)
            loss = Label_Regression_Loss(img_predict, text_predict, label_onehot)
        elif choose_loss == 'CEL':     # Cross-entropy loss
            loss = loss_fct(img_predict, label_realvalue) + loss_fct(text_predict, label_realvalue)
        elif choose_loss == 'PCL':     # Prototype contrastive loss
            loss = PAN(img_feature, torch.t(centers), label_realvalue) \
                   + PAN(text_feature, torch.t(centers), label_realvalue)
	elif choose_loss == 'PNCA':     # Prototype contrastive loss, sample anchor
            loss = Proxy_NCA(img_feature, torch.t(centers), label_realvalue) \
                   + Proxy_NCA(text_feature, torch.t(centers), label_realvalue)
	elif choose_loss == 'P_Anchor':     # sample contrastive loss, prototypical anchor
            loss = Proxy_Anchor(img_feature, torch.t(centers), label_realvalue) \
                   + Proxy_Anchor(text_feature, torch.t(centers), label_realvalue)

        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    return running_loss / len(loader)


def evaluate(model, loader):
    model.eval()
    running_loss = 0.0
    t_imgs, t_txts, t_labels = [], [], []
    with torch.no_grad():
        for img, text, labels, id in loader:
            text = text.to(device)
            img = img.to(device)
            labels = labels.int().to(device)
            _, img_feature, text_feature, img_predict, text_predict = model(img, text)
            t_imgs.append(img_feature.cpu().numpy())
            t_txts.append(text_feature.cpu().numpy())
            t_labels.append(labels.cpu().numpy())

    t_imgs = np.concatenate(t_imgs)  # for visualization
    t_txts = np.concatenate(t_txts)  # for visualization
    t_labels = np.concatenate(t_labels)
    i_map = fx_calc_map_label(t_imgs, t_txts, t_labels)
    t_map = fx_calc_map_label(t_txts, t_imgs, t_labels)
    print('Image to Text: MAP: {:.4f}'.format(i_map))
    print('Text to Image: MAP: {:.4f}'.format(t_map))

    return i_map, t_map, t_imgs, t_txts, t_labels


def figure_plt(Train_Loss, Valid_Loss, png_path):
    plt.figure()
    Epoch = len(Train_Loss)
    X = range(1, Epoch + 1)
    plt.plot(X, Train_Loss, label='Train loss')
    plt.plot(X, Valid_Loss, label='Valid loss')
    plt.legend()
    # plt.title('Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.savefig(png_path)
    # plt.show()


def load_dataset(name, bsz):
    train_loc = 'data/'+name+'/clip_train.pkl'
    test_loc = 'data/'+name+'/clip_test.pkl'
    with open(train_loc, 'rb') as f_pkl:
        data = pickle.load(f_pkl)
        train_labels = data['label']
        train_texts = data['text']
        train_images = data['image']
        train_ids = data['ids']
    with open(test_loc, 'rb') as f_pkl:
        data = pickle.load(f_pkl)
        test_labels = data['label']   
        test_texts = data['text']      
        test_images = data['image']   
        test_ids = data['ids']        
    imgs = {'train': train_images, 'test': test_images}
    texts = {'train': train_texts,  'test': test_texts}
    labs = {'train': train_labels, 'test': test_labels}
    ids = {'train': train_ids, 'test': test_ids}

    dataset = {x: CustomDataSet(images=imgs[x], texts=texts[x], labs=labs[x], ids=ids[x])
               for x in ['train', 'test']}

    shuffle = {'train': True, 'test': False}

    dataloader = {x: DataLoader(dataset[x], batch_size=bsz,
                                shuffle=shuffle[x], num_workers=0) for x in ['train', 'test']}

    return dataloader


if __name__ == '__main__':
    batch_size = 300
    dataloaders = load_dataset('nus-wide', batch_size)  # wiki, nus-wide, pascal, xmedianet
    train_loader = dataloaders['train']
    test_loader = dataloaders['test']
    print(len(train_loader), len(test_loader))

    num_class = 10

    os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1, 2, 3'
    USE_CUDA = torch.cuda.is_available()
    device = torch.device(""cuda:3"" if USE_CUDA else ""cpu"")

    MAX_EPOCH = 500
    temperature = 1.0
    lr = 1e-4
    betas = (0.5, 0.999)
    weight_decay = 0
    early_stop = 10

    model_ft = model(num_class=num_class).to(device)
    model_ft = nn.DataParallel(model_ft, device_ids=[3, 1])
    model_ft.to(device)

    params_to_update = list(model_ft.parameters())
    total = sum([param.nelement() for param in params_to_update])
    print(""Number of parameter: %.2fM"" % (total / 1e6))
    # Observe that all parameters are being optimized
    optimizer_all = optim.Adam(params_to_update, lr=lr, betas=betas)
    for state in [1]:
        print('...Training is beginning...', state)
        # Train and evaluate
        train_loss_history = []
        test_loss_history = []
        i_map = []
        t_map = []
        best_map = 0.0
        no_up = 0  # early stop
        best_model_wts = copy.deepcopy(model_ft.state_dict())  

        for epoch in range(MAX_EPOCH):
            print('==============================')
            start_time = time.time()
            train_loss = train(model_ft, train_loader, optimizer_all, num_class=num_class)
            print('Train loss: ', train_loss)

            img2text, text2img, t_imgs, t_txts, t_labels = evaluate(model_ft, test_loader)
            i_map.append(img2text)
            t_map.append(text2img)

            time_elapsed = time.time() - start_time
            print(f'Epoch: {epoch + 1:02} | Epoch Time: {int(time_elapsed // 60)}m {int(time_elapsed % 60)}s')

            if (img2text + text2img) / 2. > best_map:
                best_map = (img2text + text2img) / 2.
                print('New Best model')
                no_up = 0
                best_model_wts = copy.deepcopy(model_ft.state_dict())
                torch.save(model_ft.state_dict(), 'result/nuswide_pan.pt')
                np.savez('result/nuswide/{}.npz'.format(best_map), image=t_imgs, text=t_txts, label=t_labels)
            else:
                no_up += 1
            if no_up >= early_stop:
                break
        print('==============================')
        print(f'Best average mAP: {best_map:.4f}, Epoch: {epoch+1-early_stop}')
        # print(i_map)  # for visualization
        # print(t_map)  # for visualization

"
31,2201.02772,"from torch.utils.data.dataset import Dataset
from scipy.io import loadmat, savemat
from torch.utils.data import DataLoader
import numpy as np
import pickle
import os
import json
import clip
import torch


class CustomDataSet(Dataset):
    def __init__(
            self,
            images,
            texts,
            labs,
            ids
    ):
        self.images = images
        self.texts = texts
        self.labs = labs
        self.ids = ids

    def __getitem__(self, index):
        img = self.images[index]
        text = self.texts[index]
        lab = self.labs[index]
        id = self.ids[index]
        return img, text, lab, id

    def __len__(self):
        count = len(self.texts)
        return count


def load_dataset(name, bsz=100):
    train_loc = 'data/'+name+'/train.pkl'
    test_loc = 'data/' + name + '/test.pkl'
    with open(train_loc, 'rb') as f_pkl:
        data = pickle.load(f_pkl)
        train_labels = data['label']
        train_texts = data['text']
        train_images = data['image']
        train_ids = data['ids']
    with open(test_loc, 'rb') as f_pkl:
        data = pickle.load(f_pkl)
        test_labels = data['label']
        test_texts = data['text']
        test_images = data['image']
        test_ids = data['ids']
    imgs = {'train': train_images, 'test': test_images}
    texts = {'train': train_texts,  'test': test_texts}
    labs = {'train': train_labels, 'test': test_labels}
    ids = {'train': train_ids, 'test': test_ids}

    dataset = {x: CustomDataSet(images=imgs[x], texts=texts[x], labs=labs[x], ids=ids[x])
               for x in ['train', 'test']}

    shuffle = {'train': False, 'test': False}

    dataloader = {x: DataLoader(dataset[x], batch_size=bsz,
                                shuffle=shuffle[x], num_workers=0) for x in ['train', 'test']}

    return dataloader




if __name__ == '__main__':
    device = ""cuda:1"" if torch.cuda.is_available() else ""cpu""
    model, preprocess = clip.load(""RN50"", device=device)  # ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']

    dataloaders = load_dataset('nus-wide')  # wiki/pascal/xmedianet/nus-wide
    train_loader = dataloaders['train']
    test_loader = dataloaders['test']

    train_imgs, dev_imgs, test_imgs = [], [], []
    train_caps, dev_caps, test_caps = [], [], []
    train_labs, dev_labs, test_labs = [], [], []
    train_ids, dev_ids, test_ids = [], [], []

    for img, text, lab, id in test_loader:
        img = img.squeeze().to(device)
        text = text.squeeze().to(device)
        with torch.no_grad():
            image_features = model.encode_image(img)
            text_features = model.encode_text(text)
            print(image_features, image_features.shape)  # [bsz, 1024]
            print(text_features, text_features.shape)    # [bsz, 1024]
            image_features = image_features.detach().cpu().numpy()
            text_features = text_features.detach().cpu().numpy()
            test_imgs.append(image_features)
            test_caps.append(text_features)
            test_labs.append(lab.numpy())
            test_ids.append(id)
    test_imgs = np.concatenate(test_imgs)
    test_caps = np.concatenate(test_caps)
    test_labs = np.concatenate(test_labs)
    test_ids = np.concatenate(test_ids)
    test_data = {'image': test_imgs, ""text"": test_caps, ""label"": test_labs, 'ids': test_ids}
    with open('clip_test.pkl', 'wb') as f:
        pickle.dump(test_data, f)
    print('Successfully process test data')

    for img, text, lab, id in train_loader:
        # print(img, img.shape)    # [bsz, 1, 3, 224, 224]
        # print(text, text.shape)  # [bsz, 1, 77]
        # print(lab, lab.shape)    # [bsz]
        img = img.squeeze().to(device)
        text = text.squeeze().to(device)
        with torch.no_grad():
            image_features = model.encode_image(img)
            text_features = model.encode_text(text)
            print(image_features, image_features.shape)  # [bsz, 1024]
            print(text_features, text_features.shape)    # [bsz, 1024]
            image_features = image_features.detach().cpu().numpy()
            text_features = text_features.detach().cpu().numpy()
            train_imgs.append(image_features)
            train_caps.append(text_features)
            train_labs.append(lab.numpy())
            train_ids.append(id)
    train_imgs = np.concatenate(train_imgs)
    train_caps = np.concatenate(train_caps)
    train_labs = np.concatenate(train_labs)
    train_ids = np.concatenate(train_ids)
    train_data = {'image': train_imgs, ""text"": train_caps, ""label"": train_labs, 'ids': train_ids}
    with open('clip_train.pkl', 'wb') as f:
        pickle.dump(train_data, f)
    print('Successfully process training data')"
32,2201.02772,"""""""
evaluation indicators, including mAP, recall@K.
mAP is the most common metric in supervised cross-modal retrieval, which measures the performance of the retrieval model on each category.
""""""
import numpy as np
import scipy.spatial
import torch
from collections import Counter

def fx_calc_map_label(image, text, label, k=0, dist_method='L2'):
    if dist_method == 'L2':
        dist = scipy.spatial.distance.cdist(image, text, 'euclidean')
    elif dist_method == 'COS':
        dist = scipy.spatial.distance.cdist(image, text, 'cosine')
    ord = dist.argsort() # [batch, batch]
    numcases = dist.shape[0]
    if k == 0:
      k = numcases
    res = []
    for i in range(numcases):
        order = ord[i]
        p = 0.0
        r = 0.0
        for j in range(k):
            if label[i] == label[order[j]]:
                r += 1
                p += (r / (j + 1))
        if r > 0:
            res += [p / r]
        else:
            res += [0]

    return np.mean(res)


def fx_calc_recall(image, text, label, k=0, dist_method='L2'):
    if dist_method == 'L2':
        dist = scipy.spatial.distance.cdist(image, text, 'euclidean')
    elif dist_method == 'COS':
        dist = scipy.spatial.distance.cdist(image, text, 'cosine')

    ord = dist.argsort() # [batch, batch]
    ranks = np.zeros(image.shape[0])

    # R@K
    for i in range(image.shape[0]):
        q_label = label[i]
        r_labels = label[ord[i]]
        ranks[i] = np.where(r_labels == q_label)[0][0]
    r1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)
    r5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)
    r10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)

    # Prec@K
    for K in [1, 2, 4, 8, 16]:
        prec_at_k = calc_precision_at_K(ord, label, K)
        print(""P@{} : {:.3f}"".format(k, 100 * prec_at_k))

    return r1, r5, r10


def calc_mean(label, res, num_class):
    num_list = [0 for i in range(10)]
    value_list = [0 for i in range(10)]
    for i in range(len(res)):
        num_list[label[i]] += 1
        value_list[label[i]] += res[i]
    for i in range(num_class):
        if num_list[i] != 0:
            value_list[i] = value_list[i]/num_list[i]
            value_list[i] = round(value_list[i], 4)
        else:
            value_list[i] = 0
    return value_list
"
33,2201.02772,"""""""The visual encoder of CLIP utilizes the form of ResNet50 or ViT, here we provide an unofficial implementation of some vision transformers. 
Some codes are originally from https://github.com/rwightman/pytorch-image-models""""""

import math
import logging
import torch
import torch.nn as nn
import torch.nn.functional as F

from functools import partial
from collections import OrderedDict
from copy import deepcopy
from utils.drop import DropPath
from utils.weight_init import trunc_normal_, lecun_normal_
from utils.helps import to_2tuple
from torchvision import models


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

    
class Block(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x):
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

    
class PatchEmbed(nn.Module):
    """""" Image to Patch Embedding
    """"""
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f""Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).""
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x

    
class VisionTransformer(nn.Module):
    """""" Vision Transformer (https://arxiv.org/abs/2010.11929)
    A PyTorch implement by https://github.com/rwightman/pytorch-image-models
    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`
        - https://arxiv.org/abs/2012.12877
    """"""

    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,
                 num_heads=12, mlp_ratio=4., qkv_bias=True, qk_scale=None, representation_size=None, distilled=False,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, norm_layer=None,
                 act_layer=None, weight_init=''):
        """"""
        Args:
            img_size (int, tuple): input image size
            patch_size (int, tuple): patch size
            in_chans (int): number of input channels
            num_classes (int): number of classes for classification head
            embed_dim (int): embedding dimension
            depth (int): depth of transformer
            num_heads (int): number of attention heads
            mlp_ratio (int): ratio of mlp hidden dim to embedding dim
            qkv_bias (bool): enable bias for qkv if True
            qk_scale (float): override default qk scale of head_dim ** -0.5 if set
            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set
            distilled (bool): model includes a distillation token and head as in DeiT models
            drop_rate (float): dropout rate
            attn_drop_rate (float): attention dropout rate
            drop_path_rate (float): stochastic depth rate
            embed_layer (nn.Module): patch embedding layer
            norm_layer: (nn.Module): normalization layer
            weight_init: (str): weight init scheme
        """"""
        super().__init__()
        self.num_classes = num_classes
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
        self.num_tokens = 2 if distilled else 1
        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
        act_layer = act_layer or nn.GELU

        self.patch_embed = embed_layer(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))
        self.pos_drop = nn.Dropout(p=drop_rate)

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        self.blocks = nn.Sequential(*[
            Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)
            for i in range(depth)])
        self.norm = norm_layer(embed_dim)

        # Representation layer
        if representation_size and not distilled:
            self.num_features = representation_size
            self.pre_logits = nn.Sequential(OrderedDict([
                ('fc', nn.Linear(embed_dim, representation_size)),
                ('act', nn.Tanh())
            ]))
        else:
            self.pre_logits = nn.Identity()

        # Classifier head(s)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
        self.head_dist = None
        if distilled:
            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()

        # Weight init
        assert weight_init in ('jax', 'jax_nlhb', 'nlhb', '')
        head_bias = -math.log(self.num_classes) if 'nlhb' in weight_init else 0.
        trunc_normal_(self.pos_embed, std=.02)
        if self.dist_token is not None:
            trunc_normal_(self.dist_token, std=.02)
        if weight_init.startswith('jax'):
            # leave cls token as zeros to match jax impl
            for n, m in self.named_modules():
                _init_vit_weights(m, n, head_bias=head_bias, jax_impl=True)
        else:
            trunc_normal_(self.cls_token, std=.02)
            self.apply(_init_vit_weights)

    def _init_weights(self, m):
        # this fn left here for compat with downstream users
        _init_vit_weights(m)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token', 'dist_token'}

    def get_classifier(self):
        if self.dist_token is None:
            return self.head
        else:
            return self.head, self.head_dist

    def reset_classifier(self, num_classes, global_pool=''):
        self.num_classes = num_classes
        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
        if self.num_tokens == 2:
            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()

    def forward_features(self, x):
        x = self.patch_embed(x)
        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # stole cls_tokens impl from Phil Wang, thanks
        if self.dist_token is None:
            x = torch.cat((cls_token, x), dim=1)
        else:
            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)
        x = self.pos_drop(x + self.pos_embed)
        x = self.blocks(x)
        # x = self.norm(x)  # [natch, sequence_len, dim]
        x_mean = torch.mean(x, dim=1)
        if self.dist_token is None:
            return x, self.pre_logits(x_mean)
        else:                                # 返回两个分类器的均值
            return x[:, 0], x[:, 1]

    def forward(self, x):
        sequence_output, x = self.forward_features(x)   # torch.size([1, 768])
        if self.head_dist is not None:
            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple
            if self.training and not torch.jit.is_scripting():
                # during inference, return the average of both classifier predictions
                return x, x_dist
            else:
                return (x + x_dist) / 2
        else:
            preds = self.head(x)
        return x, preds, sequence_output


def _init_vit_weights(m, n: str = '', head_bias: float = 0., jax_impl: bool = False):
    """""" transformer weight initialization
    * When called without n, head_bias, jax_impl args it will behave exactly the same
      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).
    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl
    """"""
    if isinstance(m, nn.Linear):
        if n.startswith('head'):
            nn.init.zeros_(m.weight)
            nn.init.constant_(m.bias, head_bias)
        elif n.startswith('pre_logits'):
            lecun_normal_(m.weight)
            nn.init.zeros_(m.bias)
        else:
            if jax_impl:
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    if 'mlp' in n:
                        nn.init.normal_(m.bias, std=1e-6)
                    else:
                        nn.init.zeros_(m.bias)
            else:
                trunc_normal_(m.weight, std=.02)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    elif jax_impl and isinstance(m, nn.Conv2d):
        # NOTE conv was left to pytorch default in my original init
        lecun_normal_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.LayerNorm):
        nn.init.zeros_(m.bias)
        nn.init.ones_(m.weight)


# CNN + Transformer
class hybrid_VisionTransformer(nn.Module):
    def __init__(self, backbone, img_size=224, patch_size=16, in_chans=3, num_classes=1000, feature_dim=2048, embed_dim=768, depth=3,
                 num_heads=3, mlp_ratio=4., qkv_bias=True, qk_scale=None, representation_size=None, distilled=False,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, norm_layer=None,
                 act_layer=None, weight_init=''):
        super().__init__()
        self.backbone = backbone
        self.proj = nn.Conv2d(feature_dim, embed_dim, kernel_size=1, stride=1)
        self.num_classes = num_classes
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
        self.num_tokens = 2 if distilled else 1
        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
        act_layer = act_layer or nn.GELU

        self.patch_embed = embed_layer(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        # num_patches = self.patch_embed.num_patches
        num_patches = 49

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))
        self.pos_drop = nn.Dropout(p=drop_rate)

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        self.blocks = nn.Sequential(*[
            Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)
            for i in range(depth)])
        self.norm = norm_layer(embed_dim)

        # Representation layer
        if representation_size and not distilled:
            self.num_features = representation_size
            self.pre_logits = nn.Sequential(OrderedDict([
                ('fc', nn.Linear(embed_dim, representation_size)),
                ('act', nn.Tanh())
            ]))
        else:
            self.pre_logits = nn.Identity()

        # Classifier head(s)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
        self.head_dist = None
        if distilled:
            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()

        # Weight init
        assert weight_init in ('jax', 'jax_nlhb', 'nlhb', '')
        head_bias = -math.log(self.num_classes) if 'nlhb' in weight_init else 0.
        trunc_normal_(self.pos_embed, std=.02)
        if self.dist_token is not None:
            trunc_normal_(self.dist_token, std=.02)
        if weight_init.startswith('jax'):
            # leave cls token as zeros to match jax impl
            for n, m in self.named_modules():
                _init_vit_weights(m, n, head_bias=head_bias, jax_impl=True)
        else:
            trunc_normal_(self.cls_token, std=.02)
            self.apply(_init_vit_weights)

    def _init_weights(self, m):
        # this fn left here for compat with downstream users
        _init_vit_weights(m)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token', 'dist_token'}

    def get_classifier(self):
        if self.dist_token is None:
            return self.head
        else:
            return self.head, self.head_dist

    def reset_classifier(self, num_classes, global_pool=''):
        self.num_classes = num_classes
        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
        if self.num_tokens == 2:
            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()

    def forward_features(self, x):
        x = self.backbone(x)
        x = self.proj(x).flatten(2).transpose(1, 2)  # [batch, 49, 768]
        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # stole cls_tokens impl from Phil Wang, thanks
        if self.dist_token is None:
            x = torch.cat((cls_token, x), dim=1)
        else:
            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)
        x = self.pos_drop(x + self.pos_embed)
        x = self.blocks(x)
        # x = self.norm(x)  # [natch, sequence_len, dim]
        x_mean = torch.mean(x, dim=1)
        if self.dist_token is None:
            return x, self.pre_logits(x_mean)
        else:                                # 返回两个分类器的均值
            return x[:, 0], x[:, 1]

    def forward(self, x):
        sequence_output, x = self.forward_features(x)   # torch.size([1, 768])
        if self.head_dist is not None:
            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple
            if self.training and not torch.jit.is_scripting():
                # during inference, return the average of both classifier predictions
                return x, x_dist
            else:
                return (x + x_dist) / 2
        else:
            preds = self.head(x)
        return x, preds, sequence_output

# model = VisionTransformer(img_size=224, patch_size=16, in_chans=3, num_classes=100, embed_dim=768, depth=12,num_heads=12)
# # for name, para in model.named_parameters():
# # #     print(name, para)
# img = torch.randn(1, 3, 224, 224)
# a, b, _ = model(img)
# print(a.shape)
"
34,2201.02772,"import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
from transformers import BertConfig, BertTokenizer, BertModel
from torchvision.models import alexnet, resnet18, resnet50, inception_v3, vgg19
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import torch
import pickle
import math
import clip

seed = 1
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)


def l1norm(X, dim, eps=1e-8):
    """"""L1-normalize columns of X
    """"""
    norm = torch.abs(X).sum(dim=dim, keepdim=True) + eps
    X = torch.div(X, norm)
    return X


def l2norm(X, dim, eps=1e-8):
    """"""L2-normalize columns of X
    """"""
    norm = torch.pow(X, 2).sum(dim=dim, keepdim=True).sqrt() + eps
    X = torch.div(X, norm)
    return X


def gelu(x):
    """"""Implementation of the gelu activation function.
        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):
        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))
        Also see https://arxiv.org/abs/1606.08415
    """"""
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))


class ImgNN(nn.Module):
    """"""Network to learn image representations""""""
    def __init__(self, input_dim=2048, mindum_dim=512, out_dim=128, dropout_prob=0.1):
        super(ImgNN, self).__init__()
        self.denseL1 = nn.Linear(input_dim, mindum_dim)
        self.denseL2 = nn.Linear(mindum_dim, out_dim)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, x):
        # out = F.relu(self.denseL1(x))
        out = gelu(self.denseL1(x))
        out = self.dropout(self.denseL2(out))
        return out


class TextNN(nn.Module):
    """"""Network to learn text representations""""""
    def __init__(self, input_dim=768, mindum_dim=512, out_dim=128, dropout_prob=0.1):
        super(TextNN, self).__init__()
        self.denseL1 = nn.Linear(input_dim, mindum_dim)
        self.denseL2 = nn.Linear(mindum_dim, out_dim)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, x):
        # out = F.relu(self.denseL1(x))
        out = gelu(self.denseL1(x))
        out = self.dropout(self.denseL2(out))
        return out


class model(nn.Module):
    def __init__(self, num_class, img_dim=1024, text_dim=1024, mid_dim=256, feature_dim=1024, init_weight=True):
        super(model, self).__init__()

        self.imgnn = ImgNN(input_dim=img_dim, mindum_dim=mid_dim, out_dim=feature_dim)
        self.textnn = TextNN(input_dim=text_dim, mindum_dim=mid_dim, out_dim=feature_dim)

        self.n_classes = num_class
        self.feat_dim = feature_dim
        self.predictLayer = nn.Linear(self.feat_dim, self.n_classes, bias=True)  # 不考虑bias，权重归一化之后就是Proxy-NCA, Normlized Softmax
        self.centers = nn.Parameter(torch.randn(self.feat_dim, self.n_classes), requires_grad=False)
        if init_weight:
            self.__init_weight()

    def __init_weight(self):
        nn.init.kaiming_normal_(self.centers, mode='fan_out')
        nn.init.kaiming_normal_(self.predictLayer.weight.data, mode='fan_out')

    def forward(self, img, text):
        # the normalization of class proxies, the more obvious its effect is in the higher-dimensional representation space
        self.predictLayer.weight.data = l2norm(self.predictLayer.weight.data, dim=-1)
        self.centers.data = l2norm(self.centers.data, dim=0)

        img_features = self.imgnn(img.float())
        img_features = l2norm(img_features, dim=1)
        img_pred = self.predictLayer(img_features)

        text_features = self.textnn(text.float())
        text_features = l2norm(text_features, dim=1)
        text_pred = self.predictLayer(text_features)

        return self.centers, img_features, text_features, img_pred, text_pred


"
35,2201.02772,""""""" DropBlock, DropPath
PyTorch implementations of DropBlock and DropPath (Stochastic Depth) regularization layers.
Papers:
DropBlock: A regularization method for convolutional networks (https://arxiv.org/abs/1810.12890)
Deep Networks with Stochastic Depth (https://arxiv.org/abs/1603.09382)
Code:
DropBlock impl inspired by two Tensorflow impl that I liked:
 - https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_model.py#L74
 - https://github.com/clovaai/assembled-cnn/blob/master/nets/blocks.py
Hacked together by / Copyright 2020 Ross Wightman
""""""
import torch
import torch.nn as nn
import torch.nn.functional as F


def drop_block_2d(
        x, drop_prob: float = 0.1, block_size: int = 7,  gamma_scale: float = 1.0,
        with_noise: bool = False, inplace: bool = False, batchwise: bool = False):
    """""" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf
    DropBlock with an experimental gaussian noise option. This layer has been tested on a few training
    runs with success, but needs further validation and possibly optimization for lower runtime impact.
    """"""
    B, C, H, W = x.shape
    total_size = W * H
    clipped_block_size = min(block_size, min(W, H))
    # seed_drop_rate, the gamma parameter
    gamma = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / (
        (W - block_size + 1) * (H - block_size + 1))

    # Forces the block to be inside the feature map.
    w_i, h_i = torch.meshgrid(torch.arange(W).to(x.device), torch.arange(H).to(x.device))
    valid_block = ((w_i >= clipped_block_size // 2) & (w_i < W - (clipped_block_size - 1) // 2)) & \
                  ((h_i >= clipped_block_size // 2) & (h_i < H - (clipped_block_size - 1) // 2))
    valid_block = torch.reshape(valid_block, (1, 1, H, W)).to(dtype=x.dtype)

    if batchwise:
        # one mask for whole batch, quite a bit faster
        uniform_noise = torch.rand((1, C, H, W), dtype=x.dtype, device=x.device)
    else:
        uniform_noise = torch.rand_like(x)
    block_mask = ((2 - gamma - valid_block + uniform_noise) >= 1).to(dtype=x.dtype)
    block_mask = -F.max_pool2d(
        -block_mask,
        kernel_size=clipped_block_size,  # block_size,
        stride=1,
        padding=clipped_block_size // 2)

    if with_noise:
        normal_noise = torch.randn((1, C, H, W), dtype=x.dtype, device=x.device) if batchwise else torch.randn_like(x)
        if inplace:
            x.mul_(block_mask).add_(normal_noise * (1 - block_mask))
        else:
            x = x * block_mask + normal_noise * (1 - block_mask)
    else:
        normalize_scale = (block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-7)).to(x.dtype)
        if inplace:
            x.mul_(block_mask * normalize_scale)
        else:
            x = x * block_mask * normalize_scale
    return x


def drop_block_fast_2d(
        x: torch.Tensor, drop_prob: float = 0.1, block_size: int = 7,
        gamma_scale: float = 1.0, with_noise: bool = False, inplace: bool = False, batchwise: bool = False):
    """""" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf
    DropBlock with an experimental gaussian noise option. Simplied from above without concern for valid
    block mask at edges.
    """"""
    B, C, H, W = x.shape
    total_size = W * H
    clipped_block_size = min(block_size, min(W, H))
    gamma = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / (
            (W - block_size + 1) * (H - block_size + 1))

    if batchwise:
        # one mask for whole batch, quite a bit faster
        block_mask = torch.rand((1, C, H, W), dtype=x.dtype, device=x.device) < gamma
    else:
        # mask per batch element
        block_mask = torch.rand_like(x) < gamma
    block_mask = F.max_pool2d(
        block_mask.to(x.dtype), kernel_size=clipped_block_size, stride=1, padding=clipped_block_size // 2)

    if with_noise:
        normal_noise = torch.randn((1, C, H, W), dtype=x.dtype, device=x.device) if batchwise else torch.randn_like(x)
        if inplace:
            x.mul_(1. - block_mask).add_(normal_noise * block_mask)
        else:
            x = x * (1. - block_mask) + normal_noise * block_mask
    else:
        block_mask = 1 - block_mask
        normalize_scale = (block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-7)).to(dtype=x.dtype)
        if inplace:
            x.mul_(block_mask * normalize_scale)
        else:
            x = x * block_mask * normalize_scale
    return x


class DropBlock2d(nn.Module):
    """""" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf
    """"""
    def __init__(self,
                 drop_prob=0.1,
                 block_size=7,
                 gamma_scale=1.0,
                 with_noise=False,
                 inplace=False,
                 batchwise=False,
                 fast=True):
        super(DropBlock2d, self).__init__()
        self.drop_prob = drop_prob
        self.gamma_scale = gamma_scale
        self.block_size = block_size
        self.with_noise = with_noise
        self.inplace = inplace
        self.batchwise = batchwise
        self.fast = fast  # FIXME finish comparisons of fast vs not

    def forward(self, x):
        if not self.training or not self.drop_prob:
            return x
        if self.fast:
            return drop_block_fast_2d(
                x, self.drop_prob, self.block_size, self.gamma_scale, self.with_noise, self.inplace, self.batchwise)
        else:
            return drop_block_2d(
                x, self.drop_prob, self.block_size, self.gamma_scale, self.with_noise, self.inplace, self.batchwise)


def drop_path(x, drop_prob: float = 0., training: bool = False):
    """"""Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.
    """"""
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """"""Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """"""
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)"
36,2201.02772,""""""" Layer/Module Helpers
Hacked together by / Copyright 2020 Ross Wightman
""""""
from itertools import repeat
import collections.abc


# From PyTorch internals
def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable):
            return x
        return tuple(repeat(x, n))

    return parse


to_1tuple = _ntuple(1)
to_2tuple = _ntuple(2)
to_3tuple = _ntuple(3)
to_4tuple = _ntuple(4)
to_ntuple = _ntuple


def make_divisible(v, divisor=8, min_value=None):
    min_value = min_value or divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v"
37,2201.02772,"import torch.utils.data as data
import torch.nn as nn
import numpy as np
import os
import pickle
import h5py
from PIL import Image
import torch
import random
from torchvision import models, transforms
torch.manual_seed(1) # cpu
torch.cuda.manual_seed(1) #gpu
random.seed(1)
np.random.seed(1)

class xmediaPedes(data.Dataset):
    pklname_list = ['train.pkl', 'test.pkl']
    h5name_list = ['train.h5', 'test.h5']
    def __init__(self, root, split, transform=None):

        self.root = root
        self.split = split.lower()
        self.transform = transform

        if self.split == 'train':
            self.pklname = self.pklname_list[0]
            self.h5name = self.h5name_list[0]
            with open(os.path.join(self.root, self.pklname), 'rb') as f_pkl:
                data = pickle.load(f_pkl)
                self.train_labels = data['labels']
                self.train_captions = data['text_ids']
                self.train_ids = data['ids']
            data_h5py = h5py.File(os.path.join(self.root, self.h5name), 'r')
            self.train_images = data_h5py['images']

        elif self.split == 'valid':
            self.pklname = self.pklname_list[1]
            self.h5name = self.h5name_list[1]
            with open(os.path.join(self.root, self.pklname), 'rb') as f_pkl:
                data = pickle.load(f_pkl)
                self.val_labels = data['labels']
                self.val_captions = data['text_ids']
                self.val_ids = data['ids']
            data_h5py = h5py.File(os.path.join(self.root, self.h5name), 'r')
            self.val_images = data_h5py['images']

    def __getitem__(self, index):
        if self.split == 'train':
            img, caption, label, id = self.train_images[index], self.train_captions[index], self.train_labels[index], self.train_ids[index]
        else:
            img, caption, label, id = self.val_images[index], self.val_captions[index], self.val_labels[index], self.val_ids[index] # add:vgg=self.val_vgg[index]

        img = img.transpose((1, 2, 0))
        img = Image.fromarray(img)

        if self.transform is not None:
            img = self.transform(img)

        label_one_hot = np.zeros(200)  # NUS-WIDE 10类， Pascal 20类
        label_one_hot[label - 1] = 1

        return img, caption, label_one_hot, id

    def __len__(self):
        if self.split == 'train':
            return len(self.train_labels)
        else:
            return len(self.val_labels)


def collate_fn(batch):
    #  batch是一个列表，其中是一个一个的元组，每个元组是dataset中_getitem__的结果
    batch = list(zip(*batch))
    img = torch.tensor([t.numpy() for t in batch[0]])
    caption = torch.tensor([t.numpy() for t in batch[1]])
    label_one_hot = torch.tensor([t.numpy() for t in batch[2]])
    id = torch.tensor([t.numpy() for t in batch[3]])
    del batch
    return img, caption, label_one_hot, id

def load_data(batch_size):
    transform = transforms.Compose([
                        transforms.Resize((224,224)),
                        transforms.ToTensor(),
                        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                             std=[0.229, 0.224, 0.225])
    ])   # 这个是imgnet的均值和方差
    train_data = xmediaPedes('data/xmedia_bert', 'train', transform=transform)
    val_data = xmediaPedes('data/xmedia_bert', 'valid', transform=transform)
    train_loader = data.DataLoader(train_data, batch_size, shuffle=True)
    val_loader = data.DataLoader(val_data, batch_size, shuffle=False)
    return train_loader, val_loader"
38,2201.02772,"import torch
import math
import warnings

from torch.nn.init import _calculate_fan_in_and_fan_out


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(""mean is more than 2 std from [a, b] in nn.init.trunc_normal_. ""
                      ""The distribution of values may be incorrect."",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r""""""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """"""
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def variance_scaling_(tensor, scale=1.0, mode='fan_in', distribution='normal'):
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
    if mode == 'fan_in':
        denom = fan_in
    elif mode == 'fan_out':
        denom = fan_out
    elif mode == 'fan_avg':
        denom = (fan_in + fan_out) / 2

    variance = scale / denom

    if distribution == ""truncated_normal"":
        # constant is stddev of standard normal truncated to (-2, 2)
        trunc_normal_(tensor, std=math.sqrt(variance) / .87962566103423978)
    elif distribution == ""normal"":
        tensor.normal_(std=math.sqrt(variance))
    elif distribution == ""uniform"":
        bound = math.sqrt(3 * variance)
        tensor.uniform_(-bound, bound)
    else:
        raise ValueError(f""invalid distribution {distribution}"")


def lecun_normal_(tensor):
    variance_scaling_(tensor, mode='fan_in', distribution='truncated_normal')"
39,2201.02772,"
"
40,2201.02693,"import argparse
import datetime
import time
import numpy as np
import torch
from torch import distributed as dist
from torch.backends import cudnn
from torch.nn import DataParallel
from torch.nn.parallel.distributed import DistributedDataParallel

from myutils.common import file_util, yaml_util
from myutils.pytorch import func_util
from structure.logger import MetricLogger, SmoothedValue
from utils import main_util, mimic_util, module_util


def get_argparser():
    argparser = argparse.ArgumentParser(description='PyTorch model runner')
    argparser.add_argument('--config', required=True, help='yaml file path')
    argparser.add_argument('--epoch', type=int, help='epoch (higher priority than config if set)')
    argparser.add_argument('--lr', type=float, help='learning rate (higher priority than config if set)')
    argparser.add_argument('-init', action='store_true', help='overwrite checkpoint')
    argparser.add_argument('-evaluate', action='store_true', help='evaluation option')
    # distributed training parameters
    argparser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')
    argparser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')
    return argparser


def train_epoch(model, train_loader, optimizer, criterion, epoch, device, interval):
    model.train()
    metric_logger = MetricLogger(delimiter='  ')
    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value}'))
    metric_logger.add_meter('img/s', SmoothedValue(window_size=10, fmt='{value}'))
    header = 'Epoch: [{}]'.format(epoch)
    for sample_batch, targets in metric_logger.log_every(train_loader, interval, header):
        start_time = time.time()
        sample_batch, targets = sample_batch.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(sample_batch)
        loss = sum((criterion(o, targets) for o in outputs)) if isinstance(outputs, tuple)\
            else criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        batch_size = sample_batch.shape[0]
        metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])
        metric_logger.meters['img/s'].update(batch_size / (time.time() - start_time))


def save_ckpt(model, acc, epoch, ckpt_file_path, model_type):
    print('Saving..')
    state = {
        'type': model_type,
        'model': model.state_dict(),
        'acc': acc,
        'epoch': epoch,
    }
    file_util.make_parent_dirs(ckpt_file_path)
    torch.save(state, ckpt_file_path)


def test(model, data_loader, device, interval=1000, split_name='Test'):
    num_threads = torch.get_num_threads()
    torch.set_num_threads(1)
    model.eval()
    metric_logger = MetricLogger(delimiter='  ')
    header = '{}:'.format(split_name)
    proc_time_list = list()
    with torch.no_grad():
        for image, target in metric_logger.log_every(data_loader, interval, header):
            image = image.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)
            start_time = time.time()
            output = model(image)
            end_time = time.time()
            proc_time_list.append(end_time - start_time)
            acc1, acc5 = main_util.compute_accuracy(output, target, topk=(1, 5))
            # FIXME need to take into account that the datasets
            # could have been padded in distributed setup
            batch_size = image.shape[0]
            metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)
            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    top1_accuracy = metric_logger.acc1.global_avg
    top5_accuracy = metric_logger.acc5.global_avg
    print(' * Acc@1 {:.4f}\tAcc@5 {:.4f}\n'.format(top1_accuracy, top5_accuracy))
    print('Processing time [sec]: {} +- {}'.format(np.average(proc_time_list), np.std(proc_time_list)))
    torch.set_num_threads(num_threads)
    return metric_logger.acc1.global_avg


def validate(model, valid_loader, device):
    acc = test(model, valid_loader, device, split_name='Validation')
    return acc


def train(model, train_loader, valid_loader, best_valid_acc, criterion, device, distributed, device_ids, train_config,
          num_epochs, start_epoch, init_lr, ckpt_file_path, model_type):
    model_without_ddp = model
    if distributed:
        model = DistributedDataParallel(model_without_ddp, device_ids=device_ids)
    elif device.type == 'cuda':
        model = DataParallel(model_without_ddp)

    optim_config = train_config['optimizer']
    if init_lr is not None:
        optim_config['params']['lr'] = init_lr

    optimizer = func_util.get_optimizer(model, optim_config['type'], optim_config['params'])
    scheduler_config = train_config['scheduler']
    scheduler = func_util.get_scheduler(optimizer, scheduler_config['type'], scheduler_config['params'])
    interval = train_config['interval']
    if interval <= 0:
        num_batches = len(train_loader)
        interval = num_batches // 20 if num_batches >= 20 else 1

    end_epoch = start_epoch + train_config['epoch'] if num_epochs is None else start_epoch + num_epochs
    start_time = time.time()
    for epoch in range(start_epoch, end_epoch):
        if distributed:
            train_loader.sampler.set_epoch(epoch)

        train_epoch(model, train_loader, optimizer, criterion, epoch, device, interval)
        valid_acc = validate(model, valid_loader, device)
        if valid_acc > best_valid_acc and main_util.is_main_process():
            print('Updating ckpt (Best top1 accuracy: {:.4f} -> {:.4f})'.format(best_valid_acc, valid_acc))
            best_valid_acc = valid_acc
            save_ckpt(model_without_ddp, best_valid_acc, epoch, ckpt_file_path, model_type)
        scheduler.step()

    dist.barrier()
    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print('Training time {}'.format(total_time_str))


def run(args):
    distributed, device_ids = main_util.init_distributed_mode(args.world_size, args.dist_url)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if device.type == 'cuda':
        cudnn.benchmark = True

    print(args)
    config = yaml_util.load_yaml_file(args.config)
    train_loader, valid_loader, test_loader = main_util.get_data_loaders(config, distributed)
    if 'mimic_model' in config:
        model = mimic_util.get_mimic_model_easily(config, device)
        model_config = config['mimic_model']
    else:
        model = module_util.get_model(config, device)
        model_config = config['model']

    model_type, best_valid_acc, start_epoch, ckpt_file_path =\
        module_util.resume_from_ckpt(model, model_config, args.init)
    train_config = config['train']
    criterion_config = train_config['criterion']
    criterion = func_util.get_loss(criterion_config['type'], criterion_config['params'])
    if not args.evaluate:
        train(model, train_loader, valid_loader, best_valid_acc, criterion, device, distributed, device_ids,
              train_config, args.epoch, start_epoch, args.lr, ckpt_file_path, model_type)
    test(model, test_loader, device)


if __name__ == '__main__':
    parser = get_argparser()
    run(parser.parse_args())
"
41,2201.02693,"import argparse
import os
import random

from PIL import Image

from myutils.common import file_util


def get_argparser():
    parser = argparse.ArgumentParser(description='PyTorch dataset converter')
    parser.add_argument('--input', required=True, help='input dir path')
    parser.add_argument('--dataset', default='caltech', help='dataset type')
    parser.add_argument('--val', type=float, default=0.1, help='validation data rate')
    parser.add_argument('--test', type=float, default=0.1, help='test data rate')
    parser.add_argument('--output', required=True, help='output dir path')
    parser.add_argument('-rgb', action='store_true', help='option to ignore non-RGB image files')
    return parser


def write_converted_dataset(data_list, rgb_only, output_file_path, delimiter='\t'):
    file_util.make_parent_dirs(output_file_path)
    with open(output_file_path, 'w') as fp:
        for label_name, image_file_paths in data_list:
            for image_file_path in image_file_paths:
                if rgb_only:
                    img = Image.open(image_file_path)
                    if img.mode != 'RGB':
                        continue
                fp.write('{}{}{}\n'.format(image_file_path, delimiter, label_name))


def convert_caltech_dataset(input_dir_path, val_rate, test_rate, rgb_only, output_dir_path):
    sub_dir_path_list = file_util.get_dir_path_list(input_dir_path, is_sorted=True)
    dataset_dict = {'train': [], 'valid': [], 'test': []}
    for sub_dir_path in sub_dir_path_list:
        label_name = os.path.basename(sub_dir_path)
        image_file_paths = file_util.get_file_path_list(sub_dir_path, is_sorted=True)
        random.shuffle(image_file_paths)
        total_size = len(image_file_paths)
        train_end_idx = int(total_size * (1 - val_rate - test_rate))
        valid_end_idx = train_end_idx + int((total_size - train_end_idx) * val_rate / (val_rate + test_rate))
        dataset_dict['train'].append((label_name, image_file_paths[:train_end_idx]))
        dataset_dict['valid'].append((label_name, image_file_paths[train_end_idx:valid_end_idx]))
        dataset_dict['test'].append((label_name, image_file_paths[valid_end_idx:]))

    write_converted_dataset(dataset_dict['train'], rgb_only, os.path.join(output_dir_path, 'train.txt'))
    write_converted_dataset(dataset_dict['valid'], rgb_only, os.path.join(output_dir_path, 'valid.txt'))
    write_converted_dataset(dataset_dict['test'], rgb_only, os.path.join(output_dir_path, 'test.txt'))


def convert_imagenet_dataset(input_dir_path, output_dir_path):
    dataset_dict = dict()
    for key in ['train', 'val']:
        pair_list = list()
        sub_dir_path_list = file_util.get_dir_path_list(os.path.join(input_dir_path, key), is_sorted=True)
        for sub_dir_path in sub_dir_path_list:
            label_name = os.path.basename(sub_dir_path)
            image_file_paths = file_util.get_file_path_list(sub_dir_path, is_sorted=True)
            random.shuffle(image_file_paths)
            pair_list.append((label_name, image_file_paths))
        dataset_dict[key] = pair_list

    write_converted_dataset(dataset_dict['train'], False, os.path.join(output_dir_path, 'train.txt'))
    write_converted_dataset(dataset_dict['val'], False, os.path.join(output_dir_path, 'valid.txt'))


def run(args):
    input_dir_path = args.input
    dataset_type = args.dataset
    valid_rate = args.val
    test_rate = args.test
    output_dir_path = args.output
    rgb_only = args.rgb
    if dataset_type == 'caltech':
        convert_caltech_dataset(input_dir_path, valid_rate, test_rate, rgb_only, output_dir_path)
    elif dataset_type == 'imagenet':
        convert_imagenet_dataset(input_dir_path, output_dir_path)
    else:
        raise ValueError('dataset_type `{}` is not expected'.format(dataset_type))


if __name__ == '__main__':
    parser = get_argparser()
    run(parser.parse_args())
"
42,2201.02693,"import argparse
import datetime
import time

import torch
from torch import distributed as dist
from torch import nn
from torch.backends import cudnn
from torch.nn import DataParallel
from torch.nn.parallel.distributed import DistributedDataParallel

from myutils.common import file_util, yaml_util
from myutils.pytorch import func_util, module_util
from structure.logger import MetricLogger, SmoothedValue
from utils import main_util, mimic_util, dataset_util


def get_argparser():
    argparser = argparse.ArgumentParser(description='Mimic Learner')
    argparser.add_argument('--config', required=True, help='yaml file path')
    argparser.add_argument('--device', default='cuda', help='device')
    argparser.add_argument('--aux', type=float, default=100.0, help='auxiliary weight')
    argparser.add_argument('-test_only', action='store_true', help='only test model')
    argparser.add_argument('-student_only', action='store_true', help='test student model only')
    # distributed training parameters
    argparser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')
    argparser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')
    return argparser


def distill_one_epoch(student_model, teacher_model, train_loader, optimizer, criterion,
                      epoch, device, interval, aux_weight):
    student_model.train()
    teacher_model.eval()
    metric_logger = MetricLogger(delimiter='  ')
    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value}'))
    metric_logger.add_meter('img/s', SmoothedValue(window_size=10, fmt='{value}'))
    header = 'Epoch: [{}]'.format(epoch)
    for sample_batch, targets in metric_logger.log_every(train_loader, interval, header):
        start_time = time.time()
        sample_batch, targets = sample_batch.to(device), targets.to(device)
        optimizer.zero_grad()
        student_outputs = student_model(sample_batch)
        teacher_outputs = teacher_model(sample_batch)
        if isinstance(student_outputs, tuple):
            student_outputs, aux = student_outputs[0], student_outputs[1]
            loss = criterion(student_outputs, teacher_outputs) + aux_weight * nn.functional.cross_entropy(aux, targets)
        else:
            loss = criterion(student_outputs, teacher_outputs)

        loss.backward()
        optimizer.step()
        batch_size = sample_batch.shape[0]
        metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])
        metric_logger.meters['img/s'].update(batch_size / (time.time() - start_time))


@torch.no_grad()
def evaluate(model, data_loader, device, interval=1000, split_name='Test', title=None):
    if title is not None:
        print(title)

    num_threads = torch.get_num_threads()
    torch.set_num_threads(1)
    model.eval()
    metric_logger = MetricLogger(delimiter='  ')
    header = '{}:'.format(split_name)
    with torch.no_grad():
        for image, target in metric_logger.log_every(data_loader, interval, header):
            image = image.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)
            output = model(image)

            acc1, acc5 = main_util.compute_accuracy(output, target, topk=(1, 5))
            # FIXME need to take into account that the datasets
            # could have been padded in distributed setup
            batch_size = image.shape[0]
            metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)
            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    top1_accuracy = metric_logger.acc1.global_avg
    top5_accuracy = metric_logger.acc5.global_avg
    print(' * Acc@1 {:.4f}\tAcc@5 {:.4f}\n'.format(top1_accuracy, top5_accuracy))
    torch.set_num_threads(num_threads)
    return metric_logger.acc1.global_avg


def validate(student_model_without_ddp, data_loader, config, device, distributed, device_ids):
    teacher_model_config = config['teacher_model']
    org_model, teacher_model_type = mimic_util.get_org_model(teacher_model_config, device)
    mimic_model = mimic_util.get_mimic_model(config, org_model, teacher_model_type, teacher_model_config,
                                             device, head_model=student_model_without_ddp)
    mimic_model_without_dp = mimic_model.module if isinstance(mimic_model, DataParallel) else mimic_model
    if distributed:
        mimic_model = DistributedDataParallel(mimic_model_without_dp, device_ids=device_ids)
    return evaluate(mimic_model, data_loader, device, split_name='Validation')


def save_ckpt(student_model, epoch, best_valid_value, ckpt_file_path, teacher_model_type):
    print('Saving..')
    module =\
        student_model.module if isinstance(student_model, (DataParallel, DistributedDataParallel)) else student_model
    state = {
        'type': teacher_model_type,
        'model': module.state_dict(),
        'epoch': epoch + 1,
        'best_valid_value': best_valid_value,
        'student': True
    }
    file_util.make_parent_dirs(ckpt_file_path)
    torch.save(state, ckpt_file_path)


def distill(train_loader, valid_loader, input_shape, aux_weight, config, device, distributed, device_ids):
    teacher_model_config = config['teacher_model']
    teacher_model, teacher_model_type = mimic_util.get_teacher_model(teacher_model_config, input_shape, device)
    module_util.freeze_module_params(teacher_model)
    student_model_config = config['student_model']
    student_model = mimic_util.get_student_model(teacher_model_type, student_model_config, config['dataset']['name'])
    student_model = student_model.to(device)
    start_epoch, best_valid_acc = mimic_util.resume_from_ckpt(student_model_config['ckpt'], student_model,
                                                              is_student=True)
    if best_valid_acc is None:
        best_valid_acc = 0.0

    train_config = config['train']
    criterion_config = train_config['criterion']
    criterion = func_util.get_loss(criterion_config['type'], criterion_config['params'])
    optim_config = train_config['optimizer']
    optimizer = func_util.get_optimizer(student_model, optim_config['type'], optim_config['params'])
    scheduler_config = train_config['scheduler']
    scheduler = func_util.get_scheduler(optimizer, scheduler_config['type'], scheduler_config['params'])
    interval = train_config['interval']
    if interval <= 0:
        num_batches = len(train_loader)
        interval = num_batches // 20 if num_batches >= 20 else 1

    student_model_without_ddp = student_model
    if distributed:
        teacher_model = DataParallel(teacher_model, device_ids=device_ids)
        student_model = DistributedDataParallel(student_model, device_ids=device_ids)
        student_model_without_ddp = student_model.module

    ckpt_file_path = student_model_config['ckpt']
    end_epoch = start_epoch + train_config['epoch']
    start_time = time.time()
    for epoch in range(start_epoch, end_epoch):
        if distributed:
            train_loader.sampler.set_epoch(epoch)

        distill_one_epoch(student_model, teacher_model, train_loader, optimizer, criterion,
                          epoch, device, interval, aux_weight)
        valid_acc = validate(student_model, valid_loader, config, device, distributed, device_ids)
        if valid_acc > best_valid_acc and main_util.is_main_process():
            print('Updating ckpt (Best top1 accuracy: {:.4f} -> {:.4f})'.format(best_valid_acc, valid_acc))
            best_valid_acc = valid_acc
            save_ckpt(student_model_without_ddp, epoch, best_valid_acc, ckpt_file_path, teacher_model_type)
        scheduler.step()

    dist.barrier()
    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print('Training time {}'.format(total_time_str))
    del teacher_model
    del student_model


def run(args):
    distributed, device_ids = main_util.init_distributed_mode(args.world_size, args.dist_url)
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    if torch.cuda.is_available():
        cudnn.benchmark = True

    print(args)
    config = yaml_util.load_yaml_file(args.config)
    dataset_config = config['dataset']
    input_shape = config['input_shape']
    train_config = config['train']
    test_config = config['test']
    train_loader, valid_loader, test_loader =\
        dataset_util.get_data_loaders(dataset_config, batch_size=train_config['batch_size'],
                                      rough_size=train_config['rough_size'], reshape_size=input_shape[1:3],
                                      test_batch_size=test_config['batch_size'], jpeg_quality=-1,
                                      distributed=distributed)
    teacher_model_config = config['teacher_model']
    if not args.test_only:
        distill(train_loader, valid_loader, input_shape, args.aux, config, device, distributed, device_ids)

    org_model, teacher_model_type = mimic_util.get_org_model(teacher_model_config, device)
    if not args.student_only:
        if distributed:
            org_model = DataParallel(org_model, device_ids=device_ids)
        evaluate(org_model, test_loader, device, title='[Original model]')

    mimic_model = mimic_util.get_mimic_model(config, org_model, teacher_model_type, teacher_model_config, device)
    mimic_model_without_dp = mimic_model.module if isinstance(mimic_model, DataParallel) else mimic_model
    file_util.save_pickle(mimic_model_without_dp, config['mimic_model']['ckpt'])
    if distributed:
        mimic_model = DistributedDataParallel(mimic_model_without_dp, device_ids=device_ids)
    evaluate(mimic_model, test_loader, device, title='[Mimic model]')


if __name__ == '__main__':
    parser = get_argparser()
    run(parser.parse_args())
"
43,2201.02693,"import argparse
import time

import numpy as np
import torch
from torch import nn
from torch.backends import cudnn
from torch.nn import DataParallel
from torch.nn.parallel import DistributedDataParallel

from model_distiller import load_ckpt
from myutils.common import file_util, yaml_util
from myutils.pytorch import tensor_util
from utils import mimic_util, module_util, dataset_util


def get_argparser():
    argparser = argparse.ArgumentParser(description='PyTorch deployment helper')
    argparser.add_argument('--config', required=True, help='yaml file path')
    argparser.add_argument('--partition', type=int, default=-1, help='partition index (starts from 0)')
    argparser.add_argument('--head', help='output file path for head network pickle')
    argparser.add_argument('--tail', help='output file path for tail network pickle')
    argparser.add_argument('--model', help='output file path for original network pickle')
    argparser.add_argument('--device', help='device for original network pickle')
    argparser.add_argument('--spbit', help='casting or quantization at splitting point: '
                                           '`8bits`, `16bits` or None (32 bits)')
    argparser.add_argument('-org', action='store_true', help='option to split an original DNN model')
    argparser.add_argument('-mimic', action='store_true', help='option to split a mimic DNN model')
    argparser.add_argument('-scpu', action='store_true', help='option to make sensor-side model runnable without cuda')
    argparser.add_argument('-ecpu', action='store_true', help='option to make edge-server model runnable without cuda')
    argparser.add_argument('-test', action='store_true', help='option to check if performance changes after splitting')
    return argparser


def predict(preds, targets):
    loss = nn.functional.cross_entropy(preds, targets)
    _, pred_labels = preds.max(1)
    correct_count = pred_labels.eq(targets).sum().item()
    return correct_count, loss.item()


def test_split_model(model, head_network, tail_network, sensor_device, edge_device, spbit, config):
    dataset_config = config['dataset']
    _, _, test_loader =\
        dataset_util.get_data_loaders(dataset_config, batch_size=config['train']['batch_size'],
                                      rough_size=config['train']['rough_size'],
                                      reshape_size=tuple(config['input_shape'][1:3]),
                                      test_batch_size=config['test']['batch_size'], jpeg_quality=-1)
    print('Testing..')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if device.type == 'cuda':
        cudnn.benchmark = True

    head_network = module_util.use_multiple_gpus_if_available(head_network, sensor_device)
    tail_network = module_util.use_multiple_gpus_if_available(tail_network, edge_device)
    model.to(device)
    head_network.to(sensor_device)
    tail_network.to(edge_device)
    head_network.eval()
    tail_network.eval()
    model.eval()
    split_correct_count = 0
    split_test_loss = 0
    org_correct_count = 0
    org_test_loss = 0
    total = 0
    file_size_list = list()
    head_proc_time_list = list()
    tail_proc_time_list = list()
    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(test_loader):
            total += targets.size(0)
            inputs, targets = inputs.to(sensor_device), targets.to(edge_device)
            head_start_time = time.time()
            zs = head_network(inputs)
            if spbit in ['8bits', '16bits']:
                if spbit == '8bits':
                    # Quantization and dequantization
                    qzs = tensor_util.quantize_tensor(zs)
                    head_end_time = time.time()
                    file_size_list.append(file_util.get_binary_object_size(qzs))
                    zs = tensor_util.dequantize_tensor(qzs)
                else:
                    # Casting and recasting
                    zs = zs.half()
                    head_end_time = time.time()
                    file_size_list.append(file_util.get_binary_object_size(zs))
                    zs = zs.float()
            else:
                head_end_time = time.time()
                file_size_list.append(file_util.get_binary_object_size(zs))

            preds = tail_network(zs.to(edge_device))
            tail_end_time = time.time()
            sub_correct_count, sub_test_loss = predict(preds, targets)
            split_correct_count += sub_correct_count
            split_test_loss += sub_test_loss
            inputs, targets = inputs.to(device), targets.to(device)
            preds = model(inputs)
            sub_correct_count, sub_test_loss = predict(preds, targets)
            org_correct_count += sub_correct_count
            org_test_loss += sub_test_loss
            head_proc_time_list.append(head_end_time - head_start_time)
            tail_proc_time_list.append(tail_end_time - head_end_time)

    org_acc = 100.0 * org_correct_count / total
    print('[Before splitting]\tAverage Loss: {:.4f}, Accuracy: {}/{} [{:.4f}%]\n'.format(
        org_test_loss / total, org_correct_count, total, org_acc))
    split_acc = 100.0 * split_correct_count / total
    print('[After splitting]\tAverage Loss: {:.4f}, Accuracy: {}/{} [{:.4f}%]\n'.format(
        split_test_loss / total, split_correct_count, total, split_acc))
    print('Output file size at splitting point [KB]: {} +- {}'.format(
        np.average(file_size_list), np.std(file_size_list)))
    print('Local processing time [sec]: {} +- {}'.format(np.average(head_proc_time_list), np.std(head_proc_time_list)))
    print('Edge processing time [sec]: {} +- {}'.format(np.average(tail_proc_time_list), np.std(tail_proc_time_list)))


def split_original_model(model, input_shape, device, config, sensor_device, edge_device, partition_idx,
                         head_output_file_path, tail_output_file_path, require_test, spbit):
    print('Splitting an original DNN model')
    modules = list()
    z = torch.rand(1, *input_shape).to(device)
    module_util.extract_decomposable_modules(model, z, modules)
    head_module_list = list()
    tail_module_list = list()
    if partition_idx < 0:
        teacher_model_config = config['teacher_model']
        start_idx = teacher_model_config['start_idx']
        end_idx = teacher_model_config['end_idx']
        head_module_list.extend(modules[start_idx:end_idx])
        tail_module_list.extend(modules[end_idx:])
    else:
        head_module_list.extend(modules[:partition_idx])
        tail_module_list.extend(modules[partition_idx:])

    head_network = nn.Sequential(*head_module_list)
    tail_network = mimic_util.get_tail_network(config, tail_module_list)
    file_util.save_pickle(head_network.to(sensor_device), head_output_file_path)
    file_util.save_pickle(tail_network.to(edge_device), tail_output_file_path)
    if require_test:
        test_split_model(model, head_network, tail_network, sensor_device, edge_device, spbit, config)


def split_within_student_model(model, input_shape, device, config, teacher_model_type, sensor_device, edge_device,
                               partition_idx, head_output_file_path, tail_output_file_path, require_test, spbit):
    print('Splitting within a student DNN model')
    org_modules = list()
    z = torch.rand(1, *input_shape).to(device)
    plain_model = model.module if isinstance(model, (DataParallel, DistributedDataParallel)) else model
    module_util.extract_decomposable_modules(plain_model, z, org_modules)
    student_model = mimic_util.load_student_model(config, teacher_model_type, device)
    student_modules = list()
    module_util.extract_decomposable_modules(student_model, z, student_modules)
    head_module_list = list()
    tail_module_list = list()
    teacher_model_config = config['teacher_model']
    end_idx = teacher_model_config['end_idx']
    if partition_idx < 0:
        head_module_list.extend(student_modules)
    else:
        head_module_list.extend(student_modules[:partition_idx])
        tail_module_list.extend(student_modules[partition_idx:])

    tail_module_list.extend(org_modules[end_idx:])
    head_network = nn.Sequential(*head_module_list)
    tail_network = mimic_util.get_tail_network(config, tail_module_list)
    file_util.save_pickle(head_network.to(sensor_device), head_output_file_path)
    file_util.save_pickle(tail_network.to(edge_device), tail_output_file_path)
    if require_test:
        device = torch.device('cuda' if next(model.parameters()).is_cuda else 'cpu')
        mimic_model = mimic_util.get_mimic_model(config, model, teacher_model_type, teacher_model_config, device)
        test_split_model(mimic_model, head_network, tail_network, sensor_device, edge_device, spbit, config)


def convert_model(model, device, output_file_path):
    if device.type == 'cpu' and isinstance(model, nn.parallel.DataParallel):
        model = model.module

    for module in model.modules():
        module.to(device)
    file_util.save_pickle(model, output_file_path)


def run(args):
    print(args)
    config = yaml_util.load_yaml_file(args.config)
    sensor_device = torch.device('cpu' if args.scpu else 'cuda')
    edge_device = torch.device('cpu' if args.ecpu else 'cuda')
    partition_idx = args.partition
    head_output_file_path = args.head
    tail_output_file_path = args.tail
    input_shape = config['input_shape']
    if 'teacher_model' not in config:
        model = module_util.get_model(config, torch.device('cuda') if torch.cuda.is_available() else None)
        module_util.resume_from_ckpt(model, config['model'], False)
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model, teacher_model_type =\
            mimic_util.get_org_model(config['teacher_model'], device)
        if args.org and head_output_file_path is not None and tail_output_file_path is not None:
            split_original_model(model, input_shape, device, config, sensor_device, edge_device, partition_idx,
                                 head_output_file_path, tail_output_file_path, args.test, args.spbit)
        elif args.mimic:
            model = mimic_util.get_mimic_model_easily(config, sensor_device)
            student_model_config = config['mimic_model']
            load_ckpt(student_model_config['ckpt'], model=model, strict=True)
            split_original_model(model, input_shape, device, config, sensor_device, edge_device, partition_idx,
                                 head_output_file_path, tail_output_file_path, args.test, args.spbit)
        elif head_output_file_path is not None and tail_output_file_path is not None:
            split_within_student_model(model, input_shape, device, config, teacher_model_type,
                                       sensor_device, edge_device, partition_idx,
                                       head_output_file_path, tail_output_file_path, args.test, args.spbit)

    if args.model is not None and args.device is not None:
        convert_model(model, torch.device(args.device), args.model)


if __name__ == '__main__':
    parser = get_argparser()
    run(parser.parse_args())
"
44,2201.02693,"import argparse

import matplotlib.pyplot as plt
import numpy as np
import torch
from torch.backends import cudnn
from torch.nn import DataParallel

from myutils.common import file_util, yaml_util
from structure.wrapper import RepresentationWrapper
from utils import main_util, mimic_util, module_util, module_wrap_util


def get_argparser():
    argparser = argparse.ArgumentParser(description='Layer-wise representation analyzer')
    argparser.add_argument('--config', required=True, help='yaml file path')
    argparser.add_argument('--split', default='train', help='dataset split')
    argparser.add_argument('--method', default='tsne', help='representation method')
    argparser.add_argument('--dim', type=int, default=2, help='number of dimensions after transformation')
    argparser.add_argument('--output', help='output plot file path')
    argparser.add_argument('-cpu', action='store_true', help='use CPU')
    return argparser


def extract_transformed_outputs(parent_module, transformed_output_list, name_list):
    for name, child_module in parent_module.named_children():
        if isinstance(child_module, RepresentationWrapper):
            transformed_output_list.append(child_module.get_transformed_list())
            name_list.append(type(child_module.org_module).__name__)
        elif list(child_module.children()):
            extract_transformed_outputs(child_module, transformed_output_list, name_list)
        else:
            print('RepresentationWrapper is missing for {}: {}'.format(name, type(child_module).__name__))


def assess_discriminabilities(transformed_outputs):
    value_list = list()
    for transformed_output in transformed_outputs:
        dist_list = list()
        for i, transformed_output_x in enumerate(transformed_output):
            for transformed_output_y in transformed_outputs[i + 1:]:
                dist = np.linalg.norm(transformed_output_x - transformed_output_y)
                dist_list.append(dist)
        value_list.append(np.mean(dist_list))
    return value_list


def analyze_with_mean_inputs(model, input_shape, data_loader, device, split_name,
                             method, dim, model_type, output_file_path):
    if output_file_path is None:
        output_file_path = './{}_with_mean_inputs_by_{}.eps'.format(model_type, '{}_{}-dim'.format(method, dim))

    file_util.make_parent_dirs(output_file_path)
    model = model.module if isinstance(model, DataParallel) else model
    input_batch = torch.rand(input_shape).unsqueeze(0).to(device)
    module_wrap_util.wrap_decomposable_modules(model, RepresentationWrapper, input_batch, method=method, dim=dim)
    if device.type == 'cuda':
        model = DataParallel(model)

    model.eval()
    accumulated_tensor_dict = dict()
    with torch.no_grad():
        print('Computing mean inputs ...')
        for batch_idx, (sample_batch, targets) in enumerate(data_loader):
            for x, y in zip(sample_batch, targets):
                class_label = y.item()
                if class_label not in accumulated_tensor_dict:
                    accumulated_tensor_dict[class_label] = [x, 1]
                else:
                    accumulated_tensor_dict[class_label][0] += x
                    accumulated_tensor_dict[class_label][1] += 1

        mean_input_list = list()
        for y, (x, num_samples) in accumulated_tensor_dict.items():
            mean_x = x / num_samples
            mean_input_list.append(mean_x)

        mean_batch = torch.stack(mean_input_list)
        print('Analyzing layer-wise discriminability ...')
        preds = model(mean_batch)

    transformed_output_list = list()
    name_list = list()
    extract_transformed_outputs(model, transformed_output_list, name_list)
    xs = list(range(len(name_list)))
    discriminabilities = assess_discriminabilities(transformed_output_list)
    plt.plot(xs, discriminabilities, label=method)
    plt.xticks(xs, name_list, rotation=90)
    plt.xlabel('Layer')
    plt.ylabel('Discriminability')
    plt.title(split_name)
    plt.legend()
    plt.savefig(output_file_path)
    plt.show()


def run(args):
    device = torch.device('cuda' if torch.cuda.is_available() and not args.cpu else 'cpu')
    if device.type == 'cuda':
        cudnn.benchmark = True

    config = yaml_util.load_yaml_file(args.config)
    train_loader, valid_loader, test_loader = main_util.get_data_loaders(config, False)
    input_shape = config['input_shape']
    if 'mimic_model' in config:
        model = mimic_util.get_mimic_model_easily(config, device)
        model_config = config['mimic_model']
    else:
        model = module_util.get_model(config, device)
        model_config = config['model']

    model_type, _, _, _ = module_util.resume_from_ckpt(model, model_config, False)
    split_name = args.split
    data_loader = train_loader if split_name == 'train' else valid_loader if split_name == 'valid' else test_loader
    analyze_with_mean_inputs(model, input_shape, data_loader, device, split_name,
                             args.method, args.dim, model_type, args.output)


if __name__ == '__main__':
    parser = get_argparser()
    run(parser.parse_args())
"
45,2201.02693,"import argparse
import os

import numpy as np
import torch
import torchvision

from models.classification.lenet5 import MnistLeNet5
from myutils.common import file_util, yaml_util
from utils import ae_util, data_util, mimic_util, module_util, net_measure_util


def get_argparser():
    parser = argparse.ArgumentParser(description=os.path.basename(__file__))
    parser.add_argument('--isize', default='3,224,224', help='input data shape (delimited by comma)')
    parser.add_argument('--model', default='alexnet', help='network model (default: alexnet)')
    parser.add_argument('--config', nargs='+', help='yaml file path')
    parser.add_argument('--pkl', help='pickle file path')
    parser.add_argument('-scale', action='store_true', help='data size scaling option')
    parser.add_argument('-submodule', action='store_true', help='submodule extraction option')
    parser.add_argument('-ts', action='store_true', help='teacher-student models option')
    return parser


def get_model(model_type):
    lower_model_type = model_type.lower()
    if lower_model_type == 'mnist':
        return MnistLeNet5()
    elif model_type in torchvision.models.__dict__:
        return torchvision.models.__dict__[model_type]()
    raise ValueError('model_type `{}` is not expected'.format(model_type))


def read_config(config_file_path):
    config = yaml_util.load_yaml_file(config_file_path)
    if config['model']['type'] == 'inception_v3':
        config['model']['params']['aux_logits'] = False

    model = module_util.get_model(config, torch.device('cpu'))
    model_type = config['model']['type']
    input_shape = config['input_shape']
    return model, model_type, input_shape


def analyze(model, input_shape, model_type, scaled, submoduled, plot):
    if submoduled:
        return net_measure_util.compute_model_complexity_and_data_size(model, model_type, input_shape,
                                                                       scaled=scaled, plot=plot)
    return net_measure_util.compute_layerwise_complexity_and_data_size(model, model_type, input_shape,
                                                                       scaled=scaled, plot=plot)


def analyze_single_model(config_file_path, args, plot=True):
    cpu_device = torch.device('cpu')
    if file_util.check_if_exists(config_file_path):
        config = yaml_util.load_yaml_file(config_file_path)
        if 'teacher_model' in config:
            teacher_model_config = config['teacher_model']
            org_model, teacher_model_type = mimic_util.get_org_model(teacher_model_config, cpu_device)
            model = mimic_util.get_mimic_model(config, org_model, teacher_model_type, teacher_model_config, cpu_device)
            model_type = config['mimic_model']['type']
            input_shape = config['input_shape']
        elif 'autoencoder' in config:
            model, model_type = ae_util.get_autoencoder(config, cpu_device, True)
            input_shape = config['input_shape']
        else:
            model, model_type, input_shape = read_config(config_file_path)
    else:
        pickle_file_path = args.pkl
        model_type = args.model
        input_shape = list(data_util.convert2type_list(args.isize, ',', int))
        model = file_util.load_pickle(pickle_file_path) if file_util.check_if_exists(pickle_file_path)\
            else get_model(model_type)
    op_counts, data_sizes, accum_complexities =\
        analyze(model, input_shape, model_type, args.scale, args.submodule, plot)
    return op_counts, data_sizes, accum_complexities, model_type


def analyze_multiple_models(config_file_paths, args):
    op_counts_list = list()
    data_sizes_list = list()
    accum_complexities_list = list()
    model_type_list = list()
    for config_file_path in config_file_paths:
        op_counts, data_sizes, accum_complexities, model_type = analyze_single_model(config_file_path, args, False)
        op_counts_list.append(op_counts)
        data_sizes_list.append(data_sizes)
        accum_complexities_list.append(accum_complexities)
        model_type_list.append(model_type)

    net_measure_util.plot_model_complexities(op_counts_list, model_type_list)
    net_measure_util.plot_accumulated_model_complexities(accum_complexities_list, model_type_list)
    net_measure_util.plot_model_data_sizes(data_sizes_list, args.scale, model_type_list)


def get_teacher_and_student_models(mimic_config, input_shape):
    teacher_model_config = mimic_config['teacher_model']
    teacher_model, teacher_model_type =\
        mimic_util.get_teacher_model(teacher_model_config, input_shape, torch.device('cpu'))
    student_model = mimic_util.get_student_model(teacher_model_type, mimic_config['student_model'],
                                                 mimic_config['dataset']['name'])
    return teacher_model_type, teacher_model, student_model


def analyze_teacher_student_models(mimic_config_file_paths, args):
    scaled = args.scale
    submoduled = args.submodule
    model_type_list = list()
    teacher_complexity_list = list()
    student_complexity_list = list()
    teacher_data_size_list = list()
    student_data_size_list = list()
    for mimic_config_file_path in mimic_config_file_paths:
        mimic_config = yaml_util.load_yaml_file(mimic_config_file_path)
        input_shape = mimic_config['input_shape']
        teacher_model_type, teacher_model, student_model = get_teacher_and_student_models(mimic_config, input_shape)
        _, teacher_data_sizes, teacher_accum_complexities = analyze(teacher_model, input_shape, None, scaled=scaled,
                                                                    submoduled=submoduled, plot=False)
        _, student_data_sizes, student_accum_complexities = analyze(student_model, input_shape, None, scaled=scaled,
                                                                    submoduled=submoduled, plot=False)
        student_model_config = mimic_config['student_model']
        student_model_version = student_model_config['version']
        made_bottleneck = student_model_version.endswith('b')
        model_type_list.append('Ver.{}'.format(student_model_version))
        teacher_complexity_list.append(teacher_accum_complexities[-1])
        teacher_data_size_list.append(teacher_data_sizes[-1] / teacher_data_sizes[0])
        bottleneck_idx = np.argmin(student_data_sizes) if made_bottleneck else -1
        if student_data_sizes[bottleneck_idx] >= student_data_sizes[0] or not made_bottleneck:
            student_complexity_list.append(student_accum_complexities[-1])
            student_data_size_list.append(student_data_sizes[-1] / student_data_sizes[0])
        else:
            student_complexity_list.append(student_accum_complexities[bottleneck_idx - 1])
            student_data_size_list.append(student_data_sizes[bottleneck_idx] / student_data_sizes[0])

    net_measure_util.plot_teacher_and_student_complexities(teacher_complexity_list, student_complexity_list,
                                                           model_type_list)
    net_measure_util.plot_bottleneck_data_size_vs_complexity(teacher_data_size_list, teacher_complexity_list,
                                                             student_data_size_list, student_complexity_list,
                                                             model_type_list)


def run(args):
    config_file_paths = args.config
    if config_file_paths is None or (len(config_file_paths) <= 1 and not args.ts):
        analyze_single_model(None if config_file_paths is None else config_file_paths[0], args)
    elif not args.ts:
        analyze_multiple_models(config_file_paths, args)
    else:
        analyze_teacher_student_models(config_file_paths, args)


if __name__ == '__main__':
    argparser = get_argparser()
    run(argparser.parse_args())
"
46,2201.02693,"import argparse
import sys
import zlib

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.backends.cudnn as cudnn

from myutils.common import file_util, yaml_util
from structure.wrapper import CompressionWrapper, RunTimeWrapper
from utils import misc_util, module_util, module_wrap_util, dataset_util


def get_argparser():
    parser = argparse.ArgumentParser(description='Compression Analyzer')
    parser.add_argument('--config', required=True, help='yaml file path')
    parser.add_argument('--pkl', help='model pickle file path')
    parser.add_argument('--mode', default='comp_rate', help='evaluation option')
    parser.add_argument('--comp_layer', type=int, default=-1, help='index of layer to compress its input'
                                                                   ' (starts from 1, no compression if 0 is given)')
    parser.add_argument('-cpu', action='store_true', help='use CPU')
    return parser


def resume_from_ckpt(model, model_config, device):
    ckpt_file_path = model_config['ckpt']
    if not file_util.check_if_exists(ckpt_file_path):
        return model_config['type'], 0, 1, ckpt_file_path

    print('Resuming from checkpoint..')
    checkpoint = torch.load(ckpt_file_path)
    model_state_dict = checkpoint['model']
    if device.type == 'cpu':
        for key in list(model_state_dict.keys()):
            if key.startswith('module.'):
                val = model_state_dict.pop(key)
                model_state_dict[key[7:]] = val

    model.load_state_dict(model_state_dict)
    model_type = checkpoint['type']
    best_acc = checkpoint['acc']
    start_epoch = checkpoint['epoch']
    return model_type, best_acc, start_epoch, ckpt_file_path


def save_ckpt(model, acc, epoch, ckpt_file_path, model_type):
    print('Saving..')
    state = {
        'type': model_type,
        'model': model.state_dict(),
        'acc': acc,
        'epoch': epoch,
    }
    file_util.make_parent_dirs(ckpt_file_path)
    torch.save(state, ckpt_file_path)


def test(model, test_loader, device, data_type='Test'):
    model.eval()
    correct = 0
    total = 0
    data_size = 0
    compressed_data_size = 0
    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(test_loader):
            np_input = inputs.clone().cpu().detach().numpy()
            data_size += np_input.nbytes
            compressed_input = zlib.compress(np_input, 9)
            compressed_data_size += sys.getsizeof(compressed_input)
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    acc = 100.0 * correct / total
    print('\n{} set: Accuracy: {}/{} ({:.4f}%)\n'.format(data_type, correct, total, acc))
    return acc, data_size / total, compressed_data_size / total


def validate(model, valid_loader, epoch, device, best_acc, ckpt_file_path, model_type):
    acc, _, _ = test(model, valid_loader, device, 'Validation')
    if acc > best_acc:
        save_ckpt(model, acc, epoch, ckpt_file_path, model_type)
        best_acc = acc
    return best_acc


def extract_compression_rates(parent_module, org_data_size_list, compressed_data_size_list, name_list):
    for name, child_module in parent_module.named_children():
        if isinstance(child_module, CompressionWrapper):
            org_data_size_list.append(child_module.get_average_org_data_size())
            compressed_data_size_list.append(child_module.get_average_compressed_data_size())
            name_list.append(type(child_module.org_module).__name__)
        elif list(child_module.children()):
            extract_compression_rates(child_module, org_data_size_list, compressed_data_size_list, name_list)
        else:
            print('CompressionWrapper is missing for {}: {}'.format(name, type(child_module).__name__))


def plot_compression_rates(model, avg_input_data_size, avg_compressed_input_data_size):
    org_data_size_list = list()
    compressed_data_size_list = list()
    name_list = list()
    org_data_size_list.append(avg_input_data_size)
    compressed_data_size_list.append(avg_compressed_input_data_size)
    name_list.append('Input')
    extract_compression_rates(model, org_data_size_list, compressed_data_size_list, name_list)
    xs = list(range(len(org_data_size_list)))
    if not misc_util.check_if_plottable():
        print('Average Input Data Size: {}\tCompressed: {}'.format(avg_input_data_size, avg_compressed_input_data_size))
        print('Layer\tOriginal Data Size\tCompressed Data Size')
        for i in range(len(xs)):
            print('{}\t{}\t{}'.format(name_list[i], org_data_size_list[i], compressed_data_size_list[i]))
        return

    plt.plot(xs, [avg_input_data_size for _ in range(len(name_list))], label='Input')
    plt.plot(xs, org_data_size_list, label='Original')
    plt.plot(xs, compressed_data_size_list, label='Compressed')
    plt.xticks(xs, name_list, rotation=90)
    plt.xlabel('Layer')
    plt.ylabel('Average Data Size [Bytes]')
    plt.legend()
    plt.show()


def analyze_compression_rate(model, input_shape, test_loader, device):
    input_batch = torch.rand(input_shape).unsqueeze(0).to(device)
    module_wrap_util.wrap_decomposable_modules(model, CompressionWrapper, input_batch)
    _, avg_input_data_size, avg_compressed_input_data_size = test(model, test_loader, device)
    plot_compression_rates(model, avg_input_data_size, avg_compressed_input_data_size)


def extract_running_times(wrapped_modules):
    num_samples = len(wrapped_modules[0].get_timestamps())
    start_times = np.array(wrapped_modules[0].start_timestamp_list)
    time_mat = np.zeros((num_samples, len(wrapped_modules)))
    comp_time_mat = np.zeros_like(time_mat)
    for i, wrapped_module in enumerate(wrapped_modules):
        target_times = np.array(wrapped_module.get_compression_timestamps() if wrapped_module.is_compressed
                                else wrapped_module.get_timestamps())
        time_mat[:, i] = (target_times - start_times).reshape(1, start_times.size)
        if wrapped_module.is_compressed:
            comp_time_mat[:, i] = np.array(wrapped_module.get_compression_time_list()).reshape(1, start_times.size)
    return time_mat, comp_time_mat


def plot_running_time(wrapped_modules):
    name_list = ['{}{}: {}'.format(type(wrapped_module.org_module).__name__,
                                   '*' if wrapped_module.is_compressed else '', i + 1)
                 for i, wrapped_module in enumerate(wrapped_modules)]
    time_mat, comp_time_mat = extract_running_times(wrapped_modules)
    mean_times = time_mat.mean(axis=0)
    mean_comp_times = comp_time_mat.mean(axis=0)
    xs = list(range(len(name_list)))
    if not misc_util.check_if_plottable():
        print('Layer\tAverage Accumulated Elapsed Time\tAverage Elapsed Time for Compression')
        for i in range(len(xs)):
            print('{}\t{}\t{}'.format(name_list[i], mean_times[i], mean_comp_times[i]))
        return

    fig, ax1 = plt.subplots()
    ax1.plot(xs, mean_comp_times, '-')
    ax1.set_xticks(xs)
    ax1.set_xticklabels(name_list)
    ax1.set_xlabel('Layer')
    ax1.set_ylabel('Average Elapsed Time for Compression [sec]', color='b')
    for tick in ax1.get_xticklabels():
        tick.set_rotation(90)

    ax2 = ax1.twinx()
    ax2.plot(xs, mean_times, 'r--')
    ax2.set_ylabel('Average Accumulated Elapsed Time [sec]', color='r')
    plt.tight_layout()
    plt.show()


def analyze_running_time(model, input_shape, comp_layer_idx, test_loader, device):
    wrapped_modules = list()
    input_batch = torch.rand(input_shape).unsqueeze(0).to(device)
    module_wrap_util.wrap_decomposable_modules(model, RunTimeWrapper, input_batch,
                                               wrapped_list=wrapped_modules)
    wrapped_modules[0].is_first = True
    if comp_layer_idx < 0:
        for wrapped_module in wrapped_modules:
            wrapped_module.is_compressed = True
    elif 0 < comp_layer_idx <= len(wrapped_modules):
        wrapped_modules[comp_layer_idx - 1].is_compressed = True

    _, avg_input_data_size, _ = test(model, test_loader, device)
    plot_running_time(wrapped_modules)


def run(args):
    device = torch.device('cuda' if torch.cuda.is_available() and not args.cpu else 'cpu')
    if device.type == 'cuda':
        cudnn.benchmark = True

    config = yaml_util.load_yaml_file(args.config)
    dataset_config = config['dataset']
    train_config = config['train']
    test_config = config['test']
    compress_config = test_config['compression']
    input_shape = config['input_shape']
    train_loader, valid_loader, test_loader =\
        dataset_util.get_data_loaders(dataset_config, batch_size=train_config['batch_size'],
                                      compression_type=compress_config['type'], compressed_size=compress_config['size'],
                                      rough_size=train_config['rough_size'], reshape_size=input_shape[1:3],
                                      test_batch_size=test_config['batch_size'], jpeg_quality=test_config['jquality'])

    pickle_file_path = args.pkl
    if not file_util.check_if_exists(pickle_file_path):
        model = module_util.get_model(config, device)
        resume_from_ckpt(model, config['model'], device)
    else:
        model = file_util.load_pickle(pickle_file_path).to(device)

    analysis_mode = args.mode
    model.eval()
    if analysis_mode == 'comp_rate':
        analyze_compression_rate(model, input_shape, test_loader, device)
    elif analysis_mode == 'run_time':
        analyze_running_time(model, input_shape, args.comp_layer, test_loader, device)
    else:
        raise ValueError('mode argument `{}` is not expected'.format(analysis_mode))


if __name__ == '__main__':
    parser = get_argparser()
    run(parser.parse_args())
"
47,2201.02693,"import argparse
import datetime
import sys
import time

import torch
import torchvision
from torch import distributed as dist
from torch.nn import DataParallel, SyncBatchNorm
from torch.nn.parallel import DistributedDataParallel

from myutils.common import file_util, yaml_util
from myutils.pytorch import func_util, module_util
from structure.logger import MetricLogger, SmoothedValue
from tools.distillation import DistillationBox
from utils import main_util, mimic_util, dataset_util

try:
    from apex import amp
except ImportError:
    amp = None


def get_argparser():
    parser = argparse.ArgumentParser(description='Knowledge distillation for image classification models')
    parser.add_argument('--config', required=True, help='yaml file path')
    parser.add_argument('--device', default='cuda', help='device')
    parser.add_argument('--start_epoch', default=0, type=int, metavar='N', help='start epoch')
    parser.add_argument('-sync_bn', action='store_true', help='Use sync batch norm')
    parser.add_argument('-test_only', action='store_true', help='Only test the models')
    parser.add_argument('-student_only', action='store_true', help='Test the student model only')
    # Mixed precision training parameters
    parser.add_argument('-apex', action='store_true',
                        help='Use apex for mixed precision training')
    parser.add_argument('--apex_opt_level', default='O1', type=str,
                        help='For apex mixed precision training'
                             'O0 for FP32 training, O1 for mixed precision training.'
                             'For further detail, see https://github.com/NVIDIA/apex/tree/master/examples/imagenet')
    # distributed training parameters
    parser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')
    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')
    return parser


def load_ckpt(ckpt_file_path, model=None, optimizer=None, lr_scheduler=None, strict=True):
    if not file_util.check_if_exists(ckpt_file_path):
        print('ckpt file is not found at `{}`'.format(ckpt_file_path))
        return None, None

    ckpt = torch.load(ckpt_file_path, map_location='cpu')
    if model is not None:
        print('Loading model parameters')
        model.load_state_dict(ckpt['model'], strict=strict)
    if optimizer is not None:
        print('Loading optimizer parameters')
        optimizer.load_state_dict(ckpt['optimizer'])
    if lr_scheduler is not None:
        print('Loading scheduler parameters')
        lr_scheduler.load_state_dict(ckpt['lr_scheduler'])
    return ckpt.get('best_value', 0.0), ckpt['config'], ckpt['args']


def get_model(model_config, device, distributed, sync_bn):
    model_name = model_config['type']
    model = torchvision.models.__dict__[model_name](**model_config['params'])
    if distributed and sync_bn:
        model = SyncBatchNorm.convert_sync_batchnorm(model)

    ckpt_file_path = model_config['ckpt']
    load_ckpt(ckpt_file_path, model=model, strict=True)
    return model.to(device)


def save_ckpt(model, optimizer, lr_scheduler, best_value, config, args, output_file_path):
    file_util.make_parent_dirs(output_file_path)
    model_state_dict =\
        model.module.state_dict() if isinstance(model, DistributedDataParallel) else model.state_dict()
    main_util.save_on_master({'model': model_state_dict, 'optimizer': optimizer.state_dict(), 'best_value': best_value,
                              'lr_scheduler': lr_scheduler.state_dict(), 'config': config, 'args': args},
                             output_file_path)


def distill_one_epoch(distillation_box, train_data_loader, optimizer, device, epoch, interval, use_apex=False):
    metric_logger = MetricLogger(delimiter='  ')
    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value}'))
    metric_logger.add_meter('img/s', SmoothedValue(window_size=10, fmt='{value}'))
    header = 'Epoch: [{}]'.format(epoch)
    for sample_batch, targets in metric_logger.log_every(train_data_loader, interval, header):
        start_time = time.time()
        sample_batch, targets = sample_batch.to(device), targets.to(device)
        loss = distillation_box(sample_batch, targets)
        optimizer.zero_grad()
        if use_apex:
            with amp.scale_loss(loss, optimizer) as scaled_loss:
                scaled_loss.backward()
        else:
            loss.backward()
        optimizer.step()

        batch_size = sample_batch.shape[0]
        metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])
        metric_logger.meters['img/s'].update(batch_size / (time.time() - start_time))


@torch.no_grad()
def evaluate(model, data_loader, device, interval=1000, split_name='Test', title=None):
    if title is not None:
        print(title)

    num_threads = torch.get_num_threads()
    torch.set_num_threads(1)
    model.eval()
    metric_logger = MetricLogger(delimiter='  ')
    header = '{}:'.format(split_name)
    with torch.no_grad():
        for image, target in metric_logger.log_every(data_loader, interval, header):
            image = image.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)
            output = model(image)

            acc1, acc5 = main_util.compute_accuracy(output, target, topk=(1, 5))
            # FIXME need to take into account that the datasets
            # could have been padded in distributed setup
            batch_size = image.shape[0]
            metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)
            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    top1_accuracy = metric_logger.acc1.global_avg
    top5_accuracy = metric_logger.acc5.global_avg
    print(' * Acc@1 {:.4f}\tAcc@5 {:.4f}\n'.format(top1_accuracy, top5_accuracy))
    torch.set_num_threads(num_threads)
    return metric_logger.acc1.global_avg


def distill(teacher_model, student_model, train_data_loader, val_data_loader, device,
            distributed, start_epoch, config, args):
    print('Start knowledge distillation')
    train_config = config['train']
    distillation_box = DistillationBox(teacher_model, student_model, train_config['criterion'])
    ckpt_file_path = config['mimic_model']['ckpt']
    optim_config = train_config['optimizer']
    optimizer = func_util.get_optimizer(student_model, optim_config['type'], optim_config['params'])
    scheduler_config = train_config['scheduler']
    lr_scheduler = func_util.get_scheduler(optimizer, scheduler_config['type'], scheduler_config['params'])
    best_val_top1_accuracy = 0.0
    if file_util.check_if_exists(ckpt_file_path):
        best_val_map, _, _ = load_ckpt(ckpt_file_path, optimizer=optimizer, lr_scheduler=lr_scheduler)

    interval = train_config['interval']
    if interval <= 0:
        num_batches = len(train_data_loader)
        interval = num_batches // 20 if num_batches >= 20 else 1

    student_model_without_ddp = \
        student_model.module if isinstance(student_model, DistributedDataParallel) else student_model
    start_time = time.time()
    for epoch in range(start_epoch, train_config['epoch']):
        if distributed:
            train_data_loader.sampler.set_epoch(epoch)

        teacher_model.eval()
        student_model.train()
        distill_one_epoch(distillation_box, train_data_loader, optimizer, device, epoch, interval, args.apex)
        val_top1_accuracy =\
            evaluate(student_model, val_data_loader, device=device, interval=interval, split_name='Validation')
        if val_top1_accuracy > best_val_top1_accuracy and main_util.is_main_process():
            print('Updating ckpt (Best top1 accuracy: {:.4f} -> {:.4f})'.format(best_val_top1_accuracy,
                                                                                val_top1_accuracy))
            best_val_top1_accuracy = val_top1_accuracy
            save_ckpt(student_model_without_ddp, optimizer, lr_scheduler,
                      best_val_top1_accuracy, config, args, ckpt_file_path)
        lr_scheduler.step()

    dist.barrier()
    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print('Training time {}'.format(total_time_str))


def main(args):
    if args.apex:
        if sys.version_info < (3, 0):
            raise RuntimeError('Apex currently only supports Python 3. Aborting.')
        if amp is None:
            raise RuntimeError('Failed to import apex. Please install apex from https://www.github.com/nvidia/apex '
                               'to enable mixed-precision training.')

    distributed, device_ids = main_util.init_distributed_mode(args.world_size, args.dist_url)
    print(args)
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True

    config = yaml_util.load_yaml_file(args.config)
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    dataset_config = config['dataset']
    input_shape = config['input_shape']
    train_config = config['train']
    test_config = config['test']
    train_data_loader, val_data_loader, test_data_loader =\
        dataset_util.get_data_loaders(dataset_config, batch_size=train_config['batch_size'],
                                      rough_size=train_config['rough_size'], reshape_size=input_shape[1:3],
                                      jpeg_quality=-1, test_batch_size=test_config['batch_size'],
                                      distributed=distributed)

    teacher_model_config = config['teacher_model']
    teacher_model, teacher_model_type = mimic_util.get_org_model(teacher_model_config, device)
    module_util.freeze_module_params(teacher_model)

    student_model = mimic_util.get_mimic_model_easily(config, device)
    student_model_config = config['mimic_model']

    optim_config = train_config['optimizer']
    optimizer = func_util.get_optimizer(student_model, optim_config['type'], optim_config['params'])
    use_apex = args.apex
    if use_apex:
        student_model, optimizer = amp.initialize(student_model, optimizer, opt_level=args.apex_opt_level)

    if distributed:
        teacher_model = DataParallel(teacher_model, device_ids=device_ids)
        student_model = DistributedDataParallel(student_model, device_ids=device_ids)

    start_epoch = args.start_epoch
    if not args.test_only:
        distill(teacher_model, student_model, train_data_loader, val_data_loader, device,
                distributed, start_epoch, config, args)
        student_model_without_ddp =\
            student_model.module if isinstance(student_model, DistributedDataParallel) else student_model
        load_ckpt(student_model_config['ckpt'], model=student_model_without_ddp, strict=True)

    if not args.student_only:
        evaluate(teacher_model, test_data_loader, device, title='[Teacher: {}]'.format(teacher_model_type))
    evaluate(student_model, test_data_loader, device, title='[Student: {}]'.format(student_model_config['type']))


if __name__ == '__main__':
    argparser = get_argparser()
    main(argparser.parse_args())
"
48,2201.02693,"import argparse
import datetime
import time

import torch
from torch import distributed as dist
from torch.backends import cudnn
from torch.nn import DataParallel
from torch.nn.parallel.distributed import DistributedDataParallel

from myutils.common import file_util, yaml_util
from myutils.pytorch import func_util, module_util
from structure.logger import MetricLogger, SmoothedValue
from utils import ae_util, main_util


def get_argparser():
    argparser = argparse.ArgumentParser(description='PyTorch autoencoder trainer')
    argparser.add_argument('--config', required=True, help='yaml file path')
    argparser.add_argument('--device', default='cuda', help='device')
    argparser.add_argument('-test_only', action='store_true', help='only test model')
    argparser.add_argument('-extended_only', action='store_true', help='test extended model only')
    # distributed training parameters
    argparser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')
    argparser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')
    return argparser


def resume_from_ckpt(ckpt_file_path, autoencoder):
    if not file_util.check_if_exists(ckpt_file_path):
        print('Autoencoder checkpoint was not found at {}'.format(ckpt_file_path))
        return 1, None

    print('Resuming from checkpoint..')
    checkpoint = torch.load(ckpt_file_path)
    state_dict = checkpoint['model']
    autoencoder.load_state_dict(state_dict)
    start_epoch = checkpoint['epoch']
    return start_epoch, checkpoint['best_value']


def train_epoch(autoencoder, head_model, train_loader, optimizer, criterion, epoch, device, interval):
    autoencoder.train()
    head_model.eval()
    metric_logger = MetricLogger(delimiter='  ')
    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value}'))
    metric_logger.add_meter('img/s', SmoothedValue(window_size=10, fmt='{value}'))
    header = 'Epoch: [{}]'.format(epoch)
    for sample_batch, targets in metric_logger.log_every(train_loader, interval, header):
        start_time = time.time()
        sample_batch = sample_batch.to(device)
        optimizer.zero_grad()
        head_outputs = head_model(sample_batch)
        ae_outputs = autoencoder(head_outputs)
        loss = criterion(ae_outputs, head_outputs) if not isinstance(ae_outputs, tuple) else ae_outputs[1]
        loss.backward()
        optimizer.step()
        batch_size = sample_batch.shape[0]
        metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])
        metric_logger.meters['img/s'].update(batch_size / (time.time() - start_time))


@torch.no_grad()
def evaluate(model, data_loader, device, interval=1000, split_name='Test', title=None):
    if title is not None:
        print(title)

    num_threads = torch.get_num_threads()
    torch.set_num_threads(1)
    model.eval()
    metric_logger = MetricLogger(delimiter='  ')
    header = '{}:'.format(split_name)
    with torch.no_grad():
        for image, target in metric_logger.log_every(data_loader, interval, header):
            image = image.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)
            output = model(image)

            acc1, acc5 = main_util.compute_accuracy(output, target, topk=(1, 5))
            # FIXME need to take into account that the datasets
            # could have been padded in distributed setup
            batch_size = image.shape[0]
            metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)
            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    top1_accuracy = metric_logger.acc1.global_avg
    top5_accuracy = metric_logger.acc5.global_avg
    print(' * Acc@1 {:.4f}\tAcc@5 {:.4f}\n'.format(top1_accuracy, top5_accuracy))
    torch.set_num_threads(num_threads)
    return metric_logger.acc1.global_avg


def validate(ae_without_ddp, data_loader, config, device, distributed, device_ids):
    input_shape = config['input_shape']
    extended_model, _ = ae_util.get_extended_model(ae_without_ddp, config, input_shape, device, True)
    if distributed:
        extended_model = DistributedDataParallel(extended_model, device_ids=device_ids)
    return evaluate(extended_model, data_loader, device, split_name='Validation')


def save_ckpt(autoencoder, epoch, best_avg_loss, ckpt_file_path, ae_type):
    print('Saving..')
    module = autoencoder.module if isinstance(autoencoder, (DistributedDataParallel, DataParallel)) else autoencoder
    state = {
        'type': ae_type,
        'model': module.state_dict(),
        'epoch': epoch + 1,
        'best_value': best_avg_loss
    }
    file_util.make_parent_dirs(ckpt_file_path)
    torch.save(state, ckpt_file_path)


def train(train_loader, valid_loader, input_shape, config, device, distributed, device_ids):
    ae_without_ddp, ae_type = ae_util.get_autoencoder(config, device)
    head_model = ae_util.get_head_model(config, input_shape, device)
    module_util.freeze_module_params(head_model)
    ckpt_file_path = config['autoencoder']['ckpt']
    start_epoch, best_valid_acc = resume_from_ckpt(ckpt_file_path, ae_without_ddp)
    if best_valid_acc is None:
        best_valid_acc = 0.0

    train_config = config['train']
    criterion_config = train_config['criterion']
    criterion = func_util.get_loss(criterion_config['type'], criterion_config['params'])
    optim_config = train_config['optimizer']
    optimizer = func_util.get_optimizer(ae_without_ddp, optim_config['type'], optim_config['params'])
    scheduler_config = train_config['scheduler']
    scheduler = func_util.get_scheduler(optimizer, scheduler_config['type'], scheduler_config['params'])
    interval = train_config['interval']
    if interval <= 0:
        num_batches = len(train_loader)
        interval = num_batches // 20 if num_batches >= 20 else 1

    autoencoder = ae_without_ddp
    if distributed:
        autoencoder = DistributedDataParallel(ae_without_ddp, device_ids=device_ids)
        head_model = DataParallel(head_model, device_ids=device_ids)
    elif device.type == 'cuda':
        autoencoder = DataParallel(ae_without_ddp)
        head_model = DataParallel(head_model)

    end_epoch = start_epoch + train_config['epoch']
    start_time = time.time()
    for epoch in range(start_epoch, end_epoch):
        if distributed:
            train_loader.sampler.set_epoch(epoch)

        train_epoch(autoencoder, head_model, train_loader, optimizer, criterion, epoch, device, interval)
        valid_acc = validate(ae_without_ddp, valid_loader, config, device, distributed, device_ids)
        if valid_acc > best_valid_acc and main_util.is_main_process():
            print('Updating ckpt (Best top1 accuracy: {:.4f} -> {:.4f})'.format(best_valid_acc, valid_acc))
            best_valid_acc = valid_acc
            save_ckpt(ae_without_ddp, epoch, best_valid_acc, ckpt_file_path, ae_type)
        scheduler.step()

    dist.barrier()
    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print('Training time {}'.format(total_time_str))
    del head_model


def run(args):
    distributed, device_ids = main_util.init_distributed_mode(args.world_size, args.dist_url)
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    if device.type == 'cuda':
        cudnn.benchmark = True

    print(args)
    config = yaml_util.load_yaml_file(args.config)
    input_shape = config['input_shape']
    ckpt_file_path = config['autoencoder']['ckpt']
    train_loader, valid_loader, test_loader = main_util.get_data_loaders(config, distributed)
    if not args.test_only:
        train(train_loader, valid_loader, input_shape, config, device, distributed, device_ids)

    autoencoder, _ = ae_util.get_autoencoder(config, device)
    resume_from_ckpt(ckpt_file_path, autoencoder)
    extended_model, model = ae_util.get_extended_model(autoencoder, config, input_shape, device)
    if not args.extended_only:
        if device.type == 'cuda':
            model = DistributedDataParallel(model, device_ids=device_ids) if distributed \
                else DataParallel(model)
        evaluate(model, test_loader, device, title='[Original model]')

    if device.type == 'cuda':
        extended_model = DistributedDataParallel(extended_model, device_ids=device_ids) if distributed \
            else DataParallel(extended_model)

    evaluate(extended_model, test_loader, device, title='[Mimic model]')


if __name__ == '__main__':
    parser = get_argparser()
    run(parser.parse_args())
"
49,2201.02693,"import torchvision.transforms as transforms


def convert2type_list(str_var, delimiter, var_type):
    return list(map(var_type, str_var.split(delimiter)))


def convert2type_range(str_var, delimiter, var_type):
    return range(*convert2type_list(str_var, delimiter, var_type))


def build_normalizer(dataset, mean=None, std=None):
    if mean is not None and std is not None:
        return transforms.Normalize(mean=mean, std=std)
    return transforms.Normalize(mean=dataset.mean(axis=(0, 1, 2)) / 255, std=dataset.std(axis=(0, 1, 2)) / 255)
"
50,2201.02693,"from utils import module_util


def wrap_all_child_modules(model, wrapper_class, member_name=None, member_module=None, wrapped_list=list(), **kwargs):
    named_children = model.named_children() if member_module is None else member_module.named_children()
    named_children = list(named_children)
    if not named_children and member_name is not None and member_module is not None:
        wrapped_module = wrapper_class(member_module, **kwargs)
        setattr(model, member_name, wrapped_module)
        wrapped_list.append(wrapped_module)
        return

    parent = model if member_module is None else member_module
    for name, child_module in named_children:
        wrap_all_child_modules(parent, wrapper_class, name, child_module, wrapped_list, **kwargs)


def wrap_decomposable_modules(middle_module, wrapper_class, z, middle_name=None, upper_module=None,
                              wrapped_list=list(), first=True, **kwargs):
    named_children = list(middle_module.named_children())
    if not named_children and upper_module is not None:
        try:
            return middle_module(z), True
        except (RuntimeError, ValueError):
            try:
                return middle_module(z.view(z.size(0), -1)), True
            except RuntimeError:
                print('Error\t', type(middle_module).__name__)
        return z, False

    try:
        expected_z = middle_module(z)
    except (RuntimeError, ValueError):
        return z, False

    for name, child_module in named_children:
        z, flag = wrap_decomposable_modules(child_module, wrapper_class, z, name,
                                            middle_module, wrapped_list, False, **kwargs)

    named_children = list(middle_module.named_children())
    if flag and expected_z.size() == z.size() and expected_z.isclose(z).all().item() == 1:
        for name, child_module in named_children:
            no_wrap = True
            for m in child_module.children():
                if isinstance(m, wrapper_class):
                    no_wrap = False
                    break

            if not no_wrap or isinstance(child_module, wrapper_class):
                continue

            wrapped_module = wrapper_class(child_module, **kwargs)
            setattr(middle_module, name, wrapped_module)
    elif upper_module is not None:
        no_wrap = True
        for m in middle_module.children():
            if isinstance(m, wrapper_class):
                no_wrap = False
                break

        if no_wrap:
            wrapped_module = wrapper_class(middle_module, **kwargs)
            setattr(upper_module, middle_name, wrapped_module)

    if first:
        module_util.extract_target_modules(middle_module, wrapper_class, wrapped_list)
    return expected_z, True
"
51,2201.02693,"import builtins as __builtin__
import json
import os

import torch
import torch.distributed as dist
from utils import dataset_util


def overwrite_dict(org_dict, sub_dict):
    for sub_key, sub_value in sub_dict.items():
        if sub_key in org_dict:
            if isinstance(sub_value, dict):
                overwrite_dict(org_dict[sub_key], sub_value)
            else:
                org_dict[sub_key] = sub_value
        else:
            org_dict[sub_key] = sub_value


def overwrite_config(config, json_str):
    overwrite_dict(config, json.loads(json_str))


def setup_for_distributed(is_master):
    """"""
    This function disables printing when not in master process
    """"""
    builtin_print = __builtin__.print

    def print(*args, **kwargs):
        force = kwargs.pop('force', False)
        if is_master or force:
            builtin_print(*args, **kwargs)

    __builtin__.print = print


def init_distributed_mode(world_size=1, dist_url='env://'):
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        device_id = int(os.environ['LOCAL_RANK'])
    elif 'SLURM_PROCID' in os.environ:
        rank = int(os.environ['SLURM_PROCID'])
        device_id = rank % torch.cuda.device_count()
    else:
        print('Not using distributed mode')
        return False, None

    torch.cuda.set_device(device_id)
    dist_backend = 'nccl'
    print('| distributed init (rank {}): {}'.format(rank, dist_url), flush=True)
    torch.distributed.init_process_group(backend=dist_backend, init_method=dist_url,
                                         world_size=world_size, rank=rank)
    torch.distributed.barrier()
    setup_for_distributed(rank == 0)
    return True, [device_id]


def is_dist_avail_and_initialized():
    if not dist.is_available():
        return False
    if not dist.is_initialized():
        return False
    return True


def get_rank():
    if not is_dist_avail_and_initialized():
        return 0
    return dist.get_rank()


def save_on_master(*args, **kwargs):
    if is_main_process():
        torch.save(*args, **kwargs)


def is_main_process():
    return get_rank() == 0


def get_data_loaders(config, distributed):
    print('Loading data')
    dataset_config = config['dataset']
    train_config = config['train']
    test_config = config['test']
    compress_config = test_config.get('compression', dict())
    compress_type = compress_config.get('type', None)
    compress_size = compress_config.get('size', None)
    jpeg_quality = test_config.get('jquality', 0)
    dataset_name = dataset_config['name']
    if dataset_name.startswith('caltech') or dataset_name.startswith('imagenet'):
        return dataset_util.get_data_loaders(dataset_config, train_config['batch_size'],
                                             compress_type, compress_size, rough_size=train_config['rough_size'],
                                             reshape_size=config['input_shape'][1:3],
                                             test_batch_size=test_config['batch_size'], jpeg_quality=jpeg_quality,
                                             distributed=distributed)
    raise ValueError('dataset_name `{}` is not expected'.format(dataset_name))


def compute_accuracy(output, target, topk=(1,)):
    maxk = max(topk)
    batch_size = target.size(0)
    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target[None])
    acc_list = []
    for k in topk:
        correct_k = correct[:k].flatten().sum(dtype=torch.float32)
        acc_list.append(correct_k * (100.0 / batch_size))
    return acc_list
"
52,2201.02693,"import torch
from torch import nn

from models.classification.inception import Inception3
from models.mimic.densenet_mimic import DenseNetHeadMimic, DenseNetMimic
from models.mimic.inception_mimic import InceptionHeadMimic, InceptionMimic
from models.mimic.mobilenet_mimic import MobileNetHeadMimic, MobileNetMimic
from models.mimic.resnet_mimic import ResNet152HeadMimic, ResNetMimic
from myutils.common import file_util, yaml_util
from utils import mimic_util, module_util


def resume_from_ckpt(ckpt_file_path, model, is_student=False):
    if not file_util.check_if_exists(ckpt_file_path):
        print('{} checkpoint was not found at {}'.format(""Student"" if is_student else ""Teacher"", ckpt_file_path))
        if is_student:
            return 1, None
        return 1

    print('Resuming from checkpoint..')
    ckpt = torch.load(ckpt_file_path)
    state_dict = ckpt['model']
    if not is_student and isinstance(model, Inception3) or\
            (hasattr(model, 'module') and isinstance(model.module, Inception3)):
        for key in list(state_dict.keys()):
            if key.startswith('AuxLogits') or key.startswith('module.AuxLogits'):
                state_dict.pop(key)

    model.load_state_dict(state_dict)
    start_epoch = ckpt['epoch']
    if is_student:
        return start_epoch, ckpt['best_avg_loss'] if 'best_avg_loss' in ckpt else ckpt['best_valid_value']
    return start_epoch


def extract_teacher_model(model, input_shape, device, teacher_model_config):
    modules = list()
    module = model.module if isinstance(model, nn.DataParallel) else model
    module_util.extract_decomposable_modules(module, torch.rand(1, *input_shape).to(device), modules)
    start_idx = teacher_model_config['start_idx']
    end_idx = teacher_model_config['end_idx']
    return nn.Sequential(*modules[start_idx:end_idx]).to(device)


def get_teacher_model(teacher_model_config, input_shape, device):
    teacher_config = yaml_util.load_yaml_file(teacher_model_config['config'])
    model_config = teacher_config['model']
    if model_config['type'] == 'inception_v3':
        model_config['params']['aux_logits'] = False

    model = module_util.get_model(teacher_config, device)
    resume_from_ckpt(model_config['ckpt'], model)
    return extract_teacher_model(model, input_shape, device, teacher_model_config), model_config['type']


def get_student_model(teacher_model_type, student_model_config, dataset_name):
    student_model_type = student_model_config['type']
    student_model_version = student_model_config['version']
    params_config = student_model_config['params']
    if teacher_model_type.startswith('densenet')\
            and student_model_type in ['densenet169_head_mimic', 'densenet201_head_mimic']:
        return DenseNetHeadMimic(teacher_model_type, student_model_version, dataset_name, **params_config)
    elif teacher_model_type == 'inception_v3' and student_model_type == 'inception_v3_head_mimic':
        return InceptionHeadMimic(student_model_version, dataset_name, **params_config)
    elif teacher_model_type.startswith('resnet') and student_model_type == 'resnet152_head_mimic':
        return ResNet152HeadMimic(student_model_version, dataset_name, **params_config)
    elif teacher_model_type == 'mobilenet_v2' and student_model_type == 'mobilenet_v2_head_mimic':
        return MobileNetHeadMimic(student_model_version, **params_config)
    raise ValueError('teacher_model_type `{}` is not expected'.format(teacher_model_type))


def load_student_model(config, teacher_model_type, device):
    student_model_config = config['student_model']
    student_model = get_student_model(teacher_model_type, student_model_config, config['dataset']['name'])
    student_model = student_model.to(device)
    resume_from_ckpt(student_model_config['ckpt'], student_model, True)
    return student_model


def get_org_model(teacher_model_config, device):
    teacher_config = yaml_util.load_yaml_file(teacher_model_config['config'])
    if teacher_config['model']['type'] == 'inception_v3':
        teacher_config['model']['params']['aux_logits'] = False

    model = module_util.get_model(teacher_config, device)
    model_config = teacher_config['model']
    resume_from_ckpt(model_config['ckpt'], model)
    return model, model_config['type']


def get_tail_network(config, tail_modules):
    mimic_model_config = config['mimic_model']
    mimic_type = mimic_model_config['type']
    if mimic_type.startswith('densenet'):
        return DenseNetMimic(None, tail_modules)
    elif mimic_type.startswith('inception'):
        return InceptionMimic(None, tail_modules)
    elif mimic_type.startswith('resnet'):
        return ResNetMimic(None, tail_modules)
    elif mimic_type.startswith('mobilenet'):
        return MobileNetMimic(None, tail_modules)
    raise ValueError('mimic_type `{}` is not expected'.format(mimic_type))


def get_mimic_model(config, org_model, teacher_model_type, teacher_model_config, device, head_model=None):
    target_model = org_model.module if isinstance(org_model, nn.DataParallel) else org_model
    student_model =\
        load_student_model(config, teacher_model_type, device) if head_model is None else head_model.to(device)
    org_modules = list()
    input_batch = torch.rand(config['input_shape']).unsqueeze(0).to(device)
    module_util.extract_decomposable_modules(target_model, input_batch, org_modules)
    end_idx = teacher_model_config['end_idx']
    tail_modules = org_modules[end_idx:]
    mimic_model_config = config['mimic_model']
    mimic_type = mimic_model_config['type']
    if mimic_type.startswith('densenet'):
        mimic_model = DenseNetMimic(student_model, tail_modules)
    elif mimic_type.startswith('inception'):
        mimic_model = InceptionMimic(student_model, tail_modules)
    elif mimic_type.startswith('resnet'):
        mimic_model = ResNetMimic(student_model, tail_modules)
    elif mimic_type.startswith('mobilenet'):
        mimic_model = MobileNetMimic(student_model, tail_modules)
    else:
        raise ValueError('mimic_type `{}` is not expected'.format(mimic_type))
    return mimic_model.to(device)


def get_mimic_model_easily(config, device=torch.device('cpu')):
    teacher_model_config = config['teacher_model']
    org_model, teacher_model_type = get_org_model(teacher_model_config, device)
    return get_mimic_model(config, org_model, teacher_model_type, teacher_model_config, device)
"
53,2201.02693,"import torch
from torch import nn
from torch.nn import DataParallel
from torch.nn.parallel.distributed import DistributedDataParallel

from models.autoencoder.base import BaseExtendedModel
from models.autoencoder.input_ae import InputAutoencoder, InputVAE
from models.autoencoder.middle_ae import MiddleAutoencoder
from myutils.common import yaml_util
from utils import module_util


def get_autoencoder(config, device=None, is_static=False):
    autoencoder = None
    ae_config = config['autoencoder']
    ae_type = ae_config['type']
    if ae_type == 'input_ae':
        autoencoder = InputAutoencoder(**ae_config['params'])
    elif ae_type == 'input_vae':
        autoencoder = InputVAE(**ae_config['params'], is_static=is_static)
    elif ae_type == 'middle_ae':
        autoencoder = MiddleAutoencoder(**ae_config['params'])

    if autoencoder is None:
        raise ValueError('ae_type `{}` is not expected'.format(ae_type))

    if device is None:
        return autoencoder, ae_type

    autoencoder = autoencoder.to(device)
    return autoencoder, ae_type


def extract_head_model(model, input_shape, device, partition_idx):
    if partition_idx is None or partition_idx == 0:
        return nn.Sequential()

    modules = list()
    module = model.module if isinstance(model, (DataParallel, DistributedDataParallel)) else model
    module_util.extract_decomposable_modules(module, torch.rand(1, *input_shape).to(device), modules)
    return nn.Sequential(*modules[:partition_idx]).to(device)


def get_head_model(config, input_shape, device):
    org_model_config = config['org_model']
    model_config = yaml_util.load_yaml_file(org_model_config['config'])
    sub_model_config = model_config['model']
    if sub_model_config['type'] == 'inception_v3':
        sub_model_config['params']['aux_logits'] = False

    model = module_util.get_model(model_config, device)
    module_util.resume_from_ckpt(model, sub_model_config, False)
    return extract_head_model(model, input_shape, device, org_model_config['partition_idx'])


def extend_model(autoencoder, model, input_shape, device, partition_idx, skip_bottleneck_size):
    if partition_idx is None or partition_idx == 0:
        return nn.Sequential(autoencoder, model)

    modules = list()
    module = model.module if isinstance(model, (DataParallel, DistributedDataParallel)) else model
    x = torch.rand(1, *input_shape).to(device)
    module_util.extract_decomposable_modules(module, x, modules)
    extended_model = BaseExtendedModel(modules[:partition_idx], autoencoder, modules[partition_idx:]).to(device)
    if not skip_bottleneck_size:
        extended_model.compute_ae_bottleneck_size(x, True)
    return extended_model


def get_extended_model(autoencoder, config, input_shape, device, skip_bottleneck_size=False):
    org_model_config = config['org_model']
    model_config = yaml_util.load_yaml_file(org_model_config['config'])
    sub_model_config = model_config['model']
    if sub_model_config['type'] == 'inception_v3':
        sub_model_config['params']['aux_logits'] = False

    model = module_util.get_model(model_config, device)
    module_util.resume_from_ckpt(model, sub_model_config, False)
    return extend_model(autoencoder, model, input_shape, device,
                        org_model_config['partition_idx'], skip_bottleneck_size), model
"
54,2201.02693,"import os


def check_if_plottable():
    return os.environ.get('DISPLAY', '') != ''
"
55,2201.02693,"import multiprocessing

import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from torch.utils.data.distributed import DistributedSampler
from torchvision import transforms

from structure.dataset import AdvRgbImageDataset
from utils import data_util


def get_test_transformer(dataset_name, normalizer, compression_type, compressed_size, org_size):
    normal_list = [transforms.CenterCrop(org_size)] if dataset_name == 'imagenet' else []
    normal_list.append(transforms.ToTensor())
    if normalizer is not None:
        normal_list.append(normalizer)

    normal_transformer = transforms.Compose(normal_list)
    if compression_type is None or compressed_size is None:
        return normal_transformer

    if compression_type == 'base':
        comp_list = [transforms.Resize(compressed_size), transforms.Resize(org_size), transforms.ToTensor()]
        if normalizer is not None:
            comp_list.append(normalizer)
        return transforms.Compose(comp_list)
    return normal_transformer


def get_data_loaders(dataset_config, batch_size=100, compression_type=None, compressed_size=None, normalized=True,
                     rough_size=None, reshape_size=(224, 224), test_batch_size=1, jpeg_quality=0, distributed=False):
    data_config = dataset_config['data']
    dataset_name = dataset_config['name']
    train_file_path = data_config['train']
    valid_file_path = data_config['valid']
    test_file_path = data_config['test']
    normalizer_config = dataset_config['normalizer']
    mean = normalizer_config['mean']
    std = normalizer_config['std']
    train_dataset = AdvRgbImageDataset(train_file_path, reshape_size)
    normalizer = data_util.build_normalizer(train_dataset.load_all_data() if mean is None or std is None else None,
                                            mean, std) if normalized else None
    train_comp_list = [transforms.Resize(rough_size), transforms.RandomCrop(reshape_size)]\
        if rough_size is not None else list()
    train_comp_list.extend([transforms.RandomHorizontalFlip(), transforms.ToTensor()])
    valid_comp_list = [transforms.ToTensor()]
    if normalizer is not None:
        train_comp_list.append(normalizer)
        valid_comp_list.append(normalizer)

    pin_memory = torch.cuda.is_available()
    num_cpus = multiprocessing.cpu_count()
    num_workers = data_config.get('num_workers', 0 if num_cpus == 1 else min(num_cpus, 8))
    train_transformer = transforms.Compose(train_comp_list)
    valid_transformer = transforms.Compose(valid_comp_list)
    test_transformer = get_test_transformer(dataset_name, normalizer, compression_type, compressed_size, reshape_size)
    train_dataset = AdvRgbImageDataset(train_file_path, reshape_size, train_transformer)
    eval_reshape_size = rough_size if dataset_name == 'imagenet' else reshape_size
    if dataset_name == 'imagenet':
        valid_transformer = test_transformer
    
    valid_dataset = AdvRgbImageDataset(valid_file_path, eval_reshape_size, valid_transformer)
    test_dataset = AdvRgbImageDataset(test_file_path, eval_reshape_size, test_transformer, jpeg_quality)

    if distributed:
        train_sampler = DistributedSampler(train_dataset)
        valid_sampler = DistributedSampler(valid_dataset)
        test_sampler = DistributedSampler(test_dataset)
    else:
        train_sampler = RandomSampler(train_dataset)
        valid_sampler = SequentialSampler(valid_dataset)
        test_sampler = SequentialSampler(test_dataset)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,
                              num_workers=num_workers, pin_memory=pin_memory)
    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, sampler=valid_sampler,
                              num_workers=num_workers, pin_memory=pin_memory)
    if 1 <= test_dataset.jpeg_quality <= 100:
        test_dataset.compute_compression_rate()

    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, sampler=test_sampler,
                             num_workers=num_workers, pin_memory=pin_memory)
    return train_loader, valid_loader, test_loader
"
56,2201.02693,
57,2201.02693,"import torch
import torchvision
from torch import nn

from models.classification.alexnet import AlexNet
from models.classification.densenet import densenet_model
from models.classification.inception import inception_v3
from models.classification.lenet5 import LeNet5
from models.classification.mobilenet import mobilenet_model
from models.classification.resnet import resnet_model
from myutils.common import file_util


def use_multiple_gpus_if_available(model, device):
    return nn.DataParallel(model) if device.type == 'cuda' else model


def get_model(config, device=None):
    model = None
    model_config = config['model']
    model_type = model_config['type']
    if model_type == 'alexnet':
        model = AlexNet(**model_config['params'])
    elif model_type.startswith('densenet'):
        model = densenet_model(model_type, model_config['params'], model_config['pretrained'])
    elif model_type == 'lenet5':
        model = LeNet5(**model_config['params'])
    elif model_type.startswith('resnet'):
        model = resnet_model(model_type, model_config['params'], model_config['pretrained'])
    elif model_type.startswith('mobilenet'):
        model = mobilenet_model(model_type, model_config['params'], model_config['pretrained'])
    elif model_type.startswith('inception_v3'):
        model = inception_v3(model_config['pretrained'], **model_config['params'])
    elif model_type in torchvision.models.__dict__:
        model = torchvision.models.__dict__[model_type](**model_config['params'])

    if model is None:
        raise ValueError('model_type `{}` is not expected'.format(model_type))
    elif device is None:
        return model
    return model.to(device)


def resume_from_ckpt(model, model_config, init):
    ckpt_file_path = model_config['ckpt']
    if init or not file_util.check_if_exists(ckpt_file_path):
        return model_config['type'], 0, 1, ckpt_file_path

    print('Resuming from checkpoint..')
    checkpoint = torch.load(ckpt_file_path)
    model.load_state_dict(checkpoint['model'])
    model_type = checkpoint['type']
    best_acc = checkpoint['acc']
    start_epoch = checkpoint['epoch']
    return model_type, best_acc, start_epoch, ckpt_file_path


def extract_target_modules(parent_module, target_class, module_list):
    if isinstance(parent_module, target_class):
        module_list.append(parent_module)

    child_modules = list(parent_module.children())
    for child_module in child_modules:
        extract_target_modules(child_module, target_class, module_list)


def extract_all_child_modules(parent_module, module_list, extract_designed_module=True):
    child_modules = list(parent_module.children())
    if not child_modules or (not extract_designed_module and len(module_list) > 0 and
                             type(parent_module) != nn.Sequential):
        module_list.append(parent_module)
        return

    for child_module in child_modules:
        extract_all_child_modules(child_module, module_list, extract_designed_module)


def extract_decomposable_modules(parent_module, z, module_list, output_size_list=list(), first=True, exception_size=-1):
    parent_module = parent_module.eval().cpu()
    z = z.cpu()
    child_modules = list(parent_module.children())
    if not child_modules:
        module_list.append(parent_module)
        try:
            z = parent_module(z)
            output_size_list.append([*z.size()])
            return z, True
        except (RuntimeError, ValueError):
            try:
                z = parent_module(z.view(z.size(0), exception_size))
                output_size_list.append([*z.size()])
                return z, True
            except RuntimeError:
                ValueError('Error\t', type(parent_module).__name__)
        return z, False

    try:
        expected_z = parent_module(z)
    except (RuntimeError, ValueError):
        try:
            resized_z = z.view(z.size(0), exception_size)
            expected_z = parent_module(resized_z)
            z = resized_z
        except RuntimeError:
            ValueError('Error\t', type(parent_module).__name__)
            return z, False

    submodule_list = list()
    sub_output_size_list = list()
    decomposable = True
    for child_module in child_modules:
        z, decomposable = extract_decomposable_modules(child_module, z, submodule_list, sub_output_size_list, False)
        if not decomposable:
            break

    if decomposable and expected_z.size() == z.size() and expected_z.allclose(z):
        module_list.extend(submodule_list)
        output_size_list.extend(sub_output_size_list)
        return expected_z, True

    if not first:
        module_list.append(parent_module)
        output_size_list.append([*expected_z.size()])
    return expected_z, True


def count_params(model):
    return sum(param.numel() for param in model.parameters())
"
58,2201.02693,"import matplotlib.pyplot as plt
import numpy as np
import torch
from torch import nn

from utils import module_util


def calc_sequential_feature_size(sequential, input_shape):
    input_data = torch.rand(input_shape).unsqueeze(0)
    return np.prod(sequential(input_data).unsqueeze(0).size())


def convert2kb(data_size_list, bit=32):
    return np.array(data_size_list) * bit / (8 * 1024)


def convert2accumulated(op_count_list):
    return np.array([sum(op_count_list[0:i + 1]) for i in range(len(op_count_list))])


def format_metrics(data_size_list, op_count_list, scaled):
    data_sizes = convert2kb(data_size_list)
    data_size_label = 'Data Size [kB]'
    accum_complexities = convert2accumulated(op_count_list)
    accum_complexity_label = 'Accumulated Complexity'
    if scaled:
        data_sizes /= data_sizes[0]
        data_size_label = 'Scaled Data Size'
        accum_complexities /= accum_complexities[-1]
        accum_complexity_label = 'Scaled Accumulated Complexity'
    return data_sizes, accum_complexities, data_size_label, accum_complexity_label


def plot_model_complexity(xs, op_count_list, layer_list, model_name):
    plt.semilogy(xs[1:], op_count_list, label=model_name)
    plt.xticks(xs[1:], layer_list[1:], rotation=90, fontsize=12)
    plt.yticks(fontsize=12)
    plt.xlim(xs[1] - 1, xs[-1] + 1)
    plt.xlabel('Layer', fontsize=14)
    plt.ylabel('Complexity', fontsize=14)
    plt.legend(fontsize=13)
    plt.tight_layout()
    plt.show()


def plot_accumulated_model_complexity(xs, accumulated_op_counts, layer_list, accum_complexity_label, model_name):
    plt.plot(xs[1:], accumulated_op_counts, label=model_name)
    plt.xticks(xs[1:], layer_list[1:], rotation=90, fontsize=12)
    plt.xlim(xs[1] - 1, xs[-1] + 1)
    plt.xlabel('Layer', fontsize=14)
    plt.yticks(fontsize=12)
    plt.ylabel(accum_complexity_label, fontsize=14)
    plt.legend(fontsize=13)
    plt.tight_layout()
    plt.show()


def plot_model_data_size(xs, data_sizes, layer_list, data_size_label, model_name):
    plt.semilogy(xs, data_sizes, label=model_name)
    plt.semilogy(xs, [data_sizes[0] for x in xs], '-', label='Input')
    plt.xticks(xs, layer_list, rotation=90, fontsize=12)
    plt.yticks(fontsize=12)
    plt.xlim(xs[0] - 1, xs[-1] + 1)
    plt.xlabel('Layer', fontsize=14)
    plt.ylabel(data_size_label, fontsize=14)
    plt.legend(fontsize=13)
    plt.tight_layout()
    plt.show()


def plot_data_size_vs_model_complexity(data_sizes, op_count_list, data_size_label, model_name):
    plt.scatter(data_sizes[1:], op_count_list, label=model_name)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.yscale('log')
    plt.xlabel(data_size_label, fontsize=14)
    plt.ylabel('Complexity', fontsize=14)
    plt.legend(fontsize=13)
    plt.tight_layout()
    plt.show()


def plot_accumulated_model_complexity_vs_data_size(accumulated_op_counts, data_sizes,
                                                   data_size_label, accum_complexity_label, model_name):
    plt.plot(accumulated_op_counts, data_sizes[1:], marker='o', label=model_name)
    plt.plot(accumulated_op_counts, [data_sizes[0] for x in accumulated_op_counts], '-', label='Input')
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.xlabel(accum_complexity_label, fontsize=14)
    plt.ylabel(data_size_label, fontsize=14)
    plt.legend(fontsize=13)
    plt.tight_layout()
    plt.show()


def plot_accumulated_model_complexity_and_data_size(xs, accumulated_op_counts, data_sizes, layer_list,
                                                    data_size_label, accum_complexity_label):
    fig, ax1 = plt.subplots()
    ax1.semilogy(xs, data_sizes, '-')
    ax1.set_xticks(xs)
    ax1.set_xlim(xs[0] - 1, xs[-1] + 1)
    ax1.set_xticklabels(layer_list, fontsize=12)
    ax1.set_xlabel('Layer', fontsize=14)
    ax1.set_ylabel(data_size_label, color='b', fontsize=14)
    for tick in ax1.get_xticklabels():
        tick.set_rotation(90)
        tick.set_fontsize(12)

    for tick in ax1.get_yticklabels():
        tick.set_fontsize(12)

    ax2 = ax1.twinx()
    ax2.plot(xs[1:], accumulated_op_counts, 'r--')
    for tick in ax2.get_yticklabels():
        tick.set_fontsize(12)

    ax2.set_ylabel(accum_complexity_label, color='r', fontsize=14)
    plt.tight_layout()
    plt.show()


def plot_model_complexity_and_data_size(op_count_list, accum_complexities, data_sizes, layer_list,
                                        data_size_label, accum_complexity_label, model_name):
    print('Number of Operations: {:.5f}M'.format(sum(op_count_list) / 1e6))
    xs = np.arange(len(layer_list))
    plot_model_complexity(xs, op_count_list, layer_list, model_name)
    plot_accumulated_model_complexity(xs, accum_complexities, layer_list, accum_complexity_label, model_name)
    plot_model_data_size(xs, data_sizes, layer_list, data_size_label, model_name)
    plot_data_size_vs_model_complexity(data_sizes, op_count_list, data_size_label, model_name)
    plot_accumulated_model_complexity_vs_data_size(accum_complexities, data_sizes,
                                                   data_size_label, accum_complexity_label, model_name)
    plot_accumulated_model_complexity_and_data_size(xs, accum_complexities, data_sizes, layer_list,
                                                    data_size_label, accum_complexity_label)


def compute_layerwise_complexity_and_data_size(model, model_name, input_shape, scaled=False, plot=True):
    # Referred to https://zhuanlan.zhihu.com/p/33992733
    #  and https://github.com/sovrasov/flops-counter.pytorch/blob/master/ptflops/flops_counter.py
    multiply_adds = False
    op_count_list = list()
    data_size_list = list()
    layer_list = list()

    def conv_hook(self, input_batch, output_batch):
        batch_size, input_channels, input_height, input_width = input_batch[0].size()
        output_channels, output_height, output_width = output_batch[0].size()
        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups)\
                     * (2 if multiply_adds else 1)
        bias_ops = 1 if self.bias is not None else 0
        params = output_channels * (kernel_ops + bias_ops)
        op_size = batch_size * params * output_height * output_width
        op_count_list.append(op_size)
        data_size_list.append(np.prod(output_batch[0].size()))
        layer_list.append('{}: {}'.format(type(self).__name__, len(layer_list)))

    def deconv_hook(self, input_batch, output_batch):
        batch_size, input_channels, input_height, input_width = input_batch[0].size()
        kernel_height, kernel_width = self.kernel_size
        in_channels = self.in_channels
        out_channels = self.out_channels
        groups = self.groups
        filters_per_channel = out_channels // groups
        conv_per_position_flops = kernel_height * kernel_width * in_channels * filters_per_channel
        active_elements_count = batch_size * input_height * input_width
        overall_conv_flops = conv_per_position_flops * active_elements_count
        bias_flops = 0
        if self.bias is not None:
            output_height, output_width = output_batch.shape[2:]
            bias_flops = out_channels * batch_size * output_height * output_height

        op_size = overall_conv_flops + bias_flops
        op_count_list.append(op_size)
        data_size_list.append(np.prod(output_batch[0].size()))
        layer_list.append('{}: {}'.format(type(self).__name__, len(layer_list)))

    def linear_hook(self, input_batch, output_batch):
        batch_size = input_batch[0].size(0) if input_batch[0].dim() == 2 else 1
        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)
        bias_ops = self.bias.nelement()
        op_size = batch_size * (weight_ops + bias_ops)
        op_count_list.append(op_size)
        data_size_list.append(np.prod(output_batch[0].size()))
        layer_list.append('{}: {}'.format(type(self).__name__, len(layer_list)))

    def pooling_hook(self, input_batch, output_batch):
        batch_size, input_channels, input_height, input_width = input_batch[0].size()
        output_channels, output_height, output_width = output_batch[0].size()
        kernel_ops = self.kernel_size * self.kernel_size
        params = output_channels * kernel_ops
        op_size = batch_size * params * output_height * output_width
        op_count_list.append(op_size)
        data_size_list.append(np.prod(output_batch[0].size()))
        layer_list.append('{}: {}'.format(type(self).__name__, len(layer_list)))

    def simple_hook(self, input_batch, output_batch):
        op_size = input_batch[0].nelement()
        op_count_list.append(op_size)
        data_size_list.append(np.prod(output_batch[0].size()))
        layer_list.append('{}: {}'.format(type(self).__name__, len(layer_list)))

    def move_next_layer(net):
        children = list(net.children())
        if not children:
            if isinstance(net, nn.Conv2d):
                net.register_forward_hook(conv_hook)
            elif isinstance(net, nn.ConvTranspose2d):
                net.register_forward_hook(deconv_hook)
            elif isinstance(net, nn.Linear):
                net.register_forward_hook(linear_hook)
            elif isinstance(net, (nn.MaxPool2d, nn.AvgPool2d)):
                net.register_forward_hook(pooling_hook)
            elif isinstance(net, (nn.BatchNorm2d, nn.ReLU, nn.ReLU6, nn.Sigmoid,
                                  nn.LeakyReLU, nn.Dropout, nn.Softmax, nn.LogSoftmax)):
                net.register_forward_hook(simple_hook)
            else:
                print('Non-registered instance:', type(net))
                net.register_forward_hook(simple_hook)
            return

        for child in children:
            move_next_layer(child)

    move_next_layer(model)
    data_size_list.append(np.prod(input_shape))
    layer_list.append('Input')
    rand_input = torch.rand(input_shape).unsqueeze(0)
    model(rand_input)
    data_sizes, accum_complexities, data_size_label, accum_complexity_label =\
        format_metrics(data_size_list, op_count_list, scaled)
    if plot:
        plot_model_complexity_and_data_size(np.array(op_count_list), accum_complexities, data_sizes, layer_list,
                                            data_size_label, accum_complexity_label, model_name)
    return op_count_list, data_sizes, accum_complexities


def compute_model_complexity_and_data_size(model, model_name, input_shape, scaled=False, plot=True, **kwargs):
    submodules = list()
    output_sizes = list()
    module_util.extract_decomposable_modules(model, torch.rand(1, *input_shape), submodules, output_sizes, **kwargs)
    layer_list = ['Input']
    op_count_list = list()
    data_size_list = [np.prod(input_shape)]
    for i, submodule in enumerate(submodules):
        input_shape = input_shape if i == 0 else output_sizes[i - 1][1:]\
            if not isinstance(submodule, nn.Linear) else output_sizes[i - 1][1]
        module_name = '{}: {}'.format(type(submodule).__name__, i + 1).replace('_', ' ')
        layer_list.append(module_name)
        sub_op_counts, sub_data_sizes, _ =\
            compute_layerwise_complexity_and_data_size(submodule, module_name, input_shape, scaled=False, plot=False)
        op_count_list.append(sum(sub_op_counts))
        data_size_list.append(np.prod(output_sizes[i][1:]))

    data_sizes, accum_complexities, data_size_label, accum_complexity_label =\
        format_metrics(data_size_list, op_count_list, scaled)
    if plot:
        plot_model_complexity_and_data_size(np.array(op_count_list), accum_complexities, data_sizes, layer_list,
                                            data_size_label, accum_complexity_label, model_name)
    return op_count_list, data_sizes, accum_complexities


def plot_model_complexities(op_counts_list, model_type_list):
    for i in range(len(op_counts_list)):
        op_counts = op_counts_list[i]
        plt.semilogy(list(range(len(op_counts))), op_counts, label=model_type_list[i])

    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.xlabel('Layer', fontsize=14)
    plt.ylabel('Complexity', fontsize=14)
    plt.legend(fontsize=13)
    plt.tight_layout()
    plt.show()


def plot_accumulated_model_complexities(accum_complexities_list, model_type_list):
    for i in range(len(accum_complexities_list)):
        accum_complexities = accum_complexities_list[i]
        plt.plot(list(range(len(accum_complexities))), accum_complexities, label=model_type_list[i])

    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.xlabel('Layer', fontsize=14)
    plt.ylabel('Accumulated Complexity', fontsize=14)
    plt.legend(fontsize=13)
    plt.tight_layout()
    plt.show()


def plot_model_data_sizes(data_sizes_list, scaled, model_type_list):
    max_length = 0
    for i in range(len(data_sizes_list)):
        data_sizes = data_sizes_list[i]
        plt.semilogy(list(range(len(data_sizes))), data_sizes, label=model_type_list[i])
        if len(data_sizes) > max_length:
            max_length = len(data_sizes)

    xs = list(range(max_length))
    plt.semilogy(xs, [data_sizes[0] for _ in xs], '-', label='Input')
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.xlabel('Layer', fontsize=14)
    plt.ylabel('Scaled Data Size' if scaled else 'Data Size [kB]', fontsize=14)
    plt.legend(fontsize=13)
    plt.tight_layout()
    plt.show()


def plot_teacher_and_student_complexities(teacher_complexities, student_complexities, names=None):
    xs = np.array(list(range(len(teacher_complexities))))
    plt.bar(xs - 0.125, teacher_complexities, width=0.25, label='Teacher')
    plt.bar(xs + 0.125, student_complexities, width=0.25, label='Student')
    plt.xticks(xs, ['Ver.{}'.format(x + 1) for x in xs] if names is None else names, fontsize=12)
    plt.yticks(fontsize=12)
    plt.ylabel('Total complexity', fontsize=14)
    for i in range(len(teacher_complexities)):
        txt = '{:.1f}x\nfaster'.format(teacher_complexities[i] / student_complexities[i])
        plt.annotate(txt, (xs[i] + 0.05, student_complexities[i] + 10 ** int(np.log10(student_complexities[i])) / 5))

    plt.legend(fontsize=13)
    plt.tight_layout()
    plt.show()


def plot_bottleneck_data_size_vs_complexity(teacher_data_sizes, teacher_complexities,
                                            student_data_sizes, student_complexities, names=None):
    plt.scatter(teacher_data_sizes, teacher_complexities, label='Teacher')
    plt.scatter(student_data_sizes, student_complexities, label='Student')
    for i in range(len(teacher_data_sizes)):
        plt.arrow(teacher_data_sizes[i], teacher_complexities[i], student_data_sizes[i] - teacher_data_sizes[i],
                  student_complexities[i] - teacher_complexities[i], color='black', length_includes_head=True,
                  head_length=10 ** int(np.log10(student_complexities[i])), head_width=0.005, zorder=0)
        if names is not None:
            plt.annotate(names[i], (teacher_data_sizes[i] - 0.005,
                                    teacher_complexities[i] + 10 ** int(np.log10(teacher_complexities[i]) - 1)))

    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.xlabel('Scaled Bottleneck Data Size', fontsize=14)
    plt.ylabel('Total complexity', fontsize=14)
    plt.legend(fontsize=13)
    plt.tight_layout()
    plt.show()
"
59,2201.02693,"from torch import nn
from torch.nn import DataParallel
from torch.nn.parallel.distributed import DistributedDataParallel
from myutils.pytorch import module_util
from tools.loss import KDLoss, get_single_loss, get_custom_loss


class DistillationBox(nn.Module):
    def __init__(self, teacher_model, student_model, criterion_config):
        super().__init__()
        self.teacher_model = teacher_model
        self.student_model = student_model
        self.target_module_pairs = list()

        def extract_output(self, input, output):
            self.__dict__['distillation_box']['output'] = output

        sub_terms_config = criterion_config.get('sub_terms', None)
        if sub_terms_config is not None:
            teacher_model_without_dp =\
                teacher_model.module if isinstance(teacher_model, DataParallel) else teacher_model
            student_model_without_ddp = \
                student_model.module if isinstance(student_model, DistributedDataParallel) else student_model
            for loss_name, loss_config in sub_terms_config.items():
                teacher_path, student_path = loss_config['ts_modules']
                self.target_module_pairs.append((teacher_path, student_path))
                teacher_module = module_util.get_module(teacher_model_without_dp, teacher_path)
                student_module = module_util.get_module(student_model_without_ddp, student_path)
                teacher_module.__dict__['distillation_box'] = {'loss_name': loss_name, 'path_from_root': teacher_path,
                                                               'is_teacher': True}
                student_module.__dict__['distillation_box'] = {'loss_name': loss_name, 'path_from_root': student_path,
                                                               'is_teacher': False}
                teacher_module.register_forward_hook(extract_output)
                student_module.register_forward_hook(extract_output)

        org_term_config = criterion_config['org_term']
        org_criterion_config = org_term_config['criterion']
        self.org_criterion = get_single_loss(org_criterion_config)
        self.org_factor = org_term_config['factor']
        self.criterion = get_custom_loss(criterion_config)
        self.use_teacher_output = isinstance(self.org_criterion, KDLoss)

    def forward(self, sample_batch, targets):
        teacher_outputs = self.teacher_model(sample_batch)
        student_outputs = self.student_model(sample_batch)
        # Model with auxiliary classifier returns multiple outputs
        if isinstance(student_outputs, (list, tuple)):
            org_loss_dict = dict()
            if self.use_teacher_output:
                for i, sub_student_outputs, sub_teacher_outputs in enumerate(zip(student_outputs, teacher_outputs)):
                    org_loss_dict[i] = self.org_criterion(sub_student_outputs, sub_teacher_outputs, targets)
            else:
                for i, sub_outputs in enumerate(student_outputs):
                    org_loss_dict[i] = self.org_criterion(sub_outputs, targets)
        else:
            org_loss = self.org_criterion(student_outputs, teacher_outputs, targets) if self.use_teacher_output\
                else self.org_criterion(student_outputs, targets)
            org_loss_dict = {0: org_loss}

        output_dict = dict()
        teacher_model_without_dp = \
            self.teacher_model.module if isinstance(self.teacher_model, DataParallel) else self.teacher_model
        student_model_without_ddp = \
            self.student_model.module if isinstance(self.student_model, DistributedDataParallel) else self.student_model
        for teacher_path, student_path in self.target_module_pairs:
            teacher_dict = module_util.get_module(teacher_model_without_dp, teacher_path).__dict__['distillation_box']
            student_dict = module_util.get_module(student_model_without_ddp, student_path).__dict__['distillation_box']
            output_dict[teacher_dict['loss_name']] = ((teacher_dict['path_from_root'], teacher_dict['output']),
                                                      (student_dict['path_from_root'], student_dict['output']))

        total_loss = self.criterion(output_dict, org_loss_dict)
        return total_loss
"
60,2201.02693,
61,2201.02693,"import torch
from torch import nn

from myutils.pytorch import func_util


class KDLoss(nn.KLDivLoss):
    def __init__(self, temperature, alpha=None, reduction='batchmean', **kwargs):
        super().__init__(reduction=reduction)
        self.temperature = temperature
        self.alpha = alpha
        cel_reduction = 'mean' if reduction == 'batchmean' else reduction
        self.cross_entropy_loss = nn.CrossEntropyLoss(reduction=cel_reduction, **kwargs)

    def forward(self, student_output, teacher_output, labels=None):
        soft_loss = super().forward(torch.log_softmax(student_output / self.temperature, dim=1),
                                    torch.softmax(teacher_output / self.temperature, dim=1))
        if self.alpha is None or self.alpha == 0 or labels is None:
            return soft_loss

        hard_loss = self.cross_entropy_loss(student_output, labels)
        return self.alpha * hard_loss + (1 - self.alpha) * soft_loss * (self.temperature ** 2)


SINGLE_LOSS_DICT = {
    'kd': KDLoss
}


def get_single_loss(single_criterion_config):
    loss_type = single_criterion_config['type']
    if loss_type in SINGLE_LOSS_DICT:
        return SINGLE_LOSS_DICT[loss_type](**single_criterion_config['params'])
    return func_util.get_loss(loss_type, single_criterion_config['params'])


class CustomLoss(nn.Module):
    def __init__(self, criterion_config):
        super().__init__()
        term_dict = dict()
        sub_terms_config = criterion_config.get('sub_terms', None)
        if sub_terms_config is not None:
            for loss_name, loss_config in sub_terms_config.items():
                sub_criterion_config = loss_config['criterion']
                sub_criterion = func_util.get_loss(sub_criterion_config['type'], sub_criterion_config['params'])
                term_dict[loss_name] = (loss_config['ts_modules'], sub_criterion, loss_config['factor'])
        self.term_dict = term_dict

    def forward(self, *args, **kwargs):
        raise NotImplementedError('forward function is not implemented')


class GeneralizedCustomLoss(CustomLoss):
    def __init__(self, criterion_config):
        super().__init__(criterion_config)
        self.org_loss_factor = criterion_config['org_term']['factor']

    def forward(self, output_dict, org_loss_dict):
        loss_dict = dict()
        for loss_name, ((teacher_path, teacher_output), (student_path, student_output)) in output_dict.items():
            _, criterion, factor = self.term_dict[loss_name]
            loss_dict[loss_name] = criterion(student_output, teacher_output) * factor

        sub_total_loss = sum(loss for loss in loss_dict.values()) if len(loss_dict) > 0 else 0
        if self.org_loss_factor == 0:
            return sub_total_loss
        return sub_total_loss + self.org_loss_factor * sum(loss for loss in org_loss_dict.values())


CUSTOM_LOSS_DICT = {
    'general': GeneralizedCustomLoss
}


def get_custom_loss(criterion_config):
    criterion_type = criterion_config['type']
    if criterion_type in CUSTOM_LOSS_DICT:
        return CUSTOM_LOSS_DICT[criterion_type](criterion_config)
    raise ValueError('criterion_type `{}` is not expected'.format(criterion_type))
"
62,2201.02693,
63,2201.02693,"import re
from collections import OrderedDict

import torch
import torch.utils.model_zoo as model_zoo
from torch import nn

MODEL_URLS = {
    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',
    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',
    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',
    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',
}


class _DenseLayer(nn.Sequential):
    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):
        super(_DenseLayer, self).__init__()
        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),
        self.add_module('relu1', nn.ReLU(inplace=True)),
        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *
                        growth_rate, kernel_size=1, stride=1, bias=False)),
        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),
        self.add_module('relu2', nn.ReLU(inplace=True)),
        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,
                        kernel_size=3, stride=1, padding=1, bias=False)),
        self.drop_rate = drop_rate

    def forward(self, x):
        new_features = super(_DenseLayer, self).forward(x)
        if self.drop_rate > 0:
            new_features = nn.functional.dropout(new_features, p=self.drop_rate, training=self.training)
        return torch.cat([x, new_features], 1)


class _DenseBlock(nn.Sequential):
    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):
        super(_DenseBlock, self).__init__()
        for i in range(num_layers):
            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)
            self.add_module('denselayer%d' % (i + 1), layer)


class _Transition(nn.Sequential):
    def __init__(self, num_input_features, num_output_features):
        super(_Transition, self).__init__()
        self.add_module('norm', nn.BatchNorm2d(num_input_features))
        self.add_module('relu', nn.ReLU(inplace=True))
        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,
                                          kernel_size=1, stride=1, bias=False))
        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))


class DenseNet(nn.Module):
    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),
                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000,
                 conv2d_ksize=7, conv2d_stride=2, conv2d_padding=3,
                 maxpool_2d_ksize=3, maxpool_2d_stride=2, maxpool_2d_padding=1, avg_pool2d_ksize=7):

        super().__init__()

        # First convolution
        self.features = nn.Sequential(OrderedDict([
            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=conv2d_ksize, stride=conv2d_stride,
                                padding=conv2d_padding, bias=False)),
            ('norm0', nn.BatchNorm2d(num_init_features)),
            ('relu0', nn.ReLU(inplace=True)),
            ('pool0', nn.MaxPool2d(kernel_size=maxpool_2d_ksize, stride=maxpool_2d_stride, padding=maxpool_2d_padding)),
        ]))

        # Each denseblock
        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,
                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)
            self.features.add_module('denseblock%d' % (i + 1), block)
            num_features = num_features + num_layers * growth_rate
            if i != len(block_config) - 1:
                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)
                self.features.add_module('transition%d' % (i + 1), trans)
                num_features = num_features // 2

        # Final batch norm
        self.features.add_module('norm5', nn.BatchNorm2d(num_features))

        self.relu = nn.ReLU(inplace=True)
        self.avg_pool2d = nn.AvgPool2d(kernel_size=avg_pool2d_ksize, stride=1)

        # Linear layer
        self.classifier = nn.Linear(num_features, num_classes)

        # Official init from torch repo.
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        features = self.features(x)
        out = self.relu(features)
        out = self.avg_pool2d(out)
        out = self.classifier(out.view(features.size(0), -1))
        return out


def densenet_model(model_type, param_config, pretrained=False):
    model = DenseNet(**param_config)
    if pretrained:
        print('Loading pretrained weights..')
        pattern = re.compile(
            r'^(.*denselayer\d+\.(?:norm|relu|conv))\.((?:[12])\.(?:weight|bias|running_mean|running_var))$')
        state_dict = model_zoo.load_url(MODEL_URLS[model_type])
        for key in list(state_dict.keys()):
            res = pattern.match(key)
            if res:
                new_key = res.group(1) + res.group(2)
                state_dict[new_key] = state_dict[key]
                del state_dict[key]
        model.load_state_dict(state_dict)
    return model
"
64,2201.02693,"import torch
from torch import nn
from torch.utils import model_zoo


MODEL_URLS = {
    # Inception v3 ported from TensorFlow
    'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',
}


def inception_v3(pretrained=False, **kwargs):
    r""""""Inception v3 model architecture from
    `""Rethinking the Inception Architecture for Computer Vision"" <http://arxiv.org/abs/1512.00567>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """"""
    model = Inception3(**kwargs)
    if pretrained:
        print('Loading pretrained weights..')
        if 'transform_input' not in kwargs:
            kwargs['transform_input'] = True

        state_dict = model_zoo.load_url(MODEL_URLS['inception_v3_google'])
        if 'aux_logits' in kwargs and not kwargs['aux_logits']:
            for key in list(state_dict.keys()):
                if key.startswith('AuxLogits.'):
                    state_dict.pop(key)
        model.load_state_dict(state_dict)
    return model


class Inception3(nn.Module):
    def __init__(self, num_classes=1000, aux_logits=True, transform_input=False):
        super(Inception3, self).__init__()
        self.aux_logits = aux_logits
        self.transform_input = transform_input
        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2)
        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)
        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)
        self.max_pool_2d1 = nn.MaxPool2d(kernel_size=3, stride=2)
        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)
        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)
        self.max_pool_2d2 = nn.MaxPool2d(kernel_size=3, stride=2)
        self.Mixed_5b = InceptionA(192, pool_features=32)
        self.Mixed_5c = InceptionA(256, pool_features=64)
        self.Mixed_5d = InceptionA(288, pool_features=64)
        self.Mixed_6a = InceptionB(288)
        self.Mixed_6b = InceptionC(768, channels_7x7=128)
        self.Mixed_6c = InceptionC(768, channels_7x7=160)
        self.Mixed_6d = InceptionC(768, channels_7x7=160)
        self.Mixed_6e = InceptionC(768, channels_7x7=192)
        if aux_logits:
            self.AuxLogits = InceptionAux(768, num_classes)
        self.Mixed_7a = InceptionD(768)
        self.Mixed_7b = InceptionE(1280)
        self.Mixed_7c = InceptionE(2048)
        self.adaptive_avg_pool_2d = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout()
        self.fc = nn.Linear(2048, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                import scipy.stats as stats
                stddev = m.stddev if hasattr(m, 'stddev') else 0.1
                X = stats.truncnorm(-2, 2, scale=stddev)
                values = torch.Tensor(X.rvs(m.weight.numel()))
                values = values.view(m.weight.size())
                m.weight.data.copy_(values)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        if self.transform_input:
            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5
            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5
            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5
            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)
        # 299 x 299 x 3
        x = self.Conv2d_1a_3x3(x)
        # 149 x 149 x 32
        x = self.Conv2d_2a_3x3(x)
        # 147 x 147 x 32
        x = self.Conv2d_2b_3x3(x)
        # 147 x 147 x 64
        x = self.max_pool_2d1(x)
        # 73 x 73 x 64
        x = self.Conv2d_3b_1x1(x)
        # 73 x 73 x 80
        x = self.Conv2d_4a_3x3(x)
        # 71 x 71 x 192
        x = self.max_pool_2d2(x)
        # 35 x 35 x 192
        x = self.Mixed_5b(x)
        # 35 x 35 x 256
        x = self.Mixed_5c(x)
        # 35 x 35 x 288
        x = self.Mixed_5d(x)
        # 35 x 35 x 288
        x = self.Mixed_6a(x)
        # 17 x 17 x 768
        x = self.Mixed_6b(x)
        # 17 x 17 x 768
        x = self.Mixed_6c(x)
        # 17 x 17 x 768
        x = self.Mixed_6d(x)
        # 17 x 17 x 768
        x = self.Mixed_6e(x)
        # 17 x 17 x 768
        if self.training and self.aux_logits:
            aux = self.AuxLogits(x)
        # 17 x 17 x 768
        x = self.Mixed_7a(x)
        # 8 x 8 x 1280
        x = self.Mixed_7b(x)
        # 8 x 8 x 2048
        x = self.Mixed_7c(x)
        # 8 x 8 x 2048
        x = self.adaptive_avg_pool_2d(x)
        # 1 x 1 x 2048
        x = self.dropout(x)
        # 1 x 1 x 2048
        x = x.view(x.size(0), -1)
        # 2048
        x = self.fc(x)
        # 1000 (num_classes)
        if self.training and self.aux_logits:
            return x, aux
        return x


class InceptionA(nn.Module):

    def __init__(self, in_channels, pool_features):
        super(InceptionA, self).__init__()
        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)

        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)
        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)

        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)
        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)
        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)
        self.avg_pool2d = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)

        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)

        branch_pool = self.avg_pool2d(x)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class InceptionB(nn.Module):

    def __init__(self, in_channels):
        super(InceptionB, self).__init__()
        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2)

        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)
        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)
        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)
        self.max_pool2d = nn.MaxPool2d(kernel_size=3, stride=2)

    def forward(self, x):
        branch3x3 = self.branch3x3(x)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)

        branch_pool = self.max_pool2d(x)

        outputs = [branch3x3, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class InceptionC(nn.Module):

    def __init__(self, in_channels, channels_7x7):
        super(InceptionC, self).__init__()
        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)

        c7 = channels_7x7
        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)
        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(3, 0))

        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)
        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))
        self.avg_pool2d = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)

        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch7x7 = self.branch7x7_1(x)
        branch7x7 = self.branch7x7_2(branch7x7)
        branch7x7 = self.branch7x7_3(branch7x7)

        branch7x7dbl = self.branch7x7dbl_1(x)
        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)

        branch_pool = self.avg_pool2d(x)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]
        return torch.cat(outputs, 1)


class InceptionD(nn.Module):

    def __init__(self, in_channels):
        super(InceptionD, self).__init__()
        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)
        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2)

        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)
        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2)
        self.max_pool2d = nn.MaxPool2d(kernel_size=3, stride=2)

    def forward(self, x):
        branch3x3 = self.branch3x3_1(x)
        branch3x3 = self.branch3x3_2(branch3x3)

        branch7x7x3 = self.branch7x7x3_1(x)
        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)
        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)
        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)

        branch_pool = self.max_pool2d(x)
        outputs = [branch3x3, branch7x7x3, branch_pool]
        return torch.cat(outputs, 1)


class InceptionE(nn.Module):

    def __init__(self, in_channels):
        super(InceptionE, self).__init__()
        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)

        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)
        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))
        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))

        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)
        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)
        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))
        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))
        self.avg_pool2d = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)

        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch3x3 = self.branch3x3_1(x)
        branch3x3 = [
            self.branch3x3_2a(branch3x3),
            self.branch3x3_2b(branch3x3),
        ]
        branch3x3 = torch.cat(branch3x3, 1)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = [
            self.branch3x3dbl_3a(branch3x3dbl),
            self.branch3x3dbl_3b(branch3x3dbl),
        ]
        branch3x3dbl = torch.cat(branch3x3dbl, 1)

        branch_pool = self.avg_pool2d(x)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class InceptionAux(nn.Module):

    def __init__(self, in_channels, num_classes):
        super(InceptionAux, self).__init__()
        self.avg_pool2d = nn.AvgPool2d(kernel_size=5, stride=3)
        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)
        self.conv1 = BasicConv2d(128, 768, kernel_size=5)
        self.conv1.stddev = 0.01
        self.adaptive_avg_pool2d = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(768, num_classes)
        self.fc.stddev = 0.001

    def forward(self, x):
        # 17 x 17 x 768
        x = self.avg_pool2d(x)
        # 5 x 5 x 768
        x = self.conv0(x)
        # 5 x 5 x 128
        x = self.conv1(x)
        # 1 x 1 x 768
        x = self.adaptive_avg_pool2d(x)
        x = x.view(x.size(0), -1)
        # 768
        x = self.fc(x)
        # 1000
        return x


class BasicConv2d(nn.Module):

    def __init__(self, in_channels, out_channels, **kwargs):
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return self.relu(x)
"
65,2201.02693,"from torch import nn
from torch.utils import model_zoo


MODEL_URLS = {
    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',
}


class AlexNet(nn.Module):
    def __init__(self, num_classes=1000, first_conv2d_ksize=11, first_conv2d_stride=4, first_conv2d_padding=2,
                 last_maxpool2d_ksize=3, last_maxpool2d_stride=2):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=first_conv2d_ksize, stride=first_conv2d_stride, padding=first_conv2d_padding),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=last_maxpool2d_ksize, stride=last_maxpool2d_stride)
        )
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes)
        )
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        z = self.features(x)
        return self.classifier(z.view(x.size(0), -1))


def alexnet(pretrained=False, **kwargs):
    r""""""AlexNet model architecture from the
    `""One weird trick..."" <https://arxiv.org/abs/1404.5997>`_ paper.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """"""
    model = AlexNet(**kwargs)
    if pretrained:
        state_dict = model_zoo.load_url(MODEL_URLS['alexnet'])
        model.load_state_dict(state_dict)
    return model
"
66,2201.02693,"from torch import nn


class MnistLeNet5(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 6, kernel_size=5),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(6, 16, kernel_size=5),
            nn.MaxPool2d(kernel_size=2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(16 * 5 * 5, 120),
            nn.Linear(120, 84),
            nn.ReLU(inplace=True),
            nn.Linear(84, 10),
            nn.LogSoftmax(1)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(-1, 16 * 5 * 5)
        return self.classifier(x)


class LeNet5(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 6, kernel_size=5),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(6, 16, kernel_size=5),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(16 * 5 * 5, 120),
            nn.ReLU(inplace=True),
            nn.Linear(120, 84),
            nn.ReLU(inplace=True),
            nn.Linear(84, num_classes),
            nn.LogSoftmax(1)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)
"
67,2201.02693,"from torch import nn
from torchvision.models.utils import load_state_dict_from_url


model_urls = {
    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',
}


def _make_divisible(v, divisor, min_value=None):
    """"""
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by 8
    It can be seen here:
    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    :param v:
    :param divisor:
    :param min_value:
    :return:
    """"""
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


class ConvBNReLU(nn.Sequential):
    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):
        padding = (kernel_size - 1) // 2
        super(ConvBNReLU, self).__init__(
            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),
            nn.BatchNorm2d(out_planes),
            nn.ReLU6(inplace=True)
        )


class InvertedResidual(nn.Module):
    def __init__(self, inp, oup, stride, expand_ratio):
        super(InvertedResidual, self).__init__()
        self.stride = stride
        assert stride in [1, 2]

        hidden_dim = int(round(inp * expand_ratio))
        self.use_res_connect = self.stride == 1 and inp == oup

        layers = []
        if expand_ratio != 1:
            # pw
            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))
        layers.extend([
            # dw
            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),
            # pw-linear
            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
            nn.BatchNorm2d(oup),
        ])
        self.conv = nn.Sequential(*layers)

    def forward(self, x):
        if self.use_res_connect:
            return x + self.conv(x)
        else:
            return self.conv(x)


class MeanLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x.mean([2, 3])


class MobileNetV2(nn.Module):
    def __init__(self,
                 num_classes=1000,
                 width_mult=1.0,
                 inverted_residual_setting=None,
                 round_nearest=8,
                 block=None):
        """"""
        MobileNet V2 main class
        Args:
            num_classes (int): Number of classes
            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount
            inverted_residual_setting: Network structure
            round_nearest (int): Round the number of channels in each layer to be a multiple of this number
            Set to 1 to turn off rounding
            block: Module specifying inverted residual building block for mobilenet
        """"""
        super(MobileNetV2, self).__init__()

        if block is None:
            block = InvertedResidual
        input_channel = 32
        last_channel = 1280

        if inverted_residual_setting is None:
            inverted_residual_setting = [
                # t, c, n, s
                [1, 16, 1, 1],
                [6, 24, 2, 2],
                [6, 32, 3, 2],
                [6, 64, 4, 2],
                [6, 96, 3, 1],
                [6, 160, 3, 2],
                [6, 320, 1, 1],
            ]

        # only check the first element, assuming user knows t,c,n,s are required
        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:
            raise ValueError(""inverted_residual_setting should be non-empty ""
                             ""or a 4-element list, got {}"".format(inverted_residual_setting))

        # building first layer
        input_channel = _make_divisible(input_channel * width_mult, round_nearest)
        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)
        features = [ConvBNReLU(3, input_channel, stride=2)]
        # building inverted residual blocks
        for t, c, n, s in inverted_residual_setting:
            output_channel = _make_divisible(c * width_mult, round_nearest)
            for i in range(n):
                stride = s if i == 0 else 1
                features.append(block(input_channel, output_channel, stride, expand_ratio=t))
                input_channel = output_channel
        # building last several layers
        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))
        # make it nn.Sequential
        self.features = nn.Sequential(*features)

        self.mean_layer = MeanLayer()

        # building classifier
        self.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(self.last_channel, num_classes),
        )

        # weight initialization
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)

    def _forward_impl(self, x):
        # This exists since TorchScript doesn't support inheritance, so the superclass method
        # (this one) needs to have a name other than `forward` that can be accessed in a subclass
        x = self.features(x)
        x = self.mean_layer(x)
        x = self.classifier(x)
        return x

    def forward(self, x):
        return self._forward_impl(x)


def mobilenet_v2(pretrained=False, progress=True, **kwargs):
    """"""
    Constructs a MobileNetV2 architecture from
    `""MobileNetV2: Inverted Residuals and Linear Bottlenecks"" <https://arxiv.org/abs/1801.04381>`_.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """"""
    model = MobileNetV2(**kwargs)
    if pretrained:
        print('Loading pretrained weights..')
        state_dict = load_state_dict_from_url(model_urls['mobilenet_v2'],
                                              progress=progress)
        model.load_state_dict(state_dict)
    return model


def mobilenet_model(model_type, param_config, pretrained=False):
    if model_type == 'mobilenet_v2':
        return mobilenet_v2(pretrained=pretrained, **param_config)
    return None
"
68,2201.02693,
69,2201.02693,"from torch import nn
from torch.utils import model_zoo
from torchvision.models.resnet import BasicBlock, Bottleneck


MODEL_URLS = {
    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',
    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',
    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',
    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',
    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',
}


class ResNet(nn.Module):
    def __init__(self, block, layers, num_classes=101, num_init_features=64,
                 first_conv2d_ksize=7, first_conv2d_stride=2, first_conv2d_padding=3,
                 last_avgpool2d_ksize=7, last_avgpool2d_stride=1):
        self.inplanes = num_init_features
        super().__init__()
        self.conv1 = nn.Conv2d(3, num_init_features, kernel_size=first_conv2d_ksize,
                               stride=first_conv2d_stride, padding=first_conv2d_padding, bias=False)
        self.bn1 = nn.BatchNorm2d(num_init_features)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, num_init_features, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        self.avgpool = nn.AvgPool2d(last_avgpool2d_ksize, stride=last_avgpool2d_stride)
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


def resnet18(param_config):
    return ResNet(BasicBlock, [2, 2, 2, 2], **param_config)


def resnet34(param_config):
    return ResNet(BasicBlock, [3, 4, 6, 3], **param_config)


def resnet50(param_config):
    return ResNet(Bottleneck, [3, 4, 6, 3], **param_config)


def resnet101(param_config):
    return ResNet(Bottleneck, [3, 4, 23, 3], **param_config)


def resnet152(param_config):
    return ResNet(Bottleneck, [3, 8, 36, 3], **param_config)


def resnet_model(model_type, param_config, pretrained=False):
    model = None
    if model_type == 'resnet18':
        model = resnet18(param_config)
    elif model_type == 'resnet34':
        model = resnet34(param_config)
    elif model_type == 'resnet50':
        model = resnet50(param_config)
    elif model_type == 'resnet101':
        model = resnet101(param_config)
    elif model_type == 'resnet152':
        model = resnet152(param_config)

    if model is None:
        raise ValueError('model_type `{}` is not expected'.format(model_type))
    elif pretrained:
        print('Loading pretrained weights..')
        model.load_state_dict(model_zoo.load_url(MODEL_URLS[model_type]))
    return model
"
70,2201.02693,"from torch import nn

from models.mimic.base import BaseHeadMimic, BaseMimic, SeqWithAux


def mimic_version1(make_bottleneck, bottleneck_channel):
    if make_bottleneck:
        return nn.Sequential(
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(bottleneck_channel),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(bottleneck_channel, 256, kernel_size=4, stride=2, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=1)
        )
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 128, kernel_size=2, stride=1, padding=1, bias=False),
        nn.BatchNorm2d(128),
        nn.ReLU(inplace=True),
        nn.Conv2d(128, 256, kernel_size=1, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=1)
    )


def mimic_version2b_with_aux(modules, aux_idx, bottleneck_channel, aux_output_size=1000):
    return SeqWithAux(modules, aux_idx=aux_idx, aux_input_channel=bottleneck_channel, aux_output_size=aux_output_size)


def mimic_version2(make_bottleneck, dataset_name, bottleneck_channel, use_aux):
    if make_bottleneck:
        modules = [
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(bottleneck_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(bottleneck_channel, 64, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=1)
        ]
        aux_idx = 2
        aux_output_size = 101
        if dataset_name == 'imagenet':
            modules = [
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
                nn.BatchNorm2d(bottleneck_channel),
                nn.ReLU(inplace=True),
                nn.Conv2d(bottleneck_channel, 512, kernel_size=2, stride=1, padding=1, bias=False),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, kernel_size=2, stride=1, padding=1, bias=False),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, kernel_size=2, stride=1, bias=False),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, kernel_size=2, stride=1, bias=False),
                nn.AvgPool2d(kernel_size=2, stride=1)
            ]
            aux_output_size = 1000
        return mimic_version2b_with_aux(modules, aux_idx, bottleneck_channel, aux_output_size) if use_aux \
            else nn.Sequential(*modules)
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 128, kernel_size=2, stride=1, padding=1, bias=False),
        nn.BatchNorm2d(128),
        nn.ReLU(inplace=True),
        nn.Conv2d(128, 256, kernel_size=2, stride=1, padding=1, bias=False),
        nn.BatchNorm2d(256),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 512, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )


def mimic_version3(make_bottleneck, bottleneck_channel):
    if make_bottleneck:
        return nn.Sequential(
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(bottleneck_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(bottleneck_channel, 64, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 1024, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(1024),
            nn.ReLU(inplace=True),
            nn.Conv2d(1024, 2048, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(2048),
            nn.ReLU(inplace=True),
            nn.Conv2d(2048, 2048, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=7, stride=2)
        )
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=2, bias=False),
        nn.BatchNorm2d(128),
        nn.ReLU(inplace=True),
        nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=2, bias=False),
        nn.BatchNorm2d(256),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=2, bias=False),
        nn.BatchNorm2d(512),
        nn.ReLU(inplace=True),
        nn.Conv2d(512, 1024, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(1024),
        nn.ReLU(inplace=True),
        nn.Conv2d(1024, 2048, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(2048),
        nn.ReLU(inplace=True),
        nn.Conv2d(2048, 2048, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=7, stride=2)
    )


class ResNet152HeadMimic(BaseHeadMimic):
    # designed for input image size [3, 224, 224]
    def __init__(self, version, dataset_name, bottleneck_channel=3, use_aux=False):
        super().__init__()
        self.extractor = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )
        if version in ['1', '1b']:
            self.module_seq = mimic_version1(version == '1b', bottleneck_channel)
        elif version in ['2', '2b']:
            self.module_seq = mimic_version2(version == '2b', dataset_name, bottleneck_channel, use_aux)
        elif version in ['3', '3b']:
            self.module_seq = mimic_version3(version == '3b', bottleneck_channel)
        else:
            raise ValueError('version `{}` is not expected'.format(version))
        self.initialize_weights()

    def forward(self, sample_batch):
        zs = self.extractor(sample_batch)
        return self.module_seq(zs)


class ResNetMimic(BaseMimic):
    def __init__(self, student_model, tail_modules):
        super().__init__(student_model, tail_modules)

    def forward(self, sample_batch):
        aux = None
        zs = sample_batch
        if self.student_model is not None:
            zs = self.student_model(zs)
            if isinstance(zs, tuple):
                zs, aux = zs[0], zs[1]

        zs = self.features(zs)
        zs = self.classifier(zs.view(zs.size(0), -1))
        return zs if aux is None else (zs, aux)
"
71,2201.02693,"from torch import nn

from models.mimic.base import BaseHeadMimic, BaseMimic, SeqWithAux


def mimic_version1b_with_aux(bottleneck_channel, aux_output_size=1000):
    modules = [
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, bottleneck_channel, kernel_size=3, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(bottleneck_channel),
        nn.ReLU(inplace=True),
        nn.ConvTranspose2d(bottleneck_channel, 512, kernel_size=4, stride=2, bias=False),
        nn.BatchNorm2d(512),
        nn.ReLU(inplace=True),
        nn.Conv2d(512, 256, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(256),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 32, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(32),
        nn.ReLU(inplace=True),
        nn.Conv2d(32, 32, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=2, padding=1)
    ]
    return SeqWithAux(modules, aux_idx=2, aux_input_channel=bottleneck_channel, aux_output_size=aux_output_size)


def mimic_version1(make_bottleneck, bottleneck_channel, use_aux):
    if make_bottleneck:
        modules = [
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, bottleneck_channel, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(bottleneck_channel),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(bottleneck_channel, 512, kernel_size=4, stride=2, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 32, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2, padding=1)
        ]
        return mimic_version1b_with_aux(bottleneck_channel) if use_aux else nn.Sequential(*modules)
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(128),
        nn.ReLU(inplace=True),
        nn.Conv2d(128, 64, kernel_size=2, stride=1, padding=1, bias=False),
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 32, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=1)
    )


def mimic_version2(make_bottleneck, bottleneck_channel):
    if make_bottleneck:
        return nn.Sequential(
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(bottleneck_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(bottleneck_channel, 512, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 192, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(128),
        nn.ReLU(inplace=True),
        nn.Conv2d(128, 256, kernel_size=2, stride=1, padding=1, bias=False),
        nn.BatchNorm2d(256),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 192, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )


def mimic_version3(make_bottleneck, bottleneck_channel):
    if make_bottleneck:
        return nn.Sequential(
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(bottleneck_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(bottleneck_channel, 512, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 160, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(128),
        nn.ReLU(inplace=True),
        nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(256),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=1, bias=False),
        nn.BatchNorm2d(512),
        nn.ReLU(inplace=True),
        nn.Conv2d(512, 160, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )


def mimic_version_test0(bottleneck_channel):
    # end_idx: 24
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, bottleneck_channel, kernel_size=3, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(bottleneck_channel),
        nn.ReLU(inplace=True),
        nn.ConvTranspose2d(bottleneck_channel, 512, kernel_size=4, stride=2, bias=False),
        nn.BatchNorm2d(512),
        nn.ReLU(inplace=True),
        nn.Conv2d(512, 256, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(256),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 32, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(32),
        nn.ReLU(inplace=True),
        nn.Conv2d(32, 32, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=2, padding=1)
    )


def mimic_version_test1(bottleneck_channel):
    # end_idx: 24
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(bottleneck_channel),
        nn.ReLU(inplace=True),
        nn.ConvTranspose2d(bottleneck_channel, 512, kernel_size=4, stride=2, bias=False),
        nn.BatchNorm2d(512),
        nn.ReLU(inplace=True),
        nn.Conv2d(512, 256, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(256),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 32, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(32),
        nn.ReLU(inplace=True),
        nn.Conv2d(32, 32, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )


class MobileNetHeadMimic(BaseHeadMimic):
    # designed for input image size [3, 224, 224]
    def __init__(self, version, bottleneck_channel=3, use_aux=False):
        super().__init__()
        self.extractor = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
        )
        if version in ['1', '1b']:
            self.module_seq = mimic_version1(version == '1b', bottleneck_channel, use_aux)
        elif version in ['2', '2b']:
            self.module_seq = mimic_version2(version == '2b', bottleneck_channel)
        elif version in ['3', '3b']:
            self.module_seq = mimic_version3(version == '3b', bottleneck_channel)
        else:
            raise ValueError('version `{}` is not expected'.format(version))
        self.initialize_weights()

    def forward(self, sample_batch):
        zs = self.extractor(sample_batch)
        return self.module_seq(zs)


class MobileNetMimic(BaseMimic):
    def __init__(self, student_model, tail_modules):
        super().__init__(student_model, tail_modules)

    def forward(self, sample_batch):
        aux = None
        zs = sample_batch
        if self.student_model is not None:
            zs = self.student_model(zs)
            if isinstance(zs, tuple):
                zs, aux = zs[0], zs[1]

        zs = self.features(zs)
        zs = self.classifier(zs.view(zs.size(0), -1))
        return zs if aux is None else (zs, aux)
"
72,2201.02693,"from torch import nn

from models.mimic.base import BaseHeadMimic, BaseMimic, SeqWithAux


def mimic_version1b_with_aux(modules, aux_idx, bottleneck_channel, aux_output_size=1000):
    return SeqWithAux(modules, aux_idx=aux_idx, aux_input_channel=bottleneck_channel, aux_output_size=aux_output_size)


def mimic_version1(make_bottleneck, dataset_name, bottleneck_channel, use_aux):
    if make_bottleneck:
        modules = [
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(bottleneck_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(bottleneck_channel, 64, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 192, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=1)
        ]
        aux_idx = 2
        aux_output_size = 101
        if dataset_name == 'imagenet':
            modules = [
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
                nn.BatchNorm2d(bottleneck_channel),
                nn.ReLU(inplace=True),
                nn.Conv2d(bottleneck_channel, 256, kernel_size=2, stride=1, padding=1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, kernel_size=2, stride=1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 192, kernel_size=2, stride=1, bias=False),
                nn.AvgPool2d(kernel_size=2, stride=1)
            ]
            aux_output_size = 1000
        return mimic_version1b_with_aux(modules, aux_idx, bottleneck_channel, aux_output_size) if use_aux \
            else nn.Sequential(*modules)
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 128, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(128),
        nn.ReLU(inplace=True),
        nn.Conv2d(128, 192, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )


def mimic_version2(make_bottleneck, bottleneck_channel):
    if make_bottleneck:
        return nn.Sequential(
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(bottleneck_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(bottleneck_channel, 64, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 768, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(128),
        nn.ReLU(inplace=True),
        nn.Conv2d(128, 256, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(256),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 512, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(512),
        nn.ReLU(inplace=True),
        nn.Conv2d(512, 768, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )


def mimic_version3(make_bottleneck, bottleneck_channel):
    if make_bottleneck:
        return nn.Sequential(
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(bottleneck_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(bottleneck_channel, 64, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 768, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(768),
            nn.ReLU(inplace=True),
            nn.Conv2d(768, 1024, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(1024),
            nn.ReLU(inplace=True),
            nn.Conv2d(1024, 1280, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(1280),
            nn.ReLU(inplace=True),
            nn.Conv2d(1280, 1536, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(1536),
            nn.ReLU(inplace=True),
            nn.Conv2d(1536, 1280, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(128),
        nn.ReLU(inplace=True),
        nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(256),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=1, bias=False),
        nn.BatchNorm2d(512),
        nn.ReLU(inplace=True),
        nn.Conv2d(512, 768, kernel_size=2, stride=1, padding=1, bias=False),
        nn.BatchNorm2d(768),
        nn.ReLU(inplace=True),
        nn.Conv2d(768, 1024, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(1024),
        nn.ReLU(inplace=True),
        nn.Conv2d(1024, 1280, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(1280),
        nn.ReLU(inplace=True),
        nn.Conv2d(1280, 1536, kernel_size=2, stride=1, bias=False),
        nn.BatchNorm2d(1536),
        nn.ReLU(inplace=True),
        nn.Conv2d(1536, 1280, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )


class InceptionHeadMimic(BaseHeadMimic):
    # designed for input image size [3, 299, 299]
    def __init__(self, version, dataset_name, bottleneck_channel=3, use_aux=False):
        super().__init__()
        self.extractor = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2)
        )
        if version in ['1', '1b']:
            self.module_seq = mimic_version1(version == '1b', dataset_name, bottleneck_channel, use_aux)
        elif version in ['2', '2b']:
            self.module_seq = mimic_version2(version == '2b', bottleneck_channel)
        elif version in ['3', '3b']:
            self.module_seq = mimic_version3(version == '3b', bottleneck_channel)
        else:
            raise ValueError('version `{}` is not expected'.format(version))
        self.initialize_weights()

    def forward(self, sample_batch):
        zs = self.extractor(sample_batch)
        return self.module_seq(zs)


class InceptionMimic(BaseMimic):
    def __init__(self, student_model, tail_modules):
        super().__init__(student_model, tail_modules)

    def forward(self, sample_batch):
        return super().forward(sample_batch)
"
73,2201.02693,"from torch import nn


class SeqWithAux(nn.Module):
    def __init__(self, modules, aux_idx, aux_input_channel, aux_output_size):
        super().__init__()
        self.head_modules = nn.Sequential(*modules[:aux_idx + 1])
        self.aux_seq = nn.Sequential(
            nn.AvgPool2d(kernel_size=5, stride=3),
            nn.Conv2d(aux_input_channel, 128, kernel_size=1, bias=False),
            nn.BatchNorm2d(128, eps=0.001),
            nn.Conv2d(128, 768, kernel_size=5, bias=False),
            nn.BatchNorm2d(768, eps=0.001),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.linear = nn.Linear(768, aux_output_size)
        self.tail_modules = nn.Sequential(*modules[aux_idx + 1:])

    def forward(self, sample_batch):
        zs = self.head_modules(sample_batch)
        if self.training:
            zs_aux = self.aux_seq(zs)
            return self.tail_modules(zs), self.linear(zs_aux.view(zs_aux.size(0), -1))
        return self.tail_modules(zs)


class BaseHeadMimic(nn.Module):
    def __init__(self):
        super().__init__()

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

    def forward(self, sample_batch):
        raise NotImplementedError('forward function must be implemented')


class BaseMimic(nn.Module):
    def __init__(self, student_model, tail_modules):
        super().__init__()
        self.student_model = student_model
        self.features = nn.Sequential(*tail_modules[:-1])
        self.classifier = tail_modules[-1]

    def forward(self, sample_batch):
        zs = sample_batch
        if self.student_model is not None:
            zs = self.student_model(zs)

        zs = self.features(zs)
        return self.classifier(zs.view(zs.size(0), -1))
"
74,2201.02693,"from torch import nn

from models.mimic.base import BaseHeadMimic, BaseMimic, SeqWithAux


def mimic_version1(make_bottleneck, bottleneck_channel):
    if make_bottleneck:
        return nn.Sequential(
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(bottleneck_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(bottleneck_channel, 64, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 128, kernel_size=2, stride=1, padding=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )


def mimic_version2b_with_aux(modules, aux_idx, bottleneck_channel, aux_output_size=1000):
    return SeqWithAux(modules, aux_idx=aux_idx, aux_input_channel=bottleneck_channel, aux_output_size=aux_output_size)


def mimic_version2(make_bottleneck, dataset_name, bottleneck_channel, use_aux):
    if make_bottleneck:
        modules = [
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(bottleneck_channel),
            nn.ReLU(inplace=True),
            nn.Conv2d(bottleneck_channel, 64, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        ]
        aux_idx = 2
        aux_output_size = 101
        if dataset_name == 'imagenet':
            modules = [
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
                nn.BatchNorm2d(bottleneck_channel),
                nn.ReLU(inplace=True),
                nn.Conv2d(bottleneck_channel, 512, kernel_size=2, stride=1, padding=1, bias=False),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, kernel_size=2, stride=1, padding=1, bias=False),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 256, kernel_size=2, stride=1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, kernel_size=2, stride=1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, kernel_size=2, stride=1, bias=False),
                nn.AvgPool2d(kernel_size=2, stride=2)
            ]
            aux_output_size = 1000
        return mimic_version2b_with_aux(modules, aux_idx, bottleneck_channel, aux_output_size) if use_aux \
            else nn.Sequential(*modules)
    return nn.Sequential(
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=1, bias=False),
        nn.BatchNorm2d(128),
        nn.ReLU(inplace=True),
        nn.Conv2d(128, 256, kernel_size=2, stride=1, bias=False),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )


def mimic_version3(teacher_model_type, make_bottleneck, bottleneck_channel):
    if teacher_model_type == 'densenet169':
        if make_bottleneck:
            return nn.Sequential(
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
                nn.BatchNorm2d(bottleneck_channel),
                nn.ReLU(inplace=True),
                nn.Conv2d(bottleneck_channel, 64, kernel_size=2, stride=2, padding=1, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, kernel_size=2, stride=1, padding=1, bias=False),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, kernel_size=2, stride=1, padding=1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, kernel_size=2, stride=1, bias=False),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 640, kernel_size=2, stride=1, bias=False),
                nn.AvgPool2d(kernel_size=2, stride=2)
            )
        return nn.Sequential(
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 1024, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(1024),
            nn.ReLU(inplace=True),
            nn.Conv2d(1024, 640, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )
    elif teacher_model_type == 'densenet201':
        if make_bottleneck:
            return nn.Sequential(
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, bottleneck_channel, kernel_size=2, stride=2, padding=1, bias=False),
                nn.BatchNorm2d(bottleneck_channel),
                nn.ReLU(inplace=True),
                nn.Conv2d(bottleneck_channel, 64, kernel_size=2, stride=2, padding=1, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, kernel_size=2, stride=1, padding=1, bias=False),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                nn.Conv2d(128, 256, kernel_size=2, stride=1, padding=1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, kernel_size=2, stride=1, bias=False),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 896, kernel_size=2, stride=1, bias=False),
                nn.AvgPool2d(kernel_size=2, stride=2)
            )
        return nn.Sequential(
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 1024, kernel_size=2, stride=1, bias=False),
            nn.BatchNorm2d(1024),
            nn.ReLU(inplace=True),
            nn.Conv2d(1024, 896, kernel_size=2, stride=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )
    raise ValueError('teacher_model_type `{}` is not expected'.format(teacher_model_type))


class DenseNetHeadMimic(BaseHeadMimic):
    # designed for input image size [3, 224, 224]
    def __init__(self, teacher_model_type, version, dataset_name, bottleneck_channel=3, use_aux=False):
        super().__init__()
        self.extractor = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )
        if version in ['1', '1b']:
            self.module_seq = mimic_version1(version == '1b', bottleneck_channel)
        elif version in ['2', '2b']:
            self.module_seq = mimic_version2(version == '2b', dataset_name, bottleneck_channel, use_aux)
        elif version in ['3', '3b']:
            self.module_seq = mimic_version3(teacher_model_type, version == '3b', bottleneck_channel)
        else:
            raise ValueError('version `{}` is not expected'.format(version))
        self.initialize_weights()

    def forward(self, sample_batch):
        zs = self.extractor(sample_batch)
        return self.module_seq(zs)


class DenseNetMimic(BaseMimic):
    def __init__(self, student_model, tail_modules):
        super().__init__(student_model, tail_modules)

    def forward(self, sample_batch):
        aux = None
        zs = sample_batch
        if self.student_model is not None:
            zs = self.student_model(zs)
            if isinstance(zs, tuple):
                zs, aux = zs[0], zs[1]

        zs = self.features(zs)
        zs = self.classifier(zs.view(zs.size(0), -1))
        return zs if aux is None else (zs, aux)
"
75,2201.02693,
76,2201.02693,"import pickle
import sys

import numpy as np
from torch import nn

from utils import data_util, module_util
from myutils.pytorch import tensor_util


class BaseAutoencoder(nn.Module):
    def __init__(self):
        super().__init__()

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

    def forward(self, sample_batch):
        raise NotImplementedError('forward function must be implemented')


class BaseExtendedModel(nn.Module):
    def __init__(self, head_modules, autoencoder, tail_modules):
        super().__init__()
        self.head_model = nn.Sequential(*head_modules)
        self.autoencoder = autoencoder
        self.tail_model = nn.Sequential(*tail_modules[:-1])
        self.linear = tail_modules[-1]

    def forward(self, sample_batch):
        zs = self.head_model(sample_batch)
        zs = self.autoencoder(zs)
        zs = self.tail_model(zs)
        return self.linear(zs.view(zs.size(0), -1))

    def compute_ae_bottleneck_size(self, x, print_info=False):
        z = self.head_model(x)
        modules = list()
        module_util.extract_decomposable_modules(self.autoencoder, z, modules)
        modules = [module.to(x.device) for module in modules]
        org_size = np.prod(x.size())
        min_rate = None
        bo = None
        bqo = None
        for i in range(len(modules)):
            if isinstance(modules[i], nn.Linear):
                z = z.view(z.size(0), -1)

            z = modules[i](z)
            rate = np.prod(z.size()) / org_size
            if min_rate is None or rate < min_rate:
                min_rate = rate
                bo = pickle.dumps(z)
                bqo = pickle.dumps(tensor_util.quantize_tensor(z))

        output_data_size = sys.getsizeof(bo) / 1024
        quantized_output_data_size = sys.getsizeof(bqo) / 1024
        if print_info:
            print('[Autoencoder bottleneck]\tScaled output size: {} [%]\tOutput data size: {} [KB]'
                  '\tQuantized output data size: {} [KB]'.format(min_rate * 100.0, output_data_size,
                                                                 quantized_output_data_size))
        # Scaled bottleneck size, bottleneck data size [KB], Quantized bottleneck data size [KB]
        return min_rate, output_data_size, quantized_output_data_size
"
77,2201.02693,
78,2201.02693,"import torch
from torch import nn

from models.autoencoder.base import BaseAutoencoder


class InputAutoencoder(BaseAutoencoder):
    def __init__(self, input_channel=3, bottleneck_channel=3):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(input_channel, 64, kernel_size=5), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 64, kernel_size=5), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 32, kernel_size=4), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(32, bottleneck_channel, kernel_size=2), nn.ReLU(inplace=True)
        )

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(bottleneck_channel, 32, kernel_size=5, stride=3), nn.ReLU(inplace=True),
            nn.ConvTranspose2d(32, 64, kernel_size=5, stride=3), nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 128, kernel_size=3, stride=1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, kernel_size=2, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(32, 16, kernel_size=2), nn.ReLU(inplace=True),
            nn.Conv2d(16, 8, kernel_size=2), nn.ReLU(inplace=True),
            nn.Conv2d(8, 3, kernel_size=2)
        )
        self.initialize_weights()

    def forward(self, sample_batch):
        # Encoding
        zs = self.encoder(sample_batch)

        # Decoding
        zs = self.decoder(zs)
        return zs


# Referred to https://github.com/sksq96/pytorch-vae/blob/master/vae-cnn.ipynb
class Flatter(nn.Module):
    def forward(self, zs):
        return zs.view(zs.size(0), -1)


class UnFlatter(nn.Module):
    def __init__(self, h_dim):
        super().__init__()
        self.h_dim = h_dim

    def forward(self, zs):
        return zs.view(zs.size(0), self.h_dim, 1, 1)


class Bottleneck(nn.Module):
    def __init__(self, h_dim, z_dim, is_static):
        super().__init__()
        self.flatter = Flatter()
        self.fc1 = nn.Linear(h_dim, z_dim)
        self.fc2 = nn.Linear(h_dim, z_dim)
        self.fc3 = nn.Linear(z_dim, h_dim)
        self.unflatter = UnFlatter(h_dim)
        self.is_static = is_static

    def reparameterize(self, mu, logvar):
        std = logvar.mul(0.5).exp_()
        # return torch.normal(mu, std)
        esp = torch.randn(*mu.size()).to(mu.device) if not self.is_static else 1
        z = mu + std * esp
        return z

    def forward(self, hs):
        hs = self.flatter(hs)
        mu, logvar = self.fc1(hs), self.fc2(hs)
        zs = self.reparameterize(mu, logvar)
        zs = self.fc3(zs)
        zs = self.unflatter(zs)
        return (zs, mu, logvar) if self.training else zs


class InputVAE(BaseAutoencoder):
    def __init__(self, input_channel=3, h_dim=18432, z_dim=512, is_static=False):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(input_channel, 32, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.Conv2d(256, 512, kernel_size=3, stride=2),
            nn.ReLU()
        )
        self.bottleneck = Bottleneck(h_dim, z_dim, is_static)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(h_dim, 512, kernel_size=4, stride=3),
            nn.ReLU(),
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=3),
            nn.ReLU(),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(32, input_channel, kernel_size=2, stride=2),
            nn.Sigmoid()
        )
        self.initialize_weights()

    def representation(self, x):
        return self.bottleneck(self.encoder(x))[0]

    def loss_function(self, outputs, sample_batch, mu, logvar):
        bce = nn.functional.mse_loss(outputs, sample_batch, reduction='sum')
        # see Appendix B from VAE paper:
        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014
        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        return bce + kld

    def forward(self, sample_batch):
        hs = self.encoder(sample_batch)
        if self.training:
            zs, mu, logvar = self.bottleneck(hs)
            zs = self.decoder(zs)
            return zs, self.loss_function(zs, sample_batch, mu, logvar)

        zs = self.bottleneck(hs)
        zs = self.decoder(zs)
        return zs
"
79,2201.02693,"from torch import nn

from models.autoencoder.base import BaseAutoencoder


class MiddleAutoencoder(BaseAutoencoder):
    def __init__(self, input_channel=256, bottleneck_channel=3):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(input_channel, 128, kernel_size=2, stride=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, kernel_size=2, stride=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, kernel_size=2, stride=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),
            nn.Conv2d(32, bottleneck_channel, kernel_size=2, stride=1), nn.BatchNorm2d(bottleneck_channel)
        )

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(bottleneck_channel, 32, kernel_size=2, stride=1),
            nn.BatchNorm2d(32), nn.ReLU(inplace=True),
            nn.ConvTranspose2d(32, 64, kernel_size=2, stride=1),
            nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 128, kernel_size=2, stride=1),
            nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, input_channel, kernel_size=2, stride=1)
        )
        self.initialize_weights()

    def forward(self, sample_batch):
        # Encoding
        zs = self.encoder(sample_batch)

        # Decoding
        zs = self.decoder(zs)
        return zs
"
80,2201.02693,"import datetime
import time
from collections import defaultdict, deque

import torch
import torch.distributed as dist

from utils import main_util


class SmoothedValue(object):
    """"""Track a series of values and provide access to smoothed values over a
    window or the global series average.
    """"""

    def __init__(self, window_size=20, fmt=None):
        if fmt is None:
            fmt = ""{median:.4f} ({global_avg:.4f})""
        self.deque = deque(maxlen=window_size)
        self.total = 0.0
        self.count = 0
        self.fmt = fmt

    def update(self, value, n=1):
        self.deque.append(value)
        self.count += n
        self.total += value * n

    def synchronize_between_processes(self):
        """"""
        Warning: does not synchronize the deque!
        """"""
        if not main_util.is_dist_avail_and_initialized():
            return
        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')
        dist.barrier()
        dist.all_reduce(t)
        t = t.tolist()
        self.count = int(t[0])
        self.total = t[1]

    @property
    def median(self):
        d = torch.tensor(list(self.deque))
        return d.median().item()

    @property
    def avg(self):
        d = torch.tensor(list(self.deque), dtype=torch.float32)
        return d.mean().item()

    @property
    def global_avg(self):
        return self.total / self.count

    @property
    def max(self):
        return max(self.deque)

    @property
    def value(self):
        return self.deque[-1]

    def __str__(self):
        return self.fmt.format(
            median=self.median,
            avg=self.avg,
            global_avg=self.global_avg,
            max=self.max,
            value=self.value)


class MetricLogger(object):
    def __init__(self, delimiter=""\t""):
        self.meters = defaultdict(SmoothedValue)
        self.delimiter = delimiter

    def update(self, **kwargs):
        for k, v in kwargs.items():
            if isinstance(v, torch.Tensor):
                v = v.item()
            assert isinstance(v, (float, int))
            self.meters[k].update(v)

    def __getattr__(self, attr):
        if attr in self.meters:
            return self.meters[attr]
        if attr in self.__dict__:
            return self.__dict__[attr]
        raise AttributeError(""'{}' object has no attribute '{}'"".format(
            type(self).__name__, attr))

    def __str__(self):
        loss_str = []
        for name, meter in self.meters.items():
            loss_str.append(
                ""{}: {}"".format(name, str(meter))
            )
        return self.delimiter.join(loss_str)

    def synchronize_between_processes(self):
        for meter in self.meters.values():
            meter.synchronize_between_processes()

    def add_meter(self, name, meter):
        self.meters[name] = meter

    def log_every(self, iterable, print_freq, header=None):
        i = 0
        if not header:
            header = ''
        start_time = time.time()
        end = time.time()
        iter_time = SmoothedValue(fmt='{avg:.4f}')
        data_time = SmoothedValue(fmt='{avg:.4f}')
        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'
        if torch.cuda.is_available():
            log_msg = self.delimiter.join([
                header,
                '[{0' + space_fmt + '}/{1}]',
                'eta: {eta}',
                '{meters}',
                'time: {time}',
                'data: {data}',
                'max mem: {memory:.0f}'
            ])
        else:
            log_msg = self.delimiter.join([
                header,
                '[{0' + space_fmt + '}/{1}]',
                'eta: {eta}',
                '{meters}',
                'time: {time}',
                'data: {data}'
            ])
        MB = 1024.0 * 1024.0
        for obj in iterable:
            data_time.update(time.time() - end)
            yield obj
            iter_time.update(time.time() - end)
            if i % print_freq == 0:
                eta_seconds = iter_time.global_avg * (len(iterable) - i)
                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
                if torch.cuda.is_available():
                    print(log_msg.format(
                        i, len(iterable), eta=eta_string,
                        meters=str(self),
                        time=str(iter_time), data=str(data_time),
                        memory=torch.cuda.max_memory_allocated() / MB))
                else:
                    print(log_msg.format(
                        i, len(iterable), eta=eta_string,
                        meters=str(self),
                        time=str(iter_time), data=str(data_time)))
            i += 1
            end = time.time()
        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        print('{} Total time: {}'.format(header, total_time_str))
"
81,2201.02693,"from io import BytesIO

import numpy as np
import torchvision.transforms.functional as functional
from PIL import Image
from torchvision.datasets import ImageFolder
from torchvision.datasets.folder import default_loader

from myutils.pytorch.vision.dataset import RgbImageDataset


class AdvRgbImageDataset(RgbImageDataset):
    def __init__(self, file_path, size, transform=None, jpeg_quality=0):
        super().__init__(file_path, size, transform=transform, delimiter='\t')
        self.jpeg_quality = jpeg_quality
        self.org_file_sizes = []
        self.comp_file_sizes = []
        self.compression_rates = []
        self.avg_org_file_size = 0
        self.sd_org_file_size = 0
        self.avg_comp_file_size = 0
        self.sd_comp_file_size = 0
        self.avg_compression_rate = 0
        self.sd_compression_rate = 0

    def compress_img(self, img):
        img_buffer = BytesIO()
        img.save(img_buffer, 'JPEG', quality=100)
        org_file_size = img_buffer.tell()
        img_buffer.close()
        img_buffer = BytesIO()
        img.save(img_buffer, 'JPEG', quality=self.jpeg_quality)
        comp_file_size = img_buffer.tell()
        recon_img = Image.open(img_buffer)
        return recon_img, org_file_size, comp_file_size

    def __getitem__(self, idx):
        file_path, target = self.file_paths[idx], self.labels[idx]
        img = Image.open(file_path).convert('RGB')
        img = functional.resize(img, self.size, interpolation=2)
        if 1 <= self.jpeg_quality <= 100:
            img, org_file_size, comp_file_size = self.compress_img(img)
            self.org_file_sizes.append(org_file_size / 1024)
            self.comp_file_sizes.append(comp_file_size / 1024)
            self.compression_rates.append(1 - comp_file_size / org_file_size)

        if self.transform is not None:
            img = self.transform(img)
        return img, target

    def load_all_data(self):
        data = []
        self.org_file_sizes = []
        self.comp_file_sizes = []
        self.compression_rates = []
        for i in range(len(self.labels)):
            img, _ = self.__getitem__(i)
            data.append(img)

        data = np.concatenate(data)
        if len(self.compression_rates) > 0:
            self.avg_org_file_size = np.average(self.org_file_sizes)
            self.sd_org_file_size = np.std(self.org_file_sizes)
            self.avg_comp_file_size = np.average(self.comp_file_sizes)
            self.sd_comp_file_size = np.std(self.comp_file_sizes)
            self.avg_compression_rate = np.average(self.compression_rates)
            self.sd_compression_rate = np.std(self.compression_rates)
            print('[Original]')
            print('File size [KB]:', self.avg_org_file_size, '+-', self.sd_org_file_size)
            print('[JPEG quality={}]'.format(self.jpeg_quality))
            print('File size [KB]:', self.avg_comp_file_size, '+-', self.sd_comp_file_size)
            print('Compression rate:', self.avg_compression_rate, '+-', self.sd_compression_rate)
        return data.reshape(len(self.labels), self.size[0], self.size[1], 3)

    def compute_compression_rate(self):
        if self.jpeg_quality < 1 or self.jpeg_quality > 100:
            print('Compression rate: 0 +- 0')
            return

        self.org_file_sizes = []
        self.comp_file_sizes = []
        self.compression_rates = []
        for i in range(len(self.labels)):
            _, _ = self.__getitem__(i)

        self.avg_org_file_size = np.average(self.org_file_sizes)
        self.sd_org_file_size = np.std(self.org_file_sizes)
        self.avg_comp_file_size = np.average(self.comp_file_sizes)
        self.sd_comp_file_size = np.std(self.comp_file_sizes)
        self.avg_compression_rate = np.average(self.compression_rates)
        self.sd_compression_rate = np.std(self.compression_rates)
        print('[Original]')
        print('File size [KB]:', self.avg_org_file_size, '+-', self.sd_org_file_size)
        print('[JPEG quality={}]'.format(self.jpeg_quality))
        print('File size [KB]:', self.avg_comp_file_size, '+-', self.sd_comp_file_size)
        print('Compression rate:', self.avg_compression_rate, '+-', self.sd_compression_rate)


class AdvImageFolder(ImageFolder):
    def __init__(self, root, size, transform=None, target_transform=None, loader=default_loader, jpeg_quality=0):
        super().__init__(root, transform, target_transform, loader)
        self.size = size
        self.jpeg_quality = jpeg_quality
        self.org_file_sizes = []
        self.comp_file_sizes = []
        self.compression_rates = []
        self.avg_org_file_size = 0
        self.sd_org_file_size = 0
        self.avg_comp_file_size = 0
        self.sd_comp_file_size = 0
        self.avg_compression_rate = 0
        self.sd_compression_rate = 0

    def compress_img(self, img):
        img_buffer = BytesIO()
        img.save(img_buffer, 'JPEG', quality=100)
        org_file_size = img_buffer.tell()
        img_buffer.close()
        img_buffer = BytesIO()
        img.save(img_buffer, 'JPEG', quality=self.jpeg_quality)
        comp_file_size = img_buffer.tell()
        recon_img = Image.open(img_buffer)
        return recon_img, org_file_size, comp_file_size

    def __getitem__(self, idx):
        file_path, target = self.samples[idx]
        img = Image.open(file_path).convert('RGB')
        img = functional.resize(img, self.size, interpolation=2)
        if 1 <= self.jpeg_quality <= 100:
            img, org_file_size, comp_file_size = self.compress_img(img)
            self.org_file_sizes.append(org_file_size / 1024)
            self.comp_file_sizes.append(comp_file_size / 1024)
            self.compression_rates.append(1 - comp_file_size / org_file_size)

        if self.transform is not None:
            img = self.transform(img)

        if self.target_transform is not None:
            target = self.target_transform(target)
        return img, target

    def load_all_data(self):
        data = []
        self.org_file_sizes = []
        self.comp_file_sizes = []
        self.compression_rates = []
        for i in range(len(self.samples)):
            img, _ = self.__getitem__(i)
            data.append(img)

        data = np.concatenate(data)
        if len(self.compression_rates) > 0:
            self.avg_org_file_size = np.average(self.org_file_sizes)
            self.sd_org_file_size = np.std(self.org_file_sizes)
            self.avg_comp_file_size = np.average(self.comp_file_sizes)
            self.sd_comp_file_size = np.std(self.comp_file_sizes)
            self.avg_compression_rate = np.average(self.compression_rates)
            self.sd_compression_rate = np.std(self.compression_rates)
            print('[Original]')
            print('File size [KB]:', self.avg_org_file_size, '+-', self.sd_org_file_size)
            print('[JPEG quality={}]'.format(self.jpeg_quality))
            print('File size [KB]:', self.avg_comp_file_size, '+-', self.sd_comp_file_size)
            print('Compression rate:', self.avg_compression_rate, '+-', self.sd_compression_rate)
        return data.reshape(len(self.targets), self.size[0], self.size[1], 3)

    def compute_compression_rate(self):
        if self.jpeg_quality < 1 or self.jpeg_quality > 100:
            print('Compression rate: 0 +- 0')
            return

        self.org_file_sizes = []
        self.comp_file_sizes = []
        self.compression_rates = []
        for i in range(len(self.samples)):
            self.__getitem__(i)

        self.avg_org_file_size = np.average(self.org_file_sizes)
        self.sd_org_file_size = np.std(self.org_file_sizes)
        self.avg_comp_file_size = np.average(self.comp_file_sizes)
        self.sd_comp_file_size = np.std(self.comp_file_sizes)
        self.avg_compression_rate = np.average(self.compression_rates)
        self.sd_compression_rate = np.std(self.compression_rates)
        print('[Original]')
        print('File size [KB]:', self.avg_org_file_size, '+-', self.sd_org_file_size)
        print('[JPEG quality={}]'.format(self.jpeg_quality))
        print('File size [KB]:', self.avg_comp_file_size, '+-', self.sd_comp_file_size)
        print('Compression rate:', self.avg_compression_rate, '+-', self.sd_compression_rate)

"
82,2201.02693,
83,2201.02693,"import sys
import time
import zlib

import numpy as np
from sklearn.manifold import TSNE
from torch import nn


class CompressionWrapper(nn.Module):
    def __init__(self, org_module, compression_level=9):
        super().__init__()
        self.org_module = org_module
        self.compression_level = compression_level
        self.org_data_size = 0
        self.compressed_data_size = 0
        self.count = 0

    def forward(self, *input):
        output = self.org_module(*input)
        np_output = output.clone().cpu().detach().numpy()
        compressed_output = zlib.compress(np_output, self.compression_level)
        self.org_data_size += np_output.nbytes
        self.compressed_data_size += sys.getsizeof(compressed_output)
        self.count += len(np_output)
        return output

    def get_compression_rate(self):
        return self.compressed_data_size / self.org_data_size

    def get_average_org_data_size(self):
        return self.org_data_size / self.count

    def get_average_compressed_data_size(self):
        return self.compressed_data_size / self.count


class RunTimeWrapper(CompressionWrapper):
    def __init__(self, org_module, compression_level=9):
        super().__init__(org_module, compression_level)
        self.is_first = False
        self.is_compressed = False
        self.start_timestamp_list = list()
        self.timestamp_list = list()
        self.comp_timestamp_list = list()

    def forward(self, *input):
        if self.is_first:
            self.start_timestamp_list.append(time.time())

        output = self.org_module(*input)
        self.timestamp_list.append(time.time())
        if not self.is_compressed:
            return output

        np_output = output.clone().cpu().detach().numpy()
        compressed_output = zlib.compress(np_output, self.compression_level)
        self.org_data_size += np_output.nbytes
        self.compressed_data_size += sys.getsizeof(compressed_output)
        self.count += len(np_output)
        self.comp_timestamp_list.append(time.time())
        return output

    def get_timestamps(self):
        return self.timestamp_list

    def get_compression_timestamps(self):
        return self.comp_timestamp_list

    def get_compression_time_list(self):
        return [self.comp_timestamp_list[i] - self.timestamp_list[i] for i in range(len(self.comp_timestamp_list))]


class RepresentationWrapper(nn.Module):
    def __init__(self, org_module, method='tsne', dim=2):
        super().__init__()
        self.org_module = org_module
        self.method = method
        self.dim = dim
        self.transformed_list = list()

    @staticmethod
    def normalize(np_mat):
        min_values = np.min(np_mat, axis=0, keepdims=True)
        max_values = np.max(np_mat, axis=0, keepdims=True)
        return (np_mat - min_values) / (max_values - min_values)

    def transform_by_tsne(self, np_flat_output):
        transformed_output = TSNE(n_components=self.dim).fit_transform(np_flat_output)
        return self.normalize(transformed_output)

    def forward(self, *input):
        output = self.org_module(*input)
        np_flat_output = output.clone().cpu().detach().flatten(1).numpy()
        if self.method == 'tsne':
            transformed_output = self.transform_by_tsne(np_flat_output)
        else:
            transformed_output = self.normalize(np_flat_output)

        self.transformed_list.append(transformed_output)
        return output

    def get_transformed_list(self):
        return self.transformed_list.copy()
"
84,2201.02656,"import argparse
import os
from solver import Solver
from data_loader import get_loader
from torch.backends import cudnn
import random

def main(config):
    cudnn.benchmark = True
    if config.model_type not in ['U_Net','R2U_Net','AttU_Net','R2AttU_Net', 'MixU_Net', 'MixAttU_Net', 'MixR2U_Net', 'MixR2AttU_Net', 'GhostU_Net', 'GhostU_Net1', 'GhostU_Net2']:
        print('ERROR!! model_type should be selected in U_Net/R2U_Net/AttU_Net/R2AttU_Net')
        print('Your input for model_type was %s'%config.model_type)
        return

    # Create directories if not exist
    if not os.path.exists(config.model_path):
        os.makedirs(config.model_path)
    if not os.path.exists(config.result_path):
        os.makedirs(config.result_path)
    config.result_path = os.path.join(config.result_path,config.model_type)
    if not os.path.exists(config.result_path):
        os.makedirs(config.result_path)
    
    #lr = random.random()*0.0005 + 0.0000005
    #augmentation_prob= random.random()*0.7
    #epoch = random.choice([100,150,200,250])
    #decay_ratio = random.random()*0.8
    #decay_epoch = int(epoch*decay_ratio)

    #config.augmentation_prob = augmentation_prob
    #config.num_epochs = epoch
    #config.lr = lr
    #config.lr = config.lr + lr
    #config.num_epochs_decay = decay_epoch

    print(config)
    #raise


    train_loader = get_loader(image_path=config.train_path,
                            image_size=config.image_size,
                            batch_size=1, #config.batch_size
                            num_workers=config.num_workers,
                            mode='train',
                            augmentation_prob=config.augmentation_prob)
    valid_loader = get_loader(image_path=config.valid_path,
                            image_size=config.image_size,
                            batch_size=1,
                            num_workers=config.num_workers,
                            mode='valid',
                            augmentation_prob=0.)
    test_loader = get_loader(image_path=config.test_path,
                            image_size=config.image_size,
                            batch_size=1,
                            num_workers=config.num_workers,
                            mode='test',
                            augmentation_prob=0.)

    solver = Solver(config, train_loader, valid_loader, test_loader)

    
    # Train and sample the images
    if config.mode == 'train':
        solver.train()
    elif config.mode == 'test':
        solver.test()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    
    # model hyper-parameters
    parser.add_argument('--image_size', type=int, default=224)
    parser.add_argument('--t', type=int, default=3, help='t for Recurrent step of R2U_Net or R2AttU_Net')
    
    # training hyper-parameters
    parser.add_argument('--img_ch', type=int, default=3)
    parser.add_argument('--output_ch', type=int, default=1)
    parser.add_argument('--num_epochs', type=int, default=100)
    parser.add_argument('--num_epochs_decay', type=int, default=10)
    parser.add_argument('--epochs_decay_rate', type=float, default=0.1)
    parser.add_argument('--batch_size', type=int, default=4)
    parser.add_argument('--num_workers', type=int, default=8)
    parser.add_argument('--lr', type=float, default=0.001)
    parser.add_argument('--beta1', type=float, default=0.5)        # momentum1 in Adam
    parser.add_argument('--beta2', type=float, default=0.999)      # momentum2 in Adam    
    parser.add_argument('--augmentation_prob', type=float, default=0.7)

    parser.add_argument('--log_step', type=int, default=2)
    parser.add_argument('--val_step', type=int, default=2)

    # misc
    parser.add_argument('--mode', type=str, default='train')
    parser.add_argument('--model_type', type=str, default='U_Net', help='U_Net/R2U_Net/AttU_Net/R2AttU_Net/MixU_Net/MixAttU_Net/GhostU_Net/GhostU_Net1/GhostU_Net2')
    parser.add_argument('--model_path', type=str, default='./models/luna')
    parser.add_argument('--train_path', type=str, default='./datasets/luna_dataset/train/')
    parser.add_argument('--valid_path', type=str, default='./datasets/luna_dataset/train/')
    parser.add_argument('--test_path', type=str, default='./datasets/luna_dataset/test/')
    parser.add_argument('--result_path', type=str, default='./result/luna/')

    parser.add_argument('--cuda_idx', type=int, default=1)

    config = parser.parse_args()
    main(config)
"
85,2201.02656,"import os
import random
from random import shuffle
import numpy as np
import torch
from torch.utils import data
from torchvision import transforms as T
from torchvision.transforms import functional as F
from PIL import Image

class ImageFolder(data.Dataset):
	def __init__(self, root,image_size=224,mode='train',augmentation_prob=0.4):
		""""""Initializes image paths and preprocessing module.""""""
		self.root = root
		
		# GT : Ground Truth
		self.GT_paths = root[:-1]+'_GT/'
		self.image_paths = list(map(lambda x: os.path.join(root, x), os.listdir(root)))
		self.image_size = image_size
		self.mode = mode
		self.RotationDegree = [0,90,180,270]
		self.augmentation_prob = augmentation_prob
		print(""image count in {} path :{}"".format(self.mode,len(self.image_paths)))

	def __getitem__(self, index):
		""""""Reads an image from a file and preprocesses it and returns.""""""
		image_path = self.image_paths[index]

		#filename = image_path.split('_')[-1][:-len("".jpg"")]
		#GT_path = self.GT_paths + 'ISIC_' + filename + '_segmentation.png'

		filename = os.path.basename(image_path).split('.')[0]
		GT_path = self.GT_paths + filename + '_segmentation.png'
		#GT_path = self.GT_paths + filename + '.png' #for dsb2018 only

		#image = Image.open(image_path)
		image = Image.open(image_path).convert('RGB') #for luna_dataset only

		GT = Image.open(GT_path)
		#GT = Image.open(GT_path).convert('L') #for skin_dataset only

		#print (image.size)
		#raise

		aspect_ratio = image.size[1]/image.size[0]

		Transform = []

		#ResizeRange = random.randint(250,270)
		#ResizeRange = random.randint(90,100)
		#Transform.append(T.Resize((int(ResizeRange*aspect_ratio),ResizeRange)))
		p_transform = random.random()

		if (self.mode == 'train') and p_transform <= self.augmentation_prob:
			RotationDegree = random.randint(0,3)
			RotationDegree = self.RotationDegree[RotationDegree]
			if (RotationDegree == 90) or (RotationDegree == 270):
				aspect_ratio = 1/aspect_ratio

			Transform.append(T.RandomRotation((RotationDegree,RotationDegree)))
						
			RotationRange = random.randint(-10,10)
			Transform.append(T.RandomRotation((RotationRange,RotationRange)))
			CropRange = random.randint(250,270)
			#CropRange = random.randint(90,100)
			Transform.append(T.CenterCrop((int(CropRange*aspect_ratio),CropRange)))
			Transform = T.Compose(Transform)
			
			image = Transform(image)
			GT = Transform(GT)

			ShiftRange_left = random.randint(0,20)
			ShiftRange_upper = random.randint(0,20)
			ShiftRange_right = image.size[0] - random.randint(0,20)
			ShiftRange_lower = image.size[1] - random.randint(0,20)
			image = image.crop(box=(ShiftRange_left,ShiftRange_upper,ShiftRange_right,ShiftRange_lower))
			GT = GT.crop(box=(ShiftRange_left,ShiftRange_upper,ShiftRange_right,ShiftRange_lower))

			if random.random() < 0.5:
				image = F.hflip(image)
				GT = F.hflip(GT)

			if random.random() < 0.5:
				image = F.vflip(image)
				GT = F.vflip(GT)

			Transform = T.ColorJitter(brightness=0.2,contrast=0.2,hue=0.02)

			image = Transform(image)

			Transform =[]


		#Transform.append(T.Resize((int(256*aspect_ratio)-int(256*aspect_ratio)%16,256)))
		#Transform.append(T.Resize((192,256))) #skin_dataset
		#Transform.append(T.Resize((48,48))) #drive_dataset
		#Transform.append(T.Resize((576,576))) #drive_whole_dataset
		#Transform.append(T.Resize((960,960))) #CHASEDB1_whole_dataset
		Transform.append(T.Resize((256,256))) #luna_dataset
		#Transform.append(T.Resize((96,96))) #dsb2018

		#Transform.append(T.Resize((224,224))) #by heng

		Transform.append(T.ToTensor())
		Transform = T.Compose(Transform)
		
		image = Transform(image)
		GT = Transform(GT)

		Norm_ = T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
		image = Norm_(image)

		return image, GT, filename

	def __len__(self):
		""""""Returns the total number of font files.""""""
		return len(self.image_paths)

def get_loader(image_path, image_size, batch_size, num_workers=2, mode='train',augmentation_prob=0.4):
	""""""Builds and returns Dataloader.""""""
	
	dataset = ImageFolder(root = image_path, image_size =image_size, mode=mode,augmentation_prob=augmentation_prob)
	data_loader = data.DataLoader(dataset=dataset,
								  batch_size=batch_size,
								  shuffle=True,
								  num_workers=num_workers)
	return data_loader
"
86,2201.02656,"import os
import numpy as np
import time
import datetime
import torch
import torchvision
from torch import optim
from torch.autograd import Variable
import torch.nn.functional as F
from evaluation import *
from network import U_Net,R2U_Net,AttU_Net,R2AttU_Net,MixU_Net,MixAttU_Net, MixR2U_Net, MixR2AttU_Net, GhostU_Net, GhostU_Net1, GhostU_Net2
import csv
from skimage.io import imsave, imread
import matplotlib.pyplot as plt
from thop import profile
from torchstat import stat

activation = {}
def get_activation(name):
    def hook(model, input, output):
        activation[name] = output.detach()
    return hook


class Solver(object):
	def __init__(self, config, train_loader, valid_loader, test_loader):

		# Data loader
		self.train_loader = train_loader
		self.valid_loader = valid_loader
		self.test_loader = test_loader

		# Models
		self.unet = None
		self.optimizer = None
		self.img_ch = config.img_ch
		self.output_ch = config.output_ch
		self.criterion = torch.nn.BCELoss()
		self.augmentation_prob = config.augmentation_prob

		# Hyper-parameters
		self.lr = config.lr
		self.beta1 = config.beta1
		self.beta2 = config.beta2

		# Training settings
		self.num_epochs = config.num_epochs
		self.num_epochs_decay = config.num_epochs_decay
		self.epochs_decay_rate = config.epochs_decay_rate
		self.batch_size = config.batch_size


		# Step size
		self.log_step = config.log_step
		self.val_step = config.val_step

		# Path
		self.model_path = config.model_path
		self.result_path = config.result_path
		self.mode = config.mode

		self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
		self.model_type = config.model_type
		self.t = config.t
		self.build_model()

	def build_model(self):
		""""""Build generator and discriminator.""""""
		if self.model_type =='U_Net':
			self.unet = U_Net(img_ch=self.img_ch,output_ch=1)
		elif self.model_type =='R2U_Net':
			self.unet = R2U_Net(img_ch=self.img_ch,output_ch=1,t=self.t)
		elif self.model_type =='AttU_Net':
			self.unet = AttU_Net(img_ch=self.img_ch,output_ch=1)
		elif self.model_type == 'R2AttU_Net':
			self.unet = R2AttU_Net(img_ch=self.img_ch,output_ch=1,t=self.t)
		elif self.model_type =='MixU_Net':
			self.unet = MixU_Net(img_ch=self.img_ch,output_ch=1)
		elif self.model_type =='MixAttU_Net':
			self.unet = MixAttU_Net(img_ch=self.img_ch,output_ch=1)
		elif self.model_type =='MixR2U_Net':
			self.unet = MixR2U_Net(img_ch=self.img_ch,output_ch=1)
		elif self.model_type =='MixR2AttU_Net':
			self.unet = MixR2AttU_Net(img_ch=self.img_ch,output_ch=1)
		elif self.model_type =='GhostU_Net':
			self.unet = GhostU_Net(img_ch=self.img_ch,output_ch=1)
		elif self.model_type =='GhostU_Net1':
			self.unet = GhostU_Net1(img_ch=self.img_ch,output_ch=1)
		elif self.model_type =='GhostU_Net2':
			self.unet = GhostU_Net2(img_ch=self.img_ch,output_ch=1)

		#pytorch_total_params = sum(p.numel() for p in self.unet.parameters() if p.requires_grad)
		#print (pytorch_total_params)
		#raise
		self.optimizer = optim.Adam(list(self.unet.parameters()),
									  self.lr, [self.beta1, self.beta2])
		self.unet.to(self.device)

		# self.print_network(self.unet, self.model_type)

	def print_network(self, model, name):
		""""""Print out the network information.""""""
		num_params = 0
		for p in model.parameters():
			num_params += p.numel()
		print(model)
		print(name)
		print(""The number of parameters: {}"".format(num_params))

	def to_data(self, x):
		""""""Convert variable to tensor.""""""
		if torch.cuda.is_available():
			x = x.cpu()
		return x.data

	def update_lr(self, g_lr, d_lr):
		for param_group in self.optimizer.param_groups:
			param_group['lr'] = lr

	def reset_grad(self):
		""""""Zero the gradient buffers.""""""
		self.unet.zero_grad()

	def compute_accuracy(self,SR,GT):
		SR_flat = SR.view(-1)
		GT_flat = GT.view(-1)

		acc = GT_flat.data.cpu()==(SR_flat.data.cpu()>0.5)

	def tensor2img(self,x):
		img = (x[:,0,:,:]>x[:,1,:,:]).float()
		img = img*255
		return img


	def train(self):
		""""""Train encoder, generator and discriminator.""""""

		#====================================== Training ===========================================#
		#===========================================================================================#
		
		#unet_path = os.path.join(self.model_path, '%s-%d-%.4f-%d-%.4f.pkl' %(self.model_type,self.num_epochs,self.lr,self.num_epochs_decay,self.augmentation_prob))

		unet_path = os.path.join(self.model_path, '%s-%d-%.4f-%d-%.4f.pkl' %(self.model_type,self.num_epochs,self.lr,self.batch_size,self.augmentation_prob))

		print (unet_path)

		# U-Net Train
		if os.path.isfile(unet_path):
			# Load the pretrained Encoder
			self.unet.load_state_dict(torch.load(unet_path))
			print('%s is Successfully Loaded from %s'%(self.model_type,unet_path))
			#=================================== Test ==============================================#
			print ('testing the model ...')

			self.unet.Conv1.register_forward_hook(get_activation('Conv1'))

			self.unet.train(False)
			self.unet.eval()


			acc = 0.	# Accuracy
			SE = 0.		# Sensitivity (Recall)
			SP = 0.		# Specificity
			PC = 0. 	# Precision
			F1 = 0.		# F1 Score
			JS = 0.		# Jaccard Similarity
			DC = 0.		# Dice Coefficient
			length=0
			for i, (images, GT, filename) in enumerate(self.valid_loader):

				images = images.to(self.device)
				print (images.size())
				#continue

				model = self.unet
				flops, params = profile(model, (images,))
				print (flops, params)			
				raise



				GT = GT.to(self.device)
				SR = F.sigmoid(self.unet(images))
				#SR = self.unet(images)

				SR = (SR > 0.5).type(torch.uint8)
				#GT = (GT > torch.max(GT)/2).type(torch.uint8)

				#TP = ((SR==1).type(torch.uint8)+(GT==1).type(torch.uint8))==2
				#FN = ((SR==0).type(torch.uint8)+(GT==1).type(torch.uint8))==2

				#print ((SR==1).data.cpu().numpy().max())
				#print ((GT==1).data.cpu().numpy().max())
				#print (TP.data.cpu().numpy().max())

				#SE = float(torch.sum(TP))/(float(torch.sum(TP+FN)) + 1e-6)
				#print ('float(torch.sum(TP)): ', float(torch.sum(TP)))

				#print (type(SR.data.cpu().numpy()))
				#print(SR.data.cpu().numpy()[0,0,...].shape)

				#print (type(filename))
				#print (SR.data.cpu().numpy().shape)
				#raise

				img_save_path = os.path.join(self.result_path, 'res_vis')
				os.makedirs(img_save_path, exist_ok=True)

				#for i in range(len(filename)):
				#	#imsave(f'vis/skin/unet/{filename[0]}_GT.jpg', GT.data.cpu().numpy()[0,0,...]*255)				
				#	imsave(os.path.join(img_save_path, f'{filename[i]}_SR.jpg'), SR.data.cpu().numpy()[i,0,...]*255)
				#	#imsave(f'vis/skin/unet/{filename[0]}_images.jpg', images.data.cpu().numpy()[0,0,...])
				#continue

				act = activation['Conv1'].squeeze()
				print (act.size())
				print (act.max())
				print (act.min())
				act = (act-act.min())/(act.max()-act.min())
				print (act.max())
				print (act.min())
				raise
				row_n = 8
				for img_i in range(act.size(0)):
					fig, axarr = plt.subplots(row_n, act.size(1)//row_n, gridspec_kw = {'wspace':0.03, 'hspace':0.03})
					for idx_x in range(row_n):
						for idx_y in range(act.size(1)//row_n):
							axarr[idx_x, idx_y].imshow(act[img_i, idx_x*row_n+idx_y].cpu())
							axarr[idx_x, idx_y].axis('off')
					#plt.show()
					save_path = os.path.join(self.result_path, 'feature_img_train_Conv1')
					os.makedirs(save_path, exist_ok=True)
					f_name = filename[img_i]
					plt.savefig(os.path.join(save_path, f'{f_name}.jpg'))
					#print (filename)
					#print (self.result_path)
				#raise


				acc += get_accuracy(SR,GT)
				SE += get_sensitivity(SR,GT)
				SP += get_specificity(SR,GT)
				PC += get_precision(SR,GT)
				F1 += get_F1(SR,GT)
				JS += get_JS(SR,GT)
				DC += get_DC(SR,GT)
						
				#length += images.size(0)
				length += 1
					
			acc = acc/length
			SE = SE/length
			SP = SP/length
			PC = PC/length
			F1 = F1/length
			JS = JS/length
			DC = DC/length

			print('[Training] Acc: %.4f, SE: %.4f, SP: %.4f, PC: %.4f, F1: %.4f, JS: %.4f, DC: %.4f' % (
					  acc,SE,SP,PC,F1,JS,DC))

			f = open(os.path.join(self.result_path,'result_test.csv'), 'a', encoding='utf-8', newline='')
			wr = csv.writer(f)
			wr.writerow(['model_type','acc','SE','SP','PC','F1','JS','DC'])
			wr.writerow([self.model_type,acc,SE,SP,PC,F1,JS,DC])
			f.close()
			

		else:

			print ('training the model ...')
			# Train for Encoder
			lr = self.lr
			#best_unet_score = 0.
			best_unet_score = 1000
			best_train_loss = 1000
			best_train_epoch = 0

			for epoch in range(self.num_epochs):

				self.unet.train(True)
				epoch_loss = 0
				
				acc = 0.	# Accuracy
				SE = 0.		# Sensitivity (Recall)
				SP = 0.		# Specificity
				PC = 0. 	# Precision
				F1 = 0.		# F1 Score
				JS = 0.		# Jaccard Similarity
				DC = 0.		# Dice Coefficient
				length = 0

				for i, (images, GT, filename) in enumerate(self.train_loader):
					# GT : Ground Truth
					#print (images.shape)
					#print (GT.shape)
					#raise

					#model = self.unet
					#flops, params = profile(model, (images,))
					#print (flops, params)			
					#raise


					images = images.to(self.device)
					GT = GT.to(self.device)

					# SR : Segmentation Result
					SR = self.unet(images)
					SR_probs = F.sigmoid(SR)
					SR_flat = SR_probs.view(SR_probs.size(0),-1)

					GT_flat = GT.view(GT.size(0),-1)
					loss = self.criterion(SR_flat,GT_flat)
					epoch_loss += loss.item()

					# Backprop + optimize
					self.reset_grad()
					loss.backward()
					self.optimizer.step()

					acc += get_accuracy(SR,GT)
					SE += get_sensitivity(SR,GT)
					SP += get_specificity(SR,GT)
					PC += get_precision(SR,GT)
					F1 += get_F1(SR,GT)
					JS += get_JS(SR,GT)
					DC += get_DC(SR,GT)

					#length += images.size(0)
					length += 1

				acc = acc/length
				SE = SE/length
				SP = SP/length
				PC = PC/length
				F1 = F1/length
				JS = JS/length
				DC = DC/length

				# Print the log info
				print('Epoch [%d/%d], Loss: %.4f, \n[Training] Acc: %.4f, SE: %.4f, SP: %.4f, PC: %.4f, F1: %.4f, JS: %.4f, DC: %.4f' % (
					  epoch+1, self.num_epochs, \
					  epoch_loss,\
					  acc,SE,SP,PC,F1,JS,DC))

			

				# Decay learning rate
				#if (epoch+1) > (self.num_epochs - self.num_epochs_decay):
					#lr -= (self.lr / float(self.num_epochs_decay))
					#for param_group in self.optimizer.param_groups:
						#param_group['lr'] = lr
					#print ('Decay learning rate to lr: {}.'.format(lr))

				if epoch_loss < best_train_loss:
					best_train_epoch = epoch

				if (epoch-best_train_epoch) > self.num_epochs_decay:
					best_train_epoch = epoch
					lr = self.lr * self.epochs_decay_rate
					for param_group in self.optimizer.param_groups:
						param_group['lr'] = lr
					print ('Decay learning rate to lr: {}.'.format(lr))				
				
				#===================================== Validation ====================================#
				self.unet.train(False)
				self.unet.eval()

				acc = 0.	# Accuracy
				SE = 0.		# Sensitivity (Recall)
				SP = 0.		# Specificity
				PC = 0. 	# Precision
				F1 = 0.		# F1 Score
				JS = 0.		# Jaccard Similarity
				DC = 0.		# Dice Coefficient
				length=0
				val_loss = 0

				for i, (images, GT, filename) in enumerate(self.test_loader):

					images = images.to(self.device)
					GT = GT.to(self.device)
					SR = F.sigmoid(self.unet(images))

					SR_flat = SR.view(SR.size(0),-1)

					GT_flat = GT.view(GT.size(0),-1)
					loss = self.criterion(SR_flat,GT_flat)
					val_loss += loss.item()

					acc += get_accuracy(SR,GT)
					SE += get_sensitivity(SR,GT)
					SP += get_specificity(SR,GT)
					PC += get_precision(SR,GT)
					F1 += get_F1(SR,GT)
					JS += get_JS(SR,GT)
					DC += get_DC(SR,GT)
						
					#length += images.size(0)
					length += 1					

				acc = acc/length
				SE = SE/length
				SP = SP/length
				PC = PC/length
				F1 = F1/length
				JS = JS/length
				DC = DC/length
				#unet_score = JS + DC
				unet_score = val_loss

				print('[Validation] Acc: %.4f, SE: %.4f, SP: %.4f, PC: %.4f, F1: %.4f, JS: %.4f, DC: %.4f'%(acc,SE,SP,PC,F1,JS,DC))
				
				'''
				torchvision.utils.save_image(images.data.cpu(),
											os.path.join(self.result_path,
														'%s_valid_%d_image.png'%(self.model_type,epoch+1)))
				torchvision.utils.save_image(SR.data.cpu(),
											os.path.join(self.result_path,
														'%s_valid_%d_SR.png'%(self.model_type,epoch+1)))
				torchvision.utils.save_image(GT.data.cpu(),
											os.path.join(self.result_path,
														'%s_valid_%d_GT.png'%(self.model_type,epoch+1)))
				'''


				# Save Best U-Net model
				if unet_score < best_unet_score:
					best_unet_score = unet_score
					best_epoch = epoch
					best_unet = self.unet.state_dict()
					print('Best %s model score : %.4f'%(self.model_type,best_unet_score))
					torch.save(best_unet,unet_path)
					
			#===================================== test ====================================#
			del self.unet
			del best_unet
			self.build_model()
			self.unet.load_state_dict(torch.load(unet_path))
			
			self.unet.train(False)
			self.unet.eval()

			acc = 0.	# Accuracy
			SE = 0.		# Sensitivity (Recall)
			SP = 0.		# Specificity
			PC = 0. 	# Precision
			F1 = 0.		# F1 Score
			JS = 0.		# Jaccard Similarity
			DC = 0.		# Dice Coefficient
			length=0
			for i, (images, GT, filename) in enumerate(self.test_loader):

				images = images.to(self.device)
				GT = GT.to(self.device)
				SR = F.sigmoid(self.unet(images))
				acc += get_accuracy(SR,GT)
				SE += get_sensitivity(SR,GT)
				SP += get_specificity(SR,GT)
				PC += get_precision(SR,GT)
				F1 += get_F1(SR,GT)
				JS += get_JS(SR,GT)
				DC += get_DC(SR,GT)
						
				#length += images.size(0)
				length += 1					

			acc = acc/length
			SE = SE/length
			SP = SP/length
			PC = PC/length
			F1 = F1/length
			JS = JS/length
			DC = DC/length


			f = open(os.path.join(self.result_path,'result_val.csv'), 'a', encoding='utf-8', newline='')
			wr = csv.writer(f)
			wr.writerow(['model_type','acc','SE','SP','PC','F1','JS','DC','lr','best_epoch'])
			wr.writerow([self.model_type,acc,SE,SP,PC,F1,JS,DC,self.lr,best_epoch])
			f.close()
			














			
"
87,2201.02656,"import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import init
from ghostnet import *

def init_weights(net, init_type='normal', gain=0.02):
    def init_func(m):
        classname = m.__class__.__name__
        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):
            if init_type == 'normal':
                init.normal_(m.weight.data, 0.0, gain)
            elif init_type == 'xavier':
                init.xavier_normal_(m.weight.data, gain=gain)
            elif init_type == 'kaiming':
                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
            elif init_type == 'orthogonal':
                init.orthogonal_(m.weight.data, gain=gain)
            else:
                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)
            if hasattr(m, 'bias') and m.bias is not None:
                init.constant_(m.bias.data, 0.0)
        elif classname.find('BatchNorm2d') != -1:
            init.normal_(m.weight.data, 1.0, gain)
            init.constant_(m.bias.data, 0.0)

    print('initialize network with %s' % init_type)
    net.apply(init_func)

class conv_block(nn.Module):
    def __init__(self,ch_in,ch_out):
        super(conv_block,self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),
            nn.BatchNorm2d(ch_out),
            nn.ReLU(inplace=True),
            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),
            nn.BatchNorm2d(ch_out),
            nn.ReLU(inplace=True)
        )


    def forward(self,x):
        x = self.conv(x)
        return x

class mix_conv_block(nn.Module):
    def __init__(self,ch_in,ch_out):
        super(mix_conv_block,self).__init__()
        self.conv3 = nn.Sequential(
            nn.Conv2d(ch_in, ch_out//4, kernel_size=3,stride=1,padding=1,bias=True),
            nn.BatchNorm2d(ch_out//4),
            nn.ReLU(inplace=True)
        )
        self.conv5 = nn.Sequential(
            nn.Conv2d(ch_in, ch_out//4, kernel_size=5,stride=1,padding=2,bias=True),
            nn.BatchNorm2d(ch_out//4),
            nn.ReLU(inplace=True)
        )
        self.conv7 = nn.Sequential(
            nn.Conv2d(ch_in, ch_out//4, kernel_size=7,stride=1,padding=3,bias=True),
            nn.BatchNorm2d(ch_out//4),
            nn.ReLU(inplace=True)
        )
        self.conv = nn.Sequential(
            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),
            nn.BatchNorm2d(ch_out),
            nn.ReLU(inplace=True)
        )

    def forward(self,x):
        x3 = self.conv3(x)
        x5 = self.conv5(x)
        x7 = self.conv7(x)
        x33 = self.conv3(x)
        x = torch.cat((x3,x5,x7,x33),dim=1)
        x = self.conv(x)
        return x


class up_conv(nn.Module):
    def __init__(self,ch_in,ch_out):
        super(up_conv,self).__init__()
        self.up = nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),
		    nn.BatchNorm2d(ch_out),
			nn.ReLU(inplace=True)
        )

    def forward(self,x):
        x = self.up(x)
        return x

class Recurrent_block(nn.Module):
    def __init__(self,ch_out, t=2, kernel_size=3, padding=1):
        super(Recurrent_block,self).__init__()
        self.t = t
        self.ch_out = ch_out
        self.conv = nn.Sequential(
            nn.Conv2d(ch_out,ch_out,kernel_size=kernel_size,stride=1,padding=padding,bias=True),
		    nn.BatchNorm2d(ch_out),
			nn.ReLU(inplace=True)
        )

    def forward(self,x):
        for i in range(self.t):

            if i==0:
                x1 = self.conv(x)
            
            x1 = self.conv(x+x1)
        return x1


class RRCNN_block(nn.Module):
    def __init__(self,ch_in,ch_out,t=2):
        super(RRCNN_block,self).__init__()
        self.RCNN = nn.Sequential(
            Recurrent_block(ch_out,t=t),
            Recurrent_block(ch_out,t=t)
        )
        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)

    def forward(self,x):
        x = self.Conv_1x1(x)
        x1 = self.RCNN(x)
        return x+x1


class mix_RRCNN_block(nn.Module):
    def __init__(self,ch_in,ch_out,t=2):
        super(mix_RRCNN_block,self).__init__()

        self.RCNN3 = nn.Sequential(
            Recurrent_block(ch_out//4,t=t, kernel_size=3),
            Recurrent_block(ch_out//4,t=t, kernel_size=3)
        )

        self.RCNN5 = nn.Sequential(
            Recurrent_block(ch_out//4,t=t, kernel_size=5, padding=2),
            Recurrent_block(ch_out//4,t=t, kernel_size=5, padding=2)
        )

        self.RCNN7 = nn.Sequential(
            Recurrent_block(ch_out//4,t=t, kernel_size=7, padding=3),
            Recurrent_block(ch_out//4,t=t, kernel_size=7, padding=3)
        )

        #self.RCNN3 = Recurrent_block(ch_out//4,t=t, kernel_size=3)
        #self.RCNN5 = Recurrent_block(ch_out//4,t=t, kernel_size=5, padding=2)
        #self.RCNN7 = Recurrent_block(ch_out//4,t=t, kernel_size=7, padding=3)
        self.RCNN = Recurrent_block(ch_out,t=t)
        self.Conv_1x1_0 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)
        self.Conv_1x1_1 = nn.Conv2d(ch_out,ch_out//4,kernel_size=1,stride=1,padding=0)

    def forward(self,x):
        x0 = self.Conv_1x1_0(x)
        x = self.Conv_1x1_1(x0)
        x3 = self.RCNN3(x)
        x5 = self.RCNN5(x)
        x7 = self.RCNN7(x)
        x33 = self.RCNN3(x)
        x_cat = torch.cat((x3,x5,x7,x33),dim=1)
        #x1 = self.RCNN(x_cat)
        return x0+x_cat

'''
class mix_RRCNN_block(nn.Module):
    def __init__(self,ch_in,ch_out,t=2):
        super(mix_RRCNN_block,self).__init__()

        self.RCNN3 = nn.Sequential(
            Recurrent_block(ch_out//4,t=t, kernel_size=3),
            Recurrent_block(ch_out//4,t=t, kernel_size=3)
        )

        self.RCNN5 = nn.Sequential(
            Recurrent_block(ch_out//4,t=t, kernel_size=5, padding=2),
            Recurrent_block(ch_out//4,t=t, kernel_size=5, padding=2)
        )

        self.RCNN7 = nn.Sequential(
            Recurrent_block(ch_out//4,t=t, kernel_size=7, padding=3),
            Recurrent_block(ch_out//4,t=t, kernel_size=7, padding=3)
        )

        #self.RCNN3 = Recurrent_block(ch_out//4,t=t, kernel_size=3)
        #self.RCNN5 = Recurrent_block(ch_out//4,t=t, kernel_size=5, padding=2)
        #self.RCNN7 = Recurrent_block(ch_out//4,t=t, kernel_size=7, padding=3)
        #self.RCNN = Recurrent_block(ch_out,t=t)
        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)

    def forward(self,x):
        x = self.Conv_1x1(x)
        x0 = torch.chunk(x, 4, dim=1)
        x3 = self.RCNN3(x0[0])
        x5 = self.RCNN5(x0[1])
        x7 = self.RCNN7(x0[2])
        x33 = self.RCNN3(x0[3])
        x_cat = torch.cat((x3,x5,x7,x33),dim=1)
        #x1 = self.RCNN(x_cat)
        return x+x_cat
'''

'''
class mix_RRCNN_block(nn.Module):
    def __init__(self,ch_in,ch_out,t=2):
        super(mix_RRCNN_block,self).__init__()
        self.RCNN = nn.Sequential(
            Recurrent_block(ch_out,t=t),
            Recurrent_block(ch_out,t=t)
        )
        self.Conv_1x1 = nn.Conv2d(ch_out,ch_out,kernel_size=1,stride=1,padding=0)
        self.Conv_1x1_3 = nn.Conv2d(ch_in,ch_out//4,kernel_size=3,stride=1,padding=1)
        self.Conv_1x1_5 = nn.Conv2d(ch_in,ch_out//4,kernel_size=5,stride=1,padding=2)
        self.Conv_1x1_7 = nn.Conv2d(ch_in,ch_out//4,kernel_size=7,stride=1,padding=3)

    def forward(self,x):
        x3 = self.Conv_1x1_3(x)
        x5 = self.Conv_1x1_5(x)
        x7 = self.Conv_1x1_7(x)
        x33 = self.Conv_1x1_3(x)
        x = torch.cat((x3,x5,x7,x33),dim=1)
        x = self.Conv_1x1(x)
        x1 = self.RCNN(x)
        return x+x1
'''

class single_conv(nn.Module):
    def __init__(self,ch_in,ch_out):
        super(single_conv,self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),
            nn.BatchNorm2d(ch_out),
            nn.ReLU(inplace=True)
        )

    def forward(self,x):
        x = self.conv(x)
        return x

class Attention_block(nn.Module):
    def __init__(self,F_g,F_l,F_int):
        super(Attention_block,self).__init__()
        self.W_g = nn.Sequential(
            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),
            nn.BatchNorm2d(F_int)
            )
        
        self.W_x = nn.Sequential(
            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),
            nn.BatchNorm2d(F_int)
        )

        self.psi = nn.Sequential(
            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )
        
        self.relu = nn.ReLU(inplace=True)
        
    def forward(self,g,x):
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        psi = self.relu(g1+x1)
        psi = self.psi(psi)

        return x*psi


class U_Net(nn.Module):
    def __init__(self,img_ch=3,output_ch=1):
        super(U_Net,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)

        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)
        self.Conv2 = conv_block(ch_in=64,ch_out=128)
        self.Conv3 = conv_block(ch_in=128,ch_out=256)
        self.Conv4 = conv_block(ch_in=256,ch_out=512)
        self.Conv5 = conv_block(ch_in=512,ch_out=1024)

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)

        self.Up4 = up_conv(ch_in=512,ch_out=256)
        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)
        
        self.Up3 = up_conv(ch_in=256,ch_out=128)
        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)
        
        self.Up2 = up_conv(ch_in=128,ch_out=64)
        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.Conv1(x)

        x2 = self.Maxpool(x1)
        x2 = self.Conv2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.Conv3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.Conv4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.Conv5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        d5 = torch.cat((x4,d5),dim=1)
        
        d5 = self.Up_conv5(d5)
        
        d4 = self.Up4(d5)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_conv4(d4)

        d3 = self.Up3(d4)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_conv3(d3)

        d2 = self.Up2(d3)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_conv2(d2)

        d1 = self.Conv_1x1(d2)

        return d1


class R2U_Net(nn.Module):
    def __init__(self,img_ch=3,output_ch=1,t=2):
        super(R2U_Net,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)
        self.Upsample = nn.Upsample(scale_factor=2)

        self.RRCNN1 = RRCNN_block(ch_in=img_ch,ch_out=64,t=t)

        self.RRCNN2 = RRCNN_block(ch_in=64,ch_out=128,t=t)
        
        self.RRCNN3 = RRCNN_block(ch_in=128,ch_out=256,t=t)
        
        self.RRCNN4 = RRCNN_block(ch_in=256,ch_out=512,t=t)
        
        self.RRCNN5 = RRCNN_block(ch_in=512,ch_out=1024,t=t)
        

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512,t=t)
        
        self.Up4 = up_conv(ch_in=512,ch_out=256)
        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256,t=t)
        
        self.Up3 = up_conv(ch_in=256,ch_out=128)
        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128,t=t)
        
        self.Up2 = up_conv(ch_in=128,ch_out=64)
        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64,t=t)

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.RRCNN1(x)

        x2 = self.Maxpool(x1)
        x2 = self.RRCNN2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.RRCNN3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.RRCNN4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.RRCNN5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        d5 = torch.cat((x4,d5),dim=1)
        d5 = self.Up_RRCNN5(d5)
        
        d4 = self.Up4(d5)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_RRCNN4(d4)

        d3 = self.Up3(d4)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_RRCNN3(d3)

        d2 = self.Up2(d3)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_RRCNN2(d2)

        d1 = self.Conv_1x1(d2)

        return d1



class AttU_Net(nn.Module):
    def __init__(self,img_ch=3,output_ch=1):
        super(AttU_Net,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)

        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)
        self.Conv2 = conv_block(ch_in=64,ch_out=128)
        self.Conv3 = conv_block(ch_in=128,ch_out=256)
        self.Conv4 = conv_block(ch_in=256,ch_out=512)
        self.Conv5 = conv_block(ch_in=512,ch_out=1024)

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)
        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)

        self.Up4 = up_conv(ch_in=512,ch_out=256)
        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)
        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)
        
        self.Up3 = up_conv(ch_in=256,ch_out=128)
        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)
        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)
        
        self.Up2 = up_conv(ch_in=128,ch_out=64)
        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)
        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.Conv1(x)

        x2 = self.Maxpool(x1)
        x2 = self.Conv2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.Conv3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.Conv4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.Conv5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        x4 = self.Att5(g=d5,x=x4)
        d5 = torch.cat((x4,d5),dim=1)        
        d5 = self.Up_conv5(d5)
        
        d4 = self.Up4(d5)
        x3 = self.Att4(g=d4,x=x3)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_conv4(d4)

        d3 = self.Up3(d4)
        x2 = self.Att3(g=d3,x=x2)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_conv3(d3)

        d2 = self.Up2(d3)
        x1 = self.Att2(g=d2,x=x1)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_conv2(d2)

        d1 = self.Conv_1x1(d2)

        return d1


class R2AttU_Net(nn.Module):
    def __init__(self,img_ch=3,output_ch=1,t=2):
        super(R2AttU_Net,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)
        self.Upsample = nn.Upsample(scale_factor=2)

        self.RRCNN1 = RRCNN_block(ch_in=img_ch,ch_out=64,t=t)

        self.RRCNN2 = RRCNN_block(ch_in=64,ch_out=128,t=t)
        
        self.RRCNN3 = RRCNN_block(ch_in=128,ch_out=256,t=t)
        
        self.RRCNN4 = RRCNN_block(ch_in=256,ch_out=512,t=t)
        
        self.RRCNN5 = RRCNN_block(ch_in=512,ch_out=1024,t=t)
        

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)
        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512,t=t)
        
        self.Up4 = up_conv(ch_in=512,ch_out=256)
        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)
        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256,t=t)
        
        self.Up3 = up_conv(ch_in=256,ch_out=128)
        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)
        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128,t=t)
        
        self.Up2 = up_conv(ch_in=128,ch_out=64)
        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)
        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64,t=t)

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.RRCNN1(x)

        x2 = self.Maxpool(x1)
        x2 = self.RRCNN2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.RRCNN3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.RRCNN4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.RRCNN5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        x4 = self.Att5(g=d5,x=x4)
        d5 = torch.cat((x4,d5),dim=1)
        d5 = self.Up_RRCNN5(d5)
        
        d4 = self.Up4(d5)
        x3 = self.Att4(g=d4,x=x3)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_RRCNN4(d4)

        d3 = self.Up3(d4)
        x2 = self.Att3(g=d3,x=x2)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_RRCNN3(d3)

        d2 = self.Up2(d3)
        x1 = self.Att2(g=d2,x=x1)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_RRCNN2(d2)

        d1 = self.Conv_1x1(d2)

        return d1



class MixU_Net(nn.Module):
    def __init__(self,img_ch=3,output_ch=1):
        super(MixU_Net,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)

        self.Conv1 = mix_conv_block(ch_in=img_ch,ch_out=64)
        self.Conv2 = mix_conv_block(ch_in=64,ch_out=128)
        self.Conv3 = mix_conv_block(ch_in=128,ch_out=256)
        self.Conv4 = mix_conv_block(ch_in=256,ch_out=512)
        self.Conv5 = mix_conv_block(ch_in=512,ch_out=1024)

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)

        self.Up4 = up_conv(ch_in=512,ch_out=256)
        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)
        
        self.Up3 = up_conv(ch_in=256,ch_out=128)
        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)
        
        self.Up2 = up_conv(ch_in=128,ch_out=64)
        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.Conv1(x)

        x2 = self.Maxpool(x1)
        x2 = self.Conv2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.Conv3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.Conv4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.Conv5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        d5 = torch.cat((x4,d5),dim=1)
        
        d5 = self.Up_conv5(d5)
        
        d4 = self.Up4(d5)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_conv4(d4)

        d3 = self.Up3(d4)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_conv3(d3)

        d2 = self.Up2(d3)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_conv2(d2)

        d1 = self.Conv_1x1(d2)

        return d1


class MixAttU_Net(nn.Module):
    def __init__(self,img_ch=3,output_ch=1):
        super(MixAttU_Net,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)

        self.Conv1 = mix_conv_block(ch_in=img_ch,ch_out=64)
        self.Conv2 = mix_conv_block(ch_in=64,ch_out=128)
        self.Conv3 = mix_conv_block(ch_in=128,ch_out=256)
        self.Conv4 = mix_conv_block(ch_in=256,ch_out=512)
        self.Conv5 = mix_conv_block(ch_in=512,ch_out=1024)

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)
        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)

        self.Up4 = up_conv(ch_in=512,ch_out=256)
        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)
        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)
        
        self.Up3 = up_conv(ch_in=256,ch_out=128)
        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)
        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)
        
        self.Up2 = up_conv(ch_in=128,ch_out=64)
        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)
        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.Conv1(x)

        x2 = self.Maxpool(x1)
        x2 = self.Conv2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.Conv3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.Conv4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.Conv5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        x4 = self.Att5(g=d5,x=x4)
        d5 = torch.cat((x4,d5),dim=1)        
        d5 = self.Up_conv5(d5)
        
        d4 = self.Up4(d5)
        x3 = self.Att4(g=d4,x=x3)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_conv4(d4)

        d3 = self.Up3(d4)
        x2 = self.Att3(g=d3,x=x2)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_conv3(d3)

        d2 = self.Up2(d3)
        x1 = self.Att2(g=d2,x=x1)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_conv2(d2)

        d1 = self.Conv_1x1(d2)

        return d1



class MixR2U_Net(nn.Module):
    def __init__(self,img_ch=3,output_ch=1,t=2):
        super(MixR2U_Net,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)
        self.Upsample = nn.Upsample(scale_factor=2)

        self.RRCNN1 = mix_RRCNN_block(ch_in=img_ch,ch_out=64,t=t)

        self.RRCNN2 = mix_RRCNN_block(ch_in=64,ch_out=128,t=t)
        
        self.RRCNN3 = mix_RRCNN_block(ch_in=128,ch_out=256,t=t)
        
        self.RRCNN4 = mix_RRCNN_block(ch_in=256,ch_out=512,t=t)
        
        self.RRCNN5 = mix_RRCNN_block(ch_in=512,ch_out=1024,t=t)
        

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512,t=t)
        
        self.Up4 = up_conv(ch_in=512,ch_out=256)
        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256,t=t)
        
        self.Up3 = up_conv(ch_in=256,ch_out=128)
        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128,t=t)
        
        self.Up2 = up_conv(ch_in=128,ch_out=64)
        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64,t=t)

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.RRCNN1(x)

        x2 = self.Maxpool(x1)
        x2 = self.RRCNN2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.RRCNN3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.RRCNN4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.RRCNN5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        d5 = torch.cat((x4,d5),dim=1)
        d5 = self.Up_RRCNN5(d5)
        
        d4 = self.Up4(d5)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_RRCNN4(d4)

        d3 = self.Up3(d4)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_RRCNN3(d3)

        d2 = self.Up2(d3)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_RRCNN2(d2)

        d1 = self.Conv_1x1(d2)

        return d1

class MixR2AttU_Net(nn.Module):
    def __init__(self,img_ch=3,output_ch=1,t=2):
        super(MixR2AttU_Net,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)
        self.Upsample = nn.Upsample(scale_factor=2)

        self.RRCNN1 = mix_RRCNN_block(ch_in=img_ch,ch_out=64,t=t)

        self.RRCNN2 = mix_RRCNN_block(ch_in=64,ch_out=128,t=t)
        
        self.RRCNN3 = mix_RRCNN_block(ch_in=128,ch_out=256,t=t)
        
        self.RRCNN4 = mix_RRCNN_block(ch_in=256,ch_out=512,t=t)
        
        self.RRCNN5 = mix_RRCNN_block(ch_in=512,ch_out=1024,t=t)
        

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)
        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512,t=t)
        
        self.Up4 = up_conv(ch_in=512,ch_out=256)
        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)
        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256,t=t)
        
        self.Up3 = up_conv(ch_in=256,ch_out=128)
        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)
        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128,t=t)
        
        self.Up2 = up_conv(ch_in=128,ch_out=64)
        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)
        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64,t=t)

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.RRCNN1(x)

        x2 = self.Maxpool(x1)
        x2 = self.RRCNN2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.RRCNN3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.RRCNN4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.RRCNN5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        x4 = self.Att5(g=d5,x=x4)
        d5 = torch.cat((x4,d5),dim=1)
        d5 = self.Up_RRCNN5(d5)
        
        d4 = self.Up4(d5)
        x3 = self.Att4(g=d4,x=x3)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_RRCNN4(d4)

        d3 = self.Up3(d4)
        x2 = self.Att3(g=d3,x=x2)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_RRCNN3(d3)

        d2 = self.Up2(d3)
        x1 = self.Att2(g=d2,x=x1)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_RRCNN2(d2)

        d1 = self.Conv_1x1(d2)

        return d1


class GhostU_Net(nn.Module):
    def __init__(self,img_ch=3,output_ch=1):
        super(GhostU_Net,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)

        #self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)
        self.Conv1 = GhostBottleneck(img_ch, 64, 64)

        #self.Conv2 = conv_block(ch_in=64,ch_out=128)
        self.Conv2 = GhostBottleneck(64, 128, 128)

        #self.Conv3 = conv_block(ch_in=128,ch_out=256)
        self.Conv3 = GhostBottleneck(128, 256, 256)

        #self.Conv4 = conv_block(ch_in=256,ch_out=512)
        self.Conv4 = GhostBottleneck(256, 512, 512)

        #self.Conv5 = conv_block(ch_in=512,ch_out=1024)
        self.Conv5 = GhostBottleneck(512, 1024, 1024)

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)

        self.Up4 = up_conv(ch_in=512,ch_out=256)
        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)
        
        self.Up3 = up_conv(ch_in=256,ch_out=128)
        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)
        
        self.Up2 = up_conv(ch_in=128,ch_out=64)
        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.Conv1(x)

        x2 = self.Maxpool(x1)
        x2 = self.Conv2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.Conv3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.Conv4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.Conv5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        d5 = torch.cat((x4,d5),dim=1)
        
        d5 = self.Up_conv5(d5)
        
        d4 = self.Up4(d5)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_conv4(d4)

        d3 = self.Up3(d4)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_conv3(d3)

        d2 = self.Up2(d3)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_conv2(d2)

        d1 = self.Conv_1x1(d2)

        return d1

class GhostU_Net1(nn.Module):
    def __init__(self,img_ch=3,output_ch=1):
        super(GhostU_Net1,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)

        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)
        #self.Conv1 = GhostBottleneck(img_ch, 64, 64)

        self.Conv2 = conv_block(ch_in=64,ch_out=128)
        #self.Conv2 = GhostBottleneck(64, 128, 128)

        self.Conv3 = conv_block(ch_in=128,ch_out=256)
        #self.Conv3 = GhostBottleneck(128, 256, 256)

        self.Conv4 = conv_block(ch_in=256,ch_out=512)
        #self.Conv4 = GhostBottleneck(256, 512, 512)

        self.Conv5 = conv_block(ch_in=512,ch_out=1024)
        #self.Conv5 = GhostBottleneck(512, 1024, 1024)

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        #self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)
        self.Up_conv5 = GhostBottleneck(1024, 512, 512)

        self.Up4 = up_conv(ch_in=512,ch_out=256)
        #self.Up_conv4 = conv_block(ch_in=512, ch_out=256)
        self.Up_conv4 = GhostBottleneck(512, 256, 256)        

        self.Up3 = up_conv(ch_in=256,ch_out=128)
        #self.Up_conv3 = conv_block(ch_in=256, ch_out=128)
        self.Up_conv3 = GhostBottleneck(256, 128, 128)          

        self.Up2 = up_conv(ch_in=128,ch_out=64)
        #self.Up_conv2 = conv_block(ch_in=128, ch_out=64)
        self.Up_conv2 = GhostBottleneck(128, 64, 64) 

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.Conv1(x)

        x2 = self.Maxpool(x1)
        x2 = self.Conv2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.Conv3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.Conv4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.Conv5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        d5 = torch.cat((x4,d5),dim=1)
        
        d5 = self.Up_conv5(d5)
        
        d4 = self.Up4(d5)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_conv4(d4)

        d3 = self.Up3(d4)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_conv3(d3)

        d2 = self.Up2(d3)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_conv2(d2)

        d1 = self.Conv_1x1(d2)

        return d1


class GhostU_Net2(nn.Module):
    def __init__(self,img_ch=3,output_ch=1):
        super(GhostU_Net2,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)

        #self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)
        self.Conv1 = GhostBottleneck(img_ch, 64, 64)

        #self.Conv2 = conv_block(ch_in=64,ch_out=128)
        self.Conv2 = GhostBottleneck(64, 128, 128)

        #self.Conv3 = conv_block(ch_in=128,ch_out=256)
        self.Conv3 = GhostBottleneck(128, 256, 256)

        #self.Conv4 = conv_block(ch_in=256,ch_out=512)
        self.Conv4 = GhostBottleneck(256, 512, 512)

        #self.Conv5 = conv_block(ch_in=512,ch_out=1024)
        self.Conv5 = GhostBottleneck(512, 1024, 1024)

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        #self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)
        self.Up_conv5 = GhostBottleneck(1024, 512, 512)

        self.Up4 = up_conv(ch_in=512,ch_out=256)
        #self.Up_conv4 = conv_block(ch_in=512, ch_out=256)
        self.Up_conv4 = GhostBottleneck(512, 256, 256)        

        self.Up3 = up_conv(ch_in=256,ch_out=128)
        #self.Up_conv3 = conv_block(ch_in=256, ch_out=128)
        self.Up_conv3 = GhostBottleneck(256, 128, 128)          

        self.Up2 = up_conv(ch_in=128,ch_out=64)
        #self.Up_conv2 = conv_block(ch_in=128, ch_out=64)
        self.Up_conv2 = GhostBottleneck(128, 64, 64) 

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.Conv1(x)

        x2 = self.Maxpool(x1)
        x2 = self.Conv2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.Conv3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.Conv4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.Conv5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        d5 = torch.cat((x4,d5),dim=1)
        
        d5 = self.Up_conv5(d5)
        
        d4 = self.Up4(d5)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_conv4(d4)

        d3 = self.Up3(d4)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_conv3(d3)

        d2 = self.Up2(d3)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_conv2(d2)

        d1 = self.Conv_1x1(d2)

        return d1



"
88,2201.02656,"import os
import argparse
import random
import shutil
from shutil import copyfile
from misc import printProgressBar


def rm_mkdir(dir_path):
    if os.path.exists(dir_path):
        shutil.rmtree(dir_path)
        print('Remove path - %s'%dir_path)
    os.makedirs(dir_path)
    print('Create path - %s'%dir_path)

def main(config):

    rm_mkdir(config.train_path)
    rm_mkdir(config.train_GT_path)
    rm_mkdir(config.valid_path)
    rm_mkdir(config.valid_GT_path)
    rm_mkdir(config.test_path)
    rm_mkdir(config.test_GT_path)

    filenames = os.listdir(config.origin_data_path)
    data_list = []
    GT_list = []

    for filename in filenames:
        ext = os.path.splitext(filename)[-1]
        if ext =='.jpg':
            filename = filename.split('_')[-1][:-len('.jpg')]
            data_list.append('ISIC_'+filename+'.jpg')
            GT_list.append('ISIC_'+filename+'_segmentation.png')

    num_total = len(data_list)
    num_train = int((config.train_ratio/(config.train_ratio+config.valid_ratio+config.test_ratio))*num_total)
    num_valid = int((config.valid_ratio/(config.train_ratio+config.valid_ratio+config.test_ratio))*num_total)
    num_test = num_total - num_train - num_valid

    print('\nNum of train set : ',num_train)
    print('\nNum of valid set : ',num_valid)
    print('\nNum of test set : ',num_test)

    Arange = list(range(num_total))
    random.shuffle(Arange)

    for i in range(num_train):
        idx = Arange.pop()
        
        src = os.path.join(config.origin_data_path, data_list[idx])
        dst = os.path.join(config.train_path,data_list[idx])
        copyfile(src, dst)
        
        src = os.path.join(config.origin_GT_path, GT_list[idx])
        dst = os.path.join(config.train_GT_path, GT_list[idx])
        copyfile(src, dst)

        printProgressBar(i + 1, num_train, prefix = 'Producing train set:', suffix = 'Complete', length = 50)
        

    for i in range(num_valid):
        idx = Arange.pop()

        src = os.path.join(config.origin_data_path, data_list[idx])
        dst = os.path.join(config.valid_path,data_list[idx])
        copyfile(src, dst)
        
        src = os.path.join(config.origin_GT_path, GT_list[idx])
        dst = os.path.join(config.valid_GT_path, GT_list[idx])
        copyfile(src, dst)

        printProgressBar(i + 1, num_valid, prefix = 'Producing valid set:', suffix = 'Complete', length = 50)

    for i in range(num_test):
        idx = Arange.pop()

        src = os.path.join(config.origin_data_path, data_list[idx])
        dst = os.path.join(config.test_path,data_list[idx])
        copyfile(src, dst)
        
        src = os.path.join(config.origin_GT_path, GT_list[idx])
        dst = os.path.join(config.test_GT_path, GT_list[idx])
        copyfile(src, dst)


        printProgressBar(i + 1, num_test, prefix = 'Producing test set:', suffix = 'Complete', length = 50)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    
    # model hyper-parameters
    parser.add_argument('--train_ratio', type=float, default=0.6)
    parser.add_argument('--valid_ratio', type=float, default=0.2)
    parser.add_argument('--test_ratio', type=float, default=0.2)

    # data path
    parser.add_argument('--origin_data_path', type=str, default='../ISIC/dataset/ISIC2018_Task1-2_Training_Input')
    parser.add_argument('--origin_GT_path', type=str, default='../ISIC/dataset/ISIC2018_Task1_Training_GroundTruth')
    
    parser.add_argument('--train_path', type=str, default='./dataset/train/')
    parser.add_argument('--train_GT_path', type=str, default='./dataset/train_GT/')
    parser.add_argument('--valid_path', type=str, default='./dataset/valid/')
    parser.add_argument('--valid_GT_path', type=str, default='./dataset/valid_GT/')
    parser.add_argument('--test_path', type=str, default='./dataset/test/')
    parser.add_argument('--test_GT_path', type=str, default='./dataset/test_GT/')

    config = parser.parse_args()
    print(config)
    main(config)"
89,2201.02656,"# 2020.06.09-Changed for building GhostNet
#            Huawei Technologies Co., Ltd. <foss@huawei.com>
""""""
Creates a GhostNet Model as defined in:
GhostNet: More Features from Cheap Operations By Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, Chang Xu.
https://arxiv.org/abs/1911.11907
Modified from https://github.com/d-li14/mobilenetv3.pytorch and https://github.com/rwightman/pytorch-image-models
""""""
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


def _make_divisible(v, divisor, min_value=None):
    """"""
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by 8
    It can be seen here:
    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    """"""
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


def hard_sigmoid(x, inplace: bool = False):
    if inplace:
        return x.add_(3.).clamp_(0., 6.).div_(6.)
    else:
        return F.relu6(x + 3.) / 6.


class SqueezeExcite(nn.Module):
    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None,
                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):
        super(SqueezeExcite, self).__init__()
        self.gate_fn = gate_fn
        reduced_chs = _make_divisible((reduced_base_chs or in_chs) * se_ratio, divisor)
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv_reduce = nn.Conv2d(in_chs, reduced_chs, 1, bias=True)
        self.act1 = act_layer(inplace=True)
        self.conv_expand = nn.Conv2d(reduced_chs, in_chs, 1, bias=True)

    def forward(self, x):
        x_se = self.avg_pool(x)
        x_se = self.conv_reduce(x_se)
        x_se = self.act1(x_se)
        x_se = self.conv_expand(x_se)
        x = x * self.gate_fn(x_se)
        return x    

    
class ConvBnAct(nn.Module):
    def __init__(self, in_chs, out_chs, kernel_size,
                 stride=1, act_layer=nn.ReLU):
        super(ConvBnAct, self).__init__()
        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size//2, bias=False)
        self.bn1 = nn.BatchNorm2d(out_chs)
        self.act1 = act_layer(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn1(x)
        x = self.act1(x)
        return x


class GhostModule(nn.Module):
    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True): #ratio=2 default, ratio=6 for gpunet
        super(GhostModule, self).__init__()
        self.oup = oup
        init_channels = math.ceil(oup / ratio)
        new_channels = init_channels*(ratio-1)

        self.primary_conv = nn.Sequential(
            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),
            nn.BatchNorm2d(init_channels),
            nn.ReLU(inplace=True) if relu else nn.Sequential(),
        )

        self.cheap_operation = nn.Sequential(
            #nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),
            nn.Conv2d(init_channels, init_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False), #for gpunet
            #nn.BatchNorm2d(new_channels),
            nn.BatchNorm2d(init_channels), #for gpunet
            nn.ReLU(inplace=True) if relu else nn.Sequential(),
        )

        '''
        self.atrous_block1 = nn.Sequential(
            #nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),
            nn.Conv2d(init_channels, init_channels, 1, 1, groups=init_channels, bias=False),
            #nn.Conv2d(init_channels, init_channels, 1, 1, bias=False),
            nn.BatchNorm2d(init_channels),
            nn.ReLU(inplace=True) if relu else nn.Sequential(),
        )
        self.atrous_block6 = nn.Sequential(
            #nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),
            nn.Conv2d(init_channels, init_channels, 3, 1, padding=6, dilation=6, groups=init_channels, bias=False),
            #nn.Conv2d(init_channels, init_channels, 3, 1, padding=6, dilation=6, bias=False),
            nn.BatchNorm2d(init_channels),
            nn.ReLU(inplace=True) if relu else nn.Sequential(),
        )
        self.atrous_block12 = nn.Sequential(
            #nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),
            nn.Conv2d(init_channels, init_channels, 3, 1, padding=12, dilation=12, groups=init_channels, bias=False),
            #nn.Conv2d(init_channels, init_channels, 3, 1, padding=12, dilation=12, bias=False),
            nn.BatchNorm2d(init_channels),
            nn.ReLU(inplace=True) if relu else nn.Sequential(),
        )
        self.atrous_block18 = nn.Sequential(
            #nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),
            nn.Conv2d(init_channels, init_channels, 3, 1, padding=18, dilation=18, groups=init_channels, bias=False),
            #nn.Conv2d(init_channels, init_channels, 3, 1, padding=18, dilation=18, bias=False),
            nn.BatchNorm2d(init_channels),
            nn.ReLU(inplace=True) if relu else nn.Sequential(),
        )

        #self.conv_1x1_output = nn.Conv2d(new_channels * 6, oup, 1, 1)
        '''

    #'''
    def forward(self, x):
        x1 = self.primary_conv(x)
        x2 = self.cheap_operation(x1)
        out = torch.cat([x1,x2], dim=1)
        return out[:,:self.oup,:,:]
    #'''

    '''
    ### aspp+ghost module
    def forward(self, x):
        x1 = self.primary_conv(x)
        x2 = self.cheap_operation(x1)
        atrous_block1 = self.atrous_block1(x1)
        atrous_block6 = self.atrous_block6(x1)
        atrous_block12 = self.atrous_block12(x1)
        atrous_block18 = self.atrous_block18(x1)
        #out = self.conv_1x1_output(torch.cat([x1,x2,atrous_block1,atrous_block6,atrous_block12,atrous_block18], dim=1))
        out = torch.cat([x1,x2,atrous_block1,atrous_block6,atrous_block12,atrous_block18], dim=1)
        return out[:,:self.oup,:,:]
    '''


class GhostBottleneck(nn.Module):
    """""" Ghost bottleneck w/ optional SE""""""

    def __init__(self, in_chs, mid_chs, out_chs, dw_kernel_size=3,
                 stride=1, act_layer=nn.ReLU, se_ratio=0.):
        super(GhostBottleneck, self).__init__()
        has_se = se_ratio is not None and se_ratio > 0.
        self.stride = stride

        # Point-wise expansion
        self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)

        # Depth-wise convolution
        if self.stride > 1:
            self.conv_dw = nn.Conv2d(mid_chs, mid_chs, dw_kernel_size, stride=stride,
                             padding=(dw_kernel_size-1)//2,
                             groups=mid_chs, bias=False)
            self.bn_dw = nn.BatchNorm2d(mid_chs)

        # Squeeze-and-excitation
        if has_se:
            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio)
        else:
            self.se = None

        # Point-wise linear projection
        self.ghost2 = GhostModule(mid_chs, out_chs, relu=False)
        
        # shortcut
        if (in_chs == out_chs and self.stride == 1):
            self.shortcut = nn.Sequential()
        else:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_chs, in_chs, dw_kernel_size, stride=stride,
                       padding=(dw_kernel_size-1)//2, groups=in_chs, bias=False),
                nn.BatchNorm2d(in_chs),
                nn.Conv2d(in_chs, out_chs, 1, stride=1, padding=0, bias=False),
                nn.BatchNorm2d(out_chs),
            )


    def forward(self, x):
        residual = x

        # 1st ghost bottleneck
        x = self.ghost1(x)

        # Depth-wise convolution
        if self.stride > 1:
            x = self.conv_dw(x)
            x = self.bn_dw(x)

        # Squeeze-and-excitation
        if self.se is not None:
            x = self.se(x)

        # 2nd ghost bottleneck
        x = self.ghost2(x)
        
        x += self.shortcut(residual)
        return x


class GhostNet(nn.Module):
    def __init__(self, cfgs, num_classes=1000, width=1.0, dropout=0.2):
        super(GhostNet, self).__init__()
        # setting of inverted residual blocks
        self.cfgs = cfgs
        self.dropout = dropout

        # building first layer
        output_channel = _make_divisible(16 * width, 4)
        self.conv_stem = nn.Conv2d(3, output_channel, 3, 2, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(output_channel)
        self.act1 = nn.ReLU(inplace=True)
        input_channel = output_channel

        # building inverted residual blocks
        stages = []
        block = GhostBottleneck
        for cfg in self.cfgs:
            layers = []
            for k, exp_size, c, se_ratio, s in cfg:
                output_channel = _make_divisible(c * width, 4)
                hidden_channel = _make_divisible(exp_size * width, 4)
                layers.append(block(input_channel, hidden_channel, output_channel, k, s,
                              se_ratio=se_ratio))
                input_channel = output_channel
            stages.append(nn.Sequential(*layers))

        output_channel = _make_divisible(exp_size * width, 4)
        stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))
        input_channel = output_channel
        
        self.blocks = nn.Sequential(*stages)        

        # building last several layers
        output_channel = 1280
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.conv_head = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=True)
        self.act2 = nn.ReLU(inplace=True)
        self.classifier = nn.Linear(output_channel, num_classes)

    def forward(self, x):
        x = self.conv_stem(x)
        x = self.bn1(x)
        x = self.act1(x)
        x = self.blocks(x)
        x = self.global_pool(x)
        x = self.conv_head(x)
        x = self.act2(x)
        x = x.view(x.size(0), -1)
        if self.dropout > 0.:
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.classifier(x)
        return x


def ghostnet(**kwargs):
    """"""
    Constructs a GhostNet model
    """"""
    cfgs = [
        # k, t, c, SE, s 
        # stage1
        [[3,  16,  16, 0, 1]],
        # stage2
        [[3,  48,  24, 0, 2]],
        [[3,  72,  24, 0, 1]],
        # stage3
        [[5,  72,  40, 0.25, 2]],
        [[5, 120,  40, 0.25, 1]],
        # stage4
        [[3, 240,  80, 0, 2]],
        [[3, 200,  80, 0, 1],
         [3, 184,  80, 0, 1],
         [3, 184,  80, 0, 1],
         [3, 480, 112, 0.25, 1],
         [3, 672, 112, 0.25, 1]
        ],
        # stage5
        [[5, 672, 160, 0.25, 2]],
        [[5, 960, 160, 0, 1],
         [5, 960, 160, 0.25, 1],
         [5, 960, 160, 0, 1],
         [5, 960, 160, 0.25, 1]
        ]
    ]
    return GhostNet(cfgs, **kwargs)


if __name__=='__main__':
    model = ghostnet()
    model.eval()
    print(model)
    input = torch.randn(32,3,320,256)
    y = model(input)
    print(y.size())
"
90,2201.02656,"import torch
import numpy as np

# SR : Segmentation Result
# GT : Ground Truth

def get_accuracy(SR,GT,threshold=0.5):
    SR = SR > threshold
    #GT = GT == torch.max(GT)
    GT = GT > torch.max(GT)/2

    corr = torch.sum(SR==GT)
    tensor_size = SR.size(0)*SR.size(1)*SR.size(2)*SR.size(3)
    #print (SR.size(0), SR.size(1), SR.size(2), SR.size(3))
    #print (GT.size(0), GT.size(1), GT.size(2), GT.size(3))

    #corr = torch.sum(GT==GT)
    #print (corr)
    #raise
    acc = float(corr)/float(tensor_size)
    #print ('acc: ', acc)
    #raise
    return acc

def get_sensitivity(SR,GT,threshold=0.5):
    # Sensitivity == Recall
    SR = (SR > threshold).type(torch.uint8)
    #GT = GT == torch.max(GT)
    GT = (GT > torch.max(GT)/2).type(torch.uint8)

    # TP : True Positive
    # FN : False Negative
    TP = ((SR==1).type(torch.uint8)+(GT==1).type(torch.uint8))==2
    FN = ((SR==0).type(torch.uint8)+(GT==1).type(torch.uint8))==2
    #TP = (SR==1)&(GT==1)
    #FN = (SR==0)&(GT==1)

    SE = float(torch.sum(TP))/(float(torch.sum(TP+FN)) + 1e-6)
    #print ('float(torch.sum(TP)): ', float(torch.sum(TP)))
    #raise
    return SE

def get_specificity(SR,GT,threshold=0.5):
    SR = (SR > threshold).type(torch.uint8)
    #GT = GT == torch.max(GT)
    GT = (GT > torch.max(GT)/2).type(torch.uint8)

    # TN : True Negative
    # FP : False Positive
    TN = ((SR==0).type(torch.uint8)+(GT==0).type(torch.uint8))==2
    FP = ((SR==1).type(torch.uint8)+(GT==0).type(torch.uint8))==2

    SP = float(torch.sum(TN))/(float(torch.sum(TN+FP)) + 1e-6)
    
    return SP

def get_precision(SR,GT,threshold=0.5):
    SR = (SR > threshold).type(torch.uint8)
    #GT = GT == torch.max(GT)
    GT = (GT > torch.max(GT)/2).type(torch.uint8)

    #SR = SR.data.cpu().numpy()[0,0,...]
    #GT = GT.data.cpu().numpy()[0,0,...]
    #print (np.sum(SR==1))
    #print (np.sum(((SR==1).astype(int)+(GT==1).astype(int))==2))
    #raise

    # TP : True Positive
    # FP : False Positive
    TP = ((SR==1).type(torch.uint8)+(GT==1).type(torch.uint8))==2
    FP = ((SR==1).type(torch.uint8)+(GT==0).type(torch.uint8))==2

    PC = float(torch.sum(TP))/(float(torch.sum(TP+FP)) + 1e-6)
    #print ('PC: ', PC)
    return PC

def get_F1(SR,GT,threshold=0.5):
    # Sensitivity == Recall
    SE = get_sensitivity(SR,GT,threshold=threshold)
    PC = get_precision(SR,GT,threshold=threshold)

    F1 = 2*SE*PC/(SE+PC + 1e-6)

    return F1

def get_JS(SR,GT,threshold=0.5):
    # JS : Jaccard similarity
    SR = (SR > threshold).type(torch.uint8)
    #GT = GT == torch.max(GT)
    GT = (GT > torch.max(GT)/2).type(torch.uint8)
    
    Inter = torch.sum((SR+GT)==2)
    Union = torch.sum((SR+GT)>=1)
    
    JS = float(Inter)/(float(Union) + 1e-6)
    
    return JS

def get_DC(SR,GT,threshold=0.5):
    # DC : Dice Coefficient
    SR = (SR > threshold).type(torch.uint8)
    #GT = GT == torch.max(GT)
    GT = (GT > torch.max(GT)/2).type(torch.uint8)

    Inter = torch.sum((SR+GT)==2)
    DC = float(2*Inter)/(float(torch.sum(SR)+torch.sum(GT)) + 1e-6)

    return DC



"
91,2201.03356,"from lire.experiment_tools import knrm
from lire.experiment_tools import monoT5
from lire.experiment_tools import cedr
import argparse
import json
import os

# testing command 
# python experiments_scripts/knrm_ddp_training.py  --master-addr localhost --master-port 12345 --node 0 --n-nodes 1 --n-gpus 1 --data-folder /net/cal/gerald/CPD/data --xp-folder /net/cal/gerald/ddp-lire-xps
parser = argparse.ArgumentParser(description='Experiments launcher')

# multi-gpu options

parser.add_argument('--master-addr', type=str, dest='master_addr', default='localhost',
                    help='The address of the master process should be launched first')
parser.add_argument('--master-port', type=str, dest='master_port',  default='123456',
                    help='The port of the master process')
parser.add_argument('--node', type=int, dest='node', default=0,
                    help='Where the xps files will be saved')
parser.add_argument('--n-nodes', type=int, dest='n_nodes', default=1,
                    help='Where the xps files will be saved')
parser.add_argument('--n-gpus', type=int, dest='n_gpus', default=1,
                    help='Where the xps files will be saved')
parser.add_argument('--gpus', type=str, dest='gpus', default=None,
                    help='Where the xps files will be saved')
parser.add_argument('--init-method', type=str, dest='init_method', default='env://',
                    help='Where the xps files will be saved')


# xp options
parser.add_argument('--xp-config', type=str, dest='xp_config', default='experiments_configs/test.json',
                    help='The location of the experiment configuration file\
                    (a json file see experiments/parameters_search.json for example)')
parser.add_argument('--data-folder', type=str, dest='data_folder',  default='/net/cal/gerald/CPD/data',
                    help='The data where saving, finding or downloading the data')
parser.add_argument('--xp-folder', type=str, dest='xp_folder', default='/net/sundays/gerald/xps',
                    help='Where the xps files will be saved')
parser.add_argument('--tokenizer_path', type=str, dest='tokenizer_path', 
                    default='/net/sundays/gerald/data/word_embedding/wiki.en/wiki.en.bin',
                    help='Where the xps files will be saved')
parser.add_argument('--model', type=str, dest='model', 
                    default='MONOT5',
                    help='The ranker to use [KNRM, MONOT5, CEDR]')
parser.add_argument('--batch-size', dest='batch_size',type=int, default=None)
parser.add_argument('--accumulation', dest='accumulation',type=int, default=1)

parser.add_argument('--switch', dest='switch', default=False, action='store_true')



args = parser.parse_args()

MODELS = {
    ""KNRM"": knrm.DDPKNRM,
    ""MONOT5"": monoT5.DDPMonoT5,
    ""VBERT"": cedr.DDPCEDRVanilla,
    ""CEDRKNRM"": cedr.DDPCEDRKNRM
}



class struct(dict):
    ''' A structure for access dictionary like python objects
    '''
    def __getattr__(self, name):
        if name in self:
            return self[name]
        else:
            raise AttributeError(""No such attribute: "" + name)

    def __setattr__(self, name, value):
        self[name] = value

    def __delattr__(self, name):
        if name in self:
            del self[name]
        else:
            raise AttributeError(""No such attribute: "" + name)



if __name__ == ""__main__"":
    if args.gpus is not None:
        gpus = [int(gpu) for gpu in args.gpus.split(',')]
    else:
        gpus = [i for i in range(args.n_gpus)]
    with open(args.xp_config, 'r') as f:
        options = json.load(f)
    print(gpus)
    for k in options:
        if isinstance(options[k],list):
            print(""list"",options[k])
            nl = []
            for l in options[k]:
                try:
                    r = l
                    if '$' in r:
                        r = os.path.expandvars(r) 
                    if '~' in r:
                        r = os.path.expanduser(r)
                    nl.append(r)
                except TypeError:
                    nl.append(l)
            options[k] = nl
        else:
            try:
                if '$' in options[k]:
                    options[k] = os.path.expandvars(options[k]) 
                if '~' in options[k]:
                    options[k] = os.path.expanduser(options[k])
            except TypeError:
                pass
    
    if args.batch_size is not None:
        options[""batch_size""] = args.batch_size
    
    if ""subrank"" not in options:
        options[""subrank""] = None 
    if ""subrank"" not in options:
        options[""subrank""] = 100
    options[""accumulation""] = args.accumulation
    options[""switch""] = args.switch

    for i in options[""topics_folder_path""]:
        hyperparameters = struct(options)
        hyperparameters[""topics_folder_path""] = i
        hyperparameters[""rerank_path""] = os.path.join(i, hyperparameters[""rerank_path""])
        hyperparameters[""model""] = args.model
        print(""batch process "", i, ""->"", hyperparameters)
        experiment =\
            MODELS[args.model](n_gpus=args.n_gpus, 
                                n_nodes=args.n_nodes,
                                node=args.node, 
                                master_address=args.master_addr, 
                                master_port=args.master_port,
                                gpus=gpus,
                                hyperparameters=hyperparameters)
        experiment.run(main_func=MODELS[args.model].fit)

# python experiments_scripts/ddp_training.py --xp-config experiments_configs/original/monot5_all.json --n-gpus 1 --gpus 1 --master-port 12345 --model MONOT5
# python experiments_scripts/ddp_training.py --xp-config experiments_configs/original/monot5_all.json --n-gpus 1 --gpus 2 --master-port 12346 --model VBERT
# python experiments_scripts/ddp_training.py --xp-config experiments_configs/original/monot5.json --n-gpus 1 --gpus 4 --master-port 12347 --model MONOT5
# python experiments_scripts/ddp_training.py --xp-config experiments_configs/original/vbert.json --n-gpus 1 --gpus 4 --master-port 12348 --model VBERT
# python experiments_scripts/ddp_training.py --xp-config experiments_configs/scenarios/informationupdate_cedr.json --n-gpus 2 --gpus 5,6 --master-port 12349 --model CEDRKNRM
# python experiments_scripts/ddp_training.py --xp-config experiments_configs/scenarios/informationupdate_vbert.json --n-gpus 2 --gpus 5,6 --master-port 12350 --model VBERT"
92,2201.03356,"import argparse
import os
import time
import datetime
import json


import pytrec_eval

# ray ressources
from ray import tune
from ray.tune import run




import torch
import torch.utils.data as data_utils
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter


from lire import data_tools
from lire.log_tools.logger import ConfigurationFile
from lire.misc.class_tools import struct
from lire import experiment_tools 
from lire.data_tools.dataset import NamedDataset
from lire.experiment_tools import ranker_v2 as ranker
from lire.data_tools.dataset import MSMarco

from experiments_scripts.evaluation import evaluation

parser = argparse.ArgumentParser(description='Performs prediction on all finished experiments inside a folder')

parser.add_argument('--data-folder', type=str, dest='data_folder',  default='/net/cal/gerald/CPD/data',
                    help='The data where saving, finding or downloading the data')
parser.add_argument('--xp-folder', type=str, dest='xp_folder', default='/net/sundays/gerald/xps',
                    help='Where the xps files will be saved')

args = parser.parse_args()



def evaluate(evaluation_set, model):
    queries_id = evaluation_set.get_queries_id()
    ground_truth = evaluation_set.qrels.get_dictionary()
    ground_truth = { str(q_id):{str(d_id): v for d_id, v in ground_truth[str(q_id)].items() }
                    for q_id in queries_id }

    prediction = model.rank(model.prepare_evaluation_dataset(evaluation_set))
    prediction = { str(q_id):{str(d_id): v for d_id, v in prediction[q_id].items() }
                    for q_id in queries_id }

    _, mean_score = evaluation.evaluation(evaluation.subrank(prediction, 10), ground_truth)
    return prediction, mean_score



def ray_evaluation(config, checkpoint_dir=None):
    print(""Evaluating the following config: "", config)

    print(""Loading the dataset"")
    dataset = MSMarco.MSMarcoPassageRanking(config['data_folder'],
                                            set_name='dev',
                                            mode='eval',
                                            eval_key = config[""eval_key""],
                                            ressource_file=os.path.join('ressource',
                                                config['dataset']['ressource']+'.json'))
    print(""Loading the model"")
    model = ranker.models[config['model']['model_name']](config['trial_folder'], ""best_model_validation.pth"",
                                                         config)
    model.init_optimizer()
    model._load_checkpoint(os.path.join(config['trial_folder'], ""best_model_validation.pth""))

    prediction, average_performances = evaluate(dataset, model)
    torch.save(prediction, os.path.join(config['trial_folder'], 'prediction.pkl'))
    with open(os.path.join(config['trial_folder'], 'prediction.json'), 'w') as prediction_file:
        json.dumps({""prediction"":prediction, 'average_performances': average_performances}, prediction_file)

if __name__ == ""__main__"":
    os.environ[""SLURM_JOB_NAME""] = ""bash""
    print(""Finiding experiments inside the folder"")
    experiments_folder = os.listdir(args.xp_folder)

    print(""Building ray config from experiments"")
    ray_config = {""grid_search"":[]}

    for folder_xp in experiments_folder:
        ray_xp_config = {}
        configuration_filepath = os.path.join(args.xp_folder, folder_xp,'configuration.json')
        if not os.path.exists(configuration_filepath):
            continue
        # reading configuration
        with open(configuration_filepath, 'r') as configuration_file:
            configuration = json.load(configuration_file)
        # ensure xp is finished
        if(""ending"" in configuration and configuration[""ending""]):
            ray_xp_config['dataset'] = configuration['dataset']
            ray_xp_config['model'] = configuration['model']
            ray_xp_config['trial_folder'] = os.path.join(args.xp_folder, folder_xp)
            ray_xp_config['data_folder'] = args.data_folder
            ray_xp_config['eval_key'] = configuration['eval_key']

            ray_config['grid_search'].append(ray_xp_config)
    print(""The following experiments will be evaluated : "", ray_config)


    es = {
        'name': ""evaluation"",
        'run': ray_evaluation,
        'config': ray_config,
        'local_dir': args.xp_folder,
        'resources_per_trial':{
            'cpu':4, 'gpu': 1
        }
    }
    tune.run(es['run'], es['name'], config=es[""config""], local_dir=args.xp_folder,
             resources_per_trial=es['resources_per_trial'])"
93,2201.03356,"import torch
import torch.nn

from lire.experiment_tools.experiment_template import DDPLIReExperiment
from lire.models import cedr
from lire.dataset import scenarios as sc
from lire.dataset import MSMarco as ms


class MonoT5EvaluationDataset():
    def __init__(self, dataset,
                    positive_token='true',
                    negative_token='false'):
        self.dataset = dataset
        self.rerank_collection = None
        self.positive_token = positive_token
        self.negative_token = negative_token
    
    def __getitem__(self, index):
        return self.rerank_collection[index]

    def __len__(self):
        return len(self.rerank_collection)

    def get_relevant_documents_ids(self):
        return self.dataset.get_relevant_documents_ids()

    def set_current_task_by_id(self, task_id, subrank=None):
        self.dataset.set_task(task_id)
        self.rerank_collection = self.dataset.get_rerank_collection(subrank=subrank)

    def set_task(self, task_id, subrank=None):
        self.dataset.set_task(task_id)
        self.rerank_collection = self.dataset.get_rerank_collection(subrank=subrank)

    def get_nb_tasks(self):
        return self.dataset.get_nb_tasks()

class DDPCEDRVanilla(DDPLIReExperiment):
    def __init__(self,
                 n_gpus=1,
                 n_nodes=1,
                 node=0,
                 master_address='localhost',
                 master_port='8879', 
                 gpu_shift=0,
                 init_method='env://',
                 gpus=None,
                 hyperparameters=None,
                 logger=None):
        super().__init__(n_gpus, n_nodes, node, master_address,
                                master_port, gpu_shift, init_method, gpus, hyperparameters)
        self.loss_func = torch.nn.MarginRankingLoss(margin=1)
        self.dataset = None
        self.log_train_first = 0
        self.log_test_first = 0

    @staticmethod
    def padding_tensor(sequences):
        """"""
        :param sequences: list of tensors
        :return:
        """"""
        num = len(sequences)
        max_len = max([s.size(0) for s in sequences])
        out_dims = (num, max_len)
        out_tensor = sequences[0].data.new(*out_dims).fill_(0)
        mask = sequences[0].data.new(*out_dims).fill_(0)
        for i, tensor in enumerate(sequences):
            length = tensor.size(0)
            out_tensor[i, :length] = tensor
            mask[i, :length] = 1
        return out_tensor, mask

    def init_optim(self):
        params = [(k, v) for k, v in self.model.named_parameters() if(v.requires_grad )]

        non_bert_params = {'params': [v for k, v in params if not k.startswith('module.bert.')]}

        bert_params = {'params': [v for k, v in params if k.startswith('module.bert.')], 'lr': 2e-5}
        # print('KEYS', [k for k, v in params])
        print(""BERT PARAMS "", len(bert_params['params']))
        print(""NB PARAMS"", len(non_bert_params['params']))
        self.optimizer = torch.optim.Adam([non_bert_params, bert_params], lr=1e-3)

    def init_model(self):
        # tokenizer + embedding

        self.model = cedr.VanillaBertRanker()
        self.tokenizer = self.model
        batch = {""qtxt"":['tes t', 'te st'],""dtxt"":['te st', 'test']}
        print(""Some parameters in this model are unused with the current framework.\
             We proceed to a forward pass to check which are used"")
        qids, qmask, dids, dmask = self.batch_transform(batch)
        scores = self.model(qids, qmask, dids, dmask)
        scores.sum().backward()

        initial_pg =  len([(k, v) for k, v in self.model.named_parameters() if(v.requires_grad)])
        no_grad_set =  {""bert.pooler.dense.weight"",""bert.pooler.dense.bias""}
        for k, v in self.model.named_parameters():
            if k in no_grad_set:
                print(""set  no grad for defined "", k)
                v.requires_grad = False
            if v.grad is None:
                print(""set  no grad for "", k)
                v.requires_grad = False
        for v in self.model.parameters():
            if v.grad is None:
                print(""set  no grad for "", k)
                v.requires_grad = False
        batch = {""qtxt"":['tes t', 'te st'],""dtxt"":['te st', 'test']}
        print(""Second pass to verify"")
        qids, qmask, dids, dmask = self.batch_transform(batch)
        scores = self.model(qids, qmask, dids, dmask)
        scores.sum().backward()

        initial_pg =  len([(k, v) for k, v in self.model.named_parameters() if(v.requires_grad)])
        no_grad_set =  {""bert.pooler.dense.weight"",""bert.pooler.dense.bias""}
        for k, v in self.model.named_parameters():
            if k in no_grad_set:
                print(""S2 set  no grad for defined "", k)
                v.requires_grad = False
            if v.grad is None:
                print(""S2 set  no grad for "", k)
                v.requires_grad = False


    def build_dataset(self):
        dataset_types = {
            'scenario_informationUpdate': sc.MSMarcoInformationUpdateDataset,
            'scenario_languageDrift': sc.MSMarcoLanguageDriftDataset,
            'scenario_directTransfer': sc.MSMarcoDirectTransferDataset,
            'scenario_iudtbaseline':sc.MSMarcoIULDBaselineDataset,
            'topics': ms.MSMarcoRankingDataset,
            'all': ms.MSMarcoRankingDatasetAllTasks,
            }
        if self.dataset is None:
            self.dataset = dataset_types[self.hyperparameters['dataset']](
                self.hyperparameters[""data_folder_path""],
                self.hyperparameters[""topics_folder_path""],
                load_triplets=True, 
                rerank_path=self.hyperparameters[""rerank_path""],
                seed=self.hyperparameters[""seed""],
                nb_init_task=self.hyperparameters[""nb_init_task""], 
                nb_evaluated_task=self.hyperparameters[""nb_evaluated_task""],
                switch=self.hyperparameters[""switch""]
                )
            if hasattr(self.dataset, ""scenario_tasks""):
                self.hyperparameters[""grouped_scenario_task""] = self.dataset.scenario_tasks

    def load_training_set(self):
        self.build_dataset()
        self.train_set = self.dataset.clone()
        self.train_set.set_n_sample_negative(1)
        self.train_set.set_split('train')

    def load_validation_set(self):
        self.build_dataset()
        self.val_set = self.dataset.clone()
        self.val_set.set_n_sample_negative(1)
        self.val_set.set_uniform_negative_sampling(False)
        self.val_set.set_split('val')


    def load_testing_set(self):
        self.build_dataset()
        self.test_set = MonoT5EvaluationDataset(self.dataset.clone())
        self.test_set.dataset.set_n_sample_negative(1)
        self.test_set.dataset.set_split('dev')


    def batch_transform(self, batch):

        queries = [torch.LongTensor(self.tokenizer.tokenize(query)) for query in batch['qtxt']]
        if 'pdtxt' in batch:
            docs =\
                [torch.LongTensor(self.tokenizer.tokenize(d))[:512] for d in ([dp[0] for dp in batch['pdtxt']] + [dn[0] for dn in batch['ndtxt']])]
        else:
            docs = [torch.LongTensor(self.tokenizer.tokenize(d))[:512] for d in batch['dtxt']]
        return (*DDPCEDRVanilla.padding_tensor(queries), *DDPCEDRVanilla.padding_tensor(docs))

    def train_step(self, batch, batch_idx):
        qids, qmask, dids, dmask = self.batch_transform(batch)
        if self.log_train_first < 10:
            print(""TEST:"",qids.shape, qmask.shape, dids.shape, dmask.shape)
            self.log_train_first += 1
        qids = torch.cat((qids, qids), 0).to(self.device)
        qmask = torch.cat((qmask, qmask), 0).to(self.device)
        scores = self.model(qids, qmask, dids.to(self.device), dmask.to(self.device))

        # get relevant, irrelevant 
        scores = scores.view(len(qids), 1)
        scores = torch.cat((scores[:len(qids)//2], scores[len(qids)//2:]), -1)

        # compute loss
        loss = torch.mean(1. - scores.softmax(dim=1)[:, 0])
        return loss

    def rank_step(self, batch, batch_idx):
        qids, qmask, dids, dmask = self.batch_transform(batch)

        if self.log_test_first < 10:
            print(""TEST:"",qids.shape, qmask.shape, dids.shape, dmask.shape)
            self.log_test_first += 1
        score = self.model(qids.to(self.device), qmask.to(self.device), dids.to(self.device), dmask.to(self.device))
        return batch['qid'], batch['did'], score.squeeze()

    @staticmethod
    def test():
        hyperparameters = {
            ""data_folder_path"": ""/net/cal/gerald/CPD/total_data"",
            ""topics_folder_path"": ""/net/cal/gerald/CPD/data/MSMarco-task-clustering-74-0.75-0.55/"",
            # ""data_folder_path"": ""/net/cal/gerald/CPD/small_data"",
            # ""topics_folder_path"": ""/net/cal/gerald/CPD/small_data"",
            ""rerank_path"":""/net/cal/gerald/CPD/data/MSMarco-task-clustering-74-0.75-0.55/dev-bm25_top_1000.tsv"",
            ""patience"":3,
            ""ranking_examples"":[1, 2],
            ""log_folder"": ""/local/gerald/test"",
            ""task_perm_seed"": -1,
            ""lr"":2e-5,
            ""n_epoch"": 10,
            ""batch_size"":8,
            ""pretrained_name"":""t5-small"",
            ""nb_init_task"": 2,
            ""dataset"": ""scenario_informationUpdate"",
            ""seed"":42,
            ""nb_evaluated_task"": 2
        }
        hyperparameters['ranking_examples'] = [0, 1, 2]
        hyperparameters['log_folder'] = ""/local/gerald/test""
        hyperparameters['task_perm_seed'] = -1
        hyperparameters['lr'] = 2e-5
        hyperparameters['n_epoch'] = 5
        hyperparameters['batch_size'] = 8
        hyperparameters['pretrained_name'] = 't5-small'

        experiment = DDPCEDRVanilla(n_gpus=1, 
                               n_nodes=1,
                               node=0, 
                               master_address=""localhost"", 
                               master_port=""12346"",
                               gpus=[0],
                               hyperparameters=hyperparameters)
        #experiment._run(0, experiment, DDPCEDRVanilla.train)
        experiment.run(DDPCEDRVanilla.train)

class DDPCEDRKNRM(DDPLIReExperiment):
    def __init__(self,
                 n_gpus=1,
                 n_nodes=1,
                 node=0,
                 master_address='localhost',
                 master_port='8879', 
                 gpu_shift=0,
                 init_method='env://',
                 gpus=None,
                 hyperparameters=None,
                 logger=None):
        super().__init__(n_gpus, n_nodes, node, master_address,
                                master_port, gpu_shift, init_method, gpus, hyperparameters)
        self.loss_func = torch.nn.MarginRankingLoss(margin=1)
        self.dataset = None
        self.log_train_first = 0
        self.log_test_first = 0

    @staticmethod
    def padding_tensor(sequences):
        """"""
        :param sequences: list of tensors
        :return:
        """"""
        num = len(sequences)
        max_len = max([s.size(0) for s in sequences])
        out_dims = (num, max_len)
        out_tensor = sequences[0].data.new(*out_dims).fill_(0)
        mask = sequences[0].data.new(*out_dims).fill_(0)
        for i, tensor in enumerate(sequences):
            length = tensor.size(0)
            out_tensor[i, :length] = tensor
            mask[i, :length] = 1
        return out_tensor, mask

    def init_optim(self):


        params = [(k, v) for k, v in self.model.named_parameters() if(v.requires_grad )]
        util_parameters  = [v for v in self.model.parameters() if(v.requires_grad )]
        non_bert_params = {'params': [v for k, v in params if not k.startswith('module.bert.')]}

        bert_params = {'params': [v for k, v in params if k.startswith('module.bert.')], 'lr': 2e-5}
        # print('KEYS', [k for k, v in params])
        print(""BERT PARAMS "", len(bert_params['params']))
        print(""NB PARAMS"", len(non_bert_params['params']))
        print(""NB UTILS PARAMETERS"", len(util_parameters))


        self.optimizer = torch.optim.Adam([non_bert_params, bert_params], lr=5e-4)

    def init_model(self):
        # tokenizer + embedding

        self.model = cedr.CedrKnrmRanker()
        self.tokenizer = self.model
        batch = {""qtxt"":['tes t', 'te st'],""dtxt"":['te st', 'test']}
        print(""Some parameters in this model are unused with the current framework.\
             We proceed to a forward pass to check which are used"")
        qids, qmask, dids, dmask = self.batch_transform(batch)
        scores = self.model(qids, qmask, dids, dmask)
        scores.sum().backward()

        initial_pg =  len([(k, v) for k, v in self.model.named_parameters() if(v.requires_grad)])
        no_grad_set =  {""bert.pooler.dense.weight"",""bert.pooler.dense.bias""}
        for k, v in self.model.named_parameters():
            if k in no_grad_set:
                print(""set  no grad for defined "", k)
                v.requires_grad = False
            if v.grad is None:
                print(""set  no grad for "", k)
                v.requires_grad = False
        for v in self.model.parameters():
            if v.grad is None:
                print(""set  no grad for "", k)
                v.requires_grad = False
        batch = {""qtxt"":['tes t', 'te st'],""dtxt"":['te st', 'test']}
        print(""Second pass to verify"")
        qids, qmask, dids, dmask = self.batch_transform(batch)
        scores = self.model(qids, qmask, dids, dmask)
        scores.sum().backward()

        initial_pg =  len([(k, v) for k, v in self.model.named_parameters() if(v.requires_grad)])
        no_grad_set =  {""bert.pooler.dense.weight"",""bert.pooler.dense.bias""}
        for k, v in self.model.named_parameters():
            if k in no_grad_set:
                print(""S2 set  no grad for defined "", k)
                v.requires_grad = False
            if v.grad is None:
                print(""S2 set  no grad for "", k)
                v.requires_grad = False

    def build_dataset(self):
        dataset_types = {
            'scenario_informationUpdate': sc.MSMarcoInformationUpdateDataset,
            'scenario_languageDrift': sc.MSMarcoLanguageDriftDataset,
            'scenario_directTransfer': sc.MSMarcoDirectTransferDataset,
            'scenario_iudtbaseline':sc.MSMarcoIULDBaselineDataset,
            'topics': ms.MSMarcoRankingDataset,
            'all': ms.MSMarcoRankingDatasetAllTasks
            }
        if self.dataset is None:
            self.dataset = dataset_types[self.hyperparameters['dataset']](
                self.hyperparameters[""data_folder_path""],
                self.hyperparameters[""topics_folder_path""],
                True, 
                rerank_path=self.hyperparameters[""rerank_path""],
                seed=self.hyperparameters[""seed""],
                nb_init_task=self.hyperparameters[""nb_init_task""], 
                nb_evaluated_task=self.hyperparameters[""nb_evaluated_task""])
            if hasattr(self.dataset, ""scenario_tasks""):
                self.hyperparameters[""grouped_scenario_task""] = self.dataset.scenario_tasks

    def load_training_set(self):
        self.build_dataset()
        self.train_set = self.dataset.clone()
        self.train_set.set_n_sample_negative(1)
        self.train_set.set_split('train')

    def load_validation_set(self):
        self.build_dataset()
        self.val_set = self.dataset.clone()
        self.val_set.set_n_sample_negative(1)
        self.val_set.set_uniform_negative_sampling(False)
        self.val_set.set_split('val')


    def load_testing_set(self):
        self.build_dataset()
        self.test_set = MonoT5EvaluationDataset(self.dataset.clone())
        self.test_set.dataset.set_n_sample_negative(1)
        self.test_set.dataset.set_split('dev')


    def batch_transform(self, batch):

        queries = [torch.LongTensor(self.tokenizer.tokenize(query)) for query in batch['qtxt']]
        if 'pdtxt' in batch:
            docs =\
                [torch.LongTensor(self.tokenizer.tokenize(d))[:512] for d in ([dp[0] for dp in batch['pdtxt']] + [dn[0] for dn in batch['ndtxt']])]
        else:
            docs = [torch.LongTensor(self.tokenizer.tokenize(d))[:512] for d in batch['dtxt']]
        return (*DDPCEDRVanilla.padding_tensor(queries), *DDPCEDRVanilla.padding_tensor(docs))

    def train_step(self, batch, batch_idx):
        qids, qmask, dids, dmask = self.batch_transform(batch)
        if self.log_train_first < 10:
            print(""TEST:"",qids.shape, qmask.shape, dids.shape, dmask.shape)
            self.log_train_first += 1
        qids = torch.cat((qids, qids), 0).to(self.device)
        qmask = torch.cat((qmask, qmask), 0).to(self.device)
        scores = self.model(qids, qmask, dids.to(self.device), dmask.to(self.device))

        # get relevant, irrelevant 
        scores = scores.view(len(qids), 1)
        scores = torch.cat((scores[:len(qids)//2], scores[len(qids)//2:]), -1)

        # compute loss
        loss = torch.mean(1. - scores.softmax(dim=1)[:, 0])
        return loss

    def rank_step(self, batch, batch_idx):
        qids, qmask, dids, dmask = self.batch_transform(batch)

        if self.log_test_first < 10:
            print(""TEST:"",qids.shape, qmask.shape, dids.shape, dmask.shape)
            self.log_test_first += 1
        score = self.model(qids.to(self.device), qmask.to(self.device), dids.to(self.device), dmask.to(self.device))
        return batch['qid'], batch['did'], score.squeeze()


    @staticmethod
    def test():
        hyperparameters = {
            ""data_folder_path"": ""/net/cal/gerald/CPD/total_data"",
            ""topics_folder_path"": ""/net/cal/gerald/CPD/data/MSMarco-task-clustering-74-0.75-0.55/"",
            # ""data_folder_path"": ""/net/cal/gerald/CPD/small_data"",
            # ""topics_folder_path"": ""/net/cal/gerald/CPD/small_data"",
            ""rerank_path"":""/net/cal/gerald/CPD/data/MSMarco-task-clustering-74-0.75-0.55/dev-bm25_top_1000.tsv"",
            ""patience"":3,
            ""ranking_examples"":[1, 2],
            ""log_folder"": ""/local/gerald/test"",
            ""task_perm_seed"": -1,
            ""lr"":2e-5,
            ""n_epoch"": 10,
            ""batch_size"":8,
            ""pretrained_name"":""t5-small"",
            ""nb_init_task"": 2,
            ""dataset"": ""scenario_informationUpdate"",
            ""seed"":42,
            ""nb_evaluated_task"": 2
        }
        hyperparameters['ranking_examples'] = [0, 1, 2]
        hyperparameters['log_folder'] = ""/local/gerald/test""
        hyperparameters['task_perm_seed'] = -1
        hyperparameters['lr'] = 2e-5
        hyperparameters['n_epoch'] = 5
        hyperparameters['batch_size'] = 8
        hyperparameters['pretrained_name'] = 't5-small'

        experiment = DDPCEDRVanilla(n_gpus=1, 
                               n_nodes=1,
                               node=0, 
                               master_address=""localhost"", 
                               master_port=""12346"",
                               gpus=[0],
                               hyperparameters=hyperparameters)
        #experiment._run(0, experiment, DDPCEDRVanilla.train)
        experiment.run(DDPCEDRVanilla.train)

if __name__ == '__main__':
    import os
    os.environ['WANDB_SILENT'] = 'true'
    #os.environ['TOKENIZERS_PARALLELISM']='false'
    DDPCEDRVanilla.test()

# python experiments_scripts/knrm_ddp_training.py  --master-addr localhost --master-port 12345 --node 0 --n-nodes 1 --n-gpus 1 --data-folder /net/cal/gerald/CPD/data --xp-folder /net/cal/gerald/ddp-lire-xps

    
        
"
94,2201.03356,"

import torch
import torch.nn

from lire.trainer.lire_experiment import DDPLIReExperiment
from lire.dataset import scenarios as sc
from lire.dataset import MSMarco as ms


from transformers import T5Tokenizer, T5ForConditionalGeneration

class T5RankerTrainDataset():
    def __init__(self, dataset,
                    positive_token='true',
                    negative_token='false'):
        self.dataset = dataset
        self.positive_token = positive_token
        self.negative_token = negative_token
    
    def __getitem__(self, index):
        query_id, document_id, query_document = self.dataset[index]
        query, document = query_document
        return (query_id, document_id, 'Query: ' + query +' Document: '+ document + ' Relevant:')

    def __len__(self):
        return len(self.dataset)

    def set_current_task_by_id(self, task_id):
        self.dataset.set_current_task_by_id(task_id)

    def set_task(self, task_id):
        self.dataset.set_task(task_id)

    def get_nb_tasks(self):
        return self.dataset.get_nb_tasks()

class MonoT5TrainingDataset():
    def __init__(self, dataset,
                    positive_token='true',
                    negative_token='false'):
        self.dataset = dataset

        self.positive_token = positive_token
        self.negative_token = negative_token
    
    def __getitem__(self, index):
        sample = self.dataset[index]
        sample[""pinput""] = 'Query:' + sample['qtxt'] +' Document:'+ sample['pdtxt'][0] + ' Relevant:'
        sample[""ninput""] = 'Query:' + sample['qtxt'] +' Document:'+ sample['ndtxt'][0] + ' Relevant:'
        sample[""spdid""] = sample[""pdid""][0]
        sample[""npdid""] = sample[""ndid""][0]
        return sample
    def __len__(self):
        return len(self.dataset)

    def set_current_task_by_id(self, task_id):
        self.dataset.set_current_task_by_id(task_id)

    def set_task(self, task_id):
        self.dataset.set_task(task_id)

    def get_nb_tasks(self):
        return self.dataset.get_nb_tasks()

class MonoT5EvaluationDataset():
    def __init__(self, dataset,
                    positive_token='true',
                    negative_token='false'):
        self.dataset = dataset
        self.rerank_collection = None
        self.positive_token = positive_token
        self.negative_token = negative_token
    
    def __getitem__(self, index):
        sample = self.rerank_collection[index]
        sample[""input""] = 'Query:' + sample['qtxt'] +' Document:'+ sample['dtxt'] + ' Relevant:'
        return sample
    def __len__(self):
        return len(self.rerank_collection)

    def get_relevant_documents_ids(self):
        return self.dataset.get_relevant_documents_ids()

    def set_current_task_by_id(self, task_id, subrank=None):
        self.dataset.set_task(task_id)
        self.rerank_collection = self.dataset.get_rerank_collection(subrank=subrank)


    def set_task(self, task_id, subrank=None):
        self.dataset.set_task(task_id)
        self.rerank_collection = self.dataset.get_rerank_collection(subrank=subrank)

    def get_nb_tasks(self):
        return self.dataset.get_nb_tasks()



class DDPMonoT5(DDPLIReExperiment):
    def __init__(self,
                 n_gpus=1,
                 n_nodes=1,
                 node=0,
                 master_address='localhost',
                 master_port='8879', 
                 gpu_shift=0,
                 init_method='env://',
                 gpus=None,
                 hyperparameters=None,
                 logger=None):
        super().__init__(n_gpus, n_nodes, node, master_address,
                                master_port, gpu_shift, init_method, gpus,
                                hyperparameters)
        self.dataset = None
        self.plabels = None
        self.nlabels = None
        self.start_decode_id = None
        self.positive_token = 'true'
        self.negative_token = 'false'

    def build_dataset(self):
        dataset_types = {
            'scenario_informationUpdate': sc.MSMarcoInformationUpdateDataset,
            'scenario_languageDrift': sc.MSMarcoLanguageDriftDataset,
            'scenario_directTransfer': sc.MSMarcoDirectTransferDataset,
            'scenario_iudtbaseline':sc.MSMarcoIULDBaselineDataset,
            'topics': ms.MSMarcoRankingDataset,
            'random': ms.MSMarcoRankingDataset,
            'all': ms.MSMarcoRankingDatasetAllTasks
            }
        if self.dataset is None:
            self.dataset = dataset_types[self.hyperparameters['dataset']](
                self.hyperparameters[""data_folder_path""],
                self.hyperparameters[""topics_folder_path""],
                load_triplets=True, 
                rerank_path=self.hyperparameters[""rerank_path""],
                seed=self.hyperparameters[""seed""],
                nb_init_task=self.hyperparameters[""nb_init_task""], 
                nb_evaluated_task=self.hyperparameters[""nb_evaluated_task""],
                switch=self.hyperparameters[""switch""],
                randomize=self.hyperparameters['randomize']
                )
            if hasattr(self.dataset, ""scenario_tasks""):
                self.hyperparameters[""grouped_scenario_task""] = self.dataset.scenario_tasks

    def load_training_set(self):
        self.build_dataset()
        self.train_set = MonoT5TrainingDataset(self.dataset.clone())
        self.train_set.dataset.set_n_sample_negative(1)
        self.train_set.dataset.set_split('train')

    def load_validation_set(self):
        self.build_dataset()
        self.val_set = MonoT5TrainingDataset(self.dataset.clone())
        self.val_set.dataset.set_n_sample_negative(1)
        self.val_set.dataset.set_uniform_negative_sampling(False)
        self.val_set.dataset.set_split('val')

    def load_testing_set(self):
        self.build_dataset()
        self.test_set = MonoT5EvaluationDataset(self.dataset.clone())
        self.test_set.dataset.set_n_sample_negative(1)
        self.test_set.dataset.set_split('dev')



    def init_optim(self):
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hyperparameters['lr'])
        self.optimizer = optimizer

    def init_model(self):
        self.model = T5ForConditionalGeneration.from_pretrained(self.hyperparameters['pretrained_name'])
        self.tokenizer = T5Tokenizer.from_pretrained(self.hyperparameters['pretrained_name'])
        self.model_config = self.model.config
        print('Initial tokenizer size', len(self.tokenizer))
        self.tokenizer.add_tokens(['Query:', 'Document:', 'Relevant:'])
        torch.manual_seed(1)
        print('Final size', len(self.tokenizer))
        self.model.resize_token_embeddings(len(self.tokenizer))
        self.model.to(self.device)

    def batch_transform(self, batch):
        
        input_tokens_index =\
            self.tokenizer(batch[""pinput""] + batch[""ninput""], return_tensors=""pt"",
                           max_length=512, padding=True, truncation=True)
        input_ids = input_tokens_index.input_ids
        attention_mask = input_tokens_index.attention_mask

        if self.plabels is None:
            self.plabels =\
                self.tokenizer(self.positive_token, return_tensors=""pt"").input_ids.squeeze().unsqueeze(0) 
        if self.nlabels is None:
            self.nlabels =\
                self.tokenizer(self.negative_token, return_tensors=""pt"").input_ids.squeeze().unsqueeze(0)   

        labels =\
            torch.cat((self.plabels.expand(len(batch[""pinput""]), self.plabels.shape[-1]),
                      self.nlabels.expand(len(batch[""ninput""]), self.nlabels.shape[-1])), 0)
        return {""input_ids"":input_ids.to(self.device), 'attention_mask':attention_mask.to(self.device),
                 'labels':labels.to(self.device)}

    def evaluation_batch_transform(self, batch):
        input_tokens_index =\
            self.tokenizer(batch[""input""], return_tensors=""pt"",
                            padding=True, truncation=True, max_length=512)
        input_ids = input_tokens_index.input_ids
        attention_mask = input_tokens_index.attention_mask

        return {""input_ids"":input_ids.to(self.device), 'attention_mask':attention_mask.to(self.device)}       

    def train_step(self, batch, batch_idx):
        mt5_input = self.batch_transform(batch)
        outputs_prediction = self.model(**mt5_input)
        loss = outputs_prediction.loss
        return loss

    def rank_step(self, batch, batch_idx):
        mt5_input = self.evaluation_batch_transform(batch)
        if(self.start_decode_id is None):
            # retrieve the start decoder token (often 0)
            self.start_decode_id = torch.rand(1,1).to(self.device).long()
            self.start_decode_id[0,0] = self.model_config.decoder_start_token_id
            
            self.positive_token_id =\
                self.tokenizer(self.positive_token)
            self.positive_token_id = self.positive_token_id.input_ids[0]
            self.negative_token_id =\
                self.tokenizer(self.negative_token)
            self.negative_token_id  = self.negative_token_id.input_ids[0]


        # apply one decoding step (see hugging face transformer forward)
        output = self.model(**{**mt5_input, 
                            'decoder_input_ids':self.start_decode_id.expand(len(batch['qid']), 1)})
        # get logits on tokens
        out_logits = output['logits']
        # compute log_softmax on [positive_token, negative_token] for each batch elements
        pn_score = out_logits[:, -1, [self.positive_token_id, self.negative_token_id]]

        n_log_prob = torch.nn.functional.log_softmax(pn_score, dim=1)
        # get and return the score for positive token
        return batch['qid'], batch['did'], torch.exp(n_log_prob[:,0]).squeeze()"
95,2201.03356,"'''An experiment template for lire for mono-node multi-node training.
'''
import torch
from torch import distributed as dist
import torch.multiprocessing as mp
import argparse
import os
import tqdm
import time
import datetime
import copy
import wandb
import json
import numpy as np
import sys
import pytrec_eval
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.tensorboard.writer import SummaryWriter
def dict_collate_fn(batch):
    dct = {}
    for dictionary_sample in batch:
        for key, value in dictionary_sample.items():
            if key not in dct:
                dct[key] = []
            dct[key].append(value)
    return dct
def sum_by_key(values):
    sum_on_key = {}
    for value in values:
        for key, score in value.items():
            if(key not in sum_on_key):
                sum_on_key[key] = 0
            sum_on_key[key] += score/len(values)
    return sum_on_key
def evaluation(prediction, ground_truth, metrics={'map', 'ndcg', 'recip_rank'}):
    # ensure same queries are availabel in both prediction and ground truth
    gt_final = {}
    pd_final = {}
    for query in prediction:
        if(query not in ground_truth):
            print(""The query "", query, "" is not available in ground truth set"")
        else:
            gt_final[query] = ground_truth[query]
            pd_final[query] = prediction[query]
    print('evaluate ', len(pd_final), ' queries')
    pytrec_evaluator = pytrec_eval.RelevanceEvaluator(gt_final, metrics)
    scores = pytrec_evaluator.evaluate(pd_final)
    mean_score = sum_by_key(scores.values())
    return scores, mean_score

def subrank(pred, at):
    new_pred = {}
    for query, document in  pred.items():
        tuple_d_v = [(k,v) for k,v in document.items()]
        d = [k for k,v in document.items()]
        s = [v for k,v in document.items()]
        index_at_score = (-np.array(s)).argsort()[:min(at, len(s))]
        new_pred[query] = {d[i]:s[i] for i in index_at_score}
    return new_pred

def evaluate(prediction, ground_truth):
    ground_truth = { str(q_id):{str(d_id): v for d_id, v in ground_truth[q_id].items() }
                    for q_id in ground_truth }
    prediction = { str(q_id):{str(d_id): v for d_id, v in prediction[q_id].items() }
                    for q_id in prediction}
    _, mean_score = evaluation(subrank(prediction, 10), ground_truth)
    return mean_score

class LIReExperiment(object):
    def __init__(self, options, dataset):
        self.dataset = dataset
        self.options = options
        self.current_state = {}

    @property
    def state_dict(self):
        ''' Get metadata from experiments.
        get metadata of experiments object
        with important informations to continue
        experiments at this step. It also recursivelly call
        the state_dict property of contained object.
        '''
        state_dict = {}
        for k, v in self.__dict__.items():
            if(hasattr(v, ""load_state_dict"") and hasattr(v, 'state_dict')):
                state_dict[k] = v.state_dict()
            elif(hasattr(v, 'state_dict')):
                state_dict[k] = v.state_dict
        state_dict[""options""] = self.options
        state_dict[""experiment_state""] = self.current_state
        return state_dict

    @state_dict.setter
    def state_dict(self, value):
        ''' set metadata from experiments.
        set metadata of experiments object.
        Allow to load an experiment checkpoint
        '''
        for k, v in self.__dict__.items():
            if hasattr(v, ""load_state_dict""):
                v.load_state_dict(value[k])
            elif hasattr(v, 'state_dict'):
                v.state_dict = value[k]

        self.options = value[""options""]
        self.current_state = value[""experiment_state""]

    def save_experiment(self, filepath):
        torch.save(self.state_dict, filepath)
    
    def __getitem__(self, key):
        return self.options.__dict__[key]
    
    def train(self):
        raise NotImplementedError
    
    def ending_task(self):
        raise NotImplementedError

    def begin_task(self):
        raise NotImplementedError

    def run(self):
        raise NotImplementedError

class DDPExperiment(object):
    def __init__(self, n_gpus=1, n_nodes=1, node=0,
                master_address='localhost', master_port='8879', 
                gpu_shift=0, init_method='env://', gpus=None):
        self.n_gpus = n_gpus
        self.n_nodes = n_nodes
        self.node = node
        self.world_size = n_gpus * n_nodes
        self.master_process = False
        self.log_data = {}
        self.gpu_shift = gpu_shift
        self.master_address = master_address
        self.master_port = master_port
        self.citer = 0
        self.init_method = init_method
        self.print_redirection = None
        if gpus is not None:
            self.gpus = gpus
        else:
            self.gpus = [i+self.gpu_shift for i in range(n_gpus)]
        os.environ['MASTER_ADDR'] = master_address
        os.environ['MASTER_PORT'] = master_port
        print('DDPLIReExperiment using master node ', os.environ['MASTER_ADDR'], 'on', os.environ['MASTER_PORT'])

    def init_model(self, args):
        pass        
    def load_training_set(self, args):
        pass
    def load_validation_set(self, args):
        pass
    def load_testing_set(self, args):
        pass
    def train(self):
        pass
    def is_master(self):
        return self.master_process
    
    def log(self, key, value):
        if key not in self.log_data:
            self.log_data[key] = []
        self.log_data[key].append([None for i in range(self.world_size)])
        dist.all_gather_object(self.log_data[key][-1], value)
        dist.barrier()

    def set_print_redirection(self, path):
        self.print_redirection = path

    def print(self, *args):
        if self.is_master():
            if self.print_redirection is not None:
                with open(self.print_redirection, mode='a') as file_object:
                    print(*args, file=file_object)
            else:
                print(*args)

    def print_log(self, key):
        self.print(self.log_data[key])

    def _multiprocessing_start_routine(self, local_rank):
        # rank is also the gpu used on the local machine
        print('GPU used', self.gpus)
        self.device = self.gpus[local_rank]
        print('DEVICE used', self.device)
        self.local_rank = local_rank
        self.rank = self.node * self.n_gpus + self.local_rank
        print(torch.cuda.device_count())
        # Setting the cuda device using naccl backend is mandatory
        torch.cuda.set_device(self.device)

        print('Init process of rank ', self.rank, ""on"", self.world_size)
        if self.world_size > 1:
            dist.init_process_group(                  
                backend='nccl',                                  
                world_size=self.world_size,                              
                rank=self.rank
                        
            )
   
    def _multiprocessing_stop_routine(self):
        dist.destroy_process_group()

    @staticmethod
    def _run(rank, xp, main_func):
        print(""start routine rank "", rank)
        xp._multiprocessing_start_routine(rank)
        print('rank, node:',rank, xp.node)
        if(rank == 0 and xp.node == 0):
            print('It is the master')
            xp.master_process = True
        print(""Start main func"")
        print(""ALL IS OK (printing on stderr due to bufferization of stdout)"", file=sys.stderr)
        main_func(xp)
        xp._multiprocessing_stop_routine()

    def run(self, main_func=None):
        print('start launching')
        if main_func is None:
            main_func = type(self).fit
        if self.world_size == 1:
            type(self)._run(0, self, main_func)
        else:
            mp.spawn(type(self)._run, args=(self, main_func), nprocs=self.n_gpus, join=True)

    @staticmethod
    def test_ddp(args):
        gpus = None
        if args.gpus is not None:
            gpus = [int(gpu) for gpu in gpus.split(',')]
        else:
            gpus = [i for i in range(args.n_gpus)]

        ddp_test = DDPTEST(args.n_gpus, args.n_nodes, args.node, args.master_addr, args.master_port, 
                          args.gpu_shift, args.init_method, gpus)
        ddp_test.run()

    def gather(self, tensor):
        '''
            Gather tensor from all process to the master one.

            Use all_gather instead as not supported by nccl, which is clearly
            unefficient in term of bandwidth. It will be updated when the backend 
            will supports it. Notice that gpu tensor is mandatory in case of nccl 
            backend.
            Morever notice that only the master process will get the real data
            for other nones is returned.
        '''
        gathered_list = [copy.deepcopy(tensor) for i in range(self.world_size)]

        dist.all_gather(gathered_list, tensor)
        if self.is_master():
            return torch.cat(gathered_list, 0)
        else:
            return None

    def gather_safe(self, tensor):
        ''' similar to gather additionally ensure tensor have the same first shape.
        '''
        if self.world_size <= 1:
            return tensor
        tensor_shape = torch.LongTensor([tensor.shape[0]]).to(self.device)
        gathered_list = [copy.deepcopy(tensor_shape) for i in range(self.world_size)]
        dist.all_gather(gathered_list, tensor_shape)
        maximum_shape = 0
        for ts in gathered_list:
            if ts.item() > maximum_shape:
                maximum_shape = ts.item()
        if tensor.shape[0] < maximum_shape:
            add_zero = x.new(maximum_shape - tensor.shape[0], *tensor.shape[1:])
            gathered_tensor = torch.cat((tensor, add_zero))
        else:
            gathered_tensor = tensor
        gathered_list_final = [copy.deepcopy(gathered_tensor) for i in range(self.world_size)]
        dist.all_gather(gathered_list_final, gathered_tensor)

        if self.is_master():
            return torch.cat([gathered_list_final[i][:gathered_list[i]] 
                              for i in range(len(gathered_list_final))]
                             , 0)

    def log(self, data, path='~/.runs'):
        if self.is_master():
            if not hasattr(self, 'is_log_init'):
                self.writer = SummaryWriter(path)
                self.is_log_init = True
            for k, v in data.items():
                self.writer.add_scalar(k, v, self.citer)



class DDPLIReExperiment(DDPExperiment):
    def __init__(self,
                 n_gpus=1,
                 n_nodes=1,
                 node=0,
                 master_address='localhost',
                 master_port='8879', 
                 gpu_shift=0,
                 init_method='env://',
                 gpus=None,
                 hyperparameters=None,
                 logger=None, 
                 print_redirection=None):
        super().__init__(n_gpus, n_nodes, node, master_address,
                                master_port, gpu_shift, init_method, gpus)
        
        ## def init word embedding
        self.hyperparameters = hyperparameters
        self.xp_name = self.__class__.__name__
        print('checking data_folder')
        if 'rerank_path' in self.hyperparameters:
            self.hyperparameters['rerank_path'] =\
                os.path.expanduser(os.path.expandvars(self.hyperparameters['rerank_path']))
        if 'data_folder_path' in self.hyperparameters:
            self.hyperparameters['data_folder_path'] =\
                os.path.expanduser(os.path.expandvars(self.hyperparameters['data_folder_path']))
        if 'topics_folder_path' in self.hyperparameters:
            self.hyperparameters['topics_folder_path'] =\
                os.path.expanduser(os.path.expandvars(self.hyperparameters['topics_folder_path']))
            print(self.hyperparameters['topics_folder_path'])
        if 'log_folder_run' not in self.hyperparameters:
            assert('log_folder' in self.hyperparameters)

            lf = self.hyperparameters['log_folder']
            # make def instead of running this code here
            lf_exp = os.path.expandvars(os.path.expanduser(lf))
            lf_exp = os.path.join(lf_exp, self.xp_name+""-""+self.hyperparameters[""dataset""])
            lf_exp += ""-switch"" if(self.hyperparameters[""switch""]) else """"
            if not os.path.exists(lf_exp):
                os.makedirs(lf_exp)
            lf_dir = set(os.listdir(lf_exp))
            lfr_exist = True
            lfr_cpt = 0
            lfr_name = 'run_0'
            while(lfr_exist):
                lfr_name = 'run_'+str(lfr_cpt)
                if os.path.exists(os.path.join(lf_exp, lfr_name)):
                    lfr_cpt += 1
                else:
                    lfr_exist = False
            self.experiment_name  = lfr_name
            self.hyperparameters['log_folder_run'] = os.path.join(lf_exp, self.experiment_name)
            os.makedirs(self.hyperparameters['log_folder_run'])
            print('Logs will be find at ', self.hyperparameters['log_folder_run'])
            if print_redirection is None:
                self.set_print_redirection(os.path.join(self.hyperparameters['log_folder_run'], 'log.txt'))

    def init_model(self):
        pass

    def save_checkpoint(self, name='default'):
        if self.is_master():
            checkpoint_path = os.path.join(self.hyperparameters['log_folder_run'], name+'-model.pth')
            model_state_dict = self.model.state_dict()
            torch.save(model_state_dict, checkpoint_path)
            optim_state_dict = self.optimizer.state_dict()
            checkpoint_path = os.path.join(self.hyperparameters['log_folder_run'], name+'-optim.pth')
            torch.save(optim_state_dict, checkpoint_path)
        dist.barrier()

    def load_checkpoint(self, name='default'):
        dist.barrier()
        checkpoint_path = os.path.join(self.hyperparameters['log_folder_run'], name+'-model.pth')
        self.model.load_state_dict(torch.load(checkpoint_path, map_location='cuda:'+ str(self.device)))
        checkpoint_path = os.path.join(self.hyperparameters['log_folder_run'], name+'-optim.pth')
        self.optimizer.load_state_dict(torch.load(checkpoint_path))

        

    def load_training_set(self):
        # training set
        self.train_set = NamedDataset[self.hyperparameters.dataset['type']](self.hyperparameters['data_folder'],
                                        set_name='train',
                                        mode='training',
                                        ressource_file=os.path.join('ressource',
                                                                    self.hyperparameters.dataset['ressource']+'.json'))
    def load_validation_set(self):
        self.val_set = NamedDataset[self.hyperparameters.dataset['type']](self.hyperparameters['data_folder'],
                                        set_name='val',
                                        mode='training',
                                        ressource_file=os.path.join('ressource',
                                                                    self.hyperparameters.dataset['ressource']+'.json'))
    def load_testing_set(self):
        self.test_set = NamedDataset[self.hyperparameters.dataset['type']](self.hyperparameters['data_folder'],
                                        set_name='dev',
                                        mode='test',
                                        ressource_file=os.path.join('ressource',
                                                                    self.hyperparameters.dataset['ressource']+'.json'))

    def init_log_folder(self):
        pass


    def init_optim(self):
        pass

    def init_finetune(self):
        self.init_model()
        self.model.to(self.device)
        self.initial_model = self.model
        self.model = DDP(self.model, device_ids=[self.device])
        self.init_optim()

        if 'checkpoint' in self.hyperparameters:
            dist.barrier()
            self.model.load_state_dict(torch.load(self.hyperparameters['checkpoint-model'], 
                                              map_location='cuda:' + str(self.device))
                                             )
            # init optim here to refer optimizer to current model weights
            self.optimizer.load_state_dict(torch.load(checkpoint_data['optim']))
        self.loss_func = torch.nn.MarginRankingLoss(margin=1)

    def init_task_order(self):
        self.nb_tasks = self.train_set.get_nb_tasks()

        if self.hyperparameters['task_perm_seed'] >= 0 :
            torch.manual_seed(self.hyperparameters['task_perm_seed'])
            self.task_perm = torch.randperm(self.nb_tasks)

        else: 
            self.task_perm = torch.arange(self.nb_tasks)
        self.hyperparameters['task_perm_list'] = self.task_perm.tolist()

    def batch_transform(self, batch):
        pass

    def train_step(self, batch, batch_idx):
        pass


    def validate(self, task_id, metric=""recip_rank""):
        with torch.no_grad():
            prediction = self.rank_task(task_id, self.val_set, subrank=self.hyperparameters['subrank-val'])
            ground_truth = self.val_set.get_relevant_documents_ids()
            mean_score = evaluate(prediction, ground_truth)
            print('Mean Score :', mean_score)
        return {""v-score"": -mean_score[metric]}

    def init_train(self):
        self.print(""Initialization: please wait until training start"")
        self.init_finetune()
        self.load_training_set()
        self.load_validation_set()
        self.init_task_order()
        self.max_patience = self.hyperparameters['patience']
        self.default_gt = torch.ones(1).to(self.device)
        if 'ranking_examples' in self.hyperparameters:
            self.print(""Load testing set to compute performances for:"", self.hyperparameters[""ranking_examples""])
            self.load_testing_set()
        with open(os.path.join(self.hyperparameters['log_folder_run'],'configuration.json'), 'w') as conf_file:
            json.dump(self.hyperparameters, conf_file, indent=4)

        if hasattr(self.dataset, ""get_generated_documents_ids""):
            print(""Saving special tasks"")
            with open(os.path.join(self.hyperparameters['log_folder_run'],'special-task.json'), 'w') as st_file:
                json.dump(self.dataset.get_generated_documents_ids(), st_file, indent=4)
    


    def train_epoch(self, report_loss=100):
        self.train_sampler = torch.utils.data.distributed.DistributedSampler(self.train_set, 
                                                                             num_replicas=self.world_size,
                                                                             rank=self.rank,
                                                                             seed=self.training_epoch)
        train_loader = torch.utils.data.DataLoader(self.train_set, num_workers=0, batch_size=self.hyperparameters['batch_size'],
                                pin_memory=True, sampler=self.train_sampler, collate_fn=dict_collate_fn)
        self.model.train()
        epoch_loss = 0
        log_loss = 0

        self.training_epoch += 1
        acc = 0
        self.optimizer.zero_grad()
        for batch_idx, batch in enumerate(train_loader):
            self.citer += 1
            loss = self.train_step(batch, batch_idx)
            loss.backward()
            epoch_loss += loss
            log_loss += loss
            if acc % self.hyperparameters['accumulation'] == 0 and acc != 0:
                self.optimizer.step()
                self.optimizer.zero_grad()
            acc += 1
            if batch_idx % report_loss == 0  and batch_idx != 0:
                dist.reduce(log_loss, 0, op=dist.ReduceOp.SUM)
                self.log({'train_step_loss': log_loss.item() / (self.world_size * report_loss)}, os.path.join(self.hyperparameters['log_folder_run'], 'log'))
                print({'train_step_loss': log_loss.item() / (self.world_size * report_loss)})
                log_loss = 0
        if acc % self.hyperparameters['accumulation'] != 0 :
            # we finally not update here may cause issue in the gradient value and
            # thus not consider last batch-size
            # self.optimizer.step()
            self.optimizer.zero_grad()
        
        dist.reduce(epoch_loss, 0, op=dist.ReduceOp.SUM)
        train_loss = epoch_loss.item() / (self.world_size * len(train_loader))
        return {""training_loss"": train_loss}


    def checkpoint_routine(self, validation_score):
        ''' Save checkpoint according to certain conditions
        '''
        if self.training_best_validation > validation_score['v-score']:
            self.print('best validation reached ('+str(training_best_validation)+'->'
                       +str(validation_score['v-score'])+') saving the model')
            self.training_best_validation = validation_score['v-score']
            self.save_checkpoint()
            self.training_patience = 0
            self.print(""VAL"", self.citer, validation_score['v-score'])

    def break_routine(self, validation_score):
        ''' Return if the model have to stop training
        '''
        if self.training_patience > self.max_patience:
            self.print('Stopping at epoch', epoch, 'for task', i)
            self.load_checkpoint()
            return True
        return False
          
    def test(self, end_training=False):
        if 'ranking_examples' not in self.hyperparameters and not end_training:
            return
        elif not end_training: 
            prefix = """"
            testing_task = self.hyperparameters['ranking_examples']
        elif 'end_eval' not in self.hyperparameters:
            return
        else:
            prefix = ""FINAL-""
            testing_task = self.hyperparameters['end_eval']    

        with torch.no_grad():
            self.print('Evaluate the tasks:', self.hyperparameters['ranking_examples'])
            res = {}
            for ttid in self.hyperparameters['ranking_examples']:
                res[ttid] = self.rank_task(ttid, self.test_set, subrank=self.hyperparameters['subrank'])
            # only master process got the complete results
            if self.is_master():

                result_set = {
                    ""current_task_id"":self.training_task_idn,
                    ""current_task"":self.training_task_idx,
                    ""results"":res
                    }
                tracked_task_path = os.path.join(self.hyperparameters['log_folder_run'], 'tracked_task')
                os.makedirs(tracked_task_path, exist_ok=True)

                with open(os.path.join(tracked_task_path, prefix+'pred-'+str(self.training_task_idx)+'-best.json'), 'w') as tracked_task_file:
                    json.dump(result_set, tracked_task_file, indent=4)
                
                for k, prediction in res.items():
                    qualitative_results = {}
                    for query, document in  prediction.items():
                        tuple_d_v = [(k,v) for k,v in document.items()]
                        d = [k for k,v in document.items()]
                        s = [v for k,v in document.items()]
                        index_at_score = (-np.array(s)).argsort()
                        qualitative_results[query] =\
                            [self.test_set.dataset.get_query(query),
                                d[index_at_score[0]],
                                self.test_set.dataset.get_document(d[index_at_score[0]]),
                                d[index_at_score[-1]],
                                self.test_set.dataset.get_document(d[index_at_score[-1]])
                            ]


                    with open(os.path.join(tracked_task_path, prefix+'text-'+str(self.training_task_idx)+'-best-'+str(k)+'.json'), 'w') as tracked_task_file:
                        json.dump(qualitative_results, tracked_task_file, indent=4)
                    self.test_set.set_task(k)
                    ground_truth = self.test_set.get_relevant_documents_ids()


                    mean_score = evaluate(prediction, ground_truth)

                    with open(os.path.join(tracked_task_path, prefix+'score-'+str(self.training_task_idx)+'-best-'+str(k)+'-'+str(len(ground_truth))+'.json'), 'w') as tracked_task_file:
                        json.dump(mean_score, tracked_task_file, indent=4)
                    self.print(mean_score)
                    self.log({'MRR_T'+str(k): mean_score['recip_rank']})
                

    def train(self):
        self.model.train()

    def eval(self):
        self.model.eval()

    def fit(self):
        self.init_train()
        # load start time for log
        start_time = time.time()
        # var for total iteration
        self.citer = 0
        if not hasattr(self, ""training_task_idx""):
            self.training_task_idx = 0
        if not hasattr(self, ""training_patience""):
            self.training_patience = 0
        if not hasattr(self, ""training_best_validation""):
            self.training_best_validation = float(""inf"")
        if not hasattr(self, ""training_epoch""):
            self.training_epoch = 0
        print('Start training')
        for tr_index, task_idx in enumerate(range(self.training_task_idx, len(self.task_perm))):
            if tr_index != 0:
                self.training_task_idx = task_idx
                self.training_patience = 0
                self.training_best_validation = float(""inf"")
                self.training_epoch = 0
            
            if tr_index != 0:
                self.optimizer.zero_grad()
                self.load_checkpoint()

            self.training_task_idn = self.task_perm[task_idx].item()

            self.print(""setting train/validation set to task: (%d,%d)"",
                       self.training_task_idx,
                       self.training_task_idn)

            self.train_set.set_task(self.training_task_idn)
            self.val_set.set_task(self.training_task_idn)

            print(""Training epoch %d"", self.hyperparameters['n_epoch'])
            for ep_index, tr_epoch in enumerate(range(self.hyperparameters['n_epoch'])):
                self.train()
                epoch_loss = self.train_epoch(report_loss=40)
                self.eval()
                
                validation_score = self.validate(self.training_task_idn)
                print(validation_score)
                self.checkpoint_routine(validation_score)
                self.end_epoch_routine(dict(**epoch_loss, **validation_score))
                do_break = self.break_routine(validation_score)
                self.training_patience += 1
                if do_break:
                    break
            self.load_checkpoint()
            self.test(False)
        self.test(True)

    def rank_step(self, batch, batch_idx):
        pass

    def rank_task(self, ranking_set, task_id, subrank=None):
        self.model.eval()
        self.print('start rank_task', task_id)
        ranking_set.set_task(task_id, subrank=subrank)

        sampler = torch.utils.data.distributed.DistributedSampler(ranking_set, 
                                                                 num_replicas=self.world_size,
                                                                 rank=self.rank)
        dataloader = torch.utils.data.DataLoader(ranking_set, num_workers=0,
                                                 batch_size=self.hyperparameters['batch_size']*2,
                                                 pin_memory=True, sampler=sampler, collate_fn=dict_collate_fn) 
        progress_bar = range(len(dataloader))
        queries_id, documents_id, scores = [], [], []

        for batch_idx, batch in zip(progress_bar, dataloader):
            query_id, document_id, score = self.rank_step(batch, batch_idx)
            queries_id.append(torch.LongTensor(query_id))
            documents_id.append(torch.LongTensor(document_id))
            if score.dim() == 0:
                score = score.unsqueeze(0)
            scores.append(score)
            if batch_idx % 100 == 0:
                self.print('prediction completed at: '+str(batch_idx/len(dataloader) * 1000 // 1 /10)+'%')
        
        queries_id = self.gather_safe(torch.cat(queries_id, 0).to(self.device))
        documents_id = self.gather_safe(torch.cat(documents_id, 0).to(self.device))
        scores = self.gather_safe(torch.cat(scores, 0))

        prediction_set = {}
        if self.is_master():
            prediction_set = {}
            for i in range(len(queries_id)):
                qid = queries_id[i].item()
                did = documents_id[i].item()
                s = scores[i].item()
                if qid not in prediction_set:
                    prediction_set[qid] = {}
                prediction_set[qid][did] = s
        return prediction_set
"
96,2201.03356,"import os
import pandas as pd
import numpy as np
import json

def bm25_rank(queries_collection, documents_collection, storage_filepath, doc_storage_filepath=None, top_k=1000,
            threads=1):
    if not os.path.exists(os.path.join(doc_storage_filepath, 'index')):
        # compute the index
        print(""Compute the index..."")
        documents =\
            [{""id"": str(doc_id), ""contents"": doc_content} for doc_id, doc_content in documents_collection]
        os.makedirs(os.path.join(doc_storage_filepath,'documents_collection'), exist_ok=True)
        with open(os.path.join(doc_storage_filepath,'documents_collection/documents.json'), 'w') as tmp_file_index:
            json.dump(documents, tmp_file_index) 

        command_line =\
            'python -m pyserini.index -collection JsonCollection \
            -generator DefaultLuceneDocumentGenerator -threads '+str(threads)+' -input '+\
            os.path.join(doc_storage_filepath, 'documents_collection') + ' -index ' +\
            os.path.join(doc_storage_filepath, 'index') + ' -storePositions -storeDocvectors -storeRaw'
        os.system(command_line)  

    if not os.path.exists(os.path.join(storage_filepath, 'bm25-' + str(top_k) + '.txt')):
        print(""Creating queries tsv file"")
        queries_df = pd.DataFrame(queries_collection)
        queries_df.to_csv(os.path.join(storage_filepath, 'queries.tsv') , sep='\t', header=False, index=False)
        print('Start retrieval')
        command_line = 'python -m pyserini.search --topics ' + os.path.join(storage_filepath, 'queries.tsv') + ' \
            --index ' + os.path.join(doc_storage_filepath, 'index') + ' \
            --output ' + os.path.join(storage_filepath, 'bm25-' + str(top_k) + '.txt') + ' \
            --bm25 --hits '+str(top_k)
        print(command_line)
        os.system(command_line)  
    print(""loading scoreddocs"")
    scoreddocs = pd.read_csv(os.path.join(storage_filepath, 'bm25-' + str(top_k) + '.txt'), sep=' ',
                                            header=None, usecols=[0, 2, 4],  names=['query_id', 'doc_id', 'score'],
                                            dtype={'query_id': str, 'doc_id':str, 'score': np.float32})
    return scoreddocs"
97,2201.03356,"import torch
import numpy as np
import pandas as pd
import json
import tqdm

from lire.data_tools.dataset.MSMarco import MSMarcoRankingDataset

from sentence_transformers import SentenceTransformer, util
from sklearn.cluster import KMeans
from k_means_constrained import KMeansConstrained
import copy
# class MSMarcoScenarios(MSMarcoRankingDataset):
#     def 

class MSMarcoIULDBaselineDataset(MSMarcoRankingDataset):
    def __init__(self, data_folder_path, topics_folder_path,
                 load_triplets=True, rerank_path=None, seed=42, nb_init_task=5, nb_evaluated_task=2, switch=False):
        super().__init__(data_folder_path, topics_folder_path, 
                         load_triplets=load_triplets, rerank_path=rerank_path, seed=seed)

        np.random.seed(self.seed)
        torch.manual_seed(self.seed)
        task_ids = torch.LongTensor(super().get_task_ids())
        task_permutation = torch.randperm(len(task_ids))
        permuted_task_ids = task_ids[task_permutation]
        self.scenario_task = 0
        self.scenario_tasks =\
            [ permuted_task_ids[0:nb_init_task].tolist() ,
             permuted_task_ids[nb_init_task:nb_init_task+nb_evaluated_task].tolist()]
        print(""Tinit"", self.scenario_tasks[0])
        print(""T1 & T2"", self.scenario_tasks[1])

        self._build_scenario(self.scenario_tasks[1])

        self.set_task(0)
        if(load_triplets):
            self.set_negative_docs_return(True)

    def set_split(self, split_name):
        self.set_queries_index(self._get_queryids_task_split(self.working_task, split_name))
        self.working_split = split_name

    def set_task(self, task_id):
        task_id_list = self.scenario_tasks[task_id]
        self.working_task = task_id_list
        self.scenario_task = task_id
        self.set_split(self.working_split)

    def __getitem__(self, index):
        output_dict = super().__getitem__(index)
        return output_dict

    def get_task_ids(self):
        return [0, 1]
    
    def get_nb_tasks(self):
        return 2
    # def get_relevant_documents_ids(self):
    #     return super().get_relevant_documents_ids()

    # def get_rerank_collection(self, queries_index=None, subrank=None):
    #     return super().get_rerank_collection()
    def _build_scenario(self, topics_scenario):
        pass

class MSMarcoInformationUpdateDataset(MSMarcoRankingDataset):
    def __init__(self, data_folder_path, topics_folder_path,
                 load_triplets=True, rerank_path=None, seed=42, nb_init_task=5, nb_evaluated_task=2, switch=False):
        super().__init__(data_folder_path, topics_folder_path, 
                         load_triplets=load_triplets, rerank_path=rerank_path, seed=seed)

        np.random.seed(self.seed)
        torch.manual_seed(self.seed)
        task_ids = torch.LongTensor(super().get_task_ids())
        task_permutation = torch.randperm(len(task_ids))
        permuted_task_ids = task_ids[task_permutation]
        self.scenario_task = 0
        self.scenario_tasks =\
            [ permuted_task_ids[0:nb_init_task].tolist() ,
             permuted_task_ids[nb_init_task:nb_init_task+nb_evaluated_task].tolist(),
             permuted_task_ids[nb_init_task:nb_init_task+nb_evaluated_task].tolist()]
        print(""Tinit"", self.scenario_tasks[0])
        print(""T1 & T2"", self.scenario_tasks[1])

        self._build_scenario(self.scenario_tasks[1], switch=switch)

        self.set_task(0)
        if(load_triplets):
            self.set_negative_docs_return(True)

    def _doc_embedding_kmeans(self, embedding_model, switch=False):
        qids, dids, dtxts = [], [], []
        for sample_idx in tqdm.trange(super().__len__()):
            sample = super().__getitem__(sample_idx)
            qids.append(sample[""qid""])
            dids.append(sample[""pdid""][0])
            dtxts.append(sample[""pdtxt""][0])
        qids = torch.LongTensor(qids)
        dids = torch.LongTensor(dids)
        documents_embedding =\
            embedding_model.encode(dtxts, show_progress_bar=True, convert_to_numpy=True)
        print(documents_embedding.shape)
        length = np.sqrt((documents_embedding**2).sum(axis=1))[:,None]
        documents_embedding_cos = documents_embedding / length
        
        kmeans_documents = KMeansConstrained(size_min=len(documents_embedding_cos)//2.1,n_clusters=2, random_state=5).fit_predict(documents_embedding_cos)
        torch_embeddings = torch.Tensor(documents_embedding)
        print(np.sum(kmeans_documents))
        cluster_1, cluster_2 = np.where(kmeans_documents == 0)[0], np.where(kmeans_documents == 1)[0]
        if switch:
            cluster_temp = cluster_1
            cluster_1 = cluster_2
            cluster_2 = cluster_temp
        qids_cluster_1, qids_cluster_2 = qids[cluster_1], qids[cluster_2]
        dids_cluster_1 = dids[cluster_1]

        print(""Cluster 1/2 shape "", str(len(set(qids_cluster_1)))+""/""+str(len(set(qids_cluster_2))))
        query_dev = self._get_queryids_task_split(self.working_task, ""dev"")
        nb_cluster_1_dev = len(set(query_dev).intersection(set(qids_cluster_1.tolist())))
        nb_cluster_2_dev = len(set(query_dev).intersection(set(qids_cluster_2.tolist())))
        print(""DEV SET for cluster 1"" , nb_cluster_1_dev)

        print(""DEV SET for cluster 2"" , nb_cluster_2_dev)
        cluster_2_to_cluster_1 = []
        embedding_cluster_1, embedding_cluster_2 = torch_embeddings[cluster_1], torch_embeddings[cluster_2]
        for id_cluster_2 in range(len(cluster_2)):
            _, reference_to_cluster_1 =\
                torch.cosine_similarity(embedding_cluster_2[id_cluster_2].unsqueeze(0), embedding_cluster_1).max(-1)
            # id_cluster_1 -> reference_cluster_1
            cluster_2_to_cluster_1.append(dids_cluster_1[reference_to_cluster_1].item())

        return cluster_2_to_cluster_1, qids_cluster_1.tolist(), qids_cluster_2.tolist()

    def _build_scenario(self, topics_scenario, switch=False):
        embedding_model = SentenceTransformer('stsb-roberta-large')
        self.topics_scenario = topics_scenario
        self.set_negative_docs_return(False)
        self.dids_task_1 = []
        self.qids_scenario = [[], [], []]
        np.random.seed(self.seed)
        torch.manual_seed(self.seed)

        for topic_scenario in topics_scenario:
            super().set_task(topic_scenario)
            dids, qids_task_1, qids_task_2 = self._doc_embedding_kmeans(embedding_model, switch=switch)
            self.dids_task_1 += dids
            self.qids_scenario[1] += qids_task_1
            self.qids_scenario[2] += qids_task_2
            assert(len(self.qids_scenario[2]) == len(self.dids_task_1))
        self.reversed_cluster_1 =\
            {self.qids_scenario[2][i]: i
             for i in range(len(self.dids_task_1))}

    def clone(self):
        return copy.copy(self)

    def get_task_ids(self):
        return [0, 1, 2]
    
    def get_nb_tasks(self):
        return 3

    def set_split(self, split_name):
        self.set_queries_index(self._get_queryids_task_split(self.working_task, split_name))
        self.working_split = split_name
        if (split_name == ""dev"" and self.scenario_task != 0) or self.scenario_task == 2:
            filtering_query = set(self.qids_scenario[self.scenario_task])
            self.set_queries_index([qid for qid in self.queries_index if qid in filtering_query])

    def get_generated_documents_ids(self):
        # (query in 2, document in 1)
        res = {}
        for qid, did_remap_index in self.reversed_cluster_1.items():
            did = self.dids_task_1[did_remap_index]
            res[qid] = {did:1}
        return res



    def set_task(self, task_id):
        task_id_list = self.scenario_tasks[task_id]
        self.working_task = task_id_list
        self.scenario_task = task_id
        self.set_split(self.working_split)


    def __getitem__(self, index):
        output_dict = super().__getitem__(index)
        query_index = output_dict[""qid""]
        if self.scenario_task == 1 and query_index in self.reversed_cluster_1:
            try:
                documents_index = self.dids_task_1[self.reversed_cluster_1[query_index]]
                if not isinstance(documents_index, list):
                    documents_index = [documents_index]
                documents_text = self.documents_collection.loc[documents_index][1].tolist()
            except KeyError as err:
                documents_index, documents_text = None, None
            
            output_dict[""pdid""] = documents_index
            output_dict[""pdtxt""] = documents_text

        return output_dict


    @staticmethod
    def test():
        import os
        import torch
        from lire.data_tools.dataset import scenarios as sc
        from lire.data_tools.dataset import MSMarco as ms
        d_folder = ""/net/cal/gerald/CPD/total_data""
        tp_folder = ""/net/cal/gerald/CPD/data/MSMarco-task-clustering-74-0.75-0.55/new_split""
        r_file = os.path.join(tp_folder, ""dev-bm25_top_1000.tsv"")
        #dt_o = ms.MSMarcoRankingDataset(d_folder, tp_folder, True, seed=42, rerank_path=r_file)
        dt = sc.MSMarcoInformationUpdateDataset(d_folder, tp_folder, True, seed=42, rerank_path=r_file)


class MSMarcoLanguageDriftDataset(MSMarcoInformationUpdateDataset):


    def _doc_embedding_kmeans(self, embedding_model, switch=False):
        qids, qtxt = [], []
        for sample_idx in tqdm.trange(MSMarcoRankingDataset.__len__(self)):
            sample = MSMarcoRankingDataset.__getitem__(self, sample_idx)
            qids.append(sample['qid'])
            qtxt.append(sample['qtxt'])

        qids = torch.LongTensor(qids)
        sts_embedding =\
            embedding_model.encode(qtxt, show_progress_bar=True, convert_to_numpy=True)
        
        # Using cosine similarity
        length = np.sqrt((sts_embedding**2).sum(axis=1))[:,None]
        sts_embedding_cos = sts_embedding / length
        pkmeans = KMeansConstrained(size_min=len(sts_embedding_cos)//2.1,n_clusters=2, random_state=5).fit_predict(sts_embedding_cos)
        torch_embeddings = torch.Tensor(sts_embedding)

        cluster_1, cluster_2 = np.where(pkmeans == 1)[0], np.where(pkmeans == 0)[0]
        if switch:
            cluster_temp = cluster_1
            cluster_1 = cluster_2
            cluster_2 = cluster_temp
        qids_cluster_1, qids_cluster_2 = qids[cluster_1], qids[cluster_2]
        print(""Cluster 1/2 shape "", str(len(cluster_1))+""/""+str(len(cluster_2)))
        query_dev = self._get_queryids_task_split(self.working_task, ""dev"")
        nb_cluster_1_dev = len(set(query_dev).intersection(set(qids_cluster_1.tolist())))
        nb_cluster_2_dev = len(set(query_dev).intersection(set(qids_cluster_2.tolist())))
        print(""DEV SET for cluster 1"" , nb_cluster_1_dev)

        print(""DEV SET for cluster 2"" , nb_cluster_2_dev)
        
        cluster_2_to_cluster_1 = []
        embedding_cluster_1, embedding_cluster_2 = torch_embeddings[cluster_1], torch_embeddings[cluster_2]
        for id_cluster_2 in range(len(cluster_2)):
            _, reference_to_cluster_1 =\
                torch.cosine_similarity(embedding_cluster_2[id_cluster_2].unsqueeze(0), embedding_cluster_1).max(-1)
            # id_cluster_1 -> reference_cluster_1
            cluster_2_to_cluster_1.append(qids_cluster_1[reference_to_cluster_1].item())

        return cluster_2_to_cluster_1, qids_cluster_1.tolist(), qids_cluster_2.tolist()

    def _build_scenario(self, topics_scenario, switch=False):
        embedding_model = SentenceTransformer('stsb-roberta-large')
        self.topics_scenario = topics_scenario
        self.set_negative_docs_return(False)
        self.dids_task_1 = []
        self.qids_scenario = [[], [], []]
        self.qids_task_1 = []
        for topic_scenario in topics_scenario:
            MSMarcoRankingDataset.set_task(self, topic_scenario)
            qids, qids_task_1, qids_task_2 = self._doc_embedding_kmeans(embedding_model, switch=switch)
            self.qids_task_1 += qids
            self.qids_scenario[1] += qids_task_1
            self.qids_scenario[2] += qids_task_2
            assert(len(self.qids_scenario[2]) == len(self.qids_task_1))
        self.reversed_cluster_1 =\
            {self.qids_scenario[2][i]: i
                for i in range(len(self.qids_task_1))}

        

    def set_split(self, split_name):
        self.set_queries_index(self._get_queryids_task_split(self.working_task, split_name))
        self.working_split = split_name
        if (split_name == ""dev"" and self.scenario_task != 0) or self.scenario_task == 2:
            filtering_query = set(self.qids_scenario[self.scenario_task])
            queries_index = [qid for qid in self.queries_index if qid in filtering_query]
            self.set_queries_index(queries_index)
            print(""Set Size ->"",len(queries_index), 
                  "" ID ->"", self.scenario_task,
                  "" ORSIZE ->"", len(self.qids_scenario[self.scenario_task]))

    def get_generated_documents_ids(self):
        rqdids = MSMarcoRankingDataset.get_relevant_documents_ids(self, queries_index=self.qids_scenario[2])
        res = {}
        for qid, index_map_qid in self.reversed_cluster_1.items():
            res[self.qids_task_1[index_map_qid]] = rqdids[qid]
        return res

    def __getitem__(self, index):
        output_dict = super(MSMarcoInformationUpdateDataset, self).__getitem__(index)
        query_index = output_dict['qid']
        if self.scenario_task == 1 and query_index in self.reversed_cluster_1:
            try:
                queries_index = self.qids_task_1[self.reversed_cluster_1[query_index]]
                if not isinstance(queries_index, list):
                    queries_index = [queries_index]
                queries_text = self.queries_collection.loc[queries_index][1].tolist()[0]
            except KeyError as err:
                queries_index, queries_text = ""None"", ""None""
            
            output_dict[""qid""] = queries_index
            output_dict[""qtxt""] = queries_text
        return output_dict

    @staticmethod
    def test():
        import os
        import torch
        from lire.data_tools.dataset import scenarios as sc
        from lire.data_tools.dataset import MSMarco as ms
        d_folder = ""/data/gerald/CPD/total_data""
        tp_folder = ""/data/gerald/CPD/data/MSMarco-task-clustering-74-0.75-0.55/new_split""
        r_file = os.path.join(tp_folder, ""dev-bm25_top_1000.tsv"")
        #dt_o = ms.MSMarcoRankingDataset(d_folder, tp_folder, True, seed=42, rerank_path=r_file)
        dt = sc.MSMarcoLanguageDriftDataset(d_folder, tp_folder, True, seed=42, rerank_path=r_file)

class MSMarcoDirectTransferDataset(MSMarcoRankingDataset):
    def __init__(self, data_folder_path, topics_folder_path,
                 load_triplets=True, rerank_path=None, seed=42, nb_init_task=5, nb_evaluated_task=2, switch=False):

        super().__init__(data_folder_path, topics_folder_path, 
                         load_triplets=load_triplets, rerank_path=rerank_path, seed=seed)

        np.random.seed(self.seed)
        torch.manual_seed(self.seed)
        task_ids = torch.LongTensor(super().get_task_ids())
        task_permutation = torch.randperm(len(task_ids))
        permuted_task_ids = task_ids[task_permutation]
        self.scenario_task = 0
        self.scenario_tasks =\
            [ permuted_task_ids[0:nb_init_task].tolist() ,
             permuted_task_ids[nb_init_task:nb_init_task+nb_evaluated_task].tolist(),
             [permuted_task_ids[nb_init_task+nb_evaluated_task].item()],
             permuted_task_ids[nb_init_task:nb_init_task+nb_evaluated_task].tolist()]
        print(""Tinit"", self.scenario_tasks[0])
        print(""T1 & T2"", self.scenario_tasks[1])

        self._build_scenario(self.scenario_tasks[1])

        self.set_task(0)
        if(load_triplets):
            self.set_negative_docs_return(True)

    def _split_task(self, embedding_model):
        qids, dids, qtxt = [], [], []
        for sample_idx in tqdm.trange(super().__len__()):
            d = super().__getitem__(sample_idx)
            qids.append(d['qid'])
        qids = torch.LongTensor(qids)
        perm = torch.randperm(len(qids))
        qids_cluster_1 = qids[perm[:int(len(perm) * 0.75)]]
        qids_cluster_2 = qids[perm[int(len(perm) * 0.75):]]
        return qids_cluster_1.tolist(), qids_cluster_2.tolist()

    def _build_scenario(self, topics_scenario):
        embedding_model = SentenceTransformer('stsb-roberta-large')
        self.topics_scenario = topics_scenario
        self.set_negative_docs_return(False)
        self.dids_task_1 = []
        self.qids_scenario = [[], [], [], []]
        for topic_scenario in topics_scenario:
            super().set_task(topic_scenario)
            qids_task_1, qids_task_2 = self._split_task(embedding_model)
            print(""TASK SPLIT SIZE"",len(qids_task_1), len(qids_task_2))
            self.qids_scenario[1] += qids_task_1
            self.qids_scenario[3] += qids_task_2

    def set_split(self, split_name):
        queries_split = self._get_queryids_task_split(self.working_task, split_name)
        self.set_queries_index(queries_split)
        self.working_split = split_name
        if self.scenario_task == 1 or self.scenario_task == 3:
            filtering_query = set(self.qids_scenario[self.scenario_task])
            queries_index = [qid for qid in queries_split if qid in filtering_query]
            self.set_queries_index(queries_index)
            print(self.scenario_task, "","", split_name, ""->"", len(self))
        

    def get_task_ids(self):
        return [0, 1, 2, 3 ]
    
    def get_nb_tasks(self):
        return 4

    def set_task(self, task_id):
        task_id_list = self.scenario_tasks[task_id]
        self.working_task = task_id_list
        self.scenario_task = task_id
        self.set_split(self.working_split)

    # def get_relevant_documents_ids(self):
    #     if self.scenario_task in {1,2}:
    #         return super().get_relevant_documents_ids(self.qids_scenario[self.scenario_task])
    #     else:
    #         return super().get_relevant_documents_ids()

    # def get_rerank_collection(self, queries_index=None, subrank=None):
    #     if self.scenario_task in {1,2}:
    #         return super().get_rerank_collection(self.qids_scenario[self.scenario_task])
    #     else:
    #         return super().get_rerank_collection()
    @staticmethod
    def test():
        from lire.data_tools.dataset.scenarios import  MSMarcoInformationUpdateDataset
        dt = MSMarcoInformationUpdateDataset(""/net/cal/gerald/CPD/total_data"", 
                                ""/net/cal/gerald/CPD/data/MSMarco-task-clustering-74-0.75-0.55"",
                                True)"
98,2201.03356,"''' Clustering query to build tasks.
The following script create a train/val/test set with(n, 20, 40) queries respectivelly
'''

import pandas as pd
import numpy as np
import argparse
import pickle
import os
import torch
import tqdm
import random
import inspect
from sentence_transformers import SentenceTransformer, util



class ContinualGenerator():
    ''' Continual generator that split corpus based on topics.

        Parameters:

            query_set : [(query_id, query_txt)]
                The set of queries information (query_id and the text 
                associated to the query)
            pre_computed_embedding_path : str(path) Optional
                The path to the queries embedding if not given
                embeddings will be computed instead (can take time).
                If no file is available at the current path but the 
                parameter is given the embedding will be saved at the 
                location

    '''
    def __init__(self, query_set, pre_computed_embedding_path=None):
        self.query_set = query_set
        self.pre_computed_embedding_path = pre_computed_embedding_path


    def _get_embeddings(self, queries_content): 
        if self.pre_computed_embedding_path is None:
            model = SentenceTransformer('stsb-roberta-large')
            corpus_embeddings = model.encode(queries_content, show_progress_bar=True, convert_to_numpy=True)

        elif not os.path.exists(self.pre_computed_embedding_path):
            os.makedirs(os.path.dirname(self.pre_computed_embedding_path), exist_ok=True)
            model = SentenceTransformer('stsb-roberta-large')
            corpus_embeddings = model.encode(queries_content, show_progress_bar=True, convert_to_numpy=True)

            with open(self.pre_computed_embedding_path, ""wb"") as fOut:
                pickle.dump({'sentences': queries_content , 'embeddings': corpus_embeddings}, fOut)

        else:
            with open(self.pre_computed_embedding_path, ""rb"") as fOut:
                corpus_embeddings = pickle.load(fOut)['embeddings']

        return corpus_embeddings
    
    @staticmethod
    def community_filtering(cos_scores, alpha):
        return torch.arange(len(cos_scores))[cos_scores >= alpha]

    @staticmethod
    def community_detection_clustering(embeddings, ss_embeddings_estimation=50000,
                                       alpha=0.75, beta=0.55, mcs=2000):
        rp = torch.randperm(len(embeddings))
        sse = rp[:ss_embeddings_estimation]
        ss_embeddings = embeddings[sse]
        emcs = round((ss_embeddings_estimation/len(embeddings)) * mcs)
        print('Computing the cos score on ', len(ss_embeddings), ' embeddings')
        cos_scores = util.pytorch_cos_sim(ss_embeddings, ss_embeddings)

        print('Retrieve the ', emcs, ' closest neigbhors for each embeddings (max similarity)')
        top_k_values, top_k_indexes = cos_scores.topk(k=emcs, largest=True)

        print('Create the communities')
        communities = [(rp[top_k_indexes[i][0]], rp[ContinualGenerator.community_filtering(cos_scores[i], alpha)]) 
                    for i in range(len(top_k_values))  
                    if(top_k_values[i][-1] >= alpha)]
        print('Filtering communities to avoid overlapping (at the time there is ', len(communities),' communities)')
        sorted_communities = sorted(communities, key=lambda x: len(x[1]), reverse=True)

        unique_communities = []
        extracted_ids = set()

        for centroid, community in sorted_communities:
            add_cluster = True
            for idx in community:
                if idx in extracted_ids:
                    add_cluster = False
                    break

            if add_cluster:
                unique_communities.append((centroid, community))
                for idx in community:
                    extracted_ids.add(idx)
        centroids = [torch.Tensor(embeddings[centroid]) for centroid, _ in unique_communities]

        print('creating real clusters according to number of examples by clusters')
        print(len(embeddings))
        cluster_center = torch.stack(centroids)
        complete_communities = [[] for i in range(len(cluster_center))]
        for i, d in enumerate(embeddings):
            cos_sim = util.pytorch_cos_sim(cluster_center, d).flatten()
            max_value, max_index = cos_sim.max(-1)
            if max_value >= beta:
                complete_communities[max_index].append(i)
        
    
        return centroids, complete_communities




    def generate(self, t1=0.75, t2=0.5, mcs=2000, estimation_set_size=50000):
        ''' Generate groups of queries.
            
            Parameters:
                t1: float
                    the first threshold
                t2: float
                    the second threshold (all dataset)
                msc: int
                    minimum number of groups representent
                estimation_set_size: int 
                    the number of elements to perform clustering
                    according to t1
        '''
        queries_ids = [k for (k, v) in self.query_set]
        queries_content = [v for (k, v) in self.query_set]
        embeddings = self._get_embeddings(queries_content)
        centroids, communities =\
            ContinualGenerator.community_detection_clustering(embeddings, estimation_set_size, t1, t2, mcs)
        fcom = [[queries_ids[query_index] for j, query_index in enumerate(community)]
                 for i, community in enumerate(communities)]
        return fcom, centroids

def create_new_continual_set(self, queries_set, documents_set,
                             queries_embeddings_path=None, documents_bm25_index_path=None,
                             t1=0.75, t2=0.5, mcs=2000, estimation_set_size=50000,
                             val_size=50, test_psize=50, topk=1000):
    def split_set(queries, val_set_size, test_set_size):
        queries = queries[torch.randperm(len(queries))]
        end_val = val_set_size if(isinstance(val_set_size, int)) else int(val_set_size * len(queries))
        end_test = end_val + test_set_size if(isinstance(test_set_size, int)) else val_set_size + int(test_set_size * len(queries))
        assert(end_val + end_test <= len(queries))
        return {'val': queries[:end_val], 'test': queries[end_val:end_test], 'train': queries[end_test:]}

    #TODO
    bm = ranker.BM25('/net/cal/gerald/CPD/data/set')
    bm.create_index(ms_documents, '/net/cal/gerald/CPD/data/set')
    topk_documents = bm.rank_corpus(queries_final, top_k=topk)



# print('Filtering according to existance of triplet')
# csv = train_dataset.load_triplets()
# query_set_check = set([query for cluster in clusters for query in cluster])
# a = csv.index.isin(query_set_check)
# b = set(np.unique(csv.index[a]).tolist())
# if(query_set_check != len(b)):
#     print(""Missing some queries in triplet file train : ""+ str(len(query_set_check - b)))
# print(""Number of triplets : "" + str(a.sum()) + "" for "" + str(len(b))+"" queries"")
# assert(len(a) == len(csv))
# triplet_subset = csv.iloc[a]
# query_available = query_available.intersection(set(triplet_subset.index))

# print(""Process the clusters"")
# clusters = [[query for query in cluster if(query in query_available)] for cluster in clusters]


# print(len(clusters), '  final clusters find with the following size ', [len(cluster) for cluster in clusters])


# example_cluster = [[queries_content[inv_queries_id[index]] for index in topic[:2]] for topic in clusters]

# print('Representative samples by clusters : ', example_cluster)


# print('Get the closest neigbhor documents according to bm25 (if you are not satisfy and want to change k of top k use the prediction/bm25_triplet script)')

# print('Loading documents')
# documents = train_dataset.load_documents()
# ms_documents = {i_doc: documents[i_doc][0] for i_doc in documents}

# print('Transform queries for correct format')

# queries_final  = {qid: queries_dct[qid][0] for qid in query_available}

# print('computing top100 for current documents')

# bm = ranker.BM25('/net/cal/gerald/CPD/data/set')
# bm.create_index(ms_documents, '/net/cal/gerald/CPD/data/set')
# topk_documents = bm.rank_corpus(queries_final, top_k = args.topk)

# print('Splitting into train val test')

# def split_set(queries, val_set_size, test_set_size):
#     queries = queries[torch.randperm(len(queries))]
#     end_val = val_set_size if(isinstance(val_set_size, int)) else int(val_set_size * len(queries))
#     end_test = end_val + test_set_size if(isinstance(test_set_size, int)) else val_set_size + int(test_set_size * len(queries))
#     assert(end_val + end_test <= len(queries))
#     return {'val': queries[:end_val], 'test': queries[end_val:end_test], 'train': queries[end_test:]}


# clusters = [split_set(torch.LongTensor(cluster), args.val_set_size, args.test_set_size) for cluster in clusters]

# print([{'val':len(s['val']), 'train': len(s['train']), 'test':len(s['test'])} for s in c  for c in clusters])
# print('Saving the dataset: it can take a while ')


"
99,2201.03356,"import os
from os.path import join, basename, exists
from os import makedirs, path, rename

from lire.dataset import data_tools 
import json

import torch
import numpy as np
import pandas as pd

from hashlib import blake2s
import ir_datasets

class MSMarcoBaseDataset():
    def __init__(self, load_triplets=False, seed=42,  
                 data_folder='$DATA_FOLDER/lire/MSMarco'):
        torch.manual_seed(seed)
        np.random.seed(seed)
        self.seed = seed
        self.data_folder = os.path.expanduser(os.path.expandvars(data_folder))
        self.load_triplets = load_triplets
        dataset = ir_datasets.load('msmarco-passage/train/judged')
        # load the tsv files
        print(""Loading qrels"")
        self.qrels_collection =\
            pd.DataFrame.from_records(dataset.qrels_iter(),
                                        columns=['query_id', 'doc_id', 'relevance','iteration'],
                                        index='query_id')
        print(""Loading documents"")
        self.documents_collection =\
            pd.DataFrame.from_records(dataset.docs_iter(),
                                        columns=['doc_id', 'text'],
                                        index='doc_id')
        print(""Loading queries"")
        self.queries_collection =\
            pd.DataFrame.from_records(dataset.queries_iter(),
                                        columns=['query_id', 'text'],
                                        index='query_id')
        self.return_negative_docs = False
        self.triplets_collection = None
        if self.load_triplets:
            self.return_negative_docs = True
            self.triplets_collection_raw =\
                pd.DataFrame.from_records(dataset.docpairs_iter(), 
                                            columns=['query_id', 'relevant_doc', 'irrelevant_doc'])
            pd.read_csv(self.triplets_path, sep='\t', header=None)
            self.triplets_collection = self.triplets_collection_raw.groupby('query_id')
            self.group_keys = set(self.triplets_collection.groups.keys())

        self.queries_index = self.queries_collection.index
        self.n_sample_negative = None
        self.uniform_negative_sampling = True
        self.rerank_path = None
        self.rerank_group = None


    def get_ir_dataset(self):
        return ir_datasets.load('msmarco-passage/train/judged')
    
    def get_rerank_group_by_query_id(self, query_id):
        query_text = self.queries_collection.loc[query_id]['text']    
        documents_index = self.rerank_groups.get_group(query_id)[1].tolist()

        if not isinstance(documents_index, list):
            documents_index = [documents_index]
        documents_text = self.documents_collection.loc[documents_index]['text'].tolist()

        return {'qid':query_id, 'qtxt': query_text, 'pdid': documents_index, 'pdtxt': documents_text}

    def filter_rerank_group(self, queries_index):
        return self.rerank_collection[self.rerank_collection[0].isin(queries_index)]


    def get_relevant_documents_ids(self, queries_index=None):
        queries_index = self.queries_index if(queries_index is None) else queries_index
        # to faster collection of data we temporary remove ngeative docs
        temp_return_negative_docs = self.return_negative_docs
        self.return_negative_docs = False 
        relevant_documents = {}
        for qid in queries_index:
            sample = self.get_data_by_query_id(qid)
            if sample['pdid'] is not None:
                relevant_documents[qid] = {}
                for did in  sample['pdid']:
                    relevant_documents[qid][did] = 1
        self.return_negative_docs = temp_return_negative_docs
        return relevant_documents

    def get_rerank_collection(self, queries_index=None, subrank=None):
        rerank_df =\
            self.filter_rerank_group(self.queries_index) if(queries_index is None)\
                else self.filter_rerank_group(queries_index)
        if subrank is not None:
            rerank_df = rerank_df.loc[rerank_df.groupby(['query_id'])['score'].nlargest(subrank).reset_index()[""level_1""]]

        
        class RerankCorpus():
            def __init__(self, queries_collection, documents_collection, rerank_collection):
                self.queries_collection = queries_collection
                self.documents_collection = documents_collection
                self.rerank_collection = rerank_collection

            def __len__(self):
                return len(self.rerank_collection)

            def __getitem__(self, index):
                rerank_serie = self.rerank_collection.iloc[index]
                init_score = rerank_serie['score']
                query_index, document_index = rerank_serie['query_id'], rerank_serie['doc_id']
                query_txt  = self.queries_collection.loc[query_index][""text""]
                document_txt = self.documents_collection.loc[document_index][""text""]

                return {
                    'qid':int(query_index),
                    'qtxt':query_txt,
                    'did':int(document_index),
                    'dtxt':document_txt,
                    'score': init_score
                }
        return RerankCorpus(self.queries_collection, self.documents_collection, rerank_df)

    def get_query(self, query_index):
        return self.queries_collection.loc[query_index][""text""]
    
    def get_document(self, document_index):
        return self.documents_collection.loc[document_index][""text""]

    def get_data_by_query_id(self, query_index):
        query_text = self.queries_collection.loc[query_index][""text""]
        try:
            documents_index = self.qrels_collection.loc[query_index][""doc_id""]

            if isinstance(documents_index, str):
                documents_index = [documents_index]
            else: 
                documents_index = documents_index.tolist()
            documents_text = self.documents_collection.loc[documents_index][""text""].tolist()
        except KeyError as err:
            documents_index, documents_text = None, None
        output_dict = {""qid"":query_index, ""qtxt"":query_text, ""pdid"":documents_index, ""pdtxt"":documents_text}
        if self.return_negative_docs and self.load_triplets:
            try:
                negative_documents_index = self.triplets_collection.get_group(query_index)[""irrelevant_doc""]

                if isinstance(documents_index, str):
                    negative_documents_index = [negative_documents_index]
                else: 
                    negative_documents_index = negative_documents_index.tolist()

                if self.n_sample_negative is not None:
                    rp = torch.randperm(len(negative_documents_index))
                    if self.uniform_negative_sampling:
                        negative_documents_index =\
                            torch.LongTensor(negative_documents_index)[rp][:self.n_sample_negative].tolist()
                    else:
                        negative_documents_index =\
                            torch.LongTensor(negative_documents_index)[:self.n_sample_negative].tolist()            

                negative_documents_text = self.documents_collection.loc[negative_documents_index][""text""].tolist()
            except KeyError as err:
                negative_documents_index, negative_documents_text = None, None
            output_dict[""ndid""] = negative_documents_index
            output_dict[""ndtxt""] = negative_documents_text
        return output_dict

    def set_uniform_negative_sampling(self, boolean_value):
        self.uniform_negative_sampling = boolean_value

    def __getitem__(self, index):
        query_index = self.queries_index[index]
        return self.get_data_by_query_id(query_index)

    def set_n_sample_negative(self, max_negative):
        self.n_sample_negative = max_negative

    def set_negative_docs_return(self, return_negative_docs):
        if self.return_negative_docs and not self.load_triplets:
            print('Provide triplet file when init the dataset')
            return
        self.return_negative_docs = return_negative_docs

    def set_queries_index(self, queries_ids):
        self.queries_index = queries_ids
        if self.triplets_collection is not None:
            self.queries_index = [query_id for query_id in queries_ids if query_id in self.group_keys]


    def get_queries_index(self):
        return copy.deepcopy(self.queries_index)

    def __len__(self):
        return len(self.queries_index)

    def clone(self):
        return copy.copy(self)


class MSMarcoRankingDataset(MSMarcoBaseDataset):
    def __init__(self, topics_folder_path, 
                 load_triplets=True, seed=42, 
                 data_folder='$DATA_FOLDER/lire/MSMarcoRanking', 
                 randomize=False, bm25_compute=False, **args):
        super().__init__(load_triplets=False, data_folder=data_folder, seed=seed)
        self.data = {}
        str_hash = '' 
        h = blake2s(digest_size=20)
        for filename in (""train"", ""val"", ""test""):
            with open(os.path.join(topics_folder_path, 'topics.'+filename+'.json'), 'r') as topic_file:
                content = topic_file.read()
                str_hash += content
                self.data[filename] = [[str(query) for query in topic] for topic in json.loads(content)]
        h.update(str_hash.encode())
        self.data_folder = os.path.join(self.data_folder, h.hexdigest())
        os.makedirs(self.data_folder, exist_ok=True)
        if randomize:
            new_data = {}
            # for the random setting
            for split_name, topics in self.data.items():
                queries_split =  [query for topic in topics for query in topic]
                random_permutation = torch.randperm(len(queries_split))
                cpt = 0
                new_data[split_name] = []
                for topic in topics:
                    new_data[split_name].append([]) 
                    for _ in topic:
                        new_data[split_name][-1].append(queries_split[random_permutation[cpt].item()])
                        cpt +=1
            self.data = new_data
        if load_triplets:
            self.load_triplets = True
            data_filename = os.path.join(self.data_folder, 'triplets.tsv')
            if os.path.exists(data_filename):
               self.triplets_collection = pd.read_csv(data_filename, sep='\t')
            if not os.path.exists(data_filename):
                print('Filtering training pairs (can make time)')
                train_queries = {query for topic in self.data['train'] for query in topic}
                doc_pairs_iterator = self.get_ir_dataset().docpairs_iter()
                doc_pair_dict = {'query_id':[], 'relevant_doc':[], 'irrelevant_doc':[]}
                for i, doc_pair in enumerate(doc_pairs_iterator):
                    if doc_pair.query_id in train_queries:
                        doc_pair_dict['query_id'].append(doc_pair.query_id)
                        doc_pair_dict['relevant_doc'].append(doc_pair.doc_id_a)
                        doc_pair_dict['irrelevant_doc'].append(doc_pair.doc_id_b)
                    if i % 1000000 == 0:
                        print(i, 'docpairs processed')
                df = pd.DataFrame.from_dict(doc_pair_dict).to_csv(data_filename, index=False, sep='\t')
            print(""loading triplet"")
            self.triplets_collection = pd.read_csv(data_filename, sep='\t', index_col='query_id').groupby('query_id')
       # creating or loading the bm25 relevant documents with by default top1000 for val and top1000 for test
        queries_test_collection = self.queries_collection.loc[[query for topic in self.data['test'] for query in topic]]
        queries_val_collection = self.queries_collection.loc[[query for topic in self.data['val'] for query in topic]]
        
        os.makedirs(os.path.join(self.data_folder, 'bm25_test'), exist_ok=True)
        os.makedirs(os.path.join(self.data_folder, 'bm25_val'), exist_ok=True)
        os.makedirs(os.path.join(self.data_folder, 'msmarco_base'), exist_ok=True)

        scoreddoctest = data_tools.bm25_rank(queries_test_collection.itertuples(), self.documents_collection.itertuples(),
                             storage_filepath=os.path.join(self.data_folder, 'bm25_test'), top_k=1000,
                             doc_storage_filepath=os.path.join(self.data_folder, 'msmarco_base'))

        scoreddocval = data_tools.bm25_rank(queries_val_collection.itertuples(), self.documents_collection.itertuples(),
                             storage_filepath=os.path.join(self.data_folder, 'bm25_val'), top_k=1000,
                             doc_storage_filepath=os.path.join(self.data_folder, 'msmarco_base'))
        self.rerank_collection = pd.concat([scoreddoctest, scoreddocval]).groupby('query_id')
        self.rerank_group = self.rerank_collection
        test_assert = [{q for t in v for q in t} for k, v in self.data.items()]
        assert(len(test_assert[0].intersection(test_assert[1])) == 0)
        assert(len(test_assert[0].intersection(test_assert[2])) == 0)
        assert(len(test_assert[1].intersection(test_assert[2])) == 0)
        
        self.working_task = ""all""
        self.working_split = ""all""
        self.set_queries_index(self._get_queryids_task_split(""all"", ""all""))
        self.query2task_collection = {q:i for k, v in self.data.items() 
                                      for i,t in enumerate(v) 
                                      for q in t }

        self.seed = seed
        MSMarcoRankingDataset.set_split(self, 'all')


    def clone(self):
        return copy.copy(self)

    def _get_queryids_task_split(self, task_id, split_name):
        if isinstance(split_name, str):
            if split_name == 'all':
                split_name_list = list(self.data.keys())
            else:
                split_name_list = [split_name]
        else: 
            split_name_list = split_name
        task_query = []
        for split_key in split_name_list:
            for task_key, queries_id in enumerate(self.data[split_key]):
                if len(task_query) <= task_key:
                    task_query.append([])
                task_query[task_key] += queries_id

        if task_id == 'all':
            task_id_list = list(range(len(task_query)))
        elif isinstance(task_id, int):
            task_id_list = [task_id]
        else: 
            task_id_list = task_id

        return sum([task_query[i] for i in task_id_list],[])

    def set_split(self, split_name):
        self.set_queries_index(self._get_queryids_task_split(self.working_task, split_name))
        self.working_split = split_name
        
    def set_task(self, task_id):
        self.set_queries_index(self._get_queryids_task_split(task_id, self.working_split))
        self.working_task = task_id

    def get_task_ids(self):
        return [i for i in range(len(list(self.data.values())[0]))]

    def get_nb_tasks(self):
        return len(list(self.data.values())[0])

    def __getitem__(self, index):
        output_dict = super().__getitem__(index)
        output_dict[""tid""] = self.query2task_collection[output_dict[""qid""]]
        return output_dict
    
    # deprecated
    def set_current_task_by_id(self, task_id):
        self.set_task(task_id)


class MSMarcoRankingDatasetAllTasks(MSMarcoRankingDataset):
    def __init__(self,  topics_folder_path,
                 load_triplets=False, seed=42, 
                 rerank_path=None, **args):
        super().__init__( topics_folder_path, load_triplets=load_triplets,  seed=seed, rerank_path=rerank_path)

    def get_task_ids(self):
        return [0]

    def get_nb_tasks(self):
        return 1

    def set_task(self, task_id):
        self.set_queries_index(self._get_queryids_task_split('all', self.working_split))
        self.working_task = 'all'"
100,2201.03356,"from pytools import memoize_method
import torch

from torch import nn
import torch.nn.functional as F
import pytorch_pretrained_bert
from . import modeling_util
import os

# https://github.com/Georgetown-IR-Lab/cedr/
class BertRanker(torch.nn.Module):
    def __init__(self):
        print(""BertRanker INIT"")
        super().__init__()
        self.BERT_MODEL = 'bert-base-uncased'
        self.CHANNELS = 12 + 1 # from bert-base-uncased
        self.BERT_SIZE = 768 # from bert-base-uncased
        cache_dir = os.environ.get('DATA_FOLDER')
        if cache_dir is not None:
            cache_dir = os.path.join(cache_dir, ""PPB_PRETRAINED"")

        self.bert = CustomBertModel.from_pretrained(self.BERT_MODEL, cache_dir=cache_dir)
        self.tokenizer = pytorch_pretrained_bert.BertTokenizer.from_pretrained(self.BERT_MODEL, cache_dir=cache_dir)

    def forward(self, **inputs):
        raise NotImplementedError

    def save(self, path):
        state = self.state_dict(keep_vars=True)
        for key in list(state):
            if state[key].requires_grad:
                state[key] = state[key].data
            else:
                del state[key]
        torch.save(state, path)

    def load(self, path):
        self.load_state_dict(torch.load(path), strict=False)

    @memoize_method
    def tokenize(self, text):
        toks = self.tokenizer.tokenize(text)
        toks = [self.tokenizer.vocab[t] for t in toks]
        return toks

    def encode_bert(self, query_tok, query_mask, doc_tok, doc_mask):
        BATCH, QLEN = query_tok.shape
        DIFF = 3 # = [CLS] and 2x[SEP]
        maxlen = self.bert.config.max_position_embeddings
        MAX_DOC_TOK_LEN = maxlen - QLEN - DIFF

        doc_toks, sbcount = modeling_util.subbatch(doc_tok, MAX_DOC_TOK_LEN)
        doc_mask, _ = modeling_util.subbatch(doc_mask, MAX_DOC_TOK_LEN)

        query_toks = torch.cat([query_tok] * sbcount, dim=0)
        query_mask = torch.cat([query_mask] * sbcount, dim=0)

        CLSS = torch.full_like(query_toks[:, :1], self.tokenizer.vocab['[CLS]'])
        SEPS = torch.full_like(query_toks[:, :1], self.tokenizer.vocab['[SEP]'])
        ONES = torch.ones_like(query_mask[:, :1])
        NILS = torch.zeros_like(query_mask[:, :1])

        # build BERT input sequences
        toks = torch.cat([CLSS, query_toks, SEPS, doc_toks, SEPS], dim=1)
        mask = torch.cat([ONES, query_mask, ONES, doc_mask, ONES], dim=1)
        segment_ids = torch.cat([NILS] * (2 + QLEN) + [ONES] * (1 + doc_toks.shape[1]), dim=1)
        toks[toks == -1] = 0 # remove padding (will be masked anyway)

        # execute BERT model
        result = self.bert(toks, segment_ids.long(), mask)

        # extract relevant subsequences for query and doc
        query_results = [r[:BATCH, 1:QLEN+1] for r in result]
        doc_results = [r[:, QLEN+2:-1] for r in result]
        doc_results = [modeling_util.un_subbatch(r, doc_tok, MAX_DOC_TOK_LEN) for r in doc_results]

        # build CLS representation
        cls_results = []
        for layer in result:
            cls_output = layer[:, 0]
            cls_result = []
            for i in range(cls_output.shape[0] // BATCH):
                cls_result.append(cls_output[i*BATCH:(i+1)*BATCH])
            cls_result = torch.stack(cls_result, dim=2).mean(dim=2)
            cls_results.append(cls_result)

        return cls_results, query_results, doc_results


class VanillaBertRanker(BertRanker):
    def __init__(self):
        print(""VanillaBertRanker INIT"")
        super().__init__()
        self.dropout = torch.nn.Dropout(0.1)
        self.cls = torch.nn.Linear(self.BERT_SIZE, 1)

    def forward(self, query_tok, query_mask, doc_tok, doc_mask):
        cls_reps, _, _ = self.encode_bert(query_tok, query_mask, doc_tok, doc_mask)
        return self.cls(self.dropout(cls_reps[-1]))

class CedrPacrrRanker(BertRanker):
    def __init__(self):
        super().__init__()
        QLEN = 20
        KMAX = 2
        NFILTERS = 32
        MINGRAM = 1
        MAXGRAM = 3
        self.simmat = modeling_util.SimmatModule()
        self.ngrams = torch.nn.ModuleList()
        self.rbf_bank = None
        for ng in range(MINGRAM, MAXGRAM+1):
            ng = modeling_util.PACRRConvMax2dModule(ng, NFILTERS, k=KMAX, channels=self.CHANNELS)
            self.ngrams.append(ng)
        qvalue_size = len(self.ngrams) * KMAX
        self.linear1 = torch.nn.Linear(self.BERT_SIZE + QLEN * qvalue_size, 32)
        self.linear2 = torch.nn.Linear(32, 32)
        self.linear3 = torch.nn.Linear(32, 1)

    def forward(self, query_tok, query_mask, doc_tok, doc_mask):
        cls_reps, query_reps, doc_reps = self.encode_bert(query_tok, query_mask, doc_tok, doc_mask)
        simmat = self.simmat(query_reps, doc_reps, query_tok, doc_tok)
        scores = [ng(simmat) for ng in self.ngrams]
        scores = torch.cat(scores, dim=2)
        scores = scores.reshape(scores.shape[0], scores.shape[1] * scores.shape[2])
        scores = torch.cat([scores, cls_reps[-1]], dim=1)
        rel = F.relu(self.linear1(scores))
        rel = F.relu(self.linear2(rel))
        rel = self.linear3(rel)
        return rel


class CedrKnrmRanker(BertRanker):
    def __init__(self):
        print(""CedrKnrmRanker INIT"")
        super().__init__()

        MUS = [-0.9, -0.7, -0.5, -0.3, -0.1, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
        SIGMAS = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.001]
        # the following line come from the orginal model, it is not used during inference process
        # and double the memory need for the model
        # self.bert_ranker = VanillaBertRanker()
        self.simmat = modeling_util.SimmatModule()
        self.kernels = modeling_util.KNRMRbfKernelBank(MUS, SIGMAS)
        self.combine = torch.nn.Linear(self.kernels.count() * self.CHANNELS + self.BERT_SIZE, 1)

    def forward(self, query_tok, query_mask, doc_tok, doc_mask):

        cls_reps, query_reps, doc_reps = self.encode_bert(query_tok, query_mask, doc_tok, doc_mask)
        simmat = self.simmat(query_reps, doc_reps, query_tok, doc_tok)
        kernels = self.kernels(simmat)
        BATCH, KERNELS, VIEWS, QLEN, DLEN = kernels.shape
        kernels = kernels.reshape(BATCH, KERNELS * VIEWS, QLEN, DLEN)
        simmat = simmat.reshape(BATCH, 1, VIEWS, QLEN, DLEN) \
                       .expand(BATCH, KERNELS, VIEWS, QLEN, DLEN) \
                       .reshape(BATCH, KERNELS * VIEWS, QLEN, DLEN)
        result = kernels.sum(dim=3) # sum over document
        mask = (simmat.sum(dim=3) != 0.) # which query terms are not padding?
        result = torch.where(mask, (result + 1e-6).log(), mask.float())
        result = result.sum(dim=2) # sum over query terms
        result = torch.cat([result, cls_reps[-1]], dim=1)
        scores = self.combine(result) # linear combination over kernels
        return scores


class CedrDrmmRanker(BertRanker):
    def __init__(self):
        super().__init__()
        NBINS = 11
        HIDDEN = 5
        self.bert_ranker = VanillaBertRanker()
        self.simmat = modeling_util.SimmatModule()
        self.histogram = modeling_util.DRMMLogCountHistogram(NBINS)
        self.hidden_1 = torch.nn.Linear(NBINS * self.CHANNELS + self.BERT_SIZE, HIDDEN)
        self.hidden_2 = torch.nn.Linear(HIDDEN, 1)

    def forward(self, query_tok, query_mask, doc_tok, doc_mask):
        cls_reps, query_reps, doc_reps = self.encode_bert(query_tok, query_mask, doc_tok, doc_mask)
        simmat = self.simmat(query_reps, doc_reps, query_tok, doc_tok)
        histogram = self.histogram(simmat, doc_tok, query_tok)
        BATCH, CHANNELS, QLEN, BINS = histogram.shape
        histogram = histogram.permute(0, 2, 3, 1)
        output = histogram.reshape(BATCH * QLEN, BINS * CHANNELS)
        # repeat cls representation for each query token
        cls_rep = cls_reps[-1].reshape(BATCH, 1, -1).expand(BATCH, QLEN, -1).reshape(BATCH * QLEN, -1)
        output = torch.cat([output, cls_rep], dim=1)
        term_scores = self.hidden_2(torch.relu(self.hidden_1(output))).reshape(BATCH, QLEN)
        return term_scores.sum(dim=1)


class CustomBertModel(pytorch_pretrained_bert.BertModel):
    """"""
    Based on pytorch_pretrained_bert.BertModel, but also outputs un-contextualized embeddings.
    """"""
    def forward(self, input_ids, token_type_ids, attention_mask):
        """"""
        Based on pytorch_pretrained_bert.BertModel
        """"""
        embedding_output = self.embeddings(input_ids, token_type_ids)

        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0

        encoded_layers = self.encoder(embedding_output, extended_attention_mask, output_all_encoded_layers=True)

        return [embedding_output] + encoded_layers"
101,2201.03356," 
import math
import torch


def subbatch(toks, maxlen):
    _, DLEN = toks.shape[:2]
    SUBBATCH = math.ceil(DLEN / maxlen)
    S = math.ceil(DLEN / SUBBATCH) if SUBBATCH > 0 else 0 # minimize the size given the number of subbatch
    stack = []
    if SUBBATCH == 1:
        return toks, SUBBATCH
    else:
        for s in range(SUBBATCH):
            stack.append(toks[:, s*S:(s+1)*S])
            if stack[-1].shape[1] != S:
                nulls = torch.zeros_like(toks[:, :S - stack[-1].shape[1]])
                stack[-1] = torch.cat([stack[-1], nulls], dim=1)
        return torch.cat(stack, dim=0), SUBBATCH


def un_subbatch(embed, toks, maxlen):
    BATCH, DLEN = toks.shape[:2]
    SUBBATCH = math.ceil(DLEN / maxlen)
    if SUBBATCH == 1:
        return embed
    else:
        embed_stack = []
        for b in range(SUBBATCH):
            embed_stack.append(embed[b*BATCH:(b+1)*BATCH])
        embed = torch.cat(embed_stack, dim=1)
        embed = embed[:, :DLEN]
        return embed


class PACRRConvMax2dModule(torch.nn.Module):

    def __init__(self, shape, n_filters, k, channels):
        super().__init__()
        self.shape = shape
        if shape != 1:
            self.pad = torch.nn.ConstantPad2d((0, shape-1, 0, shape-1), 0)
        else:
            self.pad = None
        self.conv = torch.nn.Conv2d(channels, n_filters, shape)
        self.activation = torch.nn.ReLU()
        self.k = k
        self.shape = shape
        self.channels = channels

    def forward(self, simmat):
        BATCH, CHANNELS, QLEN, DLEN = simmat.shape
        if self.pad:
            simmat = self.pad(simmat)
        conv = self.activation(self.conv(simmat))
        top_filters, _ = conv.max(dim=1)
        top_toks, _ = top_filters.topk(self.k, dim=2)
        result = top_toks.reshape(BATCH, QLEN, self.k)
        return result


class SimmatModule(torch.nn.Module):

    def __init__(self, padding=-1):
        super().__init__()
        self.padding = padding
        self._hamming_index_loaded = None
        self._hamming_index = None

    def forward(self, query_embed, doc_embed, query_tok, doc_tok):
        simmat = []

        for a_emb, b_emb in zip(query_embed, doc_embed):
            BAT, A, B = a_emb.shape[0], a_emb.shape[1], b_emb.shape[1]
            # embeddings -- cosine similarity matrix
            a_denom = a_emb.norm(p=2, dim=2).reshape(BAT, A, 1).expand(BAT, A, B) + 1e-9 # avoid 0div
            b_denom = b_emb.norm(p=2, dim=2).reshape(BAT, 1, B).expand(BAT, A, B) + 1e-9 # avoid 0div
            perm = b_emb.permute(0, 2, 1)
            sim = a_emb.bmm(perm)
            sim = sim / (a_denom * b_denom)

            # nullify padding (indicated by -1 by default)
            nul = torch.zeros_like(sim)
            sim = torch.where(query_tok.reshape(BAT, A, 1).expand(BAT, A, B) == self.padding, nul, sim)
            sim = torch.where(doc_tok.reshape(BAT, 1, B).expand(BAT, A, B) == self.padding, nul, sim)

            simmat.append(sim)
        return torch.stack(simmat, dim=1)

    # def forward(self, query_embed, doc_embed):
    #     BATCH_SIZE, QUERY_SIZE, DOC_SIZE = query_embed.shape[0], query_embed.shape[1], doc_embed.shape[1]


class DRMMLogCountHistogram(torch.nn.Module):
    def __init__(self, bins):
        super().__init__()
        self.bins = bins

    def forward(self, simmat, dtoks, qtoks):
        # THIS IS SLOW ... Any way to make this faster? Maybe it's not worth doing on GPU?
        BATCH, CHANNELS, QLEN, DLEN = simmat.shape
        # +1e-5 to nudge scores of 1 to above threshold
        bins = ((simmat + 1.000001) / 2. * (self.bins - 1)).int()
        # set weights of 0 for padding (in both query and doc dims)
        weights = ((dtoks != -1).reshape(BATCH, 1, DLEN).expand(BATCH, QLEN, DLEN) * \
                  (qtoks != -1).reshape(BATCH, QLEN, 1).expand(BATCH, QLEN, DLEN)).float()

        # no way to batch this... loses gradients here. https://discuss.pytorch.org/t/histogram-function-in-pytorch/5350
        bins, weights = bins.cpu(), weights.cpu()
        histogram = []
        for superbins, w in zip(bins, weights):
            result = []
            for b in superbins:
                result.append(torch.stack([torch.bincount(q, x, self.bins) for q, x in zip(b, w)], dim=0))
            result = torch.stack(result, dim=0)
            histogram.append(result)
        histogram = torch.stack(histogram, dim=0)

        # back to GPU
        histogram = histogram.to(simmat.device)
        return (histogram.float() + 1e-5).log()


class KNRMRbfKernelBank(torch.nn.Module):
    def __init__(self, mus=None, sigmas=None, dim=1, requires_grad=True):
        super().__init__()
        self.dim = dim
        kernels = [KNRMRbfKernel(m, s, requires_grad=requires_grad) for m, s in zip(mus, sigmas)]
        self.kernels = torch.nn.ModuleList(kernels)

    def count(self):
        return len(self.kernels)

    def forward(self, data):
        return torch.stack([k(data) for k in self.kernels], dim=self.dim)


class KNRMRbfKernel(torch.nn.Module):
    def __init__(self, initial_mu, initial_sigma, requires_grad=True):
        super().__init__()
        self.mu = torch.nn.Parameter(torch.tensor(initial_mu), requires_grad=requires_grad)
        self.sigma = torch.nn.Parameter(torch.tensor(initial_sigma), requires_grad=requires_grad)

    def forward(self, data):
        adj = data - self.mu
        return torch.exp(-0.5 * adj**2 / self.sigma**2)"
102,2201.03356,"'''The T5 ranking baseline.
The baseline implemented here rely on the paper
""Document Ranking with a Pretrained Sequence-to-Sequence Model""
published at EMNLP (2020).
'''
import logging
from lire.data_tools.dataset import MSMarco


from experiments import t5ranker
# Loading dataset
data_folder = '/local/gerald/CPD/data'
split = 'train'
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(message)s')

logging.info('Loading MSMarcoPassageRanking')
# loading the corpus using positive
dataset = MSMarco.MSMarcoPassageRankingDataset(data_folder,
                                               download=False,
                                               split=split,
                                               getter='triplet')

logging.info('Configure the experiment')

nb_queries = dataset.get_nb_annotated_queries()
options = t5ranker.default_options
options.batch_size = 20
options.device = 1
batch_size = options.batch_size

def f_iter():
    dataset.get_nb_queries() * 5

nb_iteration = f_iter
options.number_iteration_by_task = nb_iteration
options.save = '/local/gerald/T5RankerFineTunedMSMarco.pth'

# we train from 10 epochs according to number of training queries
my_experiment = t5ranker.T5TaskRanker(options, dataset)
my_experiment.run()
"
103,2201.03356,"'''The T5 ranking baseline.
The baseline implemented here rely on the paper
""Document Ranking with a Pretrained Sequence-to-Sequence Model""
published at EMNLP (2020).
'''
import torch
import logging
from lire.data_tools.dataset import MSMarco
import tqdm
import torch.utils.data as data_utils
import pytrec_eval

from lire.misc import struct
import time, datetime
from torch.cuda.amp import autocast
from experiments import t5ranker
# Loading dataset
data_folder = '/local/gerald/CPD/data'
split = 'train'
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(message)s')

logging.info('Loading MSMarcoPassageRanking')
# loading the corpus using positive
dataset = MSMarco.MSMarcoPassageRankingDataset(data_folder,
                                               download=False,
                                               split=split,
                                               getter='triplet')
dataset.set_current_task_by_id = lambda x : 1 
def fnull():
    return 1
dataset.get_nb_tasks = fnull

logging.info('Configure the experiment')

nb_queries = dataset.get_nb_annotated_queries()
options = t5ranker.default_options
options.batch_size = 4
options.device = 1
batch_size = options.batch_size
nb_iteration = nb_queries * 9
options.number_iteration_by_task = int(nb_iteration)
options.save = '/local/gerald/T5RankerBaselineMSMarco-25-02-2021.pth'
print(options)
logging.info(""Number of iteration considering the batch size: "" +
             str(nb_iteration))
my_experiment = t5ranker.T5TaskRanker(options, dataset)
# logging.info(""Loading check point"")
# state_dict = torch.load('/local/gerald/T5RankerBaselineMSMarco.pth')
# my_experiment.state_dict = state_dict
# my_experiment.options.batch_size = 4
# logging.info(""Continue experiment"")

# an eval set

dataset_eval = MSMarco.MSMarcoPassageRankingDataset(data_folder,
                                               download=False,
                                               split=""dev"",
                                               getter='top1000', 
                                               subset_size=30)
dataset_eval.set_output_transformation(lambda x : x[0] + x[1])
dataloader_eval =\
    data_utils.DataLoader(dataset_eval,
                            batch_size=32,
                            drop_last=False,
                            shuffle=False,
                            num_workers=4)

gt = dataset_eval.qrels.get_dictionary()
def evaluation_method(xp, dataloader, evaluator):
    prediction_set = {}
    with torch.no_grad():
        token_positive = ""true""
        index_token_positive =\
                xp.tokenizer(token_positive, return_tensors=""pt"").input_ids
        index_token_positive =\
            index_token_positive.repeat(32,
                                        1).to(xp.options.device)

        with autocast(enabled=True):
            for q_id, d_id, input_tokens in  dataloader:
                positive_index =\
                    xp.tokenizer(input_tokens, return_tensors=""pt"", padding=True, max_length=512).input_ids.to(xp.options.device)
                outputs_rank = xp.model(input_ids=positive_index, labels=index_token_positive[:len(q_id)])
                out_rank = outputs_rank[""logits""][:,0,index_token_positive[0,0]]
                for index  in range(len(q_id)):
                    q_id_c, d_id_c , rank = q_id[index].item(), d_id[index].item(), out_rank[index].item()
                    if(q_id_c not in prediction_set):
                        prediction_set[q_id_c] = {}
                    prediction_set[q_id_c][d_id_c] = rank
    pred ={ str(q_id):{str(d_id): v for d_id, v in ds_id.items() } for q_id, ds_id in prediction_set.items()}
    results = {}
    evaluation =  evaluator.evaluate(pred)
    print(evaluation)
    for query, scores in evaluation.items():
        for score_key, score_val in scores.items():
            if(score_key not in results):
                results[score_key] = 0.
            results[score_key] += score_val
    results = {k: v/len(evaluation) for k, v in results.items()}
    print(results)
    xp.writer.add_scalar('dev/mrr',
                         results['recip_rank'],
                         xp.current_state[""iteration""])
    xp.writer.add_scalar('dev/map',
                         results['map'],
                         xp.current_state[""iteration""])
    xp.writer.add_scalar('dev/ndcg',
                         results['ndcg'],
                         xp.current_state[""iteration""])
gt = { str(q_id):{str(d_id): v for d_id, v in ds_id.items() } for q_id, ds_id in gt.items()}
evaluator = pytrec_eval.RelevanceEvaluator(gt, {'map', 'ndcg', 'recip_rank.10'})
my_experiment.add_evaluation(evaluation_method, [my_experiment, dataloader_eval, evaluator])
my_experiment.run()
"
104,2201.03356,"from lire.data_tools import data_reader
Qrels_reader = data_reader.Qrels()
Qrels_reader.set_ressource(""/media/gerald/00B1B02B44A76AB2/CPD/data/MSMarcoPassageRankingDataset/train-qrels.data"")
"
105,2201.03356,"import torch
from torch import nn
from torch import autograd

module = nn.Linear(2, 3)
parameters = list(module.parameters())

x = torch.rand(10, 2)
grad = autograd.functional.jacobian(module, x)
print(grad.sum())
"
106,2201.03356,"from lire.data_tools.dataset import MSMarco

# where are or will be downloaded the corpus
data_folder = '/media/gerald/00B1B02B44A76AB2/CPD/data'
# what split to use
split = 'train'
# laod the dataset with triplet output (query, positive, negative)
dataset = MSMarco.MSMarcoPassageRankingDataset(data_folder, download=True, split=split, storage='full', getter='triplet')"
107,2201.03356,"'''An experiment template for lire

'''
class LIReExperiment(object):
    def __init__(self, options, dataset):
        self.dataset = dataset
        self.options = options
        self.current_state = {}

    @property
    def state_dict(self):
        ''' Get metadata from experiments.
        get metadata of experiments object
        with important informations to continue
        experiments at this step. It also recursivelly call
        the state_dict property of contained object.
        '''
        state_dict = {}
        for k, v in self.__dict__.items():
            if(hasattr(v, ""load_state_dict"") and hasattr(v, 'state_dict')):
                state_dict[k] = v.state_dict()
            elif(hasattr(v, 'state_dict')):
                state_dict[k] = v.state_dict
        state_dict[""options""] = self.options
        state_dict[""experiment_state""] = self.current_state
        return state_dict

    @state_dict.setter
    def state_dict(self, value):
        ''' set metadata from experiments.
        set metadata of experiments object.
        Allow to load an experiment checkpoint
        '''
        for k, v in self.__dict__.items():
            if hasattr(v, ""load_state_dict""):
                v.load_state_dict(value[k])
            elif hasattr(v, 'state_dict'):
                v.state_dict = value[k]

        self.options = value[""options""]
        self.current_state = value[""experiment_state""]

    def save_experiment(self, filepath):
        torch.save(self.state_dict, filepath)
    
    def __getitem__(self, key):
        return self.options.__dict__[key]
    
    def train(self):
        raise NotImplementedError
    
    def ending_task(self):
        raise NotImplementedError

    def begin_task(self):
        raise NotImplementedError

    def run(self):
        raise NotImplementedError"
108,2201.03356,"import torch
from torch import nn


class RbfKernelBank(nn.Module):
    def __init__(self, mus=None, sigmas=None, dim=0, requires_grad=True):
        super().__init__()
        self.mus = nn.Parameter(torch.tensor(mus), requires_grad=requires_grad)
        self.sigmas = nn.Parameter(torch.tensor(sigmas), requires_grad=requires_grad)
        self.dim = dim

    def forward(self, data):
        shape = list(data.shape)
        shape.insert(self.dim, 1)
        data = data.reshape(*shape)
        shape = [1]*len(data.shape)
        shape[self.dim] = -1
        mus, sigmas = self.mus.reshape(*shape), self.sigmas.reshape(*shape)
        adj = data - mus
        return torch.exp(-0.5 * adj * adj / sigmas / sigmas)

    def count(self):
        return self.mus.shape[0]

    @staticmethod
    def from_strs(mus='-0.9,-0.7,-0.5,-0.3,-0.1,0.1,0.3,0.5,0.7,0.9,1.0', \
        sigmas='0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.001', dim=-1, requires_grad=True):
        mus = [float(x) for x in mus.split(',')]
        sigmas = [float(x) for x in sigmas.split(',')]
        return RbfKernelBank(mus, sigmas, dim=dim, requires_grad=requires_grad)

    @staticmethod
    def evenly_spaced(count=11, sigma=0.1, rng=(-1, 1), dim=-1, requires_grad=True):
        mus = [x.item() for x in torch.linspace(rng[0], rng[1], steps=count)]
        sigmas = [sigma for _ in mus]
        return RbfKernelBank(mus, sigmas, dim=dim, requires_grad=requires_grad)
"
109,2201.03356,"import torch
from torch import nn


def binmat(a, b, padding=None):
    BAT, A, B = a.shape[0], a.shape[1], b.shape[1]
    a = a.reshape(BAT, A, 1)
    b = b.reshape(BAT, 1, B)
    result = (a == b)
    if padding is not None:
        result = result & (a != padding) & (b != padding)
    return result.float()


def cos_simmat(a, b, amask=None, bmask=None):
    BAT, A, B = a.shape[0], a.shape[1], b.shape[1]
    a_denom = a.norm(p=2, dim=2).reshape(BAT, A, 1) + 1e-9 # avoid 0div
    b_denom = b.norm(p=2, dim=2).reshape(BAT, 1, B) + 1e-9 # avoid 0div
    result = a.bmm(b.permute(0, 2, 1)) / (a_denom * b_denom)
    if amask is not None:
        result = result * amask.reshape(BAT, A, 1)
    if bmask is not None:
        result = result * bmask.reshape(BAT, 1, B)
    return result


class InteractionMatrix(nn.Module):

    def __init__(self, padding=-1):
        super().__init__()
        self.padding = padding

    def forward(self, a_embed, b_embed, a_tok, b_tok):
        wrap_list = lambda x: x if isinstance(x, list) else [x]

        a_embed = wrap_list(a_embed)
        b_embed = wrap_list(b_embed)

        BAT, A, B = a_embed[0].shape[0], a_embed[0].shape[1], b_embed[0].shape[1]

        simmats = []

        for a_emb, b_emb in zip(a_embed, b_embed):
            if a_emb.dtype is torch.long and len(a_emb.shape) == 2 and \
               b_emb.dtype is torch.long and len(b_emb.shape) == 2:
                # binary matrix
                simmats.append(binmat(a_emb, b_emb, padding=self.padding))
            else:
                # cosine similarity matrix
                a_mask = (a_tok.reshape(BAT, A, 1) != self.padding).float()
                b_mask = (b_tok.reshape(BAT, 1, B) != self.padding).float()
                simmats.append(cos_simmat(a_emb, b_emb, a_mask, b_mask))
        return torch.stack(simmats, dim=1)

    def encode_query_doc(self, encoder, **inputs):
        enc = encoder.enc_query_doc(**inputs)
        return self(enc['query'], enc['doc'], inputs['query_tok'], inputs['doc_tok'])
"
110,2201.03356,"import torch
from torch import nn


class Ranker(nn.Module):
    name = None

    @staticmethod
    def default_config():
        return {
            'qlen': 20,
            'dlen': 2000,
            'add_runscore': False
        }

    def __init__(self, config, random):
        super().__init__()
        self.config = config
        self.random = random
        seed = random.randint((2**32)-1)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        if self.config.get('add_runscore'):
            self.runscore_alpha = torch.nn.Parameter(torch.full((1, ), -1.))

    def input_spec(self):
        # qlen_mode and dlen_mode possible values:
        # 'strict': query/document must be exactly this length
        # 'max': query/document can be at most this length
        result = {
            'fields': set(),
            'qlen': self.config['qlen'],
            'qlen_mode': 'strict',
            'dlen': self.config['dlen'],
            'dlen_mode': 'strict'
        }
        if self.config.get('add_runscore'):
            result['fields'].add('runscore')
        return result

    def forward(self, **inputs):
        result = self._forward(**inputs)
        if len(result.shape) == 2 and result.shape[1] == 1:
            result = result.reshape(result.shape[0])
        if self.config.get('add_runscore'):
            alpha = torch.sigmoid(self.runscore_alpha)
            result = alpha * result + (1 - alpha) * inputs['runscore']
        return result

    def _forward(self, **inputs):
        raise NotImplementedError

    def path_segment(self):
        raise NotImplementedError

    def save(self, path):
        state = self.state_dict(keep_vars=True)
        for key in list(state):
            if state[key].requires_grad:
                state[key] = state[key].data
            else:
                del state[key]
        torch.save(state, path)

    def load(self, path):
        self.load_state_dict(torch.load(path), strict=False)
"
111,2201.03356,"from lire.modules import base
from lire.modules.interaction_matrix import InteractionMatrix
from lire.modules.rbf_kernel_bank import RbfKernelBank"
112,2201.03356,"import sys
import tqdm
import requests 
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('-u', dest='url', type=str,
                    help='URL from download the file')
parser.add_argument('-d', dest='destination', type=str,
                    help='file to download the file')


def _download_large_file(url, filepath, file_size=None):
    r = requests.get(url, stream=True)

    initial_pos = 0

    with open(filepath, 'wb') as f:
        with tqdm.tqdm(total=file_size, unit='B',
                unit_scale=True, unit_divisor=1024,
                desc=filepath, initial=initial_pos,
                miniters=1) as pbar:
            for chunk in r.iter_content(32 * 1024):
                f.write(chunk)
                pbar.update(len(chunk))

def _get_file_size(url):
    requests_instance = requests.head(url)
    file_size = int(requests_instance.headers.get('content-length', 0))
    return file_size

def download_to(url, destination):
    file_size = _get_file_size(url)
    _download_large_file(url, destination, file_size=file_size)

if __name__ == ""__main__"":
    args = parser.parse_args()
    url, destination = args.url, args.destination
    file_size = _get_file_size(url)
    _download_large_file(url, destination, file_size=file_size)
"
113,2201.03356,"import h5py
import tqdm
import sys
import numpy as np
from itertools import islice

class CSVReader():
    @classmethod
    def from_csv_to_dictionary(cls, csv_filepath, split='\t', col_name=None,
                               group_by_key=None, col_type=None, batch_size=int(1e4),
                               group_by_func=""identity"", head=None):
        nb_lines = 0
        csv_dictionary = {}
        store_key = True
        if(group_by_func == ""identity""):
            store_key = False
            group_by_func = lambda x: x

        group_by_key = str(group_by_key)

        with open(csv_filepath, 'r') as csv_file:
            for lines in iter(lambda: tuple(islice(csv_file, batch_size)), ()):
                
                if(nb_lines == 0):
                    # checking coherence of data with given parameters
                    col_count = len(lines[0].split(split))
                    if(col_name is not None and col_count < len(col_name)):
                        raise Exception(""Number of column is lower than indecated"")

                    if(col_name is None):
                        col_name = [str(i) for i in range(col_count)]
                    

                    if(group_by_key is not None):
                        col_key_index = [i for i, name in enumerate(col_name)
                                            if(name == group_by_key)][0]
                        col_key_selector = lambda x : x[col_key_index]

                    if(col_type is None):
                        col_type = int

                    if(not isinstance(col_type, list)):
                        col_type = [col_type for col  in col_name]
                    col_transform = lambda seq_list: [t(x) for x, t in zip(seq_list, col_type)]

                if (head is not None and nb_lines + len(lines) > head):
                    if head - nb_lines == 0:
                        break
                    lines = lines[: head - nb_lines]
                nb_lines += len(lines)

                sys.stdout.flush()
                for line in lines:
                    line_list = line.split(split, len(col_name))
                    data_list = col_transform(line_list)
                    if(group_by_key is not None):

                        key = group_by_func(col_key_selector(data_list))
                        if(not store_key):
                            data_list = data_list[:col_key_index] +\
                                data_list[col_key_index + 1:]
                        if(len(data_list) == 0):
                            raise Exception(""At least to columns is needed"")
                        if(len(data_list) == 1):
                            data_list = data_list[0]
                        if key not in csv_dictionary:
                            csv_dictionary[key] = []
                        csv_dictionary[key].append(data_list)
                    else:
                        csv_dictionary[len(dict)] = data_list
                print('Nb lines processed: ' + str(nb_lines), end='\r')
                if(head is not None and nb_lines >= head):
                    return csv_dictionary
        return csv_dictionary

class HDF5DatasetManager():
    type_str = {int:'i', float:'f'}

    @classmethod
    def batched_csv_to_hdf5(cls, hdf5_file, group_path : str, csv_filepath : str, 
                            split='\t', col_name=None, group_by_key=None,
                            col_type=None, batch_size=int(1e6), group_by_func=lambda x: x,
                            dataset_compression=None, head=None
                            ):
        ''' Write csv to dataset HDF5 using batch.
        The objective of the current function is to provide
        efficient way of writing HDF5 dataset from a csv source
        file. Mainly avoiding storing in memory all the csv content
        which would lead to full all the ram. User should provide 
        a batch size, a number of lines that can fit in memory. More
        the batch is large more the writting will be time efficient.

        Attributes
        ----------
        hdf5_file : str
            filepath of the hdf5 file.
        group_path : str
            the hierarchy of hdf5 group to store dataset(s) '/train/train_data/'.
        csv_filepath : str
            the filepath of the csv file to read and copy to hdf5.
        split : str
            the split token for csv file (for csv use ',' token, tsv '\t' token).
        col_name : list<str>
            the name of the columns, if not given store all the columns, if the 
            number of colums is lower than those contained in the csv file
            use only the columns given (or first n columns).
        group_by_key : str
            the columns that will be used for grouping, thus the name of the 
            dataset will be the key, if not given store all in 'data' named dataset.
        col_type : type or list<type>
            the type of the columns by default int (only float and int is currently 
            implemented).
        batch_size : int
            the size of the batch lines stored in memory (default=1e6).
        group_by_func : func
            a function applied on the key given, it transform the key into
            an other key, can merge keys.
        dataset_compression : str
            which type of compression used to store dataset. By default no compression
            is used. Can be ""gzip"", ""lzf"", ""szip"", for futher details refer to 
            https://docs.h5py.org/en/stable/high/dataset.html.
        '''
        data_group = h5py.File(hdf5_file, 'a')[group_path]
        nb_lines = 0
        key_count = {}
        print(""Preprocessing and counting nb lines, can be long"")
        with open(csv_filepath, 'r') as csv_file:
            for lines in iter(lambda: tuple(islice(csv_file, batch_size)), ()):
                if(nb_lines == 0):
                    # checking coherence of data with given parameters
                    col_count = len(lines[0].split(split))
                    if(col_name is not None and col_count < len(col_name)):
                        raise Exception(""Number of column is lower than indecated"")
                    
                    if(col_name is not None):
                        if(group_by_key is not None):
                            col_key_index = [i for i, name in enumerate(col_name)
                                             if(name == group_by_key)][0]
                            col_key_selector = lambda x : x[col_key_index]

                    if(col_name is None):
                        col_name = [str(i) for i in range(col_count)]

                    if(col_type is None):
                        col_type = int

                    if(not isinstance(col_type, list)):
                        col_type = [col_type for col  in col_name]
                    col_transform = lambda seq_list: [t(x) for x, t in zip(seq_list, col_type)]
                    dataset_type = cls.type_str[col_type[0]]
                if (head is not None and nb_lines + len(lines) > head):
                    if head - nb_lines == 0:
                        break
                    lines = lines[: head - nb_lines]
                nb_lines += len(lines)
                sys.stdout.flush()

                sys.stdout.flush()
                if(group_by_key is not None):
                    for line in lines:
                        line_list = line.split(split, len(col_name))
                        key = group_by_func(col_key_selector(line_list))
                        if key not in key_count:
                            key_count[key] = 0
                        key_count[key] += 1
                print('Nb lines processed: ' + str(nb_lines), end='\r')
                if(head is not None and nb_lines >= head):
                    break
        print(""\n"")
        if(group_by_key is None):
            key_count[""data""] = nb_lines
        for i, (k, v) in zip(tqdm.trange(len(key_count)), key_count.items()):
            data_group.create_dataset(str(k), (v, len(col_name)) , dataset_type, compression=dataset_compression)
        count_lines = 0
        batch_dict = {}
        key_incremental_count = {}
        nb_iteration = nb_lines // batch_size  + (0 if(nb_lines % batch_size == 0) else 1)
        with open(csv_filepath, 'r') as csv_file:
            for i, lines in zip(tqdm.trange(nb_iteration), iter(lambda: tuple(islice(csv_file, batch_size)), ())):
                if(head is not None and count_lines + len(lines) > head):
                    lines = lines[: head - count_lines]
            
                count_lines += len(lines)
                for line in lines:
                    line_list = line.split(split, len(col_name))
                    key = group_by_func(col_key_selector(line_list))

                    if(group_by_key is None):
                        key = 'data'

                    if(key not in batch_dict):
                        batch_dict[key] = []

                    batch_dict[key].append(col_transform(line_list))

                for k, v in batch_dict.items():
                    if(k not in key_incremental_count):
                        key_incremental_count[k] = 0
                    data_group[str(k)][key_incremental_count[k]: key_incremental_count[k] + len(v)] = np.array(v)
                    key_incremental_count[k] += len(v)
                batch_dict = {}
"
114,2201.03356,"import mimetypes
import zipfile as FZ
import bz2
import gzip
from os import path
import shutil

def untar(input_path, output_path):
    """"""
    function allowing to untar files
    """"""
    print(""Reading archive at "" + str(input_path))
    ext = mimetypes.guess_type(input_path)
    ext = str(ext[0]) + str(ext[1])

    # if mime type is None
    if ext is None:
        ext = mimetypes.read_mime_types(input_path)
    # if mime type is still None - read the extension
    if ext is None:
        raise UnknowMimeType(path.split(input_path)[1])

    if 'bzip2' in ext:
        with open(input_path, 'rb') as zipfile:

            data = bz2.decompress(zipfile.read())
            print(output_path + (path.split(input_path)[1].split('.')[0]))
            fzip = open(path.join(output_path,
                        path.split(input_path)[1].split('.')[0] + '.txt'), 'wb')
            fzip.write(data)
    if 'gz' in ext:
        with gzip.open(input_path, 'rb') as f_in:
            with open(output_path, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)

    elif 'zip' in ext:
        zip_ref = FZ.ZipFile(input_path, 'r')
        zip_ref.extractall(output_path)
        zip_ref.close()

    else:
        raise NotImplementedError()
"
115,2201.03356,"from torch.utils.data import Dataset
from torch.distributions.distribution import Distribution

class ContinualDataset(Dataset):
    def __init__(self):
        super(ContinualDataset, self).__init__()
        self._task_id = 0

    def set_current_task_by_id(self, task_id):
        self._task_id = task_id
    
    def get_current_task_id(self):
        return self._task_id

    @property
    def state_dict(self):
        ''' Get metadata from experiments.
        get metadata of experiments object
        with important informations to continue
        experiments at this step. 
        '''
        return {""task_id"": self.get_current_task_id()}

    @state_dict.setter
    def state_dict(self, value):
        ''' set metadata from experiments.
        set metadata of experiments object.
        Allow to load an experiment checkpoint
        '''
        self.set_current_task_by_id(value[""task_id""])

    def get_nb_tasks(self):
        raise NotImplementedError

class ContinualDatasetSplitter(ContinualDataset):
    def __init__(self, dataset,
                 fold={'train': 0.8, 'val':0.1, 'test':0.1},
                 n_tasks=5, size=[0.1,0.3,0.4,0.1,0.1], seed=42):
        super(ContinualDatasetSplitter, self).__init__()
        self._n_tasks = n_tasks 
        self._fold = fold
        self._size = size
        self._dataset = dataset
        self._seed = seed
        self._tasks = None
        self._current_fold = list(fold.keys())[0]
        assert(issubclass(size.__class__, Distribution) or n_tasks == len(size))
        assert(sum(fold.values(), 0) == 1)
        self._prepare()

    def _get_task_size(self, task_index):
        if hasattr(self.size, ""__getitem__""):
            return self.size[task_index]


    def _prepare(self):
        dataset_size = len(self._dataset)
        torch.manual_seed(self._seed)
        random_permutation = torch.randperm(dataset_size)
        tasks = list() 
        head_index = 0
        for i in range(self._n_tasks):
            task_size = self._get_task_size(i)
            task_indexes = random_permutation[head_index: head_index + task_size]

            task_fold = {}
            head_fold = 0
            for k,v in self.fold.items():
                task_fold[k] = task_indexes[head_fold:int(head_fold + v*task_size)]
            
            tasks.append(task_fold)
        self._tasks = task


    # def set_current_fold(self, fold):
    #     assert(fold is in self.fold)
    
    # def get_current_fold(self):
    #     self._getter = lambda x: self.
    #     return self._current_fold

    def __len__(self):
        return len(self._tasks[self._current_fold][self.get_current_task_id()])
    
    def __getitem__(self, index):
        return self._dataset[self._tasks[self._current_fold][self.get_current_task_id()][index]]

    def get_nb_tasks(self):
        return len(self._tasks)
    
    def save_tasks(self, folder_path):
        pass

class ContinualDatasetTaskCat(ContinualDataset):
    def __init__(self, continual_dataset_list,
                 intra_order_rule=""random"",
                 extra_order_rule=""random""):
        super(ContinualDatasetTaskCat, self).__init__()
        self._datasets = continual_dataset_list
        self._intra_order = intra_order_rule
        self._extra_order = extra_order_rule
        self._tasks = []
    
    def _prepare(self):
        self._meta_task_indexes = {i * len(dataset) + j: (i, j) 
                                    for i, d in  enumerate(self._datasets) 
                                    for j in range(d.get_nb_tasks())}
        if(self._intra_order == ""random"" and self._extra_order == ""random""):
            self.task_indexes = torch.randperm(len(self._meta_task_indexes))

    def get_nb_tasks(self):
        return len(self._tasks)

    def __len__(self):
        task_id = self.get_current_task_id()
        dataset_index, sub_task_index = self._meta_task_indexes[task_id]
        self.current_dataset = self._datasets[dataset_index]
        self.current_dataset.set_current_task_by_id(sub_task_index)

        return len(self.current_dataset)
    
    def __getitem__(self, index):
        task_id = self.get_current_task_id()
        dataset_index, sub_task_index = self._meta_task_indexes[task_id]
        self.current_dataset = self._datasets[dataset_index]
        self.current_dataset.set_current_task_by_id(sub_task_index)

        return self.current_dataset[index]
    "
116,2201.03356,"import random
import copy

class Qrels():
    def __init__(self, irrelevant=True,
                 use_inverse_dictionary=True,
                 getter_expression=""qi -> q, dr, dn"",
                 filepath=None):
        
        self.irrelevant = irrelevant

        self.use_inverse_dictionary = use_inverse_dictionary

        self.query_document_relevant = {}
        self.query_document_irrelevant = {}

        self.document_query_relevant = {}
        self.document_query_irrelevant = {}
    
        self._query_index = []
        self._document_index = []

        self.get_mod = getter_expression
        self.set_getter(self.get_mod)
        if(filepath is not None):
            self.set_ressource(filepath)
    @property
    def use_inverse_dictionary(self) -> bool:
        return  self._use_inverse_dictionary
    
    @use_inverse_dictionary.setter
    def use_inverse_dictionary(self, value: bool) -> None:
        if(""_use_inverse_dictionary"" not in self.__dict__):
            self._use_inverse_dictionary = value
        if value == self._use_inverse_dictionary:
            return
        else:
            if(value):
                self.document_query_relevant, self.document_query_irrelevant = \
                    self._process_document_query_dictionary()
            else:
                self.document_query_relevant = None
                self.document_query_irrelevant = None

        self._use_inverse_dictionary = value
    
    def get_dictionary(self):
       return { k: {d: 1 for d in v} for k, v in self.query_document_relevant.items()}

    def _process_document_query_dictionary(self):

        relevant =\
            Qrels._process_inverse_dictionary(self.query_document_relevant)
        irrelevant = {}
        if(self.irrelevant):
            irrelevant =\
                Qrels._process_inverse_dictionary(self.query_document_irrelevant)

        return relevant, irrelevant

    @staticmethod
    def _process_inverse_dictionary(source: dict) -> dict:
        target = {}
        for k, vs in source.items():
            for v in vs:
                if v not in target:
                    target[v] = set()
                target[v].add(k)
        return target

    @staticmethod
    def _merge_(source: dict, new_content:dict) -> None:
        for k, v in new_content.items():
            if(k not in source):
                source[k] = set()
            source[k].update(v)

    @staticmethod
    def _merge(source: dict, new_content:dict) -> dict:
        new_source = copy.deepcopy(source)
        Qrels._merge_(new_source, new_content)
        return new_source

    @staticmethod
    def _load_qrels_file(filepath, store_irrelevant=True):
        relevant, irrelevant = {}, {}

        with open(filepath) as qrels_file:
            while True:
                line = qrels_file.readline()
                if not line:
                    break
                qrel = line.split()
                query_id = qrel[0]
                document_id = qrel[2]
                relevance = int(qrel[3])

                if relevance == 1:
                    if(query_id not in relevant):
                        relevant[query_id] = set()
                    relevant[query_id].add(document_id)
                
                elif(relevance == 0 and store_irrelevant):
                    if(query_id not in irrelevant):
                        irrelevant[query_id] = set()
                    irrelevant[query_id].add(document_id)

        return relevant, irrelevant
    
    def _update_query_index(self, queries):
        for k in queries:
            self._query_index.append(k)


    def add_ressource(self, filepath_or_qrels) -> None:
        if(isinstance(filepath_or_qrels, str)):
            relevant, irrelevant =\
                Qrels._load_qrels_file(filepath_or_qrels, self.irrelevant)
        else:
            relevant = filepath_or_qrels.query_document_relevant
            irrelevant = filepath_or_qrels.query_document_irrelevant

        Qrels._merge_(self.query_document_relevant, relevant)
        Qrels._merge_(self.query_document_irrelevant, irrelevant)
        self._update_query_index(relevant)
        self._update_query_index(irrelevant)

        if(self.use_inverse_dictionary):
            relevant_inversed =\
                Qrels._process_inverse_dictionary(relevant)
            Qrels._merge_(self.document_query_relevant, relevant_inversed)
            if(self.irrelevant):
                irrelevant_inversed =\
                    Qrels._process_inverse_dictionary(irrelevant)
                Qrels._merge_(self.document_query_irrelevant, irrelevant_inversed)

    @staticmethod
    def merge_qrels(*args):
        qrels = Qrels()
        for arg in args:
            qrels.add_ressource(arg)
        return qrels

    def set_ressource(self, filepath: str) -> None:
        self.query_document_relevant = {}
        self.query_document_irrelevant = {}

        self.document_query_relevant = {}
        self.document_query_irrelevant = {}

        self.add_ressource(filepath)

    def get_qdr(self, index):
        if(index in self.query_document_relevant):
            return self.query_document_relevant[index]
        else:
            return set()

    def get_qdi(self, index):
        if(index in self.query_document_irrelevant):
            return self.query_document_irrelevant[index]
        else:
            return set()

    def set_getter(self, expression) -> None:
        self.input_getter, self.output_getters =\
            self._create_getter(expression)

    def __contains__(self, query_id):
        return (query_id in self.query_document_relevant or 
                query_id in self.query_document_irrelevant)

    def _create_getter(self, expression):
        expression = expression.replace(' ', '')
        splitted = expression.split('->')
        if len(splitted) > 2 :
            raise Exception(""Only one input is allowed"")
        input_token = splitted[0]
        output_tokens = splitted[1].split(',')
        if ""q"" in input_token:
            # input
            input_getter = lambda x: x
            
            if ""i"" in input_token:
                input_getter = lambda x: self._query_index[x]
            
            output_getters = []
            # output
            for output_token in output_tokens:
                output_getter = lambda x: None
                if(""d"" in output_token):
                    output_getter =\
                        lambda x: self.get_qdr(x).union(self.get_qdi(x))
                    if(""r"" in output_token):
                        output_getter = self.get_qdr
                    if(""n"" in output_token):
                        output_getter = self.get_qdi
                if ""q"" in output_token:
                    # input
                    output_getter = lambda x: x
                    
                    if ""i"" in output_token:
                        output_getter = lambda x: x
                output_getters.append(output_getter)
            
            return input_getter, output_getters

    def __getitem__(self, index):
       return tuple([ output_getter(self.input_getter(index)) for output_getter in self.output_getters])

    def __len__(self):
        return len(self._query_index)


# class TSVQueryRankingFile():
#     def __init__(self, filepath, structure=[""query_id"", ""document_id"", ""score""]):
#         for 
"
117,2201.03356,
118,2201.03356,"from os.path import join, basename, exists
from os import makedirs, rename
import os
from pathlib import Path
import pandas as pd
# import dask.dataframe as dd
import inspect

from lire.log_tools import logger

# from lire.data_tools import data_loader
from lire.data_tools import data_downloader
from lire.data_tools import data_compress
from lire.data_tools import data_reader

class MicroblogRankingDataset(object):

    configuration_path = ""dataset_info/MicroblogRankingDataset.json""

    def __init__(self, storage=""full"", chunk_size=""100MB""):
        self.storage = storage
        self.chunk_size = chunk_size
        
        self.folder = folder
        self.download = download
        self.split = split
        self.getter = getter
        self.configuration_path = join(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))),self.configuration_path)

        self.configuration = logger.ConfigurationFile(self.configuration_path)
        self.subset_size = subset_size
        try: 
            self._load_from_file()
        except Exception:

            self._download_files(self.folder, self.split, self.configuration_path)
            if(not self.download):
                raise Exception(""No dataset at "", self.folder, "", use download=True, to download the corpus"")
        self.document_transformation = lambda x: x 
        self.query_transformation = lambda x: x
        self.output_transformation = lambda x: x

    @classmethod
    def _download_files(cls, root_folder, key_data, configuration_path):
        config = logger.ConfigurationFile(configuration_path).content
        print(config)
        foldername = config[""folder""][""foldername""]


        folder_path = join(root_folder, foldername)
        makedirs(folder_path, exist_ok=True)

        download_url +=  [(key_val, url, basename(url)) for key_val, url in config[""download""][""common""].items()]

        for download_item in download_url:
            key_val, url, filename = download_item
            if(not exists(join(folder_path, key_val+"".data""))):
                data_downloader.download_to(url, join(folder_path, filename))
                try:
                    data_compress.untar(join(folder_path, filename), join(folder_path, key_val+"".data"")) 
                except Exception:
                    # TODO : check not a compressed file
                    rename(join(folder_path, filename), join(folder_path, key_val+"".data""))
            else : 
                print('Already downloaded ""', filename, '"" at ""',
                    url, '"" to ""', join(folder_path, filename),'""')
            
        return {key_val:join(folder_path, filename)
                for key_val, url, filename in download_url}


    def _load_from_file(self):
        root_path = join(self.folder, self.configuration['folder']['foldername'])
        raise Exception(""Ex"")
        # loading content
"
119,2201.03356,"from os.path import join, basename, exists
from os import makedirs, rename
import os
from pathlib import Path
import pandas as pd
import logging
import sys
import h5py
# import dask.dataframe as dd
import inspect

from lire.log_tools import logger

import json
# from lire.data_tools import data_loader
from lire.data_tools import data_downloader
from lire.data_tools import data_compress
from lire.data_tools import data_reader
from lire.data_tools.continual_routine import ContinualDataset
class MSMarcoDataset(object):
    def __init__(self, storage=""full"", chunk_size=""100MB""):
        self.storage = storage
        self.chunk_size = chunk_size

    @staticmethod
    def _load_pandas_dataframe(filepath, sep='\t', nrows=None):
        dataframe = pd.read_csv(filepath, sep=sep, header=None, index_col=0, nrows=nrows)
        return dataframe

    @staticmethod
    def _load_dask_dataframe(filepath, sep='\t', nrows=None):
        ''' Loading a dataframe using dask api for chunk loading.

            This is much memory efficient than pandas, however it is 
            longer to access data. We advise using fast storage to works
            efficiently with this type of loading.

        '''
        dataframe = dd.read_csv(filepath, sep=sep, header=None,
                                          index_col=0, encoding='utf8',
                                          blocksize=chunk_size)
        
        return dataframe

    def _load_dataframe(self, filepath, sep='\t', nrows=None):
        if(self.storage == 'chunk'):
            path = Path(filepath)
            filename = path.stem
            directory = str(path.parent)
            chunk_directory = join(directory, filename+""_parquet"")

            if(not exists(chunk_directory)):
                df = dd.read_csv(filepath, sep=sep, header=None, 
                                           blocksize=self.chunk_size).set_index(0)
                df.repartition(partition_size = self.chunk_size)
                df.to_parquet(chunk_directory)
            
        else:
            return self._load_pandas_dataframe(filepath, sep=sep, nrows=nrows)
    
    @classmethod
    def _download_files(cls, root_folder, key_data, configuration_path):
        config = logger.ConfigurationFile(configuration_path).content
        print(config)
        foldername = config[""folder""][""foldername""]


        folder_path = join(root_folder, foldername)
        makedirs(folder_path, exist_ok=True)

        download_url = [(key_val, url, basename(url)) for key_val, url in config[""download""][key_data].items()]
        download_url +=  [(key_val, url, basename(url)) for key_val, url in config[""download""][""common""].items()]

        for download_item in download_url:
            key_val, url, filename = download_item
            if(not exists(join(folder_path, key_val+"".data""))):

                print('Download ""', filename, '"" at ""',
                    url, '"" to ""', join(folder_path, filename),'""')
                data_downloader.download_to(url, join(folder_path, filename))
                try:
                    data_compress.untar(join(folder_path, filename), join(folder_path, key_val+"".data"")) 
                except Exception:
                    # TODO : check not a compressed file
                    rename(join(folder_path, filename), join(folder_path, key_val+"".data""))
            else : 
                print('Already downloaded ""', filename, '"" at ""',
                    url, '"" to ""', join(folder_path, filename),'""')
        return {key_val:join(folder_path, filename)
                for key_val, url, filename in download_url}

class MSMarcoRankingDataset(MSMarcoDataset):

    configuration_path = ""dataset_info/MSMarcoRankingDataset.json""


    def __init__(self, folder, download=False, split=""dev"", force=False, subset=None):
        super(MSMarcoRankingDataset).__init__()
        self.folder = folder
        self.download = download
        self.split = split

        self.configuration_path = join(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))),self.configuration_path)

        self.configuration = logger.ConfigurationFile(self.configuration_path)
        self.subset = subset
        try: 
            # loading from local
            raise Exception()
        except Exception:
            self._download_files(self.folder, self.split)
            if(not self.download):
                raise Exception(""No dataset at "", self.folder, "", use download=True, to download the corpus"")
    
    @staticmethod
    def _load_collection(filepath, sep='\t'):
        dataframe = pd.read_csv(filepath, sep=sep)
        return dataframe



    def _load_from_file(self):
        self.documents_collection = cls._load_collection(os.path.join(self.folder, cls.documents_collection))
        self.queries_collection = cls._load_collection(os.path.join(self.folder, cls.queries_collection))
        self.queries_documents_top100 = cls._load_collection(os.path.join(self.folder, cls.queries_documents), sep=""\s"")


    def get_documents_related_to_a_query(self, index):
        query_str, query_index = self.queries_collection.get[i]
        documents_id = self.queries_documents_top100.get(query_index)
        documents = [(self.documents_collection.get(id)[1]) for id in documents_id]
        return query_str, documents

class MSMarcoPassageRankingDataset(MSMarcoDataset):

    configuration_path = ""dataset_info/MSMarcoPassageRankingDataset.json""


    def __init__(self, folder, download=False, split=""train"",
                 force=False, storage=""full"", getter=""positive"",
                 subset_size=None, load_data=True):
        super(MSMarcoPassageRankingDataset, self).__init__(storage=storage)
        self.folder = folder
        self.download = download
        self.split = split
        self.getter = getter
        self.configuration_path =\
            join(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))),self.configuration_path)
        self.configuration = logger.ConfigurationFile(self.configuration_path)
        self.subset_size = subset_size

        self.document_transformation = lambda x: x
        self.query_transformation = lambda x: x
        self.output_transformation = lambda x: x
        self.documents = None
        self.queries = None
        self.qrels = None

        if(load_data):
            self._load_data()


    def _load_data(self):
        try:
            self._load_from_file()
        except Exception:
            if(not self.download):
                print(sys.exc_info()[0])
                raise Exception(""No dataset at "", self.folder,
                                "", use download=True, to download the corpus"")
            self._download_files(self.folder, self.split,
                                 self.configuration_path)
            self._load_from_file()

    def set_common_data(self, documents=None, queries=None, qrels=None):
        '''Set common data reference.
           The objective of this method is to avoid
           duplicate memory on common data
        '''
        if(documents is not None):
            self.documents = documents
        if(queries is not None):
            self.queries = queries
        if(qrels is not None):
            self.qrels = qrels

    @classmethod
    def load_queries(cls, split, folder):
        configuration_path = join(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))),
                                  cls.configuration_path)
        configuration = logger.ConfigurationFile(configuration_path)
        root_path = join(folder, configuration['folder']['foldername'])
        queries =\
            MSMarcoDataset._load_pandas_dataframe(join(root_path, split+""-queries.data""))
        return queries
    
    @classmethod
    def load_qrels(cls, split, folder):
        configuration_path = join(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))),
                                  cls.configuration_path)
        configuration = logger.ConfigurationFile(configuration_path)
        root_path = join(folder, configuration['folder']['foldername'])
        qrels =\
            data_reader.Qrels(filepath=join(root_path, split+""-qrels.data""),
                              getter_expression=""qi -> q, dr"")
        return qrels

    def get_nb_queries(self):
        return len(self.queries)

    def get_nb_annotated_queries(self):
        return len(self.qrels)

    def _load_from_file(self):
        root_path = join(self.folder, self.configuration['folder']['foldername'])
        # loading content
        if(self.documents is None):
            self.documents =\
                self._load_dataframe(join(root_path, ""documents.data""))
        if(self.queries is None):
            self.queries =\
                self._load_dataframe(join(root_path, self.split+""-queries.data""))
        # loading relations
        if(self.qrels is None):
            self.qrels =\
                data_reader.Qrels(filepath=join(root_path, self.split+""-qrels.data""), getter_expression=""qi -> q, dr"")
        if(self.split == ""train"" and self.getter == ""triplet""):
            self.query_positive_negative =\
                self._load_dataframe(join(root_path, ""train_positive_negative.data""), 
                                     nrows=self.subset_size)

        if(self.split == ""dev"" and self.getter == ""top1000""):
            with open(join(root_path, ""dev_top_1000.json"")) as f:
                self.top_1000_json = json.load(f)
                if(self.subset_size is None):
                    self.keys_top_1000 = list(self.top_1000_json.keys())
                else:
                    self.keys_top_1000 = list(self.top_1000_json.keys())[:self.subset_size]
                self.top_1000 = [[int(k), v] for k in self.keys_top_1000 
                                 for v in self.top_1000_json[k]]

    def get_query_positive_negative(self, index):

        index_row = self.query_positive_negative.iloc[index] 
        query_id, positive_document_id, negative_document_id =\
            index_row.name, index_row[1], index_row[2]
        query_str = self.queries.loc[query_id][1]
        positive_str = self.documents.loc[positive_document_id][1]
        negative_str = self.documents.loc[negative_document_id][1]
        
        return (query_str, positive_str, negative_str)

    def set_document_transform(self, transformation):
        self.document_transformation = transformation

    def set_query_transform(self, transformation):
        self.query_transformation = transformation

    def set_output_transformation(self, transformation):
        self.output_transformation = transformation


    def __getitem__(self, index):
        if(self.getter == ""positive""):
            query_id, documents_id = self.qrels[index]
            query_tr = self.query_transformation(self.queries.loc[int(query_id)][1])
            documents_tr = [self.document_transformation(self.documents.loc[int(document_id)][1]) for document_id in documents_id]
            return self.output_transformation((query_tr, documents_tr))

        if(self.getter == ""triplet""):
            q, p, n = self.get_query_positive_negative(index)
            return self.output_transformation((self.query_transformation(q),
                                               self.document_transformation(p),
                                               self.document_transformation(n)))
        if(self.getter == ""top1000""):
            query_doc = self.top_1000[index]
            query_id = query_doc[0]
            document_id = query_doc[1]
            return query_id, document_id,\
                   self.output_transformation((self.query_transformation(self.queries.loc[query_id][1]),
                                               self.document_transformation(self.documents.loc[
                                                   document_id][1])))

    def __len__(self):
        if(self.getter == ""positive""):
            return len(self.qrels)
        elif(self.getter == ""top1000""):
            return len(self.top_1000)
        else:
            return len(self.query_positive_negative)


class MSMarcoPassageRankingTopicQueryDataset(MSMarcoPassageRankingDataset, ContinualDataset):
    configuration_path = ""dataset_info/MSMarcoPassageRankingTopicQueryDataset.json""

    def __init__(self, *args, **kwargs):
        MSMarcoPassageRankingDataset.__init__(self, *args, **kwargs)
        ContinualDataset.__init__(self)

    def _load_from_file(self):
        root_path = join(self.folder, self.configuration['folder']['foldername'])
        # loading content
        self.documents =\
            self._load_dataframe(join(root_path, ""documents.data""))
        self.queries =\
             self._load_dataframe(join(root_path, self.split+""-queries.data""))
        # loading relations
        self.qrels =\
            data_reader.Qrels(filepath=join(root_path, self.split+""-qrels.data""), getter_expression=""qi -> q, dr"")
        if(self.split == ""train"" and self.getter == ""triplet""):
            self.h5_split = h5py.File(os.path.join(root_path, ""hdf5-data.data""), ""r"")


        with open(join(root_path, self.split+""-topics.data"")) as f:
            self.topics = json.load(f)
        self.qrels.set_getter(""q -> q, dr"")
        print(""Creating the index"")
        self.set_current_task_by_id(0)


    def set_current_task_by_id(self, task_id):
        super(MSMarcoPassageRankingTopicQueryDataset, self).set_current_task_by_id(task_id)
        if(self.getter == ""triplet"" and self.split == ""train""):
            self.current_task_index =\
                self.h5_split[""/train/qid-pid-nid-cluster""][str(task_id)]

        if(self.getter == ""positive""):
            self.current_task_index =\
                self.topics[task_id]

        # if(self.getter == ""top1000""):
        #     top_1000 = self.h5_split[""/""+self.split+""/""+self.split+""-top-1000.data""]
        #     data = []
        #     cumsize = []
        #     for k, v in self.topics[task_id].items():
        #         try:
        #             rel_doc = top_1000[str(k)
        #             data.append(k, rel_doc ])
        #             cumsize.append[len(rel_doc)]
                    
        #         except Exception:
        #             print(str(k) + "" not in set"")
            
                

                    
            

    def get_query_positive_negative(self, index):
        query_id, positive_document_id, negative_document_id =\
            self.current_task_index[index]
        query_str = self.queries.loc[query_id][1]
        positive_str = self.documents.loc[positive_document_id][1]
        negative_str = self.documents.loc[negative_document_id][1]
        
        return (query_str, positive_str, negative_str)

    def __getitem__(self, index):
        if(self.getter == ""positive""):
            query_id, documents_id = self.qrels[str(self.current_task_index[index])]
            query_tr = self.query_transformation(self.queries.loc[int(query_id)][1])
            documents_tr = [self.document_transformation(self.documents.loc[int(document_id)][1]) for document_id in documents_id]
            return self.output_transformation((query_tr, documents_tr))

        if(self.getter == ""triplet""):
            q, p, n = self.get_query_positive_negative(index)
            return self.output_transformation((self.query_transformation(q),
                    self.document_transformation(p), 
                    self.document_transformation(n)))
    
    def get_nb_queries(self):
        return len(self.topics[self.get_current_task_id()])

    def get_nb_tasks(self):
        return len(self.topics)

    def __len__(self):
        if(self.getter == ""positive""):
            return len(self.current_task_index)
        else:
            return len(self.current_task_index)


class MSMarcoPassageRankingTopicQueryCleanedDataset(MSMarcoPassageRankingTopicQueryDataset):
    configuration_path = ""dataset_info/MSMarcoPassageRankingTopicQueryCleanedDataset.json""
    def __init__(self, *args, **kwargs):
        super(MSMarcoPassageRankingTopicQueryCleanedDataset, self).__init__(*args, **kwargs)

class MSMarcoPassageRankingTopicSmall(MSMarcoPassageRankingTopicQueryDataset):
    configuration_path = ""dataset_info/MSMarcoPassageRankingTopicSmall.json""
    def __init__(self, *args, **kwargs):
        super(MSMarcoPassageRankingTopicSmall, self).__init__(*args, **kwargs)"
120,2201.03356,"from lire.misc.class_tools import struct
"
121,2201.03356,"class struct(dict):
    def __getattr__(self, name):
        if name in self:
            return self[name]
        else:
            raise AttributeError(""No such attribute: "" + name)

    def __setattr__(self, name, value):
        self[name] = value

    def __delattr__(self, name):
        if name in self:
            del self[name]
        else:
            raise AttributeError(""No such attribute: "" + name)"
122,2201.03356,"from torch.nn import Module
from torch.distributions import Distribution
from collections.abc import Iterable


def estimate_fisher_diag(input_iterator: Iterable,
                         parametric_function: Module,
                         expectation_estimation_sample: int = 1) -> Iterable:

    parameters = list(parametric_function.parameters())
    fisher_diag = []
    for w in parameters:
        # Initialise and ensure their is no gradient on parameters
        fisher_diag.append(w.new_zeros(w.size()))
        w.grad.data.zero_()

    for _ in range(expectation_estimation_sample):
        for data in input_iterator:
            if isinstance(data, Distribution):
                parametric_function(data).backward()
            else:
                parametric_function(data).backward()

            for f, w in zip(fisher_diag, parameters):
                f.add_(w.grad.data.pow(2))
            parametric_function.zero_grad()
    for f in fisher_diag:
        f.div_(expectation_estimation_sample * len(input_iterator))

    return fisher_diag
"
123,2201.03356,"import io
import os
import json

class ConfigurationFile(object):
    def __init__(self, filepath, mod=""fill""):
        self.filepath = filepath
        self.mod = mod
        self.content = {}
        # create the file if does not exist
        if(not os.path.exists(self.filepath)):
            print(""Creating the file at "", self.filepath)

            os.makedirs(os.path.dirname(self.filepath), exist_ok=True)
            self.clear()

        # update the object in reading the file
        self.update()

    # clear all the config file 
    def clear(self):
        if os.path.exists(self.filepath):
            os.remove(self.filepath)
        # TODO make property
        self.content = {}
        ConfigurationFile._writing_json(self.filepath, {}, mod=""w+"")
        self.last_correct_save = {}

    # return a list containg all the config file existing keys 
    def keys(self):
        return [k for k in self.content]

    # update the config file
    def update(self):
        file_content = ConfigurationFile._reading_json(self.filepath)
        self.last_correct_save = file_content
        self.content = dict(file_content, **self.content)
        try:
            ConfigurationFile._writing_json(self.filepath, self.content)
        except Exception:
            ConfigurationFile._writing_json(self.filepath,  self.last_correct_save)
            print(""Error writing JSON"")
            raise Exception
           
    
    def set_mod(self, mod=""""):
        self.mod = mod

    def __getitem__(self, index ):

        if(index not in self.content and self.mod==""fill""):
            value = input(""A value is necessary for variable ""+str(index)+""\n value : "")
            self.content[index] = value
            self.update()
        return  self.content[index] if(index in self.content) else None

    def __setitem__(self, index, value):
        self.content[index] = value
        self.update()

    def __str__(self):
        _cf_str = """"
        max_len = max([0]+[len(k) for k in self.content])
        for k, v in self.content.items():
            _cf_str += (k+("" ""*(max_len - len(k)))+"" : ""+str(v)) + '\n'
        return _cf_str

    def __repr__(self):
        return self.__str__()

    def __contains__(self, b):
        return b in self.content

    @staticmethod
    def _reading_json(filepath):
        with io.open(filepath, 'r') as file_sconf:
            return json.load(file_sconf)

    @staticmethod
    def _writing_json(filepath, dictionary, mod='w'):
        with io.open(filepath, mod) as file_sconf:
            file_sconf.write(json.dumps(dictionary, sort_keys=True, indent=4))
        
"
124,2201.03356,"import torch
from torch import nn
import lire.modules.base as rankers
from lire import modules


class Knrm(rankers.Ranker):
    """"""
    Implementation of the K-NRM model from:
      > Chenyan Xiong, Zhuyun Dai, James P. Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End
      > Neural Ad-hoc Ranking with Kernel Pooling. In SIGIR.
    """"""

    @staticmethod
    def default_config():
        result = rankers.Ranker.default_config()
        result.update({
            'mus': '-0.9,-0.7,-0.5,-0.3,-0.1,0.1,0.3,0.5,0.7,0.9,1.0',
            'sigmas': '0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.001',
            'grad_kernels': True
        })
        return result

    def __init__(self, vocab, config, logger, random):
        super().__init__(config, random)
        self.logger = logger
        self.encoder = vocab.encoder()
        self.simmat = modules.InteractionMatrix()
        self.kernels = modules.RbfKernelBank.from_strs(config['mus'], config['sigmas'], dim=1, requires_grad=config['grad_kernels'])
        self.combine = nn.Linear(self.kernels.count() * self.encoder.emb_views(), 1)

    def input_spec(self):
        result = super().input_spec()
        result['fields'].update({'query_tok', 'doc_tok', 'query_len', 'doc_len'})
        # combination does not enforce strict lengths for doc or query
        result['qlen_mode'] = 'max'
        result['dlen_mode'] = 'max'
        return result

    def _forward(self, **inputs):
        simmat = self.simmat.encode_query_doc(self.encoder, **inputs)
        kernel_scores = self.kernel_pool(simmat)
        result = self.combine(kernel_scores) # linear combination over kernels
        return result

    def kernel_pool(self, simmat):
        kernels = self.kernels(simmat)
        BATCH, KERNELS, VIEWS, QLEN, DLEN = kernels.shape
        kernels = kernels.reshape(BATCH, KERNELS * VIEWS, QLEN, DLEN)
        result = kernels.sum(dim=3) # sum over document
        simmat = simmat.reshape(BATCH, 1, VIEWS, QLEN, DLEN) \
                       .expand(BATCH, KERNELS, VIEWS, QLEN, DLEN) \
                       .reshape(BATCH, KERNELS * VIEWS, QLEN, DLEN)
        mask = (simmat.sum(dim=3) != 0.) # which query terms are not padding?
        result = torch.where(mask, (result + 1e-6).log(), mask.float())
        result = result.sum(dim=2) # sum over query terms
        return result

    def path_segment(self):
        result = '{name}_{qlen}q_{dlen}d'.format(name=self.name, **self.config)
        if not self.config['grad_kernels']:
            result += '_gradkernels'
        if self.config['add_runscore']:
            result += '_addrun'
        return result
"
125,2201.03356,
126,2201.03356,"from lire.function_tools import gradient_tools

class EWC(object):
    def __init__(self, model_shared):
        self.model = model_shared
        self.weights = list(self.model.parameters())
        self.fisher = [w.new_zeros(w.shape).data for w in  self.weights]

    def next_task(self, data, log_prob):
        self.star_weights = [w.data.clone() for w in  self.weights]
        self.local_fisher = gradient_tools.estimate_fisher_diag(data, log_prob)
        self.fisher = [f + lf for f, lf in  zip(self.fisher, self.local_fisher)]

    def ewc_loss(self, alpha=1):
        ewc_loss_backward = 0.
        for f, w, sw in zip(self.fisher, self.weights, self.star_weights):
            ewc_loss_backward += (f.detach() * (w - sw.detach())**2).sum()
        return ewc_loss_backward
"
127,2201.03356,"class ContinualModel(object):

    def __init__(self):
        super().__init__()

    def at_end_train(self, **kwargs) -> None:
        raise NotImplementedError
"
128,2201.03356,"'''The T5 ranking baseline.
The baseline implemented here rely on the paper
""Document Ranking with a Pretrained Sequence-to-Sequence Model""
published at EMNLP (2020).
'''
import torch
from torch.cuda.amp import autocast
import logging
from lire.data_tools.dataset import MSMarco
import tqdm
import torch.utils.data as data_utils
from experiments import t5ranker
# Loading dataset
data_folder = '/local/gerald/CPD/data'
split = 'dev'
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(message)s')

logging.info('Loading MSMarcoPassageRanking')
# loading the corpus using positive
dataset = MSMarco.MSMarcoPassageRankingDataset(data_folder,
                                               download=False,
                                               split=split,
                                               getter='top1000', 
                                               subset_size=5)
dataset.set_current_task_by_id = lambda x : 1 

def fnull():
    return 1
dataset.get_nb_tasks = fnull

logging.info('Configure the experiment')

nb_queries = dataset.get_nb_annotated_queries()
options = t5ranker.default_options
options.batch_size = 32
options.device = 0
batch_size = options.batch_size
nb_iteration = nb_queries * 9
options.number_iteration_by_task = int(nb_iteration)
options.save = '/net/sledge/gerald/T5RankerBaselineMSMarco.pth'
print(options)
logging.info(""Number of iteration considering the batch size: "" +
             str(nb_iteration))
# we train from 10 epochs according to number of training queries
my_experiment = t5ranker.T5TaskRanker(options, dataset)
logging.info(""Loading check point"")
state_dict = torch.load('/net/sledge/gerald/T5RankerBaselineMSMarco.pth', map_location='cpu')
my_experiment.state_dict = state_dict
dataset.set_output_transformation(lambda x : x[0] + x[1])
my_experiment.options.batch_size = 32
prediction_set = {}

dataloader =\
    data_utils.DataLoader(dataset,
                            batch_size=my_experiment.options.batch_size,
                            drop_last=False,
                            shuffle=False,
                            num_workers=4)

with torch.no_grad():
    with autocast(enabled=True):
        for it, (q_id, d_id, input_tokens) in zip(tqdm.trange(len(dataloader)),(dataloader)):
            positive_index =\
                my_experiment.tokenizer(input_tokens, return_tensors=""pt"", padding=True, max_length=512).input_ids.cuda()
            outputs_rank = my_experiment.model(input_ids=positive_index.cuda(), labels=my_experiment.index_token_positive[:len(q_id)])
            out_rank = outputs_rank[""logits""][:,0,my_experiment.index_token_positive[0,0]]
            for index  in range(len(q_id)):
                q_id_c, d_id_c , rank = q_id[index].item(), d_id[index].item(), out_rank[index].item()
                if(q_id_c not in prediction_set):
                    prediction_set[q_id_c] = {}
                prediction_set[q_id_c][d_id_c] = rank
#getting the model
gt = dataset.qrels.get_dictionary()
gt = { str(q_id):{str(d_id): v for d_id, v in ds_id.items() } for q_id, ds_id in gt.items()}
pred ={ str(q_id):{str(d_id): v for d_id, v in ds_id.items() } for q_id, ds_id in prediction_set.items()}

import pytrec_eval

evaluator = pytrec_eval.RelevanceEvaluator(gt, {'map', 'ndcg', 'recip_rank.10'})
results = {}
evaluation =  evaluator.evaluate(pred)
for query, scores in evaluation.items():
    for score_key, score_val in scores.items():
        if(score_key not in results):
            results[score_key] = 0.
        results[score_key] += score_val
results = {k: v/len(evaluation) for k, v in results.items()}
print(results)"
129,2201.03356,"
import torch
import torchvision
import torchvision.transforms as transforms
from torch import nn 
from torch import optim
from lire.function_tools import gradient_tools
from lire.continual_models import ewc
# Loading MNIST
dataset = torchvision.datasets.MNIST

training_data = dataset(""./data"", train=True, download=True)
testing_data = dataset(""./data"", train=False, download=True)


def sub_dataset(dataset, classes):
    new_dataset = []
    for x, y  in dataset:
        if(y in classes):
            new_dataset.append((x,y))
    return new_dataset

def flat_tranform(x):
    return x.view(-1)

# Loading MNIST
dataset = torchvision.datasets.MNIST

training_data = dataset(""./data"", train=True)
testing_data = dataset(""./data"", train=False)

data_transform = torchvision.transforms.Compose([
                               torchvision.transforms.ToTensor(),
                               torchvision.transforms.Normalize((0.5,), (0.5,)),
                               flat_tranform]
)


def sub_dataset(dataset, classes):
    new_dataset = []
    X, Y = [], []
    classes = classes.tolist()
    print(classes)
    for x, y  in dataset:
        if(y in classes):
            X.append(data_transform(x))
            Y.append(torch.LongTensor([y]))
        if(len(X) >= 2000):
            return X, Y
    return X, Y

def flat_tranform(x):
    return x.view(-1)

print(""Building tasks"")
sub_classes = torch.randperm(10)
sub_classes = [sub_classes[0:2], sub_classes[2:4], sub_classes[4:6], sub_classes[6:8], sub_classes[8:]]
tasks = [sub_dataset(training_data, task_class) for task_class in sub_classes]

print(""Instanciate model"")
encoder = nn.Linear(784, 20)
decoder = nn.Linear(20, 10)
model = nn.Sequential(encoder, nn.ReLU(), decoder, nn.ReLU())
criterion = nn.CrossEntropyLoss()

class LogProbModel(nn.Module):
    def __init__(self, model, criterion, parameters_list):
        super(LogProbModel, self).__init__()
        self.model = model
        self.criterion = criterion
        self.parameters_list = parameters_list
    
    def forward(self, data):
        data
        y_pred = self.model(x.unsqueeze(0))
        log_prob = self.criterion(y_pred, y)
        return log_prob
    
    def parameters(self): 
        return self.parameters_list

print(""Getting weights"")
weights = list(model.parameters())
print(""w shape "", [i.size() for  i in weights])

optimizer = optim.SGD(weights, lr=1e-1)
# log_prob_model = LogProbModel(model, criterion, weights)
# ewc_model = ewc.EWC(log_prob_model)

class SDataset(object):
    def __init__(self, X, Y):
        self.X = X
        self.Y = Y

    def __getitem__(self, index):
        return self.X[index], self.Y[index]
    
    def __len__(self):
        return len(self.X)


for task_index, task_data in enumerate(tasks):
    # if(task_index > 0):
    #     ewc_model.next_task(task_data[0], task_data[1], log_prob_model)
    dataloader = torch.utils.data.DataLoader(SDataset(task_data[0], task_data[1]), batch_size=50, shuffle=True, num_workers=2)
    for _ in range(50):

        for j in range(0, task_index+1):
            with torch.no_grad():
                dataloader = torch.utils.data.DataLoader(SDataset(tasks[j][0], tasks[j][1]), batch_size=100, shuffle=False, num_workers=2)
                total_classifier  = 0
                for x, y in dataloader:
                    y_pred = model(x)
                    # print(x.shape, y.shape)
                    _, index = y_pred.max(-1)
                    # print(index.shape)
                    total_classifier += (index == y.squeeze()).sum()
                print(j, '(',sub_classes[j],')','->', total_classifier/len(dataloader.dataset))

        for x, y in dataloader:
            optimizer.zero_grad()
            y_pred = model(x)
            loss = criterion(y_pred, y.squeeze())
            # if(task_index > 0):
            #     loss += 0. *  ewc_model.ewc_loss()
            loss.backward()
            optimizer.step()


    print(""------"")


# # EWC
# # 0 ( [0, 1] ) -> tensor(0.9912)
# # 1 ( [2, 3] ) -> tensor(0.8789)
# # 2 ( [4, 5] ) -> tensor(0.9730)
# # 3 ( [6, 7] ) -> tensor(0.9881)
# # 4 ( [8, 9] ) -> tensor(0.9730)

# # No EWC
# # 0 ( [0, 1] ) -> tensor(0.8654)
# # 1 ( [2, 3] ) -> tensor(0.9361)
# # 2 ( [4, 5] ) -> tensor(0.9747)
# # 3 ( [6, 7] ) -> tensor(0.9888)
# # 4 ( [8, 9] ) -> tensor(0.9752)"
130,2201.03356,"from lire.data_tools.dataset import MSMarco

MSMarco.MSMarcoPassageRankingTopicQueryCleanedDataset()"
131,2201.03356,"import sys
import h5py

import os
import json
import pandas as pd

from lire.data_tools.dataset import MSMarco
from lire.data_tools import data_large

with open('/web/gerald/public_html/lire_data/msmarco_topics_cleaned_train_stsb-roberta-large.json', 'r') as infile:
    train_set = json.load(infile)
with open('/web/gerald/public_html/lire_data/msmarco_topics_cleaned_dev_stsb-roberta-large.json', 'r') as infile:
    dev_set   = json.load(infile)
with open('/web/gerald/public_html/lire_data/msmarco_topics_cleaned_eval_stsb-roberta-large.json', 'r') as infile:
    eval_set  = json.load(infile)

data_folder   = ""/media/gerald/00B1B02B44A76AB2/CPD/data""
train_queries = MSMarco.MSMarcoPassageRankingDataset.load_queries(""train"", data_folder)
dev_queries   = MSMarco.MSMarcoPassageRankingDataset.load_queries(""dev"", data_folder)
eval_queries  = MSMarco.MSMarcoPassageRankingDataset.load_queries(""eval"", data_folder)
queries_set   = pd.concat([train_queries, dev_queries, eval_queries])

queries_set_unique = set(queries_set.index.unique().tolist())
data_folder = os.path.join(data_folder, ""MSMarcoPassageRankingDataset"")
dev_top_1000_path = os.path.join(data_folder, ""dev-top-1000.data"")
train_top_1000_path = os.path.join(data_folder, ""train-id-top-1000.data"")
train_triplet_path = os.path.join(data_folder,'train_positive_negative.data')

dev_inverse_cluster = {qid:cid for cid, c in enumerate(dev_set) for qid in c}
train_inverse_cluster = {qid:cid for cid, c in enumerate(train_set) for qid in c}

hdf5_filepath = os.path.join(data_folder, ""data.hdf5"")
# if(os.path.exists(hdf5_filepath)):
#     os.remove(hdf5_filepath)

# with h5py.File(hdf5_filepath,'a') as root:
#     dev = root.create_group(""dev"")
#     dev_1000 = dev.create_group(""dev-top-1000"")
#     train = root.create_group(""train"")
#     train_1000 = train.create_group(""train-top-1000"")
#     train_triple = train.create_group(""qid-pid-nid-cluster"")

data_large.HDF5DatasetManager.batched_csv_to_hdf5(hdf5_filepath, '/train/qid-pid-nid-cluster',
                                                  train_triplet_path, col_name=[""qid"", ""pid"", ""nid""],
                                                  group_by_key=""qid"", group_by_func=lambda x : str(train_inverse_cluster[int(x)]),
                                                  col_type=int, batch_size=int(1e7), head=int(40e7)
                                                 )

data_large.HDF5DatasetManager.batched_csv_to_hdf5(hdf5_filepath, ""dev/dev-top-1000"",
                                                  dev_top_1000_path, col_name=[""qid"", ""did""],
                                                  group_by_key=""qid"", col_type=int, batch_size=int(1e7)
                                                 )
# print(""Training data"")
# data_large.HDF5DatasetManager.batched_csv_to_hdf5(hdf5_filepath, ""train/train-top-1000"",
#                                                   train_top_1000_path, col_name=[""qid"", ""did""],
#                                                   group_by_key=""qid"", col_type=int, batch_size=int(1e7)
#                                                  )"
132,2201.03356,"import logging
from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration
from torch import optim

from lire.function_tools import gradient_tools




tokenizer = T5Tokenizer.from_pretrained(""t5-small"")


positive_document = tokenizer(""true"", return_tensors=""pt"").input_ids.cuda()
negative_document = tokenizer(""false"", return_tensors=""pt"").input_ids.cuda()

print(""The id of positive document is "", positive_document)
print(""The id of negative document is "", negative_document)

print(""Example of one backward for a query and a document : \n"")

queries = ""What is the name of the 2019 epidemic ?""
document = ""In 2019 the coronavirus lead to the largest pandemic of the 21th century.""

print(""\t The query is "", queries)
print(""\t The document is "", document)
tokenized_sentence = tokenizer(queries+""</s>""+document, return_tensors=""pt"").input_ids.cuda()
 
print(""Tokenized concatenation of query document "", tokenized_sentence)

detokenized_sentence = tokenizer.convert_ids_to_tokens(tokenized_sentence.squeeze().tolist())
print(""\tDetokenized sentence: "", detokenized_sentence)

print(""\n Example using T5: \n"")

model = T5ForConditionalGeneration.from_pretrained('t5-small').cuda()

adam_optimizer = optim.Adam(model.parameters())

outputs = model(input_ids=tokenized_sentence, labels=positive_document)
print(""Loss "", outputs.loss)
print(""score on true token before training "", outputs.logits[0,1,positive_document[0][0]].item())
print(""score on true token before training "", outputs.logits[0,0,positive_document[0][0]].item())
print(""max item "", outputs.logits[0,0,:].argmax())
outputs = model.generate(input_ids=tokenized_sentence)
print(tokenizer.convert_ids_to_tokens(outputs.squeeze().tolist()))
for i in range(100):

    adam_optimizer.zero_grad()

    outputs = model(input_ids=tokenized_sentence, labels=positive_document)
    outputs.loss.backward()
    print(i, outputs.loss.item())
    adam_optimizer.step()


outputs = model(input_ids=tokenized_sentence, labels=positive_document)
print(""Loss "", outputs.loss)
print(""score on true token after training "", outputs.logits[0,1,positive_document[0][0]].item())
print(""score on true token after training "", outputs.logits[0,0,positive_document[0][0]].item())
print(""max item "", outputs.logits[0,0,:].argmax())
print(""Generating sentence "")
outputs = model.generate(input_ids=tokenized_sentence)
print(tokenizer.convert_ids_to_tokens(outputs.squeeze().tolist()))

class local_model(object):
    def __init__(self, hf_model):
        self.hf_model = hf_model
    
    def __call__(self, x, y):
        outputs = self.hf_model(input_ids=x, labels=y)
        return outputs.logits[0, 0, positive_document[0][0]]
    
    def parameters(self):
        return self.hf_model.parameters()

lm = local_model(model)
y = lm(tokenized_sentence, positive_document)
print(y)

fisher_diag = gradient_tools.estimate_f
isher_diag([tokenized_sentence], [positive_document], lm)
print([layer.shape for layer in fisher_diag])


"
133,2201.03356,"from lire.experiments_tools.experiement_template import LIReExperiment

class KNRMMonoTaskRanking(LIReExperiment):
    def __init__(self, options, dataset):
        super(KNRMMonoTaskRanking, self).__init__(options, dataset)
        
    def "
134,2201.03356,"
import logging
import torch
from torch import optim
import math
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch.utils.data as data_utils
from torch.utils.tensorboard import SummaryWriter
from lire.experiment_tools.experiment_template import LIReExperiment
from lire.misc import struct
import time, datetime
from torch.cuda.amp import autocast
from torch.cuda.amp import GradScaler
default_options = struct({'save': '/local/gerald/default.pth',
                          'batch_size': 16,
                          'lr': 1e-3,
                          'beta1': 0.9,
                          'beta2': 0.999,
                          'device': 0,
                          'epoch_by_task': 1,
                          'loss_estimation_iter': 10000,
                          'number_iteration_by_task': None,
                          'model': 't5-base'})


class T5TaskRanker(LIReExperiment):
    def __init__(self, options, dataset):
        super(T5TaskRanker, self).__init__(options, dataset)
        logging.basicConfig(level=logging.INFO, 
                            format='%(asctime)s %(message)s')
        logging.info(""Initialise tokenizer"")
        self.tokenizer = T5Tokenizer.from_pretrained(options.model)
        logging.info(""Loading pretrained model"")
        self.model = T5ForConditionalGeneration.from_pretrained(options.model)
        self.model = self.model.to(self.options.device)

        logging.info(""Use default optimizer Adam"")
        self.optimiser = optim.Adam(self.model.parameters(),
                                    lr=self.options.lr,
                                    betas=(self.options.beta1,
                                           self.options.beta2))
        self.evaluation_task = []
        logging.info(""Initialise meta info"")
        self.training = True
        self.current_state = {'task': 0}
        token_positive = ""true""
        token_negative = ""false""
        index_token_positive =\
            self.tokenizer(token_positive, return_tensors=""pt"").input_ids
        index_token_negative =\
            self.tokenizer(token_negative, return_tensors=""pt"").input_ids
        self.index_token_positive =\
            index_token_positive.repeat(self.options.batch_size,
                                        1).to(options.device)
        self.index_token_negative =\
            index_token_negative.repeat(self.options.batch_size,
                                        1).to(options.device)
        self.state_dict_file = self.options.save

        def query_transform(x):
            return x + '</s>'

        def output_transformation(x):
            return (x[0] + x[1], x[0] + x[2])

        self.dataset.set_output_transformation(output_transformation)
        self.dataset.set_query_transform(query_transform)
        self.writer = SummaryWriter()
        self.current_state[""min_loss""] = {}
        self.current_state[""iteration""] = 0

    def step(self, positive, negative):
        with autocast(enabled=True):
            self.optimiser.zero_grad()
            positive_index, negative_index =\
                self.tokenizer(list(positive), return_tensors=""pt"",
                            padding=True, max_length=512).input_ids,\
                self.tokenizer(list(negative), return_tensors=""pt"",
                            padding=True, max_length=512).input_ids
            
            positive_index, negative_index =\
                positive_index.to(self.options.device),\
                negative_index.to(self.options.device)
            outputs_positive = self.model(input_ids=positive_index,
                                        labels=self.index_token_positive)
            outputs_negative = self.model(input_ids=negative_index,
                                        labels=self.index_token_negative)

            loss_positive = outputs_positive.loss
            loss_negative = outputs_negative.loss

            loss = loss_positive + loss_negative


        self.writer.add_scalar('train/batch_loss', loss.item(),
                            self.current_state[""iteration""])
        return loss

    def begin_task(self):
        self.dataset.set_current_task_by_id(self.current_state['task'])
        self.dataloader =\
            data_utils.DataLoader(self.dataset,
                                  batch_size=self.options.batch_size,
                                  drop_last=True,
                                  shuffle=True,
                                  num_workers=4)

    def ending_task(self):
        pass

    def _train_task(self, max_iteration, dataloader, shift=0):
        logging.info(""Number of examples for the current task : "" +
                     str(len(self.dataset)))
        if(self.current_state[""task""] not in self.current_state[""min_loss""]):
            self.current_state[""min_loss""][self.current_state[""task""]] =\
                math.inf
        start_time = time.time()
        for i in range(max_iteration):
            loss_sum = 0.
            loss_n = 0
            for it, (pos, neg) in enumerate(self.dataloader):

                batch_loss = self.step(pos, neg)
                
                self.scaler.scale(batch_loss).backward()
                self.scaler.step(self.optimiser)
                self.scaler.update()
                loss_sum += batch_loss.item() * len(pos)
                loss_n += len(pos)

                self.current_state[""iteration""] += 1

                if(it % self.options.loss_estimation_iter == 10):
                    task_iteration = i*len(self.dataset) + it * self.options.batch_size
                    estimated_loss = loss_sum/loss_n
                    self.writer.add_scalar('train/avg_loss',
                                           estimated_loss,
                                           self.current_state[""iteration""])
                    self.evaluate()
                    if(self.current_state[""min_loss""]
                       [self.current_state[""task""]] > estimated_loss):
                        torch.save(self.state_dict, self.state_dict_file)
                        self.current_state[""min_loss""][self.current_state[""task""]] = estimated_loss
                    
                    if(self.options.number_iteration_by_task is not None):
                        if(callable(self.options.number_iteration_by_task )):
                            nb_it_task = self.options.number_iteration_by_task()
                        else:
                            nb_it_task = self.options.number_iteration_by_task
                        EAT = (time.time() - start_time)/task_iteration *\
                            (nb_it_task - task_iteration)
                        logging.info(""Loss for iteration "" + str(task_iteration) +
                                     "" is "" + str(estimated_loss) +
                                     "" EAT: "" +str(datetime.timedelta(seconds=EAT)))
                        if(nb_it_task <=
                           task_iteration):
                            return
                    else:
                        logging.info(""Loss for iteration "" + str(task_iteration) +
                                 "" is "" + str(estimated_loss))

    def add_evaluation(self, function, parameters):
        self.evaluation_task.append((function, parameters))

    def evaluate(self):
        for k, v in self.evaluation_task:
            k(*v)

    def run(self):
        self.scaler = GradScaler(enabled=True)
        if(self.training):
            epoch_by_task = self.options.epoch_by_task
            for i in range(self.dataset.get_nb_tasks()):
                self.current_state['task'] = i

                self.begin_task()
                self._train_task(epoch_by_task, self.dataloader)
                self.ending_task()
                self.optimiser = optim.Adam(self.model.parameters(),
                                            lr=self.options.lr,
                                            betas=(self.options.beta1,
                                                   self.options.beta2))"
135,2201.03356,"from lire.data_tools.dataset import MSMarco
from lire.data_tools import continual_routine"
136,2201.03356,"import argparse
import logging
from os.path import join
import os
import time
import tqdm
import torch
import torch.utils.data as data_utils
from torch import optim
from torch.cuda.amp import autocast
from torch.cuda.amp import GradScaler
import random
from ray import tune
from transformers import T5Tokenizer, T5TokenizerFast
from transformers import T5ForConditionalGeneration

from lire.data_tools.dataset import MSMarco

parser = argparse.ArgumentParser(description='Configuration of the experiment Based on the model proposed in ""Document Ranking with Pretrained Sequence to Sequence model""')

# Mandatory parameters

parser.add_argument('--data-folder', type=str, dest='data_folder',
                    help='The folder where data are or will be downloaded/decompressed')
parser.add_argument('--model-storage-folder', type=str, dest='model_storage_folder',
                    help='folder to save the model')
# Optional parameters (with default values but important for model performances/training time)

parser.add_argument('--batch-size', type=int, dest='batch_size', default=10,
                    help='The size of the batch')
parser.add_argument('--max-iter', type=int, dest='max_iter', default=5000,
                    help='Maximum number of iteration by epoch')
parser.add_argument('--max-epoch', type=int, dest='max_epoch', default=1,
                    help='Maximum number of epoch')
parser.add_argument('--devices', type=int, dest='devices', default=-1,
                    help='Device to use (GPU) if -1 using cpu')
parser.add_argument('--subset-size', type=int, dest='subset_size', default=None,
                    help='Considering only a subset of MSMarco (for dev or testing)')

# Less important parameters (with default value not impacting learning)

parser.add_argument('--loss-estimation-iter', type=int, dest='loss_estimation_iter', default=500,
                    help='Number of iteration to average and print the loss')

args = parser.parse_args()


logging.info(""Instanciate the model"")

def training_function(config):
    args = type('new_dict', (object,), config)
    key_time = round(time.time() * 1000)
    os.makedirs(args.model_storage_folder, exist_ok=True)
    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')

    logging.info(""Configuring lambda functions"")


    logging.info(""Loading MSMarco Training set"")

    data_folder = args.data_folder
    split = 'train'
    dataset = MSMarco.MSMarcoPassageRankingDataset(data_folder, download=True, split=split, storage='full', getter='triplet', subset_size=args.subset_size)

    logging.info(""Configuring dataset and pipeline"")

    set_device = lambda x : x if(args.devices == -1) else x.to(args.devices)

    identity_function = lambda x : x

    tokenizer = T5TokenizerFast.from_pretrained(""t5-small"")

    query_transform = lambda x: x+'</s>'

    output_transformation = lambda x: (x[0] + x[1], x[0] + x[2])
    dataset.set_output_transformation(output_transformation)
    dataset.set_query_transform(query_transform)
    training_dataloader = data_utils.DataLoader(dataset, batch_size=args.batch_size, drop_last=True)

    logging.info(""Configuring label set"")

    token_positive = ""true""
    token_negative = ""false""
    index_token_positive = tokenizer(token_positive,return_tensors=""pt"").input_ids
    index_token_negative = tokenizer(token_negative, return_tensors=""pt"").input_ids
    index_token_positive = set_device(index_token_positive.repeat(args.batch_size, 1))
    index_token_negative = set_device(index_token_negative.repeat(args.batch_size, 1))


    model = T5ForConditionalGeneration.from_pretrained('t5-small')
    model = set_device(model)
    adam_optimizer = optim.Adam(model.parameters())

    logging.info(""Start fine tunning"")

    scaler = GradScaler(enabled=True)
    loss_accumulator = 0.
    progress_bar = tqdm.trange(len(training_dataloader))
    min_loss = 10000.
    for epoch in range(args.max_epoch):
        for it, (positive, negative) in zip(progress_bar, training_dataloader):
            adam_optimizer.zero_grad()
            with autocast(enabled=True):
                positive_index, negative_index =\
                    tokenizer(list(positive), return_tensors=""pt"", padding=True, max_length=512).input_ids,\
                    tokenizer(list(negative), return_tensors=""pt"", padding=True, max_length=512).input_ids
                positive_index, negative_index = set_device(positive_index), set_device(negative_index)

                outputs_positive = model(input_ids=positive_index, labels=index_token_positive)
                outputs_negative = model(input_ids=negative_index, labels=index_token_negative)
                loss_positive = outputs_positive.loss
                loss_negative = outputs_negative.loss
            
                loss = loss_positive + loss_negative

                loss_accumulator = loss.item()
                
                if(it%args.loss_estimation_iter == 0 and it != 0):
                    progress_bar.set_postfix({""L"":loss_accumulator/args.loss_estimation_iter })
                    if(min_loss > loss_accumulator/args.loss_estimation_iter):
                        
                        torch.save(model.state_dict(), join(args.model_storage_folder, str(key_time)+"".data""))
                        min_loss = loss_accumulator/args.loss_estimation_iter
                    tune.report(mean_loss=loss_accumulator/args.loss_estimation_iter)
                    loss_accumulator = 0.


            scaler.scale(loss).backward()
            
            scaler.step(adam_optimizer)
            scaler.update()
            if(it == args.max_iter):
                break








config = args.__dict__
config[""max_iter""] =  tune.grid_search([5001, 10001, 15001])
analysis = tune.run(training_function, config=config,  resources_per_trial={""gpu"": 1})

print(""Best config: "", analysis.get_best_config(
    metric=""mean_loss"", mode=""min""))

# Get a dataframe for analyzing trial results.
df = analysis.results_df"
137,2201.03211,"# Configuration file for the Sphinx documentation builder.
#
# This file only contains a selection of the most common options. For a full
# list see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
import sys
# sys.path.insert(0, os.path.abspath('.'))


# -- Project information -----------------------------------------------------

project = 'Toolbox for Adaptive Fourier Decomposition'
copyright = '2021, Ze Wang'
author = 'Ze Wang'


# -- General configuration ---------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinx.ext.mathjax',
    'sphinx.ext.graphviz',
    'sphinxcontrib.ghcontributors',
    'sphinx.ext.viewcode',
    'sphinx.ext.autodoc',
    'sphinxcontrib.matlab'
]

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']


# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
import sphinx_rtd_theme

extensions.append('sphinx_rtd_theme')
html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]
html_theme = 'sphinx_rtd_theme'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named ""default.css"" will overwrite the builtin ""default.css"".
html_static_path = ['_static']

# Add markdown support
from recommonmark.parser import CommonMarkParser
source_parsers = {
    '.md': CommonMarkParser,
}
source_suffix = ['.rst', '.md']

# The master toctree document.
master_doc = 'index'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = [u'_build', 'Thumbs.db', '.DS_Store', '**.ipynb_checkpoints']"
138,2201.03211,"# -*- coding: utf-8 -*-
from numpy import *
from numpy.matlib import repmat
from numpy.fft import fft, ifft
from scipy.signal import hilbert
    
def e_a(a,t):
    """"""
    evaluator function
    """"""
    y=((1-absolute(a)**2)**0.5)/(1-conjugate(a)*(e**(1j*t)))
    return y
        
        
def weight(n,order):
    """"""
    weight function of the numerical integration
    """"""
    """"""
    # Old Code: 
    y=zeros((n,1),'complex')
    Newton=array([[41.0/840],[9.0/35],[9.0/280],[34.0/105],[9.0/280],[9.0/35],[41.0/840]])
    k=floor((n*1.0-1)/order)
    if k>0:
        for iter in arange(1,k+1):
            y[(iter*order-order):(iter*order+1)]=y[(iter*order-order):(iter*order+1)]+Newton
            
    y=y*order*1.0/(n-1)
    nleft=n-k*order-1;
    if nleft==1:
        Newton=array([[1.0/2],[1.0/2]])
    elif nleft==2:
        Newton=array([[1.0/6],[4.0/6],[1.0/6]])
    elif nleft==3:
        Newton=array([[1.0/8],[3.0/8],[3.0/8],[1.0/8]])
    elif nleft==4:
        Newton=array([[7.0/90],[16.0/45],[2.0/15],[16.0/45],[7.0/90]])
    elif nleft==5:
        Newton=array([[19.0/288],[25.0/96],[25.0/144],[25.0/144],[25.0/96],[19.0/288]])
        
    if nleft>0:
        y[(n-nleft-1):(n+1)]=y[(n-nleft-1):(n+1)]+Newton*nleft/(n-1)
    """"""
    y=ones((n,1),'complex')
        
    return y
    
    
def intg(f,g,W):
    if ndim(g)==1:
        g=array([g])
    y=f.dot(g.T*W)
    if ndim(y)!=1:
        y=y[0,0]
    return y/(shape(f)[1])
        

def conv_AFD(s,max_level=50,M=20,L=2000):
    """"""
    AFD based on the conventional exhaustive basis searching
    Inputs:
        s: 1*K pocessed discrete signal
        max_level: maximum decomposition level
        M: if it is a integer number, it is the maximum number of the magnitude
           values of a_n in the searching dictionary, and the dictionary of the
           magnitude values is unique distributed in [0,1).
           if it is an array, it is the dictionary of the magnitude values.
        L: if it is a integer number, it is the maximum number of the phase
           values of a_n in the searching dictionary, and the dictionary of the
           phase values is unique distributed in [0,2*pi).
    Outputs:
        state: -1-> Error, 1->No Error
        an: values of a_n for n=0,2,...,N
        coef: values of coef_n for n=0,2,...,N
    """"""
    if ndim(s)==1:
        s=array([s])
    # Convert the signal s to its discrete time format
    K=shape(s)[1]
    t=array([arange(0,2*pi,2*pi/K)])
    # Convert the signal s to its analytic representation
    if isreal(s).all():
        G=hilbert(s)
    else:
        G=s.copy()
            
    # Generate a_n dictionary
    if size(M)==1:
        abs_a=array([arange(0,1,1.0/M)])
    else:
        if ndim(M)==1:
            M=array([M])
            abs_a=M.copy()
        elif shape(M)[0]==1:
            abs_a=M.copy()
        elif shape(M)[1]==1:
            abs_a=M.T.copy()
        else:
            return -1,array([]),array([])

    if size(L)==1:
        phase_a=array([arange(0,2*pi,2*pi/L)])
    else:
        if ndim(L)==1:
            L=array([L])
            phase_a=L.copy()
        elif shape(L)[0]==1:
            phase_a=L.copy()
        elif shape(L)[1]==1:
            phase_a=L.T.copy()
        else:
            return -1,array([]),array([])
             
    dic_an=zeros((size(abs_a),size(phase_a)),'complex')
    for m in arange(0,size(abs_a)):
        for l in arange(0,size(phase_a)):
            dic_an[m,l]=abs_a[0,m]*(e**(1j*phase_a[0,l]))            
         
    dic_an=dic_an.reshape(1,size(abs_a)*size(phase_a)).copy()
    # Generate evaluators
    Base=zeros((size(abs_a)*size(phase_a),K),'complex')
    for k in arange(0,shape(Base)[0]):
        Base[k,:]=e_a(dic_an[0,k],t)
        
    # Generate weight for the numerical integration
    Weight=weight(K,6)
    # Initialization
    an=zeros((1,max_level+1),'complex')
    coef=zeros((1,max_level+1),'complex')
    coef[0,0]=intg(G,ones((1,size(t))),Weight)
    for n in arange(1,size(an)):
        e_an=e_a(an[0,n-1],t)
        G=(G-coef[0,n-1]*e_an)*(1-conjugate(an[0,n-1])*(e**(1j*t)))/(e**(1j*t)-an[0,n-1])
        S1=conjugate(Base.dot(G.conj().T*Weight))
        I=nonzero(absolute(S1)==absolute(S1).max())[0][0]
        an[0,n]=dic_an[0,I]
        coef[0,n]=conjugate(e_a(an[0,n],t).dot(G.conj().T*Weight))[0,0]/K
        
    return 1,an,coef,t
    
    
def component_AFD(an,coef,t):
    """"""
    Decomposition components of AFD
    Inputs:
        1. a_n array for n=0,1,2,...,N
        2. coef: coefficient array for n=0,1,2,...,N
        3. t: time sample points of the discrete time signal
    Outputs:
        1. e_an: dictionary components
        2. B_n: basis components
        3: F_n: decomposition components
    """"""
    if ndim(an)==1:
        an=array([an])
    if ndim(coef)==1:
        coef=array([coef])
    if ndim(t)==1:
        t=array([t])
    e_an=zeros((size(an),size(t)),'complex')
    B_n=zeros((size(an),size(t)),'complex')
    F_n=zeros((size(an),size(t)),'complex')
    n=0
    e_an[n,:]=e_a(an[0,n],t)
    B_n[n,:]=(sqrt(1-absolute(an[0,0])**2)/(1-conjugate(an[0,0])*e**(t*1j)))
    F_n[n,:]=coef[0,n]*B_n[n,:]
    n=1
    while n<size(an):
        e_an[n,:]=e_a(an[0,n],t)
        B_n[n,:]=(sqrt(1-absolute(an[0,n])**2)/(1-conjugate(an[0,n])*e**(t*1j)))*((e**(1j*t)-an[0,n-1])/(sqrt(1-absolute(an[0,n-1])**2)))*B_n[n-1,:]
        F_n[n,:]=coef[0,n]*B_n[n,:]
        n=n+1
    return e_an,B_n,F_n
    

def inverse_AFD(an,coef,t,standard='level',standard_value=float(""inf"")):
    """"""
    Inverse AFD
    Inputs:
        1. a_n array for n=0,1,2,...,N
        2. coef: coefficient array for n=0,1,2,...,N
        3. t: time sample points of the discrete time signal
        4. standard: 
            (1) 'level': reconstruction based on the decomposition level from n=0 to n=min((size(an),standard_value))
            (2) 'energy': reconstruction based on the energy. The energy of the reconstructed signal is smaller or equal to standard_value
    Output:
        1. G_recovery: reconstructed signal
        2. n: reconstructed_level
    """"""
    if ndim(an)==1:
        an=array([an])
    if ndim(coef)==1:
        coef=array([coef])
    if ndim(t)==1:
        t=array([t])
    Weight=weight(size(t),6)
    tem_B=(sqrt(1-absolute(an[0,0])**2)/(1-conjugate(an[0,0])*e**(t*1j)))
    G_recovery=coef[0,0]*tem_B
    n=0;
    if standard.lower()=='level':
        current_value=0
        target_value=min((size(an)-1,standard_value))
    elif standard.lower()=='energy':
        current_value=intg(real(G_recovery),real(G_recovery),Weight)
        target_value=standard_value
    while n<size(an)-1 and current_value<target_value:
        n=n+1
        tem_B=(sqrt(1-absolute(an[0,n])**2)/(1-conjugate(an[0,n])*e**(t*1j)))*((e**(1j*t)-an[0,n-1])/(sqrt(1-absolute(an[0,n-1])**2)))*tem_B
        G_recovery=G_recovery+coef[0,n]*tem_B
        if standard.lower()=='level':
            current_value=n
        elif standard.lower()=='energy':
            current_value=intg(real(G_recovery),real(G_recovery),Weight)
    return G_recovery,n
    
    
def FFT_AFD(s,max_level=50,M=20):
    """"""
    AFD based on the FFT (phase searching length is equal to the signal length)
    Inputs:
        s: 1*K pocessed discrete signal
        max_level: maximum decomposition level
        M: if it is a integer number, it is the maximum number of the magnitude
           values of a_n in the searching dictionary, and the dictionary of the
           magnitude values is unique distributed in [0,1).
           if it is an array, it is the dictionary of the magnitude values.
    Outputs:
        state: -1-> Error, 1->No Error
        an: values of a_n for n=0,2,...,N
        coef: values of coef_n for n=0,2,...,N
    """"""
    if ndim(s)==1:
        s=array([s])
    # Convert the signal s to its discrete time format
    K=shape(s)[1]
    t=array([arange(0,2*pi,2*pi/K)])
    # Convert the signal s to its analytic representation
    if isreal(s).all():
        G=hilbert(s)
    else:
        G=s.copy()
            
    # Generate a_n dictionary
    if size(M)==1:
        abs_a=array([arange(0,1,1.0/M)])
    else:
        if ndim(M)==1:
            M=array([M])
            abs_a=M.copy()
        elif shape(M)[0]==1:
            abs_a=M.copy()
        elif shape(M)[1]==1:
            abs_a=M.T.copy()
        else:
            return -1,array([]),array([])
    temp=zeros((1,size(abs_a)),'complex')
    for k in arange(0,size(abs_a)):
        temp[0,k]=complex(abs_a[0,k])
    abs_a=temp.copy()
    del temp
    # Generate evaluators
    Base=zeros((size(abs_a),size(t)),'complex')
    for k in arange(0,shape(Base)[0]):
        Base[k,:]=fft(e_a(abs_a[0,k],t),size(t))
        
    # Generate weight for the numerical integration
    Weight=weight(K,6)
    # Initialization
    an=zeros((1,max_level+1),'complex')
    coef=zeros((1,max_level+1),'complex')
    coef[0,0]=intg(G,ones((1,size(t))),Weight)
    for n in arange(1,size(an)):
        e_an=e_a(an[0,n-1],t)
        G=(G-coef[0,n-1]*e_an)*(1-conjugate(an[0,n-1])*(e**(1j*t)))/(e**(1j*t)-an[0,n-1])
        S1=ifft(repmat(fft(G*Weight.conj().T,size(t)),shape(Base)[0],1)*Base,size(t),1)        
        max_loc=nonzero(absolute(S1)==absolute(S1).max())
        an[0,n]=abs_a[0,max_loc[0][0]]*e**(1j*t[0,max_loc[1][0]])
        coef[0,n]=conjugate(e_a(an[0,n],t).dot(G.conj().T*Weight))[0,0]/K
        
    return 1,an,coef,t    
    
    
if __name__ == ""__main__"":
    print(""-----------Test Basic Functions--------------------------------"")
    t=array([arange(0,2*pi,2*pi/2000)])
    print(""Test e_a"")
    print(e_a(0.5+0.5j,t))
    print(""Test weight"")
    print(weight(2000,6))
    print(""Test intg"")
    print(intg(array([arange(0,12)]),array([arange(1,13)]),weight(12,6)))
    print(""-----------Test conventional method--------------------------"")
    from scipy.io import loadmat
    import time
    s=loadmat('.\\Example\\bump_signal.mat')
    s=s['G'].copy()
    # Repeat 50 times for measuring time
    measure_time=arange(0,50,dtype=dtype(float))
    for k in arange(0,50):
        t1=time.clock()
        state,an,coef,t=conv_AFD(s,50,50,size(s))
        t2=time.clock()
        measure_time[k]=t2-t1
    s_re,n=inverse_AFD(an,coef,t)
    print(""Time for the conventional method:"")
    print(mean(measure_time))
    print(""--------------Test FFT method-------------------------------"")
    # Repeat 50 times for measuring time
    measure_time=arange(0,50,dtype=dtype(float))
    for k in arange(0,50):
        t3=time.clock()
        state_FFT,an_FFT,coef_FFT,t_FFT=FFT_AFD(s,50,50)
        t4=time.clock()
        measure_time[k]=t4-t3
    s_re_FFT,n_FFT=inverse_AFD(an_FFT,coef_FFT,t_FFT)
    print(""Time for the FFT:"")
    print(mean(measure_time))
    print(""---------------Plot results---------------------------------"")
    from matplotlib import pyplot
    pyplot.figure(1)
    pyplot.plot(real(an),imag(an),'rx',real(an_FFT),imag(an_FFT),'go')
    pyplot.xlabel('Real')
    pyplot.ylabel('Imag')
    pyplot.show()
    pyplot.figure(2)
    pyplot.plot(t,real(s),'ro-',t,real(s_re),'go-',t,real(s_re_FFT),'ko-')
    pyplot.xlabel('Phase')
    pyplot.show()
    e_an,B_n,F_n=component_AFD(an,coef,t)
    pyplot.figure(3)
    pyplot.plot(t[0,:],real(s[0,:]),'ro-',t[0,:],real(F_n[0,:]),'go-',t[0,:],real(F_n[1,:]),'ko-',t[0,:],real(F_n[2,:]),'yo-')
    pyplot.xlabel('Phase')
    pyplot.show()
    e_an,B_n,F_n=component_AFD(an_FFT,coef_FFT,t_FFT)
    pyplot.figure(4)
    pyplot.plot(t_FFT[0,:],real(s[0,:]),'ro-',t_FFT[0,:],real(F_n[0,:]),'go-',t_FFT[0,:],real(F_n[1,:]),'ko-',t_FFT[0,:],real(F_n[2,:]),'yo-')
    pyplot.xlabel('Phase')
    pyplot.show()"
139,2201.03156,"import jax
import jax.numpy as jnp
import haiku as hk

class FermiNet(hk.Module):
    def __init__(self, depth, spsize, tpsize, L, init_stddev=0.01):
        super().__init__()
        self.depth = depth
        self.L = L
        self.init_stddev = init_stddev
        self.splayers = [hk.Linear(spsize, w_init=hk.initializers.RandomNormal(stddev=self.init_stddev))
                            for _ in range(depth)]
        self.tplayers = [hk.Linear(tpsize, w_init=hk.initializers.RandomNormal(stddev=self.init_stddev))
                            for _ in range(depth-1)]

    def _spstream0(self, x):
        """""" Initial spstream, with shape (n, spsize0). """"""
        return jnp.zeros_like(x)

    def _tpstream0(self, x):
        """""" Initial tpstream, with shape (n, n, tpsize0). """"""
        rij = x[:, None, :] - x
        cos_rij, sin_rij = jnp.cos(2*jnp.pi/self.L * rij), jnp.sin(2*jnp.pi/self.L * rij)
        n, _ = x.shape
        dij = jnp.linalg.norm(sin_rij + jnp.eye(n)[..., None], axis=-1) * (1 - jnp.eye(n))
        return jnp.concatenate((cos_rij, sin_rij, dij[..., None]), axis=-1)

    def _f(self, spstream, tpstream):
        """"""
            The feature `f` as input to the sptream network.
            `f` has shape (n, fsize), where fsize = 2*spsize + tpsize.
        """"""
        n, _ = spstream.shape
        f = jnp.concatenate((spstream,
                             spstream.mean(axis=0, keepdims=True).repeat(n, axis=0),
                             tpstream.mean(axis=1)), axis=-1)
        return f

    def __call__(self, x):
        spstream, tpstream = self._spstream0(x), self._tpstream0(x)

        for i in range(self.depth-1):
            f = self._f(spstream, tpstream)
            if i==0:
                spstream = jax.nn.softplus( self.splayers[i](f) )
                tpstream = jax.nn.softplus( self.tplayers[i](tpstream) )
            else:
                spstream += jax.nn.softplus( self.splayers[i](f) )
                tpstream += jax.nn.softplus( self.tplayers[i](tpstream) )

        f = self._f(spstream, tpstream)
        spstream += jax.nn.softplus( self.splayers[-1](f) )
        _, dim = x.shape
        final = hk.Linear(dim, w_init=hk.initializers.RandomNormal(stddev=self.init_stddev))
        return x + final(spstream)
"
140,2201.03156,"import jax
from jax.config import config
config.update(""jax_enable_x64"", True)
import jax.numpy as jnp
from jax.flatten_util import ravel_pytree

print(""jax.__version__:"", jax.__version__)
key = jax.random.PRNGKey(42)

import argparse
parser = argparse.ArgumentParser(description=""Finite-temperature VMC for the homogeneous electron gas"")

parser.add_argument(""--folder"", default=""/data/xiehao/CoulombGas/master/"", help=""the folder to save data"")

# physical parameters.
parser.add_argument(""--n"", type=int, default=29, help=""total number of electrons"")
parser.add_argument(""--dim"", type=int, default=2, help=""spatial dimension"")
parser.add_argument(""--rs"", type=float, default=10.0, help=""rs"")
parser.add_argument(""--Theta"", type=float, default=0.15, help=""dimensionless temperature T/Ef"")

# many-body state distribution: autoregressive transformer.
parser.add_argument(""--Emax"", type=int, default=25, help=""energy cutoff for the single-particle orbitals"")
parser.add_argument(""--nlayers"", type=int, default=2, help=""CausalTransformer: number of layers"")
parser.add_argument(""--modelsize"", type=int, default=16, help=""CausalTransformer: embedding dimension"")
parser.add_argument(""--nheads"", type=int, default=4, help=""CausalTransformer:number of heads"")
parser.add_argument(""--nhidden"", type=int, default=32, help=""CausalTransformer: number of hidden units of the MLP within each layer"")

# normalizing flow.
parser.add_argument(""--depth"", type=int, default=2, help=""FermiNet: network depth"")
parser.add_argument(""--spsize"", type=int, default=16, help=""FermiNet: single-particle feature size"")
parser.add_argument(""--tpsize"", type=int, default=16, help=""FermiNet: two-particle feature size"")

# parameters relevant to th Ewald summation of Coulomb interaction.
parser.add_argument(""--Gmax"", type=int, default=15, help=""k-space cutoff in the Ewald summation of Coulomb potential"")
parser.add_argument(""--kappa"", type=int, default=10, help=""screening parameter (in unit of 1/L) in Ewald summation"")

# MCMC.
parser.add_argument(""--mc_therm"", type=int, default=10, help=""MCMC thermalization steps"")
parser.add_argument(""--mc_steps"", type=int, default=50, help=""MCMC update steps"")
parser.add_argument(""--mc_stddev"", type=float, default=0.1, help=""standard deviation of the Gaussian proposal in MCMC update"")

# technical miscellaneous
parser.add_argument(""--hutchinson"", action='store_true',  help=""use Hutchinson's trick to compute the laplacian"")

# optimizer parameters.
parser.add_argument(""--lr"", type=float, default=1e-3, help=""learning rate (valid only for adam)"")
parser.add_argument(""--sr"", action='store_true',  help=""use the second-order stochastic reconfiguration optimizer"")
parser.add_argument(""--damping"", type=float, default=1e-3, help=""damping"")
parser.add_argument(""--max_norm"", type=float, default=1e-3, help=""gradnorm maximum"")

# training parameters.
parser.add_argument(""--batch"", type=int, default=2048, help=""batch size (per single gradient accumulation step)"")
parser.add_argument(""--num_devices"", type=int, default=8, help=""number of GPU devices"")
parser.add_argument(""--acc_steps"", type=int, default=4, help=""gradient accumulation steps"")
parser.add_argument(""--epoch_finished"", type=int, default=0, help=""number of epochs already finished"")
parser.add_argument(""--epoch"", type=int, default=3000, help=""final epoch"")

args = parser.parse_args()

n, dim = args.n, args.dim
if dim == 3:
    L = (4/3*jnp.pi*n)**(1/3)
    beta = 1 / ((4.5*jnp.pi)**(2/3) * args.Theta)
elif dim == 2:
    L = jnp.sqrt(jnp.pi*n)
    beta = 1/ (4 * args.Theta)
print(""n = %d, dim = %d, L = %f"" % (n, dim, L))

####################################################################################

print(""\n========== Initialize single-particle orbitals =========="")

from orbitals import sp_orbitals
sp_indices, Es = sp_orbitals(dim, args.Emax)
sp_indices, Es = jnp.array(sp_indices), jnp.array(Es)
Ef = Es[n-1]
print(""beta = %f, Ef = %d, Emax = %d, corresponding delta_logit = %f""
        % (beta, Ef, args.Emax, beta * (2*jnp.pi/L)**2 * (args.Emax - Ef)))

sp_indices, Es = sp_indices[::-1], Es[::-1]
num_states = Es.size
print(""Number of available single-particle orbitals: %d"" % num_states)
from scipy.special import comb
print(""Total number of many-body states (%d in %d): %f"" % (n, num_states, comb(num_states, n)))

####################################################################################

print(""\n========== Initialize many-body state distribution =========="")

import haiku as hk
from autoregressive import Transformer
def forward_fn(state_idx):
    model = Transformer(num_states, args.nlayers, args.modelsize, args.nheads, args.nhidden)
    return model(state_idx)
van = hk.transform(forward_fn)
state_idx_dummy = sp_indices[-n:].astype(jnp.float64)
params_van = van.init(key, state_idx_dummy)

raveled_params_van, _ = ravel_pytree(params_van)
print(""#parameters in the autoregressive model: %d"" % raveled_params_van.size)

from sampler import make_autoregressive_sampler, make_classical_score
sampler, log_prob_novmap = make_autoregressive_sampler(van, sp_indices, n, num_states)
log_prob = jax.vmap(log_prob_novmap, (None, 0), 0)

####################################################################################

print(""\n========== Pretraining =========="")

# Pretraining parameters for the free-fermion model.
pre_lr = 1e-3
pre_sr, pre_damping, pre_maxnorm = True, 0.001, 0.001
pre_batch = 8192

freefermion_path = args.folder + ""freefermion/pretraining/"" \
                + ""n_%d_dim_%d_Theta_%f_Emax_%d/"" % (n, dim, args.Theta, args.Emax) \
                + ""nlayers_%d_modelsize_%d_nheads_%d_nhidden_%d"" % \
                    (args.nlayers, args.modelsize, args.nheads, args.nhidden) \
                + (""_damping_%.5f_maxnorm_%.5f"" % (pre_damping, pre_maxnorm)
                    if pre_sr else ""_lr_%.3f"" % pre_lr) \
                + ""_batch_%d"" % pre_batch

import os
if not os.path.isdir(freefermion_path):
    os.makedirs(freefermion_path)
    print(""Create freefermion directory: %s"" % freefermion_path)

import checkpoint
pretrained_model_filename = checkpoint.pretrained_model_filename(freefermion_path)
if os.path.isfile(pretrained_model_filename):
    print(""Load pretrained free-fermion model parameters from file: %s"" % pretrained_model_filename)
    params_van = checkpoint.load_data(pretrained_model_filename)
else:
    print(""No pretrained free-fermion model found. Initialize parameters from scratch..."")
    from freefermion.pretraining import pretrain
    params_van = pretrain(van, params_van,
                          n, dim, args.Theta, args.Emax,
                          freefermion_path, key,
                          pre_lr, pre_sr, pre_damping, pre_maxnorm,
                          pre_batch, epoch=5000)
    print(""Initialization done. Save the model to file: %s"" % pretrained_model_filename)
    checkpoint.save_data(params_van, pretrained_model_filename)

####################################################################################

print(""\n========== Initialize normalizing flow =========="")

from flow import FermiNet
def flow_fn(x):
    model = FermiNet(args.depth, args.spsize, args.tpsize, L)
    return model(x)
flow = hk.transform(flow_fn)
x_dummy = jax.random.uniform(key, (n, dim), minval=0., maxval=L)
params_flow = flow.init(key, x_dummy)

raveled_params_flow, _ = ravel_pytree(params_flow)
print(""#parameters in the flow model: %d"" % raveled_params_flow.size)

from logpsi import make_logpsi, make_logphi_logjacdet, make_logpsi_grad_laplacian, \
                   make_logp, make_quantum_score
logpsi_novmap = make_logpsi(flow, sp_indices, L)
logphi, logjacdet = make_logphi_logjacdet(flow, sp_indices, L)
logp = make_logp(logpsi_novmap)

####################################################################################

print(""\n========== Initialize relevant quantities for Ewald summation =========="")

from potential import kpoints, Madelung
G = kpoints(dim, args.Gmax)
Vconst = n * args.rs/L * Madelung(dim, args.kappa, G)
print(""(scaled) Vconst:"", Vconst/(n*args.rs/L))

####################################################################################

print(""\n========== Initialize optimizer =========="")

import optax
if args.sr:
    classical_score_fn = make_classical_score(log_prob_novmap)
    quantum_score_fn = make_quantum_score(logpsi_novmap)
    from sr import hybrid_fisher_sr
    fishers_fn, optimizer = hybrid_fisher_sr(classical_score_fn, quantum_score_fn,
            args.damping, args.max_norm)
    print(""Optimizer hybrid_fisher_sr: damping = %.5f, max_norm = %.5f."" %
            (args.damping, args.max_norm))
else:
    optimizer = optax.adam(args.lr)
    print(""Optimizer adam: lr = %.3f."" % args.lr)

####################################################################################

print(""\n========== Checkpointing =========="")

from utils import shard, replicate

path = args.folder + ""n_%d_dim_%d_rs_%f_Theta_%f"" % (n, dim, args.rs, args.Theta) \
                   + ""_Emax_%d"" % args.Emax \
                   + ""_nlayers_%d_modelsize_%d_nheads_%d_nhidden_%d"" % \
                      (args.nlayers, args.modelsize, args.nheads, args.nhidden) \
                   + ""_depth_%d_spsize_%d_tpsize_%d"" % \
                      (args.depth, args.spsize, args.tpsize) \
                   + ""_Gmax_%d_kappa_%d"" % (args.Gmax, args.kappa) \
                   + ""_mctherm_%d_mcsteps_%d_mcstddev_%.2f"" % (args.mc_therm, args.mc_steps, args.mc_stddev) \
                   + (""_hutchinson"" if args.hutchinson else """") \
                   + (""_damping_%.5f_maxnorm_%.5f"" % (args.damping, args.max_norm)
                        if args.sr else ""_lr_%.3f"" % args.lr) \
                   + ""_batch_%d_ndevices_%d_accsteps_%d"" % (args.batch, args.num_devices, args.acc_steps)
if not os.path.isdir(path):
    os.makedirs(path)
    print(""Create directory: %s"" % path)
load_ckpt_filename = checkpoint.ckpt_filename(args.epoch_finished, path)

num_devices = args.num_devices
print(""Number of GPU devices:"", num_devices)
if num_devices != jax.device_count():
    raise ValueError(""Expected %d GPU devices. Got %d."" % (num_devices, jax.device_count()))

from VMC import sample_stateindices_and_x

if os.path.isfile(load_ckpt_filename):
    print(""Load checkpoint file: %s"" % load_ckpt_filename)
    ckpt = checkpoint.load_data(load_ckpt_filename)
    keys, x, params_van, params_flow, opt_state = \
        ckpt[""keys""], ckpt[""x""], ckpt[""params_van""], ckpt[""params_flow""], ckpt[""opt_state""]
    x, keys = shard(x), shard(keys)
    params_van, params_flow = replicate((params_van, params_flow), num_devices)
else:
    print(""No checkpoint file found. Start from scratch."")

    opt_state = optimizer.init((params_van, params_flow))

    print(""Initialize key and coordinate samples..."")

    if args.batch % num_devices != 0:
        raise ValueError(""Batch size must be divisible by the number of GPU devices. ""
                         ""Got batch = %d for %d devices now."" % (args.batch, num_devices))
    batch_per_device = args.batch // num_devices

    x = jax.random.uniform(key, (num_devices, batch_per_device, n, dim), minval=0., maxval=L)
    keys = jax.random.split(key, num_devices)
    x, keys = shard(x), shard(keys)
    params_van, params_flow = replicate((params_van, params_flow), num_devices)

    for i in range(args.mc_therm):
        print(""---- thermal step %d ----"" % (i+1))
        keys, _, x, accept_rate = sample_stateindices_and_x(keys,
                                   sampler, params_van,
                                   logp, x, params_flow,
                                   args.mc_steps, args.mc_stddev, L)
    print(""keys shape:"", keys.shape, ""\t\ttype:"", type(keys))
    print(""x shape:"", x.shape, ""\t\ttype:"", type(x))

####################################################################################

print(""\n========== Training =========="")

logpsi, logpsi_grad_laplacian = \
        make_logpsi_grad_laplacian(logpsi_novmap, hutchinson=args.hutchinson,
                                   logphi=logphi, logjacdet=logjacdet)

from VMC import make_loss
observable_and_lossfn = make_loss(log_prob, logpsi, logpsi_grad_laplacian,
                                  args.kappa, G, L, args.rs, Vconst, beta)

from functools import partial

@partial(jax.pmap, axis_name=""p"",
        in_axes=(0, 0, None, 0, 0, 0, 0, 0, 0, 0) +
                ((0, 0, 0, None) if args.sr else (None, None, None, None)),
        out_axes=(0, 0, None, 0, 0, 0, 0) +
                ((0, 0, 0) if args.sr else (None, None, None)),
        static_broadcasted_argnums=13 if args.sr else (10, 11, 12, 13),
        donate_argnums=(3, 4))
def update(params_van, params_flow, opt_state, state_indices, x, key,
        data_acc, grads_acc, classical_score_acc, quantum_score_acc,
        classical_fisher_acc, quantum_fisher_acc, quantum_score_mean_acc, final_step):

    data, classical_lossfn, quantum_lossfn = observable_and_lossfn(
            params_van, params_flow, state_indices, x, key)

    grad_params_van, classical_score = jax.jacrev(classical_lossfn)(params_van)
    grad_params_flow, quantum_score = jax.jacrev(quantum_lossfn)(params_flow)
    grads = grad_params_van, grad_params_flow
    grads, classical_score, quantum_score = jax.lax.pmean((grads, classical_score, quantum_score), axis_name=""p"")
    data_acc, grads_acc, classical_score_acc, quantum_score_acc = jax.tree_multimap(lambda acc, i: acc + i, 
                                        (data_acc, grads_acc, classical_score_acc, quantum_score_acc),
                                        (data, grads, classical_score, quantum_score))

    if args.sr:
        classical_fisher, quantum_fisher, quantum_score_mean = fishers_fn(params_van, params_flow, state_indices, x)
        classical_fisher_acc += classical_fisher
        quantum_fisher_acc += quantum_fisher
        quantum_score_mean_acc += quantum_score_mean

    if final_step:
        data_acc, grads_acc, classical_score_acc, quantum_score_acc = jax.tree_map(lambda acc: acc / args.acc_steps,
                                            (data_acc, grads_acc, classical_score_acc, quantum_score_acc))
        grad_params_van, grad_params_flow = grads_acc
        grad_params_van = jax.tree_multimap(lambda grad, classical_score: grad - data_acc[""F_mean""] * classical_score,
                                            grad_params_van, classical_score_acc)
        grad_params_flow = jax.tree_multimap(lambda grad, quantum_score: grad - data_acc[""E_mean""] * quantum_score,
                                            grad_params_flow, quantum_score_acc)
        grads_acc = grad_params_van, grad_params_flow

        if args.sr:
            classical_fisher_acc /= args.acc_steps
            quantum_fisher_acc /= args.acc_steps
            quantum_score_mean_acc /= args.acc_steps
        updates, opt_state = optimizer.update(grads_acc, opt_state,
                                params=(classical_fisher_acc, quantum_fisher_acc, quantum_score_mean_acc) if args.sr else None)
        params_van, params_flow = optax.apply_updates((params_van, params_flow), updates)

    return params_van, params_flow, opt_state, data_acc, grads_acc, classical_score_acc, quantum_score_acc, \
            classical_fisher_acc, quantum_fisher_acc, quantum_score_mean_acc

log_filename = os.path.join(path, ""data.txt"")
f = open(log_filename, ""w"" if args.epoch_finished == 0 else ""a"",
            buffering=1, newline=""\n"")

for i in range(args.epoch_finished + 1, args.epoch + 1):

    data_acc = replicate({""F_mean"": 0., ""F2_mean"": 0.,
                          ""E_mean"": 0., ""E2_mean"": 0.,
                          ""K_mean"": 0., ""K2_mean"": 0.,
                          ""V_mean"": 0., ""V2_mean"": 0.,
                          ""S_mean"": 0., ""S2_mean"": 0.,
                         }, num_devices)
    grads_acc = shard( jax.tree_map(jnp.zeros_like, (params_van, params_flow)) )
    classical_score_acc, quantum_score_acc = shard( jax.tree_map(jnp.zeros_like, (params_van, params_flow)) )
    if args.sr:
        classical_fisher_acc = replicate(jnp.zeros((raveled_params_van.size, raveled_params_van.size)), num_devices)
        quantum_fisher_acc = replicate(jnp.zeros((raveled_params_flow.size, raveled_params_flow.size)), num_devices)
        quantum_score_mean_acc = replicate(jnp.zeros(raveled_params_flow.size), num_devices)
    else:
        classical_fisher_acc = quantum_fisher_acc = quantum_score_mean_acc = None
    accept_rate_acc = shard(jnp.zeros(num_devices))

    for acc in range(args.acc_steps):
        keys, state_indices, x, accept_rate = sample_stateindices_and_x(keys,
                                               sampler, params_van,
                                               logp, x, params_flow,
                                               args.mc_steps, args.mc_stddev, L)
        accept_rate_acc += accept_rate
        final_step = (acc == (args.acc_steps-1))

        params_van, params_flow, opt_state, data_acc, grads_acc, classical_score_acc, quantum_score_acc, \
        classical_fisher_acc, quantum_fisher_acc, quantum_score_mean_acc \
            = update(params_van, params_flow, opt_state, state_indices, x, keys,
                     data_acc, grads_acc, classical_score_acc, quantum_score_acc,
                     classical_fisher_acc, quantum_fisher_acc, quantum_score_mean_acc, final_step)

    data = jax.tree_map(lambda x: x[0], data_acc)
    accept_rate = accept_rate_acc[0] / args.acc_steps
    F, F2_mean = data[""F_mean""], data[""F2_mean""]
    E, E2_mean = data[""E_mean""], data[""E2_mean""]
    K, K2_mean = data[""K_mean""], data[""K2_mean""]
    V, V2_mean = data[""V_mean""], data[""V2_mean""]
    S, S2_mean = data[""S_mean""], data[""S2_mean""]
    F_std = jnp.sqrt((F2_mean - F**2) / (args.batch*args.acc_steps))
    E_std = jnp.sqrt((E2_mean - E**2) / (args.batch*args.acc_steps))
    K_std = jnp.sqrt((K2_mean - K**2) / (args.batch*args.acc_steps))
    V_std = jnp.sqrt((V2_mean - V**2) / (args.batch*args.acc_steps))
    S_std = jnp.sqrt((S2_mean - S**2) / (args.batch*args.acc_steps))

    # Note the quantities with energy dimension obtained above are in units of Ry/rs^2.
    print(""iter: %04d"" % i,
            ""F:"", F/args.rs**2, ""F_std:"", F_std/args.rs**2,
            ""E:"", E/args.rs**2, ""E_std:"", E_std/args.rs**2,
            ""K:"", K/args.rs**2, ""K_std:"", K_std/args.rs**2,
            ""V:"", V/args.rs**2, ""V_std:"", V_std/args.rs**2,
            ""S:"", S, ""S_std:"", S_std,
            ""accept_rate:"", accept_rate)
    f.write( (""%6d"" + ""  %.6f""*10 + ""  %.4f"" + ""\n"") % (i,
                                                F/args.rs**2, F_std/args.rs**2,
                                                E/args.rs**2, E_std/args.rs**2,
                                                K/args.rs**2, K_std/args.rs**2,
                                                V/args.rs**2, V_std/args.rs**2,
                                                S, S_std, accept_rate) )

    if i % 100 == 0:
        ckpt = {""keys"": keys, ""x"": x,
                ""params_van"": jax.tree_map(lambda x: x[0], params_van),
                ""params_flow"": jax.tree_map(lambda x: x[0], params_flow),
                ""opt_state"": opt_state
               }
        save_ckpt_filename = checkpoint.ckpt_filename(i, path)
        checkpoint.save_data(ckpt, save_ckpt_filename)
        print(""Save checkpoint file: %s"" % save_ckpt_filename)

f.close()"
141,2201.03156,"""""""
    Second-order optimization algorithm using stochastic reconfiguration.
    The design of API signatures is in parallel with the package `optax`.
""""""
import jax
import jax.numpy as jnp
from jax.flatten_util import ravel_pytree

from optax._src import base

FisherSRState = base.EmptyState

def fisher_sr(score_fn, damping, max_norm):
    """"""
        SR for a purely classical probabilistic model, which is also known as the
    natural gradient descent in machine-learning literatures.
    """"""

    def init_fn(params):
        return FisherSRState()

    def update_fn(grads, state, params):
        """"""
            NOTE: as the computation of Fisher information metric calls for the
        Monte-Carlo sample `state_indices`, we manually place them within the
        `params` argument.
        """"""
        params, state_indices = params

        grads_raveled, grads_unravel_fn = ravel_pytree(grads)
        print(""grads.shape:"", grads_raveled.shape)

        score = score_fn(params, state_indices)
        score_raveled = jax.vmap(lambda pytree: ravel_pytree(pytree)[0])(score)
        print(""score.shape:"", score_raveled.shape)

        batch_per_device = score_raveled.shape[0]

        fisher = score_raveled.T.dot(score_raveled) / batch_per_device
        fisher += damping * jnp.eye(fisher.shape[0])
        updates_raveled = jax.scipy.linalg.solve(fisher, grads_raveled)
        #scale gradient according to gradnorm
        gnorm = jnp.sum(grads_raveled * updates_raveled)
        scale = jnp.minimum(jnp.sqrt(max_norm/gnorm), 1)
        updates_raveled *= -scale
        updates = grads_unravel_fn(updates_raveled)

        return updates, state

    return base.GradientTransformation(init_fn, update_fn)

####################################################################################

HybridFisherSRState = base.EmptyState

def hybrid_fisher_sr(classical_score_fn, quantum_score_fn, damping, max_norm):
    """"""
        Hybrid SR for both a classical probabilistic model and a set of
    quantum basis wavefunction ansatz.
    """"""

    def init_fn(params):
        return HybridFisherSRState()

    def fishers_fn(params_van, params_flow, state_indices, x):
        classical_score = classical_score_fn(params_van, state_indices)
        classical_score = jax.vmap(lambda pytree: ravel_pytree(pytree)[0])(classical_score)

        quantum_score = quantum_score_fn(x, params_flow, state_indices)
        quantum_score = jax.vmap(lambda pytree: ravel_pytree(pytree)[0])(quantum_score)
        print(""classical_score.shape:"", classical_score.shape)
        print(""quantum_score.shape:"", quantum_score.shape)
        quantum_score_mean = jax.lax.pmean(quantum_score.mean(axis=0), axis_name=""p"")

        batch_per_device = classical_score.shape[0]

        classical_fisher = jax.lax.pmean(
                    classical_score.T.dot(classical_score) / batch_per_device,
                    axis_name=""p"")
        quantum_fisher = jax.lax.pmean(
                    quantum_score.conj().T.dot(quantum_score).real / batch_per_device,
                    axis_name=""p"")

        return classical_fisher, quantum_fisher, quantum_score_mean

    def update_fn(grads, state, params):
        """"""
            NOTE: as the computation of (classical and quantum) Fisher information
        metrics calls for the Monte-Carlo sample `state_indices` and `x`, we manually
        place them within the `params` argument.
        """"""
        grad_params_van, grad_params_flow = grads
        classical_fisher, quantum_fisher, quantum_score_mean = params
        quantum_fisher -= (quantum_score_mean.conj()[:, None] * quantum_score_mean).real

        grad_params_van_raveled, params_van_unravel_fn = ravel_pytree(grad_params_van)
        grad_params_flow_raveled, params_flow_unravel_fn = ravel_pytree(grad_params_flow)
        print(""grad_params_van.shape:"", grad_params_van_raveled.shape)
        print(""grad_params_flow.shape:"", grad_params_flow_raveled.shape)


        classical_fisher += damping * jnp.eye(classical_fisher.shape[0])
        update_params_van_raveled = jax.scipy.linalg.solve(classical_fisher, grad_params_van_raveled)
        #scale gradient according to gradnorm
        gnorm = jnp.sum(grad_params_van_raveled * update_params_van_raveled)
        scale = jnp.minimum(jnp.sqrt(max_norm/gnorm), 1)
        update_params_van_raveled *= -scale
        update_params_van = params_van_unravel_fn(update_params_van_raveled)


        quantum_fisher += damping * jnp.eye(quantum_fisher.shape[0])
        update_params_flow_raveled = jax.scipy.linalg.solve(quantum_fisher, grad_params_flow_raveled)
        #scale gradient according to gradnorm
        gnorm = jnp.sum(grad_params_flow_raveled * update_params_flow_raveled)
        scale = jnp.minimum(jnp.sqrt(max_norm/gnorm), 1)
        update_params_flow_raveled *= -scale
        update_params_flow = params_flow_unravel_fn(update_params_flow_raveled)


        return (update_params_van, update_params_flow), state

    return fishers_fn, base.GradientTransformation(init_fn, update_fn)
"
142,2201.03156,"import jax
import jax.numpy as jnp

shard = jax.pmap(lambda x: x)

def replicate(pytree, num_devices):
    dummy_input = jnp.empty(num_devices)
    return jax.pmap(lambda _: pytree)(dummy_input)
"
143,2201.03156,"import jax
import jax.numpy as jnp

from slater import logslaterdet
from functools import partial

def make_logpsi(flow, sp_indices, L):

    def logpsi(x, params, state_idx):

        """"""
            Generic function that computes ln Psi_n(x) given a single-particle orbital
        `state_idx`, a set of electron coordinates `x`, and flow parameters `params`.

        INPUT:
            x: (n, dim)     state_idx: (n,), with elements being integers in [0, num_states).

        OUTPUT:
            a single complex number ln Psi_n(x), given in the form of a 2-tuple (real, imag).
        """"""

        z = flow.apply(params, None, x)
        log_phi = logslaterdet(sp_indices[state_idx], z, L)

        n, dim = x.shape
        x_flatten = x.reshape(-1)
        flow_flatten = lambda x: flow.apply(params, None, x.reshape(n, dim)).reshape(-1)
        jac = jax.jacfwd(flow_flatten)(x_flatten)
        _, logjacdet = jnp.linalg.slogdet(jac)
        return jnp.stack([log_phi.real + 0.5*logjacdet,
                          log_phi.imag])

    return logpsi

def make_logphi_logjacdet(flow, sp_indices, L):
    """"""
        The same functionality as `make_logpsi`, but the two terms involving the base
    wavefunction and the jacobian determinant are separated.
    """"""
    def logphi(x, params, state_idx):
        z = flow.apply(params, None, x)
        log_phi = logslaterdet(sp_indices[state_idx], z, L)
        return jnp.stack([log_phi.real, log_phi.imag])

    def logjacdet(x, params):
        n, dim = x.shape
        x_flatten = x.reshape(-1)
        flow_flatten = lambda x: flow.apply(params, None, x.reshape(n, dim)).reshape(-1)
        jac = jax.jacfwd(flow_flatten)(x_flatten)
        _, logjacdet = jnp.linalg.slogdet(jac)
        return 0.5*logjacdet

    return logphi, logjacdet

def make_logpsi_grad_laplacian(logpsi, forloop=True, hutchinson=False,
                               logphi=None, logjacdet=None):

    @partial(jax.vmap, in_axes=(0, None, 0), out_axes=0)
    def logpsi_vmapped(x, params, state_idx):
        logpsix = logpsi(x, params, state_idx)
        return logpsix[0] + 1j * logpsix[1]

    @partial(jax.vmap, in_axes=(0, None, 0, None), out_axes=0)
    def logpsi_grad_laplacian(x, params, state_idx, key):
        """"""
            Computes the gradient and laplacian of logpsi w.r.t. electron coordinates x.
        The final result is in complex form.

        Relevant dimensions: (after vmapped)

        INPUT:
            x: (batch, n, dim)  state_idx: (batch, n)
        OUTPUT:
            grad: (batch, n, dim)   laplacian: (batch,)
        """"""

        grad = jax.jacrev(logpsi)(x, params, state_idx)
        grad = grad[0] + 1j * grad[1]
        print(""Computed gradient."")

        n, dim = x.shape
        x_flatten = x.reshape(-1)
        grad_logpsi = jax.jacrev(lambda x: logpsi(x.reshape(n, dim), params, state_idx))

        def _laplacian(x):
            if forloop:
                print(""forloop version..."")
                def body_fun(i, val):
                    _, tangent = jax.jvp(grad_logpsi, (x,), (eye[i],))
                    return val + tangent[0, i] + 1j * tangent[1, i]
                eye = jnp.eye(x.shape[0])
                laplacian = jax.lax.fori_loop(0, x.shape[0], body_fun, 0.+0.j)
            else:
                print(""vmap version..."")
                def body_fun(x, basevec):
                    _, tangent = jax.jvp(grad_logpsi, (x,), (basevec,))
                    return (tangent * basevec).sum(axis=-1)
                eye = jnp.eye(x.shape[0])
                laplacian = jax.vmap(body_fun, (None, 1), 1)(x, eye).sum(axis=-1)
                laplacian = laplacian[0] + 1j * laplacian[1]
            return laplacian

        laplacian = _laplacian(x_flatten)
        print(""Computed laplacian."")

        return grad, laplacian

    def logpsi_grad_laplacian_hutchinson(x, params, state_indices, key):

        v = jax.random.normal(key, x.shape)

        @partial(jax.vmap, in_axes=(0, None, 0, 0), out_axes=0)
        def logpsi_grad_random_laplacian(x, params, state_idx, v):
            """"""
                Compute the laplacian as a random variable `v^T hessian(ln Psi_n(x)) v`
            using the Hutchinson's trick.

                The argument `v` is a random ""vector"" that has the same shape as `x`,
            i.e., (after vmapped) (batch, n, dim).
            """"""

            grad, hvp = jax.jvp( jax.jacrev(lambda x: logpsi(x, params, state_idx)),
                                 (x,), (v,) )

            grad = grad[0] + 1j * grad[1]
            print(""Computed gradient."")

            random_laplacian = (hvp * v).sum(axis=(-2, -1))
            random_laplacian = random_laplacian[0] + 1j * random_laplacian[1]
            print(""Computed Hutchinson's estimator of laplacian."")

            return grad, random_laplacian

        @partial(jax.vmap, in_axes=(0, None, 0, 0), out_axes=0)
        def logpsi_grad_random_logjacdet(x, params, state_idx, v):
            grad_logphi = jax.jacrev(logphi)(x, params, state_idx)
            grad_logphi = grad_logphi[0] + 1j * grad_logphi[1]
            grad_logjacdet, hvp = jax.jvp( jax.grad(lambda x: logjacdet(x, params)),
                                 (x,), (v,) )
            grad = grad_logphi + grad_logjacdet
            print(""Computed gradient."")

            n, dim = x.shape
            x_flatten = x.reshape(-1)
            grad_logphi = jax.jacrev(lambda x: logphi(x.reshape(n, dim), params, state_idx))

            def _laplacian(x):
                if forloop:
                    print(""forloop version..."")
                    def body_fun(i, val):
                        _, tangent = jax.jvp(grad_logphi, (x,), (eye[i],))
                        return val + tangent[0, i] + 1j * tangent[1, i]
                    eye = jnp.eye(x.shape[0])
                    laplacian = jax.lax.fori_loop(0, x.shape[0], body_fun, 0.+0.j)
                return laplacian

            laplacian_logphi = _laplacian(x_flatten)
            print(""Computed exact laplacian of logphi."")

            random_logjacdet = (hvp * v).sum(axis=(-2, -1))
            print(""Computed Hutchinson's estimator of logjacdet."")
            laplacian = laplacian_logphi + random_logjacdet

            return grad, laplacian

        logpsi_grad_laplacian = logpsi_grad_random_laplacian \
                                if (logphi is None and logjacdet is None) else \
                                logpsi_grad_random_logjacdet
        return logpsi_grad_laplacian(x, params, state_indices, v)

    return logpsi_vmapped, \
           (logpsi_grad_laplacian_hutchinson if hutchinson else logpsi_grad_laplacian)

def make_logp(logpsi):

    @partial(jax.vmap, in_axes=(0, None, 0), out_axes=0)
    def logp(x, params, state_idx):
        """""" logp = logpsi + logpsi* = 2 Re logpsi """"""
        return 2 * logpsi(x, params, state_idx)[0]

    return logp

def make_quantum_score(logpsi):

    @partial(jax.vmap, in_axes=(0, None, 0), out_axes=0)
    def quantum_score_fn(x, params, state_idx):
        """"""
            Computes the ""quantum score function"", i.e., the gradient of ln Psi_n(x)
        w.r.t. the flow parameters.
            This function can be useful for stochastic reconfiguraton, the
        second-order optimization algorithm based on quantum (as well as classical)
        Fisher information matrix.

        Relevant dimension: (after vmapped)

        OUTPUT:
            a pytree of the same structure as `params`, in which each leaf node has
        an additional leading batch dimension.
        """"""
        grad_params = jax.jacrev(logpsi, argnums=1)(x, params, state_idx)
        return jax.tree_map(lambda jac: jac[0] + 1j * jac[1], grad_params)

    return quantum_score_fn
"
144,2201.03156,"import pickle
import os

def pretrained_model_filename(freefermion_path):
    return os.path.join(freefermion_path, ""params_van.pkl"")

def ckpt_filename(epoch, path):
    return os.path.join(path, ""epoch_%06d.pkl"" % epoch)

def load_data(filename):
    with open(filename, ""rb"") as f:
        data = pickle.load(f)
    return data

def save_data(data, filename):
    with open(filename, ""wb"") as f:
        pickle.dump(data, f)
"
145,2201.03156,"import numpy as np 

def subsets(k, Pmax, Ps):
    """"""
        Given a set of several items with ""prices"" specified by the list Ps, find
    all subsets of length k whose total price do not exceed Pmax.
    """"""
    Nelements = len(Ps)
    result = ( ((), 0), )
    for i in range(1, k+1):
        result_new = []
        for subset, Ptotal in result:
            next_idx = subset[-1] + 1 if subset else 0
            while (next_idx + k - i < Nelements):
                if sum(Ps[next_idx:next_idx+k-i+1]) <= Pmax - Ptotal:
                    result_new.append( (subset + (next_idx,), Ptotal + Ps[next_idx]) )
                next_idx += 1
        result = tuple(result_new)
    indices, Ptotals = zip( *sorted(result, key=lambda index_P: index_P[1]) )
    return indices, Ptotals

def sp_orbitals(dim, Emax=60):
    """"""
        Compute index (n_1, ..., n_dim) and corresponding energy n_1^2 + ... + n_dim^2
    of all single-particle plane wave in spatial dimension `dim` whose energy
    does not exceed `Emax`.

    OUTPUT SHAPES:
        indices: (n_orbitals, dim), Es: (n_orbitals)
        (n_orbitals stands for total number of single-particle plane wave orbitals
    that fulfil the criteria.)
    """"""
    n_max = int(np.floor(np.sqrt(Emax)))
    indices = []
    Es = []
    if dim == 2:
        for nx in range(-n_max, n_max+1):
            for ny in range(-n_max, n_max+1):
                E = nx**2 + ny**2
                if E <= Emax:
                    indices.append((nx, ny))
                    Es.append(E)
    elif dim == 3:
        for nx in range(-n_max, n_max+1):
            for ny in range(-n_max, n_max+1):
                for nz in range(-n_max, n_max+1):
                    E = nx**2 + ny**2 + nz**2
                    if E <= Emax:
                        indices.append((nx, ny, nz))
                        Es.append(E)
    indices, Es = np.array(indices), np.array(Es)
    sort_idx = Es.argsort()
    indices, Es = indices[sort_idx], Es[sort_idx]
    return indices, Es

def manybody_orbitals(n, dim, Ecut):
    """"""
        Compute the many-body plane-wave indices of `n` (spinless) fermions
    in spatial dimension `dim` whose total energy does not exceed E0 + `Ecut`,
    where E0 is the ground-state energy.

    OUTPUT SHAPES:
        manybody_indices: (n_manybody_states, n, dim)
        manybody_Es: (n_manybody_states,)
        (n_manybody_states stands for total number of many-body states of `n` fermions
    that fulfil the energy cutoff criteria.)
    """"""
    indices, Es = sp_orbitals(dim)
    manybody_E0 = Es[:n].sum()
    manybody_indices, manybody_Es = subsets(n, manybody_E0 + Ecut, list(Es))
    manybody_indices, manybody_Es = np.array(manybody_indices), np.array(manybody_Es)

    # indices.shape: (n_orbitals, dim);
    # manybody_indices.shape: (n_manybody_states, n)
    # --> indices[manybody_indices, :].shape: (n_manybody_states, n, dim)
    manybody_indices = indices[manybody_indices, :]

    return manybody_indices, manybody_Es

if __name__ == ""__main__"":
    for dim in (2, 3):
        indices, Es = sp_orbitals(dim)
        print(""Es:"", Es, Es.shape)
        #print(""indices:\n"", indices, indices.shape)

        print(""---- Closed-shell (spinless) electron numbers in dim = %d ----"" % dim)
        Ef = Es[0]
        for i in range(Es.size):
            if (Es[i] != Ef):
                print(""n = %d, Ef = %d"" % (i, Ef))
                Ef = Es[i]
        print(""n = %d, Ef = %d"" % (Es.size, Es[-1]))

    n, dim = 7, 3
    print(""---- %d (spinless) electrons in dim = %d ----"" % (n, dim))
    for Ecut in range(4):
        _, manybody_Es = manybody_orbitals(n, dim, Ecut)
        print(""Ecut = %d: number of many-body states = %6d"" % (Ecut, manybody_Es.size))

    n, dim = 33, 3
    print(""---- %d (spinless) electrons in dim = %d ----"" % (n, dim))
    for Ecut in range(3):
        _, manybody_Es = manybody_orbitals(n, dim, Ecut)
        print(""Ecut = %d: number of many-body states = %6d"" % (Ecut, manybody_Es.size))

    n, dim = 13, 2
    print(""---- %d (spinless) electrons in dim = %d ----"" % (n, dim))
    for Ecut in range(9):
        _, manybody_Es = manybody_orbitals(n, dim, Ecut)
        print(""Ecut = %d: number of many-body states = %6d"" % (Ecut, manybody_Es.size))

    n, dim = 25, 2
    print(""---- %d (spinless) electrons in dim = %d ----"" % (n, dim))
    for Ecut in range(9):
        _, manybody_Es = manybody_orbitals(n, dim, Ecut)
        print(""Ecut = %d: number of many-body states = %6d"" % (Ecut, manybody_Es.size))
"
146,2201.03156,"import jax
import jax.numpy as jnp

from functools import partial

@partial(jax.jit, static_argnums=0)
def mcmc(logp_fn, x_init, key, mc_steps, mc_stddev=0.02):
    """"""
        Markov Chain Monte Carlo sampling algorithm.

    INPUT:
        logp_fn: callable that evaluate log-probability of a batch of configuration x.
            The signature is logp_fn(x), where x has shape (batch, n, dim).
        x_init: initial value of x, with shape (batch, n, dim).
        key: initial PRNG key.
        mc_steps: total number of Monte Carlo steps.
        mc_stddev: standard deviation of the Gaussian proposal.

    OUTPUT:
        x: resulting batch samples, with the same shape as `x_init`.
    """"""
    def step(i, state):
        x, logp, key, num_accepts = state
        key, key_proposal, key_accept = jax.random.split(key, 3)

        x_proposal = x + mc_stddev * jax.random.normal(key_proposal, x.shape)
        logp_proposal = logp_fn(x_proposal)
        ratio = jnp.exp(logp_proposal - logp)
        accept = jax.random.uniform(key_accept, ratio.shape) < ratio

        x_new = jnp.where(accept[:, None, None], x_proposal, x)
        logp_new = jnp.where(accept, logp_proposal, logp)
        num_accepts += accept.sum()
        return x_new, logp_new, key, num_accepts
    
    logp_init = logp_fn(x_init)
    x, logp, key, num_accepts = jax.lax.fori_loop(0, mc_steps, step, (x_init, logp_init, key, 0.))
    batch = x.shape[0]
    accept_rate = jax.lax.pmean(num_accepts / (mc_steps * batch), axis_name=""p"")
    return x, accept_rate
"
147,2201.03156,"import jax
import jax.numpy as jnp

from functools import partial

from MCMC import mcmc

@partial(jax.pmap, axis_name=""p"",
                   in_axes=(0, None, 0, None, 0, 0, None, None, None),
                   static_broadcasted_argnums=(1, 3),
                   donate_argnums=4)
def sample_stateindices_and_x(key,
                              sampler, params_van,
                              logp, x, params_flow,
                              mc_steps, mc_stddev, L):
    """"""
        Generate new state_indices of shape (batch, n), as well as coordinate sample
    of shape (batch, n, dim), from the sample of last optimization step.
    """"""
    key, key_state, key_MCMC = jax.random.split(key, 3)
    batch = x.shape[0]
    state_indices = sampler(params_van, key_state, batch)
    x, accept_rate = mcmc(lambda x: logp(x, params_flow, state_indices), x, key_MCMC, mc_steps, mc_stddev)
    x -= L * jnp.floor(x/L)
    return key, state_indices, x, accept_rate

####################################################################################

from potential import potential_energy

def make_loss(log_prob, logpsi, logpsi_grad_laplacian, kappa, G, L, rs, Vconst, beta):

    def observable_and_lossfn(params_van, params_flow, state_indices, x, key):
        logp_states = log_prob(params_van, state_indices)
        grad, laplacian = logpsi_grad_laplacian(x, params_flow, state_indices, key)
        print(""grad.shape:"", grad.shape)
        print(""laplacian.shape:"", laplacian.shape)

        kinetic = -laplacian - (grad**2).sum(axis=(-2, -1))
        potential = potential_energy(x, kappa, G, L, rs) + Vconst
        Eloc = kinetic + potential
        Floc = logp_states / beta + Eloc.real

        K_mean, K2_mean, V_mean, V2_mean, \
        E_mean, E2_mean, F_mean, F2_mean, S_mean, S2_mean = \
        jax.tree_map(lambda x: jax.lax.pmean(x, axis_name=""p""), 
                     (kinetic.real.mean(), (kinetic.real**2).mean(),
                      potential.mean(), (potential**2).mean(),
                      Eloc.real.mean(), (Eloc.real**2).mean(),
                      Floc.mean(), (Floc**2).mean(),
                      -logp_states.mean(), (logp_states**2).mean()
                     )
                    )
        observable = {""K_mean"": K_mean, ""K2_mean"": K2_mean,
                      ""V_mean"": V_mean, ""V2_mean"": V2_mean,
                      ""E_mean"": E_mean, ""E2_mean"": E2_mean,
                      ""F_mean"": F_mean, ""F2_mean"": F2_mean,
                      ""S_mean"": S_mean, ""S2_mean"": S2_mean}

        def classical_lossfn(params_van):
            logp_states = log_prob(params_van, state_indices)

            tv = jax.lax.pmean(jnp.abs(Floc - F_mean).mean(), axis_name=""p"")
            Floc_clipped = jnp.clip(Floc, F_mean - 5.0*tv, F_mean + 5.0*tv)
            gradF_phi = (logp_states * Floc_clipped).mean()
            classical_score = logp_states.mean()
            return gradF_phi, classical_score

        def quantum_lossfn(params_flow):
            logpsix = logpsi(x, params_flow, state_indices)

            tv = jax.lax.pmean(jnp.abs(Eloc - E_mean).mean(), axis_name=""p"")
            Eloc_clipped = jnp.clip(Eloc, E_mean - 5.0*tv, E_mean + 5.0*tv)
            gradF_theta = 2 * (logpsix * Eloc_clipped.conj()).real.mean()
            quantum_score = 2 * logpsix.real.mean()
            return gradF_theta, quantum_score

        return observable, classical_lossfn, quantum_lossfn

    return observable_and_lossfn
"
148,2201.03156,"""""""
    Autoregressive model of momenta occupation with fixed particle number.

    The present implementation is based on a Transformer composed of causal self-attention layers.

    Adapted from https://github.com/deepmind/dm-haiku/blob/main/examples/transformer/model.py
""""""

import jax
import jax.numpy as jnp
import haiku as hk

from typing import Optional

class CausalSelfAttention(hk.MultiHeadAttention):
    """"""Self attention with a causal mask applied.""""""
    
    def __call__(self,
                 query: jnp.ndarray,
                 key: Optional[jnp.ndarray] = None,
                 value: Optional[jnp.ndarray] = None,
                ) -> jnp.ndarray:

        key = key if key is not None else query
        value = value if value is not None else query

        seq_len = query.shape[0]
        mask = jnp.tril(jnp.ones((seq_len, seq_len)))[None, ...]

        return super().__call__(query, key, value, mask)

class DenseBlock(hk.Module):
    """"""A 2-layer MLP which widens then narrows the input.""""""

    def __init__(self,
                 hidden_size: int,
                 init_scale: float,
                 name: Optional[str] = None):
        super().__init__(name=name)
        self.hidden_size = hidden_size
        self.init_scale = init_scale

    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        size = x.shape[-1]
        initializer = hk.initializers.VarianceScaling(self.init_scale)
        x = hk.Linear(self.hidden_size, w_init=initializer)(x)
        x = jnp.tanh(x)
        return hk.Linear(size, w_init=initializer)(x)

class Transformer(hk.Module):

    def __init__(self,
                 output_size: int,
                 num_layers: int, model_size: int, num_heads: int,
                 hidden_size: int,
                 name: Optional[str] = None):
        super().__init__(name=name)
        self.output_size = output_size

        self.num_layers, self.model_size, self.num_heads = \
                num_layers, model_size, num_heads
        if model_size % num_heads != 0:
            raise ValueError(""Model_size of the transformer must be divisible ""
                    ""by the number of heads. Got model_size=%d and num_heads=%d.""
                    % (model_size, num_heads))
        self.key_size = model_size // num_heads

        self.hidden_size = hidden_size

    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:
        init_scale = 0.02 / self.num_layers

        x = hk.Linear(self.model_size,
                      w_init=hk.initializers.VarianceScaling(init_scale, ""fan_out""),
                      name=""embedding_mlp"")(x)
        x = jnp.tanh(x)

        for i in range(self.num_layers):
            x_attn = CausalSelfAttention(self.num_heads,
                                         self.key_size,
                                         init_scale,
                                         name=f""layer{i}_attn"")(x)
            x = x + x_attn
            x_dense = DenseBlock(self.hidden_size, init_scale, name=f""layer{i}_mlp"")(x)
            x = x + x_dense

        x = jnp.tanh(x)
        x = hk.Linear(self.output_size,
                      w_init=hk.initializers.VarianceScaling(init_scale),
                      name=""output_mlp"")(x)

        x1init = hk.initializers.TruncatedNormal(jnp.sqrt(init_scale / self.output_size))
        x1hat = hk.get_parameter(""x1hat"", shape=(self.output_size,), init=x1init)
        x = jnp.vstack((x1hat, x[:-1]))

        return x
"
149,2201.03156,"import jax
import jax.numpy as jnp

def logslaterdet0(indices, x, L):
    """"""
        Compute the logarithm of the slater determinant of several plane-wave
    orbitals ln det(phi_j(r_i)), where phi_j(r_i) = 1/sqrt(L^dim) e^(i 2pi/L n_j r_i).

    INPUT SHAPES:
        indices: (n, dim) (array of integers)
        x: (n, dim)
    """"""
    #print(""Call here!!!"")
    k = 2*jnp.pi/L * indices
    k_dot_x = (k * x[:, None, :]).sum(axis=-1)
    _, dim = x.shape
    D = 1 / L**(dim/2) * jnp.exp(1j * k_dot_x)
    phase, logabsdet = jnp.linalg.slogdet(D)
    return logabsdet + jnp.log(phase)

logslaterdet = jax.custom_jvp(logslaterdet0)

@logslaterdet.defjvp
def logslaterdet_jvp(primals, tangents):
    """"""
        This implementation of the jvp rule makes use of specifics of slater
    determinants of plane-wave wavefunctions, thus is more efficient.
    """"""
    #print(""Call here with jvp!!!"")
    indices, x, L = primals
    _, dx, _ = tangents

    k = 2*jnp.pi/L * indices
    k_dot_x = (k * x[:, None, :]).sum(axis=-1)
    _, dim = x.shape
    D = 1 / L**(dim/2) * jnp.exp(1j * k_dot_x)

    phase, logabsdet = jnp.linalg.slogdet(D)
    primal_out = logabsdet + jnp.log(phase)

    k_dot_dx = (k * dx[:, None, :]).sum(axis=-1)
    tangent_out = (D * 1j * k_dot_dx * jnp.linalg.inv(D).T).sum()

    return primal_out, tangent_out
"
150,2201.03156,"import jax
import jax.numpy as jnp
from jax.scipy.special import erfc 

import numpy as np 

def kpoints(dim, Gmax):
    """"""
        Compute all the integer k-mesh indices (n_1, ..., n_dim) in spatial
    dimention `dim`, whose length do not exceed `Gmax`.
    """"""
    n = np.arange(-Gmax, Gmax+1)
    nis = np.meshgrid(*( [n]*dim ))
    G = np.array([ni.flatten() for ni in nis]).T
    G2 = (G**2).sum(axis=-1)
    G = G[(G2<=Gmax**2) * (G2>0)]
    return jnp.array(G)

def Madelung(dim, kappa, G):
    """"""
        The Madelung constant of a simple cubic lattice of lattice constant L=1
    in spatial dimension `dim`, namely the electrostatic potential experienced by
    the unit charge at a lattice site.
    """"""
    Gnorm = jnp.linalg.norm(G, axis=-1)

    if dim == 3:
        g_k = jnp.exp(-jnp.pi**2 * Gnorm**2 / kappa**2) / (jnp.pi * Gnorm**2)
        g_0 = -jnp.pi / kappa**2
    elif dim == 2:
        g_k = erfc(jnp.pi * Gnorm / kappa) / Gnorm
        g_0 = -2 * jnp.sqrt(jnp.pi) / kappa

    return g_k.sum() + g_0 - 2*kappa/jnp.sqrt(jnp.pi)

def psi(x, kappa, G):
    """"""
        The electron coordinate-dependent part 1/2 \sum_{i}\sum_{j neq i} psi(r_i, r_j)
    of the electrostatic energy (per cell) for a periodic system of lattice constant L=1.
        NOTE: to account for the Madelung part `Vconst` returned by the function
    `Madelung`, add the term 0.5*n*Vconst.
    """"""
    n, dim = x.shape

    i, j = jnp.triu_indices(n, k=1)
    rij = (x[:, None, :] - x)[i, j]
    rij -= jnp.rint(rij)
    # Only the nearest neighbor is taken into account in the present implementation of real-space summation.
    dij = jnp.linalg.norm(rij, axis=-1)
    V_shortrange = (erfc(kappa * dij) / dij).sum()

    Gnorm = jnp.linalg.norm(G, axis=-1)

    if dim == 3:
        g_k = jnp.exp(-jnp.pi**2 * Gnorm**2 / kappa**2) / (jnp.pi * Gnorm**2)
        g_0 = -jnp.pi / kappa**2
    elif dim == 2:
        g_k = erfc(jnp.pi * Gnorm / kappa) / Gnorm
        g_0 = -2 * jnp.sqrt(jnp.pi) / kappa

    V_longrange = ( g_k * jnp.cos(2*jnp.pi * G.dot(rij.T)).sum(axis=-1) ).sum() \
                    + g_0 * rij.shape[0]

    potential = V_shortrange + V_longrange
    return potential

from functools import partial

@partial(jax.vmap, in_axes=(0, None, None, None, None), out_axes=0)
def potential_energy(x, kappa, G, L, rs):
    """"""
        Potential energy for a periodic box of size L, only the nontrivial
    coordinate-dependent part. Unit: Ry/rs^2.
        To account for the Madelung part `Vconst` returned by the function `Madelung`,
    add the term n*rs/L*Vconst. See also the docstring for function `psi`.
    """"""
    return 2*rs/L * psi(x/L, kappa, G)
"
151,2201.03156,"import jax
import jax.numpy as jnp

from functools import partial

def make_autoregressive_sampler(network, sp_indices, n, num_states, mask_fn=False):

    def _mask(state_idx):
        mask = jnp.tril(jnp.ones((n, num_states)), k=num_states-n)
        idx_lb = jnp.concatenate( (jnp.array([-1]), state_idx[:-1]) )
        mask = jnp.where(jnp.arange(num_states) > idx_lb[:, None], mask, 0.)
        return mask

    def _logits(params, state_idx):
        """"""
            Given occupation state indices `state_idx` of the electrons, compute
        the masked logits for the family of autoregressive conditional probabilities.

        Relevant dimensions: (before vmapped)

        INPUT:
            state_idx: (n,), with elements being integers in [0, num_states).
        OUTPUT:
            masked_logits: (n, num_states)
        """"""

        logits = network.apply(params, None, sp_indices[state_idx])
        mask = _mask(state_idx)
        masked_logits = jnp.where(mask, logits, -1e50)
        return masked_logits

    def sampler(params, key, batch):
        state_indices = jnp.zeros((batch, n), dtype=jnp.int32)
        for i in range(n):
            key, subkey = jax.random.split(key)
            # logits.shape: (batch, n, num_states)
            logits = jax.vmap(_logits, (None, 0), 0)(params, state_indices)
            state_indices = state_indices.at[:, i].set(
                            jax.random.categorical(subkey, logits[:, i, :], axis=-1))
        return state_indices

    def log_prob(params, sample):
        logits = _logits(params, sample)
        logp = jax.nn.log_softmax(logits, axis=-1)
        logp = logp[jnp.arange(n), sample].sum()
        return logp

    if mask_fn:
        # Return the function `_mask` only for test and illustration purpose.
        return _mask, sampler, log_prob
    else:
        return sampler, log_prob


""""""
    Classical score function: params, sample -> score
    This function can be useful for stochastic reconfiguration, the second-order
optimization algorithm based on classical Fisher information matrix.

Relevant dimension: (after vmapped)

INPUT:
    params: a pytree    sample: (batch, n), with elements being integers in [0, num_states).
OUTPUT:
    a pytree of the same structure as `params`, in which each leaf node has
an additional leading batch dimension.
""""""
make_classical_score = lambda log_prob: jax.vmap(jax.grad(log_prob), (None, 0), 0)


if __name__ == ""__main__"":
    from jax.config import config
    config.update(""jax_enable_x64"", True)

    n, num_states = 4, 10
    mask_fn, _, _ = make_autoregressive_sampler(None, None, n, num_states, mask_fn=True)

    state_idx = jnp.array([1, 4, 5, 7])
    mask = mask_fn(state_idx)
    """"""
        For this example, the resulting mask is illustrated as follows:

                possible state indices
        0   1   2   3   4   5   6   7   8   9
        -------------------------------------
     1  *   *   *   *   *   *   *   0   0   0   1hat
     2  0   0   *   *   *   *   *   *   0   0   2hat(1)
     3  0   0   0   0   0   *   *   *   *   0   3hat(1, 2)
     4  0   0   0   0   0   0   *   *   *   *   4hat(1, 2, 3)

        The symbols ""*"" and ""0"" stand for allowed and prohibited states, respectively.
    """"""
    print(""mask:\n"", mask)
"
152,2201.03156,"from mpmath import mpf, mp

def thermo_quantities(dim, Theta):
    """"""
        Thermodynamic quantities of free (spinless) fermions in dimension `dim` and
    temperature `Theta`, computed in thermodynamic limit.

    OUTPUT:
        z: fugacity
        f, e, s: free energy, energy and entropy density (per particle), respectively.
            f and e are in unit Ry/rs^2.
    """"""
    d = mpf(dim)
    z = mp.findroot(lambda z: mp.gamma(d/2+1) * mp.polylog(d/2, -z) * Theta**(d/2) + 1, 10)
    epsilon_F = 4 * mp.gamma(d/2+1)**(4/d)
    e = d/2 * mp.polylog(d/2+1, -z) / mp.polylog(d/2, -z) * Theta * epsilon_F
    s = (d/2+1) * mp.polylog(d/2+1, -z) / mp.polylog(d/2, -z) - mp.log(z)
    f = e - Theta * epsilon_F * s
    return z, f, e, s

if __name__ == ""__main__"":
    import os
    dim = 2
    Thetas = mp.linspace(mpf(""0.02""), mpf(""0.60""), 59)

    path = ""/data1/xieh/CoulombGas/master/freefermion/analytic""
    filename = os.path.join(path, ""n_inf_dim_%d.txt"" % dim)
    if os.path.isfile(filename):
        print(""The freefermion data file %s already exists. Skip..."" % filename)
        exit(0)

    fp = open(filename, ""w"", buffering=1, newline=""\n"")
    fp.write(""#Theta\tf\te\ts\n"")

    for Theta in Thetas:
        _, f, e, s = thermo_quantities(dim, Theta)
        f, e, s = f.real, e.real, s.real
        fp.write( (""%s"" + ""\t%s""*3 + ""\n"") %
                    (mp.nstr(Theta),
                     mp.nstr(f), mp.nstr(e), mp.nstr(s)) )
        print(""Theta: %s\tf: %s\te: %s\ts: %s"" %
                    (mp.nstr(Theta),
                     mp.nstr(f), mp.nstr(e), mp.nstr(s)))

    fp.close()
"
153,2201.03156,"from mpmath import mpf, mp
mp.dps = 1200

def z_e(dim, L, beta, Emax=None):
    """"""
        The partition function and expected energy of a single particle in a cubic 
    box of dimension `dim` and size `L`, at inverse temperature `beta`.

        The infinite sum is truncated according to the energy cutoff `Emax`. When
    `Emax` takes the special value `None`, the infinite sum is evaluated exactly
    (to a given precision).
    """"""
    if Emax:
        from orbitals import sp_orbitals
        _, Es = sp_orbitals(dim, Emax)
        Es = [(2*mp.pi/L)**2 * E for E in Es]
        z = mp.fsum(mp.exp(-beta*E) for E in Es)
        e = mp.fsum(E*mp.exp(-beta*E) for E in Es) / z
    else:
        z_single_dim = mp.jtheta(3, 0, mp.exp(-beta * (2*mp.pi/L)**2))
        e_single_dim = mp.jtheta(3, 0, mp.exp(-beta * (2*mp.pi/L)**2), derivative=2) \
                            / (-4) * (2*mp.pi/L)**2 / z_single_dim
        z = z_single_dim**dim
        e = dim * e_single_dim
    return z, e

def Z_E(n, dim, Theta, Emax=None):
    """"""
        The partition function and relevant thermodynamic quantities of `n` free
    (spinless) fermions in dimension `dim` and temperature `Theta`, computed using
    recursion relations.

        `Theta` is measured relative to the Fermi energy corresponding to certain
    dimensionless density parameter rs. The resulting physical quantities with
    energy dimension, such as the expected energy `E` and free energy `F`, have
    unit Ry/rs^2.

        The argument `Emax` determine the energy cutoff used in evaluating the
    single-particle partition function. See function ""z_e"" for details.
    """"""
    if dim == 3:
        L = (mpf(""4/3"") * mp.pi * n)**mpf(""1/3"")
        beta = 1 / ((mpf(""4.5"") * mp.pi)**mpf(""2/3"") * Theta)
    elif dim == 2:
        L = mp.sqrt(mp.pi*n)
        beta = 1 / (4 * Theta)

    #print(""L:"", L, ""\nbeta:"", beta)

    zs, es = tuple(zip( *[z_e(dim, L, k*beta, Emax) for k in range(1, n+1)] )) 
    #print(""zs:"", zs)
    #print(""es:"", es)

    Zs = [mpf(1)]
    Es = [mpf(0)]
    for N in range(1, n+1):
        Z = mp.fsum( (-1)**(k-1) * zs[k-1] * Zs[N-k]
                     for k in range(1, N+1)
                   ) / N
        E = mp.fsum( (-1)**(k-1) * zs[k-1] * Zs[N-k] * (k * es[k-1] + Es[N-k])
                     for k in range(1, N+1)
                   ) / N / Z
        Zs.append(Z)
        Es.append(E)
    #print(""Zs:"", Zs)

    F = -mp.log(Zs[-1])/beta
    E = Es[-1]
    S = beta*(E - F)
    return F, E, S

if __name__ == ""__main__"":
    n, dim, Theta = 37, 2, mpf(""0.15"")
    print(""---- n = %d, dim = %d, Theta = %s ----"" % (n, dim, mp.nstr(Theta)))
    for Emax in [25, 36, None]:
        F, E, S = Z_E(n, dim, Theta, Emax=Emax)
        print(""Emax:"", Emax, ""\nF:"", F, ""\nE:"", E, ""\nS:"", S)

    n, dim, Theta = 49, 2, mpf(""0.15"")
    print(""---- n = %d, dim = %d, Theta = %s ----"" % (n, dim, mp.nstr(Theta)))
    for Emax in [36, 49, None]:
        F, E, S = Z_E(n, dim, Theta, Emax=Emax)
        print(""Emax:"", Emax, ""\nF:"", F, ""\nE:"", E, ""\nS:"", S)

    n, dim, Theta = 57, 2, mpf(""0.15"")
    print(""---- n = %d, dim = %d, Theta = %s ----"" % (n, dim, mp.nstr(Theta)))
    for Emax in [49, 64, None]:
        F, E, S = Z_E(n, dim, Theta, Emax=Emax)
        print(""Emax:"", Emax, ""\nF:"", F, ""\nE:"", E, ""\nS:"", S)
"
154,2201.03156,"import jax
from jax.config import config
config.update(""jax_enable_x64"", True)
import jax.numpy as jnp

from sampler import make_autoregressive_sampler

def make_loss(log_prob, Es, beta):
    
    def loss_fn(params, state_indices):
        logp = log_prob(params, state_indices)
        E = Es[state_indices].sum(axis=-1)
        F = jax.lax.stop_gradient(logp / beta + E)

        E_mean = E.mean()
        F_mean = F.mean()
        S_mean = -logp.mean()
        E_std = E.std()
        F_std = F.std()
        S_std = (-logp).std()

        gradF = (logp * (F - F_mean)).mean()

        auxiliary_data = {""E_mean"": E_mean, ""E_std"": E_std,
                          ""F_mean"": F_mean, ""F_std"": F_std,
                          ""S_mean"": S_mean, ""S_std"": S_std,
                         }

        return gradF, auxiliary_data

    return loss_fn

def pretrain(van, params_van,
             n, dim, Theta, Emax,
             path, key,
             lr, sr, damping, max_norm,
             batch, epoch=10000):

    # We recompute the relevant system parameters here for convenience.
    if dim == 3:
        L = (4/3*jnp.pi*n)**(1/3)
        beta = 1 / ((4.5*jnp.pi)**(2/3) * Theta)
    elif dim == 2:
        L = jnp.sqrt(jnp.pi*n)
        beta = 1/ (4 * Theta)

    from orbitals import sp_orbitals
    sp_indices, Es = sp_orbitals(dim, Emax)
    sp_indices = jnp.array(sp_indices[::-1])
    Es = (2*jnp.pi/L)**2 * jnp.array(Es[::-1])

    from mpmath import mpf, mp
    from freefermion.analytic import Z_E
    F, E, S = Z_E(n, dim, mpf(str(Theta)), Emax)
    print(""Analytic results for the thermodynamic quantities: ""
            ""F: %s, E: %s, S: %s"" % (mp.nstr(F), mp.nstr(E), mp.nstr(S)))

    num_states = Es.size
    sampler, log_prob_novmap = make_autoregressive_sampler(van, sp_indices, n, num_states)
    log_prob = jax.vmap(log_prob_novmap, (None, 0), 0)

    loss_fn = make_loss(log_prob, Es, beta)

    import optax
    if sr:
        from sampler import make_classical_score
        score_fn = make_classical_score(log_prob_novmap)
        from sr import fisher_sr
        optimizer = fisher_sr(score_fn, damping, max_norm)
        print(""Optimizer fisher_sr: damping = %.5f, max_norm = %.5f."" % (damping, max_norm))
    else:
        optimizer = optax.adam(lr)
        print(""Optimizer adam: lr = %.3f."" % lr)
    opt_state = optimizer.init(params_van)

    @jax.jit
    def update(params_van, opt_state, key):
        key, subkey = jax.random.split(key)
        state_indices = sampler(params_van, subkey, batch)

        grads, aux = jax.grad(loss_fn, argnums=0, has_aux=True)(params_van, state_indices)
        updates, opt_state = optimizer.update(grads, opt_state,
                                params=(params_van, state_indices) if sr else None)
        params_van = optax.apply_updates(params_van, updates)

        return params_van, opt_state, key, aux

    import os
    log_filename = os.path.join(path, ""data.txt"")
    f = open(log_filename, ""w"", buffering=1, newline=""\n"")

    for i in range(1, epoch+1):
        params_van, opt_state, key, aux = update(params_van, opt_state, key)
        E, E_std, F, F_std, S, S_std = aux[""E_mean""], aux[""E_std""], \
                                       aux[""F_mean""], aux[""F_std""], \
                                       aux[""S_mean""], aux[""S_std""]
        print(""iter: %04d"" % i,
                ""F:"", F, ""F_std:"", F_std / jnp.sqrt(batch),
                ""E:"", E, ""E_std:"", E_std / jnp.sqrt(batch),
                ""S:"", S, ""S_std:"", S_std / jnp.sqrt(batch))

        f.write( (""%6d"" + ""  %.6f""*6 + ""\n"") % (i, F, F_std / jnp.sqrt(batch),
                                                   E, E_std / jnp.sqrt(batch),
                                                   S, S_std / jnp.sqrt(batch)) )

    return params_van
"
155,2201.03156,"from orbitals import sp_orbitals, manybody_orbitals

def test_sp_orbitals():
    for dim in (2, 3):
        indices, Es = sp_orbitals(dim)
        print(""single-particle plane-wave orbitals in dim = %d:"" % dim)
        print(""indices:\n"", indices, indices.shape)
        print(""Es:\n"", Es, Es.shape)
        assert indices.shape == (Es.shape[0], dim)

def test_manybody_orbitals():
    n, dim = 7, 3
    Ecut = 2
    manybody_indices, manybody_Es = manybody_orbitals(n, dim, Ecut)
    print(""manybody_indices:\n"", manybody_indices, manybody_indices.shape)
    print(""manybody_Es:\n"", manybody_Es, manybody_Es.shape)
    assert manybody_indices.shape == (manybody_Es.shape[0], n, dim)
"
156,2201.03156,"import jax
from jax.config import config
config.update(""jax_enable_x64"", True)
import jax.numpy as jnp
import haiku as hk

import numpy as np

def generic_test_flow(model, L):
    """"""
        Generic test function for various flow models `model`.
    """"""
    n, dim = 7, 3
    key = jax.random.PRNGKey(42)
    x = jnp.array( np.random.uniform(0., L, (n, dim)) )
    params = model.init(key, x)
    z = model.apply(params, None, x)

    # Test that flow results of two ""equivalent"" (under lattice translations of PBC)
    # particle configurations are equivalent.
    print(""---- Test the flow is well-defined under lattice translations of PBC ----"")
    image = np.random.randint(-5, 6, size=(n, dim)) * L
    #print(""image:"", image / L)
    imagez = model.apply(params, None, x + image)
    assert jnp.allclose(imagez, z + image)

    # Test the translation equivariance.
    print(""---- Test translation equivariance ----"")
    shift = jnp.array( np.random.randn(dim) )
    #print(""shift:"", shift)
    shiftz = model.apply(params, None, x + shift)
    assert jnp.allclose(shiftz, z + shift)

    # Test of permutation equivariance.
    print(""---- Test permutation equivariance ----"")
    P = np.random.permutation(n)
    Pz = model.apply(params, None, x[P, :])
    assert jnp.allclose(Pz, z[P, :])

def test_FermiNet():
    from flow import FermiNet
    depth = 3
    spsize, tpsize = 16, 16
    L = 1.234
    
    def flow_fn(x):
        model = FermiNet(depth, spsize, tpsize, L)
        return model(x)
    flow = hk.transform(flow_fn)
    generic_test_flow(flow, L)
"
157,2201.03156,"import jax
from jax.config import config
config.update(""jax_enable_x64"", True)
import jax.numpy as jnp

import numpy as np
from potential import kpoints, Madelung, psi

def test_kpoints():
    Gmax = 4
    for dim in (2, 3):
        G = kpoints(dim, Gmax)
        print(""dim:"", dim, ""Gmax:"", Gmax)
        print(G)
        print(""G.shape:"", G.shape)
        assert G.shape[1] == dim

def generic_test_ewald(dim, x):
    for kappa in range(5, 11):
        print(""---- dim = %d, kappa = %d ----"" % (dim, kappa))
        for Gmax in range(5, 16):
            G = kpoints(dim, Gmax)
            print(""Gmax:"", Gmax, ""\t\tG.shape:"", G.shape, end=""\t\t"")
            Vconst = Madelung(dim, kappa, G)
            print(""Vconst:"", Vconst, end=""\t\t"")
            potential = psi(x, kappa, G)
            print(""potential:"", potential)

def test_ewald_3D():
    n, dim = 19, 3
    x = jnp.array( np.random.uniform(0., 1., (n, dim)) )
    generic_test_ewald(dim, x)

def test_ewald_2D():
    n, dim = 13, 2
    x = jnp.array( np.random.uniform(0., 1., (n, dim)) )
    generic_test_ewald(dim, x)
"
158,2201.03156,"import jax
from jax.config import config
config.update(""jax_enable_x64"", True)
key = jax.random.PRNGKey(42)
import jax.numpy as jnp

from orbitals import sp_orbitals
import haiku as hk
from sampler import make_autoregressive_sampler

def transformer(M):
    from autoregressive import Transformer

    num_layers, model_size, num_heads = 2, 32, 4
    hidden_size = 48

    def forward_fn(x):
        model = Transformer(M, num_layers, model_size, num_heads, hidden_size)
        return model(x)

    van = hk.transform(forward_fn)
    return van

def test_shapes():
    n, num_states = 13, 40
    sp_indices = jnp.array( sp_orbitals(2)[0] )[:num_states]

    van = transformer(num_states)
    dummy_state_idx = sp_indices[:n].astype(jnp.float64)
    params = van.init(key, dummy_state_idx)

    sampler, log_prob_novmap = make_autoregressive_sampler(van, sp_indices, n, num_states)
    log_prob = jax.vmap(log_prob_novmap, (None, 0), 0)
    batch = 200
    state_indices = sampler(params, key, batch)
    print(""state_indices:"", state_indices, ""\nstate_indices.shape:"", state_indices.shape)
    assert state_indices.shape == (batch, n)
    assert jnp.alltrue(state_indices < num_states)
    assert jnp.alltrue(state_indices[:, 1:] > state_indices[:, :-1])

    logp = log_prob(params, state_indices)
    print(""logp:"", logp, ""\nlogp.shape:"", logp.shape)
    assert logp.shape == (batch,)

def test_normalization():
    """"""
        Check probability normalization of the autoregressive model. Note this is a
    VERY STRONG CHECK of autoregressive property of the probability distribution.
    """"""
    import itertools

    n, num_states = 4, 10
    sp_indices = jnp.array( sp_orbitals(2)[0] )[:num_states]

    van = transformer(num_states)
    dummy_state_idx = sp_indices[:n].astype(jnp.float64)
    params = van.init(key, dummy_state_idx)

    state_indices = jnp.array( list(itertools.combinations(range(num_states), n)) )
    print(""state_indices:"", state_indices, ""\nstate_indices.shape:"", state_indices.shape)
    assert jnp.alltrue(state_indices[:, 1:] > state_indices[:, :-1])

    _, log_prob_novmap = make_autoregressive_sampler(van, sp_indices, n, num_states)
    log_prob = jax.vmap(log_prob_novmap, (None, 0), 0)

    logp = log_prob(params, state_indices)
    norm = jnp.exp(logp).sum()
    print(""logp:"", logp, ""\nnorm:"", norm)
    assert jnp.allclose(norm, 1.)
"
159,2201.03156,"import jax
from jax.config import config
config.update(""jax_enable_x64"", True)
import jax.numpy as jnp

import numpy as np
import haiku as hk
from orbitals import sp_orbitals
from logpsi import make_logpsi, make_logpsi_grad_laplacian, make_logp

key = jax.random.PRNGKey(42)

def fermiflow(depth, spsize, tpsize, L, n, dim):
    from flow import FermiNet
    def flow_fn(x):
        model = FermiNet(depth, spsize, tpsize, L)
        return model(x)
    flow = hk.transform(flow_fn)

    x = jnp.array( np.random.uniform(0., L, (n, dim)) )
    params = flow.init(key, x)
    return flow, x, params

""""""
    Below two tests are meant to check the various transformation properties of 
the functions logpsi and logp.
""""""
def test_logpsi():
    depth, spsize, tpsize, L = 3, 16, 16, 1.234
    n, dim = 7, 3
    flow, x, params = fermiflow(depth, spsize, tpsize, L, n, dim)

    sp_indices = jnp.array( sp_orbitals(dim)[0] )
    state_idx = jnp.array( np.random.choice(sp_indices.shape[0], size=n, replace=False))
    print(""indices:\n"", sp_indices[state_idx])

    logpsi = make_logpsi(flow, sp_indices, L)
    logpsix = logpsi(x, params, state_idx)

    print(""---- Test ln Psi_n(x + R) = ln Psi_n(x) under any lattice translation `R` of PBC ----"")
    image = np.random.randint(-5, 6, size=(n, dim)) * L
    logpsix_image = logpsi(x + image, params, state_idx)
    print(""logpsix:"", logpsix)
    print(""logpsix_image:"", logpsix_image)
    assert jnp.allclose(logpsix_image, logpsix)

    print(""---- Test permutation invariance: Psi_n(Px) = +/- Psi_n(x) ----"")
    P = np.random.permutation(n)
    logpsix_P = logpsi(x[P, :], params, state_idx)
    psix_P, psix = jnp.exp(logpsix_P[0] + 1j * logpsix_P[1]), \
                   jnp.exp(logpsix[0] + 1j *logpsix[1])
    print(""psix:"", psix)
    print(""psix_P:"", psix_P)
    assert jnp.allclose(psix_P, psix) or jnp.allclose(psix_P, -psix)

def test_logp():
    depth, spsize, tpsize, L = 3, 16, 16, 1.234
    n, dim = 7, 3
    flow, x, params = fermiflow(depth, spsize, tpsize, L, n, dim)

    sp_indices = jnp.array( sp_orbitals(dim)[0] )
    state_idx = jnp.array( np.random.choice(sp_indices.shape[0], size=n, replace=False))
    print(""indices:\n"", sp_indices[state_idx])

    logpsi = make_logpsi(flow, sp_indices, L)
    logp = make_logp(logpsi)
    logpx = logp(x[None, ...], params, state_idx[None, ...])

    print(""---- Test ln p_n(x + R) = ln p_n(x) under any lattice translation `R` of PBC ----"")
    image = np.random.randint(-5, 6, size=(n, dim)) * L
    logpx_image = logp(x[None, ...] + image, params, state_idx[None, ...])
    assert jnp.allclose(logpx_image, logpx)

    print(""---- Test translation invariance: p_n(x + a) = p_n(x), where `a` is a common translation of all electrons ----"")
    shift = jnp.array( np.random.randn(dim) )
    logpx_shift = logp(x[None, ...] + shift, params, state_idx[None, ...])
    assert jnp.allclose(logpx_shift, logpx)

def test_kinetic_energy():
    """"""
        Test the present kinetic energy (i.e., laplacian) implementation, where
    the real and imaginary part are separated, yield correct result in the special
    case of identity flow.
    """"""
    n, dim = 7, 3
    L = 1.234

    identity = hk.transform(lambda x: x)
    x = jnp.array( np.random.uniform(0., L, (n, dim)) )
    params = identity.init(key, x)

    sp_indices = jnp.array( sp_orbitals(dim)[0] )
    state_idx = jnp.array( np.random.choice(sp_indices.shape[0], size=n, replace=False))
    print(""indices:\n"", sp_indices[state_idx])

    logpsi = make_logpsi(identity, sp_indices, L)
    _, logpsi_grad_laplacian = make_logpsi_grad_laplacian(logpsi)
    grad, laplacian = logpsi_grad_laplacian(x[None, ...], params, state_idx[None, ...], key)
    assert grad.shape == (1, n, dim)
    assert laplacian.shape == (1,)

    kinetic = -laplacian - (grad**2).sum(axis=(-2, -1))
    print(""kinetic energy:"", kinetic)
    kinetic_analytic = (2*jnp.pi/L)**2 * (sp_indices[state_idx]**2).sum()
    print(""analytic result:"", kinetic_analytic)
    assert jnp.allclose(kinetic, kinetic_analytic)

def test_laplacian():
    """""" Check the two implementations of logpsi laplacian are equivalent. """"""
    depth, spsize, tpsize, L = 2, 4, 4, 1.234
    n, dim = 7, 3
    flow, x, params = fermiflow(depth, spsize, tpsize, L, n, dim)

    sp_indices = jnp.array( sp_orbitals(dim)[0] )
    state_idx = jnp.array( np.random.choice(sp_indices.shape[0], size=n, replace=False))
    print(""indices:\n"", sp_indices[state_idx])

    logpsi = make_logpsi(flow, sp_indices, L)
    _, logpsi_grad_laplacian1 = make_logpsi_grad_laplacian(logpsi)
    _, logpsi_grad_laplacian2 = make_logpsi_grad_laplacian(logpsi, forloop=False)
    grad1, laplacian1 = logpsi_grad_laplacian1(x[None, ...], params, state_idx[None, ...], key)
    grad2, laplacian2 = logpsi_grad_laplacian2(x[None, ...], params, state_idx[None, ...], key)
    assert jnp.allclose(grad1, grad2)
    assert jnp.allclose(laplacian1, laplacian2)

def test_laplacian_hutchinson():
    """"""
        Use a large batch sample to (qualitatively) check the Hutchinson estimator
    of the laplacian of logpsi.
    """"""
    depth, spsize, tpsize, L = 2, 4, 4, 1.234
    n, dim = 7, 3
    flow, x, params = fermiflow(depth, spsize, tpsize, L, n, dim)

    sp_indices = jnp.array( sp_orbitals(dim)[0] )
    state_idx = jnp.array( np.random.choice(sp_indices.shape[0], size=n, replace=False))
    print(""indices:\n"", sp_indices[state_idx])

    batch = 4000

    logpsi = make_logpsi(flow, sp_indices, L)
    logpsi_grad_laplacian = jax.jit(make_logpsi_grad_laplacian(logpsi)[1])
    grad, laplacian = logpsi_grad_laplacian(x[None, ...], params, state_idx[None, ...], key)

    logpsi_grad_laplacian2 = jax.jit(make_logpsi_grad_laplacian(logpsi, hutchinson=True)[1])
    grad2, random_laplacian2 = logpsi_grad_laplacian2(
            jnp.tile(x, (batch, 1, 1)), params, jnp.stack([state_idx]*batch), key)
    laplacian2_mean = random_laplacian2.mean()
    laplacian2_std = random_laplacian2.std() / jnp.sqrt(batch)

    assert jnp.allclose(grad2, grad)
    print(""batch:"", batch)
    print(""laplacian:"", laplacian)
    print(""laplacian hutchinson mean:"", laplacian2_mean, ""\tstd:"", laplacian2_std)
"
160,2201.03156,"import jax
from jax import config
config.update(""jax_enable_x64"", True)
import jax.numpy as jnp

import numpy as np
import time

key = jax.random.PRNGKey(10)

def test_jit1():
    def f(x):
        x -= x.mean(0)
        x /= x.std(0)
        return x
    f_jit = jax.jit(f)

    n = 100
    x = jax.random.normal(key, (100, 5))
    print(""x:"", x.shape, x.device(), x.dtype)

    print(""--- Original jax version ---"")
    for i in range(n):
        start = time.time()
        y = f(x)
        print(i+1, ""time to take (ms):"", 1000 * (time.time() - start))

    print(""--- With jit ---"")
    for i in range(n):
        start = time.time()
        y_jit = f_jit(x)
        print(i+1, ""time to take (ms):"", 1000 * (time.time() - start))

    assert jnp.allclose(y, y_jit)

def test_jit2():
    @jax.jit
    def f(x):
        print(""x:"", x)
        x -= x.mean(0)
        x /= x.std(0)
        return x

    n = 100
    x1 = jax.random.normal(key, (100, 5))
    print(""x1:"", x1.shape, x1.device())
    print(""--- function f applied to x1 ---"")
    for i in range(n):
        start = time.time()
        f(x1)
        print(i+1, ""time to take (ms):"", 1000 * (time.time() - start))

    x2 = jax.random.normal(key, (200, 30))
    print(""x2:"", x2.shape, x2.device())
    print(""--- function f applied to x2 ---"")
    for i in range(n):
        start = time.time()
        f(x2)
        print(i+1, ""time to take (ms):"", 1000 * (time.time() - start))

def test_jit3():
    @jax.jit
    def f(x):
        return x.reshape( (np.prod(x.shape),) )

    n = 100
    x1 = jax.random.normal(key, (20, 30))
    print(""x1:"", x1.shape, x1.device())
    print(""--- function f applied to x1 ---"")
    for i in range(n):
        start = time.time()
        f(x1)
        print(i+1, ""time to take (ms):"", 1000 * (time.time() - start))

    x2 = jax.random.normal(key, (40, 50))
    print(""x2:"", x2.shape, x2.device())
    print(""--- function f applied to x2 ---"")
    for i in range(n):
        start = time.time()
        f(x2)
        print(i+1, ""time to take (ms):"", 1000 * (time.time() - start))

def test_jit_foriloop():
    @jax.jit
    def sum1(n):
        val = 0
        for i in range(n): val += i
        return val
    @jax.jit
    def sum2(n):
        body_fun = lambda i, val: val + i
        return jax.lax.fori_loop(0, n, body_fun, 0)

    n = 10
    try:
        print(""sum1(%d):"" % n, sum1(n))     # sum1(n) will fail!
    except Exception as e:
        print(""Exception message:\n{}"".format(e))
    print(""sum2(%d):"" % n, sum2(n))     # sum2(n) using jax.lax.fori_loop is OK!!!

####################################################################################

def test_stop_gradient():
    fun1 = lambda x: (jnp.sin(x) * x**3).sum()
    fun2 = lambda x: (jnp.sin(x) * jax.lax.stop_gradient(x**3)).sum()
    x = jnp.array( np.random.randn(3, 4) )

    grad1 = jax.grad(fun1)(x)
    grad1_analytic = jnp.cos(x) * x**3 + jnp.sin(x) * 3*x**2
    assert jnp.allclose(grad1, grad1_analytic)

    grad2 = jax.grad(fun2)(x)
    grad2_analytic = jnp.cos(x) * x**3
    assert jnp.allclose(grad2, grad2_analytic)

####################################################################################

def hvp(f, primals, tangents):
    """""" Hessian-vector product: forward-over-reverse """"""
    _, tangent_out = jax.jvp(jax.grad(f), primals, tangents)
    return tangent_out

def test_hvp():
    f = lambda x: x[0]**3 + jnp.sin(x[0]) * x[1]**2
    x = jnp.array( np.random.randn(2) )
    v = jnp.array( np.random.randn(2) )

    hessianf_v_p = hvp(f, (x,), (v,))

    hessianf = jax.hessian(f)(x)
    hessianf_analytic = jnp.array([[6*x[0]-jnp.sin(x[0])*x[1]**2, 2*x[1]*jnp.cos(x[0])],
                                   [2*x[1]*jnp.cos(x[0]), 2*jnp.sin(x[0])]])
    assert jnp.allclose(hessianf, hessianf_analytic)

    assert jnp.allclose(hessianf_v_p, hessianf.dot(v))

####################################################################################
# Test various implementations of divergence of a R^n -> R^n function.

def div_jvp_fori(f):
    def div_f(x):
        n, = x.shape
        eye = jnp.eye(n)

        def body_fun(i, val):
            primal, tangent = jax.jvp(f, (x,), (eye[i],))
            return val + tangent[i]

        return jax.lax.fori_loop(0, n, body_fun, 0.)
    return div_f

def div_jvp_vmap(f):
    def div_f(x):
        def body_fun(x, basevec):
            _, tangent = jax.jvp(f, (x,), (basevec,))
            return (tangent * basevec).sum()
        return jax.vmap(body_fun, (None, 0), 0)(x, jnp.eye(x.shape[0])).sum()
    return div_f

def test_div():
    f = lambda x: x**3 * jnp.sin(x)
    x = jnp.array( np.random.randn(1000) )
    divf_analytic = (3*x**2*jnp.sin(x) + x**3*jnp.cos(x)).sum()
    divf_jvp_fori = div_jvp_fori(f)
    divf_jvp_fori_jit = jax.jit(divf_jvp_fori)
    divf_jvp_vmap = div_jvp_vmap(f)
    assert jnp.allclose(divf_jvp_fori(x), divf_analytic)
    assert jnp.allclose(divf_jvp_fori_jit(x), divf_analytic)
    assert jnp.allclose(divf_jvp_vmap(x), divf_analytic)

    for _ in range(50):
        start = time.time()
        divf_jvp_fori(x)
        print(time.time() - start, end=""\t\t"")

        start = time.time()
        divf_jvp_fori_jit(x)
        print(time.time() - start, end=""\t\t"")

        start = time.time()
        divf_jvp_vmap(x)
        print(time.time() - start)
"
161,2201.03156,"import jax
from jax.config import config
config.update(""jax_enable_x64"", True)
import jax.numpy as jnp

import numpy as np
from orbitals import sp_orbitals
from slater import logslaterdet0, logslaterdet

def test_slaterdet():
    """"""
        Basic check of the logslaterdet primitive, including antisymmetry property
    and invariance under translations.
    """"""
    n, dim = 7, 3
    sp_indices, _ = sp_orbitals(dim)
    indices = sp_indices[ list(np.random.choice(sp_indices.shape[0],
                                                size=(n,), replace=False)) ]
    print(""indices:\n"", indices)
    assert indices.shape == (n, dim)
    L = 1.234

    x = jnp.array( np.random.randn(n, dim) )
    det = jnp.exp(logslaterdet(indices, x, L))

    # Test the antisymmetry property.
    P = np.random.permutation(n)
    Pdet = jnp.exp(logslaterdet(indices, x[P, :], L))
    print(""det:"", det, ""Pdet:"", Pdet)
    assert jnp.allclose(Pdet, det) or jnp.allclose(Pdet, -det)

    # Test the translational invariance of the slater determinant.
    shift = jnp.array( np.random.randn(dim) )
    shifted_det = jnp.exp(logslaterdet(indices, x + shift, L))
    additional_phase = jnp.exp(1j * (2*jnp.pi/L*indices).dot(shift).sum())
    assert jnp.allclose(shifted_det, additional_phase * det)
    assert jnp.allclose(jnp.linalg.norm(shifted_det), jnp.linalg.norm(det))

def test_logslaterdet_AD():
    """"""
        Check AD behaviors of `logslaterdet` primitive against the ""generic""
    implementation `logslaterdet0` which explicitly differentiating through
    `jnp.linalg.slotdet`.
    """"""
    n, dim = 7, 3
    sp_indices, _ = sp_orbitals(dim)
    indices = sp_indices[ list(np.random.choice(sp_indices.shape[0],
                                                size=(n,), replace=False)) ]
    print(""indices:\n"", indices)
    assert indices.shape == (n, dim)
    L = 1.234

    x = jnp.array( np.random.randn(n, dim) )
    dx = jnp.array( np.random.randn(n, dim) )

    # jvp test.
    logdet0, dlogdet0 = jax.jvp(lambda x: logslaterdet0(indices, x, L), (x,), (dx,))
    logdet, dlogdet = jax.jvp(lambda x: logslaterdet(indices, x, L), (x,), (dx,))
    assert jnp.allclose(logdet, logdet0)
    #print(""dlogdet:"", dlogdet)
    #print(""dlogdet0:"", dlogdet0)
    assert jnp.allclose(dlogdet, dlogdet0)

    # (1st order) gradient test.
    grad_logslaterdet0 = jax.grad(lambda x: logslaterdet0(indices, x, L), holomorphic=True)
    grad_logslaterdet = jax.grad(lambda x: logslaterdet(indices, x, L), holomorphic=True)
    grad_x0 = grad_logslaterdet0(x+0j)
    grad_x = grad_logslaterdet(x+0j)
    #print(""grad_x:"", grad_x)
    #print(""grad_x0:"", grad_x0)
    assert jnp.allclose(grad_x, grad_x0)

    # (2nd order) hessian test, via hessian-vector product with a random tangent dx.
    grad_x0_again, hvp0 = jax.jvp(grad_logslaterdet0, (x+0j,), (dx+0j,))
    grad_x_again, hvp = jax.jvp(grad_logslaterdet, (x+0j,), (dx+0j,))
    assert jnp.allclose(grad_x0_again, grad_x0) and jnp.allclose(grad_x_again, grad_x)
    #print(""hvp:"", hvp)
    #print(""hvp0:"", hvp0)
    assert jnp.allclose(hvp, hvp0)

def test_logslaterdet_eigenstate():
    """"""
        Check the plane-wave slater determinants are eigenstates of the laplacian
    (i.e., the kinetic) operator, by using AD to compute the local energy
    nabla^2 psi(x) / psi(x).
    """"""
    n, dim = 7, 3
    sp_indices, _ = sp_orbitals(dim)
    indices = sp_indices[ list(np.random.choice(sp_indices.shape[0],
                                                size=(n,), replace=False)) ]
    print(""indices:\n"", indices)
    assert indices.shape == (n, dim)
    L = 1.234

    x = jnp.array( np.random.randn(n*dim) )

    def div(f):
        def div_f(x):
            def body_fun(x, basevec):
                _, tangent = jax.jvp(f, (x,), (basevec,))
                return (tangent * basevec).sum()
            eye = jnp.eye(x.shape[0], dtype=x.dtype)
            return jax.vmap(body_fun, (None, 0), 0)(x, eye).sum()
        return div_f

    grad = jax.grad(lambda x: logslaterdet(indices, x.reshape(n, dim), L), holomorphic=True)
    laplacian = div(grad)
    E_analytic = (2*jnp.pi/L)**2 * (indices**2).sum()
    E = - laplacian(x+0j) - (grad(x+0j)**2).sum()
    print(""E_analytic:"", E_analytic)
    print(""E:"", E)
    assert jnp.allclose(E, E_analytic)
"
162,2201.03156,"import jax
from jax.config import config
config.update(""jax_enable_x64"", True)
key = jax.random.PRNGKey(42)
import jax.numpy as jnp

import numpy as np
import haiku as hk
from autoregressive import CausalSelfAttention, Transformer
from jax.flatten_util import ravel_pytree

def test_CausalSelfAttention_params():

    """""" Test parameter number of a single causal MHA layer. """"""

    model_size, num_heads = 32, 4
    key_size = model_size // num_heads
    init_scale = 1.0

    def forward_fn(x):
        model = CausalSelfAttention(num_heads, key_size, init_scale)
        return model(x)
    attn = hk.transform(forward_fn)

    n = 50
    x = jnp.array( np.random.randn(n, model_size) )
    params = attn.init(key, x)

    raveled_params, _ = ravel_pytree(params)
    print(""Total number of parameters:"", raveled_params.size)
    num_params = (model_size + 1) * key_size * 3 * num_heads \
                 + (num_heads * key_size + 1) * model_size
    print(""num_params:"", num_params)
    assert raveled_params.size == num_params

def test_CausalSelfAttention_autoregressive():

    """""" Test autoregressive property of a single causal MHA layer. """"""

    model_size, num_heads = 32, 4
    key_size = model_size // num_heads
    init_scale = 1.0

    def forward_fn(x):
        model = CausalSelfAttention(num_heads, key_size, init_scale)
        return model(x)
    attn = hk.transform(forward_fn)

    n = 50
    x = jnp.array( np.random.randn(n, model_size) )
    params = attn.init(key, x)

    output = attn.apply(params, None, x)
    print(""x.shape:"", x.shape, ""output.shape:"", output.shape)
    assert output.shape == (n, model_size)

    random_vec = jnp.array( np.random.randn(n, model_size) )
    jac = jax.jacrev(lambda x: (attn.apply(params, None, x) * random_vec).sum(axis=-1))(x)
    assert jac.shape == (n, n, model_size)
    jac = jnp.linalg.norm(jac, axis=-1)
    print(""jac:"", jac)
    print(""jac.shape:"", jac.shape)

    depends = (jac != 0.).astype(int)
    assert jnp.allclose(depends, jnp.tril(jnp.ones((n, n))))

def test_Transformer_params():

    """""" Test parameter number of a complete masked transformer. """"""

    n, M = 13, 81
    num_layers, model_size, num_heads = 2, 32, 4
    key_size = model_size // num_heads
    hidden_size = 48

    def forward_fn(x):
        model = Transformer(M, num_layers, model_size, num_heads, hidden_size)
        return model(x[..., None])
    van = hk.transform(forward_fn)

    x = jnp.array( np.random.choice(M, size=n, replace=False), dtype=jnp.float64 )
    params = van.init(key, x)

    raveled_params, _ = ravel_pytree(params)
    print(""Total number of parameters:"", raveled_params.size)

    embedding_mlp = (1+1) * model_size
    output_mlp = (model_size + 1) * M
    MHA = (model_size + 1) * key_size * 3 * num_heads \
                 + (num_heads * key_size + 1) * model_size
    mlp_block = (model_size + 1) * hidden_size + (hidden_size + 1) * model_size
    print(""x1hat:"", M, ""\tembedding_mlp:"", embedding_mlp, ""\toutput_mlp:"", output_mlp,
            ""\tMHA:"", MHA, ""\tmlp_block:"", mlp_block)
    num_params = M + embedding_mlp + output_mlp + num_layers * (MHA + mlp_block)
    print(""num_params:"", num_params)
    assert raveled_params.size == num_params

def test_Transformer_autoregressive():

    """""" Test autoregressive property of a complete masked transformer. """"""

    n, M = 13, 81
    num_layers, model_size, num_heads = 2, 32, 4
    hidden_size = 48

    def forward_fn(x):
        model = Transformer(M, num_layers, model_size, num_heads, hidden_size)
        return model(x[..., None])
    van = hk.transform(forward_fn)

    x = jnp.array( np.random.choice(M, size=n, replace=False), dtype=jnp.float64 )
    params = van.init(key, x)

    output = van.apply(params, None, x)
    print(""x:"", x.astype(int))
    print(""x.shape:"", x.shape, ""output.shape:"", output.shape)
    assert output.shape == (n, M)

    random_vec = jnp.array( np.random.randn(n, M) )
    jac = jax.jacrev(lambda x: (van.apply(params, None, x) * random_vec).sum(axis=-1))(x)
    print(""jac:"", jac)
    print(""jac.shape:"", jac.shape)
    assert jac.shape == (n, n)

    depends = (jac != 0.).astype(int)
    print(depends)
    assert jnp.allclose(depends, jnp.tril(jnp.ones((n, n)), k=-1))
"
163,2201.02967,"""""""
Models for classical robust Student-t Linear and Quadratic
Discriminant Analyses.
""""""
import numpy as np
from scipy import stats, special
from ._algo_utils import fit_t, fit_t2
from ._models_lda import LDA

class t_LDA(LDA):
    """"""Student-t based Linear Discriminant Analysis.
        See `_models_lda.LDA` for more details. Inherits from LDA and redefines
        estimation and decision to use t-distributions instead of Gaussians.
        Note that this is mostly redundant: pooling covs still means
        discriminant depends on x^2 because of differing dofs 
        (assuming same dofs = MMD classifier)
    """"""
    def __init__(self, method=""distributional"", pool_covs=True):
        super().__init__(method=method, pool_covs=pool_covs)
        self.test = ""me""

    def _kth_likelihood(self, k):
        return stats.multivariate_t(loc=self.means_[:,k], 
                                    shape=self.covariance_[k,:,:], 
                                    df=self.dofs_[k],
                                    allow_singular=True)
    
    def _estimate_parameters(self, X):
        """"""Estimate parameters of one class according to Student-t class
        conditional density (mean, scatter and degree of freedom).

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.

        Returns
        -------
        params : list of [ndarray of shape (n_features,),
                ndarray of shape (n_features, n_features), float]\
            Estimated mean vector, covariance matrix, degree of freedom.
        """"""
#        if self.test == ""me"":
#            return fit_t(X)
#        elif self.test == ""test"":
#            return fit_t2(X)
#        else:
#            raise ValueError(""bleuh"")
        return fit_t(X)
    
    def _bose_k(self):
        """""" Generalised discriminant coefficient according to
        Bose et al. (2015), for t-distributions.
        """"""
        return (0.5 * (1 + self._M / self.dofs_))
        
    def _general_discriminants(self, X):
        # Generalised discriminant formula according to Bose et al. (2015)
        v = self.dofs_
        return super()._general_discriminants(X) \
            + (special.gammaln((v + self._M) / 2) - special.gammaln(v / 2) \
            - 0.5 * self._M * np.log(v))[:,None]
            
    def fit(self, X, y):
        # Inherit LDA.fit documentation
        super().fit(X,y)

        if len(self.parameters_[0]) < 3:
            self.parameters_ = [param + [200] for param in self.parameters_]
        
        self.dofs_ = np.array([param[2] for param in self.parameters_]) \
            .squeeze() #1xK
        return self

class t_QDA(t_LDA):
    """"""Student-t based Quadratic Discriminant Analysis. See t_LDA` for more 
        details. Inherits from t_LDA and unsets covariance pooling. 
    """"""
    def __init__(self, method='distributional'):
        super().__init__(method=method, pool_covs=False)"
164,2201.02967,"""""""
FEMDA: Flexible EM-Inspired Discriminant Analysis
""""""

from ._models_femda import FEMDA as _FEMDA
class FEMDA(_FEMDA):
    """"""FEMDA: Flexible EM_Inspired Discriminant Analysis
    ...

    See `_models_lda.LDA` for more details and definition of other
    non-public methods.

    Parameters
    ----------
    None

    Attributes
    ----------
    covariance_ : array-like of shape (n_classes, n_features, n_features)
        Unbiased sample covariance matrix per class. In LDA, this is 
        simply the pooled covariance matrix repeated for every class.

    means_ : array-like of shape (n_features, n_classes)
        Class-wise means. Note this is transpose of means_ in 
        sklearn.discriminant_analysis.LinearDiscriminantAnalysis

    priors_ : array-like of shape (n_classes,)
        Class priors (sum to 1).

    classes_ : array-like of shape (n_classes,)
        Unique class labels.

    Examples
    --------
    >>> from sklearn.datasets import load_iris
    >>> from femda import FEMDA
    >>> X, y = load_iris(return_X_y=True)
    >>> FEMDA().fit(X, y).score(X, y)
    """"""
    def __init__(self):
        super().__init__()
    
    def fit(self, X, y):
        """"""Fit FEMDA model according to data.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.

        y : array-like of shape (n_samples,)
            Target values.
        """"""
        return super().fit(X, y)
    
    def decision_function(self, X):
        """"""Apply decision function (discriminant) to new data, i.e.
        the log-posteriors. In binary case, this is instead defined as
        the log posterior ratio of the class 1/class 0.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Array of samples (test vectors).

        Returns
        -------
        dk : ndarray of shape (n_samples,) or (n_samples, n_classes)
            Decision function of new data per class. In 2-class case, 
            returns log likelihood ratio (n_samples,).
        """"""
        return super().decision_function(X)
    
    def predict(self, X, percent_outliers=0):
        """"""Classify new data X and return predicted labels y.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
        percent_outliers : float, default=0
            Optionally estimate outliers and label as -1

        Returns
        -------
        y_pred : ndarray of shape (n_samples,)
        """"""
        return super().predict(X, percent_outliers=percent_outliers)
    
    def predict_proba(self, X):
        """"""Estimate probability of class membership.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Input data.

        Returns
        -------
        C : ndarray of shape (n_samples, n_classes)
            Estimated probabilities.
        """"""
        return super().predict_proba(X)
"
165,2201.02967,"""""""
Models for classical robust Generalised Quadratic Discriminant Analysis (Bose 
et al. 2015) and Robust Generalised Quadratic Discriminant Analysis (Ghosh
et al. 2020) using various robust estimators.
""""""

import numpy as np
import pandas as pd

from ._algo_utils import fit_t
from ._models_lda import QDA

import rpy2.robjects as robjects
from rpy2.robjects.packages import importr
from rpy2.robjects import pandas2ri

# Import R libraries for robust estimators
pandas2ri.activate()
r = robjects.r
psych = importr('psych')
rrcov = importr('rrcov')
SpatialNP = importr('SpatialNP')
LaplacesDemon = importr('LaplacesDemon')


class GQDA(QDA):
    """"""Generalised Quadratic Discriminant Analysis.
        See `_models_lda.QDA` for more details. Inherits from QDA and fits an
        additional parameter on top of the classic estimation, which models a
        large class of distributions, by minimisation of misclassification 
        error. The method 'generalised' must be used to benefit from this.
        When `c_ = 1`, this is equal to QDA.
    """"""
    def __init__(self):
        super().__init__(method='generalised')
    
    def _bose_k(self):
        """""" Generalised discriminant coefficient according to
        Bose et al. (2015).
        """"""
        return np.array([0.5/self.c_])

    def fit(self, X, y, c_=None):
        """"""Fit GQDA model parameters according to data.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.

        y : array-like of shape (n_samples,)
            Target values.

        c_ : float, default=None.
            The generalised coefficient. If set, don't fit this parameter.
            If not, estimate using method of error minimisation.
        """"""
        super().fit(X,y) #Kx[n_k, M]
        
        if c_ is not None:
            self.c_ = c_
            return self
            
        uijs = [np.zeros((self.X_classes_[k].shape[0], self._K, self._K)) 
                for k in range(self._K)] #Kx[n_kxIxJ]
        sij = np.zeros((self._K,self._K))
        logdets = np.log(np.linalg.det(self.covariance_)) #K,  
        for i in range(self._K):
            for j in range(self._K):
                dij_on_i = self._mahalanobis(self.X_classes_[i], ki=j) \
                        - self._mahalanobis(self.X_classes_[i], ki=i) #Kxn_i
                dij_on_j = self._mahalanobis(self.X_classes_[j], ki=j) \
                        - self._mahalanobis(self.X_classes_[j], ki=i) #Kxn_j
                sij[i,j] = logdets[j] - logdets[i]
                
                uijs[i][:, i, j] = dij_on_i / sij[i,j]
                uijs[i][:, j, j] = np.inf
                uijs[j][:, i, j] = dij_on_j / sij[i,j]
        
        T = []
        for uij in uijs:
            T.append(uij[(uij > 0) * (uij<1)])
        T = np.sort(np.concatenate(T))
        T = np.concatenate([np.array([0]), T])
        #print(T)
        MCc = np.zeros((len(T)))
        for e,c_ in enumerate(T):
            for i in range(self._K):
                Rijc = []
                for j in range(self._K):
                    if i==j: continue
                    p = uijs[i][:, i,j]
                    to_app = p > -c_ if sij[i,j]>0 else p < -c_ 
                    Rijc.append(self.X_classes_[i][to_app])
                Rijc = np.vstack(Rijc)
                Ric = np.unique(Rijc, axis=0)
                #print(Ric.shape, Rijc.shape)
                lenRic = Ric.shape[0]
                MCic = self.X_classes_[i].shape[0] - lenRic
                #print(MCic, Ric.shape)
                MCc[e] += MCic
                
        #return uijs, MCc, T
        c_star = T[MCc.argmin()]
        self.c_ = c_star if c_star > 0 else 0.001
        print(""optimal c is"", c_star)
        return self        


class RGQDA(GQDA):
    """"""Robust Generalised Quadratic Discriminant Analysis.
        See `GQDA` for more details. Inherits from GQDA and replaces classical
        mean and covariance estimation with robust estimators, as used by
        Ghosh et al. (2020). Note that when `c_ = 1`, this becomes classical
        QDA with robust estimators.

        Additional Parameters
        ---------------------
        estimation : str, {'gaussian', 't-EM', 'winsorised', 'MVE', 'MCD', 
                           'M-estimator', 'S-estimator', 'SD-estimator'},
                        default='gaussian'
            Method of robust estimation.
    """"""
    def __init__(self, estimation='gaussian'):
        super().__init__()
        self.estimation = estimation

    def _estimate_t_EM(self, X):
        """"""Estimate by fitting t-distribution using EM
        """"""
        return fit_t(X) #discarding dof parameters

    def _estimate_gaussian_MLE(self, X):
        """"""Estimate by fitting Gaussian according to the MLE
        """"""
        return [X.mean(axis=0), np.cov(X.T)]

    def _get_r_frame(self, X):
        """"""Prepare data for passing into R
        """"""
        return pandas2ri.py2rpy(pd.DataFrame(X))

    def _estimate_winsorised(self, X):
        """"""Winsorise data and fit Gaussian
        """"""
        frame = self._get_r_frame(X)
        winsorised = psych.winsor(frame, trim=0.1)
        return self._estimate_gaussian_MLE(winsorised)

    def _estimate_MVE(self, X):
        """"""Fit data with Minimum Variance Ellipsoid estimator
        """"""
        frame = self._get_r_frame(X)
        MVE = rrcov.CovMve(frame, alpha=0.5)
        return [MVE.slots['center'], MVE.slots['cov']]

    def _estimate_MCD(self, X):
        """"""Fit data with Minimum Covariance Determinant estimator
        """"""
        print(""estimating..."")
        frame = self._get_r_frame(X)
        MCD = rrcov.CovMcd(frame, alpha=0.5)
        return [MCD.slots['center'], MCD.slots['cov']]   
    
    def _estimate_S_estimator(self, X):
        """"""Fit data with robust S-estimators
        """"""
        frame = self._get_r_frame(X)
        S = rrcov.CovSest(frame)
        return [S.slots['center'], S.slots['cov']]  

    def _estimate_SD_estimator(self, X):
        """"""Fit data with robust SD-estimators
        """"""
        frame = self._get_r_frame(X)
        SD = rrcov.CovSde(frame)
        return [SD.slots['center'], SD.slots['cov']] 
    
    def _estimate_M_estimator(self, X):
        """"""Fit data with robust Maronna M-estimators
        """"""
        frame = self._get_r_frame(X)
        M = SpatialNP.mvhuberM(frame)
        #print(list(M))
        return list(M)   

    def _estimate_parameters(self, X):
        """"""Estimate parameters of one class using robust estimators.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.

        Returns
        -------
        params : list of [ndarray of shape (n_features,),
                ndarray of shape (n_features, n_features)]\
            Estimated mean vector, covariance matrix.
        """"""
        if self.estimation == 'gaussian':
            return self._estimate_gaussian_MLE(X)
        elif self.estimation == 't-EM':
            return self._estimate_t_EM(X)
        elif self.estimation == 'winsorised':
            return self._estimate_winsorised(X)
        elif self.estimation == 'MVE':
            return self._estimate_MVE(X)
        elif self.estimation == 'MCD':
            return self._estimate_MCD(X)
        elif self.estimation == 'M-estimator':
            return self._estimate_M_estimator(X)
        elif self.estimation == 'S-estimator':
            return self._estimate_S_estimator(X)
        elif self.estimation == 'SD-estimator':
            return self._estimate_SD_estimator(X)
"
166,2201.02967,"""""""
TODO: t_QDA, GQDA etc. all have short descriptions referring back to LDA.
TODO: all other methods have full descriptions
Models for classical Linear Discriminant Analysis and 
Quadratic Discriminant Analysis.
""""""
import numpy as np
import time

from scipy import stats
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.utils.multiclass import unique_labels

from ._algo_utils import label_outliers, regularize, fit_gaussian

class LDA(BaseEstimator, ClassifierMixin):
    """"""Linear Discriminant Analysis
    Base classifier implementing classical Linear Discriminant Analysis,
    with Gaussian class conditional densities, and assuming equal covariance.

    Robust classifiers, such as t_QDA, RGQDA and FEMDA are implemented by 
    inheriting this class and modifying the appropriate methods, i.e.
    ones for estimating the parameters and calculating the likelihoods.

    All classifiers inheriting this class will implement all basic methods
    required by the scikit-learn API for linear classifiers, namely `fit`, 
    `predict`, `predict_proba` and `decision_function`.

    Parameters
    ----------
    method : {'distributional', 'generalised'}, default='distributional'
        Discriminant function calculation method:
          - 'distributional': uses the appropriate exact multivariate pdf
          - 'generalised': follows the approximated form proposed by
            Bose et al. (2015) for Elliptical-Symmetrical distributions.

    pool_covs : bool, default=True
        If True, all class covariance matrices are equal to the pooled
        covariance matrix estimate (LDA). If False, no pooling happens
        (QDA and other estimators).

    fudge : float, default=1
        Experimental factor in generalised discriminant function calculation.
        Ignore!

    Attributes
    ----------
    covariance_ : array-like of shape (n_classes, n_features, n_features)
        Unbiased sample covariance matrix per class. In LDA, this is 
        simply the pooled covariance matrix repeated for every class.

    means_ : array-like of shape (n_features, n_classes)
        Class-wise means. Note this is transpose of means_ in 
        sklearn.discriminant_analysis.LinearDiscriminantAnalysis

    priors_ : array-like of shape (n_classes,)
        Class priors (sum to 1).

    classes_ : array-like of shape (n_classes,)
        Unique class labels.
    """"""

    def __init__(self, method='distributional', pool_covs=True, fudge=1):
        self.method = method
        self.pool_covs = pool_covs
        self.fudge = fudge
    
    def _bose_k(self):
        """"""
        Return coefficient used by Bose et al. (2015) in calculation of
        generalised discriminant, to distinguish between different
        Elliptically Symmetrical distributions. 
        """"""
        return np.array([0.5])
    
    def _mahalanobis(self, X, ki=None): #NxM -> KxN
        """"""Calculate Mahalanobis distances for new data, per class,
        according to estimated model parameters.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            New data.
        ki : int, default=None
            If set, only calculate according to ki-th class. If not,
            calculate for all k classes.

        Returns
        -------
        d : ndarray of shape (n_classes, n_samples)
            Mahalanobis distances. If ki is set, returns distances for
            ki-th class, shape (n_samples,)
        """"""
        ret = []
        r = range(self._K) if ki is None else [ki]
        for k in r:
            m = X - self.means_[:,k]
            kth_maha = np.array(list(map(lambda d:
                d @ np.linalg.inv(self.covariance_)[k,:,:] @ d[:,None], m
            ))).T
            #kth_maha = np.diag(m \
            # @ np.linalg.inv(self.covariances)[k,:,:] @ m.T)]
            ret += [kth_maha]
        
        return np.vstack(ret) if ki is None else ret[0]
    
    def _general_discriminants(self, X): #KxN
        """"""Calculate generalised discriminant function of new data according 
        to model per class. This is only used if the method is generalsied.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            New data.

        Returns
        -------
        p : ndarray of shape (n_classes, n_samples)
            Log likelihood per class of new data.
        """"""
        return -0.5 * np.log(np.linalg.det(self.covariance_))[:,None] \
            * self.fudge - self._bose_k()[:,None] * self._mahalanobis(X)
    
    def _kth_likelihood(self, k): # non-log likelihood
        """"""Return random variable which calculates likelihood of
        kth class according to model parameters. Must have `pdf` method.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            New data.

        Returns
        -------
        p : multi_rv_generic
            Random variable calculating kth likelihood.
        """"""
        return stats.multivariate_normal(mean=self.means_[:,k],
                                         cov=self.covariance_[k,:,:],
                                         allow_singular=True)
    
    def _log_likelihoods(self, X):
        """"""Calculate log likelihood of new data according to model per class.
        This is only used if the method is distributional.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            New data.

        Returns
        -------
        p : ndarray of shape (n_classes, n_samples)
            Log likelihood per class of new data.
        """"""
        return np.log(np.array([self._kth_likelihood(k).pdf(X) 
                                for k in range(self._K)]))
       
    def _estimate_parameters(self, X): #NxM -> [1xM, MxM]
        """"""Estimate parameters of one class according to Gaussian class
        conditional density. This corresponds to sample mean and sample
        unbiased covariance.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data of one class.

        Returns
        -------
        params : list of [ndarray of shape (n_features,),
                ndarray of shape (n_features, n_features)]\
            Estimated mean vector and covariance matrix.
        """"""
        return fit_gaussian(X)#[X.mean(axis=0), np.cov(X.T)]

    def _dk_from_method(self, X): #NxM -> KxN
        """"""
        Choose between generalised and distributional discriminants.
        """"""
        if not(type(self.method) is str 
           and self.method in ['generalised', 'distributional']):
            raise ValueError('Method must be generalised or distributional')
        if self.method=='generalised':
            return self._general_discriminants(X)
        elif self.method=='distributional':
            return self._log_likelihoods(X)
        
    def fit(self, X, y):
        """"""Fit Discriminant Analysis model according to data.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.

        y : array-like of shape (n_samples,)
            Target values.
        """"""
        X, y = check_X_y(X, y)
        X, y = self._validate_data(X, y, ensure_min_samples=2, estimator=self,
                        dtype=[np.float64, np.float32], ensure_min_features=2)

        st=time.time()

        self.classes_ = unique_labels(y) #1xK
        self._K = len(self.classes_)
        self._M = X.shape[1]
        self.X_classes_ = [X[np.where(y == k), :][0,:,:] 
                           for k in self.classes_] #Kxn_kxM
        n = np.array([c.shape[0] for c in self.X_classes_])
        
        self.priors_ = n / n.sum()
        
        try:
            self.parameters_ = [self._estimate_parameters(c) 
                                for c in self.X_classes_]
        except np.linalg.LinAlgError:
            print(""didn't even fit..."")
            self.parameters_ = [[np.zeros(self._M), np.eye(self._M)] 
                                for c in self.X_classes_]

        self.means_ = np.array([param[0] for param in self.parameters_]).T
        self.covariance_ = np.array([param[1] for param in self.parameters_])
        self.covariance_ = self.covariance_ if not self.pool_covs else \
            np.repeat(np.sum(n[:,None,None] * self.covariance_, \
            axis=0)[None,:], self._K, axis=0) / n.sum()
        
        assert(n.sum() == X.shape[0])
        assert(self._M == self.covariance_.shape[2])
        assert (np.allclose(self.priors_.sum(), 1))
        #print(""Fitting time"", time.time()-st)
        return self

    def _decision_function(self, X):
        """"""
        Base function for all inference, so validate here.
        Compute log-posterior of new data with likelihoods and priors.
        """"""
        check_is_fitted(self, [""means_"", ""covariance_"", 
                               ""priors_"", ""parameters_"", ""classes_""])
        X = check_array(X)
        
        #try:
        dk = self._dk_from_method(X)
        try:
            pass
        except np.linalg.LinAlgError:
            print(""oops"")
            dk = np.zeros((len(self.classes_), X.shape[0]))
        dk = dk + np.log(self.priors_[:, None])
        return dk.T #return in standard sklearn shape NxK

    def decision_function(self, X):
        """"""Apply decision function (discriminant) to new data, i.e.
        the log-posteriors. In binary case, this is instead defined as
        the log posterior ratio of the class 1/class 0.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Array of samples (test vectors).

        Returns
        -------
        dk : ndarray of shape (n_samples,) or (n_samples, n_classes)
            Decision function of new data per class. In 2-class case, 
            returns log likelihood ratio (n_samples,).
        """"""
        dk = self._decision_function(X)
        return dk[:,1] - dk[:,0] if len(self.classes_) == 2 else dk #NxK

    def predict(self, X, percent_outliers=0):
        """"""Classify new data X and return predicted labels y.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
        percent_outliers : float, default=0
            Optionally estimate outliers and label as -1

        Returns
        -------
        y_pred : ndarray of shape (n_samples,)
        """"""
        dk = self._decision_function(X)
        y = self.classes_[np.nanargmax(dk, axis=1)]
        return label_outliers(X, y, self.means_, self.covariance_, 
                              thres=percent_outliers)
       
    def predict_proba(self, X):
        """"""Estimate probability of class membership.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Input data.

        Returns
        -------
        C : ndarray of shape (n_samples, n_classes)
            Estimated probabilities.
        """"""
        dk = self._decision_function(X)
        likelihood = np.exp(dk - dk.max(axis=1)[:, np.newaxis])
        return likelihood / likelihood.sum(axis=1)[:, np.newaxis]
    

class QDA(LDA):
    """"""Quadratic Discriminant Analysis classifier.
        See `LDA` for more details.
        Inherits from LDA and unsets covariance pooling. 
    """"""
    def __init__(self, method='distributional'):
        super().__init__(method=method, pool_covs=False)"
167,2201.02967,from .femda_ import FEMDA
168,2201.02967,"import numpy as np
from scipy import special, optimize
import math

def get_reg_lambd():
    """"""Return default regularization parameter.
    """"""
    return 1e-5

def regularize(sigma, lambd = get_reg_lambd()):    
    """"""Regularizes matrix to avoid singular matrix issues.

    Args:
        sigma (array-like of shape (n_features, n_features)): scatter matrix
        lambd (float, optional): reg parameter. Defaults to get_reg_lambd().

    Returns:
        array-like of shape (n_features, n_features):
            regularized scatter matrix
    """"""
    return sigma + np.eye(len(sigma)) * lambd

def fit_gaussian(X):
    """"""Fit Gaussian distribution parameters to data, with regularization.

    Args:
        X (array-like of shape (n_samples, n_features)): Training data.

    Returns:
        list of [ndarray of shape (n_features,),
                ndarray of shape (n_features, n_features)] :
            Estimated mean vector and covariance matrix.
    """"""
    n_reg = X.shape[0] + get_reg_lambd()
    m = (X.sum(axis=0) / n_reg)[None, :]
    return m[0], regularize(np.dot( (X-m).T, X-m ) / (n_reg - 1))

def t_EM_e_step(D, dof, mu, cov):
    """"""Run one E-step of the EM algorithm to fit Student-t. See Murphy,
    Machine Learning: A Probabilistic Perspective for details.

    Args:
        D (int): n_features of data
        dof (float): estimated degrees of freedom
        mu (array-like of shape (n_samples, n_features)): X - mean
        cov (array-like of shape (n_features, n_features)): estimated scatter

    Returns:
        z
        delta
        See Murphy for details.
    """"""
    delta = np.einsum('ij,ij->i', mu, np.linalg.solve(regularize(cov),mu.T).T)
    z = (dof + D) / (dof + delta)
    return z,delta

def fit_t_dof(X, mean, cov, dof_0, max_iter=200, mu=None, tol=1e-3):
    """"""Fit degrees of freedom to data according to Student-t, given other 
    parameters.

    Args:
        X (array-like of shape (n_samples, n_features)): Training data.
        mean (array-like of shape (n_features,)): Mean vector.
        cov (array-like of shape (n_features, n_features)): Scatter matrix.
        dof_0 (float): Initial guess of degrees of freedom.
        max_iter (int, optional): Max number of iterations. Defaults to 200.
        mu (array-like, optional): [description]. Defaults to None.
            X - mean. If set, skips this calculation.
        tol (float, optional): Convergence tolerance. Defaults to 1e-3.

    Returns:
        float: Estimated degree of freedom.
    """"""
    N, D = X.shape
    mu = mu if mu is not None else X - mean.squeeze()[None,:]
    dof = dof_0
    i = 0

    while i < max_iter:
        z,_ = t_EM_e_step(D, dof, mu, cov)

        d_t = (np.log(z) + special.digamma((dof + D) / 2)
                 - np.log((dof + D) / 2) - z).sum()
        dof_obj = lambda v: - (-N * special.gammaln(v/2)
                             + N * v * np.log(v/2) / 2 + v * d_t / 2 )
        dof_grad = lambda v: - (N / 2 * (-special.digamma(v/2)
                                        + np.log(v/2) + 1) + d_t/2)        
        dof_new = optimize.minimize(dof_obj, 
                                    dof, 
                                    jac=dof_grad,
                                    bounds=[(0, None)]
        ).x

        if abs(dof_new-dof)/dof <= tol: 
            dof = dof_new
            break
        dof = dof_new
        i += 1

    return dof
    

def fit_t(X, iter=20, eps=1e-6):
    """"""Fit Student-t distribution to data, according to EM-algorithm as
    described in Murphy, Machine Learning: A Probabilistic Perspective.
    Initialise with Gaussian MLE estimations.

    Args:
        X (array-like of shape (n_samples, n_features)): Training data.
        iter (int, optional): Max number of EM iterations. Defaults to 200.
        eps (float, optional): EM convergence tolerance. Defaults to 1e-6.

    Returns:
        list of [ndarray of shape (n_features,),
                ndarray of shape (n_features, n_features), float] :
            Estimated mean vector,  covariance matrix and degree of freedom.
    """"""
    N,D = X.shape
    mean, cov = fit_gaussian(X)
    mu = X - mean[None,:]
    dof = 3
    obj = []

    for i in range(iter):
        #print(""mean at start"", i, mean)
        if i>198: print(""t not converged"", obj[-1])
        # E step
        z,delta = t_EM_e_step(D, dof, mu, cov)
        
        obj.append(
            - N * np.linalg.slogdet(cov)[1]/2 - (z * delta).sum()/2 \
            - N * special.gammaln(dof/2) + N * dof*np.log(dof/2)/2 
            + dof * (np.log(z)-z).sum()/2)

        if len(obj) > 1 and np.abs(obj[-1] - obj[-2]) < eps: 
            break
        
        # M step
        mean = (X * z[:,None]).sum(axis=0).reshape(-1,1) / z.sum()
        #print(""mean at end"", i, mean)
        mu = X - mean.squeeze()[None,:]
        cov = np.einsum('ij,ik->jk', mu, mu * z[:,None])/N
        dof = fit_t_dof(X, None, cov, dof, max_iter=1, mu=mu)
    #plt.plot(obj)
    #plt.show()
    #print(obj)
    return mean.squeeze(), regularize(cov), dof

def fit_t2(X, eps = 1e-5, max_iter = 20):

    n, m          = X.shape
    n_clusters = n + 1e-5
    mean, covariance = fit_gaussian(X)
    v    = 1

    convergence      = False
    ite              = 1

    while (not convergence) and ite<max_iter:
        print(""m at beginning of "", ite, mean)
        prev_mean = mean.copy()
        prev_cov = covariance.copy()
        prev_v = v

        ite              = ite + 1 
        mean             = np.zeros(m)
        covariance       = np.zeros([m,m])
        sum_mean_weights = 1e-5
        sum_nu_weights   = 1e-5
        for i in range(n):
            mean_weight      = (v + m) / (v + np.dot(np.array([X[i]-mean]), np.dot(np.linalg.inv(regularize(covariance)), np.array([X[i]-mean]).T))[0][0])
            
            sum_mean_weights += mean_weight
            covariance += np.dot(np.array([X[i]-mean]).T, np.array([X[i]-mean])) * mean_weight
            mean             += mean_weight * X[i]
            sum_nu_weights  += (np.log(mean_weight) - mean_weight - np.log((v + m)/2) + special.digamma((v + m)/2)) / n_clusters
        def f(nu):
            return special.gammaln(nu/2) - 0.5*nu*np.log(nu/2) - 0.5*nu*sum_nu_weights
        def grad_f(nu):
            return 0.5*special.digamma(nu/2) - 0.5*np.log(nu/2) - 0.5 - 0.5*sum_nu_weights
        v = optimize.minimize(f, v, jac = grad_f,bounds=[(0,None)]).x[0]
        
        mean /= sum_mean_weights
        covariance /= n_clusters
        print(""m at end of "", ite, mean)
        convergence = abs(v-prev_v) + sum(abs(mean-prev_mean)) + sum(sum(abs(covariance-prev_cov)))
        #print(convergence, ""at step"", ite)
        convergence = False#convergence < eps
    
    return mean, regularize(covariance), v



def label_outliers_kth2(X_k, mean, cov, thres=0):
    """"""Label outliers for kth class according to Mahalanobis distance.

    Args:
        X_k (array-like of shape (n_samples, n_features)): Training data for
            kth class.
        mean (array-like of shape (n_features,)): Mean vector.
        cov (array-like of shape (n_features, n_features)): Scatter matrix.
        thres (float, optional): Mahalanobis outlier threshold. Defaults to 0.

    Returns:
        array-like of type bool, shape (n_samples,) : whether samples are 
            outliers.
    """"""
    diff = X_k - mean
    maha = (np.dot(diff, np.linalg.inv(cov)) * diff).sum(1)
    def split(n, perc):
        a = int(np.floor(n *perc))
        return a, n-a
    _,n_to_keep = split(X_k.shape[0], thres)
    t = maha[np.argsort(maha)[n_to_keep - 1]]
    outlierness = (maha > t)
    return outlierness

def label_outliers(X, y, means, covs, thres=0.05):
    """"""Label outliers in data according to Mahalanobis distance.

    Args:
        X_k (array-like of shape (n_samples, n_features)): Training data for
            kth class.
        means (array-like of shape (n_features, n_classes)): Means vectors.
        covs (array-like of shape (n_classes, n_features, n_features)): 
            Scatter matrices.
        thres (float, optional): Mahalanobis threshold. Defaults to 0.05.

    Returns:
        array-like (shape (n_samples,)) : preds with outliers relabelled as -1.
    """"""
    if thres == 0:
        return y

    y_new = y.copy()
    ks = np.unique(y)

    for ki, k in enumerate(ks):
        k = int(k)
        outlierness = label_outliers_kth2(X[y==k,:], means[:,ki], covs[ki,:,:], 
                                          thres=thres)
        b = np.where(y==k)[0][outlierness]
        y_new[b] = -1#k+5

    return y_new

"
169,2201.02967,"import numpy as np
import random
import pandas as pd

# MATH and STATS:
import math
from scipy.stats import multivariate_normal
from scipy.stats import chi2
from scipy.stats._multivariate import _PSD  

# for initialization of cluster's centers:
from sklearn.cluster import KMeans
from scipy.spatial import cKDTree

from._algo_utils import regularize
import matplotlib.pyplot as plt

class FEM():
    '''Implements the F-EM algorithm     
    
    Parameters
    ----------
    K : int
        The number of mixture components.
    max_iter: int
        maximum number of iterations of the algorithm.
    rand_initialization: bool
        True if random initialization
        False if K-Means initialization.
    version: {1, 2, 3, 4}
        version of the algorithm
        1: with old, old, not square root
        2: with old, new, not square root
        3: with old, old, with square root
        4: with old, new, with square root.
     max_iter_fp: integer>0
        maximum number of fixed-point iterations
    
    
    Attributes
    ----------
    alpha_ : array-like, shape (n,)
        The weight of each mixture components.
    mu_ : array-like, shape (n, p)
        The mean of each mixture component.
    Sigma_ : array-like, shape (p, p)
        The covariance of each mixture component.
    tau_ : array-like, shape (n, K)
        The collection of tau values.
    labels_ : array-like, shape (n,)
        The clustering labels of the data points.
    converged_ : bool
        True when convergence was reached in fit(), False otherwise.
    n_iter_ : int
        Number of step used to reach the convergence.               
    '''
    
    def __init__(self, K, max_iter = 200, 
                 rand_initialization = False, 
                 version = 1, max_iter_fp = 20, thres = None):
        self.K = K
        self.converged_ = False
        self.version = version
        self.rand_initialization = rand_initialization
        self.max_iter = max_iter
        self.max_iter_fp = max_iter_fp
        self.thres = thres
        self.alpha_ = None
        self.mu_ = None
        self.Sigma_ = None
        self.tau_ = None
        self.n_iter_ = None
        self.labels_ = None
    
    def _initialize(self, X):
        '''Initialize all the parameters of the model:
        theta = (alpha, mu, sigma, tau)
        Either randomly or with kmeans centers.
    
        Parameters
        ----------
        X: array-like, shape (n, p)
    
        '''
        
        n, p = X.shape

        if self.rand_initialization:
            self.alpha_ = np.random.rand(self.K)
            self.alpha_ /= np.sum(self.alpha_) 
            self.mu_ = (np.amax(X, axis=0)-np.amin(X, axis=0)) * np.random.random_sample((self.K, p))+ np.amin(X, axis=0)
            self.Sigma_ = np.zeros((self.K, p, p))
            self.tau_ = np.ones((n, self.K))
            for k in range(self.K):
                self.Sigma_[k] = np.eye(p)

        else:
            
            one_point_clusters = False
            
            kmeans = KMeans(n_clusters = self.K, max_iter = 200).fit(X)
            
            for k in range(self.K):
                
                nk = np.count_nonzero(kmeans.labels_ == k)
                
                if nk <= 2 and n>10:
                    one_point_clusters = True
                    
            ite_filter = 0
            n_filter = n
        
            if one_point_clusters:
                
                tree = cKDTree(X)#tree of nearest neighbors
                KNN=4
                dd, index = tree.query(X, k=[KNN]) # query for all points in data the Kth NN, returns distances and indexes
                
                dd = np.reshape(dd, (n,))
                
                alpha_quantile = 0.95
            
                while one_point_clusters and alpha_quantile > 0.5:
                    
                    ite_filter += 1
                    
                    alpha_quantile -= (0.1) * (ite_filter - 1)
                    
                    one_point_clusters = False
                    
                    X_without_extremes = X[dd < np.quantile(dd, alpha_quantile) , :]
                    
                    n_filter = X_without_extremes.shape[0]

                    kmeans = KMeans(n_clusters=self.K, max_iter=200).fit(X_without_extremes)
                    
                    for k in range(self.K):
                
                        nk = np.count_nonzero(kmeans.labels_ == k)
                
                        if nk <= 2:
                        
                            one_point_clusters = True
            
            self.alpha_ = np.zeros((self.K,))
            self.mu_ = np.zeros((self.K, p))
            self.Sigma_ = np.zeros((self.K, p, p))            

            for k in range(self.K):
                nk = np.count_nonzero(kmeans.labels_ == k)
                self.alpha_[k] = float(nk)/float(n_filter)
                self.mu_[k] = kmeans.cluster_centers_[k]
                self.Sigma_[k] = np.eye(p) # cov result in nan sometimes

            self.tau_ = np.ones((n, self.K))                   

    
    def _e_step(self, X):
        ''' E-step of the algorithm
        Computes the conditional probability of the model
        
        Parameters
        ----------
        X: array-like, shape (n, p)
            data
    
        Returns
        ----------
        cond_prob_matrix: array-like, shape (n, K)
             (cond_prob_matrix)_ik = P(Z_i=k|X_i=x_i)
        '''
        n, p = X.shape
        
        K = len(self.alpha_)
        
        cond_prob_matrix = np.zeros((n,K))
    
        for k in range(K):
            
            psd = _PSD(self.Sigma_[k])
            prec_U, logdet = psd.U, psd.log_pdet
            diff = X - self.mu_[k]
            logdensity = -0.5 * (p * np.log(2 * np.pi) + p * np.log(self.tau_[:, k]) + logdet + p)
            #print(self.tau_[:,k])
            cond_prob_matrix[:, k] = np.exp(logdensity)  * self.alpha_[k]            
        
        sum_row = np.sum(cond_prob_matrix, axis = 1) 
        bool_sum_zero = (sum_row == 0)
        
        cond_prob_matrix[bool_sum_zero, :] = self.alpha_      
        cond_prob_matrix /= cond_prob_matrix.sum(axis=1)[:,np.newaxis]

        return cond_prob_matrix
    
    def _m_step(self, X, cond_prob):
        ''' M-step of the algorithm
        Updates all the parameters with the new conditional probabilities
        
        Parameters
        ----------
        X: array-like, shape (n, p)
            data 
        cond_prob_matrix: array-like, shape (n, K)
             (cond_prob_matrix)_ik = P(Z_i=k|X_i=x_i)
    
        Returns
        ----------
        alpha_new: array-like, shape (n,)
            The new weights of each mixture components.
        mu_new: array-like, shape (n, p)
            The new mean of each mixture component.
        Sigma_new: array-like, shape (p, p)
            The new covariance of each mixture component.
        tau_new: array-like, shape (n, K)
            The collection of tau values.
        '''
        
        n, p = X.shape
        
        alpha_new = np.zeros((self.K,))
        mu_new = np.zeros((self.K, p))
        Sigma_new = np.zeros((self.K, p, p))
        tau_new = np.ones((n, self.K))

        for k in range(self.K):

            # UPDATE alpha:
            alpha_new[k] = np.mean(cond_prob[:, k])

            # Fixed-point equation for Sigma and mu:
            # UPDATE mu
            # UPDATE Sigma
            mu_fixed_point = self.mu_[k].copy()
            Sigma_fixed_point = self.Sigma_[k].copy()
            tau_ite = np.ones((n, ))
            tau_ite_sr = np.ones((n, ))
            convergence_fp = False
            ite_fp = 1
            mean_error = []
            while not(convergence_fp) and ite_fp<self.max_iter_fp:
                if ite_fp > 198: 
                    print(""m-step not converged"", mean_error[-1])
                inv_Sigma_fixed_point = np.linalg.inv(regularize(Sigma_fixed_point))
                diff = X - mu_fixed_point             
                sq_maha = (np.dot(diff, inv_Sigma_fixed_point) * diff).sum(1) # multiple quadratic form
                
                tau_ite = sq_maha / p 
                tau_ite_sr = (sq_maha**(0.5))/p
                tau_ite = np.where(tau_ite<10**(-8) , 10**(-8),
                                   np.where(tau_ite>10**(8), 10**(8), tau_ite))
                tau_ite_sr = np.where(tau_ite_sr<10**(-8) , 10**(-8),
                                      np.where(tau_ite_sr>10**(8), 10**(8), tau_ite_sr))

                if self.version == 1 or self.version ==2:
                    Ck = (cond_prob[:, k]/tau_ite)/np.sum(cond_prob[:,k]/tau_ite)
                else: # 3 or 4
                    Ck = (cond_prob[:, k]/tau_ite_sr)/np.sum(cond_prob[:,k]/tau_ite_sr)

                mu_fixed_point_new = np.sum(np.multiply(X, Ck[:, np.newaxis]), 0)
                
                if self.version == 2 or self.version == 4: # if usig new estim, update denominator
                        
                    diff = X - mu_fixed_point_new             
                    sq_maha = (np.dot(diff, inv_Sigma_fixed_point) * diff).sum(1) # multiple quadratic form
                    tau_ite = sq_maha / p 
                    tau_ite_sr = (sq_maha**(0.5))/p
                    tau_ite = np.where(tau_ite<10**(-8) , 10**(-8),
                                       np.where(tau_ite>10**(8), 10**(8), tau_ite))
                    tau_ite_sr = np.where(tau_ite_sr<10**(-8) , 10**(-8),
                                          np.where(tau_ite_sr>10**(8), 10**(8), tau_ite_sr))
                    
                if self.version==1:
                    
                    diff = X - mu_fixed_point
                    Sigma_fixed_point_new = np.dot(cond_prob[:, k]/tau_ite * diff.T, diff) / (n * alpha_new[k])
                    Sigma_fixed_point_new *= p / np.trace(Sigma_fixed_point_new)
                    
                if self.version==2:
                    
                    diff = X - mu_fixed_point_new
                    Sigma_fixed_point_new = np.dot(cond_prob[:, k]/tau_ite * diff.T, diff) / (n * alpha_new[k])
                    Sigma_fixed_point_new *= p / np.trace(Sigma_fixed_point_new)
                    
                if self.version==3:
                    
                    diff = X - mu_fixed_point
                    Sigma_fixed_point_new = np.dot(cond_prob[:, k]/tau_ite_sr * diff.T, diff) / (n * alpha_new[k])
                    Sigma_fixed_point_new *= p / np.trace(Sigma_fixed_point_new)

                if self.version==4: 
                    
                    diff = X - mu_fixed_point_new
                    Sigma_fixed_point_new = np.dot(cond_prob[:, k]/tau_ite_sr * diff.T, diff) / (n * alpha_new[k])
                    Sigma_fixed_point_new *= p / np.trace(Sigma_fixed_point_new)

                convergence_fp = True
                convergence_fp = convergence_fp and (math.sqrt(np.inner(mu_fixed_point - mu_fixed_point_new, mu_fixed_point - mu_fixed_point_new)/p) < 10**(-6))
                mean_error += [(math.sqrt(np.inner(mu_fixed_point - mu_fixed_point_new, mu_fixed_point - mu_fixed_point_new)/p))]
                #if convergence_fp : print(""mean convergence"")
                convergence_fp = convergence_fp and (np.linalg.norm(Sigma_fixed_point_new-Sigma_fixed_point, ord='fro')/p) < 10**(-6)
                if convergence_fp: print(""m-step converged"")
                mu_fixed_point = mu_fixed_point_new.copy()
                Sigma_fixed_point = Sigma_fixed_point_new.copy() 

                ite_fp += 1

            mu_new[k] = mu_fixed_point
            Sigma_new[k] = regularize(Sigma_fixed_point)

            # UPDATE tau
            diff = X - mu_new[k]
            tau_new[:, k] = (np.dot(diff, np.linalg.inv(regularize(Sigma_new[k]))) * diff).sum(1) / p
            tau_new[:, k] = np.where(tau_new[:, k] < 10**(-12) , 10**(-12),
                                     np.where(tau_new[:, k] > 10**(12), 10**(12), tau_new[:, k]))
            #print(mean_error)
            #plt.plot(mean_error)
            #plt.show()

        return alpha_new, mu_new, Sigma_new, tau_new
    
    def fit(self, X):
        ''' Fit the data to the model running the F-EM algorithm
        
        Parameters
        ----------
        X: array-like, shape (n, p)
            data 
    
        Returns
        ----------
        self
        '''
        
        n, p = X.shape
        
        self._initialize(X)

        convergence = False

        ite = 0
        
        while not(convergence) and  ite < self.max_iter:

            # Compute conditional probabilities:
            cond_prob = self._e_step(X)

            # Update estimators:
            alpha_new, mu_new, Sigma_new, tau_new = self._m_step(X, cond_prob)

            # Check convergence:
            if ite > 5:  # tol from fixed point should be bigger than general tolerance rate 
                convergence = True
                k = 0
                while convergence and k<self.K:
                    
                    convergence = convergence and math.sqrt(np.inner(mu_new[k]-self.mu_[k], mu_new[k]-self.mu_[k])/p) < 10**(-5)
                    convergence = convergence and ((np.linalg.norm(Sigma_new[k]-self.Sigma_[k], ord='fro')/(p)) < 10**(-5))
                    convergence = convergence and (math.fabs(alpha_new[k]-self.alpha_[k]) < 10**(-3))
                    
                    k += 1
                    
            self.alpha_ = np.copy(alpha_new)
            self.mu_ = np.copy(mu_new)
            self.Sigma_ = np.copy(Sigma_new)
            self.tau_ = np.copy(tau_new)
            
            ite += 1
        
        self.labels_ = np.array([i for i in np.argmax(cond_prob, axis=1)])
        self.n_iter_ = ite
        self.converged_ = convergence
        
        # Outlier rejection 
        
        outlierness = np.zeros((n, )).astype(bool)
        
        if self.thres is None :
            self.thres = 0.05         
        thres = chi2.ppf(1 - self.thres, p)
            
        for k in range(self.K):
            
            data_cluster = X[self.labels_ == k,:]
            diff_cluster = data_cluster - self.mu_[k]
            sig_cluster = np.mean(diff_cluster * diff_cluster) 
            maha_cluster = (np.dot(diff_cluster, np.linalg.inv(self.Sigma_[k])) * diff_cluster).sum(1) / sig_cluster
            outlierness[self.labels_ == k] = (maha_cluster >  thres)
            
        self.labels_[outlierness] = -1
        
        self.labels_ = self.labels_.astype(str)
        
        return(self)
    
    def predict(self, Xnew, thres = None):
        
        n, p = Xnew.shape
        
        cond_prob_matrix = np.zeros((n, self.K))
    
        for k in range(self.K):
            
            psd = _PSD(self.Sigma_[k])
            prec_U, logdet = psd.U, psd.log_pdet
            diff = Xnew - self.mu_[k]
            sig = np.mean(diff * diff) 
            maha = (np.dot(diff, np.linalg.inv(self.Sigma_[k])) * diff).sum(1) 
            logdensity = -0.5 * (logdet + maha)            
            cond_prob_matrix[:, k] = np.exp(logdensity)  * self.alpha_[k]            
        
        sum_row = np.sum(cond_prob_matrix, axis = 1) 
        bool_sum_zero = (sum_row == 0)
        
        cond_prob_matrix[bool_sum_zero, :] = self.alpha_      
        cond_prob_matrix /= cond_prob_matrix.sum(axis=1)[:,np.newaxis]
        
        new_labels = np.array([i for i in np.argmax(cond_prob_matrix, axis=1)])
        
        outlierness = np.zeros((n, )).astype(bool)
        
        if thres is None :
            thres = self.thres           
        thres = chi2.ppf(1 - thres, p)
            
        for k in range(self.K):
            
            data_cluster = Xnew[new_labels == k,:]
            diff_cluster = data_cluster - self.mu_[k]
            sig_cluster = np.mean(diff_cluster * diff_cluster) 
            maha_cluster = (np.dot(diff_cluster, np.linalg.inv(self.Sigma_[k])) * diff_cluster).sum(1) / sig_cluster
            outlierness[new_labels == k] = (maha_cluster >  thres)
            
        new_labels[outlierness] = -1
        
        new_labels = new_labels.astype(str)
        
        return(new_labels)
"
170,2201.02967,"""""""
Base models and related models for our robust FEMDA:
Flexible EM-Inspired Discriminant Analysis.
""""""
import numpy as np
from math import sqrt

from ._algo_utils import regularize, get_reg_lambd, fit_t_dof, fit_gaussian
from ._models_lda import LDA
from ._models_t_lda import t_LDA


class _FEM_base():
    """"""Class to use the EM steps contained in the FEM clustering algorithm for
    parameter estimation of class labelled data. The EM steps here are 
    modified from the original clustering steps: they only take one class, and
    the E-step is deterministic since this is now a supervised algorithm.
    """"""
    def _e_step_indicator(self, X):
        ''' Pseudo E-step of clustering algorithm where all conditional 
        probabilities are replaced by ones according to the supervised labels.
        
        Parameters
        ----------
        X: array-like of shape (n_samples, n_features)
            data
    
        Returns
        ----------
        indicator_matrix: ndarray of shape (n_samples,)
             Matrix representing determined ""class conditional probabilities"".
             Single row as we only are concerned with one class.
        '''
        return np.ones((X.shape[0]))

    def _m_step(self, X, cond_prob, max_iter = 20, eps=1e-6):
        ''' M-step of clustering algorithm used to estimate parameters given
        conditional probabilities.
        
        Parameters
        ----------
        X: array-like of shape (n_samples, n_features)
            data
        cond_prob: array-like of shape (n_samples,)
            Conditional probability matrix from E-step, for one class only, 
            i.e (cond_prob)_ik = P(Z_i=k|X_i=x_i) where k = 0
        max_iter: int, default=20
            Max number of fixed point iterations
        eps: float, default=1e-6
            Convergence tolerance
    
        Returns
        ----------
        mean: ndarray of shape (n_features,)
            The new mean of each mixture component.
        sigma: ndarray of shape (n_features, n_features)
            The new regularized covariance of each mixture component.
        '''
        n, m = X.shape
        mean, sigma = fit_gaussian(X)
        convergence_fp      = False
        ite_fp              = 1
        safety = lambda x : np.minimum(0.5, x)
        while (not convergence_fp) and ite_fp<max_iter:
            ite_fp += 1

            sigma_inv = np.linalg.inv(regularize(sigma))
            diff = X - mean
            sq_maha = (np.dot(diff, sigma_inv) * diff).sum(1)     
            
            mean_new = np.dot(safety(cond_prob / sq_maha), X) \
                    / (safety(cond_prob / sq_maha).sum() + get_reg_lambd())

            sigma_new = np.dot(safety(cond_prob / sq_maha) * diff.T, diff) \
                    / (n + get_reg_lambd()) * m

            convergence_fp = sqrt(((mean - mean_new)**2).sum() / m) < eps \
                    and np.linalg.norm(sigma_new - sigma, ord='fro') / m < eps

            mean  = mean_new.copy()
            sigma = sigma_new.copy()

        return mean, regularize(sigma)

    def _estimate_parameters_with_FEM(self, X):
        """"""Estimate parameters (mean, scatter) of one class with FEM algorithm,
        according to flexible Elliptically Symmetrical model.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data of one class (i.e. {X: I(Zi=k|Xi=xi)=1})

        Returns
        -------
        params : list of [ndarray of shape (n_features,),
                ndarray of shape (n_features, n_features)]\
            Estimated mean vector and covariance matrix.
        """"""

        cond_prob = self._e_step_indicator(X)

        #run M-step
        mean, cov = self._m_step(X, cond_prob)
        return mean, cov * X.shape[1] / np.trace(cov)


class LDA_FEM(LDA, _FEM_base):
    """"""Linear Discriminant Analysis with FEM-Inspired parameter estimation.
    Stepping stone to FEMDA! See `_models_LDA.LDA`, `_FEM_base` for more.
    Inherits from `_models_LDA.LDA` and `_FEM_base`.
    """"""        
    def _estimate_parameters(self, X):
        """"""Estimate parameters (mean, scatter) of one class with FEM algorithm,
        according to flexible Elliptically Symmetrical model.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data of one class (i.e. {X: I(Zi=k|Xi=xi)=1})

        Returns
        -------
        params : list of [ndarray of shape (n_features,),
                ndarray of shape (n_features, n_features)]\
            Estimated mean vector and covariance matrix.
        """"""

        return self._estimate_parameters_with_FEM(X)

class QDA_FEM(LDA_FEM):
    """"""Quadratic Discriminant Analysis with FEM-Inspired parameter estimation.
    Stepping stone to FEMDA! See `LDA_FEM` for more.
    Inherits from LDA_FEM and unsets covariance pooling. 
    """"""
    def __init__(self, method='distributional'):
        super().__init__(method=method, pool_covs=False)
        
class t_LDA_FEM(t_LDA, _FEM_base):
    """"""Student-t Linear Discriminant Analysis with FEM-Inspired parameter
    estimation (discriminant function as usual).
    """"""
    def _estimate_parameters(self, X):
        """"""Estimate parameters (mean, scatter) of one class with FEM algorithm,
        according to flexible Elliptically Symmetrical model, along with usual
        degrees of freedom.
        """"""
        params = self._estimate_parameters_with_FEM(X)
        return params + [fit_t_dof(X, *params, dof_0=3)]
    
class t_QDA_FEM(t_LDA_FEM):
    """"""Student-t Quadratic Discriminant Analysis with FEM-Inspired parameter
    estimation (discriminant function as usual).
    """"""
    def __init__(self, method='distributional'):
        super().__init__(method=method, pool_covs=False)
        

class FEMDA(QDA_FEM):
    """"""FEM-Inspired Discriminant Analysis implementation.
    FEM-Inspired parameter estimation along with FEM E_step discrimination.
    See `QDA_FEM` for more. Inherits from `QDA_FEM`.
    """"""
    def __init__(self):
        super().__init__(method='distributional')
    
    def _simple_mahalanobis(self, X, m, S):
        """"""Calculate Mahalanobis distance of data points.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
        m : array-like of shape (n_features,)
            Mean vector.
        S : array-like of shape (n_features, n_features)
            Scatter matrix.

        Returns
        -------
        d : ndarray of shape (n_samples,)
            Mahalanobis distances.
        """"""
        diff = X - m
        return (np.dot(diff, np.linalg.inv(S)) * diff).sum(axis=1)
    
    def _log_likelihoods(self, X):
        """"""Calculate log likelihood of new data per class according to Flexible
        Elliptically Symmetrical model using FEM E-step implementation.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            New data.

        Returns
        -------
        p : ndarray of shape (n_classes, n_samples)
            Log likelihood per class of new data.
        """"""
        N,p = X.shape
        log_maha = np.zeros((self._K, N))
        for k in range(self._K):
            log_maha[k, :] = np.log(self._simple_mahalanobis(X, 
                                    self.means_[:,k], self.covariance_[k,:,:]))
        
        _,logdets = np.linalg.slogdet(self.covariance_)
        pik = -0.5 * (p * log_maha + logdets[:,None])
        return pik


class FEMDA_N(FEMDA):
    """"""Experimental FEM-Inspired Discriminant Analysis implementation, but
    with pre-normalisation of data. This should produce similar results,
    since the FEMDA model fits an arbitrary scale parameter.
    See `FEMDA` for more.
    """"""
    def _normalise_centered(self, X, y, mean_estimator=fit_gaussian):
        """"""Estimate means and normalise data based around them.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.

        y : array-like of shape (n_samples,)
            Target values.
        
        mean_estimator : func, default=`_algo_utils.fit_gaussian`
            Fucnction which returns list of parameters, the first element of
            which returns a mean vector.
        """"""        
        def _normalise(a):
            return (a.T / np.linalg.norm(a, axis=1)).T
        X_copy = X.copy()
        for k in np.unique(y):
            mean = mean_estimator(X[y==k])[0]
            X_copy[y==k] = _normalise(X[y==k] - mean) + mean
        return X_copy

    def fit(self, X, y):
        """"""Fit pre-normalised FEMDA model according to data. The data are 
        pre-normalised using the means pre-estimated with FEM estimation.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.

        y : array-like of shape (n_samples,)
            Target values.
        """"""
        X_n = self._normalise_centered(X, y, mean_estimator = 
                    lambda x:_FEM_base()._estimate_parameters_with_FEM(x))
        print(""X"", X)
        print(""X-N"", X_n)
        return super().fit(X_n,y)

"
171,2201.02967,"from . import decision_rules_comparison_simulated_data as drcs
from . import decision_rules_comparison_real_data as drcr

simulation_gi                        = False
etude_ionosphere                     = True
etude_statlog                        = False
etude_breast_cancer                  = False
etude_ecoli                          = False
etude_spambase                       = False
visualization                        = True

# ""0-0.605-0.295-0.1-0 ; 1-0-0-0-0 ; 0-0.5-0.5-0-0 : 2""
path_results_simulated_data = ""C:/Users/a/Desktop/Stage Robust QDA_LDA L2S/TEST GIT/Simulations données simulées/""
path_results_real_data      = ""C:/Users/a/Desktop/Stage Robust QDA_LDA L2S/TEST GIT/Simulations données réelles/""
path_dataset                = ""C:/Users/a/Desktop/Stage Robust QDA_LDA L2S/TEST GIT/Datasets/""

nb_simulations_MC   = 100
p_conta             = 0.
perc_train_set_used = 1
freq_shuffle        = 10
pas                 = 0.1


def labo_test(path_results_simulated_data=path_results_simulated_data,
              path_results_real_data=path_results_real_data,
              path_dataset=path_dataset
             ):

    ####################################################################################################################################
    ###################################                                                              ###################################
    ###################################                  Simulations données simulées                ###################################
    ###################################                                                              ###################################
    ####################################################################################################################################

    if simulation_gi:
    
        n                 = 25000 # 80% for train set and 20% for test set
        K                 = 5
        priors            = [1/K for i in range(K)]
        m                 = 10
        simulation_id     = 1
        list_scenarios    = [""1-0-0-0-0 : 1"",
                            ""0-1-0-0-0 : 1"",
                            ""0-0-1-0-0 : 1"",
                            ""0-0-0-1-0 : 1"",
                            ""0-0-0-0-1 : 1"",
                            ""1-0-0-0-0 : 2"",
                            ""0-1-0-0-0 : 2"",
                            ""0-0-1-0-0 : 2"",
                            ""0-0-0-1-0 : 2"",
                            ""0-0-0-0-1 : 2"",
                            ""1-0-0-0-0 : 3"",
                            ""0-1-0-0-0 : 3"",
                            ""0-0-1-0-0 : 3"",
                            ""0-0-0-1-0 : 3"",
                            ""0-0-0-0-1 : 3"",
                            ""0-0.5-0-0.5-0 : 1"",
                            ""0-0.5-0-0.5-0 : 2"",
                            ""0-0.5-0-0.5-0 : 3"",
                            ""0-0.34-0-0.33-0.33 : 1"",
                            ""0-0.34-0-0.33-0.33 : 2"",
                            ""0-0.34-0-0.33-0.33 : 3"",
                            ""0-0.25-0.25-0.25-0.25 : 1"",
                            ""0-0.25-0.25-0.25-0.25 : 2"",
                            ""0-0.25-0.25-0.25-0.25 : 3"",
                            ""0-0.25-0.5-0.25-0 : 3""
                            ]
        
        drcs.write_parameters_file(path_results_simulated_data, m, n, K, priors, 0.00, list_scenarios, simulation_id)
        drcs.save_results(path_results_simulated_data, m, n, K, priors, list_scenarios, 0.00, simulation_id, nb_simulations_MC)

        drcs.write_parameters_file(path_results_simulated_data, m, n, K, priors, 0.10, list_scenarios, simulation_id)
        drcs.save_results(path_results_simulated_data, m, n, K, priors, list_scenarios, 0.10, simulation_id, nb_simulations_MC)

        drcs.write_parameters_file(path_results_simulated_data, m, n, K, priors, 0.25, list_scenarios, simulation_id)
        drcs.save_results(path_results_simulated_data, m, n, K, priors, list_scenarios, 0.25, simulation_id, nb_simulations_MC)

    ####################################################################################################################################
    ###################################                                                              ###################################
    ###################################                  Simulations données réelles                 ###################################
    ###################################                                                              ###################################
    ####################################################################################################################################

    if etude_ionosphere:
        dataset_name = ""Ionosphere""
        for P_conta in [i * pas for i in range(int(1/pas) + 1)]:
            drcr.save_results(path_results_real_data, dataset_name, nb_simulations_MC = nb_simulations_MC, freq_shuffle = freq_shuffle, p_conta = P_conta, perc_train_set_used = perc_train_set_used)
        #for Perc_train_set_used in [i * pas for i in range(1, int(1/pas))]:
            #drcr.save_results(path_results_real_data, dataset_name, nb_simulations_MC = nb_simulations_MC, freq_shuffle = freq_shuffle, p_conta = p_conta, perc_train_set_used = Perc_train_set_used)
                
    if etude_statlog:
        dataset_name = ""Statlog""
        for P_conta in [i * pas for i in range(int(1/pas) + 1)]:
            drcr.save_results(path_results_real_data, dataset_name, nb_simulations_MC = nb_simulations_MC, freq_shuffle = freq_shuffle, p_conta = P_conta, perc_train_set_used = perc_train_set_used)
        #for Perc_train_set_used in [i * pas for i in range(1, int(1/pas))]:
            #drcr.save_results(path_results_real_data, dataset_name, nb_simulations_MC = nb_simulations_MC, freq_shuffle = freq_shuffle, p_conta = p_conta, perc_train_set_used = Perc_train_set_used)

    if etude_breast_cancer:
        dataset_name = ""Breast cancer""
        for P_conta in [i * pas for i in range(int(1/pas) + 1)]:
            drcr.save_results(path_results_real_data, dataset_name, nb_simulations_MC = nb_simulations_MC, freq_shuffle = freq_shuffle, p_conta = P_conta, perc_train_set_used = perc_train_set_used)
        #for Perc_train_set_used in [i * pas for i in range(1, int(1/pas))]:
            #drcr.save_results(path_results_real_data, dataset_name, nb_simulations_MC = nb_simulations_MC, freq_shuffle = freq_shuffle, p_conta = p_conta, perc_train_set_used = Perc_train_set_used)
            
    if etude_ecoli:
        dataset_name = ""Ecoli""
        for P_conta in [i * pas for i in range(int(1/pas) + 1)]:
            drcr.save_results(path_results_real_data, dataset_name, nb_simulations_MC = nb_simulations_MC, freq_shuffle = freq_shuffle, p_conta = P_conta, perc_train_set_used = perc_train_set_used)
        #for Perc_train_set_used in [i * pas for i in range(1, int(1/pas))]:
            #drcr.save_results(path_results_real_data, dataset_name, nb_simulations_MC = nb_simulations_MC, freq_shuffle = freq_shuffle, p_conta = p_conta, perc_train_set_used = Perc_train_set_used)
            
    if etude_spambase:
        dataset_name = ""Spambase""
        for P_conta in [i * pas for i in range(int(1/pas) + 1)]:
            drcr.save_results(path_results_real_data, dataset_name, nb_simulations_MC = nb_simulations_MC, freq_shuffle = freq_shuffle, p_conta = P_conta, perc_train_set_used = perc_train_set_used)
        #for Perc_train_set_used in [i * pas for i in range(1, int(1/pas))]:
            #drcr.save_results(path_results_real_data, dataset_name, nb_simulations_MC = nb_simulations_MC, freq_shuffle = freq_shuffle, p_conta = p_conta, perc_train_set_used = Perc_train_set_used)
                
    ####################################################################################################################################
    ###################################                                                              ###################################
    ###################################                     Tracé des résultats                      ###################################
    ###################################                                                              ###################################
    ####################################################################################################################################     
        
    if visualization :   
        conta_min         = 0.
        conta_max         = 1
        dataset_used_min  = 0.1
        dataset_used_max  = 1
        methods1          = [""LDA_g - classic"", ""LDA_g - M"", ""QDA_g - classic"", ""LDA_t""]
        methods2          = [ ""QDA_g - M"", ""GQDA"", ""QDA_t"", ""FEMDA""]
        
        dataset_name      = ""Ionosphere""
        name_conta        = ""evolution - contamination - All results""
        name_dataset_used = ""evolution - dataset used - All results""
        
        drcr.plot_results_contamination_rate(path_results_real_data, dataset_name, methods2, name_conta, conta_min = conta_min, conta_max = conta_max)
        #drcr.plot_results_dataset_used(path_results_real_data, dataset_name, methods2, name_dataset_used, dataset_used_min = dataset_used_min, dataset_used_max = dataset_used_max)


if __name__ == ""__main__"":
    labo_test()"
172,2201.02967,"from .labo_test import labo_test

import json, os

def run_experiments():
    def full_path(path):
        return os.path.join(os.path.abspath(os.path.dirname(__file__)), path)

    with open(full_path(""paths.config"")) as f:
        data = json.load(f)

    labo_test(path_results_simulated_data=data[""path_results_simulated_data""],
              path_results_real_data=data[""path_results_real_data""],
              path_dataset=data[""path_dataset""])

if __name__ == ""__main__"":
    run_experiments()"
173,2201.02967,"from numpy.random import multivariate_normal, gamma, wald
from scipy.linalg import sqrtm
import numpy as np

I_WANT_FIXED_PARAMETERS_FOR_SIMULATIONS = True
# Works with K = 5 and m = 10

SIGMA0 = np.array([[ 0.65963099, -0.23174501, -0.37255014,  0.16047307, -0.06986632,
                    -0.20351644, -0.04244815, -0.20316376,  0.01801177, -0.12203187],
                   [-0.23174501,  0.77008355,  0.2700138 , -0.05366752,  0.11668053,
                    0.01736836,  0.38286752, -0.43575001,  0.15860259,  0.11176599],
                   [-0.37255014,  0.2700138 ,  0.80912915,  0.1266928 ,  0.28429233,
                    0.21658211, -0.15497937, -0.01667027, -0.11857219, -0.2050802 ],
                   [ 0.16047307, -0.05366752,  0.1266928 ,  1.07968243,  0.13513597,
                    0.04036425,  0.15355428, -0.19240103, -0.02517887,  0.05847   ],
                   [-0.06986632,  0.11668053,  0.28429233,  0.13513597,  0.70265271,
                    -0.19692394, -0.15044429, -0.02987165, -0.26396171,  0.070615  ],
                   [-0.20351644,  0.01736836,  0.21658211,  0.04036425, -0.19692394,
                    0.97534942, -0.02522256, -0.07920685, -0.1409119 ,  0.08512524],
                   [-0.04244815,  0.38286752, -0.15497937,  0.15355428, -0.15044429,
                    -0.02522256,  1.21658996, -0.07048257,  0.15690868, -0.16288668],
                   [-0.20316376, -0.43575001, -0.01667027, -0.19240103, -0.02987165,
                    -0.07920685, -0.07048257,  1.2744286 ,  0.02920179,  0.25563273],
                   [ 0.01801177,  0.15860259, -0.11857219, -0.02517887, -0.26396171,
                    -0.1409119 ,  0.15690868,  0.02920179,  1.38021804, -0.12277992],
                   [-0.12203187,  0.11176599, -0.2050802 ,  0.05847   ,  0.070615  ,
                    0.08512524, -0.16288668,  0.25563273, -0.12277992,  1.13223515]])

SIGMA1 = np.array([[ 1.57255113,  0.15394067,  0.05463296,  0.00341056,  0.11387236,
                    0.07881721, -0.42883195, -0.07760481,  0.13847189, -0.07038395],
                   [ 0.15394067,  0.94004185, -0.01871733,  0.0551    , -0.10265995,
                    0.03227707, -0.1653636 , -0.13222739,  0.02981121,  0.01419475],
                   [ 0.05463296, -0.01871733,  0.76406987,  0.03940517, -0.02125184,
                    0.00638847, -0.07361563,  0.00727309,  0.08105846, -0.12739615],
                   [ 0.00341056,  0.0551    ,  0.03940517,  0.96789186,  0.3015674 ,
                    0.01943675, -0.26457125,  0.36272305, -0.11250757,  0.07590622],
                   [ 0.11387236, -0.10265995, -0.02125184,  0.3015674 ,  1.12694743,
                    0.27093418, -0.23708886,  0.25502555,  0.0948158 ,  0.01077614],
                   [ 0.07881721,  0.03227707,  0.00638847,  0.01943675,  0.27093418,
                    1.10353602,  0.04659414,  0.46909059,  0.03359708,  0.20640832],
                   [-0.42883195, -0.1653636 , -0.07361563, -0.26457125, -0.23708886,
                    0.04659414,  0.82719324,  0.19670008, -0.10413831, -0.01143578],
                   [-0.07760481, -0.13222739,  0.00727309,  0.36272305,  0.25502555,
                    0.46909059,  0.19670008,  0.79450201, -0.12368953, -0.21250651],
                   [ 0.13847189,  0.02981121,  0.08105846, -0.11250757,  0.0948158 ,
                    0.03359708, -0.10413831, -0.12368953,  0.39661602,  0.23270783],
                   [-0.07038395,  0.01419475, -0.12739615,  0.07590622,  0.01077614,
                    0.20640832, -0.01143578, -0.21250651,  0.23270783,  1.50665057]])

SIGMA2 = np.array([[ 0.74616095, -0.14289427,  0.24475873, -0.34032334, -0.46570708,
                    0.13192312, -0.0472028 , -0.08081196,  0.03042543,  0.2510243 ],
                   [-0.14289427,  1.59983138,  0.11662554,  0.21404798, -0.15736453,
                    0.30960642,  0.572066  , -0.1396824 ,  0.33606045,  0.00773204],
                   [ 0.24475873,  0.11662554,  1.35307178, -0.26712472,  0.38760697,
                    0.5444736 , -0.19175407, -0.07336361, -0.14210624,  0.11434187],
                   [-0.34032334,  0.21404798, -0.26712472,  1.033906  ,  0.29934718,
                    -0.17624372, -0.11972883,  0.2397499 ,  0.20891969,  0.18148452],
                   [-0.46570708, -0.15736453,  0.38760697,  0.29934718,  1.24835245,
                    0.22939918, -0.23697436,  0.21181617,  0.0595435 ,  0.18773035],
                   [ 0.13192312,  0.30960642,  0.5444736 , -0.17624372,  0.22939918,
                    0.74671528, -0.00181501, -0.05344971,  0.01432925, -0.10097918],
                   [-0.0472028 ,  0.572066  , -0.19175407, -0.11972883, -0.23697436,
                    -0.00181501,  0.47972939,  0.0031951 ,  0.09609802,  0.00612826],
                   [-0.08081196, -0.1396824 , -0.07336361,  0.2397499 ,  0.21181617,
                    -0.05344971,  0.0031951 ,  0.67084171,  0.04583288,  0.14154079],
                   [ 0.03042543,  0.33606045, -0.14210624,  0.20891969,  0.0595435 ,
                    0.01432925,  0.09609802,  0.04583288,  0.91453598,  0.35854144],
                   [ 0.2510243 ,  0.00773204,  0.11434187,  0.18148452,  0.18773035,
                    -0.10097918,  0.00612826,  0.14154079,  0.35854144,  1.20685509]])

SIGMA3 = np.array([[ 0.68654009, -0.18871367,  0.00418124, -0.2736131 , -0.23854285,
                    0.11708568, -0.17460049,  0.09652099, -0.06888699,  0.07304049],
                   [-0.18871367,  0.73127449,  0.17724311,  0.02935562, -0.09440512,
                    0.30058656,  0.34717253,  0.10387657,  0.364108  , -0.00674574],
                   [ 0.00418124,  0.17724311,  1.13667454, -0.00905685,  0.24217548,
                    0.44949837,  0.08138781,  0.0207203 ,  0.28462587, -0.15617147],
                   [-0.2736131 ,  0.02935562, -0.00905685,  0.91970987, -0.19524422,
                    0.14813278, -0.06289064, -0.16049415, -0.01612038, -0.26884326],
                   [-0.23854285, -0.09440512,  0.24217548, -0.19524422,  0.63638707,
                    -0.26494111,  0.14423224,  0.2617986 , -0.06097454, -0.13733077],
                   [ 0.11708568,  0.30058656,  0.44949837,  0.14813278, -0.26494111,
                    0.83568667, -0.23482211,  0.10365356,  0.00956471, -0.19038602],
                   [-0.17460049,  0.34717253,  0.08138781, -0.06289064,  0.14423224,
                    -0.23482211,  1.18284553, -0.28575775,  0.01723174, -0.4623737 ],
                   [ 0.09652099,  0.10387657,  0.0207203 , -0.16049415,  0.2617986 ,
                    0.10365356, -0.28575775,  1.05365748, -0.42985385, -0.08982747],
                   [-0.06888699,  0.364108  ,  0.28462587, -0.01612038, -0.06097454,
                    0.00956471,  0.01723174, -0.42985385,  1.82280996, -0.01411021],
                   [ 0.07304049, -0.00674574, -0.15617147, -0.26884326, -0.13733077,
                    -0.19038602, -0.4623737 , -0.08982747, -0.01411021,  0.99441431]])

SIGMA4 = np.array([[ 0.79614009, -0.15534088, -0.3745037 , -0.1634612 ,  0.08233212,
                    -0.04322898,  0.05513867, -0.0729146 ,  0.1232276 ,  0.09514593],
                   [-0.15534088,  0.77474391,  0.36996305,  0.11754211, -0.1706926 ,
                    -0.07565772,  0.13957162,  0.21140293,  0.06393028,  0.00444412],
                   [-0.3745037 ,  0.36996305,  1.2007165 ,  0.06394929, -0.47870594,
                    -0.25006592, -0.28264067,  0.13747703, -0.08897225, -0.14165621],
                   [-0.1634612 ,  0.11754211,  0.06394929,  1.04927075, -0.03410715,
                    0.37253947, -0.114177  ,  0.26939607, -0.12586309,  0.18616308],
                   [ 0.08233212, -0.1706926 , -0.47870594, -0.03410715,  0.86659916,
                    -0.00596463,  0.03910985, -0.16473423,  0.04261439,  0.07442695],
                   [-0.04322898, -0.07565772, -0.25006592,  0.37253947, -0.00596463,
                    1.24058473, -0.19709553, -0.13078999, -0.28692008,  0.09286908],
                   [ 0.05513867,  0.13957162, -0.28264067, -0.114177  ,  0.03910985,
                    -0.19709553,  0.87597244,  0.13034726,  0.4095738 ,  0.31523726],
                   [-0.0729146 ,  0.21140293,  0.13747703,  0.26939607, -0.16473423,
                    -0.13078999,  0.13034726,  0.94480859,  0.22053224,  0.19272972],
                   [ 0.1232276 ,  0.06393028, -0.08897225, -0.12586309,  0.04261439,
                    -0.28692008,  0.4095738 ,  0.22053224,  1.17925115,  0.3258996 ],
                   [ 0.09514593,  0.00444412, -0.14165621,  0.18616308,  0.07442695,
                    0.09286908,  0.31523726,  0.19272972,  0.3258996 ,  1.07191267]])

SIGMA_NOISE_0 = np.array([[ 0.70387844,  0.14173733, -0.1872618 , -0.16934332, -0.0779969 ,
                          0.01233009,  0.22669491,  0.13406542,  0.02045725, -0.60579917],
                          [ 0.14173733,  1.77504211, -0.17394353, -0.48658065, -0.23040451,
                         -0.48490723,  0.05100652,  0.04386135, -0.02668856, -0.41524843],
                          [-0.1872618 , -0.17394353,  1.16927814,  0.10914491, -0.01737274,
                          0.13384749, -0.10386102, -0.45846455,  0.86628261, -0.32060205],
                          [-0.16934332, -0.48658065,  0.10914491,  1.075194  ,  0.462886  ,
                          0.3316134 , -0.2486594 , -0.16670795, -0.09845273,  0.34838196],
                          [-0.0779969 , -0.23040451, -0.01737274,  0.462886  ,  0.55475284,
                         -0.25200362, -0.10616487,  0.10608942, -0.22494921,  0.06748856],
                          [ 0.01233009, -0.48490723,  0.13384749,  0.3316134 , -0.25200362,
                          1.14017806, -0.09850892, -0.24585623,  0.33054262,  0.15891042],
                          [ 0.22669491,  0.05100652, -0.10386102, -0.2486594 , -0.10616487,
                         -0.09850892,  0.27150049,  0.15222821,  0.04563598, -0.26080494],
                          [ 0.13406542,  0.04386135, -0.45846455, -0.16670795,  0.10608942,
                         -0.24585623,  0.15222821,  0.65093622, -0.44480501,  0.17001313],
                          [ 0.02045725, -0.02668856,  0.86628261, -0.09845273, -0.22494921,
                          0.33054262,  0.04563598, -0.44480501,  1.48565505, -0.37306758],
                          [-0.60579917, -0.41524843, -0.32060205,  0.34838196,  0.06748856,
                          0.15891042, -0.26080494,  0.17001313, -0.37306758,  1.17358465]])

SIGMA_NOISE_1 = np.array([[ 0.71380881, -0.22519285, -0.48650475, -0.48859699, -0.03111683,
                        -0.23206183,  0.23228126, -0.2687057 ,  0.34174352, -0.35568404],
                         [-0.22519285,  0.81695701,  0.12153592, -0.23279644, -0.06985542,
                         0.01058409,  0.0554797 , -0.2229638 , -0.06271049, -0.34301576],
                         [-0.48650475,  0.12153592,  0.93295689,  0.3588545 ,  0.22169986,
                         0.19905399, -0.38066591, -0.10445448, -0.48790529,  0.10227753],
                         [-0.48859699, -0.23279644,  0.3588545 ,  1.25567426, -0.22228897,
                         0.49895338, -0.06066179,  0.39322836, -0.50709515,  0.65615351],
                         [-0.03111683, -0.06985542,  0.22169986, -0.22228897,  2.13340116,
                        -0.88626188, -0.19748381, -0.01316109, -0.39868582,  0.33222362],
                         [-0.23206183,  0.01058409,  0.19905399,  0.49895338, -0.88626188,
                         0.85506613,  0.03975364,  0.07713491, -0.14040749,  0.17435679],
                         [ 0.23228126,  0.0554797 , -0.38066591, -0.06066179, -0.19748381,
                         0.03975364,  0.40687872, -0.19462902,  0.04109253, -0.13466775],
                         [-0.2687057 , -0.2229638 , -0.10445448,  0.39322836, -0.01316109,
                         0.07713491, -0.19462902,  0.75310185, -0.10314714,  0.3866746 ],
                         [ 0.34174352, -0.06271049, -0.48790529, -0.50709515, -0.39868582,
                        -0.14040749,  0.04109253, -0.10314714,  0.83657234, -0.17576316],
                         [-0.35568404, -0.34301576,  0.10227753,  0.65615351,  0.33222362,
                         0.17435679, -0.13466775,  0.3866746 , -0.17576316,  1.29558282]])

MU0 = np.array([-0.13040322,  0.21831241,  0.13650351,  0.43166859, -0.37257364,
                0.6214003 ,  0.02152636,  0.33358624,  0.306053  , -0.00162893])

MU1 = np.array([ 0.06371455,  0.43615313, -0.21163921, -0.31489917,  0.23063918,
                0.50978355,  0.36228166, -0.1824809 ,  0.42808702, -0.02964434])

MU2 = np.array([ 0.16112972, -0.32765945,  0.00568319,  0.44179632,  0.21672135,
                0.29812011, -0.13066803,  0.51344744, -0.10274407, -0.49432552])

MU3 = np.array([ 0.52828442,  0.03491522,  0.18162774,  0.31647269,  0.24746236,
                -0.48090486, -0.10598252,  0.39150647,  0.26663308,  0.24174984])

MU4 = np.array([ 0.12424547,  0.04525731, -0.23328742,  0.22147227,  0.003485  ,
             -0.20504156, -0.06600664,  0.07885775, -0.9089108 , -0.0171292 ])

MU_NOISE_0 = np.array([ 0.09142525, -0.21008614,  0.12088316, -0.1330825 , -0.22217068,
                       -0.4905775 , -0.07622752, -0.54425252, -0.36449634,  0.43620687])

MU_NOISE_1 = np.array([-0.07642326, -0.21307132,  0.39790428, -0.4972497 , -0.07474425,
                       -0.10843697, -0.18178622, -0.4420889 ,  0.54399567,  0.03754497])

def genereRandomCovarianceMatrix(m, shape=1):
    
    """""" Randomly generates a covariance matrix with Tr = m by first generating random eigenvalues and then 
        a random orthogonal matrix. The orthogonal matrix is drawn uniformly on O(m) and the 
        eigenvalues are drawn with a truncated N(1, shape**2).
    
    Parameters
    ----------
    m     : integer > 0
            dimension of the data
    shape : float 
            Standard deviation of the gaussian distribution of eigenvalues 
    Returns
    -------
    sigma : 2-d array, positive-definite matrix
            random covariance matrix with controlled eigenvalues and Tr = m
    """"""
    
    stretch = 1 # parameter to stretch the covariance matrix 
    sigma_diag = np.diag(shape*np.random.rand(m))
    for i in range(m):
        rnd = np.random.rand()
        if 0.00 < rnd < 0.25:
            sigma_diag[i][i] = sigma_diag[i][i] / stretch
        if 0.25 < rnd < 0.50:
            sigma_diag[i][i] = sigma_diag[i][i] / (2 * stretch)
        if 0.50 < rnd < 0.75:
            sigma_diag[i][i] = sigma_diag[i][i] * stretch
        if 0.75 < rnd < 1.00:
            sigma_diag[i][i] = sigma_diag[i][i] * 2 * stretch
            
    u, s, vh   = np.linalg.svd(np.random.randn(m, m), full_matrices=False)
    mat_rot    = np.dot(u, vh)
    sigma      = np.dot(mat_rot, np.dot(sigma_diag, mat_rot.T))

    return sigma * m / np.matrix.trace(sigma)

def genere_all_mu(m, K, r=1):
    
    """""" Randomly generates the centers of the clusters on the m-dimensional r-sphere.
    
    Parameters
    ----------
    m      : integer > 0
             dimension of the data
    K      : integer > 0
             number of clusters
    r      : float > 0
             radius of the sphere where the centers are randomly drawn
    Returns
    -------
    all_mu         : 2-d array of size K*m
                     Matrix of the mean vectors of size m of the K clusters
    """"""
    
    all_mu = []
    for k in range(K):
         all_mu.append(random_sphere_point(m)*r)
         
    if I_WANT_FIXED_PARAMETERS_FOR_SIMULATIONS:
        return np.array([MU0, MU1, MU2, MU3, MU4])
    else:
        return np.array(all_mu)

def genere_all_sigma(m, K):
    
    """""" Randomly generates the shape matrix of the clusters.
    
    Parameters
    ----------
    m      : integer > 0
             dimension of the data
    K      : integer > 0
             number of clusters
    Returns
    -------
    all_sigma      : 3-d array of size K*m*m
                     Tensor of the shape matrix of the K clusters
    """"""
    
    all_sigma = []
    for k in range(K):
        all_sigma.append(genereRandomCovarianceMatrix(m))
        
    if I_WANT_FIXED_PARAMETERS_FOR_SIMULATIONS:
        return np.array([SIGMA0, SIGMA1, SIGMA2, SIGMA3, SIGMA4])  
    else:       
        return np.array(all_sigma)

def genere_all_PDF(scenario, K, test_real_shift, range_beta = 10, range_nu = 10, q = 1000):
    
    """""" Randomly generates the matrix of the eventually identical K*q distributions that will be used
        to generate the points of each cluster. Row k of the matrix corresponds to the q distributions
        available to generate the points of cluster k. There are four families of disributions available :
            -> Generalized gaussian distributions
            -> Inverse gaussian distributions
            -> t-distributions
            -> k-distributions
        distributions are generated according to a scenario. For example,  ""0-0.5-0-0.5-0 : 1"" means :
            -> 0%  of multivariate classic gaussian distributions
            -> 50% of multivariate generalized gaussian distributions
            -> 0%  of multivariate inverse gaussian distributions
            -> 50% of multivariate t-distributions
            -> 0%  of multivariate k-distributions
            
            -> 1 : parameters for all distributions families will be the same for all the points of all clusters
            -> 2 : parameters for all distributions families will be the same for all the points of the same clusters
            -> 3 : parameters for all distributions families will be different for all the points
        
        finally, it is possible to combine different scenarios for the clusters by concatenating different mixture with
        a ; such as ""0-0.25-0.25-0.25-0.25 ; 0-0.5-0-0.5-0 ; 0-0.34-0-0.33-0.33 ; 0-1-0-0-0 : 3"".
            
    Parameters
    ----------
    scenario   : str
                 scenario used to generate the data
    K          : integer > 0
                 number of clusters
    range_beta : integer >=0
                 beta parameter for generalized and inverse gaussian distribution families are drawn in [1 ; 1 + range_beta]
    range_nu   : integer >=0
                 nu parameter for generalized and inverse gaussian distribution families are drawn in [1 ; 1 + range_beta]
    q          : integer >=0
                 number of distributions used to generate the points of one cluster
    Returns
    -------
    all_PDF    : 2-d array of distributions of size K*q
                 matrix of the eventually identical K*q distributions used to generate the points of all the clusters
    """"""
    
    type_melanges, parametres = scenario.split("" : "")[0], int(scenario.split("" : "")[1])
    types_clusters    = type_melanges.split("" ; "")
    nb_types_clusters = len(types_clusters)
    
    if parametres == 1:
        matrix_beta = np.ones([K, q]) * (0.25 + np.random.rand() * range_beta)
        matrix_nu   = np.ones([K, q]) * (1 + np.random.rand() * range_nu)
    if parametres == 2:
        matrix_beta = 0.25 + np.random.rand(K, 1) @ np.ones([1, q]) * range_beta
        matrix_nu   = 1 + np.random.rand(K, 1) @ np.ones([1, q]) * range_nu
    if parametres == 3:
        matrix_beta = 0.25 + np.random.rand(K, q) * range_beta
        matrix_nu   = 1 + np.random.rand(K, q) * range_nu
    
    def genere_cluster_PDF(type_cluster):
        
        a, b, c, d, _ = float(type_cluster.split(""-"")[0]), float(type_cluster.split(""-"")[1]), float(type_cluster.split(""-"")[2]), float(type_cluster.split(""-"")[3]), float(type_cluster.split(""-"")[4])
        rnd = np.random.rand(q)
        return [0*(rnd[j]<a) + 1*(a<=rnd[j]<a+b) + 2*(a+b<=rnd[j]<a+b+c) + 3*(a+b+c<=rnd[j]<a+b+c+d) + 4*(a+b+c+d <= rnd[j]) for j in range(q)]
    
    matrix_PDF = [genere_cluster_PDF(types_clusters[np.random.randint(nb_types_clusters)]) for i in range(K)]
    all_PDF    = [[lambda mu, sigma, tau, nu = matrix_nu[i][j], beta = matrix_beta[i][j], PDF = matrix_PDF[i][j] : multivariate_generalized_gaussian(mu, sigma, tau, 1) * (PDF == 0) + multivariate_generalized_gaussian(mu, sigma, tau, beta) * (PDF == 1) + multivariate_inverse_gaussian(mu, sigma, tau, beta) * (PDF == 2) + multivariate_t(mu, sigma, tau, nu) * (PDF == 3) + multivariate_k(mu, sigma, tau, nu) * (PDF == 4) for j in range(q)] for i in range(K)]
    
    if test_real_shift:
        list_betas = [i for i in range(1,100,10)]
        all_PDF    = [[lambda mu, sigma, tau, beta = list_betas[i] : multivariate_generalized_gaussian(mu, sigma, tau, beta)] for i in range(10)]
    
    return all_PDF

def genere_parametres_simulation(m, n, K, priors, scenario, p_conta, test_real_shift = False):
    
    """""" Generates the parameters for the simulation.
    
    Parameters
    ----------
    m        : integer > 0
               dimension of the data
    n        : integer > 0
               number of samples generated      
    K        : integer > 0
               number of clusters
    priors   : 1-d list of float of size K
               list of probability of all clusters
    scenario : str
               scenario used to generate the data
    p_conta  : float >= 0
               probability of drawing a contaminated sample  
    Returns
    -------
    n         : integer > 0
                number of samples generated 
    priors    : 1-d list of float of size K
                list of probability of all clusters
    all_mu    : 2-d array of size K*m
                matrix of the mean vectors of size m of the K clusters
    all_sigma : 3-d array of size K*m*m
                tensor of the shape matrix of the K clusters
    all_tau   : 1-d list of size K
                list of K functions to simulate tau for each cluster
    all_PDF        : list of K lists of potentially different sizes, each sub-list
                     indicates all PDF available to generate a sample for each cluster. 
                     For each generation, a PDF is chosen uniformly randomly among the 
                     ones availables.
    p_conta : float >= 0
                     Probability of drawing a contaminated sample
    conta   : function 
                     Takes as input mean and covariance matrix and returns a contaminated sample      
    """"""
    
    def conta():
    
        """""" Generate a contaminated sample using one of the two fixed-distributions to add noise.
        
        Returns
        -------
        x     : 1-d array of size m
                contaminated sample generated
        """"""
        if np.random.rand() > 0.5:
            return multivariate_normal(MU_NOISE_0, SIGMA_NOISE_0)
        else:
            return multivariate_normal(MU_NOISE_1, SIGMA_NOISE_1)

    def Tau(a, b):
        
        """""" Generates a nuisance parameter as a random real drawn between a and b.
        
        Parameters
        ----------
        a   : float > 0
              lower bound for the random drawing
        b   : float > 0
              upper bound for the random drawing
        Returns
        -------
        tau : float > 0
              nuisance parameter
        """"""
        return a + np.random.rand() * (b - a)

    list_range_tau = [(1, 1) for k in range(K)]
    all_tau = [lambda a = list_range_tau[i][0], b = list_range_tau[i][1] : Tau(1,10) for i in range(K)]
    all_PDF = genere_all_PDF(scenario, K, test_real_shift)

    return n, priors, genere_all_mu(m, K), genere_all_sigma(m, K), all_tau, all_PDF, p_conta, conta
    
def random_sphere_point(m):

    """""" Generate a point uniformly drawn on the unit m-dimensional sphere
    
    Parameters
    ----------
    m : integer > 0
        dimension of the data
    Returns
    -------
    x : 1-d array of size m
        sample generated
    """"""   
    
    Z = np.random.normal(0, 1, m)
    
    return Z / np.sqrt(sum(Z**2))

def multivariate_generalized_gaussian(mu, sigma, p, beta):
    
    """""" Generate a sample drawn from a multivariate generalized gaussian distribution.
    
    Parameters
    ----------
    mu    : 1-d array of size m
            mean of the distribution
    sigma : 2-d array of size m*m
            shape matrix with det = 1
    p     : float > 0
            scale parameter
    beta  : float > 0
            shape parameter
    Returns
    -------
    x     : 1-d array of size m
            sample generated
    """"""

    return mu + gamma(len(mu) / (2 * beta), 2) ** (1 / (2 * beta)) * np.dot(sqrtm(p * sigma), random_sphere_point(len(mu)))

def multivariate_inverse_gaussian(mu, sigma, p, beta):
    
    """""" Generate a sample drawn from a multivariate t distribution.
    
    Parameters
    ----------
    mu    : 1-d array of size m
            mean of the distribution
    sigma : 2-d array of size m*m
            shape matrix with det = 1
    p     : float > 0
            scale parameter
    beta  : float > 0
            shape parameter
    Returns
    -------
    x     : 1-d array of size m
            sample generated
    """"""
    
    return mu + multivariate_normal(np.zeros(len(mu)), p * sigma) * np.sqrt(wald(1, beta))

def multivariate_t(mu, sigma, p, nu):
    
    """""" Generate a sample drawn from a multivariate t distribution.
    
    Parameters
    ----------
    mu    : 1-d array of size m
            mean of the distribution
    sigma : 2-d array of size m*m
            shape matrix with det = 1
    p     : float > 0
            scale parameter
    nu    : integer > 0
            Degree of freedom of the distribution
    Returns
    -------
    x     : 1-d array of size m
            sample generated
    """"""
    
    return mu + multivariate_normal(np.zeros(len(mu)), p * sigma) * np.sqrt(1/gamma(nu/2, 2/nu))

def multivariate_k(mu, sigma, p, nu):
    
    """""" Generate a sample drawn from a multivariate t distribution.
    
    Parameters
    ----------
    mu    : 1-d array of size m
            mean of the distribution
    sigma : 2-d array of size m*m
            shape matrix with det = 1
    p     : float > 0
            scale parameter
    nu    : integer > 0
            Degree of freedom of the distribution
    Returns
    -------
    x     : 1-d array of size m
            sample generated
    """"""
  
    return mu + multivariate_normal(np.zeros(len(mu)), p * sigma) * np.sqrt(gamma(nu, 1/nu))

class dataSimulation():
    
    """""" Implements an object to simulate data.   
    
    Parameters
    ----------
    n         : integer > 0
                number of samples generated
    all_pi    : 1-d array of size K
                vector of probability of all clusters
    all_mu    : 2-d array of size K*m
                matrix of the mean vectors of size m of the K clusters
    all_sigma : 3-d array of size K*m*m
                tensor of the shape matrix of the K clusters
    all_tau   : 1-d list of size K
                list of K functions to simulate tau for each cluster
    all_PDF   : list of K lists of potentially different sizes, each sub-list
                indicates all PDF available to generate a sample for each cluster. 
                For each generation, a PDF is chosen uniformly randomly among the 
                ones availables.
    p_conta   : float >= 0
                probability of drawing a contaminated sample
    conta     : function 
                takes as input mean and covariance matrix and returns a contaminated sample
    Attributes
    ----------
    m         : integer > 0  
                dimension of each sample
    K         : integer > 0
                number of clusters
    n         : integer > 0
                number of samples generated
    all_pi    : 1-d array of size K
                vector of probability of all clusters
    all_mu    : 2-d array of size K*m
                matrix of the mean vectors of size m of the K clusters
    all_sigma : 3-d array of size K*m*m
                tensor of the shape matrix of the K clusters
    all_tau   : 1-d list of size K
                list of K functions to simulate tau for each cluster
    all_PDF   : list of K lists of potentially different sizes, each sub-list
                indicates all PDF available to generate a sample for each cluster. 
                For each generation, a PDF is chosen uniformly randomly among the 
                ones availables.
    p_conta   : float >= 0
                probability of drawing a contaminated sample
    conta     : function 
                takes as input mean and covariance matrix and returns a contaminated sample
    X         : 2-d array of size n*m
                matrix of all the samples generated
    labels    : 1-d array of size n
                vector of the label of each sample
    PDFs      : 1-d array of size n
                vector of the index of the distribution chosen to draw the samples 
    Methods
    ----------
    generateSample  : Generates a random sample for cluster k
    generateSamples : Generates a random dataset of size n
    """"""
    
    
    def __init__(self, n, all_pi, all_mu, all_sigma, all_tau, all_PDF, p_conta, conta):
        
        self.m         = all_sigma.shape[1]
        self.K         = all_sigma.shape[0]
        self.n         = n
        self.all_pi    = all_pi
        self.all_mu    = all_mu
        self.all_sigma = all_sigma
        self.all_tau   = all_tau
        self.all_PDF   = all_PDF
        self.p_conta   = p_conta
        self.conta     = conta
        self.X         = np.zeros([n, all_sigma.shape[1]])
        self.labels    = np.zeros(n)
        self.PDFs      = np.zeros(n)
        
    def generateSample(self, k):

        mu    = self.all_mu[k]
        sigma = self.all_sigma[k]
        tau   = self.all_tau[k]()
        j     = np.random.randint(0, len(self.all_PDF[k]))
        PDF   = self.all_PDF[k][j]
        if np.random.rand() < self.p_conta:
            return self.conta(mu, tau*sigma), -1
        else:
            return PDF(mu, sigma, tau), j
    
    def generateSamples(self):
        
        for i in range(self.n):
            RND = np.random.rand()
            k = 0
            while RND > self.all_pi[k]:
                
                RND = RND - self.all_pi[k]
                k = k + 1
            self.X[i], self.PDFs[i] = self.generateSample(k)    
            self.labels[i] = k
        return self.X, self.labels, self.PDFs"
174,2201.02967,"import numpy as np
from sklearn.covariance import EllipticEnvelope
from scipy.special import digamma, loggamma, gamma
from scipy.optimize import minimize

def regularize(sigma, lambd = 1e-5):
    
    """""" Returns a regularized version of the matrix sigma to avoid singular matrix issues.
    
    Parameters
    ----------
    sigma : 2-d array of size m*m
            covariance matrix to regularize
    lambd : float
            covariance matrix is regularized by lambd * Id
    Returns
    -------
    sigma : 2-d array of size m*m
            regularized covariance matrix
    """""" 
    
    return sigma + np.eye(len(sigma)) * lambd

def classic_estimator(X, labels):

    """""" Estimates the matrix of means and the tensor of covariances matrix of the dataset.
    
    Parameters
    ----------
    X           : 2-d array of size n*m
                  matrix of all the samples generated
    labels      : 1-d array of size n
                  vector of the label of each sample
    Returns
    -------
    means       : 2-d array of size K*m
                  matrix of the estimation of the mean of the K clusters
    covariances : 3-d array of size K*m*m
                  tensor of the estimation of covariance matrix of the K clusters
    """""" 
    
    n, m        = X.shape
    K           = int(max(set(labels)) + 1)
    means       = np.zeros((K, m))
    covariances = np.array([np.eye(m)*1e-5 for i in range(K)])
    n_clusters  = np.zeros(K) + 1e-5
    
    for i in range(n): 
        means     [int(labels[i])] = means     [int(labels[i])] + X[i]
        n_clusters[int(labels[i])] = n_clusters[int(labels[i])] + 1
        
    for k in range(K):
        means[k] = means[k] / n_clusters[k]

    for i in range(n):    
        covariances[int(labels[i])] = covariances[int(labels[i])] + np.dot(np.array([X[i]-means[int(labels[i])]]).T, np.array([X[i]-means[int(labels[i])]])) / (n_clusters[int(labels[i])] - 1)
        
    return means, covariances
        
def M_estimator(X, labels, eps = 1e-5, max_iter = 20):
    
    """""" Estimates the matrix of means and the tensor of covariances matrix of the dataset using M-estimators.
        To tackle singular matrix issues, we use regularization.
        
    Parameters
    ----------
    X           : 2-d array of size n*m
                  matrix of all the samples generated
    labels      : 1-d array of size n
                  vector of the label of each sample
    eps         : float > 0
                  criterion of termination when solving the fixed-point equation
    max_iter    : integer > 1
                  number of maximum iterations to solve the fixed-point equation
    Returns
    -------
    means       : 2-d array of size K*m
                  matrix of the robust estimation of the mean of the K clusters
    covariances : 3-d array of size K*m*m
                  tensor of the robust estimation of covariance matrix of the K clusters
    """"""
    
    n, m          = X.shape
    K             = int(max(set(labels)) + 1)
    n_clusters    = np.zeros(K)
    for i in range(n): 
        n_clusters[int(labels[i])] = n_clusters[int(labels[i])] + 1
    means, covariances = classic_estimator(X, labels)
    for k in range(K):
        convergence      = False
        ite              = 1
        while (not convergence) and ite<max_iter:
            ite                      = ite + 1 
            mean                     = np.zeros(m)
            covariance               = np.zeros([m,m])
            sum_mean_weights         = 1e-5
            sum_mean_weights_squared = 1e-5
            for i in range(n):
                if labels[i] == k:
                    mean_weight              = min([[0.5]], 1 / np.sqrt(np.dot(np.array([X[i]-means[k]]), np.dot(np.linalg.inv(regularize(covariances[k])), np.array([X[i]-means[k]]).T))))[0][0]                       
                    mean                     = mean + mean_weight * X[i]
                    sum_mean_weights         = sum_mean_weights + mean_weight
                    sum_mean_weights_squared = sum_mean_weights_squared + mean_weight**2
                    covariance               = covariance + np.dot(np.array([X[i]-means[k]]).T, np.array([X[i]-means[k]])) * mean_weight**2
            delta_mean       = mean / sum_mean_weights - means[k]
            delta_covariance = covariance / sum_mean_weights_squared - covariances[k]
            means[k]         = means[k] + delta_mean
            covariances[k]   = covariances[k] + delta_covariance
            convergence      = sum(abs(delta_mean)) + sum(sum(abs(delta_covariance))) < eps
        covariances[k] = regularize(covariances[k])
    return means, covariances

def femda_estimator(X, labels, eps = 1e-5, max_iter = 20):
    
    """""" Estimates the matrix of means and the tensor of scatter matrix of the dataset using MLE estimator.
        To tackle singular matrix issues, we use regularization.
        
    Parameters
    ----------
    X        : 2-d array of size n*m
               matrix of all the samples generated
    labels   : 1-d array of size n
               vector of the label of each sample
    eps      : float > 0
               criterion of termination when solving the fixed-point equation
    max_iter : integer > 1
               number of maximum iterations to solve the fixed-point equation
    Returns
    -------
    means    : 2-d array of size K*m
               matrix of the robust estimation of the mean of the K clusters
    shapes   : 3-d array of size K*m*m
               tensor of the robust estimation of shape matrix of the K clusters
    """"""   
    
    n, m          = X.shape
    K             = int(max(set(labels)) + 1)
    n_clusters = np.zeros(K) + 1e-5
    for i in range(n): 
        n_clusters[int(labels[i])] = n_clusters[int(labels[i])] + 1
    means, shapes = classic_estimator(X, labels)

    for k in range(K):
        convergence      = False
        ite              = 1
        while (not convergence) and ite<max_iter:
            ite              = ite + 1 
            mean             = np.zeros(m)
            shape            = np.zeros([m,m])
            sum_mean_weights = 1e-5
            for i in range(n):
                if labels[i] == k:
                    mean_weight      = min([[0.5]], 1 / np.dot(np.array([X[i]-means[k]]), np.dot(np.linalg.inv(regularize(shapes[k])), np.array([X[i]-means[k]]).T)))[0][0]             
                    #print(mean_weight)
                    mean             = mean + mean_weight * X[i]
                    sum_mean_weights = sum_mean_weights + mean_weight
                    shape            = shape + np.dot(np.array([X[i]-means[k]]).T, np.array([X[i]-means[k]])) * mean_weight
            delta_mean  = mean / sum_mean_weights - means[k]
            delta_shape = shape * m / n_clusters[k] - shapes[k]
            means[k]    = means[k] + delta_mean
            shapes[k]   = shapes[k] + delta_shape
            print(""trace at"", ite, np.trace(shapes[k]))
            convergence = sum(abs(delta_mean)) + sum(sum(abs(delta_shape))) < eps
        shapes[k] = regularize(shapes[k])
    return means, shapes

def t_distribution_estimator(X, labels, eps = 1e-5, max_iter = 20):
    
    """""" Estimates the matrix of means and the tensor of covariances matrix of the dataset using S-estimators.
        To tackle singular matrix issues, we use regularization.
        
    Parameters
    ----------
    X           : 2-d array of size n*m
                  matrix of all the samples generated
    labels      : 1-d array of size n
                  vector of the label of each sample
    eps         : float > 0
                  criterion of termination when solving the fixed-point equation
    max_iter    : integer > 1
                  number of maximum iterations to solve the fixed-point equation
    Returns
    -------
    means       : 2-d array of size K*m
                  matrix of the estimation of the mean of the K clusters
    covariances : 3-d array of size K*m*m
                  tensor of the estimation of covariance matrix of the K clusters
    nus         : 1-d array of size K
                  vector of the estimation fof degrees of freedom of each cluster
    """"""  
    
    n, m          = X.shape
    K             = int(max(set(labels)) + 1)
    n_clusters = np.zeros(K) + 1e-5
    for i in range(n): 
        n_clusters[int(labels[i])] = n_clusters[int(labels[i])] + 1
    means, covariances = classic_estimator(X, labels)
    nus                = [1 for k in range(K)]

    for k in range(K):
        convergence      = False
        ite              = 1
        while (not convergence) and ite<max_iter:
            ite              = ite + 1 
            mean             = np.zeros(m)
            covariance       = np.zeros([m,m])
            sum_mean_weights = 1e-5
            sum_nu_weights   = 1e-5
            for i in range(n):
                if labels[i] == k:
                    mean_weight      = (nus[k] + m) / (nus[k] + np.dot(np.array([X[i]-means[k]]), np.dot(np.linalg.inv(regularize(covariances[k])), np.array([X[i]-means[k]]).T))[0][0])
                    mean             = mean + mean_weight * X[i]
                    sum_mean_weights = sum_mean_weights + mean_weight
                    covariance       = covariance + np.dot(np.array([X[i]-means[k]]).T, np.array([X[i]-means[k]])) * mean_weight
                    sum_nu_weights   = sum_nu_weights + (np.log(mean_weight) - mean_weight - np.log((nus[k] + m)/2) + digamma((nus[k] + m)/2)) / n_clusters[k]
            def f(nu):
                return np.log(gamma(nu/2)) - 0.5*nu*np.log(nu/2) - 0.5*nu*sum_nu_weights
            def grad_f(nu):
                return 0.5*digamma(nu/2) - 0.5*np.log(nu/2) - 0.5 - 0.5*sum_nu_weights
            res = minimize(f, nus[k], jac = grad_f,bounds=[(0,None)])
            delta_mean       = mean / sum_mean_weights - means[k]
            delta_covariance = covariance / n_clusters[k] - covariances[k]
            delta_nu         = res.x[0] - nus[k]
            means[k]         = means[k] + delta_mean
            covariances[k]   = covariances[k] + delta_covariance
            nus[k]           = nus[k] + delta_nu
            convergence      = abs(delta_nu) + sum(abs(delta_mean)) + sum(sum(abs(delta_covariance))) < eps
        covariances[k] = regularize(covariances[k])
    return means, covariances, nus

"
175,2201.02967,"from . import estimateurs as est
from . import decision_rules as dr
from . import simulateur as simu
import numpy as np
import pandas as pd
import pickle as pk
import dataframe_image as dfi
import os
from sklearn.metrics.cluster import adjusted_rand_score, normalized_mutual_info_score, adjusted_mutual_info_score
import time

def write_parameters_file(path, m, n, K, priors, p_conta, list_scenarios, simulation_id):

    """""" Writes the parameters of the simulation in a file .txt located in the file created to contain the
        results of the simulation. The file is created by the function and is located by the variable path.
    """"""    

    os.mkdir(path+""Simulation "" + str(simulation_id) + "" - m=""+str(m)+"" - K=""+str(K)+"" - n=""+str(n)+"" - p_conta=""+str(p_conta)) 
    
    fichier = open(path+""Simulation "" + str(simulation_id) + "" - m=""+str(m)+"" - K=""+str(K)+"" - n=""+str(n)+"" - p_conta=""+str(p_conta) + ""/"" + ""Simulation parameters.txt"", ""w"")
    fichier.write(""\n"")
    fichier.write(""                         ########################################################################################################################################## \n"")
    fichier.write(""                         #######################################################                            ####################################################### \n"")
    fichier.write(""                         #######################################################         Parameters         ####################################################### \n"")
    fichier.write(""                         #######################################################                            ####################################################### \n"")
    fichier.write(""                         ########################################################################################################################################## \n"")
    fichier.write(""                         ###                                                                                                                                    ### \n"")
    fichier.write(""                         ###     Dimension                                       : "" + str(m) + ""                                                                           ### \n"")
    fichier.write(""                         ###     Number of samples simulated                     : "" + str(n) + ""                                                                         ### \n"")
    fichier.write(""                         ###     Number of clusters                              : "" + str(K) + ""                                                                            ### \n"")
    fichier.write(""                         ###     Priors on the clusters                          : "" + str(priors) + ""                                                     ### \n"")
    fichier.write(""                         ###     Probability of contamination                    : "" + str(p_conta) + ""                                                                          ### \n"")
    fichier.write(""                         ###     Liste des scénarios étudiés                     :                                                                              ### \n"")
    for scenario in list_scenarios:
        fichier.write(""                         ###                                                     : "" + scenario + ""                                                                ### \n"")
    fichier.write(""                         ###                                                                                                                                    ### \n"")
    fichier.write(""                         ########################################################################################################################################## \n"")
    fichier.write(""                         ########################################################################################################################################## \n"")
    fichier.write(""                         ########################################################################################################################################## \n"")
    fichier.close()

def time_needed(nb_sec):
    
    """""" Returns a string with nb_sec converted in hours, minutes and seconds.
    """"""    
    
    nb_heures = nb_sec // 3600
    nb_min    = (nb_sec - nb_heures * 3600) // 60
    nb_s      = (nb_sec - nb_heures * 3600 - nb_min * 60)
    
    return str(nb_heures) + "" h "" + str(nb_min) + "" min "" + str(nb_s) + ""s""

def decision_rules_specific_performance_evaluation(X, labels, all_mu, all_sigma):
    
    """""" Evaluates the error of the estimators and the performances of the different
        decision rules : accuracy, ARI, NMI and AMI index are computed on the test set.
        The test set is built using the last 80% of the dataset generated.
    
    Parameters
    ----------
    X         : 2-d array of size n*m
                dataset generated
    labels    : list of n integers
                labels of the samples generated
    all_mu    : list containing 1-d array of size m
                real mean of each cluster
    all_sigma : list containing 2-d array of size m*m 
                real covariance matrix of each cluster
    
    Returns
    -------
    errors   : list of floats
               relative error of each estimator
    accuracy : list of floats
               accuracy of each decision rule
    ARI      : list of floats
               ARI index of each decision rule
    NMI      : list of floats
               NMI index of each decision rule
    AMI      : list of floats
               AMI index of each decision rule
    """"""  

    X_train, labels_train = X[:int(0.20*len(X))], labels[:int(0.20*len(X))]
    X_test , labels_test  = X[int(0.20*len(X)):], labels[int(0.20*len(X)):]

    means_classic, covariances_classic = est.classic_estimator                       (X_train, labels_train)
    means_M      , covariances_M       = est.M_estimator                             (X_train, labels_train)
    means_t      , covariances_t, nus  = est.t_distribution_estimator                (X_train, labels_train)
    means_femda  , covariances_femda   = est.femda_estimator                         (X_train, labels_train)

    c_classic = dr.find_optimal_c(X_train, labels_train, means_classic, covariances_classic)
    c_M       = dr.find_optimal_c(X_train, labels_train, means_M, covariances_M)

    def same_covariance_estimator(covariances):
        
        covariance = 0
        for covariance_cluster in covariances:
            covariance = covariance + covariance_cluster / len(covariances)
            
        return covariance

    avg_mean_error_classic = 0
    avg_mean_error_M       = 0
    avg_mean_error_t       = 0
    avg_mean_error_femda   = 0
    r                      = np.sqrt(np.sum(all_mu[0]**2))
    
    for k in range(len(all_mu)):
        avg_mean_error_classic = avg_mean_error_classic + np.sqrt(np.dot(np.array([all_mu[k]-means_classic[k]]), np.dot(np.linalg.inv(all_sigma[k]), np.array([all_mu[k]-means_classic[k]]).T)))[0][0] / (len(all_mu)*r)
        avg_mean_error_M       = avg_mean_error_M       + np.sqrt(np.dot(np.array([all_mu[k]-means_M      [k]]), np.dot(np.linalg.inv(all_sigma[k]), np.array([all_mu[k]-means_M      [k]]).T)))[0][0] / (len(all_mu)*r) 
        avg_mean_error_t       = avg_mean_error_t       + np.sqrt(np.dot(np.array([all_mu[k]-means_t      [k]]), np.dot(np.linalg.inv(all_sigma[k]), np.array([all_mu[k]-means_t      [k]]).T)))[0][0] / (len(all_mu)*r)
        avg_mean_error_femda   = avg_mean_error_femda   + np.sqrt(np.dot(np.array([all_mu[k]-means_femda  [k]]), np.dot(np.linalg.inv(all_sigma[k]), np.array([all_mu[k]-means_femda  [k]]).T)))[0][0] / (len(all_mu)*r)

    good_classification_LDA_g_classic = 0
    good_classification_LDA_g_M       = 0
    good_classification_QDA_g_classic = 0
    good_classification_QDA_g_M       = 0
    good_classification_GQDA_classic  = 0
    good_classification_GQDA_M        = 0
    good_classification_LDA_t         = 0
    good_classification_QDA_t         = 0
    good_classification_FEMDA         = 0
    
    predicted_label_LDA_g_classic = []
    predicted_label_LDA_g_M       = []
    predicted_label_QDA_g_classic = []
    predicted_label_QDA_g_M       = []
    predicted_label_GQDA_classic  = []
    predicted_label_GQDA_M        = []
    predicted_label_LDA_t         = []
    predicted_label_QDA_t         = []
    predicted_label_FEMDA         = []
    
    for i in range(len(X_test)):
        predicted_label = dr.LDA_g(X_test[i], means_classic, same_covariance_estimator(covariances_classic))
        if labels_test[i] == predicted_label:
            good_classification_LDA_g_classic = good_classification_LDA_g_classic + 1 / len(X_test)
        predicted_label_LDA_g_classic.append(predicted_label)
        
        predicted_label = dr.LDA_g(X_test[i], means_M, same_covariance_estimator(covariances_M))
        if labels_test[i] == predicted_label:
            good_classification_LDA_g_M       = good_classification_LDA_g_M       + 1 / len(X_test)
        predicted_label_LDA_g_M      .append(predicted_label)
            
        predicted_label = dr.QDA_g(X[i], means_classic, covariances_classic)
        if labels[i] == predicted_label:
            good_classification_QDA_g_classic = good_classification_QDA_g_classic + 1 / len(X_test)
        predicted_label_QDA_g_classic.append(predicted_label)
            
        predicted_label = dr.QDA_g(X_test[i], means_M, covariances_M)
        if labels_test[i] == predicted_label:
            good_classification_QDA_g_M       = good_classification_QDA_g_M       + 1 / len(X_test)
        predicted_label_QDA_g_M      .append(predicted_label)
        
        predicted_label = dr.GQDA (X_test[i], means_classic, covariances_classic, c_classic)
        if labels_test[i] == predicted_label:
            good_classification_GQDA_classic  = good_classification_GQDA_classic + 1 / len(X_test)
        predicted_label_GQDA_classic .append(predicted_label)
            
        predicted_label = dr.GQDA (X_test[i], means_M, covariances_M, c_M)
        if labels_test[i] == predicted_label:
            good_classification_GQDA_M        = good_classification_GQDA_M       + 1 / len(X_test)
        predicted_label_GQDA_M       .append(predicted_label)
        
        predicted_label = dr.LDA_t(X_test[i], means_t, same_covariance_estimator(covariances_t), nus)
        if labels_test[i] ==predicted_label:
            good_classification_LDA_t         = good_classification_LDA_t        + 1 / len(X_test)
        predicted_label_LDA_t        .append(predicted_label)
            
        predicted_label= dr.QDA_t(X_test[i], means_t, covariances_t, nus )
        if labels_test[i] == predicted_label:
            good_classification_QDA_t         = good_classification_QDA_t        + 1 / len(X_test)
        predicted_label_QDA_t        .append(predicted_label)
        
        predicted_label = dr.FEMDA(X_test[i], means_femda, covariances_femda)
        if labels_test[i] == predicted_label:
            good_classification_FEMDA         = good_classification_FEMDA        + 1 / len(X_test)
        predicted_label_FEMDA        .append(predicted_label)
        
    if good_classification_GQDA_M > good_classification_GQDA_classic:
        good_classification_GQDA = good_classification_GQDA_M
        predicted_label_GQDA     = predicted_label_GQDA_M
    else:
        good_classification_GQDA = good_classification_GQDA_classic
        predicted_label_GQDA     = predicted_label_GQDA_classic

    errors   = [avg_mean_error_classic, avg_mean_error_M, avg_mean_error_t, avg_mean_error_femda]
    accuracy = [good_classification_LDA_g_classic, good_classification_LDA_g_M,
                good_classification_QDA_g_classic, good_classification_QDA_g_M,
                good_classification_GQDA         , good_classification_LDA_t  , 
                good_classification_QDA_t        , good_classification_FEMDA]
    ARI      = [adjusted_rand_score(labels_test, predicted_label_LDA_g_classic), 
                adjusted_rand_score(labels_test, predicted_label_LDA_g_M      ), 
                adjusted_rand_score(labels_test, predicted_label_QDA_g_classic), 
                adjusted_rand_score(labels_test, predicted_label_QDA_g_M      ), 
                adjusted_rand_score(labels_test, predicted_label_GQDA         ), 
                adjusted_rand_score(labels_test, predicted_label_LDA_t        ),
                adjusted_rand_score(labels_test, predicted_label_QDA_t        ),
                adjusted_rand_score(labels_test, predicted_label_FEMDA        )]
    NMI      = [normalized_mutual_info_score(labels_test, predicted_label_LDA_g_classic), 
                normalized_mutual_info_score(labels_test, predicted_label_LDA_g_M      ), 
                normalized_mutual_info_score(labels_test, predicted_label_QDA_g_classic), 
                normalized_mutual_info_score(labels_test, predicted_label_QDA_g_M      ), 
                normalized_mutual_info_score(labels_test, predicted_label_GQDA         ), 
                normalized_mutual_info_score(labels_test, predicted_label_LDA_t        ),
                normalized_mutual_info_score(labels_test, predicted_label_QDA_t        ),
                normalized_mutual_info_score(labels_test, predicted_label_FEMDA        )]
    AMI      = [adjusted_mutual_info_score(labels_test, predicted_label_LDA_g_classic), 
                adjusted_mutual_info_score(labels_test, predicted_label_LDA_g_M      ), 
                adjusted_mutual_info_score(labels_test, predicted_label_QDA_g_classic), 
                adjusted_mutual_info_score(labels_test, predicted_label_QDA_g_M      ), 
                adjusted_mutual_info_score(labels_test, predicted_label_GQDA         ), 
                adjusted_mutual_info_score(labels_test, predicted_label_LDA_t        ),
                adjusted_mutual_info_score(labels_test, predicted_label_QDA_t        ),
                adjusted_mutual_info_score(labels_test, predicted_label_FEMDA        )] 

    return errors, accuracy, ARI, NMI, AMI

def decision_rules_overall_performance_evaluation(m, n, K, priors, scenario, p_conta, nb_simulations_MC):
    
    """""" Uses the function evalue_performances_on_simulated_data over multiple Monte Carlo simulations to evaluate
        the performances of the different estimators and decision rules for given paramereters for the data
        generation. Returns many lists of size nb_simulations_MC containing all the results. Saves all the results
        under dataframe images in the file created for the simulation.
    """"""
    
    vector_errors, vector_accuracy, vector_ARI, vector_NMI, vector_AMI = [], [], [], [], []
    n, all_pi, all_mu, all_sigma, all_tau, all_PDF, p_conta, conta = simu.genere_parametres_simulation(m, n, K, priors, scenario, p_conta)
    
    for nb_simu in range(nb_simulations_MC):
        
        start = time.time()

        data_simul = simu.dataSimulation(n, all_pi, all_mu, all_sigma, all_tau, all_PDF, p_conta, conta)
        X, labels, _ = data_simul.generateSamples()
        errors, accuracy, ARI, NMI, AMI = decision_rules_specific_performance_evaluation(X, labels, all_mu, all_sigma)
        vector_errors.append(errors)
        vector_accuracy.append(accuracy)
        vector_ARI.append(ARI)
        vector_NMI.append(NMI)
        vector_AMI.append(AMI)

        finish = time.time()
        print(str(np.round(100 * nb_simu / nb_simulations_MC)) + "" %                  "" + time_needed(int(finish-start)))
    
    return vector_errors, vector_accuracy, vector_ARI, vector_NMI, vector_AMI
    
def save_results(path, m, n, K, priors, list_scenarios, p_conta, simulation_id, nb_simulations_MC = 10):
    
    """""" Uses the function evalue_performances_for given_parameters to evaluate the performances of the different 
        estimators and decision rules for given paramereters for the data generation over multiple scenarios.
    """"""
    
    matrix_errors, matrix_accuracy, matrix_ARI, matrix_NMI, matrix_AMI = [], [], [], [], []
    
    for i in range(len(list_scenarios)):
        
        start = time.time()
        print(""Début de l'étude du scénario"")
        
        scenario = list_scenarios[i]
        vector_errors, vector_accuracy, vector_ARI, vector_NMI, vector_AMI = decision_rules_overall_performance_evaluation(m, n, K, priors, scenario, p_conta, nb_simulations_MC) 
        matrix_errors.append(vector_errors)
        matrix_accuracy.append(vector_accuracy)
        matrix_ARI.append(vector_ARI)
        matrix_NMI.append(vector_NMI)
        matrix_AMI.append(vector_AMI)
        
        finish = time.time()
        print(""Fin de l'étude du scénario - ""+ str(1+i) + "" / "" + str(len(list_scenarios)) + ""                  "" + time_needed(int(finish-start)))
        
    restructured_matrix_errors   = [[[] for i in range(5 )] for i in range(len(list_scenarios))]
    restructured_matrix_accuracy = [[[] for i in range(11)] for i in range(len(list_scenarios))]
    restructured_matrix_ARI      = [[[] for i in range(11)] for i in range(len(list_scenarios))]
    restructured_matrix_NMI      = [[[] for i in range(11)] for i in range(len(list_scenarios))]
    restructured_matrix_AMI      = [[[] for i in range(11)] for i in range(len(list_scenarios))]
    matrix_mean_errors  , matrix_median_errors  , matrix_std_errors   = np.zeros([len(list_scenarios), 4]), np.zeros([len(list_scenarios), 4]), np.zeros([len(list_scenarios), 4])
    matrix_mean_accuracy, matrix_median_accuracy, matrix_std_accuracy = np.zeros([len(list_scenarios), 8]), np.zeros([len(list_scenarios), 8]), np.zeros([len(list_scenarios), 8])
    matrix_mean_ARI     , matrix_median_ARI     , matrix_std_ARI      = np.zeros([len(list_scenarios), 8]), np.zeros([len(list_scenarios), 8]), np.zeros([len(list_scenarios), 8])
    matrix_mean_NMI     , matrix_median_NMI     , matrix_std_NMI      = np.zeros([len(list_scenarios), 8]), np.zeros([len(list_scenarios), 8]), np.zeros([len(list_scenarios), 8])
    matrix_mean_AMI     , matrix_median_AMI     , matrix_std_AMI      = np.zeros([len(list_scenarios), 8]), np.zeros([len(list_scenarios), 8]), np.zeros([len(list_scenarios), 8])

    for i in range(len(list_scenarios)):
        for j in range(8):
            for k in range(nb_simulations_MC):
                if j < 4:
                    restructured_matrix_errors[i][j].append(matrix_errors[i][k][j])
                restructured_matrix_accuracy[i][j].append(matrix_accuracy[i][k][j])
                restructured_matrix_ARI[i][j].append(matrix_ARI[i][k][j])
                restructured_matrix_NMI[i][j].append(matrix_NMI[i][k][j])
                restructured_matrix_AMI[i][j].append(matrix_AMI[i][k][j])

    for i in range(len(list_scenarios)):
        for j in range(8):
            if j < 4:
                matrix_mean_errors  [i][j] = np.mean  (np.array(restructured_matrix_errors  [i][j]))
                matrix_median_errors[i][j] = np.median(np.array(restructured_matrix_errors  [i][j]))
                matrix_std_errors   [i][j] = np.std   (np.array(restructured_matrix_errors  [i][j]))
            matrix_mean_accuracy  [i][j]   = np.mean  (np.array(restructured_matrix_accuracy[i][j]))
            matrix_median_accuracy[i][j]   = np.median(np.array(restructured_matrix_accuracy[i][j]))
            matrix_std_accuracy   [i][j]   = np.std   (np.array(restructured_matrix_accuracy[i][j]))
            matrix_mean_ARI       [i][j]   = np.mean  (np.array(restructured_matrix_ARI     [i][j]))
            matrix_median_ARI     [i][j]   = np.median(np.array(restructured_matrix_ARI     [i][j]))
            matrix_std_ARI        [i][j]   = np.std   (np.array(restructured_matrix_ARI     [i][j]))
            matrix_mean_NMI       [i][j]   = np.mean  (np.array(restructured_matrix_NMI     [i][j]))
            matrix_median_NMI     [i][j]   = np.median(np.array(restructured_matrix_NMI     [i][j]))
            matrix_std_NMI        [i][j]   = np.std   (np.array(restructured_matrix_NMI     [i][j]))
            matrix_mean_AMI       [i][j]   = np.mean  (np.array(restructured_matrix_AMI     [i][j]))
            matrix_median_AMI     [i][j]   = np.median(np.array(restructured_matrix_AMI     [i][j]))
            matrix_std_AMI        [i][j]   = np.std   (np.array(restructured_matrix_AMI     [i][j]))

    df_mean_errors     = pd.DataFrame(matrix_mean_errors    , index = [scenario for scenario in list_scenarios], columns = ['Classic estimator', 'M-estimator', 't-distribution estimator', 'FEMDA estimator'])
    df_median_errors   = pd.DataFrame(matrix_median_errors  , index = [scenario for scenario in list_scenarios], columns = ['Classic estimator', 'M-estimator', 't-distribution estimator', 'FEMDA estimator'])
    df_std_errors      = pd.DataFrame(matrix_std_errors     , index = [scenario for scenario in list_scenarios], columns = ['Classic estimator', 'M-estimator', 't-distribution estimator', 'FEMDA estimator'])
    df_mean_accuracy   = pd.DataFrame(matrix_mean_accuracy  , index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_median_accuracy = pd.DataFrame(matrix_median_accuracy, index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_std_accuracy    = pd.DataFrame(matrix_std_accuracy   , index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_mean_ARI        = pd.DataFrame(matrix_mean_ARI       , index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_median_ARI      = pd.DataFrame(matrix_median_ARI     , index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_std_ARI         = pd.DataFrame(matrix_std_ARI        , index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_mean_NMI        = pd.DataFrame(matrix_mean_NMI       , index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_median_NMI      = pd.DataFrame(matrix_median_NMI     , index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_std_NMI         = pd.DataFrame(matrix_std_NMI        , index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_mean_AMI        = pd.DataFrame(matrix_mean_AMI       , index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_median_AMI      = pd.DataFrame(matrix_median_AMI     , index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_std_AMI         = pd.DataFrame(matrix_std_AMI        , index = [scenario for scenario in list_scenarios], columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])

    path2 = path + ""Simulation "" + str(simulation_id) + "" - m=""+str(m)+"" - K=""+str(K)+"" - n=""+str(n)+"" - p_conta=""+str(p_conta)+ ""/""

    f = open(path2 + ""Matrix errors"", ""wb"")
    pk.dump(restructured_matrix_errors, f)
    f.close()
    f = open(path2 + ""Matrix Accuracy"", ""wb"")
    pk.dump(restructured_matrix_accuracy, f)
    f.close()
    f = open(path2 + ""Matrix ARI"", ""wb"")
    pk.dump(restructured_matrix_ARI, f)
    f.close()
    f = open(path2 + ""Matrix NMI"", ""wb"")
    pk.dump(restructured_matrix_NMI, f)
    f.close()
    f = open(path2 + ""Matrix AMI"", ""wb"")
    pk.dump(restructured_matrix_AMI, f)
    f.close()
    
    df_styled = df_mean_errors.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - Errors - mean.png"")
    df_styled = df_median_errors.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - Errors - median.png"")
    df_styled = df_std_errors.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - Errors - standard deviation.png"")
    df_styled = df_mean_accuracy.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - Accuracy - mean.png"")
    df_styled = df_median_accuracy.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - Accuracy - median.png"")
    df_styled = df_std_accuracy.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - Accuracy - standard deviation.png"")
    df_styled = df_mean_ARI.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - ARI - mean.png"")
    df_styled = df_median_ARI.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - ARI - median.png"")
    df_styled = df_std_ARI.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - ARI - standard deviation.png"")
    df_styled = df_mean_NMI.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - NMI - mean.png"")
    df_styled = df_median_NMI.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - NMI - median.png"")
    df_styled = df_std_NMI.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - NMI - standard deviation.png"")
    df_styled = df_mean_AMI.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - AMI - mean.png"")
    df_styled = df_median_AMI.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - AMI - median.png"")
    df_styled = df_std_AMI.style.background_gradient(axis = 0)
    dfi.export(df_styled, path2 + ""Comparaison des datasets - AMI - standard deviation.png"")
    
    df_styled = df_mean_errors.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - Errors - mean.png"")
    df_styled = df_median_errors.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - Errors - median.png"")
    df_styled = df_std_errors.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - Errors - standard deviation.png"")
    df_styled = df_mean_accuracy.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - Accuracy - mean.png"")
    df_styled = df_median_accuracy.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - Accuracy - median.png"")
    df_styled = df_std_accuracy.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - Accuracy - standard deviation.png"")
    df_styled = df_mean_ARI.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - ARI - mean.png"")
    df_styled = df_median_ARI.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - ARI - median.png"")
    df_styled = df_std_ARI.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - ARI - standard deviation.png"")
    df_styled = df_mean_NMI.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - NMI - mean.png"")
    df_styled = df_median_NMI.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - NMI - median.png"")
    df_styled = df_std_NMI.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - NMI - standard deviation.png"")
    df_styled = df_mean_AMI.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - AMI - mean.png"")
    df_styled = df_median_AMI.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - AMI - median.png"")
    df_styled = df_std_AMI.style.background_gradient(axis = 1)
    dfi.export(df_styled, path2 + ""Comparaison des decision rules - AMI - standard deviation.png"")
    
    print(""La liste des scénarios a été traitée et sauvegardée"")"
176,2201.02967,from .run_experiments import run_experiments
177,2201.02967,"from . import estimateurs as est
from . import decision_rules as dr
from . import preprocessing as pre

import numpy as np
import pandas as pd
import pickle as pk
import os
from numpy.random import multivariate_normal
import dataframe_image as dfi
import matplotlib.pyplot as pl
from sklearn.metrics.cluster import adjusted_rand_score, normalized_mutual_info_score, adjusted_mutual_info_score
import time    
import warnings
warnings.filterwarnings(""ignore"")
    
def time_needed(nb_sec):
    
    """""" Returns a string with nb_sec converted in hours, minutes and seconds.
    """"""    
    
    nb_heures = nb_sec // 3600
    nb_min    = (nb_sec - nb_heures * 3600) // 60
    nb_s      = (nb_sec - nb_heures * 3600 - nb_min * 60)
    
    return str(nb_heures) + "" h "" + str(nb_min) + "" min "" + str(nb_s) + ""s""

def contamine(X, p_conta, dataset_name):
    
    """""" Contaminates observations of dataset X with probability p_conta. The
    type of contamination is specific for each dataset.
    
    Parameters
    ----------
    X              : 2-d array of size n*m
                     dataset 
    p_conta_distri : float between 0 and 1
                    probability to contaminate an observation
    dataset_name   : str
                    Dataset to contaminate
                    
    Returns
    -------
    X              : 2-d array of size n*m
                     dataset contaminated
    """"""

    if dataset_name == ""Ionosphere"":
        return contamine_ionosphere(X, p_conta)
    if dataset_name == ""Statlog"":
        return contamine_statlog(X, p_conta)
    if dataset_name == ""Breast cancer"":
        return contamine_breast_cancer(X, p_conta)
    if dataset_name == ""Ecoli"":
        return contamine_ecoli(X, p_conta)
    if dataset_name == ""Spambase"":
        return contamine_spambase(X, p_conta)
    
def contamine_ionosphere(X, p_conta):

    """""" Contamination funtion for Ionosphere dataset.
    """"""
    
    for i in range(len(X)):
        if np.random.rand() < p_conta:
            X[i] = 2 * np.random.rand(34) - 1
    return contamine_statlog(X, p_conta)

def contamine_statlog(X, p_conta):

    """""" Contamination funtion for Statlog Landsat Satellite dataset.
    """"""
    
    for i in range(len(X)):
        if np.random.rand() < p_conta:
            X[i] = multivariate_normal(np.zeros(len(X[i])), 1e8 * np.identity(len(X[i])))
            X[i] = np.random.rand(len(X[i])) * 140 + 20
    return X

def contamine_breast_cancer(X, p_conta):

    """""" Contamination funtion for breast cancer dataset.
    """"""
    
    for i in range(len(X)):
        if np.random.rand() < p_conta:
            X[i] = multivariate_normal(np.zeros(len(X[i])), 1e8 * np.identity(len(X[i])))
            X[i] = np.random.rand(len(X[i])) * 140 + 20
    return X

def contamine_ecoli(X, p_conta):

    """""" Contamination funtion for Ecoli dataset.
    """"""
    
    for i in range(len(X)):
        if np.random.rand() < p_conta:
            X[i] = multivariate_normal(np.zeros(len(X[i])), 1e8 * np.identity(len(X[i])))
            X[i] = np.random.rand(len(X[i])) * 140 + 20
    return X

def contamine_spambase(X, p_conta):

    """""" Contamination funtion for Spambase dataset.
    """"""
    
    for i in range(len(X)):
        if np.random.rand() < p_conta:
            X[i] = multivariate_normal(np.zeros(len(X[i])), 1e8 * np.identity(len(X[i])))
            X[i] = np.random.rand(len(X[i])) * 140 + 20
    return X

def contamine_labels(labels, p_conta):

    confuse_dict = {}
    for i in list(set(labels)):        
        confuse_dict[str(i)] = list(set(labels))[np.random.randint(0, len(list(set(labels))))]
    for i in range(len(labels)):
        if np.random.rand() < p_conta:
            labels[i] = confuse_dict[str(labels[i])]
    return labels

def decision_rules_specific_performance_evaluation(X, labels, X_test, labels_test, p_conta, dataset_name):
    
    """""" Evaluates the error of the estimators and the performances of the different
        decision rules : accuracy, ARI, NMI and AMI index are computed on the train and test set.
    
    Parameters
    ----------
    X           : 2-d array of size n*m
                  train set
    labels      : list of n integers
                  labels of the samples of the train set
    X_test      : 2-d array of size n*m
                  test set
    labels_test : list of n integers
                  labels of the samples of the test set
    X_test      : list containing 1-d array of size m
                  real mean of each cluster
    p_conta     : float between 0 and 1
                  probability to contaminate an observation
    
    Returns
    -------
    accuracy : list of floats
               accuracy of each decision rule for train and test set
    ARI      : list of floats
               ARI index of each decision rule for train and test set
    NMI      : list of floats
               NMI index of each decision rule for train and test set
    AMI      : list of floats
               AMI index of each decision rule for train and test set
    """"""  

    #X = contamine(X, p_conta, dataset_name)
    labels = contamine_labels(labels, p_conta)

    means_classic, covariances_classic = est.classic_estimator                       (X, labels)
    means_M      , covariances_M       = est.M_estimator                             (X, labels)
    means_t      , covariances_t, nus  = est.t_distribution_estimator                (X, labels)
    means_femda  , covariances_femda   = est.femda_estimator                         (X, labels)
    
    c_classic = dr.find_optimal_c(X, labels, means_classic, covariances_classic)
    c_M       = dr.find_optimal_c(X, labels, means_M, covariances_M)
    
    def same_covariance_estimator(covariances):
        
        covariance = 0
        for covariance_cluster in covariances:
            covariance = covariance + covariance_cluster / len(covariances)
            
        return covariance

    good_classification_LDA_g_classic_train = 0
    good_classification_LDA_g_M_train       = 0
    good_classification_QDA_g_classic_train = 0
    good_classification_QDA_g_M_train       = 0
    good_classification_GQDA_classic_train  = 0
    good_classification_GQDA_M_train        = 0
    good_classification_LDA_t_train         = 0
    good_classification_QDA_t_train         = 0
    good_classification_FEMDA_train         = 0
    
    predicted_label_LDA_g_classic_train = []
    predicted_label_LDA_g_M_train       = []
    predicted_label_QDA_g_classic_train = []
    predicted_label_QDA_g_M_train       = []
    predicted_label_GQDA_classic_train  = []
    predicted_label_GQDA_M_train        = []
    predicted_label_LDA_t_train         = []
    predicted_label_QDA_t_train         = []
    predicted_label_FEMDA_train         = []
    
    for i in range(len(X)):
        predicted_label = dr.LDA_g(X[i], means_classic, same_covariance_estimator(covariances_classic))
        if labels[i] == predicted_label:
            good_classification_LDA_g_classic_train = good_classification_LDA_g_classic_train + 1 / len(X)
        predicted_label_LDA_g_classic_train.append(predicted_label)
        
        predicted_label = dr.LDA_g(X[i], means_M, same_covariance_estimator(covariances_M))
        if labels[i] == predicted_label:
            good_classification_LDA_g_M_train       = good_classification_LDA_g_M_train       + 1 / len(X)
        predicted_label_LDA_g_M_train      .append(predicted_label)
            
        predicted_label = dr.QDA_g(X[i], means_classic, covariances_classic)
        if labels[i] == predicted_label:
            good_classification_QDA_g_classic_train = good_classification_QDA_g_classic_train + 1 / len(X)
        predicted_label_QDA_g_classic_train.append(predicted_label)
            
        predicted_label = dr.QDA_g(X[i], means_M, covariances_M)
        if labels[i] == predicted_label:
            good_classification_QDA_g_M_train       = good_classification_QDA_g_M_train       + 1 / len(X)
        predicted_label_QDA_g_M_train      .append(predicted_label)
            
        predicted_label = dr.GQDA (X[i], means_classic, covariances_classic, c_classic)
        if labels[i] == predicted_label:
            good_classification_GQDA_classic_train  = good_classification_GQDA_classic_train + 1 / len(X)
        predicted_label_GQDA_classic_train .append(predicted_label)
        
        predicted_label = dr.GQDA (X[i], means_M, covariances_M, c_M)
        if labels[i] == predicted_label:
            good_classification_GQDA_M_train  = good_classification_GQDA_M_train + 1 / len(X)
        predicted_label_GQDA_M_train .append(predicted_label)
        
        predicted_label = dr.LDA_t(X[i], means_t, same_covariance_estimator(covariances_t), nus)
        if labels[i] ==predicted_label:
            good_classification_LDA_t_train         = good_classification_LDA_t_train        + 1 / len(X)
        predicted_label_LDA_t_train        .append(predicted_label)
            
        predicted_label= dr.QDA_t(X[i], means_t, covariances_t, nus )
        if labels[i] == predicted_label:
            good_classification_QDA_t_train         = good_classification_QDA_t_train        + 1 / len(X)
        predicted_label_QDA_t_train        .append(predicted_label)
        
        predicted_label = dr.FEMDA(X[i], means_femda, covariances_femda)
        if labels[i] == predicted_label:
            good_classification_FEMDA_train         = good_classification_FEMDA_train        + 1 / len(X)
        predicted_label_FEMDA_train        .append(predicted_label)
        
    if good_classification_GQDA_M_train > good_classification_GQDA_classic_train:
        good_classification_GQDA_train = good_classification_GQDA_M_train
        predicted_label_GQDA_train     = predicted_label_GQDA_M_train
    else:
        good_classification_GQDA_train = good_classification_GQDA_classic_train
        predicted_label_GQDA_train     = predicted_label_GQDA_classic_train

    accuracy_train = [good_classification_LDA_g_classic_train, good_classification_LDA_g_M_train, 
                      good_classification_QDA_g_classic_train, good_classification_QDA_g_M_train,
                      good_classification_GQDA_train , good_classification_LDA_t_train  , 
                      good_classification_QDA_t_train  , good_classification_FEMDA_train]
    ARI_train      = [adjusted_rand_score(labels, predicted_label_LDA_g_classic_train), 
                      adjusted_rand_score(labels, predicted_label_LDA_g_M_train      ), 
                      adjusted_rand_score(labels, predicted_label_QDA_g_classic_train), 
                      adjusted_rand_score(labels, predicted_label_QDA_g_M_train      ), 
                      adjusted_rand_score(labels, predicted_label_GQDA_train         ), 
                      adjusted_rand_score(labels, predicted_label_LDA_t_train        ),
                      adjusted_rand_score(labels, predicted_label_QDA_t_train        ),
                      adjusted_rand_score(labels, predicted_label_FEMDA_train        )]
    NMI_train      = [normalized_mutual_info_score(labels, predicted_label_LDA_g_classic_train), 
                      normalized_mutual_info_score(labels, predicted_label_LDA_g_M_train      ), 
                      normalized_mutual_info_score(labels, predicted_label_QDA_g_classic_train), 
                      normalized_mutual_info_score(labels, predicted_label_QDA_g_M_train      ), 
                      normalized_mutual_info_score(labels, predicted_label_GQDA_train         ), 
                      normalized_mutual_info_score(labels, predicted_label_LDA_t_train        ),
                      normalized_mutual_info_score(labels, predicted_label_QDA_t_train        ),
                      normalized_mutual_info_score(labels, predicted_label_FEMDA_train        )]
    AMI_train      = [adjusted_mutual_info_score(labels, predicted_label_LDA_g_classic_train), 
                      adjusted_mutual_info_score(labels, predicted_label_LDA_g_M_train      ), 
                      adjusted_mutual_info_score(labels, predicted_label_QDA_g_classic_train), 
                      adjusted_mutual_info_score(labels, predicted_label_QDA_g_M_train      ), 
                      adjusted_mutual_info_score(labels, predicted_label_GQDA_train         ), 
                      adjusted_mutual_info_score(labels, predicted_label_LDA_t_train        ),
                      adjusted_mutual_info_score(labels, predicted_label_QDA_t_train        ),
                      adjusted_mutual_info_score(labels, predicted_label_FEMDA_train        )]
    
    good_classification_LDA_g_classic_test = 0
    good_classification_LDA_g_M_test       = 0
    good_classification_QDA_g_classic_test = 0
    good_classification_QDA_g_M_test       = 0
    good_classification_GQDA_classic_test  = 0
    good_classification_GQDA_M_test        = 0
    good_classification_LDA_t_test         = 0
    good_classification_QDA_t_test         = 0
    good_classification_FEMDA_test          = 0
    
    predicted_label_LDA_g_classic_test = []
    predicted_label_LDA_g_M_test       = []
    predicted_label_QDA_g_classic_test = []
    predicted_label_QDA_g_M_test       = []
    predicted_label_GQDA_classic_test  = []
    predicted_label_GQDA_M_test        = []
    predicted_label_LDA_t_test         = []
    predicted_label_QDA_t_test         = []
    predicted_label_FEMDA_test          = []
    
    for i in range(len(X_test)):
        predicted_label = dr.LDA_g(X_test[i], means_classic, same_covariance_estimator(covariances_classic))
        if labels_test[i] == predicted_label:
            good_classification_LDA_g_classic_test = good_classification_LDA_g_classic_test + 1 / len(X_test)
        predicted_label_LDA_g_classic_test.append(predicted_label)
        
        predicted_label = dr.LDA_g(X_test[i], means_M, same_covariance_estimator(covariances_M))
        if labels_test[i] == predicted_label:
            good_classification_LDA_g_M_test       = good_classification_LDA_g_M_test       + 1 / len(X_test)
        predicted_label_LDA_g_M_test      .append(predicted_label)
            
        predicted_label = dr.QDA_g(X_test[i], means_classic, covariances_classic)
        if labels_test[i] == predicted_label:
            good_classification_QDA_g_classic_test = good_classification_QDA_g_classic_test + 1 / len(X_test)
        predicted_label_QDA_g_classic_test.append(predicted_label)
            
        predicted_label = dr.QDA_g(X_test[i], means_M, covariances_M)
        if labels_test[i] == predicted_label:
            good_classification_QDA_g_M_test       = good_classification_QDA_g_M_test       + 1 / len(X_test)
        predicted_label_QDA_g_M_test      .append(predicted_label)
            
        predicted_label = dr.GQDA (X_test[i], means_classic, covariances_classic, c_classic)
        if labels_test[i] == predicted_label:
            good_classification_GQDA_classic_test  = good_classification_GQDA_classic_test + 1 / len(X_test)
        predicted_label_GQDA_classic_test .append(predicted_label)
        
        predicted_label = dr.GQDA (X_test[i], means_M, covariances_M, c_M)
        if labels_test[i] == predicted_label:
            good_classification_GQDA_M_test        = good_classification_GQDA_M_test + 1 / len(X_test)
        predicted_label_GQDA_M_test       .append(predicted_label)
        
        predicted_label = dr.LDA_t(X_test[i], means_t, same_covariance_estimator(covariances_t), nus)
        if labels_test[i] ==predicted_label:
            good_classification_LDA_t_test         = good_classification_LDA_t_test        + 1 / len(X_test)
        predicted_label_LDA_t_test        .append(predicted_label)
            
        predicted_label = dr.QDA_t(X_test[i], means_t, covariances_t, nus )
        if labels_test[i] == predicted_label:
            good_classification_QDA_t_test         = good_classification_QDA_t_test        + 1 / len(X_test)
        predicted_label_QDA_t_test        .append(predicted_label)
        
        predicted_label = dr.FEMDA(X_test[i], means_femda, covariances_femda)
        if labels_test[i] == predicted_label:
            good_classification_FEMDA_test          = good_classification_FEMDA_test         + 1 / len(X_test)
        predicted_label_FEMDA_test         .append(predicted_label)
        
    if good_classification_GQDA_M_test > good_classification_GQDA_classic_test:
        good_classification_GQDA_test = good_classification_GQDA_M_test
        predicted_label_GQDA_test     = predicted_label_GQDA_M_test
    else:
        good_classification_GQDA_test = good_classification_GQDA_classic_test
        predicted_label_GQDA_test     = predicted_label_GQDA_classic_test
    
    accuracy_test = [good_classification_LDA_g_classic_test, good_classification_LDA_g_M_test,
                     good_classification_QDA_g_classic_test, good_classification_QDA_g_M_test, 
                     good_classification_GQDA_test , good_classification_LDA_t_test  , 
                     good_classification_QDA_t_test  , good_classification_FEMDA_test]
    ARI_test      = [adjusted_rand_score(labels_test, predicted_label_LDA_g_classic_test), 
                     adjusted_rand_score(labels_test, predicted_label_LDA_g_M_test      ), 
                     adjusted_rand_score(labels_test, predicted_label_QDA_g_classic_test), 
                     adjusted_rand_score(labels_test, predicted_label_QDA_g_M_test      ), 
                     adjusted_rand_score(labels_test, predicted_label_GQDA_test         ), 
                     adjusted_rand_score(labels_test, predicted_label_LDA_t_test        ),
                     adjusted_rand_score(labels_test, predicted_label_QDA_t_test        ),
                     adjusted_rand_score(labels_test, predicted_label_FEMDA_test         )]
    NMI_test      = [normalized_mutual_info_score(labels_test, predicted_label_LDA_g_classic_test), 
                     normalized_mutual_info_score(labels_test, predicted_label_LDA_g_M_test      ), 
                     normalized_mutual_info_score(labels_test, predicted_label_QDA_g_classic_test), 
                     normalized_mutual_info_score(labels_test, predicted_label_QDA_g_M_test      ), 
                     normalized_mutual_info_score(labels_test, predicted_label_GQDA_test         ), 
                     normalized_mutual_info_score(labels_test, predicted_label_LDA_t_test        ),
                     normalized_mutual_info_score(labels_test, predicted_label_QDA_t_test        ),
                     normalized_mutual_info_score(labels_test, predicted_label_FEMDA_test         )]
    AMI_test     =  [adjusted_mutual_info_score(labels_test, predicted_label_LDA_g_classic_test), 
                     adjusted_mutual_info_score(labels_test, predicted_label_LDA_g_M_test      ), 
                     adjusted_mutual_info_score(labels_test, predicted_label_QDA_g_classic_test), 
                     adjusted_mutual_info_score(labels_test, predicted_label_QDA_g_M_test      ), 
                     adjusted_mutual_info_score(labels_test, predicted_label_GQDA_test         ), 
                     adjusted_mutual_info_score(labels_test, predicted_label_LDA_t_test        ),
                     adjusted_mutual_info_score(labels_test, predicted_label_QDA_t_test        ),
                     adjusted_mutual_info_score(labels_test, predicted_label_FEMDA_test         )]

    return accuracy_train, ARI_train, NMI_train, AMI_train, accuracy_test, ARI_test, NMI_test, AMI_test

def decision_rules_overall_performance_evaluation(path_dataset, nb_simulations_MC, p_conta, perc_train_set_used, dataset_name, freq_shuffle):
    
    """""" Uses the function evalue_performances_on_simulated_data over multiple Monte Carlo simulations to evaluate
        the performances of the different decision rules over multiple samples of the dataset.
        Returns many lists of size nb_simulations_MC containing all the results. Saves all the results
        under dataframe images in the file created for the simulation.

    Parameters
    ----------
    path_dataset        : str
                          path to download and preprocess the dataset
    nb_simulations_MC   : integer > 0
                          number of Monte_Carlo simulations run to average the results
    p_conta             : float between 0 and 1
                          probability to contaminate an observation
    perc_train_set_used : float between 0 and 1
                          percentage of the train set that is used in practice
    dataset_name        : str
                          name of the dataset used, it is useful for the contamination function
    freq_shuffle         : integer > 0
                          if the index of the run modulo freq_shuffle == 0, we resplit the dataset
                          between a train and a test set, to have a different train and test set
    
    Returns
    -------
    vector_accuracy : list of list of floats
                      list of size nb_simulations_MC of all accuracy lists returned by decision_rules_specific_performances_evaluation
    vector_ARI      : list of list of floats
                      list of size nb_simulations_MC of all list of ARI index returned by decision_rules_specific_performances_evaluation
    vector_NMI      : list of list of floats
                      list of size nb_simulations_MC of all list of NMI index returned by decision_rules_specific_performances_evaluation
    vector_AMI      : list of list of floats
                      list of size nb_simulations_MC of all list of AMI index returned by decision_rules_specific_performances_evaluation
    """"""  
    
    vector_accuracy_train, vector_ARI_train, vector_NMI_train, vector_AMI_train, vector_accuracy_test, vector_ARI_test, vector_NMI_test, vector_AMI_test = [], [], [], [], [], [], [], []
    
    for nb_simu in range(nb_simulations_MC):
    
        start = time.time()
        if nb_simu % freq_shuffle == 0:
            if dataset_name == ""Ionosphere"":
                X_train, labels_train, X_test, labels_test = pre.ionosphere(path_dataset)
            if dataset_name == ""Statlog"":
                X_train, labels_train, X_test, labels_test = pre.statlog(path_dataset)
            if dataset_name == ""Breast cancer"":
                X_train, labels_train, X_test, labels_test = pre.breast_cancer(path_dataset)
            if dataset_name == ""Ecoli"":
                X_train, labels_train, X_test, labels_test = pre.ecoli(path_dataset)
            if dataset_name == ""Spambase"":
                X_train, labels_train, X_test, labels_test = pre.spambase(path_dataset)
        X_train_reduced, labels_train_reduced = [], []
        list_index_train_set = pre.select_random_index(len(X_train), int(len(X_train) * perc_train_set_used))
        for i in range(len(X_train)):
            if i in list_index_train_set:
                X_train_reduced.append(X_train[i])
                labels_train_reduced.append(labels_train[i])
        X_train_reduced = np.array(X_train_reduced)
        accuracy_train, ARI_train, NMI_train, AMI_train, accuracy_test, ARI_test, NMI_test, AMI_test = decision_rules_specific_performance_evaluation(X_train_reduced, labels_train_reduced, X_test, labels_test, p_conta, dataset_name)
        vector_accuracy_train.append(accuracy_train)
        vector_ARI_train.append(ARI_train)
        vector_NMI_train.append(NMI_train)
        vector_AMI_train.append(AMI_train)
        vector_accuracy_test.append(accuracy_test)
        vector_ARI_test.append(ARI_test)
        vector_NMI_test.append(NMI_test)
        vector_AMI_test.append(AMI_test)

        finish = time.time()
        print(str(np.round(100 * nb_simu / nb_simulations_MC)) + "" %                  "" + time_needed(int(finish-start)))
    
    return vector_accuracy_train, vector_ARI_train, vector_NMI_train, vector_AMI_train, vector_accuracy_test, vector_ARI_test, vector_NMI_test, vector_AMI_test
    
def save_results(path, dataset_name, nb_simulations_MC = 10, p_conta = 0, perc_train_set_used = 1, freq_shuffle = 10):
    
    """""" Uses the function decision_rules_overall_performance_evaluation to evaluate the performances of the different 
        decision rules. The results are then saved in a dataframe image and the dataframe itself is saved in a pickle
        object.
    """"""
       
    start = time.time()
    print(""Début de l'étude du dataset"")
    path_dataset = """"
    for s in path.split(""/"")[:-2]:
        path_dataset = path_dataset + s + ""/""
    path_dataset = path_dataset + ""Datasets/""
    vector_accuracy_train, vector_ARI_train, vector_NMI_train, vector_AMI_train, vector_accuracy_test, vector_ARI_test, vector_NMI_test, vector_AMI_test = decision_rules_overall_performance_evaluation(path_dataset, nb_simulations_MC, p_conta, perc_train_set_used, dataset_name, freq_shuffle) 
    finish = time.time()
    print(""Fin de l'étude du dataset -                   "" + time_needed(int(finish-start)))
        
    restructured_vector_accuracy_train = [[] for i in range(8)]
    restructured_vector_ARI_train      = [[] for i in range(8)]
    restructured_vector_NMI_train      = [[] for i in range(8)]
    restructured_vector_AMI_train      = [[] for i in range(8)]
    
    for j in range(8):
        for k in range(nb_simulations_MC):
            restructured_vector_accuracy_train[j].append(vector_accuracy_train[k][j])
            restructured_vector_ARI_train     [j].append(vector_ARI_train     [k][j])
            restructured_vector_NMI_train     [j].append(vector_NMI_train     [k][j])
            restructured_vector_AMI_train     [j].append(vector_AMI_train     [k][j])
    
    results_array_train = np.zeros((20, 8))
    for j in range(8):
        results_array_train[0 ][j] = np.mean  (np.array(restructured_vector_accuracy_train[j]))
        results_array_train[1 ][j] = np.median(np.array(restructured_vector_accuracy_train[j]))
        results_array_train[2 ][j] = np.std   (np.array(restructured_vector_accuracy_train[j]))
        results_array_train[3 ][j] = np.min   (np.array(restructured_vector_accuracy_train[j]))
        results_array_train[4 ][j] = np.max   (np.array(restructured_vector_accuracy_train[j]))
        results_array_train[5 ][j] = np.mean  (np.array(restructured_vector_ARI_train     [j]))
        results_array_train[6 ][j] = np.median(np.array(restructured_vector_ARI_train     [j]))
        results_array_train[7 ][j] = np.std   (np.array(restructured_vector_ARI_train     [j]))
        results_array_train[8 ][j] = np.min   (np.array(restructured_vector_ARI_train     [j]))
        results_array_train[9 ][j] = np.max   (np.array(restructured_vector_ARI_train     [j]))
        results_array_train[10][j] = np.mean  (np.array(restructured_vector_NMI_train     [j]))
        results_array_train[11][j] = np.median(np.array(restructured_vector_NMI_train     [j]))
        results_array_train[12][j] = np.std   (np.array(restructured_vector_NMI_train     [j]))
        results_array_train[13][j] = np.min   (np.array(restructured_vector_NMI_train     [j]))
        results_array_train[14][j] = np.max   (np.array(restructured_vector_NMI_train     [j]))
        results_array_train[15][j] = np.mean  (np.array(restructured_vector_AMI_train     [j]))
        results_array_train[16][j] = np.median(np.array(restructured_vector_AMI_train     [j]))
        results_array_train[17][j] = np.std   (np.array(restructured_vector_AMI_train     [j]))
        results_array_train[18][j] = np.min   (np.array(restructured_vector_AMI_train     [j]))
        results_array_train[19][j] = np.max   (np.array(restructured_vector_AMI_train     [j]))
        
    lines            = [(""Accuracy"", ""Mean""), (""Accuracy"", ""Median""), (""Accuracy"", ""Std""), (""Accuracy"", ""Min""), (""Accuracy"", ""Max""),
                        (""ARI     "", ""Mean""), (""ARI     "", ""Median""), (""ARI     "", ""Std""), (""ARI     "", ""Min""), (""ARI     "", ""Max""),
                        (""NMI     "", ""Mean""), (""NMI     "", ""Median""), (""NMI     "", ""Std""), (""NMI     "", ""Min""), (""NMI     "", ""Max""),
                        (""AMI     "", ""Mean""), (""AMI     "", ""Median""), (""AMI     "", ""Std""), (""AMI     "", ""Min""), (""AMI     "", ""Max"")]
    df_results_train = pd.DataFrame(results_array_train, index = pd.MultiIndex.from_tuples(lines) , columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_styled        = df_results_train.style.background_gradient(axis = 0)
    dfi.export(df_styled, path + dataset_name + ""/Train set/"" + ""conta = "" + str(np.round(p_conta, 2)) + "" - data used = "" + str(np.round(perc_train_set_used, 2)) + "".png"")
    f = open(path + dataset_name + ""/Pickles/"" + ""Train set - conta = "" + str(np.round(p_conta, 2)) + "" - data used = "" + str(np.round(perc_train_set_used, 2)), ""wb"")
    pk.dump(results_array_train, f)
    f.close()
    
    restructured_vector_accuracy_test = [[] for i in range(8)]
    restructured_vector_ARI_test      = [[] for i in range(8)]
    restructured_vector_NMI_test      = [[] for i in range(8)]
    restructured_vector_AMI_test      = [[] for i in range(8)]
    
    for j in range(8):
        for k in range(nb_simulations_MC):
            restructured_vector_accuracy_test[j].append(vector_accuracy_test[k][j])
            restructured_vector_ARI_test     [j].append(vector_ARI_test     [k][j])
            restructured_vector_NMI_test     [j].append(vector_NMI_test     [k][j])
            restructured_vector_AMI_test     [j].append(vector_AMI_test     [k][j])
    
    results_array_test = np.zeros((20, 8))
    for j in range(8):
        results_array_test[0 ][j] = np.mean  (np.array(restructured_vector_accuracy_test[j]))
        results_array_test[1 ][j] = np.median(np.array(restructured_vector_accuracy_test[j]))
        results_array_test[2 ][j] = np.std   (np.array(restructured_vector_accuracy_test[j]))
        results_array_test[3 ][j] = np.min   (np.array(restructured_vector_accuracy_test[j]))
        results_array_test[4 ][j] = np.max   (np.array(restructured_vector_accuracy_test[j]))
        results_array_test[5 ][j] = np.mean  (np.array(restructured_vector_ARI_test     [j]))
        results_array_test[6 ][j] = np.median(np.array(restructured_vector_ARI_test     [j]))
        results_array_test[7 ][j] = np.std   (np.array(restructured_vector_ARI_test     [j]))
        results_array_test[8 ][j] = np.min   (np.array(restructured_vector_ARI_test     [j]))
        results_array_test[9 ][j] = np.max   (np.array(restructured_vector_ARI_test     [j]))
        results_array_test[10][j] = np.mean  (np.array(restructured_vector_NMI_test     [j]))
        results_array_test[11][j] = np.median(np.array(restructured_vector_NMI_test     [j]))
        results_array_test[12][j] = np.std   (np.array(restructured_vector_NMI_test     [j]))
        results_array_test[13][j] = np.min   (np.array(restructured_vector_NMI_test     [j]))
        results_array_test[14][j] = np.max   (np.array(restructured_vector_NMI_test     [j]))
        results_array_test[15][j] = np.mean  (np.array(restructured_vector_AMI_test     [j]))
        results_array_test[16][j] = np.median(np.array(restructured_vector_AMI_test     [j]))
        results_array_test[17][j] = np.std   (np.array(restructured_vector_AMI_test     [j]))
        results_array_test[18][j] = np.min   (np.array(restructured_vector_AMI_test     [j]))
        results_array_test[19][j] = np.max   (np.array(restructured_vector_AMI_test     [j]))
        
    df_results_test = pd.DataFrame(results_array_test, index = pd.MultiIndex.from_tuples(lines) , columns = ['LDA_g - Classic', 'LDA_g - M', 'QDA_g - Classic', 'QDA_g - M', 'GQDA', 'LDA_t', 'QDA_t', 'FEMDA'])
    df_styled        = df_results_test.style.background_gradient(axis = 0)
    dfi.export(df_styled, path + dataset_name + ""/Test set/"" + ""conta = "" + str(np.round(p_conta, 2)) + "" - data used = "" + str(np.round(perc_train_set_used, 2)) + "".png"")
    f = open(path + dataset_name + ""/Pickles/"" + ""/Test set - conta = "" + str(np.round(p_conta, 2)) + "" - data used = "" + str(np.round(perc_train_set_used, 2)), ""wb"")
    pk.dump(results_array_test, f)
    f.close()
    
def plot_results_contamination_rate(path, dataset_name, methods, name, conta_min = 0, conta_max = 1, test_set_results = True):
    
    path_pickle     = path + dataset_name + ""/Pickles/""
    list_conta      = []
    dict_accuracy   = {""LDA_g - classic"" : [], ""LDA_g - M"" : [],
                       ""QDA_g - classic"" : [], ""QDA_g - M"" : [],
                       ""GQDA"" : [], ""LDA_t"" : [], ""QDA_t"" : [],
                       ""FEMDA"" : []} 
    if test_set_results:
        files = os.listdir(path_pickle)[:int(len(os.listdir(path_pickle))/2)]
    else:
        files = os.listdir(path_pickle)[int(len(os.listdir(path_pickle))/2):]
    files.sort

    for file in files:
        f            = open(path_pickle + file, ""rb"")
        results      = pk.load(f)
        conta        = float(file.split("" - "")[1][8:])
        dataset_used = float(file.split("" - "")[2][12:])
        if dataset_used > 0.999 and conta >= conta_min and conta <= conta_max:
            list_conta.append(conta)
            dict_accuracy[""LDA_g - classic""].append(results[0][0])
            dict_accuracy[""LDA_g - M""].append(results[0][1])
            dict_accuracy[""QDA_g - classic""].append(results[0][2])
            dict_accuracy[""QDA_g - M""].append(results[0][3])
            dict_accuracy[""GQDA""].append(results[0][4])
            dict_accuracy[""LDA_t""].append(results[0][5])
            dict_accuracy[""QDA_t""].append(results[0][6])
            dict_accuracy[""FEMDA""].append(results[0][7])
            
    pl.clf()
    for method in methods:
        pl.plot(list_conta, dict_accuracy[method], ""s-"", label = method)
    pl.legend()
    pl.xlabel(""Contamination rate"")
    pl.ylabel(""Accuracy in %"")
    pl.grid()
    pl.savefig(path + dataset_name + ""/"" + name + "".png"")

def plot_results_dataset_used(path, dataset_name, methods, name, dataset_used_min = 0, dataset_used_max = 1, test_set_results = True):
    
    path_pickle       = path + dataset_name + ""/Pickles/""
    list_dataset_used = []
    dict_accuracy     = {""LDA_g - classic"" : [], ""LDA_g - M"" : [],
                       ""QDA_g - classic"" : [], ""QDA_g - M"" : [],
                       ""GQDA"" : [], ""LDA_t"" : [], ""QDA_t"" : [],
                       ""FEMDA"" : []} 
    
    if test_set_results:
        files = os.listdir(path_pickle)[:int(len(os.listdir(path_pickle))/2)]
    else:
        files = os.listdir(path_pickle)[int(len(os.listdir(path_pickle))/2):]
    files.sort()
    for file in files:
        f            = open(path_pickle + file, ""rb"")
        results      = pk.load(f)
        conta        = float(file.split("" - "")[1][8:])
        dataset_used = float(file.split("" - "")[2][12:])
        if conta < 0.001 and dataset_used >= dataset_used_min and dataset_used <= dataset_used_max:
            list_dataset_used.append(dataset_used)
            dict_accuracy[""LDA_g - classic""].append(results[0][0])
            dict_accuracy[""LDA_g - M""].append(results[0][1])
            dict_accuracy[""QDA_g - classic""].append(results[0][2])
            dict_accuracy[""QDA_g - M""].append(results[0][3])
            dict_accuracy[""GQDA""].append(results[0][4])
            dict_accuracy[""LDA_t""].append(results[0][5])
            dict_accuracy[""QDA_t""].append(results[0][6])
            dict_accuracy[""FEMDA""].append(results[0][7])
            
    pl.clf()
    for method in methods:
        pl.plot(list_dataset_used, dict_accuracy[method], ""s-"", label = method)
    pl.legend()
    pl.xlabel(""Percentage of training set used"")
    pl.ylabel(""Accuracy in %"")
    pl.grid()
    pl.savefig(path + dataset_name + ""/"" + name + "".png"")"
178,2201.02967,"import numpy as np
from scipy.special import gamma

def LDA_g(x, means, covariance):
    
    """""" Determines the label of observation x using the K estimations of the mean of 
        each cluster and the estimation of the mutual covariance matrix using Linear
        Discriminant Analysis decision rule.
    
    Parameters
    ----------
    x          : m-dimensional vector
                 observation to classify
    means      : list containing 1-d array of size m
                 estimation of the mean of each cluster
    covariance : 2-d array of size m*m 
                 estimation of the mutual covariance of all clusters
    Returns
    -------
    label      : integer
                 label predicted
    """"""
    
    label                 = 0
    dist_mahal_square_min = np.inf
    
    for k in range(len(means)):
        if np.dot(np.array([x-means[k]]), np.dot(np.linalg.inv(covariance), np.array([x-means[k]]).T))[0][0] < dist_mahal_square_min:
            label                 = k
            dist_mahal_square_min = np.dot(np.array([x-means[k]]), np.dot(np.linalg.inv(covariance), np.array([x-means[k]]).T))[0][0]
    
    return label

def QDA_g(x, means, covariances):
    
    """""" Determines the label of observation x using the K estimations of the mean
        and covariance matrix of each cluster using Quadratic Discriminant Analysis 
        decision rule.
    
    Parameters
    ----------
    x           : m-dimensional vector
                  observation to classify
    means       : list containing 1-d array of size m
                  estimation of the mean of each cluster
    covariances : list containing 2-d array of size m*m 
                  estimation of the covariance matrix of each cluster
    Returns
    -------
    label      : integer
                 label predicted
    """"""
    
    label                 = 0
    dist_mahal_square_min = np.inf
    
    for k in range(len(means)):
        if np.dot(np.array([x-means[k]]), np.dot(np.linalg.inv(covariances[k]), np.array([x-means[k]]).T))[0][0] + np.log(np.linalg.det(covariances[k])) < dist_mahal_square_min:
            label                 = k
            dist_mahal_square_min = np.dot(np.array([x-means[k]]), np.dot(np.linalg.inv(covariances[k]), np.array([x-means[k]]).T))[0][0] + np.log(np.linalg.det(covariances[k]))
    
    return label

def FEMDA(x, means, covariances):
    
    """""" Determines the label of observation x using the K estimations of the mean
        and covariance matrix of each cluster using FEMDA decision rule. 
    
    Parameters
    ----------
    x           : m-dimensional vector
                  observation to classify
    means       : list containing 1-d array of size m
                  estimation of the mean of each cluster
    covariances : list containing 2-d array of size m*m 
                  estimation of the covariance matrix of each cluster
    Returns
    -------
    label      : integer
                 label predicted
    """"""
    
    label                 = 0
    dist_mahal_square_min = np.inf
    m                     = len(means[0])

    for k in range(len(means)):
        if 0.5 * m * np.log(np.dot(np.array([x-means[k]]), np.dot(np.linalg.inv(covariances[k]), np.array([x-means[k]]).T))[0][0]) + 0.5*np.log(np.linalg.det(covariances[k])) < dist_mahal_square_min:
            label                 = k
            dist_mahal_square_min = 0.5 * m * np.log(np.dot(np.array([x-means[k]]), np.dot(np.linalg.inv(covariances[k]), np.array([x-means[k]]).T))[0][0]) + 0.5 * np.log(np.linalg.det(covariances[k]))

    return label

def LDA_t(x, means, covariance, nus):
    
    """""" Determines the label of observation x using the K estimations of the mean, the
        degree of freedom of each cluster and the estimation of the mutual covariance 
        matrix using Linear Discriminant Analysis decision rule adapted to t-distributions.
    
    Parameters
    ----------
    x          : m-dimensional vector
                 observation to classify
    means      : list containing 1-d array of size m
                 estimation of the mean of each cluster
    covariance : 2-d array of size m*m 
                 estimation of the mutual covariance of all clusters
    nus        : list containing float
                 estimation of the degree of freedom of each cluster
    Returns
    -------
    label      : integer
                 label predicted
    """"""
    
    label                 = 0
    max_likelihood        = -np.inf
    m                     = len(means[0])
    
    for k in range(len(means)):
        d_mahal_square = np.dot(np.array([x-means[k]]), np.dot(np.linalg.inv(covariance), np.array([x-means[k]]).T))[0][0]
        likelihood     = np.log(gamma((m+nus[k])/2)) - np.log(gamma(nus[k]/2)) + m/2*np.log(nus[k]) - (m+nus[k])/2*np.log(1+d_mahal_square/nus[k])
        if likelihood > max_likelihood:
            label          = k
            max_likelihood = likelihood
    
    return label

def QDA_t(x, means, covariances, nus):
    
    """""" Determines the label of observation x using the K estimations of the mean,
        covariance matrix and degree of freedom of each cluster using Quadratic Discriminant 
        Analysis decision rule adapted to t-distributions.
    
    Parameters
    ----------
    x           : m-dimensional vector
                  observation to classify
    means       : list containing 1-d array of size m
                  estimation of the mean of each cluster
    covariances : list containing 2-d array of size m*m 
                  estimation of the covariance matrix of each cluster
    nus        : list containing float
                 estimation of the degree of freedom of each cluster
    Returns
    -------
    label      : integer
                 label predicted
    """"""
    
    label                 = 0
    max_likelihood        = -np.inf
    m                     = len(means[0])
    
    for k in range(len(means)):
        d_mahal_square = np.dot(np.array([x-means[k]]), np.dot(np.linalg.inv(covariances[k]), np.array([x-means[k]]).T))[0][0]
        likelihood     = np.log(gamma((m+nus[k])/2)) - np.log(gamma(nus[k]/2)) - m/2*np.log(nus[k]) - 0.5*np.log(np.linalg.det(covariances[k])) - 0.5*(m+nus[k])*np.log(1+d_mahal_square/nus[k])
        if likelihood > max_likelihood:
            label          = k
            max_likelihood = likelihood
    
    return label

def GQDA(x, means, covariances, c):
    
    """""" Determines the label of observation x using the K estimations of the mean of 
        and covariance matrix of each cluster using Generalized Quadratic Discriminant Analysis 
        decision rule with threshold equal to c.
    
    Parameters
    ----------
    x           : m-dimensional vector
                  observation to classify
    means       : list containing 1-d array of size m
                  estimation of the mean of each cluster
    covariances : list containing 2-d array of size m*m 
                  estimation of the covariance matrix of each cluster
    c           : float between 0 and 1
                  threshold when comparing Mahalanobis distances to covariance matrix determinants
    Returns
    -------
    label      : integer
                 label predicted
    """"""
    
    index_min             = 0
    dist_mahal_square_min = np.inf

    for k in range(len(means)):
        if np.dot(np.array([x-means[k]]), np.dot(np.linalg.inv(covariances[k]), np.array([x-means[k]]).T))[0][0] + c * np.log(np.linalg.det(covariances[k])) < dist_mahal_square_min:
            index_min             = k
            dist_mahal_square_min = np.dot(np.array([x-means[k]]), np.dot(np.linalg.inv(covariances[k]), np.array([x-means[k]]).T))[0][0] + c * np.log(np.linalg.det(covariances[k]))
    
    return index_min

def find_optimal_c(X, labels, means, covariances, max_candidates = 10):
    
    """""" Determines the optimal threshold between 0 and 1 that maximizes the accuracy 
        on the train set X.
    
    Parameters
    ----------
    X              : 2-d array of size n*m
                     matrix containing the n observations
    labels         : list containing integer
                     list containing the label of all observations
    means          : list containing 1-d array of size m
                     estimation of the mean of each cluster
    covariances    : list containing 2-d array of size m*m 
                     estimation of the covariance matrix of each cluster
    max_candidates : integer > 0
                     number of candidates tested in [0,1] to find the optimal threshold
    Returns
    -------
    best_c         : float between 0 and 1
                     best candidate that maximizes the accuracy
    """"""
    
    n, m       = X.shape
    K          = int(max(set(labels)) + 1)
    sigma_d    = np.zeros((K, K))
    n_clusters = np.zeros(K)
    all_X = [[] for k in range(K)]

    for i in range(n): 
        all_X[int(labels[i])].append(list(X[i]))
        n_clusters[int(labels[i])] = n_clusters[int(labels[i])] + 1
    
    for i in range(K):
        for j in range(K):
            sigma_d[i][j] = np.log(np.linalg.det(covariances[i])/np.linalg.det(covariances[j]))

    c_candidates = c_candidates = [1/max_candidates*i for i in range(max_candidates+1)]
    best_c  = c_candidates[0]
    best_MC = np.inf
    
    for c in c_candidates:
        MC = 0
        for i in range(K):
            R = []
            for j in range(K):
                if i !=j:
                    R.append([])
                    for l in range(len(all_X[i])):
                        delta_mahal_square = np.dot(np.array([all_X[i][l]-means[j]]), np.dot(np.linalg.inv(covariances[j]), np.array([all_X[i][l]-means[j]]).T))[0][0] - np.dot(np.array([all_X[i][l]-means[i]]), np.dot(np.linalg.inv(covariances[i]), np.array([all_X[i][l]-means[i]]).T))[0][0]
                        if delta_mahal_square / sigma_d[i][j] > c and sigma_d[i][j] > 0:
                            R[-1].append(l)
                        if delta_mahal_square / sigma_d[i][j] < c and sigma_d[i][j] < 0:
                            R[-1].append(l)

            MCi = n_clusters[i]
            for l in range(len(all_X[i])):
                is_well_classified = True
                for j in range(K-1):
                    is_well_classified = is_well_classified and (l in R[j])
                if is_well_classified:
                    MCi = MCi - 1
            MC = MC + MCi
        if MC < best_MC:
            best_MC = MC
            best_c  = c
    
    return best_c"
179,2201.02967,"import numpy as np

def select_random_index(n, k):
    
    """""" Select randomly without repetition k integers taken in the interval [0, n-1].
        Parameters
        ----------
        n : int
            n-1 is the maximum value of the index that can be drawn
        k : int
            number of index drawn
    Returns
    -------
    list_index : list of integers
                 list containing the index randomly drawn
    """"""
    
    list_index = []
    while len(list_index) < k:
        random_index = np.random.randint(0, n)
        if random_index not in list_index:
            list_index.append(random_index)
    return list_index

def ionosphere(path):
    
    """""" Extracts the data from .data file downloaded on the website : 
        http://archive.ics.uci.edu/ml/datasets/Ionosphere. Data are then
        randomly splitted into a train set (70% of the data) and a test set (30% of the data).
        Data are stored in bidimensional arrays.
    Parameters
    ----------
    path : str
           path locating the .data files
    Returns
    -------
    X_train      : 2-d array 
                   train set
    labels_train : 1-d array
                   labels of the observations in the train set
    X_test       : 2-d array 
                   test set
    labels_test  : 1-d array
                   labels of the observations in the test set                   
    """"""
    
    X, labels, X_train, labels_train, X_test, labels_test = [], [], [], [], [], []
    f = open(path + ""ionosphere.data"", ""r"")
    lines = f.readlines()
    for line in lines:
        clean_line = line[:-1]
        X.append(clean_line.split("",""))
    for i in range(len(X)):
        for j in range(len(X[i]) - 1):
            X[i][j] = float(X[i][j])
        if X[i][-1] == 'g':
            labels.append(1)
        if X[i][-1] == 'b':
            labels.append(0)
        X[i] = X[i][:-1]
    list_index_train_set = select_random_index(len(X), int(0.7*len(X)))
    for i in range(len(X)):
        if i in list_index_train_set:
            X_train.append(X[i])
            labels_train.append(labels[i])
        else:
            X_test.append(X[i])
            labels_test.append(labels[i])
            
    return np.array(X_train), np.array(labels_train), np.array(X_test), np.array(labels_test)

def statlog(path):
    
    """""" Extracts the data from .data file downloaded on the website : 
        http://archive.ics.uci.edu/ml/datasets/Statlog+%28Landsat+Satellite%29. There is a 
        file for the train set and the test set. Data are stored in bidimensional arrays.
    Parameters
    ----------
    path : str
           path locating the .data files
    Returns
    -------
    X_train      : 2-d array 
                   train set
    labels_train : 1-d array
                   labels of the observations in the train set
    X_test       : 2-d array 
                   test set
    labels_test  : 1-d array
                   labels of the observations in the test set                   
    """"""
    
    X_train, labels_train, X_test, labels_test = [], [], [], []
    f = open(path + ""sat_trn.data"", ""r"")
    lines = f.readlines()
    for line in lines:
        clean_line = line[:-1]
        X_train.append(clean_line.split("" ""))
    for i in range(len(X_train)):
        for j in range(len(X_train[i])):
            X_train[i][j] = float(X_train[i][j])
        labels_train.append(int(X_train[i][-1]))
        X_train[i] = X_train[i][:-1]

    f = open(path + ""sat_tst.data"", ""r"")
    lines = f.readlines()
    for line in lines:
        clean_line = line[:-1]
        X_test.append(clean_line.split("" ""))
    for i in range(len(X_test)):
        for j in range(len(X_test[i])):
            X_test[i][j] = float(X_test[i][j])
        labels_test.append(int(X_test[i][-1]))
        X_test[i] = X_test[i][:-1]
        
    return np.array(X_train), np.array(labels_train), np.array(X_test), np.array(labels_test)

def breast_cancer(path):
    
    """""" Extracts the data from .data file downloaded on the website : 
        https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
        Data file was renamed from breast-cancer-wisconsin to breast_cancer_wisconsin.
        Data are then randomly splitted into a train set (70% of the data) and a test set (30% of the data). 
    Data are stored in bidimensional arrays.
    Parameters
    ----------
    path : str
           path locating the .data files
    Returns
    -------
    X_train      : 2-d array 
                   train set
    labels_train : 1-d array
                   labels of the observations in the train set
    X_test       : 2-d array 
                   test set
    labels_test  : 1-d array
                   labels of the observations in the test set                   
    """"""
    
    X, labels, X_train, labels_train, X_test, labels_test = [], [], [], [], [], []
    f = open(path + ""breast_cancer_wisconsin.data"", ""r"")
    lines = f.readlines()
    for line in lines:
        clean_line = line[:-1]
        X.append(clean_line.split("",""))
    # There are missing values for the 6th coordinate of some data, we replace it 
    # by the value of another observation randomly drawn.
    t = [402, 30, 28, 19, 30, 4, 8, 21, 9, 132] # numbers of 1.0, 2.0..., 10.0 values
    values_coordinate_6 = [] # empirical distribution of the 6th coordinate's values
    for i in range(10):
        values_coordinate_6 = values_coordinate_6 + [float(i+1) for j in range(t[i])]
    for i in [23, 40, 139, 145, 158, 164, 235, 249, 275, 292, 294, 297, 315, 321, 411, 617]: # corrupted observations
        X[i][6] = str(values_coordinate_6[np.random.randint(683)])
    for i in range(len(X)):
        for j in range(1, len(X[i]) - 1):
            X[i][j] = float(X[i][j])
        if X[i][-1] == '2':
            labels.append(0)
        if X[i][-1] == '4':
            labels.append(1)
        X[i] = X[i][1:-1]
    list_index_train_set = select_random_index(len(X), int(0.7*len(X)))
    for i in range(len(X)):
        if i in list_index_train_set:
            X_train.append(X[i])
            labels_train.append(labels[i])
        else:
            X_test.append(X[i])
            labels_test.append(labels[i])
            
    return np.array(X_train), np.array(labels_train), np.array(X_test), np.array(labels_test)
    
def ecoli(path):
    
    """""" Extracts the data from .data file downloaded on the website : 
        https://archive.ics.uci.edu/ml/datasets/ecoli
        Data are then randomly splitted into a train set (70% of the data) and a test set (30% of the data). 
    Data are stored in bidimensional arrays.
    Parameters
    ----------
    path : str
           path locating the .data files
    Returns
    -------
    X_train      : 2-d array 
                   train set
    labels_train : 1-d array
                   labels of the observations in the train set
    X_test       : 2-d array 
                   test set
    labels_test  : 1-d array
                   labels of the observations in the test set                   
    """"""
    
    X, labels, X_train, labels_train, X_test, labels_test = [], [], [], [], [], []
    f = open(path + ""ecoli.data"", ""r"")
    lines = f.readlines()
    for line in lines:
        clean_line = line[:-1]
        X.append(clean_line.split(""  ""))
    for i in range(len(X)):
        try:
            X[i].remove("""")
        except:
            pass
        for j in range(1, len(X[i]) - 1):
            X[i][j] = float(X[i][j])
        if X[i][-1] == ' cp':
            labels.append(0)
        if X[i][-1] == ' im':
            labels.append(1)
        if X[i][-1] == 'imS':
            labels.append(2)
        if X[i][-1] == 'imL':
            labels.append(3)
        if X[i][-1] == 'imU':
            labels.append(4)
        if X[i][-1] == ' om':
            labels.append(5)
        if X[i][-1] == 'omL':
            labels.append(6)
        if X[i][-1] == ' pp':
            labels.append(7)
        X[i] = X[i][1:-1]
    list_index_train_set = select_random_index(len(X), int(0.7*len(X)))
    for i in range(len(X)):
        if i in list_index_train_set:
            X_train.append(X[i])
            labels_train.append(labels[i])
        else:
            X_test.append(X[i])
            labels_test.append(labels[i])
            
    return np.array(X_train), np.array(labels_train), np.array(X_test), np.array(labels_test)
    
def spambase(path):
    
    """""" Extracts the data from .data file downloaded on the website : 
        https://archive.ics.uci.edu/ml/datasets/spambase
        Data are then randomly splitted into a train set (70% of the data) and a test set (30% of the data). 
    Data are stored in bidimensional arrays.
    Parameters
    ----------
    path : str
           path locating the .data files
    Returns
    -------
    X_train      : 2-d array 
                   train set
    labels_train : 1-d array
                   labels of the observations in the train set
    X_test       : 2-d array 
                   test set
    labels_test  : 1-d array
                   labels of the observations in the test set                   
    """"""
    
    X, labels, X_train, labels_train, X_test, labels_test = [], [], [], [], [], []
    f = open(path + ""spambase.data"", ""r"")
    lines = f.readlines()
    for line in lines:
        clean_line = line[:-1]
        X.append(clean_line.split("",""))
    for i in range(len(X)):
        for j in range(len(X[i])):
            X[i][j] = float(X[i][j])
        labels.append(int(X[i][-1]))
        X[i] = X[i][:-1]
    list_index_train_set = select_random_index(len(X), int(0.7*len(X)))
    for i in range(len(X)):
        if i in list_index_train_set:
            X_train.append(X[i])
            labels_train.append(labels[i])
        else:
            X_test.append(X[i])
            labels_test.append(labels[i])
            
    return np.array(X_train), np.array(labels_train), np.array(X_test), np.array(labels_test)"
