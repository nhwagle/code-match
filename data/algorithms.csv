,id,alg
0,1703.01887,"
\caption{Cooperative neuroevolution  }
\small
\label{ccframework}
\begin{algorithmic}

\STATE \textbf{Step 1:} Decompose the problem   (neuron  or synapse level 
decomposition)
 
\STATE \textbf{Step 2:} Initialise and cooperatively evaluate each 
sub-population\\

\FOR{  each \textit{cycle} until termination }
   \FOR{ each Sub-population}   
     \FOR{  $n$ Generations}
\STATE i) Select and create new offspring 

\STATE ii) Cooperatively evaluate the new offspring 

\STATE iii) Update sub-population\\


 


\ENDFOR
 
 \ENDFOR
 \ENDFOR
  
\end{algorithmic}
"
1,1703.01887,"[!htb]
\smaller
 \KwData{Requires input  $\Psi_{m} $  taken from data 
$\Sigma$ for respective subtasks $\theta_m$.}
 \KwResult{  Prediction error $E$ for dynamic time series  } 
 
 %$\Phi_1  = [\omega_1, \upsilon_1]$.
 
 \For{  each   module $m$   }{

 
 1. Assign fixed depth of search  $\beta$ that defines the 
number of 
generation of evolution for each subcomponent $S_{m}$;  (eg.  $\beta = 5$ )

   2. Define the weight space (input - hidden and hidden - output layer) for 
the different subtasks $\theta_m$ defined by the respective network module 
$\Phi_m  = [\omega_m, \upsilon_m]$.\; 


3. Create $X_m$ vector of subtask solutions from $\Theta_m$.  Note 
that the size of  $X_m$ depends on number of  network module considered in the 
subtask.
 
  4. Initialise  genotype of  the sub-populations 
$S_{m_j}$  given by $j$ individuals in a range $[-\alpha, \alpha]$ drawn from 
a uniform distribution  where $\alpha$ defines the range.    
 }


 \While{each phase until termination }{
 
 
 \For{each subtask $m$ }{
 
  
 
 \For{each generation until $\beta$}{  
  

\For{  each Individual $j$ in subpopulation $\Pi_{i,j}$   }{
** Stage 1:\\
 * Assign $B_m$ as  best  individual for subtask $m$ from subpopulation 
$\Pi_{i,j}$ \\
 \If{$m ==1 $}{
  *  Get the current  individual from  subpopulation; $V_1   = 
\Pi_{i,j}$ where     $i$ includes all    the  
variables (weights).  Assign the current individual as $X_1 = V_1$

 }
 \Else{
 *   Get the current   individual from  subpopulation; $V_m  = 
\Pi_{i,j}$. Append  the current subtask solution   with best solutions 
from previous subtasks, $X_{m} = [B_1,...,  B_{m-1}, V_m]$
where $B$ is the best individual from previous subtask and $V$ is the current 
individual that needs to be evaluated. 




}
 ** Stage 2:\\
* Execute Algorithm \ref{alg:two}: This will encode the $X_{m}$ into knowledge
modules 
 $ \Phi=(\Phi_1,\ldots,\Phi_M) $ for subtasks $f(\theta_m, \Psi_{m} )$

*   Calculate subtask output $y_m = f(\theta_m, \Psi_{m})$  and evaluate the 
given
genotype  though the loss function  $ E  = 
\frac{1}{T}  \sum_{t=1}^T 
L_t   $, where $L_t$ is given in Equation \ref{eq:loss}. \\

 }
 
 
\For{  each Individual $j$ in $S_m$  }{
 * Create new offspring via evolutionary operators such 
selection,  crossover and mutation\\
 
}

* Update $S_m$
\;
  
 
   
  
  }
  
  }
 }
 
 * Test the obtained solution 
 
 \For{  each module $m$  }{

 1.  Load best solution $S_{best}$ from   $S_{m}$. \;
 
 2. Encode   into the respective
weight space for the subtask $\Phi_m  = [\omega_m, \upsilon_m]$.\;

3. Calculate loss $E$ based on the test dataset.
 
  

 }
 
 
 \caption{ Co-evolutionary multi-task learning }
 
\label{alg:one}
"
2,1703.01887,"[!htb] 
 \smaller
 \textbf{Parameters: }
  Subtask $m$,   module subtask solution $X$  , Input $I$, Hidden $H$  and 
Output $O$ \\  
 
 
$b = 1$; ( Base task)

Step 1 



\For{each $j=1$   to $H_{b}$ } {
\For{each  $i=1$   to $I_{b}$ } {

    $W_{ij}  = X_t$  \; \\
    $t = t+1$ \; 
 }
 }  

 Step 2 

\For{each $k=1$  to $O_{b}$ } {
\For{each $j=1$   to $H_{b}$} {

    $W_{jk}  = X_t$  \; \\
    $t = t+1$ \; 
 }
 } 
 
  
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   
   \If{m $>=$ 2}{ 
   
  Step 3 
   
   
\For{each  $j= H_{m -1} + 1$ to $H_{m}$ } {
\For{each  $i=1$   to $I_{m}$ } {

       $W_{ij}  = X_t$  \; \\
    $t = t+1$ \; 
 }
 }  
 
 Step 4
   
\For{each  $j=1$   to $H_{m}$ -1 } {
\For{each  $i=I_{m-1}+1$ to $I_{m}$} {

        $W_{ij}  = X_t$  \; \\
    $t = t+1$ \; 
 }
 } 
  
   Step 5 
    
  \For{each $k=1$  to $O_{m}$ } {
   \For{each $j=H_{m-1}+ 1$   to $H_{m}$} {

      $W_{j,k}  = X_t$  \; \\
     $t = t+1$ \; 
   }
   } 
  
     
   }
    
   
 
 
 \caption{Transfer of knowledge from previous subtasks}
 
\label{alg:two}
"
3,1706.03922,"
\caption{Robust\_1NN($S_n$, $r$, $\Delta$, $\delta$, $x$)}
\begin{algorithmic}
\FOR {$(x_i, y_i)\in S_n$}
\STATE {$f(x_i) = \mbox{Confident-Label}(S_n, \Delta, \delta, x_i)$}
\ENDFOR
\STATE{$S_{RED}=\emptyset$}
\FOR {$(x_i, y_i) \in S_n$}
\IF {$f(x_i) = y_i$ and $f(x_i) = f(x_j)$ for all $x_j$ such that $\| x_i - x_j \| \leq r$ and $(x_j, y_j) \in S_n$}
\STATE {$S_{RED} = S_{RED}\bigcup \{(x_i, y_i)\}$}
\ENDIF
\ENDFOR
\STATE {Let $S'$ be the largest $2r$-separated subset of $S_n$ that contains all points in $S_{RED}$.}
\RETURN new training set $S'$
\end{algorithmic}
\label{alg:robust_1nn}
"
4,1706.03922,"
\caption{Confident-Label($S_n$, $\Delta$, $\delta$, $x$)}
\begin{algorithmic}
\STATE{$k_n = 3\log(2n/\delta)/\Delta^2$}
\STATE{$\bar{y} = (1/k_n)\sum_{i=1}^{k_n}Y^{(i)}(x)$}
\IF {$\bar{y} \in [ \frac{1}{2} -\Delta, \frac{1}{2} + \Delta]$}
\RETURN{$\bot$}
\ELSE
\RETURN{$\frac{1}{2} sgn(\bar{y} - \frac{1}{2}) + \frac{1}{2}$}
\ENDIF
\end{algorithmic}
\label{alg:confident_label}
"
5,1805.06837,"[ht]
	\caption{Recover the word-topic matrix $A$ from $\M$}\label{alg}
	\begin{algorithmic}[1]
		\Require true word-document frequency matrix $\M\in\RR^{p\times n}$
		\Procedure{Top}{$\M$}
		\State compute $\Theta = n^{-1}\M\M^T$ and $R$ from (\ref{def_R})
		\State recover $\I$ via \textsc{FindAnchorWords}$(R)$
		\State construct $L = \{i_1, \ldots, i_K\}$ by choosing any $i_k \in I_k$, for $k\in [K]$
		\State compute $B_I$ from (\ref{bj}) and $B_J$ from (\ref{eq_iden_BI})
		\State recover $A$ by normalizing $B$ to unit column sums
		\State\Return $\I$ and $A$
		\EndProcedure
		\Statex
		\Procedure{FindAnchorWords}{$R$}
		\State initialize $\I = \emptyset$ and $
		\mathcal{P} = [p]$
		\While{$\mathcal{P}\ne \emptyset$} 
		\State take any $i\in \mathcal{P} $, compute $S_i$ and $T_i$ from (\ref{comp})
		\If {$\exists j\in S_i$ s.t. $T_i \ne T_j$}
		\State $\mathcal{P} = \mathcal{P} \setminus \{i\}$
		\Else 
		\State {$\mathcal{P} = \mathcal{P} \setminus S_i$ and $S_i \in \I$}
		\EndIf
		\EndWhile
		\State\Return $\I$
		\EndProcedure
	\end{algorithmic}
"
6,1805.06837,"[ht]
	\caption{Estimate the partition of the anchor words $\I$ by $\wh \I$}\label{alg1}
	\begin{algorithmic}[1]
		\Require matrix $\wh R\in\RR^{p\times p}$, $C_1$ and $Q\in\RR^{p\times p}$ such that $Q[j,\ell] := C_1\delta_{j\ell}$ 
		\Procedure{FindAnchorWords}{$\wh R$, $Q$}
		\State initialize $\wh \I = \emptyset$
		\For{$i\in [p]$} 
		\State $ \wh a_i = \argmax_{1\le j\le p}\wh R_{ij}$
		%\{j\in [p]: \wh R_ {ij}\ge \wh R_{il}, \forall\ l\in [p]\}$.
		\State set $\wh I^{(i)} =  \{\ell\in [p]: \wh R_{i\wh a_i}-\wh R_{il} \le Q[i,\wh a_i]+ Q[i,\ell]\}$ and $\textsc{Anchor}(i) = \textsc{True}$
		\For {$j \in \wh I^{(i)}$}
		\State $\wh a_j = \argmax_{1\le k\le p}\wh R_{jk}$
		\If {$\Bigl|\wh R_{ij}-\wh R_{j\wh a_j}\Bigr| > Q[i,j] + Q[j, \wh a_j]$}   
		\State $\textsc{Anchor}(i) =\textsc{False}$
		\State \textbf{break}
		\EndIf	
		\EndFor
		\If {$\textsc{Anchor}(i) $}
		\State $\wh \I = \textsc{Merge}(\wh I^{(i)}$, $\wh \I$)
		\EndIf
		\EndFor
		\State\Return $\wh \I = \{ \wh I_1, \wh I_2, \ldots, \wh I_{\wh K}\}$ 
		\EndProcedure
		\Statex
		
		\Procedure{Merge}{$\wh I^{(i)}$, $\wh\I$}
		\For {$G \in \wh \I$}
		\If {$G \cap \wh I^{(i)}\ne \emptyset$} 
		\State replace $G$ in $\wh \I$ by $G\cap \wh I^{(i)}$
		\State\Return $\wh \I$
		\EndIf
		\EndFor
		\State {$\wh I^{(i)} \in \wh \I$}
		\State\Return $\wh \I$
		\EndProcedure
	\end{algorithmic}
"
7,1805.06837,"[H]
	\caption{Estimate the word-topic matrix $A$}\label{alg2}
	\begin{algorithmic}[1]
		\Require frequency data matrix $X\in\RR^{p\times n}$ with document lengths $N_1, \ldots, N_n$; two positive constants $C_0, C_1$ and positive integer $T$
		\Procedure{Top}{$X, N_1,\ldots, N_n; C_0, C_1$}
		\State compute $\wh \Theta$ from (\ref{est_Theta}) and $\wh R$ from (\ref{def_R_hat})
		\State compute $\wh \eta_{ij}$ and $Q[i, j]:=C_1 \wh\delta_{ij}$ from (\ref{delta3}) and (\ref{def_eta3}), for $i, j \in [p]$
		\State estimate $\I$ via \textsc{FindAnchorWords}$(\wh R, Q)$
		\For {$i = 1,\ldots, T$}
		\State randomly select $\wh L$ and solve $\wh \Omega$ from (\ref{obj_omega}) by using $\lambda = C_0\max_{i\in \wh L}\sum_{j\in \wh L}\wh \eta_{ij}$ in (\ref{constr_omega})
		\State estimate $B$ from (\ref{est_BI}) and (\ref{est_B})
		\State compute $\wh A^i$ from (\ref{est_A})
		\EndFor
		\State \Return $\wh \I = \{ \wh I_1, \wh I_2, \ldots, \wh I_{\wh K}\}$ and $\wh A = T^{-1}\sum_{i =1}^T\wh A^i$
		\EndProcedure
	\end{algorithmic}
"
8,1806.04743,"[H]
  \caption{Inference-Aware Neural Optimisation.}
  \begin{flushleft}
    {\it Input 1:} differentiable simulator or variational
    approximation $g(\boldsymbol{\theta})$. \\
    {\it Input 2:} initial parameter values $\boldsymbol{\theta}_s.$ \\
    {\it Input 3:} parameter of interest $\omega_0=\theta_k$.
     \\
    {\it Output:} learned summary statistic
      $\boldsymbol{s}(D; \boldsymbol{\phi})$.\\
 \end{flushleft}
 \begin{algorithmic}[1]
 \For{$i=1$ to $N$}
  \State{Sample a representative mini-batch $G_s$ from
  $g(\boldsymbol{\theta}_s)$.}
  \State{Compute differentiable summary statistic
    $\hat{\boldsymbol{s}}(G_s;\boldsymbol{\phi})$.}
  \State{Construct Asimov likelihood
    $\mathcal{L}_A(\boldsymbol{\theta}, \boldsymbol{\phi})$.}
  \State{Get information matrix inverse $I(\boldsymbol{\theta})^{-1}
  = \boldsymbol{H}_{\boldsymbol{\theta}}^{-1}(\log
  \mathcal{L}_A(\boldsymbol{\theta}, \boldsymbol{\phi}))$.}
  \State{Obtain loss
    $U= I_{kk}^{-1}(\boldsymbol{\theta}_s)$.}
  \State{Update network parameters $\boldsymbol{\phi} \rightarrow
  \textrm{SGD}(\nabla_{\boldsymbol{\phi}} U)$.}  
 \EndFor
 \end{algorithmic}
 \label{alg:simple_algorithm}
"
9,1806.02185,"[H]
\caption{Affine Invariant Frank-Wolfe}
  \label{algo:FW}
\begin{algorithmic}[1]
%
  \STATE \textbf{init} $q^{0} \in \conv(\cA)$, $\cS:=\{q^{0}\}$, \edit{and accuracy $\delta > 0$}
  \STATE \textbf{for} {$t=0\dots T$}
  \STATE \qquad Find $s^t := (\text{Approx-}) \lmo_\cA(\nabla f(q^{t}))$
  %
  %
  \STATE \qquad Variant 0:  $\gamma=\frac{2}{\edit{\delta }t+2}$\\
  \STATE \qquad Variant 1: $\gamma=\argmin_{\gamma\in[0,1]} f((1-\gamma)q^{t} + \gamma s^t )$ \\
  \STATE \qquad $q^{t+1}:= (1-\gamma)q^{t} + \gamma s^t$\\
  \STATE \qquad Variant 2: $\cS = \cS \cup s^t$\\
  \STATE \qquad\qquad\qquad $q^{t+1} = \argmin_{q\in\conv(\cS)} f(q)$
%
%
  %
  \STATE \textbf{end for}
\end{algorithmic}
"
10,1806.04610,"[!t]
	\caption{Gibbs sampler for Gaussian copula factor model with missing values}
	\label{GS_GCFM}
	\begin{algorithmic}[1]
		\REQUIRE Prior graph $G$, observed data $\bm{Y}$. \\
		\# \textbf{Step 1} and \textbf{Step 2}:
		%sample $\bm{Z} \sim P(\bm{Z}|\bm{\eta},\bm{Z} \in D(\bm{Y}),\Omega)$.
		%Iteratively over $(i,j)$, sample $z_{i,j}$ from $P(z_{i,j}|\eta_{[-i,j]}, \bm{z}_j \in D(\bm{y}_j),\Omega)$ as follows:
		\FOR{$j \in \{1,\ldots,p\}$}
		\STATE $q=$ factor index of $Z_j$
		\STATE $ a = \Sigma_{[j,q+p]} / \Sigma_{[q+p,q+p]}$
		\STATE $\sigma_j^2 = \Sigma_{[j,j]}-a \times \Sigma_{[q+p,j]}$ \\
		\# \textbf{Step 1}: $\bm{Z}_{obs} \sim P(\bm{Z}_{obs}|\bm{\eta},\bm{Z}_{obs} \in \D(\bm{Y}_{obs}),\Omega)$
		\FOR{$y \in \unique \{y_{1,j},\ldots,y_{n,j}\}$}
		\STATE $z_l = \max\{z_{i,j}:y_{i,j}<y\}$
		\STATE $z_u = \min\{z_{i,j}:y<y_{i,j}\}$
		\FOR{$i$ such that $\: y_{i,j} = y$}
		\STATE $\mu_{i,j} = \bm{\eta}_{[i,q]} \times a$
		\STATE $u_{i,j} \sim \mathcal{U}\big(\Phi\big[\frac{z_l-\mu_{i,j}}{\sigma_j}\big],\Phi\big[\frac{z_u-\mu_{i,j}}{\sigma_j}\big]\big)$
		\STATE $z_{i,j} = \mu_{i,j} + \sigma_j \times \Phi^{-1}(u_{i,j})$
		\ENDFOR
		\ENDFOR \\
		\# \textbf{Step 2}: $\bm{Z}_{miss} \sim P(\bm{Z}_{miss}|\bm{\eta},\bm{Z}_{obs},\Omega)$
		\FOR{$i$ such that $y_{i,j} \in \bm{Y}_{miss}$}
		\STATE $\mu_{i,j} = \bm{\eta}_{[i,q]} \times a$
		\STATE $z_{i,j} \sim \mathcal{N}(\mu_{i,j}, \sigma_j^2)$
		\ENDFOR	
		\ENDFOR
		\STATE $\bm{Z} = (\bm{Z}_{obs}, \bm{Z}_{miss})$ 
		\STATE $\bm{Z} = (\bm{Z}^T - \bm{\mu})^T$, with $\bm{\mu}$ the mean vector of $\bm{Z}$ \\
		\# \textbf{Step 3}: $\bm{\eta} \sim P(\bm{\eta}|\bm{Z},\Omega)$
		%Iteratively over $(i,j)$, sample $\eta_{i,j}$ from $P(\eta_{i,j}|\bm{X}_{[-i,-j]},\Omega)$ as follows:
		\STATE $A = \Sigma_{[\bm{\eta},\bm{Z}]}\Sigma_{[\bm{Z},\bm{Z}]}^{-1}$
		\STATE $B = \Sigma_{[\bm{\eta},\bm{\eta}]}-A\Sigma_{[\bm{Z},\bm{\eta}]}$
		\FOR{$i \in \{1,\ldots,n\}$}		
		\STATE $\bm{\mu}_i = (\bm{Z}_{[i,:]}A^T)^T$
		\STATE  $\bm{\eta}_{[i,:]} \sim \mathcal{N}(\bm{\mu}_i, B)$
		\ENDFOR
		\STATE $\bm{\eta}_{[:,j]} = \bm{\eta}_{[:,j]} \times \sign(\Cov{\bm{\eta}_{[:,j]}, \bm{Z}_{[:,f(j)]}}), \: \forall j$, where $f(j)$ is the index of the first indicator of $\eta_j$.  \\
		\# \textbf{Step 4}: $\Omega \sim P(\Omega|\bm{Z},\bm{\eta},G)$
		\STATE $\bm{X} = (\bm{Z}, \bm{\eta})$
		\STATE  $\Omega \sim \Wish_G(\nu_0 + n, \Psi_0 + \bm{X}^T\bm{X})$
		\STATE $\Sigma = \Omega^{-1}$
		\STATE $\Sigma_{ij} = \Sigma_{ij}/\sqrt{\Sigma_{ii}\Sigma_{jj}}, \forall i,j$
	\end{algorithmic}
"
11,1806.04610,"[H]
	\caption{Pseudo code of MLR for regression.}
	\label{alg:MLR_reg}
	%\setstretch{1.4}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} $\bm{Y}^{(train)}$ and $\bm{Y}_{\backslash j}^{(new)}$.
		\STATE \textbf{Output:} $Y_j^{(new)}$.
		\STATE \textbf{Training Stage:}
		\STATE Fit the model using MLR on $\bm{Y}^{(train)}$;
		\STATE Extract the model-implied covariance matrix from the fitted model, denoted by $\hat{S}$;
		\STATE Extract regression coefficients $\bm{b}$ of $Y_j$ on $\bm{Y}_{\backslash j}$ from $\hat{S}$, that is, $\bm{b} = \hat{S}_{[\backslash j,\backslash j]}^{-1} \hat{S}_{[\backslash j,j]}$;
		\STATE Obtain the regression intercept $b_0$, that is, \\ $b_0 = \E (Y_j^{(train)}) - \bm{b} \cdot \E (\bm{Y}_{\backslash j}^{(train)})$.
		\STATE \textbf{Prediction Stage:}
		\STATE $Y_j^{(new)} = b_0 + \bm{b} \cdot \bm{Y}_{\backslash j}^{(new)}$.
	\end{algorithmic}
"
12,1806.04610,"[H]
	\caption{Pseudo code of BGCF for regression.}
	\label{alg:BGCF_reg}
	%\setstretch{1.4}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} $\bm{Y}^{(train)}$ and $\bm{Y}_{\backslash j}^{(new)}$.
		\STATE \textbf{Output:} $Y_j^{(new)}$.
		\STATE \textbf{Training Stage:}
		\STATE Apply BGCF to learn the correlation matrix over response variables, i.e., $\hat{S} = \hat{\Sigma}_{[\bm{Z}, \bm{Z}]}$;
		\STATE Learn the empirical cumulative distribution function of $Y_j$, denoted by $\hat{F}_j$.
		\STATE \textbf{Prediction Stage:}
		\STATE Sample $Z_j^{(new)}$ from $P(Z_j^{(new)} |\hat{S}, \bm{Z}_{\backslash j} \in \mathcal{D}(\bm{Y}_{\backslash j}))$;
		\STATE Obtain $Y_j^{(new)}$, i.e., $Y_j^{(new)} = \hat{F}_j^{-1} \big(\Phi[Z_j^{(new)}]\big)$.
	\end{algorithmic}
"
13,1805.07984,"[t]
	\SetKwData{Score}{score}
	\SetKwData{EdgeScores}{edge\_scores}
	\SetKwData{BestEdge}{$e^*$}
	\SetKwData{BestFeature}{$f^*$}
	\SetKwData{FeatScores}{feature\_scores}
	\SetKwData{ModEdges}{edge\_changes}
	\SetKwData{ModFeats}{feature\_changes}
	\SetKwFunction{CandEdges}{candidate\_edge\_perturbations}	
	\SetKwFunction{EdgeScore}{score\_edges}	
	\SetKwFunction{FeatScore}{score\_features}
		\SetKwFunction{Append}{append}	
		\SetKwFunction{AddEdge}{add\_edge}	
		\SetKwFunction{RemEdge}{remove\_edge}
		\SetKwFunction{AddFeat}{add\_feature}	
		\SetKwFunction{RemFeat}{remove\_feature}	
		\SetKwFunction{Argmax}{argmax}	
		\SetKwFunction{Len}{len}	
	\SetKwFunction{CandFeatures}{candidate\_feature\_perturbations}
	\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	\SetKwInput{Otp}{Output}
	\SetKwInput{Itp}{Input}
	\SetKwInOut{ForIn}{in}
	\SetKwInOut{Return}{return}\small
	\Itp{Graph $G^{(0)} \leftarrow (A^{(0)}, X^{(0)})$, target node $\Vtarget$,\qquad\qquad\quad   attacker nodes $\mathcal{A}$, modification budget $\Delta$}
	\Otp{Modified Graph $G' = (A', X')$}
	\BlankLine
	Train surrogate model on $G^{(0)}$ to obtain $W$ // Eq.~\eqref{eq:surrogate}\;
	$t \leftarrow 0$ \;
	\While{$|A^{(t)} - A^{(0)}| + |X^{(t)} - X^{(0)}| < \Delta$}{
		
		$C_{struct} \leftarrow \CandEdges(A^{(t)},\mathcal{A})$ \; 

		$e^*= (u^*,v^*) \leftarrow \underset{e \in {C_{struct}}}{\arg \max} \: s_{struct}\left( e ; G^{(t)} ,\Vtarget \right)$ \;
		$C_{feat} \leftarrow \CandFeatures(X^{(t)},\mathcal{A})$ \;
		$f^*= (u^*,i^*) \leftarrow \underset{f \in {C_{feat}}}{\arg \max} \: s_{feat}\left( f ; G^{(t)},\Vtarget \right)$ \;
		
		\lIf{$s_{struct}(e^*; G^{(t)},\Vtarget) > s_{feat}(f^*; G^{(t)},\Vtarget)$ }{
			$G^{(t+1)} \leftarrow G^{(t)} \pm e^* $
		}
		\lElse{
			$G^{(t+1)} \leftarrow G^{(t)} \pm f^* $
		}
		$t \leftarrow t+1$\;
	}
	\Return{$G^{(t)}$}
	// Train final graph model on the corrupted graph $G^{(t)}$\;
	\caption{\ours: Adversarial attacks on graphs }\label{alg:adv_attack}
"
14,1803.11262,"[t]
%\setstretch{1.6}%\large 
\caption{Fast Gradient Method}
\label{alg:fgm}
\begin{algorithmic}%[1]
{
\REQUIRE stepsize $\eta > 0$\\
\STATE $u^0 = u_\omega$ \\
\STATE $g^0 =0 \in E$\\
\FOR{$t = 0, 1, ...$}
\STATE $u_t = \prox_{\eta \Psi, u_\omega}\left(\eta g^t\right)$ \\
\STATE $\tau_t = \frac{2(t+2)}{(t+1)(t+4)}$ \\
\STATE $u_{t+\frac{1}{3}} = \tau_t u_t + (1-\tau_t) u^t$ \\
\STATE $g_{t} = \frac{t+2}{2} \nabla f(u_{t+\frac{1}{3}})$ \\
\STATE $u_{t+\frac{2}{3}} = \prox_{\eta \Psi, u_t}\left(\eta g_{t}\right)$ \vspace{0.05cm}\\
\STATE $u^{t+1} = \tau_t u_{t+\frac{2}{3}} + (1-\tau_t) u^t$\\
\STATE $g^{t+1} = \sum_{\tau=0}^t g_{t}$\\
\ENDFOR
}
\end{algorithmic}
"
15,1803.11262,"[p]
%\setstretch{1.6} %\large 
%\caption{: \texttt{Fast Gradient Method with Adaptive Stepsize}}
%\vspace{0.3cm}
%\label{alg:fgm-ls}
%\begin{algorithmic}[1]
%{
%\STATE $y_0 := z_\omega$\\
%\STATE $G_0 := 0 \in E$\\
%\FOR{$t := 0, 1, ...$}
%\STATE $s := 0$\\
%\REPEAT
%\STATE $L_{t} := 2^s \underline{L}$\\
%\STATE $z_t := \prox_{\frac{1}{L_t}\Psi, z_\omega}\left(\frac{1}{L_t} G_t\right)$ \\
%\STATE $\tau_t := \frac{2(t+2)}{(t+1)(t+4)}$ \\
%\STATE $x_{t+1} := \tau_t z_t + (1-\tau_t) y_t$ \\ 
%\STATE $g_{t+1} := \frac{t+2}{2} \nabla f(x_{t+1})$ \\
%\STATE $\widehat x_{t+1} := \prox_{\frac{1}{L_t} \Psi, z_t}\left(\frac{1}{L_t} g_{t+1}\right)$ \\
%\STATE $y_{t+1} := \tau_t \widehat x_{t+1} + (1-\tau_t) y_t$\\
%\STATE $\delta_{t} := \frac{L_t}{2} \|y_{t+1} - x_{t+1}\|^2  + \langle \nabla f(x_{t+1}), y_{t+1} - x_{t+1} \rangle + f(x_{t+1}) - f(y_{t+1})$\\
%\STATE $s := s+1$\\
%\UNTIL $\delta_t > 0$
%\STATE $G_{t+1} := G_t + g_{t+1}$\\
%\ENDFOR
%}
%\end{algorithmic}
%"
16,1803.11262,"[t]
%\setstretch{1.6} %\large 
\caption{Composite Mirror Prox}
\label{alg:cmp}
\begin{algorithmic}%[1]
\REQUIRE stepsize $\eta > 0$\\
%$\{\gamma_t\}_{t = 0, 1, ...}$\\
\STATE $w_0 := [u_0; v_0] = w_\omega$
\FOR{$t = 0, 1, ...$}
\STATE $w_{t+\frac{1}{2}} = \text{Prox}_{\eta \Psi, w_t}(\eta F(w_t))$
\STATE $w_{t+1} = \text{Prox}_{\eta \Psi, w_t}(\eta F(w_{t+\frac{1}{2}}))$
\STATE $w^{t+1} := [u^{t+1}; v^{t+1}] = \frac{1}{t+1}\sum_{\tau = 0}^{t} w_\tau$ %[\sum_{\tau=1}^t \gamma_\tau]^{-1} \sum_{\tau=1}^t \gamma_\tau
\ENDFOR
\end{algorithmic}
"
17,1805.05809,"[!hbtp]
   \small
   \caption{Learning algorithm}
   \label{alg:procedure}
   \begin{algorithmic}[1]
      \INPUT $\bftheta_b^{\text{emb}}$ (pretrained metric learning base model); $\bftheta_d \in \mathbb{R}^d$
      \REQUIRE $\bftheta_f = [\bftheta_b, \bftheta_d]$
      \FOR{ $t=1,\ldots,$ MAXITER}
      \STATE Sample a minibatch $\{\bfx_j\}$
      \STATE Update the flow network $G'$ by recomputing the cost vectors for all classes in the minibatch\\
      $\bfc_i = \frac{1}{m} \sum_{k: y_k = i} f(\bfx_k; \bftheta_f)$
      \STATE Compute the hash codes $\{\bfh_i\}$ minimizing \Cref{eqn:k_energy_avg} via finding the minimum cost flow on $G'$
      \STATE Update the network parameter given the hash codes\\
      $\bftheta_f \leftarrow \bftheta_f - \eta^{(t)} \partial \ell_\text{metric}(\bftheta_f; ~\bfh_{1,\ldots,n_c}) / \partial \bftheta_f$
      \STATE Update stepsize $\eta^{(t)} \leftarrow$ ADAM rule \cite{adam}
   \ENDFOR
   \OUTPUT $\bftheta_{f}$ (final estimate);
\end{algorithmic}
"
18,1806.04310,"[tb]
   \caption{MISSION}
   \label{alg:algorithm}
\begin{algorithmic}
   \STATE {\bf Initialize}: $\beta^0 = 0$, $\mathcal{S}$ (Count sketch), $\lambda$ (Learning Rate)
   \WHILE {not stopping criteria}
   \STATE Find the gradient update $g_i~=~\lambda \left( 2\left( y_i - x_i {\boldsymbol \beta}^t \right)^T x_i \right)$ 
   \STATE Add the gradient update to the sketch $g_i \rightarrow \mathcal{S}$
   \STATE Get the top-$k$ heavy-hitters from the sketch ${\boldsymbol \beta}^{t+1} \leftarrow  \mathcal{S}$
   \ENDWHILE
   \STATE {\bf Return:} The top-$k$ heavy-hitters from the count-sketch
\end{algorithmic}
"
19,1806.04310,"[tb]
   \caption{\mission{}}
   \label{alg:algorithm}
\begin{algorithmic}
   \STATE {\bf Initialize}: $\beta^0 = 0$, $\mathcal{S}$ (Count-Sketch), $\lambda$ (Learning Rate)
   \WHILE {not stopping criteria}
   \STATE Find the gradient update $g_i=\lambda \left( 2\left( y_i - {\bf X}_i {\boldsymbol \beta}^t \right)^T {\bf X}_i \right)$ 
   \STATE Add the gradient update to the sketch $g_i \rightarrow \mathcal{S}$
   \STATE Get the top-$k$ heavy-hitters from the sketch ${\boldsymbol \beta}^{t+1} \leftarrow  \mathcal{S}$
   \ENDWHILE
   \STATE {\bf Return:} The top-$k$ heavy-hitters from the Count-Sketch
\end{algorithmic}
"
20,1806.04308,"
\caption{Conditional Feature Sampling using DPP}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE Best candidate feature set: $X \in \mathbb{R}^{d\times n}$ , new set of features $G$, reconstruction error $\alpha$
\ENSURE  Sample of features from $G$
\\ \STATE Construct similarity matrix $L$ based on $X \cup G$. 
\\ \STATE Sample features from $G$ conditioning on $X$ using conditional DPP
\end{algorithmic}
"
21,1806.04308,"
\caption{Wilcoxon Criterion}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE Best candidate feature set: $X \in \mathbb{R}^{d\times k}$, proposed single new feature $f$, significance level $\alpha$
\ENSURE  Boolean, if feature $f$ is discarded or kept
\\ \FOR {each feature $x$ in $X$}
\STATE $p \leftarrow$ Wilcoxon signed-rank test. 
\STATE If $p > \alpha$. Then discard $f$ and terminate, otherwise continue
\ENDFOR
\STATE Keep feature $f$
\end{algorithmic}
"
22,1806.04308,"
\caption{Supervised Criterion}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE Incoming set of features: $U \in \mathbb{R}^{d\times k}$, significance level $\alpha$
\ENSURE  A set $G$, representing the set of selectioned features
\\ \textit{Initialize:} $G = \{\}$
\\ \STATE Sort $U$ according to function $s$
\\ \FOR {each feature $f$ in $U$}
\STATE If $F(G \cup f) - F(G) > \epsilon$ then $G = G \cup f$
\STATE If $t(f, G) > \alpha$ then $G = G \cup f$
\ENDFOR
\\ \STATE Return $G$
\end{algorithmic}
"
23,1806.04308,"
\caption{Global Criterion}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE Incoming set of features: $U \in \mathbb{R}^{d\times n}$, tolerance level $\lambda$
\ENSURE  A set $G$, representing the set of selectioned features
\\ \textit{Initialize:} $G = \{\}$
\\ \STATE Fit a model with elasticnet regularizer
\\ \FOR {each feature $f$ in $U$}
\STATE If $\lvert \beta_f \rvert \geq \lambda$ then $G = G \cup f$
\ENDFOR
\\ \STATE Return $G$
\end{algorithmic}
"
24,1806.04308,"
\caption{Diverse Online Feature Selection}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE Best feature candidate feature set $X$, Feature stream $F$, label vector $Y$
\ENSURE  A set $G$, representing the set of selectioned features
\\ \WHILE{features are arriving}
\STATE $G \leftarrow$ generate new group of features
\\ \STATE Sample features from $G$ using DPP to get sampled subset $G'$ conditional on $X$
\\ \FOR {$f$ in $G'$}
\STATE \textbf{Local Criterion}: Evaluate feature $f$ using unsupervised and/or supervised criterions to determine relevancy
\\ \STATE \textbf{Global Criterion}: Perform redundacy check based on regulariser
\ENDFOR
\ENDWHILE
\end{algorithmic}
"
25,1806.04090,"[H]
		\SetKwInOut{Input}{Input}
		\SetKwInOut{Output}{Output}
		\Input{$\lambda \in \real^n$ with $|\lambda_1| \geq \ldots |\lambda_n| $; sparsity budget $s$ such that $0 < s \leq n$.}
		\Output{$p \in \real^n$ solving \eqref{eq:opt_prob1}.}
		$i = 1$\;
		\While{$i \leq n$}{
			\eIf{ $|\lambda_i|s \leq \sum_{j=i}^n |\lambda_i|$}{
				\For{$k = i,\ldots, n$}{
				$p_k = |\lambda_k|s\left(\sum_{j=i}^n |\lambda_i|\right)^{-1}$\;
				}
				$i = n+1$\;				
			}{
				$p_i = 1, s = s-1$\;
				$i = i+1$\;
			}
		}
		\caption{\atomsgd{} probabilities}
		\label{alg:opt_sparse}
	"
26,1802.04325,"
	\caption{Variational State Tabulation.
				\label{alg:VaST}}
	Initialize replay memory $\mathcal{M}$ with capacity $\mathcal{N}$\\
	Initialize sweeping table process $\mathcal{B}$ with transition add queue $\mathcal{Q}^+$ and delete queue $\mathcal{Q}^-$
   \begin{algorithmic}[1]
	\FOR{each episode}
		\STATE Set $t \leftarrow 0$
		\STATE Get initial observations $o_0$\\
		\STATE Process initial state $\bar{s}_0 \leftarrow \argmax_{s} q_\phi(s|o_{0})$\\
		\STATE Store memory $(o_{0},\bar{s}_{0})$ in $\mathcal{M}$
		\WHILE{not terminal}
			\STATE Set $t \leftarrow t + 1$
			\STATE Take action $a_{t}$ with $\epsilon$-greedy strategy based on $\tilde{Q}(s_{t-1},a)$ from $\mathcal{B}$
			\STATE Receive $r_t$, $o_t$
			\STATE Process new state $\bar{s}_t \leftarrow \argmax_{s} q_\phi(s|o_{t-k:t})$
			\STATE Store memory $(o_{t},\bar{s}_{t},a_{t},r_{t})$ in $\mathcal{M}$
			\STATE Put transition $(\bar{s}_{t-1},a_{t},r_{t},\bar{s}_{t})$ on $\mathcal{Q}^+$
			\IF{training step}
				\STATE Set gradient list $\mathcal{G} \leftarrow \{\}$
				\FOR{sample in minibatch}
					\STATE Get $(o_{j-k - 1:j}, a_j)$ from random episode and step $j$ in $\mathcal{M}$
					\STATE Process $q_\phi(s_{j-1}|o_{j-k-1:j-1})$, $q_\phi(s_j|o_{j-k:j})$ with encoder
					\STATE Sample $\hat{s}_{j-1}$, $\hat{s}_{j} \sim \hat{q}_\phi$ with temperature $\lambda$
		  			\STATE Process $p_\theta(o_j|\hat{s}_j)$, $p_\theta(\hat{s}_j|a_j,\hat{s}_{j-1})$ with decoder and transition network
		  			\STATE Append $\nabla_{\theta, \phi} \mathcal{F}(\theta, \phi; o_{j-k - 1:j})$ to $\mathcal{G}$
					\FOR{$i$ in \{$j-1$, $j$\}}
						\STATE Process $\bar{s}^{new}_i \leftarrow \argmax_{s} q_\phi(s|o_{i-k:i})$
						\STATE Get ($\bar{s}_{i-1}$, $a_{i}$, $r_i$, $\bar{s}_i$, $a_{i+1}$, $r_{i+1}$, $\bar{s}_{i+1}$) from $\mathcal{M}$
						\IF{$\bar{s}_{i} \neq \bar{s}^{new}_{i}$}
							\STATE Put $(\bar{s}_{i-1},a_{i},r_{i},\bar{s}_{i})$,  $(\bar{s}_{i},a_{i+1},r_{i+1},\bar{s}_{i+1})$ on $\mathcal{Q}^-$
							\STATE Put $(\bar{s}_{i-1},a_{i},r_{i},\bar{s}^{new}_{i})$,  $(\bar{s}^{new}_{i},a_{i+1},r_{i+1},\bar{s}_{i+1})$ on $\mathcal{Q}^+$
							\STATE Update $\bar{s}_{i} \leftarrow \bar{s}^{new}_{i}$ in $\mathcal{M}$
						\ENDIF
					\ENDFOR
				\ENDFOR
				\STATE Perform a gradient descent step according to $\mathcal{G}$ with given optimizer
			\ENDIF
	  	\ENDWHILE
	\ENDFOR
   \end{algorithmic}
"
27,1802.04325,"
    \caption{Prioritized Sweeping Process.}\label{alg:ps}
	Initialize $V(s) = U(s) = 0$ for all s \\
	Initialize $Q(s,a) = 0$ for all s, a \\
	Initialize $N_{sa}, N^{s'}_{sa} = 0$ for all $s$, $a$, $s'$ \\
	Initialize priority queue $\mathcal{P}$ with minimum priority cutoff $p_{min}$\\
	Initialize add queue $\mathcal{Q}^+$ and delete queue $\mathcal{Q}^-$
   \begin{algorithmic}[1]
   \WHILE{True}
		\WHILE{$\mathcal{Q}^+$, $\mathcal{Q}^-$ empty}
			\STATE Remove top state $s'$ from $\mathcal{P}$
			\STATE $\Delta U \leftarrow V(s') - U(s')$
			\STATE $U(s') \leftarrow V(s')$
			\FOR{$\mathbf{all}$ $(s,a)$ pairs with $N^{s'}_{sa} > 0$}
				\STATE $Q(s,a) \leftarrow Q(s,a) + \gamma N^{s'}_{sa}/N_{sa}\cdot \Delta U$
				\STATE $V(s) \leftarrow \max_b\{Q(s, b)|N_{sb}>0\}$
				\STATE add/update $s$ in $\mathcal{P}$ with priority $| U(s) - V(s) |$ if $| U(s) - V(s) | > p_{min}$
			\ENDFOR
		\ENDWHILE
		\FOR{$(s,a,r,s')$ in $\mathcal{Q}^+$}
			\STATE $N_{sa} \leftarrow N_{sa} + 1$; $N^{s'}_{sa} \leftarrow N^{s'}_{sa} + 1$
			\STATE $Q(s,a) \leftarrow [Q(s,a)(N_{sa} - 1) + r + \gamma U(s')]/N_{sa}$
			\STATE $V(s) \leftarrow \max_b\{Q(s, b)|N_{sb}>0\}$
			\STATE add/update $s$ in $\mathcal{P}$ with priority $| U(s) - V(s) |$ if $| U(s) - V(s) | > p_{min}$
		\ENDFOR
		\FOR{$(s,a,r,s')$ in $\mathcal{Q}^-$}
			\STATE $N_{sa} \leftarrow N_{sa} - 1$; $N^{s'}_{sa} \leftarrow N^{s'}_{sa} - 1$
			\IF{$N_{sa} > 0$}
				\STATE $Q(s,a) \leftarrow [Q(s,a)(N_{sa} + 1) - (r + \gamma U(s'))]/N_{sa}$
			\ELSE
				\STATE $Q(s,a) \leftarrow 0$
			\ENDIF
			\IF{$\sum_bN_{sb} > 0$}
				\STATE $V(s) \leftarrow \max_b\{Q(s, b)|N_{sb}>0\}$
			\ELSE
				\STATE $V(s) \leftarrow 0$
			\ENDIF
			\STATE add/update $s$ in $\mathcal{P}$ with priority $| U(s) - V(s) |$ if $| U(s) - V(s) | > p_{min}$
		\ENDFOR
	\ENDWHILE
   \end{algorithmic}
"
28,1712.03333,"[t!]
\caption{ADFQ algorithm}
\begin{algorithmic}[1]
\small
\State Initialize randomly $\mu_{s,a}$, $\sigma_{s,a}$ $\forall s \in \mathcal{S}$ and $\forall a \in \mathcal{A}$
\For{each episode}
\State Initialize $s_0$
\For{each time step $t$}
\State Choose an action, $a_t \sim$ $\pi^{action}(s_t;\theta_t)$ 
\State Perform the action and observe $r_t$ and $s_{t+1}$
\For{each $b \in \mathcal{A}$}
\State Compute $\mu^*_{\tau,b}$, $\sigma^*_{\tau,b}$, $k^*_{\tau,b}$ using Eq.\ref{eq:peak}-\ref{eq:magnitude}
\EndFor
\State Update $\mu_{s_t, a_t}$ and $\sigma_{s_t,a_t}$ using Eq.\ref{eq:approx_v1}
\EndFor
\EndFor
\end{algorithmic} \label{table:adfq}
"
29,1704.03581,"[Partially Collapsed Gibbs Sampler]
\label{PC-LDA}
Repeat until convergence:
\begin{itemize}
\item Sample $\v{\phi}_k \dist[Dir](\v{n}_k + \v{\beta})$ in parallel for $k=1,..,K$.
\item Sample $z_{i,d} \propto \phi_{k,v(i)}\,\alpha_k + \phi_{k,v(i)}\, m_{d,k}^{-i}$ in parallel for $d=1,..,D$.
\end{itemize}
"
30,1704.03581,"[P\'{o}lya Urn Partially Collapsed Gibbs Sampler] \label{polya-urn-LDA}
Repeat until convergence:
\begin{itemize}
\item Sample $\v{\phi}_k \dist[PPU](\v{n}_k + \v{\beta})$ in parallel for $k=1,..,K$.
\item Sample $z_{i,d} \propto \phi_{k,v(i)}\,\alpha_k + \phi_{k,v(i)}\, m_{d,k}^{-i}$ in parallel for $d=1,..,D$.
\end{itemize}
"
31,1704.03581,"[Collapsed Gibbs Sampler]\label{t-gibbs}
Repeat until convergence:
\begin{itemize}
\item Sample $z_1 \given z_2 \dist[T][\rho z_2, (0.8 + 0.2z_2^2)(1 - \rho^2), 5]$
\item Sample $z_2 \given z_1 \dist[T][\rho z_1, (0.8 + 0.2z_1^2)(1 - \rho^2), 5]$
\end{itemize}
"
32,1704.03581,"[Uncollapsed Gibbs Sampler]\label{t-ig-gibbs}
Repeat until convergence:
\begin{itemize}
\item Sample $\Phi \dist[IG] [3, 0.5 \v{z}^T \m{\Sigma}^{-1} \v{z} + 2]$
\item Sample $\v{z} \dist[N]_2 [\v{0}, \Phi \m{\Sigma}]$
\end{itemize}
"
33,1806.03674,"[ht]
\SetKwInOut{Output}{output}
\caption{Statistical sampling by $(1,\lambda)$-selection}
$t \leftarrow 0$\;
$\mathcal{S} \leftarrow$ $\emptyset$\;
\Repeat{$t \geq N_{\texttt{iter}}$} {
 \For{$k\leftarrow 1$ \KwTo $\lambda$}{
 $\vec{x}^{(t+1)}_{k} \leftarrow \vec{x}_{0} + \vec{z}_k,~~~\vec{z}_k \sim \mathcal{N} ( \vec{0},\mathbf{I} )$\;
 $J^{(t+1)}_{k}\leftarrow$ \texttt{evaluate} $\left(\vec{x}^{(t+1)}_{k}\right)$\;
 } 
 $m_{t+1} \leftarrow\arg\min \left(\left\{J^{(t+1)}_{\imath}\right\}_{\imath=1}^{\lambda} \right)$\;
 $\mathcal{S} \leftarrow \mathcal{S} \cup \left\{ \vec{x}^{(t+1)}_{m_{t+1}} \right\} $\;
 $t \leftarrow t+1$\;
 }
%\BlankLine
\Output{$\mathcal{C}^{\texttt{stat}}=$\texttt{statCovariance}$\left(\mathcal{S}\right)$}
\label{algo:ES_sampling}
"
34,1802.07191,"%[H]
\vspace{0.02in}
\begin{algorithmic}[1]
\REQUIRE $\Gcal=(\Lcal,\Ecal)$,
$\Lcal$ is topologically sorted in $S$.
% \STATE $S \leftarrow$ initialise queue with $\{\opnode\}$.
\STATE $\distoprw(\opnode) = 0$, $\distoprw(u) = \textrm{\inlabelfont{nan}}\;\,
  \forall u\neq \opnode$.
% \STATE ${\rm pop\_last}(S)$
\WHILE{$S$ is not empty}
\STATE $u\leftarrow {\rm pop\_last}(S)$
\STATE $\Delta \leftarrow \{\distoprw(c): c \in {\rm children}(u)\}$ \label{line:child}
\STATE $\distoprw(u) \leftarrow 1 + {\rm average}(\Delta)$
\ENDWHILE
\STATE \textbf{Return} $\distoprw$.
\end{algorithmic}
\caption{$\;$Compute $\distoprw(u)$ for all $u\in\Lcal$ \label{alg:rwpl}}
"
35,1711.06288,"
       \caption{Stochastic Inference of the Fusion Network}
       \label{alg:inference}
       \begin{algorithmic}
       \Require $V\in\mathbb R^{D\times (M\times N)}$: Spatial feature map of image.
       \Require $U\in\mathbb R^{K\times L}$: Language feature map of expression. 
       \Ensure Fusion feature map $O\in\mathbb R^{D\times (M\times N)}$. 
       \Function {Fusion}{$V,U$}
       	  \State Initialize $S^0=V$. 
           \ForAll  {$t=0$ to $t_{max}-1$}
           	\State $\hat U^t = \textbf{Attention}(U,S^t;\theta_a) $ 
 			\State $S^{t+1},\hat{O}^{t} =\textbf{C-GRUs}(S^t,\hat U^t;\theta_c) $
%              \ForAll {$i=1$ to $M\times N$}
%            	  \If {$\tau_{i}=0$}
%                  \State Sample ${\tau}_{i}\sim p(\cdot|f_{tg} (s_{i}^t;\theta_{tg})) $
%                  \If {$\tau_{i}=1$}
%                    \State Set $o_i=\hat{o}_i^t $
%                  \EndIf
%                \EndIf
%              \EndFor          
           \State Sample $\pmb{\tau}^{t+1}\sim p(\cdot|f_{tg} (S^{t+1};\theta_{tg}))$ 
           \If {$\tau_{i}^{t+1}=1$ and $\tau_{i}^s=0$ for $s\leq t$}
             \State Set $O_{i}=\hat O_{i}^{t+1}$.
           \EndIf
           \EndFor  
          
           \ForAll {$i=1$ to $M\times N$}
% %          	\If {$\tau_{i}=0$ for $s\leq T-1$}
           	\If {$\tau_{i}=0$}
             	\State Set $o_{i}=\hat{o}_{i}^{t_{max}-1} $
             \EndIf
 		  \EndFor	
 	  \EndFunction
       \end{algorithmic}
 "
36,1803.00942,"
\caption{Deep Learning with Importance Sampling} \label{alg:training}
\begin{algorithmic}[1]
    \STATE Inputs $B, b, \tau_{th}, a_{\tau}, \theta_0$
    \STATE $t \gets 1$
    \STATE $\tau \gets 0$
    \REPEAT
        \IF{$\tau > \tau_{th}$}
            \STATE $\mathcal{U} \gets B$ uniformly sampled datapoints
            \STATE $g_i \propto \hat{G_i} \quad \forall i \in \mathcal{U}$ 
                according to eq~\ref{eq:upper_bound}
            \STATE $\mathcal{G} \gets b$ datapoints sampled with $g_i$
                from $\mathcal{U}$
            \STATE $w_i \gets \frac{1}{B g_i} \quad \forall i
                \in \mathcal{G}$
            \STATE $\theta_t \gets \text{sgd\_step}(w_i,
                \mathcal{G}, \theta_{t-1})$
        \ELSE
            \STATE $\mathcal{U} \gets b$ uniformly sampled datapoints
            \STATE $w_i \gets 1 \quad \forall i \in \mathcal{U}$
            \STATE $\theta_t \gets \text{sgd\_step}(w_i,
                \mathcal{U}, \theta_{t-1})$
            \STATE $g_i \propto \hat{G_i} \quad \forall i \in \mathcal{U}$
                \label{alg:line:uniform_gi}
        \ENDIF
        \STATE $\tau \gets a_{\tau} \tau + (1-a_{\tau}) \left(1 - \frac{1}{\sum_{i} g_i^2} \norm{g - \frac{1}{|\mathcal{U}|}}_2^2\right)^{-1}$
    \UNTIL{convergence}
\end{algorithmic}
"
37,1802.02664,"[htb!]
   \caption{The algorithm to compute RLT of a dataset. See \cref{sec:stochastic} for details. Suggested default values of the parameters for a dataset $X \in \mathbb{R}^{N \times D}$ are $L_0 = 64$, $\gamma = \frac{1}{128} / \frac{N}{5000}$, $i_{\max}=100$, $n=10000$.}
   \label{alg:mrlt}
\begin{algorithmic}
   \STATE {\bfseries Require:} $X$: $2D$ array representing the dataset
   \STATE {\bfseries Require:} $L_0$: Number of landmarks to use
   \STATE {\bfseries Require:} $\gamma$: Coefficient determining $\alpha_{\max}$
   \STATE {\bfseries Require:} $i_{\max}$: Upper bound on $i$ in $\RLT(i, 1, X, L)$
   \STATE {\bfseries Require:} $n$: Number of experiments
   \STATE {\bfseries Require:} \texttt{dist}$(A,\ B)$: Function computing the matrix of pairwise (Euclidean) distances between samples from $A$ and $B$
   \STATE {\bfseries Require:} \texttt{witness}$(d,\ \alpha,\ k)$: Function computing the family of witness complexes using the matrix of pairwise distances $d$, maximal value of persistence parameter $\alpha$ and maximal dimension of simplices $k$
   \STATE {\bfseries Require:} \texttt{persistence}$(w, k)$: Function computing the persistence intervals of a family $w$ in dimension $k$
   \STATE {\bfseries Returns:} An array of size $n \times i_{\max}$ of the obtained RLT for each experiment
   \STATE {\bfseries Initialize:} $\mathrm{rlt} = \texttt{zeros}(n,\ i_{\max})$
   \FOR{$i=0$ {\bfseries to} $n-1$}
   \STATE $L^{(i)} \leftarrow$ \texttt{random\_choice}$(X,\ \texttt{size}\negmedspace=\negmedspace L_0)$
   \STATE $d^{(i)} \leftarrow$  \texttt{dist}$(L^{(i)},\ X)$ 
   \STATE $\alpha^{(i)}_{\max} \leftarrow$ $\gamma \cdot  \texttt{max}(\texttt{dist}(L^{(i)},\ L^{(i)}))$
   \STATE $W^{(i)} \leftarrow \texttt{witness}(d^{(i)},\ \alpha^{(i)}_{\max},\ 2)$
   \STATE $\mathcal{I}^{(i)} \leftarrow \texttt{persistence}(W^{(i)},\ 1)$
   \FOR{$j=0$ {\bfseries to} $i_{\max} - 1$} 
   \STATE Compute $\RLT(j, 1, X, L^{(i)})$ using \cref{eq:rlt-def,eq:beta-def}
   \STATE $\mathrm{rlt}[i,\ j] \leftarrow \RLT(j, 1, X, L^{(i)})$
   \ENDFOR
   \ENDFOR
\end{algorithmic}
"
38,1802.02664,"[htb!]
   \caption{\emph{Geometry Score}, the proposed algorithm to compute topological similarity between datasets}
   \label{alg:top-score}
\begin{algorithmic}
   \STATE {\bfseries Require:} $X_1, X_2$: arrays representing the datasets
   \STATE {\bfseries Returns:} $s$: a number representing the topological similarity of $X_1$ and $X_2$
   \STATE {\bfseries Initialize:} $s=0$
	\STATE For $X_1$ and $X_2$ run \cref{alg:mrlt} with the same collection of parameters, obtaining arrays $\mathrm{rlt}_1$ and $\mathrm{rlt}_2$
    \STATE $\mathrm{mrlt}_1 \leftarrow \texttt{mean}(\mathrm{rlt}_1,\ \texttt{axis}\negmedspace=\negmedspace 0)$
    \STATE $\mathrm{mrlt}_2 \leftarrow \texttt{mean}(\mathrm{rlt}_2,\ \texttt{axis}\negmedspace=\negmedspace 0)$
    \STATE $s \leftarrow \texttt{sum} ((\mathrm{mrlt}_1 - \mathrm{mrlt}_2)^2)$
\end{algorithmic}
"
39,1806.03467,"
\begin{algorithmic}
  \STATE{\textbf{Input}: data set $S$ of size $s$}
  \STATE{\textbf{Initialize}: For each $z_i\in S$, set weight
    $a_{ib} = 0$ for all $b\in[B]$}
  \INDSTATE{Let $S^1$, $S^2$, and $S^3$ denote the first,
    second, and third $s/3$ parts of the input data set $S$}

    \INDSTATE{Let $K$ be the kernel function computed by $\cL_k$
      using data set $S^1$}

    \INDSTATE{Let $H = \{z \in S^2\mid K(x, x_i, S^2) > 0\}$ be
      the set of points with non-zero weight}

    \INDSTATE{Compute nuisance estimator $\hat h$ using the data set
      $H$}
    
    \STATE{\textbf{Output}:
    $\sum_{z_i \in S^3} K(x, x_i, S^3) \psi(z_i; \theta, \hat h)$}
   
  \caption{Base function $f_0$, parameterized by a target point $x$,
    an estimate $\theta$, and a base kernel learner
    $\cL_k$} \label{alg:basefunction}

\end{algorithmic}
"
40,1806.03467,"[h]
\begin{algorithmic}[1]
  \STATE{\textbf{Input}: a dataset $D$ of size $n$, target point $x$,
    kernel base learner $\cL_k$, parameter $B$: number of kernels,
    parameter $s$ subsample size} 

  \STATE{Randomly split the data set $D$ into two data sets of equal
    size $D_1$ and $D_2$}
  \STATE{\textbf{First stage: nuisance estimation.}}
  \STATE{Compute nuisance estimate $\hat h$ by performing a kernel
    nuisance estimation using dataset $D_1$}

    \STATE{\textbf{Second stage: moment estimation with $\hat h$}.}

    \STATE{For each $Z_i\in D_2$, set weight $a_{ib} = 0$ for all
      $b\in[B]$}

    \STATE{\textbf{For} $b = 1 ,\ldots , B$: }

    \INDSTATE{Let $S_{b}$ be a set of $s$ observations randomly
      subsampled without replacement from $D_2$}
    % ; Randomly partition
    % $S_{b}$ into two data sets of even size: $S_{b}^1$ and
    % $S_{b}^2$

    \INDSTATE{Run tree learner on $S_b$ and compute tree weight on
      each observation $i\in D_2$.}
    \STATE{\textbf{For} each $Z_i\in D_2$, let $a_i = \frac{1}{B}\sum_{b=1}^B a_{ib}$}
  \STATE{Compute $\hat \theta$ by solving the weighted moment
    condition in \Cref{weighted_moment}.
    }
    \caption{Orthogonal Random Forest}
  \label{alg:mainalg}
\end{algorithmic}
"
41,1806.03467,"[htpb]
  \label{alg:stage-two}
\begin{algorithmic}
  \STATE{\textbf{Input}: data set $S$, target point $x$, Kernel
    function $\hat{K}(x, x_i, S)$ using an auxiliary sample $S'$, such
    that $\sum_{i=1}^n \hat{K}(x, x_i, S)=1$. }


 \STATE Using a samples $S''$
  estimate nuisance function $\hat{h}$ \STATE For each data target
  $x$, estimate $\theta(x)$ by solving the local empirical moment:
\begin{equation}
\sum_{i\in S} \hat{K}(x, x_i, S_{-i})\cdot \psi(Z_i; \theta, \hat{h})=0
\end{equation}
\end{algorithmic}
\caption{Stage 2: estimation via orthogonal machine learning}
\label{alg:general}
"
42,1806.03467,"[h]
\begin{algorithmic}[1]
  \STATE{\textbf{Input}: a dataset $D$ of size $n$, target point $x$,
    kernel base learner $\cL_k$, parameter $B$: number of kernels,
    parameter $s$ subsample size} 

  \STATE{Randomly split the data set $D$ into two data sets of equal
    size $D_1$ and $D_2$}
  \STATE{\textbf{First stage: nuisance estimation.}}
  \STATE{Compute nuisance estimate $\hat h$ by performing a kernel
    nuisance estimation using dataset $D_1$}

    \STATE{\textbf{Second stage: moment estimation with $\hat h$}.}

    \STATE{For each $Z_i\in D_2$, set weight $a_{ib} = 0$ for all
      $b\in[B]$}

    \STATE{\textbf{For} $b = 1 ,\ldots , B$: }

    \INDSTATE{Let $S_{b}$ be a set of $s$ observations randomly
      subsampled without replacement from $D_2$}
    % ; Randomly partition
    % $S_{b}$ into two data sets of even size: $S_{b}^1$ and
    % $S_{b}^2$

    \INDSTATE{Run tree learner on $S_b$ and compute tree weight on
      each observation $i\in D_2$.}
    \STATE{\textbf{For} each $Z_i\in D_2$, let $a_i = \frac{1}{B}\sum_{b=1}^B a_{ib}$}
  \STATE{Compute $\hat \theta$ by solving the weighted moment
    condition in \Cref{weighted_moment}.
    }
    \caption{Orthogonal Random Forest}
  \label{alg:mainalg}
\end{algorithmic}
"
43,1806.03461,"[H]
  \caption{Comparator}
  {\bf Inputs:}~~Encrypted $\mathbb{B}[\tilde{S}]$, unencrypted $\mathbb{B}[(d-b)/2]$, size $d$ of $\mathbb{B}[(d-b)/2]$,$\mathbb{B}[\tilde{S}]$ \\
  {\bf Output:}~~Result of $2\tilde{S} \geq d - b$
  \begin{algorithmic}[1]
  \label{alg:compare}
  	\STATE $o = 0$
  	\FOR{$i = 1,\ldots,d$}
    	\IF{$\mathbb{B}[(d-b)/2]_i = 0$}
    		\STATE $o = \textsc{MUX}(\mathbb{B}[\tilde{S}]_i, \tilde{1}, o)$
    	\ELSE
    		\STATE $o = \textsc{MUX}(\mathbb{B}[\tilde{S}]_i, o, \tilde{0})$
    	\ENDIF
    \ENDFOR
    \STATE {\bf Return:} $o$
  \end{algorithmic}
"
44,1805.10209,"[H]
\caption{SESTRA: \textbf{S}ingl\textbf{e}-\textbf{st}ep \textbf{R}eward Observ\textbf{a}tion.}\label{alg:learning}
\begin{algorithmic}[1]
\footnotesize
\Require Training data $\{(\instruction^\dataindex_\turnindex, \state_{\turnindex, 1}^\dataindex, \langle \instruction^\dataindex_1, \dots, \instruction^\dataindex_{\turnindex-1} \rangle, $ $\goalstate^\dataindex_\turnindex) \}_{j=1,\turnindex=1}^{N,\interactionlength^\dataindex}$, learning rate $\mu$, entropy regularization coefficient   $\lambda$, episode limit horizon $M$.
\Definitions $\policy_\theta$ is a policy parameterized by $\theta$, $\act{BEG}$ is a special action to use for the first decoder step, and $\act{STOP}$ indicates end of an execution. $\transition(\state, \action)$ is the state transition function, $H$ is an entropy function, $R^\dataindex_\turnindex(\state, \action,\state')$ is the reward function for example $j$ and instruction $\turnindex$, and $\textsc{RMSProp}$ divides each weight by a running average of its squared gradient~\cite{Tieleman:12}.
\Ensure Parameters $\theta$ defining a learned policy $\policy_\theta$.
\For{$t = 1, \dots, T, j = 1, \dots, N$}\label{algline:epoch}
	\For{$i = 1, \dots, n^\dataindex$}
    \State $\execution \leftarrow \langle ~ \rangle, \outputindex \gets 0, \action_0 \gets \mathtt{BEG}$ 
	\State \Comment{Rollout up to $\act{STOP}$ or episode limit.}
    \While{$\action_{\outputindex} \neq \mathtt{STOP} \land \outputindex < M$}\label{algline:rollout}
        \State $\outputindex \leftarrow \outputindex + 1$    	
    	\State $\acontext_\outputindex \gets  (\instruction_\turnindex, \langle \instruction_1, \dots, \instruction_{\turnindex-1}\rangle, \state_\outputindex, \execution[:\outputindex])$
		\State \Comment{Sample an action from policy.}
        \State $\action_\outputindex \sim \policy_\theta (\acontext_\outputindex, \cdot)$
        \State $\state_{\outputindex+1} \leftarrow \transition(\state_\outputindex, \action_\outputindex)$
        \State $\execution \leftarrow [ \execution ;  \langle (\state_\outputindex, \action_\outputindex)\rangle ]$\label{algline:rolloutend}
    \EndWhile
    \State $\Delta \gets \bar{0}$
    \For{$k' = 1,\dots,k$}
%
        \State \Comment{Compute the entropy of $\pi_\theta(\acontext_{k'}, \cdot)$.}
        \State $\Delta \gets \Delta + \lambda \nabla_\theta  H(\policy_\theta(\acontext_{k'}, \cdot))$ \label{algline:entropy}
        \For{$\action \in \actions$}
        	\State $\state' \gets \transition(\state_{k'}, \action)$
			\State \Comment{Compute gradient for action $\action$.}
            \State $\Delta \leftarrow \Delta + R^\dataindex_\turnindex(\state_{k'}, \action, \state')\nabla_\theta \policy_\theta(\acontext_{k'}, \action)$ \label{algline:action}
        \EndFor
    \EndFor
    \State $\theta \leftarrow \theta + \mu \textsc{RMSProp}\left(\dfrac{\Delta}{\outputindex}\right)$ \label{algline:update}
	\EndFor
\EndFor
\State \Return $\theta$
\end{algorithmic}
\label{alg:learn} 
"
45,1802.02550,"[tb]

   \caption{Semi-Amortized Variational Autoencoders}
   \label{alg:savi}
\begin{algorithmic}
   \STATE {\bfseries Input:} inference network $\phi$, generative model $\theta$, \\ 
   \hspace{10mm} inference steps $K$, learning rate $\alpha$, momentum $\gamma$,   \\
   \hspace{10mm} loss function $f(\lambda, \theta, \xvec) = -\ELBO(\lambda, \theta, \xvec)$

   \STATE Sample $\xvec^{} \sim p_\mathcal{D}(\xvec)$
   \STATE $\lambda_0 \gets \enc(\xvec^{} \param \phi)$ 
   \STATE $v_0 \gets 0$
   \FOR{$k=0$ {\bfseries to} $K-1$} 
   \STATE $v_{k+1} \gets \gamma v_{k} - \nabla_{\lambda}f(\lambda_k, \theta, \xvec^{})$
   \STATE $\lambda_{k+1} \gets \lambda_k + \alpha v_{k+1}$
   \ENDFOR
   \STATE  $\mcL \gets f(\lambda_K, \theta, \xvec^{})$ 
   \STATE $\overline{\lambda}_K \gets \nabla_{\lambda}f(\lambda_K, \theta, \xvec^{}) $
   \STATE $\overline{\theta} \gets \nabla_{\theta}f(\lambda_K, \theta, \xvec^{})$
   \STATE $\overline{v}_K \gets 0$
   \FOR{$k=K-1$ {\bfseries to} $0$}
   \STATE $\overline{v}_{k+1} \gets \overline{v}_{k+1} + \alpha \overline{\lambda}_{k+1}$
   \STATE $\overline{\lambda}_k \gets \overline{\lambda}_{k+1} - \Hess_{\lambda, \lambda}f(\lambda_k, \theta, \xvec^{}) \overline{v}_{k+1} $
   \STATE $\overline{\theta} \gets \overline{\theta} - \Hess_{\theta,\lambda}f(\lambda_k, \theta, \xvec^{}) \overline{v}_{k+1}$
   \STATE $\overline{v}_{k} \gets \gamma \overline{v}_{k+1}$
   \ENDFOR
   \STATE $\frac{\diff \mcL}{\diff \theta} \gets \overline{\theta} $   
   \STATE $\frac{\diff \mcL}{\diff \phi} \gets \frac{\diff \lambda_0}{\diff \phi}\overline{\lambda}_0 $ 
   \STATE Update $\theta, \phi$ based on $\frac{\diff \mcL}{\diff \theta}, \frac{\diff \mcL}{\diff \phi}$
\end{algorithmic}
"
46,1802.02550,"[tb]
   \caption{Semi-Amortized Variational Autoencoders with Gradient Clipping}
   \label{alg:savi2}
\begin{algorithmic}
   \STATE {\bfseries Input:} inference network $\phi$, generative model $\theta$, \\ 
   \hspace{10mm} inference steps $K$, learning rate $\alpha$, momentum $\gamma$,   \\
   \hspace{10mm} loss function $f(\lambda, \theta, \xvec) = -\ELBO(\lambda, \theta, \xvec)$, \\
   \hspace{10mm} gradient clipping parameter $\eta$

   \STATE Sample $\xvec^{} \sim p_\mathcal{D}(\xvec)$
   \STATE $\lambda_0 \gets \enc(\xvec^{} \param \phi)$ 
   \STATE $v_0 \gets 0$
   \FOR{$k=0$ {\bfseries to} $K-1$} 
   \STATE $v_{k+1} \gets \gamma v_{k} - \clip(\nabla_{\lambda}f(\lambda_k, \theta, \xvec^{}), \eta)$
   \STATE $\lambda_{k+1} \gets \lambda_k + \alpha v_{k+1}$
   \ENDFOR
   \STATE  $\mcL \gets f(\lambda_K, \theta, \xvec^{})$ 
   \STATE $\overline{\lambda}_K \gets \nabla_{\lambda}f(\lambda_K, \theta, \xvec^{}) $
   \STATE $\overline{\theta} \gets \nabla_{\theta}f(\lambda_K, \theta, \xvec^{}) $ 
   \STATE $\overline{v}_K \gets 0$
   \FOR{$k=K-1$ {\bfseries to} $0$}
   \STATE $\overline{v}_{k+1} \gets \overline{v}_{k+1} + \alpha \overline{\lambda}_{k+1}$
   \STATE $\overline{\lambda}_k \gets \overline{\lambda}_{k+1} - \Hess_{\lambda, \lambda}f(\lambda_k, \theta, \xvec^{}) \overline{v}_{k+1} $
   \STATE $\overline{\lambda}_k \gets \clip(\overline{\lambda}_k , \eta)$
   \STATE $\overline{\theta} \gets \overline{\theta} - \clip(\Hess_{\theta,\lambda}f(\lambda_k, \theta, \xvec^{}) \overline{v}_{k+1}, \eta)$
    \STATE $\overline{v}_{k} \gets \gamma \overline{v}_{k+1}$
   \ENDFOR
   \STATE $\frac{\diff \mcL}{\diff \theta} \gets \overline{\theta} $   
   \STATE $\frac{\diff \mcL}{\diff \phi} \gets \frac{\diff \lambda_0}{\diff \phi} \overline{\lambda}_0 $ 
   \STATE Update $\theta, \phi$ based on $\frac{\diff \mcL}{\diff \theta}, \frac{\diff \mcL}{\diff \phi}$
 
\end{algorithmic}
"
47,1801.09624,"[!t]
  \caption{Hallucinated DAgger-MC (+ reward learning)}\label{alg:HDAggerMC}
  \begin{algorithmic}[1]
    \REQUIRE \textsc{Learn-Dynamics}{}, \textsc{Learn-Reward}, exploration distr. $\nu$,
    \textsc{MC-Planner}(blind rollout policy $\rho$, depth
    $T$), \# iterations $N$, \# rollouts per iteration $K$. 
    \STATE Get initial datasets $\mathcal{D}^{1:T-1}_1$ and $\mathcal{E}_1$ (maybe using $\nu$)
    \STATE Initialize $\hat{P}^{1:T-1}_1 \gets$ \textsc{Learn-Dynamics}($\mathcal{D}^{1:T-1}_1$).
    \STATE Initialize $\hat{R}_1 \gets$ \textsc{Learn-Reward}($\mathcal{E}_1$).
    \STATE Initialize $\hat{\pi}_1 \gets$ \textsc{MC-Planner}($\hat{P}^{1:T-1}_1$, $\hat{R}_1$).
    \FOR{$n \gets 2 \ldots N$}
    \FOR{$k \gets 1 \ldots K$}
    \STATE With probability... \COMMENT{First sample from $\xi$}\label{line:samplestart}
    \STATE \hspace{1em} $\nicefrac{1}{2}$: Sample $(x, b) \sim D_{\mu}^{\hat{\pi}_n}$
    \STATE \hspace{1em} $\nicefrac{1}{4}$: Reset to $(x, b) \sim \nu$.
    \STATE \hspace{1em} $\nicefrac{(1-\gamma)}{4}$: Sample $x \sim \mu$, $b \sim
     \hat{\pi}_n(\cdot \mid x)$. 
     \STATE \hspace{1em} $\nicefrac{\gamma}{4}$: Reset to $(y, c)
     \sim \nu$
     \STATE \hspace{1em}\hspace{1em}\hspace{0.1in}
     Sample $x \sim P(\cdot \mid y, c)$, $b \sim \hat{\pi}_n(\cdot \mid x)$\label{line:sampleend}
    \STATE Let $s \gets x$, $z \gets x$, $a \gets b$.
    \FOR[Parallel rollouts...]{$t \leftarrow 1\ldots T-1$} \label{line:startproll}
    \STATE Sample $s' \sim P(\cdot \mid s, a)$.
    \STATE Add $\langle z, a, s' \rangle$ to
    $\mathcal{D}^t_n$. \\
    \hfill \COMMENT{(DAgger-MC adds $\langle s, a, s' \rangle$)}
    \STATE Add $\langle z, a, R_s^a, \gamma^{t-1} \rangle$ to
    $\mathcal{E}_n$.\\
    \hfill \COMMENT{(Standard approach adds $\langle s,
    a, R_s^a, \gamma^{t-1} \rangle$)} 
    \STATE Sample $z' \sim \hat{P}^t_{n-1}(\cdot \mid z, a)$.
    \STATE Let $s \gets s'$, $z \gets z'$, and sample $a \sim \rho$.\label{line:endproll}
    \ENDFOR
    \STATE Add $\langle z, a, R_s^a, \gamma^{T-1} \rangle$ to
    $\mathcal{E}_n$.\\
    \hfill \COMMENT{(Standard approach adds $\langle s,
    a, R_s^a, \gamma^{T-1} \rangle$)} 
    \ENDFOR
    \STATE $\hat{P}^{1:T-1}_n \gets$ \textsc{Learn-Dynamics}($\hat{P}^{1:T-1}_{n-1}$, $\mathcal{D}^{1:T-1}_n$) 
    \STATE $\hat{R}_n \gets$ \textsc{Learn-Reward}($\hat{R}_{n-1}$, $\mathcal{E}_n$) 
    \STATE $\hat{\pi}_n \gets$ \textsc{MC-Planner}($\hat{P}^{1:T-1}_{n}$, $\hat{R}_n$).
    \ENDFOR
    \STATE \textbf{return} the sequence $\hat{\pi}_{1:N}$
  \end{algorithmic}
"
48,1806.03335,"[h!]
{\small
\caption{Randomized prior functions for ensemble posterior.}
\label{alg:randomized_prior}
\begin{algorithmic}[1]
{\medmuskip=0mu
\thinmuskip=0mu
\thickmuskip=1mu
  \Require{Data $\Dc \subseteq \{(x, y) | x \in \Xc, y \in \Yc\}$, loss function $\Lc$, neural model $f_\theta : \Xc \rightarrow \Yc$, \newline Ensemble size $K \in \Nat$, noise procedure \texttt{data\_noise}, distribution over priors $\Pc \subseteq \{ \Prob(p) \mid p : \Xc \rightarrow \Yc\}$.}}
  \For{$k = 1,..,K$}
    \State initialize $\theta_k \sim$ Glorot initialization \cite{glorot2010understanding}.
    \State form $\Dc_k = \mathtt{data\_noise}(\Dc)$ (e.g. Gaussian noise or bootstrap sampling \cite{osband2015bootstrapped}).
    \State {sample prior function $p_k \sim \Pc$.}
    \State optimize $ \nabla_{\theta \mid \theta=\theta_k} \Lc(f_{\theta} + p_k; \Dc_k)$ via ADAM \cite{kingma2014adam}.
  \EndFor
  \State {\bf return} ensemble $\{ f_{\theta_k} + p_k \}_{k=1}^K$.

\end{algorithmic}
}
"
49,1806.03335,"[!ht]
\caption{$\mathtt{learn\_bootstrapped\_dqn\_with\_prior}$}
\label{alg:learn_ensemble_rlsvi}

{\small
\begin{tabular}{lll}
\textbf{Agent:} & $\theta_1, .., \theta_K$ & trainable network parameters \\
& $p_1, .., p_K$ & fixed prior functions \\
& $\Lc_\gamma(\theta \ass ; \theta^- \ass, p \ass, \Dc \ass)$ & TD error loss function \\
& $\mathtt{ensemble\_buffer}$ & replay buffer of $K$-parallel perturbed data \\
\textbf{Updates:} & $\theta_1, .., \theta_K$ & agent value function estimate
\end{tabular}

\begin{algorithmic}[1]
\For{$k$ in $(1,\ldots,K)$}
\State Data $\Dc_k \leftarrow \mathtt{ensemble\_buffer[k].sample\_minibatch()}$

\State optimize $\nabla_{\theta \mid \theta=\theta_k} \Lc(\theta; \theta_k, p_k, \Dc_k)$ via ADAM \cite{kingma2014adam}.

\EndFor
% \State update $\theta^{\rm active} \leftarrow \theta_{j}$ for $j \sim {\rm Unif}({1,..,K})$ \Comment{Sample active ensemble member.}
\end{algorithmic}
}
"
50,1806.03335,"[!htpb]
\caption{$\mathtt{live}$}
\label{alg:live}
\begin{tabular}{lll}
\textbf{Input:} & \texttt{agent} & methods $\mathtt{act}, \mathtt{update\_buffer}, \mathtt{learn\_from\_buffer}$ \\
 % &  & value function family $\Qc_\theta \ (\forall \theta \in \Theta, \ \Qc_\theta : \Sc \times \Ac \rightarrow \Real)$ \\
                & \texttt{environment} & methods $\mathtt{reset}, \mathtt{step}$ \\
                % & \texttt{transition} & \texttt{NamedTuple(old\_state, new\_state, action, reward, is\_terminal, timestep)} \\
\end{tabular}
\begin{algorithmic}[1]
\For{episode $= 1,2,\ldots$}
\State \texttt{agent.learn\_from\_buffer()}
\State \texttt{transition} $\leftarrow$ \texttt{environment.reset()}
\While{\texttt{transition.new\_state} is not {\bf null}}
\State \texttt{action} $\leftarrow$ \texttt{agent.act(transition.new\_state)}
\State \texttt{transition} $\leftarrow$ \texttt{environment.step(action)}
\State \texttt{agent.update\_buffer(transition)}
\EndWhile
\EndFor
\end{algorithmic}
"
51,1806.03335,"[!htpb]
\caption{$\mathtt{ensemble\_buffer.update\_bootstrap}(\cdot)$}
\label{alg:ensemble_update_bootstrap}

\begin{tabular}{lll}
\textbf{Input:} & $\mathtt{transition}$ & $(s_t, a_t, r_t, s'_t, t)$ \\
\textbf{Updates:} & $\mathtt{ensemble\_buffer}$ & replay buffer of $K$-parallel perturbed data
\end{tabular}

\begin{algorithmic}[1]
\For{$k$ in $(1,\ldots,K)$}
\If{$m^k_t \sim {\rm Unif}(\{0, 1\}) = 1$}
\State $\mathtt{ensemble\_buffer[k].enqueue(}(s_t, a_t,  r_t, s'_t, t))$
\EndIf
\EndFor
\end{algorithmic}
"
52,1803.10161,"[h!]
\caption{Nelder-Mead}
\label{alg: nm}
\begin{algorithmic}[1]
\INPUT $f$, $t$, $n_{\mathrm{init}}$, $n_{\mathrm{delay}}$, $\mu_0$, $\Sigma_0$, $\{x^{\mathrm{curr}}_j\}_{j=1}^{n_{\mathrm{curr}}}$, $\lambda$, $l$, $u$
\OUTPUT $x^*$
\FUNCTION{NM}
\FOR{$i \gets 1:n_{\mathrm{init}}$}
\IF{$t \le n_{\mathrm{delay}}$}
\STATE $x^{\mathrm{init}}_{i} \leftlsquigarrow \mathrm{trunc}_{l}^{u}\left[\mathcal{N}(\mu_0, \Sigma_0)\right]$
\ELSE
\STATE $x^{\mathrm{init}}_{i} \leftlsquigarrow \mathrm{trunc}_{l}^{u}\left[\Pi(\{x^{\mathrm{curr}}_j\}_{j=1}^{n_{\mathrm{curr}}}, \lambda)\right]$
\ENDIF
\STATE $x^{\mathrm{local}}_{i} \gets \mathrm{NelderMead}_{x}\left[f(x), x^{\mathrm{init}}_{i}, l, u\right]$
\ENDFOR
\STATE $i^* \gets \argmin_{i \in\{1 \ldots n_{\mathrm{init}}\}} f(x^{\mathrm{local}}_{i})$
\STATE $x^* \gets x^{\mathrm{local}}_{i^*}$
\ENDFUNCTION
\end{algorithmic}
"
53,1803.10161,"[h!]
\caption{Monte Carlo}
\label{alg: mc}
\begin{algorithmic}[1]
\INPUT $f$, $t$, $n_{\mathrm{test}}$, $n_{\mathrm{delay}}$, $\mu_0$, $\Sigma_0$, $\{x^{\mathrm{curr}}_j\}_{j=1}^{n_{\mathrm{curr}}}$, $\lambda$, $l$, $u$
\OUTPUT $x^*$
\FUNCTION{MC}
\IF{$t \le n_{\mathrm{delay}}$}
\STATE $\{x^{\mathrm{test}}_{i}\}_{i=1}^{n_{\mathrm{test}}} \leftlsquigarrow \mathrm{trunc}_{l}^{u}\left[\mathcal{N}(\mu_0, \Sigma_0)\right]$
\ELSE
\STATE $\{x^{\mathrm{test}}_{i}\}_{i=1}^{n_{\mathrm{test}}} \leftlsquigarrow \mathrm{trunc}_{l}^{u}\left[\Pi(\{x^{\mathrm{curr}}_j\}_{j=1}^{n_{\mathrm{curr}}}, \lambda)\right]$
\ENDIF
\STATE $i^* \gets \argmin_{i \in\{1 \ldots n_{\mathrm{test}}\}} f(x^{\mathrm{test}}_{i})$
\STATE $x^* \gets x^{\mathrm{test}}_{i^*}$
\ENDFUNCTION
\end{algorithmic}
"
54,1803.10161,"[h!]
\caption{Grid Search}
\label{alg: gs}
\begin{algorithmic}[1]
\INPUT $f$, $t$, $l$, $u$, $n_0$
\OUTPUT $x^*$
\FUNCTION{GS}
\STATE $n_{\mathrm{grid}} \gets n_0 + \mathrm{Round}(\sqrt{t})$
\STATE $\delta_{\mathrm{grid}} \gets (u - l) / (n_{\mathrm{grid}} - 1)$
\STATE $X_{\mathrm{grid}} \gets \{l, l + \delta_{\mathrm{grid}}, \ldots, u\}^d$
\STATE $x^* \gets \argmin_{x \in X_{\mathrm{grid}}} f(x)$
\ENDFUNCTION
\end{algorithmic}
"
55,1806.03281,"[htb]
  \caption{Fair model training with private sensitive values using Lagrangian multipliers for~$\F(\btheta) = \nicefrac{1}{n} |\Z^{\top} X| - \const$.}
  {\bf Parties:}~~$\dc$, $\reg$.\\
  {\bf Input:}~~($\dc$) $\mat{\share{Z}{1}} \in \mathbb{Z}_q^{n\times p}$\\
  {\bf Input:}~~($\reg$) $\mat{X} \in \mathbb{Z}_q^{n\times d}$, $\vec{y} \in \mathbb{Z}_q^n$, $\mat{\share{Z}{2}} \in \mathbb{Z}_q^{n\times p}$\\
  {\bf Input:}~~(Public) Learning rates $\eta_{\btheta}, \eta_{\blambda}$, number of training examples~$n$, minibatch size $2^s$, constraints $\vec{c}\in \mathbb{Z}_q^p$, number of epochs $N_e$.

  \begin{algorithmic}[1]
    \STATE $\btheta \gets \vec{0}$, $\blambda \gets \vec{0}$

    \STATE $\A \gets \textsc{BlockedMultShiftAvg}(\Z^{\top}, \X)$

    \FORALL{$j$ from $1$ to $N_e$}

    \FORALL{$i$ from $1$ to $\nicefrac{n}{2^s}$}

    \STATE $(\X_{i},\y_{i}) \gets \textsc{SampleMinibatch}(\X, \y)$

    \STATE $\F \gets |\A \btheta| - c$

    \STATE $\nabla_{\blambda} \gets \max\{\F, \vec{0} \}$

    \STATE $\sigma \gets \textsc{SigmoidApprox}(\X_i \btheta)$

    \STATE $\nabla_{\btheta}^{\mathrm{BCE}} \gets \textsc{ShiftDivide}(\X_i^{\top}(\sigma - \y_i), 2^s)$

    \STATE $\nabla_{\btheta}^{\mathrm{CON}} \gets
      \begin{cases}
      \A^{\top} \blambda, &\text{if } \A > \vec{0} \wedge \F > 0 \\
      - \A^{\top} \blambda, &\text{if } \A < \vec{0} \wedge \F > 0\\
      \vec{0}, &\text{if } \F \le 0
      \end{cases}$

    \STATE $\btheta \gets \btheta - \eta_{\btheta} (\xi^{\mathrm{BCE}}_j \nabla_{\btheta}^{\mathrm{BCE}} + \xi^{\mathrm{CON}}_j \nabla_{\btheta}^{\mathrm{CON}})$

    \STATE $\blambda \gets \max\{\blambda + \eta_{\blambda} \nabla_{\blambda}, \vec{0}\}$
    \ENDFOR
    \ENDFOR
  \end{algorithmic}

  {\bf Output:}~~Parameters~$\btheta$
  \label{algo:lagrange}
"
56,1704.03718,"[tb]
	\caption{DXML: Training Algorithm}\label{train_algo_proposed}
	\begin{algorithmic}[1]
		\REQUIRE $\mathcal{D}=\left\lbrace \left( \bm{x}_{1},\bm{y}_{1} \right),  \ldots, ( \bm{x}_{n},\bm{y}_{n} ) \right\rbrace $, Embedding dimensionality: $\ell$, No. of clusters: $m$, No. of iteration: $T$
		\STATE Build the label graph, in which there exist an edge between labels if the two labels ever co-exist with each other in any sample;
		\STATE Use DeepWalk~\cite{perozzi2014deepwalk} transfer label graph to embedded label matrix $\bm{V}$;
		\STATE Project original label matrix $Y$ to
		$$f_{Y} = \left\{ f_{\bm{y}}\ \big|\ f_{\bm{y}} = \frac{1}{nnz (\bm{y})} \bm{V} \bm{y},\ \bm{y}\in Y \right\};$$
		\STATE Train the deep neural network shown in Figure \ref{fig:framework} to obtain the mapping from original feature vector set $X$ to embedded feature vector set $f_{X} = \left\{ f_{\bm{x}} \right\}$. Update $\bm{W}$ for $T$ epochs to
		\begin{displaymath}
		\min \mathcal{L}(\bm{W}) = \sum_{i=1}^{n}d (f_{\bm{x}_i},f_{\bm{y}_i});
		\end{displaymath}
		\STATE Partition $f_{X}$ into $Z^{1}, \ldots, Z^{m}$.
		\ENSURE $\left\lbrace Z^{1}, \ldots, Z^{m} \right\rbrace $
	\end{algorithmic}
	\normalsize
"
57,1704.03718,"[tb]
	\caption{DXML: Test Algorithm}\label{test_algo_proposed}
	\begin{algorithmic}[1]
		\REQUIRE Test point $\bm{x}$, No. of $k$-NN: $k$, No. of desired labels: $p$
		\STATE $\bar{\bm{x}} \leftarrow f(\bm{x})$
		\STATE $Z^{i^{\star}}$: partition closest to $\bar{\bm{x}}$;
		\STATE $K_{\bar{\bm{x}}} \leftarrow k$ nearest neighbors of $\bar{\bm{x}}$ in $Z^{i^{\star}}$;
		\STATE $P_{\bm{x}} \leftarrow$ empirical label dist, for points $\in K_{\bar{\bm{x}}}$;
		\STATE $\bm{y}_{pred} \leftarrow Top_{p} (P_{\bm{x}})$
		\ENSURE $\bm{y}_{pred}$
		
	\end{algorithmic}
	\normalsize
"
58,1806.03085,"[H]
\caption{One iteration of the Stein variational gradient algorithm}\label{alg_stein}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Particles $\{x_i^l\}_{i=1}^n$ at previous iteration $l$; step size $\varepsilon_{l+1}$}
\Output{Particles $\{x_i^{l+1}\}_{i=1}^n$ at new iteration $l+1$}
\begin{algorithmic}[1]
\FOR{$i=1,2,\ldots, n$}
\STATE Set $x_i^{l+1} \gets x_i^l + \varepsilon_{l+1}\, G(x_i^{l})$, where $G$ is defined in \eqref{eq:MCgradJ}.
\ENDFOR
\end{algorithmic}
"
59,1806.03085,"[H]
\caption{One iteration of the Stein variational Newton algorithm}\label{alg_stein_newton}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Particles $\{x_i^{l}\}_{i=1}^n$ at stage $l$; step size $\varepsilon$}
\Output{Particles $\{x_i^{l+1}\}_{i=1}^n$ at stage $l+1$}
\begin{algorithmic}[1]
\FOR{$i=1,2,\ldots,n$}
\STATE Solve the linear system \eqref{eq:semi-comp} for $\alpha^{1}, \ldots, \alpha^{n}$ 
\STATE Set $x_i^{l+1} \gets x_i^{l} + \varepsilon W(x_i^{l})$ given $\alpha^{1}, \ldots, \alpha^{n}$ 
\ENDFOR
\end{algorithmic}
"
60,1709.04347,"
%	\caption{\\Stacked Zoom Out-and-In Net with Recursive Training}
%	
%	\begin{algorithmic}[1]
%		\State \textbf{Input:} Image $\mathcal{I}$, Ground-truth boxes $\mathcal{G}$ 
%		\Statex~~~~~~~~~~~~Pre-learned Anchor Candidates $\mathcal{A}^m$
%		\State \textbf{Output:} A set of region proposals $\mathcal{P}_1, \mathcal{P}_2$
%		\medskip
%		\State Initiate weights of zoom net $\textbf{W}_{zoi}$ 
%		\State Initiate weights of RoI-followup net $\textbf{W}_{RoI}$
%		\smallskip
%		\Statex \small{\texttt{// prepare inputs on multiple GPUs}} \normalsize
%		%\smallskip
%		\State Resize image $\mathcal{I}_{re} = {\texttt{dynamic\_scale}}(\mathcal{I}, \mathcal{A}^m, \mathcal{G})$
%		\State $t^{m*}_1 = {\texttt{compute\_target}} ( %\mathcal{I}_{re},  % deleted for brevity
%		\mathcal{A}^m, \mathcal{G})$
%		\State $\mathcal{B}^m_1 = {\texttt{prepare\_blob}} (\mathcal{I}_{re}, t^{m*}_1, \mathcal{G})$ 
%		\Statex  \Comment{consider gray cls and neg sample ratio}
%		\smallskip
%		\Statex \small{\texttt{// forward and back\_prop}} \normalsize
%		%\smallskip
%		\State Train model $(p^m, t^m) = \texttt{forward}(\mathcal{B}^m_1, \textbf{W}_{zoi})$
%		\State Compute loss $L$ based on Enq.(\ref{loss}) and (\ref{total_loss})
%		\State Merge outputs $\mathcal{P}_1 = \texttt{post\_process}(m, p, t)$
%		\Statex  \Comment{ multi-scale test and NMS}
%		\smallskip
%		\Statex \small{\texttt{// recursive training}} \normalsize
%		\State Init RoI regions $\mathcal{R}_0 = \mathcal{P}_1$
%		\For  {$q=0$ to $T_{tr}$} 
%		\State $t^{m*}_2 = {\texttt{compute\_target}} (\mathcal{R}_q, \mathcal{G})$
%		\Statex  \Comment{each level $m$ has the same RoI inputs}
%		\State $\mathcal{B}^m_2 = {\texttt{prepare\_roi\_blob}} (\mathcal{I}_{re}, t^{m*}_2, \mathcal{R}^m_q, \mathcal{G})$ 
%		\State Get $(p^m, t^m) = \texttt{forward}(\mathcal{B}^m_2, \textbf{W}_{zoi}, \textbf{W}_{RoI})$
%		\State Compute loss $L$ based on Enq.(\ref{loss}) and (\ref{total_loss_roi})
%		\State Outputs $\mathcal{R}_{q+1} = \texttt{post\_process\_roi}(m, p, t) $
%		\EndFor
%		\State Get final proposals $\mathcal{P}_2 = \mathcal{R}_{T_{tr}+1}$
%		%		\While{ ( linearlyIndependentServerIndex.length!=K ) } 
%		%		\Statex\Comment{\%comment: remove all the server index which were not inserted in Z\%}  
%		%		\State temp[]=serverIndex[]-linearlyIndependentServerIndex 
%		%		\If{  (linearlyIndependentServerIndex.length=K) }
%		%		\State break
%		%		\EndIf  
%		%		\EndWhile  
%	\end{algorithmic}
%	\label{algorithm}
%"
61,1709.04347,"
%	\caption{Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection}	
%	\begin{algorithmic}[1]
%		\State \textbf{Input:} Image $\mathcal{I}$, Ground-truth boxes $\mathcal{G}$ 
%		\Statex~~~~~~~~~~~~Pre-learned Anchor Candidates $\mathcal{A}^m$
%		\State \textbf{Output:} A set of region proposals $\mathcal{P}_1, \mathcal{P}_2$
%		\medskip
%		\State Initiate weights of zoom net $\textbf{W}_{zoi}$ 
%		\State Initiate weights of RoI-followup net $\textbf{W}_{RoI}$
%		\smallskip
%		\Statex \small{\texttt{// prepare inputs on multiple GPUs}} \normalsize
%		%\smallskip
%		\State Resize image $\mathcal{I}_{re} = {\texttt{dynamic\_scale}}(\mathcal{I}, \mathcal{A}^m, \mathcal{G})$
%		\State $t^{m*}_1 = {\texttt{compute\_target}} ( %\mathcal{I}_{re},  % deleted for brevity
%		\mathcal{A}^m, \mathcal{G})$
%		\State $\mathcal{B}^m_1 = {\texttt{prepare\_blob}} (\mathcal{I}_{re}, t^{m*}_1, \mathcal{G})$ 
%		\Statex  \Comment{consider gray cls and neg sample ratio}
%		\smallskip
%		\Statex \small{\texttt{// forward and back\_prop}} \normalsize
%		%\smallskip
%		\State Train model $(p^m, t^m) = \texttt{forward}(\mathcal{B}^m_1, \textbf{W}_{zoi})$
%		\State Compute loss $L$ based on Enq.(\ref{loss}) and (\ref{total_loss})
%		\State Merge outputs $\mathcal{P}_1 = \texttt{post\_process}(m, p, t)$
%		\Statex  \Comment{ multi-scale test and NMS}
%		\smallskip
%		\Statex \small{\texttt{// recursive training}} \normalsize
%		\State Init RoI regions $\mathcal{R}_0 = \mathcal{P}_1$
%		\For  {$q=0$ to $T_{tr}$} 
%		\State $t^{m*}_2 = {\texttt{compute\_target}} (\mathcal{R}_q, \mathcal{G})$
%		\Statex  \Comment{each level $m$ has the same RoI inputs}
%		\State $\mathcal{B}^m_2 = {\texttt{prepare\_roi\_blob}} (\mathcal{I}_{re}, t^{m*}_2, \mathcal{R}^m_q, \mathcal{G})$ 
%		\State Get $(p^m, t^m) = \texttt{forward}(\mathcal{B}^m_2, \textbf{W}_{zoi}, \textbf{W}_{RoI})$
%		\State Compute loss $L$ based on Enq.(\ref{loss}) and (\ref{total_loss_roi})
%		\State Outputs $\mathcal{R}_{q+1} = \texttt{post\_process\_roi}(m, p, t) $
%		\EndFor
%		\State Get final proposals $\mathcal{P}_2 = \mathcal{R}_{T_{tr}+1}$
%		%		\While{ ( linearlyIndependentServerIndex.length!=K ) } 
%		%		\Statex\Comment{\%comment: remove all the server index which were not inserted in Z\%}  
%		%		\State temp[]=serverIndex[]-linearlyIndependentServerIndex 
%		%		\If{  (linearlyIndependentServerIndex.length=K) }
%		%		\State break
%		%		\EndIf  
%		%		\EndWhile  
%	\end{algorithmic}
%	\label{algorithm}
%"
62,1711.01244,"[H] 
	\caption{MLAP algorithm, meta-training phase (learning-to-learn)}
	\label{algorithm:MetaTrain}
	\begin{algorithmic}
	\STATE {\bfseries Input:} {Data sets  of observed tasks: $S_1,...,S_n$.}
	\STATE {\bfseries Output:} {Learned prior parameters $\theta$.}
	\STATE {\bfseries Initialize:}\\
		\STATE  $\theta=\pth{\mu_P,\rho_P}\in \mathbb{R}^d \times\mathbb{R}^d $.
		\STATE  $\phi_i=\pth{\mu_i,\rho_i}\in \mathbb{R}^d \times\mathbb{R}^d, \quad \mathrm{for} \quad  i=1,...,n $.
	\WHILE {\textit{not done} }
		\FOR{each task  $i \in \braces{1,..n} \footnotemark$}
			\STATE Sample a random mini-batch from the data $S'_i \subset S_i$. \\
			\STATE Approximate  $J_i(\theta, \phi_i)$ (\ref{eq:TaskObj}) using $S'_i$ and averaging Monte-Carlo draws.\\  
		\ENDFOR 
		\STATE $J \leftarrow \frac{1}{n}\sum_{i \in \braces{1,..n}} J_i(\theta, \phi_i)  + \Upsilon(\theta)$.\\
		\STATE Evaluate the gradient of $J$ w.r.t $\braces{\theta, \phi_1,...,\phi_n}$ using backpropagation.\\
		\STATE Take an optimization step.
	\ENDWHILE
	\end{algorithmic}
"
63,1711.01244,"[H]
	\caption{MLAP algorithm, meta-testing phase (learning a new task).}
	\label{algorithm:MetaTest}
	\begin{algorithmic}
	\STATE {\bfseries Input:} {Data set of a new task, $S$, and prior parameters, $\theta$.}
	\STATE {\bfseries Output:} {Posterior parameters $\phi'$ which solve the new task.}
	\STATE {\bfseries Initialize:}\\
		\STATE  $\phi' \leftarrow \theta$.
	\WHILE {\textit{not done}}
		\STATE Sample a random mini-batch from the data $S' \subset S$. \\
		\STATE Approximate the empirical loss $J$ (\ref{eq:TaskObj}) using $S'$ and averaging Monte-Carlo draws.	\\
		\STATE Evaluate the gradient of $J$ w.r.t $\phi'$ using backpropagation.\\
		\STATE Take an optimization step.
	\ENDWHILE	
	\end{algorithmic}
"
64,1804.07193,"
\caption{GVI algorithm}
\begin{algorithmic}
   \STATE {\bfseries Input:} initial $\widehat Q(s,a)$, $\delta$, and choose an operator $f$
   \REPEAT
   \FOR{each $s,a\in \mathcal{S}\times \mathcal{A}$}
   \STATE $\widehat Q(s,a)\! \leftarrow\! R(s,a)\!+\!\gamma\!\int\widehat T(s'\mid s,a) f\big(\widehat Q(s',\cdot)\big)ds'$ 
   \ENDFOR
   \UNTIL{ \textrm{convergence}}
\end{algorithmic}
\label{GVI}
"
65,1802.06403,"[t!]
	\caption{Pseudo-code of RadialGAN}
	\label{alg:pseudo}
	\begin{algorithmic}
		\STATE {\bf Initialize: }$\theta_G^1, ..., \theta_G^M, \theta_F^1, ..., \theta_F^M, \theta_D^1, ..., \theta_D^M$ 
		\WHILE{training loss has not converged}
		\STATE \textbf{(1) Update $\mathbf{D}$ with fixed $\mathbf{G}$, $\mathbf{F}$}
		\FOR{$i = 1, ..., M$}
		\STATE Draw $k_D$ samples from $\mathcal{D}_i$, $\{(x^{(i)}_k, y^i_k)\}_{k = 1}^{k_D}$
		\STATE Draw $k_D$ samples from $\bigcup_{j \neq i} \mathcal{D}_j$, $\{(x^{(j_k)}_k, y_k)\}_{k=1}^{k_D}$
		\FOR{$k = 1, ..., k_D$}
		\STATE
		\begin{equation*}
		(\hat{x}^{(i)}_k, \hat{y}_k) \gets G_i(F_{j_k}(x^{(j_k)}_k, y_k))
		\end{equation*}
		\ENDFOR
		\STATE Update $\theta_D^i$ using stochastic gradient descent(SGD)
		\begin{align*}
		\triangledown_{\theta_D^i} - \biggl(\sum_{k = 1}^{k_D}& \log D_i(x^{(i)}_k, y^i_k)\\
		&+ \sum_{k = 1}^{k_D} \log(1 - D_i(\hat{x}^{(i)}_k, \hat{y}_k))\biggr)
		\end{align*}
		\ENDFOR
		\STATE \textbf{(2) Update $\mathbf{G}$, $\mathbf{F}$ with fixed $\mathbf{D}$}
		\FOR{$i = 1, ..., M$}
		\STATE Draw $k_G$ samples from $\mathcal{D}_i$, $\{(x^{(i)}_k, y^i_k)\}_{k = 1}^{k_G}$
		\STATE Draw $k_G$ samples from $\bigcup_{j \neq i} \mathcal{D}_j$, $\{(x^{(j_k)}_k, y_k)\}_{k=1}^{k_G}$
		\ENDFOR
		\STATE Update $\theta_{\mathbf{G, F}} = (\theta_G^1, ..., \theta_G^M, \theta_F^1, ..., \theta_F^M)$ using SGD
		\begin{align*}
		&\triangledown_{\theta_{\mathbf{G, F}}} \sum_{i = 1}^M \sum_{k = 1}^{k_G} \log (1 - D_i(G_i(F_{j_k}(x^{(j_k)}_k, y_k)))) +\\
		& \lambda \sum_{i = 1}^M \sum_{k = 1}^{k_G} ||(x^{(i)}_k, y_k) - G_i(F_i(x^{(i)}_k, y_k))||_2+ \\
		& \lambda \sum_{i = 1}^M \sum_{k = 1}^{k_G} ||F_{j_k}(x^{(j_k)}_k, y_k) - F_i(G_i(F_{j_k}(x^{(j_k)}_k, y_k)))||_2
		\end{align*}
		\ENDWHILE
	\end{algorithmic}
"
66,1708.03105,"[H]
    \scriptsize
    \begin{algorithmic}[1]
        \Procedure{Compute-Model}{$Gazetteer$}

            \For{$ln \in Gazetteer$}
                \State $unigrams \leftarrow tokenize(ln)$;
                \State $bigrams, trigrams \leftarrow$ generate from $unigrams$;
            \EndFor

            \For{$n\text{-}grams \in [bigrams, trigrams]$}
                \State \textit{\textbf{CFD}} $\leftarrow$ create using $n\text{-}grams$;
                \State \textit{\textbf{CPD}} $\leftarrow$ create using \textit{\textbf{CFD}};
            \EndFor

        \EndProcedure
        \Statex
        \Procedure{Valid-N-Gram}{$string = s$}: boolean

          \State $w^{n}_1 = (w_1, \dots, w_n) \leftarrow tokenize(s)$;

          %\State calculate $P(w_1, \dots, w_N)$ using equation (\ref{equ:1});

          \Return $P(w^{n}_1) > 0$ \Comment{calculated using the equations (\ref{equ:1}-\ref{equ:n})};

        \EndProcedure
    \end{algorithmic}
    \caption{Language Model Generation}
    \label{algo:lm}
"
67,1708.03105,"
%     \scriptsize
%     \caption{LNEx Method}
%     \begin{algorithmic}[1]
%         \Procedure{LNEx}{$tweet$, $LM$}
%
%           \State preprocess tweet, filter out stop words, and split tweet into sub-tweets
%           \State spot locations in each tweet-split
%           \State filter the spotted locations
%
%           \Return locations
%
%         \EndProcedure
%     \end{algorithmic}
%     \label{algo:LNEx}
% "
68,1806.02817,"[h]
    \caption{Model-Agnostic Meta-Learning~\cite{finn2017model}}
    \small
    \label{alg:maml}
    \begin{algorithmic}[1]
    \Require $p(\task)$: distribution over tasks
    \Require $\alpha$: step size hyperparameter
    \State randomly initialize $\theta$
    \While{not done}
    \State Sample batch of tasks $\task_i \sim p(\task)$
      \ForAll{$\task_i$}
    \State $\metatrain, \metaval = \task_i$
     \State Evaluate $\nabla_\theta \lossi(\theta, \metatrain)$
     \State Compute adapted parameters with gradient descent: $\phi_i=\theta-\alpha \nabla_\theta  \lossi( \theta, \metatrain)$
     \EndFor
     \State Compute $ \nabla_\theta \sum_{\task_i}  \lossi (\phi_i, \metaval)$
     \State Update $\theta$ using Adam
    \EndWhile
    %\STATE while 
    \end{algorithmic}
"
69,1806.02817,"[h]
    \caption{ProMAML, Meta-training; differences from MAML shown in red}
    \small
    \label{alg:promaml}
    \begin{algorithmic}[1]
    \Require $p(\task)$: distribution over tasks
    %\Require $\alpha$, $\beta$: step size hyperparameters
    %\State randomly initialize $\priormean$ \diff{(denoted $\theta$ in MAML algorithm)}
    \State \diff{initialize $\Theta := \{\priormean, \priorvar, \qvar, \pmult, \qmult\}$}
    \While{not done}
    \State Sample batch of tasks $\task_i \sim p(\task)$
      \ForAll{$\task_i$}
     \State $\metatrain, \metaval = \task_i$
     \State \diff{Evaluate $\nabla_{\priormean} \lossi(\priormean, \metaval)$}
     %
     \State \diff{Sample $\theta \sim q = \gauss(\priormean -  \qmult \nabla_{\priormean} \lossi(\priormean,  \metaval), \qvar)$}
     %
     \State Evaluate $\nabla_\theta \lossi(\theta, \metatrain)$
     %
     \State Compute adapted parameters with gradient descent: $\phi_i =\theta-\alpha \nabla_\theta \lossi(\theta, \metatrain)$
     \EndFor
     %\State Compute $\nabla_\theta$, $\nabla$
     \State \diff{Let $p(\theta | \metatrain) = \gauss(\priormean - \pmult \nabla_{\priormean} \lossi(\priormean,\metatrain), \priorvar))$}
     \State Compute $\nabla_\Theta \left( \sum_{\task_i}  \lossi ( \phi_i, \metaval) \diff{+ \kl(q(\theta | \metaval) \mid\mid p(\theta | \metatrain))  } \right)$
     \State Update $\Theta$ using Adam
    \EndWhile
    %\STATE while 
    \end{algorithmic}
"
70,1806.02817,"[h]
    \caption{PLATIPUS, Meta-testing}
    \small
    \label{alg:promamltest}
    \begin{algorithmic}[1]
    \Require training data $\metatrain_\task$ new task $\task$
    \Require \diff{learned $\Theta$}
    \State \diff{Sample $\theta$ from the prior $p(\theta | \metatrain)$}
     \State Evaluate $\nabla_\theta \loss(\\theta, \metatrain)$
     \State Compute adapted parameters with gradient descent: $\phi_i=\theta-\alpha \nabla_\theta  \loss(  \theta, \metatrain)$
     %%CF: The above is missing the update to the variance and other free parameters
    %\STATE while 
    \end{algorithmic}
"
71,1806.02817,"[H]
    \caption{Meta-training, differences from MAML in red}
    \small
    \label{alg:promaml}
    \begin{algorithmic}[1]
    \Require $p(\task)$: distribution over tasks
    \State \diff{initialize $\Theta := \{\priormean, \priorvar, \qvar, \pmult, \qmult\}$}
    \While{not done}
    \State Sample batch of tasks $\task_i \sim p(\task)$
      \ForAll{$\task_i$}
     \State $\metatrain, \metaval = \task_i$
     \State \diff{Evaluate $\nabla_{\priormean} \loss(\priormean, \metaval)$}
     %
     \State \diff{Sample $\theta \sim q = \gauss(\priormean -  \qmult \nabla_{\priormean} \loss(\priormean,  \metaval), \qvar)$}
     %
     \State Evaluate $\nabla_\theta \loss(\theta, \metatrain)$
     %
     \State \begin{varwidth}[t]{\linewidth}
     Compute adapted parameters with gradient descent: \\
     $\phi_i =\theta-\alpha \nabla_\theta \loss(\theta, \metatrain)$
     \end{varwidth}
     \EndFor
     %\State Compute $\nabla_\theta$, $\nabla$
     \State \diff{Let $p(\theta | \metatrain) = \gauss(\priormean - \pmult \nabla_{\priormean} \loss(\priormean,\metatrain), \priorvar))$}
     \State \begin{varwidth}[t]{\linewidth} Compute $\nabla_\Theta \big( \sum_{\task_i}  \loss ( \phi_i, \metaval)$\\ 
     $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\diff{+ \kl(q(\theta | \metaval) \mid\mid p(\theta | \metatrain) )  } \big)$
     \end{varwidth}
     \State Update $\Theta$ using Adam
    \EndWhile
    %\STATE while 
    \end{algorithmic}
"
72,1806.02817,"[H]
    \caption{Meta-testing}
    \small
    \label{alg:promamltest}
    \begin{algorithmic}[1]
    \Require training data $\metatrain_\task$ for new task $\task$
    \Require \diff{learned $\Theta$}
    \State \diff{Sample $\theta$ from the prior $p(\theta | \metatrain)$}
     \State Evaluate $\nabla_\theta \loss(\theta, \metatrain)$
     \State \begin{varwidth}[t]{\linewidth} Compute adapted parameters with gradient descent: \\ $\phi_i=\theta-\alpha \nabla_\theta  \loss(  \theta, \metatrain)$
     \end{varwidth}
    \end{algorithmic}
"
73,1805.10367,"[H]
  \caption*{\textbf{Algorithm\,1}: $\text{SVRG}(T,m,\{\eta_k \}, b,  \tilde {\mathbf x}_0)$}
  \begin{algorithmic}[1]
     \State \textbf{Input}:  total number of iterations $T$, epoch length $m$,  number of epochs $S = \ceil{T/m}$,  step sizes $\{ \eta_k \}_{k=0}^{m-1}$, mini-batch  $b$,  and  $ \tilde {\mathbf x}_0$.
      \For{$s =  1,2,\ldots, S$}
      \State set  $ {\mathbf g}_s =  \nabla f(\tilde{\mathbf x}_{s-1})$,    $\mathbf x_0^{s} = \tilde {\mathbf x}_{s-1}$,
       \For{$k=  0,1,\ldots, m-1$}
 %  \State uniformly randomly pick $i \in [n]$,
  \State choose mini-batch ${\mathcal I}_{k}$ of size $b$,
   \State compute gradient blending via \eqref{eq: SVRG_update}:
    \hspace*{0.39in}
$
\mathbf v_{k}^{s} = {\nabla} f_{\mathcal I_k} ( {\mathbf x}_{k}^{s}) - {\nabla} f_{\mathcal I_k} ( {\mathbf x}_{0}^{s}) +  {\mathbf g}_s $,
\State update $\mathbf x_{k+1}^{s} = \mathbf  x_{k}^{s} - \eta_{k} \mathbf v_{k}^{s}$,
   \EndFor
    \State set $\tilde{\mathbf x}_{s} = \mathbf x_{m}^{s}$,
      \EndFor
       \State \textbf{return}  $\bar{\mathbf x}$ chosen uniformly random from $\{ \{ {\mathbf x}_k^s \}_{k=0}^{m-1} \}_{s=1}^S$.
  \end{algorithmic}
"
74,1805.10367,"[H]
  \caption*{\textbf{Algorithm\,2}: $\text{ZO-SVRG}(T,m,\{\eta_k \}, b,  \tilde {\mathbf x}_0, \mu)$}
  \begin{algorithmic}[1]
   \State \textbf{Input}:  In addition to parameters in SVRG, set smoothing parameter $\mu>0$.
   \For{$s =  1,2,\ldots, S$}
   \State \textit{compute ZO estimate $\hat {\mathbf g}_s = \hat \nabla f(\tilde{\mathbf x}_{s-1})$,} % via
  % \hspace*{0.18in}
   %\ref{eq: grad_rand} or  \ref{eq: grad_e},
   \State set $\mathbf x_0^{s} = \tilde {\mathbf x}_{s-1}$,
   \For{$k=  0,1,\ldots, m-1$}
   \State choose mini-batch ${\mathcal I}_{k}$ of size $b$,
   %a  mini-batch  ${\mathcal I}_{k} \in [n]$ \hspace*{0.36in} of   size $b$ (uniformly random with \hspace*{0.39in} replacement),
   \State \textit{compute ZO gradient blending \eqref{eq: ZOSVRG_update}:}
    \hspace*{0.39in}
$
\hat {\mathbf v}_{k}^{s} = \hat{\nabla} f_{{\mathcal I}_{k}} ( {\mathbf x}_{k}^{s}) - \hat{\nabla} f_{{\mathcal I}_{k}} ( {\mathbf x}_{0}^{s}) + \hat {\mathbf g}_s $,
\State update $\mathbf x_{k+1}^{s} = \mathbf  x_{k}^{s} - \eta_{k} \hat {\mathbf v}_{k}^{s}$,
   \EndFor
   \State set $\tilde{\mathbf x}_{s} = \mathbf x_{m}^{s}$,
   \EndFor
      \State \textbf{return}  
      $\bar{\mathbf x}$ chosen uniformly random from $\{ \{ {\mathbf x}_k^s \}_{k=0}^{m-1} \}_{s=1}^S$.
  \end{algorithmic}
"
75,1806.02724,"  
  \caption{State-factored search
    \label{alg:supp_state_factored}}
  \begin{algorithmic}[5]  
    \Class{State}
        \State $location$ \Comment{the agent's physical position in the environment}
        \State $completed$ \Comment{whether the route has been completed (the \textsc{Stop} action has been taken)}
    \EndClass \\
    \Class{Route}
        \State $states$ \Comment{list of \textsc{State}s in the route}
        \State $score$ \Comment{route probability under the follower model, $P_F$}
        \State $expanded$ \Comment{whether this route has been expanded in search}
    \EndClass \\
      %\LineComment{\textsc{State}s contain the physical location in the world, as well as whether the route has been completed (the \textsc{Stop} action has been taken)}
      %\LineComment{\textsc{Route}s store sequences of states, the route score (probability), and whether the route has been expanded}
    \Function{StateFactoredSearch}{$start\_state: \textsc{State}$, $K: int$}  
      %\Let{$z$}{$x \oplus y$} \Comment{$\oplus$: bitwise exclusive-or}  
      % \State $state\_scores \gets \{start\_state: 1.0\}$
      \LineComment{a mapping from \textsc{State}s to the best \textsc{Route} ending in that state found so far (whether expanded or unexpanded)}
      \State $partial = \{\}$
      \
      \LineComment{a similar mapping, but containing routes that have been completed}
      \State $completed = \{\}$
      \State $start\_route = \textsc{Route}([start\_state], 1.0, False)$
      \State $partial[start\_state] = start\_route$
      \State $candidates = [start\_route]$
      \While{$|completed| < K \textbf{ and } |candidates| > 0$}
        \LineComment{choose the highest-scoring unexpanded route to expand (route may be complete)}
        \State $route = argmax_{r \in candidates } r.score$
        \State $route.expanded = True$
        \LineComment{\textsc{Successor} generates \textsc{Route}s by taking all possible actions, each of which extends this route by one state, with the resulting total model $score$, and $expanded$ set to False}
        \For{$route' \textbf{ in } \textsc{Successors}(route)$} 
          \State $state' = route'.states.last$
          \State $cache = completed \textbf{ if } route'.completed \textbf{ else } partial$
          \If{$state' \textbf{ not in } cache.keys \textbf{ or } cache[state'].score < route'.score$}
            \State $cache[state'] = route'$
          \EndIf
        \EndFor
        \State $candidates = [route \textbf{ in } partial.values \textbf{ if not } route.expanded ]$
      \EndWhile
      \State \Return{$completed$}
    \EndFunction  
  \end{algorithmic}  
"
76,1805.06530,"[t]
\DontPrintSemicolon
\caption{Analytic Gaussian Mechanism}\label{alg:newGM}
\KwSty{Public Inputs:} $f$, $\Delta$, $\varepsilon$, $\delta$\;
\KwSty{Private Inputs:} $x$\;
%\BlankLine
Let $\delta_0 = \Phi(0) - e^{\varepsilon} \Phi(- \sqrt{2 \varepsilon})$\;
\eIf{$\delta \geq \delta_0$}{
Define $B_{\varepsilon}^+(v) = \Phi(\sqrt{\varepsilon v})
- e^{\varepsilon} \Phi(-\sqrt{\varepsilon (v + 2)})$\;
Compute $v^* = \sup \{ v \in \R_{\geq 0} : B_{\varepsilon}^+(v) \leq \delta \}$\;
Let $\alpha = \sqrt{1+ v^*/2} - \sqrt{v^*/2}$\;
}{
Define $B_{\varepsilon}^-(u) = \Phi(-\sqrt{\varepsilon u})
- e^{\varepsilon} \Phi(-\sqrt{\varepsilon (u + 2)})$\;
Compute $u^* = \inf \{ u \in \R_{\geq 0} : B_{\varepsilon}^-(u) \leq \delta \}$\;
Let $\alpha = \sqrt{1 + u^*/2} + \sqrt{u^*/2}$\;
}
Let $\sigma = \alpha \Delta / \sqrt{2 \varepsilon}$\;
Return $f(x) + \cN(0,\sigma^2 I)$
"
77,1802.06501,"[t]
	\caption{\label{alg:model1} Off-policy Training of DEERS Framework.}
	\raggedright
	\begin{algorithmic} [1]
		\STATE Initialize the capacity of replay memory $\mathcal{D}$
		\STATE Initialize action-value function $Q$ with random weights
		\FOR{session $=1, M$}
		\STATE  Initialize state $s_{0}$ from previous sessions
		\FOR{$t=1, T$}
        \STATE  Observe state $s_t=\{i_1, \cdots, i_{N},j_1, \cdots, j_{N}\}$ 
		\STATE  Execute action $a_{t}$ following off-policy $b(s_t)$ 
        \STATE  Observe reward $r_{t}$ from users
		\STATE  Set $s_{t+1}=
		\left\{\begin{array}{ll}
		\{i_2, \cdots, i_{N},a_t, j_1, \cdots, j_{N}\}
		& r_{t} > 0\\
		\{i_1, \cdots, i_{N}, j_2, \cdots, j_{N}, a_t\} 
		& r_{t} = 0
		\end{array}\right.$ 
		\STATE  Find competitor item $a_t^C$ of $a_{t}$
		\STATE  Store transition $(s_{t}, a_{t},  r_{t}, s_{t+1},a_t^C/null)$ in $\mathcal{D}$
		\STATE  Sample minibatch of transitions $(s, a, r, s',a^C/null)$ from $\mathcal{D}$
		\STATE  Set $y=
		\left\{\begin{array}{ll}
		r 
		& \mathrm{terminal\,}  s'\\
		r+\gamma\max_{a'}Q(s',a';\theta) 
		& \mathrm{non-terminal\,} s'
		\end{array}\right.$
		\IF{$a^C$ exists} 
		\STATE Minimize $\big(y-Q(s, a;\theta)\big)^{2}-\alpha\big(Q(s, a;\theta)-Q(s, a^C;\theta)\big)^{2}$ according to Equ (\ref{equ:differentiating2})
		\ELSE
		\STATE Minimize  $\big(y-Q(s, a;\theta)\big)^{2}$ according to Equ (\ref{equ:differentiating1})
		\ENDIF
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
"
78,1802.06501,"
	\caption{\label{alg:test} Offline Test of DEERS Framework.}
	\raggedright
    {\bf Input}: Initial state $s_{1}$, items ${ l_1, \cdots, l_T}$ and corresponding rewards ${r_1, \cdots, r_T}$  of a session $\mathcal{L}$.\\
    {\bf Output}:Recommendation list with new order\\
	\begin{algorithmic} [1]
		\STATE \textbf{for} $t =1, T$ \textbf{do}
		\STATE \quad Observe state $s_t = \{i_1, \cdots, i_{N}, j_1, \cdots, j_{N}\}$
        \STATE \quad Calculate Q-values of items in $\mathcal{L}$
        \STATE \quad Recommend item $l_{max}$ with maximal Q-value
        \STATE \quad Observe reward $r_t$ from users (historical logs)
		\STATE \quad Set $s_{t+1}=
		\left\{\begin{array}{ll}
		\{i_2, \cdots, i_{N},l_{max}, j_1, \cdots, j_{N}\}
		& r > 0\\
		\{i_1, \cdots, i_{N}, j_2, \cdots, j_{N}, l_{max}\} 
		& r = 0
		\end{array}\right.$ 
		\STATE \quad Remove $l_{max}$ from $\mathcal{L}$
		\STATE \textbf{end for}
	\end{algorithmic}
"
79,1802.06501,"
	\caption{\label{alg:test_on}Online Test of DEERS Framework.}
	\raggedright
	\begin{algorithmic} [1]
		\STATE Initialize action-value function $Q$ with well trained weights
		\STATE \textbf{for} session $=1, M$ \textbf{do}
		\STATE \quad Initialize state $s_{1}$ from previous sessions
		\STATE \quad \textbf{for} $t=1, T$ \textbf{do}
		\STATE \qquad Observe state $s_t=\{i_1, \cdots, i_{N},j_1, \cdots, j_{N}\}$ 
		\STATE \qquad Execute action $a_{t}$ following policy $\pi$ 
		\STATE \qquad Observe reward $r_{t}$ from users (online simulator)
		\STATE \qquad Set $s_{t+1}=
		\left\{\begin{array}{ll}
		\{i_2, \cdots, i_{N},a_t, j_1, \cdots, j_{N}\}
		& r_{t} > 0\\
		\{i_1, \cdots, i_{N}, j_2, \cdots, j_{N}, a_t\} 
		& r_{t} = 0
		\end{array}\right.$ 
		\STATE \quad \textbf{end for}
		\STATE \textbf{end for}
	\end{algorithmic}
"
80,1711.03243,"[tb]
   \caption{CEGIS}
   \label{alg:alg_cegis}
\begin{algorithmic}
  \STATE {\bfseries Input:} data $\D$, initial subset $\subD = \{\}$
  \STATE {\bfseries Output:} satisfying program $s$
  \REPEAT
	\STATE $s \leftarrow synthesize(\subD)$
    \STATE $ce \leftarrow check(s,\D)$
    \STATE $\subD \leftarrow \subD \cup \{ce\}$
  \UNTIL{$check(s, \D) =$ None}
  \STATE {\bfseries return} $s$
\end{algorithmic}
"
81,1711.03243,"[tb]
   \caption{Synthesis with Representative Examples}
   \label{alg:alg_ours}
\begin{algorithmic}
  \STATE {\bfseries Require:} trained committee model $nn(\neib_{x,k}(\subD), x)$\\
  approximating $Pr(y | \neib_{x,k}(\subD), x)$
  \STATE {\bfseries Input:} data $\D$
  \STATE {\bfseries Output:} satisfying program $s$
  \STATE Initialize $\subD = \{\}$.
  \REPEAT
  	\STATE \# Find the least likely input-output example
	\STATE $(x,y) \leftarrow \argmin_{(x_j,y_j)} nn(\neib_{x_j,k}(\subD), x_j)(y_j)$
    \STATE $\subD \leftarrow \subD \cup \{(x,y)\}$
  \UNTIL{confident$(nn, \subD, \D)$}
  \STATE {\bfseries return} CEGIS$(\D, \subD)$
\end{algorithmic}
"
82,1711.03243,"[ht]
   \caption{Is Representative}
   \label{alg:alg_repr_check}
\begin{algorithmic}
  \STATE {\bfseries Input:} data $\D$, subset $\subD$
  \STATE {\bfseries Output:} boolean
  \FOR{$d \in \D$ \textbackslash $\subD$}
  	\IF{synthesize($\subD \cup \{\neg d\}$}
    	\STATE{\bfseries return} false
    \ENDIF
  \ENDFOR
  \STATE {\bfseries return} true
\end{algorithmic}
"
83,1711.03243,"[tb]
   \caption{greedy selection with count oracle $c$}
   \label{alg:alg_count}
\begin{algorithmic}
  \STATE {\bfseries Input:} data $\D$
  \STATE {\bfseries Output:} data subset $\subD$
  \STATE Initialize $\subD = \{\}$.
  \REPEAT
	\STATE $(x,y) \leftarrow \argmin_{(x_j,y_j)} c(\subD \cup \{(x_j,y_j)\})$ \# selection
    \STATE $\subD \leftarrow \subD \cup \{(x,y)\}$
  \UNTIL{$c(\subD) = c(\subD \cup \{(x,y)\})$}
  \STATE {\bfseries return} $\subD$
\end{algorithmic}
"
84,1711.03243,"[tb]
   \caption{Bubble Sort}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $x_i$, size $m$
   \REPEAT
   \STATE Initialize $noChange = true$.
   \FOR{$i=1$ {\bfseries to} $m-1$}
   \IF{$x_i > x_{i+1}$}
   \STATE Swap $x_i$ and $x_{i+1}$
   \STATE $noChange = false$
   \ENDIF
   \ENDFOR
   \UNTIL{$noChange$ is $true$}
\end{algorithmic}
"
85,1805.11640,"[htb] \caption{$K$-beam $\epsilon$-subgradient descent} \label{alg:proposed}
{Input}: $f, K, N, (\rho_i), (\eta_{i}), (\epsilon_i)$\\
{Output}: $u_N, A_N$ \\
Initialize $u_0, A_0=(v^1_0,...,v^K_0)$\\
{Begin}
\begin{algorithmic}
\FOR{$i=1,\;...\;,N$}
	\STATE\hspace{-0.075in}{{\it Min step:}}
%	\STATE{Let $v^j_{i-1} = \arg\max_{v \in V_{i-1}} f(u_{i-1},v)$.}
%	\STATE{Update $u_i = u_{i-1} - \rho \nabla_u f(u_{i-1},v^j_{i-1})$.}
%	\STATE{Let $v^j_{i-1} = \arg\max_{v \in V_{i-1}} f(u_{i-1},v)$.}
	\STATE{Update $u_i = u_{i-1} + \rho_i\; g(u_i,A_i,\epsilon_i)$ where
	$g$ is a descent direction from Alg.~\ref{alg:descent direction}.}	
%	\STATE{Let $f_i^m=\max_{v \in V_{i-1}} f(u^i,v)$.}
	\STATE\hspace{-0.075in}{{\it Max step:}}
	\FOR{$k=1,\;...\;,K$ in parallel}
		%\STATE{Set $v=v^k_{i-1}$	.}
		%\FOR{$j=1,\;..,\;,J$}
		%	\STATE{Update $v \leftarrow  v + \eta_{ij}\; \nabla_v f(u_i,v)$.}
		%\ENDFOR
		%\STATE{Set $v^k_{i} = v$.}
		\STATE{Update $v_i^k \leftarrow  v_{i-1}^{k} + \eta_{i}\; \nabla_v f(u_i,v_{i-1}^k)$.}		
	\ENDFOR
	\STATE{Set $A_i = (v^1_i,\;...\;,v^K_i)$.}
\ENDFOR
\end{algorithmic}
%\vspace{0.1in}
"
86,1805.11640,"[htb] \caption{Descent direction} \label{alg:descent direction}
%{\bf I. Aggregation by voting}\\
{Input}: $f, u, A=(v^1,...,v^K), \epsilon$\\
{Output}: $g$ \\
{Begin}
\begin{algorithmic}
\STATE{Find $k_{\max} = \arg\max_{1 \leq k \leq K} f(u,v^k)$.}
%\STATE{Find $I=\{i\;|\; f(u,v^{i_{\max}})-f(u,v^i)\leq \epsilon \}$.} 
\STATE{Find $\{v^{k_1},...,v^{k_n}\}=R^{\epsilon}_A(u)=\{v\in A\;|\; f(u,v^{k_{\max}})-f(u,v^k)\leq \epsilon \}$.} 
\STATE{Compute $z_j = \nabla_u f(u,v^{k_j})$ for $j=1,..,n$.}
%and let $L_{R^{\epsilon}_A}(u)=\mathrm{co}\{ z_1 \cup ... \cup z_n\}$.}
\STATE\hspace{-0.075in}{\it Optional stopping criterion:}
\IF{$0 \in \mathrm{co}\{ z_1 \cup ... \cup z_n\}$}%L_{R^{\epsilon}_A}(u)$}
\STATE{Found a stationary point. Quit optimization.}
\ENDIF
\STATE\hspace{-0.075in}{\it Decent direction:}
\IF{$n=1$}
\STATE{Return $g=-z_1$.}%^/\|z_1\|$.}
\ELSE
%\STATE{\bf 1. Subgradient method:}
\STATE{Randomly choose $z \in \mathrm{co}\{ z_1 \cup ... \cup z_n\}$ and return 
$g= -z$.}%/\|z\|$}
%\STATE{\bf 2. Steepest descent method:}
%\STATE\hspace{0.25in}{Solve the QP (\ref{eq:qp}) and return 
%$g= -\sum_j a_j z_j / \|\sum_j a_j z_j\|$.}
%\IF{$\|z\|\leq \theta$} 
%	\STATE{Found a stationary point.}
%\ELSE
%	\STATE{Return $g= -\sum_j a_j z_j / \|\sum_j a_j z_j\|$.}
%\ENDIF
\ENDIF
\end{algorithmic}
%\vspace{0.1in}
"
87,1806.02485,"
	\begin{algorithmic}
		\State Input $A$, $W$, $\hat{n}$\,.
		\State Initialize $g$ so that nodes are assigned to communities uniformly at random.
		\State Let $\mathrm{eW} = e^{-W}$\, be the entry-wise exponential of $W$\,.
		\While{not converged}

		\State Let $U_{i\alpha} = \delta_{g_i\al}$ for each $i,\alpha$, where $\delta$ is the Kronecker delta.
		\State Let $X = AU$\,. \quad  // Counts the number of neighbors that each node has in each community
		\State Let $\vol = (k^TU)$\,.
		\For{$a'=1$ to $\hat{n}$}

		\State Let $I_{a'}$ be the set of nodes that are currently assigned to community.
		$a'$\,.

		\For{a=1 to $\hat{n}$}

		\State Let $I$ be the indices $1, \ldots, \hat{n}$ aside from $a$ and $a'$\,.
		\State Let $\mathrm{Delta}(I_{a'},a)$ be given by the following formula:
		\begin{align*}
			\mathrm{Delta}(I_{a'},a)& = 2X(I_{a'},I)W(I,a) \\
			& - 2X(I_{a'},I)W(I,a') \\
			& + 2X(I_{a'},a)W(a,a) \\
			& - 2X(I_{a'},a)W(a,a') + 2X(I_{a'}, a')W(a,a') \\
			& - 2X(I_{a'},a')W(a',a') \\
			& + \frac{1}{2m}\left(  2k(I_{a'})\vol(\mathrm{eW}(:,a) - \mathrm{eW}(:,a'))\right. \\
			& + \left.k(I_{a'})^2( \mathrm{eW}(a,a) + \mathrm{eW}(a',a') - 2\mathrm{eW}(a,a') )  \right).
		\end{align*}

		\EndFor
		\EndFor

		\For{$i=1$ to $N$}
		\State $g_i = \argmin(\mathrm{Delta(i,:)})$\,. \quad // [Choose uniformly at random in case of a tie.]
		\EndFor
		\EndWhile
		\State Output $g$\,.
	\end{algorithmic}
	\caption{Modified graph mean-curvature flow (MCF) for SBM MLE~\cref{exp:SBM_MLE}.
	}
	\label{alg:mcf}
"
88,1806.02485,"
	\begin{algorithmic}
		\State Input the initial domain.
		\State Initialize $u$ as the characteristic function of the initial domain.
		\For{$i=1,\ldots$}
		\State $u^{i+1/2}$ is the solution at time $\dt$ of $u_t = \Delta u$ with initial condition $u^i$\,.
		\State $u^{i+1} = \lfloor u^{i+1/2} + 0.5 \rfloor$\,, where $\lfloor \cdot\rfloor$ is the floor function. 
		\EndFor
		\State Output the set of points for which $u=1$\,.
	\end{algorithmic}
	\caption{A two-phase, continuum MBO scheme.
	}
	\label{alg:mbo}
"
89,1806.02485,"
	\begin{algorithmic}
		\State Input the initial state of the domain.
		\State Initialize $u_1,\ldots,u_{\hat{n}}$ as the characteristic functions of the initial domains.
		\For{$i=1,\ldots$}
		\For{$\al=1,\ldots,\hat{n}$}
		\State $u^{i+1/2}_\alpha$ is the solution at time $\dt$ to $u_{\al,t} = \Delta u_\al$ with initial condition $u^i_\al$\,.
		\EndFor
		\For{each point $x$}
		\State $\hat{\al} = \argmin_{\al} \sum_{\be} \sigma_{\al\be}u_{\be}(x)\,.$ \quad // [Choose uniformly at random in case of a tie] 
		\State $u_{\hat{\al}}(x) = 1$ and $u_{\be}(x)=0$ if $\be \ne \hat{\al}$\,.
		\EndFor
		\EndFor
		\State Output $u$\,.
	\end{algorithmic}
	\caption{A multiphase, continuum MBO scheme.} 
	\label{alg:multi_mbo}
"
90,1806.02485,"
	\begin{algorithmic}
		\State Input $A$, $\hat{n}_{\text{expected}}$\,.
		\State Place all nodes in the same community and add this community to a queue.
		\While{the queue is not empty}
		\State Save $g_{\text{old}}=g$ and $W_{\text{old}}=W$\,.
		\State Save the current objective-function value as $Q_{\text{old}}$\,.
		\State Partition the next community (as an induced subgraph, as we include all associated edges\footnotemark) in the queue 
		into $\min\{\hat{n}_{\text{expected}},\sqrt{N}\}$ communities using MCF, AC, or MBO with $w_{\al\be} = \begin{cases}
			1\,, & \al=\be\,, \\
			0.1\,, & \al\ne\be\,.
		      \end{cases}\,.$
		\While{it is possible to improve the objective-function value by merging two partition elements}
		\State Perform the merge that most improves the objective function.
		\EndWhile
		\If{the objective-function value is larger than $Q_{\text{old}}$}
		\State Add any newly created communities to the queue.
		\Else
		\State Set $g=g_{\text{old}}$ and $W=W_{\text{old}}$\,.
		\State Remove the current community from the queue.
		\EndIf
		\EndWhile
		\State Output $g$, $W$\,.
	\end{algorithmic}
	\caption{Our splitting--merging wrapper for escaping from bad local minima. In this algorithm, $Q$ is the objective function from~\cref{exp:cut_formulation} multiplied by $[1 + 0.1 \left( \nhat - \nhat_{\text{expected}} \right)^2]$, where we chose the value $0.1$ based on hand-tuning.
	}
	\label{alg:wrapper}
"
91,1802.06070,"[H]
    \small
    \DontPrintSemicolon
    \SetAlgoLined
    \While{not converged}{
        Sample skill $z \sim p(z)$ and initial state $s_0 \sim p_0(s)$\;
        \For{$t \leftarrow 1$ \KwTo $steps\_per\_episode$}{
            Sample action $a_t \sim \pi_{\theta}(a_t \mid s_t, z)$ from skill.\;
            Step environment: $s_{t+1} \sim p(s_{t+1} \mid s_t, a_t)$.\;
            Compute $q_{\phi}(z \mid s_{t+1})$ with discriminator.\;
            Set skill reward $r_t = \log q_{\phi}(z \mid s_{t+1}) - \log p(z)$\;
            Update policy ($\theta$) to maximize $r_t$ with SAC.\;
            Update discriminator ($\phi$) with SGD.\;
        }
    } 
    \caption{DIAYN}
    "
92,1806.02453,"[t!]
\caption{\small Computation performed by our Progressive Module Network, for one module $M_n$}
\begin{footnotesize}
\begin{algorithmic}[1]
\Function{$M_n$}{$q_n$}                               \Comment{$\E$ and $\mathcal L_n$ are global variables}
  \State $s^1$ = $I_n(q_n)$                           \Comment{initialize the state variable}
  \For{$t \gets 1\,$ to $\, T_n$}                     \Comment{$T_n$ is the maximum time step}
    \State $V = []$                                   \Comment{wipe out scratch pad $V$}
    \State $g_n^1,\ldots,g_n^{|\mathcal L_n|} = G_n(s^t)$ \Comment{compute importance scores}
    \For{$k \gets 1$ to $|\mathcal L_n|$}             \Comment{$\mathcal L_n$ is the sequence of lower modules $[M_m,...,M_l]$}
      \State $q_k = Q_{n\rightarrow k}(s^t, V, G_n(s^t))$                  \Comment{produce query for $M_k$}
      \State $o_k = \mathcal L_n[k](q_k)$                    \Comment{call $k^{th}$ module $M_k = \mathcal L_n[k]$, generate output}
      \State $v_k = R_{k\rightarrow n}(s^t, o_k)$                       \Comment{receive and project output}
      \State $V.\mathrm{append}(v_k)$     \Comment{write $v_k$ to pad $V$}
    \EndFor
    \State $s^{t+1} = U_n(s^{t}, V, \E, G_n(s^t))$              \Comment{update module state}
  \EndFor
  \State $o_n$ = \Call{$\Psi_n$}{$s^1, \ldots, s^{T_n}, q_n, \E $} \Comment{produce the output}
  \State \Return $o_n$
\EndFunction
\end{algorithmic}
\end{footnotesize}
\label{alg:comp}
"
93,1707.06792,"[H] \label{alg:SMC:update}
     \KwData{$\{ (\epart{m}{i}, \wgt{m}{i}) \}_{i = 1}^\N$}
     \KwResult{$\{ (\epart{m + 1}{i}, \wgt{m + 1}{i}) \}_{i = 1}^\N$}
     \For{$i \gets 1, \ldots, \N$}{
         draw $\ind{m + 1}{i} \sim \disc( \{ \wgt{m}{\ell} \}_{\ell = 1}^\N)$\;
         draw $\combpart{m + 1}{i} \sim \combkernelpath{m}
         (\combpart{m}{\ind{m + 1}{i}}, \rmd \comb{m + 1})$\;
         draw $\treepart{m + 1}{i} \sim \prop[\combpart{m}{\ind{m + 1}{i}}, \combpart{m + 1}{i}]{m}
         (\treepart{m}{\ind{m + 1}{i}}, \rmd \tree[m + 1])$\;
         set $\epart{m + 1}{i} \gets (\combpart{m + 1}{i}, \treepart{m + 1}{i})$\;
         set $\displaystyle \wgt{m + 1}{i} \gets \frac{p(\testdataset[\testdatasize_{\class}] | \trgr(\zpart{\comb{m+1}}{i} )) \ntrees{\trgr(\zpart{\comb{m}}{\ind{m + 1}{i}} )}
         \bk{m}(\epart{m}{\ind{m + 1}{i}}, \epart{m + 1}{i})}
         {p(\testdataset[\testdatasize_{\class}] |{\trgr(\zpart{\comb{m}}{\ind{m + 1}{i}} )}) \ntrees{\trgr(\zpart{\comb{m+1}}{i} )}
         \prop[\combpart{m}{\ind{m + 1}{i}}, \combpart{m + 1}{i}]{m}
         (\zpart{m}{\ind{m + 1}{i}}, \zpart{m + 1}{i})}$\;
         %set $\displaystyle \wgt{m + 1}{i} \gets \frac{\untarg[marg]{\combpart{m + 1}{i}}
         %(\zpart{m + 1}{i}) \bk{m}(\epart{m}{\ind{m + 1}{i}}, \epart{m + 1}{i})}
         %{\untarg[marg]{\combpart{m}{\ind{m + 1}{i}}}(\zpart{m}{\ind{m + 1}{i}})
         %\prop[\combpart{m}{\ind{m + 1}{i}}, \combpart{m + 1}{i}]{m}
         %(\zpart{m}{\ind{m + 1}{i}}, \zpart{m + 1}{i})}$\;
     }
     \caption{SMC update.}
"
94,1707.06792,"[H] \label{alg:particle:Gibbs:kernel}
     \KwData{a reference trajectory $\x{1:p} \in \xsp{1:p}$}
     \KwResult{a draw $\X{1:p}$ from $\PG{p}(\x{1:p},\cdot)$} %$\PG{p}(\x{1:p}, \rmd \x{1:p}')$}
     \For{$i \gets 1, \ldots, \N - 1$}{
         %draw $\combpart{1}{i} \sim \unifdens{\{1,\dots,\p\}}$ \;
         draw $\combpart{1}{i} \sim \combmeas{1}(\comb{1})$ \;
         set $\zpart{1}{i} \gets (\{\combpart{1}{i}\}, \emptyset)$ \;
         %draw $\zpart{1}{i} \sim \partinit[\combpart{1}{i}](\rmd \tree[1])$\;
         set $\epart{1}{i} \gets (\combpart{1}{i}, \zpart{1}{i})$\;
     }
     set $\epart{1}{\N} \gets \x{1}$\;
     \For{$i \gets 1, \ldots, \N$}{
         set $\wgt{1}{i} \gets 1$\;
        % / \partinit[\combpart{1}{i}](\zpart{1}{i})$\;
     }
     \For{$m \gets 1, \ldots, p - 1$}{
         \For{$i \gets 1, \ldots, \N - 1$}{
             draw $\ind{m + 1}{i} \sim \disc( \{ \wgt{m}{\ell} \}_{\ell = 1}^\N)$\;
             draw $\combpart{m + 1}{i} \sim \combkernelpath{m} (\combpart{m}{\ind{m + 1}{i}}, \cdot)$\;
             %draw $s \sim \unifdens{ \{1,\dots,\p\} \setminus \combpart{m}{\ind{m + 1}{i}} }$\;
             %$\combpart{m + 1}{i} \gets \combpart{m}{i} \cup s$ \;

             draw $\zpart{m + 1}{i} \sim \prop[\combpart{m}{\ind{m + 1}{i}}, \combpart{m + 1}{i}]{m}
             (\zpart{m}{\ind{m + 1}{i}}, \cdot)$\;
             set $\epart{m + 1}{i} \gets (\combpart{m + 1}{i}, \zpart{m + 1}{i})$\;
         }
         set $\epart{m + 1}{\N} \gets \x{m + 1}$\;
         \For{$i \gets 1, \ldots, \N$}{
             % set $\displaystyle \wgt{m + 1}{i} \gets \frac{\untarg[marg]{\combpart{m + 1}{i}}
             % (\zpart{m + 1}{i}) \bk{m}(\epart{m}{\ind{m + 1}{i}}, \epart{m + 1}{i})}
             % {\untarg[marg]{\combpart{m}{\ind{m + 1}{i}}}(\zpart{m}{\ind{m + 1}{i}})
             % \prop[\combpart{m}{\ind{m + 1}{i}}, \combpart{m + 1}{i}]{m}
             % (\zpart{m}{\ind{m + 1}{i}}, \zpart{m + 1}{i})}$\;
             set $\displaystyle \wgt{m + 1}{i} \gets \frac{p(\testdataset[\testdatasize_{\class}] | \trgr(\zpart{\comb{m+1}}{i} )) \ntrees{\trgr(\zpart{\comb{m}}{\ind{m + 1}{i}} )} \bk{m}(\epart{m}{\ind{m + 1}{i}}, \epart{m + 1}{i})}
             {p( \testdataset[\testdatasize_{\class}]| \trgr(\zpart{\comb{m}}{\ind{m + 1}{i}} )) \ntrees{\trgr(\zpart{\comb{m+1}}{i} )}
             \prop[\combpart{m}{\ind{m + 1}{i}}, \combpart{m + 1}{i}]{m}
             (\zpart{m}{\ind{m + 1}{i}}, \zpart{m + 1}{i})}$\;
         }
     }
     draw $\gen{p} \sim \disc( \{ \wgt{p}{\ell} \}_{\ell = 1}^\N )$\;
     set $\Z{p} \gets \zpart{p}{\gen{p}}$\;
     set $\X{p} \gets (\intvect{1}{p}, \Z{p})$\;

     \For{$m \gets p - 1, \ldots, 1$}{
         draw $\X{m} \sim \bk{m}(\X{m + 1}, \cdot)$\;
     }
     set $\X{1:p} \gets (\X{1}, \ldots, \X{p})$\;

     % \For{$m \gets p - 1, \ldots, 1$}{
     %     set $\gen{m} \gets \ind{m}{\gen{m + 1}}$\;
     %     set $\X{m} \gets \epart{m}{\gen{m + 1}}$\;
     % }
     % set $\X{1:p} \gets (\X{1}, \ldots, \X{p})$\;
     \Return{$\X{1:p}$}
     \caption{One transition of PG.}
"
95,1707.06792,"[H] \label{alg:particle:Gibbs:refreshment}
%      \KwData{a reference trajectory $\x{1:p} \in \xsp{1:p}$}
%      \KwResult{a draw $\X{1:p}$ from $\PG{p}\G{p}(\x{1:p}, \rmd \x{1:p}')$}
%      draw, using Algorithm~\ref{alg:particle:Gibbs:kernel},
%      $\X{1:p}' \sim \PG{p}(\x{1:p}, \rmd \x{1:p}')$\;
%      set $\X{p} = (\intvect{1}{p}, \Z{p}) \gets \X{p}'$\;
%      \For{$m \gets p - 1, \ldots, 1$}{
%          draw $\X{m} \sim \bk{m}(\X{m + 1}, \rmd \x{m})$\;
%      }
%      set $\X{1:p} \gets (\X{1}, \ldots, \X{p})$\;
%      \Return{$\X{1:p}$}
%      \caption{One transition
%      of PG with systematic refreshment.}
% "
96,1806.02390,"[t]
\caption{Variational Implicit Processes (VIP)}
\label{alg:VIP}
 {\bfseries Require:} data $\mathcal{D} = (\Xvec, \yvec)$; IP $\mathcal{IP}(g_{\theta}(\cdot, \cdot), p_{\zvec})$; variational distribution $q_{\varphi}(\avec)$; hyper-parameter $\alpha$
\begin{algorithmic}[1]
 \WHILE{not converged}
 \STATE sample mini-batch $\{(\xvec_m, y_m)\}_{m=1}^M \sim
 \mathcal{D}^M$
 \STATE sample $S$ function values: \\$\zvec_s \sim p(\zvec), f_s^\theta(\xvec_m) = g_{\theta}(\xvec_m, \zvec_s) $
 \STATE solutions of \textbf{sleep phase}:\\ $m^\star(\xvec_m) = \frac{1}{S} \sum_{s=1}^S f_s^\theta(\xvec_m)$,\\ $\Delta_s(\xvec_m) = f_s^\theta(\xvec_m) - m^{\star}(\xvec_m)$
 \STATE compute the \textbf{wake phase} energy $\mathcal{L}_{\mathcal{GP}}^{\alpha}(\theta, \varphi)$ in (\ref{eq:qa}) using (\ref{eq:bayesian_lr})
 \STATE gradient descent on $\mathcal{L}_{\mathcal{GP}}^{\alpha}(\theta, \varphi)$ w.r.t~$\theta, \varphi$, via reparameterization tricks
 \ENDWHILE
\end{algorithmic}
"
97,1806.02326,"[h]
\caption{Soft regression algorithm}\label{algo1}
\begin{algorithmic}
   \STATE {\bfseries Input:} terms $t_{1:m}$
    \STATE {\bfseries Output:} parameters $\hat{\vec{w}}_{1:m}$ and a matrix $\hat{Y}$
   \STATE Initialize $c_{1:m} \gets (1,\ldots, 1) $, $\lambda \gets \frac{\sqrt{8\mu} N \Mgood \Ss}{r}$
   \REPEAT
   \STATE Let $\hat{\vec{w}}_{1:m}, \hat{Y}$ be the solution to SDP:
   \begin{equation}\label{SDP}
       \begin{aligned}
        & \underset{\vec{w}_1,\ldots,\vec{w}_m,Y}{\text{minimize}}
        & & \sum_{i=1}^{m} c_i |t_i| f_i (\vec{w}_i) + \lambda \tr(Y)\\
        & \text{subject to}
        & & \vec{w}_i {\vec{w}_i}^{\top} \preceq Y \ \mathrm{for} \ \mathrm{all} \ i=1,\ldots,m.  
        \end{aligned}
   \end{equation}
   \IF{ $\tr(\hat{Y}) > \frac{6r^2}{\mu}$ }
        \STATE $ c \gets \text{UpdateWeights} (c,\hat{\vec{w}}_{1:m}, \hat{Y})$
  \ENDIF
  \UNTIL $\tr(\hat{Y}) \leq \frac{6r^2}{\mu}$
  \STATE {\bfseries Return} $\hat{\vec{w}}_{1:m}, \hat{Y}$
\end{algorithmic}
"
98,1806.02326,"
\caption{Algorithm for updating outlier weights}\label{UpdateWeights}
\begin{algorithmic}
   \STATE {\bfseries Input:} $c, \hat{\vec{w}}_{1:m}, \hat{Y} $.
    \STATE {\bfseries Output:} $c'$.
    \FOR{$i=1$ {\bfseries to} $m$}
    \STATE Let $\tilde{\vec{w}}_i$ be the solution to
    \begin{equation}
    \begin{aligned}
    & \underset{\tilde{\vec{w}}_i,a_{i1},\ldots,a_{im}}{\text{minimize}}
    & & f_i(\tilde{\vec{w}}_i)\\
    & \text{subject to}
    & & \tilde{\vec{w}}_i = \sum_{j=1}^{m} a_{ij} \hat{\vec{w}}_j,\quad   \sum_{j=1}^{m} a_{ij} = 1 \\
    & & &   0 \leq  a_{ij} \leq \frac{2}{\mu N}|t_j|, \quad \forall j   
    \end{aligned}
    \end{equation}
    $z_i \gets f_i(\tilde{\vec{w}}_i) - f_i(\hat{\vec{w}}_i)$
    \ENDFOR
    \STATE $ z_{max} \gets max\{\ z_i\ \big|\ c_i \neq 0\}  $
    \STATE $c'_i \gets c_i . \frac{z_{max} - z_i}{z_{max}}$ for $i = 1,\ldots ,n$
    \STATE {\bfseries Return}   $c'$
\end{algorithmic}
"
99,1806.02326,"
\caption{Padded Decomposition}\label{algo_padded}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\hat{\vec{w}}_{1:m}, \rho, \tau$.
    \STATE {\bfseries Output:} Partition $\mathcal{P} = \{T\}$.
    \STATE Initialize: let $\mathcal{P} = \emptyset$, $\mathcal{W} = \hat{\vec{w}}_{1:m}$. Sample $k \sim $ Uniform$(2,\rho)$.
    \WHILE{$\mathcal{W} \neq \emptyset$}
        \STATE Sample $i \sim $ Uniform$(1,m)$.
        \STATE Let $T \gets$ Ball$(\Wh_i, k\tau) \cap \mathcal{W}$.
        \STATE Update:  $\mathcal{P} = \mathcal{P} \cup \{T\}$. $\mathcal{W} \gets \mathcal{W} \backslash T$.
    \ENDWHILE
    \STATE {\bfseries Return:} partition $\mathcal{P}$.
\end{algorithmic}
"
100,1806.02326,"[h]
\caption{List-regression algorithm}\label{algo4}
\begin{algorithmic}
   \STATE {\bfseries Input:} $m$ terms, target radius $r_{final}$.
    \STATE {\bfseries Output:} candidate solutions $\{ \vec{u}_1,...,\vec{u}_s \}$ and $\hat{\vec{w}}_{1:m}$.
    \STATE Initialize $r^{(1)} \gets r$, \\
    $\hat{\vec{w}}^{(1)}_{1:m} \gets $ Algorithm \ref{algo1} with origin $0$ and radius $r$ (all $i=1,\ldots,m$ are ``assigned'' an output).
\FOR{$\ell = 1,2,\ldots$}
    \STATE    $\Ww \gets \{ \hat{\vec{w}}^{(\ell)}_{i} |  \ \hat{\vec{w}}^{(\ell)}_{i}  \text{is assigned} \} $
    \IF{$r^{(\ell)} < \frac{1}{2}r_{final}$}
        \STATE Greedily find a maximal set of points $\vec{u}_1,...,\vec{u}_s$  s.t. \\
            \hspace*{0.5cm}{I}: $|B(\vec{u}_{j};2r_{\text{final}}) \cap \Ww | \geq (1 - \varepsilon)\mu N, \quad \forall j. $\\ 
            \hspace*{0.5cm}{II}: $\| \vec{u}_j - \vec{u}_{j'} \|_2 > 4r_{\text{final}}, \quad \forall j \neq j'$.
       \STATE {\bfseries Return} $\mathcal{U} = \{ \vec{u}_1,...,\vec{u}_s \}, \hat{\vec{w}}_{1:m}^{(\ell)}$.
   \ENDIF
   \FOR{$h=1$ {\bfseries to} $112\log(\frac{\ell(\ell+1)}{\delta})$}
   \STATE $\bar{\vec{w}}_{1:m}(h) \gets$ \text{unassigned} \\
    Let $\mathcal{P}_h$ be a $(\rho , 2r^{(\ell)}, \frac{7}{8})$-padded decomposition of $\Ww$ with 
    $\rho = \mathcal{O} (r^{(\ell)} log (\frac{2}{\mu}))$.
        \FOR{$T \in \mathcal{P}_h$}
            \STATE      Let $B(u,\rho) $ be a ball containing $T$. Run Algorithm \ref{algo1} on $\Hh \cap B(u,\rho) $, with radius $r = \rho$ and origin shifted to $u$.
    \STATE  for each $\Wh_i \in T$ assign $\bar{\vec{w}}_i(h)$ as the outputs of Algorithm \ref{algo1}.
    \ENDFOR
   \ENDFOR
    \FOR{$i=1$ {\bfseries to} $m$}
   \STATE Find a  $h_0$ such that $ \| \bar{\vec{w}}_i(h_0) - \bar{\vec{w}}_i(h)\|_2 \leq \frac{1}{3} r^{(\ell)} $ for at least $ \frac{1}{2} \text{\ of the \ } h\text{'s}$.
\STATE   $\hat{\vec{w}}_i^{(\ell+1)} \gets \bar{\vec{w}}_i (h_0)$ (or ``unassigned'' if no such $h_0$ exists)
   \ENDFOR
   \STATE $ r^{(\ell+1)} \gets \frac{1}{2} r^{(\ell)}$ 
\ENDFOR
\end{algorithmic}
"
101,1806.02326,"[htb]
        \caption{Partial Greedy Algorithm}\label{greedy-alg}
        \textbf{Input:} finite set $\mathcal{T}=\{T_1,...,T_m\}$, costs 
$\{\omega_1,...,\omega_m\}, \mu,\gamma \in (0,1]$ \\
        \textbf{Output:} Condition $\hat{\vec{c}}$
        \begin{algorithmic}[1]
        \STATE Initialize $\hat{\vec{c}} = \emptyset$\\
        \WHILE{$(1-(2/3)\gamma)\mu N >  \left|\bigcup_{T_j\in \hat{\vec{c}}}T_j\right|$}
        \STATE Choose the first $T_j\in\mathcal{T}\setminus\hat{\vec{c}}$ covering at least $\frac{\mu\gamma}{3\Mgood}N$ additional examples, that minimizes $\omega_j/|T_j|$, for $T_j \in \mathcal{T}\setminus\hat{\vec{c}}$.\\ %In case of a tie, take the smallest such $i$.
        \STATE Add $T_j$ to $\hat{\vec{c}}$, set each other $T_{j'}=T_{j'}\setminus T_j$
        \ENDWHILE
        \STATE Return $\hat{\vec{c}}$
        \end{algorithmic}
        \label{alg:Insert}
"
102,1806.02315,"[H] 
  \caption{MNF-DQN}
    \label{alg:mnf}  
  \begin{algorithmic}[1]
  \State \textbf{Input:} $m$ mini-batch size; $D$ empty replay buffer; $K$ update frequency, $U$ target update
  frequency
  \For{episode $e \in {1,  ..., M}$}  
     \State Set $s \leftarrow s_0$ 
     \State Sample noise variables $\epsilon_w$ and $\epsilon_z$ from standard normal distribution.
    %  \State Set network weights and auxiliary variable $w, z_K = h(\epsilon_w,\epsilon_z, \phi, \theta)$
     \For{$t \in {1, \dots}$}
        \State Select an action $a_t \leftarrow \arg \max_{a\in A} Q(s, a, \epsilon_z, \epsilon_w; \phi, \theta)$
        \State Observe next state $s_{t+1}$ and reward ${r_t}$ after taking action $a_t$
        \State Store transition $(s_t, a_t, r_t, s_{t+1})$ in the replay buffer $D$
        \If{$t \mod K == 0$}
            \State Sample a mini-batch of $m$ transitions $((s_j, a_j, r_j, s_{j}') \sim D)^m_{j=1}$
            \For{$j=1, \ldots, m$}
                \If{ $s_j'$ is terminal state}
                    $y_j = r_j$ 
                \Else
                    \State $y_j = r_j + \gamma Q(s, a, 0, 0; \phi^{-}, \theta^{-})$
                \EndIf
            \EndFor 
            \State Re-sample noise variables $\epsilon_w$ and $\epsilon_z$ and perform gradient step with respect to $(\phi, \theta)$:
            \State $\frac{1}{m} \sum_{j=1, \ldots m} 
    \left( y_j - Q(s_j, a_j, \epsilon_z, \epsilon_w; \phi, \theta)\right)^2 + \lambda    \textrm{regularization\_cost}(\epsilon_z, \epsilon_w; \phi, \theta)$
    \State Re-sample noise variables $\epsilon_w$ and $\epsilon_z$
    % and set $w, z_K = h(\epsilon_w,\epsilon_z, \phi, \theta)$
        \EndIf
        \If{$t \mod U == 0$}
            \State Set $\theta^{-}, \phi^{-} \leftarrow \theta, \phi$
        \EndIf
        \State Set $s \leftarrow s_{t+1}$
     \EndFor
  \EndFor
  
  \end{algorithmic} 
 
"
103,1806.02315," 
  \caption{MNF-DDPG algorithm}
    \label{alg:mnf-ddpg}  
    \begin{algorithmic}
    \State \textbf{Input:} $m$ mini-batch size; $D$ empty replay buffer; $K$ update frequency, $U$ target update frequency
    \State Initialize critic network $ Q(s, a, \epsilon_z, \epsilon_w ; \phi, \theta)$ and actor $\pi(s;  \psi)$ with weights $ \theta$, $\phi$ and $ \psi$. 
    \State Initialize target critic network $Q(s,a, \epsilon_z, \epsilon_w; \phi^{-1}, \theta^{-})$ and actor $\pi(s; \psi^{-})$ with weights $\theta^{-} \leftarrow \theta, \phi^{-1} \leftarrow \phi,  \psi^{-} \leftarrow \psi$
    \For{episode $e \in {1,  ..., M}$}  
     \State Set $s \leftarrow s_0$ 
     \State Sample noise variables $\epsilon_w$ and $\epsilon_z$ from standard normal distribution.
    %  \State Set network weights and auxiliary variable $w, z_K = h(\epsilon_w,\epsilon_z, \phi, \theta)$
     \For{$t \in {1, \dots}$}
        \State Select an action $a_t \leftarrow \pi(s; \psi)$
        \State Observe next state $s_{t+1}$ and reward ${r_t}$ after taking action $a_t$
        \State Store transition $(s_t, a_t, r_t, s_{t+1})$ in the replay buffer $D$
        \If{$t \mod K == 0$}
            \State Sample a mini-batch of $m$ transitions $((s_j, a_j, r_j, s_{j}') \sim D)^m_{j=1}$
            \For{$j=1, \ldots, m$}
                \If{ $s_j'$ is terminal state}
                    $y_j = r_j$ 
                \Else
                    \State $y_j = r_j + \gamma Q(s, \pi(s; \psi^{-}), 0, 0; \phi^{-1} \theta^{-})$
                \EndIf
            \EndFor 
            % update the critic 
            \State Re-sample noise variables $\epsilon_w$ and $\epsilon_z$ and perform gradient step with respect to $(\phi, \theta)$:
            \State $\frac{1}{m} \sum_{j=1, \ldots m} 
    \left( y_j - Q(s_j, a_j, \epsilon_z, \epsilon_w; \phi, \theta)\right)^2 + \lambda   \textrm{regularization\_cost}(\epsilon_z, \epsilon_w; \phi, \theta)$
    
    
    % update the actor 
    \State Update the actor using sampled policy gradient: $\frac{1}{m} \sum_{j=1, \ldots m} \nabla_{a} Q(s_j, a, \epsilon_z, \epsilon_w; \phi, \theta)|_{a = \pi(s_j; \psi)} \nabla_{\psi} \pi(s_j; \psi)
    $
    
    \State Re-sample noise variables $\epsilon_w$ and $\epsilon_z$
    % and set $w, z_K = h(\epsilon_w,\epsilon_z, \phi, \theta)$
    
        
    
        \EndIf
        \If{$t \mod U == 0$}
            \State Set $\theta^{-} \leftarrow \tau \theta + (1 - \tau) \theta^{-}$
            \State Set $\phi^{-} \leftarrow \tau \phi + (1 - \tau) \phi^{-}$
            \State Set $\psi^{-} \leftarrow \tau \psi + (1 - \tau) \psi^{-}$
        \EndIf
        \State Set $s \leftarrow s_{t+1}$
     \EndFor
  \EndFor
  
  \end{algorithmic} 
 
"
104,1806.02296,"[t]
 \caption{ADMM \cite{Boyd:FTML:11}}
 \begin{algorithmic}[1] \label{alg:ADMM}
 \REQUIRE{$\ell(\cdot;\vec{y}),\rho(\cdot),
           \beta,\lambda,
           \vec{v}_0,\vec{u}_0$,
           and $K$} \label{line:ADMM_init}
 \FOR{$k = 1,2,\dots,K$}
 	\STATE{$\vec{x}_{k}=\arg\min_{\vec{x}}\{\ell(\vec{x};\vec{y})+\frac{\beta}{2}\norm{\vec{x}-\vec{v}_{k-1}+\vec{u}_{k-1}}^2\}$}\label{line:ADMM_x_update}
 	\STATE{$\vec{v}_k = \arg\min_{\vec{v}}\{\lambda\rho(\vec{v})+\frac{\beta}{2}\norm{\vec{v}-\vec{x}_k-\vec{u}_{k-1}}^2\}$}\label{line:ADMM_v_update}
 	\STATE{$\vec{u}_k = \vec{u}_{k-1}+\vec{x}_k-\vec{v}_k$}\label{line:ADMM_u_update}
 \ENDFOR
 \STATE{Return $\vec{x}_K$}
 \end{algorithmic}
 "
105,1806.02296,"[t]
\caption{RED-ADMM with $I$ Inner Iterations\cite{Romano:JIS:17}}
\begin{algorithmic}[1] \label{alg:RED_ADMM}
\REQUIRE{$\ell(\cdot;\vec{y}),\vec{f}(\cdot),
          \beta,\lambda,
          \vec{v}_0,\vec{u}_0,K$,
          and $I$} \label{line:RED_ADMM_init}
\FOR{$k = 1,2,\dots,K$}
 	\STATE{$\vec{x}_{k}=\arg\min_{\vec{x}}\{\ell(\vec{x};\vec{y})+\frac{\beta}{2}\norm{\vec{x}-\vec{v}_{k-1}+\vec{u}_{k-1}}^2\}$}\label{line:RED_ADMM_x_update}
 	\STATE{$\vec{z}_0 = \vec{v}_{k-1}$}\label{line:RED_ADMM_z_init}
 	\FOR{$i=1,2,\dots,I$}
 		\STATE{$\vec{z}_i=\frac{\lambda}{\lambda+\beta} \vec{f}(\vec{z}_{i-1})+\frac{\beta}{\lambda+\beta}(\vec{x}_k+\vec{u}_{k-1})$}\label{line:RED_ADMM_z_update}
 	\ENDFOR \label{line:RED_ADMM_z_end}
 	\STATE{$\vec{v}_k = \vec{z}_{I}$}\label{line:RED_ADMM_v_update}
 	\STATE{$\vec{u}_k = \vec{u}_{k-1}+\vec{x}_k-\vec{v}_k$}\label{line:RED_ADMM_u_update}
\ENDFOR
\STATE{Return $\vec{x}_K$}
\end{algorithmic}
"
106,1806.02296,"[t]
\caption{RED-ADMM with $I=1$}
\begin{algorithmic}[1] \label{alg:RED_ADMM_inexact}
\REQUIRE{$\ell(\cdot;\vec{y}),\vec{f}(\cdot),
          \beta,\lambda,
          \vec{v}_0,\vec{u}_0$,
          and $K$} \label{line:ADMM2_init}
\FOR{$k = 1,2,\dots,K$}
 	\STATE{$\vec{x}_{k}=\arg\min_{\vec{x}}\{\ell(\vec{x};\vec{y})+\frac{\beta}{2}\norm{\vec{x}-\vec{v}_{k-1}+\vec{u}_{k-1}}^2\}$}\label{line:ADMM2_x_update}
 	\STATE{$\vec{v}_k = \frac{\lambda}{\lambda+\beta} \vec{f}(\vec{v}_{k-1})+\frac{\beta}{\lambda+\beta}(\vec{x}_k+\vec{u}_{k-1})$}\label{line:ADMM2_v_update}
 	\STATE{$\vec{u}_k = \vec{u}_{k-1}+\vec{x}_k-\vec{v}_k$}\label{line:ADMM2_u_update}
\ENDFOR
\STATE{Return $\vec{x}_K$}
\end{algorithmic}
"
107,1806.02296,"[t]
 \caption{RED-PG Algorithm}
 \begin{algorithmic}[1] \label{alg:RED_PG}
 \REQUIRE{$\ell(\cdot;\vec{y}),\vec{f}(\cdot),
          \lambda,
          \vec{v}_0, L>0$,
          and $K$} \label{line:PG_init}
 \FOR{$k = 1,2,\dots,K$}
 	\STATE{$\vec{x}_{k}=\arg\min_{\vec{x}}\{\ell(\vec{x};\vec{y})+\frac{\lambda L}{2}\norm{\vec{x}-\vec{v}_{k-1}}^2\}$} \label{line:PG_x_update}
 	\STATE{$\vec{v}_k = \frac{1}{L}\vec{f}(\vec{x}_k)-\frac{1-L}{L}\vec{x}_{k}$} \label{line:PG_v_update}
 \ENDFOR
 \STATE{Return $\vec{x}_K$}
 \end{algorithmic}
 "
108,1806.02296,"[t]
 \caption{RED-DPG Algorithm}
 \begin{algorithmic}[1] \label{alg:RED_DPG}
 \REQUIRE{$\ell(\cdot;\vec{y}),\vec{f}(\cdot),
          \lambda,
          \vec{v}_0, L_0>0, L_\infty>0$,
          and $K$} \label{line:DPG_init}
 \FOR{$k = 1,2,\dots,K$}
 	\STATE{$\vec{x}_{k}=\arg\min_{\vec{x}}\{\ell(\vec{x};\vec{y})+\frac{\lambda L_{k-1}}{2}\norm{\vec{x}-\vec{v}_{k-1}}^2\}$}\label{line:DPG_x_update}
        \STATE{$L_k = \big(\frac{1}{L_\infty} + (\frac{1}{L_0}-\frac{1}{L_\infty})\frac{1}{\sqrt{k+1}}\big)^{-1}$}\label{line:DPG_Lk_update}
 	\STATE{$\vec{v}_k = \frac{1}{L_k}\vec{f}(\vec{x}_k)-\frac{1-L_k}{L_k}\vec{x}_{k}$}\label{line:DPG_v_update}
 \ENDFOR
 \STATE{Return $\vec{x}_K$}
 \end{algorithmic}
 "
109,1806.02296,"[t]
 \caption{RED-APG Algorithm}
 \begin{algorithmic}[1] \label{alg:RED_APG}
 \REQUIRE{$\ell(\cdot;\vec{y}),\vec{f}(\cdot),
          \lambda,
          \vec{v}_0, L>0$,
          and $K$} \label{line:APG_init}
 \STATE{$t_0=1$}
 \FOR{$k = 1,2,\dots,K$}
 	\STATE{$\vec{x}_{k}=\arg\min_{\vec{x}}\{\ell(\vec{x};\vec{y})+\frac{\lambda L}{2}\norm{\vec{x}-\vec{v}_{k-1}}^2\}$}\label{line:APG_x_update}
        \STATE{$t_k=\frac{1+\sqrt{1+4t_{k-1}^2}}{2}$}\label{line:APG_t_update}
 	\STATE{$\vec{z}_k = \vec{x}_k + \frac{t_{k-1}-1}{t_k}(\vec{x}_k-\vec{x}_{k-1})$}\label{line:APG_z_update}
 	\STATE{$\vec{v}_k = \frac{1}{L}\vec{f}(\vec{z}_k)-\frac{1-L}{L}\vec{z}_k $}\label{line:APG_v_update}
 \ENDFOR
 \STATE{Return $\vec{x}_K$}
 \end{algorithmic}
 "
110,1711.00673," [h]
\caption{FITBO acquisition function}\label{alg:FITBO}
\begin{algorithmic}[1]
       \STATE {\bfseries Input:} a test input $\mathbf{x}$; noisy observation data 
       \\ $D_n=\{(\mathbf{x}_{i},y_{i}) \vert i=1, \dots, n\}$

	\STATE Sample hyperparameters and $\eta$ from $p(\boldsymbol{\psi} \vert D_n)$: 
	\\ $\boldsymbol{\Psi}= \{\boldsymbol{\theta}^{(j)},\eta^{(j)} \vert j=1, \dots, M\}$ 
	\FOR {j = 1, $\dots, M$}
	\STATE Use $f(\mathbf{x}) =\eta+\sfrac{1}{2}g(\mathbf{x})^2 $ to approximate $p(f \vert D_n,\mathbf{x},\boldsymbol{\theta}^{(j)}, \eta^{(j)})= \mathcal{GP} \big(m_f(\cdot),K_f(\cdot,\cdot)\big ) $
	
%	3.1) compute $D_g=\{(\mathbf{x}_{i},g_{i}) \vert i=1, \dots, n\} $ where $g_{i}=\sqrt {2(y_{i}-\eta^{(j)})}$
%	
%	3.2) compute posterior distribution $ p(g \vert  D_g, \mathbf{x}, \boldsymbol{\theta}^{(j)},\eta^{(j)})$
%	
%	3.3) approximate $p(f \vert D_n,\mathbf{x},\boldsymbol{\theta}^{(j)}, \eta^{(j)})$ using the linearisation technique 
	
	\STATE Compute $p(y \vert D_n,\mathbf{x},\boldsymbol{\theta}^{(j)}, \eta^{(j)}) $
	
	\STATE Compute $ H[p(y \vert D_n,\mathbf{x},\boldsymbol{\theta}^{(j)}, \eta^{(j)})]  $
	
	\ENDFOR	
			
	\STATE Estimate the entropy of the Gaussian mixture : 
	
	$ \text{E}_1(\mathbf{x} \vert D_n) = H \Big[  \frac{1}{M} \sum_j^{M} p(y \vert D_n,\mathbf{x},\boldsymbol{\theta}^{(j)}, \eta^{(j)}) \Big] $
	
	\STATE Compute the entropy expectation: 
	\\ $ \text{E}_2(\mathbf{x} \vert D_n)= \frac{1}{M} \sum_j^{M} H[p(y \vert D_n,\mathbf{x},\boldsymbol{\theta}^{(j)}, \eta^{(j)})]
	= \frac{1}{2M} \sum_j^{M} \log \big[ 2\pi e \big(v_f(\mathbf{x} \vert D_n, \boldsymbol{\theta}^{(j)}, \eta^{(j)})+\sigma_n^2\big) \big] $ 	
	
	\STATE \textbf{return}  $\alpha_n(\mathbf{x} \vert D_n)=\text{E}_1(\mathbf{x} \vert D_n) - \text{E}_2(\mathbf{x} \vert D_n)$
	
\end{algorithmic}
"
111,1711.00673,"[tb]
%   \caption{Bubble Sort}
%   \label{alg:example}
%\begin{algorithmic}
%   \STATE {\bfseries Input:} data $x_i$, size $m$
%   \REPEAT
%   \STATE Initialize $noChange = true$.
%   \FOR{$i=1$ {\bfseries to} $m-1$}
%   \IF{$x_i > x_{i+1}$}
%   \STATE Swap $x_i$ and $x_{i+1}$
%   \STATE $noChange = false$
%   \ENDIF
%   \ENDFOR
%   \UNTIL{$noChange$ is $true$}
%\end{algorithmic}
%"
112,1806.02261,"[h!]
%\floatname{algorithm}{Algoritmo}
	\caption*{\textbf{Stochastic Variance Reduced Gradient (\SVRG) inference for \BOCPD}}
   \label{Algorithm_BOCPDMS}
\begin{algorithmic}
   \State {\bfseries Input at time $0$:} Window \& batch sizes $W$, $B^{\ast}$, $b^{\ast}$; frequency $m$, prior $\*\theta_0$, \#steps  $K$, step size $\eta$
   \State \hspace*{2.4cm}  s.t. $W > B^{\ast} > b^{\ast}$; and $\sim$ denotes sampling without replacement 
  % \STATE {\bfseries Input at time $t$:} next observation $\*y_t$ 
   %\STATE {\bfseries Output at time} $t$: $\widehat{\*y}_{(t+1):(t+h_{\max})}$, $S_t$, $\mathbb{P}(m_t|\*y_{1:t})$
   % \\[5pt]
   \For{ next observation $\*y_t$ at time $t$}   %\vspace*{0.15cm}
   %\STATE{$\mathcal{Y} \leftarrow \*y_{\max(0, t-W):t}$}% \quad \quad //Add new observation and slide window}
   \For{retained run-lengths $r \in R(t)$ }
   %\STATE //STEP 1: Compute a new anchor for the gradient estimates every $\tau_r$ steps
   	\If{$\tau_r = 0$} 
   		\If{$r < W$}
   			\State $\*\theta_r \leftarrow \*\theta^{\ast}_r \leftarrow \text{FullOpt}\left(\text{\ELBO}(\*y_{t-r:t})\right)$; %\quad \quad //\:Compute the new anchor
   			%\State 
            $\tau_r \leftarrow  m$
   			 %\STATE $\*\theta_r \leftarrow \*\theta^{\ast}_r$
   		\ElsIf{ $r \geq W$}
   			\State $\*\theta^{\ast}_r \leftarrow \*\theta_r$;
   			%\State 
            $\tau_r \leftarrow  \text{Geom}\left(B^{\ast}/(B^{\ast}+b^{\ast})\right)$
   		\EndIf 
        \State $B \leftarrow \min(B^{\ast}, r)$
   		\State $g_r^{\text{anchor}}  \leftarrow \frac{1}{B}\sum_{i \in \mathcal{I}} \nabla \text{\ELBO}(\*\theta^{\ast}_r, \*y_{t-i})$, 
   			where $\mathcal{I} \sim \text{Unif}\{0,\dots,\min(r,W)\}$,
   			 $|\mathcal{I}| = B$
   		%\STATE $\tau_r = m \cdot \bm{1}_{r<W} +
   		 %\text{Geom}\left(B/(B+b)\right) \cdot \bm{1}_{r\geq W}$
    \EndIf
    %\STATE //STEP 2: Update step with anchored gradient estimate
    \For{ $j = 1,2,\dots, K$}
    	  		\State $b \leftarrow \min(b^{\ast}, r)$ and $\widetilde{\mathcal{I}} \sim  \text{Unif}\{0,\dots,\min(r,W)\}$ and $|\widetilde{\mathcal{I}}| = b$
    	  		\State $g_r^{\text{old}} \leftarrow  \frac{1}{b}\sum_{i \in \widetilde{\mathcal{I}}} \nabla \text{\ELBO}(\*\theta^{\ast}_r, \*y_{t-i})$, \:
    	  		$g_r^{\text{new}} \leftarrow  \frac{1}{b}\sum_{i \in \widetilde{\mathcal{I}}} \nabla \text{\ELBO}(\*\theta_r, \*y_{t-i})$
    	  		\State $\*\theta_r \leftarrow \*\theta_r + \eta\cdot\left(g_r^{\text{new}} - 
    	  						g_r^{\text{old}} + g_r^{\text{anchor}}\right) $; 
    	  		%\State 
                $\tau_r \leftarrow  \tau_r -1$
    \EndFor
   \EndFor
     %\STATE //STEP 3: Advance run-lengths before next observation comes in
     \State $r \leftarrow r+1$ for all $r \in R(t)$; $R(t) \leftarrow R(t) \cup \{0\}$
   \EndFor
\end{algorithmic}
"
113,1805.05383,"[t!]
%\floatname{algorithm}{Algoritmo}
   \caption*{\textbf{\BOCPD with Model Selection (\BOCPDMS)}}
   \label{Algorithm_BOCPDMS}
\begin{algorithmic}
   \STATE {\bfseries Input at time $0$:} model universe $\mathcal{M}$; hazard $H$; prior $q$
   \STATE {\bfseries Input at time $t$:} next observation $\*y_t$ 
   \STATE {\bfseries Output at time} $t$: $\widehat{\*y}_{(t+1):(t+h_{\max})}$, $S_t$, $p(m_t|\*y_{1:t})$
    \\[5pt]
   \FOR{ next observation $\*y_t$ at time $t$}   \vspace*{0.15cm}
   \STATE{$//$ STEP I:  Compute model-specific quantities} \vspace*{0.05cm}
   	\FOR{ $m \in \mathcal{M}$ }
   		\IF{ $t-1 = \text{lag\_length(m)}$ }
   			\STATE [I.A] Initialize $p(\*y_{1:t}, r_t=0, m_t=m)$ with prior
   		\ELSIF{ $t-1 > \text{lag\_length(m)}$ }
   			\STATE [I.B.1] Update $p(\*y_{1:t}, r_t, m_t=m)$  via \eqref{growth_probability}, \eqref{cp_probability}
   			\STATE [I.B.2] Prune model-specific run-length distribution
   			\STATE [I.B.3] Perform hyperparameter inference via \eqref{gradient_descent_caron}
   		\ENDIF 
   	\ENDFOR \vspace*{0.15cm}
   	%\hrulefill
   	\STATE {$//$ STEP II:  Aggregate over models} \vspace*{0.05cm}
   	\IF{ $t >= \min(\text{lag\_length(m)})$ }
   		\STATE [II.1] Obtain joint distribution over $\mathcal{M}$ via \eqref{evidence}--\eqref{conditional_model_posterior}
   		\STATE [II.2] Compute  	
   			\eqref{posterior_predictive}--\eqref{MAP_estimator}  
   		\STATE [II.3] \textbf{Output:} $\widehat{\*y}_{(t+1):(t+h_{\max})}, S_t, p(m_t|\*y_{1:t})$  
   	\ENDIF 
   \ENDFOR
\end{algorithmic}
"
114,1806.02215,"[t]
\caption{Learning in Spectral Inference Networks}\label{alg:spin}
\begin{algorithmic}[1]
\State \textbf{given} symmetric kernel $k$, decay rates $\beta_t$, first order optimizer \textsc{Optim}
\State \textbf{initialize} parameters $\theta_0$, average covariance $\bar{\mathbf{\Sigma}}_0 = \mathbf{I}$, average Jacobian of covariance $\bar{\mathbf{J}}_{\mathbf{\Sigma}_0} = 0$
\While{not converged}
    \State Get minibatches $\mathbf{x}_{t1}, \ldots, \mathbf{x}_{tN}$ and $\mathbf{x}'_{t1}, \ldots, \mathbf{x}'_{tN}$
    \State $\hat{\mathbf{\Sigma}}_t = \frac{1}{2}\left(\frac{1}{N}\sum_{i}\mathbf{u}_{\theta_t}(\mathbf{x}_{ti})\mathbf{u}_{\theta_t}(\mathbf{x}_{ti})^T +\frac{1}{N}\sum_{i} \mathbf{u}_{\theta_t}(\mathbf{x}'_{ti})\mathbf{u}_{\theta_t}(\mathbf{x}'_{ti})^T\right)$, covariance of minibatches
    \State $\hat{\mathbf{\Pi}}_t = \frac{1}{N}\sum_{i}k(\mathbf{x}_{ti}, \mathbf{x}'_{ti})\mathbf{u}_{\theta_t}(\mathbf{x}_{ti})\mathbf{u}_{\theta_t}( \mathbf{x}'_{ti})^T$
    \State $\bar{\mathbf{\Sigma}}_t \gets (1-\beta_t) \bar{\mathbf{\Sigma}}_{t-1} + \beta_t\hat{\mathbf{\Sigma}}_t$
    \State $\bar{\mathbf{J}}_{\mathbf{\Sigma}_t} \gets (1-\beta_t) \bar{\mathbf{J}}_{\mathbf{\Sigma}_{t-1}} + \beta_t\hat{\mathbf{J}}_{\mathbf{\Sigma}_t}$
    \State $\bar{\mathbf{L}}_t \gets$ Cholesky decomposition of $\bar{\mathbf{\Sigma}}_t$
    \State Compute gradient $\tilde{\nabla}_{\mathbf{\theta}}\mathrm{Tr}(\mathbf{\Lambda}(\hat{\mathbf{\Pi}}_t, \bar{\mathbf{\Sigma}}_t, \hat{\mathbf{J}}_{\mathbf{\Pi}_t}, \bar{\mathbf{J}}_{\mathbf{\Sigma}_t}))$ according to Eq.~\ref{eqn:symmetry_broken_gradient}
    \State $\theta_t \gets \mathrm{\textsc{Optim}}(\theta_{t-1}, \tilde{\nabla}_{\theta}\mathrm{Tr}(\mathbf{\Lambda}(\hat{\mathbf{\Pi}}_t, \bar{\mathbf{\Sigma}}_t, \hat{\mathbf{J}}_{\mathbf{\Pi}_t}, \bar{\mathbf{J}}_{\mathbf{\Sigma}_t}))$
\EndWhile
\State \textbf{result} Eigenfunctions $\mathbf{v}_{\theta^*}(\mathbf{x})=\mathbf{L}^{-1}\mathbf{u}_{\theta^*}(\mathbf{x})$ of $\mathcal{K}[f](\mathbf{x}) = \mathbb{E}_{\mathbf{x}'}[k(\mathbf{x}, \mathbf{x}')f(\mathbf{x}')]$
\end{algorithmic}
"
115,1806.00416,"[H]
\caption{General Pattern Search (GPS)}
\label{alg: GPS algorithm}
\begin{algorithmic}[1] 
% Where $x_0 \in \mathbb{R}^n$, $\Delta_0 > 0$
\Procedure{GPS\_SOLVER}{$\mathbf{x}^{(0)}$, $\Delta^{(0)}$, $\mathbf{C}^{(0)}$, $\mathbf{B}$}
\State $k=-1$
\Do
	\State $k = k+1$
% 	\State Compute $f(x_k)$
    \State $\mathbf{s}^{(k)} =$ EXPLORE\_MOVES($\mathbf{B}\mathbf{C}^{(k)}$, $\mathbf{x}^{(k)}$, $\Delta^{(k)}$) \label{func: GPS exploratory moves}
    \State $\rho^{(k)}=f(\mathbf{x}^{(k)}+\mathbf{s}^{(k)})-f(\mathbf{x}^{(k)})$
    \If{$\rho^{(k)} < 0$}
    	\State $\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)} + \mathbf{s}^{(k)}$
        \Comment{Successful iteration}
    \Else 
    	\State $\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}$
        \Comment{Unsuccessful iteration}
    \EndIf
    \State $\Delta^{(k+1)}, \mathbf{C}^{(k+1)}=$ UPDATE($\mathbf{C}^{(k)}$, $\Delta^{(k)}$,  $\rho^{(k)}$) \label{func: GPS update ck and dk}
    
\doWhile{convergence criterion == False}
\EndProcedure
\end{algorithmic}
"
116,1806.00416,"[H]
\caption{Proposed MDS}
\label{a:proposed}
\begin{algorithmic}[1] 

\Procedure{MDS}{$\mathbf{T}$, $L$, $r^{(0)}$}
    \State $k$ $\gets$ $0$
    \Comment{k is the number of epochs}
	\State $\mathbf{X}^{(k)}$ $\gets$ UNIFORM($N \times L$)
    \State $\mathbf{D}^{(k)}$ $\gets$ DISTANCE\_MATRIX($\mathbf{X}^{(k)}$)
    \State $e^{(k)}$ $\gets$ $f(\mathbf{T}, \mathbf{D}^{(k)})$
    \State $e^{(k-1)}$ $\gets$ $+\infty$
    \State $r^{(k)}$ $\gets$ $r^{(0)}$
    \While{$r^{(k)} > \delta$}
        \If{$e^{(k-1)} - e^{(k)} \leq \epsilon \cdot e^{(k)}$} \label{al:eps}
        	\State $r^{(k)}$ $\gets$ $\frac{r^{(k)}}{2}$
        \EndIf
    	\State $\mathbf{S}$ $\gets$ SEARCH\_DIRECTIONS($r^{(k)}$, $L$)
		\ForAll{$x \in \mathbf{X}^{(k)}$}
			\State $\mathbf{X^{*}}, e^{*}$ $\gets$ OPTIMAL\_MOVE($\mathbf{X}^{(k)}$,$x$,$\mathbf{S}$,$e^{(k)}$) \label{al:move_line}
			\State $e^{(k-1)} \gets e^{(k)}$
			\State $e^{(k)}$ $\gets$ $e^*$
        	\State $\mathbf{X}^{(k)}$ $\gets$ $\mathbf{X^*}$
        \EndFor
        \State $k = k + 1$
	\EndWhile
\EndProcedure
\end{algorithmic}
"
117,1806.00416,"[H]
\caption{Define search directions}
\label{a:search_directions}
\begin{algorithmic}[1] 

\Function{SEARCH\_DIRECTIONS}{$r$, $L$}
	\State $\mathbf{S^+}$ $\gets$ $r \cdot \mathbf{I}_{L }$
	\State $\mathbf{S^-}$ $\gets$ $-r \cdot \mathbf{I}_{L }$
	\State $\mathbf{S}$ $\gets$ $[\frac{\mathbf{S^+}}{\mathbf{S^-}}]$
    \State \Return $S$
\EndFunction
\end{algorithmic}
"
118,1806.00416,"[H]
\caption{Find optimal move for a point}
\label{a:move}
\begin{algorithmic}[1] 

\Function{OPTIMAL\_MOVE}{$\mathbf{X}^{(k)}$, $x$, $S$, $e$}
	\State $e^* \gets e$
	\ForAll{$s \in \mathbf{S}$}
		\State $\tilde{x}$ $\gets$ $x + s$
    	\State $\mathbf{\tilde{X}}$ $\gets$ UPDATE\_POINT($\mathbf{X}^{(k)}$, $x$, $\tilde{x}$)
    	\Comment{Update $x$ point of $\mathbf{X}^{(k)}$ with $\tilde{x}$}
    	\State $\mathbf{D}$ $\gets$ DISTANCE\_MATRIX($\mathbf{\tilde{X}}$) \label{al:distance_matrix}
    	\State $\tilde{e}$ $\gets$ f($\mathbf{T}$, $\mathbf{D}$)
    	\If{$\tilde{e} < e^*$}
    		\State $e^*$ $\gets$ $\tilde{e}$
    		\State $\mathbf{X}^*$ $\gets$ $\mathbf{\tilde{X}}$
   		\EndIf
	\EndFor
    \State \Return $X^*, e^*$
\EndFunction
\end{algorithmic}
"
119,1806.00416,"[H]
% \caption{alg}
% \label{a:proposed}
% \begin{algorithmic}[1] 

% \Procedure{MDS}{$D_t$, $L$}

% 	\Comment{$D_t$ is an $N \times N$ dissimilarity matrix}
    
% 	\Comment{$N$ is the number of data points}
% 	\State $\mathbf{X}$ $\gets$ uniform($N \times L$)
%     \State $D$ $\gets$ distance\_matrix($\mathbf{X}$)
%     \State $e$ $\gets$ mse($D_t$, $D$)
%     \State $r$ $\gets$ starting\_radius()
    
%     \Comment{starting\_radius initializes the search radius}
    
%     \Comment{It may be a constant value or found heuristically}
    
%     \While{$r > \delta$}
%     	\ForAll{$x \in \mathbf{X}$}
%         	\State $S_+$ $\gets$ $r \cdot I_{L \times L}$
%             \State $S_-$ $\gets$ $-r \cdot I_{L \times L}$
%            	\State $S$ $\gets$ $[\frac{S_+}{S_-}]$
            
%             \Comment{$S$ is a $2L \times L$ matrix of the search dimensions (vertical concatenation of $S_+$ and $S_-$)}
%             \State $e_S$ $\gets$ $+\infty$
%             \State $\mathbf{X_S}$ $\gets$ $\mathbf{X}$
%             \ForAll{$s \in S$}
%             	\State $x_{temp}$ $\gets$ $x + s$
%             	\State $\mathbf{X_{temp}}$ $\gets$ swap\_row($\mathbf{X}$, $x_{temp}$)
% 				\State $D$ $\gets$ distance\_matrix($\mathbf{X}$)
%                 \State $e_{temp}$ $\gets$ mse($D_t$, $D$)
%                 \If{$e_{temp} < e_S \land e_{temp} < e$}
%                 	\State $e_S$ $\gets$ $e_{temp}$
%                     \State $\mathbf{X_S}$ $\gets$ $\mathbf{X_{temp}}$
%                 \EndIf
% 			\EndFor
%             \If{$e - e_S < \epsilon$}
%             	\State $r$ $\gets$ $\frac{r}{2}$
%             \EndIf
% 			\State $e$ $\gets$ $e_S$
%         	\State $\mathbf{X}$ $\gets$ $\mathbf{X_S}$
%         \EndFor
%     \EndWhile
% \EndProcedure
% \end{algorithmic}
% "
120,1806.00416,"[H]
\begin{algorithmic}
\State $\mathbf{X^{*}}, e^{*}$ $\gets$ OPTIMAL\_MOVE($\mathbf{X}^{(k)}$, $x$, $S$, $+\infty$)
\end{algorithmic}
"
121,1806.00416,"[H]
% \caption{Find optimal move for a point}
% \label{a:move_opt}
% \begin{algorithmic}[1] 

% \Function{OPTIMAL\_MOVE}{$D_k$, $\mathbf{X_k}$, $x$, $S$, $e$}
% 	\State $e^* \gets e$
%     \State $D \gets D_k$
% 	\ForAll{$s \in S$}
% 		\State $\tilde{x}$ $\gets$ $x + s$
%     	\State $\mathbf{\tilde{X}}$ $\gets$ UPDATE\_POINT($\mathbf{X_k}$, $x$, $\tilde{x}$)
%     	\Comment{Update $x$ point of $\mathbf{X_k}$ with $\tilde{x}$}
        
%         \State $d$ $\gets$ NON\_ZERO($s$)
%         \Comment{Modified dimension $d$ is the single non zero element of $s$}
%     	\ForAll{$y \in \{ \mathbf{X_k} \smallsetminus x \}$}
%         	\State $D_{xy} \gets u(D_k, x_d, \tilde{x_d}, y_d)$ 
%         \EndFor
%     	\State $\tilde{e}$ $\gets$ f($\Delta$, $D$)
%     	\If{$\tilde{e} < e^*$}
%     		\State $e_*$ $\gets$ $\tilde{e}$
%     		\State $\mathbf{X^*}$ $\gets$ $\mathbf{\tilde{X}}$
%            	\State $D_k$ $\gets$ $D$
%    		\EndIf
%         \State $D$ $\gets$ $D_k$
% 	\EndFor
%     \State \Return $X^*, e^*, D_k$
% \EndFunction
% \end{algorithmic}
% "
122,1806.02136,"[t!]
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithmic}[1]
\State \Comment{Returns an expression including both the original and the derivative computation.}
\Function{deriv}{\expr{}, \text{x}}
\State args $\gets \emptyset$
\State f $\gets$ \expr{}
\ForAll{v $\gets$ \textsc{freeVars}(e)}
\State f $\gets$ \vabs{v}{f}
% \If{v = x}
% \State  args $\gets$ args $\cup$ \Call{dual}{v, \textsc{oneHot}(v)}
% \Else
% \State  args $\gets$ args $\cup$ \Call{dual}{v, \textsc{zero}(v)}
% \EndIf
\State  args $\gets$ args $\cup$ \Call{dual}{v, \iif{}(v = x) \then{} \textsc{oneHot}(v) \elsee{} \textsc{zero}(v)}
\EndFor
\State df $\gets$ (\difftrans{f}) args
\If{\Call{Type}{x} = \typedouble}
\State \Return{df}
\ElsIf{\Call{Type}{x} = \typevector}
\State \Return{\vbuildk{} (\vlength{x}) (\vabs{ri}{} df)}
\ElsIf{\Call{Type}{x} = \typematrix}
\State \Return{\vbuildk{} (matrixRows x) (\vabs{ri}{} \vbuildk{} (matrixCols x) (\vabs{ci}{} df))}
\EndIf
\EndFunction
\State \Comment{Returns the dual number encoding of the two input expressions.}
\Function{dual}{\exprind{1}, \exprind{2}}
\If{\Call{Type}{\exprind{1}} = \typedouble}
\State \Return{\adpair{\exprind{1}}{\exprind{2}}}
\ElsIf{\Call{Type}{\exprind{1}} = \typevector}
\State \Return{vectorZip \exprind{1} \exprind{2}}
\ElsIf{\Call{Type}{\exprind{1}} = \typematrix}
\State \Return{matrixZip \exprind{1} \exprind{2}}
\EndIf
\EndFunction
\State \Comment{Returns a zero scalar, vector, or matrix expression based on the type of input.}
\Function{zero}{\expr}
\If{\Call{Type}{\expr} = \typedouble}
\State \Return{0}
\ElsIf{\Call{Type}{\expr} = \typevector}
\State \Return{vectorZeros (\vlength{\expr})}
\ElsIf{\Call{Type}{\expr} = \typematrix}
\State \Return{matrixZeros (matrixRows \expr) (matrixCols \expr)}
\EndIf
\EndFunction
\State \Comment{Returns a one-hot encoding scalar, vector, or matrix expression.}
\Function{oneHot}{\expr}
\If{\Call{Type}{\expr} = \typedouble}
\State \Return{1}
\ElsIf{\Call{Type}{\expr} = \typevector}
\State \Return{vectorHot (\vlength{\expr}) ri}
\ElsIf{\Call{Type}{\expr} = \typematrix}
\State \Return{matrixHot (matrixRows \expr) (matrixCols \expr) ri ci}
\EndIf
\EndFunction
\end{algorithmic}
\caption{A pseudo-code implementation of the \derivk{} construct.}
\label{fig:deriv}
"
123,1806.02071,"
\caption{Simulation with the Latent Space Integration Network}
\begin{algorithmic}
\State{{$\vec{c_{0}} \gets G^{\dagger}(\vec{u_{0}})$}}
\While{simulating from $t$ to $t+1$}
  \State{$\vec{x}_t \gets [\vec{c}_t; \Delta\vec{p}_t] \quad$ // $\vec{c}_t$ from previous step, $\vec{p}$ is given} 
  \State{$\vec{z}_{t+1} \gets \vec{z}_t + T(\vec{x}_t)\quad$ // latent code inference }
  \State{$\vec{c}_{t+1} \gets [\vec{z}_{t+1}; \vec{p}_{t+1}]$}
  \State{$\vec{u}_{t+1} \gets G(\vec{c}_{t+1}) \quad$ // velocity field reconstruction}
\EndWhile
\end{algorithmic}
\label{alg:algorithm}
"
124,1705.03854,"[b]
\caption{INFERENCE. At test time, the data extracted from the resized videoclip is input to the three branches and their output is summed and normalized to obtain the final FoA prediction.}
\label{alg:test_algorithm}
\begin{algorithmic}[1]
%%%% INFERENCE
\vspace{0.1cm}
\Statex \hspace{-0.5cm} {\bf input:} data $X=\{x_{1}, x_{2}, \dots, x_{16}\}$ for all domains
\Statex \hspace{-0.5cm} {\bf output:} predicted FoA map $\hat{y}$
\State $X_\text{res} \gets \texttt{\small resize}(X, (112, 112))$
\For{branch $b \in \{\text{RGB}, \text{flow}, \text{seg}\}$}
\State $\hat{y}_b \gets$ \texttt{\footnotesize REFINE}(\texttt{\small stack}($x_{b_{16}}$, \texttt{\small upsample}(\texttt{\footnotesize COARSE}($X_{b_\text{res}}$))))
\EndFor
\State $\hat{y} \gets \sum_b \hat{y}_b / \sum_i \sum_b \hat{y}_b(i)$
\end{algorithmic}
"
125,1806.02056,"[t]
\begin{description}
\item[Inputs:] $\mathcal{R}$ --- a collection of binary user histories, $\tau$ --- upper bound on the number of top-level categories, $\mu$ --- maximum category size, $\delta$ --- threshold used in model building test, $\kappa$ --- number of EM steps for each category. \\
\item[Outputs:] An HLTM $m$.
\end{description}

\begin{algorithmic}[1]
\State $\mathcal{R}_1 \gets \mathcal{R}$, $m \gets null$.
\Repeat \label{alg1.start}
    \State $m_1 \gets$ \sysname{LearnFlatForestModel}($\mathcal{R}_1$, $\delta$, $\mu$, $\kappa$);\label{alg1:learnFlat}
    \State $\mathcal{R}_1 \gets$ \sysname{HardAssignment}($m_1$, $\mathcal{R}_1$); \label{alg1.hardassign}

    \If{${m}= null$}
        \State ${m} \gets {m}_1$;\label{alg1.m}
    \Else
        \State $m\gets$ \sysname{StackModels}(${m}_1$, ${m}$); \label{alg1.stackmodel}
    \EndIf
    

\Until number of top-level nodes in $m  \leq \tau$.  \label{alg1.end}
\State Link the top level latent nodes of $m$ to form a tree.
\State \Return $m$. \label{alg1.return}
\end{algorithmic}
%\end{multicols}
\caption{\sysname{HLTA-Forest}($\mathcal{R}$, $\tau$, $\mu$, $\delta$)}
\label{alg.top}
"
126,1806.02056,"[t]
\begin{description}
\item[Inputs:] $\mathcal{R}_1$, $\mu$, $\delta$, $\kappa$. \\
\item[Outputs:] A flat forest model $m_1$.
\end{description}

\begin{algorithmic}[1]
\State $\mathcal{I} \gets$ all items in $\mathcal{R}_1$, $m_1 \gets \emptyset$.
\While {$|\mathcal{I}| > 0$}
    \State $c \gets$ \sysname{OneCategory}($\mathcal{R}_1$, $\mathcal{I}$, $\delta$, $\mu$, $\kappa$);
    \State $m_1 \gets m_1 \cup \{c\}$;
    \State $\mathcal{I} \gets $ item in $\mathcal{R}_1$ but not in any $c \in m_1$;
\EndWhile
\State \Return $m_1$.
\end{algorithmic}
\caption{\sysname{LearnFlatForestModel}($\mathcal{R}_1$, $\delta$, $\mu$, $\kappa$)}
\label{alg.buildislands}
"
127,1806.02056,"[t]
\begin{description}
\item[Inputs:] $\mathcal{I}$, $\mathcal{R}_1$, $\mu$, $\delta$, $\kappa$. \\
\item[Outputs:] A category $c$.
\end{description}

%\begin{multicols}{2}
\begin{algorithmic}[1]
\State \textbf{if} number of items $|\mathcal{I}| \leq 3$, $c \gets$ \sysname{LearnLCM}($\mathcal{R}_1$, $\mathcal{I}$), \textbf{return} $c$.\label{alg3.line1}
\State $\mathcal{S} \gets$ a random variable from $\mathcal{I}$ \label{alg3.s1}
\State {\scriptsize $X \gets \argmax_{A \in \mathcal{I} \setminus \mathcal{S}} CosineSim(A, \mathcal{S})$};\label{alg3.s2}
\State {$\mathcal{S} \gets \mathcal{S} \cup {X} $};\label{alg3.s3}
\State $\mathcal{I}_1 \gets  \mathcal{I} \setminus \mathcal{S}$;
\State $\mathcal{R}_2 \gets$ \sysname{ProjectData}($\mathcal{R}_1$, $\mathcal{S}$); \\
$c \gets$ \sysname{LearnLCM}($\mathcal{R}_2$, $\mathcal{S}$);\label{alg3.firstlcm}
\Loop
    \State {\scriptsize $X \gets \argmax_{A \in \mathcal{I}_1} CosineSim(A, \mathcal{S})$; \label{alg3.pick-x}
    \State $W \gets \argmax_{A \in S} CosineSim(A, X)$}\label{alg3.pick-w};
    \State $\mathcal{R}_2 \gets$ \sysname{ProjectData}$(\mathcal{R}_1, \mathcal{S} \cup \{X\})$, $\mathcal{V}_1 \gets \mathcal{V}_1 \setminus \{X\}$;
    \State $c_1 \gets$ \sysname{Pem-Lcm}$(c, \mathcal{S}, X, \mathcal{R}_2)$;\label{alg3.lcm}
    \State \textbf{if} $|\mathcal{I}_1| = 0$ \Return $c_1$.\label{alg3.return-lcm}
    \State $c_2 \gets$ \sysname{Pem-Ltm-2l}($c$, $\mathcal{S} \setminus \{W\}$, $\{W, X\}$, $\mathcal{R}_2$);\label{alg3.ltm}
    \If{$BIC(c_2|\mathcal{R}_2) - BIC(c_1|\mathcal{R}_2) > \delta$}\label{alg3.compare}
    	\State $c \gets$ Run EM on $c_2$ for $\kappa$ steps;
        \State \Return $c$ with $W$, $X$ and their parent removed.\label{alg3.return-island}
    \EndIf
    \If{$|\mathcal{S}| \geq \mu$}\label{alg3.maxisland}
    	\State $c \gets $Run EM on $c_1$ for $\kappa$ steps, \Return $c$.
    \EndIf
    \State $c \gets c_1$, $\mathcal{S} \gets \mathcal{S} \cup \{X\}$;\label{alg3.add-x}
\EndLoop

\end{algorithmic}
%\end{multicols}
\caption{\sysname{OneCategory}($\mathcal{R}_1$, $\mathcal{I}$, $\delta$, $\mu$, $\kappa$)}\label{alg.oneisland}
"
128,1806.02056,"[t]
\begin{description}
\item[Inputs:] A HTLM $m$, category level $l$, base recommendation list $B$, set of items $\mathcal{R}_u$ consumed by user $u$,  $\alpha$, $K$. \\
\item[Outputs:] A top $K$ recommendation list $L$.
\end{description}

%\begin{multicols}{2}
\begin{algorithmic}[1]
\State ${C_1 \dots C_m} \gets$ latent variable at level $l$ of $m$;
\State $n \gets |R_u|$, $n_i \gets 0 , i \in [1,m], B_{C_i}=\varnothing, C=\varnothing$;
\State $\forall C_i, i \in [1,m],$ get the set of items of this category $I_{C_i}$ from $m$; \label{alg.car.set}
\State $B_{C_i} \gets B_{C_i} \cup I \; s.t. \; I \in I_{C_i},\forall I \in B$;\label{alg.car.base}
\State $n_i \gets n_i+1 \; s.t. \; I \in I_{C_i}, \forall I \in \mathcal{R}_u$;\label{alg.car.ni}
\State sort $n_i, i \in [1,m], \; s.t. \; n_1>n_2 \dots > n_m$;
\For{$k =1 \dots m$}
\State \textbf{if} $n_k \geq \alpha$ \textbf{then} $r_{C_k}=\frac{n_k}{n}K$\label{alg.car.count}
\State \textbf{else break;}
\EndFor
\For{$j =1 \dots k$}\label{alg.car.cars}
\State $L \gets $ top $r_{C_j}$ elements from $B_{C_j}$;
\State $B \gets B\setminus L$ and $B_{C_j} \gets B_{C_j}\setminus L;$\label{alg.car.del}
\State $C \gets C \cup C_j;$
\EndFor\label{alg.car.care}
\While{$|L|< K$}% \wedge |B| \neq 0$}
\State $I \gets$ top item from $B$\label{alg.car.pick}
\State  \textbf{if} $(I \in I_{C_i}) \wedge (I_{C_i} \notin C) \wedge (n_i \neq 0)$ \textbf{then} $L \gets I;$\label{alg.car.new}
\State $B \gets I \setminus B;$
\EndWhile
%\State \textbf{if} $|L| < K$, \textbf{then} $L \gets top \; |K-L|$ items from $B_{C_1}$
\State \textbf{return} $L$.
\end{algorithmic}
%\end{multicols}
\caption{\sysname{CAR}($R_u$, $m$, $l$, $B$, $K$, $\alpha$)}\label{alg.car}
"
129,1806.02034,"
%\algsetup{linenosize=\tiny}
\small
\begin{algorithmic}
\STATE {\bf Input} Data matrix ($\X$), set of numbers of clusters to consider ($\{k_1, ..., k_m\}$), set of values for residual variance ($\{\sigma^2_1, ..., \sigma^2_l\}$)\\
\STATE
\STATE $K_{\mathrm{est}} \gets \mathrm{integer}(l)$ \hfill \# {\em initialise estimates of $k$}\\
\FOR{$i \in \{1, ..., l\}$}
\STATE $\mathrm{edf} \gets \mathrm{numeric}(m)$ \hfill \# {\em initialise/reset estimated degrees of freedom}\\
\FOR{$j \in \{1, ..., m\}$}
\STATE $\mathrm{edf}[j] \gets EDF(\mbox{$k$-means}(\X; k_j)|\hat \mmu\left(\X; k_j+1), \sigma^2_i\right)$ \hfill \# {\em compute effective degrees of freedom}\\
\STATE \hfill {\em estimates using $\sigma^2_i$}
\ENDFOR
\STATE $\mathrm{edf} \gets \mathrm{smooth}([k_1, ..., k_m], \mathrm{edf}| 3)$ \hfill \# {\em smooth degrees of freedom estimates}
\STATE $\mathrm{BIC} \gets \mbox{numeric}(m)$ \hfill \# {\em initialise/reset BIC estimates}\\
\FOR{$j\in \{1, ..., m\}$}
\STATE  $\mathrm{BIC}[j] \gets \frac{1}{\sigma^2_i} \sum_{r=1}^n||\X_{r\_}-\hat \mmu(\X;k_j)_{c(r)\_}||^2 + \log(nd)\mathrm{edf}[j]$ \hfill \# {\em compute BIC estimates}
\ENDFOR
\STATE $K_{\mathrm{est}}[i] \gets \mathrm{fmin}\left(\mathrm{BIC}\right)$ \hfill \# {\em estimate $k$ as the first local minimum in BIC estimates}
\ENDFOR
\IF{$\max_{k \in \{k_1, k_m\}}\Sigma_{i=1}^l I_{(K_{\mathrm{est}}[i]== k)}\geq \frac{2l}{3}$} 
\STATE {\bf return} $\mathrm{argmax}_{k \in \{k_1, k_m\}}\Sigma_{i=1}^l I_{(K_{\mathrm{est}}[i]== k)}$ \hfill \# {\em return $k_1$ or $k_m$ if estimated sufficiently often}
\ELSE
\STATE {\bf return} $\mathrm{argmax}_{k \in \{k_2, ..., k_{m-1}\}} \Sigma_{i=1}^l I_{(K_{\mathrm{est}}[i]== k)}$ \hfill \# {\em else return most frequent other value for $k$}
\ENDIF
\end{algorithmic}
\caption{Estimating $k$ using the BIC and effective degrees of freedom \label{alg:edfkmeans}}
"
130,1805.01553,"[t]
\caption{Bandit Interactive-Predictive}
\label{alg:BIP-NMT}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\theta_{0}$, $\phi_{0}$, $\alpha_A$, $\alpha_C$, $\epsilon$
\STATE \textbf{Output:} Estimates $\theta^\ast$, $\phi^\ast$
\STATE $k \leftarrow 1$
\FOR{i $\leftarrow$ 1, $\ldots$ N}
	\STATE Receive $\mathbf{x}_{i}$, Initialize $\gamma_{0} \leftarrow 0$, $\Xi \leftarrow \emptyset$
	\FOR{t $\leftarrow$ 1 $\ldots$ $T_{\max}$}
		\STATE Sample $\hat{y}_{1:t} \sim p_{\theta_{k-1}}(\cdot | \mathbf{x}_{i},\mathbf{\hat{y}}_{<t}, \Xi)$
		\STATE Compute $\bar{H}(\hat{y}_{1:t})$ using Eq. \eqref{eq:H_ph}
		\IF{$\bar{H}(\hat{y}_{1:t}) - \gamma_{t-1} \geq \epsilon \cdot \gamma_{t-1}$ \\
		 or $<$eos$>$ in $\hat{y}_{1:t}$}
			\STATE Receive feedback $\textrm{R}(\hat{y}_{1:t})$
				\IF{$\textrm{R}(\hat{y}_{1:t})\geq \mu$}
					\STATE $\Xi \leftarrow \hat{y}_{1:t}$
				\ENDIF
			    \STATE $\theta_k \leftarrow \theta_{k-1} -\alpha_A \nabla L_{\theta_{k-1}}(\hat{y}_{1:t})$ \\
			    (Eq. \eqref{eq:sgd})
				\STATE $\phi_k \leftarrow \phi_{k-1} -\alpha_C \nabla L_{\phi_{k-1}}(\hat{y}_{1:t})$ \\
				(see Eq. (7) in Nguyen et al. \shortcite{NguyenETAL:17})
				\STATE $k \leftarrow k + 1$
		\ENDIF
		\STATE Update $\gamma_{t} \leftarrow \gamma_{t-1}  + \frac{1}{t} \left( \bar{H}(\hat{y}_{1:t}) - \gamma_{t-1} \right) $
		\STATE \textbf{break} if $<$eos$>$ in $\hat{y}_{1:t}$
	\ENDFOR
\ENDFOR
\end{algorithmic}
"
131,1805.01553,"[t]
%\caption{Algorithm BIP-NMT}
%\label{alg:BIP-NMT}
%\scriptsize
%\begin{algorithmic}[1]
%\STATE \textbf{Input:} $\theta_{0}$, $\phi_{0}$, $\alpha_A$, $\alpha_C$
%\STATE \textbf{Output:} Estimates $\theta^\ast$, $\phi^\ast$
%\FOR{i = 1, $\ldots$ N}
%	\STATE Receive $\mathbf{x_{i}}$
%	\STATE Initialize $\gamma \leftarrow 0$, $\Xi \leftarrow \emptyset$
%	\FOR{t = 1 $\ldots$ $T_{\max}$}
%		\STATE Sample $\hat{y}_{1:t} \sim p_{\theta_{t-1}}(\cdot | \mathbf{x_{i}},\mathbf{y}_{<t}, \Xi)$
%		\STATE Compute $\bar{H}(\hat{y}_{1:t})$ using Eq. \eqref{eq:H_ph}
%		\IF{$\bar{H}(\hat{y}_{1:t}) - \gamma_{t-1} \geq \epsilon \times \gamma_{t-1}$ or $<$eos$>$ in $\hat{y}_{1:t}$}
%			\STATE Receive feedback $\textrm{R}(\hat{y}_{1:t})$
%				\IF{$\textrm{R}(\hat{y}_{1:t})\geq \mu$}
%					\STATE $\Xi \leftarrow \hat{y}_{1:t}$
%					%\STATE R($\hat{y}_{1}^{n}, y_{1}^{n}$) = f(R($\hat{y}_{1}^{n}, y_{1}^{n}$))
%				\ENDIF
%			        \STATE Update $\theta_t \leftarrow \theta_{t-1} -\alpha_A \nabla L_{\theta_{t-1}}(\hat{y}_t)$ (Eq. \eqref{eq:sgd})
%			\STATE Update $\phi_t \leftarrow \phi_{t-1} -\alpha_C \nabla L_{\phi_{t-1}}(\hat{y}_t)$ (see Eq. (7) in Nguyen et al. \shortcite{NguyenETAL:17})
%		\ENDIF
%		\STATE Update $\gamma_{t} = \gamma_{t-1}  + \frac{1}{t} \left( \bar{H}(\hat{y}_{1:t}) - \gamma_{t-1} \right) $ 
%		\STATE \textbf{break} if $<$eos$>$ in $\hat{y}_{1:t}$
%	\ENDFOR
%\ENDFOR
%\end{algorithmic}
%"
132,1802.04205,"[t]
%   \algsetup{linenosize=\tiny}
  \footnotesize
\SetKwProg{Fn}{Function}{}{}
\Fn{$high\_level\_plan\_to\_countinuous\_state\_goals$ (high-level plan)}{
    \For{each $q^{k}$ in high-level plan}{
        Define corresponding full-confidence vector,  $W^{k}_{full\_conf}$ = 
            $\begin{cases}
                1, & \text{if}\ q = q_{goal}^{k} \\
                0, & \text{else} 
            \end{cases}$\\
        Sample $n$ random points:  $X_{sample}$ = $\{x_1,...,x_n \} \sim \mathcal{X}$; \\
        \For{each $x_i \in X_{sample}$}
        {
            Find confidence distribution on discrete states $w_i \in W_{sample}$: \\
            \hspace{0.3cm}{Sample a random set $X' \sim \mathcal{X}$; \\
            \hspace{0.3cm}\For{each $q' \in \mathcal{Q}$}{
                \hspace{0.2cm}$w_i(q') = \dfrac{|x' \in X' \cap G(q', q'')~\forall q''|}{|X'|}$ ; \\
                }}
            Find cost of divergence $c_i \in C' \subset \mathbb{R}$:
            $c_i(x_i)$ = $Hellinger(w_i, W^{k}_{full\_conf})$;\\
        }
        Define cost map on complete domain $\mathcal{X}$:
        $C_{complete}$(\textbf{x}) = Interpolate($C'$); \\
        Find best representative point in continuous state:
        $x^k_{best}$ = $global\_optimization$(\textbf{x}, $C_{complete}$); \\
        Append $x^k_{best}$ to $X_{cs\_goals}$;
        % Projected to boundary
        }
    \textbf{return} $X_{cs\_goals}$;
    }
\caption{ High-Level Plan $\boldsymbol{\xrightarrow{}}$ Continuous State Goals}
\label{alg:alg1}
"
133,1801.02124,"[ht]
%   \algsetup{linenosize=\tiny}
%    \scriptsize
  \caption{Inverse Reinforcement Learning in Zero-Sum Discounted Stochastic Games}
  \begin{algorithmic}[1]
    \Require
      \Statex Observed experts demonstrations $\mathcal{D} = \{(s_i,a^f_i,a^g_i)\text{ }|\text{ }i = 1,2,\ldots,N_{\mathcal{D}}\}$;
      \Statex Positive integers $K_{R},I_{R}$; Nash Equilibrium threshold $\tau$; learning rate $\lambda$.
    \Statex
\Statex \textbf{Initialize:} Parameters $\theta_R$ for the reward function, and $\theta_f,\text{ }\theta_g$ to parametrize $f_{\theta_f}(a|s),\text{ }g_{\theta_g}(a|s)$ for Nash Equilibrium policies

\For{$\text{i} = 1, 2, 3, \ldots $} \Comment{policy step}

\State Update $\theta_f$ to find Nash Equilibrium strategy for $f$ under current $R_{\theta_R}$, return also $\hat{v}^{f,g^{\text{best}}}$ 
\State Update $\theta_g$ to find Nash Equilibrium strategy for $g$ under current $R_{\theta_R}$, return also $\hat{v}^{f^{\text{best}},g}$ 

\If{$i\text{ } \% \text{ }K_{R} = 0 $}

\If{$\hat{v}^{f^{\text{best}},g} - \hat{v}^{f,g^{\text{best}}} < \tau$ } \Comment{check performances of Nash Equilibrium policies}

\For{$\text{j} = 1,2,3,\ldots,I_R$} \Comment{reward step}
\State Sample one observation from $\mathcal{D}$: $(s,a^{E,f},a^{E,g})$
\State Use \eqref{sample_1and2} to obtain $\{s^{f^*(R_{\theta_R}),E}_1,s^{f^*(R_{\theta_R}),E}_2,\ldots,s^{f^*(R_{\theta_R}),E}_T\}, \{s^{E,g^*(R_{\theta_R})}_1,s^{E,g^*(R_{\theta_R})}_2,\ldots,s^{E,g^*(R_{\theta_R})}_T\}$

\State $\hat{v}^{f}(\bar{\theta_R}) \leftarrow R_{\bar{\theta_R}}(s) + \sum_{t=1}^T \gamma^{t-1} R_{\bar{\theta_R}}\Big(s^{f^*(R_{\theta_R}),E}_t\Big)$
\State $\hat{v}^{g}(\bar{\theta_R}) \leftarrow R_{\bar{\theta_R}}(s) + \sum_{t=1}^T \gamma^{t-1} R_{\bar{\theta_R}}\Big(s^{E,g^*(R_{\theta_R})}_t\Big)$
\State $\theta_R \leftarrow \theta_R - \lambda\nabla_{\bar{\theta_R}}\Big( \hat{v}^{f}(\bar{\theta_R}) - \hat{v}^{g}(\bar{\theta_R}) + \phi(\bar{\theta_R}) \Big)|_{\bar{\theta_R} = \theta_R}$
\EndFor

\EndIf


% \State Sample one observation from $\mathcal{D}$: $(s,a^{E,f},a^{E,g})$
% \State Use \eqref{sample_1} to obtain a sequence $\{s^{f^*(R_{\theta_R}),E}_1,s^{f^*(R_{\theta_R}),E}_2,\ldots,s^{f^*(R_{\theta_R}),E}_T\}$ 
% \State Use \eqref{sample_2} to obtain a sequence $\{s^{E,g^*(R_{\theta_R})}_1,s^{E,g^*(R_{\theta_R})}_2,\ldots,s^{E,g^*(R_{\theta_R})}_T\}$

% \State $\hat{v}^{f}(\bar{\theta_R}) \leftarrow R_{\bar{\theta_R}}(s) + \sum_{t=1}^T \gamma^{t-1} R_{\bar{\theta_R}}\Big(s^{f^*(R_{\theta_R}),E}_t\Big)$
% \State $\hat{v}^{g}(\bar{\theta_R}) \leftarrow R_{\bar{\theta_R}}(s) + \sum_{t=1}^T \gamma^{t-1} R_{\bar{\theta_R}}\Big(s^{E,g^*(R_{\theta_R})}_t\Big)$
% \State $\theta_R \leftarrow \theta_R - \lambda\nabla_{\bar{\theta_R}}\Big( \hat{v}^{f}(\bar{\theta_R}) - \hat{v}^{g}(\bar{\theta_R}) + \phi(\bar{\theta_R}) \Big)|_{\bar{\theta_R} = \theta_R}$
\EndIf



\EndFor
    
  \end{algorithmic}
"
134,1801.02124,"[ht]
  \caption{Adversarial Training Algorithm for Solving $f^*(R)$ in Zero-Sum Games}
  \begin{algorithmic}[1]
    \Require
      \Statex Integers $K_g,K_{\text{cycle}}, K_{\text{refresh}}$; learning rates $\lambda_g$, $\lambda_f$; horizon length $T$.
    \Statex
\State \textbf{Initialize: } Parameters $\theta_f,\theta_g$ for policy models and $\theta_v$ for state value model.
\State Set $\theta^{\text{target}}_f \leftarrow \theta_f$, $\theta^{\text{target}}_g \leftarrow \theta_g$, and $\theta^{\text{target}}_v \leftarrow \theta_v$ .

\For{$i = 1, 2, 3, .... $}

\State Randomly initialize the starting state $s_0$.
\State From initial state $s_0$, run $f_{\theta^{\text{target}}_f}(a|s),\text{ } g_{\theta^{\text{target}}_g}(a|s)$ for $T$ steps
\State Calculate estimated advantages for player $f$ and $g$: $\hat{A}^{f/g}_0,\hat{A}^{f/g}_1,...,\hat{A}^{f/g}_{T-1}$.


\If{$i\text{ } \% \text{ }K_{\text{cycle}} \leq K_g  $} \Comment{$g$ step}
\State $\theta_g \leftarrow \theta_g - \lambda_g \nabla_{\theta} L^{g,\text{CLIP}}(\theta)|_{\theta = \theta_g}$
\State $\theta_{v,f} \leftarrow \theta_{v,f} - \lambda_f \nabla_{\theta} L^v(\theta,\theta^\prime)|_{\theta = \theta_{v,f},\theta^\prime = \theta_{v,g}}$
\State $\theta_{v,g} \leftarrow \theta_{v,g} - \lambda_g \nabla_{\theta^\prime} L^v(\theta,\theta^\prime)|_{\theta = \theta_{v,f},\theta^\prime = \theta_{v,g}}$
\Else \Comment{$f$ step}
\State  $\theta_f \leftarrow \theta_f - \lambda_f \nabla_{\theta} L^{f,\text{CLIP}}(\theta)|_{\theta = \theta_f}$
\EndIf

\If{$i\text{ } \% \text{ }K_{\text{refresh}}\text{ } = 0  $} \Comment{refresh target networks}
\State Set $ \theta_f \leftarrow \theta^{\text{target}}_f$, $\theta_{g}  \leftarrow \theta^{\text{target}}_{g}$, $\theta_{v,f} \leftarrow  \theta^{\text{target}}_{v,f} $ , and $\theta_{v,g} \leftarrow  \theta^{\text{target}}_{v,g} $ .
\EndIf



\EndFor
  \end{algorithmic}
"
135,1801.02124,"
% \caption{Adversarial Training Algorithm for Solving Nash Equilibrium in Zero-Sum Games }
% \begin{algorithmic}[1] 
% \State \textbf{Initialize: } Parameters $\theta_f,\theta_g$ for policy models and $\theta_v$ for state value model.
% \State Set $\theta^{\text{target}}_f \leftarrow \theta_f$, $\theta^{\text{target}}_g \leftarrow \theta_g$, and $\theta^{\text{target}}_v \leftarrow \theta_v$ .

% \For{$i = 1, 2, 3, .... $}

% \State Randomly initialize the starting point $s_0$.
% \State Run $f_{\theta^{\text{target}}_f}(a|s),\text{ } g_{\theta^{\text{target}}_g}(a|s)$ for $T$ steps
% \State Calculate estimated advantages for player $g$: $\hat{A}^f_0,\hat{A}^f_1,...,\hat{A}^f_{T-1}$


% \State Optimize surrogate loss function $L^{g,\text{CLIP}}$ w.r.t. $\theta_{g}$ \Comment{$g$ step}
% \State Optimize state value estimation loss $L^v$ w.r.t. $\theta_v$

% \If{$i\text{ } \% \text{ }K_{\text{f}} = 0  $} \Comment{$f$ step}
% \State Optimize surrogate loss function $L^{f,\text{CLIP}}$ w.r.t. $\theta_f$
% \EndIf

% \If{$i\text{ } \% K_{\text{refresh}}\text{ } = 0  $} \Comment{refresh target networks}
% \State Set $ \theta_f \leftarrow \theta^{\text{target}}_f$, $\theta_{g}  \leftarrow \theta^{\text{target}}_{g}$, and $\theta_v \leftarrow  \theta^{\text{target}}_v $ .
% \EndIf



% \EndFor

% \end{algorithmic}
% "
136,1806.01947,"
	\caption{CAPA Algorithm (No Pruning)}\label{CAPA}
	\begin{footnotesize}
		\begin{tabular}[h]{ll}
			{\bf Input:} & A set of observations of the form, $(x_1,x_2,\ldots,x_n)$ where $x_i \in \mathbb{R}$. \\ & Penalty constants $\beta$ and $\beta'$ for the introduction of a collective and a point anomaly respectively \\ & A minimum segment length $l \geq 2$\\ {\bf Initialise:} & Set $C(0)=0$, $Anom(0)=NULL$. 
		\end{tabular}
		
		
		\begin{algorithmic}[1]
			\State $\hat{\mu} \gets MEDIAN(x_1,x_2,\ldots,x_n)$ \Comment{Obtain robust estimates of the mean and variance}
			\State $\hat{\sigma} \gets IQR(x_1,x_2,\ldots,x_n)$
			\For{$i \in \{1,...,n\} $}
			\State $x_i \gets \frac{x_i - \hat{\mu} }{\hat{\sigma}}$ \Comment{Centralise the data}
			\EndFor
			\For{$m \in \{1,...,n\} $}
			\State $C_1(m) \gets \min_{0 \leq k \leq m - l} \Big[ 
			C(k) + (m -k) \left[\log \left( \frac{1}{m-k}\sum_{t=k+1}^{m}\left(x_t - \bar{x}_{(k+1):m}\right)^2\right) + 1\right] + \beta \Big] $
			\Comment{Collective Anom.}
			\State $ s \gets \argmin_{0 \leq k \leq m - l} \Big[ 
			C(k) + (m -k) \left[\log \left( \frac{1}{m-k}\sum_{t=k+1}^{m}\left(x_t - \bar{x}_{(k+1):m}\right)^2\right) + 1\right] + \beta \Big] $
			\State $C_2(m) \gets 
			C(m-1) + x_m^2 $ \Comment{No Anomaly}
			\State $C_3(m) \gets  
			C(m-1) + 1 + \log \left( \gamma  + x_m ^2 \right) + \beta'
			\Big], $ \Comment{Point Anomaly}
			\State $C(m) \gets \min\left[C_1(m),C_2(m),C_3(m)\right] $
			\Switch{$\argmin\left[C_1(m),C_2(m),C_3(m)\right]$} \Comment{Select type of anomaly giving the lowest cost}
			\Case{$1$} : $Anom(m) \gets [Anom(s),(s+1,m) ]$ 
			\EndCase
			\Case{$2$} : $Anom(m) \gets Anom(m-1) $
			\EndCase
			\Case{$3$} : $Anom(m) \gets[ Anom(m-1),(m)] $
			\EndCase
			\EndSwitch
			\EndFor
		\end{algorithmic}
		{\bf Output} The points and segments recorded in $Anom(n)$
	\end{footnotesize}
"
137,1806.01946,"
\small
    \caption{AGILE Discriminator Training}
    \label{algo:discriminator}
    \begin{algorithmic}[1]
    \REQUIRE 
        The policy network $\pi_{\theta}$, the discriminator network $D_{\phi}$, the anticipated negative rate $\rho$, a dataset $\mathcal{D}$, a replay buffer $B$, the batch size $BS$, a stream of training instances $\mathcal{G}$, the episode length $T$, the rollout length $R$.
    \WHILE{Not Converged}
        \STATE Sample a training instance $(c, s_0) \in \mathcal{G}$.
        \STATE $t \leftarrow 0$
        \WHILE{t < T}
            \STATE Act with $\pi_{\theta}(c, s)$ and produce a rollout $(c, s_{t \ldots t+R})$.
            \STATE Add $(c, s)$ pairs from $(c, s_{t \ldots t+R})$ to the replay buffer $B$. Remove old pairs from $B$ if it is overflowing.
            \STATE Sample a batch $D_+$ of $BS / 2$ positive  examples from $\mathcal{D}$.
            \STATE Sample a batch $D_-$ of $BS / (2 \cdot (1 - \rho))$ negative $(c, s)$ examples from $B$.
            \STATE Compute $\kappa=D_{\phi}(c, s)$ for all $(c, s) \in D_-$ and reject the top $1 - \rho$ percent of $D_-$ with the highest~$\kappa$. The resulting $D_-$ will contain $BS / 2$ examples.
            \STATE Compute $\tilde{L}_D(\phi) = \frac{1}{BS} \sum\limits_{(c, s) \in D_-} - \log (1 - D_{\phi}(c, s)) + \sum\limits_{(c, g) \in D_+} - \log D_{\phi}(c_i, g_i)$.
            \STATE Compute the gradient $\frac{d\tilde{L}_D(\phi)}{d\phi}$ and use it to update $\phi$.
            \STATE Synchronise $\theta$ and $\phi$ with other workers. 
            \STATE $\textit{t} \leftarrow t + R$
        \ENDWHILE            
    \ENDWHILE
    \end{algorithmic}
"
138,1806.01946,"
\small
    \caption{AGILE Policy Training}
    \label{algo:policy}
    \begin{algorithmic}[1]
    \REQUIRE 
        The policy network $\pi_{\theta}$, the discriminator network $D_{\phi}$, a dataset $\mathcal{D}$, a replay buffer $B$, a stream of training instances $\mathcal{G}$, the episode length $T$.
    \WHILE{Not Converged}
        \STATE Sample a training instance $(c, s_0) \in \mathcal{G}$.
        \STATE $t \leftarrow 0$
        \WHILE{t < T}
            \STATE Act with $\pi_{\theta}(c, s)$ and produce a rollout $(c, s_{t \ldots t+R})$.
            \STATE Use the discriminator $D_{\phi}$ to compute the rewards $r_{\tau}=\left[ D_{\phi}(c, s_{\tau}) > 0.5 \right]$.
            \STATE Perform an RL update for $\theta$ using the rewards $r_{\tau}$. 
            \STATE Synchronise $\theta$ and $\phi$ with other workers. 
            \STATE $\textit{t} \leftarrow t + R$
        \ENDWHILE            
    \ENDWHILE    
    \end{algorithmic}
"
139,1803.01798,"[htb]
	% \SetAlgoNoLine
	\SetAlFnt{\small}
	\DontPrintSemicolon
	\SetKwInOut{Inputs}{Inputs}\SetKwInOut{Outputs}{Outputs}
	\Inputs{Training dataset $M_{\text{benign}}=\{\mathcal{X}_{1}, \cdots, \mathcal{X}_{N}\}$,  \\
			Training epochs for LSTM-Autoencoder \\  $Epoch_{AE}$ and GAN $Epoch_{GAN}$}
	\Outputs{Well-trained LSTM-Autoencoder and complementary GAN}

	initialize parameters in LSTM-Autoencoder and complementary GAN;

	$j \leftarrow 0$;

	\While{$j<Epoch_{AE}$}{
	\label{algr:start_ae}
		\ForEach {user $u$ in $M_{\text{benign}}$}{			
			% compute the last hidden vector $\mathbf{h}_T$ by LSTM (Eq. \ref{eq:en});
			% \label{line:encoder}
			compute the reconstructed sequence of user activities by LSTM-Autoencoder (Eq. \ref{eq:en}, \ref{eq:de}, and \ref{eq:re});
			\label{line:decoder}
			
			optimize the parameters in LSTM-Autoencoder with the loss function Eq. \ref{eq:ae_loss};
			\label{line:ae_optimize}
		}

		$j \leftarrow j+1$;
	}
	\label{algr:end_ae}

	$\mathcal{V}=\emptyset$;

	\ForEach {user $u$ in $M_{\text{benign}}$}{
	\label{algr:start_u}
		compute the benign user representation $\mathbf{v}_u$ by the encoder of LSTM-Autoencoder (Eq. \ref{eq:en}, \ref{eq:user});

		$\mathcal{V} += \mathbf{v}_u$;
	}
	\label{algr:end_u}

	$j \leftarrow 0$;

	\While{$j<Epoch_{GAN}$}{
	\label{algr:start_gan}
		\ForEach {benign user representation $\mathbf{v}_u$ in $\mathcal{V}$}{
			optimize the discriminator $D$ and generator $G$ with loss functions Eq. \ref{eq:d_loss}, \ref{eq:g_loss}, respectively;
		}
	}
	\label{algr:end_gan}
	\Return well-trained LSTM-Autoencoder and complementary GAN
\caption{Training One-Class Adversarial Nets}
\label{algr:train}
"
140,1803.01798,"[h]
	% \SetAlgoNoLine
	\DontPrintSemicolon
	\SetKwInOut{Inputs}{Inputs}\SetKwInOut{Outputs}{Outputs}
	\Inputs{Testing dataset $M_{\text{test}}=\{\mathcal{X}_{1}, \cdots, \mathcal{X}_{N}\}$,  \\
			Well-trained LSTM-Autoencoder and GAN}
	\Outputs{the user labels $\mathcal{Y}$ in $M_{\text{test}}$ }

	$\mathcal{Y}=\emptyset$;

	\ForEach {user $u$ in $M_{\text{test}}$}{

		compute the user representation $\mathbf{v}_u$ by the encoder in LSTM-Autoencoder (Eq. \ref{eq:en}, \ref{eq:user});
		\label{algr:ur}

		predict the label $\hat y_u$ of the user by $D(\mathbf{v}_u)$
		\label{algr:pr}

		$\mathcal{Y} += \hat y_u$
	}

	\Return the user labels $\mathcal{Y}$
\caption{Fraud Detection}
\label{algr:test}
"
141,1806.01678,"[tb]
	\caption{Dykstra's Method for Quadratic Programming}
	\begin{algorithmic}[5]
		%= \begin{bmatrix} \va_1 & \cdots & \va_M \end{bmatrix}^T
		\State \textbf{Input:} $\mA  \in \mathbb{R}^{N\times M}, \vb \in \mathbb{R}^M, \vc \in \mathbb{R}^N, \gamma > 0, \mW \in \mathbb{R}^{N\times N} (\text{diagonal, positive definite})$ 
		\State \textbf{Output:} $\hat{\vx} = \argmin_{\vx \in \mathcal{A}} Q(\vx)$ where $\mathcal{A} = \{ \vx \in \mathbb{R}^N: \mA \vx \leq \vb \}$ 
		\State $\vy := \textbf{0} \in \mathbb{R}^M$ 
		\State $\vx := -\gamma\mW^{-1}\vc$, $k := 0$
		\While{\emph{not converged}}
		\State $k := k+1$
		\State (Visit constraints cyclically): $i := (k-1) \bmod M + 1$ 
		\State (Perform correction step):  $\vx := \vx + y_i (\gamma\mW^{-1} \va_i)$ 
		where $\va_i$ is the $i$th row of $\mA$
		\State (Perform projection step): $\vx := \vx - \theta_i^+ (\gamma\mW^{-1} \va_i)$
		where $\theta_i^+ = \frac{ \max \{\va_i^T \vx - b_i, 0 \}}{\gamma \va_i^T \mW^{-1} \va_i }$
		\State (Update dual variables): $y_i := \theta_i^+ \geq 0$
		\EndWhile
	\end{algorithmic}
	\label{dykstra}
"
142,1806.01678,"[tb]
	\caption{MetricProjection$(i,j,k)$}
	\begin{algorithmic}[5]
		\State $t := \text{unique ID for $(i,j,k)$}$
		\State Obtain $(x_{ij},x_{ik}, x_{jk})$ and weights $(w_{ij},w_{ik}, w_{jk})$ from $\mX$ and $\mW_\gamma$
		\If{$y_t > 0$}
		\State $x_{ij} \leftarrow x_{ij} + y_t\frac{ x_{ij}}{w_{ij}}, \hspace{.1cm} x_{ik} \leftarrow x_{ik} + y_t\frac{x_{ik}}{w_{ik}}, \hspace{.1cm} x_{jk} \leftarrow x_{jk} + y_t  \frac{x_{jk}}{w_{jk} }$.
		\EndIf
		\State $\delta := x_{ij} - x_{ik} - x_{jk}$
		\If{$\delta > 0$}
		\State $\theta_t = \frac{\delta w_{ij}w_{ik}w_{jk}}{w_{ij}w_{ik}+w_{ij}w_{jk} + w_{ik}w_{ij}}$
		\State $x_{ij} \leftarrow x_{ij} - \theta_t\frac{ x_{ij}}{w_{ij}}, \hspace{.1cm} x_{ik} \leftarrow x_{ik} - \theta_t\frac{x_{ik}}{w_{ik}}, \hspace{.1cm} x_{jk} \leftarrow x_{jk} - \theta_t  \frac{x_{jk}}{w_{jk} }$.
		\State Store $y_t = \theta_t$
		\EndIf
	\end{algorithmic}
	\label{metproj}
"
143,1806.01619,"[tb]
   \caption{Bayesian Optimization}
   \label{alg:bayesian_optimization_workflow}
\begin{algorithmic}[1]
	\STATE {\bfseries Input:} surrogate model $\mathcal{M}$, acquisition function $\alpha$, search space $X$, initial training data $\mathcal{D}_{init}$, function $f$
   	
    \STATE {\bfseries Output:} optimum $\x_{opt} \in X$ of $f$
   	
    \STATE Initialize $\mathcal{D} = \mathcal{D}_{init}$
   	\WHILE{evaluation budget available}
   	
    \STATE Set $\mu(\cdot \vert \mathcal{D}), \sigma^2(\cdot \vert \mathcal{D}) \leftarrow \mathcal{M} \vert \mathcal{D}$ \textcolor{darkgray}{\small{\textit{$//$ Surrogate function returns predictive mean function and predictive variance function by fitting} $\mathcal{M}$ to $\mathcal{D}$}}
   	
    \STATE Maximize $\widehat{\x} =  \displaystyle\argmax_{\x \in X} \alpha(\mu(\x \vert \mathcal{D}),\sigma^2(\x \vert \mathcal{D}))$\\ \textcolor{darkgray}{\small{\textit{$//$ Acquisition function suggests next evaluation by maximization}}}
   	
    \STATE Evaluate $\hat{y}=f(\widehat{\x})$ \textcolor{darkgray}{\small{\textit{$//$ Evaluate the score of the point selected by the acquisition function}}}
    
   	\STATE Set $\mathcal{D} \leftarrow \mathcal{D} \cup \{(\widehat{\x}, \hat{y})\}$     \textcolor{darkgray}{\small{\textit{$//$ Update the training dataset by including the newly evaluated pair $(\widehat{\x}, \hat{y})$}}}
    
   	\ENDWHILE
\end{algorithmic}
\caption{Bayesian Optimization pipeline.}
"
144,1806.01619,"[tb]
%    \caption{Bayesian Optimization}
%    \label{alg:bayesian_optimization_workflow}
% \begin{algorithmic}[1]
% 	\STATE {\bfseries Input:} surrogate model $\mathcal{M}$, acquisition function $\alpha$, search space $X$, initial data $\mathcal{D}_{init}$, function $f$
%    	\STATE {\bfseries Output:} optimum $\x_{opt} \in X$ of $f$
%    	\newline
%    	\STATE Set $\mathcal{D} = \mathcal{D}_{init}$
%    	\WHILE{have evaluation budget}
%    	\STATE Obtain predictive mean function and predictive variance function by fitting $\mathcal{M}$ to $\mathcal{D}$ \vspace{-1em}\[\mu(\cdot \vert \mathcal{D}), \sigma^2(\cdot \vert \mathcal{D}) \leftarrow \mathcal{M} \vert \mathcal{D}\]
%    	\STATE Suggest next evaluation by maximization \vspace{-1em}\[\x_{next} =  \displaystyle\argmax_{\x \in X} \alpha(\mu(\x \vert \mathcal{D}),\sigma^2(\x \vert \mathcal{D}))\]
%    	\STATE Evaluate $f$ at $\x_{next}$ \vspace{-1em}\[y=f(\x_{next})\]
%    	\STATE Update $\mathcal{D}$ \vspace{-1em}\[\mathcal{D} = \mathcal{D} \cup \{(\x_{next}, y)\} \]
%    	\ENDWHILE
% \end{algorithmic}
% "
145,1806.01579,"[!b]
	\small
	\begin{algorithmic}[1]
		\caption{Separate-and-conquer rule induction.}
		\label{alg:conquer}
		
		\Require
		$D(A,\delta)$---training dataset,
		\mincov---minimum number of yet uncovered examples that a new rule has to cover.
		\Ensure $R$---rule set.
		\State $D_{U} \gets D$	\Comment{set of uncovered examples}
		\State $R \gets \emptyset$ \Comment{start from an empty rule set}
		\Repeat
		\State $r \gets \emptyset$ \Comment{start from an empty premise}
		\State $r \gets \Call{Grow}{r, D, D_{U}, \mincov}$ \Comment{grow conditions}
		\State $r \gets \Call{Prune}{r,D}$ \Comment{prune conditions}
		\State $R \gets R \cup \{r\}$
		\State $D_{U} \gets D_{U}\setminus\Call{Cov}{r, D_U}$ \Comment{remove from $D_U$ examples covered by $r$}
		\Until{$D_{U} = \emptyset$}	
	\end{algorithmic}
"
146,1806.01579,"[!t]
	\begin{algorithmic}[1]
		\caption{Growing a rule.}
		\label{alg:grow}
		\Require
		$r$---input rule,
		$D$---training dataset,
		$D_{U}$---set of uncovered examples,
		\mincov---minimum number of previously uncovered examples that a new rule has to cover.
		\Ensure
		$r$---grown rule.
		
		\Function{Grow}{$r$, $D$, $D_U$, $mincov$}
		
		\Repeat \Comment{iteratively add conditions}
		\State $c_\textrm{best} \gets \emptyset$ \Comment{current best condition}
		\State $q_\textrm{best} \gets -\infty,\quad \textrm{cov}_\textrm{best} \gets -\infty$ \Comment{best quality and coverage}
		
		\State $D_{r} \gets$ \Call{Cov}{$r$, $D$} \Comment{examples from $D$ satisfying $r$ premise}
		
		\For{$c \in$ \Call{GetPossibleConditions}{$D_r$}}
		\State $r_c \gets r \AND c$ \Comment{rule extended with condition $c$}
		\State $D_{r_c} \gets \Call{Cov}{r_c, D}$
		\If {$|D_{r_c} \cap D_U| \geq \mincov$} \Comment{verify coverage requirement}
		\State $q \gets$ \Call{Quality}{$D_{r_c}$, $D \setminus D_{r_c}$} \Comment{rule quality measure}
		
		\If {$q > q_\textrm{best}$ \textbf{or} ($q = q_\textrm{best}$ \textbf{and} $|D_{r_c}| > \textrm{cov}_\textrm{best}$)}
		\State $c_\textrm{best} \gets c,\quad q_\textrm{best} \gets q$,\quad $\textrm{cov}_\textrm{best} \gets |D_{r_c}|$
		\EndIf
		
		\EndIf
		\EndFor			
		\State $r \gets r \AND c_\textrm{best}$
		\Until{$c_\textrm{best} = \emptyset$}
		\State \Return{$r$}
		\EndFunction
	\end{algorithmic}
"
147,1806.01579,"[!p]
	\small
	\begin{algorithmic}[1]
		\caption{Guided rule induction. Function \textsc{Grow} operate as in the fully automatic algorithm, but it excludes  attributes already present in $r$, forbidden attributes, and conditions intersecting with forbidden conditions.}
		\label{alg:conquer-expert}
		
		\Require
		$D(A,\delta)$---training dataset, $\Rexp$---set of initial rules,
		\Statex $\Cp$/$\Ap$---multiset of preferred conditions/attributes,
		\Statex $\Cm$/$\Am$---set of forbidden conditions/attributes,
		\Statex $K_C$/$K_A$---maximum number of preferred conditions/attributes per rule,
		\Statex $\Epref$/$\Eauto$---extend initial rules with preferred/automatic conditions (bool),
		\Statex $\Ypref$/$\Yauto$---induce new rules with preferred/automatic conditions (bool),
		\Statex \mincov---minimum number of yet uncovered examples that a new rule has to cover.
		
		\Ensure
		$R$---Rule set.
		
		\State $R \gets \emptyset$ \Comment start from empty rule set
		\State $D_{U} \gets D$	\Comment{set of uncovered examples}
		\For{$r \in \Rexp$}  \Comment{iterate over initial rules}
		
		\If{ $\Epref$} \Comment extend with preferred conditions/attributes
		\State $r \gets \Call{GuidedGrow}{r, D, D_U, \mincov, \Cp, \Ap, K_C, K_A}$
		\EndIf
		\If{$\Eauto$} \Comment extend with automatic conditions
		\State $r \gets \Call{Grow}{r, D, D_U, \mincov, \Cm, \Am}$ \Comment exclude forbidden knowledge
		\State $r \gets \Call{Prune}{r,D}$ \Comment{prune the rule}
		\EndIf
		\State $R \gets R \cup \{r\}$ \Comment add rule to rule set
		\State $D_{U} \gets D_{U}\setminus\Call{Cov}{r, D_U}$	
		\EndFor
		
		\If{$\Ypref$ \textbf{or} $\Yauto$} \Comment induce non-user rules
		\While{$D_{U} \neq \emptyset$}
		\State $r \gets \emptyset$ \Comment start from empty rule  		
		\If{$\Ypref$}
		\State $r \gets \Call{GuidedGrow}{r, D, D_U, \mincov, \Cp, \Ap, K_C, K_A}$
		\EndIf
		\If{$\Yauto$}
		\State $r \gets \Call{Grow}{r, D, D_U, \mincov, \Cm, \Am}$
		\State $r \gets \Call{Prune}{r,D}$ \Comment{prune the rule}
		\EndIf
		\State $R \gets R \cup \{r\}$ \Comment add rule to rule set
		\State $D_{U} \gets D_{U}\setminus\Call{Cov}{r, D_U}$	
		\EndWhile{}	
		\EndIf
		\State \Return{$R$}
	\end{algorithmic}
"
148,1806.01579,"[!b]
	\small
	\begin{algorithmic}[1]
		\caption{Growing a rule using preferred conditions and attributes.  }
		\label{alg:grow-expert}
		
		\Require
		$r$---input rule, $D(A, \delta)$---training dataset,
		$D_{U}$---set of uncovered examples,
		\Statex $\Cp$/$\Ap$---multiset of preferred conditions/attributes,
		\Statex $K_C$/$K_A$---maximum number of preferred conditions/attributes per rule,
		\Statex \mincov---minimum number of yet uncovered examples that a new rule has to cover.
		
		\Ensure
		$r$---grown rule.
		
		\Function{GuidedGrow}{$r, D, D_U, \mincov, \Cp, \Ap, K_C, K_A$}
		\State $A_\textrm{free} \gets A \setminus \{ \Call{Attr}{r} \}$  \Comment exclude attributes already present in the rule
		
		\State $k_C \gets 0,\quad k_A \gets 0$ \Comment initialize counters with 0
		
		\Repeat \Comment{analyze preferred conditions}
		\State $c_\textrm{best} \gets \emptyset$ \Comment{current best condition}	
		
		\For{$c \in \Cp$}  \Comment{analyze all preferred conditions}
		\If{\Call{Attr}{$c$} $\in A_\textrm{free}$ \textbf{and} $|\Call{Cov}{r \AND c, D_U}| \geq \mincov$ \textbf{and}
			\par \hskip\algorithmicindent $\quad(r \AND c)$ \textrm{is better than} $(r \AND c_{best})$}
		\State $c_{best} \gets c$
		\EndIf
		\EndFor
		
		\State $r \gets r \AND c_{best}$	\Comment add condition to the rule premise
		\State $\Cp \gets \Cp \setminus \{c_{best}\}$ \Comment remove preferred condition
		\State $A_\textrm{free} \gets A_\textrm{free} \setminus \{\Call{Attr}{c_{best}}\}$ \Comment remove used attribute
		\State $k_C \gets k_C + 1$
		
		\Until{$k_C = K_C$ \textbf{or} $c_\textrm{best} = \emptyset$}
		
		\Repeat \Comment{analyze preferred attributes}
		\State $c_\textrm{best} \gets \emptyset$ \Comment{current best condition}	
		\For{$a \in \Ap \cap A_\textrm{free}$}  
		\State $c \gets \Call{InduceBestCondition}{a, D, D_U}$
		\If{ $|\Call{Cov}{r \AND c, D_U}| \geq \mincov$ \textbf{and} 
			\par \hskip\algorithmicindent $\quad(r \AND c)$ \textrm{is better than} $(r \AND c_{best})$}
		\State 	$c_{best} \gets c$
		\EndIf
		\EndFor
		\State $r \gets r \AND  c_{best}$	\Comment add condition to the rule premise
		\State $\Ap \gets \Ap \setminus \{a\}$ \Comment remove preferred attribute
		\State $A_\textrm{free} \gets A_\textrm{free} \setminus\{a\}$ \Comment remove used attribute
		\State $k_A \gets k_A + 1$
	
		\Until{$k_A = K_A$ \textbf{or} $c_\textrm{best} = \emptyset$}
		
		\State \Return{$r$}
		\EndFunction
		% $c \AND {\bigcup \Cm} \neq \emptyset
	\end{algorithmic}
"
149,1806.01502,"[H]
	\caption{Homeo-heterostatic value gradients}
	\label{alg:hhvg}
	\begin{algorithmic}[1]
		\State \textbf{Variables} \par
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent 
			outer loop time $t$ \par
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent 
			gradient step counter $\ell, i, j, k$ \par
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent 
			state $\bs{s}^t := \bs{s}(t)$ and action $\bs{a}^t := \bs{a}(t)$ \par
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent 
			learning rate $\lambda^\theta, \lambda^\psi, \lambda^\nu, \lambda^\varphi$ \par 
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent discount factor 
			$\gamma$ \par 
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent 
			experience pool $\mathcal D$

		\State \textbf{Models and parameters} \par 
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent 
			forward model $P(S^\prime|\bs{s}, \bs{a}; \theta)$ \par
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent 
			meta-model $Q(S^\prime|\bs{s};\psi)$ \par
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent 
			value apprximator $\hat{V}(\bs{s}; \nu)$ \par
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent 
			action policy $\pi(A|\bs{s}; \varphi)$

		\State \textbf{Objectives} \par
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent
			forward-model learning $\mathcal{L}_{fm}(\theta)$ \par
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent
			meta-model learning $\mathcal{L}_{mm}(\psi)$ \Comment{Eq.\ref{eq:mm-loss}} \par
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent
			value learning $\mathcal{L}_{vf}(\nu)$ \Comment{Eq.\ref{eq:vf-loss}} \par
		\Statex \hskip\algorithmicindent\hskip\algorithmicindent
			policy learning $\mathcal{L}_{ap}(\varphi)$ \Comment{Eq.\ref{eq:policy-loss}} \par
		
		\For {$t=0\dots T$}
		\State From $\bs{s}^t$, sample action $\bs{a}^t \sim \pi(\cdot | \bs{s}^t;\varphi)$
		\State Perform $\bs{a}^t$ and advance to $\bs{s}^{t+1}$
		\State Insert tuple $\left( \bs{s}^t, \bs{a}^t, \pi(\bs{a}^t|\bs{s}^t), \bs{s}^{t+1} \right)$ into $\mathcal D$
		
		\State Sample $\mathcal D$ and train forward model: \par
		\State \hskip\algorithmicindent 
			$\mathcal{L}_{fm}(\theta) := 
			\mathcal{L}(\bs{s}^\prime, \bs{a}, \bs{s}; \theta) = 
			\Vert \bs{s}^\prime - f(\bs{a}, \bs{s}; \theta) \Vert^2$ \Comment{Eq.\ref{eq:imp-fm}}\par
		\State \hskip\algorithmicindent 
			$\theta^{(\ell+1)} \gets \theta^{(\ell)} - 
			\lambda_\theta \nabla_\theta \mathcal{L}_{fm}(\theta^{(\ell)})$ \par
		
		\State Value learning ($M$ updates, see Algorithm \ref{alg:fpeval})
		
		\State Sample $\mathcal D$ and perform devaluation: \par
		\State \hskip\algorithmicindent 
		$\psi^{(i+1)} \gets \psi^{(i)} - 
		\lambda_\psi \nabla_\psi \mathcal{L}_{mm}(\psi^{(i)})$
		
		
		\State Sample $\mathcal D$ and train action policy:
		\State \hskip\algorithmicindent
			evaluate $R_\psi^{(i+1)} = \mathcal{L}_{mm}(\psi^{(i)}) - 
			\mathcal{L}_{mm}(\psi^{(i+1)})$  \par 
		\State \hskip\algorithmicindent 
			evaluate $\hat{V}^\prime = \hat{V}(\bs{s}^\prime; \nu^{(j+M)})$ \par
		\State \hskip\algorithmicindent 
			$w \gets \pi(\bs{a}|\bs{s};\varphi^{(k)}) / {\pi(\bs{a}|\bs{s};\varphi^{(<k)})}$ 
		\State \hskip\algorithmicindent 
			$\varphi^{(k+1)} \gets \varphi^{(k)} + \lambda^\varphi \nabla_\varphi w \mathcal{L}_{ap}(\varphi^{(k)})$
			given $R_\psi^{(i+1)}$, $\hat{V}^\prime$
		\EndFor
	\end{algorithmic}
"
150,1806.01502,"[H]
	\caption{Fitted Policy Evaluation (cf. \citet{svg})}
	\label{alg:fpeval}
	\begin{algorithmic}[1]
		\State \textbf{Given} \par 
		\hskip\algorithmicindent outer loop time $t$ \par 
		\hskip\algorithmicindent experience pool $\mathcal D$ \par 
		\hskip\algorithmicindent value function $V(\bs{s}; \nu^{(j)})$ \par 
		\hskip\algorithmicindent gradient step counter $i$, $j$, $k$
		\State Clone parameter $\tilde\nu \gets \nu^{(j)}$
		\For {$m = 1\dots M$}
		\State Sample 
			$\left(\bs{s}^\tau, \bs{a}^\tau, \pi(\bs{a}^\tau|\bs{s}^\tau; \varphi^{(<k)}), 	\bs{s}^{\tau+1}\right)$ 
			from $\mathcal D$ ($\tau < t$)
		\State Evaluate 
			$R_\psi^{(i+1)}=\mathcal{L}_{mm}(\psi^{(i)}) - \mathcal{L}_{mm}(\psi^{(i+1)})$
		\State $y = R_\psi^{(i+1)} + \gamma \hat{V}(\bs{s}^{\tau+1}; \tilde\nu)$ 
		\State $w = \pi(\bs{a}^\tau|\bs{s}^\tau;\varphi^{(k)}) / {\pi(\bs{a}^\tau|\bs{s}^\tau;\varphi^{(<k)})}$
		\State Apply updates 
			$\nu^{(j+m)} \gets \nu^{(j+m-1)} - 
			\nabla_\nu \frac w2 \left( y - V(\bs{s}; \nu^{(j+m-1)})\right)^2$
		\State Every $C$ updates, $\tilde\nu \gets \nu^{(j+m)}$
		\EndFor
	\end{algorithmic}
"
151,1806.01363,"[t!]
  \caption{IDVQ}
  \label{alg:idvq}
  \begin{algorithmic}
    \Inputs{%
      $\trainset$: ~training set, $\orig \in \trainset$  \\
      $\dict$: ~current dictionary\\
      $\delta$: ~~minimal aggregated residual for inclusion\\
    }
    \Initialize{%
      $\dict \gets \varnothing$
        \Comment{dictionary initialized empty}
    }
    \For{$\orig \text{ in } \trainset$}
      \State $\resids \gets \orig$
        \Comment{residual information to encode}
      \State $\code \gets DRSC(\orig, \dict, \epsilon, \Omega)$
        \Comment{$\epsilon$ and $\Omega$ given}
      \State $\recres \gets \code \dict$
      \State $\recerr \gets \resids - \recres$
      \State $\recerrel_i \gets \max(0, \recerrel_i),~\forall \recerrel_i \in \recerr$
        \Comment{remove artifacts}
      \If{$\Sigma|\recerr| > \delta$}
        \State $\dict \append \recerr$
          \Comment{append $\recerr$ to $\dict$}
      \EndIf
    \EndFor\\
    \Return $\dict$
  \end{algorithmic}
"
152,1806.01363,"[t!]
  \caption{DRSC}
  \label{alg:drsc}
  \begin{algorithmic}
   \Inputs{%
      $\orig$: ~vector to encode (observation)\\
      $\dict$: ~dictionary trained with IDVQ\\
      $\epsilon$: ~~minimal aggregated residual loss\\
      $\maxncodes$: ~maximum nonzero elements in the code
    }
    \Initialize{%
      $\resids \gets X$
        \Comment{residual information to encode}\\
      $\code \gets \vec{0}$
        \Comment{output code}\\
      $\ncodes \gets 0$
        \Comment{non-zero elements in the code}
    }
    \While{$\Sigma|\resids| > \epsilon$ and $\ncodes < \maxncodes$}
      \State $\simils \gets \similfn(\resids, \centr_i), \forall \centr_i \in \dict$
      \State $\msc \gets \maxidx(\simils)$
      \State $\code_\msc \gets 1$
        \Comment{$\code = [\code_1 \dots \code_n]$}
      \State $\ncodes \gets \ncodes + 1$
      \State $\resids \gets \resids - \centr_\msc$
        \Comment{$\dict = [\centr_1 \dots \centr_n]$}
      \State $\resid_i \gets \max (0, \resid_i),~\forall \resid_i \in \resids$
    \EndWhile\\
    \Return $\code$
  \end{algorithmic}
"
153,1806.01242,"[tb]
   \caption{Forward prediction algorithm.}
   \label{sm:alg:forward_algorithm}
\begin{algorithmic}
   \STATE {\bfseries Input:}  trained GNs $\textrm{GN}_1$, $\textrm{GN}_2$ and normalizers $\textrm{Norm}_{in}$, $\textrm{Norm}_{out}$.
   \STATE {\bfseries Input:} dynamic state $\textbf{x}^{t_0}$ and actions applied $\textbf{x}^{t_0}$ to a system at the current timestep.
   \STATE {\bfseries Input:} system parameters  $\textbf{p}$
   \STATE Build static graph $G_s$ using $\textbf{p}$
   \STATE Build input dynamic nodes $N^{t_0}_d$ using $\textbf{x}^{t_0}$
   \STATE Build input dynamic edges $E^{t_0}_d$ using $\textbf{a}^{t_0}$
   \STATE Build input dynamic graph $G_d$ using $N^{t_0}_d$ and $E^{t_0}_d$
   \STATE Build input graph $G_i = \textrm{concat}(G_s, G_d)$
   \STATE Obtain normalized input graph $G^n_i = \textrm{Norm}_{in}(G_i)$
   \STATE Obtain graph after the first GN:
   $G^{'}= \textrm{GN}_1(G^n_i)$ 
   \STATE Obtain normalized predicted delta dynamic graph:
   $G^{*}= \textrm{GN}_2(\textrm{concat}(G^n_i, G^{'}))$ 
   \STATE Obtain normalized predicted delta dynamic nodes:
   ${\Delta}N^{n}_d= G^{*}\textrm{.nodes}$  
   \STATE Obtain predicted delta dynamic nodes:
   ${\Delta}N_d = \textrm{Norm}^{-1}_{out}({\Delta}N^{n}_d)$
   \STATE Obtain next dynamic nodes $N^{t_0+1}_d$ by updating $N^{t_0}_d$ with ${\Delta}N_d$
   \STATE Extract next dynamic state $\textbf{x}^{t_0+1}$ from $N^{t_0+1}_d$
   \STATE {\bfseries Output:} next system state $\textbf{x}^{t_0+1}$
\end{algorithmic}
"
154,1806.01242,"[tb]
   \caption{Forward prediction with System ID.}
   \label{sm:alg:sysid_forward}
\begin{algorithmic}
   \STATE {\bfseries Input:} trained parameter inference recurrent GN $\textrm{GN}_p$.
   \STATE {\bfseries Input:} trained GNs and normalizers from Algorithm \ref{sm:alg:forward_algorithm}.
   \STATE {\bfseries Input:} dynamic state $\textbf{x}^{t_0}$ and actions applied $\textbf{x}^{t_0}$ to a parametrized system at the current timestep.
   \STATE {\bfseries Input:} a 20-step sequence of observed dynamic states $x^\textrm{seq}$ and actions $x^\textrm{seq}$ for same instance of the system.
   
   \STATE Build dynamic graph sequence $G^\textrm{seq}_d$ using
   $x^\textrm{seq}_i$ and $a^\textrm{seq}_i$
   \STATE Obtain empty graph hidden state $\textrm{G}_\textrm{h}$.
   \FOR{each graph $G^t_d$ in $G^\textrm{seq}_d$}
   \STATE $\textrm{G}_{o},  \textrm{G}_\textrm{h} = \textrm{GN}_p(\textrm{Norm}_{in}(G^t_d), \textrm{G}_\textrm{h})$,
   \ENDFOR
   \STATE Assign $\textrm{G}_{ID} =\textrm{G}_{o}$
   \STATE Use $\textrm{G}_{ID}$ instead of $G_s$ in Algorithm \ref{sm:alg:forward_algorithm} to obtain $\textbf{x}^{t_0+1}$ from $\textbf{x}^{t_0}$ and $\textbf{x}^{t_0}$
   \STATE {\bfseries Output:} next system state $\textbf{x}^{t_0+1}$
\end{algorithmic}
"
155,1806.01242,"[t]
   \caption{One step of the training algorithm}
   \label{sm:alg:training_one_step}
\begin{algorithmic}
   \STATE {\bfseries Before training:}  initialize weights of GNs $\textrm{GN}_1$, $\textrm{GN}_2$ and accumulators of normalizers $\textrm{Norm}_{in}$, $\textrm{Norm}_{out}$.
   \STATE {\bfseries Input:} batch of dynamic states of the system $\{\textbf{x}^{t_0}\}$ and actions applied $\{\textbf{a}^{t_0}\}$ at the current timestep
   \STATE {\bfseries Input:} batch of dynamic states of the system at the next timestep $\{\textbf{x}^{t_0+1}\}$ 
   \STATE {\bfseries Input:} batch of system parameters  $\{\textbf{p}_i\}$
   \FOR{each example in batch}
   \STATE Build static graph $G_s$ using $\textbf{p}_i$
   \STATE Build input dynamic nodes $N^{t_0}_d$ using $\textbf{x}^{t_0}$
   \STATE Build input dynamic edges $E^{t_0}_d$ using $\textbf{a}^{t_0}$
   \STATE Build output dynamic nodes $N^{t_0+1}_d$ using $\textbf{x}^{t_0+1}$
   \STATE Add noise to input dynamic nodes $N^{t_0}_d$
   \STATE Build input dynamic graph $G_d$ using $N^{t_0}_d$ and $E^{t_0}_d$
   \STATE Build input graph $G_i = \textrm{concat}(G_s, G_d)$
   
   \STATE Obtain target delta dynamic nodes $\Delta N{'}_{d}$ from $N^{t_0+1}_d$ and $N^{t_0}_d$
   \STATE Update $\textrm{Norm}_{in}$ using $G_i$
   \STATE Update $\textrm{Norm}_{out}$ using ${\Delta}N_{d}$
   
   \STATE Obtain normalized input graph $G^n_i = \textrm{Norm}_{in}(G_i)$
   \STATE Obtain normalized target nodes:
   ${\Delta}N^{n'}_d= \textrm{Norm}_{out}({\Delta}N{'}_{d})$
   \STATE Obtain normalized predicted delta dynamic nodes:
   ${\Delta}N^{n}_d= \textrm{GN}_2(\textrm{concat}(G^n_i, \textrm{GN}_1(G^n_i)))\textrm{.nodes}$  
   \STATE Calculate dynamics prediction loss between ${\Delta}N^{n}_d$ and ${\Delta}N^{n'}_d$.
   \ENDFOR
   \STATE Update weights of $\textrm{GN}_1$, $\textrm{GN}_2$ using Adam optimizer on the total loss with gradient clipping.
\end{algorithmic}
"
156,1806.01242,"[t]
   \caption{End-to-end training algorithm for System ID.}
   \label{sm:alg:sysid_training}
\begin{algorithmic}
   \STATE {\bfseries Before training:} initialize weights of parameter inference recurrent GN $\textrm{GN}_p$, as well as weights from Algorithm \ref{sm:alg:training_one_step}.
   \STATE {\bfseries Input:} a batch of 100-step sequences with dynamic states $\{x^\textrm{seq}_i\}$ and actions $\{x^\textrm{seq}_i\}$
   \FOR{each sequence in batch}
   \STATE Pick a random 20-step subsequence $x^\textrm{subseq}_i$ and $a^\textrm{subseq}_i$.
   \STATE Build dynamic graph sequence $G^\textrm{subseq}_d$ using
   $x^\textrm{subseq}_i$ and $a^\textrm{subseq}_i$
   \STATE Obtain empty graph hidden state $\textrm{G}_\textrm{h}$.
   \FOR{each graph $G^t_d$ in $G^\textrm{subseq}_d$}
   \STATE $\textrm{G}_{o},  \textrm{G}_\textrm{h} = \textrm{GN}_p(\textrm{Norm}_{in}(G^t_d), \textrm{G}_\textrm{h})$,
   \ENDFOR
   \STATE Assign $\textrm{G}_{ID} =\textrm{G}_{o}$
   \STATE Pick a different random timestep $t_0$ from  $\{x^\mathrm{seq}_i\}$, $\{x^\textrm{seq}_i\}$ 
   \STATE Apply Algorithm \ref{sm:alg:training_one_step} to timestep $t_0$ using final $\textrm{G}_{ID}$ instead $G_s$ to obtain the dynamics prediction loss.
   \ENDFOR
   \STATE Update weights of $\textrm{GN}_p$, $\textrm{GN}_1$, $\textrm{GN}_2$ using Adam optimizer on the total loss with gradient clipping.
\end{algorithmic}
"
157,1806.01242,"[tb]
   \caption{MPC algorithm}
   \label{sm:alg:MPC}
\begin{algorithmic}
   \STATE {\bfseries Input:} initial system state $\textbf{x}^{0}$,
   \STATE {\bfseries Input:} randomly initialized sequence of actions $\{\textbf{a}^{t}\}$.
   \STATE {\bfseries Input:} pretrained dynamics model $M$ such
   
   $\textbf{x}^{t_0+1} = M(\textbf{x}^{t_0}, \textbf{a}^{t_0})$
   \STATE {\bfseries Input:}  Trajectory cost function $L$ such
   
   $c = C(\{\textbf{x}^{t}\}, \{\textbf{a}^{t}\})$
   \FOR{a number of iterations}
   \STATE $\textbf{x}^{0}_r=\textbf{x}^{0}$
   \FOR{t in range(0, horizon)}
     \STATE $\textbf{x}^{t+1}_r=M(\textbf{x}^{t}_r, \textbf{a}^{t})$
   \ENDFOR
   \STATE Calculate trajectory cost $c = C(\{\textbf{x}_r^{t}\}, \{\textbf{a}^{t}\})$
   \STATE Calculate gradients $\{\textbf{g}_a^{t}\} = \frac{\partial c}{\partial \{\textbf{a}^{t}\}}$
   \STATE Apply gradient based update to $\{\textbf{a}^{t}\}$
   \ENDFOR
   \STATE {\bfseries Output:} optimized action sequence $\{\textbf{a}^{t}\}$ 
\end{algorithmic}
"
158,1806.01151,"[h]
%         \caption{Procedure to create the set of prunings starting from a given formula.}
%         \label{prunings}      	
%         \begin{algorithmic}
%         \Function {Prunings}{$h$}
%         \State $\mathcal{H} \leftarrow \mathcal{H} + \{h\}$ \Comment{init set of prunings}
%         \For {$l \in \Call{leaves}{h}$}
%         	\State $h^{p} \leftarrow$ \Call{prune}{$h,l$} \Comment{prune leaf $l$ from $h$}
% 			\State $\mathcal{H} \leftarrow \mathcal{H} +$ \Call{Prunings}{$h^{p}$}
%         \EndFor
%         \State\Return $\mathcal{H}$
%         \EndFunction
%         \end{algorithmic}
%       "
159,1801.04260,"
\caption{My algorithm}\label{euclid}
\begin{algorithmic}[1]
    \State $\textit{central\_idx} \gets \ceil{(f_W \cdot f_H \cdot f_D) / 2}$
    \State $\textit{current\_idx} \gets 1$
    \State $\textit{mask} \gets f_W \times f_H \times f_D \text{-dimensional matrix of zeros}$
    \For {$d \in \{1, \dots, f_D\}$}
        \For {$h \in \{1, \dots, f_H\}$}
            \For {$w \in \{1, \dots, f_W\}$}
                \If {$\textit{current\_idx} < \textit{central\_idx}$} \label{alg:maskmaker:lineLT}
                    \State $mask(w, h, d) = 1$
                \Else
                    \State $mask(w, h, d) = 0$
                \EndIf
                \State $\textit{current\_idx} \gets \textit{current\_idx} + 1$
            \EndFor
        \EndFor
    \EndFor
\end{algorithmic}
\caption{Constructing 3D Masks}
\label{alg:maskmaker}
"
160,1705.06366,"[H]
 \caption{Generative Goal Learning}
 \label{alg:overall}
 \begin{algorithmic}
 \STATE{\bfseries Input:} Policy $\pi_0$
 \STATE{\bfseries Output:} Policy $\pi_N$
%  \Output{Policy $\pi_N$}
 
 \STATE $(G, D) \leftarrow \texttt{initialize\_GAN}()$
 
 \STATE $goals_{\rm old} \leftarrow \varnothing$
 
 \FOR{$i \leftarrow \ 1$ {\bfseries to} $N$}
 \STATE $z \leftarrow \texttt{sample\_noise}(p_z(\cdot))$\;
  
  \STATE $goals \leftarrow G(z) \cup \texttt{sample}(goals_{\rm old})$\;
    
  \STATE $\pi_i \leftarrow \texttt{update\_policy}(goals, \pi_{i-1})$\;
  
  \STATE $returns \leftarrow \texttt{evaluate\_policy}(goals, \pi_i)$\;
  
  \STATE $labels \leftarrow \texttt{label\_goals}(returns)$
  
  \STATE $(G, D) \leftarrow \texttt{train\_GAN}(goals, labels, G, D)$\;
    
  \STATE $goals_{\rm old} \leftarrow \texttt{update\_replay}(goals)$
 \ENDFOR 
 \end{algorithmic}
"
161,1705.06366,"
\caption{Generative Goal with Sagg-RIAC}
\label{alg:overall-saggRIC}
\begin{algorithmic}
 \STATE{\bfseries Hyperparameters:} window size $\zeta$, tolerance threshold $\epsilon_{max}$, competence threshold $\epsilon_{C}$, maximum time horizon $t_{max}$, number of new goals $N_{new}$, maximum number of goals $g_{max}$, mode proportions $(p_1, p_2, p_3)$\;
 \STATE{\bfseries Input:} Policy $\pi_{\theta_0}(s_{start}, y_g)$, goal bounds $B_Y$, reset position $s_{rest}$
 \STATE{\bfseries Output:} Policy $\pi_{\theta_N}(s_{start}, y_g)$
 
 \STATE \textbf{R} $\leftarrow \big\{ (R_0 , \Gamma_{R_0})\big\}$ where $R_0 = Region(B_Y)$, $\Gamma_{R_0}=0$\; 
 
 \FOR{$i \leftarrow \ 1$ \textbf{to} $N$}
 
     \STATE $goals \leftarrow $ \textbf{Self-generate} $N_{new}$ goals: $\{y_j\}_{j=0\dots N_{new}}$\;
     
     \STATE $paths = [~]$\;
     
     \WHILE{$number\_steps\_in(paths) < batch\_size$}
        \STATE Reset $s_0 \leftarrow s_{rest}$\;
        \STATE $y_g \leftarrow {\rm Uniform}(goals)$\;
        \STATE $y_f$, $\Gamma_{y_g}$, $path \leftarrow \texttt{collect\_rollout}(\pi_{\theta_i}(\cdot,y_g), s_{reset})$\;
        \STATE $paths\texttt{.append}(path)$\;
        \STATE \textbf{UpdateRegions(R, }$y_f, 0)$  \;
        \STATE \textbf{UpdateRegions(R, }$y_g, \Gamma_{y_g})$\;
     \ENDWHILE
     \STATE $\pi_{\theta_{i+1}} \leftarrow $ train $\pi_{\theta_{i}}$ with TRPO on collected $paths$\;
 \ENDFOR
\end{algorithmic}
"
162,1806.00989,"[AIS]\label{algo:pop_monte_carlo}\ \\
\begin{minipage}{13cm}
\textbf{Inputs}: The number of stages $T \in \mathbb N^*$, the allocation policy $(n_t)_{t=1,\ldots T}\subset \mathbb N^*$, the sampler update procedure, the initial density $q_0$.
\medskip\hrule\medskip
Set $  S_0 = 0$, $N_0 = 0$. \noindent For $t$ in $1,\ldots T$ :
\begin{enumerate}[(i)]
\item (Explore) Generate $(x_{t,1},\ldots x_{t,n_t})$ from $ q_{t-1}$
\item (Exploit) 
\begin{enumerate}[(a)]
\item \begin{minipage}[t]{.4\textwidth} Update the estimate: \\ \end{minipage}\begin{minipage}[t]{.4\textwidth} \vspace{-.8cm} \begin{align*}
& S_{t} =  S_{t-1} + \sum_{i = 1} ^ {n_t} \frac{\varphi(x_{t,i}) }{ q_{t-1}( x_{t,i}) }\\
& N_t = N_{t-1} + n_t\\
& I_t = N _t^{-1}  S_{t} 
\end{align*}
\end{minipage}
\item Update the sampler $ q_{t}$
\end{enumerate}
\end{enumerate} 
\hrule
\end{minipage}
"
163,1806.00989,"[AIS at sample scale]\label{algo:pop_monte_carlobis} ~ \\ %\setcounter{algorithm}{2}~ \\
\begin{minipage}{13cm}
\textbf{Inputs}: The number of stages $T \in \mathbb N^*$, the allocation policy $(n_t)_{t=1,\ldots T}\subset \mathbb N^*$, the sampler update procedure, the initial density $q_0$.
\medskip\hrule\medskip
Set $  S_0 = 0$. For $j$ in $1,\ldots n$ :
\begin{enumerate}[(i)]
\item (Explore) Generate $x_j$ from $ q_{j-1}$
\item (Exploit) 
\begin{enumerate}[(a)]
\item \begin{minipage}[t]{.4\textwidth} Update the estimate: \\ \end{minipage}\begin{minipage}[t]{.4\textwidth} \vspace{-.8cm}\begin{align*}
& S_j =  S_{j-1} + \frac{\varphi(x_j)}{ q_{j-1}(x_j)}\\
& I_j = j^{-1}  S_j
\end{align*}
\end{minipage}
\item Update the sampler $ q_j$ whenever  $j\in \{ N_t = \sum_{s=1}^tn_s: t\ge 1\}$
\end{enumerate}
\end{enumerate}
\hrule
\end{minipage}
"
164,1806.00910,"[htbp]
	\DontPrintSemicolon
	\SetAlgoLined
	%\KwResult{Write here the result}
	\SetKwProg{Fn}{Function}{}{}
	\SetKwInOut{Input}{Inputs}\SetKwInOut{Output}{Output}
	\Input{\texttt{s} :- the seed keyword/term\\
		\texttt{w} :- the word vector model \\
		\texttt{ssl} :- integer specifying the limit of semantically close terms to search\\%; typically between\\ 100 and 10000 \\
		\texttt{lt} :- the levenshtein ratio threshold; range $[0,1]$ 
		%\texttt{max\_levels} :- the maximum number of\\ recursions allowed; optional \\
	}
	\Output{A set of spelling variants for \texttt{s}}
	\BlankLine
	\Fn{gen\_vars (\texttt{s},\texttt{w},\texttt{ssl},\texttt{lt})}{
		$\texttt{vars}\leftarrow \{\}$ \tcp*{empty dictionary}
		$\texttt{tte}\leftarrow [s]$  \tcp*{stack holding s}  
		$\texttt{aet}\leftarrow [ ]$ \tcp*{empty array}
		%$\texttt{level}\leftarrow 0$
		\BlankLine
		\While{not \texttt{tte} is \texttt{empty()}}{
			$\texttt{t}\leftarrow \texttt{tte.pop()}$\;
			$\texttt{aet.push(t)}$\;
			$\texttt{sls}\leftarrow \texttt{mostsimilar(w,t,ssl)}$\;%\tcp*{finds ssl most similar terms to s from w}
			\ForEach{\texttt{sl} in \texttt{sls}}{
				$\texttt{lss}\leftarrow \texttt{lev\_ratio(sl,s)}$\;
				\If{$\texttt{lss}\eqslantgtr \texttt{lt}$}{
					$\texttt{vars[s]} \stackrel{+}{=} \texttt{lss}$\;
					\If{$! \texttt{lss}$ in $\texttt{aet}$ and $! \texttt{lss}$ in $\texttt{tte}$  }{
						\texttt{tte.push(lss)}	
					}
				}
			}
			%\texttt{$level \stackrel{+}{=} 1$}	
		}
		\Return \texttt{vars}
	}
	\label{alg11}
	\caption{Spelling variant generation}
	
"
165,1806.00880,"  
  \caption{Disentagled Manifold Learning GAN with Prior Learning (DMGAN-PL)\label{alg:dmganpl}. Replace $V_d$ and $V_g$ according to Eq~\ref{eq:dmwgan_d} and Eq~\ref{eq:dmwgan_g} in lines~\ref{alg:dmganpl:vd}~and~\ref{alg:dmganpl:vg} for the Wasserstein version (DMWGAN-PL).}  
  \begin{algorithmic}[1]  
    \Require{$p(z)$ prior on $\Zc$, $m$ batch size, $k$ number of discriminator updates, $n_g$ number of generators, $\lambda$, $\lambda'$ and $\lambda''$ are weight coefficients, $\alpha$ is decay rate, and $t=0$}
    \Repeat
    \For{$j \in \{ 1 \dots k\}$}
        \Sample{$\{x^i\}_{i=1}^m$}{$p_r(x)$} \Comment{A batch from real data}
        \Sample{$\{z^i\}_{i=1}^m$}{$p(z)$} \Comment{A batch from $\Zc$ prior}
        \Sample{$\{c^i\}_{i=1}^m$}{$r(c; \zeta)$} \Comment{A batch from generator's prior}
        \Let{$\{x_g^i\}_{i=1}^m$}{$G(z^i;\theta_{c^i})$} \Comment{Generate batch using selected generators}    
        %\Let{$x_g^i$}{$G(z^i; \theta_{c^i})$}
        \Let{$g_w$}{$\nabla_w \frac{1}{m}\sum_i \left[ \ln D(x^i; w) + \ln (1-D(x_g^i; w)) + \lambda'V_{regul} \right]$ \label{alg:dmganpl:vd}}
        \Let{$w$}{Adam($w$, $g_w$)} \Comment{Maximize $V_d$ wrt. $w$}
    \EndFor
    \Sample{$\{x^i\}_{i=1}^m$}{$p_r(x)$}
    \Sample{$\{z^i\}_{i=1}^m$}{$p(z)$}
    \Sample{$\{c^i\}_{i=1}^m$}{$r(c; \zeta)$}
    \Let{$\{x_g^i\}_{i=1}^m$}{$G(z^i;\theta_{c^i})$}
    \For{$j \in \{ 1 \dots n_g\}$}
        \Let{$g_{\theta_j}$}{$\nabla_{\theta_j} \frac{1}{m}\sum_i \left[ \ln D(x_g^i; w) - \lambda \ln Q(x_g^i; \gamma) \right]$} \Comment{$\theta_j$ is short for $\theta_{c^j}$ \label{alg:dmganpl:vg}}
        \Let{$\theta_j$}{Adam($g_{\theta_j}$, $\theta_j$)} \Comment{Maximize $V_g$ wrt. $\theta$}
    \EndFor
    \Let{$g_\gamma$}{$\nabla_\gamma \frac{1}{m} \sum_i \ln Q(x_g^i; \gamma)$} 
    \Let{$\gamma$}{Adam($g_\gamma$, $\gamma$)} \Comment{Minimize $L_c$ wrt. $\gamma$}
    \Let{$g_\zeta$}{$\nabla_\zeta \frac{1}{m} \sum_i \left[ H(Q(x^i; \gamma), r(c; \zeta)) \right] - \alpha^t \lambda'' H(r(c; \zeta))$} 
    \Let{$\zeta$}{Adam($g_\zeta$, $\zeta$)} \Comment{Minimize $L_{prior}$ wrt. $\zeta$}
    \Let{$t$}{$t+1$}
    \Until{convergence.}
  \end{algorithmic}  
"
166,1806.00880,"[H] 
  \caption{Disconnected Manifold Learning WGAN with Prior Learning (DMWGAN-PL)\label{alg:dmganpl}. Replace $V_d$ and $V_g$ according to Eq~\ref{eq:dmgan_d} and Eq~\ref{eq:dmgan_g} in lines~\ref{alg:dmganpl:vd}~and~\ref{alg:dmganpl:vg} for the Modified GAN version (DMGAN-PL).}  
  \begin{algorithmic}[1]  
    \Require{$p(z)$ prior on $\Zc$, $m$ batch size, $k$ number of discriminator updates, $n_g$ number of generators, $\lambda=1$, $\lambda'=10$ and $\lambda''=1000$ are weight coefficients, $\alpha=0.999$ is decay rate, and $t=0$}
    \Repeat
    \For{$j \in \{ 1 \dots k\}$}
        \Sample{$\{x^i\}_{i=1}^m$}{$p_r(x)$} \Comment{A batch from real data}
        \Sample{$\{z^i\}_{i=1}^m$}{$p(z)$} \Comment{A batch from $\Zc$ prior}
        \Sample{$\{c^i\}_{i=1}^m$}{$r(c; \zeta)$} \Comment{A batch from generator's prior}
        \Let{$\{x_g^i\}_{i=1}^m$}{$G(z^i;\theta_{c^i})$} \Comment{Generate batch using selected generators}    
        %\Let{$x_g^i$}{$G(z^i; \theta_{c^i})$}
        \Let{$g_w$}{$\nabla_w \frac{1}{m}\sum_i \left[ D(x^i; w) - D(x_g^i; w) + \lambda'V_{regul} \right]$ \label{alg:dmganpl:vd}}
        \Let{$w$}{Adam($w$, $g_w$)} \Comment{Maximize $V_d$ wrt. $w$}
    \EndFor
    \Sample{$\{x^i\}_{i=1}^m$}{$p_r(x)$}
    \Sample{$\{z^i\}_{i=1}^m$}{$p(z)$}
    \Sample{$\{c^i\}_{i=1}^m$}{$r(c; \zeta)$}
    \Let{$\{x_g^i\}_{i=1}^m$}{$G(z^i;\theta_{c^i})$}
    \For{$j \in \{ 1 \dots n_g\}$}
        \Let{$g_{\theta_j}$}{$\nabla_{\theta_j} \frac{1}{m}\sum_i \left[ D(x_g^i; w) - \lambda \ln Q(x_g^i; \gamma) \right]$} \Comment{$\theta_j$ is short for $\theta_{c^j}$ \label{alg:dmganpl:vg}}
        \Let{$\theta_j$}{Adam($g_{\theta_j}$, $\theta_j$)} \Comment{Maximize $V_g$ wrt. $\theta$}
    \EndFor
    \Let{$g_\gamma$}{$\nabla_\gamma \frac{1}{m} \sum_i \ln Q(x_g^i; \gamma)$} 
    \Let{$\gamma$}{Adam($g_\gamma$, $\gamma$)} \Comment{Minimize $L_c$ wrt. $\gamma$}
    \Let{$g_\zeta$}{$\nabla_\zeta \frac{1}{m} \sum_i \left[ H(Q(x^i; \gamma), r(c; \zeta)) \right] - \alpha^t \lambda'' H(r(c; \zeta))$} 
    \Let{$\zeta$}{Adam($g_\zeta$, $\zeta$)} \Comment{Minimize $L_{prior}$ wrt. $\zeta$}
    \Let{$t$}{$t+1$}
    \Until{convergence.}
  \end{algorithmic}  
"
167,1802.07045,"[t]

\SetAlgoLined
\SetKwInput{Input}{input}\SetKwInput{Output}{output}

\hrulefill

\Input{ (incremental) A candidate transform (matrix) $\!f$}

\textbf{parameters:} number of tables $L$;$\;$ tolerance $t$;$\;$ cell dim. $c$;$\;$ parametrization dim. $\lambda$;
\vspace{-5pt}

\hrulefill

\hspace{-0pt}\parbox{.93\columnwidth}{

\vspace{-4pt}
\textbf{\underline{initialization}:}\vspace{4pt}\\
\ForEach{$i=1,\ldots,L$}
{
\vspace{4pt}
%
1. Initialize an empty hash table $T_i$.\\ 
%
2. Randomize offset $O_i\sim U([0,c]^\lambda)$ \vspace{-0pt}
%
}

\vspace{5pt}
\textbf{\underline{insertion and collision check}} for hypothesis $f$:\vspace{4pt}\\
\ForEach{$i=1,\ldots,L$}
{
\vspace{4pt}
%
1. Let $\textbf{v}$ be the embedding of $f$ \vspace{4pt}\\
%
2. The hash index for $\textbf{v}$ is: $\tau_{\textbf{v}}=hash\left(\left\lfloor\frac{\textbf{v}+O_i}{c}\right\rfloor\right)\;$\vspace{4pt} \\
%
3. If the cell $T_i[\tau_{\textbf{v}}]$ is occupied by a vector $\textbf{u}$, report a collision of $f$ if 
$\|\textbf{v}-\textbf{u}\|_\infty<t$
\vspace{4pt} \\%
4. Store $\textbf{v}$ in $T_i[\tau_{\textbf{v}}]$ \vspace{2pt}
%
}
}

\hrulefill

\medskip
\caption{Latent-RANSAC hypothesis handling.  \label{alg:filtering}  }
"
168,1701.01000,"[htb]         %Ëã·¨µÄ¿ªÊ¼
%\caption{The proposed algorithm}             %Ëã·¨µÄ±êÌâ
%\label{alg_THong}                  %¸øËã·¨Ò»¸ö±êÇ©£¬ÕâÑù·½±ãÔÚÎÄÖÐ¶ÔËã·¨µÄÒýÓÃ
%\begin{algorithmic}[1]
%\REQUIRE ~\\
%Sparsify dictionary matrix $\bm\Psi$, initialize $\bm\Phi_0$ to a random matrix, and the number of iterations: $Iter_{outer}$ and $Iter_{inner}$.
%\lastcon ~\\          %OUTPUT
%  Projection matrix $\bm\Phi$.
%\ENSURE
%\STATE {$\bm\Phi\leftarrow\bm\Phi_0$}
%\FOR {$l=1$ {\bf to} $Iter_{outer}$}
%\STATE ${_t}\leftarrow \bm\Psi^\mathcal T\bm\Phi^\mathcal T\bm\Phi\bm\Psi$\label{ap1}
%\STATE Update $\bm G_t$:
%\STATE $\text{diag}({\bm G_t})\leftarrow \bm 1$
%\FORALL { $i\neq j$}
%\STATE
%\en
%{\bm G_t(i,j)}\leftarrow
%\left\{\begin{array}{ll}
%{\bm G_t(i,j)}&\text{if}~|{\bm G_t(i,j)}|\leq\zeta\\
%\zeta\cdot\text{sign}({\bm G_t(i,j)})&\text{otherwise}
%\end{array}\right.
%\een
%%where sign($\cdot$) is a sign function.
%\ENDFOR
%\STATE Update $\bm\Phi$:
%%\STATE $J\leftarrow 1$
%%\STATE Compute $\varrho(\bm\omega_{J})$
%\FOR {$k=1$ {\bf to} $Iter_{inner}$}%[Until Convergence]
%\FOR {$J=1$ {\bf to} $M$}
%\STATE Compute $\bm E_J$\label{ap2}
%\STATE Compute the ED of $\bm E_J$\label{ap3}
%\IF {$\lambda_{1J}>0$}
%\STATE $\bm\omega_J\leftarrow\sqrt{\lambda_{1J}}\bm u_{1J}$\label{ap4}
%%\STATE Compute $\varrho(\bar{\bm\omega}_J)$
%%\IF {$\varrho(\bar{\bm \omega}_J)\geq\varrho(\bm\omega_{J})$}
%%\STATE Go to \ref{endloop1}
%%\ELSE
%%\STATE $\bm\omega_J\leftarrow\bar{\bm \omega}_J$
%%\STATE $\varrho(\bm\omega_J)\leftarrow \varrho(\bar{\bm\omega}_J)$
%%\STATE $J\leftarrow J+1$
%%\ENDIF
%%\ELSE
%%\STATE Go to \ref{endloop1}
%\ENDIF
%%\IF{$J=M+1$}
%%\STATE $J\leftarrow 1$
%%\ENDIF
%\ENDFOR
%\ENDFOR%\label{endloop1}
%\STATE $\bar{\bm \Phi}_1\leftarrow \bm\Omega\bm\Sigma_{\bm\Psi}^{-1}$\label{ap5}
%\STATE $\bm\Phi\leftarrow \bar{\bm\Phi}\bm U_{\bm\Psi}^\mathcal T$\label{ap6}
%\ENDFOR
%\end{algorithmic}
%"
169,1701.01000,"[htb]
	\caption{Online Joint Optimization of SMSD}%Projection Matrix and Sparsifying Dictionary with considering projection noise}
	\label{Alg_joint_projection_dictionary}
	\begin{algorithmic}[1]
		\REQUIRE ~\\
		Initial dictionary $\bm\Psi_0$, number of iterations $Iter_{sendic}$.
		\lastcon ~\\          %OUTPUT
		The sensing matrix $\bm\Phi$ and the sparsifying dictionary $\bm\Psi$.
		
		\FOR {$i=1$ {\bf to} $Iter_{sendic}$}
		\STATE Update the sensing matrix $\bm\Phi_i$ with fixed $\bm \Psi=\bm\Psi_{i-1}$ by $\phi(\bm \Psi)$ (which is specified in \eqref{solution_robust_projection_free_lambda}) and compute the two matrices $\bm\Xi_1$ and $\bm \Xi_2$
		\STATE Solve \eqref{dictionary_involve_projection} through \eqref{Dic:surrogate:easy} by {\bf Algorithm \ref{Alg_dictionary_involve_projection}} to update the dictionary $\bm\Psi$ with fixed $\bm\Phi=\bm\Phi_i$
		\ENDFOR
		\RETURN $\bm \Phi$ and $\bm\Psi$
	\end{algorithmic}
"
170,1701.01000,"[htb]
	\caption{Online Dictionary Learning with Projected SRE}% with considering projection noise}
	\label{Alg_dictionary_involve_projection}
	\begin{algorithmic}[1]
		\REQUIRE ~\\
		Training data $\bm X\in \Re^{N\times P}$, trade-off parameter $\gamma$, initial sensing matrix $\bm\Phi$ and dictionary $\bm\Psi_0$, batch size $\eta\geq1$, the sparsity level $K$, the power parameter $\rho$, number of iterations $Iter_{dic}$.
		\lastcon ~\\          %OUTPUT
		Dictionary $\bm\Psi$.
		%\ENSURE
		\STATE $\bm A_0 \leftarrow \bm 0$, $\bm B_0 \leftarrow \bm 0$, $i\leftarrow 1$
		%\STATE shuffle $\bm X$
		\FOR {$t=1$ {\bf to} $Iter_{dic}$}
		\IF {$i+\eta\leq P$}
		\STATE $\bm X_{t}\leftarrow\bm X(:,i:i+\eta-1)$, $\bm Y_t\leftarrow \bm\Phi\bm X_t$\\
		$i\leftarrow i+\eta$
		\ELSE
		\STATE Shuffle $\bm X$, $i\leftarrow 1$
		\STATE $\bm X_t\leftarrow\bm X(:,i:i+\eta-1)$, $\bm Y_t\leftarrow \bm\Phi\bm X_t$\\
		$i\leftarrow i+\eta$
		\ENDIF
		\STATE Sparse coding
		\e
		\left.\begin{array}{rl}\bm \Theta_{t}= &\arg\min_{\tilde{\bm\Theta}_{t}}\left\|\bmat\sqrt{\gamma} \bm X_{t}\\\bm Y_t\emat-\bmat\sqrt{\gamma}~ \bm\Psi_{t-1}\\ \bm\Phi\bm\Psi_{t-1}\emat\tilde{\bm\Theta}_t\right\|_F^2 \\
			&\text{s.t.}~\|\tilde{\bm\Theta}_t(:,k)\|_0\leq K, \forall k
		\end{array}\right.\label{sparse_coding}
		\ee
		\STATE $\bm A_t\leftarrow (1-\frac{1}{t})^\rho\bm A_{t-1}+\frac{1}{\eta}\bm \Theta_{t}\bm \Theta_{t}^\mathcal T$\label{Alg_dictionary_involve_projection:A_t}
		\STATE $\bm B_t\leftarrow (1-\frac{1}{t})^\rho\bm B_{t-1}+\frac{1}{\eta}\bm X_{t}\bm \Theta_{t}^\mathcal T$\label{Alg_dictionary_involve_projection:B_t}
		%\STATE $\bm C_t \leftarrow \bm\Phi\bm B_t$\label{Alg_dictionary_involve_projection:C_t}
		\STATE Compute $\bm\Psi_t$ using {\bf Algorithm \ref{Alg_dictionary_updating}} with $\bm\Psi_{t-1}$  as the initial value, so that
		\en
		%\left.\begin{array}{rl}
		\bm\Psi_t=\arg\min_{\bm\Psi\in\mathcal C}\hat \sigma_t(\bm\Psi)
		%\sigma(\bm\Psi)\triangleq&\frac{1}{2}\sum_{i=1}^t\left(\gamma\|\bm X_i-\bm \Psi_{i-1}\bm\Theta_i\|_F^2\right.\\
		%&\left.+\|\bm Y_i-\bm\Phi\bm\Psi_{i-1}\bm\Theta_{i}\|_F^2\right)\\
		%\Rightarrow&\frac{1}{2}\text{Tr}\left(\bm \Psi^\mathcal T\bm \Psi\left(\gamma\bm A_t\right)\right)-\text{Tr}\left(\bm \Psi^\mathcal T\left(\gamma\bm B_t\right)\right)\\
		%&+\frac{1}{2}\text{Tr}\left(\bm\Psi^\mathcal T\bm\Phi^\mathcal T\bm\Phi\bm\Psi\bm A_t\right)-\text{Tr}\left(\bm\Psi^\mathcal T\bm\Phi^\mathcal T\bm\Phi\bm B_t\right)
		% \end{array}\right.\label{dictionary_updating_problem}
		\een
		
		\ENDFOR
		\RETURN $\bm\Psi_{Iter_{dic}}$ (learned dictionary)
	\end{algorithmic}
"
171,1701.01000,"[!htb]
	\caption{Dictionary Update}% with considering projection noise}
	\label{Alg_dictionary_updating}
	\begin{algorithmic}[1]
		\REQUIRE ~\\
		{$\bm A_{t-1}=\left[\bm a_1,\cdots,\bm a_L\right], ~\bm B_{t-1}=\left[\bm b_1,\cdots,\bm b_L\right]$, $\bm\Xi_1, ~\bm\Xi_2$,\\
			$\bm \Psi_{t-1}=\left[\bm \psi_1,\cdots,\bm\psi_L\right]$.}
		\lastcon ~\\          %OUTPUT
		Dictionary $\bm\Psi_l$.
		\REPEAT
		\FOR {$j=1$ {\bf to} $L$}
		\STATE Update the $j$-th column to optimize \eqref{Dic:surrogate:easy}:
		\e
		\left.\begin{array}{rcl}
			\bm u_j&\leftarrow&\bm\Xi_1\left[\frac{\bm b_j-\bm\Psi_{t-1}\bm a_j}{\bm A_{t-1}(j,j)}+\bm\psi_j\right]+\\[5pt]
			&&\bm \Xi_2\left[\frac{\bm bj}{\bm A_{t-1}(j,j)\gamma}+\frac{1}{\gamma}\bm \psi_j-\frac{\bm\Psi_{t-1}\bm a_j}{\bm A_{t-1}(j,j)}\right]\\[5pt]
			\bm\Psi_{t-1}(:,j)&\leftarrow&\frac{\bm u_j}{\|\bm u_j\|_2}
		\end{array}\right.\label{dictionary_updating_block_updating}
		\ee
		\ENDFOR
		\UNTIL
		\RETURN $\bm \Psi_{t-1}$ (updated dictionary)
	\end{algorithmic}
"
172,1710.09513,"
	\SetAlgoLined
	Initialize: $\theta^0\in \mathcal{U}$ \;
	\For{$k = 0$ \KwTo \#Iterations}{
		Solve $\dot{X}^{\theta^k}_t = f(t,X^{\theta^k}_t,\theta^k_t), \quad X^{\theta^k}_0 = x$\;
		Solve $\dot{P}^{\theta^k}_t = -\nabla_x H(t,X^{\theta^k}_t,P^{\theta^k}_t,\theta^k_t), \quad P^{\theta^k}_T = -\nabla \Phi(X^{\theta^k}_T)$\;
		Set $\theta^{k+1}_t = \argmax_{\theta\in\Theta} H(t,X^{\theta^k}_t,P^{\theta^k}_t,\theta)$ for each $t\in[0,T]$\;
	}
	\caption{Basic MSA}
	\label{alg:basic_msa}
"
173,1710.09513,"
	\SetAlgoLined
	Initialize: $\theta^0\in \mathcal{U}$. Hyper-parameter: $\rho$ \;
	\For{$k = 0$ \KwTo \#Iterations}{
		Solve $\dot{X}^{\theta^k}_t = f(t,X^{\theta^k}_t,\theta^k_t), \quad X^{\theta^k}_0 = x$\;
		Solve $\dot{P}^{\theta^k}_t = -\nabla_x H(t,X^{\theta^k}_t,P^{\theta^k}_t,\theta^k_t), \quad P^{\theta^k}_T = -\nabla \Phi(X^{\theta^k}_T)$\;
		Set $\theta^{k+1}_t = \argmax_{\theta\in\Theta} \tilde{H}(t,X^{\theta^k}_t,P^{\theta^k}_t,\theta,\dot{X}^{\theta^k}_t,\dot{P}^{\theta^k}_t)$ for each $t\in[0,T]$\;
	}
	\caption{Extended MSA}
	\label{alg:extended_msa}
"
174,1710.09513,"
	\SetAlgoLined
	Initialize: Initialize: $\vartheta^0_n \in \Theta_n$, $n=0,\dots,N-1$. Hyper-parameter: $\rho$ \;
	\For{$k = 0$ \KwTo \#Iterations}{
		Set $x^{\theta^k}_0=x$ \;
		\For{$n = 0$ \KwTo $N-1$}{
			$x^{\vartheta^k}_{n+1} = g_n(x^{\vartheta^k}_n,\vartheta^k_n)$ \;
		}
		Set $p^{\vartheta^k}_{N}=-\nabla \Phi(x^{\vartheta^k}_{N})$ \;
		\For{$n = N-1$ \KwTo $0$}{
			$p^{\vartheta^k}_{n} = \nabla_x H_n(x^{\vartheta^k}_n, p^{\vartheta^k}_{n+1}, \vartheta^k_n)$ \;
		}
		\For{$n = 0$ \KwTo $N-1$}{
			Set
			$\vartheta^{k+1}_n = \argmax_{\vartheta\in\Theta_n}
			{H}_n(x^{\vartheta^k}_n,p^{\vartheta^k}_{n+1},\vartheta) 
			- \tfrac{1}{2} \rho \Vert x^{\vartheta^k}_{n+1} - g_n(x^{\vartheta^k}_n,\vartheta) \Vert_2^2
			- \tfrac{1}{2} \rho \Vert p^{\vartheta^k}_{n} - \nabla_x H_n(x^{\vartheta^k}_n,p^{\vartheta^k}_{n+1},\vartheta) \Vert_2^2
			$\;
		}
	}
	\caption{Discrete-time E-MSA}\label{alg:discrete_emsa}
"
175,1803.01299,"[tb]
  \caption{Basic MSA}
  \label{alg:basic_msa}
\begin{algorithmic}
  \STATE Initialize: $\bftheta^0=\{\theta^0_t\in\Theta_t:t=0\dots,T-1\}$;
  \small
  \FOR{$k = 0$ {\bfseries to} \#Iterations}
  \STATE $x^{\bftheta^k}_{s,t+1} = f_t(x^{\bftheta^k}_{s,t},\theta^k_t)$, $x^{\bftheta^k}_{s,0}=x_{s,0}$, $\forall s,t$;
  \STATE $p^{\bftheta^k}_{s,t} = \nabla_x H_t(x^{\bftheta^k}_{s,t}, p^{\bftheta^k}_{s,t+1},\theta^k_t), p^{\bftheta^k}_{s,T}=-\frac{1}{S}\nabla \Phi_s(x_{s,T})$, $\forall s,t$;
  % $x^{\bftheta^k}_{s,t+1} = f_t(x^{\bftheta^k}_{s,t},\theta^k_t)$ for $s=1,\dots,S$ and $t=0,\dots,T-1$ with $x^{\bftheta^k}_{s,0}=x_{s,0}$
  \STATE $\theta^{k+1}_t = \argmax_{\theta\in\Theta_t} \sum_{s=1}^{S}H_t(x^{\bftheta^k}_{s,t},p^{\bftheta^k}_{s,t+1},\theta)$ for $t=0,\dots,T-1$;
  \normalsize
  \ENDFOR
\end{algorithmic}
"
176,1803.01299,"[tb]
  \caption{Binary MSA}
\label{alg:binary_msa}
\begin{algorithmic}
  \small
  \STATE Initialize: $\bftheta^0$, $\overline{\mathbf{M}}^0$;
  \STATE Hyper-parameters: $\rho_{k,t}$, $\alpha_{k,t}$;
  \FOR{$k = 0$ {\bfseries to} \#Iterations}
  \STATE $x^{\bftheta^k}_{s,t+1} = f_t(x^{\bftheta^k}_{s,t},\theta^k_t) \qquad \forall s,t$
  \STATE $\qquad$ with $x^{\bftheta^k}_{s,0}=x_{s,0}$;
  \STATE $p^{\bftheta^k}_{s,t} = \nabla_x H_t(x^{\bftheta^k}_{s,t},p^{\bftheta^k}_{s,t+1},\theta^k_t) \qquad \forall s,t$
  \STATE $\qquad$ with $p^{\bftheta^k}_{s,T}=-\tfrac{1}{S}\nabla \Phi_s(x_{s,T})$;
  \STATE $\overline{M}^{k+1}_t = \alpha_{k,t}\overline{M}^{k}_t + (1-\alpha_{k,t})\sum_{s=1}^S p^{\bftheta^k}_{s,t+1}(x^{\bftheta^k}_{s,t})^T$
  \STATE $[\theta^{k+1}_t]_{ij}=
          \begin{cases}
            \sgn([\overline{M}^{k+1}_t]_{ij}) & \vert [\overline{M}^{k+1}_t]_{ij} \vert \geq 2\rho_{k,t}\\
            [\theta^k_t]_{ij} & \text{otherwise}
          \end{cases}$
  \STATE $\forall t$, $i$, and $j$;
  % \STATE $\theta^{k+1}_t = \argmax_{\theta\in\Theta_t} \sum_{s=1}^{S}H_t(x^{\bftheta^k}_{s,t},p^{\bftheta^k}_{s,t+1},\theta)$ for $t=0,\dots,T-1$;
  \normalsize
  \ENDFOR
\end{algorithmic}
"
177,1803.01299,"[tb]
  \caption{Ternary MSA}
\label{alg:ternary_msa}
\begin{algorithmic}
  \small
  \STATE Initialize: $\bftheta^0$, $\overline{\mathbf{M}}^0$;
  \STATE Hyper-parameters: $\rho_{k,t}$, $\alpha_{k,t}$;
  \FOR{$k = 0$ {\bfseries to} \#Iterations}
  \STATE $x^{\bftheta^k}_{s,t+1} = f_t(x^{\bftheta^k}_{s,t},\theta^k_t) \qquad \forall s,t$
  \STATE $\qquad$ with $x^{\bftheta^k}_{s,0}=x_{s,0}$;
  \STATE $p^{\bftheta^k}_{s,t} = \nabla_x H_t(x^{\bftheta^k}_{s,t},p^{\bftheta^k}_{s,t+1},\theta^k_t) \qquad \forall s,t$
  \STATE $\qquad$ with $p^{\bftheta^k}_{s,T}=-\tfrac{1}{S}\nabla \Phi_s(x_{s,T})$;  \STATE $\overline{M}^{k+1}_t = \alpha_{k,t}\overline{M}^{k}_t + (1-\alpha_{k,t})\sum_{s=1}^S p^{\bftheta^k}_{s,t+1}(x^{\bftheta^k}_{s,t})^T$
  \STATE $[\theta^{k+1}_t]_{ij} = 
  \begin{cases}
    +1 & [\overline{M}^{k+1}_{t}]_{ij} \geq  \rho_{k,t} (1-2[\theta^k_t]_{ij}) + \lambda_t \\
    -1 & [\overline{M}^{k+1}_{t}]_{ij} \leq -\rho_{k,t} (1+2[\theta^k_t]_{ij}) - \lambda_t \\
    0 & \text{otherwise.}
  \end{cases}$
  \STATE $\forall t$, $i$, and $j$; 
  % \STATE $\theta^{k+1}_t = \argmax_{\theta\in\Theta_t} \sum_{s=1}^{S}H_t(x^{\bftheta^k}_{s,t},p^{\bftheta^k}_{s,t+1},\theta)$ for $t=0,\dots,T-1$;
  \normalsize
  \ENDFOR
\end{algorithmic}
"
178,1802.03065,"[tb]
   \caption{Bubble Sort}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $x_i$, size $m$
   \REPEAT
   \STATE Initialize $noChange = true$.
   \FOR{$i=1$ {\bfseries to} $m-1$}
   \IF{$x_i > x_{i+1}$}
   \STATE Swap $x_i$ and $x_{i+1}$
   \STATE $noChange = false$
   \ENDIF
   \ENDFOR
   \UNTIL{$noChange$ is $true$}
\end{algorithmic}
"
179,1806.00499,"[t]
\caption{Procedure to estimate~$\ln \det$ using the Chebyshev
approximation.\label{alg:log_det_chebyshev}}

\begin{algorithmic}
\Require{$A \in \reals^{n \times n}$ is the implicit matrix;~$m$ the desired order;~$p$ the number
of probe vectors for the trace estimator;~$t$ the number of power iterations;~$g \geq 1$ a
multiplier for the estimate returned by the power method; and~$\epsilon$ the stipulated lower bound
on~$\Sp(A)$.}

\Procedure{StochasticLogDet}{$A, m, p, t, g, \epsilon$}
\State $\hat{\lambda}_\text{max} \gets \Call{PowerMethod}{A, t}$
\State $\mu, \nu \gets \epsilon, g \,\hat{\lambda}_\text{max}$
\State $a, b \gets \mu / (\mu + \nu), \nu / (\mu + \nu)$
\State Define~$\varphi$ and~$\varphi^{-1}$ using Equation~\ref{eq:rescale_func}.
\State $\{c_i\}_{i \in [0, m]} \gets \Call{ChebyshevCoefficients}{\ln \circ\, \varphi}$
\State $\bar{A} \gets A / (\mu + \nu)$
\State $\Gamma \gets \Call{StochasticChebyshevTrace}{\varphi^{-1}(\bar{A}), \{c_i\}, p}$
\State \Return $n\ln(a + b) + \Gamma$
\EndProcedure

\Procedure{StochasticChebyshevTrace}{$A, \{c_i\}_{i \in [0, m]}, p$}
\State $r \gets 0$
\For{$j \in [1, p]$}
	\State $v \gets \Call{RandomRademacher}{n}$
	\State $w_0, w_1 \gets v, Av$
	\State $s \gets c_0 w_0 + c_1 w_1$

	\For{$i \in [2, m]$}
		\State $w_i \gets 2 A w_{i - 1} - w_{i - 2}$
		\State $s \gets s + c_i w_i$
	\EndFor

	\State $r \gets r + \inner{v}{s}$
\EndFor
\State \Return $r / p$
\EndProcedure
\end{algorithmic}
"
180,1711.01333,"[H]
	\begin{algorithmic}
		\State Let $\pi_1(b) =\frac{1}{|B|}$ for all $b\in B$ (i.e. the uniform distribution over bids), $\eta = \sqrt{\frac{\log\left(|B| \right)}{2T|O|}}$
		\For{each iteration t}
		\State Draw an action $b_t$ from the multinomial distribution based on $\pi_t(\cdot)$
		\State Observe $x_t(\cdot)$, observe chosen outcome $o_t$ and associated reward function $r_t(\cdot, o_t)$
		\State Compute estimate of utility: 
		\begin{equation}
		\tilde{u}_t(b) = \frac{(r_t(b, o_t) - 1) \Pr_t[o_t | b]}{\Pr_t[o_t]}
		\end{equation}
		\State Update $\pi_t(\cdot)$ based on the Exponential Weights Update: 
		\begin{equation}
		\forall b\in B: \pi_{t+1}(b) \propto \pi_{t}(b)\cdot \exp\left\{\eta \cdot \tilde{u}_t(b)\right\}
		\end{equation}
		%
		\EndFor
	\end{algorithmic}
	\caption{$\winexp$ algorithm for learning with outcome-based feedback}\label{alg:winexp2}
"
181,1711.01333,"[h]
\begin{algorithmic}
\State Let $\pi_1(b) =\frac{1}{|B|}$ for all $b\in B$ (i.e. the uniform distribution over bids), $\eta = \sqrt{\frac{2\log\left(|B| \right)}{5T}}$
\For{each iteration t}
\State Draw a bid $b_t$ from the multinomial distribution based on $\pi_t(\cdot)$
\State Observe $x_t(\cdot)$ and if reward is won also observe $r_t(\cdot)$
\State Compute estimate of utility:

\State\indent If reward is won $\tilde{u}_t(b) = \frac{(r_t(b) - 1) \Pr[A_t | b_t=b]}{\Pr[A_t]}$; otherwise,  $\tilde{u}_t(b) = -\frac{\Pr[\neg A_t | b_t = b]}{\Pr[\neg A_t]}$.
%
%
%
%
%
%
%
%
\State Update $\pi_t(\cdot)$ as in Exponential Weights Update: $\forall b\in B: \pi_{t+1}(b) \propto \pi_{t}(b)\cdot \exp\left\{\eta \cdot \tilde{u}_t(b)\right\}$
%
%
%
%
\EndFor
\end{algorithmic}
\caption{$\winexp$ algorithm for learning with win-only feedback}\label{alg:winexp}
"
182,1711.01333,"[H]
	\begin{algorithmic}
		\State Let $\pi_1(b) =\frac{1}{|B|}$ for all $b\in B$ (i.e. the uniform distribution over bids), $\eta = \sqrt{\frac{\log\left(|B| \right)}{2T|O|}}$
		\For{each iteration t}
		\State Draw an action $b_t$ from the multinomial distribution based on $\pi_t(\cdot)$
		\State Observe $x_t(\cdot)$, chosen outcomes $o_\tau, \forall \tau \in I_t$, average reward function conditional on each realized outcome $Q_t(b,o)$ and the realized frequencies for each outcome $f_t(o) = \frac{|I_{to}|}{|I_t|}$.
		\State Compute estimate of utility: 
		\begin{equation}
		\tilde{u}_t(b) = \sum_{o \in O} \frac{\Pr_t \left[o|b \right]}{\Pr_t[o]} f_t(o) \left(Q_t(b,o)-1 \right)
		\end{equation}
		\State Update $\pi_t(\cdot)$ based on the Exponential Weights Update: 
		\begin{equation}
		\forall b\in B: \pi_{t+1}(b) \propto \pi_{t}(b)\cdot \exp\left\{\eta \cdot \tilde{u}_t(b)\right\}
		\end{equation}
		%
		\EndFor
	\end{algorithmic}
	\caption{$\winexp$ algorithm for learning with outcome-based batch-reward feedback}\label{alg:winexp4}
"
183,1711.01333,"[H]
\begin{algorithmic}
\State Let $\pi_1(b) =\frac{1}{|B|}$ for all $b\in B$ (i.e. the uniform distribution over bids), $\eta = \sqrt{\frac{\log\left(|B| \right)}{2T|O|}}$
\For{each iteration t}
\State Draw an action $b_t$ from the multinomial distribution based on $\pi_t(\cdot)$
\State Observe $x_t(\cdot)$, chosen outcomes $o_\tau, \forall \tau \in I_t$, average reward function conditional on each realized outcome $Q_t(b,o)$ and the realized frequencies for each outcome $f_t(o) = \frac{|I_{to}|}{|I_t|}$.
\State Compute estimate of utility: 
\begin{equation}
\tilde{u}_t(b) = \sum_{o \in O} \frac{\Pr_t \left[o|b \right]}{\Pr_t[o]} f_t(o) \left(Q_t(b,o)-1 \right)
\end{equation}
\State Update $\pi_t(\cdot)$ based on the Exponential Weights Update: 
\begin{equation}
\forall b\in B: \pi_{t+1}(b) \propto \pi_{t}(b)\cdot \exp\left\{\eta \cdot \tilde{u}_t(b)\right\}
\end{equation}
%
\EndFor
\end{algorithmic}
\caption{$\winexp$ algorithm for learning with outcome-based batch-reward feedback}\label{alg:winexp4}
"
184,1711.01333,"[H]
\begin{algorithmic}
\State Let $\pi_1(b) =\frac{1}{|B|}$ for all $b\in B$ (i.e. the uniform distribution over bids), $\eta = \sqrt{\frac{\log(|B|)}{8T\alpha \ln\left(\frac{16|O|^2 T}{\alpha}\right)}}$
\For{each iteration t}
\State Draw an action $b_t \sim \pi_t(\cdot)$, multinomial
\State Observe $x_t(\cdot)$, chosen outcome $o_t$ and associated reward function $r_t(\cdot, o_t)$
\State Observe and associated reward function $r_t(\cdot, \cdot)$ for all neighbor outcomes $N_\eps^{in}, N_\eps^{out}$ 
\State Compute estimate of utility:
\begin{equation}
\tilde{u}_t(b) = \1\{o_t\in O_{\epsilon}\} \sum_{o \in N_{\epsilon}^{out}(o_t)}
\frac{(r_t(b, o) - 1) \Pr_t[o|b]}{\sum_{o'\in N_{\epsilon}^{in}(o)}\Pr_t[o']}
\end{equation}
\State Update $\pi_t(\cdot)$ based on the Exponential Weights Update: 
\begin{equation}
%
\end{equation}
\EndFor
\end{algorithmic}
\caption{$\winexpG$ algorithm for learning with outcome-based feedback and a feedback graph over outcomes}\label{alg:winexpG}
"
185,1806.00398,"[t]
  \caption{DNNAE-MSE+CE training algorithm.}
  \label{alg.train}
  \small
  \begin{algorithmic}[1]
    \STATE \textbf{Input:} samples $S$ and labels $L$ 
    \STATE \textbf{Input:} $epochs$ and $batchsize$
    \STATE batches = length(labels) / batchsize 
    \FOR {$i = 1 : epochs$}
    	\FOR {$j = 1 : batches$}
    	\STATE {$S_b = S[(j)*batchsize+1 : (j+1)*batchsize]$}
    	\STATE{$L_b = L[(j)*batchsize+1 : (j+1)*batchsize]$}
    	\STATE {Feedforward $S_b$ and $L_b$ to obtain $L_\mathrm{CE}$ and $L_\mathrm{MSE}$}
    	\STATE {Backpropogate $L_\mathrm{CE}$ to parameters in the encoder subnet}
    	\STATE {Backpropogate $L_\mathrm{MSE}$ to parameters in the whole net}
    	\ENDFOR
    \ENDFOR
  \end{algorithmic}
"
186,1806.00370,"[H]
	\caption{Regularized Nonlinear Acceleration (\textbf{RNA}), (and computational complexity).}
	\label{algo:acc_fixedpoint_reg}
	\begin{algorithmic}[1]
		\REQUIRE Sequence of vectors $\{\theta_0,\,\theta_1,\ldots,\theta_{K}\}\in\reals^d$ , regularization parameter $\lambda > 0$.
		\STATE Compute $R = [\theta_{1}-\theta_{0},\ldots,\theta_{K}-\theta_{K-1}]$ \hfill $O(K)$
		\STATE Solve $(R^T R+\lambda I) z = \ones$. \hfill $O(K^2 d + K^3)$
		\STATE Normalize $ c = z/(\sum_{k\leq K}z_k)$. \hfill $O(K)$
		\ENSURE $\hat \theta = \sum_{k\leq K} c_k\theta_{k}$. \hfill $O(Kd)$
	\end{algorithmic}
"
187,1703.09165,"[!t]  
	\caption{PWLS-ULTRA Algorithm}\label{alg: ultra}
	\begin{algorithmic}[0]
		\State \textbf{Input:}
		initial image $\tilde{\x}^{(0)}$, pre-learned $\{\omg_k\}$, threshold $\gamma$, $\alpha = 1.999$,
		$\D_{\A} $ in \eqref{eq:DA}, $\D_\R $ in \eqref{eq:DR}, number of outer iterations $T$, number of inner iterations $N$, and number of subsets $M$.											
		\State \textbf{Output:}  reconstructed image $\tilde{\x}^{(T)}$, cluster indices $\{\tilde{C}_k^{(T)}\}$.
		\For {$t =0,1,2,\cdots,{T-1}$}		
		
		\State \textbf{1) Image Update}: $\{ \tilde{\z}_j^{(t)}\}$ and  $\{\tilde{C}_k^{(t)}\}$ fixed,	
		
		\textbf{Initialization:} $\rho=1$, $\x^{(0)} = \tilde{\x}^{(t)}$, $\g^{(0)} = \ze^{(0)}  = M\A_M^{T}\W_M(\A_M\x^{(0)}-\y_M) $ and $\h^{(0)} = \D_\A \x^{(0)} - \ze^{(0)}$.	
		
		
		\For {$n =0,1,2,\cdots,N-1$}	
		\For {$m =0,1,2,\cdots,M-1$}  $r = nM +m$			
		\begin{equation*}
			\left\{			
			\begin{aligned}
				\s^{(r+1)} &= \rho(\D_\A \x^{(r)} -\h^{(r)}) + (1-\rho)\g^{(r)} \\
				\x^{(r+1)} &= [\x^{(r)} - (\rho\D_\A+\D_\R)^{-1}(\s^{(r+1)} +\nabla \R_2(\x^{(r)}))]_+ \\
				\ze^{(r+1)} & \triangleq  M \A^T_m\W_m(\A_m\x^{(r+1)}-\y_m)   \\
				\g^{(r+1)} &= \frac{\rho}{\rho+1}(\alpha \ze^{(r+1)} + (1-\alpha)\g^{(r)}) +  \frac{1}{\rho+1}\g^{(r)}\\
				\h^{(r+1)}  &= \alpha(\D_{\A} \x^{(r+1)} -\ze^{(r+1)}) + (1-\alpha)\h^{(r)} 
			\end{aligned}
			\right.
		\end{equation*}  
		\State decreasing $\rho$ using \eqref{eq:rho}. % where $r = nM +m$.		 
		\EndFor			
		\EndFor	
		\State   $\tilde{\x}^{(t+1)} = \x^{(NM)}$. 
		\State \textbf{2) Sparse Coding and Clustering}: with $\tilde{\x}^{(t+1)}$ fixed, for each $1\leq j \leq N$, obtain $\hat{k}_j$ using \eqref{eq:k_j}. Then $\tilde{\z}_{j}^{(t+1)} = H_{\gamma}(\omg_{\hat{k}_j} \P_j \tilde{\x}^{(t+1)}) $, and update $\tilde{C}_{\hat{k}_j}^{(t+1)}$.
		\EndFor		
	\end{algorithmic}
"
188,1805.12421,"[!ht]
%   \SetAlgoLined
 	\textbf{\textit{Input:}} { $Dataset: (G, S, U, X, Y)$, \\ No: differentiable hops: $C$, No: of iterations: T} \\
 	\textbf{\textit{Output:}} $\hat{Y}$ \\
 	
 	$\hat{Y}[S]$ = $0$; $\hat{Y}[U]$ = $0$; $\Tilde{Y}$ = $\hat{Y}$  	\\
 	\For {$t$ in 1:T}{
 	    // Learning \\
     	\For {epoch\_id in 1:Max\_Epochs} {
            \For {$nodes$ in $S$}{
                $\tilde{Y}[nodes]$ = predict$(nodes, G, X, \hat{Y}, K)$ \\
                min Loss$(\tilde{Y}[nodes], Y[nodes])$
            }  }
        // Inference \\
        \For {$nodes$ in $U$}{
            $\Tilde{Y}[nodes]$ = predict$(nodes, G, X, \hat{Y}, K)$
        }
        $\hat{Y}[S]$ = $Y$ \\%\Comment{True labels don't change}\\
        // Temporal averaging of predicted labels \\
        $\hat{Y}[U]$= $(T - t)/T *\Tilde{Y} + (t/T)*\hat{Y}[U]$ 
 	}
    \Fn{predict$(nodes, G, X, \hat{Y}, K)$}
    {
        $A, nodes^*$ = get\_subgraph$(G, nodes, K)$ \\
        $X = X[nodes^*]$; $\hat{Y} = \hat{Y}[nodes^*]$ \\
        // Compute 0-hop features\\
        $h_0$ = $\sigma(XW_0)$ \\
        // Compute K-hop features\\
        \For {$k$ in $1:K$}{
              $h_k$ = $\sigma(\alpha \boldsymbol{[h_{0}]}W_k^\phi + \beta F(A, \boldsymbol{[h_{k-1}, \hat{Y}}]W_k^\psi$)) 
            %   \Comment{Equation: \ref{eqn:I-NIP}}
        }
        // Predict labels\\
        $\Tilde{Y} = \sigma(h_K[nodes]W_{L})$ \\
        \Return{$\Tilde{Y}$}
     }     
 	\caption{I-NIP-MEAN: Iterative NIP Mean Kernel}
 	\label{Alg:1}  
 "
189,1806.00125,"[t]
\caption{{\sf CIAG} and {\sf A-CIAG} Method.}\label{alg:aciag}
  \begin{algorithmic}[1]
  \STATE \textbf{Input}: Initial point $\prm^1 \in \RR^d$, step size parameter $\gamma > 0$, (for {\sf A-CIAG} only) extrapolation constant $\alpha$.
  \STATE Initialize the vectors/matrices in the memory:\vspace{-.1cm}
  \beq \label{eq:ciag_n_init}
   \prm_i^1 = \prm^1,~\tau_i^1 = 0,~\forall~i,~ {\bm b}^0 = {\bm 0},~{\bm H}^0 = {\bm 0} \eqs. \vspace{-.5cm}
  \eeq
  \FOR {$k=1,2,\dots$}
  \STATE \label{ciag_n:sel} Select $i_k \in \{1,...,m\}$, e.g., $i_k = (k~{\rm mod}~m) + 1$,
  and the algorithm is given access to $f_{i_k}(\prm)$.
  Set the counter variables as $\tau_{i_k}^k = k$, $\tau_j^k = \tau_j^{k-1}$ for all $j \neq i_k$.
%  \IF {$k > 1$}
  \STATE  \emph{(For {\sf A-CIAG} only)} If $k = 1$, we take $\eprm^{1} = \prm^1$; otherwise we compute the extrapolated variable as
%  :\vspace{-.1cm}
%  \beq
 $ \eprm^k = \prm^k + \alpha ( \prm^k - \prm^{k-1} )$. 
%   \eqs. \vspace{-.3cm}
%  \eeq
  \STATE Update the vector in the memory:\vspace{-.2cm}
  \beq
  \prm_{i_k}^k = \begin{cases}
\prm^k, & \text{for {\sf CIAG} \eqs,} \\
\eprm^k, & \text{for {\sf A-CIAG} \eqs,} 
\end{cases}
  \eeq 
  and set $\prm_{j}^k = \prm_j^{k-1}$ for all $j \neq i_k$. 
   \STATE \label{ciag_n:sums} 
   If $\tau_{i_k}^{k-1} = 0$, then update: (\emph{self-initialization})\vspace{-.1cm}
   \beq \label{eq:ciag_n_inc_init}
   \begin{split}
   {\bm b}^{k} & = {\bm b}^{k-1} + \grd f_{i_k} (\prm_{i_k}^k) - \grd^2 f_{i_k} (\prm_{i_k}^k) \prm_{i_k}^k \eqs, \\
   {\bm H}^{k} & = {\bm H}^{k-1} + \grd^2 f_{i_k} (\prm_{i_k}^k) \eqs.\\[-.3cm]
   \end{split}
   \eeq
   Otherwise, update:\vspace{-.1cm}
   \beq \label{eq:ciag_n_inc}
   \begin{split}
   {\bm b}^{k} & = {\bm b}^{k-1} - \grd f_{i_k}( \prm_{i_k}^{k-1} ) + \grd f_{i_k} (\prm_{i_k}^k)  + \grd^2 f_{i_k}( \prm_{i_k}^{k-1} ) \prm_{i_k}^{k-1} - \grd^2 f_{i_k} (\prm_{i_k}^k) \prm_{i_k}^k \eqs, \\
   {\bm H}^{k} & = {\bm H}^{k-1} - \grd^2 f_{i_k}( \prm_{i_k}^{k-1} ) + \grd^2 f_{i_k} (\prm_{i_k}^k) \eqs.\\[-.3cm]
   \end{split}
   \eeq
%   \ENDIF
   \STATE \label{ciag_n:upd} Compute the update:\vspace{-.1cm}
   \beq \label{eq:ciag_n_imp} \textstyle
   \prm^{k+1} = \prm_{i_k}^k - \gamma \big( {\bm b}^k + {\bm H}^k \prm_{i_k}^k   \big) \eqs. \vspace{-.1cm}
   \eeq
%   \STATE \label{ciag:mem} Update the parameter in memory $\prm_{i_k} \leftarrow \eprm^k$. 
%   \STATE Set $\tau_{i_k}^k = k$ and $\tau_j^k = \tau_j^{k-1}$ for all $ j \neq {i_k} $.
\ENDFOR
\STATE \textbf{Return}: an approximate solution to \eqref{eq:opt}, $\prm^{k+1}$.
  \end{algorithmic}
"
190,1806.00047,"[t]
\begin{algorithmic}
\State $\mathcal{D^*} \leftarrow \texttt{collect\_dataset} (\pi^*, N_s)$
%
\State $\pi_{\theta_1} \leftarrow \texttt{train\_supervised}(\mathcal{D^*})$
\State Sample initial dataset $\mathcal{D} \sim \mathcal{D^*}$ of size $N$
\For {$i = 1$ to $K$}
	\State Discard $N_d$ trajectories from $\mathcal{D}$ uniformly at random
	\State Decay $\beta$: $\beta \leftarrow (\beta_0)^{i}$
	\State Let $\hat{\pi}_i = \beta\pi^* + (1 - \beta)\pi_i$
	\State $\mathcal{D}_i \leftarrow \texttt{collect\_dataset} (\hat{\pi}_i, N_d)$ of size $N_d$
	\State $\mathcal{D} \leftarrow \mathcal{D} \cup \mathcal{D}_i$
	\State $\pi_{i+1} \leftarrow \texttt{train\_epoch}(\mathcal{D}, \pi_i)$
\EndFor
\Return $\pi_K$

\end{algorithmic}
\caption{DAggerFM Training Algorithm for imitation learning with capped dataset size.}
\label{alg:full}
"
191,1805.11749,"[!h]
  \begin{algorithmic}[0]
    \Input Data set of two different styles $\B{X}$, $\B{Y}$. \State Parameters:
    weight $\lambda$ and $\gamma$, temperature $\tau$. \State Initialized model
    parameters $\theta_\B{E}, \theta_\B{G}, \theta_{\text{LM}_{\B{x}}},
    \theta_{\text{LM}_{\B{y}}}$. \Repeat \State Update
    $\theta_{\text{LM}_{\B{x}}}$ and $\theta_{\text{LM}_{\B{y}}}$ by minimizing
    $\mathcal{L}_{\text{LM}}^{\B{x}}(\theta_{\text{LM}_{\B{x}}})$ and
    $\mathcal{L}_{\text{LM}}^{\B{y}}(\theta_{\text{LM}_{\B{y}}})$ respectively.
    \State Update $\theta_{\B{E}}, \theta_{\B{G}}$ by minimizing:
    $\mathcal{L}_{\text{rec}} - \lambda (\mathcal{L}^{\B{x}}_{\text{LM}} +
    \mathcal{L}^{\B{y}}_{\text{LM}}) $ using Equation~\ref{eq:gradient}.
    \Until{convergence} \Output A text style transfer model with parameters
    $\theta_{\B{E}}$, $\theta_{\B{G}}$.
      \end{algorithmic}
      \caption{Unsupervised text style transfer.}
      \label{alg:tsf}
    "
192,1806.00007,"[H]
\caption{Training multi-layered GBDT (mGBDT) Forest}
\label{alg:mGBDT}
\KwIn{Number of layers $M$, layer dimension $d_{i}$, training data $X$,$Y$, Final Loss function $L$, $\alpha, \gamma,K_{1},K_{2}$, Epoch $E$, noise injection $\sigma^{2}$ }
\KwOut{A trained mGBDT}

$F^{0}_{1:M}$ $\leftarrow$ Initialize($M$,$d_{i}$); $G^{0}_{2:M}$ $\leftarrow$ $null$; $\mathbf{o}_{0}$ $\leftarrow$ $X$; $\mathbf{o}_{j} \leftarrow F^{0}_{j}(\mathbf{o}_{j - 1})$ for $j = 1, 2, \dots, M$\\
\For{$t$ = $1$ to $E$}
{

    $\mathbf{z}^{t}_{M} \leftarrow \mathbf{o}_{M} - \alpha \frac{\partial L(\mathbf{o}_{M}, Y)}{\partial \mathbf{o}_{M}}$ \\
    \For{$j$ = $M$ to $2$}
    {
        $G^t_{j} \leftarrow G^{t-1}_{j}$ \\
        $\mathbf{o}^{noise}_{j-1} \leftarrow \mathbf{o}_{j-1} + \mathbf{\varepsilon}, \mathbf{\varepsilon} \sim \mathcal{N}(\mathbf{0},\,diag(\sigma^{2})) $ \\
        $L^{inv}_{j}$ $\leftarrow$ $\lVert {G^t_{j}(F^{t-1}_{j}(\mathbf{o}^{noise}_{j-1}))-\mathbf{o}^{noise}_{j-1}}  \rVert$  \\

        \For{$k$ = $1$ to $K_{1}$}
        {
            $\mathbf{r}_k \leftarrow -[\frac{\partial L^{inv}_{j}}{\partial G^{t}_{j}(F^{t-1}_{j}(\mathbf{o}^{noise}_{j-1}))}]$ \\
            Fit regression tree $h_{k}$ to $\mathbf{r}_{k}$, i.e. using the training set ($F^{t-1}_{j}(\mathbf{o}^{noise}_{j-1})$, $\mathbf{r}_k$) \\
            $G^t_{j} \leftarrow G^t_{j} + \gamma h_{k}$

        }

        $\mathbf{z}^{t}_{j - 1} \leftarrow G^{t}_{j}(\mathbf{z}^{t}_{j})$\\
    }

    \For{$j$ = $1$ to $M$}
    {
        $F^{t}_{j} \leftarrow F^{t-1}_{j}$ \\
        $L_{j}$ $\leftarrow$ $\lVert {F^{t}_{j}(\mathbf{o}_{j-1})-\mathbf{z}^{t}_{j}} \rVert$ \\

        \For{$k$ = $1$ to $K_{2}$}
        {
            $\mathbf{r}_k \leftarrow -[\frac{\partial L_{j}}{\partial F^{t}_{j}(\mathbf{o}_{j-1})}]$ \\
            Fit regression tree $h_{k}$ to $\mathbf{r}_{k}$, i.e. using the training set ($\mathbf{o}_{j-1}$, $\mathbf{r}_k$) \\
            $F^t_{j} \leftarrow F^t_{j} + \gamma h_{k}$
        }
        $\mathbf{o}_{j} \leftarrow F^{t}_{j}(\mathbf{o}_{j-1})$ \\
    }
}
return $F^{T}_{1:M}$, $G^{T}_{2:M}$\\
"
193,1805.12375,"[t]
\small
\caption{Episodic Backward Update for Tabular $Q$-Learning (single episode, tabular)}
\label{alg:1}
\begin{algorithmic}[1]
\State Initialize the $Q$- table $Q \in \mathbb{R}^{\mathcal{S} \times \mathcal{A}}$ with all-zero matrix. \newline $Q(s,a) = 0$ for all state action pairs $(s,a) \in \mathcal{S} \times \mathcal{A}$.
\State Experience an episode $ E = \{(s_1,a_1,r_1,s_2), \ldots, (s_{T},a_{T},r_{T},s_{T+1})\}$
\For {$t = T$ to 1}
\State $Q(s_t,a_t) \leftarrow r_t + \gamma \max_{a^\prime} Q(s_{t+1},a')$

\EndFor

\end{algorithmic}
"
194,1805.12375,"[t]
\small
\caption{Episodic Backward Update}
\label{alg:2}
\begin{algorithmic}[1]
\State \textbf{Initialize}: replay memory $D$ to capacity $N$, on-line action-value function $Q(\cdot; \bm{\theta})$, target action-value function $\hat{Q}(\cdot; \bm{\theta^-})$
\For {episode = 1 to $M$ }
\For {$t = 1$ to \mbox{Terminal} }

\State With probability $\epsilon$ select a random action $a_t$, otherwise select $a_t = \argmax_a Q\left(s_t,a;\bm{\theta}\right)$
\State Execute action $a_t$, observe reward $r_t$ and next state $s_{t+1}$
\State Store transition $(s_t,a_t,r_t,s_{t+1})$ in $D$

\State Sample a random episode $E = \{\bm{S,A,R,S^\prime}\}$ from $D$, set $T=\operatorname{length}(E)$

\State Generate a temporary target $Q$-table, $\tilde{Q} = \hat{Q}\left(\bm{S^\prime},\bm{\cdot};\bm{\theta^-}\right)$
\State Initialize the target vector $\bm{y} = \operatorname{zeros}(T)$, $\bm{y}_T \leftarrow \bm{R}_T$
\For {$k = T-1$ to 1}
\State $\tilde{Q}\left[\bm{A}_{k+1}, k\right] \leftarrow \beta \bm{y}_{k+1} + (1-\beta) \tilde{Q}\left[\bm{A}_{k+1}, k\right]$
\State $\bm{y}_k \leftarrow \bm{R}_k + \gamma\max_{a} \tilde{Q}\left[a, k\right] $
\EndFor

\State Perform a gradient descent step on $\left(\bm{y}-Q\left(\bm{S},\bm{A};\bm{\theta}\right)\right)^2$ with respect to $\bm{\theta}$
\State Every $C$ steps reset $\hat{Q} = Q$
\normalsize
\EndFor
\EndFor
\end{algorithmic}
"
195,1805.12375,"[h]
\small
   \caption{Adaptive Episodic Backward Update}
   \label{alg:2}
    \begin{algorithmic}[1]
      \State \textbf{Initialize}: replay memory $D$ to capacity $N$, $K$ on-line action-value function $Q_{1}(\cdot; \bm{\theta_1}), \ldots, Q_{K}(\cdot; \bm{\theta_K})$, $K$ target action-value function $\hat{Q}_{1}(\cdot; \bm{\theta^-_1}), \ldots, \hat{Q}_{K}(\cdot; \bm{\theta^-_K})$, training score recorder  $TS=\operatorname{zeros}(K)$, diffusion factors $\beta_1,\ldots,\beta_K$ for each learner network
      \For {episode = 1 to $M$ }
        \State Select $Q_{\operatorname{actor}}=Q_{i}$  as the actor network for the current episode, where $i=(\operatorname{episode}-1)\%K+1$
        \For {$t = 1$ to \mbox{Terminal} } 

          \State With probability $\epsilon$ select a random action $a_t$
	  \State Otherwise select $a_t = \argmax_a Q_{\operatorname{actor}}\left(s_t,a\right)$     
          \State Execute action $a_t$, observe reward $r_t$ and next state $s_{t+1}$
          \State Store transition $(s_t,a_t,r_t,s_{t+1})$ in $D$
          \State Add training score for the current learner $TS[i] += r_t$
          \State Sample a random episode $E = \{\bm{S,A,R,S^\prime}\}$ from $D$, set $T=\operatorname{length}(E)$
	\For {$j=1$ to $K \textrm{ (this loop is processed in parallel)}$}
          \State Generate temporary target $Q$-table, $\tilde{Q}_j = \hat{Q}_i\left(\bm{S^\prime},\bm{\cdot};\bm{\theta^-_j}\right)$
          \State Initialize target vector $\bm{y} =  \operatorname{zeros}(T)$, $\bm{y}_T \leftarrow \bm{R}_T$
          \For {$k = T-1$ to 1}
            \State $\tilde{Q}_j\left[\bm{A}_{k+1}, k\right] \leftarrow \beta_j \bm{y}_{k+1} + (1-\beta_j) \tilde{Q}_j\left[\bm{A}_{k+1}, k\right]$
            \State $\bm{y}_k \leftarrow  \bm{R}_k + \gamma\max_{a}  \tilde{Q}_j\left[a, k\right] $
          \EndFor
          \State Perform a gradient descent step on $\left(\bm{y}-Q_j\left(\bm{S},\bm{A};\bm{\theta_j}\right)\right)^2$ with respect to $\bm{\theta_j}$
	\EndFor
          \State Every $C$ steps reset $\hat{Q}_1 = Q_1, \ldots, \hat{Q}_K = Q_K$
        \EndFor
	\State Every $B$ steps synchronize all learners with the best training score, $b=\argmax_k{TS[k]}$. \newline  $Q_{1}(\cdot; \bm{\theta_1})=Q_{b}(\cdot; \bm{\theta_b}) ,\ldots, Q_{K}(\cdot; \bm{\theta_K})=Q_{b}(\cdot; \bm{\theta_b})$ and $\hat{Q}_{1}(\cdot; \bm{\theta_1})=\hat{Q}_{b}(\cdot; \bm{\theta_b^-}) ,\ldots, \hat{Q}_{K}(\cdot; \bm{\theta_K})=\hat{Q}_{b}(\cdot; \bm{\theta_b^-})$. Reset the training score recorder $TS=\operatorname{zeros}(K)$.
      \EndFor
    \end{algorithmic}
"
196,1805.12369,"[tb]
  \caption{RCL for Continual Learning}
  \label{alg:rcl}
  \begin{algorithmic}[1]
    \STATE {\bfseries Input:} A sequence of dataset $\Dcal=\{ \Dcal_1, \Dcal_2,\dots,\Dcal_T \}$ 
    \STATE {\bfseries Output:} $W_T^a$
    %\STATE Initialize $W_1$ deal with $D_1$
    \FOR{$t=1,...,T $}
    \IF {$t=1$}
    \STATE Train the base network using (~\ref{init}) on the first datasest $\Dcal_1$ and obtain $W_1^a$.
    \ELSE
    \STATE Expand  the network by Algorithm~\ref{alg:expanding}, and obtain the trained  $W_t^a$.
    \ENDIF
    \ENDFOR
  \end{algorithmic}
"
197,1805.12369,"[tb]
  \caption{Routine for Network Expansion}
  \label{alg:expanding}
  \begin{algorithmic}[1]
    \STATE {\bfseries Input:} Current dataset $\Dcal_t$; previous parameter $W_{t-1}^a$; the size of action space for each layer $n_i, i=1\dots,m$; number of epochs for training the controller and value network, $T_e$.
    \STATE {\bfseries Output:} Network parameter $W_t^a$
    %\STATE Initialize a controller $C_t$ by LSTM,whose parameters are $\theta_c$
    \FOR{$i=1,\dots,T_e $}
    \STATE Generate actions $a_{1:m}$ by controller's policy;
    \STATE Generate $W_t^{(i)}$ by expanding parameters $W_{t-1}^a$ according to $a_{1:m}$; 
    \STATE Train the expanded network using Eq.~\eqref{update} to obtain $W_t^{(i)}$.
    \STATE Evaluate the gradients of the controller and value network by Eq.~\eqref{policy} and Eq.\eqref{valuenet},
    \begin{align}
    \theta_c = \theta_c + \eta_c \nabla_{\theta_c}J(\theta_c), \quad
    \theta_v = \theta_v - \eta_v \nabla_{\theta_v}L_v(\theta_v). \nonumber
    \end{align}
    %$\theta$=$\theta$+
    \ENDFOR
    \STATE Return the best network parameter configuration, $W_t^a = \argmax_{W_t^{(i)}} R_t(W_t^{(i)})$.
  \end{algorithmic}
"
198,1805.12254,"
    \textbf{Forward Pass:}\\
    \Indp
	\ForAll{Boundary voxels, $\phi_b$ \emph{\textbf{in parallel}}}{
				$\eta_b $= {$Forward_{Fine-level CNN}$($\vartheta_{2_{b}})$)};\\
                $\vartheta_1(\phi_b) = \eta_b$
	}
    $y_{pred} = Forward_{Coarse-level CNN}(\vartheta_1)$\\
    \Indm
    \textbf{Backward Pass:}\\
    \Indp
    $d\vartheta_1 = Backward_{Coarse-level CNN}(\vartheta_1, dy_{pred})$
	
	\ForAll{Boundary voxels, $\phi_b$ \emph{\textbf{in parallel}}}{
				$d\eta_b = d\vartheta_1(\phi_b)$\\
                $d\vartheta_{2_{b}} = Backward_{Coarse-level CNN}(\vartheta_{2_{b}}, d\eta_b)$;\\
                
	}
    \Indm
	\caption{$\text{MRCNN}$}
	\label{Alg:MRCNN}
"
199,1805.12254,"[t!]
    $\theta_1 \leftarrow Pretrain_{Coarse-level CNN}$\\
    \ForAll {Epochs}{
    $\gamma = 3DGradCAM(\vartheta_1(\Phi), \theta_1)$\\
    $y_{pred} \leftarrow Forward_{MRCNN}(\vartheta_2(\gamma(\phi_b)),\vartheta_1)$\\
    $L = Cross-entropy(y_{true}, y_{pred})$\\
    $\theta_1, \theta_2(Update) \leftarrow Backward_{MRCNN}(Loss)$\\
	}
	\caption{$\text{Feedback}_{\text{MRCNN}}$}
	\label{Alg:Feedback}
"
200,1802.04064,"[tb]
\caption{Generic contextual bandit algorithm}
\label{alg:cb}
\begin{algorithmic}
\FOR{$t = 1, \ldots$}
	\STATE Observe context~$x_t$, compute $p_t := \verb+explore+(x_t)$;
	\STATE Choose action $a_t \sim p_t$, observe loss $\ell_t(a_t)$;
	\STATE $\verb+learn+(x_t, a_t, \ell_t(a_t), p_t)$;
\ENDFOR
\end{algorithmic}
"
201,1802.04064,"[tb]
\caption{$\epsilon$-greedy}
\label{alg:egreedy}
\textbf{Inputs}: exploration rate $\epsilon > 0$ (or $\epsilon = 0$ for Greedy), off-policy learning oracle \verb+opl_update+ (IPS/DR/IWR for $\epsilon$-Greedy, IWR for Greedy).

\textbf{Global state}: policy $\pi$.

$\verb+explore+(x_t)$:
\begin{algorithmic}
  \STATE {\bfseries return} $p_t(a) = \epsilon / K + (1 - \epsilon) \1\{\pi(x_t) = a\}$;
\end{algorithmic}
$\verb+learn+(x_t, a_t, \ell_t(a_t), p_t)$:
\begin{algorithmic}
  \STATE $\verb+opl_update+(\pi, (x_t, a_t, \ell_t(a_t), p_t(a_t)))$;
\end{algorithmic}
"
202,1802.04064,"[th]
\caption{Bag / Online BTS}
\label{alg:bag}
\textbf{Inputs}: number of policies~$N$, off-policy learning oracle \verb+opl_update+ (IPS/DR/IWR).

\textbf{Global state}: policies~$\pi^1, \ldots, \pi^N$.

$\verb+explore+(x_t)$:
\begin{algorithmic}
  \STATE {\bfseries return} $p_t(a) \propto |\{i : \pi^i(x_t) = a\}|$;\footnotemark
\end{algorithmic}
$\verb+learn+(x_t, a_t, \ell_t(a_t), p_t)$:
\begin{algorithmic}
  \FOR{$i = 1, \ldots, N$}
  	\STATE $\tau^i \sim Poisson(1)$; \hfill \COMMENT{or $\tau^1 := 1$ for bag-greedy}
    \FOR{$s = 1, \ldots, \tau^i$}
      \STATE $\verb+opl_update+(\pi^i, (x_t, a_t, \ell_t(a_t), p_t(a_t)))$;
    \ENDFOR
  \ENDFOR
\end{algorithmic}
"
203,1802.04064,"[tb]
\caption{Cover}
\label{alg:cover}
\textbf{Inputs}: number of policies~$N$, exploration parameters $\epsilon_t = \min(1/K, 1/\sqrt{Kt})$ and $\psi > 0$, loss estimator \verb+estimator+ (IPS or DR).

\textbf{Global state}: policies $\pi^1, \ldots, \pi^N$.

$\verb+explore+(x_t)$:
\begin{algorithmic}
  \STATE $p_t(a) \propto |\{i : \pi^i(x_t) = a\}|$;
  \STATE {\bfseries return} $\epsilon_t + (1 - \epsilon_t) p_t$; \hfill \COMMENT{for cover}
  \STATE {\bfseries return} $p_t$; \hfill \COMMENT{for cover-nu}
\end{algorithmic}
$\verb+learn+(x_t, a_t, \ell_t(a_t), p_t)$:
\begin{algorithmic}
  \STATE $\hat{\ell}_t := \verb+estimator+(x_t, a_t, \ell_t(a_t), p_t(a_t))$;
  \STATE $\verb+csc_update+(\pi^1, (x_t, \hat \ell_t))$;
  \FOR{$i = 2, \ldots, N$}
    \STATE $q_i(a) \propto |\{j \leq i - 1 : \pi^j(x_t) = a\}|$;
    \STATE $\hat{c}(a) := \hat{\ell}_t(a) - \frac{\psi \epsilon_t}{\epsilon_t + (1 - \epsilon_t)q_i(a)}$;
  	\STATE $\verb+csc_update+(\pi^i, (x_t, \hat{c}))$;
  \ENDFOR
\end{algorithmic}
"
204,1802.04064,"[tb]
\caption{RegCB}
\label{alg:regcb}
\textbf{Inputs}: exploration parameters~$C_0 > 0$ and~$\Delta_{t,C_0}$ as in~\eqref{eq:regcb_delta}.

\textbf{Global state}: regressor~$f$.

$\verb+explore+(x_t)$:
\begin{algorithmic}
  \STATE $l_t(a) := \verb+lcb+(f, x_t, a, \Delta_{t,C_0})$;
  \STATE $u_t(a) := \verb+ucb+(f, x_t, a, \Delta_{t,C_0})$;
  \STATE $p_t(a) \propto \1\{a \in \arg\min_{a'} l_t(a')\}$; \hfill \COMMENT{RegCB-opt variant}
  \STATE $p_t(a) \propto \1\{l_t(a) \leq \min_{a'} u_t(a')\}$; \hfill \COMMENT{RegCB-elim variant}
  \STATE {\bfseries return} $p_t$;
\end{algorithmic}
$\verb+learn+(x_t, a_t, \ell_t(a_t), p_t)$:
\begin{algorithmic}
  \STATE $\verb+reg_update+(f, (x_t, a_t, \ell_t(a_t)))$;
\end{algorithmic}
"
205,1802.04064,"[h]
\caption{Online regression update}
\label{alg:reg_update}
\textbf{Input}: step-size~$\eta$.

$\verb+reg_update+(f_\theta, (x, a, y, \omega))$:
\begin{algorithmic}
  \STATE $\theta := \theta - \eta \omega (\theta^\top \Phi(x,a) - y) \Phi(x, a)$
\end{algorithmic}
"
206,1802.04064,"[h]
\caption{CSC implementation and online update}
\label{alg:csc_update}
\textbf{Input}: number of actions~$K$.


$\verb+call+(\pi_f, x)$: {\it (implementation of the call~$\pi_f(x)$)}
\begin{algorithmic}
	\STATE \textbf{return} $\arg\min_a f(x, a)$;
\end{algorithmic}

$\verb+csc_update+(\pi_f, (x, c))$:
\begin{algorithmic}
  	\FOR{$a = 1, \ldots, K$}
  		\STATE $\verb+reg_update+(f, (x, a, c(a)))$;
  	\ENDFOR
\end{algorithmic}
"
207,1802.04064,"[h]
\caption{IPS and DR Loss estimators}
\label{alg:loss_estimators}
\textbf{Global state}: loss regressor for DR~$\hat \ell$.

\vspace{0.2cm}
$\verb+ips_estimator+(x_t, a_t, y_t, p_t)$:
\begin{algorithmic}
	\STATE $\hat{\ell}_t(a) := \frac{y_t}{p_t} \1\{a = a_t\}$;
	\STATE \textbf{return} $\hat \ell_t$;
\end{algorithmic}

\vspace{0.2cm}
$\verb+dr_estimator+(x_t, a_t, y_t, p_t)$:
\begin{algorithmic}
	\STATE $\verb+reg_update+(\hat \ell, (x_t, a_t, y_t))$;
	\STATE $\hat{\ell}_t(a) := \frac{y_t - \hat{\ell}(x_t, a_t)}{p_t} \1\{a = a_t\} + \hat{\ell}(x_t, a)$;
	\STATE \textbf{return} $\hat \ell_t$;
\end{algorithmic}

"
208,1802.04064,"[h]
\caption{Off-policy learning updates}
\label{alg:opl_updates}
$\verb+ips_opl_update+(\pi, (x_t, a_t, y_t, p_t))$:
\begin{algorithmic}
	\STATE $\hat \ell_t := \verb+ips_estimator+(x_t, a_t, y_t, p_t)$;
	\STATE $\verb+csc_oracle+(\pi, (x_t, \hat \ell_t))$;
\end{algorithmic}

\vspace{0.2cm}
$\verb+dr_opl_update+(\pi, (x_t, a_t, y_t, p_t))$:
\begin{algorithmic}
	\STATE $\hat \ell_t := \verb+dr_estimator+(x_t, a_t, y_t, p_t)$;
	\STATE $\verb+csc_oracle+(\pi, (x_t, \hat \ell_t))$;
\end{algorithmic}

\vspace{0.2cm}
$\verb+iwr_opl_update+(\pi_f, (x_t, a_t, y_t, p_t))$:
\begin{algorithmic}
	\STATE $\verb+reg_oracle+(f, (x_t, a_t, y_t, 1/p_t))$;
\end{algorithmic}
"
209,1802.04064,"[tb]
\caption{Active $\epsilon$-greedy}
\label{alg:egreedy_active}
$\pi_1$; $\epsilon$; $C_0 > 0$.

$\verb+explore+(x_t)$:
\begin{algorithmic}
  \STATE $A_t = \{a : \verb+loss_diff+(\pi_t, x_t, a) \leq \Delta_{t,C_0}\}$;
  \STATE $p_t(a) = \frac{\epsilon}{K} \1\{a \in A_t\} + (1 - \frac{\epsilon |A_t|}{K}) \1\{\pi_t(x_t) = a\}$;
  \STATE {\bfseries return} $p_t$;
\end{algorithmic}
$\verb+learn+(x_t, a_t, \ell_t(a_t), p_t)$:
\begin{algorithmic}
  \STATE $\hat{\ell}_t = \verb+estimator+(x_t, a_t, \ell_t(a_t), p_t(a_t))$;
  \STATE $\hat{c}_t(a) = \begin{cases}
    \hat{\ell}_t(a), &\text{ if }p_t(a) > 0\\
    1, &\text{ otherwise.}
  \end{cases}$
  \STATE $\pi_{t+1} = \verb+csc_oracle+(\pi_t, x_t, \hat{c}_t)$;
\end{algorithmic}
"
210,1802.04064,"[tb]
\caption{active $\epsilon$-greedy: analyzed version}
\label{alg:greedy_active}
\begin{algorithmic}
\STATE {\bfseries Input:} exploration probability $\epsilon$.
\STATE Initialize: $\hat{Z}_0 := \emptyset$.
\FOR{$t = 1, \ldots$}
  \STATE Observe context $x_t$. Let
  \begin{align}
  \pi_t &:= \arg\min_{\pi} L(\pi, \hat{Z}_{t-1}) \nonumber \\
  \pi_{t,a} &:= \arg\min_{\pi : \pi(x_t) = a} L(\pi, \hat{Z}_{t-1}) \label{eq:csc_constraint} \\
  A_t &:= \{a : L(\pi_{t,a}, \hat{Z}_{t-1}) - L(\pi_{t}, \hat{Z}_{t-1}) \leq \Delta_t \} \label{eq:at_def}
  \end{align}
  \STATE Let
  \[
  p_t(a) = \begin{cases}
  	1 - (|A_t| - 1) \epsilon / K, &\text{ if }a = \pi_t(x_t)\\
  	\epsilon / K, &\text{ if }a \in A_t \setminus \{\pi_t(x_t)\} \\
  	0, &\text{ otherwise.}
  \end{cases}
  \]
  \STATE Play action $a_t \sim p_t$, observe $\ell_t(a_t)$ and set $\hat{Z}_t = \hat{Z}_{t-1} \cup \{(x_t, \hat{\ell}_t)\}$, where $\hat{\ell}_t$ is defined in~\eqref{eq:ell_hat}.
\ENDFOR
\end{algorithmic}
"
211,1709.04060,"[t]
	\centering
	\footnotesize
	\begin{algorithmic}
		\algrenewcommand\algorithmicindent{1.0em}%
		\Function{BinaryGEMM}{$W, A, \mathrm{res}, \alpha$}
		\For{$r \gets 0 \dots rows-1$}
			\For{$c \gets 0 \dots cols-1$}
				\For{$d \gets 0 \dots \myceil{depth/wordsize}-1$}
					\State $res[r][c] \pluseq \alpha \cdot \Call{Popcount}{W(r, d) \mathrel{\&} A(c, d)} $
				\EndFor
			\EndFor
		\EndFor

		\EndFunction
	\end{algorithmic}
	\caption{\qnn{1}{1} GEMM using AND-popcount.}
	\label{alg:bingemm}
"
212,1709.04060,"[t]
	\centering
	\footnotesize
	\begin{algorithmic}
		\algrenewcommand\algorithmicindent{1.0em}%
		\Function{BitSerialGEMM}{$W, A, \mathrm{res}$}
		\For{$i \gets 0 \dots w-1$}
		\For{$j \gets 0 \dots a-1$}
		\State $\mathrm{sgnW} \gets (i == w-1 \mathrel{?} -1 : 1)$
		\State $\mathrm{sgnA} \gets (j == a-1 \mathrel{?} -1 : 1)$
		\State \Call{BinaryGEMM}{\bitplane{W}{i}, \bitplane{A}{j}, res, $\mathrm{sgnW} \cdot \mathrm{sgnA} \cdot 2^{i+j}$}
		\EndFor
		\EndFor
		\EndFunction
	\end{algorithmic}
	\caption{Signed \qnn{w}{a} GEMM using \textsc{BinaryGEMM}.}
	\label{alg:bsgemm}
"
213,1711.05512,"[h]
%		\setstretch{1.5}
		\caption{\cslcnn.}\label{sslcnn}
		\SetKwInput{Input}{Input}
		\SetKwInput{Output}{Output}
		
		\Input{Hyperspectral image $P$ partitioned into a labeled training set
			$(\textbf{x},\textbf{y})$ and an unlabeled test set $\textbf{x}^\text{test}$. }
		\Output{Predicted labels $\hat{\mathbf{y}}^\text{test}$ of $\textbf{x}^\text{test}$. }
		\begin{algorithmic}[1]
			\State $(\textbf{x},\textbf{y}) =\text{Augment-train-set}(\textbf{x},\textbf{y},P)$ (see Equations (\ref{eq:noise_augmentation}) and (\ref{eq:gaussian_smoothing}), Section \ref{sssec:data_augmentation})
			\State $\mathcal{M}=\text{Train-model}(\textbf{x},\textbf{y},P)$ (uses loss Equation (\ref{eq:objective_function}))
			\State $\hat{\mathbf{y}}^\text{test}= \text{Predict-test-labels}(\textbf{x}^\text{test},\mathcal{M})$
			\State\Return $\hat{\mathbf{y}}^\text{test}$
		\end{algorithmic}
	"
214,1805.10765,"[!t]
\caption{\small Instance-Aware DPP Inference (IDPP).}
%\textbf{Input}: Candidate boxes $\mathcal{Y}$, similarity matrix $\matr{S}$ and detection quality $\textbf{q}$.
\begin{algorithmic}[1]\itemindent=-0.9pc \small
\label{dpp_infer}
% \REQUIRE, 
\STATE{$Y^* = \emptyset$}
  \WHILE{$\mathcal{Y} \neq \emptyset$}\itemindent=-0.9pc
    \STATE{$j^* = {\argmax}_{j \in \mathcal{Y}} \log(\prod_{ i \in Y^* \cup \{j\} } \textbf{q}_i^2 \cdot \det (\matr{S}_{{Y^* \cup \{j\}}})) $}
    \STATE{$Y = Y^* \cup \{j^*\} $}
    \IF{$\text{Cost}(Y) > \text{Cost}(Y^*)$}\itemindent=-0.9pc
        \STATE{$Y^* \gets Y $}
        \STATE{delete $j^*$ from $\mathcal{Y}$}
    \ELSE\itemindent=-0.9pc
        \RETURN{$Y^*$}
    \ENDIF
  \ENDWHILE
  \RETURN {$Y^*$}
\end{algorithmic} 
"
215,1805.09965,"[H]
  \small
    \caption{LAG-WK}\label{algo:f-iag}
	\begin{algorithmic}[1]
		\State\textbf{Input:}~Stepsize $\alpha>0$, and $\{\xi_d\}$.
%		\State\textbf{Initialize:}~$\bbtheta^1, \{\nabla {\cal L}_m(\hat{\bbtheta}^0_m)\}$.~~~\Comment{at the server}\\
%		\qquad \qquad \, $\nabla {\cal L}_m(\hat{\bbtheta}^0_m)$.~~~~~\Comment{at each worker $m$}
		\State\textbf{Initialize:}~$\bbtheta^1, \{\nabla {\cal L}_m(\hat{\bbtheta}^0_m),\,\forall m\}$.
		%\\\Comment{Variable $\hat{\bbtheta}^k_m$ is a snapshot of previous $\bbtheta^k$.}
		\For {$k= 1, 2,\ldots, K$}
		\State Server \textbf{broadcasts} $\bbtheta^k$ to all workers.
	    \For {worker $m=1, \ldots, M$}
		\State Worker $m$ \textbf{computes} $\nabla {\cal L}_m(\bbtheta^k)$.
	    \State Worker $m$ \textbf{checks} condition \eqref{eq.trig-cond1}.	
		\If {worker $m$ violates \eqref{eq.trig-cond1}}
		\State Worker $m$ \textbf{uploads} $\delta\nabla^k_m$.\\
		\qquad\qquad\Comment{Save $\nabla {\cal L}_m(\hat{\bbtheta}_m^k)=\nabla {\cal L}_m(\bbtheta^k$)}
		\Else \State{Worker $m$ uploads nothing.}
%		\\
%		\qquad\qquad\Comment{Set $\nabla {\cal L}_m(\hat{\bbtheta}_m^k)=\nabla {\cal L}_m(\hat{\bbtheta}_m^{k-\!1})$}}
		\EndIf
		\EndFor
		\State Server \textbf{updates} via \eqref{eq.LAG1}.
		\EndFor
	\end{algorithmic}
  "
216,1805.09965,"[H]
  \small
    \caption{LAG-PS}\label{algo:f-iag2}
	\begin{algorithmic}[1]
		\State\textbf{Input:}~Stepsize $\alpha>0$, $\{\xi_d\}$, and $L_m,\,\forall m$.
		\State\textbf{Initialize:}~$\bbtheta^1, \{\hat{\bbtheta}^0_m,\!\nabla {\cal L}_m(\hat{\bbtheta}^0_m),\forall m\}$.
%		\qquad \qquad \, $\nabla {\cal L}_m(\hat{\bbtheta}^0_m)$.~~~~~\Comment{at each worker $m$}
		\For {$k= 1, 2,\ldots, K$}
%	\State Server \textbf{sends} $\bbtheta^k$ to workers violating \eqref{eq.trig-cond2}.
	    \For {worker $m=1, \ldots, M$}
	    \State Server \textbf{checks} condition \eqref{eq.trig-cond2}.	
	    \If {worker $m$ violates \eqref{eq.trig-cond2}}
	    \State Server \textbf{sends} $\bbtheta^k$ to worker $m$.\\\qquad\qquad\qquad\Comment{Save $\hat{\bbtheta}_m^k=\bbtheta^k$ at server}
%		\State Server \textbf{requests} $\nabla {\cal L}_m(\bbtheta^k)$.
		\State Worker $m$ \textbf{computes} $\nabla\! {\cal L}_m(\bbtheta^k)$.
		\State Worker $m$ \textbf{uploads} $\delta\nabla^k_m$.		
		\Else \State{No actions at server and worker $m$.}
%		\\\qquad\qquad\qquad\Comment{Set $\hat{\bbtheta}_m^k\!=\!\hat{\bbtheta}_m^{k-\!1}\!$ at server}}
	    \EndIf
		\EndFor
		\State Server \textbf{updates} via \eqref{eq.LAG1}.
		\EndFor
	\end{algorithmic}
  "
217,1805.11783,"[H]
   \caption{Estimating $\alpha$-high-density-set}
   \label{alg:alpha-level-set}
\begin{algorithmic}
   \STATE Parameters: $\alpha$ (density threshold), $k$.
   \STATE Inputs: Sample points $X := \{x_1,..,x_n\}$ drawn from $f$.
   \STATE Define $k$-NN radius $r_k(x) := \inf\{ r > 0 : |B(x, r) \cap X| \ge k\}$ and let $\varepsilon := \inf\{ r > 0 : |\{x \in X : r_k(x) > r\}| \le \alpha\cdot n\}$.
   \STATE \textbf{return} $\widehat{H_\alpha}(f) := \{ x \in X : r_k(x) \le \varepsilon \}$.
\end{algorithmic}
"
218,1805.11783,"[H]
   \caption{Trust Score}
   \label{alg:distance-ratio}
   \begin{algorithmic}
   \STATE Parameters: $\alpha$ (density threshold), $k$.
   \STATE Input: Classifier $h : \mathcal{X} \rightarrow \mathcal{Y}$. Training data $(x_1,y_1),..., (x_n,y_n)$. Test example $x$.
\STATE For each $\ell \in \mathcal{Y}$, let $\widehat{H_\alpha}(f_\ell)$ be the output of Algorithm~\ref{alg:alpha-level-set} with parameters $\alpha, k$ and sample points $\{x_j : 1 \le j \le n, y_j = \ell \}$. Then, return the trust score, defined as:
   \vspace{-0.4cm}
   \STATE \begin{align*}
\xi(h, x) := d\left(x, \widehat{H_\alpha}(f_{\widetilde{h}(x)})\right)/d\left(x, \widehat{H_\alpha}(f_{h(x)})\right),
\end{align*}
where $\widetilde{h}(x) = \argmin_{l\in \mathcal{Y}, l\neq h(x)} d\left(x, \widehat{H_\alpha}(f_l)\right)$.
\end{algorithmic}
"
219,1805.11770,"[t]
	\caption{AutoZOOM for black-box attacks on DNNs}
	\label{algo_autozoom}
	\begin{algorithmic}
		\State \textbf{Input:} Black-box DNN model $F$, original example $\bx_0$, distortion measure $\Dist(\cdot)$, attack objective $\Loss(\cdot)$, monotonic transformation $M(\cdot)$, decoder $D(\cdot) \in \{\textnormal{AE},\textnormal{BiLIN}\}$, initial coefficient $\lambda_{\textnormal{ini}}$, query budget $Q$
		\While{query count $\leq Q$}        
		\State \textbf{1. Exploration:} use $\bx=\bx_0+D(\bdelta^\prime)$ and apply the random gradient estimator in \eqref{eqn_grad_rand_avg} with
		$q=1$ 	 to the downstream optimizer (e.g., ADAM) for solving \eqref{eqs_attack_optimization_problem} until an initial attack is found.
		\State \textbf{2. Exploitation (post-success stage):}  continue to fine-tune the adversarial perturbation $D(\bdelta^\prime)$ for solving \eqref{eqs_attack_optimization_problem} while setting $q \geq 1$ in \eqref{eqn_grad_rand_avg}.
		\EndWhile        
		\State \textbf{Output:} Least distorted successful adversarial example		
	\end{algorithmic}
"
220,1805.11769,"[t]
	\caption{FINGER-JSdist (Fast)}
	\label{algo_JSfast}
	\begin{algorithmic}
		   \STATE \textbf{Input:} Two graphs $G$ and $\pG$ from a graph sequence
		   \STATE \textbf{Output:} \textsf{JSdist}($G, \pG$)
		   \STATE 1. Obtain $\overline{G}=\frac{G \oplus \pG}{2}$ and compute $\hH(G)$, $\hH(\pG)$, and $\hH(\overline{G})$ via FINGER-$\hH$ from (\ref{eqn_H_hat})
		   \STATE 2.   $\textsf{JSdist}(G,\pG)=\lb \hH(\overline{G})- \frac{1}{2} [\hH(G) + \hH(\pG)] \rb^{1/2}$
	\end{algorithmic}
"
221,1805.11769,"[t]
	\caption{FINGER-JSdist (Incremental)}
	\label{algo_JSinc}
	\begin{algorithmic}
		   \STATE \textbf{Input:} A graph $G$,  graph changes $\Delta G$, and $\tH(G)$ 
		   \STATE \textbf{Output:} \textsf{JSdist}($G,G \oplus \Delta G$)
		\STATE 1. Compute $\tH(G \oplus \frac{\Delta G}{2})$ and  $\tH(G \oplus \Delta G)$ via FINGER-$\tH$ from (\ref{eqn_tH_inc}) and Theorem \ref{thm_incremental}
		   \STATE 2.  $\textsf{JSdist}(G,G \oplus \Delta G)=$
		   \STATE $\lb \tH(G \oplus \frac{\Delta G}{2})- \frac{1}{2} [\tH(G) + \tH(G \oplus \Delta G)] \rb^{1/2}$
	\end{algorithmic}
"
222,1805.11769,"[H]
%			\caption{FINGER-JSdist (Fast)}
%			\label{algo_JSfast}
%			\begin{algorithmic}			
%				\State \textbf{Input:} Graphs $G$ and $\pG$ from graph sequence 
%				\State \textbf{Output:} \textsf{JSdist}($G, \pG$)
%				\State 1. Obtain $\overline{G}=\frac{G \oplus \pG}{2}$ and compute $\hH(G)$, $\hH(\pG)$, and $\hH(\overline{G})$ via FINGER-$\hH$ from (\ref{eqn_H_hat})
%				\State 2.  $\textsf{JSdist}(G,\pG)=$ 
%				\State $\lb \hH(\overline{G})- \frac{1}{2} [\hH(G) + \hH(\pG)] \rb^{1/2}$
%			\end{algorithmic}
%		"
223,1805.11769,"[H]
%			\caption{FINGER-JSdist (Incremental)}
%			\label{algo_JSinc}
%			\begin{algorithmic}
%				\State \textbf{Input:} Graph $G$,  graph changes $\Delta G$, and $\tH(G)$ 
%				\State \textbf{Output:} \textsf{JSdist}($G,G \oplus \Delta G$)
%				\State 1. Compute $\tH(G \oplus \frac{\Delta G}{2})$ and  $\tH(G \oplus \Delta G)$ via FINGER-$\tH$ from (\ref{eqn_tH_inc}) and Theorem \ref{thm_incremental}
%				\State 2.  $\textsf{JSdist}(G,G \oplus \Delta G)=$
%				\State $\lb \tH(G \oplus \frac{\Delta G}{2})- \frac{1}{2} [\tH(G) + \tH(G \oplus \Delta G)] \rb^{1/2}$
%			\end{algorithmic}
%		"
224,1805.11144,"
\DontPrintSemicolon
initialize list of all meta-blocks $M$\;
initialize sorted list $S$ (empty)\;
set active read block $a$ to be the largest read block\;
set active meta-block $c$ to be any meta-block containing $a$\;
\While{$M$ is not empty}{
	$X \leftarrow \{m \in M \mid a \in m\}$\;
	\uIf{$X = \emptyset$}{
		$X \leftarrow M$\;
		$a \leftarrow$ largest read block in $c$\;
	}
	$Y \leftarrow \{x \in X \mid c \subseteq x\}$\;
	\uIf{$Y = \emptyset$}{
		$Y \leftarrow X$\;
	}
	$Z \leftarrow \{y \in Y \mid | y \oplus c | = \min_{n \in Y} | n \oplus c |\}$\;
	\While{|Z| > 1}{
	  $m \leftarrow$ the next largest read block in $c$\;
		$Z \leftarrow \{z \in Z \mid m \in z\}$\;
	}
	remove the remaining $z \in Z$ from $M$ and append it to $S$\;
	$c \leftarrow z$\;
}
\caption{Meta-block sorting algorithm}
\label{alg:metablocksort}
"
225,1805.11144,"
\DontPrintSemicolon
sort the list of read blocks $B$ by increasing order of size\;
\For{$i = 1 \rightarrow n$}{
	\For{$b \in B$}{
		$O \leftarrow$ the set of \texttt{Operators} associated with $b$\;
		sort $O$ to match the order of the \texttt{Signals} in $b$\;
		$C \leftarrow$ the set of read blocks associated with $O$\;
		\For{$c \in C$}{
			sort the signals in $c$ to match the order of $O$\;
		}
	}
	if the order of the \texttt{Signals}/\texttt{Operators} did not change, terminate early\;
}
\caption{Signal/Operator sorting algorithm}
\label{alg:sigopsort}
"
226,1805.11706,"[!h]
\caption{Algorithmic description of forward-KL non-parameterized SPU}
\label{alg:spu_algo_des}
\begin{algorithmic}[1]
\Require A neural net $\pt$ that parameterizes the policy.
\Require A neural net $V_{\phi}$ that approximates $V^{\pt}$.
\Require General hyperparameters: $\gamma, \beta$ (advantage estimation using GAE), $\alpha$ (learning rate), N (number of trajectory per iteration), T (size of each trajectory), M (size of training minibatch).
\Require Algorithm-specific hyperparameters: $\delta$ (aggregated KL constraint), $\epsilon$ (disaggregated constraint), $\lambda$, $\zeta$ (max number of epoch).
\For {k = 1, 2, \ldots}
\State under policy $\ptk$, sample N trajectories, each of size T $(s_{it}, a_{it}, r_{it}, s_{i(t+1)}), \quad i = 1, \ldots, N, t = 1, \ldots, T$
%\NoNumber \#Calculate Advantage Value and Target for $V_\phi	$
%\State $\delta_{it} \leftarrow r_{it} + \gamma \Vphk(s_{i(t+1)}) - \Vphk(s_{it}), \quad i = 1, \ldots, N, t = 1, \ldots, T$
%\State $A_{it} \leftarrow \sumtb{T-t}{l=0} (\gamma \beta)^l \delta_{i(t+l)} , \quad t = 1, \ldots, T$
%\State $\mu_{A_i} \leftarrow \dfrac{1}{T} \sum_t A_{it}, \quad i = 1, \ldots, N$
%\State $std_{A_i} \leftarrow \sqrt{\dfrac{1}{T} \underset{t}{\sum} (A_{it} - \mu_{A_i})^2}, \quad i = 1, \ldots, N$
%\State $A_{it} \leftarrow \dfrac{A_{it} - \mu_{A_i}}{std_{A_i}}, \quad i = 1, \ldots, N, t = 1, \ldots, T$
%\State $V^{targ}(s_{it}) \leftarrow r_{it} + \gamma (1 - \beta) \Vphk(s_{i(t+1)}) + \gamma \beta V^{targ}(s_{i(t+1)}), \quad i = 1, \ldots, N, t = 1, \dots, T$
%\NoNumber \#Finish Calculating Advantage Value and State Value
\State Using any advantage value estimation scheme, estimate $A_{it}, \quad i = 1, \ldots, N, t = 1, \dots, T$
%\NoNumber \#Update parameters
\State $\theta \leftarrow \theta_k$
\State $\phi \leftarrow \phi_k$
\For {$\zeta$ epochs}
\State Sample M samples from the N trajectories, giving $\{s_1, a_1, A_1, \ldots, s_M, a_M, A_M\}$
\State $L(\phi) = \dfrac{1}{M} \sumtb{}{m} (V^{targ}(s_m) - V_\phi (s_m))^2$
\State $\phi \leftarrow \phi - \alpha \nabla_\phi L(\phi)$
\State $L(\theta) = \frac{1}{M} \underset{m}{\sum} \left[\nt \KL{\pt}{\polk}[s_m] - \dfrac{1}{\lambda} \dfrac{\nt \pt (a_m|s_m)}{\polk (a_m|s_m)}  A_m \right]  \mathbbm{1}_{\KL{\pt}{\polk}[s_m] \leq \epsilon} $
\State $\theta \leftarrow \theta - \alpha L(\theta)$
\If {$\dfrac{1}{m} \sum_m \KL{\pi}{\ptk}[s_m] > \delta$}
\State Break out of for loop
\EndIf
\EndFor
\State $\theta_{k+1} \leftarrow \theta$
\State $\phi_{k+1} \leftarrow \phi$
%\NoNumber \#Finish update parameters
\EndFor
\end{algorithmic}	
"
227,1805.11593,"[!h]
\caption{The algorithm used by the learner to estimate the action-value 
function.}
\label{alg:learner}
\begin{small}
\begin{algorithmic}
\State $\theta^{(0)} \leftarrow$ Random sample
\For{$k = 1,2,...$}
	\State $\theta^{(k)} \leftarrow \theta^{(k - 1)}$
	\For{$j = 1,...,T_\text{update}$}
		\Comment{$T_\text{update}$ is the target network update period}
		\State $(t_i, p_i)_{i = 1}^N \leftarrow 
		$\Call{SamplePrioritized}{N}
		\Comment{Sample 75\% agent and 25\% expert transitions}		
		\State $\theta^{(k)} \leftarrow $ 
		\Call{AdamStep}{$L(\theta^{(k)}; 
		(t_i)_{i = 1}^N, (p_i)_{i = 1}^N, \theta^{(k - 1)})$}
		\Comment{Update the parameters using Adam}
		\State $(p_i)_{i = 1}^N \leftarrow \left(\left| 
		L_\text{TD}(\theta^{(k)}; 
				t_i, 1, \theta^{(k - 1)}) \right| \right)_{i = 1}^N$
		\Comment{Compute the updated priorities based on the TD error}
		\State \Call{UpdatePriorities}{$(t_1, p_1),...,(t_N, p_N)$}
		\Comment{Send the updated priorities to the replay buffers}
	\EndFor
\EndFor
\end{algorithmic}
\end{small}
"
228,1711.11343,"[t]
	\begin{lstlisting}[language=Java,numbers=left,basicstyle={\sffamily},breaklines=true,showstringspaces=false,tabsize=2,numbersep=1em,xleftmargin=2em,xrightmargin=0em,emph={function, in, all, each, to},emphstyle={\textbf},escapechar={ß}]
function ß\textbf{WEASEL\_MUSE}ß(mts, l, wLen)
	bag = empty BagOfPattern
	// extract from each dimension
	for each dimId in mts:
	  // use multiple window lenghts
	  for each window-length w in wLen:
	    for each (prevWindow, window) in ß\texttt{SLIDING\_WINDOWS}ß(mts[dimId], wLen):
	
	    // BOP computed from unigrams
	    word = ß\texttt{SFA}ß(window,l)
	    unigram = concat(dim,w,word)
    	bag[unigram].increaseCounts()
	
	    // BOP computed from bigrams
    	prevWord=ß\texttt{SFA}ß(prevWindow,l)	
    	bigram =concat(dim,w,prevWord,word)
    	bag[bigram].increaseCounts()
    	
	// feature selection using ChiSquared
	return ß\texttt{CHI\_SQUARED\_FILTER}ß(bag)
	\end{lstlisting}
	
	\caption{Build one BOP model using SFA,
		multiple window lengths, bigrams and the Chi-squared test for feature
		selection. $l$ is the number of Fourier values to keep and $wLen$ are the window lengths used for sliding window extraction.\label{alg:The-WEASEL-representation}}
"
229,1805.11380,"
\caption{Mapping particle filter algorithm}\label{algo}
%\hrule\vskip 2mm
\small
\begin{algorithmic}
\Require{Given $\v x_{k-1}^{(1:N_p)}$,  $\v y_k$, $\mathcal{M}(\cdot)$, $p(\v y|\v x)$ and $p(\gv\eta)$}
  \For{$j=1,N_p$}
    \Let{$\v x_{k,0}^{(j)}$}{$\mathcal{M}(\v x_{k-1}^{(j)})+\gv \eta_{k}$}\Comment{Forecast stage}
    \EndFor

%\While{i<I and $N_{eff}>N_t$}\Comment{Mapping iterations}
\Repeat  \Comment{Mapping iterations}
   \For{$j=1,N_p$}
   \Let{$\v x^{(j)}_{k,i}$}{$\v x^{(j)}_{k,i-1}- \epsilon \, \nabla KL(\v x^{(j)}_{k,i-1})$}\Comment{$\nabla KL$ from \reff{gradKL1}}% and \reff{gradp}}
   \EndFor
   \Let{i}{i+1}
\Until Stopping criterion met
\Ensure{$\v x_{k,i}^{(1:N_p)}$}
\normalsize
\end{algorithmic}
"
230,1805.09496,"[!htb]
\caption{Sampling Reset Procedure}
\label{algo2}
\begin{algorithmic}[1]
\IF {the current sampling environment is the real environment}
    \STATE Initialize data set $D=\emptyset$, quality set $G=\emptyset$.
    \FOR {$i = 1:M_1$}
        \STATE Generate one initial state $s_0$ and compute its quality $\Phi(s_0)$.
        \STATE Append $s_0$ to $D$ and append $\Phi(s_0)$ to $G$.
        \IF {$i>M_2$ and $\Phi(s_0) \ge \max(G)$}
         \STATE Break.
        \ENDIF
    \ENDFOR
    \STATE  Return the last state of $D$.
\ELSE
    \IF {$u_{[0, 1]}<a_1$}
        \STATE Randomly select a state $s$ from the real data memory.
        \STATE Set the cyber environment to state $s$.
        \STATE Return $s$.
    \ELSE
        \STATE Randomly initialize the cyber environment.
        \STATE Return the current state of the cyber environment.
    \ENDIF
\ENDIF

%\EndProcedure
\end{algorithmic}
"
231,1805.09496,"[!htb]
\caption{Intelligent Trainer Enhanced Model-Based DRL Training Algorithm}
\label{algo1}
\begin{algorithmic}[1]
    \STATE Initialization: initialize the trainer agent (with a DQN network), the training process environment, and the target controller. Initialize real data memory and cyber data memory as an empty set. Sample a small data set of size $o$ to initialize the cyber emulator and initialize the real environment.
    \STATE Set number of total samples generated from real environment $n=0$. Set the maximum number of samples allowed to use as $N$.
    \STATE  $//$\emph{Training Process}:
    \WHILE{$n < N$}
        \STATE Generate action $a$ from the trainer agent.
  \STATE  $//$\emph{One step in TPE}:
   \STATE Train the target controller if there is enough data in its memory buffer.
   \STATE Sample $K_r$ data points from real environment according to the sampling reset Algorithm \ref{algo2}, and append the data to the real data memory.
   \STATE Sample $K_c$ data points from the cyber environment according to the sampling reset Algorithm \ref{algo2}, and append the data to the cyber data memory.
   \STATE Train the dynamic model.
        \STATE Update $n$.
        \STATE Collect the state, action and reward data of TPE.
        \STATE Update the trainer agent.
    \ENDWHILE
%\EndProcedure
\end{algorithmic}
"
232,1805.09496,"[!htb]
\caption{Ensemble Trainer Algorithm}
\label{algo3}
\begin{algorithmic}[1]
    \STATE Initialization: initialize the three trainer agents and the corresponding training process environments, along with the target controllers. Run the initialization process for each trainer. Initialize the best player to be NoDyna trainer and the probability to use best player to sample is $p_{ref}$.
    \STATE Set number of total samples generated from real environment $n=0$. Set maximum number of samples allowed to use as $N$.
    \STATE  $//$\emph{Training Process}:
    \WHILE{$n < N$}
     \FOR {trainer $i$ $\in$ {0, 1, 2}}
         \STATE Generate action $a$ from the trainer agent.
   \STATE  $//$\emph{One step in TPE}:
    \STATE Execute memory sharing procedure.
    \STATE Train the target controller if there is enough data in its memory buffer.
    \STATE Sample $K_r/3$ data points from real environment with reference sampling probability $p_{ref}$, and append the data to the real data memory.
    \STATE Sample data from cyber environment according to the trainer action, and append the data to the cyber data memory.
    \STATE Share the real data memory across all trainers.
    \STATE Train the dynamic model of the current trainer.
         \STATE Update $n$.
         \STATE Collect the state, action and raw reward data of TPE.
        \ENDFOR
        \STATE Compute reward for each trainer from the raw reward data and calculate the accumulative reward $R_i$ for trainers $i = 0, 1, 2$.
        \STATE Store TPE data of all three trainers into the DQN memory to train the intelligent trainer.
        \STATE Update the trainer agents.
        \STATE Execute Algorithm \ref{algo4} to do performance skewness analysis and weight transfer, update $p_{ref}$.
    \ENDWHILE
%\EndProcedure
\end{algorithmic}
"
233,1805.09496,"[!htb]
\caption{Performance Skewness Analysis Procedure}
\label{algo4}
\begin{algorithmic}[1]

\IF {$n_c > C$}
    \STATE Compute accumulative reward of trainer $i$ as $R_i$ for $i = 0, 1, 2$.
    \STATE Update best trainer index as $\argmax_i(R_i)$.
    \STATE Compute the skewness ratio $\phi$ for the best player.
    \STATE Update best player reference probability $p_{ref}$ according to \eqref{eq:pref}.
    \IF {DQN trainer is not the best trainer}
        \STATE Do weight transfer from the best trainer to DQN trainer.
    \ENDIF
    \STATE Reset $n_c=0$.
\ENDIF

%\EndProcedure
\end{algorithmic}
"
234,1805.10014,"
\DontPrintSemicolon
\KwIn{Graph $G= (V, E, \ell, \tau)$, depth $h$, labeling $\ell: V \rightarrow {\mathcal L}$, base kernel $\kappa$}
%\KwOut{The index of first location witht he same value as in a previous location in the sequence}
\For{$v \in V$}{
	Traverse the subgraph $T_v$ rooted at $v$ up to depth $h$\;
	Collect the node labels $\ell(u): u \in T_v$ in the order specified by $\tau_v$ into a string $S_v$\;
    Sketch the explicit feature map $\phi_\kappa(S_v)$ for the base string kernel $\kappa$ (without storing $S_v$)

}
$\Phi_\kappa(G) \gets \sum_{v\in V} \phi_\kappa(S_v)$\;
\Return{$\Phi_\kappa(G)$}\;
\caption{{\sc ExplicitGraphFeatureMap}}
\label{alg:general}
"
235,1805.10014,"[H]
\DontPrintSemicolon
\KwIn{Graph $G= (V, E, \ell, \tau)$, depth $h$, labeling $\ell: V \rightarrow S$}
%\KwOut{The index of first location witht he same value as in a previous location in the sequence}
\For{$v \in V$}{
	%$\mathcal{L}_v \gets [\$ \text{ for } k = 0 \text{ to } h]$\;
	$s_v^0 = \ell(v)$%$\mathcal{L}_v[0] \gets \ell(v)$
}
\For{$i = 1$ to $h$}{
\For{$v \in V$}{
	$s_v^i = \$$ \hspace*{5mm} \texttt{//$\$$ is the empty string} \;
\For{$u \in \tau_v(N_v)$}{
	$s_v^i \gets s_v^i$.append$(s_u^{i-1})$%$\mathcal{L}_v[i] \gets \mathcal{L}_v[i]$.append$(\mathcal{L}_u[i-1])$
}
}
}
\For{$v \in V$}{
%$\text{label}_v \gets \$$\;
$S^h_v = \$$\;
\For{$i = 0$ to $h$}{
$S^h_v \gets S^h_v$.append$(s_v^i)$
%$S^h_v \gets \text{label}_v$.append$(\mathcal{L}_v[i])$
}
}
\caption{{\sc Breadth-first search}}
\label{alg:bfs}
"
236,1805.10014,"[H]
    \DontPrintSemicolon
\KwIn{Graph $G= (V, E, \ell, \tau)$, depth $h$, labeling $\ell: V \rightarrow S$}
%\KwOut{The index of first location witht he same value as in a previous location in the sequence}
\For{$v \in V$}{
	%$\mathcal{L}_v \gets [\$ \text{ for } k = 0 \text{ to } h]$\;
	\For{$i = 1$ to $h$}{
	$s_v^i \gets \ell(v)$	
	%$\mathcal{L}_v[i] \gets \ell(v)$
	}
}
\For{$i = 1$ to $h$}{
\For{$v \in V$}{
\For{$u \in \tau_v(N_v)$}{
	%$\mathcal{L}_v[i] \gets \mathcal{L}_v[i]$.append$(\mathcal{L}_u[i-1])$
	$s_v^i \gets s_v^i$.append$(s_u^{i-1})$
}
}
}
\For{$v \in V$}{
$S_v^h \gets s_v^h$%$\text{label}_v \gets \mathcal{L}_v[h]$\;
}
\caption{{ \sc Weisfeiler-Lehman}}
\label{alg:wl}
  "
237,1805.11243,"[tb]
\caption{$\textsc{ECA-Trim}(\Is,\Es,b)$}\label{alg:trim-nb}
\textbf{Input:} \\  
  $\alpha$ : Bayesian classifier $(C,\Fs,T)$; \quad $B$ : budget \\
\textbf{Data:} \\
  $\Is \leftarrow \emptyset$, $\Es \leftarrow \emptyset$: set of included/excluded features \\
  $b \leftarrow B$: remaining budget \\
  $\Fs^\star,M^\star,T^\star$ : optimal subset, MAA value, and threshold\\
\textbf{Output:} Optimal trimmed classifier $\beta^\star=(C,\Fs^\star,T^\star)$
\\\hrule
  \begin{algorithmic}[1]
    \If {$b \geq 0$}  \label{line:budget-left}
        \State $(m,T_m) \leftarrow \MAA(\Is)$ \label{line:compute-score}
        \If {$m > M^\star$} $M^\star \leftarrow m$; $\Fs^\star \leftarrow \Is$; $T^\star \leftarrow T_m$ \label{line:update}
        \EndIf
    \EndIf
    \If {$\min_{F\in\Fs\setminus(\Is\cup\Es)}\text{cost}(F) \leq b$}
        \State $m \leftarrow \text{UB}(\Fs\setminus\Es)$
        \If {$m \leq M^\star$} \textbf{return} \label{line:backtrack} \EndIf
        \State $F \leftarrow$ a feature from $\Fs\setminus(\Is\cup\Es)$
        \State $\textsc{ECA-Trim}\big(\Is\cup\{F\},\Es,b-\text{cost}(F)\big)$
        \State $\textsc{ECA-Trim}\big(\Is,\Es\cup\{F\},b\big)$
    \EndIf
  \end{algorithmic}
"
238,1805.11243,"[tb]
\caption{$\textsc{Compute-MAA}$}\label{alg:maa}
\textbf{Input:} \\
  $\alpha$ : Bayesian network classifier $(C,\Fs,T)$;\quad $\Fs^\prime \subset \Fs$ \\
\textbf{Data:} \\
  $\text{CPR}(i) \leftarrow \Pr(c \given \fs_i^\prime)$ for all $i$ \\
  $\text{MAR}(i) \leftarrow \Pr(\fs_i^\prime)$ for all $i$ \\
  $\text{POS}(i) \leftarrow \SDP_{C,T,0}((\Fs\setminus\Fs^\prime)\given\fs_i^\prime)$ for all $i$ \\
\textbf{Output:} The score $\MAA(\Fs^\prime)$ and the optimal threshold $T^\prime$
\\\hrule
  \begin{algorithmic}[1]
    \State Sort instances $\fs_i^\prime$ in nondecreasing order of $\text{CPR}(i)$
    \State $m \leftarrow \sum_i \text{POS}(i)\cdot\text{MAR}(i)$;\quad$m^\star \leftarrow m$
    \State $t^\star \leftarrow [0,\text{CPR}(1)]$
    \For {$i$ in $1,2,\dots$}
        \State $m \leftarrow m - \text{MAR}(i)\cdot(2\text{POS(i)} - 1)$ \label{line:eca-update}
        \If {$m > m^\star$} 
            \State $m^\star \leftarrow m$;\;$t^\star \leftarrow (\text{CPR}(i), \text{CPR}(i+1)]$
        \EndIf
    \EndFor
    \State \textbf{return} $\MAA(\Fs^\prime)=m^\star$ and any $T^\prime \in t^\star$
  \end{algorithmic}
"
239,1805.11224,"[t]
	\KwIn{training data: $\{\mathbf{x}^{(n)}, \mathbf{y}^{(n)}\}_{n=1}^N$;
		the reference policy: $\pi_\mathcal{R}(s, \mathbf{y})$.}
	\KwOut{classifier $p(a|s)$.}
	$D \gets \emptyset$\; \label{algo:generic:gen_start}
	\For{$n \gets 1 ... N$}{
		$t \gets 0$\;
		$s_t \gets s_0(\mathbf{x}^{(n)})$\;
		\While{$s_t \notin \mathcal{S}_T$}{
			$a_t \gets \pi_\mathcal{R}(s_t, \mathbf{y}^{(n)})$\;
			$D \gets D \cup \{s_t\}$\;
			$s_{t+1}\gets \mathcal{T}(s_t, a_t)$\;
			$t \gets t + 1$\;
		}
	} \label{algo:generic:gen_end}
	optimize \(\mathcal{L}_{NLL}\)\;
	\label{algo:generic:optim}
	\caption{Generic learning algorithm for search-based structured prediction.
	}\label{algo:generic}
"
240,1805.11224,"[t]
	\KwIn{training data: $\{\mathbf{x}^{(n)}, \mathbf{y}^{(n)}\}_{n=1}^N$;
		the reference policy: $\pi_\mathcal{R}(s, \mathbf{y})$;
		the exploration policy: $\pi_\mathcal{E}(s)$ 
		which samples an action from the annealed ensemble $q(a\mid s)^{\frac{1}{T}}$}
	\KwOut{classifier $p(a\mid s)$.}
		$D \gets \emptyset$\; 
	\For{$n \gets 1 ... N$}{
		$t \gets 0$\;
		$s_t \gets s_0(\mathbf{x}^{(n)})$\;
		\While{$s_t \notin \mathcal{S}_T$}{
			\eIf{distilling from reference}{
				$a_t \gets \pi_\mathcal{R}(s_t, \mathbf{y}^{(n)})$\;
			}{
				$a_t \gets \pi_\mathcal{E}(s_t)$\;
			}
			$D \gets D \cup \{s_t\}$\;
			$s_{t+1}\gets \mathcal{T}(s_t, a_t)$\;
			$t \gets t + 1$\;
		}
	}
	\eIf{distilling from reference}{
	  	optimize $\alpha \mathcal{L}_{KD} + (1 - \alpha) \mathcal{L}_{NLL}$\;
	}{
		optimize $\mathcal{L}_{KD}$\;
	}
	\caption{Knowledge distillation for search-based structured prediction.}\label{algo:distill_ref}
"
241,1802.01737,"[t!]
\caption{GIGA: Greedy Iterative Geodesic Ascent}\label{alg:giga}
\begin{algorithmic}[1]
\Require $(\mcL_n)_{n=1}^N$, $M$, $\left<\cdot, \cdot\right>$ 
\LineComment{Normalize vectors and initialize weights to 0 }
\State $\mcL \gets \sum_{n=1}^N \mcL_n$ 
\State $\forall n \in [N] \, \, \ell_n \gets \frac{\mcL_n}{\|\mcL_n\|}$,  $\ell \gets \frac{\mcL}{\|\mcL\|}$
\State $w_0 \gets 0$, $\ell(w_0) \gets 0$
\For{$t \in \{0, \dots, M-1\}$}
\LineComment{Compute the geodesic direction for each data point }
\State $d_t \gets \frac{\ell - \left<\ell, \ell(w_t)\right>\ell(w_t)}{\|\ell - \left<\ell, \ell(w_t)\right>\ell(w_t)\|}$
\State  $\forall n \in [N]$, $d_{tn} \gets \frac{\ell_n - \left<\ell_n, \ell(w_t)\right>\ell(w_t)}{\|\ell_n - \left<\ell_n, \ell(w_t)\right>\ell(w_t)\|}$
\LineComment{Choose the best geodesic }
\State $n_t \gets \argmax_{n \in [N]} \left<d_t, d_{tn}\right>$
\State $\zeta_0 \gets \left<\ell, \ell_{n_t}\right>$, $\zeta_1 \gets \left<\ell, \ell(w_t)\right>$, $\zeta_2 \gets\left<\ell_{n_t}, \ell(w_t)\right>$
\LineComment{Compute the step size }
\State $\gamma_t \gets \frac{\zeta_0 - \zeta_1\zeta_2}{(\zeta_0 - \zeta_1\zeta_2) + (\zeta_1 - \zeta_0\zeta_2)}$
\LineComment{Update the coreset }
\State $\ell(w_{t+1}) \gets \frac{(1-\gamma_t)\ell(w_t) + \gamma_t\ell_{n_t}}{\|(1-\gamma_t)\ell(w_t) + \gamma_t\ell_{n_t}\|}$
\State $w_{t+1} \gets \frac{(1-\gamma_t) w_t + \gamma_t1_{n_t}}{\|(1-\gamma_t)\ell(w_t) + \gamma_t\ell_{n_t}\|}$
\EndFor
\LineComment{Scale the weights optimally }
\State $\forall n \in [N]$, $w_{Mn} \gets w_{Mn} \frac{\|\mcL\|}{\|\mcL_n\|}\left<\ell(w_M), \ell\right>$
\State \Return $w$
\end{algorithmic}
"
242,1705.09847,"[H]
   \caption{Data Flow}
   % \label{alg:example}
   \begin{algorithmic}
     \small
     \STATE {\bfseries Teacher:}
         \STATE {Sample Prior:} $\bm{z}_j \sim P(\bm{z})$
         \STATE {Decode:} $\hat{\bm{\mathrm{x}}}_j \sim
         P_{\bm{\Theta}}(\bm{\mathrm{x}} | \bm{z})$
         \note[Removed-16]{\STATE {Encode:} $\bm{z}_j \sim
           Q_{\bm{\Phi}}(\bm{z}|\bm{\mathrm{x}})$}
         \STATE {}
         \STATE {\bfseries Student:}
         \STATE {Sample :} $\bm{\mathrm{x}}_j \sim P(\bm{\omega})P(\bm{\mathrm{x}} | \bm{\omega})$
         \STATE {Encode :} $\bm{z}_j \sim Q_{\bm{\phi}}(\bm{z}|\bm{\mathrm{x}})$
         \STATE {Decode:} $\hat{\bm{\mathrm{x}}}_j \sim P_{\bm{\theta}}(\bm{\mathrm{x}} | \bm{z})$
\end{algorithmic}
"
243,1805.11189,"[t]
  \caption{HITS}
  \label{alg:something}
  \small
\begin{algorithmic}[1]
\Require hubness vector \bm{$i_0$}
\Require adjacency matrix \bm{$A$}
\Require iteration number $\tau$
\Ensure hubness vector \bm{$i$}  
\Ensure authority vector \bm{$p$}
\Function{HITS}{\bm{$i_0$}, \bm{$A$}, $\tau$}
  \State $\bm{i} \gets \bm{i_0}$
  \For{$t = 1, 2, ..., \tau$}
    \State $\bm{p} \gets \bm{A^{\mathrm{T}}} \bm{i}$
    \State $\bm{i} \gets \bm{A} \bm{p}$
    \State normalize \bm{$i$} and \bm{$p$} 
  \EndFor
  \State \Return \bm{$i$} and \bm{$p$}
\EndFunction
\end{algorithmic}
\label{tab:algohits}
"
244,1705.07019,"
  \caption{: Conformal prediction interval} \label{alg:conformal}
\begin{algorithmic}[1]
    \State Input: covariate $\cv$, target coverage $\beta$ and data $\data_{\groupc}$
    \For{ all $\outconf \in \ygrid$}
        \State Update $\what{\weight}$ using $(\cv, \outconf)$
        \State Compute $\{ \resid_i\}$ and $\score(\outconf)$ in \eqref{eq:score}
    \EndFor
    \State Output: $C_{\groupc, \beta}(\cv) = \big\{ \outconf \in
    \ygrid \: : \:  (n+1)\score(\outconf) \leq \lceil
    \beta(n+1) \rceil \big\}$
\end{algorithmic}
"
245,1805.11073,"\caption{Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)}\label{alg:fista}
\begin{algorithmic}[1]
\Input initial value $q^{(0)}$, step size $t$, regularization coefficient $\lambda$
\State Set $q^{(-1)} = q^{(0)}, n = 1$
\While{not converged}
  \State $p \gets q^{(n-1)} + \frac{n-2}{n+1}(q^{(n-1)}-q^{(n-2)})$ \Comment{Nesterov's Acceleration}
  \State $q^{(n)}	\gets \mathcal{S}_{\lambda t}(p-t\nabla g(p))$ \Comment{Soft-Thresholding Operator}
  \State $n\gets n+1$
\EndWhile
\Output $q^\ast \gets q^{(n)}$
\end{algorithmic}
"
246,1805.11073,"\caption{Projected FISTA with Restarting}\label{alg:pfista}
\begin{algorithmic}[1]
\Input initial $q^{(0)}$, default step size $t$, regularization coefficient $\lambda$, restart threshold $\epsilon$, backtracking line search parameter $\omega\in(0,1)$
\While{not converged}
\State Set $q^{(-1)} = q^{(0)}, \;t_1=t, \;n = 1$
\While{not converged}
  \State $p \gets q^{(n-1)} + \frac{n-2}{n+1}(q^{(n-1)}-q^{(n-2)})$ \Comment{Nesterov's Acceleration}
  \If{$g(p_+) = +\infty$} \Comment{Restarting}
  \State {\bf break the inner loop}
  \EndIf
  \State $t_n \gets t_{n-1}$
  \State Adapt $t_n$ through \emph{backtracking line search} with $\omega$
  \If{$t_n < \epsilon$} \Comment{Restarting}
  \State {\bf break the inner loop}
  \EndIf
  \State $q^{(n)}	\gets \left[\mathcal{S}_{\lambda t_n}(p_+-t_n\nabla g(p_+))\right]_+$ \Comment{Projected Soft-Thresholding Operator}
  \State $n\gets n+1$
\EndWhile
\State Set $q^{(0)}=q^{(n-1)}$
\EndWhile
\Output $q^\ast \gets q^{(n)}$
\end{algorithmic}
"
247,1805.11057,"[H]%\captionsetup{labelfont={sc,bf}}
	\small
\caption{Wasserstein++} \label{app:wppalg}
        \begin{spacing}{1.6}
        \renewcommand{\algorithmicensure}{\textbf{Output:}}
          \begin{algorithmic}[1]
          \REQUIRE{MMD regularization coefficient $\lambda_\text{MMD}$, WGAN coefficient $\gamma$, WGAN gradient penalty coefficient $\lambda_\text{GP}$, number of critic iterations per generator iteration $n_\text{critic}$, mini-batch size $b$, characteristic positive-definite kernel $k$, Adam parameters (not shown explicitly).}
          \STATE{
          \textbf{Initialize} the parameters $\phi$, $\theta$, and $\psi$ of the WAE encoder $F_\phi$, the generator $G_\theta$, and the WGAN discriminator $f_\psi$, respectively.
          }
          \WHILE{$(\phi, \theta, \psi)$ not converged}
          \FOR{$t=1, \ldots, n_\text{critic}$}
          
          \STATE
          Sample $\{x_1,\dots,x_b\}$ from the training set
          
          \STATE Sample $\bar z_i$ from $F_\phi(x_i)$, for $i = 1,\ldots, b$
          
          \STATE Sample $\{z_1,\dots,z_b\}$ from the prior $P_Z$
          
          \STATE Sample $\{\eta_1,\dots,\eta_b\}$ from $\mathrm{Uniform}(0,1)$
          
          \STATE Sample $\{\nu_1,\dots,\nu_b\}$ from $\mathrm{Uniform}(0,1)$
          
          \STATE $\tilde z_i \gets \eta_i z_i + (1-\eta_i) \bar z_i, \quad \text{for}\; i = 1,\ldots, b$
          \STATE $\hat x_i \gets G_\theta(\tilde z_i),  \quad \text{for} \; i = 1,\ldots, b $
          \STATE $\tilde x_i \gets \nu_i x_i + (1-\nu_i) \hat x_i,  \quad \text{for} \; i = 1,\ldots, b $
          \STATE $L_f \gets \frac{1}{b} \sum_{i=1}^b f_\psi(\hat x_i) - f_\psi(x_i) + \lambda_\text{GP} (\|\nabla_{\tilde x_i} f_\psi(\tilde x_i) \| - 1)^2$
          \STATE $\psi \gets \text{Adam}(\psi, L_f)$
          \ENDFOR
          \STATE $
          L_d \gets \frac{1}{b}\sum_{i=1}^b \|x_i - G_\theta (\bar z_i)\|
          $
          \STATE $
          L_\text{MMD} \gets\frac{1}{b(b-1)}\sum_{\ell\neq j}  k(z_\ell,z_j) + \frac{1}{b(b-1)}\sum_{\ell\neq j}k(\bar{z}_\ell,\bar{z}_j) - \frac{2}{b^2}\sum_{\ell, j}k(z_\ell, \bar{z}_j)
          $
          \STATE $
          L_\text{WGAN} \gets \frac{1}{b}\sum_{i=1}^b -f_\psi(G_\theta(\bar z_i))
          $
          \STATE $
          \theta \gets \text{Adam}(\theta, (1-\gamma)L_d + \gamma L_\text{WGAN})
          $
          \STATE $
          \phi \gets \text{Adam}(\phi, (1-\gamma) (L_d + \lambda_\text{MMD} L_\text{MMD})) 
          $
          
          \ENDWHILE
          \end{algorithmic}
        \end{spacing}
        "
248,1711.11511,"[t]
\caption{Thermostat-assisted continuously-tempering Hamiltonian Monte Carlo}
\label{alg:tact-hmc}
{\small
\begin{algorithmic}[1]
\Require{$\mbox{stepsize}~\eta_{\theta}, \eta_{\xi};~~ \mbox{level of injected noise}~ c_{\theta}, c_{\xi};~~ \mbox{thermal inertia}~ \gamma_{\theta}, \gamma_{\xi};~~ \mbox{\# of steps for unit interval}~ K$}
\State{$\vs{r}_{\theta} \sim \mathscr{N}({0}, \eta_{\theta}\vs{I}) ~~\mbox{and}~~ r_{\xi} \sim \mathscr{N}(0, \eta_{\xi});\quad (z_{\theta}, z_{\xi}) \gets (c_{\theta}, c_{\xi})$}
\State{$\Call{initialise}{~\vs{\theta}, \xi, \mathtt{abf}, \mathtt{samples}~}$}
\For{$k = 1,2,3,\dots$}
\State{$\lambda \gets \Call{lambda}{~\xi~};\quad \delta{\lambda} \gets \Call{lambdaDerivative}{~\xi~}$}
\State{$z_{\xi} \gets z_{\xi} + \delta{\lambda}^2 \big[r_{\xi}^2 - \eta_{\xi}\big]\big/\gamma_{\xi}$}
\State{$z_{\theta} \gets z_{\theta} + \lambda^2 \big[\vs{r}_{\theta}^\top\vs{r}_{\theta}/\dim(\vs{r}_{\theta}) - \eta_{\theta}\big]\big/\gamma_{\theta}$}
\State{$\mathscr{S} \gets \Call{nextBatch}{~\mathscr{D}, k~};\quad \delta{A} \gets \mathtt{abf}[~\Call{abfIndexing}{~\xi~}~]$}
\State{$\tilde{U} \gets \Call{modelForward}{~\vs{\theta}, \mathscr{S}~};\quad \tilde{\vs{f}} \gets \Call{modelBackward}{~\vs{\theta}, \mathscr{S}~}$}
\State{$r_{\xi} \gets r_{\xi} - \delta{\lambda} \big[\eta_{\xi}\tilde{U} + \mathscr{N}(0,2c_{\xi}\eta_{\xi})\big] - \delta{\lambda}^2z_{\xi}r_{\xi} + \eta_{\xi}\delta{A}$}
\State{$\vs{r}_{\theta} \gets \vs{r}_{\theta} + \lambda \big[\eta_{\theta}\tilde{\vs{f}} + \mathscr{N}({0},2c_{\theta}\eta_{\theta}\vs{I}) \big] - \lambda^2z_{\theta}\vs{r}_{\theta}$}
\State{$\Call{abfUpdate}{~\mathtt{abf}, \xi, \delta{\lambda}, \tilde{U}, k~}$}
\State{$\xi \gets \xi + r_{\xi}$}
\If{$\Call{isInsideWell}{~\xi~} = \texttt{false}$}\Comment{$\xi$ is restricted by the \emph{well} of infinite height.}
\State{$r_{\xi} \gets -r_{\xi};\quad \xi \gets \xi + r_{\xi}$}\Comment{$\xi$ bounces back when hitting the wall.}
\EndIf
\State{$\vs{\theta} \gets \vs{\theta} + \vs{r}_{\theta}$}
\If{$k = 0 \mod K ~~\mbox{and}~~ \lambda = 0$}
\State{$\Call{append}{~\mathtt{samples}, \vs{\theta}~}$}\Comment{$\vs{\theta}$ is collected as a new sample in $\mathtt{samples}$.}
\State{$\vs{r}_{\theta} \sim \mathscr{N}({0}, \eta_{\theta}\vs{I}) ~~\mbox{and}~~ r_{\xi} \sim \mathscr{N}(0, \eta_{\xi})$}\Comment{$\vs{r}_{\theta}, r_{\xi}$ is \emph{optionally} resampled.}
\EndIf
\EndFor
\vspace*{0.2em}
\Function{abfUpdate}{$~\mathtt{abf}, \xi, \delta{\lambda}, \tilde{U}, k~$}
\State{$j \gets \Call{abfIndexing}{~\xi~}$}\Comment{$\xi$ is mapped to the index $j$ of the associated bin.}
\State{$\mathtt{abf}[~j~] \gets [1-\nicefrac{1}{k}]\mathtt{abf}[~j~] + [\nicefrac{1}{k}]\delta{\lambda}\cdot\tilde{U}$}
\EndFunction
\end{algorithmic}
}%
"
249,1805.10982,"[ht!]
  \caption[Early Termination]{\ci- Cascaded Inference. Early
    termination takes place as soon as the confidence level reaches
    the confidence threshold. }
    \begin{algorithmic}[1]
    \STATE {\bfseries Input:} cascaded model $M$, thresholds $\hat\delta$, input $x$
    \FOR{$m=0$ \TO $\nummodels-1$}
        \STATE $(out_m(x),\delta_m(x) )\leftarrow M_m (x)$
        \IF{$\delta_m(x)\ge\hat\delta_m$}
            \RETURN $out_m(x)$
        \ENDIF
    \ENDFOR
    \RETURN $out_{\nummodels-1}(x)$
    \end{algorithmic}
    \label{alg:hierarchical}
    "
250,1805.10982,"[ht!]
    \caption [Cascaded Inference Backtrack training] {\bt\ - An algorithm for
    a backtrack training of the cascade
    $M=\{M_0,\ldots,M_{\nummodels-1}\}$. The output is the trained
    weights of the cascade $M$}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} cascaded model $M$, training set $T$
        \STATE Let $\Theta_{deep} = \Theta_{conv}\cup
             \theta_{clf_{\nummodels-1}}$
        \STATE $\Theta_{deep}=\argmin_{\Theta_{deep}}\{L_M(out_{\nummodels-1},T)\}$.
        \FOR{$m=0$ \TO $\nummodels-2$}\label{line:start_backtrack}
            \STATE $\theta_{clf_m}=\argmin_{\theta_{clf_m}}\{L_M(out_m,T)\}$.
        \ENDFOR
        \RETURN $\Theta_{conv}\cup\Theta_{clf}$\label{line:end_backtrack}
    \end{algorithmic}
    \label{alg:backtrack}
    "
251,1805.10965,"[t]
  \caption{AutoLip}
  \label{alg:AutoLip}
  \begin{algorithmic}[1]
    \REQUIRE{function $f:\R^n\rightarrow\R^m$ and its computation graph
      $(g_1,...,g_K)$}
		\ENSURE{upper bound on the Lipschitz constant: $\hat{L}_{AL} \geq \lip{f}$}
			\STATE $\mathcal{Z} = \{(z_0,...,z_K)~:~\forall k\in\set{0,K}, \theta_k\mbox{ is
  constant}\Rightarrow z_k = \theta_k(0)\}$
    \STATE $L_0 \gets 1$
    \FOR{$k=1$ to $K$}
      \STATE $L_k \gets \sum\limits_{i=1}^{k-1}
      \max\limits_{z \in \mathcal{Z}}
      \|\partial_i g_k(z) \|_2 L_i$
    \ENDFOR
  \STATE \textbf{return} $\hat{L}_{AL} = L_k$
  \end{algorithmic}
"
252,1805.10965,"[h]
 \caption{AutoGrad compliant power method}
 \label{alg:autoPowerIteration}
 \begin{algorithmic}[1]
   \REQUIRE{affine function $f: \R^n\rightarrow\R^m$, number of iteration $N$}
   \ENSURE{approximation of the Lipschitz constant $\lip{f}$}
   \FOR{$k = 1$ to $N$}
     \STATE $v \gets \nabla g(v)$ where $g(x) = \frac{1}{2}\|f(x) - f(0)\|_2^2$
     \STATE $\lambda \gets \|v\|_2$
     \STATE $v \gets v/ \lambda$
   \ENDFOR
   \STATE \textbf{return} $\lip{f} = \|f(v) - f(0)\|_2$
 \end{algorithmic}
"
253,1803.05262,"
\caption{Q-learning algorithm for OEN}
\label{alg:objdqn}
\begin{algorithmic}
\STATE // \emph{Initialise agent}
\STATE initialise empty replay memory $M$
\STATE initialise $\theta$ for OEN Q-function $Q(s,a; \theta)$
\STATE $\theta_{target} \gets \theta$
\STATE // \emph{Initialise environment}
\STATE \texttt{env.reset()}
\STATE \texttt{step} $\gets 0$
\WHILE{\texttt{step} $\leq$ \texttt{max\_step}}
    \STATE \texttt{step} $\gets$ \texttt{step} $+1$
    \STATE // \emph{Get action from Q-function}
    \STATE $s_t \gets$ \texttt{env.get\_state()}
    \STATE $O_t \gets$ \texttt{process\_observation($s_t$)}
    %\STATE $a_t \gets argmax_a (Q(O^t,a; \theta))$
    \STATE $a_t \gets \texttt{epsilon\_greedy(}Q(O^t,a; \theta)\texttt{)}$
    %\STATE // \emph{Select random action with probability epsilon}
    %\IF {$rand() < \epsilon$}
    %    \STATE $a_t \gets$ \texttt{env.get\_random\_action()}
    %\ENDIF
    \STATE // \emph{Step environment and observe result}
    \STATE $r_t, s_{t'} \gets$ \texttt{env.apply\_action($a_t$)}
    %\STATE $s_{t'} \gets$ \texttt{env.get\_state()}
    \STATE $O_{t'} \gets$ \texttt{process\_observation($s_{t'}$)}
    \STATE $T_{t'} \gets$ \texttt{env.has\_ended()}
    \STATE add tuple $(O_t, a_t, r_t, O_{t'}, T_{t'})$ to $M$
    \STATE // \emph{Train network}
    \STATE sample tuple $({O'}_t, {a'}_t, {r'}_t, {O'}_{t'}, {T'}_{t'})$ from $M$
    %\STATE compute $y_{targ}$ of $Q(s,a; \theta_{targ})$ on ${r'}_t, s_{t'}, t_{t'}$
    \STATE $Q_{targ} \gets {r'}_t + {T'}_{t'} \cdot \gamma \cdot \max_a Q({O'}_{t'}, a ; \theta_{target})$
    \STATE update $\theta$ via gradient descent on $(Q({O'}_t,{a'}_t; \theta) - Q_{targ})^2$
    %\STATE // \emph{Reset environment as needed}
    %\IF {$t_{t'}$ is $True$}
    %    \STATE \texttt{env.reset()}
    %\ENDIF
    %\STATE // \emph{Periodically update theta\_targ}
    \IF {\texttt{step} $\% 1000 == 0$}
        \STATE $\theta_{target} \gets \theta$
    \ENDIF
\ENDWHILE

\end{algorithmic}
"
254,1801.09710,"
	\revision{
		\caption{tempoGAN training algorithm}\label{alg:tempoGANalg}		
		\begin{algorithmic}[1]   \small    
			\For{number of training steps}
			\For{$k_{D_{s}}$}
			\State Compute data-augmented mini batch $x, y$
			\State Update $D_{s}$ with $\nabla_{D_{s}}[\mathcal{L}_{D_s}(D_s,G)]$
			\EndFor
			\For{$k_{D_{t}}$}
			\State Compute data-augmented mini batch $\widetilde{X}, \widetilde{Y}$
			\State Compute advected frames $\widetilde{Y}_{\mathcal{A}}$ and $\widetilde{G}_{\mathcal{A}}\Big(\widetilde{X}\Big)$
			\State Update $D_{t}$ with $\nabla_{D_{t}}[\mathcal{L}_{D_t}(D_t,G)]$
			\EndFor
			\For{$k_{G}$}
			\State Compute data-augmented mini batch $x, y, \widetilde{X}$
			\State Compute advected frames $\widetilde{G}_{\mathcal{A}}\Big(\widetilde{X}\Big)$
			\State Update $G$ with $\nabla_{G}[\mathcal{L}_{G}(D_s,D_t,G)]$
			\EndFor
			\EndFor
		\end{algorithmic}
	}
"
255,1805.10795,"[h]
    \caption{Auto-encoder pre-training }
    \label{alg_ae}
    \textbf{Input :} dataset $\setD$, $\alpha,  \lambda,  k \leq K,  \epsilon$  \\
    \textbf{Output :} auto-encoder parameters $\btheta_e, \btheta_d$
    \begin{algorithmic}[1] % The number tells where the line numbering should start
            \While{$i \leq \mathcal{N}_i$ and $|L_d^{i} - L_d^{(i-1)}| > \epsilon$} 
            \State Build: k-nearest neighbors graph $\mathcal{G}_{\setB}$
            \State Extract: anchor pairs $\mathcal{A}_{\setB}\subset \mathcal{G}_{\setB}$
          \State Solve: $ \btheta_e^\star, \btheta_d^\star = \argmin{ \btheta_e, \btheta_d}\,\,   L_d(\Z; \btheta_e) + \lambda L_r(\X, \hat{\X})$ \Comment{Back-propagate for each   $\Z, \X \in \setB$}
         \State $i \leftarrow i + 1$
                \EndWhile
            \State $\z_i = f(\x_i; \btheta_e^\star) \quad \forall \x_i \in \setD$ \Comment{Extract latent space for each data-point  $\x_i \in \setD$}
%            \State \textbf{return} $b$
    \end{algorithmic}
"
256,1805.10795,"[h]
    \caption{Clustering phase - stage I }
    \label{alg_clustering_1}
    \textbf{Input :} dataset $\setD,  \btheta_e^\star, \btheta_d^\star, \lambda_d, \epsilon$  \\
    \textbf{Output :} auto-encoder parameters $\btheta_e, \btheta_d$\\
  	 centroids parameters $\centvars$\\
 	 assignments $\S$ \\
	\textbf{Initialize :} $\bmu_k^{(0)} = \argmax{\bmu_k} \sum_{i} S_{ik} \bmu_k^T\tilde{\z}_i \quad \forall \,\,k = 1,..., K$ \Comment{Optimized over the entire dataset $\setD$}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
%            \While{$\lambda_d \geq \lambda_{min}$}  \Comment{$\lambda_d$ loop}
            \While{$i \leq \mathcal{N}_i$ and $|L_c^{(i)} - L_c^{(i-1)}| \geq \epsilon$} \Comment{ Alternating loop}
            \State $S_{ik}^\star = \left\{  \begin{array}{l l}
1 &\,\,   \tilde{\z}_i^T\bmu_k \geq \tilde{\z}_i^T\bmu_\ell \,\, \forall \ell \neq k\\
0 & \,\, o.w 
\end{array} \right.\ $
            \State  $\bmu_k^\star = \argmax{\bmu_k} \sum_{i} S_{ik} \bmu_k^T\tilde{\z}_i \quad \forall \,\,k = 1,..., K$ \Comment{eq. eq. \eqref{eq_clustering_1} is separable for each $\bmu_k$  }
            \State $ \btheta_e^\star, \btheta_d^\star = \argmax{\btheta_e, \btheta_d}\, L_c(\Z, \btheta_e, \{\bmu_k^\star\}_{k=1}^K, \S^\star ) - \lambda_d L_d(\Z, \btheta_e) - \lambda_r L_r(\X, \hat{\X})$
            \State $i \gets i +1$
            \EndWhile
%            \EndWhile
    \end{algorithmic}
"
257,1805.10795,"
%    \caption{Clustering phase - stage II }
%    \label{alg_clustering_2}
%    \textbf{Input :} dataset $\setD$, $\beta \in (0, 1],  \lambda_b, \lambda_w \in [1, 5], \text{tol}$  \\
%    \textbf{Output :} auto-encoder parameters $\btheta_e, \btheta_d$\\
%  	 centroids parameters $\centvars$\\
% 	 assignments $\S$ \\
%	\textbf{Initialize :} $\bmu_k^{(0)} = \bmu_k^\star\,\, \forall k = 1,..., K, \btheta_d^{(0)} = \btheta_e^\star, \btheta_d^{(0)} = \btheta_d^\star$ \Comment{Initialized from stage I}
%    \begin{algorithmic}[1] % The number tells where the line numbering should start
%            \While{$i \leq \mathcal{N}_i$ and $|L_c^{(i)} - L_c^{(i-1)}| \geq \text{tol}$} \Comment{ Alternating loop}
%            \State $S_{ik}^\star = \left\{  \begin{array}{l l}
%1 &\,\,   \tilde{\z}_i^T\bmu_k \geq \tilde{\z}_i^T\bmu_\ell \,\, \forall \ell \neq k\\
%0 & \,\, o.w 
%\end{array} \right.\ $
%            \State  $\bmu_k^\star = \argmax{\bmu_k} \sum_{i} S_{ik} \bmu_k^T\tilde{\z}_i \quad \forall \,\,k = 1,..., K$ \Comment{eq. eq. \eqref{eq_clustering_1} is separable for each $\bmu_k$  }
%            \State $ \btheta_e^\star, \btheta_d^\star = \argmax{\btheta_e, \btheta_d}\, L_c(\Z, \btheta_e, \centvars, \S ) +\lambda_w\, L_w(\Z, \btheta_e)  - \lambda_b L_b(\Z, \btheta_e) - \lambda_r L_r(\X, \hat{\X})$
%            \State $i \gets i +1$
%            \EndWhile
%    \end{algorithmic}
%"
258,1805.10790,"[H] 
\caption{MR-GAN, proposed algorithm. All experiments in the paper used the default values $m=1$, $n_{inter}=1$.} 
\label{alg:MRGAN}
\begin{algorithmic}[1] 
\Require $\alpha$, the learning rate. $m$, the batch size. $n_{inter}$, the number of iterations of the unpaired/paired data.
\For{number of training iterations}
  \For{$n_{iter}$ steps}
    \State Sample $\left\{I_{CT}^{\left(i\right)}\right\}_{i=1}^{m} \sim \mathcal{P}_{data} \left(I_{CT}\right)$ a batch from the unpaired CT data.
    \State Sample $\left\{I_{\!M\!R}^{\left(i\right)}\right\}_{i=1}^{m} \sim \mathcal{P}_{data}\left(I_{\!M\!R}\right)$ a batch from the unpaired MR data.
    \State Update the discriminator, $Dis_{\!M\!R}$, by ascending its stochastic gradient:
    $$\bigtriangledown_{\theta_d^{\!M\!R}}\frac{1}{m} \sum_{i=1}^{m}  \left[\left(Dis_{\!M\!R}\left(I_{\!M\!R}^{(i)}\right)-1 \right)^2 + Dis_{\!M\!R}\left(Syn_{\!M\!R}\left(I_{CT}^{(i)}\right)\right)^2\right].$$
    \State Update the generator, $Syn_{\!M\!R}$, by descending its stochastic gradient:
    $$\bigtriangledown_{\theta_g^{\!M\!R}}\frac{1}{m} \sum_{i=1}^{m} \left[Dis_{\!M\!R}\left(Syn_{\!M\!R}\left(I_{CT}^{(i)}\right)\right)^2 + \big\|Syn_{CT}\left(Syn_{\!M\!R}\left(I_{CT}^{(i)}\right)\right)-I_{CT}^{(i)}\big\|_1 \right].$$
    \State Update the discriminator, $Dis_{CT}$, by ascending its stochastic gradient:
    $$\bigtriangledown_{\theta_d^{CT}}\frac{1}{m} \sum_{i=1}^{m}  \left[\left(Dis_{CT}\left(I_{CT}^{(i)}\right)-1 \right)^2 + Dis_{CT}\left(Syn_{CT}\left(I_{\!M\!R}^{(i)}\right)\right)^2\right].$$
    \State Update the generator, $Syn_{CT}$, by descending its stochastic gradient:
    $$\bigtriangledown_{\theta_g^{CT}}\frac{1}{m} \sum_{i=1}^{m} \left[Dis_{CT}\left(Syn_{CT}\left(I_{\!M\!R}^{(i)}\right)\right)^2 + \big\|Syn_{\!M\!R}\left(Syn_{CT}\left(I_{\!M\!R}^{(i)}\right)\right)-I_{\!M\!R}^{(i)}\big\|_1 \right].$$
  \EndFor
\algstore{myalg}
\end{algorithmic}
"
259,1805.10790,"[H]
\begin{algorithmic}[1]
\algrestore{myalg}
  \For{$n_{iter}$ steps}
    \State Sample $\left\{I_{CT}^{\left(i\right)}, \ I_{\!M\!R}^{\left(i\right)} \right\}_{i=1}^{m} \sim \mathcal{P}_{data} \left(I_{CT}, I_{\!M\!R}\right)$ a batch from the paired data.
    \State Update the discriminator, $Dis_{\!M\!R}$, by ascending its stochastic gradient:
    $$\bigtriangledown_{\theta_d^{\!M\!R}}\frac{1}{m} \sum_{i=1}^{m} \left[\log Dis_{\!M\!R}\left(I_{CT}^{(i)},I_{\!M\!R}^{(i)} \right) +  \log \left(1-Dis_{\!M\!R}\left(I_{CT}^{(i)},Syn_{\!M\!R}\left(I_{CT}^{(i)}\right)\right)\right) \right]$$
    \State Update the generator, $Syn_{\!M\!R}$, by descending its stochastic gradient:
\begin{equation*}
\begin{split}
\bigtriangledown_{\theta_g^{\!M\!R}}\frac{1}{m} \sum_{i=1}^{m} &\log \left(1-Dis_{\!M\!R}\left(I_{CT}^{(i)},Syn_{\!M\!R}\left(I_{CT}^{(i)}\right)\right)\right) + \big \|Syn_{CT}\left(Syn_{\!M\!R}\left(I_{CT}^{(i)}\right)\right)-I_{CT}^{(i)} \big \|_1 \\
&+ \big \|I_{\!M\!R}^{(i)}-Syn_{\!M\!R} \left(I_{CT}^{(i)}\right) \big \|_1
\end{split}
\end{equation*}
    \State Update the discriminator, $Dis_{CT}$, by ascending its stochastic gradient:
    $$\bigtriangledown_{\theta_d^{CT}}\frac{1}{m} \sum_{i=1}^{m} \left[\log Dis_{CT}\left(I_{\!M\!R}^{(i)},I_{CT}^{(i)} \right) +  \log \left(1-Dis_{CT}\left(I_{\!M\!R}^{(i)},Syn_{CT}\left(I_{\!M\!R}^{(i)}\right)\right)\right) \right]$$
    \State Update the generator, $Syn_{CT}$, by descending its stochastic gradient:
    \begin{equation*}
    \begin{split}
    \bigtriangledown_{\theta_g^{CT}}\frac{1}{m} \sum_{i=1}^{m} &\log \left(1-Dis_{CT}\left(I_{\!M\!R}^{(i)},Syn_{CT}\left(I_{\!M\!R}^{(i)}\right)\right)\right) + \big \|Syn_{\!M\!R}\left(Syn_{CT}\left(I_{\!M\!R}^{(i)}\right)\right)-I_{\!M\!R}^{(i)} \big \|_1 \\
     &+ \big \|I_{CT}^{(i)}-Syn_{CT} \left(I_{\!M\!R}^{(i)}\right) \big \|_1
    \end{split}
    \end{equation*}
    %$$\bigtriangledown_{\theta_g^{CT}}\frac{1}{m} \sum_{i=1}^{m} \left[ \log \left(1-Dis_{CT}\left(I_{\!M\!R}^{(i)},Syn_{CT}\left(I_{\!M\!R}^{(i)}\right)\right)\right)+ \big \|Syn_{\!M\!R}\left(Syn_{CT}\left(I_{\!M\!R}^{(i)}\right)\right)-I_{\!M\!R}^{(i)} \big \|_1 + \big \|I_{CT}^{(i)}-Syn_{CT} \left(I_{\!M\!R}^{(i)}\right) \big \|_1 \right]$$
  \EndFor
\EndFor \\
\Return $\text{result}$
\end{algorithmic}
"
260,1712.00704,"[t] \small
	\caption{Tensor truncated nuclear norm for low-rank tensor completion}
	\label{alg:Tensor-TNNR}
	\begin{algorithmic}[1]
		\REQUIRE $\tensor{M}$, the original incomplete data; $\bm{\Omega}$, the index set of the known elements; $\bm{\Omega}^{\text{c}}$, the index set of the unknown elements. \\
		\hspace{-6.5mm} \textbf{Initialize:} $\tensor{X}_1=\tensor{M}_{\bm{\Omega}}$, $\varepsilon = 10^{-3}$, $\ell = 1$, $L = 50$. \\
		\REPEAT
		\STATE \textbf{Step 1:} given $\tensor{X}_\ell \Rdim{n_1 \times n_2 \times n_3}$,
		\begin{equation}
		[ \, \tensor{U}_\ell,\tensor{S}_\ell,\tensor{V}_\ell \, ]=\text{t-SVD}(\tensor{X}_\ell), \nonumber  \vspace{-0.5ex}
		\end{equation}
		where the orthogonal tensors are
		\begin{align}
		\tensor{U}_\ell \Rdim{n_1 \times n_1 \times n_3}, \ \tensor{V}_\ell \Rdim{n_2 \times n_2 \times n_3}. \nonumber
		\end{align}
		\vspace{-3ex}
		\STATE Compute $\tensor{A}_\ell$ and $\tensor{B}_\ell$ as follows $\left( r \leq \min\{n_1,n_2\} \right)$: 
		\begin{equation}
		\tensor{A}_\ell = \tensor{U}(:,1:r,:)^\transpose,\ \tensor{B}_\ell = \tensor{V}(:,1:r,:)^\transpose. \nonumber 
		\end{equation} 
		\vspace{-3ex}
		\STATE \textbf{Step 2:} solve the problem: 
		\begin{align}
		\tensor{X}_{\ell+1} = &\, \arg\min_{\tensor{X}} \ \norm{\tensor{X}}_* - \trace{\tensor{A}_\ell * \tensor{X} * \tensor{B}_\ell^\transpose} \nonumber \\
		& \quad \ \ \text{s.t.} \quad \ \ \tensor{X}_{\bm{\Omega}} = \tensor{M}_{\bm{\Omega}} . \nonumber 
		\end{align}
		\UNTIL {$\norm{\tensor{X}_{\ell+1} - \tensor{X}_\ell}_\fro \leq \varepsilon$ or $\ell > L$}
		\ENSURE the recovered tensor.		
	\end{algorithmic}
"
261,1712.00704,"[t] \small
%	\caption{Solving \eqref{eq:step_2_tnnr} by ADMM}
%	\label{alg:ADMM}
%	\begin{algorithmic}[1]
%		\REQUIRE $\tensor{A}_\ell$, $\tensor{B}_\ell$, $\tensor{M}_{\bm{\Omega}}$, $\mu$, $\xi=10^{-4}$, $K = 200$.
%		\ENSURE $\tensor{X}_1=\tensor{M}_{\bm{\Omega}}$, $\tensor{W}_1 = \tensor{Y}_1 = \tensor{X}_1$, $k=1$. \\
%		\REPEAT
%		\STATE \textbf{Step 1:} $\tensor{X}_{k+1} = \mathcal{D}_{\frac{1}{\mu}} \! \left( \tensor{W}_k - \frac{1}{\mu} \tensor{Y}_k \right)$. \vspace{0.5ex}
%		\STATE \textbf{Step 2:} $\tensor{W}_{k+1} = \tensor{X}_{k+1} + \frac{1}{\mu} \left(\tensor{A}_\ell^\transpose * \tensor{B}_\ell + \tensor{Y}_k \right) $. \vspace{0.5ex} \\
%		Fix the values of known elements, \vspace{-1ex}
%		\begin{equation}
%		\tensor{W}_{k+1} = (\tensor{W}_{k+1})_{\bm{\Omega}^\text{c}} + \tensor{M}_{\bm{\Omega}}. \nonumber
%		\end{equation}
%		\vspace{-3.5ex}		
%		\STATE \textbf{Step 3:} $\tensor{Y}_{k+1} = \tensor{Y}_k + \mu \left( \tensor{X}_{k+1} - \tensor{W}_{k+1} \right)$.
%		\UNTIL {$\norm{\tensor{X}_{k+1} - \tensor{X}_k}_\fro \leq \xi$ or $k > K$}
%		\RETURN the recovered tensor.		
%	\end{algorithmic}
%"
262,1712.00704,"[t]
%	\caption{Solving \eqref{eq:step_2_tnnr} via APGL}
%	\label{alg:APGL}
%	\begin{algorithmic}[1]
%		\REQUIRE $\tensor{A}_\ell$, $\tensor{B}_\ell$, $\tensor{M}_{\bm{\Omega}}$, $\lambda$, $\xi=10^{-4}$, $K = 200$.
%		\ENSURE $\tensor{X}_1=\tensor{M}_{\bm{\Omega}}$, $\tensor{Y}_1 = \tensor{X}_1$, $t_1 = 1$, $k=1$. \\
%		\REPEAT
%		\STATE \textbf{Step 1:} \\ $\tensor{X}_{k+1} = \mathcal{D}_{t_k} \! \Big( \tensor{Y}_k + t_k (\tensor{A}_\ell^\transpose * \tensor{B}_\ell - \lambda ( (\tensor{Y}_k)_{\bm{\Omega}} - \tensor{M}_{\bm{\Omega}} ) ) \Big)$.
%		\STATE \textbf{Step 2:} $t_{k+1} = \frac{1}{2} \left( 1 + \sqrt{1 + 4 t_k^2} \right) $.	\vspace{0.5ex}
%		\STATE \textbf{Step 3:} $\tensor{Y}_{k+1} = \tensor{X}_{k+1} + \frac{t_k - 1}{t_{k+1}} \left( \tensor{X}_{k+1} - \tensor{X}_k \right)$.
%		\UNTIL {$\norm{\tensor{X}_{k+1} - \tensor{X}_k}_\fro \leq \xi$ or $k > K$}
%		\RETURN the recovered tensor.		
%	\end{algorithmic}
%"
263,1805.04874,"[H]
   \caption{GAN Q-learning}
   \label{alg:dist_gan}
\begin{algorithmic}
   \STATE {\bfseries Input:} MDP $M$, discriminator and generator networks $D,G$, learning rate $\alpha$,$n_{disc}$ the number of updates of the discriminator, $n_{gen}$ the number of updates of the generator, gradient penalty coefficient $\lambda$, batch size $m$.\\
   
   \STATE Initialize replay buffer $\mathcal{B}$ to capacity N, $D$ and $G$ with random weights, $Z$, $s_0, a_0$.
   \STATE $t \gets 0$
   \FOR{$\text{episode}=1,...,M$} 
   \FOR{$t=1,...,T_{max}$}
   \STATE sample a batch $z \sim N(0,1)$
   \STATE $a_t \leftarrow \underset{a}{\max} G(z|(s_t,a))$
   \STATE sample $s_{t+1} \sim P(\cdot|s_t,a_t)$
   \STATE Store transition ($s_t$,$a_t$,$r_t$,$s_{t+1}$) in $\mathcal{B}$
   \STATE \COMMENT{Updating Discriminator}
   \FOR{$n=1,...,n_{disc}$}
    \STATE Sample minibatch $\{ s,a,r,s' \}_{i=1}^{m}$ from $\mathcal{B}$
   \STATE sample batch $\{z \}_{i=1}^{m} \sim N(0,1)$

   \STATE Set $y_{i}= \begin{cases}
   r_{i}, & \text{$s^\prime$ terminal} \\
    r_{i} + \gamma \underset{a_{i}}{\max}\; G(z_{i}|(s_{i}^\prime,a_{i})), & \text{otherwise}\\
    \end{cases}$
    
   \STATE sample a batch $\{ \epsilon \}_{i=1}^{m} \sim N(0,1)$
   \STATE $\tilde{x_{i}}\leftarrow\epsilon_{i} y_{i}+(1-\epsilon_{i})\underset{a_{i}}{\max}\; G(z_{i}|(s_{i}',a_{i}))$
   \STATE $  \mathcal{L}^{(i)}  \gets   D(G(z_{i}|(s_{i},a_{i}^*)|(s_{i},a_{i}^*))-D(y_{i}|(s_{i},a_{i}^*))+\lambda (| \nabla_{\tilde{x}} D(\tilde{x_{i}}|(s_{i},a_{i}^*)) |-1)^{2}$
   
  
   $w_{D} \gets$ RMSProp($\nabla_{w_{D}}\frac{1}{m}\sum_{i=1}^{m} \mathcal{L}^{(i)},\alpha $ )
   \ENDFOR
   \STATE \COMMENT{Updating Generator}
   \FOR{$n=1,...,n_{gen}$}
   \STATE sample a batch of $\{z^{(i)}\}_{i=1}^{m} \sim N(0,1)$
   \STATE $w_{G} \gets$ RMSProp($-\nabla_{w_{G}}\frac{1}{m}\sum_{i=1}^{m}\mathcal{L}^{(i)},\alpha   $ )
     \ENDFOR
    \ENDFOR
    \ENDFOR
    
\end{algorithmic}
"
264,1703.02375,"[!htbp]
\caption{Clustering algorithm DBMSTClu}\label{alg:DBMSTClu}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} $\mathcal{T}$, the MST
\STATE $dbcvi \small{\leftarrow} -1.0$; $clusters = [ \ ]$; $cut\_list \small{\leftarrow} [E(\mathcal{T})]$
\WHILE{ $dbcvi < 1.0$ }
\STATE $cut\_tp \small{\leftarrow} None$; $dbcvi\_tp \small{\leftarrow} dbcvi$
\FOR{each $cut$ in $cut\_list$}
\STATE $newDbcvi \small{\leftarrow} \evaluateCut(\mathcal{T}, cut)$
\IF{$newDbcvi \geq dbcvi\_tp$} %\textit{A cut is made if the }
\STATE $cut\_tp \small{\leftarrow} cut$; $dbcvi\_tp \small{\leftarrow}  newDbcvi$
\ENDIF
\ENDFOR
\IF{$cut\_tp \neq None$}
\STATE $clusters \small{\leftarrow} \cut(clusters, cut\_tp)$
\STATE $dbcvi \small{\leftarrow} dbcvi\_tp$; $\remove(cut\_list, cut\_tp) $
\ELSE 
\STATE {\bfseries break}
\ENDIF
\ENDWHILE
\RETURN $clusters$, $dbcvi$
\end{algorithmic}
"
265,1703.02375,"[!htbp]
\caption{Generic Double Depth-First Search}\label{alg:doubleDFS}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} $\mathcal{T}$, the MST; $e$, the edge of $\mathcal{T}$ where the search starts; $n\_src$, source node of $e$
\STATE $Q=\deque()$ \textit{//Empty priority double-ended queue}
%\textit{// Priority double-ended queue, starts with an empty one}
% Initialization
\FOR{incident edges to $n\_src$} 
\STATE $\pushBack(Q, (incident\_e, n\_src, FALSE))$
\ENDFOR
\STATE $\pushBack(Q, (e, n\_src, TRUE))$
\FOR{incident edges to $n\_trgt$}
\STATE $\pushBack(Q, (incident\_e, n\_trgt,FALSE))$
\ENDFOR
\STATE $\pushBack(Q, (e, n\_trgt, TRUE))$
% WHILE $Q$ IS NOT EMPTY
\WHILE{$Q$ is not empty}
\STATE $e, node, marked = \popFront(Q)$
\STATE $opposite\_node = \getOtherNodeEdge(e, node)$
\IF{not $marked$}
\FOR{incident edges to $node$} 
\STATE $\pushBack(Q, (incident\_e, node,F))$
\ENDFOR
\STATE $\pushBack(Q, (e, node, T))$
\STATE $\pushFront(Q, (e, opposite\_node, T))$
\ELSE
\STATE doTheJob(e) \textit{//Perform here the task }
\ENDIF
\ENDWHILE
\RETURN 
\end{algorithmic}
"
266,1805.10652,"[tb]
   \caption{Cleaning Adversarial Inputs}
   \label{cleaning-algo}
\begin{algorithmic}
   \STATE {\bfseries Input:} adversarial input $x$, learning rate $\eta$
   \STATE Sample $z_0$ from noise prior $p_z(z)$
   \FOR{$i=1$ {\bfseries to} $m$}
    \STATE Update $z_{i} \leftarrow z_{i-1} - \eta \nabla_{z}\mathcal{L}(z_{i-1})$
   \ENDFOR
   \STATE \textbf{return} $G(z_m)$, the cleaned image
\end{algorithmic}
"
267,1804.02744,"[htb]
   \caption{{\bf Finding One Cluster by Robust Loss Minimization (OCRLM)}}
   \label{alg:closs}
\begin{algorithmic}
   \STATE {\bfseries Input:} Training examples $S=\{\bx_i\in \RR^d, i=1,...,n\}$, initial standard deviation $\sigma_{\max}$.
   \STATE {\bfseries Output:} Cluster points C, cluster center $\hat \bmu$ and standard deviation $\hat \sigma$.
\end{algorithmic}
\begin{algorithmic} 
\STATE Find $i=\argmin_{i}L(\bx_{i},\sigma_{\max})$.
\STATE Obtain the positive cluster as 
\vspace{-2mm}
\[
                C=\{\bx\in S, \|\bx-\bx_i\|<\sqrt{dG}\sigma_{\max}\}
\vspace{-6mm}
 \]
 \IF {$|C|=1$}
                \STATE 
\vspace{-5mm}
                \[
                \hat \bmu=\bx_i, \hat \sigma=\sigma_{\max}
  \vspace{-2mm}
               \]
  \ELSE 
                \STATE 
\vspace{-5mm}
\[
\hat \bmu=\frac{1}{|C|}\sum_{\bx\in C}\bx, \;\;
               \hat  \sigma^2=\frac{1}{d(|C|-1)}\sum_{\bx\in C}\|\bx-\hat \bmu\|^2
 \vspace{-2mm}
               \]
  \ENDIF
\end{algorithmic}
"
268,1804.02744,"[htb]
   \caption{{\bf Clustering by  Robust Loss Minimization (CRLM)}}
   \label{alg:multclust}
\begin{algorithmic}
   \STATE {\bfseries Input:} Training examples $\{\bx_i\in \RR^d\}_{i=1}^{n}$, maximum number of clusters $k$,  initial standard deviation $\sigma_{\max}$.
   \STATE {\bfseries Output:} Cluster centers $\hat \bmu_j$ with standard deviation $\hat \sigma_j, j=1,...,k$.
\end{algorithmic}
\begin{algorithmic} 
        \FOR {j = 1 to $k$}
                \STATE Find cluster $(C_j,\hat \bmu_j,\hat \sigma_j)$ using OCRLM.
               \IF {$|C_j|=1$}
                \STATE break
               \ENDIF                
               \STATE Remove all observations $\bx_i\in C_j$.
      \ENDFOR
\end{algorithmic}
"
269,1805.10583,"[!t]
\caption{The Dual Swap Disentangling~(DSD) algorithm}
\begin{algorithmic}[1]
\label{alg:alg1}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE{ Paired observation groups \(\{\mathcal{G}_k,k=1,2,..,n\}\), unannotated observation set \(\mathbb{G}\). }
\STATE Initialize \( \phi^1 \) and \( \varphi^1 \).
\FOR{ \(t \) =\(1,3,...,T\) epochs}
\STATE Random sample \(k \in \{1,2,...,n \}\).
\STATE Sample paired observation \( (\mathcal{I}_A,\mathcal{I}_B)\) from group \(\mathcal{G}_k\).
\STATE Encode \(\mathcal{I}_A\) and \(\mathcal{I}_B\) into \(\mathcal{R}_A\) and \(\mathcal{R}_B\) with encoder \(f_{\phi^t}\).
\STATE Swap the \(k\)-th part of \(\mathcal{R}_A\) and \(\mathcal{R}_B\) and get two hybrid representations \( \ddot{\mathcal{R}_A}\) and \( \ddot{\mathcal{R}_B}\).
\STATE Construct \(\mathcal{R}_A\) and \(\mathcal{R}_B\) into \( \Bar{\mathcal{I}_A}=f_{\varphi^t}(\mathcal{R}_A) \) and \( \Bar{\mathcal{I}_B}=f_{\varphi^t}(\mathcal{R}_B)\).
\STATE Construct \(\ddot{\mathcal{R}_A}\) and \(\ddot{\mathcal{R}_B}\) into \( \ddot{\mathcal{I}_A}=f_{\varphi^t}(\ddot{\mathcal{R}_A})  \) and \( \ddot{\mathcal{I}_B}=f_{\varphi^t}(\ddot{\mathcal{R}_B})\).
\STATE Update \( \phi^{t+1}, \varphi^{t+1} \leftarrow \phi^{t}, \varphi^{t}\) by ascending the gradient estimate of \(\mathbf{\mathcal{L}_p}(\mathcal{I}_A,\mathcal{I}_B;\phi^t,\varphi^t)\).
\STATE Sample unpaired observation \( (\mathcal{I}_A,\mathcal{I}_B)\) from unannotated observation set \(\mathbb{G}\).
\STATE Encode \(\mathcal{I}_A\) and \(\mathcal{I}_B\) into \(\mathcal{R}_A\) and \(\mathcal{R}_B\) with encoder \(f_{\phi^{t+1}}\).
\STATE swap the \(k\)-th part of \(\mathcal{R}_A\) and \(\mathcal{R}_B\) and get two hybrid representations \( \ddot{\mathcal{R}_A}\) and \( \ddot{\mathcal{R}_B}\).
\STATE Construct \(\mathcal{R}_A\) and \(\mathcal{R}_B\) into \( \Bar{\mathcal{I}_A}=f_{\varphi^{t+1}}(\mathcal{R}_A) \) and \( \Bar{\mathcal{I}_B}=f_{\varphi^{t+1}}(\mathcal{R}_B)\).
\STATE Construct \(\ddot{\mathcal{R}_A}\) and \(\ddot{\mathcal{R}_B}\) into \( \ddot{\mathcal{I}_A}=f_{\varphi^{t+1}}(\ddot{\mathcal{R}_A})  \) and \( \ddot{\mathcal{I}_B}=f_{\varphi^{t+1}}(\ddot{\mathcal{R}_B})\).
\STATE Encode \( (\ddot{\mathcal{I}_A},\ddot{\mathcal{I}_B})\) into \(\ddot{\mathcal{R}'_A}\) and \(\ddot{\mathcal{R}'_B}\) with encoder \(f_{\phi^{t+1}}\).
\STATE Swap the \(k\)-th parts of \(\ddot{\mathcal{R}'_A}\) and \(\ddot{\mathcal{R}'_B}\) backward and get \( \mathcal{R}'_A\) and \( \mathcal{R}'_B\).
\STATE Construct \(\mathcal{R}'_A\) and \(\mathcal{R}'_B\) into \( \Bar{\Bar{\mathcal{I}_A}}=f_{\varphi^{t+1}}(\mathcal{R}'_A) \) and \( \Bar{\Bar{\mathcal{I}_B}}=f_{\varphi^{t+1}}(\mathcal{R}'_B)\).
\STATE Update \( \phi^{t+2}, \varphi^{t+2} \leftarrow \phi^{t+1}, \varphi^{t+1}\) by ascending the gradient estimate of \(\mathbf{\mathcal{L}_u}(\mathcal{I}_A,\mathcal{I}_B;\phi^{t+1},\varphi^{t+1})\).
\ENDFOR
\ENSURE{\(\phi^{T},\varphi^{T}\)}
\end{algorithmic}
"
270,1805.10547,"[H]
\caption*{\textbf{Algorithm 1:} Generate Computation Graph}
\begin{algorithmic}[1]
\Procedure{\textit{GenerateComputationGraph}}{tree}\label{alg:gct}
\State left\_NP  = \textit{FindNP}(tree.left) \label{alg:l1}
\State right\_NP = \textit{FindNP}(tree.right) \label{alg:l2}

\If{left\_NP == """"}
	\State return (\texttt{Locate} tree.text) \label{alg:l3}
\EndIf
\State  \texttt{Relate} = \textit{FindPP}(tree, [left\_NP, right\_NP])\label{alg:l4}
\State
\State left\_cg  = \textit{GenerateComputationGraph}(left\_NP) \label{alg:l5}
\State right\_cg = \textit{GenerateComputationGraph}(right\_NP) \label{alg:l6}
\State return (\texttt{Intersect} (left\_cg) (\texttt{Relate} right\_cg)) \label{alg:l7}
\EndProcedure
\end{algorithmic}
"
271,1805.09654,"[tp]
	\SetAlgoLined
	\SetKwInOut{Input}{Input}
	\Input{Signal $X$, atoms $D_k$, number of segments $M$, stopping parameter $\epsilon >  0$, $z_k$ initialization}
	Initialize $\beta_k[t]$ with \autoref{eq:optimal_update}. \\ % and $z_k[t]$ for all $(k,t)$\\
	\Repeat{$\|z - z'\|_\infty < \epsilon$ }{
		\For{$m = 1$ \KwTo $M$}{
			Compute $z'_{k}[t] =  \max\left(\frac{\beta_{k}[t]-\lambda}{\| D_{k}\|_2^2}, 0\right)$
			for $(k,t) \in \cC_m$ \\
			Choose $\displaystyle(k_0, t_0) = \argmax_{(k, t)\in\cC_m} |z_k[t] - z'_k[t]|$\\
			Update $\beta$ with \autoref{eq:beta_up}\\
			Update the current point estimate $z_{k_0}[t_0]\leftarrow{}z'_{k_0}[t_0]$\\

		}

	}
    \caption{Locally greedy coordinate descent (LGCD)}
	\label{alg:LGCD}
"
272,1805.09654,"[t]
	\SetAlgoLined
	\SetKwInOut{Input}{Input}
	\SetKwFunction{Armijo}{Armijo}
	\Input{Signals $X^n$, activations $z_k^n$,
		   stopping parameter $\epsilon >  0$,\\
		   initial estimate $\{u_k\}$ and $\{v_k\}$ }
	Initialize $\Phi_k$ with \autoref{eq:phi} and $\Psi_k$ with \autoref{eq:psi} . \\ % and $z_k[t]$ for all $(k,t)$\\
	\Repeat{$\sum_{k=1}^K\left\|u^{(q+1)}_k - u^{(q)}_k\right\|_1 < \epsilon$ }{
		Compute with \autoref{eq:supp:grad_u} for $k \in \interval{1, K}$,
		~$G_k = \nabla_{u_k}E(\{u_k^{(q)}\}_k, \{v_k\}_k),$\\
		Update the estimate with $\{u_k^{(q+1)}\} \leftarrow$ \KwTo  \Armijo{$\{u_k^{(q)}\}, G_k, E$}
	}
	Set $\{u_k\} \leftarrow \{u_k^{(q)}\}$\\ 
	\Repeat{$\sum_{k=1}^K\left\|v^{(q+1)}_k - v^{(q)}_k\right\|_1 < \epsilon$ }{
		Compute with \autoref{eq:supp:grad_v} for $k \in \interval{1, K}$,
		~$G_k = \nabla_{v_k}E(\{u_k\}_k, \{v_k^{(q)}\}_k),$\\
		Update the estimate with $\{v_k^{(q+1)}\} \leftarrow$ \KwTo  \Armijo{$\{v_k^{(q)}\}, G_k, E$}
	}
	Set $\{v_k\} \leftarrow \{ v_k^{(q)}\}$\\ 
	\Return $\{u_k\}_k$ and $\{v_k\}_k$  
    \caption{Projected gradient descent for updating $\{u_k\}$ and $\{v_k\}$.}
	\label{alg:supp:d_update}
"
273,1805.10469,"[!h]
    \caption{Inference in \gls{AIR}}
    \label{algo:air_inference}
    \DontPrintSemicolon
    % \SetAlgoLined
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \SetSideCommentLeft
    \Input{Image $\bm{x}$,\\ maximum number of inference steps $N$}
    $\bm{h}_0, \bm{z}^\text{what}_0, \bm{z}^\text{where}_0$ = initialize()\\
    \For{$n \in [1, \dots, N]$}{
        $\bm{w}_n, \bm{h}_n = R_\phi \left( \bm{x}, \bm{z}^\text{what}_{n-1}, \bm{z}^\text{where}_{n-1}, \bm{h}_{n-1} \right)$\\
        $p_n \sim \mathrm{Bernoulli} (p \mid \bm{w}_n)$\\
        \If{$p_n = 0$}{
            break
        }
        $\bm{z}^\text{where}_n \sim q_\phi^\text{where} \left( \bm{z}^\text{where} \mid \bm{w}_n \right)$\\
        $\bm{g}_n = \text{STN} \left( \bm{x}, \bm{z}^\text{where}_n \right)$\\
        $\bm{z}^\text{what}_n \sim q_\phi^\text{what} \left( \bm{z}^\text{what} \mid \bm{g}_n \right)$\\
    }
\Output{$\bm{z}^\text{what}_{1:n}$, $\bm{z}^\text{where}_{1:n}$, $n$}
"
274,1805.10469,"[!h]
    \caption{Generation in \gls{AIR}}
    \label{algo:air_generation}
    \DontPrintSemicolon
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \SetSideCommentLeft
    \Input{$\bm{z}^\text{what}_{1:n}$, $\bm{z}^\text{where}_{1:n}$, $n$}
    $\bm{y}_0 = \bm{0}$\\
    \For{$t \in [1, \dots, n]$}{
        $\hat{\bm{g}}_t = h_\theta^\text{dec} \left( \bm{z}^\text{what}_t \right)$\\
        $\bm{y}_t = \bm{y}_{t-1} + \text{STN}^{-1} \left( \hat{\bm{g}}_t, \bm{z}^\text{where}_t \right)$\\
    }
    $\hat{\bm{x}} \sim \mathrm{Normal} \left(\bm{x} \mid \bm{y}_n, \sigma^2_x \bm{I} \right)$\\
    \Output{$\hat{\bm{x}}$}
"
275,1803.01370,"[tb]
	\DontPrintSemicolon
\caption{Distributed SpaRSA for solving \eqref{eq:quadratic} with
LBFGS quadratic approximation on machine $k$}
\label{alg:sparsa}
\begin{algorithmic}[1]
\STATE Given $\beta > 1$, $\sigma_0 \in (0,1)$,
$M_t^{-1}$, $U_t$, and $\gamma_t$;
\STATE Set $\bp^{(0)}_{J_k} \leftarrow 0$;
	\FOR{$i=0,1,2,\dotsc$}
		\IF{$i=0$}
			\STATE $\psi = \gamma_t$;
		\ELSE
			\STATE Compute $\psi$ in \eqref{eq:psi} through
			\begin{equation*}
				\sum_{j=1}^K \left(\bp^{(i)}_{J_j} -
				\bp^{(i-1)}_{J_j}\right)^T
				\left(\nabla_{J_j} \hat f \left(\bp^{(i)} \right) - \nabla_{J_j}
				\hat f \left(\bp^{(i-1)} \right)\right),
			\end{equation*}
			and
			\begin{equation*}
				\sum_{j=1}^K
				\left\|\bp^{(i)}_{J_j} - \bp^{(i-1)}_{J_j}\right\|^2;
			\end{equation*}
			\Comment*[r]{
			$O(1)$
		comm.}
		\ENDIF
		\STATE Obtain \Comment*[r]{$O(m)$ comm.}
		\begin{equation*}
			U_t^T \bp^{(i)} = \sum_{j=1}^K \left(U_t\right)_{J_j,:}^T
			\bp^{(i)}_{J_j};
		\end{equation*}
		\STATE Compute \[\nabla_{J_k} \hat
		f\left(\bp^{(i)}\right) = \nabla_{J_k} \tilde f \left(\bw\right) + \gamma
		\bp^{(i)}_{J_k} -
	\left(U_t\right)_{J_k,:}\left(M_t^{-1} \left(U_t^T
\bp^{(i)}\right)\right)\]
by \eqref{eq:u};
		\WHILE{TRUE}
			\STATE Solve \eqref{eq:dk} on coordinates indexed by $J_k$ to
			obtain $\bp_{J_k}$;
			\IF{\eqref{eq:accept} holds \Comment*[r]{$O(1)$ comm.}}
			\STATE $\bp^{(i+1)}_{J_k} \leftarrow \bp_{J_k}$; $\psi_i \leftarrow \psi$;
			\STATE Break;
			\ENDIF
			\STATE $\psi \leftarrow \beta \psi$;
			\STATE Re-solve \eqref{eq:dk} with the new $\psi$ to
			obtain a new $\bp_{J_k}$;
		\ENDWHILE
		\STATE Break if some stopping condition is met;
	\ENDFOR
	\STATE Gather the final solution $\bp$ \Comment*[r]{$O(d)$ comm.}
\end{algorithmic}
"
276,1803.01370,"[tb]
	\DontPrintSemicolon
\caption{A distributed proximal variable-metric LBFGS method with line
	search for \eqref{eq:f}}
\label{alg:proximalbfgs}
\begin{algorithmic}[1]
\STATE Given $\theta, \sigma_1 \in (0,1)$, $\delta > 0$, an initial
point $\bw=\bw_0$, distributed $X = [X_1,\dotsc,X_K]$;
\FOR{Machines $k=1,\dotsc,K$ in parallel}
	\STATE Compute $X_k^T \bw$ and $f_k(X_k^T \bw)$;
	\STATE $H \leftarrow a I$ for some $a > 0$ (use \eqref{eq:a0} if
possible);
\STATE Obtain $F(\bw)$; \Comment*[r]{$O(1)$ comm.}
	\FOR{$t=0,1,2,\dotsc$}
		\STATE Compute $\nabla \tilde f(\bw)$ through
		\eqref{eq:grad};
		\Comment*[r]{$O(d)$ comm.}
		\IF{$t \neq 0$ and \eqref{eq:safeguard} holds for $(\bs_{t-1},
		\by_{t-1})$}
			\STATE Update $U$, $M$, and $\gamma$ by
			\eqref{eq:M}-\eqref{eq:updates};
			\Comment*[r]{$O(m)$ comm.}
			\STATE Construct a new $H$ from \eqref{eq:Hk};
		\ENDIF
		\IF{$H = aI$}
			\STATE Solve \eqref{eq:quadratic} directly to obtain
			$\bp$;
		\ELSE
			\STATE Solve \eqref{eq:quadratic} using Algorithm~\ref{alg:sparsa} either in a distributed manner or locally
			to obtain $\bp$;
		\ENDIF
		\STATE Compute $X_k^T \bp$;
		\STATE Compute $\Delta$ defined in
		\eqref{eq:Delta}; \Comment*[r]{
		$O(1)$ comm.}
		\FOR{$i=0,1,\dotsc$}
			\STATE $\alpha = \theta^i$;
			\STATE Compute $(X_k^T \bw) + \alpha (X_k^T \bp)$;
			\STATE Compute $F(\bw + \alpha \bp)$; \Comment*[r]{$O(1)$
			comm.}
			\IF{$F(\bw + \alpha \bp) \leq F(\bw) + \sigma_1 \alpha \Delta$}
				\STATE $\bw \leftarrow \bw + \alpha \bp$, $F(\bw)
				\leftarrow F(\bw + \alpha \bp)$;
				\STATE $X_k^T \bw \leftarrow X_k^T \bw + \alpha X_k^T
				\bp$;
                                \STATE $\bw_{t+1} \leftarrow \bw$;
                                \STATE
                                $\bs_t \leftarrow \bw_{t+1}-\bw_t$,
                                $\by_t \leftarrow \nabla \tilde{f}(\bw_{t+1}) -
                                \nabla \tilde{f} (\bw_t)$;
				\STATE Break;
			\ENDIF
		\ENDFOR
	\ENDFOR
\ENDFOR
\end{algorithmic}
"
277,1805.10354,"[t]
{\footnotesize
	\caption{Lifelong Learning via Continual Self-Modeling}\label{alg:mccl}
	\begin{algorithmic}[1]
		
		\State Let \textbf{\textit{T}} be the set of all Tasks encountered during the lifetime of the system
		\State Let $m$ be the size of the \textbf{Buffer}
		\State \textbf{\textit{E}} = []  
		\State initialize \textbf{AE}
		\State Set cosine\_threshold
		
		\For {idx,curr\_task in enumerate(\textbf{\textit{T}})}
    		\If {\textbf{Buffer} is \textbf{not} full}
    		
    		
        		\State - Intitialize \textbf{TN} 
        		
         		\State - Train the \textbf{TN} for curr\_task until optimized 
         		\State - \textbf{Buffer}.append(\textbf{TN})
         		
         		
         		\If {\textbf{Buffer} \textbf{is} full}
         		
         		
             		\State \textbf{\textit{R}} = []
            		
            		\For{ encoded-network in \textbf{\textit{E}}}
                		\State r = \textbf{AE}.Decoder(encoded-network)
                		\State \textbf{\textit{R}}.append(r)
            		\EndFor
            		
            		\For{ network in \textbf{Buffer}}
                		\State flat\_network = extract and flatten parameters from network
                		\State \textbf{\textit{R}}.append(flat\_network)
            		\EndFor
            		
            		\State average\_cosine\_similarity = $0.0$
            		\State \textbf{\textit{E}} = []
            		\While { average\_cosine\_similarity $<$ cosine\_threshold}
                		\For {r\_idx,\textbf{r} $\in$ enumerate(\textbf{\textit{R}})}
                    		\State calculate AE\_loss using Equation \eqref{eqn:caeLoss}.
                    		\State back-propagate \textbf{AE} w.r.t \textbf{r}
                    		\State update average\_cosine\_similarity using cos(\textbf{r},\textbf{AE}(\textbf{r}))
                    		\State \textbf{\textit{E}}[r\_idx] = \textbf{AE}.Encoder(\textbf{r})
                		\EndFor
            		\EndWhile
            		\State empty \textbf{Buffer}
         		\EndIf
     		\EndIf
		\EndFor
		
	\end{algorithmic}
}
	\label{alg:csm_algo}
"
278,1804.06481,"[t]
\caption{JEDI with omniscient teacher}
\label{firstAlg}
\begin{algorithmic}[1]
\State \textbf{Input:} Learner's memory decay rate $\beta$, initial concept $\textbf{w}_0$, target concept $\textbf{w}_*$, initial learning rate $\eta_0$, teaching set $\Phi$, MaxIter.
\State \textbf{Initialization:} 
\begin{equation*}
    \begin{split}
        \textbf{v}_0 &\leftarrow \textbf{0} \\
        t &\leftarrow 1 
    \end{split}
\end{equation*}
\State  \textbf{Repeat:}
\State  \quad (i). Among all examples ($\textbf{x},y$) in teaching set $\Phi$ and their probabilities of incorrect prediction $f$, the teacher recommends example ($\textbf{x}_t, y_t$) to the learner by solving:
\begin{equation} \label{JEDI_Omni_alg}
    \hspace{5mm}
    (\textbf{x}_t, y_t) = \argmin_{(\textbf{x}, y) \in \Phi } \norm{y f \textbf{x} - \bigg( \textbf{v}_{t-1} + \frac{\textbf{w}_* - \textbf{w}_{t-1}}{\eta_t} \bigg)}_2^2
\end{equation}
\State \quad (ii). Learner performs the labeling.
\State \quad (iii). Learner performs learning after teacher reveals $y_t$.
\State \quad (iv). t $\leftarrow$ t + 1
\State  \textbf{Until} converged \textbf{or} $t >$ MaxIter 
\State \textbf{Output:} The teaching sequence $\mathcal{D}_t$
\end{algorithmic}
"
279,1804.06481,"[t]
\caption{JEDI with harmonic function estimation}
\label{secondAlg}
\begin{algorithmic}[1]
\State \textbf{Input:} Learner's memory decay rate $\beta$, target concept $\textbf{w}_*$, initial learning rate $\eta_0$, teaching set $\Phi$, affinity matrix A, diagonal matrix D, MaxIter.
\State \textbf{Initialization:} 
\begin{equation*}
    \begin{split}
        \textbf{v}_0 &\leftarrow \textbf{0}\\
        t &\leftarrow 1 
    \end{split}
\end{equation*}

\State  \textbf{Repeat:}
\State  \quad (i). Teacher estimates $F_u$ using Eq. (\ref{harmonic}) and calculates $f_s$ and $\frac{1}{f_{-t}}$ using Eq. (\ref{estimateT1}) and Eq. (\ref{estimateT2}).
\State  \quad (ii). Teacher recommends example ($\textbf{x}_t, y_t$) to the learner:
\begin{equation} \label{JEDIharmonic}
    \begin{split}
        \hspace{5mm}
        (\textbf{x}_t,y_t) = \argmin_{(\textbf{x}, y) \in \Phi } \eta_t^2 \norm{y f \textbf{x} - \textbf{v}_{t-1}}^2_2 - 2\eta_t \mathrm{log} \frac{1+\mathrm{exp}(-y \textbf{w}_{t-1}^T \textbf{x})}{1+\mathrm{exp}(-y \textbf{w}_{*}^T \textbf{x})}
    \end{split}
\end{equation}
\State \quad (iii). Learner performs the labeling and then teacher updates $A, D$, and $F_l$.
\State \quad (iv). Learner performs learning after teacher reveals $y_t$.
\State \quad (v). t $\leftarrow$ t + 1
\State  \textbf{Until} $t >$ MaxIter 
\State \textbf{Output:} The teaching sequence $\mathcal{D}_t$
\end{algorithmic}
"
280,1805.10255,"[t]
  \caption{SHAC: Successive Halving and Classification}
  \label{alg:shac}
\begin{algorithmic} %changing S to Omega to avoid conflict with shortlist S in the text
  \STATE {\bfseries Input:} prior distribution $U(x)$, black-box objective $b(x)$, total budget $N$, classifier budget $T_{c}$
  % \mohammad{should we say total budget instead of classifier budget here?}
   \STATE {\bfseries Initialize:} $C \leftarrow \{\}$ \comment{\hspace*{3cm}$\rhd$~$C$ is the set of binary classifers.}
   \STATE {\bfseries Initialize:} $S \leftarrow \{\}$ \comment{\hspace*{3.03cm}$\rhd$~$S$ is the current dataset of input output pairs.}
   \FOR {$t=1$ to $N$}
%   \STATE Set $P_{t}(x) \propto \prod_{i=1}^{|C|}\mathbbm{1}(C_i(x) = 1)$
   \REPEAT
   \STATE Sample $x \sim U(x)$
   \UNTIL{$\forall c \in C:~c(x) > 0$} \comment{\hspace*{1.87cm}$\rhd$~Rejection sampling}
%   \STATE $\prod_{i=1}^{|C|}\mathbbm{1}(C_i(x) = 1) > 0$
   \STATE Evaluate $y = b(x)$
   \STATE $S \leftarrow S \cup \{(x, y)\}$
   \IF{$|S| = T_{c}$}
       \STATE $\{(x_i, y_i)\}_{i=1}^{\lvert S \rvert} \leftarrow S$
       \STATE $\tilde{y} = \operatorname{median}(
    \{ y_i \}_{i=1}^{\lvert S \rvert})$
       \STATE $S' = \{(x_i, \mathrm{sign}(y_i - \tilde{y})) \}_{i=1}^{\lvert S \rvert}$\comment{\hspace*{.9cm}$\rhd$~Binarize the labels}
        \STATE Train a binary classifier $c$ on $S'$
        \STATE $C \leftarrow C \cup \{ c \}$
        \STATE $S \leftarrow \{\} $
   \ENDIF
   \ENDFOR
\end{algorithmic}
"
281,1805.10212,"[t!]
	\caption{Learning \MVBoost}\label{alg:bregboost}
\label{algo1}
	\textbf{Input: } Training set $\Trn = (\mathbf{x}_i, y_i)_{1\le i\le m}$, where $\forall i, \mathbf{x}_i = (\obsel{1}_i, \dots , \obsel{\nviews}_i)$ and $y_i \in \{-1,1\}$; and a maximal number of iterations $T$.
	
%	\medskip
	
		\textbf{Initialization:}  $\ro^{(1)} \leftarrow \frac{1}{\nviews} \mathbf{1}_\nviews$ and $\forall v,  \mathbf{\pp}^{(1)}_v \leftarrow \frac{1}{n_\nviews}\mathbf{1}_{n_\view}$\newline Train the weak classifiers $(\hypos_\view)_{1\le \view\le \nviews}$ over $\Trn$ 
\newline   For $v \in [V]$ set the $m \times n_v$ matrix $\m_v$ such that $\forall i\in [m],\ \forall j\in [n_\view],\ (\m_v)_{ij} = y_i h_{\view,j}(\obsel{v}_i)$ 
%		\vspace{8pt}
			\begin{algorithmic}[1]		
		\For{$t = 1,\ldots , T$}
		\For{$i = 1,\ldots , m$}
%		\vspace{5pt}						
		\State	 $q_{i}^{(t)} = \sigma\left(y_i\displaystyle \sum_{v=1}^{V} \rho_v^{(t)} \sum_{j=1}^{n_v} \pi_{v,j}^{(t)} \  h_{v,j}(\obsel{v}_i)  \right)$
		\EndFor
		
		\For{$v = 1,\ldots , V$}
		\For{$j = 1,\ldots , n_v$}
%		\vspace{5pt}						
		\State $W_{v,j}^{(t)+}= \sum_{i:\sign ((\m_v)_{ij}) = +1 }^{} \  q_{i}^{(t)} | (\m_v)_{ij}|$ 
%		\vspace{2pt}
		\State $W_{v,j}^{(t)-}= \sum_{i:\sign ((\m_v)_{ij}) = -1 }^{} \  q_{i}^{(t)} | (\m_v)_{ij} |$ 
%		\vspace{2pt}
		\State $\delta_{v,j}^{(t)} = \frac{1}{2} \ln \bigg( \frac{W_{v,j}^{(t)+}}{W_{v,j}^{(t)-}} \bigg)$
		
		\EndFor
		\State $\pp_v^{(t+1)} = \pp_v^{(t)} + \boldsymbol{\delta}_{v}^{(t)}$
		
		\EndFor
		\State \textbf{Set} $\ro^{(t+1)}$, as the solution of :
		\vspace{-10pt}
		\begin{align}
		\mathrm{min}_{\ro} & \qquad  - \sum_{v=1}^{V} \rho_\view \sum_{j=1}^{n_v} \bigg( \sqrt{W_{v,j}^{(t)+}} - \sqrt{W_{v,j}^{(t)-}}\bigg) \pwr 2 \label{eq:Opt}\\
		\vspace{-5pt}
		\mbox{s.t. }& \qquad \sum_{v=1}^{V} \rho_\view =1 , \quad \rho_\view \ge 0 \quad \forall v \in [V]\nonumber 
		\end{align}
		\EndFor


	\end{algorithmic}
	\vspace{-10pt}
			\textbf{Return: }  Weights $\ro^{(T)}$ and $\P^{(T)}$.
"
282,1805.10054,"[t]
\SetKwInOut{Input}{Input}
\SetKwInOut{Init}{Init}
\SetKwInOut{Parameter}{Param}
\caption{Incremental MM algorithm for ICA}
\Input{Samples $X \in \bbR^{p \times n}$}
\Parameter{Number of iterations $t_{\text{max}}$, mini-batch size $n_b$, number of coordinates to update per sample $q$}
\Init{Initialize $W = I_p$, $U^{\text{mem}}= 0 \in \bbR^{n \times p}$ and $A^i= 0 \in \bbR^{p \times p},
    \enspace \forall i \in [1, p]$}
\For{$t = 1, \dots, t_{\text{max}}$}
  {
    Select a mini-batch $b$ of size $n_b$ at random\\
    \For(\tcp*[f]{Majorization}){each index $j\in b$}{
    	Select $\bm{x} = X_{:j}$\\
    	Compute $\bm{u}^\text{new} = u^*(W\bm{x})$\\
    	Compute the gaps~\eqref{eq:dualgap}\\
    	Find the $q$ sources $i_1, \dots, i_q$ corresponding to the largest gaps\\
      Update $A^i$ for $i=i_1,\dots, i_q$ using Eq.~\eqref{eq:updateamem}\\
      Update the memory: $U^{\text{mem}}_{:j} = \bm{u}^{\text{new}}$
    }
    \For(\tcp*[f]{Minimization}){$i=1,\dots,p$}{
    	Update the $i$-th row of $W$ using Eq.~\eqref{eq:minw}
    }
  }
\Return{W}
\label{alg:inc}
"
283,1805.10054,"[t]
\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\SetKwInOut{Init}{Init}
\SetKwInOut{Parameter}{Param}
\caption{Online MM algorithm for ICA}
\Input{A stream of samples $X$ in dimension $\bbR^p$}
\Parameter{Number of iterations $t_{\text{max}}$, mini-batch size $n_b$, number of coordinates to update per sample $q$}
\Init{Initialize $W = I_p$ and $A^i= 0 \in \bbR^{p \times p}, \enspace \forall i \in [1, p]$}

  \For{$t = 1, \dots, t_{\text{max}}$}{
    Fetch $n_b$ samples from the stream \\
    \For(\tcp*[f]{Majorization}){each fetched sample $\bm{x}$}{
    	Compute $\bm{u} = u^*(W\bm{x})$ \\
    	Compute $q$ indices  $i_1, \dots, i_q$  at random \\
      Update $A^i$ for $i=i_1,\dots,i_q$ using Eq.~\eqref{eq:updateastream}
    }
    \For(\tcp*[f]{Minimization}){$i=1\dots,p$}{
    	Update the $i$-th row of $W$ using Eq.~\eqref{eq:minw}
    }
  }
\Return{W}
\label{alg:online}
"
284,1805.10032,"[htb!]
\caption{Zeno}
\begin{algorithmic}
\vspace{0.2cm}
%\setcounter{ALC@line}{-1}
\STATE{\large\underline{\textbf{Server}}} 
\STATE Input: $\rho$~(defined in Definition~\ref{def:score}), $b$~(defined in Definition~\ref{def:zeno})
\STATE $x^0 \leftarrow rand()$ \COMMENT{Initialization}
\FOR{$t = 1, \ldots, T$}
	\STATE Broadcast $x^{t-1}$ to all the workers
	\STATE Wait until all the gradients $\{\tilde{v}_i^t: i \in [m]\}$ arrive
	\STATE Draw the samples for evaluating stochastic descendant score $f_r^t(\cdot)$ as defined in Definition~\ref{def:score}
	\STATE Compute $\bar{\tilde{v}}^t = \zeno_b(\{\tilde{v}_i^t: i \in [m]\})$ as defined in Definition~\ref{def:zeno}
	\STATE Update the parameter $x^t \leftarrow x^{t-1} - \gamma^t \bar{\tilde{v}}^t$
\ENDFOR
\end{algorithmic}
\begin{algorithmic}
\vspace{0.2cm}
%\setcounter{ALC@line}{-1}
\STATE{\large\underline{\textbf{Worker}} $i = 1, \ldots, m$} 
\FOR{$t = 1, \ldots, T$}
	\STATE Receive $x^{t-1}$ from the server
	\STATE Draw the samples, compute, and send the gradient $v_i^t = \nabla F_i^t(x^{t-1})$ to the server
\ENDFOR
\end{algorithmic}
\label{alg:zeno}
"
285,1805.10000,"[!t]
		\caption{GAN-SD} 
		\label{alg_gan}
		\begin{algorithmic}[1] 
			\STATE \textbf{Input:} Real data distribution $\mathcal{P}_\mathcal{D}$
			\STATE Initialize training variables $\theta_D, \theta_G$
			\FOR{$i = 0,1,2...$} 
			\FOR {$k$ steps}
			\STATE Sample minibatch from noise prior $p_{\mathcal{G}}$
			and real dataset $p_{\mathcal{D}}$
			\STATE Update the generator by gradient:
			\vspace{-0.4em}$$ {\nabla}_{\theta_G} {E}_{p_{x\sim\mathcal{G}}, p_{x\sim\mathcal{D}}} [  D(G(z)) + \alpha \mathcal{H}(G(z)) - \beta KL (G(z)||x)] $$
			\vspace{-1.5em}\ENDFOR
			\STATE Sample minibatch from noise prior $p_{\mathcal{G}}$ and real dataset $p_{\mathcal{D}}$
			\STATE Update the discriminator by gradient:
			\vspace{-.4em}$$ \nabla_{\theta_D} E_{p_{x\sim\mathcal{D}}}[\log D(x)] + E_{p_{x\sim\mathcal{G}}}[\log (1-D(G(z)))]$$                    
			\vspace{-1.5em}\ENDFOR 
			\STATE \textbf{Output:} Customer generator $G$
		\end{algorithmic} 
	"
286,1805.10000," 
		\caption{MAIL} 
		\label{alg_mail}
		\begin{algorithmic}[1] 
			\STATE \textbf{Input:} Expert trajectories $\tau_e$, customer distribution $\mathcal P^c$
			\STATE Initialize variables $\kappa, \sigma, \theta$
			%		\STATE Sample an agent from the given distribution $\delta \sim \mathcal{D}$
			\FOR{$i = 0,1,2...,I$} 
			\FOR{$j = 0,1,2...,J$} 
			\STATE $\tau_j = \emptyset, \, s \sim   \mathcal{P}^c, a\sim\pi_\sigma(s,\cdot), s^c=\left<s, a\right>$
			\WHILE{NOT TERMINATED}
			\STATE sample $a^c \sim \pi(s^c, \cdot)$, 
			add $(s^c, a^c)$ to $\tau_j$, 
			generate $s^c \sim \mathcal{T}^c_\sigma(s^c, a^c|\mathcal P^c)$
			\ENDWHILE
			\ENDFOR
			\STATE Sample trajectories $\tau_g$ from ${\tau}_{\,\mbox{\tiny{0}} \sim {\tiny{J}}}  $
			\STATE Update $\theta$ to in the direction to maximize
			\vspace{-.4em}
			\[
			E_{\tau_g}[\log(\mathcal{R}^c_\theta(s,a))]+E_{\tau_e}[\log(1 - \mathcal{R}^c_\theta(s,a))]
			\]
			\vspace{-1.5em}\STATE Update ${\kappa, \sigma}$ by optimizing $\pi^c_{\kappa, \sigma}$ with RL in $\mathcal{M}^c$
			\ENDFOR 
			\STATE \textbf{Output:} The customer agent policy $\pi^c$
		\end{algorithmic} 
	"
287,1802.03569," % enter the algorithm environment
\caption{Compute $d_{\FIM}$ for persistence diagrams} % give the algorithm a caption
\label{alg:dFIM} % and a label for \ref{} commands later in the document
\begin{algorithmic}[1] % enter the algorithmic environment
    \REQUIRE Persistence diagrams $\Dg_i$, $\Dg_j$, and a bandwith $\sigma>0$ for smoothing
    \ENSURE $d_{\FIM}$
    \STATE Let $\Theta \leftarrow \Dg_i \cup \Dg_{j\Delta} \cup \Dg_j \cup \Dg_{i\Delta}$ (a set for smoothed and normalized measures)
     \STATE Compute $\bar{\rho_i} = \rho_{\left(\Dg_{i} \cup \Dg_{j\Delta}\right)} \leftarrow \left[ \frac{1}{Z} \sum_{u \in \Dg_{i} \cup \Dg_{j\Delta}} \mathtt{N}(x; u, \sigma I)\right]_{x \in \Theta}$ \\ \qquad \qquad where $Z \leftarrow \sum_{x \in \Theta} \sum_{u \in  \Dg_{i} \cup \Dg_{j\Delta}} \mathtt{N}(x; u, \sigma I) $
     \STATE Compute $\bar{\rho_j} = \rho_{\left(\Dg_{j} \cup \Dg_{i\Delta}\right)}$ similarly as $\bar{\rho_i}$.
     \STATE Compute $d_{\FIM} \leftarrow \arccos\left(\left<\sqrt{\bar{\rho_i}}, \sqrt{\bar{\rho_j}} \right>\right)$ where $\left< \cdot, \cdot \right>$ is a dot product and $\sqrt{\cdot}$ is element-wise.	
\end{algorithmic}
"
288,1805.09991,"[t]
\LinesNumbered
\DontPrintSemicolon
\caption{Identifying Context Words from the Past}
\label{alg:ll}
\SetKwInOut{Input}{Input} 
\SetKwInOut{Output}{Output} 
\SetKwRepeat{Do}{do}{while}
\Input{a knowledge base $\mathcal{K}$ containing a vocabulary $\mathcal{K}.V_{\textit{wf}}$, a base meta-learner $\mathcal{K}.M$, and domain knowledge $\mathcal{K}_{m+1:n}$; \\a new domain corpus $D_{n+1}$.}
\Output{relevant past knowledge $\mathcal{A}$, where each element is a key-value pair $(w_t, \mathcal{C}_{w_t})$ and $\mathcal{C}_{w_t}$ is a list of context words from all similar domain contexts for $w_t$. }
\BlankLine
\BlankLine
$(V_{m+1:n}, C_{m+1:n}, E_{m+1:n}) \gets \mathcal{K}_{m+1:n}$ \;
$V_{n+1} \gets \text{BuildVocab}(D_{n+1})$ \;
$C_{n+1} \gets \text{ScanContextWord}(D_{n+1}, V_{n+1})$ \;
$E_{n+1} \gets \text{BuildFeatureVector}(D_{n+1}, \mathcal{K}.V_{\textit{wf}})$ \;
$M_{n+1} \gets \text{AdaptMeta-learner}(\mathcal{K}.M, E_{m+1:n}, E_{n+1})$ \;
$\mathcal{A} \gets \emptyset$ \;
%$W \gets \{\emptyset\}$\;
\For{$(V_j, C_j, E_j) \in (V_{m+1:n}, C_{m+1:n}, E_{m+1:n})$}{
    $O \gets V_j \cap V_{n+1}$  \;
    $F \gets \big\{(\mathbf{x}_{o, j, 1}, \mathbf{x}_{o, n+1, 1})|$  $\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ } o \in O \text{ and } \mathbf{x}_{o, j, 1} \in E_j \text{ and } \mathbf{x}_{o, n+1, 1} \in E_{n+1} \big\}$ \;
    %$F_j \gets \text{RetrieveFeatureVector}(E_j, O)$ \;
    %$F_{n+1} \gets \text{RetrieveFeatureVector}(E_{n+1}, O)$ \;
    $S \gets M_{n+1}.\text{inference}( F )$ \;
    $O \gets \{o| o\in O \text{ and } S[o]\ge \delta \}$ \;
    \For{$o \in O$}{
    %    \If{$S[o] \ge \delta $}{
        $\mathcal{A}[o].\text{append}(C_j[o] )$ \;
    %    }
    }
    %$ \mathcal{A} \gets \text{MergeCo-occurrence}(\mathcal{A}, P)$ \;
}
%$W \gets \text{InvertbyDomainIndex}(W)$ \;

%\For{$(D_i, O_i) \in (D_{m:n}, W_{m:n})$}{
%    $ \mathcal{A} \gets \mathcal{A} \cup \text{ScanCo-Occurrence}(D_i, O_i)$ \;
%}
$\mathcal{K}_{n+1} \gets (V_{n+1}, C_{n+1}, E_{n+1}) $ \;
\Return{ $\mathcal{A}$}
"
289,1805.11048,"[t]
    \caption{ RB Features Generation}
    \label{alg:RB}
    \begin{algorithmic}[1]
    \STATEx {\bf Input:}  Given a kernel function $k(\bx_i,\bx_j)=\prod_{l=1}^d k_l(|x_{i}^{(l)}-x_{j}^{(l)}|)$. Let $p_j(\omegab) \propto \omegab k_j''(\omegab)$ be a distribution over $\omegab$.
    \STATEx {\bf Output:} RB feature matrix $\bZ^{N \times D}$ for raw data $\bX$
    \FOR {$j = 1, \ldots, R$}
        \STATE Draw $\omega_{i}$ from $p_j(\omegab)$ and $u_{i}\in [0,\omega_{i}]$, for $\forall i\in[d]$ 
        \STATE Compute feature values $\bz_{j}(\bx_i)$ as the indicator vector of bin index $(\lfloor\frac{\bx_{i}^{(1)}-u_1}{\omega_1}\rfloor, ..., 
        \lfloor\frac{\bx_{i}^{(d)}-u_d}{\omega_d}\rfloor )$, for $\forall i\in[N]$.
    \ENDFOR
    \STATE $\bZ_{i,:}=\frac{1}{\sqrt{R}}[\bz_1(\bx_i);...;\bz_D(\bx_i)]$, for $\forall i\in[N]$
    \end{algorithmic}
"
290,1805.11048,"[t]
    \caption{ Scalable SC method based on RB}
    \label{alg:sc_rb}
    \begin{algorithmic}[1]
    \STATEx {\bf Input:}  Data matrix $\bX$,  number of clusters $K$, number of girds $R$, kernel parameter $\sigma$.
    \STATEx {\bf Output:} K clusters and membership matrix $\bM$
    \STATE Construct a fully connected graph using a sparse feature matrix $\bZ \in \R^{N \times D}$ generated by RB using Algorithm \ref{alg:RB}.
    \STATE Compute degree matrix $\widehat{\bD}$ using Equation \ref{eq:sc_rb_computeD} and obtain $\widehat{\bZ} = \widehat{\bD}^{-1/2}\bZ$ using Equation \ref{eq:sc_rb}.
    \STATE Compute $K$ largest left singular vectors $\bU$ of $\widehat{\bZ}$ using state-of-the-art iterative sparse SVD solver  (e.g., PRIMME).
    \STATE Obtain the matrix $\widehat{\bU}$ from $\bU$ by row normalization.
    \STATE Cluster the rows of $\widehat{\bU}$ into $K$ clusters using K-means and obtain the corresponding membership matrix $\bM$.
    \end{algorithmic}
"
291,1805.09964,"
\begin{algorithmic}[1]
\REQUIRE Prior $\pthetazero$ for $\thetatrue$, Conditional distribution
  $\PP(Y|X,\theta)$.
\STATE $\datatt{0}\leftarrow \emptyset$.
\label{step:initdata}
\FOR{$t=1,2,\dots$}
\STATE Sample $\theta\sim\pthetatt{t-1} \equiv \PP(\thetatrue|\datatmo)$.
\label{step:sampletheta}
\STATE Choose $\Xt = \argmin_{x\in\Xcal} \penlplustmo(\theta, \datatmo, x)$.
\label{step:choosext}
\STATE $\YXt\leftarrow$ conduct experiment at $\Xt$.
\label{step:experiment}
\STATE Set $\datat \leftarrow \datatmo \cup \{(\Xt, \YXt)\}$.
\label{step:adddata}
\ENDFOR
\caption{$\;$\bdoes ($\pspolicy$) \label{alg:bdoe}}
\end{algorithmic}
"
292,1805.07816,"[!htb]
  \caption{\textsc{Deriving Codes via Density Estimation}}
  \label{alg:find}
  \begin{algorithmic}[1]
    \REQUIRE A training dataset $\mathcal{D}$, parameters $r$ and $k$, a kernel function $\rho(\cdot, \cdot)$.
    \ENSURE a set of codes $\mathcal{C}=\{c_1,c_2,\cdots,c_k\}$.
    \STATE Let $\mathcal{P}$ denote all the pixels from images in $\mathcal{D}$.
    \FOR{$i=1,2,\cdots,k$}
    \STATE For each pixel value $v$, estimate its (unnormalized) density as
    $h[v] = \sum_{p \in \mathcal{P}} \rho(p, v)$.
    \STATE Set $c_i \leftarrow \arg\max_{v} h[v]$, and
    $\mathcal{P} \leftarrow \mathcal{P}-\{p\in \mathcal{P} : \|p-c_i\| \leq r \}$.
    \ENDFOR
  \end{algorithmic}
"
293,1805.07816,"[t]
  \caption{\textsc{Deriving Codes via Density Estimation}}
  \label{alg:find}
  \begin{algorithmic}[1]
    \REQUIRE A training dataset $\mathcal{D}$, distance parameter $r$ and number of codewords $k$, a kernel function $\rho(\cdot, \cdot)$.
    \ENSURE a set of codewords $\mathcal{C}=\{c_1,c_2,\cdots,c_k\}$.
    \STATE Let $\mathcal{P}$ denote all the pixels from images in $\mathcal{D}$.
    \FOR{$i=1,2,\cdots,k$}
    \STATE For each pixel value $v$, estimate its (unnormalized) density as
    $h[v] = \sum_{p \in \mathcal{P}} \rho(p, v)$.
    \STATE Set $c_i \leftarrow \arg\max_{v} h[v]$, and
    $\mathcal{P} \leftarrow \mathcal{P}-\{p\in \mathcal{P} : \|p-c_i\| \leq r \}$.
    \ENDFOR
  \end{algorithmic}
"
294,1805.07816,"[t]
  \caption{\textsc{Deriving Codes via $k$-Medoids}}
  \label{alg:find_kmedoid}
  \begin{algorithmic}[1]
    \REQUIRE A training dataset $\mathcal{D}$, number of codewords $k$, number of iterations $T$, a distance function $d(\cdot, \cdot)$.
    \ENSURE a set of codewords $\mathcal{C}=\{c_1,c_2,\cdots,c_k\}$.
    \STATE Let $\mathcal{P}$ denote all the pixels from images in $\mathcal{D}$.
		\STATE For any set of codewords $\mathcal{C}$, define the $k$-medoid cost (w.r.t.\ to the distance function $d$) as 
		$$
		\text{cost}(\mathcal{P}, \mathcal{C}) = \sum_{p\in \mathcal{P}} \min_{c' \in \mathcal{C}} d(p, c').
		$$
		\STATE Randomly pick $k$ pixels as the initial medoids $\mathcal{C}=\{c_1,c_2,\cdots,c_k\}$.
    \FOR{$t=1,2,\cdots,T$}
    %\STATE Assign each pixel $p$ to its nearest medoid $c(p) = \arg\min_{c \in \mathcal{C} } d(c, p)$. 
		\FOR{Each pixel $p \not\in \mathcal{C}$ and each $c \in \mathcal{C}$}
		\STATE Let $\mathcal{C}_{c,p} = \mathcal{C} \setminus \{c\} \cup \{p\}$
		\IF{$\text{cost}(\mathcal{P}, \mathcal{C}_{c,p}) < \text{cost}(\mathcal{P}, \mathcal{C})$}
		\STATE Set $\mathcal{C} \leftarrow \mathcal{C}_{c,p}$
		\ENDIF
		\ENDFOR
    \ENDFOR
  \end{algorithmic}
"
295,1805.09806,"[b]
\vspace{-0.2cm}
\SetAlgoLined
\KwResult{Trained Network Parameters, $(\theta, \phi, \psi, \chi)$}
Define $\lambda=(\lambda_R, \lambda_F, \lambda_M, \lambda_C)$\;
 Randomly initialize $(\theta, \phi, \psi, \chi)$\;
 Update $(\theta, \phi)$ by jointly training $(D_\theta, C_\phi)$ with $\lambda=(1.0,0.0,0.0,0.0)$\;
 Update $\psi$ by training $F_\psi$ with  $\lambda=(0.0,1.0,0.0,0.0)$\;
 Update $\chi$ by jointly training $(D_\theta, C_\phi, F_\psi, M_\chi)$ with $\lambda=(1.0,0.5,0.0,0.0)$\;
  \Loop{}{ \Competition{}{
  Update $\theta, \phi$ by jointly training $(D_\theta, C_\phi,$  $F_\psi, M_\chi)$ with $\lambda=(1.0,0.5,0.05,0)$ \;
%  Update $\phi$ by jointly training $(D_\theta, C_\phi, F_\psi, M_\chi)$ with $\lambda=(1.0,0.5,0.005,0,0)$  and $\lambda_\rho=0.005$\;
  Update $\psi$ by jointly training $(D_\theta, C_\phi, F_\psi, M_\chi)$ with $\lambda=(0.0,1.0,0.005,0)$  \;}
  \Collaboration{}{
  Update $\chi$ by jointly training $(D_\theta, C_\phi, F_\psi, M_\chi)$ with $\lambda=(1.0,0.5,0.005,0.3)$ \;}}
 \caption{Network Training Algorithm}
 \label{algo:training}
"
296,1805.09781,"[H]
\caption{LGCPN}\label{LGCPN_code}
\begin{algorithmic}[1]
\Inputs{} Observational dataset $\mathcal{D} = \{\x^{(i)}_p \in \tau, \forall p =1,...,P\}_{i=1}^I$ for bounded region $\tau$ where $I$ denotes the number of events. Number of latent \gptext{s} $Q$. Number of mini-batches $\texttt{b}$ of size $B$.
\Output{} Optimized hyper-parameters, posterior moments of $\vecS{\lambda}$ \\

\prediction{}Discretize event locations $\mathcal{D}$ in $Y \in \mathbb{R}^{N \times P}$ given the grid size. 
\Initialize{}  $i \gets 0$, $\allparameters^{(0)} = (\hyperparam, \hyperparam_w, \likeparamvector, \varparam_u, \varparam_w)$
%$\priormeanw \gets 0$, $\priormeanu \gets 0$
\Repeat
\State $\{X_{train} \in \mathbb{R}^{B\times D}$, $Y_{train} \in \mathbb{R}^{B\times P}\} \to \texttt{get-next-MiniBatch}(\mathcal{D})$
\For{\texttt{j=0 to b}}
\State $\max_{\pmb{\mu}} \elbo(\allparameters^{(i)})$ (\eqs \eqref{eq:ent_u}--\eqref{eq:crossent_w} and \eqref{lik})
\State $\allparameters^{(i)} \gets \allparameters^{(i-1)}- \rho \nabla_{\allparameters} \elbo(\allparameters^{(i-1)})$
\State $i = i +1$
\EndFor
\Until{convergence criterion is met.}
\prediction{} $\allparameters^* \gets \allparameters^{(i - 1)}$
\prediction{} $\mathbb{E}\left[\vecS{\lambda}(\x)^t\right] = \text{exp}(t\likeparamvector^*) \mgf_{\W\f|\allparameters^*}(t)$
\end{algorithmic}
"
297,1805.09781,"[H]
			\caption{LGCPN}\label{LGCPN_code}
			\begin{algorithmic}[1]
				\Inputs{} Observational dataset $\mathcal{D}$, number of latent \gptext{s} $Q$.
				\Output{} Optimized hyper-parameters, posterior moments of $\vecS{\lambda}$
				\Initialize{}  $i \gets 0$, $\allparameters^{(0)} = (\hyperparam, \hyperparam_w, \likeparamvector, \varparam_u, \varparam_w)$,  $\priormeanw \gets 0$, $\priormeanu \gets 0$
				\Repeat
				\State $\max_{\pmb{\mu}} \elbo(\allparameters^{(i)})$
				\State $\allparameters^{(i)} \gets \allparameters^{(i-1)}- \rho \nabla_{\allparameters} \elbo(\allparameters^{(i-1)})$
				\State $i = i +1$
				\Until{convergence criterion is met.}
				\prediction{} $\allparameters^* \gets \allparameters^{(i - 1)}$
				\prediction{} $\mathbb{E}\left[\vecS{\lambda}(\x)^t\right] = \text{exp}(t\likeparamvector^*) \mgf_{\W\f|\allparameters^*}(t)$
			\end{algorithmic}
		"
298,1805.09767,"[H]
\caption{\textsc{Local SGD}}\label{alg:localsgd}
\begin{algorithmic}[1]
\item Initialize variables $\xx_0^k = \xx_0$ for workers $k \in [K]$
%
 \For{$t$ \textbf{in} $0\dots T-1$}
  \ParFor{$k \in [K]$}
  \State Sample $i_t^k$ uniformly in $[n]$
  \If{$t+1 \in \mathcal{I}_T$}
  \State $\xx_{t+1}^k \gets \frac{1}{K} \sum_{k=1}^K \bigl( \xx_t^k - \eta_t \nabla f_{i_t^k}(\xx_t^k)\bigr)$ \Comment{global synchronization}
  \Else
  \State $\xx_{t+1}^k \gets \xx_{t}^k -  \eta_t \nabla f_{i_t^k}(\xx_t^k)$ \Comment{local update}
  \EndIf
  \EndParFor  
 \EndFor
%
\end{algorithmic}
"
299,1805.09767,"[H]
\begin{algorithmic}[1]
\item Initialize variables $\xx_0^k = \xx_0$, $r^k = 0$ for $k \in [K]$, aggregate $\bar{\bar{\xx}} = \xx_0$.
\ParFor{$k \in [K]$}
 \For{$t$ \textbf{in} $0\dots T-1$}
%
  \State Sample $i_t^k$ uniformly in $[n]$
  \State $\xx_{t+1}^k \gets \xx_{t}^k -  \eta_t \nabla f_{i_t^k}(\xx_t^k)$ \Comment{local update}
%
  \If{$t+1 \in \mathcal{I}_T^k$}
  \State $\bar{\bar{\xx}} \gets \operatorname{add}(\bar{\bar{\xx}},\frac{1}{K}(\xx_{t+1}^k - \xx_{r^k}^k))$ \Comment{atomic aggregation of the updates}
  \State $\xx_{t+1}^k \gets \operatorname{read}(\bar{\bar{\xx}})$;
  \State $r^k \gets t+1$ \Comment{iteration/time of last read}
  \EndIf
 \EndFor
\EndParFor
\end{algorithmic}
\caption{\textsc{Asynchronous Local SGD (schematic)}}\label{alg:async}
\vspace{-0.1cm}
"
300,1711.05240,"[t] 
\caption{Decoding with an Abstract Cache}
\label{alg:cache} 
\begin{algorithmic}[1]
{\footnotesize

%\Procedure {Decode}{$x,y,C$}
%\State encode $x$.
%\State $B_1 \leftarrow$ compute beam of programs of length $1$.
%\For{$t = 2 \dots T$} // Decode with cache
%\State $B_t \leftarrow$ construct beam from $B_{t-1}$.
%\State $\sA = \phi$.
%\For{$z_\text{prefix} \in B_{t-1}$}
%\label{line:prefix}
%\State $\sA.\text{AddAll}(C.\text{match}(x, z_\text{prefix}))$.
%\EndFor
%\State$B_t.\text{add}(\text{lexicalize}(\text{top-}D(\sA)))$.
%\EndFor
%\For{$z \in B_T$} //Update cache \label{line:cacheupdate}
%\State $C.\text{update}(\bar{x}, \bar{z}, R(z,y))$.
%\EndFor
%\State $\sZ_\text{full} \leftarrow \text{top-}D(C.\text{match}(\bar{x}))$.
%\label{line:full}
%\State return $B_T \cup \text{lexicalize}(\sZ_\text{full})$.
%\EndProcedure
%} 
\Procedure {Decode}{$x,y,C,D$}
%\State encode $x$.
%\State $\sA = C.\text{match}(\bar{x})$.
\State // C is a map where the key is an abstract utterance and the value
is a pair $(Z,\hat{R})$ of a list of abstract programs $Z$ and their average
rewards $\hat{R}$. $D$ is an integer.
\State $\bar{x} \leftarrow$ Abstract utterance of x
\State $\sA \leftarrow$ $D$ programs in $C[\bar{x}]$ with top reward values
\State $B_1 \leftarrow$ compute beam of programs of length $1$
\For{$t = 2 \dots T$} // Decode with cache
\State $B_t \leftarrow$ construct beam from $B_{t-1}$
\State $\sA_{t} = \text{truncate}(\sA, t)$
\State $B_t.\text{add}(\text{de-abstract}(\sA_{t}))$ \label{line:prefix}
\EndFor
\For{$z \in B_T$} //Update cache \label{line:cacheupdate}
\State Update rewards in $C[\bar{x}]$ using $(\bar{z}, R(z,y))$
%$C.\text{update}(\bar{x}, \bar{z}, R(z,y))$.
\EndFor
\State return $B_T \cup
\text{de-abstract}(\sA)$.\label{line:full}
\EndProcedure
}
\end{algorithmic}
% \caption{A decoding algorithm using a cache $C$ for augmenting the beam during search. The cache $C$ is a dictionary where the key $\bar{x}$ is an abstract utterance. The value is a pair $(Z,R)$, where $Z$ is a set of abstract programs, and $R$ are average rewards for these programs.}
"
301,1805.09622,"[ht!]
    \caption{SOSELETO: SOurce SELEction for Target Optimization}
    \label{alg:1}
    \begin{algorithmic}
        %\STATE Initialize: $\alpha_j = \frac{1}{2}$ for all $j$.
        \STATE Initialize: $\theta$, $\phi^s$, $\alpha$, $\phi^t$.
        \WHILE{not converged}
        \STATE Sample source batch $b \leftarrow\{ b_1, \dots, b_L \} \subset \{1, \dots, n^s \}$
        \STATE Denote by $\alpha_b = [\alpha_{b_1}, \dots, \alpha_{b_L}]$
        \STATE $Q \leftarrow [q_1 \dots q_L]$ \,\, where \,\, $q_\ell \leftarrow \frac{1}{n^s} \frac{\partial}{\partial \theta} \ell(y_{b_\ell}^s, F(x_{b_\ell}^s; \theta, \phi^s))$
        \STATE $R \leftarrow [r_1 \dots r_L]$ \,\, where \,\, $r_\ell \leftarrow \frac{1}{n^s} \frac{\partial}{\partial \phi^s} \ell(y_{b_\ell}^s, F(x_{b_\ell}^s; \theta, \phi^s))$
        \STATE $\theta \leftarrow \theta - \lambda_p Q \alpha_b$
        \STATE $\phi^s \leftarrow \phi^s - \lambda_p R \alpha_b$
        \STATE $\alpha_b \leftarrow \text{CLIP}_{[0,1]}\left(\alpha_b + \lambda_\alpha \lambda_p Q^T \frac{\partial L_t}{\partial \theta}\right)$
        \STATE $\phi^t \leftarrow \phi^t - \lambda_p \frac{\partial L_t}{\partial \phi^t}$
        \ENDWHILE
    \end{algorithmic}
"
302,1803.02349,"[htb]
	\caption{Framework of \textbf{EGES}.}
	\label{alg:Framwork}
	\begin{algorithmic}[1]
		\INPUT
		\Statex The item graph $\cG = (\cV,\cE)$, side information $S$, number of walks per node $w$, walk length $l$, Skip-Gram window size $k$, number of negatives samples $\#ns$, embedding dimension $d$;
		\OUTPUT
		\Statex The item \& side-information embeddings $\bW^0,\ldots,\bW^n$
		\Statex Weight matrix $\textbf{A}$;
		\State Initialize $\bW^0,\ldots,\bW^n,\textbf{A}$;
		\For{$i=1 \rightarrow w$}
			\For{$v \in \cV$}
			\State $SEQ$ = RandomWalk($\cG$,$v$,$l$); (Eq.~\eqref{eq:transit-prob})
			\State \textbf{WeightedSkipGram}($\bW^0,\ldots,\bW^n,\textbf{A},k,\#ns ,l,SEQ$); 
			\EndFor
		\EndFor
		\State \Return $\bW^0,\ldots,\bW^n,\textbf{A}$;
	\end{algorithmic}
"
303,1803.02349,"[htb]
	\caption{Weighted Skip-Gram.}
	\label{alg:wsg}
	\begin{algorithmic}[1]
		\Function{WeightedSkipGram}{$\textbf{W}^0,\dotsm,\textbf{W}^n,\textbf{A},k,\#ns ,l,SEQ$}
		\For{$i=1 \rightarrow l$}
		\State $v=SEQ[i]$;
		\For{$j = max(0,i-k) \rightarrow min(i+k,l)$ \& $j \neq i$ }
		\State $u=SEQ[j]$
		\State Update($v,u,1$)
		\For{$t = 0 \rightarrow \#ns$}
		\State $u=NegativeSampling(V)$
		\State Update($v,u,0$)
		\EndFor
		\EndFor
		\EndFor
		\EndFunction
		\Statex
		\Function{Update}{$v,u,y$}
		\State $ \bZ_u^{new} = \bZ_u^{old} - \eta \cdot \frac{\partial}{\partial \bZ_u}\mathcal{L}$; (Eq.~\eqref{eq:partial_z})
		\For{ $s=0 \rightarrow n $}
		\State $ a_v^{s^{new}} = a_v^s{^{old}} - \eta \cdot \frac{\partial\mathcal{L}}{\partial a_v^s}$; (Eq.~\eqref{eq:partial_a})
		\State $ \bW_v^{s^{new}} = \bW_v^{s^{old}} - \eta \cdot \frac{\partial\mathcal{L}}{\partial \bW_v^s}$;  (Eq.~\eqref{eq:partial_w})
		\EndFor
		\EndFunction
	\end{algorithmic}
"
304,1805.07894,"[ht]
	\caption{Generative Adversarial Example Targeted Attack}
	\label{alg:attack}
	\begin{algorithmic}[1] % The number tells where the line numbering should start
		\Procedure{Attack}{$y_\text{target}, y_\text{source}, f, \theta, \phi, \epsilon, \epsilon_\text{attack}, \lambda_1, \lambda_2,\alpha, T$}
		\State Define $\mathcal{L}(z, \tau\mathbin{;} z^0, y_\text{target}, y_\text{source}, f, \theta, \phi, \epsilon, \epsilon_\text{attack}, \lambda_1, \lambda_2)$
		\begin{align*}
		\mathcal{L} = 
		&-\log f(y_\text{target} \mid g_\theta(z, y_\text{source}) + \epsilon_\text{attack}\tanh(\tau)) +\lambda_1 \cdot \frac{1}{m}\sum_{i=1}^m \max\{|z_i - z_i|, \epsilon\}\\
		&\hspace{0.5cm}
		-\lambda_2 \cdot \log c_\phi(y_\text{source} \mid g_\theta(z, y_\text{source}) + \epsilon_\text{attack}\tanh(\tau))
		\end{align*}
		\State Initialize $x_\text{attack} \gets \varnothing$
		\While{$x_\text{attack} = \varnothing$}
		\State Sample $\tau \sim \mathcal{N}(0, 1)$
		\State Sample $z^0 \sim \mathcal{N}(0, 1)$
		\State Initialize $z \gets z^0$
		\For{$i = 1\ldots T$}
		\State Update $z \gets z - \alpha \cdot \frac{\partial\mathcal{L}(z, \tau)}{\partial z}$ \Comment{$\alpha$ is the learning rate.}
		\State $\Delta \gets \frac{\partial\mathcal{L}(z, \tau)}{\partial \tau}$                                 \State Update $\tau \gets \tau - \alpha \cdot \frac{\Delta}{\|\Delta\|}$ \Comment{We found gradient normalization to be effective}
		\EndFor
		\State $x \gets g_\theta(z, y_\text{source}) + \epsilon_\text{attack}\tanh(\tau)$
		\If{$y_\text{target} = \argmax_y p(f(x) = y)$}
		\State $x \gets x_\text{attack}$
		\EndIf
		\EndWhile
		\State \textbf{return} $x_\text{attack}$
		\EndProcedure
	\end{algorithmic}
"
305,1805.07894,"[ht]
	\caption{Unrestricted Adversarial Example Targeted Attack}
	\label{alg:attack}
	\begin{algorithmic}[1] % The number tells where the line numbering should start
		\Procedure{Attack}{$y_\text{target}, y_\text{source}, f, \theta, \phi, \epsilon, \epsilon_\text{attack}, \lambda_1, \lambda_2,\alpha, T$}
		\State Define $\mathcal{L}(z, \tau\mathbin{;} z^0, y_\text{target}, y_\text{source}, f, \theta, \phi, \epsilon, \epsilon_\text{attack}, \lambda_1, \lambda_2)$
		\begin{align*}
		\mathcal{L} = 
		&-\log f(y_\text{target} \mid g_\theta(z, y_\text{source}) + \epsilon_\text{attack}\tanh(\tau)) +\lambda_1 \cdot \frac{1}{m}\sum_{i=1}^m \max\{|z_i - z_i| - \epsilon, 0\}\\
		&\hspace{0.5cm}
		-\lambda_2 \cdot \log c_\phi(y_\text{source} \mid g_\theta(z, y_\text{source}) + \epsilon_\text{attack}\tanh(\tau))
		\end{align*}
		\State Initialize $x_\text{attack} \gets \varnothing$
		\While{$x_\text{attack} = \varnothing$}
		\State Sample $\tau \sim \mathcal{N}(0, 1)$
		\State Sample $z^0 \sim \mathcal{N}(0, 1)$
		\State Initialize $z \gets z^0$
		\For{$i = 1\ldots T$}
		\State Update $z \gets z - \alpha \cdot \frac{\partial\mathcal{L}(z, \tau)}{\partial z}$ \Comment{$\alpha$ is the learning rate.}
		\State $\Delta \gets \frac{\partial\mathcal{L}(z, \tau)}{\partial \tau}$                                 \State Update $\tau \gets \tau - \alpha \cdot \frac{\Delta}{\|\Delta\|}$ \Comment{We found gradient normalization to be effective}
		\EndFor
		\State $x \gets g_\theta(z, y_\text{source}) + \epsilon_\text{attack}\tanh(\tau)$
		\If{$y_\text{target} = \argmax_y p(f(x) = y)$}
		\State $x \gets x_\text{attack}$
		\EndIf
		\EndWhile
		\State \textbf{return} $x_\text{attack}$
		\EndProcedure
	\end{algorithmic}
"
306,1802.05800,"
\caption{Tree-CNN: At Inference}
\label{algo:inference-tree}
\begin{algorithmic}[1]
\State $I = $ Input Image, $node =$ Root Node of the Tree
\Procedure {ClassPredict}{$I, node$}
\State $count =$ \# of children of node
\If {$count = 0$} 
\State $label =$ class label of the node
\State return $label$
\Else 
\State $nextNode = EvaluateNode(I, node)$
\State $\blacktriangleright$ returns the address of the child node of highest output neuron
\State return $ClassPredict(I, nextNode)$
\EndIf
\EndProcedure
\end{algorithmic}
"
307,1802.05800,"
\caption{Grow Tree-CNN}
\label{algo:grow-tree}
\begin{algorithmic}[1]
\State $L =$ Likelihood Matrix
\State $maxChildren$ = max. number of children per branch node
\State $RootNode =$ Root Node of the Tree-CNN
\Procedure {GrowTree}{L, Node}
\State $S = GenenerateS(L, Node, maxChildren)$
\While{$S$ is not Empty}
\State $\blacktriangleright$ Get attributes of the first object
\State $[label, value, node] = GetAttributes(S[1])$
\If{$value[1] - value[2] > \alpha$}
\State $\blacktriangleright$ The new class has a strong preference for $n_1$
\State $\blacktriangleright$ Adds $label$ to $node[1]$
\State $RootNode = AddClasstoNode(RootNode, label, node[1])$
\Else
\If{$value[2] - value[3] > \beta$}
\State $\blacktriangleright$ The new class has similar strong preference $n_1$ and $n_2$
\State $Merge = CheckforMerge(Node, node[1], node[2])$
\State $\blacktriangleright$ $Merge$ is $True$ only if $node[2]$ is a leaf node, and, 
\State $\blacktriangleright$ the \# of children of $node[1]$ less than $maxChildren - 1$ 
\If {$Merge$}
\State $\blacktriangleright$ Merge $node[2]$ into $node[1]$
\State $RootNode = MergeNode(RootNode, node[1], node[2])$
\State $RootNode = AddClasstoNode(RootNode, label, node[1])$
\Else
\State $\blacktriangleright$ Add new class to the smaller output node
\State $sNode=$ Node with lesser children ($node[1], node[2]$)
\State $RootNode = AddClasstoNode(RootNode, label, sNode)$
\EndIf
\Else 
\State $\blacktriangleright$ Add new class as a new Leaf node to Root Node
\State $RootNode = AddNewNode(RootNode, label)$
\EndIf
\EndIf
\State $\blacktriangleright$ Remove the columns of the added class from $L$
\State $\blacktriangleright$ Remove the rows of ``full'' nodes from $L$
\State $\blacktriangleright$ Regenerate $S$
\State $S = GenenerateS(L, Node, maxChildren)$
\EndWhile
\EndProcedure
\end{algorithmic}
"
308,1805.09365,"[t]
	\caption{Dynamic Linear UCB (dLinUCB)} \label{alg}
	\begin{algorithmic}[1]
		{%\small
			\State \textbf{Inputs:}  $\lambda>0$, $\tau>0$, $\delta_1, \delta_2 \in (0,1)$, $\tilde{\delta}_1 \in [0, \delta_1]$
			\State \textbf{Initialize:} Maintain a set of slave models $\cM_t$ with $\cM_1 = \{ m_1 \}$, initialize $m_1$: $\bA_1(m_1) = \lambda \bI$, $\bb_1(m_1) = \textbf{0}$, $\hat \btheta_1(m_1) = \textbf{0}$; and initialize the `badness' statistics of it: $\hat e_1(m_1) =0$, $d_1(m_1) = 0$
			\For{ $t=1$ to $T$}	    
			\State Choose a slave model from the active slave model set $\tilde m_t = \argmin_{m \in \cM_t} \big( \hat e_t(m) - \sqrt{\ln \tau}\times d_t(m) \big)$
			\State Observe candidate arm pool $\cA_t$, with $\bx_{a}\in \mathbb{R}^d$ for $\forall a\in\cA_t$		
			\State Take action $a_{t}=\argmax_{a\in\cA_t}\big(\bx_{a}^\mt \hat \btheta_t(\tilde m_t)  + B_t(\tilde m_t,a) \big)$, in which $B_t(\tilde m_t,a)$ is defined in Eq \eqref{eq:LinUCB_arm_selection}
			\State Observe payoff $r_{a_{t}}$
			
            \State Set CreatNewFlag = True
            
			\For {$m \in \cM_t$}
			
			\State $e_t(m)=\mathds{1}\{ \lvert \hat r_t(m)-r_t \rvert > B_t(m, a)  + \epsilon$ $ \}$, where $\hat r_t(m) = \bx_{a_t}^\mt \hat \btheta_t(m)$ and $\epsilon =  \sqrt{2} \sigma \text{erf}^{-1}(\delta_1 -1)$
			%\If{$|r_{a_t} - \hat r_{a_t}(m)| > B_t(m, a)$}
			%    \State Mark $m$ as a bad model, i.e., $e_t(m) = 1$
			%\Else
			%    \State Mark $m$ as a good model, i.e., $e_t(m) = 0$
			%\EndIf
			
			 \If {$ e_t(m) = 0$}
			            \State  Update slave model: $\bA_{t+1}(m) = \bA_t(m) + \bx_{a_t} \bx_{a_t}^{\mt}$,  $\bb_{t+1}(m) = \bb_t(m) + \bx_{a_t}r_{t}$, $\hat \btheta_{t+1} = \bA_{t+1}^{-1}(m)\bb_{t+1}(m)$
			        \EndIf 
			
		 	 \State $\tilde \tau(m) = \text{min}\{ t-t_{m}, \tau \}$, where $t_m$ is when $m$ was created
		 	 %\If{$  \lVert \bx_{a_t}^\mt \hat \btheta_t(m) - \bx_{a_t}^\mt \hat \btheta_{t-\tilde \tau}(m) \rVert \leq B_t(m, a)$}
		 	 \State Update `badness' $\hat e_t(m) = \frac{\sum_{i=t-\tilde \tau}^t e_i(m)}{ \tilde \tau(m)}$, $d_t(m) = \sqrt{\frac{\ln 1/\delta_2}{2\tilde \tau(m)}}$
		 	% \EndIf
		 	 
		 	  \If{$\hat e_t(m) < \tilde{\delta_1} + d_t(m)$ }
			    \State Set CreatNewFlag = False
		 	 \ElsIf{$\hat e_t(m) \geq \delta_1  + d_t(m)$}
			\State Discard slave model $m$: $M_{t+1} = M_{t} - m$
			\EndIf  
			\EndFor 
           
        \If {\text{CreateNewFlag} or $\cM_t  = \emptyset $}
			\State Create a new slave model $m_t$:  $\cM_{t+1} = \cM_t + m_t$
			\State Initialize $m_t$: $\bA_t(m_t) = \lambda \bI$, $\bb_t(m_t) = \textbf{0}$, $\hat \btheta_t(m_t) = \textbf{0}$
			\State Initialize `badness' statistics of $m_t$: $\hat e_t(m_t) =0$, $d_t(m_t) = 0$

		\EndIf
			
	\EndFor
		}
	\end{algorithmic}
"
309,1805.09360,"[t]                    % enter the algorithm environment
  \small
  \caption{Returns the next action time}
  \label{alg:sampling-brief}
  \begin{algorithmic}[1]
  \STATE \textbf{Input: } Parameters $b_\lambda, w_t, \Vb_\lambda, \hb_i$, last event time $t'$
  \STATE \textbf{Output: } Next action time $t$
    \STATE $CDF(\bullet) \leftarrow$ Cumulative distribution of next arrival time
    \STATE $u \leftarrow $\textsc{Unif}$[0, 1]$
    \STATE $t \leftarrow {CDF}^{-1}(u)$

    \WHILE{$t < T$}
      \STATE $(s, z)\leftarrow$\textsc{WaitUntilNextFeedback}$(t)$
      \IF{feedback arrived before $t$}
        \STATE $CDF(\bullet) \leftarrow $ \textsc{Modify}$(CDF(\bullet), s, z)$
        \STATE $t \leftarrow CDF^{-1}(u)$
      \ELSE
        \RETURN t
      \ENDIF
    \ENDWHILE
    \RETURN t
  \end{algorithmic}
"
310,1805.09360,"[t]                    % enter the algorithm environment
	\small
	\caption{It returns the next action time}
	\label{alg:sampling}
	\begin{algorithmic}[1]
	\STATE \textbf{Input: } Time of previous action $t'{}$, history $\Hcal_{t'{}}$ up to $t'{}$, cut-off time $T$
	\STATE \textbf{Output: } Next action time $t$

%	\STATE\texttt{/* Takes $\Hcal_{t_\text{last}}$ and final time $T$ as input, and samples the next action. */}
% 		\STATE $t_{\text{last}}\leftarrow 0$
% 		\WHILE{$t_{\text{last}}<T$}
		 \STATE $c_1 \leftarrow \textsc{ComputeC1}(\Hcal_{t'{}})$
		 \STATE $t \leftarrow {CDF}_{1}^{-1}(u \given c_1,t'{})$
% 		 \STATE $s\leftarrow t_{\text{last}}$
		  \WHILE{$t < T$}
		  	\STATE $(e,s,z)\leftarrow$\textsc{WaitUntilNextFeedback}$(t)$
		  	\IF{$e==\text{True}$}
		  		% \STATE $t'{} \leftarrow s$
		  		\STATE $\Hcal_{t'{}}\leftarrow \Hcal_{t'{}} \cup \{ (s,z) \}$
				\STATE $c_1\leftarrow\textsc{ComputeC1}(\Hcal_{t'{}})$, $c_2\leftarrow\textsc{ComputeC2}(\Hcal_{t'{}})$
				\STATE $t \leftarrow CDF_2^{-1}(u \given c_1, t'{}, c_2, s)$
		  	\ELSE
		  		% \STATE \textsc{TakeAction}$(t)$
				\RETURN t
% 		  \STATE $t_{\text{last}} \leftarrow t$
% 		  \STATE $Q\leftarrow 1$
  		  		\STATE \textbf{break}
		   \ENDIF
		  \ENDWHILE
	  \RETURN t
% 		  \ENDWHILE
	\end{algorithmic}
% 	\label{alg:opDynamicsSim}
"
311,1802.10551,"[H]
    \caption{AvgSGD}\label{alg:AvgSGD}
    \begin{algorithmic}
    	\STATE Let $\vomega_0 \in \Omega$\\[1mm]
      \FOR{$t=0 \ldots T-1$}
      \STATE $\xi_t \sim P$ \hfill \emph{(mini-batch)}
      \STATE $\bm{d}_t \leftarrow  F( \vomega_{t},\xi_{t})$ \\[1.5mm]
      \STATE $\vomega_{t+1} \leftarrow P_\Omega[\vomega_t - \eta_t \bm{d}_t]$ \\[1mm]
      \ENDFOR
      \STATE Return $\bar \vomega_{T} \leftarrow \frac{\sum_{t=0}^{T-1}\eta_t \vomega_{t}}{\sum_{t=0}^{T-1}\eta_t}$\\[1mm]
    \end{algorithmic}
  "
312,1802.10551,"[H]
    \caption{AvgExtraSGD}\label{alg:AvgExtraSGD}
    \begin{algorithmic}
      \FOR{$t=0 \ldots T-1$}
                        	  \STATE $\xi_t,\xi_t' \sim P$ \hfill \emph{(mini-batches)}
      \STATE $\bm{d}_t \leftarrow  F( \vomega_{t},\xi_{t})$
      \STATE $\vomega'_{t} \leftarrow P_\Omega[\vomega_t - \eta_t \bm{d}_t]$
      \STATE $\bm{d}'_t \leftarrow  F( \vomega'_{t},\xi'_{t})$
            \STATE $\vomega_{t+1} \leftarrow P_\Omega[\vomega_t - \eta_t \bm{d}'_t]$       \label{algline:AvgExtraSGD_update_step}
      \ENDFOR
      \STATE Return $\bar \vomega_{T} \leftarrow \frac{\sum_{t=0}^{T-1}\eta_t \vomega'_{t}}{\sum_{t=0}^{T-1}\eta_t}$
    \end{algorithmic}
  "
313,1802.10551,"[H]
   \caption{AvgPastExtraSGD}\label{alg:AvgPastExtraSGD}
   \begin{algorithmic}
      \STATE Let $\vomega_0 \in \Omega$ \\[-1mm]
     \FOR{$t=0 \ldots T-1$}
	 \STATE $\xi_t \sim P$ \hfill \emph{(mini-batch)}
                    \STATE $\vomega'_{t} \leftarrow P_\Omega[\vomega_t - \eta_t \bm{d}_{t-1}]$
     \STATE $\bm{d}_t \leftarrow  F( \vomega'_{t},\xi_{t})$
     \STATE $\vomega_{t+1} \leftarrow P_\Omega[\vomega_t - \eta_t \bm{d}_t]$
     \ENDFOR
     \STATE Return $\bar \vomega_{T} \leftarrow \frac{\sum_{t=0}^{T-1}\eta_t \vomega'_{t}}{\sum_{t=0}^{T-1}\eta_t}$
   \end{algorithmic}
 "
314,1802.10551,"[H]
    \caption{Extra-Adam: proposed Adam with extrapolation step. \label{alg:Extra-Adam}}
    \begin{algorithmic}
      \STATE \textbf{input:} step-size $\eta$, decay rates for moment estimates $\beta_1,\beta_2$, access to the stochastic gradients $\nabla \ell_t(\cdot)$ and to the projection $P_\Omega[\cdot]$ onto the constraint set $\Omega$, initial parameter $\vomega_0$, averaging scheme $(\rho_t)_{t\geq1}$
      \FOR{$t=0 \ldots T-1$}
      \STATE \textbf{Option 1: Standard extrapolation.}
      \bindent
      \STATE Sample new mini-batch and compute stochastic gradient: $g_t \leftarrow \nabla \ell_t(\vomega_t)$
      \eindent
      \STATE \textbf{Option 2: Extrapolation from the past}
      \bindent
      \STATE Load previously saved stochastic gradient: $g_t = \nabla \ell_{t-1/2 }(\vomega_{t-1/2 }) $
      \eindent
      \STATE Update estimate of first moment for extrapolation: $m_{t-1/2 } \leftarrow \beta_1m_{t-1} + (1- \beta_1)g_t$
      \STATE Update estimate of second moment for extrapolation: $v_{t-1/2 } \leftarrow \beta_2 v_{t-1} + (1- \beta_2)g_t^2$
      \STATE Correct the bias for the moments:
      $\hat m_{t-1/2} \leftarrow m_{t-1/2} /(1-\beta^{2t-1}_1)$, $\hat v_{t-1/2} \leftarrow v_{t-1/2} /(1-\beta_2^{2t-1})$ \\[-2mm]
      \STATE Perform \emph{extrapolation} step from iterate at time $t$: $\vomega_{t-1/2} \leftarrow P_\Omega[\vomega_{t} - \eta \frac{\hat m_{t-1/2 }}{\sqrt{\hat v_{t-1/2 }}+ \epsilon}]$
      \STATE Sample new mini-batch and compute stochastic gradient:
      $g_{t+1/2 } \leftarrow \nabla \ell_{t+1/2 }(\vomega_{t+1/2 })$
      \STATE Update estimate of first moment:
      $m_{t} \leftarrow \beta_1 m_{t-1/2 } + (1- \beta_1)g_{t+1/2 }$
      \STATE Update estimate of second moment:
      $v_{t} \leftarrow \beta_2 v_{t-1/2 } + (1- \beta_2)g_{t+1/2 }^2$
      \STATE Compute bias corrected for first and second moment:
      $\hat m_{t} \leftarrow m_{t} /(1-\beta^{2t}_1)$, $\hat v_{t} \leftarrow v_{t} /(1-\beta_2^{2t})$
      \STATE Perform \emph{update} step from the iterate at time $t$:
      $\vomega_{t+1} \leftarrow P_\Omega[\vomega_{t} - \eta \frac{\hat m_{t}}{\sqrt{\hat v_{t}}+\epsilon}]$
      \ENDFOR
      \STATE \textbf{Output:} $\vomega_{T-1/2 }$, $\vomega_{T}$ or $\bar \vomega_{T} = \sum_{t=0}^{T-1}\rho_{t+1} \vomega_{t+1/2}/ \sum_{t=0}^{T-1}\rho_{t+1}$ (see \eqref{eq:online_averaging} for online averaging)
    \end{algorithmic}
  "
315,1802.10551,"[H]
\caption{Re-used mini-batches for stochastic extrapolation (ReExtraSGD)}\label{alg:ReAvgExtraSGD}
\begin{algorithmic}[1]
  \STATE Let $\vomega_0 \in \Omega$
  \FOR{$t=0 \ldots T-1$}
  \STATE Sample $\xi_t \sim P$
  \STATE $\vomega'_t \defas P_\Omega[\vomega_t - \eta_t F(\vomega_t,\xi_t)]$  \hfill $\triangleright$ Extrapolation step
  \STATE $\vomega_{t+1} \defas P_{\Omega}[\vomega_t - \eta_t F\big( \vomega'_t,\xi_t\big)]$ \hfill $\triangleright$ Update step with the \textbf{same} sample
  \ENDFOR
  \STATE Return $\bar \vomega_{T} = \sum_{t=0}^{T-1}\eta_t \vomega'_t/ \sum_{t=0}^{T-1}\eta_t$
\end{algorithmic}
"
316,1803.00710,"[t]
\caption{Deterministic Policy Gradient with Full Backup Estimation (DPG-FBE)}
\label{Alg:DPGFBE}
\begin{small}
\KwIn{Learning rate $\alpha_{\theta}$ and $\alpha_{w}$, pretrained conversion probability model $b$, continuing probability model $c$, and expected deal price model $m$ of item page histories}
Initialize the actor $\pi_{\theta}$ and the critic $Q^w$ with parameter $\theta$ and $w$\;
\ForEach{search session}{
  Use $\pi_{\theta}$ to sample a ranking action at each step with exploration\;
  %Get the trajectory $\tau = s_0, a_0, r_1, s_1, a_1, ..., a_{t-1}, r_t, s_t$\;
  Get the trajectory $\tau$ of the session with its final step index $t$\;
  $\Delta w \leftarrow 0, \Delta \theta \leftarrow 0$\;
  \For{$k = 0, 1, 2, ..., t-1$}{
    $(s_k, a_k, r_{k}, s_{k+1}) \leftarrow $ the sample tuple at step $k$\;
    $h_{k+1} \leftarrow $ the item page history of $s_{k}$\;
    \eIf{$s_{k+1} = B(h_{k+1})$}{
      Update the models $b$, $c$, and $m$ with the samples $(h_{k+1},1)$, $(h_{k+1},0)$, and $(h_{k+1}, r_{k})$, respectively\;
    }{
      Update the models $b$ and $c$ with the samples $(h_{k+1},0)$ and $(h_{k+1},1)$, respectively\;
    }
    $s' \leftarrow C(h_{k+1})$, $a' \leftarrow \pi_{\theta}(s')$\;
    $p_{k+1} \leftarrow b(h_{k+1}) m(h_{k+1})$\;
    $\delta_k \leftarrow p_{k+1} + c(h_{k+1}) Q^w(s',a') - Q^w(s_{k},a_{k})$\;
    $\Delta w \leftarrow \Delta w + \alpha_w \delta_k \nabla_{w} Q^w(s_{k},a_{k})$\;
    $\Delta \theta \leftarrow \Delta \theta + \alpha_{\theta} \nabla_{\theta} \pi_{\theta}(s_k) \nabla_{a} Q^w(s_k, a_k)$\;
  }
  $w \leftarrow w + \Delta w / t, \theta \leftarrow \theta + \Delta \theta / t$\;
}
\end{small}
"
317,1806.02682,"
%    \caption{Map names to ImageNet classes.}
%    \label{alg1}
%    \begin{algorithmic}
%        \For{class  in $ImageNet\_clases$ }
%        \State $token\_class \gets$ Tokenize and remove stopwords
%        \EndFor
%        \For{img\_name in $clipart\_dataset$ }
%        \State $token\_name \gets$ Tokenize and remove stopword
%        \For{class in $token\_class$ }
%        \For{word\_c  in $class$ }
%        \For{word\_n  in $token\_namee$ }
%        \If {word\_c = word\_n} 
%        \State Copy image to class.
%        \EndIf
%        \EndFor
%        \EndFor
%        \EndFor
%        \EndFor
%    \end{algorithmic}
%"
318,1608.08063,"[t]
\caption{Projected gradient algorithm for WDA}\label{alg:projectedgrad}
  \begin{algorithmic}[1]
\REQUIRE $\Pi_\Delta$ : projection on the Stiefel manifold
  \STATE Initialize $k = 0$, $\P^0$
  \REPEAT
  \STATE compute all the $\T^{c,c^\prime}$ as given in Equation~(\ref{eq:inner}) by means of Algorithm \ref{alg:skad}
  \STATE compute  $\C_b$ and $\C_w$
  \STATE compute Equation (\ref{eq:djdp}) for $\P^k$
  \STATE compute Equation (\ref{eq:djdt}) for $\P^k$
  \STATE compute $\frac{\partial \T^k}{\partial \P}$ using automatic differentiation based on Equations (\ref{eq:gradT}), (\ref{eq:gradu}) and (\ref{eq:gradv}) 
  \STATE compute gradient $\mathbf{G}^k = \nabla_\P J(\P^k, \T(\P^k)$ using all above elements
  \STATE compute descent direction $ \mathbf{D}^k = \Pi_\Delta (\P^k - \mathbf{G}) - \P^k$
\STATE  linesearch on the step-size $\alpha_k$  
\STATE $\P^{k+1} \leftarrow \Pi_\Delta (\P^k + \alpha_k  \mathbf{D}^k$) 
\STATE $k \leftarrow k +1$  
\UNTIL{convergence}
\end{algorithmic}
"
319,1608.08063,"[t]
\caption{Sinkhorn-Knopp algorithm with automatic differentiation \label{alg:skad}}
  \begin{algorithmic}[1]
\REQUIRE $\K = e^{-\lambda \M_{\P\X^c,\P\X^{c^\prime}}}$, $L$ the number of iterations
\STATE Initialize $k = 0$, $\u^0 =  \mathbf{1}_n$, $\frac{\partial \u_{j}^0}{ \partial \P} = \mathbf{0}$ for all $j$
\STATE compute $\frac{\partial \K_{i,j}}{ \partial \P}$ \COMMENT{store these gradients for computing $\frac{\partial \T^k}{\partial \P}$}
\FOR { $k = 1$ to  $L$}
\STATE compute $\mathbf{v^k}$ and $\mathbf{u^k}$ as given in Equation (\ref{eq:fixedpoint})
\STATE compute $\frac{\partial \v_{j}^k}{ \partial \P}$ for all $j$ \COMMENT{store these gradients for computing $\frac{\partial \T^k}{\partial \P}$}
\STATE compute $\frac{\partial \u_{j}^k}{ \partial \P}$ for all $j$ \COMMENT{store these gradients for computing $\frac{\partial \T^k}{\partial \P}$}

\ENDFOR
\OUTPUT $\u^k$, $\v$ and all the gradients
\end{algorithmic}
"
320,1805.08948,"[h]
%	\caption{$\mathtt{agent\_k\_RLSVI\_learn}$}
%	\label{lsvi}
%	
%	\begin{tabular}{lll}
%		\textbf{Input:} & $\tilde{Q}_k$ & value function family \\
%		& $\cB$ & memory buffer of observations \\
%		& $z_k$ & sequence of history perturbation seeds \\
%		& $\hat{\theta}_k$ & initial value function parameter seed \\
%		& $H$ & learning horizon \\
%		& $\gamma$ & discount factor \\
%		& $\lambda$ & level of uncertainty on true parameter of $\tilde{Q}_k$ \\
%		& $v$ & variance of noise perturbation \\
%		& $\psi(\theta)$ & a regularization function (e.g. $\psi(\theta) = \frac{1}{\lambda}\|\theta - \hat{\theta}_k\|^2$) \\
%	\end{tabular}
%	
%	\begin{algorithmic}[1]
%		\State$\theta_{k,0} \leftarrow \hat{\theta}_k$ 
%		\For{$m$ in $(1,\ldots,H)$}
%		\State $\tilde{\theta} \leftarrow \theta_{k,m-1}$
%		\For{$h$ in $(0,\ldots,H-m+1)$}
%		\State Fit
%		$$\tilde{\theta} \leftarrow \argmin_{\theta} 
%		\left(\frac{1}{v}\sum_{\substack{(s_j,a_j,r_j,s_j') \in \cB_{t_{k,m}} \\ j \in \cJ \subseteq \{1, \dots, |\cB_{t_{k,m}}|\}}} 
%		\left(r_j + \gamma \max_{a \in \cA} \tilde{Q}_{k, \tilde{\theta}}(s_j',a) + z_{k, j} - \tilde{Q}_{k,\theta}(s_j, a_j) \right)^2 + \psi(\theta)\right)$$
%		\EndFor
%		\State $\theta_{k, m} = \tilde{\theta}$
%		\State Take action $a_{k,m}$ based on $\tilde{Q}_{k, \theta_{k, m}}(s_{k,m}, \cdot)$
%		\State Observe $r_{k, m}, s_{k, m+1}$
%		\State Store $(s_{k,m},a_{k,m}, r_{k,m}, s_{k,m+1})$ in $\cB$.
%		\EndFor
%	\end{algorithmic}
%"
321,1805.08948,"[h]
%\caption{$\mathtt{agent\_k\_TD\_learn}$}
%\label{td}
%\begin{tabular}{lll}
%\textbf{Input:} & $\tilde{Q}_k$ & value function family \\
%& $\cB$ & memory buffer of observations \\
%& $z_k$ & sequence of history perturbation seeds \\
%& $\hat{\theta}_k$ & initial value function parameter seed \\
%& $N, L$ & number and size of learning batches \\
%& $\alpha$ & learning rate \\
%& $\gamma$ & discount factor \\
%& $\lambda$ & level of uncertainty on true parameter of $\tilde{Q}_k$ \\
%& $v$ & variance of noise perturbation \\
%& $\psi(\theta)$ & a regularization function (e.g. $\psi(\theta) = \frac{1}{\lambda}\|\theta - \hat{\theta}_k\|^2$) \\
%\end{tabular}
%
%\begin{algorithmic}[1]
%\State$\theta_{k,0} \leftarrow \hat{\theta}_k$ 
%\For{$m$ in $(1,\ldots,H)$}
%\State $\tilde{\theta} \leftarrow \theta_{k,m-1}$
%\For{$n$ in $(1,\ldots,N)$}
%\State Sample index batch $\cJ \subseteq \{1, \dots, |\cB_{t_{k,m}}|\}, \text{ s.t. } |\cJ| = L$ 
%\State Compute loss
%$$\cL(\theta) \leftarrow \argmin_{\theta} 
%\left(\frac{1}{v}\sum_{\substack{(s_j,a_j,r_j,s_j') \in \cB_{t_{k,m}} \\ j \in \cJ}} 
%\left(r_j + \gamma \max_{a \in \cA} \tilde{Q}_{k, \tilde{\theta}}(s_j',a) + z_{k, j} - \tilde{Q}_{k,\theta}(s_j, a_j) \right)^2 + \psi(\theta)\right)$$
%\State Update parameters based on the TD error: $\tilde{\theta} \leftarrow \tilde{\theta} - \alpha \nabla_\theta \cL(\tilde{\theta})$
%\EndFor
%\State $\theta_{k, m} = \tilde{\theta}$
%\State Take action $a_{k,m}$ based on $\tilde{Q}_{k, \theta_{k, m}}(s_{k,m}, \cdot)$
%\State Observe $r_{k, m}, s_{k, m+1}$
%\State Store $(s_{k,m},a_{k,m}, r_{k,m}, s_{k,m+1})$ in $\cB$.
%\EndFor
%\end{algorithmic}
%"
322,1805.08948,"[h]
%\caption{$\mathtt{agent\_k\_policy\_gradient\_learn}$}
%\label{policygradient}
%\begin{tabular}{lll}
%\textbf{Input:} & $\tilde{\pi}_k$ & policy function family \\
%& $\cB$ & memory buffer of observations \\
%& $z_k$ & sequence of history perturbation seeds \\
%& $L$ & size of minibatch \\
%& $\hat{\theta}_k$ & initial policy function parameter seed \\
%& $\alpha$ & learning rate \\
%& $\gamma$ & discount factor \\
%\end{tabular}
%
%\begin{algorithmic}[1]
%\State$\theta_{k,0} \leftarrow \hat{\theta}_k$ 
%\For{$m$ in $(1,\ldots,H)$}
%\State $\tilde{\theta} \leftarrow \theta_{k,m-1}$
%\State Sample index batch $\cJ \subseteq \{1, \dots, |\cB_{t_{k,m}}|\}, \text{ s.t. } |\cJ| = L$ 
%\For{$j \in \cJ$}
%\State Retrieve $j$th observation $(s_j, a_j, r_j, s'_j, t_j)$ from $\cB_{t_{k,m}}$
%\State Perturbed reward $r_j + z_{k, j}$
%\State Compute eligibility vector
%$\frac{\nabla_\theta \tilde{\pi}_k(a_j | s_j, \tilde{\theta})}{\tilde{\pi}_k(a_j | s_j, \tilde{\theta})}$
%\State Update policy parameter: $\tilde{\theta} \leftarrow \tilde{\theta} + \alpha \gamma^{t_j} (r_j + z_{k, j}) \frac{\nabla_\theta \tilde{\pi}_k(a_j | s_j, \tilde{\theta})}{\tilde{\pi}_k(a_j | s_j, \tilde{\theta})}$
%\EndFor
%\State $\theta_{k, m} = \tilde{\theta}$
%\State Take action $a_{k,m} \sim \tilde{\pi}_{k}(a | s_{k,m}, \theta_{k, m})$
%\State Observe $r_{k, m}, s_{k, m+1}$
%\State Store $(s_{k,m},a_{k,m}, r_{k,m}, s_{k,m+1}, t_{k, m})$ in $\cB$.
%\EndFor
%\end{algorithmic}
%"
323,1802.01744,"[t]
\small
\begin{algorithmic}
\State Initialize experience replay memory $\mathcal{D}$ to capacity $N$
\State Initialize $Q$-function with random or pretrained weights $\theta$
\State Initialize target action-value function $\hat{Q}$ with weights $\theta^{-} = \theta$
\For{episode $=1,M$}
\For {$t=1,T$}
	\State Sample action $a_t \sim \pi_{\alpha}(a_t \mid \tilde{s}_t, a^h_t)$ using equation~\ref{eq:beh-pol}
	\State Execute action $a_t$ and observe $(\tilde{s}_{t+1},a^h_{t+1},r_t)$
	\State Store transition $\left(\tilde{s}_t,a_t,r_t,\tilde{s}_{t+1}\right)$ in $\mathcal{D}$
	\If {$\tilde{s}_{t+1}$ \text{ is terminal}}
	\For {$k=1$ to $K$} \Comment{training loop}
	\State Sample minibatch $\left(\tilde{s}_j,a_j,r_j,\tilde{s}_{j+1}\right)$ from $\mathcal{D}$
	\State $y_j = r_j + \gamma \hat{Q}(\tilde{s}_{j+1}, \argmax_{a'} Q(\tilde{s}_{j+1}, a'; \theta); \theta^{-})$
	\State $\theta \leftarrow \theta - \eta\nabla_\theta \sum_{j} \left(y_j - Q(\tilde{s}_j, a_j; \theta) \right)^2$
	\EndFor
	\EndIf
	\State Every $C$ steps reset $\hat{Q} = Q$
\EndFor
\EndFor
\end{algorithmic}
\caption{Human-in-the-loop deep Q-learning}
\label{alg:hitl-dql}
"
324,1805.08321,"[t]
  \caption{\texttt{BMO UCB} \label{alg:genUCB}}
\begin{algorithmic}[1] 
    \State \textbf{Input: } $\{a_{i}: i \in [n]\}, \sigma, \texttt{MAX\_PULLS}, k,\delta$
    \State $t\gets 1$ \Comment{iteration counter}
    \State $B \gets \emptyset$ \Comment{set of $k$ best arms}
    \State $S \gets [n]$ \Comment{set of arms under consideration}
    \State Pull each arm once
    \While {$|B| < k$} 
    \State Compute
  $I_t = \underset{i\in S}{\argmin}\ \hat{\theta}_{i, T_i(t)}-C_{i,T_{i}(t)}$
  \If {$\hat{\theta}_{I_t, T_{I_t}(t)} + C_{I_t,T_{I_t}(t)} < \hspace{-.3cm}\underset{i\in S, i \neq I_t}{\min} \hat{\theta}_{i, T_i(t)} - C_{i,T_i(t)}$ \label{alg:line:addToSetCond}}
  \State Add $I_t$ to $B$ and remove $I_t$ from $S$ \label{alg:line:addBest}
  \State Continue to next iteration of loop 
  \EndIf
  \If {$T_{I_t}(t) < \texttt{MAX\_PULLS}$} 
  \State Pull arm $a_{I_t}$ 
  \State Construct $\hat{\theta}_{I_t, T_{I_t}(t+1)}$ and $C_{I_t,T_{I_t}(t+1)}$
  \Else 
  \State Evaluate the mean of arm $a_{I_t}$ exactly \label{alg:line:bruteForce}
  \State Set $\hat{\theta}_{I_t, T_{I_t}(t+1)} = \theta_{I_t}$, 
  $C_{I_t,T_{I_t}(t+1)}=0$. 
  \EndIf
  \State $t \gets t + 1$
  \EndWhile
\end{algorithmic}
"
325,1805.08321,"[h]
  \caption{\texttt{BMO-NN} \label{alg:UCB-knn}}
\begin{algorithmic}[1]
  \State \textbf{Input:} $x_1,\hdots x_n \in \R^d,\ \sigma,\ k,\ \delta$ 
  \For{$i =1,\hdots,n$ }
    \State \parbox[t]{195pt}{\hangindent=.5cm Construct arms $\left\{a_{j}\right\}_{j=1, j\neq i}^{n}$
    with estimators as in \eqref{eq:knn1}
    and confidence intervals as in \eqref{eq:CI} \strut}
    \State $k$-NN of $x_i \; \gets $   {\texttt{BMO UCB}}\big($\left\{a_{j}\right\}_{j\neq i},\sigma, d,k,\frac{\delta}{n} \big)$
  \EndFor
\end{algorithmic}
"
326,1805.08882,"[b]
  \begin{algorithmic}
    \STATE Randomly initialise reward network parameters $\phi_0$
    \FOR{$t=1$ to $T$}
      \STATE Sample task $i$ with demonstrations $\tau_i$
      \STATE Set $\theta_0 \leftarrow \phi_{t - 1}$
      \FOR{$n=1$ to $N$}
        \STATE $\theta_n \leftarrow \mathrm{AIRL}(\theta_{n-1}, \tau_i)$, one step of adversarial IRL
      \ENDFOR
      \STATE Update $\phi_t \leftarrow \phi_{t-1} + \alpha(\theta_N - \phi_{t-1})$
    \ENDFOR
  \end{algorithmic}
  \caption{Meta-AIRL: Reptile and adversarial IRL}
  \label{algo:reptile-irl}
"
327,1706.00046,"[t] 
% \caption{Stochastic Super Network forward algorithm}
% \begin{algorithmic}[1]
% \Procedure{SSN-forward}{$x,E,\Gamma,\theta$}
% \State $H \sim \Gamma$ \Comment{as explained in Section \ref{secion_ssn}}
% \State \textbf{For} $i \in [1..N]$, $l_i \gets $\O ~~~~~~    \textbf{EndFor} 
% \State $l_1 \gets x$
% \State \textbf{For} $i \in [2..N]$,$l_i \gets \sum\limits_{k<i} e_{k,i} h_{k,i} f_{k,i}(l_k)$~~~~ \textbf{EndFor}
% \State \textbf{return} $l_N$


% \EndProcedure
% \end{algorithmic}
% \label{algo2}
% "
328,1706.00046,"[ht] 
\caption{Stochastic Super Network forward algorithm}
\begin{algorithmic}[1]
\Procedure{SSN-forward}{$x,E,\Gamma,\theta$}
\State $H \sim \Gamma$ \Comment{as explained in Section \ref{secion_ssn}}
\For{$i \in [1..N]$}
\State $l_i \gets $\O 
\EndFor
\State $l_1 \gets x$
\For{$i \in [2..N]$}
\State $l_i \gets \sum\limits_{k<i} e_{k,i} h_{k,i} f_{k,i}(l_k)$
\EndFor 
\State \textbf{return} $l_N$
\EndProcedure
\end{algorithmic}
\label{algo2}
"
329,1802.04927,"[!tb] 
	\caption{SUGAR: Synthesis Using Geometrically Aligned Random-walks} \textbf{Input:} Dataset
	$\myvec{X} = \{\myvec{x}_1, \myvec{x}_2, \ldots, \myvec{x}_N\}, \myvec{x}_i \in \mathbb{R}^{D}$.\\
    \textbf{Output:}~Generated set of points $\myvec{Y} = \{\myvec{y}_1, \myvec{y}_2, \ldots, $
    $\myvec{y}_M\}, \myvec{y}_i \in \mathbb{R}^{D}$.~%
	\begin{algorithmic}[1]    
      \STATE Compute the diffusion geometry operators $\myvec{K}$, $\myvec{P}$, and degrees $\hat{d}(i)$, $i=1,...,N$
      (see Sec.~\ref{sec:Background}) % (Eqs.~\ref{GKernel}, \ref{EquationPDM}, and~\ref{eq:deg}).
   %   \STATE Estimate the density of points $\hat{d}(i),i=1,...,N$.
        \STATE Define a sparsity measure $\hat{s}(i),i=1,...,N$ (Eq.~\ref{eq:sparsity1}).
		\STATE Estimate a local covariance $\myvec{\Sigma}_i$, $i=1,...,N$, using $k$ nearest neighbors around each $\myvec{x}_i$. 
        \STATE For each point $i=1,...,N$ draw $ \hat{\ell}(i) $ vectors (see Sec.~\ref{sec:dense}) from a Gaussian distribution ${\cal{N}}(\myvec{x}_i,\myvec{\Sigma}_i)$. Let $\hat{\myvec{Y}}_0$ be a matrix with these $M = \sum^N_{i=1}\hat{\ell}(i)$ generated vectors as its rows.
        \STATE Compute the sparsity based diffusion operator $\hat{\myvec{P}}$ (see Sec~\ref{sec:Method}).
        \STATE Apply the operator $\hat{\myvec{P}}$ at time instant $t$ to the new generated points in $\hat{\myvec{Y}}_0$ to get diffused points as rows of ${\myvec{Y}}_t =\hat{\myvec{P}}^t \cdot {\myvec{Y}}_0$.
    \STATE Rescale $\myvec{Y}_t$ to get the output $\myvec{Y}[\cdot,j] = \myvec{Y}_t[\cdot,j] \cdot \frac{\text{percentile}(\myvec{X}[\cdot,j], .99)}{\max{\myvec{Y}_t[\cdot,j]}}$, $j=1,\ldots,D$, in order to fit the original range of feature values in the data.
	\end{algorithmic}
	\label{alg:DataGeneration}
"
330,1805.08801,"
\centering
\caption{Block Coordinate Descent Procedure}
\label{alg:bcd}
\begin{algorithmic}[1]
\REQUIRE Data set $\{\calX_i,y_i\}_{i=1}^n$, Regularization parameters $\{\lambda_k,\mu_k\}_{k=1}^K$
\STATE {\bf Initialization}: $(\calW_{(0)},b_{(0)})$, $t=0$
\WHILE{Not Converge}
\FOR{$k=1:K$}
\STATE Update $(\w_{(t)}^k,b_{(t,k)})$ by solving problem (\ref{eq:wbti})
\STATE $t=t+1$
\ENDFOR
\ENDWHILE
\end{algorithmic}
"
331,1805.08801,"
\centering
\caption{Block Proximal Gradient Descent for Multilinear Sparse Logistic Regression}
\label{alg:bpg}
\begin{algorithmic}[1]
\REQUIRE Data set $\{\calX_i,y_i\}_{i=1}^n$, Regularization parameters $\{\lambda_k,\mu_k\}_{k=1}^K$, $r_0=1$, $\delta_\omega<1$
\STATE {\bf Initialization}: $(\calW_{(0)},b_{(0)})$, $t=1$
\WHILE{Not Converge}
\FOR{$k=1:K$}
\STATE Compute $\tau_{(t)}^k$ with $\tau^k_{(t)}=\frac{\sqrt{2}}{n}\sum_{i=1}^n\left(\left\|\nabla^{(t,k)}_{\w^k} f_{(\calW,b)}(\calX^i)\right\|_2+1\right)^2$
\STATE Compute $\omega_{(t)}^k$ with $\omega_{(t)}^k=\min\left(\omega_{(t)},\delta_\omega\sqrt{\frac{\tau^k_{(t-1)}}{\tau^k_{(t)}}}\right)$
\STATE Compute $\tw^k_{(t)}$ with $\tw^{k}_{(t)}=\w^{k}_{(t-1)}+\omega^k_{(t)}(\w^{k}_{(t-1)}-\w^{k}_{(t-2)})$
\STATE Update $\w_{(t)}^k$ by $\w^k_{(t)}=\calS_{\alpha^k_{(t)}}\left(\frac{\tau^k_{(t)}\tw^k_{(t)}-\nabla_{\w^k}\ell^k_{(t)}(\tw^k_{(t)},b_{(t,k-1)}) }{\tau^k_{(t)}+\mu_k}\right)$
\STATE Compute $\tb_{(t,k)}$ with $\tb_{(t,k)}=b_{(t,k-1)}+\omega^k_{(t)}(b_{(t,k-1)}-b_{(t,k-2)})$
\STATE Update $b_{(t,k)}$ by $b_{(t,k)}=\tb_{t,k}-\frac{1}{\tau^k_{(t)}}\nabla_b\ell^k_{(t)}(\w^k_{(t)},\tb_{(t,k)}) $
\ENDFOR
\IF{$\ell(\calW_{(t-1)},b_{(t-1,K)})\leqslant\ell(\calW_{(t)},b_{(t,K)})$}
\STATE Reupdate $\w_{(t)}^k$ and $b_{(t,k)}$ using $\w^k_{(t)}=\calS_{\alpha^k_{(t)}}\left(\frac{\tau^k_{(t)}\tw^k_{(t)}-\nabla_{\w^k}\ell^k_{(t)}(\tw^k_{(t)},b_{(t,k-1)}) }{\tau^k_{(t)}+\mu_k}\right)$ and $b_{(t,k)}=\tb_{t,k}-\frac{1}{\tau^k_{(t)}}\nabla_b\ell^k_{(t)}(\w^k_{(t)},\tb_{(t,k)}) 
$, with $\tw^k_{(t)}=\w^k_{(t-1)}$ and $\tb_{(t,k)}=b_{(t,k-1)}$ 
\ENDIF
\STATE $t=t+1$
\ENDWHILE
\end{algorithmic}
"
332,1805.08610,"
\begin{algorithmic}
\STATE {\bfseries Input:}  location $x$, tolerance $\epsilon$
 \STATE $\texttt{G}\gets \texttt{GP\_model}$
 \STATE $\texttt{Hmean}, \texttt{Hvar}\gets \texttt{G.infer\_Hessian}(x)$
 \STATE $\texttt{PVEcount} \gets 0$
 \STATE $n \gets \tfrac{1}{\epsilon}-2$
\FOR {$i = 1 ... n$}
    \STATE $h \gets \texttt{draw\_Gaussian}(\texttt{Hmean}, \texttt{Hvar})$
    \STATE $h^* \gets \texttt{remove\_boundary\_elements}(h)$
    \IF {$\texttt{Cholesky}(h^*) \neq \text{FAIL}$}
    \STATE $\texttt{PVEcount} \gets \texttt{PVEcount} + 1$
    \ENDIF
\ENDFOR
\STATE $p \gets \frac{PVE\text{count} + 1}{n+2}$
\STATE  $\textbf{Return}\quad p \geq 1-\epsilon$

\end{algorithmic}
\caption{Positive Definite Test}
\label{alg:pdtest}
"
333,1805.08610,"
\caption{Positive Definite Sphere Radius}
\begin{algorithmic}
\STATE {\bfseries Input:}  center $x_{\text{min}}$, number of directions, $n_u$ tolerance $\epsilon$
\STATE $u\gets \texttt{random\_unit\_vector}$
\STATE $x_{\text{edge}} \gets \texttt{dist\_to\_domain\_boundary} $
\STATE $\hat{R} \gets \| x_{\text{min}} -x_{\text{edge}} \|$
\FOR {$i = 1 ... n_u$}
	\IF {$D(x+\hat{R}u) =0$}
        \STATE $\hat{R}\gets \texttt{binarysearch}(u,\hat{R})$
    \ENDIF
    \STATE $u\gets \texttt{random\_unit\_vector}$
\ENDFOR
\STATE $\textbf{Return}  \: \hat{R}$

\end{algorithmic}
"
334,1805.08462,"
        \DontPrintSemicolon
        \SetKwInOut{Input}{Inputs}
        \SetKwInOut{Output}{Outputs}
        \Input{$n(\le4)$, learning rate $lr$, model $f$, loss function $l$}
        $d_n^{-1} \longleftarrow 0$;\ \ $r_n^{-1} \longleftarrow 0$;\ \ $t \longleftarrow 0$\\
        initialize parameters $w^0$\\
        \While{not terminated}{
        get mini-batch input $x^t$ and label $y^t$\\
        calculate $z^t = f(x^t, w^t)$ and $l^t = l(z^t, y^t)$\\
        calculate gradient $g^t = \frac{\partial l^t}{\partial w^t}$\\
        $d_0^t \longleftarrow d_n^{t-1}$;\ \ $r_0^t \longleftarrow r_n^{t-1}$\\
        $s^t \longleftarrow \text{RNN}_s(d_0^t, r_0^t, g^t)$;\ \ $P^t \longleftarrow diag(\text{RNN}_p(d_0^t, r_0^t, g^t)) $\\
        \textbf{def} $H^tv = \frac{\partial z^t}{\partial w^t}^{\top}H^t_l\frac{\partial z^t}{\partial w^t}v + s^t \odot v, \forall v$ \\
        $d_n^t, r_n^t \longleftarrow \text{PCG}(g^t, H^t, d_0^t, P^t, n, \epsilon=0)$\\
        $w^{t+1} \longleftarrow w^t - lr * d_n^t$\\
        $t \longleftarrow t+1$ \\
        }
        \Output{$w^t$}
        \caption{Meta-Learning with Hessian-Free Approach (MLHF)\label{ML_with_HF}}
    "
335,1805.08462,"
        \DontPrintSemicolon
        \SetKwInOut{Input}{Inputs}
        \SetKwInOut{aim}{Aim}
        \SetKwInOut{Output}{Outputs}
        \aim{compute $A^{-1}b$}
        \Input{$b$, $A$, initial value  $x_0$,\\ Preconditioned Matrix $P$,\\ maximum iteration number $n$, \\
        error threshold $\epsilon$}
        $r_0 \longleftarrow b - Ax_0$\\
        $y_0 \longleftarrow \textbf{solution of } Py=r_0$\\
        $p_0 \longleftarrow y_0 $;\ \ $i \longleftarrow 0$\\
        \While{$\|r_i\|_2 \ge \epsilon $ and $i \le n$}{
        $\alpha_i \longleftarrow \frac{r_i^{\top}y_i}{p_i^{\top}Ap_i}$\\
        $x_{i+1} \longleftarrow x_i + \alpha_i p_i$;\ \ $r_{i+1} \longleftarrow r_i - \alpha_i Ap_i$\\
        $y_{i+1} \longleftarrow \textbf{solution of }Py=r_{i+1}$\\
        $\beta_{i+1} \longleftarrow \frac{r_{i+1}^{\top}y_{i+1}}{r_i^{\top}y_i}$\\
        $p_{i+1} \longleftarrow y_{i+1} +\beta_{i+1}p_i$\\
        $i \longleftarrow i+1$
        }
        \Output{$x_n\text{ with } x_n\simeq A^{-1}b, \text{residual error } r_i$}
        \caption{Preconditioned conjugate gradient algorithm (PCG)\label{PCG}}
    "
336,1805.08328,"[t]
\begin{algorithmic}
\Procedure{\toolname}{$(S,A,P,R),\pi^*,Q^*,M,N$}
\State Initialize dataset $\mathcal{D}\gets\varnothing$
\State Initialize policy $\hat{\pi}_0\gets\pi^*$
\For{$i=1$ {\bf to} $N$}
\State Sample $M$ trajectories $\mathcal{D}_i\gets\{(s,\pi^*(s))\sim d^{(\hat{\pi}_{i-1})}\}$
\State Aggregate dataset $\mathcal{D}\gets\mathcal{D}\cup\mathcal{D}_i$
\State Resample dataset $\mathcal{D}'\gets\{(s,a)\sim p((s,a))\propto\tilde{\ell}(s)\mathbb{I}[(s,a)\in\mathcal{D}]\}$
\State Train decision tree $\hat{\pi}_i\gets\text{TrainDecisionTree}(\mathcal{D}')$
\EndFor
\State\Return Best policy $\hat{\pi}\in\{\hat{\pi}_1,...,\hat{\pi}_N\}$ on cross validation
\EndProcedure
\end{algorithmic}
\caption{Decision tree policy extraction.}
\label{alg:viper}
"
337,1604.04706,"[!ht]
%  \begin{algorithmic}[1]
%    \STATE{$K$: \# classes, $P$: \# workers, $T$: total outer iterations, $t$: outer iteration index, $s$: inner epoch index}
%    \STATE{$W^{(p)}$: weights per worker, $b^{(p)}$: variational parameters per worker}
%    %\FORALL {$p = 1, 2, \ldots P \quad \textbf{\text{in parallel}}$}
%        \STATE{Initialize $W^{(p)}=0$, $b^{(p)}=\frac{1}{K}$}
%        \FORALL { $p = 1, 2, \ldots, P $ in parallel}
%	\FORALL {$t = 1, 2, \ldots, T$}
%		\FORALL{$s = 1, 2, \ldots, P$}
%			\STATE{\text{Send} $W^{(p)}$ \text{to worker on the right}}
%			\STATE{\text{Receive} $W^{(p)}$ \text{from worker on the left}}
%			\STATE{\text{Update} $W^{(p)}$ \text{stochastically using (\ref{eq:dsmlr_sgdupdatew})}}
%			\STATE{\text{Update} $b^{(p)}$ \text{stochastically using (\ref{eq:dsmlr_sgdupdateb})}}
%		\ENDFOR
%	\ENDFOR
%	\ENDFOR
%    %\ENDFOR
%  \end{algorithmic}
%  \caption{DS-MLR Synchronous}
%  \label{alg:dsmlr}
%"
338,1604.04706,"[!ht]
  \begin{algorithmic}[1]
    \STATE {$K$: \# classes, $P$: \# workers, $T$: total outer iterations, $t$: outer iteration index, $s$: inner epoch index}
    \STATE{$W^{(p)}$: weights per worker, $b^{(p)}$: variational parameters per worker}

    %\FORALL {$p = 1, 2, \ldots P \quad \textbf{\text{in parallel}}$}
        \STATE{Initialize $W^{(p)}=0$, $b^{(p)}=\frac{1}{K}$}
        \FORALL { $p = 1, 2, \ldots, P $ in parallel}
	\FORALL {$t = 1, 2, \ldots, T$}
		\FORALL{$s = 1, 2, \ldots, P$}
			\STATE{\text{Send} $W^{(p)}$ \text{to worker on the right}}
			\STATE{\text{Receive} $W^{(p)}$ \text{from worker on the left}}
			\STATE{\text{Update} $W^{(p)}$ \text{stochastically using (\ref{eq:dsmlr_sgdupdatew})}}
		\ENDFOR
		
		\FORALL{$s = 1, 2, \ldots, P$}
			\STATE{\text{Send} $W^{(p)}$ \text{to worker on the right}}
			\STATE{\text{Receive} $W^{(p)}$ \text{from worker on the left}}
                         \STATE{\text{Compute partial sums}}
		\ENDFOR
		\STATE{\text{Update} $b^{(p)}$} \text{exactly (\ref{eq:closedform_a}) using the partial sums}
	\ENDFOR
	\ENDFOR
    %\ENDFOR
  \end{algorithmic}
  \caption{DS-MLR Synchronous}
  \label{alg:dsmlr_1delay}
"
339,1604.04706,"[!ht]
\begin{algorithmic}[1]
	\STATE {$K$: total \# classes, $P$: total \# workers, $T$: total outer iterations, $W^{(p)}$: weights per worker}
	\STATE{$b^{(p)}$: variational parameters per worker, \; queue[$P$]: array of $P$ worker queues}
	\vspace*{0.1cm}
	\STATE{Initialize $W^{(p)}=0$, $b^{(p)}=\frac{1}{K}$ {\small \texttt{//Initialize parameters}}}
	\vspace*{0.1cm}
	\FOR {$k \in W^{(p)}$}
        		\STATE{Pick $q$ uniformly at random}
%        		\STATE{Compute index of queue to push: $q = k \pmod P$}		
        		\STATE{queue[$q$].push($(k, \wb_{k}))$ {\small \texttt{//Initialize worker queues}}}
        \ENDFOR
        	\vspace*{0.1cm}
	\STATE{{\small \texttt{//Start P workers}}}        
        \FORALL { $p = 1, 2, \ldots, P $ in parallel}
        		\FORALL {$t = 1, 2, \ldots, T$}
			\REPEAT
				\STATE {$(k, \wb_{k}) \leftarrow \text{queue[p].pop()}$}
				\STATE {\text{Update} $\wb_{k}$ \text{stochastically using (\ref{eq:dsmlr_sgdupdatew})}}
				\STATE{\text{Compute partial sums}}
				\STATE{Compute index of next queue to push to: $\qhat$}
				\STATE{queue[$\qhat$].push($(k, \wb_{k}))$}
			\UNTIL{\# of updates is equal to $K$}
			\STATE{\text{Update} $b^{(p)}$} \text{exactly (\ref{eq:closedform_a}) using the partial sums}
		\ENDFOR
        \ENDFOR
  \end{algorithmic}
  \caption{DS-MLR Asynchronous}
  \label{alg:dsmlr_nomad}
"
340,1706.00359,"[t]
\footnotesize
\caption{Unbounded Recurrent Neural Topic Model}
\begin{algorithmic}[1]
	\item Initialise $\Theta$ and $\Phi$;  Set active topic number $i$
	\REPEAT
	\FOR{$s \in \text{minibatches } S $}
		\FOR{$ k \in  [1, i] $}
		\STATE{ Compute topic vector $t_k = \text{RNN}_\text{Topic}(t_{k-1})$ }
		\STATE{ Compute topic distribution  $\beta_k = \text{softmax}(v \cdot t_k^T)$}
		\ENDFOR
    \FOR{$d\in  D_s$}
      \STATE{ Sample topic proportion  $\hat{\theta}  \sim  G_\text{RSB} (\theta | \mu(d), \sigma^2(d))$}
      \FOR{$w\in \text{document } d$}
      		 \STATE{ Compute log-likelihood  $\log p(w|\hat{\theta}, \beta) $}
      \ENDFOR
      \STATE{ Compute lowerbound $\mathcal{L}_d^{i-1}$ and $\mathcal{L}_d^{i}$ }
      \STATE{ Compute gradients $\nabla_{\Theta,\Phi} \mathcal{L}_d^{i}$ and update}
    \ENDFOR
    \STATE{ Compute likelihood increase $\mathcal{I}$}
    \IF {$\mathcal{I}>\gamma$}
         \STATE{ Increase active topic number $i=i+1$}
    \ENDIF
    \ENDFOR
    \UNTIL{Convergence}
\end{algorithmic}
\label{al:rntm}
\vspace{-0.5em}
"
341,1805.08249,"[t]
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\small

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\Input{an initial classifier $f^{(0)}$, an initial mapping $m^{(0)}$, \\
dataset $D$, number of iterations $K$}
\Output{the final mapping $m^{(K)}$}
Initialize a sample set $F^{(0)}=\{ f^{(0)} \}$.

\For{$k \leftarrow 1$ \KwTo $K$}{
$\theta_{f^{(k)}} \leftarrow \theta_{f^{(k-1)}} - \eta_f \nabla_{\theta_f} L(m^{(k-1)}, f^{(k-1)})$

$F^{(k)} \leftarrow F^{(k-1)} \cup \{ f^{(k)} \}$

$f' \leftarrow \text{Sample}(F^{(k)})$

$\theta_{m^{(k)}} \leftarrow \theta_{m^{(k-1)}} + \eta_m \nabla_{\theta_{m}} S(m^{(k-1)}, f')$

$F^{(k)} \leftarrow \text{Thin}(F^{(k)})$
}
\caption{\label{alg1}
Classifier-agnostic saliency map extraction}
"
342,1805.08241,"[t]
{ %
 \caption{{Pardalos and Kovoor's Algorithm}\label{alg:pardalos}}
\begin{algorithmic}[1]
  \STATE {\bfseries input:} 
   $\bs{a}, \bs{b}, \bs{c}, d$
   \STATE Initialize working set $\mathcal{W}\leftarrow \{1,\ldots,J\}$
   \STATE Initialize set of split points:\label{algline:splitpoints} 
   $$\mathcal{P}\leftarrow \{a_j, b_j\}_{j=1}^J \cup \{\pm \infty\}$$
   \STATE Initialize $\tau_{\mathrm{L}}\leftarrow -\infty$, 
    $\tau_{\mathrm{R}}\leftarrow \infty$, $s_{\mathrm{tight}}\leftarrow 0$, $\xi \leftarrow 0$.
	\WHILE{$\mathcal{W} \ne \varnothing$}
	\STATE Compute $\tau\leftarrow \mathrm{Median}(\mathcal{P})$
	\STATE Set $s \leftarrow s_{\mathrm{tight}} + 
\sum_{j\in \mathcal{W}\,|\, b_i < \tau} c_j b_j + \sum_{j\in \mathcal{W}\,|\, a_j > \tau} c_j a_j + (\xi + \sum_{j \in \mathcal{W}\,|\, a_j \le \tau \le b_j} c_j)\tau$ 	
	\STATE If $s \le d$, set $\tau_{\mathrm{L}} \leftarrow \tau$;
	if $s \ge d$, set $\tau_{\mathrm{R}} \leftarrow \tau$
	\STATE Reduce set of split points: $\mathcal{P} \leftarrow \mathcal{P} \cap [\tau_{\mathrm{L}},\tau_{\mathrm{R}}]$
	\STATE Update tight-sum: $s_{\mathrm{tight}} \leftarrow s_{\mathrm{tight}} + 
\sum_{j\in \mathcal{W}\,|\, b_i < \tau_L} c_j b_j + \sum_{j\in \mathcal{W}\,|\, a_j > \tau_R} c_j a_j$
	\STATE Update slack-sum: $\xi \leftarrow \xi + \sum_{j\in \mathcal{W}\,|\,a_j \le \tau_L \wedge b_j \ge \tau_R} c_j$
    \STATE Update working set: $\mathcal{W} \leftarrow \{j \in \mathcal{W} \,|\, \tau_L < a_j < \tau_R \vee \tau_L < b_j < \tau_R\}$
	\ENDWHILE
   \STATE Define $y^* \leftarrow(d-s_{\mathrm{tight}})/\xi$
   \STATE Set $x_j^{\star} = \max\{a_j, \min\{b_j, y\}\}, \,\, \forall j \in [J]$
   \STATE \textbf{output:} $\bs{x}^{\star}$. 
\end{algorithmic}}
"
343,1805.08241,"[t]
%   \caption{Constrained Softmax Backprop}
%\begin{algorithmic}[1]\label{alg:csoftmax_bprop}
%   \REQUIRE $\bs{z}$, $\bs{u}$, $\mathrm{d}\bs{\alpha}$ 
%   (and cached $\bs{\alpha}$, $\mathcal{A}$, $s$ from Alg.~\ref{alg:csparsemax})
%   \ENSURE $\mathrm{d}\bs{z}$, $\mathrm{d}\bs{u}$
%   \STATE $m := \sum_{i \in \mathcal{A}} \alpha_i \,\, \mathrm{d}\alpha_i / (1-s)$
%   \FOR{$i \in [L]$}
%   \IF{$i \in \mathcal{A}$}
%   \STATE $\mathrm{d}z_i := \alpha_i (\mathrm{d}\alpha_i - m)$, \quad $\mathrm{d}u_i := 0$
%%   \STATE $\mathrm{d}u_i = 0$
%   \ELSE
%   \STATE $\mathrm{d}z_i := 0$, \quad $\mathrm{d}u_i := \mathrm{d}\alpha_i - m$
%%   \STATE $\mathrm{d}u_i = \mathrm{d}\alpha_i - m$
%   \ENDIF
%   \ENDFOR
%\end{algorithmic}
%"
344,1805.08241,"[t]
%   \caption{Constrained Sparsemax Backprop}
%\begin{algorithmic}[1]\label{alg:csparsemax_bprop}
%   \REQUIRE $\bs{z}$, $\bs{u}$, $\mathrm{d}\bs{\alpha}$ 
%   (and cached $\mathcal{R}_1, \mathcal{R}_2, \mathcal{R}_3$, from Alg.~\ref{alg:csparsemax})
%   \ENSURE $\mathrm{d}\bs{z}$, $\mathrm{d}\bs{u}$
%   \STATE $m := \frac{1}{|\mathcal{R}_1|}\sum_{i \in \mathcal{R}_1} \mathrm{d}\bs{\alpha}$
%   \FOR{$i \in [L]$}
%   \IF{$i \in \mathcal{R}_1$}
%   \STATE $\mathrm{d}z_i := \mathrm{d}\alpha_i - m$, \quad $\mathrm{d}u_i := 0$
%%   \STATE $\mathrm{d}u_i = 0$
%   \ELSIF{$i \in \mathcal{R}_2$}
%   \STATE $\mathrm{d}z_i := 0$, \quad $\mathrm{d}u_i := \mathrm{d}\alpha_i - m$
%%   \STATE $\mathrm{d}u_i = \mathrm{d}\alpha_i - m$
%   \ELSE
%   \STATE $\mathrm{d}z_i := 0$, \quad $\mathrm{d}u_i := 0$
%   \ENDIF
%   \ENDFOR
%\end{algorithmic}
%"
345,1707.09049,"[!ht]
	\caption{Variational Joint Filtering (single step)}
	\label{alg:filtering}
	\begin{algorithmic}
		\Procedure{VJF}{$\vy_{t}, \bm{u}_{t-1}, \bm{\mu}_{t-1}, \vs_{t-1}, \bm{\Theta}$}
		\State $\epsilon_{t} \gets \mathcal{N}(\bm{0}, \vI), \; \epsilon_{t-1} \gets \mathcal{N}(\bm{0}, \vI)$ \Comment{Draw random samples}
		\State [$\bm{\mu}_{t}, \vs_{t}] \coloneqq \vh(\vy_{t}, \vu_{t-1}, \bm{\mu}_{t-1}, \vs_{t-1})$ \Comment{State estimation}
			\State $\tilde\vx_{t} \coloneqq \bm{\mu}_{t} + \vs_{t}^{\nicefrac{1}{2}} \epsilon_t$
			\State $\tilde\vx_{t-1} \coloneqq \bm{\mu}_{t-1} + \vs_{t-1}^{\nicefrac{1}{2}} \epsilon_{t-1}$ % 
			\State Update $\bm{\Theta}$ with $\grad_{\bm{\Theta}}
		\hat{\mathcal{L}}(\bm{\Theta}; \vy_{t}, \tilde\vx_{t}, \tilde\vx_{t-1}, \bm{u}_{t-1})$
			\Comment{Model update}
			\State\Return $\bm{\mu}_{t}, \vs_{t}$ and $\bm{\Theta}$
		\EndProcedure
	\end{algorithmic}
"
346,1805.08061,"[H]
% \KwData{Stream of data $\sample_t$, function $\AnyOpNA$, in-control value $\theta^\star$, forgetting factor $0<\updt<1$, threshold $\thres>0$, initial value $\Sketch_0$}
%% Initialize $\Sketch_0 =\initvec$\;
% \For{$t=1,2,\ldots$}{
%  $\Sketch_t = (1-\updt) \Sketch_{t-1} + \updt  \AnyOp{\sample_t}$\;
%  \If{$\norm{\Sketch_t - \theta^\star} \geq \thres$}{Flag $t$ as a change-point}
% }
%% \vspace{12pt}
%\caption{\small \EWMA{}}
%\label{alg:ewma}
%"
347,1805.08000,"
  \caption{Training with ANL}
  \label{alg1}
  \begin{algorithmic}
  %%\ENSURE $y = x^n$
  %%\STATE Randomly initialize network N
  \REQUIRE $\epsilon > 0$
  \REPEAT
%  	\STATE $\bm \eta \gets 0$
	\STATE Sample a batch $\{\bm X, \bm Y\}$ from the training data.
  	\STATE Forward-propagation calculate the loss $J(\bm X, \bm Y;\bm \theta, 0)$ and standard deviation of intermediate activations.
  	\STATE Backward-propagation calculate the gradients.
  	\FOR{$t$ := 1 to $L$}
  		\STATE update $\bm \eta_t$ with Eq.~(\ref{eq:etat})
  	\ENDFOR
  	\STATE Second forward-propagation get $J(\bm X, \bm Y;\bm \theta, \bm \eta)$
  	\STATE Second backward-propagation calculate the gradients.
  	\STATE Update network with $\bm \theta \gets \bm \theta - \lambda \nabla_{\bm \theta} J(\bm x, y; \bm \theta, \bm \eta)$
  \UNTIL training finish
  \end{algorithmic}
"
348,1805.08000,"
  \caption{Training with CANL}
  \label{alg2}
  \begin{algorithmic}
  %%\ENSURE $y = x^n$
  %%\STATE Randomly initialize network N
  \REQUIRE $\epsilon > 0$
  \STATE $\bm G_t^c$ is the $t^{th}$ layer gradient cache for class $c$. The network containing $L$ convolutional blocks. Training data containing $C$ classes.
  \FOR{$t$ := $1$ to $L$}
  	\FOR{$c$ := $1$ to $C$}
  		\STATE $\bm G_t^c \gets 0$
  	\ENDFOR
  \ENDFOR
  \REPEAT
  	\STATE Sample a batch $\{\bm X, \bm Y\}$ from the training data.
  	\FOR{$t$ := $1$ to $L$}
  		\STATE  Get gradients $\bm g_t$ from the cache $\bm G_t$ according to $\bm Y$ to calculate noise $\bm \eta_t$.
  	\ENDFOR
  	\STATE Forward-propagation calculate the loss $J(\bm X, \bm Y;\bm \theta, \bm \eta)$
  	\STATE Backward-propagation calculate the gradients.
  	\FOR{$t$ := $1$ to $L$}
  		\STATE For every class $c$ in $\bm Y$, we random sample a corresponding $\bm x$ from $\bm X$ and then update cache with $\bm G_t^c \gets \nabla_{\bm h_t} J(\bm x, y;\bm \theta, \bm \eta)$.
  	\ENDFOR
  	\STATE Update network with $\bm \theta \gets \bm \theta - \lambda \nabla_{\bm \theta} J(\bm X, \bm Y; \bm \theta, \bm \eta)$
%  	\STATE $\bm \eta \gets 0$
%  	\STATE calculate the loss $J(\bm x,y;\bm \theta, \bm \eta)|_{\bm \eta = 0}$
%  	\STATE $\bm g_t \gets \nabla_{\bm h_t} J(\bm x, y;\bm \theta,\bm \eta)|_{\bm \eta = 0}$
%  	\FOR{$\bm h_t$ in $\bm h$}
%  		\STATE calculate $s(\bm h_t)$
%  		\STATE update $\bm \eta_t$ with Eq.~(\ref{eq:etat})
%  	\ENDFOR
%  	\STATE calculate the loss $J(\bm x,y;\bm \theta, \bm \eta)$ with adversarial noise $\bm \eta$
%  	\STATE update network with $\bm \theta \gets \bm \theta - \lambda \nabla_{\bm \theta} J(\bm x, y; \bm \theta, \bm \eta)$
  \UNTIL training finish
  \end{algorithmic}
"
349,1805.07909,"[tbh]
%   \caption{Quick Shift}
%   \label{alg:quickshift}
%\begin{algorithmic}
%	\STATE Input: Samples $X_{[n]} := \{x_1,...,x_n\}$, empirical density function $\widehat{f}$, segmentation parameter $\tau > 0$.
%	\STATE Initialize directed graph $G$ with vertices $\{x_1,...,x_n\}$ and no edges.
%	\FOR{$i = 1$ to $n$}
%	\IF{there exists $x \in X_{[n]}$ such that $\widehat{f}(x) > \widehat{f}(x_i)$ and $||x-x_i|| \le \tau$}
%	\STATE Add to $G$ a directed edge from $x_i$ to $\text{argmin}_{x_j \in X_{[n]} : \widehat{f}(x_j) > \widehat{f}(x_i)} ||x_i - x_j||$.
%	\ENDIF
%	\ENDFOR
%	\STATE {\bf return} $G$.
%\end{algorithmic}
%"
350,1805.07909,"[H]
   \caption{MCores (estimating cluster-cores)}
   \label{alg:clustercore}
\begin{algorithmic}
   \STATE Parameters $k$, $\beta$
   \STATE Initialize $\widehat{\mathcal{M}}:= \emptyset$.
   \STATE Sort the $x_i$'s in decreasing order of $f_k$ values (i.e. $f_k(x_i) \geq f_k(x_{i+1})$). 
   \FOR{$i=1$ {\bfseries to} $n$}
   \STATE Define $\lambda := f_k(x_i)$.
   \STATE Let $A$ be the CC of $G(\lambda - \beta \lambda)$ containing $x_i$. 
   \IF{$A$ is disjoint from all cluster-cores in $\widehat{\mathcal{M}}$}
   \STATE Add $A$ %$\{x \in A : f_k(x) \ge \lambda - \beta \lambda\}$ 
   to $\widehat{\mathcal{M}}$. 
    \ENDIF
   \ENDFOR
   \STATE \textbf{return} $\widehat{\mathcal{M}}$.
\end{algorithmic}
"
351,1805.07909,"[H]
   \caption{Quickshift++}
   \label{alg:quickshiftpp}
\begin{algorithmic}
   \STATE Let $\widehat{\mathcal{M}}$ be the cluster-cores obtained by running Algorithm~\ref{alg:clustercore}.
   \STATE Initialize directed graph $G$ with vertices $\{x_1,...,x_n\}$ and no edges.
   \FOR{$i=1$ {\bfseries to} $n$}
   	\STATE If $x_i$ is not in any cluster-core, then add to $G$ an edge from $x_i$ to its closest sample $x \in X_{[n]}$ such that $f_k(x) > f_k(x_i)$.
   \ENDFOR
   \STATE For each cluster-core $M \in \widehat{\mathcal{M}}$, let $\widehat{\mathcal{C}}_M$ be the points $x \in X_{[n]}$ such that the directed path in $G$ starting at $x$ ends in $M$.
   \STATE \textbf{return} $\{\widehat{\mathcal{C}}_M : M \in  \widehat{\mathcal{M}}\}$.
\end{algorithmic}
"
352,1805.07898,"%[t]
	\small
	\SetAlgoNoLine
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\Input{Training dataset $\bm{X}$, total iterations $T$, model $Q(\bm{x}, \bm{w})$ with initial parameter $\bm{w}=\bm{w}_0$}
	\Indp
	\begin{algorithmic}[1]
		\FOR{$t \in \{0,\dots,T-1\}$}
		\STATE Randomly sample a batch data $\bm{x}_t$ from $\bm{X}$
		\STATE Perturbation: $\bm{w}_t = \bm{w}_t + \bm{\theta}_t$
		where $\theta_{ti} \overset{\text{i.i.d.}}{\sim} U(-a, a)$
		\STATE Back-propagation: $\bm{g}_t = \frac{\partial Q(\bm{x}_t, \bm{w}_t)}{\partial \bm{w}_t} $
		\STATE Denoising: $\bm{w}_t = \bm{w}_t - \bm{\theta}_t$
		\STATE Updating: $\bm{w}_{t+1} = \bm{w}_t - \eta_t \cdot \bm{g}_t$
		
		\ENDFOR
	\end{algorithmic}
	
	\Indm\Output{}
	\Indp
	The model $Q(\bm{x}, \bm{w})$ with final parameter $\bm{w}=\bm{w}_T$
	\caption{\sout{} in Back Propagation} 
	\label{algo:smoothbp}
"
353,1805.07898,"[H]
%		\small
%		\SetAlgoNoLine
%		\SetKwInOut{Input}{Input}
%		\SetKwInOut{Output}{Output}
%		
%		\Input{Training dataset $\bm{X}$, total iterations $T$, model $Q(\bm{x}, \bm{w})$ with initial parameter $\bm{w}=\bm{w}_0$}
%		\Indp
%		\begin{algorithmic}[1]
%		  \FOR{$t \in \{0,\dots,T-1\}$}
%		    \STATE Randomly sample a batch data $\bm{x}_t$ from $\bm{X}$
%		    \STATE Perturbation: $\bm{w}_t = \bm{w}_t + \bm{\theta}_t$
%		    where $\theta_{ti} \overset{\text{i.i.d.}}{\sim} U(-a, a)$
%		    \STATE Back-propagation: $\bm{g}_t = \frac{\partial Q(\bm{x}_t, \bm{w}_t)}{\partial \bm{w}_t} $
%		    \STATE Denoising: $\bm{w}_t = \bm{w}_t - \bm{\theta}_t$
%		    \STATE Updating: $\bm{w}_{t+1} = \bm{w}_t - \eta_t \cdot \bm{g}_t$
%
%		  \ENDFOR
%	    \end{algorithmic}
%		
%		\Indm\Output{}
%		\Indp
%		The model $Q(\bm{x}, \bm{w})$ with final parameter $\bm{w}=\bm{w}_T$
%		\caption{\sout{} in Back Propagation} 
%		\label{algo:smoothbp}
%	"
354,1710.10766,"[t]
	\caption{PixelDefend}
	\label{alg:pixeldefend}
	\begin{algorithmic}[1]
		\Require Image $\bf X$, Defense parameter $\epsilon_{\text{defend}}$, Pre-trained PixelCNN model $p_{\text{CNN}}$
		\Ensure Purified Image $\bf X^{*}$    
		\State $\mbf{X}^* \gets \mbf{X}$
		\For{each row $i$}
		\For{each column $j$}
		\For{each channel $k$}
		\State $x \leftarrow \mbf{X}[i, j, k]$
		\State Set feasible range $R \leftarrow [\max(x-\epsilon_{\text{defend}}, 0), \min(x+ \epsilon_{\text{defend}}, 255)]$
		\State Compute the 256-way softmax $p_{\text{CNN}}(\mbf{X}^*)$.
		\State Update $\mbf{X^{*}}[i, j, k] \leftarrow \argmax_{z \in R} p_{\text{CNN}}[i, j, k, z]$
		\EndFor
		\EndFor        
		\EndFor
		\begin{comment}
		\State Compute the original cross-entropy loss $l \gets L(\mbf{X}, p_{\text{CNN}}(\mbf{X}))$
		\State Compute the cross-entropy loss $l^* \gets L(\mbf{X}^*, p_{\text{CNN}}(\mbf{X}^*))$
		\If{$l < l^*$}
		\State $\mbf{X}^* \gets \mbf{X}$
		\EndIf
		\end{comment}
	\end{algorithmic}
"
355,1805.07889,"[tp]
		\caption{Decoding from the Labeling Sequence} 
		\label{alg_decode}
		\begin{algorithmic}[1]
			\REQUIRE A labeling sequence $\tau=\{t_1, t_2, \dots, t_i, \dots\, t_N\}$, and its corresponding sentence $S = \{w_1, w_2, \dots, w_i, \dots, w_N\}$.
			\ENSURE A list of aspect term triples
			\STATE $result \gets ()$
			\STATE $temp \gets \text{``''}$
			\STATE $start \gets 0$
			\FOR{$i=1$; $i \leq N$; $i++$}
			\IF{$t_i = \text{``O''}$ \AND $temp \neq \text{``''}$} 
			\STATE $result \gets result + (w_{start:i}, start, i)$
			\STATE $temp \gets \text{``''}$
			\STATE $start \gets 0$
			\ELSE 
			\IF{$t_i = \text{``B-AP''}$}
			\IF{$temp \neq \text{``''}$}
			\STATE $result \gets result + (w_{start:i}, start, i)$
			\ENDIF
			\STATE $temp \gets t_i$
			\STATE $start \gets i$
			\ENDIF
			\ENDIF
			\ENDFOR
			\IF{$temp \neq \text{``''}$}
			\STATE $result \gets result + (w_{start:i}, start, i)$
			\ENDIF
			\RETURN\!\!{$result$}
		\end{algorithmic}
	"
356,1805.07889,"[tp]
		\caption{BiDTreeCRF Training Algorithm} 
		\label{alg_bidtreecrf}
		\begin{algorithmic}[1]
			\REQUIRE A set of review sentences $\mathcal{S}$ from a particular domain, $S = \{w_1, w_2, \dots, w_i, \dots, w_N\}$ is one of the element in $\mathcal{S}$.
			\ENSURE Learned BiDTreeCRF model
			\STATE Construct dependency trees for each sentence $S$ using Stanford Parser Package.
			\STATE Initialize all learnable parameters $\Theta$
			\REPEAT
			\STATE Select a batch of instances $\mathcal{S}_b$ from $\mathcal{S}$
			\FOR{each sentence $S \in \mathcal{S}_b$} 
			\STATE Use BiDTree (\ref{eq_concatenate}-\ref{eq_bideptree_h}) to generate $h$
			\STATE Use BiLSTM (\ref{eq_bilstm}-\ref{eq_bilstm_concatnate}) to generate $g$
			\STATE Compute $L\left( W,b \right)$ through (\ref{eq_crf_pro}-\ref{eq_crf})
			\ENDFOR
			\STATE Use the backpropagation algorithm to update parameters $\Theta$ by minimizing the objective (\ref{eq_objective}) with the batch update mode
			\UNTIL{\emph{stopping criteria is met}}
		\end{algorithmic}
	"
357,1709.03441,"[ht]
\caption{Strong Weak Arm Pulls (SWAP)}\label{alg:swap}
\begin{algorithmic}[1]
\Require Confidence $\delta\ \in (0,1)$; Maximization oracle: $\doracle(\cdot):\mathbb{R}^n \rightarrow \mathcal{M}$
\State Weak pull each arm $a \in [n]$ once to initialize empirical means $\hat{\mathbf{u}}_n$
\State $\forall i \in [n]$ set $T_n(a_i)\gets 1$,
\State $\Cost_n \gets n$, total resources spent
\For{$t=n,n+1,\ldots$}
\State $M_t \gets \doracle(\hat{\mathbf{u}}_t)$
\For{$a_i = 1,\ldots,n$}
\State $\rad_t(a_i)=\sigma\sqrt{2\log\left(\frac{4n\Cost^3_t}{\delta}/T_t(a_i)\right)}$
\If {$a_i \in M_t$} 
\State $\tilde{u_t}(a_i) \gets \hat{u}_t(a_i) - \rad_t(a_i)$
\Else {} 
\State $\tilde{u_t}(a_i) \gets \hat{u}_t(a_i) + \rad_t(a_i)$
\EndIf
\EndFor
\State $\tilde{M}_t \gets \doracle(\tilde{\mathbf{u}}_t)$
\If {$w(\tilde{M}_t) = w(M_t)$}\label{line:equality}
\State $\texttt{Out} \gets M_t$
\State \Return $\texttt{Out}$
\EndIf
\State $p_t \gets \arg\max_{a \in (\tilde{M}_t \setminus M_t) \cup (M_t \setminus \tilde{M}_t)}\rad_t(a)$
\State $\alpha \gets spp(s,j)$
\State \textbf{with} probability $\alpha$ \textbf{do}
\State \hspace{\algorithmicindent} Strong pull $p_t$
\State \hspace{\algorithmicindent} $T_{t+1}(p_t) \gets T_t(p_t)+s$
\State \hspace{\algorithmicindent}$\Cost_{t+1} \gets \Cost_t+j$
\State \textbf{else}
\State \hspace{\algorithmicindent} Weak pull $p_t$
\State \hspace{\algorithmicindent} $T_{t+1}(p_t) \gets T_t(p_t)+1$
\State \hspace{\algorithmicindent} $\Cost_{t+1} \gets \Cost_t+1$
\State Update empirical mean $\hat{\mathbf{u}}_{t+1}$ using observed reward
\State $T_{t+1}(a) \gets T_t(a)\ \forall a \neq p_t$

\EndFor
\end{algorithmic}
"
358,1709.03441,"
\caption{Combinatorial Lower-Upper Confidence Bound (CLUCB)}\label{alg:clucb}
\begin{algorithmic}[1]
\Require Confidence $\delta\ \in (0,1)$; Maximization oracle: $\doracle(\cdot):\mathbb{R}^n \rightarrow \mathcal{M}$
\State Weak pull each arm $a \in [n]$ once.
\State Initialize empirical means $\bar{\mathbf{u}}_n$
\State $\forall a \in [n]$ set $T_n(a)\gets 1$
\For{$t=n,n+1,\ldots$}
	\State $M_t \gets \doracle(\bar{\mathbf{u}}_t)$
	\State $\forall a \in [n]$ compute confidence radius $\rad_t(a)$
	\For{$a = 1,\ldots,n$}
    	\If {$a \in M_t$} $\tilde{u_t}(a) \gets \bar{u}_t(a) - \rad_t(a)$
        \Else {} $\tilde{u_t}(a) \gets \bar{u}_t(a) + \rad_t(a)$
        \EndIf
	\EndFor
    \State $\tilde{M}_t \gets \doracle(\tilde{\mathbf{u}}_t)$
    \If {$\tilde{w}(\tilde{M}_t) = \tilde{w}(M_t)$}
        	\State $\texttt{Out} \gets M_t$
            \State \Return $\texttt{Out}$
        \EndIf
        \State $p_t \gets \arg\max_{a \in (\tilde{M}_t \setminus M_t) \cup (M_t \setminus \tilde{M}_t)}\rad_t(a)$
        \State Pull arm $p_t$
        \State Update empirical means $\bar{\mathbf{u}}_{t+1}$ using the observed reward
        \State $T_{t+1}(p_t) \gets T_t(p_t)+1$
        \State $T_{t+1} \gets T_t(a)\ \forall a \neq p_t$
\EndFor
\end{algorithmic}
"
359,1805.07805,"[]
\small{
\DontPrintSemicolon
\KwData{$s$, $\beta$, $A^{\beta}$, $(c_{\min}, c_{\max})$}
\KwResult{$\{\pi(a|s),\ a\in\mathcal{A}\}$}
\Begin{
$\mathcal{\tilde{A}} \longleftarrow \mathcal{A}$\;
$\Delta \longleftarrow 1 - c_{\min}$\;
$\pi(a|s) \longleftarrow c_{\min} \beta(a|s) \ \forall a \in \mathcal{A}$\;

\While{$\Delta > 0$}{
$a = \arg\max_{a\in\tilde{\mathcal{A}}} A^{\beta}(s,a)$\;
$\Delta_a = \min\{\Delta, (c_{\max} - c_{\min})\beta(a|s)\}$\;
$\mathcal{\tilde{A}} \longleftarrow \mathcal{\tilde{A}}/a$\;
$\Delta \longleftarrow \Delta - \Delta_a$\;
$\pi(a|s) \longleftarrow \pi(a|s) + \Delta_a$\;
}
}
}
\caption{Max-Reroute}
\label{alg:max_reroute}
"
360,1805.07805,"[ht!]
\small{
\DontPrintSemicolon
\KwData{$\mathcal{D}=\{(s_i,a_i,\pi_i,r_i,\delta_i),..\}$}
\KwResult{$\theta$, $\phi$}
\While {True}{
\BlankLine
$B \longleftarrow \textrm{PriorityBatchSample}(\mathcal{D}, N)$\;
$\pi_{\theta_k}\longleftarrow \pi\textrm{-net}(s)$\;
$Q^{\pi}_{\phi_k}\longleftarrow Q\textrm{-net}(s, a, \cdot)$\;
Calculate a target value with a target network:\;
$R_i=\sum_{t=1}^n\gamma^t r_{i+t} + \gamma^{n}V^{\pi}_{\bar{\phi}}(s_{i+n})$\;
$\mathcal{L}^{\pi}(\pi_{\theta_k})= \frac{1}{N}\sum_{i\in B} w_i D_{KL}\left( \pi_i, \pi_{\theta_k, i} \right)$ 
\BlankLine
$\mathcal{L}^{q}(Q^{\pi}_{\phi_k})=\frac{1}{N}\sum_{i\in B} w_i (R_i - Q^{\pi}_{\phi_k, i})^2$\;
\BlankLine
$\theta_{k+1} = \theta_{k} - \alpha \nabla_{\theta} \mathcal{L}^{\pi}(\pi_{\theta_k})$\;
\BlankLine
$\phi_{k+1} = \phi_{k} - \alpha \nabla_{\phi} \mathcal{L}^{q}(Q^{\pi}_{\phi_k})$
}
}
\caption{RBI learner}
\label{alg:learner}
"
361,1805.07805,"[ht!]
\small{
\DontPrintSemicolon
$s\longleftarrow\textrm{ResetMDP()}$\;
\While{t = 0}{
$\hat{\pi}\longleftarrow \pi\textrm{-net}(s)$\;
$Q^{\hat{\pi}}, V^{\hat{\pi}} \longleftarrow Q\textrm{-net}(s, \cdot, \hat{\pi})$\;
$\pi\longleftarrow \textrm{MaxReroute}(\hat{\pi},s, A^{\hat{\pi}})$\;
\BlankLine
$\beta \longleftarrow \textrm{AddExploration}(\pi)$\;
$a \longleftarrow \textrm{Sample}(\beta )$\;
$s,r,t \longleftarrow \textrm{StepMDP}(a)$\;
}
$\delta \longleftarrow \textrm{CalcPriorities}(\{r\}, \{V^{\hat{\pi}}\}, \gamma)$\;
\BlankLine
$\textrm{AddToDataset}(\{(s_i,a_i,\pi_i,r_i,\delta_i),...\})$\;
}
\caption{RBI actor}
\label{alg:actor}
"
362,1805.07805,"[H]
\DontPrintSemicolon
\KwData{$s$, $\beta$, $A^{\mathcal{D}}$, $\delta$}
\KwResult{$\{\pi(a|s),\ a\in\mathcal{A}\}$}
\Begin{
$\pi(a|s) \longleftarrow \beta(a|s)$, $\forall a\in\mathcal{A}$\;
$a = \arg\max_{a\in\tilde{\mathcal{A}}} A^{\mathcal{D}}(s,a)$\;
$\Delta = \min\{\delta, 1-\beta(a|s)\}$\;
$\pi(a|s) \longleftarrow \pi(a|s) + \Delta$\;
$\mathcal{\tilde{A}} \longleftarrow \mathcal{A}$\;
\While{$\Delta > 0$}{
$a = \arg\min_{a\in\tilde{\mathcal{A}}} A^{\mathcal{D}}(s,a)$\;
$\Delta_a = \min\{\Delta, \beta(a|s)\}$\;
$\mathcal{\tilde{A}} \longleftarrow \mathcal{\tilde{A}}/a$\;
$\Delta \longleftarrow \Delta - \Delta_a$\;
$\pi(a|s) \longleftarrow \pi(a|s) - \Delta_a$\;
}
}
\caption{Max-TV}
"
363,1805.07805,"[H]
\label{alg:unsafe_ppo}
\DontPrintSemicolon
\KwData{$s$, $\beta$, $A^{\beta}$, $\varepsilon$}
\KwResult{$\{\pi(a|s),\ a\in\mathcal{A}\}$}
\Begin{
$\tilde{A}=\{\tilde{A}^{+}, \tilde{A}^{-}\}$\;
$\mathcal{\tilde{A}} \longleftarrow \mathcal{A}^{+}$\;
$\Delta \longleftarrow 1 $\;
$\pi(a|s) \longleftarrow 0 \ \forall a \in \mathcal{A}$\;
\While{$\Delta > 0$ and $|\mathcal{\tilde{A}}| > 0$} {
$a = \arg\max_{a\in\tilde{\mathcal{A}}} A^{\beta}(s,a)$\;
$\Delta_a = \min\{\Delta, (1+\varepsilon)\beta(a|s)\}$\;
$\mathcal{\tilde{A}} \longleftarrow \mathcal{\tilde{A}}/a$\;
$\Delta \longleftarrow \Delta - \Delta_a$\;
$\pi(a|s) \longleftarrow \pi(a|s) + \Delta_a$\;
}
$a = \arg\max_{a\in\mathcal{A}} A^{\beta}(s,a)$\;
$\pi(a|s) \longleftarrow \pi(a|s) + \Delta$\;
}
\caption{Ad hoc PPO Maximization}
"
364,1805.07785,"[h!]
\begin{description}
\item[{\bf \color{inputcolor} Input}] (a) Pre-trained VAE $p({\bf z}) p_{\theta}({\bf t}\vert{\bf z})$ with $p_{\theta}({\bf t}\vert{\bf z})$ based on $\mathrm{Decoder}_{\theta}({\bf z})$. (Encoder ignored.) \\
\-\ \hspace{-10pt}(b) Single query ${\bf x}$ (any subset of ${\bf t}$) for which to predict ${\bf y}$. (Rest of ${\bf t}$.)
%\item Define the objective $L(\psi)=\mathbb{E}_{q(\bm{\epsilon})}\log p_{\theta}({\bf z}=\mathrm{XCoder}_{\psi}(\bm{\epsilon}),{\bf x})+\log\vert\nabla \mathrm{XCoder}_{\psi}(\bm{\epsilon})\vert.$
\item[{\bf \color{optcolor} Optimize}] Define $q({\bm \epsilon}) q_\psi({\bf z}\vert{\bm \epsilon})$ with $q_\psi({\bf z}\vert{\bm \epsilon})$ based on $\mathrm{XCoder}_{\psi}(\bm{\epsilon})$. Find $\psi$ to maximize $\text{C-ELBO}[q_{\psi}(\mathbf{Z})\Vert p_{\theta}(\mathbf{Z},\mathbf{x})]$ (Defined in Theorem \ref{thm:C-ELBO}). Estimate stochastic gradients by drawing random
$\bm{\epsilon}\sim q(\bm{\epsilon})$ and using the reparameterization trick. %(This makes $q_\psi({\bf z}\vert{\bf x})$ approximate $p_\theta({\bf z}|{\bf x})$.) 
\item[{\bf \color{predcolor} Predict}] Draw a sample $\{{\bf z}_{m}\}_{m=1}^{M} \sim q_\psi({\bf z})$ by setting ${\bf z}_{m}=\mathrm{XCoder}_{\psi}(\bm{\epsilon}_{m})$ for $\bm{\epsilon}_{m}\sim q(\bm{\epsilon})$. Predict $p_{\theta}({\bf y}\vert{\bf x})\approx\frac{1}{M}\sum_{m=1}^{M}p_{\theta}({\bf y}\vert {\bf z}_{m}).$ (Justified since the optimization phase tightened a bound (Lemma \ref{lem:joint-divergence-over-one}) on the divergence between $\int q_\psi({\bf z}) p_\theta({\bf y}\vert {\bf z}) d{\bf z}$ and $p_\theta({\bf y} \vert {\bf x})$ )
\end{description}
\caption{Conditional Inference via Cross-coding.}
\label{alg:crosscoding}
"
365,1804.07209,"[H]
        \small
          \caption{Fully Connected  Reprojection}
          \label{alg:fc_rep}
        \begin{algorithmic}
           \STATE {\bfseries Input:} $R\in\mathbb{R}^{\tilde{n}\times {n}}$, $\tilde{n}\leq n$, $\delta = 1-2\epsilon$, $\epsilon\in(0,0.5)$.\vspace{0.2cm}
                   \ \IF{$\|R^TR\|_F>\delta$}
                       \STATE
                       \STATE $\tilde{R}\leftarrow \sqrt{\delta}\frac{R}{\sqrt{\|R^TR\|_F}}$
                   \ \ELSE
                       \STATE $\tilde{R}\leftarrow{R}$
                   \ENDIF
           \STATE {\bfseries Output:} $\tilde{R}$
        \end{algorithmic}
        "
366,1804.07209,"[H]
        \small
          \caption{CNN  Reprojection}
          \label{alg:cnn_rep}
        \begin{algorithmic}
          \STATE {\bfseries Input:} \!$\delta\!\in\mathbb{R}^{N_C}\!\!,C\!\in \mathbb{R}^{n_X\times n_X\times N_C\times N_C}$,
          and $0<\epsilon<\eta<1$.
                   \FOR{each feature map $c$}
                       \STATE $\tilde{\delta}_c \leftarrow \max\bigg(\min\big(\delta_c,1-\eta\big),-1+\eta\bigg)$
                       \STATE $\tilde{C}_{i_\text{centre}}^{c}\leftarrow -1-\tilde{\delta}_c$
                       \IF{$\sum_{j\neq {i_\text{centre}}} \left|C_j^{c}\right| >1-\epsilon-|\tilde{\delta}_c|$}
                           \STATE
                           \STATE $\tilde{C}^{c}_j\leftarrow \left(1-\epsilon-|\tilde{\delta}_c|\right)\frac{C_j^{c} }{\sum_{j\neq {i_\text{centre}}} \left|{C}_j^{c}\right|}$
                       \ENDIF
                   \ENDFOR
               \STATE {\bfseries Output:} $\tilde{\delta}$, $\tilde{C}$
        \end{algorithmic}
        "
367,1805.07687,"[t] % enter the algorithm environment
\caption{Set Cover Optimal Teaching (SCOT)} % give the algorithm a caption
\label{alg:scot} % and a label for \ref{} commands later in the document
\begin{algorithmic}[1] % enter the algorithmic environment
    \REQUIRE MDP $\mathcal{M}$ with set of possible initial states $S_0$ and reward function $R^*(s) = \mathbf{w^*}^T\phi(s)$.
    \STATE  \textit{// Compute the behavioral equivalence class of $\pi^*$}
    \STATE Compute optimal policy $\pi^*$ for $\mathcal{M}$ and feature expectations $\mu_{\pi^*}^{(s,a)}$.
    \STATE Use Theorem 1 to compute BEC$(\pi^*)$.
    \STATE $U \gets \mathbf{\hat{N}}[\text{BEC}(\pi^*)]$.
    \STATE Remove redundant half-space constraints from $U$.
    \STATE  \textit{// Compute candidate demonstration trajectories}
    \STATE $\mathcal{T} = \emptyset$
    \FORALL{$s_0 \in S_0$}
    	\FOR{$i = 1,\ldots,m$}
    		\STATE Generate trajectory $\tau = (s_0, a_0, \ldots, s_{H-1}, a_{H-1})$ by starting at $s_0$ and following $\pi^*$ for $H$ steps.
    		\STATE $\mathcal{T} = \mathcal{T} \cup \tau$
    		\STATE Use Corollary~\ref{cor:feasibleDemo} to calculate BEC$(\tau | \pi^*)$
    	\ENDFOR
    \ENDFOR
    \STATE \textit{// Solve set cover using greedy approximation}
    \STATE $\mathcal{D} \gets \emptyset$, $C \gets \emptyset$
    \WHILE{$|U \setminus C| \neq 0$}
   		\STATE $\tau_{\rm greedy} = \arg\max_{\tau \in \mathcal{T}} \left| \mathbf{\hat{N}}[\text{BEC}(\tau|\pi^*)] \cap U \setminus C\right|$
   		\STATE $\mathcal{D} = \mathcal{D} \cup \tau_{\rm greedy}$
    	\STATE $C = C \cup \mathbf{\hat{N}}[\text{BEC}(\tau|\pi^*)]$

    \ENDWHILE
    \RETURN $\mathcal{D}$
\end{algorithmic}
"
368,1805.07687," % enter the algorithm environment
%\caption{Feasible Set Cover} % give the algorithm a caption
%\label{alg:feasible} % and a label for \ref{} commands later in the document
%\begin{algorithmic}[1] % enter the algorithmic environment
%    \REQUIRE Set of possible initial states $S_0$
%    \REQUIRE Feature weights $\mathbf{w}$ of the optimal reward function    
%    \STATE Initialize $\mathcal{D} \gets \emptyset$
%    \STATE Compute optimal policy $\pi^*$ based on $\mathbf{w}$
%    \STATE $H \gets H \cup w^T (\mu(s,\pi^*(s)) -  \mu(s,a)) \geq 0$
%    \STATE 
%    \REPEAT
%    		\STATE $\zeta_{\rm best} \gets \textbf{null}$
%		\FORALL{$s_0 \in S_0$}
%			\STATE Generate $K$ trajectories from $s_0$ following $\pi^*$
%	 	\ENDFOR
% 		\FORALL{$\zeta_j$, $j=1,\ldots,|S_0|\cdot K$}
%				\IF{$G(\mathcal{D}\cup \zeta_j) > G(\mathcal{D})$}
%				\STATE $\zeta_{\rm best} \gets \zeta_j$
%				\ENDIF
%		\ENDFOR
%		\STATE $\mathcal{D} \gets \mathcal{D} \cup \zeta_{\rm best}$
%    \UNTIL{$\zeta_{\rm best} \textbf{  is  } \textbf{null}$}
%    \RETURN Demonstration set $\mathcal{D}$
%\end{algorithmic}
%"
369,1805.07687,"[t] % enter the algorithm environment
\caption{infoGap$(\mathcal{D}, R)$} % give the algorithm a caption
\label{alg:infoGap} % and a label for \ref{} commands later in the document
\begin{algorithmic}[1] % enter the algorithmic environment
	\STATE Calculate $\pi^*$ under $R$.
    \STATE Calculate BEC$(\pi^*)$, BEC$(\mathcal{D} | \pi^*)$
    \STATE $\mathcal{D}^* \gets $ \textbf{SCOT}($\pi^*$)
    \STATE $m \gets$ number of trajectories in $\mathcal{D}$
    \STATE $\mathcal{D}^*_{1:m} \gets$ first $m$ trajectories in $\mathcal{D}^*$
    \STATE Calculate BEC$(\bar{\mathcal{D}}^* | \pi^*)$
    \STATE infoDemo $\gets$ \textbf{angSim}$(\mathbf{\hat{N}}[\text{BEC}(\mathcal{D}|\pi^*)],\mathbf{\hat{N}}[\text{BEC}(\pi^*)])$
    \STATE infoOpt $\gets$ \textbf{angSim}$(\mathbf{\hat{N}}[\text{BEC}(\mathcal{D}^*_{1:m}|\pi^*)], \mathbf{\hat{N}}[\text{BEC}(\pi^*)])$
    \RETURN \textbf{abs}(infoDemo - infoOpt)
\end{algorithmic}
"
370,1805.07687," % enter the algorithm environment
%\caption{Cakmak and Lopes Algorithm} % give the algorithm a caption
%\label{alg:CL} % and a label for \ref{} commands later in the document
%\begin{algorithmic}[1] % enter the algorithmic environment
%    \REQUIRE Set of possible initial states $S_0$
%    \REQUIRE Feature weights $\mathbf{w}$ of the optimal reward function    
%    \STATE Initialize $\mathcal{D} \gets \emptyset$
%    \STATE Compute optimal policy $\pi^*$ based on $\mathbf{w}$
%    \WHILE{$G(\mathcal{D}) > \epsilon$}
%    		\STATE $s_0 \gets \arg \min_{s\in S_0} \mathbf{E}_{\pi^*}(G(\{\mathcal{D} \cup \zeta_{\pi^*}(s)\}))$
%			\STATE Generate $K$ trajectories from $s_0$ following $\pi^*$
%			\FORALL{$\zeta_j$, $j=1,\ldots,K$}
%				\STATE Compute $G(\mathcal{D}\cup \zeta_j)$
%			\ENDFOR
%			\STATE $\zeta \gets \arg \min_j G(\mathcal{D} \cup \zeta_j)$
%		\IF{$G(\mathcal{D} \cup \zeta) < G(\mathcal{D})$}
%			\STATE $\mathcal{D} \gets \mathcal{D} \cup \zeta$
%		\ENDIF
%    \ENDWHILE
%    \RETURN Demonstration set $\mathcal{D}$
%\end{algorithmic}
%"
371,1805.07687," % enter the algorithm environment
 \caption{Uncertainty Volume Minimization} % give the algorithm a caption
 \label{alg:UVM} % and a label for \ref{} commands later in the document
 \begin{algorithmic}[1] % enter the algorithmic environment
     \REQUIRE Set of possible initial states $S_0$
     \REQUIRE Feature weights $\mathbf{w}$ of the optimal reward function    
     \STATE Initialize $\mathcal{D} \gets \emptyset$
     \STATE Compute optimal policy $\pi^*$ based on $\mathbf{w}$
     \REPEAT
     		\STATE $\zeta_{\rm best} \gets \textbf{null}$
 		\FORALL{$s_0 \in S_0$}
 			\STATE Generate $K$ trajectories from $s_0$ following $\pi^*$
 			\FOR{$j \in [1,K]$}
 				\IF{$G(\mathcal{D}\cup \zeta_j) < G(\mathcal{D} \cup \zeta_{\rm best}) $ and $\zeta_j \notin \mathcal{D}$}
 				\STATE $\zeta_{\rm best} \gets \zeta_j$
 				\ENDIF
 			\ENDFOR
 		\ENDFOR
 		\STATE $\mathcal{D} \gets \mathcal{D} \cup \zeta_{\rm best}$
     \UNTIL{$\zeta_{\rm best} \textbf{  is  } \textbf{null}$}
     \RETURN Demonstration set $\mathcal{D}$
 \end{algorithmic}
 "
372,1805.07683,"[t!]
	\caption{Random Walk Algorithm to Sample Node Sequences with the Gumbel-Softmax Distribution}
	\label{random_walk}
	\begin{algorithmic}[1]
		\State \textbf{Input:} $G = (V, H, A, l)$, the set of neighbors for each node $\mathcal{N}_s(\cdot)$, parameters $C \in \mathbb{R}^K$ and $\epsilon$.
		\State \textbf{Output:} the node sequences, $S(G) = (v_{\pi(1)}, v_{\pi(2)}, ..., v_{\pi(n)})$   
		\State Initialize $W \in \mathbb{R}^{n \times n}$ with $W_{ij} = \begin{cases}
		C_k \text{ if } j \in \mathcal{N}_k(i) \\
		\epsilon \text{ otherwise}
		\end{cases}$
		\State Initialize $T_j(0) = 1$ and $P_{\pi(0)j} = \frac{1}{n}$ for $j = 1 ,...,n$
		
		\For {$i = 1, ..., n$}
		\State $v_{\pi(i)} = \text{Gumbel\_Softmax}(P_{\pi(i-1)})$
		\State $T(i) = T(i-1) \odot (1-v_{\pi(i)})$
		\State $W_{\pi(i)}$ = $v_{\pi(i)} \cdot W$
		\State $P_{\pi(i)} = \text{Softmax}(W_{\pi(i-1)} \odot T(i-1)) $
		\EndFor
	\end{algorithmic}
"
373,1805.07674,"[t]\caption{Improved Bourgain Embedding}\label{alg:bourgain}
 \small
\begin{algorithmic}
\small
\STATE \textbf{Input:} A finite metric space $(Y,\dist).$
\STATE \textbf{Output:} A mapping $f:Y\rightarrow \mathbb{R}^{O(\log |Y| )}$.
\STATE //\textbf{Bourgain Embedding:}
\STATE Initialization: $m\leftarrow |Y|,$ $t\leftarrow O(\log m),$ and $\forall i\in[\lceil\log m\rceil],j\in [t],S_{i,j}\leftarrow\emptyset.$
\FOR{$i=1\rightarrow \lceil\log m\rceil$}
\FOR{$j=1\rightarrow t$}
        \STATE For each $x\in Y,$ independently choose $x$ in $S_{i,j},$ i.e. $S_{i,j}=S_{i,j}\cup \{x\}$ with probability $2^{-i}.$
    \ENDFOR
\ENDFOR
\STATE Initialize $g:Y\rightarrow \mathbb{R}^{\lceil\log m\rceil\cdot t}.$
\FOR{$x\in Y$}
    \STATE $\forall i\in[\lceil\log m\rceil],j\in[t],$ set the $((i-1)\cdot t+j)$-th coordinate of $g(x)$ as $\dist(x,S_{i,j}).$
\ENDFOR
\STATE //\textbf{Johnson-Lindenstrauss Dimentionality Reduction:}
\STATE Let $d=O(\log m)$, and let $G\in\mathbb{R}^{d\times (\lceil\log m\rceil\cdot t)}$ be a random matrix with entries drawn from i.i.d. $\mathcal{N}(0,1).$
\STATE Let $h:\mathbb{R}^{\lceil\log m\rceil\cdot t}\rightarrow \mathbb{R}^d$ satisfy $\forall x\in\mathbb{R}^{\lceil\log m\rceil\cdot t}, h(x)\leftarrow G\cdot x.$
\STATE //\textbf{Rescaling:}
\STATE Let $\beta=\min_{x,y\in Y:x\not=y}\frac{\|h(g(x))-h(g(y))\|_2}{\dist(x,y)}.$
\STATE Initialize $f:Y\rightarrow \mathbb{R}^{d}.$ For $x\in Y,$ set $f(x)\leftarrow h(g(x))/\beta.$
\STATE Return $f$.
\end{algorithmic}
"
374,1704.06447,"[t]
\caption{Algorithm for solving QDA-CMI}
\label{algo:QDACMI}
\Indm
\KwIn{The training data $\{({X}_i, y_i)\}$ where $y_i\in\{1,\cdots, K\}$}
\KwOut{$\{\boldsymbol{\Sigma}_k\}_{k=1}^K, \{\mu_k\}_{k=1}^K$ and ${\bm{\theta}}$}
\Indp
Initialize ${\bm{\theta}}^0=\trans{[1,0,0,0,0]}$, $j=0$\;
\While{not convergence}{
  \For{$k \in \{1,\cdots, K\}$}{
    $\mu_k^{j+1} = \sum_{i:y_i=k}\trans{{X}_i}{{\bm{\theta}}}^j / N_k$\; 
    $\boldsymbol{\Sigma}_k^{j+1}=\sum_{i:y_i=k}\trans{(\trans{{X}_i}{{\bm{\theta}}}^j-{\mu}_k)}(\trans{{X}_i}{{\bm{\theta}}}^j-{\mu}_k) / N_k$\;
  }
  Compute ${\bm{\theta}}^{j+1}$ using Eq. \eqref{eq:optM}\;
  $j=j+1$\;
}
$\boldsymbol{\Sigma}_k={\Sigma}_k^j, \mu_k=\mu_k^j$, for all $k\in \{1,\cdots, K\}$ and ${\bm{\theta}}={\bm{\theta}}^j$\;
"
375,1704.06447,"[h!]
\begin{enumeratenumeric}
  \item Initialize $\bm{\theta}_j = \{1,0,0,0,0\}$;
  
  \item Repeat the following steps until convergence;
  
  \item Fix ${\bm{\theta}_j}$, apply the dual approach to solve P1, which outputs the local optimal solution $\bm{\omega}_j$ and $b_j$;
  
  \item Fix $\bm{\omega}_j$, apply the gradient projection approach to solve P3, which outputs the local optimal solution $\bm{\theta}_j$;
\end{enumeratenumeric}
\caption{Algorithm for solving LSVM-CMI}
\label{algo:LSVMCMI}
"
376,1805.07575,"[ht]
    \SetAlgoHangIndent{0pt}
	\caption{c-LARS-WLasso algorithm} \label{algo:LAR}
	\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}\SetKwInOut{Init}{initialize } 
	\Input{$\y\in\C^{n}$, $\X\in\C^{n\times p}$, $\w \in\R^{\pdim}$ and $\kdim$.}
%	\BlankLine
	\Output{$\{ \setA_k, \lam_k, \bebh(\lam_k) \}_{k=0}^K$}

%	\BlankLine
		\Init{$\beb^{(0)} = \bo 0_{\pdim \times 1}$, $\setA_0 = \{ \emptyset \}$, $\Del = \bo 0_{\pdim \times 1}$, the residual $\res_{0}= \y$. Set $\X \gets \X  \mathrm{diag}(\w)^{-1}$.}

%	\BlankLine
	Compute  $ \lam_0  =\max_j  |\langle \x_j,\res_{0}\rangle|$  and  $j_1 = \arg \max_j  |\langle \x_j,\res_{0}\rangle|$, where $j = 1,\ldots,p$.
%		\BlankLine
	\For{$k=1,\ldots,K$}{
%		\BlankLine 
		Find the active set $\setA_k = \setA_{k-1} \cup \{j_k\}$ and its least- squares direction $\delb$ to have $[\Del]_{\setA_{k}} = \delb$:
		\[
		\delb = \frac{1}{\lam_{k-1}}  (\Xa{k}^\hop \Xa{k})^{-1} \Xa{k}^\hop \res_{k-1}. 
		\] 

		Define vector  $ \beb (\lam) = \beb^{(k-1)}  + (\lam_{k-1}-\lam) \Del,$ for $0 < \lam \leq \lam_{k-1}, $
		and the corresponding residual as 
		\begin{align*} 
		\res(\lam) &=  \y - \X \beb(\lam) \\  
		&= \y - \X  \beb^{(k-1)}  -  (\lam_{k-1}-\lam) \X \Del  \\ &= \res_{k-1} -  (\lam_{k-1}-\lam) \Xa{k} \delb 
		\end{align*} 
        
		The knot $\lam_k$ is the largest $\lam$-value
		$0 < \lam \leq \lam_{k-1}$ s.t.
		\beq \label{eq:target}
	     \langle \x_\ell, \res(\lam) \rangle = \lam e^{\im \theta},    \quad \ell  \not \in \setA_k 
		\eeq 
		where a new predictor (at index  $j_{k+1} \not \in \setA_k $) becomes active, thus verifying 
		$   | \langle \x_{j_{k+1}}, \res(\lam_k) \rangle | = \lam_k$ from \eqref{eq:target}.

%\smallskip
		Update the values at  a knot $ \lam_k$:
		\begin{align*} 
		% \setA &= \setA \cup \{j\} \\ 
		\beb^{(k)}  &= \beb^{(k-1)} + (\lam_{k-1} - \lam_k)\Del \\ 
		\res_{k} &= \y - \X \beb^{(k)}. 
		\end{align*}
		The Lasso solution is $\bebh(\lam_k) = \beb^{(k)} $. 
	}
	
%	\smallskip
	$\{ \bebh(\lam_k) = \bebh(\lam_k) \oslash \w \}_{k=0}^K $
	
%	\vspace*{3pt}
"
377,1805.07575,"[hbt]
	\DontPrintSemicolon
	\caption{c-PW-WEN algorithm.}\label{algo:pwen}
	\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	\BlankLine                                                   
	\Input{$\quad \y\in\C^{n}$, $\X\in\C^{n\times p}$, $\w \in\R^{\pdim}$, $[\al] \in\R^{\mdim}$ (recall $\al_1=1$), $\kdim$ and $\textsc{debias}$. 
	}
	\BlankLine
	\Output{\quad $\bebh_\kdim \in\C^{\pdim}$ and $\setA_\kdim  \in\R^{\kdim}$.}
	
\BlankLine
$ \{ \lam_k(\al_1), \bebh(\lam_k,\al_1) \}_{k=0}^K = \text{c-LARS-WLasso}\big( \y,\X, \w, \kdim \big) $

\BlankLine
   \For{$i=2$ \KwTo $\mdim$}{
      \For{$k=1$ \KwTo $K$}{

\BlankLine
 $ \tilde \eta_k  =  \lam_k(\al_{i-1}) \cdot (1-\al_i) $ 
  \BlankLine
$\big\{ \gam_k , \bebh(\lam_k,\al_i) \big\} = \text{c-LARS-WLasso}\big( \y_a, \X_a(\tilde \eta_k), \w ) \big|_k $
\BlankLine
$\lam_k(\al_{i}) = \gam_k/\al_i \quad$
}
\BlankLine
		$\setA_\kdim  = \supp\{\hat \beb(\lam_\kdim, \al_i) \}$
			
		$\bebh_{\texttt{LS}}(\lam_{\kdim},\al_i) = \X_{\setA_\kdim}^{+} \y$
		
		$\textsc{RSS}(\al_i) = \|\y - \X_{\setA_\kdim} \bebh_{\texttt{LS}}(\lam_{\kdim},\al_i) \|^2_2,$

}

			\BlankLine
	$\imath = \ \arg \min_i  \textsc{RSS}(\al_i)$
	
	$\bebh_\kdim = \bebh(\lam_\kdim,\al_\imath) \qquad$ and 
	$\qquad \setA_\kdim = \supp\{\bebh_\kdim\}$
	
  \lIf{$\textsc{debias}$}
{
		$\bebh_{\setA_\kdim} = \X_{\setA_\kdim}^{+} \y$
}

"
378,1805.07575,"[hbt]
     \SetAlgoHangIndent{0pt}
\caption{SAEN algorithm}\label{algo:saen}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output }
\BlankLine
\Input{$\quad \y\in\C^{n}$, $\X\in\C^{n\times p}$, $[\al] \in\R^{\mdim}$ and $\kdim$.} 
\BlankLine
\Output{\quad $\bebh_\kdim \in\C^{p}$}
\BlankLine 
$\big\{ \beTKi, \setA_{3 \kdim} \big\}  = \text{c-PW-WEN}\big( \y, \X, \bo 1_\pdim, [\al], 3 \kdim, 0 \big)$ 

$\big\{ \bebh, \setA_{2 \kdim} \big\}  = \text{c-PW-WEN}\big( \y, \X_{\setA_{3 \kdim}},  \bo 1_{3 \kdim} \oslash    \big| \beTKi_{\setA_{3 \kdim}} \big|  , [\al], 2 \kdim, 0 \big)$ 

$\big\{ \bebh_{\kdim}, \setA_{\kdim} \big\}  = \text{c-PW-WEN}\big( \y, \X_{\setA_{2 \kdim}},  \bo 1_{2 \kdim} \oslash \big |\bebh_{\setA_{2 \kdim}} \big|, [\al], \kdim, 1 \big)$ 

"
379,1805.07526,"[htb] 
\caption{Predictive Coding Network with local recurrent processing.} 
\label{alg:algorithm1} 
\begin{algorithmic}[1] 
\Require 
The input image $\bm{r}_0$;
\For{$l=1$ to $L$} 
    \State $\bm{r}_{l-1}^{BN}=BatchNorm(\bm{r}_{l-1})$;
    \State $\bm{r}_l(0)=ReLU\left(FFConv\left(\bm{r}_{l-1}^{BN}\right)\right)$;
    \For{$t=1$ to $T$}
        \State $\bm{p}_{l-1}(t)=FBConv\left(\bm{r}_l(t-1)\right)$;
        \State $\bm{e}_{l-1}(t)=ReLU\left(\bm{r}_{l-1}-\bm{p}_{l-1}(t)\right)$;
        \State $\bm{r}_l(t)=\bm{r}_l(t-1)+\alpha_lFFconv\left(\bm{e}_{l-1}(t)\right)$;
    \EndFor
    \State $\bm{r}_l=\bm{r}_l(T)+BPConv\left(\bm{r}_{l-1}^{BN}\right)$;
\EndFor \\
\Return $\bm{r}_L$ for classification; \\
\Comment{FFConv represents the feedforward convolution, FBConv represents the feedback convolution and BPConv represents the bypass convolution.}
\end{algorithmic} 
\renewcommand\thealgorithm{}
"
380,1805.07477,"[t]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}[1]
\REQUIRE Convolution kernel $\boldsymbol{K}$ at the current iteration
\STATE Perform the gradient descent step on the kernel $\boldsymbol{K}$. \\
\STATE Calculate $\boldsymbol{P}^{(u,v)}$ for each $u,v \in [n]\times[n]$ as $\boldsymbol{P}^{(u,v)}_{i,j} = (\mathcal{F}_n(\boldsymbol{K}_{:,:,i,j}))_{u,v}$. \\
\STATE Compute $(\boldsymbol{P}^{{(u,v)}^T}\boldsymbol{P}^{(u,v)})^{-\frac{1}{2}}$ using \eqref{eq:inv-sqrt-iters}. \\
\STATE Calculate $\hat{\boldsymbol{P}}^{(u,v)}$ using \eqref{eq:procrustes-solution}.\\
\STATE Update $\boldsymbol{K}$ using the inverse 2D Fourier transform of $\hat{\boldsymbol{P}}^{(u,v)}$. \\
\end{algorithmic}
\caption{\small Update rules for transition kernels at each iteration}
\label{alg:iteration}
"
381,1805.07457,"[t]
\SetAlgoLined
\SetNoFillComment
\For{number of training iterations} {
    \tcc{Train structure analyzer}
    
    (optional) Binarize structured predictions $S({\bf x}^{(i)})$. \\
    Update the structure analyzer $A$ and regularizer $R$ by ascending its stochastic gradient:
    \begin{multline}
    \nonumber \nabla_{\theta_{A,R}}\frac{1}{m}\sum_{i=1}^m \Big[\frac{1}{2}\norm{ A\left(S({\bf x}^{(i)}) \right) - A({\bf y}^{(i)}) }^2\\
    \nonumber + \lambda IID\left({\bf y}, R\left(A_t({\bf y})\right)\right) \Big]
    \end{multline}
    
    \tcc{Train structured predictor}
    Sample a minibatch with $m$ images $\{ {\bf x}^{(1)}, \dots, {\bf x}^{(m)} \}$ and ground truth masks $\{ {\bf y}^{(1)}, \dots, {\bf y}^{(m)} \}$. \\
    Update the structured predictor $S$ by descending its stochastic gradient:
    $$\nabla_{\theta_S}\frac{1}{2m}\sum_{i=1}^m \norm{ A\left(S({\bf x}^{(i)})\right)-A({\bf y}^{(i)}) }^2 $$
    }
The gradient-based updates can use any standard gradient-based learning rule. Structure analyzer $A$ should use a learning rate that is not larger than structured predictor $S$.
\caption{Algorithm for training structured prediction networks using ASM.}
\label{alg}
"
382,1805.06431,"[t]
% \footnotesize
\Input{$\mathcal{D}_{\text{train}}$, $K$, $\tau$, $\lambda$, $\mathbf{h}:\mathcal{X}\to\mathbb{R}^{Q}$}

Initialize $\mu_{*}, \Sigma_{*}, \Sigma_{\mathbf{Z}} \in\mathbb{R}^{Q\times D}$

$\qquad\qquad\mathbf{W}_{\mathbf{h}\to\boldsymbol{\rho}}, \mathbf{W}_{\mathbf{h}\to\boldsymbol{\pi}},\mathbf{W}_{\mathbf{h}\to\Sigma_{0}} \in\mathbb{R}^{K\times Q}$

\While{True}{
Sample $\mathbf{W}_{*}\sim\mathcal{N}(\mu_{*},\Sigma_{*})$, $\mathbf{Z}\sim\mathcal{N}(\boldsymbol{0},\Sigma_{\mathbf{Z}})$

\For{$k \in \{1,\ldots ,K\}$}{
    $\rho_{k} = \tanh(\mathbf{W}_{\mathbf{h}\to\boldsymbol{\rho}}\mathbf{h})_{k}$ $\qquad(\rho_{1}=1)$ \\
    $\pi_{k} = \text{softmax}(\mathbf{W}_{\mathbf{h}\to\boldsymbol{\pi}}\mathbf{h})_{k}$ \\
    $(\Sigma_{0})_{k}=\exp(\mathbf{W}_{\mathbf{h}\to\Sigma_{0}}\mathbf{h})_{k}$ \\ 
    $\Sigma_{k} = (1-\rho_{k}^{2})(\Sigma_{0})_{k} + \tau^{-1}$ \\
    $\widetilde{\mathbf{W}}_{k}$ = \cmtt{Cholesky}$(\mathbf{W}_{*}, \mathbf{Z},\rho_{k},\mu_{*},\Sigma_{*},\Sigma_{\mathbf{Z}})$ \\
    $\mu_{k} = \widetilde{\mathbf{W}}_{k}\mathbf{h}$
}

Compute $\mathcal{L}(\mathcal{D}_{\text{train}}|(\pi_{k},\mu_{k},\Sigma_{k})_{k=1}^{K})$ \\

Update $\mathbf{h}, \mathbf{W}_{\mathbf{h}\to\boldsymbol{\rho}}, \mathbf{W}_{\mathbf{h}\to\boldsymbol{\pi}},\mathbf{W}_{\mathbf{h}\to\Sigma_{0}},\mu_{*},\Sigma_{*}$

}
\Return{$\mathbf{W}_{*}, \mathbf{h}$}
\caption{ChoiceNet Algorithm}
\label{alg:choice-net}
"
383,1805.04715,"[t]
\caption{\textit{Triframes} frame induction}
\label{alg:triframe}
\begin{algorithmic}[1]%\footnotesize
\REQUIRE{an embedding model $v \in V \rightarrow \vec{v} \in \mathbb{R}^d$,}
\REQUIREP{a set of SVO triples $T \subseteq V^3$,}
\REQUIREP{the number of nearest neighbors $k \in \mathbb{N}$,}
\REQUIREP{a graph clustering algorithm $\textsc{Cluster}$.}
\ENSURE{a set of triframes $F$.}
\STATE{$S \gets \{t \!\rightarrow \vec{t} \in \mathbb{R}^{3d} : t \in T\}$}
\STATE{$E \gets \{(t, t') \in T^2 : t' \in \NN^S_k(\vec{t}), t \neq t'\}$}
\STATE{$F \gets \emptyset$}
\FORALL{$C \in \textsc{Cluster}(T, E)$}
\STATE{$f_s \gets \{s \in V : (s, v, o) \in C\}$}
\STATE{$f_v \gets \{v \in V : (s, v, o) \in C\}$}
\STATE{$f_o \gets \{o \in V : (s, v, o) \in C\}$}
\STATE{$F \gets F \cup \{(f_s, f_v, f_o)\}$}
\ENDFOR
\RETURN{$F$}
\end{algorithmic}
"
384,1805.07112,"[ht]
\caption{Image Captioning Via Generative Adversarial Training Method}
\label{alg::Image_Caption_GAN}
\begin{algorithmic}[1]
\REQUIRE
caption generator $G_{\bm{\theta}}$;
discriminator $D_{\bm{\phi}}$;
language evaluator $Q$, e.g. CIDEr-D;
training set $\mathbb{S}_r=\{(\bm{I},\bm{x}_{1:T})\}$ and $\mathbb{S}_w=\{(\bm{I},\hat{\bm{x}}_{1:T})\}$.
\ENSURE
optimal parameters $\bm{\theta}$, $\bm{\phi}$.
\STATE Initial $G_{\bm{\theta}}$ and $D_{\bm{\phi}}$ randomly.
\STATE Pre-train $G_{\bm{\theta}}$ on $\mathbb{S}_r$ by MLE.
\STATE Generate some fake samples based on $G_{\bm{\theta}}$ to form $\mathbb{S}_f = \{(\bm{I}, \tilde{\bm{x}}_{1:T})\}$.
\STATE Pre-train $D_{\bm{\phi}}$ on $\mathbb{S}_r\cup\mathbb{S}_f\cup\mathbb{S}_w$ by Eq.~(\ref{eq:discriminator_loss}).
\REPEAT
    \FOR{g-steps=$1:g$}
        \STATE Generate a mini-batch of image-sentence pairs $\{(\bm{I},\tilde{\bm{x}}_{1:T})\}$ by $G_{\bm{\theta}}$.
        \STATE Calculate $p$ based on Eqs.~(\ref{eq:image_sentence_feature})-(\ref{eq:CNN_prob_output}) or Eqs.~(\ref{eq:RNN_Discriminator})-(\ref{eq:RNN_prob_output}).
        \STATE Calculate $s$ based on $Q$.
        \STATE Calculate reward $r$ according to Eq.~(\ref{eq:reward_function}).
        \STATE Update generator parameters $\bm{\theta}$ by SCST method via Eq.~(\ref{eq:RL_reward_gradient_sample_SCST}).
    \ENDFOR
    \FOR{d-steps=$1:d$}
        \STATE Generate negative image-sentence pairs $\{(\bm{I},\tilde{\bm{x}}_{1:T})\}$ by $G_{\bm{\theta}}$, together with negative samples $\{(\bm{I}, \hat{\bm{x}}_{1:T})\}\subseteq\mathbb{S}_w$ and positive samples $\{(\bm{I}, \bm{x}_{1:T})\}\subseteq\mathbb{S}_r$.
        \STATE Update discriminator parameters $\bm{\phi}$ via Eq.~(\ref{eq:discriminator_loss}).
    \ENDFOR
\UNTIL generator and discriminator converge
\end{algorithmic}
"
385,1602.09130,"[t]
\scriptsize
\caption{ICLK}
\label{alg_iclk}
\begin{algorithmic}[1]
\Function {initialize}{$ \textit{corners} $}
\State
$ \textit{ssm} $.initialize($ \textit{corners} $)
\State
$ \textit{am} $.initializePixVals($ \textit{ssm} $.getPts())
\State
$ \textit{am} $.initializePixGrad($ \textit{ssm} $.getPts())
\State
$ \textit{am} $.initializeSimilarity()
\State
$ \textit{am} $.initializeGrad()
\State
$ \textit{am} $.initializeHess()
\State
$ \textit{dI0\_dps}\gets$ $ \textit{ssm} $.cmptWarpedPixJacobian($ \textit{am} $.getInitPixGrad())
%\State
%$ \textit{jacobian}\gets $ $ \textit{am} $.cmptInitJacobian($ \textit{pix\_jacobian} $)
\If{ \textit{use\_first\_order\_hessian}}
\State
$\textit{d2f\_dp2} \gets\textit{am}$.cmptSelfHessian($ \textit{dI0\_dps} $)
\Else
\State
$ \textit{am} $.initializePixHess($ \textit{ssm} $.getPts())
\State
$\textit{d2I0\_dps2}\gets \textit{ssm} $.cmptInitPixHessian(
\Statex
\-\hspace{2cm}$\textit{am} $.getInitPixHess(), $\textit{am} $.getInitPixGrad())
\State
$ \textit{d2f\_dp2} $ $\gets\textit{am}$.cmptSelfHessian($ \textit{dI0\_dps} $, $ \textit{d2I0\_dps2} $)
\EndIf
\EndFunction
\Function {update}{}
\For{$i\gets 1, \textit{max\_iters}$}
\State
\textit{am}.updatePixVals(\textit{ssm}.getPts())
\State
\textit{am}.updateSimilarity()
\State
\textit{am}.updateInitGrad()
\State
\textit{df\_dp} $ \gets $ \textit{am}.cmptInitJacobian($ \textit{dI0\_dps} $)
\State
\textit{delta\_p} $ \gets - \textit{d2f\_dp2}$.inverse()$* \textit{df\_dp}$
\State
\textit{delta\_ps} $ \gets $ \textit{delta\_p}.head(\textit{ssm}.getStateSize())
\State
\textit{delta\_pa} $ \gets $ \textit{delta\_p}.tail(\textit{am}.getStateSize())
\State
\textit{inv\_delta\_ps} $ \gets $\textit{ssm}.invertState(\textit{delta\_ps})
\State
\textit{inv\_delta\_pa} $ \gets $\textit{am}.invertState(\textit{delta\_pa})
\State
\textit{prev\_corners}$ \gets $\textit{ssm}.getCorners()
\State
\textit{ssm}.compositionalUpdate(\textit{inv\_delta\_ps})
\State
\textit{am}.update(\textit{inv\_delta\_pa})
\If{ $ \| \textit{prev\_corners} - \textit{ssm}.\text{getCorners}()\|^2 < \epsilon $}
\State
\textbf{break}
\EndIf
\EndFor
\State
\textit{am}.updateModel(\textit{ssm}.getPts())
\State
\textbf{return} \textit{ssm}.getCorners()
\EndFunction
\end{algorithmic}
"
386,1602.09130,"[!htbp]
\scriptsize
\caption{FCLK}
\label{alg_fclk}
\begin{algorithmic}[1]
\Function {initialize}{$ \textit{corners} $}
\State
lines 2-7 of Alg. \ref{alg_iclk}
\State
$ \textit{am} $.initializePixHess($ \textit{ssm} $.getPts())
\EndFunction
\Function {update}{}
\State
lines 19-20 of Alg. \ref{alg_iclk}
\State
\textit{am}.updateCurrGrad()
\State
\textit{am}.updatePixGrad(\textit{ssm}.getPts())
\State
\textit{am}.updatePixHess($ \textit{ssm} $.getPts())
\State
\textit{dIt\_dps} $\gets$ $ \textit{ssm} $.cmptWarpedPixJacobian($ \textit{am} $.getCurrPixGrad())
\State
\textit{d2It\_dps2} $\gets$ $ \textit{ssm} $.cmptWarpedPixHessian(
\Statex
\-\hspace{2cm}$\textit{am} $.getCurrPixHess(), $\textit{am} $.getCurrPixGrad())
\State
\textit{df\_dp} $ \gets $\textit{am}.cmptCurrJacobian(\textit{dIt\_dps})
\State
\textit{d2f\_dp2} $\gets$\textit{am}.cmptSelfHessian(\textit{dIt\_dps}, \textit{d2It\_dps2})
\State
\textit{delta\_p} $ \gets - \textit{d2f\_dp2}$.inverse()$* \textit{df\_dp}$
\State
\textit{ssm}.compositionalUpdate(\textit{delta\_p})
\State
\textbf{return} \textit{ssm}.getCorners()
\EndFunction
\end{algorithmic}
"
387,1602.09130,"[t]
\scriptsize
\caption{ESM}
\label{alg_esm}
\begin{algorithmic}[1]
\Function {initialize}{$ \textit{corners} $}
\State
lines 2-3 of Alg. \ref{alg_fclk}
\State
$ \textit{dI0\_dps} \gets$ $ \textit{ssm} $.cmptWarpedPixJacobian($ \textit{am} $.getInitPixGrad())
\State
\textit{d2f\_dp2\_0} $\gets$ \textit{am}.cmptSelfHessian(\textit{dI0\_dps}, \textit{d2I0\_dps2})
\EndFunction
\Function {update}{}
\State
lines 6-11 of Alg. \ref{alg_fclk}
\State
\textit{am}.updateInitGrad()
%\State
%$ \textit{curr\_pix\_jacobian} \gets$ $ \textit{ssm} $.cmptWarpedPixJacobian($ \textit{am} $.getCurrPixGrad())
\State
\textit{df\_dp}$ \gets $\textit{am}.cmptDifferenceOfJacobians(\textit{dI0\_dps}, \textit{dIt\_dps})
%\Statex
%\-\hspace{2cm}\textit{pix\_jacobian})
%\State
%$ \textit{pix\_hessian} \gets$ $ \textit{ssm} $.cmptWarpedPixHessian(
%\Statex
%\-\hspace{2cm}$\textit{am} $.getCurrPixHess(), $\textit{am} $.getCurrPixGrad())
\State
\textit{d2f\_dp2\_t} $\gets$\textit{am}.cmptSelfHessian(\textit{dIt\_dps}, \textit{d2It\_dps2})
\State
\textit{d2f\_dp2} $\gets$$  \textit{d2f\_dp2\_0} + \textit{d2f\_dp2\_t} $
\State
lines 14-16 of Alg. \ref{alg_fclk}
\EndFunction
\end{algorithmic}
"
388,1602.09130,"[!htbp]
\scriptsize
\caption{IALK}
\label{alg_ialk}
\begin{algorithmic}[1]
\Function {initialize}{$ \textit{corners} $}
\State
same as Alg. \ref{alg_fclk}
\EndFunction
\Function {update}{}
\State
lines 6-7 of Alg. \ref{alg_fclk}
\State
$ \textit{dIt\_dps} \gets$ $ \textit{ssm} $.cmptApproxPixJacobian($ \textit{am} $.getInitPixGrad())
\State
$ \textit{d2It\_dps2} \gets$ $ \textit{ssm} $.cmptApproxPixHessian($\textit{am} $.getInitPixHess(), 
\Statex
\-\hspace{2cm}$\textit{am} $.getInitPixGrad())
\State
lines 12-14 of Alg. \ref{alg_fclk}
\State
\textit{ssm}.additiveUpdate(\textit{delta\_p})
\State
\textbf{return} \textit{ssm}.getCorners()
\EndFunction
\end{algorithmic}
"
389,1602.09130,"[!htbp]
\scriptsize
\caption{FALK}
\label{alg_falk}
\begin{algorithmic}[1]
\Function {initialize}{$ \textit{corners} $}
\State
same as Alg. \ref{alg_fclk}
\EndFunction
\Function {update}{}
\State
lines 6-9 of Alg. \ref{alg_fclk}
\State
$ \textit{dIt\_dps} \gets$ $ \textit{ssm} $.cmptPixJacobian($ \textit{am} $.getCurrPixGrad())
\State
$ \textit{d2It\_dps2} \gets$ $ \textit{ssm} $.cmptPixHessian($\textit{am} $.getCurrPixHess(), 
\Statex
\-\hspace{2cm}$\textit{am} $.getCurrPixGrad())
\State
lines 8-10 of Alg. \ref{alg_ialk}
\EndFunction
\end{algorithmic}
"
390,1602.09130,"[t]
\scriptsize
\caption{NN}
\label{alg_nn}
\begin{algorithmic}[1]
\Function {initialize}{$ \textit{corners} $}
\State
lines 2-3 of Alg. \ref{alg_iclk}
\State
\textit{state\_sigma}$ \gets $ \textit{ssm}.estimateStateSigma()
\State
\textit{ssm}.initializeSampler(\textit{state\_sigma})
\State
\textit{am}.initializeDistFeat()
\For{\textit{sample\_id} $ \gets 1, $ \textit{no\_of\_samples}}
\State
\textit{ssm\_updates}.row(\textit{sample\_id}) $ \gets $ \textit{ssm}.generatePerturbation()
\State
\textit{inv\_update} $\gets $\textit{ssm}.invertState(\textit{ssm\_updates}.row(\textit{sample\_id}))
\State
\textit{ssm}.compositionalUpdate(\textit{inv\_update})
\State
\textit{am}.updatePixVals(\textit{ss}m.getPts())
\State
\textit{am}.updateDistFeat()
\State
\textit{sample\_dataset}.row(\textit{sample\_id}) $\gets$ \textit{am}.getDistFeat()
%\State
%\textit{ssm\_perturbations}.row(\textit{sample\_id}) $\gets$ \textit{ssm\_update}
\State
\textit{ssm}.compositionalUpdate(\textit{ssm\_updates}.row(\textit{sample\_id}))
\EndFor
\State
\textit{flann}.buildIndex(\textit{sample\_dataset})
\EndFunction

\Function {update}{}
\State
\textit{am}.updatePixVals(\textit{ssm}.getPts())
\State
\textit{am}.updateDistFeat()
\State
\textit{nn\_sample\_id} $ \gets $ \textit{flann}.searchIndex(\textit{am}.getDistFeat())
\State
\textit{ssm}.compositionalUpdate(\textit{ssm\_updates}.row(\textit{nn\_sample\_id}))
\State
\textbf{return} \textit{ssm}.getCorners()
\EndFunction
\end{algorithmic}
"
391,1602.09130,"[!htbp]
\scriptsize
\caption{PF}
\label{alg_pf}
\begin{algorithmic}[1]
\Function {initialize}{$ \textit{corners} $}
\State
lines 2-4 of Alg. \ref{alg_nn}
\State
\textit{am}.initializeSimilarity()
\For{\textit{particle\_id} $ \gets 1, $ \textit{no\_of\_particles}}
\State
\textit{particles}[\textit{particle\_id}].\textit{state} $ \gets $ \textit{ssm}.getState()
\State
\textit{particles}[\textit{particle\_id}].\textit{weight} $ \gets $ $ 1/\textit{no\_of\_particles} $
\EndFor
\EndFunction

\Function {update}{}
\For{\textit{particle\_id} $ \gets 1, $ \textit{no\_of\_particles}}
\State
\textit{particles}[\textit{particle\_id}].\textit{state} $\gets$ \textit{ssm}.compositionalRandomWalk(
\State
\-\hspace{2cm}\textit{particles}[\textit{particle\_id}].\textit{state})
%\Comment
%can be AutoRegression1 and additive too
\State
\textit{ssm}.setState(\textit{particles}[\textit{particle\_id}].\textit{state})
\State
\textit{am}.updatePixVals(\textit{ssm}.getPts())
\State
\textit{am}.updateSimilarity()
\State
\textit{particles}[\textit{particle\_id}].\textit{weight} $\gets $\textit{am}.getLikelihood()
\EndFor
\State
normalize weights and resample the particles
\State
\textit{mean\_state} $ \gets $ \textit{ssm}.estimateMeanOfSamples(\textit{particles});
\State
\textit{ssm}.setState(\textit{mean\_state})
\State
\textbf{return} \textit{ssm}.getCorners()
\EndFunction
\end{algorithmic}
"
392,1602.09130,"[!htbp]
\scriptsize
\caption{LMS/RANSAC}
\label{alg_lms_ransac}
\begin{algorithmic}[1]
\Function {initialize}{$ \textit{corners} $}
\State
\textit{sub\_trackers} $ \gets $ vector of 2 DOF sub patch trackers
\State
\textit{ssm}.initialize($ \textit{corners} $)
\State
\textit{curr\_pts} $ \gets $ \textit{ssm}.getPts()
\For{\textit{pt\_id} $ \gets 1, $ \textit{no\_of\_pts}}
\State
\textit{sub\_patch\_corners} $ \gets $ getRegion(\textit{curr\_pts}[\textit{pt\_id}], \textit{sub\_patch\_size})
\State
\textit{sub\_trackers}[\textit{pt\_id}].initialize(\textit{sub\_patch\_corners})
\EndFor
\EndFunction

\Function {update}{}
\State
\textit{prev\_pts} $ \gets $ \textit{curr\_pts}
\For{\textit{pt\_id} $ \gets 1, $ \textit{no\_of\_pts}}
\State
\textit{sub\_trackers}[\textit{pt\_id}].update()
\State
\textit{curr\_pts}[\textit{pt\_id}] $ \gets $ getCentroid(\textit{sub\_trackers}[\textit{pt\_id}].getRegion())
\EndFor
\State
\textit{opt\_warp} $ \gets $ \textit{ssm}.estimateWarpFromPts(\textit{prev\_pts}, \textit{curr\_pts})
\State
\textit{warped\_corners} $ \gets $ \textit{ssm}.applyWarpToCorners(\textit{ssm}.getCorners(), \textit{opt\_warp})
\State
\textit{ssm}.setCorners(\textit{warped\_corners})
\State
%\textit{curr\_pts} $ \gets $ \textit{ssm}.getPts()
%\For{\textit{pt\_id} $ \gets 1, $ \textit{no\_of\_pts}}
%\State
%\textit{sub\_patch\_corners} $ \gets $ getRegion(\textit{curr\_pts}[\textit{pt\_id}], \textit{sub\_patch\_size})
%\State
%\textit{sub\_trackers}[\textit{pt\_id}].initialize(\textit{sub\_patch\_corners})
%\EndFor
lines 4-8 of Alg. \ref{alg_lms_ransac}
\State
\textbf{return} \textit{ssm}.getCorners()
\EndFunction
\end{algorithmic}
"
393,1602.09130,"[!htbp]
\caption{Object Tracking - Simple}
\label{use_case_tracking}
\scriptsize
\begin{algorithmic}[1]
\State using namespace mtf;
\State \texttt{ICLK}$ < $\texttt{SSD}, \texttt{Homography}$ > $ \textit{tracker};
\State \texttt{GaussianSmoothing} \textit{pre\_proc}(\textit{input}.getFrame(), \textit{tracker}.inputType());
\State \textit{tracker}.initialize(\textit{pre\_proc}.getFrame(), \textit{init\_location});
\While{\textit{input}.update()}
\State \textit{pre\_proc}.update(\textit{input}.getFrame());
\State \textit{tracker}.update(\textit{pre\_proc}.getFrame());
\State \textit{new\_location} $ \gets $ \textit{tracker}.getRegion();
\EndWhile
\end{algorithmic}
"
394,1602.09130,"[!htbp]
%\begin{algorithm}[t]
\caption{Object Tracking - Composite}
\label{use_case_tracking_composite}
\scriptsize
\begin{algorithmic}[1]
\State \texttt{PF}$ < $\texttt{ZNCC}, \texttt{Affine}$ > $ \textit{tracker1};
\State \texttt{FCLK}$ < $\texttt{SSIM}, \texttt{SL3}$ > $ \textit{tracker2};
\State vector$ < $\texttt{TrackerBase*}$ > $ \textit{trackers} = \{\&\textit{tracker1}, \&\textit{tracker2}\};
\State \texttt{\texttt{CascadeTracker}} \textit{tracker}(\textit{trackers});
\State lines 3-9 of Alg. \ref{use_case_tracking}
\end{algorithmic}
"
395,1602.09130,"[t]
\caption{Object Tracking - Composite}
\label{use_case_tracking_composite}
\scriptsize
\begin{algorithmic}[1]
\State \texttt{PF}$ < $\texttt{ZNCC}, \texttt{Affine}$ > $ \textit{tracker1};
\State \texttt{FCLK}$ < $\texttt{SSIM}, \texttt{SL3}$ > $ \textit{tracker2};
\State vector$ < $\texttt{TrackerBase*}$ > $ \textit{trackers} = \{\&\textit{tracker1}, \&\textit{tracker2}\};
\State \texttt{\texttt{CascadeTracker}} \textit{tracker}(\textit{trackers});
\State lines 3-9 of Alg. \ref{use_case_tracking}
\end{algorithmic}
"
396,1602.09130,"[!htbp]
\caption{UAV Trajectory Estimation in Satellite Image}
\label{use_case_uav}
\scriptsize
\begin{algorithmic}[1]
\State \texttt{ESM}$ < $\texttt{MI}, \texttt{Similitude}$ > $ \textit{tracker};
\State \textit{uav\_img\_corners}$ \gets $getFrameCorners(\textit{input}.getFrame());
%\State \texttt{GaussianSmoothing} \textit{satellite\_pre\_proc}(\textit{satellite\_img},  \textit{tracker}.inputType());
%\State \texttt{GaussianSmoothing} \textit{uav\_pre\_proc}(\textit{input}.getFrame(),  \textit{tracker}.inputType());
\State \textit{tracker}.initialize(\textit{satellite\_img}, \textit{init\_uav\_location});
\State \textit{curr\_uav\_location}$ \gets $\textit{tracker}.getRegion();
\While{\textit{input}.update()}
%\State \textit{uav\_pre\_proc}.update(\textit{input}.getFrame());
\State \textit{tracker}.initialize(\textit{input}.getFrame(), \textit{uav\_img\_corners});
\State \textit{tracker}.setRegion(\textit{curr\_uav\_location});
\State \textit{tracker}.update(\textit{satellite\_img});
\State \textit{curr\_uav\_location}$ \gets $\textit{tracker}.getRegion();
\EndWhile
\end{algorithmic}
"
397,1602.09130,"[!htbp]
\caption{Online Image Mosaicing}
\label{use_case_mosaic}
\scriptsize
\begin{algorithmic}[1]
%\State \textit{resx}, \textit{resy} $ \gets $ \textit{input}.getFrameSize();
%\State \texttt{NCCParams} \textit{ncc\_params}(\textit{resx}, \textit{resy});
%\State \texttt{AffineParams} \textit{aff\_params}(\textit{resx}, \textit{resy});
%\State \texttt{FCLK}$ < $\texttt{MCNCC}, \texttt{Affine}$ > $ \textit{tracker}(\textit{ncc\_params}, \textit{aff\_params});
\State \texttt{FALK}$ < $\texttt{MCNCC}, \texttt{Isometry}$ > $ \textit{tracker};
%\State \textit{input\_corners} $ \gets $ getFrameCorners(\textit{input}.getFrame());
%\State \textit{input\_pts} $ \gets $ getPtsFromCorners(\textit{input\_corners}, \textit{input}.getFrameSize());
%\State \textit{mos\_center} $ \gets $ getFrameCenter(\textit{mosaic\_img}, \textit{input}.getFrameSize());
%\State \texttt{NoProcessing} \textit{pre\_proc}(\textit{input}.getFrame(), \textit{tracker}.inputType());
%\State \textit{tracker}.setImage(\textit{pre\_proc}.getFrame());
\State \textit{mos\_img} $ \gets $ writePixelsToImage(\textit{input}.getFrame(), \textit{init\_mos\_location}, \textit{mos\_size});
\State \textit{mos\_location} $ \gets $  \textit{init\_mos\_location};
%\State \textit{tracker}.initialize(\textit{input\_corners});
\While{\textit{input}.update()}
\State \textit{temp\_img} $ \gets $ writePixelsToImage(\textit{input}.getFrame(), \textit{mos\_location}, \textit{mos\_size});
\State \textit{tracker}.initialize(\textit{temp\_img}, \textit{mos\_location});
\State \textit{tracker}.update(\textit{mos\_img});
\State \textit{mos\_location} $ \gets $ \textit{tracker}.getRegion();
\State \textit{mos\_img} $ \gets $ writePixelsToImage(\textit{input}.getFrame(), \textit{mos\_location}, \textit{mos\_size});
\EndWhile
\end{algorithmic}
"
398,1804.10911,"[t]
\caption{TreeSearch}\label{alg:TreeSearch}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}[1]
\REQUIRE root $s_R$, value $V$, policy $\mathbf{p}$, search times $K$
\ENSURE Search policy $\bm{\pi}$
\FOR{$k = 0$ \TO $K-1$}
\STATE $s_L\leftarrow s_R$
\STATE \COMMENT{Selection}
\WHILE{$s_L \textrm{ is not a leaf node}$}
	\STATE $a \leftarrow \arg\max_{a\in\mathcal{A}(s_L)} Q(s_L, a) +\lambda \cdot U(s_L, a)$\COMMENT{Eq.~(\ref{eq:Selection})}
	\STATE $s_L \leftarrow \textrm{ child node pointed by edge }(s_L, a)$ 
\ENDWHILE

\STATE \COMMENT{Evaluation and expansion}
%\IF{$s_L$ can be expanded}
    \STATE $v\leftarrow V(s_L)$ \COMMENT{simulate $v$ with value function $V$}
	\FORALL{$a \in\mathcal{A}(s_L)$} 
		\STATE Expand an edge $e$ to node $ s = [s_L.\mathbf{X}_t, s_L.\mathbf{Y}_t\oplus \{a\}]$
		\STATE $e.P \leftarrow p(a|s_L); e.Q \leftarrow 0; e.N \leftarrow  0$\COMMENT{init edge properties}
	\ENDFOR
%\ELSE
%	\STATE{$v\leftarrow\left\{\begin{array}{cl} V(s_L) & R= NULL\textrm{ or }J=\emptyset \\ R(s_L.\mathcal{Z}, J) & \textrm{others}\\ \end{array} \right.$}\COMMENT{simulate with $V$ at test phase ($R=NULL$ or $J=\emptyset$), or directly set to its real value (e.g., $\alpha$-NDCG) at the training phase}
%\ENDIF
\STATE \COMMENT{Back-propagation}
\WHILE{$s_L \neq s_R$} 
	\STATE $s \leftarrow \textrm{ parent of }s_L; e \leftarrow \textrm{ edge from }s\textrm{ to }s_L$
	\STATE $e.Q \leftarrow \frac{e.Q\times e.N + v}{e.N + 1}$\COMMENT{Eq.~(\ref{eq:UpdateQN})}
	\STATE $e.N \leftarrow e.N + 1; s_L \leftarrow s$
\ENDWHILE
\ENDFOR

\STATE \COMMENT{Calculate tree search policy. Eq.~(\ref{eq:SearchProb})}
\FORALL{$a\in\mathcal{A}(s_R)$}
	\STATE $\pi(a|s_R)\leftarrow \frac{e(s_R, a).N}{\sum_{a'\in\mathcal{A}(s_R)}e(s_R, a').N}$
\ENDFOR
\RETURN $\bm{\pi}$
\end{algorithmic}
"
399,1804.10911,"[t]
\caption{Train MM-Tag model}\label{alg:Train}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}[1]
\REQUIRE Labeled data $D=\{ (\mathbf{X}^{(n)}, \mathbf{Y}^{(n)})\}_{n=1}^N$, learning rate $\eta$, number of search $K$
\ENSURE $\mathbf{\Theta}$
\STATE \text{Initialize} $\mathbf{\Theta} \leftarrow$ random values in $[-1, 1]$
\REPEAT
	\FORALL{$(\mathbf{X}, \mathbf{Y})\in D$}
		\STATE {$s \leftarrow [\mathbf{x}_1, \emptyset]; M \leftarrow |X|; E\leftarrow \emptyset$ }
		\FOR{$t=1$ \TO $M$}
            \STATE $\bm{\pi} \leftarrow \mathrm{TreeSearch}(s, V, \mathbf{p}, K)$ \COMMENT{Alg.~(\ref{alg:TreeSearch})}
			\STATE $a =\arg\max_{a\in\mathcal{A}(s)} \pi(a|s)$ \COMMENT{select the best tag}
			\STATE $E\leftarrow E \oplus \{(s, \bm{\pi})\}$
			%\STATE $[\mathbf{q}, \mathcal{Z}, X]\leftarrow s$
			\STATE $s \leftarrow [s.\mathbf{X}_t \oplus \{\mathbf{x}_{t+1}\}, s.\mathbf{Y}_t \oplus \{a\}]$
		\ENDFOR
		\STATE $r \leftarrow \textrm{Acccuracy}(\mathbf{Y}, s.\mathbf{Y}_M)$\COMMENT{overall accuracy}
		\STATE $\mathbf\Theta \leftarrow \mathbf\Theta -\eta \frac{\partial \ell(E, r)}{\partial \mathbf\Theta}$ \COMMENT{$\ell$ is defined in Eq.~(\ref{eq:loss})}
	\ENDFOR
\UNTIL {converge}
\RETURN $\mathbf{\Theta}$
\end{algorithmic}
"
400,1804.10911,"[t]
\caption{MM-Tag Inference}\label{alg:RLRank_MCTS}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithmic}[1]
\REQUIRE sentence $\mathbf{X}=\{\mathbf{x}_1, \cdots, \mathbf{x}_M\}$, value function $V$, policy function $\mathbf{p}$, and search times $K$,
\ENSURE label sequence $\mathbf{Y}$
\STATE $s \leftarrow [\{\mathbf{x}_1\}, \emptyset]; M \leftarrow |\mathbf{X}|$
\FOR{$t=1$ \TO $M$}
	\STATE $\bm{\pi} \leftarrow \mathrm{TreeSearch}(s, V, \mathbf{p}, K)$
	\STATE $a \leftarrow \arg\max_{a\in\mathcal{A}(s)} \pi(a|s)$ 
%	\STATE $[\mathbf{X}, \mathbf{Y}]\leftarrow s$
	\STATE $s \leftarrow [s.\mathbf{X} \oplus \{\mathbf{x}_{t+1}\}, s.\mathbf{Y}\oplus \{a\}]$
\ENDFOR
\RETURN $s.\mathbf{Y}$
\end{algorithmic}
"
401,1805.07010,"[tb]
   \caption{Sinkhorn Policy Gradient}
   \label{alg:SPG}
\begin{algorithmic}
   \STATE Initialize actor $\pi_\theta(s; \tau)$ and critic $Q_{\theta'}(s,a)$\\
   \STATE Initialize replay buffer $R$
   \FOR{$i=1$ {\bfseries to} $max\_train\_steps$}
   \STATE Sample state $s \sim \rho$
   \STATE $\mathbf{M} = \pi_\theta(s; \tau)$
   \STATE $\mathbf{P} = \text{H}(\mathbf{M})$
   \STATE Sample $u \sim \text{Uniform}[0,1)$
   \IF{$u < \epsilon$}
     \STATE Make $k=2$ random row exchanges for $\mathbf{P}$ and $\mathbf{M}$ 
   \ENDIF
   \STATE Apply $\mathbf{P}$ to $s$ and observe $r(s,\mathbf{P})$
   \STATE Store experience ($s$, $\mathbf{M}$, $\mathbf{P}$, $r$) in $R$
   \STATE Sample mini-batch ($s_n, \mathbf{M}_n,\mathbf{P}_n, r_n)$ $\sim R$
   \STATE Update critic by minimizing: 
   \begin{align*}
   	\text{MSE}\big(r_n, Q_{\theta'}(s_n, \mathbf{P}_n)\big)
    + \text{MSE}\big( \texttt{stop\_grad}(Q_{\theta'}(s_n, \mathbf{P}_n)), Q_{\theta'}(s_n, \mathbf{M}_n)\big)
   \end{align*}
   \STATE $\mathbf{M_n'} = \pi_\theta(s_n; \tau)$
   \STATE Update the actor policy by ascending the sampled policy gradient:
   \begin{align*}
   \nabla_\theta & \pi_\theta \approx \frac{1}{N} \sum_n \nabla_\theta \pi_\theta (s_n; \tau) \nabla_{a} Q_{\theta'}(s_n, a) \big \vert_{a=\mathbf{M_n'}}
   \end{align*}
   \ENDFOR
\end{algorithmic}
"
402,1805.05631,"
\caption{\textbf{LAPSmax Bandit}\label{algo2} Multi-Armed Bandit algorithm used as a topic choice strategy maximizing the LAPS measure. New arms are created with weights $w_a$ equal to the reward $r_i$ obtained at the end of an interaction with a new meaning.}
\begin{algorithmic}
\REQUIRE $\gamma$ rate of exploration for bandit
\REQUIRE $n$ time scale for weights decay
\REQUIRE vocabularies $V$ and $\widetilde{V}$, \#meanings $M$
%\newline
\STATE $K \leftarrow \left|V.known\_meanings()\right|$
\IF{$LAPS_A = \frac{K}{M}$}
\STATE $m \leftarrow random\left(V.unknown\_meanings()\right)$
\ELSE
\FOR{$a \in Arms$}
\STATE $\tilde{w}_a = \frac{w_a}{\sum_jw_j}$
\STATE $p_a = (1-\gamma)\cdot\tilde{w}_a + \frac{\gamma}{K}$
\ENDFOR
\STATE Sample $m \in Arms$ using distribution $(p_a)_{a \in Arms}$
\ENDIF
\RETURN $m$
%\newline
\STATE\{Interact using topic $m$ and compute reward $r$\}
%\newline
\IF{$m \in Arms$}
\STATE $w_m \leftarrow \frac{n}{n+1}\cdot w_m + r$
\ELSE
\STATE Add $m$ to $Arms$ with $w_m = r$
\ENDIF
\end{algorithmic}
"
403,1709.07359,"
	\caption{Splitting GAN}
	\label{alg:splitting}
	\begin{algorithmic}
	\REQUIRE A dataset with initial labels (the same label for all the examples in the unsupervised case).
	\REQUIRE $clustering\_iterations$, list of iterations where to make a clustering step.
	\REQUIRE $kmeans\_threshold$, do not divide classes with less samples than this.
    \WHILE{parameters have not converged}
    	\STATE Make a WGAN-GP with auxiliary classifier step, as in \cite{Gulrajani2017}.
    	\IF{current iteration is in $clustering\_iterations$}
      	\FORALL{$class$ in current classes with more than $kmeans\_threshold$ samples}
          	\STATE Propagate through the critic all samples of $class$ up to the last hidden layer.
          	\STATE Normalize these representations.
          	\STATE Apply \emph{K-Means} to the representations in order to obtain two new child classes.
          	\STATE Replace the label $class$ in the dataset with the new child classes.
      	\ENDFOR
     	 
      	\FOR {each new child class, $child$, with parent class $parent$}
          	\STATE In the last layer of the auxiliary classifier, copy its parent parameters.
          	\FOR {each Batch Normalization layer, $i$, of the Generator}
             	\STATE $\gamma_{i,child} = \gamma_{i,parent} + \Delta\gamma_{i,child}$, with initialization $\Delta\gamma_{i,child} = 0$.
             	\STATE $b_{i,child} = b_{i,parent} + \Delta b_{i,child}$, with initialization $\Delta b_{i,child} = 0$.
          	\ENDFOR
      	\ENDFOR
    	\ENDIF
    \ENDWHILE
	\end{algorithmic}
"
404,1805.06368,"[!t]

\caption{The SVFDT algorithm.}\label{alg:svfdt}

\begin{algorithmic}[1]
\scriptsize
\Require
\Statex $S$: the stream of instances
\Statex $GP$: the grace period
\Statex $\delta$: the error probability 
\Statex $\tau$: the tiebreak value
\Ensure
\Statex \textit{SVFDT}: a trained Strict Very Fast Decision Tree
\Statex

\Procedure{SVFDT}{$S, GP, \delta, \tau$}
\State Let SVFDT $\gets l_{root}$ \Comment{The root}
\State Initiate $H_{statistics}$, $IG_{statistics}$ and $n_{statistics}$ for $\varphi$ and $\varpi$ equations

\State Let $LH$ be the hash of leaves 

\State Let $n_{l_{root}} \gets 0$ \Comment{Number of elements seen at $l\textsubscript{root}$}
\State Let $LC_{l_{root}} \gets 0$ \Comment{Number of elements on last split check at $l_{root}$}
\State Let $F_{l_{root}} \gets \emptyset $ \Comment{Set of features removed from comparison}

\For {($(X, y)$ in $S$)} \Comment{X is the feature vector of an instance of class y in $S$}
\State Sort $(X, y)$ to its leaf $l$
\State Let $\hat{y} \gets$ prediction of $l$

\State Let $n_{l} \gets n_{l} + 1 $
\State Update feature estimators and class distribution at $l$ according to $(X, y)$
\If{(class distribution at $l$ is impure $\land$ $n_{l} - LC_{l} > GP$)}
\State Compute $HB$ and $IG(.)$ of features in $l\not\in F_{l}$
\State Let $rank \gets$ Sorted $IG(.)$ computed

\If{(\textit{\textbf{CanSplit}}($rank, HB, \tau, l, LH, H_{statistics}, IG_{statistics}, n_{statistics}$))}
\State Remove leaf $l$ from $LH$
\State Replace leaf $l$ with a split node
\For{each branch of the split}
\State Let $l_{new} \gets$ new leaf
\State Initiate all the feature estimators on $l_{new}$
\State Let class distribution on $l_{new}$ $ \gets $ post-split distribution of $l_{new}$
\State Let $n_{l_{new}} \gets $ sum of class distribution on $l_{new}$
\State Let $LC_{l_{new}} \gets $ $n_{l_{new}}$
\State Let $F_{l_{new}} \gets \emptyset $
\State Add leaf $l_{new}$ to $LH$
\EndFor
\Else
\State Let $LC_{l} \gets n_{l}$
\State \textit{\textbf{FeatureSelection}}($rank, HB, F_{l}$)
\EndIf

\EndIf
\EndFor
\State \Return SVFDT
\EndProcedure
\end{algorithmic}
"
405,1805.06368,"[!t]
\caption{The split check algorithm.}\label{alg:cansplit}

\begin{algorithmic}[1]
\scriptsize
\Require 
\Statex $rank$: sorted list of $IG(.)$ per feature
\Statex $HB$: the Hoeffding Bound value
\Statex $\tau$: tiebreak value
\Statex $l$: the current leaf node
\Statex $LH$: the hash of leaves
\Statex $H_{statistics}$: statistics about entropy values
\Statex $IG_{statistics}$: statistics about IG(.) values
\Statex $n_{statistics}$: statistics about the number of elements seen values
\Ensure 
\Statex Boolean value
\Statex


\Procedure{CanSplit}{$rank, HB, \tau, l, LH, H_{statistics}, IG_{statistics}, n_{statistics}$}

\State Let $IG_{best}$ and $IG_{second\_best}$ $\gets$ the highest and second highest $IG(.)$

\If{$(IG_{best} - IG_{second\_best} > HB \lor HB < \tau)$}

\State Compute $\overline{H}_{LH}$ and $\sigma(H_{LH})$ using $LH$

\State Compute $\overline{H}$ and $\sigma(H)$ using $H_{statistics}$

\State Compute $\overline{IG}$ and $\sigma(IG)$ using $IG_{statistics}$

\State Compute $\overline{n}$ and $\sigma(n)$ using $n_{statistics}$

\State Let $H_{l}$ and $n_{l} \gets$ entropy and number of elements seen at $l$

\State Update $H_{statistics}$, $IG_{statistics}$ and $n_{statistics}$ with $H_{l}$, $IG_{best}$ and $n_{l}$, respectively
\State Let $svfdt\_ii\_constraints \gets H_{l} \geq \overline{H} + \sigma(H) \land IG_{best} \geq \overline{IG} + \sigma(IG)$

\If{($svfdt\_ii\_constraints$)} \Comment{SVFDT-II version only}
\State \Return \textbf{True}
\EndIf

\State Let $\varrho \gets H_{l} \geq \overline{H}_{LH} - \sigma(H_{LH})$ \Comment{Constraint 1}

\State Let $\xi \gets H_{l} \geq \overline{H} - \sigma(H)$ \Comment{Constraint 2}

\State Let $\kappa \gets IG_{best} \geq \overline{IG} - \sigma(IG)$ \Comment{Constraint 3}

\State Let $\psi \gets n_{l} \geq \overline{n} - \sigma(n)$ \Comment{Constraint 4}

\State Let $svfdt\_constraints \gets \varrho \land \xi \land \kappa \land \psi$
\If{($svfdt\_constraints$)}
\State \Return \textbf{True}
\EndIf
\EndIf
\State \Return \textbf{False}
\EndProcedure
\end{algorithmic}
"
406,1805.06749,"
% \caption{Weighting on frames' votes}\label{al:vote}
% \begin{algorithmic}[1]
% \State $L \gets \text{length of } F$
% \State $t \gets 0$
% \State $w \gets \beta*L$
% \State $W[0...L] \gets 0$
% \While{$t<L$}
% \State $t \gets t+1$.
% \If {$F_t=0$}
% \State $W[t...L] \gets W[t...L]+\frac{1}{L-t+1}$
% \Else
% \State $W[(F_t-\frac{w}{2})...(F_t+\frac{w}{2})] \gets W[(F_t-\frac{w}{2})...(F_t+\frac{w}{2})]+G[(F_t-\frac{w}{2})...(F_t+\frac{w}{2})]$
% \EndIf
% \EndWhile
% \State $\textit{c}_p \gets \underset{t}{argmax(W_t)}$
% \State \Return \textit{$c_p$}
% \end{algorithmic}
% "
407,1805.06126,"[H]
\label{Algorithm I}
    \SetAlgoLined
    \KwData{a sequence of states and signals}
    \KwResult{the reward function, optimal policy, and value function }
     Set  the learning rates $ \alpha_{\theta}, \, \alpha_{\omega} $, batch size $ N_b $, initial parameters $ \theta^{(0)}, \,  \omega^{(0)},  
     \hat{ {\bf A}}_0^{(0)}, \, \hat{ {\bf A}}_1^{(0)}, \,  \Sigma_p^{(0)} $  \\
    Set  $ k = 1 $ \\
    \While{not converged}{
    	Draw a new mini-batch of $ N_b $  $T$-step trajectories $ ( {\bf y}_t, \ldots, {\bf y}_{t+T} ) $ (can set $ T = 1 $ for a market portfolio) \\ 
    	{\it E-step}: \\
	 Compute the free energy $ \mathcal{F}_b(\omega, \theta^{(k-1)}) $  of the mini-batch using Eq.(\ref{F_minib}) \\
	Update recognition model parameters $ \omega^{(k)} =  (1-\alpha_{\omega})  \omega^{(k-1)} 
	+ \alpha_{\omega} \frac{\partial}{\partial \omega}  \mathcal{F}_b(\omega, \theta^{(k-1)}) $  \\
    	{\it M-step}:  Maximize $ \mathcal{F}_b(\omega^{(k)}, \theta) $ as a function of $ \theta $: \\
	\For{ \mbox{each transition} $ ( {\bf y}_t, {\bf y}_{t+1} ) $ ( for a single investor, take $ t = T-1, \ldots, 0 $)}{
	    	1. Compute the expected value at time $ t $ of the F-function at time $ t + 1 $. \\
		2. Compute the reward as a function of $ \theta $. \\
		3. Use steps 1 and 2 to update the Q-function at time $ t $ \\
		4. Compute the value of the F-function at time $ t $. \\
		5. Recompute the policy distribution  $ \pi_{\theta} ( {\bf a}_t| t, {\bf y}_t) $ as a function of $ \theta $ by updating its mean and variance.   \\     
        }
        Compute the free energy $ \mathcal{F}_b(\omega^{(k)}, \theta) $  of the mini-batch using Eq.(\ref{F_minib}) \\
        Update the parameter vector $ \theta^{(k)} = (1-\alpha_{\theta})  \theta^{(k-1)} + \alpha_{\theta} 
        \frac{\partial}{\partial \theta} \mathcal{F}_B(\omega^{(k)}, \theta)  $ \\
        Use the new value $ \theta^{(k)} $ to compute $  \hat{ {\bf A}}_1^{(k)}, \, \hat{ {\bf A}}_1^{(k)}, \,  \Sigma_p^{(k)} $ \\
        Increment $ k = k + 1 $
    }
\caption{The Invisible Hand Inference with the Free energy (IH-IF) variational EM IRL algorithm that learns the reward function, optimal policy and value function from a history of prices and signals, for either a market portfolio or a single investor.}
"
408,1805.06126,"[H]
\label{Algorithm II}
    \SetAlgoLined
    \KwData{a sequence of states and signals}
    \KwResult{the reward function, optimal policy, and value function }
     Set  the learning rates $ \alpha_{\theta}, \, \alpha_{\omega} $, batch size $ N_b $, initial parameters $ \theta^{(0)}, \,  \omega^{(0)},  
     \hat{ {\bf A}}_0^{(0)}, \, \hat{ {\bf A}}_1^{(0)}, \,  \Sigma_p^{(0)} $  \\
    Set  $ k = 1 $ \\
    \While{not converged}{
    	Draw a new mini-batch of $ N_b $  $T$-step trajectories $ ( {\bf y}_t, \ldots, {\bf y}_{t+T} ) $  \\ 
    	{\it E-step}: \\
	 Compute the free energy $ \mathcal{F}_s(\omega, \theta^{(k-1)}) $  of the mini-batch using Eq.(\ref{F_Var_EM_single_agent}) \\
	Update recognition model parameters $ \omega^{(k)} =  (1-\alpha_{\omega})  \omega^{(k-1)} 
	+ \alpha_{\omega} \frac{\partial}{\partial \omega}  \mathcal{F}_s(\omega, \theta^{(k-1)}) $  \\
    	{\it M-step}:  Maximize $ \mathcal{F}_s(\omega^{(k)}, \theta) $ as a function of $ \theta $: \\
	\For{ \mbox{each transition} $ ( {\bf y}_t, {\bf y}_{t+1} ) $ for $ t = T-1, \ldots, 0 $ }{
	    	1. Compute the expected value at time $ t $ of the F-function at time $ t + 1 $. \\
		2. Compute the reward as a function of $ \theta $. \\
		3. Use steps 1 and 2 to update the Q-function at time $ t $ \\
		4. Compute the value of the F-function at time $ t $. \\
		5. Recompute the policy distribution  $ \pi_{\theta} ( {\bf a}_t| t, {\bf y}_t) $ as a function of $ \theta $ by updating its mean and variance.   \\     
        }
        Compute the free energy $ \mathcal{F}_s(\omega^{(k)}, \theta) $  of the mini-batch using Eq.(\ref{F_Var_EM_single_agent}) \\
        Update the parameter vector $ \theta^{(k)} = (1-\alpha_{\theta})  \theta^{(k-1)} + \alpha_{\theta} 
        \frac{\partial}{\partial \theta} \mathcal{F}_s(\omega^{(k)}, \theta)  $ \\
        Use the new value $ \theta^{(k)} $ to compute $  \hat{ {\bf A}}_1^{(k)}, \, \hat{ {\bf A}}_1^{(k)}, \,  \Sigma_p^{(k)} $ \\
        Increment $ k = k + 1 $
    }
\caption{IRL algorithm that learns the optimal policy, reward, and value function for a single investor.}
"
409,1706.01445,"[H]
  \caption{Mondrian Partitioning }\label{alg:mondrian}
    \begin{small}
  \begin{algorithmic}[1]
%
    \Function{MondrianPartitioning\,}{$V, N_{p}, S$}
      \While{$|V| <  N_{p}$}
      \State  $p_j\gets length(v_j) \cdot max(0, |\cd^j| - S), \forall v_j \in V$
      \If{$p_j = 0, \forall j$}
      \State break
      \EndIf
      \State Sample $ v_j \sim \frac{p_j}{\sum_j p_j}, v_j \in V$
      \State Sample a dimension $ d \sim \frac{h_d^j - l_d^j}{\sum_d h_d^j - l_d^j}, d\in [D]$
      \State Sample cut location $u_d^j \sim U[l_d^j, h_d^j]$
      \State $v_{j(left)}\gets [l_1^j, h_1^j]\times\cdots \times [l_d^j, u_d^j]\times \cdots\times \times [l_D^j, h_D^j]$
      \State $v_{j(right)}\gets [l_1^j, h_1^j]\times\cdots \times [u_d^j, h_d^j]\times \cdots\times \times [l_D^j, h_D^j]$
      \State $V \gets V\cup\{v_{j(left)},v_{j(right)}\}\setminus v_j$
      \EndWhile
      \State\Return $V$
      \EndFunction
  \end{algorithmic}
        \end{small}
"
410,1706.01445,"[H]
  \caption{Ensemble Bayesian Optimization (EBO)}\label{alg:ebo}
    \begin{small}
  \begin{algorithmic}[1]
    \Function{EBO\,}{$f, \cd_0$}
      \State Initialize $z, k$
      \For{$t = 1,\cdots, T $}
      \State $\{\cx_j\}_{j=1}^J\gets$\textsc{Mondrian}($[0,R]^D, z, k, J$)\label{mondrian}
      \ParFor{$j = 1,\cdots, J$}
      \State $z^j, k^j\gets\textsc{GibbsSampling}(z,k \mid \cd^j_{t-1})$\label{gibbs}
      \State $\eta^j_{t-1}(\cdot)\gets$\textsc{Acquisition\,}($\cd_{t-1}^{j}, z^j, k^j$) \label{acfun}
      \State $\{A_m\}_{m=1}^M\gets\textsc{Decomposition}(z^j)$
      \For{$m=1,\cdots,M$} \label{decomp}
      \State $\vx_{tj}^{A_m} \gets \argmax_{\vx\in\cx^{A_m}_j} \eta^j_{t-1}(\vx)$ \label{opt_acfun}
      \EndFor
      \EndParFor
      \State $z\gets\textsc{Sync}(\{z^j\}_{j=1}^J), \;\; k\gets\textsc{Sync}(\{k^j\}_{j=1}^J)$ \label{sync}
      \State $\{\vx_{tb}\}_{b=1}^B\gets$ \textsc{Filter\,}($\{\vx_{tj}\}_{j=1}^J \mid z, k$) \label{filter}
      \ParFor{$b=1,\cdots,B$}
      \State $y_{tb}\gets f(\vx_{tb}) $\label{eval}
      \EndParFor
      \State $\mathfrak \cd_t \gets \cd_{t-1}\cup \{\vx_{tb},y_{tb}\}_{b=1}^B$
      \EndFor
      \EndFunction

  \end{algorithmic}
        \end{small}
"
411,1706.01445,"[H]
  \caption{Generative model for TileGP}\label{alg:gen}
  \begin{small}
  \begin{algorithmic}[1]
\State Draw mixing proportions $\theta\sim\textsc{Dir}(\alpha)$
\For{$d = 1,\cdots, D$}
\State Draw additive decomposition $z_d\sim\textsc{Multi}(\theta)$
\State Draw Poisson rate parameter $\lambda_d\sim\textsc{Gamma}(\beta_0,\beta_1)$
\For{$i = 1, \cdots, L$}
\State \hspace{-1pt}Draw number of cuts $k_{di}\sim \textsc{Poisson}(\lambda_d(h_d^j-l_d^j))$ 
\State \hspace{-8pt}$\begin{cases} \text{Draw offset }\delta\sim U[0,\frac{h_d^j-l_d^j}{k_{di}}] & \text{Tile Coding}\\
\text{Draw cut locations } \vb\sim U[l_d^j,h_d^j] & \text{Mondrian Grids}
\end{cases}$
\EndFor
\EndFor
\State Construct the feature projection $\vpp$ and the kernel $\kk = \vpp\T \vpp$ from $z$ and sampled tiles
 \State Draw function $f\sim \cg\cp(0,\kk)$
\State Given input $\vx$, draw function value $y\sim \mathcal N(f(\vx), \sigma)$
\end{algorithmic}
\end{small}
"
412,1802.00981,"[h!]
	\caption{\small Adaptive Bandit with Context-Dependent Embeddings (ABaCoDE)}
	\label{alg:summary}
	\begin{algorithmic}[1]
    \STATE {\bfseries } \textbf{Input:} unlabeled dataset $\textbf{D}$, a set of unlabeled contexts for pre-training; $k$, the number of clusters (and corresponding embeddings); a Boolean variable $isOnline$.
    \STATE {\bfseries } \textbf{Initialization:} \STATE {\bfseries } \quad Cluster $\textbf{D}$ into $k$ clusters: \textbf{C} = $\{c_1, ..., c_k\}$
 \STATE {\bfseries} \quad For each cluster, train an autoencoder to construct a set of encoding functions (embeddings):    \textbf{E} = {$e_1, ..., e_k$} 
 \STATE {\bfseries} \quad Initialize the contextual Thompson Sampling parameters of bandit $B$ (line 1 in Alg. 1).
    \STATE {\bfseries } \textbf{while} there is a next data mini-batch $\textbf{M}$, \textbf{do}
    \STATE {\bfseries } \quad  \textbf{foreach} $x_t$ from  $\textbf{M}$ \textbf{do}
    \STATE {\bfseries }  \quad   \quad  \textbf{if} $isOnline$  \textbf{then}  $updateCluster(\textbf{C}, x_t, c_j)$ 
     \STATE {\bfseries } \quad   \quad  $e = selectEmbedding(c_j$)
   \STATE {\bfseries }   \quad   \quad $z=e(x_t)$ (encoded context/representation)
    \STATE {\bfseries }    \quad\quad  $contextualBandit(B,z)$
   (lines 4-7 in Alg. 1) \\
      \quad \textbf{end}
    \STATE {\bfseries }   \quad \textbf{if} $not (isOnline)$ \textbf{then} $recomputeClusters(\textbf{C}, \textbf{B}$)
    \STATE {\bfseries }   \quad$updateEmbedding(\textbf{M},\textbf{C}$) \\
\textbf{end}
    \end{algorithmic}
"
413,1802.00981,"%[H]
	\caption{\small Adaptive Compression with Context-Dependent Embeddings}
	\label{alg:compression}
	\begin{algorithmic}[1]
   \STATE {\bfseries } \textbf{Input:}  $\textbf{D}$, a set of unlabeled contexts (data for pre-training); $\textbf{k}$, a series of available compression levels (embeddings); a Boolean condition defining a specific method from algorithmic family: $isStagedCB$.
   \STATE {\bfseries } \textbf{Initialization:} 
\STATE {\bfseries} \quad for each compression level within vector \textbf{k}, construct a set of encoding functions (embeddings): \textbf{E} = {$e_1, ..., e_k$} 
 \STATE {\bfseries} \quad initialize contextual Thompson Sampling parameters for compression level selection bandit $B_1$ and classification-decision bandit $B_2$.
   \STATE {\bfseries } \textbf{While} there is a next data mini-batch $M_l$, \textbf{do}
   \STATE {\bfseries } \quad  \textbf{Foreach} $x_i$ from mini-batch $M_l$ \textbf{do}
    \STATE {\bfseries }    \quad\quad    $e = selectCompressionCB(B_1,x_i$)\\
    \STATE {\bfseries }    \quad\quad  $z = e(x_i)$
    \STATE {\bfseries }    \quad\quad  $d = contextualBandit(B_2,z$)
       \STATE {\bfseries }    \quad\quad Receive reward $r_i(d)$ 
       \STATE  {\bfseries }    \quad\quad $[r_i^k, r_i^p] = assignReward(\alpha_k, \alpha_p, r_i(d))$ 
    \STATE {\bfseries }    \quad \quad $updateBandit(r_i^k, B_1)$\\
    \STATE {\bfseries }    \quad \quad $updateBandit(r_i^p, B_2)$\\
     \quad \textbf{End}
    \STATE {\bfseries }   \quad$updateEmbedding(M_l,\textbf{k}$)
     \STATE {\bfseries }   \quad \textbf{If} $isStagedCB$  \textbf{then}   $reinitializeCB$ \\
\textbf{End}
   \end{algorithmic}
"
414,1802.00981,"[H]
	\caption{ The CBRH Problem Setting}
	% \label{alg.2}
	\label{alg:CBRH}
	\begin{algorithmic}[1]
		\STATE Obtain unlabeled
      set of contexts $\textbf{D}$
      \STATE   Learn a context representation model
        \STATE {\bfseries }\textbf{Repeat}
		\STATE {\bfseries }  \quad $({\bf x}_t,r_t)$ is drawn according to distribution $D_{c,r}$
	%	\STATE \quad  Representation $e_i \in E$ is selected 
     %   \STATE {\bfseries }  \quad Context representation
%$${\bf z}_t = e({\bf x}_t)$ is computed
\STATE \quad Choose encoding $e_i \in E$ 
\STATE \quad
Compute representation ${\bf z}_t=e_i({\bf x}_i$)  
\STATE{\bfseries}  \quad Choose an arm $k_t = \hat{\pi}_I(\mathbf{z}_t)$
				\STATE {\bfseries }  \quad The reward $r^k_t$ is revealed
		\STATE {\bfseries }  \quad  Update  policy $ \pi(\cdot) = \hat{\pi}_i(e_i(\cdot))\}$
			\STATE {\bfseries }  \quad $t=t+1$
		\STATE {\bfseries }  \textbf{Until} t=T
	\end{algorithmic}
"
415,1802.00981,"[H]
   \caption{The Contextual Thompson Sampling Algorithm}
%   \label{alg.2}
\label{alg:CTS}
\begin{algorithmic}[1]
 \STATE {\bfseries }\textbf{Initialize:}    \textbf{for } $i=1,...,k$, $B_i= I_d$, $ \hat{\mu}_i= 0_d, f_i = 0_d$.
 \STATE {\bfseries }\textbf{for } $t = 1, 2, . . . ,T$ \textbf{do}
 \STATE \quad Receive context ${\bf x}_t$
 \STATE {\bfseries }\quad   \textbf{for } $i=1,...,k$,  sample $\tilde{\mu_{i}}$ from the $N(\hat{\mu}_i, v^2 B_i^{-1})$  
 \STATE {\bfseries }\quad Choose arm $i_t= \arg \underset{i\subset I}{max}\ x(t)^\top \tilde{\mu_{i}} $
% \STATE {\bfseries } \textbf{else if} not all $x \in c$ are queried \textbf{goto} 5.
  \STATE {\bfseries }\quad Receive reward   $r^{i}_t$
 \STATE {\bfseries }\quad
% \textbf{if} $r_{i}(t)=1$ \textbf{then} \\
 $B_i= B_{i}+ x_t x_t^{T} $, $f_i = f_i + x_t r^i_t$, $\hat{\mu_i} = B_i^{-1} f_i$
 \STATE {\bfseries }\textbf{end}
   \end{algorithmic}
"
416,1804.09931,"[t]
\begin{scriptsize}
    	\KwIn{corpus $\D$, sentence {S}, text features {$\F$}, convergence threshold $t$} 
		\KwOut{relation tuples $\T$, semantic representation $\mathcal{V}$, similarity measure $\sigma$}
		generate entity and relation seeds via distant corpus linking \;
		\textbf{phrase extraction module} outputs entity phrases, relation phrases, sentence segmentation probability $\mathcal{W}$ \;
		initialize positive $E_p^{+0}$, cohesiveness measure $\sigma= 1$ \;
		generate relation tuples $\T$ among  $E_p^{+0}$ \;
		\Do{$\frac{\Delta_E}{|E_p^{+n}|} > t$}{
		    update $\mathcal{V},\sigma$ in Eq.~(\ref{eqn:global_obj}) via \textbf{global cohesiveness module} \;
		    $E_p^{+n} \leftarrow \emptyset$, $\Delta_E \leftarrow 0$ \;
			\For{each tuple $\langle e_h, p_{ht}, e_t \rangle \in \T$} {
			    construct candidate subject sets \textbf{s} of $e_t$ with at most $M_{sp}$ entities\;
			    $\sigma_* \leftarrow \sigma(e_h, p_{h,t}, e_t)$ \;
			    \For{i = 1 to $M_{sp}$}{
				generate $\langle s_i, p_{i,t}, e_t \rangle$ in Eq.~(\ref{eqn:local_obj}) via \textbf{relation tuple generation module} given $\mathcal{W}$ and $\mathcal{V}$\;
				\If{$\sigma(s_i, p_{i,t}, e_t) > \sigma_*$}{
				    $\sigma_* \leftarrow \sigma(s_i, p_{i,t}, e_t)$, $e_*\leftarrow s_i$ \;
				    }
				}
				$E_p^{+n} \leftarrow E_p^{+n} \bigcup \langle e_*, e_t \rangle$\;
				\If{$e_* \neq e_h$}{
				    $\Delta_E \leftarrow \Delta_E + 1$, $\langle e_*, p_{*t}, e_t \rangle \leftarrow \langle e_h, p_{ht}, e_t \rangle$  update $\T$ \;
				}
			}
		}
		%\doWhile{$E_p^+$ is not stable}
        \caption{The \ReMine Algorithm for Joint Optimization}
        \label{algorithm:ReMine}
\end{scriptsize}
"
417,1805.04777,"
  \caption{Mean field approximation in convolutional connected CRFs}
  \label{alg:mean_field_conv}
  \begin{algorithmic}[1]
  \State Initialize:   \Comment{$\tilde Q_i \leftarrow \frac{1}{Z_i}exp{(- \psi_u (x_i | I) }) \; \text{""softmax""}$} 
  \While{not converged}
  \State $\tilde Q_i(l) \leftarrow \sum_{i \not= j} w^{(m)} k_G^{(m)} (f_i^I, f_j^I) \tilde Q_i(l)$ \Comment{Message Passing}
  \State $\tilde Q_i(x_i) \leftarrow \sum_{l' \in L} \mu(x_i, l') \tilde Q_i(l)$ \Comment{Compatibility Transformation}
  \State $\tilde Q_i(x_i) \leftarrow  \psi_u (x_i | I ) + \tilde Q_i(x_i)$ \Comment{Adding Unary Potentials}

  \State $\tilde Q_i(x_i) \leftarrow \text{normalize}(\tilde Q_i(x_i))$ \Comment{e.g. softmax}
  \EndWhile
  \end{algorithmic}
"
418,1805.05606,"%[H]
\SetAlgoLined
\KwData{Observations $y_{1:n}$}
Hyperparameters $\alpha_1,$ $\beta_1$, $\alpha_v$, $\beta_v$, $a$, $b$, $N$\;
\KwResult{Posterior samples $\theta_{1:N}^{i}:i=1,\ldots,M$}
 Initialization $\theta_{1:N}^0,$ $\zeta_{1:N}^0$, $\eta_v^0$, $\alpha^0$\;
\While{$i \leq M$}{
    sample $x_{0:n}^i$ via FFBS\;
sample $\theta_{1:N}^i$ from the inverse Gamma full conditionals\;
sample $\zeta_{2:N}^i$ from the inverse Gamma full conditionals\;
sample $\eta_v^i$ from the inverse Gamma full conditional\;
sample $\alpha^i$ via a Metropolis-Hastings step\;
set $i=i+1$. 
    }
 \caption{Gibbs sampler for volatility learning}
\label{pseudocode}
"
419,1805.05480,"
  \caption{ \small Training set for CDE via Rejection ABC }\label{alg::naiveAbc}
  \algorithmicrequire \ {\small Tolerance level $\epsilon$, number of desired sample points $B$,  distance function $d$, sample $\x_0$}
  
  \algorithmicensure \ {\small Training set $\mathcal{T}$ which approximates the joint distribution of $(\theta,\X)$
  in a neighborhood of $\x_0$}
  \begin{algorithmic}[1]
  \State Let $\mathcal{T}=\{\}$
     \While{$|\mathcal{T}|<B$}
     \State Sample $\theta \sim f(\theta)$
\State  Sample $\X \sim f(\x|\theta)$
     \State  If $d(\x,\x_o)< \epsilon,$ let $\mathcal{T} \longleftarrow \mathcal{T} \cup \{(\theta,\x)\}$
     \EndWhile
     \State \textbf{return} $\mathcal{T}$  
  \end{algorithmic}
  "
420,1805.05814,"[tb]
                    \caption{Moving average updates:
                    for $z \in \{0,1 \}$, $p^z$ estimates $p(Z = z)$ and $\mu^z$ estimates $\E(Y\mid Z = z)$}
                    \label{alg:maupdate}
                    \begin{algorithmic}[1]
                    \State \textbf{Initialize:} $\mu^0 = -1$, $\mu^1 = 1$, $p^0 = p^1 = 0.5$, $\lambda=0.8$             
                    \renewcommand{\algorithmicforall}{\textbf{for each}}
                    \ForAll{mini-batch $\{y^{(k)}, k \in 1 .. K\}$}
                        \For{$z \in \{ 0,1 \}$}
                            \State $p^{z} \leftarrow  \lambda p^{z} +   (1- \lambda)\frac{1}{K}\sum_{k=1}^K p(z\mid y^{(k)})$
                            \State $\mu^{z} \leftarrow \lambda  \mu^{z} +  (1-\lambda)\frac{1}{K}\sum_{k=1}^K\displaystyle \frac{p(z\mid y^{(k)})}{p^z}y^{(k)}$
                        \EndFor
                    \EndFor
                \end{algorithmic}
            "
421,1805.05814,"[tb]
%                     \caption{Moving average updates:
%                     for $z \in \{0,1 \}$, $p^z$ estimates $p(Z = z)$ and $\mu^z$ estimates $\E(Y\mid Z = z)$}
%                     \label{alg:maupdate}
%                     \begin{algorithmic}[1]
%                     \State \textbf{Initialize:} $\mu^0 = -1$, $\mu^1 = 1$, $p^0 = p^1 = 0.5$, $\lambda=0.8$             
%                     \renewcommand{\algorithmicforall}{\textbf{for each}}
%                     \ForAll{mini-batch $\{y^{(k)}, k \in 1 .. K\}$}
%                         \For{$z \in \{ 0,1 \}$}
%                             \State $p^{z} \leftarrow  \lambda p^{z} +   (1- \lambda)\frac{1}{K}\sum_{k=1}^K p(z\mid y^{(k)})$
%                             \State $\mu^{z} \leftarrow \lambda  \mu^{z} +  (1-\lambda)\frac{1}{K}\sum_{k=1}^K\displaystyle \frac{p(z\mid y^{(k)})}{p^z}y^{(k)}$
%                         \EndFor
%                     \EndFor
%                 \end{algorithmic}
%             "
422,1805.05181,"[t]
\caption{The cycled reinforcement learning method for training the neutralization module $N_{\theta}$ and the emotionalization module $E_{\phi}$.}
\label{code} 
\small
  \begin{algorithmic}[1]
  
      \State Initialize the neutralization module $N_{\theta}$, the emotionalization module $E_{\phi}$ with random weights $\theta$, $\phi$ 
      \State Pre-train $N_{\theta}$ using MLE based on Eq. \ref{newi} 
      \State Pre-train $E_{\phi}$ using MLE based on Eq. \ref{em}
    \For{each iteration $i=1,2,..., M$}  
    	\State Sample a sequence $\boldsymbol{x}$ with sentiment $s$ from $X$
        \State Generate a neutralized sequence $\hat{\boldsymbol{x}}$ based on $N_{\theta}$ 
        \State Given $\hat{\boldsymbol{x}}$ and $s$, generate an output based on $E_{\phi}$
         \State Compute the gradient of $E_{\phi}$ based on Eq. \ref{dual-learning-2}
        \State Compute the reward $R_1$ based on Eq.~\ref{reward} 
        \State $\bar{s}$ = the opposite sentiment
         \State Given $\hat{\boldsymbol{x}}$ and $\bar{s}$, generate an output based on $E_{\phi}$
          \State Compute the reward $R_2$ based on Eq.~\ref{reward}
        \State Compute the combined reward $R_{c}$ based on Eq.~\ref{combined_reward}
        \State Compute the gradient of $N_{\theta}$ based on Eq. \ref{dual-learning-1}
        \State Update model parameters $\theta$, $\phi$
    \EndFor  
     
  \end{algorithmic} 
  
"
423,1805.05062,"
    \caption{Sequence-level smoothing algorithm}
		\label{alg:seq}
		\begin{algorithmic}
        \small
        \Require $x, \gt{}$
        \Ensure $\sml{seq}{\alpha}(x,\gt{})$
        \State Encode $x$ to initialize the RNN
        \State Forward $\gt{}$ in the RNN to compute the hidden states $h_t^*$
        \State Compute the MLE loss $\ml(\gt{},x)$
		\For{$l\in \{1,\ldots, L\}$}
            \State Sample $y^l \sim r(\dot|\gt{})$\;
            \If{Lazy} 
                \State Compute $\ell(y^l, x)=-\sum_t \log\p(y_t^l|h_t^*)$
             \Else
                \State Forward $y^l$ in the RNN to get its hidden states $h^l_t$
                \State Compute $\ell(y^l, x)=\ml(y^l,x)$
            \EndIf
        \EndFor
        \State $\sml{Seq}{\alpha}(x,\gt{})= \bar\alpha \ml(\gt{},x) + \frac{\alpha}{L}\sum_l \ell(y^l, x)$
       \end{algorithmic}
"
424,1805.04103,"[t]
\caption{The deep feature reshuffle algorithm.}
\small
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{One content image $I_c$ and one style image $I_s$.}
\BlankLine
 \textbf{Initialization}:\\
 \quad$\{F_{c}^{l}\}_{l=2}^4, \{F_{s}^{l}\}_{l=2}^4\leftarrow$ feed $I_c, I_s$ to VGG-19.\\
 \quad$\hat F_o^4=F_c^4$.\\
\For{$l =4$ to $2$}{
   $F_c^l \leftarrow \beta \hat F_o^l+(1-\beta)F_c^l$.\\
   $F_o^{l,(0)} \leftarrow F_c^l$.\\
   \For{$i =1$ to $max\_iter$}{
   $\text{NNC}^{l, (i)} \leftarrow$ match $F_o^{l, (i-1)}$ and $F_s^l$ (\eref{eq:nnc}).\\
   $F_s^l(\text{NNC}^{l,(i)}) \leftarrow$ warp $F_{s}^{l}$ with $\text{NNC}^{l,(i)}$.\\
   $F_o^{l,(i)} \leftarrow \alpha F_c^l+(1-\alpha)F_s^l(NNC^{l,(i)})$.\\
   }
   \If{$l>2$}{
   $\hat F_o^{l-1} \leftarrow$ decode $F_o^{l,(max\_iter)}$ from $l$ to $l-1$.
   }
   }
$I_o \leftarrow$ decode $F_o^{2,(max\_iter)}$ to image.\\
\Output{Style transfer result image $I_o$. }
\label{ag:bda}
"
425,1805.04810,"[!htb]
%\caption{Optimized version of GANG (GANG\_Opt)}
%\label{alg:GANG}
%\begin{algorithmic}
%\REQUIRE $G=(V,E)$, $L_f$, $L_n$, $\epsilon$, $w$, $\delta=10^{-3}$, and $T=20$. \\
%\ENSURE  $p_u, \forall \, u \in V$. \\
%    Initialize $\hat{\mathbf{p}}^{(0)} = \hat{\mathbf{q}}$. \\
%    Initialize $t=1$. \\
%%\noindent \textbf{Process:}
%%\PROCESS ~~\\
%    %\WHILE { $\sum_{u\in V}|\hat{{p}}_u^{(t)} - \hat{{p}}_u^{(t-1)}| \geq \delta$ and $t \leq T$} \;
%    \WHILE { $\frac{\|\hat{\mathbf{p}}^{(t)} - \hat{\mathbf{p}}^{(t-1)}\|_1} {\|\hat{\mathbf{p}}^{(t)}\|_1} \geq \delta$ and $t \leq T$} \;
%    \STATE Update residual belief vector $\hat{\mathbf{p}}^{(t)}$ using Eqn.~\ref{approx_update}; \\
%    \STATE  $t=t+1$. \\
%    \ENDWHILE \\
%    \RETURN $\hat{\mathbf{p}}^{(t)} + 0.5$. \\
%\end{algorithmic}
%"
426,1805.04810,"[t]
%\caption{Policy-Aware Noise Finding Algorithm}
%\begin{algorithmic}[1]
%\REQUIRE Public data $\textbf{x}$, classifier $\textbf{C}$, noise-type-policy $\mathcal{P}$, target attribute value $i$, and step size $\tau$. \\
%\ENSURE  Noise $\textbf{r}_i$. \\
%    %Initialize $\textbf{x}_{0}=\textbf{x}$ \\
%    Initialize $t=0,\overline{\textbf{x}}=\textbf{x}$. \;
%%\noindent \textbf{Process:}
%%\PROCESS ~~\\
%    %\WHILE { $\sum_{u\in V}|\hat{{p}}_u^{(t)} - \hat{{p}}_u^{(t-1)}| \geq \delta$ and $t \leq T$} \;
%	%\IF{$\textbf{C}(\textbf{x})==i$} \RETURN $\textbf{0}$ \ENDIF \\
%	
%	\WHILE { $\textbf{C}(\overline{\textbf{x}}) \neq i$ and $t \leq \text{maxiter}$} \;
%	\STATE //Find the entry to be modified. \;
%	\IF{$\mathcal{P}==Add\_New$} \;
%	
%	\STATE $e_{inc}=\argmax_{j} \{\frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}| \mathbf{x}_j=0\} $ \;
%	\ENDIF\;
%	
%	\IF{$\mathcal{P}==Modify\_Exist$} \;
%	\STATE $e_{inc}=\argmax_{j} \{(1-\overline{\textbf{x}}_j) \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}| \mathbf{x}_j \neq 0\}$ \;
%	
%	%(\textbf{1}-\overline{\textbf{x}})\times \triangledown \textbf{C}_{i}$ \\
%	\STATE $e_{dec}=\argmax_{j} \{-\overline{\textbf{x}}_j \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}| \mathbf{x}_j \neq 0\}$ \;
%	\ENDIF \\
%	
%	\IF{$\mathcal{P}==Modify\_Add$} \;
%	\STATE $e_{inc}=\argmax_{j} \{(1-\overline{\textbf{x}}_j) \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}\}$ \;
%	
%	%(\textbf{1}-\overline{\textbf{x}})\times \triangledown \textbf{C}_{i}$ \\
%	\STATE $e_{dec}=\argmax_{j} \{-\overline{\textbf{x}}_j \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}\}$ \;
%	\ENDIF \\
%
%
%
%	\STATE //Modify the entry $\overline{\textbf{x}}_{e_{inc}}$ or $\overline{\textbf{x}}_{e_{dec}}$ depending on which one is more beneficial. \;
%	\STATE $v_{inc}=(\textbf{1}-\overline{\textbf{x}}_{e_{inc}}) \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_{e_{inc}}}$ \;
%	\STATE $v_{dec}=-\overline{\textbf{x}}_{e_{dec}} \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_{e_{dec}}}$ \;
%	\IF {$\mathcal{P}==Add\_New$ or $v_{inc} \geq  v_{dec}$} \;
%	%\WHILE { $ \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_{e_{inc}}} > 0 $ and $\overline{\textbf{x}}_{e_{inc}}<1$} \;
%	
%	\STATE $\overline{\textbf{x}}_{e_{inc}} = clip(\overline{\textbf{x}}_{e_{inc}}+ \tau)$ \;
%	\label{clip1}
%	%\ENDWHILE \;
%	\ELSE 
%	%\WHILE { $\frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_{e_{dec}}} < 0 $ and $\overline{\textbf{x}}_{e_{dec}}>0$} \;
%	
%	\STATE $\overline{\textbf{x}}_{e_{dec}} = clip(\overline{\textbf{x}}_{e_{dec}} - \tau)$ \;
%	\label{clip2}
%	%\ENDWHILE \;
%	\ENDIF \;
%	\STATE $t=t+1$ \;
%	\ENDWHILE \;
%%	
%	\RETURN $\overline{\textbf{x}}-\textbf{x}$. \;
%
%    % \WHILE { $\textbf{F}(\textbf{x}_{i}) \neq l_{t}$ and $i \leq maxiter$} \;
%	% \STATE $i_{max1}=\argmax_{j} (\textbf{1}-\textbf{x}_{i})\times \triangledown \textbf{F}_{l_{t}}$ \\
%	% \STATE $value=(\textbf{1}-\textbf{x}_{i}) \times \textbf{F}_{l_{t}}[i_{max1}]$ \\	
%	% \STATE $i_{decrease}=\argmax_{j} \textbf{x}_{i} \times (\textbf{-1}) \times \triangledown \textbf{F}_{l_{t}}$ \\
%	% \IF {$value \geq  \textbf{x}_{i} \times (-\textbf{1})\times \triangledown \textbf{F}_{l_{t}}[i_{decrease}]$} \;
%	% %\STATE $i_{max}=i_{max1}$ \\
%	% \STATE $\textbf{r}_{i}[i_{max1}]$=1.0 \\
%	% \ELSE \;
%	% %\STATE $i_{max}=i_{max2}$ \\
%	% \STATE $\textbf{r}_{i}[i_{max2}]$=0.0 \\
%	% \ENDIF \\
%	% \STATE $\textbf{x}_{i+1}=\textbf{x}_{i}+\textbf{r}_{i}$ \\
%	% \STATE $i=i+1$ \\
%    % \ENDWHILE \\
%    % \RETURN $\widehat{\textbf{r}}=\sum_{i}\textbf{r}_{i}$. \\
%\end{algorithmic}
%\label{algorithm2}
%"
427,1805.04810,"[t]
\caption{Policy-Aware Noise Finding Algorithm}
\begin{algorithmic}[1]
\REQUIRE Public data $\textbf{x}$, classifier $\textbf{C}$, noise-type-policy $\mathcal{P}$, target attribute value $i$, and step size $\tau$. \\
\ENSURE  Noise $\textbf{r}_i$. \\
    %Initialize $\textbf{x}_{0}=\textbf{x}$ \\
    Initialize $t=0,\overline{\textbf{x}}=\textbf{x}$. \;
%\noindent \textbf{Process:}
%\PROCESS ~~\\
    %\WHILE { $\sum_{u\in V}|\hat{{p}}_u^{(t)} - \hat{{p}}_u^{(t-1)}| \geq \delta$ and $t \leq T$} \;
	%\IF{$\textbf{C}(\textbf{x})==i$} \RETURN $\textbf{0}$ \ENDIF \\
	
	\WHILE { $\textbf{C}(\overline{\textbf{x}}) \neq i$ and $t \leq \text{maxiter}$} \;
	\STATE //Find the entry to be modified. \;
	\IF{$\mathcal{P}==Add\_New$} \;
	
	\STATE $e_{inc}=\argmax_{j} \{\frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}| \mathbf{x}_j=0\} $ \;
	\ENDIF\;
	
	\IF{$\mathcal{P}==Modify\_Exist$} \;
	\STATE $e_{inc}=\argmax_{j} \{(1-\overline{\textbf{x}}_j) \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}| \mathbf{x}_j \neq 0\}$ \;
	
	%(\textbf{1}-\overline{\textbf{x}})\times \triangledown \textbf{C}_{i}$ \\
	\STATE $e_{dec}=\argmax_{j} \{-\overline{\textbf{x}}_j \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}| \mathbf{x}_j \neq 0\}$ \;
	\ENDIF \\
	
	\IF{$\mathcal{P}==Modify\_Add$} \;
	\STATE $e_{inc}=\argmax_{j} \{(1-\overline{\textbf{x}}_j) \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}\}$ \;
	
	%(\textbf{1}-\overline{\textbf{x}})\times \triangledown \textbf{C}_{i}$ \\
	\STATE $e_{dec}=\argmax_{j} \{-\overline{\textbf{x}}_j \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}\}$ \;
	\ENDIF \\



	\STATE //Modify the entry $\overline{\textbf{x}}_{e_{inc}}$ or $\overline{\textbf{x}}_{e_{dec}}$ depending on which one is more beneficial. \;
	\STATE $v_{inc}=(\textbf{1}-\overline{\textbf{x}}_{e_{inc}}) \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_{e_{inc}}}$ \;
	\STATE $v_{dec}=-\overline{\textbf{x}}_{e_{dec}} \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_{e_{dec}}}$ \;
	\IF {$\mathcal{P}==Add\_New$ or $v_{inc} \geq  v_{dec}$} \;
	%\WHILE { $ \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_{e_{inc}}} > 0 $ and $\overline{\textbf{x}}_{e_{inc}}<1$} \;
	
	\STATE $\overline{\textbf{x}}_{e_{inc}} = clip(\overline{\textbf{x}}_{e_{inc}}+ \tau)$ \;
	\label{clip1}
	%\ENDWHILE \;
	\ELSE 
	%\WHILE { $\frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_{e_{dec}}} < 0 $ and $\overline{\textbf{x}}_{e_{dec}}>0$} \;
	
	\STATE $\overline{\textbf{x}}_{e_{dec}} = clip(\overline{\textbf{x}}_{e_{dec}} - \tau)$ \;
	\label{clip2}
	%\ENDWHILE \;
	\ENDIF \;
	\STATE $t=t+1$ \;
	\ENDWHILE \;
%	
	\RETURN $\overline{\textbf{x}}-\textbf{x}$. \;

    % \WHILE { $\textbf{F}(\textbf{x}_{i}) \neq l_{t}$ and $i \leq maxiter$} \;
	% \STATE $i_{max1}=\argmax_{j} (\textbf{1}-\textbf{x}_{i})\times \triangledown \textbf{F}_{l_{t}}$ \\
	% \STATE $value=(\textbf{1}-\textbf{x}_{i}) \times \textbf{F}_{l_{t}}[i_{max1}]$ \\	
	% \STATE $i_{decrease}=\argmax_{j} \textbf{x}_{i} \times (\textbf{-1}) \times \triangledown \textbf{F}_{l_{t}}$ \\
	% \IF {$value \geq  \textbf{x}_{i} \times (-\textbf{1})\times \triangledown \textbf{F}_{l_{t}}[i_{decrease}]$} \;
	% %\STATE $i_{max}=i_{max1}$ \\
	% \STATE $\textbf{r}_{i}[i_{max1}]$=1.0 \\
	% \ELSE \;
	% %\STATE $i_{max}=i_{max2}$ \\
	% \STATE $\textbf{r}_{i}[i_{max2}]$=0.0 \\
	% \ENDIF \\
	% \STATE $\textbf{x}_{i+1}=\textbf{x}_{i}+\textbf{r}_{i}$ \\
	% \STATE $i=i+1$ \\
    % \ENDWHILE \\
    % \RETURN $\widehat{\textbf{r}}=\sum_{i}\textbf{r}_{i}$. \\
\end{algorithmic}
\label{algorithm2}
"
428,1805.04810,"[t]
%\caption{Noise }
%\label{algorithm2}
%\begin{algorithmic}
%\REQUIRE Public data $\textbf{x}$, classifier $\textbf{C}$, noise-type-policy $\mathcal{P}$ and target attribute value $i$. \\
%\ENSURE  Noise $\textbf{r}_i$. \\
%    %Initialize $\textbf{x}_{0}=\textbf{x}$ \\
%    Initialize $t=0,\overline{\textbf{x}}=\textbf{x}$. \;
%%\noindent \textbf{Process:}
%%\PROCESS ~~\\
%    %\WHILE { $\sum_{u\in V}|\hat{{p}}_u^{(t)} - \hat{{p}}_u^{(t-1)}| \geq \delta$ and $t \leq T$} \;
%	%\IF{$\textbf{C}(\textbf{x})==i$} \RETURN $\textbf{0}$ \ENDIF \\
%	
%	\WHILE { $\textbf{C}(\overline{\textbf{x}}) \neq i$ and $t \leq \text{maxiter}$} \;
%	\STATE //Find the entry to be modified. \;
%	\IF{$\mathcal{P}==Add\_New$} \;
%	
%	\STATE $e_{inc}=\argmax_{j} \{\frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}| \mathbf{x}_j=0\} $ \;
%	\ENDIF\;
%	
%	\IF{$\mathcal{P}==Modify\_Exist$} \;
%	\STATE $e_{inc}=\argmax_{j} \{(1-\overline{\textbf{x}}_j) \frac{\partial \textbf{C}_{i}(\overline{\mathbf{x}})}{\partial \mathbf{x}_j}| \mathbf{x}_j \neq 0\}$ \;
%	
%	%(\textbf{1}-\overline{\textbf{x}})\times \triangledown \textbf{C}_{i}$ \\
%	\STATE $e_{dec}=\argmax_{j} \overline{\textbf{x}} \times (\textbf{-1}) \times \triangledown \textbf{C}_{i}$ \\
%	\ENDIF \\
%
%
%	\IF{$\mathcal{P}==Modify\_Add$} \;
%	\STATE $i_{inc}=\argmax_{j} (\textbf{1}-\overline{\textbf{x}})\times \triangledown \textbf{C}_{i}$ \\
%	\STATE $value\_inc=(\textbf{1}-\overline{\textbf{x}}) \times \textbf{C}_{i}[i_{inc}]$ \\
%	\STATE $i_{dec}=\argmax_{j} \overline{\textbf{x}} \times (\textbf{-1}) \times \triangledown \textbf{C}_{i}$ \\
%	\STATE $value\_dec=(\overline{\textbf{x}} \times (\textbf{-1}) \times \triangledown \textbf{C}_{i})[i_{dec}]$ \\
%	\ENDIF \\
%
%
%	%\triangledown \textbf{C}_{i}$ \\
%%	\WHILE { $\triangledown \textbf{C}_{i}(i_{inc}) > 0 $ and $\overline{\textbf{x}}(i_{inc})<1$} \;
%%	\STATE $\overline{\textbf{x}}+=\tau$ \\	
%%	\ENDWHILE \\
%%	
%%	\ELSE \;
%%	
%%	\IF{$\mathcal{P}==Modify\_Add$} \;
%%	\STATE $i_{inc}=\argmax_{j} (\textbf{1}-\overline{\textbf{x}})\times \triangledown \textbf{C}_{i}$ \\
%%	\STATE $value\_inc=(\textbf{1}-\overline{\textbf{x}}) \times \textbf{C}_{i}[i_{inc}]$ \\
%%	\STATE $i_{dec}=\argmax_{j} \overline{\textbf{x}} \times (\textbf{-1}) \times \triangledown \textbf{C}_{i}$ \\
%%	\STATE $value\_dec=(\overline{\textbf{x}} \times (\textbf{-1}) \times \triangledown \textbf{C}_{i})[i_{dec}]$ \\
%%	\ENDIF \\
%%	
%%	\IF{$\mathcal{P}==Modify\_Existing$} \;
%%	\STATE $i_{inc}=\argmax_{j|x_{0}(j)\neq 0} (\textbf{1}-\overline{\textbf{x}})\times \triangledown \textbf{C}_{i}$ \\
%%	\STATE $value\_inc=(\textbf{1}-\overline{\textbf{x}}) \times \textbf{C}_{i}[i_{inc}]$ \\
%%	\STATE $i_{dec}=\argmax_{j|x_{0}(j)\neq 0} \overline{\textbf{x}} \times (\textbf{-1}) \times \triangledown \textbf{C}_{i}$ \\
%%	\STATE $value\_dec=(\overline{\textbf{x}} \times (\textbf{-1}) \times \triangledown \textbf{C}_{i})[i_{dec}]$ \\
%%	\ENDIF \\
%%	
%%	\IF {$value\_inc \geq  value\_dec$} \;
%%	\WHILE { $\triangledown \textbf{C}_{i}(i_{inc}) > 0 $ and $\overline{\textbf{x}}(i_{inc})<1$} \;
%%	\STATE $\overline{\textbf{x}}[i_{inc}]+=\tau$ \\
%%	\ENDWHILE \\
%%	\ELSE \;
%%	\WHILE { $\triangledown \textbf{C}_{i}(i_{dec}) < 0 $ and $\overline{\textbf{x}}(i_{dec})>0$} \;
%%	\STATE $\overline{\textbf{x}}[i_{dec}]-=\tau$ \\
%%	\ENDWHILE \\
%%	\ENDIF \\
%%	\ENDIF \\
%%	\STATE $t=t+1$ \\
%	\ENDWHILE \\
%%	
%	\RETURN $\overline{\textbf{x}}-\textbf{x}$. \\
%
%    % \WHILE { $\textbf{F}(\textbf{x}_{i}) \neq l_{t}$ and $i \leq maxiter$} \;
%	% \STATE $i_{max1}=\argmax_{j} (\textbf{1}-\textbf{x}_{i})\times \triangledown \textbf{F}_{l_{t}}$ \\
%	% \STATE $value=(\textbf{1}-\textbf{x}_{i}) \times \textbf{F}_{l_{t}}[i_{max1}]$ \\	
%	% \STATE $i_{decrease}=\argmax_{j} \textbf{x}_{i} \times (\textbf{-1}) \times \triangledown \textbf{F}_{l_{t}}$ \\
%	% \IF {$value \geq  \textbf{x}_{i} \times (-\textbf{1})\times \triangledown \textbf{F}_{l_{t}}[i_{decrease}]$} \;
%	% %\STATE $i_{max}=i_{max1}$ \\
%	% \STATE $\textbf{r}_{i}[i_{max1}]$=1.0 \\
%	% \ELSE \;
%	% %\STATE $i_{max}=i_{max2}$ \\
%	% \STATE $\textbf{r}_{i}[i_{max2}]$=0.0 \\
%	% \ENDIF \\
%	% \STATE $\textbf{x}_{i+1}=\textbf{x}_{i}+\textbf{r}_{i}$ \\
%	% \STATE $i=i+1$ \\
%    % \ENDWHILE \\
%    % \RETURN $\widehat{\textbf{r}}=\sum_{i}\textbf{r}_{i}$. \\
%\end{algorithmic}
%"
429,1805.04735,"[h] %\DontPrintSemicolon
\KwIn{$N$ unlabeled samples, $\{\mathbf{x}_n\}_{n=1}^N$, where $\mathbf{x}_n\in \mathbb{R}^d$\;
\hspace*{9mm} $M$, the maximum number of labeled samples to query\;
\hspace*{9mm} $\gamma$, the threshold for outlier identification}
\KwOut{The regression model $f(\mathbf{x})$.}
\tcp{Identify the outliers}
$S=\{\mathbf{x}_n\}_{n=1}^N$\;
$hasOutliers$=True\;
\While{$hasOutliers$}{
Perform $k$-means clustering on $S$ to obtain $d$ clusters, $C_i$, $i=1,...,d$\;
Set $p_i=|C_i|$\;
$hasOutliers$=False\;
\For{$i=1,...,k$}{
\If{$p_i\le \max(1,\gamma N)$}{
$S=S\setminus C_i$\;
$hasOutliers$=True\;}}}
\tcp{Initialize $d$ labeled samples}
\For{$i=1,...,d$}{
Select the sample closest to the centroid of $C_i$ to label\;}
\tcp{End initialization}
\For{$m=d+1,...,M$}{
{Perform a baseline ALR (e.g., QBC or EMCM) on $S$ to select a sample for labeling\;}}
Construct the regression model $f(\mathbf{x})$ from the $M$ labeled samples.
\caption{The EBMALR algorithm, when the batch size is 1.} \label{alg:EAL}
"
430,1805.04735,"[h] %\DontPrintSemicolon
\KwIn{$N$ unlabeled samples, $\{\mathbf{x}_n\}_{n=1}^N$, where $\mathbf{x}_n\in \mathbb{R}^d$\;
\hspace*{9mm} $M$, the maximum number of labeled samples to query}
\KwOut{The regression model $f(\mathbf{x})$.}
\tcp{Initialize $d$ labeled samples}
Perform $k$-means clustering on $\{\mathbf{x}_n\}_{n=1}^N$, where $k=d$\;
Select from each cluster the sample closest to its centroid, and query for its label\;
\tcp{End initialization}
\For{$m=d+1,...,M$}{
{Perform $k$-means clustering on $\{\mathbf{x}_n\}_{n=1}^N$, where $k=m$\;
Identify the largest cluster that does not already contain a labeled sample\;
\emph{Option 1}: Select the sample closest to the cluster centroid for labeling\;
\emph{Option 2}: Use QBC (Section~\ref{sect:QBC}) to select a sample from the cluster for labeling\;
\emph{Option 3}: Use EMCM (Section~\ref{sect:EMCM}) to select a sample from the cluster for labeling\;
\emph{Option 4}: Use GS (Section~\ref{sect:GS}) to select a sample from the cluster for labeling\; }}
Construct the regression model $f(\mathbf{x})$ from the $M$ labeled samples.
\caption{The proposed RD ALR algorithm, and its variations.} \label{alg:RDAL}
"
431,1805.04688,"[t]\small
	%	\caption{Learning of GM-LVeGs}
	%	\label{alg:learning}
	%	{\bf Input:} Training data $D$, number of epoches $e$, batch size $b$.
	%	\begin{algorithmic}[1]
	%		\State initialize productions $R$, weight functions $W$.
	%		\For{$1,\ldots,e$}
	%		\State $D$ = \text{shuffle}($D$)
	%		\For{i = $1,\ldots,|D|$}
	%		\State $T_i, \bw_i = D_i$
	%		\State calculate $\mathbb{E}_{P(t|T_i)}[f_r(t)]$ and $\mathbb{E}_{P(t|\bw_i)}[f_r(t)]$ 
	%		\State calculate gradients using Eq~\ref{eq:grad} for each $\Theta_r$
	%		\State cumulate the new gradients 
	%		\If{i \% b == 0}
	%		\State average gradients by $b$
	%		\State do gradient descent for $\Theta$
	%		\State reset all the cached gradients to zero
	%		\EndIf
	%		\EndFor
	%		\EndFor
	%	\end{algorithmic}
	%"
432,1709.02535,"
\caption{Mirror descent search}\algolabel{MDS}      
\begin{algorithmic}[1]
\Initialize{continuous functions: $p_0\left(\bm\omega\right):=p_\text{init.}\left(\bm\omega\right).$
}
\For{$k=1$ to $K$}
    \For{$i=1$ to $m$}
        \State Sample parameter $\bm\theta_{i}\sim p_{k-1}(\bm\omega)$.\oplabel{mds:sample}
        \State (Discretize $p_{k-1}$) $q_{k-1,i}=q(\bm\theta_{i})$.\oplabel{mds:calcq}
        \State (Evaluate) $J_{k-1,i}=J(\bm\theta_{i})$.\oplabel{mds:calcJ}
    \EndFor
    \State $\hat{\bm q}_k=\argmin_{\bm q\in\mathbb{R}^m}\left\{\langle\bm J_{k-1}, \bm q\rangle+\eta B_\phi\left(\bm q, \bm q_{k-1}\right)\right\}.$\oplabel{mds:update}
    \State Estimate continuous functions $p_k(\bm\omega)$ from $\hat{\bm q}_k$.\oplabel{mds:estimate}
\EndFor
\end{algorithmic}
"
433,1709.02535,"
\caption{Gaussian mirror descent search}\algolabel{GMDS}      
\begin{algorithmic}[1]
\Initialize{continuous Gaussian function: $p_0\left(\bm\omega\right):=p_\text{init.}\left(\bm\omega\right)$\\
               variance: $\bm\Sigma$.
}
\For{$k=1$ to $K$}
    \For{$i=1$ to $m$}
        \State Sample parameter $\bm\theta_{i} \sim p_{k-1}(\bm\omega)$.\oplabel{gmds:sample}
        \State (Discretize $p_{k-1}$) $q_{k-1,i}=q(\bm\theta_{i})$.\oplabel{gmds:calcq}
        \State (Evaluate) $J_{k-1,i}=J(\bm\theta_{i})$.\oplabel{gmds:calcJ}
    \EndFor
    \State $\hat{\bm q}_k=\argmin_{\bm q\in\mathbb{R}^m}\left\{\langle\bm J_{k-1}, \bm q\rangle+\eta\mathrm{KL}\left(\bm q, \bm q_{k-1}\right)\right\}.$\oplabel{gmds:update}
    \State Estimate the mean $\tilde{\bm\mu}_k$ from $\hat{\bm q}_k$.\oplabel{gmds:estimate}
    \State Generate continuous function $p_k(\bm\omega)$ from $\bm\mu_k$ and $\bm\Sigma$.\oplabel{gmds:generate}
\EndFor
\end{algorithmic}
"
434,1709.02535,"
\caption{Accelerated mirror descent search}\algolabel{AMDS}      
\begin{algorithmic}[1]
\Initialize{continuous functions: $p_0^{\tilde z}\left(\bm\omega\right):=p_\text{init.}^{\tilde z}\left(\bm\omega\right), p_0^{\tilde x}\left(\bm\omega\right):=p_\text{init.}^{\tilde x}\left(\bm\omega\right).$
%               vectors: $\bm q_0^{\tilde z}, \bm q_0^{\tilde x}$ by sampling from the continuous functions.\\
}
\For{$k=1$ to $K$}
    \State $p_{k}(\bm\omega)=\lambda_{k-1}p_{k-1}^{\tilde z}(\bm\omega)+\left(1-\lambda_{k-1}\right)p_{k-1}^{\tilde x}(\bm\omega), \text{with } \lambda_{k-1}=\frac{r}{r+(k-1)}$.\oplabel{amds:x}
    \For{$i=1$ to $m$}
        \State Sample parameter $\bm\theta_{i} \sim p_{k}(\bm\omega)$.\oplabel{amds:sample}
        \State (Discretize $p^{\tilde z}_{k-1}$) $q^{\tilde z}_{k-1,i}=p_{k-1}^{\tilde z}(\bm\theta_{i})$.\oplabel{amds:calcqz}
        \State (Discretize $p_{k-1}^{\tilde x}$) $q_{k-1,i}^{\tilde x}=p_{k-1}^{\tilde x}(\bm\theta_{i})$.\oplabel{amds:calcqx}
        \State (Evaluate) $J_{k-1,i}=J(\bm\theta_{i})$.\oplabel{amds:calcJ}
    \EndFor
    \State ${\bm q_{k}}=\lambda_{k-1}{\bm q^{\tilde z}_{k-1}+(1-\lambda_{k-1}){\bm q_{k-1}^{\tilde x}} }$
    \State $\hat{\bm q}_k^{\tilde z}=\argmin_{\bm q^{\tilde z}\in\mathbb{R}^m}\left\{\frac{(k-1)s}{r}\langle \bm J_{k-1}, \bm q^{\tilde z}\rangle+B_\phi\left(\bm q^{\tilde z}, \bm q^{\tilde z}_{k-1}\right)\right\}$\oplabel{amds:ztilde}
    \State $\hat{\bm q}_k^{\tilde x}=\argmin_{\bm q^{\tilde x}\in\mathbb{R}^m}\left\{\gamma s\langle \bm J_{k-1}, \bm q^{\tilde x}\rangle+R\left(\bm q^{\tilde x}, \bm q_{k}\right)\right\}$\oplabel{amds:xtilde}
    \State Estimate continuous functions $p_k^{\tilde z}(\bm\omega), p_k^{\tilde x}(\bm\omega)$ from $\hat{\bm q}_k^{\tilde z}, \hat{\bm q}_k^{\tilde x}$.\oplabel{amds:estimate}
\EndFor
\end{algorithmic}
"
435,1709.02535,"
\caption{Gaussian accelerated mirror descent search}\algolabel{GAMDS}      
\begin{algorithmic}[1]
\Initialize{continuous Gaussian functions: $p_0^{\tilde z}\left(\bm\omega\right):=p_\text{init.}^{\tilde z}\left(\bm\omega\right), p_0^{\tilde x}\left(\bm\omega\right):=p_\text{init.}^{\tilde x}\left(\bm\omega\right).$\\
               variance: $\bm\Sigma^{\tilde z}, \bm\Sigma^{\tilde x}.$
%               number of updates: $K$.
}
\For{$k=1$ to $K$}
    \State $p_{k}(\bm\omega)=\lambda_{k-1}p_{k-1}^{\tilde z}(\bm\omega)+\left(1-\lambda_{k-1}\right)p_{k-1}^{\tilde x}(\bm\omega), \text{with } \lambda_{k-1}=\frac{r}{r+(k-1)}$.\oplabel{gamds:x}
    \For{$i=1$ to $m$}
        \State Sample parameter $\bm\theta_{i} \sim p_{k}(\bm\omega)$.\oplabel{gamds:sample}
        \State (Discretize $p^{\tilde z}_{k-1}$) $q^{\tilde z}_{k-1,i}=p^{\tilde z}_{k-1}(\bm\theta_{i})$.\oplabel{gamds:calcqz}
        \State (Discretize $p_{k-1}^{\tilde x}$) $q_{k-1,i}^{\tilde x}=p_{k-1}^{\tilde x}(\bm\theta_{i})$.\oplabel{gamds:calcqx}
        \State (Evaluate) $J_{k-1,i}=J(\bm\theta_{i})$.\oplabel{gamds:calcJ}
    \EndFor
    \State ${\bm q_{k}}=\lambda_{k-1}{\bm q^{\tilde z}_{k-1}+(1-\lambda_{k-1}){\bm q_{k-1}^{\tilde x}} }$
    \State $\hat{\bm q}_k^{\tilde z}=\argmin_{\bm q^{\tilde z}\in\mathbb{R}^m}\left\{\frac{(k-1)s}{r}\langle \bm J_{k-1}, \bm q^{\tilde z}\rangle+\text{KL}\left(\bm q^{\tilde z}, \bm q^{\tilde z}_{k-1}\right)\right\}$\oplabel{gamds:ztilde}
    \State $\hat{\bm q}_k^{\tilde x}=\argmin_{\bm q^{\tilde x}\in\mathbb{R}^m}\left\{\gamma s\langle \bm J_{k-1}, \bm q^{\tilde x}\rangle+\mathrm{KL}_\varepsilon\left(\bm q^{\tilde x}, \bm q_{k}\right)\right\}$\oplabel{gamds:xtilde}
    \State Estimate the means $\bm\mu^{\tilde z}_k, \bm\mu^{\tilde x}_k$ from $\hat{\bm q}_k^{\tilde z}, \hat{\bm q}_k^{\tilde x}$.\oplabel{gamds:estimate}
    \State Generate continuous functions $p_k^{\tilde z}(\bm\omega), p_k^{\tilde x}(\bm\omega)$ from $\bm\mu^{\tilde z}_k, \bm\mu^{\tilde x}_k, \bm\Sigma^{\tilde z}, \bm\Sigma^{\tilde x}$.\oplabel{gamds:generate}
\EndFor
\end{algorithmic}
"
436,1709.02535,"[bth]
\caption{PI${}^\text{2}$ algorithm (see \cite{Theodorou2010Reinforcement} for details)}\algolabel{PI2}      
\begin{algorithmic}[1]
\Initialize{
    parameter vector: $\bm\theta_0:=\bm\theta_\text{init}$\\
    % semi-positive definite matrix: $\bm R$\\
    % immediate cost function: $r_t = q_t+\bm\theta^{\rm T}\bm R\bm\theta$\\
    % terminal cost: $\phi$\\
    % stochastic policy parameter: $\bm g_t^{\rm T}(\bm\theta+\bm\epsilon_t)$\\
    % basis function: $\bm g_t$\\
    % variance $\Sigma_\epsilon$ of mean-zero noise: $\bm\epsilon_t$.
}
\For{$k=1$ to $K$}
    \For{$i=1$ to $m$}
        \For{$t=0$ to $T-1$}
            \State Generate rollout from $\bm\theta_{k-1}+\bm\epsilon_{t,i}$
            \State Compute the projection matrix $\bm M_{t, k}=\frac{\bm R^{-1}\bm g_{t, k}\bm g^{\rm T}_{t, k}}{\bm g^{\rm T}_{t, k}\bm R^{-1}\bm g_{t, k}}$
            \State Evaluate $S\left(\bm\tau_{t, i}\right)=\phi+\sum^{T-1}_{j=t}q_{j, k}+\frac{1}{2}\left(\bm\theta_{k-1}+\bm M_{j, i}\bm\epsilon_{j,i}\right)^{\rm T}\bm R\left(\bm\theta_{k-1} +\bm M_{j,i}\bm\epsilon_{j, i}\right)$
            \State Compute the probability
            % $P\left(\bm\tau_{t, i}\right)=\frac{e^{-\frac{1}{\lambda}S\left(\bm\tau_{t,i}\right)}}{\sum^m_{j=1} \left\{ e^{-\frac{1}{\lambda}S\left(\bm\tau_{t,j}\right)} \right\} }$
            $P\left(\bm\tau_{t, i}\right)=\exp{\bigl(-\frac{1}{\lambda}S\left(\bm\tau_{t,i}\right)\bigr)}/Z$
            \State Compute time dependent differential parameter $\delta\bm\theta_{t}=\sum^m_{i=1}\left[ P\left(\bm\tau_{t,i}\right)\bm M_{t, i}\bm\epsilon_{t,i}\right]$
        \EndFor
        \State Compute time independent differential parameter $\delta\bm\theta=\frac{\sum^{T-1}_{j=0}\left(T-j\right) \delta\bm\theta_j}{\sum^{T-1}_{j=0}\left(T-j\right)}$
    \EndFor
    \State $\bm\theta_{k} = \bm\theta_{k-1} +\delta\bm\theta$
\EndFor
\end{algorithmic}
"
437,1709.02535,"
% \caption{PI${}^\text{2}$ from G-MDS}\algolabel{GMDSPI2}      
% \begin{algorithmic}[1]
% \Initialize{mean: $\bm\mu_0:=\bm\mu_\text{init}$\\
%             variance: $\bm\Sigma.$
% %             number of updates: $K$\\
% %             number of time steps: $N$\\
% }
% \For{$k=1$ to $K$}
%     \For{$i=1$ to $m$}
%         \State Sample parameter  $\bm\theta_{i}\sim\mathcal{N}\left(\bm\mu_{k-1}, \bm\Sigma\right)$.
%         \State (Evaluate) $J_{k-1,i}=J(\bm\theta_{i})$.
%     \EndFor
%     \State $\tilde{\bm\mu}_k=\tilde{\bm\mu}_{k-1}+\sum^m_{i=1}\left(\frac{\exp\left(-\eta  J_{k-1,i}\right)\bm\epsilon_{k-1,i}}{\sum^m_{j=1}\exp\left(-\eta J_{k-1,j}\right)}\right).$
% \EndFor
% \end{algorithmic}
% "
438,1709.02535,"[tbh]
% \caption{PI${}^\text{2}$}\algolabel{PI2}      
% \begin{algorithmic}[1]
% \Initialize{
%     parameter vector: $\bm\theta_0:=\bm\theta_\text{init}$\\
%     semi-positive definite matrix: $\bm R$\\
%     immediate cost function: $r_t = q_t+\bm\theta^{\rm T}\bm R\bm\theta$\\
%     terminal cost: $\phi$\\
%     stochastic policy parameter: $\bm g_t^{\rm T}(\bm\theta+\bm\epsilon_t)$\\
%     basis function: $\bm g_t$\\
%     variance $\Sigma_\epsilon$ of mean-zero noise: $\bm\epsilon_t$.
% }
% \For{$k=1$ to $K$}
%     \For{$i=1$ to $m$}
%         \For{$t=0$ to $T-1$}
%             \State Generate rollout from $\bm\theta_{k-1}+\bm\epsilon_{t,i}$
%             \State Compute the projection matrix $\bm M_{t, k}=\frac{\bm R^{-1}\bm g_{t, k}\bm g^{\rm T}_{t, k}}{\bm g^{\rm T}_{t, k}\bm R^{-1}\bm g_{t, k}}$
%             \State Evaluate $S\left(\bm\tau_{t, i}\right)=\phi+\sum^{T-1}_{j=t}q_{j, k}+\frac{1}{2}\sum^{T-1}_{j=t}\left(\bm\theta_{k-1}+\bm M_{j, i}\bm\epsilon_{j,i}\right)^{\rm T}\bm R\left(\bm\theta_{k-1} +\bm M_{j,i}\bm\epsilon_{j, i}\right)$
%             \State Compute the probability $P\left(\bm\tau_{t, i}\right)=\frac{e^{-\frac{1}{\lambda}S\left(\bm\tau_{t,i}\right)}}{\sum^m_{j=1} \left\{ e^{-\frac{1}{\lambda}S\left(\bm\tau_{t,j}\right)} \right\} }$
%             \State Compute time dependent differential parameter $\delta\bm\theta_{t}=\sum^K_{k=1}\left[ P\left(\bm\tau_{t,i}\right)\bm M_{t, i}\bm\epsilon_{t,i}\right]$
%         \EndFor
%         \State Compute time independent differential parameter $\delta\bm\theta=\frac{\sum^{T-1}_{j=0}\left(T-j\right) \delta\bm\theta_j}{\sum^{T-1}_{j=0}\left(T-j\right)}$
%     \EndFor
%     \State $\bm\theta_{k} = \bm\theta_{k-1} +\delta\bm\theta$
% \EndFor
% \end{algorithmic}
% "
439,1805.04680,"[t]
%\footnotesize
\caption{\label{alg:inference} Training procedure for \method. }
%The class $c$ has \textit{e}ntailment, \textit{c}ontradiction, and \textit{n}eutral. The rules $r$ has \textit{p}pdb, \textit{s}ick, \textit{w}ordnet, and \textit{h}and. $\textbf{X}$ and $\mathcal{Z}$ are original and generated data, respectively.}

\begin{algorithmic}[1]

\State \texttt{pretrain} discriminator $\mathbb{D}({\hat{\theta}})$ on $\textbf{X}$; 
\State \texttt{pretrain} generators $\gens_c (\hat{\phi})$ on $\textbf{X}$;
\For{ number of training iterations }
	\For{\texttt{mini-batch $B \leftarrow \orig$ }}
	   \State \texttt{generate} examples from \gen
       \Indent
       \State $\synth_{G}$$\Leftarrow$$\gen(B ; \phi)$,
       \EndIndent
       %\State \texttt{generate} examples from \genr
       %\Indent
       %\State $\synth_{R}$$\Leftarrow$$\genr(B ; \gamma)$,
       %\EndIndent
      %\State \texttt{generate} 1st-order sentences:
      %\Indent 
      %\State $\mathcal{Z}_{G}^{1}$$\Leftarrow$$\mathbb{G}(\mathcal{X}_p ; \phi)$, $\mathcal{Z}_{R}^{1}$ $\Leftarrow$$\mathbb{R}(\mathcal{X};\gamma)$;
      %\EndIndent
      %\State \texttt{generate} 2nd-order sentences:
      %\Indent 
      %\State    $\mathcal{Z}_{G}^{2}$$\Leftarrow$$\mathbb{G}(\mathcal{Z}_{R}^{1};\phi)$, $\mathcal{Z}_{R}^{2}$ $\Leftarrow$ $\mathbb{R}(\mathcal{Z}_{G}^{1};\gamma)$;
      %\EndIndent

	  \State \texttt{balance} $\orig$ and $\synth_{G}$ s.t. $|\synth_G|$ $\leq \alpha |\orig|$
      \State \texttt{optimize} discriminator:
      \Indent 
        \State $\hat{\theta} = \argmin_{\theta} L_{\mathbb{D}} (\orig+\synth_{G}; \theta)$ 
      \EndIndent
      \State \texttt{optimize} generator:
      \Indent 
        \State $\hat{\phi} = \argmin_{\phi} L_{\gens} (\mathcal{Z}_{G}; L_\disc;\phi)$
        %\State $\hat{\gamma} = \argmin_{\phi} L_{\mathbb{R}} (\mathcal{Z}_{R}; \gamma)$        
      \EndIndent
%       \State \texttt{optimize} $\hat{\phi_{\mathcal{X},c}} = \argmin_{\phi} L_{\mathbb{G}_{\mathcal{X},c}} (\tilde{H}, H; \phi)$
%       \State \texttt{optimize} $\hat{\gamma} = \argmin_{\gamma} L_{\mathbb{R}} (\gamma) = L_{\mathbb{D}} (\mathcal{Z}_{\mathbb{R}} ; \gamma)$

    	\State Update $\theta \leftarrow \hat{\theta} ; \phi \leftarrow \hat{\phi} $
	\EndFor
\EndFor
\end{algorithmic}
"
440,1805.04604,"[t]
\caption{Dropout Perturbation
\label{alg:dropout}}
\begin{algorithmic}[1]
\small
\Require $q, a$: Input and its prediction
\INPTDESCB $\mathcal{M}$: Model parameters

\For{$i \gets 1,\cdots,F$}
\Let{$\hat{\mathcal{M}}^{i}$}{Apply dropout layers to $\mathcal{M}$}\hfill\AlgCommentInLine{Figure~\ref{fig:dropout_where}}
\State{Run forward pass and compute $\hat{p}( a | q ; \hat{\mathcal{M}}^{i} )$}
\EndFor
\State{Compute variance of $\{ \hat{p}( a | q ; \hat{\mathcal{M}}^{i} ) \}_{i=1}^{F}$}\hfill\AlgCommentInLine{Equation~\eqref{eq:dropout-uncertainty-seq}}
\normalsize
\end{algorithmic}
"
441,1805.04604,"[t]
\caption{Uncertainty Interpretation
\label{alg:bp}}
\begin{algorithmic}[1]
\small
\Require $q, a$: Input and its prediction
\Ensure $\{\hat{u}_{q_t}\}_{t=1}^{|q|}$: Interpretation scores for input tokens
\FUNCDESC $\mathsf{TokenUnc}$: Get token-level uncertainty

\AlgComment{Get token-level uncertainty for predicted tokens} \label{alg:line:init}
\Let{$\{u_{a_t}\}_{t=1}^{|a|}$}{$\mathsf{TokenUnc}(q,a)$}
\AlgComment{Initialize uncertainty scores for backpropagation}
\For{$t \gets 1,\cdots,|a|$}
\Let{Decoder classifier's output neuron}{$u_{a_t}$}
\EndFor \label{alg:line:init:end}
\AlgComment{Run backpropagation} \label{alg:line:bp}
\For{$m \gets$ neuron in backward topological order}
\AlgComment{Gather scores from child neurons}
\Let{$u_{m}$}{$\sum_{c \in \textrm{Child}(m)} {v_{m}^{c} u_{c}}$}
\EndFor \label{alg:line:bp:end}
\AlgComment{Summarize scores for input words} \label{alg:line:summ}
\For{$t \gets 1,\cdots,|q|$}
\Let{$u_{q_t}$}{$\sum_{c \in \mathbf{q}_t} {u_{c}}$}
\EndFor
\Let{$\{\hat{u}_{q_t}\}_{t=1}^{|q|}$}{normalize $\{u_{q_t}\}_{t=1}^{|q|}$} \label{alg:line:summ:end}
\normalsize
\end{algorithmic}
"
442,1805.04582,"
  \caption{Computation of the full conditional of $f_{n_kl}$}
  \label{alg:update_f}
\begin{algorithmic}[tb]
  \STATE{$\text{m}=0$ // \textit{initialise integer count for the sum in eq.\ \eqref{eq:conditional}}}
  \FOR{[n] in all tensor indices with slice $n_k$ fixed}
  \STATE // \textit{check relevance of $f_{n_kl}$ for $x_{[n]}$.}
  \FOR{n in [n]}
  \IF{$f_{nl} = 0$}
  \STATE // \textit{$f_{n_kl}$ has no relevance for $x_{[n]}$.}
  \STATE continue with next [n]
  \ENDIF
  \ENDFOR
  \STATE // \textit{check for explaining away.}
  \FOR{$l'\;\text{in}\;1,\ldots, L \text{ except } l$}
    \FOR{n in [n]}
      \IF{$f_{nl'}=0$}
        \STATE continue with next l'
      \ENDIF
    \STATE // $x_{[n]}$ \textit{is explained away.}
    \STATE continue (next [n])
  \ENDFOR
  \ENDFOR
  \STATE $\text{m} =  \text{m} + \tilde{x}_{nd} $
  \ENDFOR \STATE{$p(f_{n_kl}|.) = \left(1+\exp\left(-\lambda\cdot\tilde{f}_{n_kl}\cdot\text{m} \right)\right)^{-1}$}
  % \STATE{$\text{Draw}\;z_{nl}\;\text{from}\;\text{Bernoulli}\left(\sigma\left[\lambda\cdot \tilde{z}_{nl}\cdot\text{accumulator} \right]\right)$}
\end{algorithmic}
"
443,1604.03159,"[t]
	\caption{p-value computation of V-test for the RIM test}
	\label{algo_RIM_pvalue}
	\begin{algorithmic}
		\State \textbf{Input:} An $n_i \times n_j$ interconnection matrix $\widehat{\bC}_{ij}$
		\State \textbf{Output:} \text{p-value}$(i,j)$
		\State $\bx= \widehat{\bC}_{ij} \bone_{n_j}$~(\# of nonzero entries of each row in $\widehat{\bC}_{ij}$)
		\State $\by= n_j \bone_{n_i}-\bx$~(\# of zero entries of each row in $\widehat{\bC}_{ij}$)	
		\State $X=\bx^T \bx - \bx^T \bone_{n_i}$ and  $Y=\by^T \by - \by^T \bone_{n_i}$.
		\State $N=n_i n_j (n_j-1)$ and $V=\lb \sqrt{X} + \sqrt{Y} \rb^2$.
		\State Compute test statistic $Z=\frac{V-N}{\sqrt{2N}}$
		\State Compute \text{p-value}$(i,j)$$=2 \cdot \min \{ \Phi(Z),1-\Phi(Z) \}$
	\end{algorithmic}
"
444,1604.03159,"[t]
	\caption{Automated model order selection (AMOS) algorithm for spectral graph clustering (SGC)}
	\label{algo_automated_clustering}
	\begin{algorithmic}
		\State \textbf{Input:} a connected undirected weighted graph,  p-value significance level $\eta$, homogeneous and inhomogeneous RIM confidence interval parameters $\alpha$, $\alpha^\prime$
		\State \textbf{Output:} number of clusters $K$ and identity of $\{\hG_k\}_{k=1}^K$
		\State Initialization: $K=2$. Flag $=1$.
		\While{Flag$=1$}
		\State Obtain $K$ clusters $\{\hG_k\}_{k=1}^K$ via spectral clustering ($*$)
		\For{$i=1$ to $K$}
		\For{$j=i+1$ to $K$}
		\State Calculate p-value($i,j$) from Algorithm \ref{algo_RIM_pvalue}.
		\If{p-value($i,j$) $\leq \eta$}{~Reject RIM}
		\State Go back to ($*$) with $K=K+1$.
		\EndIf
		\EndFor
		\EndFor 
		\State Estimate $\hp$, $\hWbar$, $\{ \hp_{ij}\}$, and $\htLB$ specified in Sec. \ref{subsec_phase_transition_estimator}.
		\If{$\hp$ lies within the confidence interval in (\ref{eqn_spectral_multi_confidence_interval})}
		\State \# \emph{Homogeneous RIM phase transition test} \#
		\If{$\hp \cdot \hWbar$$  < \htLB$}				 
		Flag$=0$.
		\Else~ 
		Go back to ($*$) with $K=K+1$.		
		\EndIf    
		\ElsIf{$\hp$ does not lie within (\ref{eqn_spectral_multi_confidence_interval})}
		\State \# \emph{Inhomogeneous RIM phase transition test} \#
		\If{$\prod_{i=1}^K \prod_{j=i+1}^K F_{ij}\lb \frac{\htLB}{\hWbar},\hpij \rb \geq 1-\alpha^\prime$}	\\	
		~~~~~~~~~~~Flag$=0$.
		\Else
		~Go back to ($*$) with $K=K+1$.			
		\EndIf
		
		\EndIf
		\EndWhile
		\State Output  $K$ clusters $\{\hG_k\}_{k=1}^K$.		
	\end{algorithmic}
"
445,1712.03553,"[H]
	\begin{algorithmic}[1]
		\item[ ] {\textbf{inputs:}} (a) post-treatment predictions and observed values for control units, or $\widehat{{Y}_{it}}(1) \text { and } {Y}_{it}(0) \text { for } W_{i}=0 \text { and } t \geq T_{0}$, respectively; (b) estimated ATT averaged over the post-treatment period, $\hat{\tau}^{\textup{ATT}}$; (c) number of control units, $\text{J}$
		\item[ ] {\textbf{output:}} 95\% randomization confidence interval for $\hat{\tau}_t^{\textrm{ATT}}$
		\FOR{$l=1$ to $\mathcal{L}$} 
		\STATE Randomly sample without replacement constant treatment effect $\Delta_i$ \label{endpoints}
		\STATE Subtract $\Delta_i$ from $\hat{\tau}^{\textup{ATT}}$, resulting in test statistic $\delta_{\Delta_i}$
		\FOR{$q=1$ to $\mathcal{Q}$}
		\STATE Randomly sample without replacement which $\text{J}-1$ control units are assumed to be treated \label{placebo-treated}
		\STATE Calculate average placebo treated effect $\hat{\tau}_{iq}^{\text{ATT}}$
		\STATE Count the number of $\hat{\tau}_{iq}^{\textrm{ATT}}$  that are greater than or equal to $|\delta_{\Delta_i}|$  \label{counts}
		\ENDFOR
		\STATE Divide count obtained from Step \ref{counts} by $\mathcal{Q}$ to estimate two-sided $p$ values, $\hat{p}$
		\STATE Collect all values of $\delta_{\Delta_i}$ that yield $\hat{p}$ greater than $\alpha=0.05$
		\ENDFOR
	\end{algorithmic}
		\caption{Randomization confidence intervals}
\label{alg:randomization-ci}
"
446,1802.01334,"[H]
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{$K$, $\mathbf{S}^{[0]}$, $\mathbf{D}^{[0]}$, $\mathbf{\Delta}$, $\bphi$, $Iter$}
$\mathbf{B}=\mathbf{D}^{[0]}$\;
\For{$t=0$ \KwTo $Iter$}{
	$c_{S}$ equal (or bigger) than $\left\Vert \mathbf{B}^{T}\mathbf{B}\right\Vert $\;	
	$\mathbf{A}=\frac{1}{c_{S}}\left[\mathbf{B}^{T}\mathbf{X}+\left(c_{S}\mathbf{I}_{K}-\mathbf{B}^{T}\mathbf{B}\right)\mathbf{S}^{[t]}\right]$\;
	$\mathbf{W}\leftarrow w_{ij}=\frac{1}{\left|a_{ij}\right|+\varepsilon}$ with ${\scriptstyle(\varepsilon >0)}$\;
	\For{$i=1$ \KwTo $K$}{
		\If{$\left\Vert \mathbf{a}^{i}\right\Vert _{1,\mathbf{w}^{i}}>\phi_{i}$}{
			$\mathbf{a}^{i}=\mathcal{P}_{B_{\ell_{i}}[\mathbf{w}^{i},\phi_{i}]}(\mathbf{a}^{i})$\;	
		}
	}	
	$\mathbf{S}^{[t+1]}=\mathbf{A}$\;
	\vspace{3mm}
	$c_{D}$ equal (or bigger) than $\left\Vert \mathbf{A}\mathbf{A}^{T}\right\Vert $\;
	$\mathbf{B}=\frac{1}{c_{D}}\left[\mathbf{X}\mathbf{A}^{T}+\mathbf{D}^{[t]}\left(c_{D}\mathbf{I}_{K}-\mathbf{A}\mathbf{A}^{T}\right)\right]$\;
	\For{$i=1$ \KwTo $M$}{
		\If{$\left\Vert \mathbf{b}_{i}-\bdelta_{i}\right\Vert ^{2}> c_{\delta}$}{$\mathbf{b}_{i}=\frac{c_{\delta}^{\nicefrac{1}{2}}(\mathbf{b}_{i}-\bdelta_{i})}{\left\Vert \mathbf{b}_{i}-\bdelta_{i}\right\Vert }+\bdelta_{i}$\;
		}		
	}
	\For{$i=M+1$ \KwTo $K$}{
		\If{$\left\Vert \mathbf{b}_{i}\right\Vert ^{2}>c_{d}$}{
				$\mathbf{b}_{i}=\frac{c_{d}^{\nicefrac{1}{2}}}{\left\Vert \mathbf{b}_{i}\right\Vert }\mathbf{b}_{i}$\;		
		}
	}	
	$\mathbf{D}^{[t+1]}=\mathbf{B}$\;
}
\Output{$\mathbf{D}=\mathbf{D}^{[t+1]}$, $\mathbf{S}=\mathbf{S}^{[t+1]}$}
\caption{Information Assisted DL}
\label{Alg:Main}

{\footnotesize{where $\mathcal{P}_{B_{\ell_{1}}[\mathbf{w}^{i},\phi_{i}]}$ is the projection operator over the weighted $\ell_{1}$-norm ball, $B_{\ell_{1}}[\mathbf{w}^{i},\phi_{i}]=\{ \mathbf{x}\in\mathbb{R}^{N}\;|\;\left\Vert \mathbf{x}\right\Vert _{1,\mathbf{w}^{i}}\leqslant\phi_{i}\} $, of weights $\mathbf{w}^{i}$ and radius $\phi_{i}$. This projection operator onto the weighted $\ell_{1}$-norm ball is derived in closed form in \cite{Yan-wl1-norm}.}}
"
447,1805.04437,"[H]\scriptsize
% \DontPrintSemicolon
\SetKwRepeat{Do}{do}{while}
 \KwData{Query $\boldsymbol{d}_n^{\ell_1}$, Corpus  $C^{\ell_2}$, Embeddings $E^{\ell_1}, E^{\ell_2}$}
\lIf{\upshape \texttt{idf}}
{
    Apply idf weights on $\boldsymbol{d}_n^{\ell_1}$, $\forall \boldsymbol{d}_m^{\ell_2} \in C^{\ell_2}$ 
}
$L_1$-normalize $\boldsymbol{d}_n^{\ell_1}$ and $\forall \boldsymbol{d}_m^{\ell_2} \in C^{\ell_2}$\;

\For{ \upshape document $\boldsymbol{d}_m^{\ell_2} \in C^{\ell_2}$}{
  \upshape 
  $dist[m] = Wass(\boldsymbol{d}_n^{\ell_1}, \boldsymbol{d}_m^{\ell_2}, A, \lambda )$ ; \texttt{\# \scriptsize *$Wass$ solves Eq. \eqref{eq:optimization_problem1} or Eq. \eqref{eq:optimization_problem2}*} 
 }
 \KwResult{$\arg\min(dist):$ increasing list of distances between $\boldsymbol{d}_n^{\ell_1}$ and $\forall \boldsymbol{d}_m^{\ell_2} \in C^{\ell_2}$ } 
\caption{CLDR with Wasserstein distance.}\label{algo:cldr}
"
448,1712.07495,"[t]
\caption{Centralized Frank-Wolfe algorithm to solve \eqref{eq:fw-gen}}
\label{alg:fw}
\begin{algorithmic}[0]
\State \textbf{Input:} Initial point $W^0 \in \mathcal{D}$, number of iterations $T$
\For{$t = 0,\dots,T-1$}
    \State $S^* \leftarrow \arg\min_{S\in\mathcal{D}} \langle S, \nabla F(W^t) \rangle$
    \Comment solve linear subproblem
    \State $\gamma^t \leftarrow \frac{2}{t+2}$ (or determined by line search)
    \Comment step size
    \State $W^{t+1} \leftarrow (1-\gamma^t) W^t + \gamma^t S^*$
    \Comment update
\EndFor
\State \textbf{Output:} $W^T$
\end{algorithmic}
"
449,1712.07495,"[t]
\caption{Our distributed algorithm \dfw to solve \eqref{eq:tn-opt}}
\label{alg:dfw}
\begin{algorithmic}[1]
\State \textbf{Input:} Initial point $W^0 \in \mathcal{D}$, number of iterations $T$
\For{$t = 0,\dots,T-1$}
    \State \textbf{Each worker $j$:} $\nabla F_{j}(W^t) \leftarrow \sum_{i\in I_j}\nabla f_{i}(W^t)$
    \State \textbf{All workers:} draw the same $v_{0}\in\mathbb{R}^m$ uniformly on unit sphere
    \For{$k = 0,\dots,K(t)-1$}\Comment{distributed power method}\label{line:pmstart}
        \State \textbf{Each worker $j$:} send $u_{k+1,j}\leftarrow \nabla F_{j}(W^t)v_{k}$ to master
        \State \textbf{Master:} broadcast $u_{k+1}\leftarrow(\sum_{j=1}^{N}u_{k+1,j}) / \|\sum_{j=1}^{N}u_{k+1,j}\|$ 
        \State \textbf{Each worker $j$:} send $v_{k+1,j}\leftarrow \nabla F_{j}(W^t)^\top u_{k+1}$ to master
        \State \textbf{Master:} broadcast $v_{k+1}\leftarrow(\sum_{j=1}^{N}v_{k+1,j})/\|\sum_{j=1}^{N}v_{k+1,j}\|$
    \EndFor\label{line:pmend}
    \State $\gamma^t \leftarrow \frac{2}{t+2}$ (or determined by line search)
    \Comment step size
    \State \textbf{Each worker $j$:} $W^{t+1} \leftarrow (1-\gamma^t) W^t - \gamma^t \mu u_{K(t)}v_{K(t)}^\top $
    \Comment update
\EndFor
\State \textbf{Output:} $W^T$
\end{algorithmic}
"
450,1712.07495,"[t]
% \caption{Solving the linear subproblem \eqref{eq:tn-sub} using a distributed power method}
% \label{alg:power}
% \begin{algorithmic}[0]
% \State \textbf{Input:} Local gradient $\nabla f_{i}(W)\in\mathbb{R}^{d\times m}$ for each slave $i$, number of power iterations $K$
% \State \textbf{Master:} broadcast random vector $v_{0}\in\mathbb{R}^m$
% \For{k = 0,\dots,$K-1$}
%     \State \textbf{Slave $i$:} send $u_{k+1,i}\leftarrow \nabla f_{i}(W)v_{k}$ to master
%     \State \textbf{Master:} broadcast $u_{k+1}\leftarrow(\sum_{i=1}^{p}u_{k+1,i}) / \|\sum_{i=1}^{p}u_{k+1,i}\|$ 
%     \State \textbf{Slave $i$:} send $v_{k+1,i}\leftarrow \nabla f_{i}(W)^\top u_{k+1}$ to master
%     \State \textbf{Master:} broadcast $v_{k+1}\leftarrow(\sum_{i=1}^{p}v_{k+1,i})/\|\sum_{i=1}^{p}v_{k+1,i}\|$
% \EndFor
% \State \textbf{Output:} Slaves hold approximate solution $\hat{D} \leftarrow -\mu u_{K}v_{K}^\top $
% \end{algorithmic}
% "
451,1805.04049,"[t]
\caption{Parameter server with synchronized SGD}
\small
\begin{algorithmic}
\State \textbf{Server executes:} 
\Indent
\State Initialize $\params_0$
\For{$t=1$ to $T$}
\For{each client $k$}
\State $g_t^k\gets$\textbf{ClientUpdate}($\params_{t-1}$)
\EndFor
\State $\params_t\gets \params_{t-1} - \eta \sum_k g_t^k$ \Comment{synchronized gradient updates}
\EndFor
\EndIndent
\State
\State \textbf{ClientUpdate}($\params$): 
\Indent
\State Select batch $b$ from client's data
\State \textbf{return} local gradients $\nabla L(b;\params)$
\EndIndent
\end{algorithmic}
\label{alg:ps}
"
452,1805.04049,"[t]
\caption{Federated learning with model averaging}
\small
\begin{algorithmic}
\State \textbf{Server executes:} 
\Indent
\State Initialize $\params_0$
\State $m \gets max(C \cdot K, 1)$
\For{$t=1$ to $T$}
\State $S_t \gets \text{(random set of m clients)}$
\For{each client $k \in S_t$}
\State $\params_t^k\gets$\textbf{ClientUpdate}($\params_{t-1}$)
\EndFor
\State $\params_t\gets\sum_{k} \frac{n^k}{n} \params_t^k$ \Comment{averaging local models}
\EndFor
\EndIndent
\State
\State \textbf{ClientUpdate}($\params$): 
\Indent
\For{each local iteration}
\For{each batch $b$ in client's split}
\State $\params \gets \params - \eta \nabla L(b;\params)$
\EndFor
\EndFor
\State \textbf{return} local model $\params$
\EndIndent
\end{algorithmic}
\label{alg:fl}
"
453,1805.04049,"[t]
\caption{Batch Property Classifier}
%
\small
\begin{algorithmic}
\State \textbf{Inputs:} Attacker's auxiliary data $D_\text{prop}^\text{adv}, D_\text{nonprop}^\text{adv}$
\State \textbf{Outputs:} Batch property classifier $f_\text{prop}$
\State $G_\text{prop}\gets\emptyset$\Comment{Positive training data for property inference}
\State $G_\text{nonprop}\gets\emptyset$\Comment{Negative training data for property inference}
\For{$i = 1$ to $T$}
\State Receive $\theta_t$ from server
\State Run \textbf{ClientUpdate}($\params_t$)
\State Sample $\bprop^\text{adv} \subset D_\text{prop}^\text{adv}, \bnonprop^\text{adv} \subset D_\text{nonprop}^\text{adv}$
\State Calculate $g_\text{prop}=\nabla L(\bprop^\text{adv}; \theta_t), g_\text{nonprop}=\nabla L(\bnonprop^\text{adv}; \theta_t)$
\State $G_\text{prop}\gets G_\text{prop}\cup \{g_\text{prop}\}$
\State $G_\text{nonprop}\gets G_\text{nonprop}\cup \{g_\text{nonprop}\}$
\EndFor
\State Label $G_\text{prop}$ as positive and $G_\text{nonprop}$ as negative
\State Train a binary classifier $f_\text{prop}$ given $G_\text{prop}, G_\text{nonprop}$
\end{algorithmic} 
\label{alg:train_infer}
"
454,1611.01600,"[h]
	\caption{Loss-Aware Binarization (LAB)
	for training a feedforward neural network.}\label{alg:3}
	\textbf{Input: }Minibatch 
	%of inputs and targets 
	$\{(\x_0^t,\y^t)\}$,  current full-precision weights $\{\w^t_l\}$,
	%_{l=1,\dots, L}$,  
	first moment $\{\m^{t-1}_l\}$,
	%_{l=1,\dots, L}$ 
	second moment $\{\v^{t-1}_l\}$,
	%_{l=1,\dots, L}$, 
	and learning rate $\eta^t$. 
	%\textbf{Output: }Updated binary weights $\{\hw_l^t\}$,
	%_{l=1,\dots, L}$, 
	%full-precision weights $\{\w^{t+1}_l\}$,
	%_{l=1,\dots, L}$, 
	%first moment $\{\m^{t}_l\}$,
	%_{l=1,\dots, L}$, 
	%second moment $\{\v^t_l\}$,
	%_{l=1,\dots, L}$, 
	%and learning rate $\eta^{t+1}$.
	\begin{algorithmic}[1]
		\STATE {\bf Forward Propagation}
		\FOR{$l=1$ to $L$}
		%		\STATE \quad {\bf Binarize weight in the $l$-th layer: } 	
		\STATE $\alpha^t_l =  \frac{\|\d^{t-1}_l \odot \w^t_l\|_1}{\|\d^{t-1}_l\|_1}$;
		\STATE $\b^t_l = \text{sign}(\w^t_l)$;
		%		\STATE \quad \quad $W_{b(l)}^i = \alpha^i_{(l)} B^i_{(l)}$
		\STATE rescale the layer-$l$ input: $\tilde{\x}^t_{l-1} = \alpha^t_l \x^t_{l-1}$;
		\STATE compute $\z^t_l$ with input  $\tilde{\x}^t_{l-1}$ and binary weight $\b^t_l$; 
		\STATE apply batch-normalization 
		%(for FNN and CNN) 
		and nonlinear activation to $\z^t_l$ to obtain $\x^t_l$; 
		\ENDFOR
		%		\STATE \quad Standard forward propagation using $\{ \alpha^i_{(l)}\}_{l=1,2,\cdots, L}$ and $\{B^i_{(l)}\}_{l=1,2,\cdots, L}$ .
		\STATE compute the loss $\ell$ using $\x^t_{L}$ and $\y^t$;
		%\STATE \COMMENT{Backward Propagation}
		\STATE {\bf Backward Propagation}
		\STATE initialize output layer's activation's gradient $\frac{\partial \ell}{\partial \x^t_{L}}$;
		\FOR{$l=L$ to $2$}
		\STATE compute $\frac{\partial \ell}{\partial \x^t_{l-1}}$ using $\frac{\partial
			\ell}{\partial \x^t_l}$, $\alpha^t_l$ and $\b^t_l$;
		\ENDFOR
		%		\STATE \quad Standard back propagation and compute gradients $\{\nabla L(W_{b(l)}^i)\}_{l=1,2,\cdots, L}$
		%\STATE \COMMENT{Update parameters using Adam}
		\STATE {\bf Update parameters using Adam}
		\FOR{$l=1$ to $L$}
		\STATE compute gradients $\nabla_l \ell(\hw^t)$ using $\frac{\partial \ell}{\partial \x^t_l}$ and $\x^t_{l-1}$;
		\STATE update first moment $\m^t_l = \beta_1 \m^{t-1}_l + (1-\beta_1)\nabla_l
		\ell(\hw^t)$;
		\STATE update second moment $\v^t_l = \beta_2 \v^{t-1}_l + (1-\beta_2)(\nabla_l \ell(\hw^t) \odot \nabla_l \ell(\hw^t))$;
		\STATE compute unbiased first moment $\hat{\m}^t_l = \m^t_l/(1-\beta_1^t)$;
		\STATE compute unbiased second moment $\hat{\v}^t_l = \v^t_l/(1-\beta_2^t)$;
		\STATE compute current curvature matrix $\d^t_l = \frac{1}{\eta^t}\left(\epsilon \bm{1}+\sqrt{\hat{\v}^t_l}
		\right)$;
		%\STATEcompute current curvature matrix = \frac{1}{\eta^i}(\lambda I + \sqrt{\hat{V}^i_{(l)}})$.
		\STATE update full-precision weights $\w^{t+1}_l = \w^t_l -  \hat{\m}^t_l \oslash \d^t_l$;
		\STATE update learning rate $\eta^{t+1} = \text{UpdateRule} (\eta^t, t+1)$;
		\ENDFOR
	\end{algorithmic}
"
455,1805.03901,"[H]\label{al:LCBNN_opt}
\caption{LCBNN optimisation}\label{al:LCBNN_opt}
\begin{algorithmic}[1]\label{al:LCBNN_opt}
  \scriptsize
  \STATE Given dataset $\mathcal{D} = \{\mathbf{X},\mathbf{Y}\}$, utility function $u(\mathbf{h},\mathbf{y})$ and set of all possible labels $\mathcal{C}$
\STATE Define learning rate schedule $\eta$  
  \STATE Randomly initialise weights $\boldsymbol{\omega}$ 
  \REPEAT
  \STATE Sample $S$ index set of training examples
  \FOR{$i \in S$}
  	\FOR{$t$ from $1$ to $T$}
  	\STATE Sample Bernouilli distributed random variables $\boldsymbol{\epsilon}^t \sim p(\boldsymbol{\epsilon})$ \COMMENT{for each each $\mathbf{x}_i$ we sample T dropout masks $\boldsymbol{\epsilon}^t$}
  	\STATE $\mathbf{y}_i^t =  \mathbf{f}^{g(\boldsymbol{\omega},\boldsymbol{\epsilon}^t)}(\mathbf{x}_i)$ \COMMENT{Perform a stochastic forward pass with the sampled dropout mask $\boldsymbol{\epsilon}^t$ and $\mathbf{x}_i$}
  	\ENDFOR 
  \STATE $\mathbf{h}_i^* \leftarrow \underset{\mathbf{h} \in \mathcal{H}}{\mathrm{argmax}}\ \frac{1}{T}\sum_t u(\mathbf{h},\mathbf{y}_i^t)$ \COMMENT{Choose class $\mathbf{h} = \mathbf{c}\in \mathcal{C}$ which maximises average utility}
 \ENDFOR
  \STATE Calculate the derivative w.r.t. $\boldsymbol{\omega}$:
  \begin{equation*}
  \begin{split}
  \nabla\boldsymbol{\omega} \leftarrow & -\frac{1}{T}\sum_{i \in S} \frac{\partial}{\partial \boldsymbol{\omega}}\log p\left(\mathbf{y}_i \mid \mathbf{f}^{g(\boldsymbol{\omega},\boldsymbol{\epsilon}_i)}(\mathbf{x}_i)\right)\\
  & \qquad + \frac{\partial}{\partial \boldsymbol{\omega}} KL\left(q(\boldsymbol{\omega})\mid\mid p(\boldsymbol{\omega})\right)\\
  & \qquad - \frac{1}{T}\sum_{i \in S}\frac{\partial}{\partial \boldsymbol{\omega}}\log \mathcal{G}\left(\mathbf{h}^*_i \mid \mathbf{x}_i,\boldsymbol{\omega}\right)
\end{split}
  \end{equation*}
with $\boldsymbol{\epsilon}_i \sim p(\boldsymbol{\epsilon})$ a newly sampled dropout mask for each $i$.
 \STATE Update $\boldsymbol{\omega}$: 
 $\boldsymbol{\omega} \leftarrow \boldsymbol{\omega} + \eta \nabla\boldsymbol{\omega}$
 
 
 \UNTIL{$\boldsymbol{\omega}$ has converged}
\end{algorithmic}\label{al:LCBNN_opt}
"
456,1804.02528,"
    \caption{Pseudocode for training an example GAN network. Comments point to relevant individuals in the \name{} model (Figure \ref{fig:GAN-train}).}
  \label{alg:gan-train}
\begin{verbatim}
DO UNTIL convergence:  # gan_session
   FOR i = 1..5:  # gan_trainloop
      train(Discriminator, train_data)  # gan_discriminate_mnist
      gen_output := compute_out(Generator, random_noise)  # gan_gen_fpass
      train(Discriminator, gen_output)  # gan_discriminate_generatorout
   END
   train(GAN, random_noise)  # gan_generatorstep
END
\end{verbatim}
"
457,1804.02528,"
    \caption{Pseudocode for  training an example Adversarial Autoencoder. The \name{} model is shown in Figure \ref{fig:AAE-train}.}
  \label{alg:aae-train}
\begin{verbatim}
DO UNTIL convergence:  # aae_session
   train(Autoencoder, train_data)  # aae_autoencoder_step
   style := compute_out(StyleEncoder, train_data)  # aae_style_forward
   label := compute_out(LabelEncoder, train_data)  # aae_label_forward
   train(StyleDiscriminator, guassian)     # aae_styledis_noise_step
   train(LabelDiscriminator, categorical)  # aae_labeldis_noise_step
   train(StyleDiscriminator, style)  # aae_styledis_encodings_step
   train(LabelDiscriminator, label)  # aae_labeldis_encodings_step
   train(StyleGAN, train_data) # aae_stylegen_step
   train(LabelGAN, train_data) # aae_labelgen_step
END
\end{verbatim}
"
458,1805.03887,"[H]

	\label{alg:captree}
   	\caption{\CARtree~building}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    \SetKwProg{Fn}{Function}{}{}

	\Input{A transaction DB labeled with classes - $DB$}
	\Input{A minimum support threshold - $minsup$}
	\Output{A \CARtree}
	\BlankLine

	Scan the DB once.
    Collect $L$, the list of frequent items ($support >= minsup$).
    Sort $L$ by decreasing $IG$ and filter out items with $IG \leq 0$. \label{alg:captree:1pass}
    
    Create the root of a \CARtree~$T$ and label it as \emph{null}. \label{alg:captree:2pass}
    
    \For(){each labeled transaction $t$}
    {
    select only the items in $t$ that appear in $L$ and sort them according to the order in
    $L$, obtaining $t'$ \label{alg:captree:selectitems}
    
    call insert($t'$, $T$) \label{alg:captree:callinsert}
    } \label{alg:captree:2passend}
    
    \BlankLine
	\Fn{insert (transaction $t$, node $T$)}
    {
    $h$ = first item of $t$
    
    \eIf{$T$ has a child $T'$ s.t. $T'.id = h.id$ \label{alg:captree:updatestart}} 
    {
    $T'$.freqs[t.class]+=1
    }{
    create a new node $T'$
    
    init $T'$.id = h.id and $T'$.freqs to an array of zeros
    
    $T'$.freqs[$t$.class]+=1
    
    $T'$.parent = $T$
    
    update the header table \label{alg:captree:headertable}
   }\label{alg:captree:updateend}
    $t'$ = $t \backslash h$
    
    \If{$t'$ is not empty}{insert($t'$, $T'$)}
    
    }

%
%

%
%
%
%
%
%
%
%

%
  "
459,1805.03887,"[h]
   	\caption{\CARgrowth}
    \label{alg:capgrowth}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    \SetKwProg{Fn}{Function}{}{}


	\Input{a \CARtree~}
    \Input{A minimum support threshold - $\textrm{minsup}$}
	\Input{A minimum confidence threshold - $\textrm{minconf}$}
    \Input{A minimum chi2 threshold - $\textrm{minchi2}$}
	\Output{A list of CARs}
	\BlankLine
    rules = $\emptyset$

    \For{each child $T$ of \CARtree.root}{

    rules += extract($T$)
    }
    
    return rules
    \BlankLine
	\Fn{extract(node T) \label{alg:capgrowth:extract}}
    {
    rules = $\emptyset$
    
    \If(//negative Information Gain: do not generate any rule){$IG(T) <= 0$}{
    
    return $\emptyset$ \label{alg:capgrowth:ig0}}
    \If(//pure node: try to generate a rule){$Gini(T) == 0$}{return generateRule($T$) \label{alg:capgrowth:gini0}}
    \For{each child $T'$ of $T$ \label{alg:capgrowth:childstart}}
    {
    rules += extract($T'$)
    }
    \If(//none of the children has produced a rule: try to generate a rule){rules is $\emptyset$}
    {
    return generateRule($T$)
    }\label{alg:capgrowth:childend}
    return rules
    }
    \BlankLine
    \Fn{generateRule(node T) \label{alg:capgrowth:generate}}
    {
    consequent = class with highest value in $T$.freqs[]
    
    antecedent = set of items in the path from $T$ to \CARtree.root
    
%
    
%
    
%
%
%
    
%
%
	tree = \CARtree~conditioned by the items in antecedent \label{alg:capgrowth:captree}

    freqs = tree.root.freqs \label{alg:capgrowth:freqs}
    
    sup = freqs[consequent] / totCount \label{alg:capgrowth:sup}
    
    supAntecedent = freqs.sum / totCount
    
    from sup, supAntecedent and the global frequencies of the classes computed in the first pass of Algorithm \ref{alg:captree} compute support, confidence and $\chi^2$ for the generated rule: antecedent $\Rightarrow$ consequent \label{alg:capgrowth:computerule}
    
    \If{$\textrm{sup} < \textrm{minsup}$ or $\textrm{conf} < \textrm{minconf}$ or $\chi^2 < \textrm{minchi2}$ \label{alg:capgrowth:thresholds}}{return $\emptyset$}
    return rule \label{alg:capgrowth:end}
    }
  "
460,1805.03887,"[H]

	\label{alg:compression}
   	\caption{Model consolidation}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    \SetKwProg{Fn}{Function}{}{}

	\Input{A list of models - $models$}
	\Output{A single model, as a list of CARs}
	\BlankLine

%
model = $\emptyset$

\For{each m in models}{
model = merge(model, m) \label{alg:line:reduce}
}

return model

\BlankLine

\Fn{merge(model m1, model m2)}{
m = new model

rules = m1.rules $\cup$ m2.rules \label{alg:line:union}

%
gr = group rules by same antecedent and consequent

\For{each group of rules $i$ in gr}{
%
m = m $\cup$ aggregate($i$) \label{alg:line:aggr}
}



return m
}

\Fn{aggregate(rules)}{
rule = new rule

r = rules.first

(rule.antecedent, rule.consequent) = (r.antecedent, r.consequent)

supports = $\bigcup_{r \in \emph{rules}}$r.support

confs = $\bigcup_{r \in \emph{rules}}$r.confidence

chis = $\bigcup_{r \in \emph{rules}}$r.chi2

(rule.support, rule.confidence, rule.chi2) = \emph{g}(supports, confs, chis) \label{alg:line:somefun}

return rule
}

\Fn{\emph{g}(supports, confs, chis)}{
return (max(supports), max(confs), max(chis)) \label{alg:compression:g}

}

  "
461,1710.10770,"[H]
  \caption{Euclidean Frank-Wolfe without line-search}
  \label{alg.fw}
  \begin{algorithmic}[1]
    \State Initialize with a feasible point $x_0 \in \Xc \subset \reals^n$
    \For{$k=0,1,\ldots$}
      \State Compute $z_k \gets \argmin_{z \in \Xc} \ip{\nabla\phi(x_k)}{z-x_k}$
      \State Let $s_k \gets \frac{2}{k+2}$
      \State Update $x_{k+1} \gets (1-s_k)x_k + s_kz_k$
    \EndFor
  \end{algorithmic}
"
462,1710.10770,"[htbp]
  \caption{Riemannian Frank-Wolfe (\rfw) for g-convex optimization}
  \label{alg.rfw}
  \begin{algorithmic}[1] 
     \State Initialize $x_0 \in \Xc \subseteq \mathcal{M}$; assume access to the geodesic map $\gamma: [0,1]\to \Mc$
     \For {$k=0,1,\dots$}
        \State $z_k \gets \argmin_{z \in \Xc}\ \ip{\grad\phi(x_k)}{\Exp_{x_k}^{-1}(z)}$
        \State Let $s_k \gets \frac{2}{k+2}$
        \State $x_{k+1} \gets \gamma(s_k)$, where $\gamma(0)=x_k$ and $\gamma(1)=z_k$
     \EndFor
   \end{algorithmic}
"
463,1710.10770,"[H]
  \caption{FW for fast Geometric mean (GM)/ Wasserstein mean (WM)}
  \label{alg.means}
  \begin{algorithmic}[1]
     \State $(A_1,\ldots,A_N)$, $\bm{w} \in \rplus^N$
     \State $\bar{X} \approx \argmin_{X > 0} \nlsum_iw_i \riem^2(X,A_i)$
     \State $\beta = \min_{1 \leq i \leq N} \lambda_{{\rm min}} (A_i)$
     \For {$k=0,1,\dots$}
     	\State Compute gradient.
        \State \hskip12pt GM: $\nabla\phi(X_k) = X_k^{-1}\bigl(\nlsum_iw_i \log(X_kA_i^{-1})\bigr)$
        \State \hskip12pt WM: $\nabla\phi(X_k) =  \sum_i w_i \left(I - (A_i X_k )^{-1/2} A_i \right)$
        \State Compute $Z_k$:
        \State  \hskip12pt GM: $Z_k \gets \argmin_{H \preceq Z \preceq A}\ip{X_k^{1/2}\nabla
           \phi(X_k)X_k^{1/2}}{\log(X_k^{-1/2}ZX_k^{-1/2})}$
         \State \hskip12pt WM: $Z_k \gets \argmin_{\beta I \preceq Z \preceq A}\ip{X_k^{1/2}\nabla
           \phi(X_k)X_k^{1/2}}{\log(X_k^{-1/2}ZX_k^{-1/2})}$
        \State Let $\alpha_k \gets \frac{2}{k+2}$.
        \State Update $X$:
        \State \hskip12pt $X_{k+1} \gets X_k \gm_{\alpha_k}Z_k$
     \EndFor
     \State \textbf{return} $\bar{X}=X_k$
   \end{algorithmic}
 "
464,1710.10770,"[H]
  \caption{RFW for Procrustes problem}
  \label{alg.procrustes}
  \begin{algorithmic}[1]
     \State Input: $A, B \in \mathbb{R}^{n \times k}$.
     \State Assume access to the geodesic map $\gamma(X,Z;\alpha)$ (Eq.~\ref{eq:so-map}).
     \State $\bar{X} \approx \argmin_{X \in SO(n)} - \trace \left( X^T A B^T \right)$
     \State Precompute $\nabla \phi (X) = - AB^T$.
     \For {$k=0,1,\dots$}
      \State Compute gradient: $\grad \; \phi(X_k) = X_k \cdot {\rm skew} \left(	X_k^T \nabla \phi(X_k)	\right)$.
       \State Compute $Z_k$ via Riemannian ``linear'' oracle:
        \State  \hskip12pt $ Z_k \gets \argmin_{Z \in T_{X_k}SO(n)} \; \ip{\grad \; \phi(X_k)}{ \log(X_k^{-1} Z_k)}$
        \State Let $\alpha_k \gets \frac{2}{k+2}$.
        \State Update $X_k$ as
        \State \hskip12pt $X_{k+1} \gets \gamma(X_k, Z_k; \alpha_k)$.
     \EndFor
     \State \textbf{return} $\bar{X}=X_k$.
   \end{algorithmic}
 "
465,1710.10770,"[H]
  \caption{\efw for fast Geometric mean}
  \label{alg.efw}
  \begin{algorithmic}[1]
     \State $(A_1,\ldots,A_N)$, $\bm{w} \in \rplus^N$
     \State $\bar{X} \approx \argmin_{X > 0} \nlsum_iw_i \riem^2(X,A_i)$
     \State $\beta = \min_{1 \leq i \leq N} \lambda_{{\rm min}} (A_i)$
     \For {$k=0,1,\dots$}
     	\State Compute gradient: $\; \nabla\phi(X_k) = X_k^{-1}\bigl(\nlsum_iw_i \log(X_kA_i^{-1})\bigr)$
        \State Compute $Z_k$: $\; Z_k \gets \argmin_{H \preceq Z \preceq A}\ip{\nabla\phi(X_k)}{Z-X_k}$
        \State Let $\alpha_k \gets \frac{2}{k+2}$.
        \State Update $X$: $\; X_{k+1} \gets X_k + \alpha_k(Z_k - X_k)$.
     \EndFor
     \State \textbf{return} $\bar{X}=X_k$
   \end{algorithmic}
 "
466,1805.03586,"[!htbp]
\SetAlgoLined
\caption{Policy Optimization with Second-Order Advantage Information (POSA)}
\KwIn{number of iterations $N$, number of value iterations $M_w$, batch size $B$, number of subspaces $K$, initial policy parameter $\theta$, initial value and advantage parameters $w$ and $\mu$\;}
\KwOut{Policy optimal parameter $\theta$ }
\For{each iteration $n$ in $[N]$}{
Collect a batch of trajectory data  $\{s_t^{(i)}, a_t^{(i)},r_t^{(i)}\}_{i=1}^B$ \;
\For{$M_\theta$ iterations}{
	Update $\theta$ by one SGD step using PPO with ASDG in Theorem \eqref{theorem1}\;
    }
\For{$M_w$ iterations}{
	Update $w$ and $\mu$ by minimizing $||V^w(s_t)-R_t||_2^2$ and $||\hat{A}(s_t,a_t)-A^\mu(s_t,a_t)||_2^2$ in one SGD step \;
    }
Estimate $\hat{A}(s_t,a_t)$ using $V^w(s_t)$ by GAE \eqref{gae}\;
Calculate the action subspace partition $a_{(k)}$ based on the absolute Hessian $|w_2(s) w_2(s)^T|$ by the evolutionary clustering algorithm\;
}
\label{algo}
"
467,1805.03553,"
	\tiny
	\caption{ \tiny Adversarial Loss Landscape \\
	\textbf{Requires:} \\
	$~$\hspace{\algorithmicindent}$\theta\in \mathbb{R}^{p}$: model parameters with $\theta^{(j)}\in \mathbb{R}^{p_j}$ as \\
	$~$\hspace{\algorithmicindent}$~$\hspace{\algorithmicindent} the  $j^{th}$ layer's parameters\\
	$~$\hspace{\algorithmicindent}$D^{bon}$: benign PEs dataset \\
	$~$\hspace{\algorithmicindent}$D^{mal}$: malicious PEs dataset \\
	$~$\hspace{\algorithmicindent}$D^{adv}$: adversarial malicious PEs dataset \\
	$~$\hspace{\algorithmicindent}$(\alpha_{min}, \alpha_{max})$: range of parameter $\alpha\in \mathbb{R}$\\
		$~$\hspace{\algorithmicindent}$(\beta_{min}, \beta_{max})$: range of parameter $\beta\in \mathbb{R}$
	}
	\label{loss_landscape_code}
	\begin{algorithmic}[1]
		\Statex {// filter-wise normalization~\citep{filterwise}}
		\For{each layer $j$ of the model's layers}
			\State $\delta^{(j)}_i, \eta^{(j)}_i \sim \mathcal{N}(0, 1)\;, \forall i \in [p_j]$
			\State Scale $\delta_i$ and $\eta_i$ to have the same $\ell_2$-norm as $\theta^{(j)}$
			\Statex  $$\delta^{(j)} \gets \frac{\delta^{(j)}}{||\delta^{(j)}||}  ||\theta^{(j)}|| \;,\;\eta^{(j)} \gets \frac{\eta^{(j)}}{||\eta^{(j)}||}  ||\theta^{(j)}|| $$				
		\EndFor
		\Statex
		\Statex {// generate loss value at each $\alpha$ and $\beta$ location}
		\For{$\alpha$ in $[\alpha_{min},\ldots, \alpha_{max}]$}
			\For{$\beta$ in $[\beta_{min}, \ldots, \beta_{max}]$}
				\For{each layer $j$ of the model's layers}
					\State $\hat{\theta}^{(j)} \gets \theta^{(j)} + \alpha\delta^{(j)} + \beta\eta^{(j)}$
				\EndFor
				
				\State \texttt{loss} $\gets 0$
				\For{each sample $k$ in $D^{bon} \cup D^{mal} \cup D^{adv}$}
					\State \texttt{loss} $+= L(\hat{\theta}, \vx^{(k)}, y^{(k)})$
				\EndFor
				\State plot \texttt{avg(loss)} at coordinate ($\alpha$, $\beta$)
			\EndFor
		\EndFor
 	\end{algorithmic}
"
468,1805.03553,"
	\tiny
	\caption{\tiny \\ \hfill Decision Boundary with Self-Organizing Maps \\
		\textbf{Requires:} \\
		$~$\hspace{\algorithmicindent}$\theta\in \mathbb{R}^{p}$: model parameters with $\theta^{(j)}\in \mathbb{R}^{p_j}$ as \\
		$~$\hspace{\algorithmicindent}$~$\hspace{\algorithmicindent} the  $j^{th}$ layer's parameters\\
		$~$\hspace{\algorithmicindent}$D^{bon}$: benign PEs dataset \\
		$~$\hspace{\algorithmicindent}$D^{mal}$: malicious PEs dataset \\
		$~$\hspace{\algorithmicindent}$D^{adv}$: adversarial malicious PEs dataset\\
       $~$\hspace{\algorithmicindent}$m$: feature vector size\\
}
	\label{som_code}
	\begin{algorithmic}[1]
	    \Statex // instantiate and train self-organizing map \texttt{SOM}
		\State \texttt{SOM} $\gets$ a 2D grid of neurons with weight vectors $\mathbf{w}\in \mathbb{R}^m$
		\For{each sample $i$ in  $D^{bon} \cup D^{mal} \cup D^{adv}$}
			\State train \texttt{SOM} with $\vx^{(i)}$
		\EndFor
		\Statex
		\Statex // plot the model's decision map
		\For{each neuron $j$ in \texttt{SOM}}
			\Statex $~$\hspace{\algorithmicindent}// compute benign probability at the neuron's vector
			\State calculate $p(y=0|\vx=\mathbf{w}^{(j)}, \theta)$
			\State plot the probability value at $j$'s coordinates in \texttt{SOM}
		\EndFor
		\For{each sample $i$ in  $D^{bon} \cup D^{mal} \cup D^{adv}$}
		\Statex $~$\hspace{\algorithmicindent}// get the best matching neuron
		\State $j \gets \arg\min_{j^\prime \in \texttt{SOM}} ||\vx^{(i)} - \mathbf{w}^{(j^\prime)} ||$ 
		\State Mark $i$ at $j's$ coordinates in \texttt{SOM}
		\EndFor 	
	\end{algorithmic}
"
469,1805.03409,"
% \caption{Model training and optimization}\label{alg1:opt_autoencoder}
% \begin{algorithmic}[1]
% \Procedure{optimize\_autoencoder}{$DS_{trn},DS_{opt}$}
% \State $MSE_{current}\gets \infty$ % initiation
% \For{$epochs$ in (start=50, end=1000, step=50)}
% 	\For{$\eta$ in (start=0.0001, end=1, step=0.0001)}
%       \State $MSE_{previous}\gets$ $MSE_{current}$
%       \State $model \gets train\_autoencoder(DS_{trn}, epochs, \eta)$
%       \State $MSE_{current} \gets calc\_MSE(model, DS_{opt})$
%       \State $\Delta_{MSE} \gets MSE_{previous}-MSE_{current}$ 
%       \If {$\Delta_{MSE}>0$} 
%       		\State $epochs^*\gets epochs$
%             \State $\eta^*\gets \eta$
%             \State $model^*\gets model$
%            \Else 
%       \State $Quit$ 
%       \EndIf
%   \EndFor
% \EndFor
% \State \Return $epochs^*, \eta^*, model^*$
% \EndProcedure
% \end{algorithmic}
% "
470,1805.02400,"[!b]
 \KwData{Desired review context $C_\mathrm{input}$ (given as cleartext), NMT model}
 \KwResult{Generated review $out$ for input context $C_\mathrm{input}$}
%\State{
set $b=0.3$, $\lambda=-5$, $\alpha=\frac{2}{3}$,  $p_\mathrm{typo}$, $p_\mathrm{spell}$ \\
%}
$\log p \leftarrow \text{NMT.decode(NMT.encode(}C_\mathrm{input}\text{))}$ \\
out $\leftarrow$ [~] \\
$i \leftarrow 0$ \\
$\log p \leftarrow \text{Augment}(\log p$, $b$, $\lambda$, $1$, $[~]$, 0)~~~~~~~~~~~~~~~ |~random penalty~\\ 
\While{$i=0$ or $o_i$ not EOS}{ 
$\log \Tilde{p} \leftarrow \text{Augment}(\log p$, $b$, $\lambda$, $\alpha$, $o_i$, $i$)~~~~~~~~~~~ |~start \& memory penalty~\\ 
%$\log \Tilde{p} \leftarrow \text{Augment}(\log p$, $b$, $\lambda$, $\alpha$, $o_i$) \\
$o_i \leftarrow$ \text{NMT.beam}($\log \Tilde{p}$, out) \\
out.append($o_i$) \\
$i \leftarrow i+1$
}%\EndFor
\text{return}~$\text{Obfuscate}$(out,~$p_\mathrm{typo}$,~$p_\mathrm{spell}$)
\caption{Generation of NMT-Fake* reviews.}
\label{alg:base}
"
471,1805.02400,"[!t]
 \KwData{Initial log LM $\log p$, Bernoulli probability $b$, soft-penalty $\lambda$, monotonic factor $\alpha$, last generated token $o_i$, grammar rules set $G$}
 \KwResult{Augmented log LM $\log \Tilde{p}$}
%\begin{algorithm}
\begin{algorithmic}[1]
\Procedure {Augment}{$\log p$, $b$, $\lambda$, $\alpha$, $o_i$, $i$}{ \\
generate $P_{\mathrm{1:N}} \leftarrow Bernoulli(b)$~~~~~~~~~~~~~~~|~$\text{One value} \in \{0,1\}~\text{per token}$~ \\
$I \leftarrow P>0$ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~Select positive indices~\\
$\log \Tilde{p} \leftarrow$ $\text{Discount}$($\log p$, $I$, $\lambda \cdot \alpha^i$,$G$) ~~~~~~ |~start penalty~\\ 
$\log \Tilde{p} \leftarrow$ $\text{Discount}$($\log \Tilde{p}$, $[o_i]$, $\lambda$,$G$) ~~~~~~~~~ |~memory penalty~\\ 
\textbf{return}~$\log \Tilde{p}$
}
\EndProcedure
%\end{algorithmic}
%\caption{Subroutine for decreasing log likelihoods.}
%\label{alg:discount}
\\
%\begin{algorithm}
%\begin{algorithmic}[1]
\Procedure {Discount}{$\log p$, $I$, $\lambda$, $G$}{
\State{\For{$i \in I$}{ 
\eIf{$o_i \in G$}{
$\log p_{i} \leftarrow \log p_{i} + \lambda/2$
}{
$\log p_{i} \leftarrow \log p_{i} + \lambda$}
}%\EndFor}}
\textbf{return}~$\log p$
\EndProcedure
}}
\end{algorithmic}
%\caption{Subroutine for decreasing log likelihoods.}
%\label{alg:discount}
\caption{Pseudocode for augmenting language model. }
\label{alg:aug}
"
472,1805.02400,"
\begin{algorithmic}[1]
\Procedure {Augment}{$\log p$, $b$, $\lambda$, $\alpha$, $o_i$, $i$}{ \\
generate $P_{\mathrm{1:N}} \leftarrow Bernoulli(b)$~~~~~~~~~~~~~~~|~$\text{One value} \in \{0,1\}~\text{per token}$~ \\
$I \leftarrow P>0$ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|~Select positive indices~\\
$\log \Tilde{p} \leftarrow$ $\text{Discount}$($\log p$, $I$, $\lambda \cdot \alpha^i$,$G$) ~~~~~~ |~start penalty~\\ 
$\log \Tilde{p} \leftarrow$ $\text{Discount}$($\log \Tilde{p}$, $[o_i]$, $\lambda$,$G$) ~~~~~~~~~ |~memory penalty~\\ 
\textbf{return}~$\log \Tilde{p}$
}
\EndProcedure
%\end{algorithmic}
%\caption{Subroutine for decreasing log likelihoods.}
%\label{alg:discount}
\\
%\begin{algorithm}
%\begin{algorithmic}[1]
\Procedure {Discount}{$\log p$, $I$, $\lambda$, $G$}{
\State{\For{$i \in I$}{ 
\eIf{$o_i \in G$}{
$\log p_{i} \leftarrow \log p_{i} + \lambda/2$
}{
$\log p_{i} \leftarrow \log p_{i} + \lambda$}
}%\EndFor}}
\textbf{return}~$\log p$
\EndProcedure
}}
\end{algorithmic}
%\caption{Subroutine for decreasing log likelihoods.}
%\label{alg:discount}
\caption{Pseudocode for augmenting language model. }
\label{alg:aug}
"
473,1805.02400,"
%\begin{algorithmic}[1]
\Procedure {Discount}{$\log p$, $I$, $\lambda$, $G$}{
\State{\For{$i \in I$}{ 
\eIf{$o_i \in G$}{
$\log p_{i} \leftarrow \log p_{i} + \lambda/2$
}{
$\log p_{i} \leftarrow \log p_{i} + \lambda$}
}%\EndFor}}
\textbf{return}~$\log p$
\EndProcedure
}}
\end{algorithmic}
%\caption{Subroutine for decreasing log likelihoods.}
%\label{alg:discount}
\caption{Pseudocode for augmenting language model. }
\label{alg:aug}
"
474,1804.10689,"[h]
% \label{alg:dynamics}
% \caption{Dynamics Training Algorithm}
% \begin{algorithmic}
% \STATE Initialize module parameters $\theta_{dynamics}$ \\
% \STATE Initialize hidden state $h_d$ for LSTM-D \\
% \STATE Set dynamics hyper-parameters $\lambda_{inv}, \lambda_{dec}, \lambda_{for}$ \\
% \FOR{$e\in\{1,...,E\}$}
%   \FOR{$(s_i, a_i, s_i') \in \{(s_0, a_0, s_0'),...,(s_N, a_N, s_N')\}$}
%       \STATE $z_i = f_{enc}(s_i)$
%       \STATE $z_i' = f_{enc}(s_i')$
%       \STATE $\hat{z}_i' = f_{for}(z_i, a)$ 
%       \STATE $\hat{a}_i = f_{inv}(z_i, z_i')$
%       \STATE $\hat{s}_i' = f_{dec}(\hat{z}_i')$ 
%       \STATE $\hat{s}_i = f_{dec}(z_i)$
%       \STATE Compute $\mathcal{L}_{dynamics}$
%       \STATE Update $\theta_{dynamics}$
%   \ENDFOR
% \ENDFOR
% \end{algorithmic}
% "
475,1804.10689,"[H]
    % \scriptsize
    \caption{Dynamics Training Algorithm}
    \label{alg:dynamics}
    \begin{algorithmic}
    \STATE Initialize module parameters $\theta_{dynamics}$
    \STATE Initialize hidden state $h_d$ for LSTM-D
    \STATE Set dynamics hyper-parameters $\lambda_{inv}, \lambda_{dec}, \lambda_{for}$
    \FOR{$e\in\{1,...,E\}$}
      \FOR{$(s_i, a_i, s_i') \in \{(s_0, a_0, s_0'),...,(s_N, a_N, s_N')\}$}
        \STATE Encode $s_i, s_i'$ to $z_i, z_i'$ (Eq. \ref{eq:encode})
        \STATE $\hat{z}_i'\leftarrow f_{for}(z_i, a)$ 
        \STATE $\hat{a}_i\leftarrow f_{inv}(z_i, z_i')$
        \STATE Decode $z_i, \hat{z}_i'$ to $\hat{s}_i, \hat{s}_i'$ (Eq. \ref{eq:decode})
        \STATE Compute $\mathcal{L}_{dynamics}$ (Eq. \ref{eq:dynamics_loss})
      \STATE Update $\theta_{dynamics}$
   \ENDFOR
    \ENDFOR
    \end{algorithmic}
    "
476,1804.10689,"[H]
\label{alg:reward}
% \scriptsize
\caption{Reward Training Algorithm}
\begin{algorithmic}
\STATE Freeze $\theta_{enc}$
\STATE Initialize module parameters $\theta_{reward}$
\STATE Initialize hidden state $h_r$ for LSTM-R
\STATE Set reward hyper-parameter $\lambda_{critic}$ 
\STATE $t\leftarrow 1$, $T\leftarrow 0$
\REPEAT
    \STATE Clear gradients
    \IF{episode done}
       \STATE Clear hidden states $h_r, h_d$
       \STATE Reset environment
    \ENDIF
    \STATE $t_{start}=t$
    \STATE Get state $s_t$
    \REPEAT
        \STATE $z_t = f_{enc}(s_t)$ 
        \STATE $\pi(a_t|z_t ; \theta_{actor}) = f_{actor}(z_t; \theta_{actor})$ 
        \STATE Sample $a$ from $\pi(a_t|z_t ; \theta_{actor})$, get $s_{t+1}, r_t$
        \STATE $z_{t+1}\leftarrow f_{enc}(s_t+1)$
    	\STATE $t\leftarrow t + 1$, $T\leftarrow T + 1$
    \UNTIL{terminal $s_t$ \algorithmicor $t - t_{start} == t_{max}$}
    \FOR{$i\in\{t-1,\ldots,t_{start}\}$}
        \STATE $R\leftarrow r_i + \gamma R$
        \STATE $d\theta_{actor} \leftarrow d\theta_{actor} + \nabla_{\theta_{actor}} \log \pi(a_t|z_t; \theta_{actor})$
        \STATE $\qquad (R - V(z_{t}; \theta_{critic})) $
        \STATE $d\theta_{critic} \leftarrow d\theta_{critic} + \lambda_{critic}$ 
        \STATE $\qquad \nabla_{\theta_{critic}} (R - V(z_{t}; \theta_{critic}))^2$
    \ENDFOR
    \STATE Sum losses and perform asynchronous update on 
    \STATE $\qquad \theta_{actor}, \theta_{critic}$ with $d\theta_{critic}, d\theta_{actor}$
\UNTIL{$T > T_{max}$}
\end{algorithmic}
"
477,1804.10689,"[h]
%\label{alg:bfs-simulated-search}
%\caption{Simulated Breadth-First Search}
%\begin{algorithmic}
%\INPUT Dynamics Module $\theta_{dynamics}$ \\
%\INPUT Current state (real) $s_t$ from the environment \\


%\FOR{$a \in \mathcal{A}$}
%   \FOR{$(s_i, a_i, s_i') \in \{(s_0, a_0, s_0'),...,(s_N, a_N, s_N')\}$}
%      \STATE $z_i = f_{enc}(s_i)$
%      \STATE $z_i' = f_{enc}(s_i')$
%      \STATE $\hat{z}_i' = f_{for}(z_i, a)$
%      \STATE $\hat{a}_i = f_{inv}(z_i, z_i')$
%      \STATE $\hat{s}_i' = f_{dec}(\hat{z}_i')$ 
%      \STATE $\hat{s}_i = f_{dec}(z_i)$
%      \STATE Compute $\mathcal{L}_{dynamics}$
%      \STATE Update $\theta_{dynamics}$
%   \ENDFOR
   
%Select the action which leads to the maximum value in the search tree
%\ENDFOR


%\FOR
%    \STATE Clear gradients
%    \IF{episode done}
%       \STATE Clear hidden states $h_r, h_d$
%       \STATE Reset environment
%    \ENDIF
%    \STATE $t_{start}=t$
%    \STATE Get state $s_t$
%    \REPEAT
%        	\STATE $z_t = f_{enc}(s_t)$ 
%        \STATE $\pi(a_t|z_t ; \theta_{actor}) = f_{actor}(z_t; \theta_{actor})$ 
%        \STATE Sample $a$ from $\pi(a_t|z_t ; \theta_{actor})$
%        \STATE Perform action $a$, get $s_{t+1}, r_t$
%        \STATE $z_{t+1} = f_{enc}(s_t+1)$
%        \STATE $V(z_t) = f_{val}(z_t)$
%    	\STATE Compute $\mathcal{L}_{reward}$
%    \UNTIL{terminal $s_t$ \algorithmicor $t - t_{start} == t_{max}$}
%    \STATE Sum losses, compute gradients, and perform asynchronous update
%\UNTIL{$T > T_{max}$}
%\end{algorithmic}
%"
478,1805.02867,"[ht]
\caption{Naive softmax}
\label{alg:naive_softmax}
\begin{algorithmic}[1]
\State $d_0\gets 0$
\For{$j\gets 1, V$}
\State $d_j\gets d_{j-1}+e^{x_j}$ \label{overflow_line}
\EndFor
\For{$i\gets 1, V$}
\State $y_i\gets \frac{e^{x_i}}{d_V}$
\EndFor
\end{algorithmic}
"
479,1805.02867,"[ht]
\caption{Safe softmax}
\label{alg:safe_softmax}
\begin{algorithmic}[1]
\State $m_0\gets -\infty$
\For{$k\gets 1, V$}
\State $m_k\gets \max(m_{k-1},{x_k})$
\EndFor
\State $d_0\gets 0$
\For{$j\gets 1, V$}
\State $d_j\gets d_{j-1}+e^{x_j-m_V}$
\EndFor
\For{$i\gets 1, V$}
\State $y_i\gets \frac{e^{x_i-m_V}}{d_V}$
\EndFor
\end{algorithmic}
"
480,1805.02867,"[ht]
\caption{Safe softmax with online normalizer calculation}
\label{alg:online_softmax}
\begin{algorithmic}[1]
\State $m_0\gets -\infty$\label{alg:start_thm_line}
\State $d_0\gets 0$
\For{$j\gets 1, V$}
\State $m_j\gets \max\left(m_{j-1},x_j\right)$\label{alg:m_line}
\State $d_j\gets d_{j-1}\times e^{m_{j-1}-m_j} +e^{x_j-m_j}$\label{alg:d_line}
\EndFor\label{alg:end_thm_line}
\For{$i\gets 1, V$}
\State $y_i\gets \frac{e^{x_i-m_V}}{d_V}$
\EndFor
\end{algorithmic}
"
481,1805.02867,"[ht]
\caption{Online softmax and top-k}
\label{alg:online_softmax_topk}
\begin{algorithmic}[1]
\State $m_0\gets -\infty$
\State $d_0\gets 0$
\State $u\gets \left\{-\infty,-\infty,\dots,-\infty\right\}^T, u\in\mathbb{R}^{K+1}$ \Comment The 1st $K$ elems will hold running TopK values
\State $p\gets \left\{-1,-1,\dots,-1\right\}^T, p\in \mathbb{Z}^{K+1}$ \Comment ... and their indices
\For{$j\gets 1, V$}
\State $m_j\gets \max\left(m_{j-1},x_j\right)$
\State $d_j\gets d_{j-1}\times e^{m_{j-1}-m_j} +e^{x_j-m_j}$
\State $u_{K+1}\gets x_j$ \Comment Initialize $K+1$ elem with new value from input vector
\State $p_{K+1}\gets j$ \Comment ... and its index
\State $k\gets K$\label{alg:start_partial_topk_lines} \Comment Sort $u$ in descending order, permuting $p$ accordingly. The first K elements are already sorted, so we need just a single loop, inserting the last element in the correct position.
\While{$k\geq 1 \text{ and } u_k<u_{k+1}$}
	\State swap$\left(u_k,u_{k+1}\right)$
    \State swap$\left(p_k,p_{k+1}\right)$
    \State $k\gets k-1$
\EndWhile\label{alg:end_partial_topk_lines}
\EndFor
\For{$i\gets 1, K$} \Comment The algorithm stores only K values and their indices
\State $v_i\gets \frac{e^{u_i-m_V}}{d_V}$
\State $z_i\gets p_i$
\EndFor
\end{algorithmic}
"
482,1805.02792,"
	\caption{Training Algorithm for FFNet}
	\label{alg1}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} a set of videos $\left\lbrace V \right\rbrace $ and annotations  $\left\lbrace G \right\rbrace $
		\STATE \textbf{Output:} Q-value neural network $Q$
		\STATE Init\_MLP( ) $\rightarrow Q$
		\STATE Initialize: memory $M$ = [empty], $explore\_rate $ $\epsilon$ = 1
		
		\FOR{$i = 1$ to $N$}
		\STATE Training\_Video\_Selection ($V$, $G$) $\rightarrow v_i,g_i$
		\STATE $frame_{curr} = 0$
		\STATE Process($frame_{curr}$) $\rightarrow s_{curr}$
		
		\WHILE{$frame_{curr}<$Size($v_i$)}
		\STATE $a_{curr}=	
		\begin{cases}
		a^k \in A, k = random(n), & prob. = \epsilon \\
		arg \max Q(s_{curr},a'), & o.w.
		\end{cases}	$
		\STATE $frame_{next} = $ Action($a_{curr}$, $frame_{curr}$)
		\STATE Process($frame_{next}$) $\rightarrow s_{next}$
		\STATE $r =$ Reward($s_{curr}$, $a_{curr}$, $s_{next}$, $g_i$)
		\STATE $input = s_{curr}$ 
		\STATE $target=\begin{cases}
		r+\gamma \max_{a'} Q(s_{next},a'),& a=a_{curr} \\
		Q(s_{curr}, a), & o.w.
		\end{cases}$
		\STATE $(input, target) \rightarrow M$
		\STATE $ s_{next} \rightarrow s_{curr}$
		
		\IF{ $M>batch size$}
		\STATE Training($M$,$Q$) $\rightarrow Q$
		\STATE $\epsilon = \max (\epsilon-\vartriangle \epsilon, \epsilon_{min})$
		\STATE Empty($M$)
		\ENDIF
		
		
		\ENDWHILE
		\ENDFOR
	\end{algorithmic}	
"
483,1805.08698,"
\caption{One epoch in the training for the prototypical classifier}\label{proto_cls}
\textbf{Input:} Ideal dataset $\mathcal D_{ideal}=\{(x_i^{ideal}, y_i^{ideal})\}_{i=1}^N$ where $y_i^{ideal}\in \{1,...,l\}$, max number of batches($T$), the number of samples selected from each class in each step($N_c$), the subset $\mathcal D_k$ containing all the samples from class $k$.\\
\textbf{Output:} Prototypical classifier $\mathcal C_{\theta, \psi}$.
\begin{algorithmic}[1]
\State Randomly initialize prototypes $c^0_k$ for each class.
\For{$t=1,...,T$}
\For{$k=1,...,l$}
\State Randomly select $N_c$ samples from class $k$.
\EndFor
\State Compute $\mathcal L_{\mathcal C}(\theta, \psi)$ and $\mathcal L_{\mathcal F}(\theta)$ given the samples and prototypes.
\State $\mathcal L_{batch}(\theta, \psi)\gets \mathcal L_{\mathcal C}(\theta, \psi)+\lambda \mathcal L_{\mathcal F}(\theta)$.
\State Update the parameters $\theta, \psi$ by taking an Adam step on the batch loss.
\State $c_k^t \gets \frac{1}{|\mathcal D^{ideal}_k|+1}(c^{t-1}_k+\sum_{y_i^{ideal}=k} \mathcal F_{\theta}(x_i^{ideal}))$
\EndFor
\end{algorithmic}
"
484,1805.08698,"
\caption{One epoch in the training for the refiner when labels are available}\label{rf_tar}
\textbf{Input:} Imperfect training dataset $\mathcal D_{imp}=\{(x_i^{imp}, y_i^{imp})\}_{i=1}^M$ where $y_i^{imp}\in \{1,...,l\}$, max number of batches($T$), the batch size($N_c$), the prototypical classifier $\mathcal C$ and the prototypes $c_k$.\\
\textbf{Output:} Refiner $\mathcal R_\phi$.
\begin{algorithmic}[1]
\For{t=1..T}
\State Sample $N_c$ samples from training set $D_{imp}: \{x_i, y_i\}_{i=1}^{N_c}$.
\State Let $r_i=\mathcal R(x_i)$ be the refined inputs.
\State Let $e_i=\mathcal F(r_i)$ be the embedded refined inputs.
\State Let $c_i=\mathcal G(e_i)$ be the predicted labels.
\State Compute $\mathcal L_{\mathcal R}(\phi)$ in equation (\ref{rf_loss})
\begin{itemize}[leftmargin=.5in]
\item $\ell_{\mbox{pred}}=\frac{1}{N_c}\sum_i\mathcal H(c_i, y_i)$
\item $\ell_{\mbox{proto}}=\frac{1}{N_c}\sum_i -\log \frac{\exp(-d(e_i, c_{y_i}))}{\sum_{k'}\exp (-d(e_i, c_{k'}))}$
\item $\ell_{\mbox{reg}}=\frac{1}{N_c}\sum_i ||\Psi(r_i)-\Psi(x_i)||_p$
\end{itemize}
\State Update parameters $\phi$ through back-propagation based on the loss $\mathcal L_{\mathcal R}(\phi)$.
\EndFor
\end{algorithmic}
"
485,1711.07875,"[t]
\caption{\label{alg:perp} The Choice Perceptron (\cp) algorithm.}
\begin{algorithmic}[1]
    \Procedure{\cp\;}{$T, \eta$}
        \State $\vw^1 \gets 0$
        \For{$t = 1, \ldots, T$}
            \State Receive context $x^t$ from the user
            \State $\calQ^t \gets \textsc{SelectQuery}(x^t, \vw^t)$
            \State User chooses $\bar{y}^t$ from $\calQ^t$
            \State $\vw^{t+1} \gets \vw^t + \eta \Delta^t$
        \EndFor
    \EndProcedure
\end{algorithmic}
"
486,1805.02404,"[t]
\caption{Double DQN \cite{DQN2015, van2016deep} a Model-Free \ac{RL} method.} 
\label{alg:dqn}
\begin{algorithmic}[1]
\STATE \textbf{Input}: weights: $\theta_0$, replay size: $M$, transfer steps: $N$
\STATE $E \leftarrow []$ \hfill \textit{\small // initialize experience replay} \label{alg:line:initexp}
\STATE $\theta_L \leftarrow \theta_0$ \hfill \textit{\small // initialize label network} 
\FOR{$i \in [1, 2, \ldots ,]$}
    \WHILE{$|E| < M$}
    	\STATE $\langle s,a,r,s'\rangle \leftarrow \textit{execute\_policy}(\theta_i)$ \label{alg:line:executepolicy}
         \STATE $E \leftarrow append(E, \langle s,a,s',r\rangle)$  
    \ENDWHILE
    \STATE $\mathcal{L} \leftarrow 0$ \hfill \textit{\small // initialize loss function}  \label{alg:line:initloss}
    \FOR{$j \in [1, \ldots, \textit{batch\_size}]$}
        \STATE $\langle s,a,r,s'\rangle \leftarrow \textit{sample}(E)$  \hfill \textit{\small // sample without replacement} \label{alg:line:samplereplay}
        \STATE $a' \leftarrow \argmax_a Q(s', a, \theta_i)$ \label{alg:line:maxaction}
       \STATE $\mathcal{L} \leftarrow \mathcal{L} + (Q(s, a, \theta_i) - r - \gamma Q(s', a', \theta_L))^2$ \label{alg:line:regressionloss}
    \ENDFOR
    \STATE $\theta_{i+1} = \textit{gradient\_descent\_update}(\mathcal{L})$  \label{alg:line:update}
    \IF{$i \mod N = 0$}
        \STATE $\theta_L = \theta_i$  \label{alg:line:transfer}
    \ENDIF
\ENDFOR
\end{algorithmic}
"
487,1805.02404,"[tb]
\caption{Sampling an episode with the baseline \ac{GRU} model.} 
\label{alg:grupolicy}
\begin{algorithmic}[1]
%\STATE \textbf{Input}: Model $f$, Expl. Degree $\epsilon$, Weights: $\theta$, SERP size: $k$
\STATE $\mathbf{D}_q  \leftarrow \textit{receive\_query}$  \hfill \textit{\small// query and pre-selection of doc.}
\STATE $\mathbf{h}_0 \leftarrow \mathbf{0}$ \hfill \textit{\small// initialize hidden state}
\STATE $\mathbf{R} \leftarrow []$ \label{line:gru:dinit}
\FOR{$t \leftarrow 1 \ldots  k$ }
    \IF{coinflip with $\epsilon$ probability}
        \STATE $\mathbf{d}_t \leftarrow \textit{sample\_document}(\mathbf{D}_q)$ \hfill \textit{\small// explorative action}  \label{line:gru:explore}
    \ELSE
        \STATE $\mathbf{q} \leftarrow [0, \ldots , 0] $ \hfill \textit{\small// initialize zero document score vector}
        \FOR{$d_i \in \mathbf{D}_q$ }
           \STATE $\mathbf{h}' \leftarrow GRU(\mathbf{h}_{t-1}, \mathbf{d}_i)$
            \STATE $\mathbf{q}_i \leftarrow Q(h', d_i, \theta)$ \hfill \textit{\small// estimate Q-value for doc. (Eq~\ref{eq:standardqvalue})} \label{line:gru:qvalue}
        \ENDFOR
        \STATE $i \leftarrow \argmax{\mathbf{q}}$ \hfill \textit{\small// select doc. with highest q-value}
        \STATE $\mathbf{R}_t \leftarrow \textit{gather}(\mathbf{D}_q, d_i)$ \hfill \textit{\small// add doc. to SERP}
    \ENDIF
    \STATE $\mathbf{D}_q \gets \textit{remove}(\mathbf{D}_q, \mathbf{d}_t)$  \hfill \textit{\small// remove to prevent duplicates} \label{line:gru:removedoc}
    \STATE $\mathbf{h}_t = GRU(\mathbf{h}_{t-1}, \mathbf{d}_t)$ \hfill \textit{\small// update (partial) SERP embedding} \label{line:gru:adddoc}
\ENDFOR
\RETURN $\mathbf{R}$
\end{algorithmic}
"
488,1805.02404,"[tb]
\caption{Sampling an episode with \ac{DRM}.} 
\label{alg:drmepisode}
\begin{algorithmic}[1]
%\STATE \textbf{Input}: Model $f$, Expl. Degree $\epsilon$, Weights: $\theta$, SERP size: $k$
\STATE $\mathbf{D}_q  \leftarrow \textit{receive\_query}$  \hfill \textit{\small// query and pre-selection of doc.}
\STATE $\mathbf{P} \leftarrow [p_1, \ldots ,p_ k]$ \hfill \textit{\small// available positions}
\STATE $h_0 \leftarrow \mathbf{0}$ \hfill \textit{\small// initialize hidden state}
\STATE $\mathbf{R}, \mathbf{I} \leftarrow [], []$ \hfill \textit{\small// initialize ranking and selected positions} \label{line:drm:init}
\FOR{$t \leftarrow 1 \ldots  k$ }
    \IF{coinflip with $\epsilon$ probability}
        \STATE $\mathbf{d}_t \leftarrow \textit{sample\_document}(\mathbf{D}_q)$ \hfill \textit{\small// explorative doc. action} \label{line:drm:docexplore}
    \ELSE
        \STATE  \textit{\small// find the document with highest Q-value (Eq.~\ref{eq:drm:docvalue})}
        \STATE $\mathbf{R}_t \leftarrow \argmax_{d_i\in \mathbf{D}_q}{Q(h_{t-1}, d_i, \theta)}$ \label{line:drm:docexploit}
    \ENDIF
    \STATE $\mathbf{D}_q \leftarrow \textit{remove}(\mathbf{D}_q, \mathbf{R}_t)$  \hfill \textit{\small// remove to prevent duplicates} \label{line:drm:remove}
    \IF{coinflip with $\epsilon$ probability}
        \STATE $\mathbf{p}_t \leftarrow \textit{sample\_position}(\mathbf{P})$ \hfill \textit{\small// explorative pos. action} \label{line:drm:posexplore}
    \ELSE
         \STATE  \textit{\small// find the position with highest Q-value (Eq.~\ref{eq:drm:posvalue})}
        \STATE $\mathbf{P}_t \leftarrow \argmax_{p_i\in \mathbf{P}}{Q(h_{t-1}, \mathbf{R}_t, p_i, \theta)}$ \label{line:drm:posexploit}
    \ENDIF
    \STATE $\mathbf{P} \leftarrow \textit{remove}(\mathbf{P}, \mathbf{p}_t)$  \hfill \textit{\small// make position unavailable} \label{line:drm:posremove}
    \STATE $\mathbf{h}_t = GRU(\mathbf{h}_{t-1}, \mathbf{d}_t, \mathbf{p}_t)$ \hfill \textit{\small// update (partial) SERP embedding}
\ENDFOR
\RETURN $\mathbf{R}, \mathbf{I}$
\end{algorithmic}
"
489,1805.02404,"[t]
%\caption{Updating Policy.} 
%\label{alg:simmgd}
%\begin{algorithmic}[1]
%\STATE \textbf{Input}: weights: $\theta_0$, SERP: $k$, steps: $N$, batch: $M$, transfer: $T$
%\STATE $E \leftarrow []$ \hfill \textit{\small initialize experience replay // obtain a query from a user} 
%\FOR{$i \in [1, \ldots , N]$}
%    \WHILE{$|E| < M$}
%    	\STATE $\mathbf{d} \leftarrow \textit{sample\_episode}(\epsilon, \theta_i)$
%         \STATE $E \leftarrow append(E, \mathbf{d})$
%    \ENDWHILE
%    \STATE $\mathcal{L} \leftarrow 0$
%    \FOR{$j \in [1, \ldots , B]$}
%        \STATE $\mathbf{d} \leftarrow \textit{sample}(E)$  \hfill \textit{\small // sample past experience without replacement}
%        \STATE $\mathbf{h}_0 \leftarrow \mathbf{0}$
%        \FOR{$t \in [1, \ldots , k - 1]$}
%            \STATE $\mathbf{h}_i \leftarrow f(\mathbf{h}_{t-1}, \mathbf{d}_t)$
%            \STATE $d' \leftarrow \argmax_d Q(\mathbf{h}_i, d, \theta_i)$
%            \STATE $\mathcal{L} \leftarrow \mathcal{L} + (Q(\mathbf{h}_{i-1}, \mathbf{d}_t, \theta_i) - Q(\mathbf{h}_i, d', \theta_L))^2$
%        \ENDFOR
%        \STATE $\mathcal{L} \leftarrow \mathcal{L} + (Q(\mathbf{h}_{k-1}, \mathbf{d}_k, \theta_i) - R(\mathbf{d}))^2$
%    \ENDFOR
%    \STATE $\theta_{i+1} = \textit{gradient\_descent\_update}(\mathcal{L})$
%    \IF{$i \mod T = 0$}
%        \STATE $\theta_L = \theta_i$
%    \ENDIF
%\ENDFOR
%\end{algorithmic}
%"
490,1805.02404,"[t]
%\caption{Updating Policy.} 
%\label{alg:posupdate}
%\begin{algorithmic}[1]
%\STATE \textbf{Input}: weights: $\theta_0$, SERP size: $k$, steps: $N$, batch size: $M$, steps per transfer: $T$, reward function: $R$
%\STATE $E \leftarrow []$ \hfill \textit{\small initialize experience replay // obtain a query from a user} 
%\FOR{$i \in [1, \ldots , N]$}
%    \WHILE{$|E| < M$}
%    	\STATE $\mathbf{d}, \mathbf{p} \leftarrow \textit{sample\_episode}(\epsilon, \theta_i)$   \hfill \textit{\small // Algorithm~\ref{alg:posepisode}}
%         \STATE $E \leftarrow append(E, (\mathbf{d}, \mathbf{p}))$  \hfill \textit{\small // add to experience replay buffer}
%    \ENDWHILE
%    \STATE $\mathcal{L} \leftarrow 0$
%    \FOR{$j \in [1, \ldots , B]$}
%        \STATE $\mathbf{d}, \mathbf{p} \leftarrow \textit{sample}(E)$  \hfill \textit{\small // sample without replacement}
%        \STATE $\mathbf{h}_0 \leftarrow \mathbf{0}$   \hfill \textit{\small // initialize hidden layer}
%        \FOR{$t \in [1, \ldots , k - 1]$}
%             \STATE $\mathbf{h}_t \leftarrow f(\mathbf{h}_{t-1}, \mathbf{d}_t, \mathbf{p}_t)$
%            \STATE $p' \leftarrow \argmax_p Q(\mathbf{h}_{t-1}, \mathbf{d}_t, p, \theta_i)$
%             \STATE $d' \leftarrow \argmax_d Q(\mathbf{h}_{t}, d, \theta_i)$
%            \STATE $\mathcal{L} \leftarrow \mathcal{L} + (Q(\mathbf{h}_{t-1}, \mathbf{d}_t, \theta_i) - Q(\mathbf{h}_{t-1}, \mathbf{d}_t, p', \theta_L))^2$
%            \STATE $\mathcal{L} \leftarrow \mathcal{L} + (Q(\mathbf{h}_{t-1}, \mathbf{d}_t, \mathbf{p}_t, \theta_i) - Q(\mathbf{h}_{t}, d', \theta_L))^2$
%        \ENDFOR
%        \STATE $p' \leftarrow \argmax_p Q(\mathbf{h}_{t-1}, \mathbf{d}_t, p, \theta_i)$
%        \STATE $\mathcal{L} \leftarrow \mathcal{L} + (Q(\mathbf{h}_{k-1}, \mathbf{d}_k, \theta_i) - Q(\mathbf{h}_{k-1}, \mathbf{d}_k, p', \theta_i))^2$
%        \STATE $\mathcal{L} \leftarrow \mathcal{L} + (Q(\mathbf{h}_{k-1}, \mathbf{d}_k, \mathbf{p}_k, \theta_i) - R(\mathbf{d}))^2$
%    \ENDFOR
%    \STATE $\theta_{i+1} = \textit{gradient\_descent\_update}(\mathcal{L})$
%     \STATE $\epsilon{i+1} = \textit{degrade\_exploration}(\epsilon)$
%    \IF{$i \mod T = 0$}
%        \STATE $\theta_L = \theta_i$
%    \ENDIF
%\ENDFOR
%\end{algorithmic}
%"
491,1802.02614,"[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Pre-trained word embedding set $\{U_{w}|w\in S\}$ where $U_{w}\in \mathbb{R}^{d_{1}}$ is embedding vector for word $w$. Word embedding $\{V_{w}|w\in T\}$  are generated on training set where $V_{w}\in \mathbb{R}^{d_{2}}$. P is a set of word vocabulary on the task dataset.}
\Output{ A dictionary with word embedding vectors of dimension $d_{1} + d_{2}$ for $ (S\cap P)\cup T$. } 
\BlankLine
$\mathrm{res} = \mathrm{dict}()$
\newline
\For{ $w\in (S\cap P)\cup T$} {
  \lIf{ $w \in S\cap P$ $\mathrm{and}$ $w\in T$ }{res$[w] = [U_{w}; V_{w}]$}
  \lElseIf{ $w\in S\cap P$ $\mathrm{and}$ $ w\notin T$ }{ res$[w] = [U_{w}; \vec{0}]$}
  \lElse{res$[w] = [\vec{0}; V_{w}]$}
}
\textbf{Return} res
\caption{Combine pre-trained word embedding with those generated on training set.}
\label{alg_word_embedding_combination}
"
492,1805.02338,"[htbp]
  \caption{Step computation using SdLBFGS}
 \label{alg:SdLBFGS}
  \begin{algorithmic}[1]
\REQUIRE Let $x_k$ be a current iterate. 
		Given the stochastic gradient $g_{k-1}$ at iterate $x_{k-1}$, 
		the random variable $\xi_{k-1}$, the batch size $m_{k-1}$, $s_j$, $\bar{y}_j$ 
		and $\rho_j$, $j = k-p, \dots, k-2$, and $u_0 = g_k$. 

\ENSURE $H_kg_k = v_p$.

\STATE Set $s_{k-1} = x_k - x_{k-1}$ and $y_{k-1} = g_k - g_{k-1}$. 
\STATE Set $H_{k,0} = I$
\STATE Calculate $\bar{y}_{k-1}$ through Eq. (\ref{eq: y_bar}) and $\rho_{k-1} = (s_{k-1}^T \bar{y}_{k-1})^{-1}$
% \STATE $y_{k-1} = \frac{1}{m_{k-1}}[\sum_{i=1}^{m_{k-1}}g(x_k, \xi_{k-1,i}) - g(x_{k-1}, \xi_{k-1,i})]$.
%\STATE $\gamma_k = \max\left\{\dfrac{t_{k-1}^T y_{k-1}}{s_{k-1}^T y_{k-1}}, \delta \right\}$
%\STATE $$
%\STATE 
%\STATE 

\STATE Set $c = \min\{p,k-1\}$
\FOR{$i = 0, \dots, c-1$} 
	\STATE Calculate $\mu_i = \rho_{k-i-1} u_i^T s_{k-i-1}$. 
	\STATE Calculate $u_{i+1} = u_i - \mu_i \bar{y}_{k-i-1}$. 
\ENDFOR 
\STATE Set $v_0 = u_p$
\FOR{$i = 0, \dots, c-1$} 
	\STATE Calculate $\nu_i = \rho_{k-c+i} v_i^T \bar{y}_{k-c+i}$. 
	\STATE Calculate $v_{i+1} = v_i + (\mu_{c-i-1} - \nu_i) s_{k-c+i}$. 
\ENDFOR 
\STATE $v_p = v_p / ||v_p||_2$
\end{algorithmic}
"
493,1711.08247,"[t]
    \caption{\label{alg:ordselect} An example of ordering selection procedure using a GAI network~\cite{gonzales2004gai}.}
    \begin{algorithmic}[1]
        \Procedure{\textsc{SelectOrdering}}{$\calP$}
            \State Build a GAI network $\calG$ from $I_p$, $p \in \calP$
            \State Produce sequence $p_1, \dots, p_n$ by sorting the nodes \\
                   \qquad in $\calG$ in ascending order of node degree \\ \quad\ \
            \Return $p_1, \dots, p_n$
        \EndProcedure
    \end{algorithmic}
"
494,1805.02158,"[htb]         %Ëã·¨µÄ¿ªÊ¼
%\caption{The proposed algorithm}             %Ëã·¨µÄ±êÌâ
%\label{alg_THong}                  %¸øËã·¨Ò»¸ö±êÇ©£¬ÕâÑù·½±ãÔÚÎÄÖÐ¶ÔËã·¨µÄÒýÓÃ
%\begin{algorithmic}[1]
%\REQUIRE ~\\
%Sparsify dictionary matrix $\bm\Psi$, initialize $\bm\Phi_0$ to a random matrix, and the number of iterations: $Iter_{outer}$ and $Iter_{inner}$.
%\lastcon ~\\          %OUTPUT
%  Projection matrix $\bm\Phi$.
%\ENSURE
%\STATE {$\bm\Phi\leftarrow\bm\Phi_0$}
%\FOR {$l=1$ {\bf to} $Iter_{outer}$}
%\STATE ${\bm G_t}\leftarrow \bm\Psi^\mathcal T\bm\Phi^\mathcal T\bm\Phi\bm\Psi$\label{ap1}
%\STATE Update $\bm G_t$:
%\STATE $\text{diag}({\bm G_t})\leftarrow \bm 1$
%\FORALL { $i\neq j$}
%\STATE
%\en
%{\bm G_t(i,j)}\leftarrow
%\left\{\begin{array}{ll}
%{\bm G_t(i,j)}&\text{if}~|{\bm G_t(i,j)}|\leq\zeta\\
%\zeta\cdot\text{sign}({\bm G_t(i,j)})&\text{otherwise}
%\end{array}\right.
%\een
%%where sign($\cdot$) is a sign function.
%\ENDFOR
%\STATE Update $\bm\Phi$:
%%\STATE $J\leftarrow 1$
%%\STATE Compute $\varrho(\bm\omega_{J})$
%\FOR {$k=1$ {\bf to} $Iter_{inner}$}%[Until Convergence]
%\FOR {$J=1$ {\bf to} $M$}
%\STATE Compute $\bm E_J$\label{ap2}
%\STATE Compute the ED of $\bm E_J$\label{ap3}
%\IF {$\lambda_{1J}>0$}
%\STATE $\bm\omega_J\leftarrow\sqrt{\lambda_{1J}}\bm u_{1J}$\label{ap4}
%%\STATE Compute $\varrho(\bar{\bm\omega}_J)$
%%\IF {$\varrho(\bar{\bm \omega}_J)\geq\varrho(\bm\omega_{J})$}
%%\STATE Go to \ref{endloop1}
%%\ELSE
%%\STATE $\bm\omega_J\leftarrow\bar{\bm \omega}_J$
%%\STATE $\varrho(\bm\omega_J)\leftarrow \varrho(\bar{\bm\omega}_J)$
%%\STATE $J\leftarrow J+1$
%%\ENDIF
%%\ELSE
%%\STATE Go to \ref{endloop1}
%\ENDIF
%%\IF{$J=M+1$}
%%\STATE $J\leftarrow 1$
%%\ENDIF
%\ENDFOR
%\ENDFOR%\label{endloop1}
%\STATE $\bar{\bm \Phi}_1\leftarrow \bm\Omega\bm\Sigma_{\bm\Psi}^{-1}$\label{ap5}
%\STATE $\bm\Phi\leftarrow \bar{\bm\Phi}\bm U_{\bm\Psi}^\mathcal T$\label{ap6}
%\ENDFOR
%\end{algorithmic}
%"
495,1805.02158,"[!htb]        
\caption{Minimal Polynomial Extrapolation (MPE)}           
\label{alg:MPE_Van}                 
\begin{algorithmic}[1]
\REQUIRE ~\\
A sequence of vectors $\{\vx_m,\vx_{m+1},\vx_{m+2},\cdots,\vx_{m+\kappa+1}\}$ is produced by the baseline algorithm (FP in our case).	
\lastcon ~\\          %OUTPUT
A new vector $\vx_{(m,\kappa)}$.\\
			%{\color{red} RK: Now should come the actual algorithm:}
\STATE Construct the matrix
			$$\bm U_{\kappa}^m=\bmat \vx_{m+1}-\vx_m ,\cdots,\vx_{m+\kappa+1}-\vx_{m+\kappa}\emat \in \Re^{N\times (\kappa+1)}$$ and then compute its QR factorization via {\bf Algorithm \ref{alg:MGS_imple}}, $\bm U_{\kappa}^m=\bm Q_{\kappa}\bm R_{\kappa}$.\label{alg:MPE_Van:QR}
			%\STATE {\color{red} RK: This part is unclear. Is $k$ fixed? If so, then as you do not 
			%have $\vx_{-1}$ we have a problem. 
			%I suggest }
\STATE {Denote $\bm r_{{\kappa}+1}$ as the $\kappa+1$th column of $\bm R_{\kappa}$ without the last row and solve the following ${\kappa}\times {\kappa}$ upper triangular system
\begin{equation*}
\bm R_{{\kappa}-1}\bm c'=-r_{{\kappa}+1}~~~ \bm c'=\bmat c_0,c_1,\cdots,c_{{\kappa}-1}\emat^\mathcal T
\end{equation*} where $\bm R_{{\kappa}-1}$ is the previous ${\kappa}$ columns of $\bm R_{\kappa}$ without the last row. Finally, evaluate $\{\gamma_i\}$ through $\{\frac{c_i}{\sum_{i=0}^kc_i}\}$.} \label{alg:MPE_Van:backward}
\STATE {Compute $\bm \xi=\bmat \xi_0,\xi_1,\cdots,\xi_{{\kappa}-1}\emat^\mathcal T$ through 
\begin{equation*}
\xi_0=1-\gamma_0;~~\xi_j=\xi_{j-1}-\gamma_j,~~j=1,\cdots,{\kappa}-1
\end{equation*}}
\STATE Compute $\bm \eta =\bmat\eta_0,\eta_1,\cdots,\eta_{{\kappa}-1} \emat^\mathcal T=\bm R_{{\kappa}-1}\bm \xi$. Then we attain $\vx_{(m,\kappa)} =\vx_m+\bm Q_{{\kappa}-1}\bm \eta$  as the new initial vector where $\bm Q_{{\kappa}-1}$ represents the previous $\kappa$ columns of $\bm Q_{\kappa}$.
\end{algorithmic}
"
496,1805.02158,"[!htb]        
	\caption{Modified Gram-Schmidt (MGS)}           
	\label{alg:MGS_imple}                 
	\begin{algorithmic}[1]
  	\lastcon $\mQ_\kappa$ and $\mR_\kappa$ ($r_{ij}$ denotes the $(i,j)$th element of $\mR_\kappa$ and $\vq_i$ and $\vu_i$ represent the $i$th column of $\mQ_\kappa$ and $\mU_\kappa^m$, respectively.).
  \STATE{Compute $r_{11}=\|\bm u_1\|_2$ and $\bm q_1=\bm u_1/r_{11}$. }
  \FOR{$i=2,\cdots,{\kappa}+1$}
  \STATE Set $\bm u_i^{(1)}=\bm u_{i}$
  \FOR{$j=1,\cdots,i-1$}
  \STATE $r_{ji}=\bm q_j^\mathcal T\bm u_i^{(j)}$ and $\bm u_i^{(j+1)}=\bm u_i^{(j)}-r_{ji}\bm q_j$
 \ENDFOR
\STATE {Compute $r_{ii}=\|\bm u_{i}^{(i)}\|_2$ and $\bm q_i=\bm u_i^{(i)}/r_{ii}$}
\ENDFOR
\end{algorithmic}
"
497,1805.02158,"[!htb]        
	\caption{Baseline Algorithm $+$ Vector Extrapolation}           
		\label{alg:VE:Prac}                 
		\begin{algorithmic}[1]
			\REQUIRE ~\\
			Choose nonnegative integers $m$ and $\kappa$ and an initial vector $\vx_0$. The baseline algorithm is the FP method, as given in \eqref{eq:FP:recursive:explicit}. 
			\lastcon ~\\          %OUTPUT
			Final Solution $\vx^*$.\\
			\WHILE {1}
			%{\color{red} RK: Now should come the actual algorithm:}
			\STATE Obtain the series of $\vx_i$ through the baseline algorithm where $1\leq i\leq m+\kappa+1$, and save $\vx_{m+i}$ for $0\leq i\leq \kappa+1$ to formulate $\mU_m^\kappa$. \label{alg:VE:Prac:series}\\
			\STATE Call {\bf Algorithm \ref{alg:MPE_Van}} to obtain $\vx_{(m,\kappa)}$.
			\STATE If the termination of the algorithm is satisfied, set $\vx^*=\vx_{(m,\kappa)}$ and break, otherwise, set $\vx_{(m,\kappa)}$ as the new initial point $\vx_0$ and go to Step \ref{alg:VE:Prac:series}. 
			\ENDWHILE
			%\RETURN $\vx^*=\vx$
		\end{algorithmic}
	"
498,1804.04786,"
	% %		\caption{Sample Selection Algorithm}\label{alg:sample}
	% %		\begin{algorithmic}[1]
	% %			\Procedure{MyProcedure}{}
	% %			\State $\textit{stringlen} \gets \text{length of }\textit{string}$
	% %			\State $i \gets \textit{patlen}$
	% %			\BState \emph{top}:
	% %			\If {$i > \textit{stringlen}$} \Return false
	% %			\EndIf
	% %			\State $j \gets \textit{patlen}$
	% %			\BState \emph{loop}:
	% %			\If {$\textit{string}(i) = \textit{path}(j)$}
	% %			\State $j \gets j-1$.
	% %			\State $i \gets i-1$.
	% %			\State \textbf{goto} \emph{loop}.
	% %			\State \textbf{close};
	% %			\EndIf
	% %			\State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
	% %			\State \textbf{goto} \emph{top}.
	% %			\EndProcedure
	% %		\end{algorithmic}
	% %	"
499,1805.02087,"[]
 \KwData{CI oracle}
 \KwResult{$\widehat{\mathbb{G}}$}
 \BlankLine
 
Run FCI's skeleton discovery procedure (Algorithm \ref{fci_final_skel}).\\ \label{alg_skeleton}
Run FCI's v-structure orientation procedure (Algorithm \ref{fci_vstruc}). \\\label{alg_vstruc}

For any triple of vertices $\langle O_i, O_k, O_j \rangle$ such that we have $O_k \circline \!\! * O_i$, if there is a set in $\textnormal{Sep}(O_i,O_j)$ discovered in Step \ref{alg_skeleton} such that $O_k \not \in \textnormal{Sep}(O_i,O_j)$, $O_i \not \ci_d O_k | \textnormal{Sep}(O_i,O_j) \cup \bm{S}$ and $O_j \not \ci_d O_k | \textnormal{Sep}(O_i,O_j) \cup \bm{S}$, then orient $O_k \circline \!\! * O_i$ as $O_k \leftarrow \!\! * O_i$. \label{alg_addV}

Find additional non-minimal d-separating sets using Algorithm \ref{alg_CCI_E}.\\ \label{alg_addD}

Find all quadruples of vertices $\langle O_i, O_j, O_k, O_l \rangle$ such that $O_i$ and $O_k$ non-adjacent, $O_i * \!\! \rightarrow O_l \leftarrow \!\! * O_k$, and $O_i \ci_d O_k|\bm{W} \cup \bm{S}$ with $O_j \in \bm{W}$ and $\bm{W} \subseteq \bm{O} \setminus \{O_i, O_k\}$. If $O_l \not \in \bm{W} = \textnormal{Sep}(O_i,O_k)$ as discovered in Step \ref{alg_skeleton}, then orient $O_j * \!\! \linecirc O_l$ as $O_j * \!\! \rightarrow O_l$. If we also have $O_i * \!\! \rightarrow O_j \leftarrow \!\! * O_k$ and $O_l \in \bm{W} = \textnormal{SupSep}(O_i,O_j,O_k)$ as discovered in Step \ref{alg_addD}, then orient $O_j * \!\! \linecirc O_l$ as $O_j * \!\! - O_l$.  \label{alg_step5}

If we have $O_i \ci_d O_k|\bm{W} \cup \bm{S}$ for some $\bm{W} \subseteq \bm{O} \setminus \{O_i, O_k\}$ with $O_j \in \bm{W}$ but we have $O_i \not \ci_d O_k|O_l \cup \bm{W} \cup \bm{S}$, then orient $O_l \circline \!\! * O_j$ as $O_l \leftarrow \!\! * O_j$. \\\label{alg_step6}


Execute orientation rules 1-7.\\ \label{alg_OR}
 
 \BlankLine

 \caption{Cyclic Causal Inference (CCI)} \label{alg_CCI}
"
500,1805.02087,"[]
 \KwData{$\widehat{\mathcal{G}}$, CI oracle}
 \KwResult{$\widehat{\mathcal{G}}$, \textnormal{SupSep}}
 \BlankLine
 
$m = 0$\\
\Repeat{all ordered triples $\langle O_i, O_j, O_k \rangle$ with the v-structure $O_i * \!\! \rightarrow O_j \leftarrow \!\!* O_k$ have $|\textnormal{PD-SEP}(O_i)| < m$}{
	\Repeat{ all triples $\langle O_i, O_j, O_k \rangle$ with the v-structure $O_i * \!\! \rightarrow O_j \leftarrow \!\!* O_k$ and $|\textnormal{PD-SEP}(O_i)| \geq m$ have been selected}{
    	select the ordered triple $\langle O_i, O_j, O_k \rangle$ with the v-structure $O_i * \!\! \rightarrow O_j \leftarrow \!\!* O_k$ such that $|\textnormal{PD-SEP}(O_i)| \geq m$\\
        \Repeat{all subsets $\bm{W} \subseteq \textnormal{PD-SEP}(O_i) \setminus \{\textnormal{Sep}(O_i,O_k) \cup \{O_j, O_k\}\}$ have been considered or a d-separating set of $O_i$ and $O_k$ has been recorded in $\textnormal{SupSep}(O_i,O_j,O_k)$}{
        	select a subset $\bm{W} \subseteq \textnormal{PD-SEP}(O_i) \setminus \{\textnormal{Sep}(O_i,O_k) \cup \{O_j, O_k\}\}$ with $m$ vertices\\
            $\bm{T} = \bm{W} \cup  \textnormal{Sep}(O_i,O_k) \cup O_j$ \\ \label{step4_T}
        	if $O_i$ and $O_k$ are d-separated given $\bm{T} \cup \bm{S}$, then record the set $\bm{T}$ in $\textnormal{SupSep}(O_i,O_j,O_k)$ \label{step4_CI}
        }
    }
}

 \BlankLine

 \caption{Step \ref{alg_addD} of CCI} \label{alg_CCI_E}
"
501,1805.02087,"[]
 \KwData{CI oracle}
 \KwResult{$\widehat{\mathbb{G}}$, \textnormal{Sep}, $\mathcal{M}$}
 \BlankLine
 
 Form a complete graph $\widehat{\mathbb{G}}$ on $\bm{O}$ with vertices $\circlinecirc$ \\
 $l \leftarrow -1$ \\
 \Repeat{all pairs of adjacent vertices $(O_i, O_j)$ in $\widehat{\mathbb{G}}$ satisfy $|\textnormal{Adj}(O_i)\setminus O_j| \leq l$}{
 Let $l=l+1$ \\
 \Repeat{all ordered pairs of adjacent vertices $(O_i, O_j)$ in $\widehat{\mathbb{G}}$ with $|\textnormal{Adj}(O_i)\setminus O_j| \geq l$ have been considered}{
 \ForAll{vertices in $\widehat{\mathbb{G}}$}{
 	Compute $\textnormal{Adj}(O_i)$
 }
 Select a new ordered pair of vertices $(O_i, O_j)$ that are adjacent in $\widehat{\mathbb{G}}$ and satisfy $|\textnormal{Adj}(O_i)\setminus O_j| \geq l$ \\
 
 \Repeat{$O_i$ and $O_j$ are no longer adjacent in $\widehat{\mathbb{G}}$ or all $\bm{W} \subseteq \textnormal{Adj}(O_i)\setminus O_j$ with $|\bm{W}| = l$ have been considered }{
 
 Choose a new set $\bm{W} \subseteq \textnormal{Adj}(O_i)\setminus O_j$ with $|\bm{W}|=l$ \\
 
 \If{$O_i \ci O_j|\bm{W} \cup \bm{S}$}{
 	Delete the edge $O_i \circlinecirc O_j$ from $\widehat{\mathbb{G}}$ \\
 	Let $\textnormal{Sep}(O_i,O_j) = \textnormal{Sep}(O_j,O_i) = \bm{W}$
 }
 
 
 }
 }
 }
 
 Form a list $\mathcal{M}$ of all unshielded triples $\langle O_k,\cdot,O_m \rangle$ (i.e., the middle vertex is left unspecified) in $\widehat{\mathbb{G}}$ with $k < m$

 \BlankLine

 \caption{PC's skeleton discovery procedure} \label{pc_skel}
"
502,1805.02087,"[] \label{fci_vstruc}
 \KwData{$\widehat{\mathbb{G}}$, \textnormal{Sep}, $\mathcal{M}$}
 \KwResult{$\widehat{\mathbb{G}}$}
 \BlankLine
 
 \ForAll{elements $\langle O_i, O_j, O_k \rangle$ in $\mathcal{M}$}{
 	\If{$O_j \not \in \textnormal{Sep}(O_i, O_k)$}{
    	Orient $O_i * \!\! \linecirc O_j \circline \!\! * O_k$ as $O_i* \!\! \rightarrow O_j \leftarrow \!\! * O_k$ in $\widehat{\mathbb{G}}$
    }
 }
 \BlankLine

 \caption{Orienting v-structures} \label{fci_vstruc}
"
503,1805.02087,"[]
 \KwData{$\widehat{\mathbb{G}}$, Sep}
 \KwResult{$\widehat{\mathbb{G}}$, Sep, $\mathcal{M}$}
 \BlankLine
 
 \ForAll{vertices $O_i$ in $\widehat{\mathbb{G}}$}{
 	Compute $\textnormal{PD-SEP}(O_i)$ \\
    \ForAll{vertices $O_j \in \textnormal{Adj}(O_i)$}{
     Let $l = -1$ \\
 \Repeat{$O_i$ and $O_j$ are no longer adjacent in $\widehat{\mathbb{G}}$ or $|\textnormal{PD-SEP}(O_i)\setminus O_j|<l$ }{
 		Let $l=l+1$ \\
        \Repeat{$O_i$ and $O_j$ are no longer adjacent in $\widehat{\mathbb{G}}$ or all $\bm{W} \subseteq \textnormal{PD-SEP}(O_i)\setminus O_j$ with $|\bm{W}| = l$ have been considered}{
 		Choose a (new) set $\bm{W} \subseteq \textnormal{PD-SEP}(O_i) \setminus {O_j}$ with $|\bm{W}| = l$ \\
         \If{$O_i \ci O_j | \bm{W} \cup \bm{S}$}{
         	Delete edge $O_i * \!\! - \!\! * O_j$ in $\widehat{\mathbb{G}}$\\
            Let \textnormal{Sep}($O_i, O_j$) = \textnormal{Sep}($O_j, O_i$) $= \bm{W}$
         }
 	}
    }
    
    } 
 }
 Reorient all edges in $\widehat{\mathbb{G}}$ as $\circlinecirc$\\
 Form a list $\mathcal{M}$ of all unshielded triples $\langle O_k, \cdot, O_m \rangle$ in $\widehat{\mathbb{G}}$ with $k<m$
 \BlankLine

 \caption{Obtaining the final skeleton in the FCI algorithm}  \label{fci_final_skel}
"
504,1805.02087,"[]
 \KwData{Initial skeleton $\widehat{\mathbb{G}}$, Sep, $\mathcal{M}$}
 \KwResult{$\widehat{\mathbb{G}}$, Sep}
 \BlankLine
 
 Let $\mathcal{L}$ denote an empty list \\ \label{rfci_vs_start}
 
 \While{$\mathcal{M}$ is non-empty}{
	 Choose an unshielded triple $ \langle O_i, O_j, O_k \rangle$ from $\mathcal{M}$ \\
     \If{ $O_i \ci O_j | \textnormal{Sep}(O_i,O_k) \cup \bm{S}$ and $O_j \ci O_k | \textnormal{Sep}(O_i,O_k) \cup \bm{S}$}{
     	Add $ \langle O_i, O_j, O_k \rangle$ to $\mathcal{L}$ 
	} \Else{
    	\For{ $r \in \{i,k\}$ }{
        	\If{ $O_r \ci O_j| ( \textnormal{Sep}(O_i,O_k) \setminus O_j ) \cup \bm{S}$}{
            	Find a minimal separating set $\bm{W} \subseteq \textnormal{Sep}(O_i, O_k)$ for $O_r$ and $O_j$ \\
                Let $\textnormal{Sep}(O_r, O_j) = \textnormal{Sep}(O_j, O_r) = \bm{W}$ \\
                Add all triples $\langle O_{\text{min}(r,j)}, \cdot, O_{\text{max}(r,j)} \rangle$ that form a triangle in $\widehat{\mathbb{G}}$ into $\mathcal{M}$ \\
                Delete from $\mathcal{M}$ and $\mathcal{L}$ all triples containing $(O_r,O_j):$ $\langle O_r, O_j,\cdot \rangle$, $\langle O_j, O_r,\cdot \rangle$, $\langle \cdot,O_j, O_r \rangle$ and $\langle \cdot,O_r, O_j \rangle$ \\
                Delete edge $O_r * \!\! - \!\! * O_j$ in $\widehat{\mathbb{G}}$
            }
        
        }
    }
     Remove $\langle O_i, O_j, O_k \rangle$ from $\mathcal{M}$
     }  \label{rfci_vs_end}
    \ForAll{elements $\langle O_i, O_j, O_k \rangle$ of $\mathcal{L}$}{ 
    \If{ $O_j \not \in \textnormal{Sep}(O_i,O_k)$ and both $O_i * \!\! - \!\! * O_j$ and $O_j * \!\! - \!\! * O_k$ are present in $\widehat{\mathbb{G}}$ }{
    	Orient $O_i* \!\! \linecirc O_j \circline \!\! * O_k$ as $O_i* \!\! \rightarrow O_j \leftarrow \!\! * O_k$ in $\widehat{\mathbb{G}}$ 
    }
    }  

 
 \BlankLine

 \caption{Orienting v-structures in the RFCI algorithm}  \label{rfci_vstruc}
"
505,1805.01627,"[t!]
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} Time horizon $T$, Number of arms $K$, Prior on parameters $B_0$, Reward function $f$, Exposure $\tau(t)$.
   \FOR{$t=1$ {\bfseries to} $T$}
   \STATE{$/*$ \textit{I-projection} $*/$}
   \STATE Draw arm $a_t$ such that
   \vspace{-.75em}
   \[\hspace*{-0.2cm}
    a_t = \argmin_{a} D_{\mathrm{KL}}\left(\mathbb{P}^a_{t-1}(X,\theta)\|\bar{\mathbb{Q}}_{t-1}(X,\theta)\right).
     \]
   \vspace{-1em}
   \STATE{$/*$ \textit{Accumulation of observables} $*/$}
   \STATE Sample a reward $x_t$  out of $f_{\theta_{a_t}}$.
   %\STATE Update the history to $\mathcal{H}_t$ by appending $(x_t, a_t)$ to $\mathcal{H}_{t-1}$.
   \STATE Update the belief-reward distribution of $a_t$ to $\mathbb{P}^a_{t}(X, \theta)$ using Bayes' theorem.   
   \STATE{$/*$ \emph{Reverse I-projection} $*/$} 
   \STATE Update the pseudobelief-reward distribution to%\label{line:alg:belman:1}
   \vspace{-.75em}
   \[\hspace*{-0.7cm}
   \bar{\mathbb{Q}}_{t}(X,\theta) =\argmin_{\bar{\mathbb{Q}} \in \mathcal{B}_\theta\mathcal{R}} \sum_{a = 1}^K D_{\mathrm{KL}}\left(\mathbb{P}^a_t(X,\theta)\|\bar{\mathbb{Q}}(X,\theta)\right).
   \]
   \vspace{-1em}
   \ENDFOR
\end{algorithmic}
\caption{BelMan}\label{alg:belman}
"
506,1805.01577,"[#1]}
  {"
507,1805.01209,"[H]
  \DontPrintSemicolon
  \KwIn{Graph $G = (V, E)$}
  \KwOut{Vertex cover $M$ with $\lvert M \rvert \le 2k^*$}
  \caption{Maximal matching 2-approximation for minimum vertex cover}
  $M \leftarrow \emptyset$\;
  \For{$e = (u, v) \in E$}{
    \lIf{$u \notin M$ and $v \notin M$}{$M \leftarrow M \cup \{u, v\}$}
  }
\label{alg:matching}
"
508,1708.05929,"[h]
{
	\caption{\method ~($\mA\cup \mN$): Explaining Anomalous Patterns\label{alg:final}}
	\begin{algorithmic}[1]
		\REQUIRE dataset $\mD=\mA\cup \mN$ with labeled anomalies
		\ENSURE set of anomalous patterns (represented as hyper-ellipsoids) \\$\mP=\{p_1(\mc_1,\mM_1), \ldots, p_K(\mc_K,\mM_K)\}$
		\STATE Set of hyper-rectangles $\mR = \emptyset$ 
		\STATE Obtain $\mR^{(1)}$ (1-d intervals) by kernel density estimation, varying cut-off threshold in $q=\{80,85,90,95\}$
		\STATE $\widehat{f}_a :=$  distribution of number of anomalies across $\mR^{(1)}$
		\STATE $\widehat{f}_n :=$  distribution of number of normal points across $\mR^{(1)}$ 
%		\FOR{$ms=\widehat{f}_a.\text{quantiles}(50, 60, \ldots, 90)$}
%		\FOR{$\mu=\widehat{f}_n.\text{quantiles}(50, 40, \ldots, 10)$}
		\STATE  $\mR := \; $\sub($\mD$, $ms=q(\widehat{f}_a,50)$, $\mu=q(\widehat{f}_n),50$)) by Alg. \ref{alg:subclus} in \S \ref{ssec:rectangle}
%		\ENDFOR
%		\ENDFOR
	%	\STATE $R:=$ {\tt ParetoFrontier}($R$)
		\STATE Set of hyper-ellipsoids $\mE = \emptyset$ 
		\FOR{$R\in \mR$}
			\STATE $\mE_R = \emptyset$ 
			\FOR{$\alpha=\{10^{-6}, 10^{-5},\ldots, 1\}$}
			\FOR{$\lambda=\{10^{-3}, 10^{-2},\ldots, 10^3\}$}
			\STATE  $\mE_R := \mE_R \; \cup $ solve  optimization problem in \S\ref{ssec:refine} for ($R,\alpha,\lambda$) 
			\ENDFOR
			\ENDFOR
			\STATE $\mE:= \mE \; \cup$ {ParetoFrontier}($\mE_R$)
		\ENDFOR
		
		\STATE \textbf{for} $K=1,\ldots, |\mA|$: select a subset $\mS^*_K\subset \mE$ of $K$ \ps~using the cardinality-constrained {\sc Random-Greedy} algorithm by Buchbinder et
al. \cite{conf/soda/BuchbinderFNS14} to optimize the description length reduction objective $R_{\ell}(\cdot)$ in \S\ref{ssec:summarize}.\\
		\RETURN $\mP:= \arg\max_{\mS^*_K} \; R'_\ell(\mS^*_K) - \log^*K$
	\end{algorithmic}
}
"
509,1708.05929,"[!t]{\caption{{\sc SubClus}~($\mD, ms,\mu$)}%: Finding Groups of Anomalies in Subspaces}
				\label{alg:subclus}}
			{
				\begin{algorithmic}[1]
					\REQUIRE dataset $\mD=\mA\cup \mN \in \mathbb{R}^{m\times d}$ with labeled anomalous and normal points, mass threshold $ms \in \mathbb{Z}$, purity threshold $\mu \in \mathbb{Z}$
					\ENSURE set of hyper-rectangles $\mR = \{R_1, R_2, \ldots\}$ each containing min. $ms$ anomalous \& max. $\mu$ normal points \\
					\STATE Let $\mR^{(k)}$ denote $k$-dimensional hyper-rectangles. 
					Initialize $\mR^{(1)}$ by kernel density estimation with varying quantile thresholds in $q=\{80,85,90,95\}$, set $k=1$
					
					\FOR{{\bf each} hyper-rectangle $R \in \mR^{(k)}$}
					\IF {$\text{mass}(R) \geq ms$}
					\LINEIF{$\text{impurity}(R) \leq \mu$}{$\mR_{pure}^{(k)} = \mR_{pure}^{(k)} \cup R$\\ {\bf else } $\mR_{\neg pure}^{(k)} = \mR_{\neg pure}^{(k)} \cup R$}
					\ENDIF
					\ENDFOR
					\STATE $\mR=\mR\cup \mR_{pure}^{(k)}$
					\STATE $\mR^{(k+1)} := \text{generateCandidates}(\mR_{pure}^{(k)} \cup \mR_{\neg pure}^{(k)} )$
					\LINEIF{$\mR^{(k+1)}=\emptyset$}{
						{\bf return} $\mR$} % {\tt dominantSet}$(S)$}
					\STATE  $k=k+1$, go to step 2
					
				\end{algorithmic}
			}
		"
510,1805.00980,"
\begin{footnotesize}
\caption{SaaS Algorithm}
\label{pseudo}
\begin{algorithmic}[1]
%\State {\bf SaaS Algorithm} 
	\State $P^u \sim {\cal N}(0, I)$
    \State Select learning rates $\eta$ for the weights $\eta_w$ and label posteriors $\eta_{P^u}$
    \State \textbf{Phase I}: Estimate $P^u$
    \While{$P^u$ has not stabilized}
        \State $P^u = \Pi(P^u)$ (project posterior onto the probability simplex)
        \State $w_1 \sim {\cal N}(0, I)$
        \State $\Delta P^u = 0$
        \State // Run SGD for $T$ steps (on the weights) to estimate loss decrease
        \For{$t = 1 : T$}
        \State $w_{t-\frac{1}{2}} = w_{t-1} - \eta_{w} \nabla_{w_{t-1}} \left(\ell(B^u_t, P^u; w_{t-1})+ \beta q(B^u_t; w_{t-1}) \right)$
		\State $w_{t} = w_{t-\frac{1}{2}} - \eta_{w} \nabla_{w_{t-\frac{1}{2}}} \ell(B^l_t, P^l; w_{t-\frac{1}{2}})$
        \State $\Delta P^u = \Delta P^u + \nabla_{P^u} \ell(B^u_t, P^u; w_t)$
        \EndFor
        \State // Update the posterior distribution
        \State $P^u = P^u - \eta_{P^u}  \Delta P^u$ 
    \EndWhile
    \State \textbf{Phase II}: Estimate the weights.
    \State  $\hat y_i^u = \arg\max_i P^u_i \ \forall i=1,\ldots,N^u$ 
    \While {$w$ has not stabilized}
    \State  $w_1 \sim {\cal N}(0, I)$
     \State $w_{t-\frac{1}{2}} = w_{t-1} - \eta_{w} \nabla_{w_{t-1}} \frac{1}{|B^u_t|} \sum_{i = 1}^{|B^u_t|} \ell(x^u_i, \hat y^u_i; w_{t-1})$
     \State $w_{t} = w_{t-\frac{1}{2}} - \eta_{w} \nabla_{w_{t-\frac{1}{2}}} \frac{1}{|B^l_t|} \sum_{i = 1}^{|B^l_t|} \ell(x^l_i, y^l_i; w_{t-\frac{1}{2}})$
    \EndWhile
\end{algorithmic}
\end{footnotesize}
"
511,1805.03696,"[htbp!]
 
\small
	\caption{\textit{Bayeslands} framework}
    \label{alg:alg1}
    
	 Initialize $\bs\theta=\bs\theta^{[0]}$  by drawing $\bs\theta^{[0]}$ 
from the prior $\bs\theta^{[0]}\sim p(\bs \theta)$ \\
	For $i=1:Samples$
	\begin{algorithmic}[1]
    
    %\While {true} 
    \STATE Propose a value $\bs\theta^{[p]}|\bs\theta^{i-1}\sim 
q(\bs\theta^{[i-1]})$, where $q(.)$ is the proposal distribution. 
	\STATE Given  $\bs\theta^{[p]}$, execute   \textit{Badlands} and 
compute 
$g_{Tmax,i,j}\left(\bs\theta^{[p]}\right)$, and  
$f_{t,i,j}\left(\bs\theta^{[p]}\right)$  
	\STATE Calculate the acceptance probability $\alpha$, as given by 
Equation~\ref{eqn_accept}.
 
 \STATE Generate $u \sim U(0,1)$ and set $ \bs\theta^{[i]} = \bs\theta^{[p]}$ 
if 
$\alpha < u$. Otherwise, set $\bs\theta^{[i]} =\bs\theta^{[i-1]} $.
 %\EndWhile
\end{algorithmic}
"
512,1707.02963," [h!]
   \caption{IGA algorithm.}
   \label{alg:general}
  \begin{algorithmic}[1]
    \REQUIRE{$\delta>0$, $0<\lambda\leq 1$}
    \ENSURE{$\w^{(k)}$}
    \STATE{$k=0,\w^{(k)}=0, G^{(0)}=\emptyset$}
    \WHILE{TRUE} 
    \IF{{$ \displaystyle{Q(\w^{(k)})-\min_{g\notin G^{(k)},\boldsymbol\alpha\in\real^{\abs{F_g}}}Q(\w^{(k)}+\mathbf{E}_{g} \boldsymbol\alpha)}<\delta$} }
	    \STATE{Break}
        \ENDIF
	\STATE{{  select an element as $g^{(k)}$ from the group set\\ $\mathcal A_{\lambda}:=\{ g \notin G^{(k)} \  | \  
	(Q(\w^{(k)})-\underset{\boldsymbol\alpha\in\real^{\abs{F_g}}}{\operatorname{min}}\ Q(\w^{(k)}+\mathbf{E}_{g}
        \boldsymbol\alpha))\geqslant \lambda \ (Q(\w)-\underset{\tilde
          g \notin G^{(k)},\,\boldsymbol\alpha\in\real^{\abs{F_{\tilde g}}}}{\operatorname{min}}\ 
	\ Q(\w^{(k)}+\mathbf{E}_{\tilde g} \boldsymbol\alpha))\}$ } }
      \STATE{$G^{(k+1)}={G^{(k)}\cup \{g^{(k)}\}}$}
	\STATE{$\w^{(k+1)}=\underset{\substack{\w \in \mathbb{R}^p:\\
              \textrm{supp }(\w)\subset F_{G^{(k+1)}} }}{\operatorname{argmin}}\ Q(\w)$}
        \STATE{$\delta^{(k+1)}=Q(\w^{(k)})-Q(\w^{(k+1)})$}
	\STATE{$k=k+1$}
        \WHILE{TRUE}
	\IF{$\underset{g\in G^{(k)}}{\min}Q(\w^{(k)} -\mathbf{E}_{g} \w_g^{(k)})-Q(\w^{(k)})\geqslant\frac{\delta^{(k)}}{2}$}
             \STATE{Break}
	 \ENDIF
	 \STATE{$g^{(k)}=\underset{g\in G^{(k)}}{\operatorname{argmin}} \, Q(\w^{(k)} -\mathbf{E}_{g} \w_g^{(k)})$}
	  \STATE{$k=k-1$}
	  \STATE{$G^{(k)}=G^{(k+1)} {\setminus} \{g^{(k+1)}\}$}
	           	  	  \STATE{$\w^{(k)}=\underset{\substack{\w
                                        \in \mathbb{R}^p:\\
                                        \textrm{supp }(\w)\subset
                                        F_{G^{(k)}}} }{\operatorname{argmin}}\ Q(\w)$}
        	\ENDWHILE
    \ENDWHILE
  \end{algorithmic}
"
513,1707.02963,"
   \caption{Gradient-based IGA algorithm.}
   \label{alg:gdt}
  \begin{algorithmic}[1]
    \REQUIRE{$\delta>0$, $0<\lambda\leq 1$}
    \ENSURE{$\w^{(k)}$}
    \STATE{$k=0,\w^{(k)}=0,F^{(0)}=\emptyset,G^{(0)}=\emptyset$}
    \WHILE{TRUE} 
    \IF{ { $\|\nabla Q(\w^{(k)})\|_{G,\infty}< \varepsilon$ }}
	    \STATE{Break}
        \ENDIF
	\STATE{  select any $g^{(k)} \textrm{ in } \{ g \notin G^{(k)}\ \ | 
	\ \ \|\nabla_{g} Q(\w^{(k)})\| \geqslant \ \lambda\ \underset{\tilde
          g \notin G^{(k)}}{\operatorname{max}}\|\nabla_{\tilde g} Q(\w^{(k)})\| \}$ }
	%\State{$F^{(k+1)}={F^{(k)}\cup g^{(k)}}$}
      \STATE{$G^{(k+1)}={G^{(k)}\cup \{g^{(k)}\}}$}
      	         	\STATE{$\w^{(k+1)}=\underset{\substack{\w \in
                              \mathbb{R}^p:\\ \textrm{supp
                              }(\w)\subset F_{G^{(k)}}} }{\operatorname{argmin}}\ Q(\w)$}
        \STATE{$\delta^{(k+1)}=Q(\w^{(k)})-Q(\w^{(k+1)})$}
	\STATE{$k=k+1$}

        \WHILE{TRUE}
	\IF{$\underset{g\in G^{(k)}}{\min}Q(\w^{(k)} -\mathbf{E}_{g} \w_g^{(k)})-Q(\w^{(k)})\geqslant\frac{\delta^{(k)}}{2}$}
             \STATE{Break}
	 \ENDIF
	 \STATE{$g^{(k)}=\underset{g\in G^{(k)}}{\operatorname{argmin}}\ Q(\w^{(k)} -\mathbf{E}_{ g}\w_g^{(k)})$}
	  \STATE{$k=k-1$}
	  %\State{$F^{(k)}=F^{(k+1)} {\setminus} (F^{(k+1)})^{c} \bigcap g_{i^{(k+1)}}$}
	  \STATE{$G^{(k)}=G^{(k+1)} {\setminus} \{g^{(k+1)}\}$}
	            	  	  \STATE{$\w^{(k)}=\underset{\substack{\w
                                        \in \mathbb{R}^p:\\
                                        \textrm{supp }(\w)\subset
                                        F_{G^{(k)}}} }{\operatorname{argmin}}\ Q(\w)$}
	\ENDWHILE
    \ENDWHILE
  \end{algorithmic}
"
514,1805.00597," \label{alg:learning}  
	\caption{Structured Analysis Dictionary Learning}  
	%\resizebox{\columnwidth}{!}{
	\begin{algorithmic}[1] 
%		\Require 
%		Training data $X=[x_1,\dots,x_n]$, diagonal block matrix $H$, classes labels $L$, parameter $\lambda_1$, $\lambda_2$, $\lambda_3$, $\lambda_4$ and maximum iteration $T$;
%		\Ensure 
%		Analysis dictionary $\Omega$, analysis sparse representation $U$, and linear transformation $Q$ and $W$; 
		\State Initialize $\Omega$, $Q$, and $W$ as random matrices, and initialize $U$ as a zero matrix; $T$ is maximum iteration;
		\While {not converged \textbf{and} $k < T$} %$\% T$ is maximum iteration
		\State $k=k+1;$
		\State \resizebox{0.9\columnwidth}{!}{$U_{k+1}=\tau_{\frac{\lambda_1}{\mu_k(\eta_Q+\eta_{WQ})}} \left(U_k-\frac{\bigtriangledown_UL(\Omega_k,U_k,Q_k,W_k,Y^{(1)}_k,Y^{(2)}_k)}{\mu_k (\eta_Q+\eta_{WQ})}\right);$}
		\State $Q_{k+1}=Q_k-\frac{\bigtriangledown_QL(\Omega_k,U_{k+1},Q_k,W_k,Y^{(1)}_k,Y^{(2)}_k) }{\mu_k(\eta_Q+\eta_{WU})},$
		\State $W_{k+1}=W_k-\frac{\bigtriangledown_WL(\Omega_k,U_{k+1},Q_{k+1},W_k,Y^{(1)}_k,Y^{(2)}_k) }{\mu_k\eta_{QU}};$
		\State $\Omega_{k+1}=U_{k+1}X^T(XX^T+\lambda_4 I)^{-1};$
		\State Normalize $\Omega_{k+1}$ by $\omega_i^T=\frac{\omega_i^T}{\|\omega_i^T\|_2}, \forall i;$
		\State $Y_{k+1}^{(1)}=Y_{k}^{(1)}+\mu_k(H-Q_{k+1}U_{k+1});$
		\State $Y_{k+1}^{(2)}=Y_{k}^{(2)}+\mu_k(L-W_{k+1}Q_{k+1}U_{k+1});$
		\State $\mu_{k+1}=\min\{\rho\mu_k,\mu_{max}\};$ $\%\rho$ is the learning rate
		\EndWhile
		%\State \textbf{end while}
	\end{algorithmic}  
"
515,1802.07966,"
	
	\KwData{An instance of $ILP^{DE}({B,M,\{E_1,\ldots,E_n\}}$)}
	\KwResult{A solution to the problem}
	\tcc{initialize a stack with the solutions of $ILP(B,M,E_1)$}
	\textit{stack} = $XHAIL(ILP(B,M,E_1))$\; 
	\While{stack is not empty}{
		\tcc{pop the hypothesis from the top}
		$\langle H_I, H_G \rangle$ = $stack.pop()$\;
		
		\tcc{get an example $E_i$ such that $B\cup H_I \cup O_i \nvdash E_i^+$ or $B\cup H_I \cup O_i \vdash E_i^-$}
		$E_i$ = $nextUncoveredExample(H_I)$\;
		
		\tcc{No such example exists}
		\If{$E_i$ is null}
		{\tcc{found a solution} return $H_I$.} 
		\Else{ \tcc{Find expansions of $H_I$ that also solves $E_i$}
			$refinementsStack$ = $<>$ \;
			\tcc{support set denotes the set of examples from which $<H_i,H_G>$ is created}
			$supports$ = $supportSet(H_I)\cup \{E_i\}$\;
			
			\tcc{compute a set of lower bound-upper bound pairs for the search space.}
			$H_G(E_i)$ = $findGeneralizatons(B,M,E_i)$\;
			\ForEach{H in $H_G(E_i)$}{\textit{push} $\langle H_I,H_G\cup H \rangle $ to $refinementsStack$} 
			
			\While{refinementsStack is not empty}{
				\tcc{get a candidate lower bound-upper bound pair}
				$\langle H'_I,H'_G \rangle$ = $refinementsStack.pop()$\;
				
				\tcc{get an example from $supports$ that is not covered by $H'_I$}
				$E_j$ = $nextUncoveredExampleFromS(H'_I,$\ $supports)$\;
				
				\If{$E_j$ is null}{
					\tcc{if no such example exists then we found a solution to the subproblem. Push it to the $stack$.}
					\textit{push} $\langle H'_I,H'_G \rangle$ to $stack$; 
				}			
				
				\Else{
					\tcc{Expand $H'_I$ minimaly along $H'_G$ so that it covers $E_j$}
					$expansions$ = \textit{expandMinimal}($\langle H'_I, H'_G \rangle, B, E_j $)\;
					\tcc{Push all expansions in the $refinementsStack$ for further updates.}
					\ForEach{$\langle H''_I,H''_G \rangle$ in $expansions$}{\textit{refinementsStack}.\textit{push}($\langle H''_I,H''_G \rangle$)}
				}
			}
		}
	}
	
	\caption{$I^2XHAIL$}
	\label{algo}
	
	
"
516,1704.06767,"[t]
\caption{Generation of Training / Test Sets for Benchmark MIL Datasets} \label{alg:benchmark-set-generation}
\begin{algorithmic}[1]
\Require $\mathcal{D}_\positive$: original positive bags, $\mathcal{D}_\negative$: original negative bags, $\pi$: true bag-level class prior,
$L$: \#\{labeled positive bags\}, $U$: \#\{unlabeled bags\}, $T$: \#\{test bags\}
\State $\mathcal{D}_\mathrm{L} \subset \mathcal{D}_\positive$ \Comment{$\lvert\mathcal{D}_\mathrm{L}\rvert = L$}
\State $\mathcal{D}_\positive := \mathcal{D}_\positive \setminus \mathcal{D}_\mathrm{L}$
\State $N_\unlabeled^\positive \sim B(U + T, \pi)$
\State $N_\unlabeled^\negative := U + T - N_\unlabeled^\positive$
\State $\mathcal{D}_\unlabeled := \mathcal{D}_\positive'(\subset\mathcal{D}_\positive) \cup \mathcal{D}_\negative'(\subset\mathcal{D}_\negative)$
\Comment{$\lvert\mathcal{D}_\positive'\rvert = N_\unlabeled^\positive, \lvert\mathcal{D}_\negative'\rvert = N_\unlabeled^\negative$}
\State $\mathcal{D}_\unlabeled' \subset \mathcal{D}_\unlabeled$ \Comment{$\lvert\mathcal{D}_\unlabeled'\rvert = T$}
\State $\mathcal{D}_\unlabeled := \mathcal{D}_\unlabeled \setminus \mathcal{D}_\unlabeled'$
\Ensure $\mathcal{D}_\mathrm{L} \cup \mathcal{D}_\unlabeled$: training set, $\mathcal{D}_\unlabeled'$: test set
\end{algorithmic}
"
517,1804.10745,"[H]
  \caption{\crossgrad\ training pseudocode.}\label{alg}
  \begin{algorithmic}[1]
    \STATE {\bf Input:} Labeled data $\{(\vx_i, y_i, d_i)\}_{i=1}^M$, step sizes $\epsilon_l, \epsilon_d$, learning rate $\eta$, data augmentation weights $\alpha_l, \alpha_d$, number of training steps $n$.
    \STATE {\bf Output:} Label and domain classifier parameters $\theta_l,\theta_d$
    \STATE Initialize $\theta_l,\theta_d$
    \COMMENT {$J_l, J_d$ are loss functions for the label and domain classifiers, respectively}
    \FOR{$n$ training steps}
      \STATE Sample a labeled batch $(X,Y,D)$
%      \STATE $\zeta_l(X,Y;\theta_l) := \nabla_{X} J_l(X,Y;\theta_l)$
%      \STATE $\zeta_d(X,D;\theta_d) := \nabla_{X} J_d(X,D;\theta_d)$
      \STATE $X_d := X + \epsilon_l \cdot \nabla_{X} J_d(X, D; \theta_d)$
      \STATE $ X_l  := X + \epsilon_d \cdot \nabla_{X} J_l(X,Y;\theta_l)$
      \STATE $\theta_l \leftarrow \theta_l - \eta\nabla_{\theta_l} ((1 - \alpha_l) J_l(X, Y;\theta_l) + \alpha_l J_l(X_d, Y;\theta_l))$
      \STATE $\theta_d \leftarrow \theta_d - \eta\nabla_{\theta_d} ((1 - \alpha_d) J_d(X, D; \theta_d) + \alpha_d J_d(X_l, D; \theta_d))$
    \ENDFOR
    \renewcommand{\baselinestretch}{1}
  \end{algorithmic}
"
518,1804.10745,"
%  \caption{CrossGrad Training Method}\label{alg}
%  \begin{algorithmic}[1]
%    \STATE {\bf Input :} Labeled data \{$(x_i, y_i, d_i)\}_{i=1}^M$, gradient step $\epsilon$, learning rate $\eta$
%    \FOR{number of training steps}
%      \STATE Sample a mini-batch of training data D
%      \STATE $ J_l = -\log C_l(D_y, D_x, \theta_l)$
%      \STATE $ J_d = -\log C_s(D_d, D_x, \theta_d)$
%      \STATE $\zeta_L \leftarrow \nabla_{D_x} J_l(x_i,y_i)$
%      \STATE $\zeta_D \leftarrow \nabla_{D_x} J_d(x_i,d_i)$
%      \STATE Stop gradient flow along $\zeta_L, \zeta_D$
%      \STATE \COMMENT{Crossgrad Adversarial Terms}
%      \STATE $ J_l^d = -\log C_l(D_y, D_x + \epsilon \zeta_D, \theta_l)$
%      \STATE $ J_d^l = -\log C_d(D_y, D_x + \epsilon \zeta_L, \theta_d)$
%      \STATE \COMMENT{Compute Joint objective}
%      \STATE $J \leftarrow J_l + J_d + J_l^d + J_d^l$
%      \STATE \COMMENT{Update classifiers by GD}
%      \STATE $\theta_l \leftarrow \theta_l - \eta\nabla_{\theta_l} J$
%      \STATE $\theta_d \leftarrow \theta_d - \eta\nabla_{\theta_d} J$
%    \ENDFOR
%  \end{algorithmic}
%"
519,1709.05394,"
\caption{{\bf Lexicase Selection} applied to individuals $n \in \mathcal{N}$ with errors $e_t(n)$ on training cases $t \in \mathcal{T}$. }\label{alg:lex}
\noindent{\footnotesize
\begin{tabularx}{\textwidth}{lX}
%\multicolumn{2}{c}{{\normalsize Algorithm 3.1: Lexicase Selection}} \\
\texttt{Selection}($\mathcal{N,T}$, \texttt{ns}) 	:						&	\\
\hspace{1em} $\mathcal{P} \leftarrow \emptyset$ & set of selected parents \\
\hspace{1em} \textbf{do} \texttt{ns} \textbf{times}: & \texttt{ns} is the number of selection events\\
\hspace{1em}  \hspace{1em} $\mathcal{P} \leftarrow \mathcal{P} \; \cup $ \texttt{GetParent}($\mathcal{N,T}$) & add selected program to $\mathcal{P}$ \\
\\
\texttt{GetParent}($\mathcal{N,T}$) 	:						&	\\
\hspace{1em}	$\mathcal{T}' \leftarrow \mathcal{T}$	&	training cases\\
\hspace{1em}	$S \leftarrow \mathcal{N}$	&	initial selection pool is the population\\
\hspace{1em}	\textbf{while} $|\mathcal{T}'| >0$ \textbf{and} $|\mathcal{S}|>1$:						&	main loop\\
\hspace{1em}\hspace{1em}	$t \leftarrow$ random choice from $\mathcal{T'}$ 	&	\hspace{1em}consider a random case\\
\hspace{1em}\hspace{1em}	\texttt{elite} $\leftarrow$ min $e_t(n)$ \textbf{for} $n \in \mathcal{S}$ 	&	\hspace{1em}determine elite fitness\\
\hspace{1em}\hspace{1em}	\textbf{for} $n \in \mathcal{S}$: 	&	\hspace{1em}reduce selection pool to elites\\
\hspace{1em}\hspace{1em}\hspace{1em}	 \textbf{if} $e_t(n)$ $\neq$ \texttt{elite} \textbf{then}	$\mathcal{S} \leftarrow \mathcal{S} \setminus \{n\}$			&	\\
\hspace{1em}\hspace{1em}	$\mathcal{T'} \leftarrow \mathcal{T'} \setminus \{t\}$ 				&	\hspace{1em}reduce remaining cases\\
\hspace{1em} \textbf{return} random choice from $\mathcal{S}$															&	return parent  
\end{tabularx}
}
"
520,1709.05394,"
    \caption{{\bf Static $\epsilon$-Lexicase Selection} applied to individuals $n \in \mathcal{N}$ with errors $e_t(n)$ and minimum error $e_t^*$ on training cases $t \in \mathcal{T}$. $\lambda$ is the median absolute deviation function.}\label{alg:ep-lex-s}
\noindent{\footnotesize
\begin{tabularx}{\textwidth}{lX}
%\multicolumn{2}{c}{\normalsize Algorithm~\ref{alg:ep-lex-s}: {Static $\epsilon$-Lexicase Selection}} \\ 
\texttt{Selection}($\mathcal{N,T}$, \texttt{ns}) 	:						&	\\
\hspace{1em} $\mathcal{P} \leftarrow \emptyset$ & set of selected parents \\
    \hspace{1em}	$\epsilon \leftarrow \lambda$($\mathbf{e}_t$) \textbf{for} $t \in \mathcal{T}$	&	get $\epsilon$ for each case across population\\
\hspace{1em}	\textbf{for}	 $t \in \mathcal{T}$ \textbf{and} $n \in \mathcal{N}$:	&	define fitness $f$ using within-$\epsilon$ pass condition\\
\hspace{1em}\hspace{1em}	\textbf{if} $e_t(n) \leq e^*_t + \epsilon_t$ \textbf{then} $f_t(n) \leftarrow 0$ \\
\hspace{1em}\hspace{1em} \textbf{else} $f_t(n) \leftarrow 1$ \\ 
\hspace{1em} \textbf{do} \texttt{ns} \textbf{times}: & \texttt{ns} is the number of selection events\\
\hspace{1em}  \hspace{1em} $\mathcal{P} \leftarrow \mathcal{P} \; \cup $ \texttt{GetParent}($\mathcal{N,T}$,$f$) & add selected program to $\mathcal{P}$ \\
\\
\texttt{GetParent}($\mathcal{N,T}$, $f$) 	:						&	\\
\hspace{1em}	$\mathcal{T}' \leftarrow \mathcal{T}$	&	training cases\\
\hspace{1em}	$S \leftarrow \mathcal{N}$	&	initial selection pool is the population\\

\hspace{1em}	\textbf{while} $|\mathcal{T}'| >0$ \textbf{and} $|\mathcal{S}|>1$:						&	main loop\\
\hspace{1em}\hspace{1em}	$t \leftarrow$ random choice from $\mathcal{T'}$  	&	\hspace{1em}consider a random case\\
\hspace{1em}\hspace{1em}	\texttt{elite} $\leftarrow$ min $f_t(n)$ \textbf{for} $n \in \mathcal{S}$ 	&	\hspace{1em}determine elite fitness\\
%\hspace{1em}\hspace{1em}	$\mathcal{S} \leftarrow n \in \mathcal{S}$ if fitness($n$) $\leq$ \texttt{elite}	&	\hspace{1em}reduce selection pool\\
\hspace{1em}\hspace{1em}	\textbf{for} $n \in \mathcal{S}$: 	&	\hspace{1em}reduce selection pool\\
\hspace{1em}\hspace{1em}\hspace{1em}	 \textbf{if} $f_t(n) \neq$ \texttt{elite} \textbf{then}	$\mathcal{S} \leftarrow \mathcal{S} \setminus \{n\}$			&	\\
\hspace{1em}\hspace{1em}	$\mathcal{T'} \leftarrow \mathcal{T'} \setminus \{t\}$ 				&	\hspace{1em}reduce remaining cases\\
\hspace{1em} \textbf{return} random choice from $\mathcal{S}$															&	return parent  
\end{tabularx}
}
"
521,1709.05394,"
\caption{{\bf Semi-dynamic $\epsilon$-Lexicase Selection} applied to individuals $n \in \mathcal{N}$ with errors $e_t(n)$ on training cases $t \in \mathcal{T}$. $\lambda$ is the median absolute deviation function.}\label{alg:ep-lex-sd}
\noindent{\footnotesize
\begin{tabularx}{\textwidth}{lX}
%\multicolumn{2}{c}{\normalsize Algorithm~\ref{alg:ep-lex-sd}: Semi-dynamic $\epsilon$-Lexicase Selection} \\ 
\texttt{Selection}($\mathcal{N,T}$, \texttt{ns}) 	:						&	\\
\hspace{1em} $\mathcal{P} \leftarrow \emptyset$ & set of selected parents \\
\hspace{1em}	$\epsilon \leftarrow \lambda$($\mathbf{e}_t$) \textbf{for} $t \in \mathcal{T}$	&	get $\epsilon$ for each case across population\\
\hspace{1em} \textbf{do} \texttt{ns} \textbf{times}: & \texttt{ns} is the number of selection events\\
\hspace{1em}  \hspace{1em} $\mathcal{P} \leftarrow \mathcal{P} \; \cup $ \texttt{GetParent}($\mathcal{N,T},\epsilon$) & add selected program to $\mathcal{P}$ \\
\\
\texttt{GetParent}($\mathcal{N,T},\epsilon$) 	:						&	\\
\hspace{1em}	$\mathcal{T}' \leftarrow \mathcal{T}$	&	training cases\\
\hspace{1em}	$S \leftarrow \mathcal{N}$	&	initial selection pool is the population\\
\hspace{1em}	\textbf{while} $|\mathcal{T}'| >0$ \textbf{and} $|\mathcal{S}|>1$:						&	main loop\\
\hspace{1em}\hspace{1em}	$t$ $\leftarrow$ random choice from $\mathcal{T'}$ &	\hspace{1em}consider a random case\\
\hspace{1em}\hspace{1em}	\texttt{elite} $\leftarrow$ min $e_t(n)$ \textbf{for} $n \in \mathcal{S}$ 	&	\hspace{1em}determine elite fitness\\
%\hspace{1em}\hspace{1em}	$\mathcal{S} \leftarrow n \in \mathcal{S}$ if $e_t(n)$ $\leq$ \texttt{elite}$+\epsilon_{t}$	&	\hspace{1em}reduce selection pool\\
\hspace{1em}\hspace{1em}	\textbf{for} $n \in \mathcal{S}$: 	&	\hspace{1em}reduce selection pool\\
\hspace{1em}\hspace{1em}\hspace{1em}	 \textbf{if} $e_t(n)$ $>$ \texttt{elite}$+\epsilon_{t}$ \textbf{then}	$\mathcal{S} \leftarrow \mathcal{S} \setminus \{n\}$	 \\
\hspace{1em}\hspace{1em}	$\mathcal{T'} \leftarrow \mathcal{T'} \setminus \{t\}$				&	\hspace{1em}reduce remaining cases\\
\hspace{1em} \textbf{return} random choice from $\mathcal{S}$															&	return parent  
\end{tabularx}
}
"
522,1709.05394,"
\caption{{\bf Dynamic $\epsilon$-Lexicase Selection} applied to individuals $n \in \mathcal{N}$ with errors $e_t(n)$ on training cases $t \in \mathcal{T}$. $\lambda$ is the median absolute deviation function.}\label{alg:ep-lex-d}
\noindent{\footnotesize
\begin{tabularx}{\textwidth}{lX}
%\multicolumn{2}{c}{\normalsize Algorithm~\ref{alg:ep-lex-d}: Dynamic $\epsilon$-Lexicase Selection} \\ 
\texttt{Selection}($\mathcal{N,T}$, \texttt{ns}) 	:						&	\\
\hspace{1em} $\mathcal{P} \leftarrow \emptyset$ & set of selected parents \\
\hspace{1em} \textbf{do} \texttt{ns} \textbf{times}: & \texttt{ns} is the number of selection events\\
\hspace{1em}  \hspace{1em} $\mathcal{P} \leftarrow \mathcal{P} \; \cup $ \texttt{GetParent}($\mathcal{N,T}$) & add selected program to $\mathcal{P}$ \\
\\
\texttt{GetParent}($\mathcal{N,T}$) 	:						&	\\
\hspace{1em}	$T' \leftarrow \mathcal{T}$	&	training cases\\
\hspace{1em}	$S \leftarrow \mathcal{N}$	&	initial selection pool is the population\\
\hspace{1em}	\textbf{while} $|T'| >0$ \textbf{and} $|\mathcal{S}|>1$:						&	main loop\\
\hspace{1em}\hspace{1em}	$t$ $\leftarrow$ random choice from $\mathcal{T'}$  	&	\hspace{1em}consider a random case\\
\hspace{1em}\hspace{1em}	\texttt{elite} $\leftarrow$ min $e_t(n)$ \textbf{for} $n \in \mathcal{S}$ 	&	\hspace{1em}determine elite fitness\\
\hspace{1em}\hspace{1em}	$\epsilon_t \leftarrow \lambda(\mathbf{e}_t(\mathcal{S}))$	&	\hspace{1em}determine $\epsilon$ for case $t$\\
%\hspace{1em}\hspace{1em}	$\mathcal{S} \leftarrow n \in \mathcal{S}$ if $e_t(n)$ $\leq$ \texttt{elite}$+\epsilon_{t}$	&	\hspace{1em}reduce selection pool\\
\hspace{1em}\hspace{1em}	\textbf{for} $n \in \mathcal{S}$: 	&	\hspace{1em}reduce selection pool\\
\hspace{1em}\hspace{1em}\hspace{1em}	 \textbf{if} $e_t(n)$ $>$ \texttt{elite}$+\epsilon_{t}$ \textbf{then}	$\mathcal{S} \leftarrow \mathcal{S} \setminus \{n\}$	 \\
\hspace{1em}\hspace{1em}	$\mathcal{T'} \leftarrow \mathcal{T'} \setminus \{t\}$				&	\hspace{1em}reduce remaining cases\\
\hspace{1em} \textbf{return} random choice from $\mathcal{S}$															&	return parent  
\end{tabularx}
}
"
523,1802.03989,"
  \caption{S-SVDD optimization}\label{algo}
\SetAlgoLined

\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{$\mathbf{X}, \beta, \eta, d, C$}
 \Output{$\textbf{Q}$, $R$, $\mbox{\boldmath$\alpha$}$}  
 \vspace{3mm}
 // Initialize $\mathbf{Q}$\\
 Random initialization of $\mathbf{Q}$\;
 Orthogonalize $\mathbf{Q}$ using QR decomposition\;
 Row normalize $\mathbf{Q}$ using $l_2$ norm\;
 Initialize $k= 1$\;
 \vspace{3mm}
 \While{$k< k_{max}$ }{
    \vspace{3mm}
    // SVDD in the subspace defined by $\mathbf{Q}$ \\
    Calculate $\mathbf{Y}$ using (\ref{eq:Y_i})\;
    Calculate $\alpha_i,\:i=1,\dots,N$ using (\ref{Lang2})\;
    
    \vspace{3mm}
    // Update $\mathbf{Q}$ based on the SVDD solution\\
    Calculate $\Delta L$ using (\ref{generalconstraint})-(\ref{deltapsi1})\;
    Update $\mathbf{Q} \leftarrow \mathbf{Q} - \eta \Delta L$\;
    
    \vspace{3mm}
    // Normalize the updated $\mathbf{Q}$\\
    Orthogonalize $\mathbf{Q}$ using QR decomposition\;
    Row normalize $\mathbf{Q}$ using $l_2$ norm\;
   
    % \textbf{Get} $\textbf{Q}^{*}=\textbf{Q}^{(k)}$\;
      $k \leftarrow k+1$ 
   }
   \vspace{3mm}
   
   // SVDD in the optimized subspace\\
   Calculate $\mathbf{Y}$ using (\ref{eq:Y_i})\;
   Calculate $\alpha_i,\:i=1,\dots,N$ using (\ref{Lang2})\;
% \vspace{5mm}
"
524,1804.10974,"
	\caption{VAML Phase 1: Soft Q-Learning to approximate $Q^*$}
	\label{algo:soft_q_learning}
	\begin{algorithmic}[1]
		\REQUIRE 
		A Q-function approximator $Q_{\phi}$ with parameter $\phi$, and the hyper-parameters $\tau$, $M$. 
		\WHILE{Not Converged}
		\STATE Receive a random example $(\mb{x}^*, \mb{y}^*)$.
		\STATE Sample $M-1$ sequences $\{\mb{y}^{(i)}\}_{i=1}^{M-1}$ from $P_\text{ngram}(\mb{Y} \mid \mb{x}^*, \mb{y}^*)$ and let $\mb{y}^{(M)} = \mb{y}^*$.
		\STATE Compute all the rewards $r(\mb{y}_{1}^{t-1}, y_t; \mb{y}^*)$ for each $\mb{y} \in \{\mb{y}^{(i)}\}_{i=1}^{M}$ and $t = 1, \dots, |\mb{y}|$.
		\STATE Compute the target Q-values for each $\mb{y} \in \{\mb{y}^{(i)}\}_{i=1}^{M}$  and $t = 1, \dots, |\mb{y}|$
		\[ \hat{Q}_\phi(\mb{y}_{1}^{t-1}, y_t; \mb{y}^*) = r(\mb{y}_{1}^{t-1}, y_t; \mb{y}^*) + \tau \log \sum_{w \in \mathcal{W}} \EXP{ Q_\phi(\mb{y}_{1}^{t}, w; \mb{y}^*) / \tau}. \]
		\STATE Compute the Soft-Q Learning loss 
		\[ \mathcal{L}_\text{SoftQ} = \frac{1}{M} \sum_{i=1}^{M} \sum_{t=1}^{|\mb{y}^{(i)}|} \left\| Q_\phi({\mb{y}^{(i)}}_{1}^{t-1}, y_t^{(i)}; \mb{y}^*) - \hat{Q}_\phi({\mb{y}^{(i)}}_{1}^{t-1}, y_t^{(i)}; \mb{y}^*) \right\|_2^2. \]
		\STATE Update $Q_\phi$ according to the loss $\mathcal{L}_\text{SoftQ}$. 
		\ENDWHILE
	\end{algorithmic}
"
525,1804.10974,"
	\caption{VAML Phase 2: Sequence model training with token-level target}
	\label{algo:vaml}
	\begin{algorithmic}[1]
		\REQUIRE 
		A sequence prediction model $P_{\theta}$ with parameter $\theta$, 
		a pre-trained Q-function approximator $Q_{\phi}$, 
		and hyper-parameters $\tau$, $M$, $\kappa$
		\WHILE{Not Converged}
		\STATE Receive a random example $(\mb{x}^*, \mb{y}^*)$.
		\STATE Sample $M-1$ sequences $\{\mb{y}^{(i)}\}_{i=1}^{M-1}$ from $P_\text{ngram}(\mb{Y} \mid \mb{x}^*, \mb{y}^*)$ and let $\mb{y}^{(M)} = \mb{y}^*$.
		\STATE Compute the VAML loss using
		\[ \mathcal{L}_\text{VAML} = \sum_{i=1}^{M} \frac{ \EXP{R(\mb{y}^{(i)}, \mb{y}^*) / \tau} }{ \sum_{i=1}^{M} \EXP{R(\mb{y}^{(i)}, \mb{y}^*) / \tau} } \left[ \sum_{t=1}^{|\mb{y}^{(i)}|} \mathcal{J}_\kappa({\mb{y}^{(i)}}_1^t) \right]. \]
		\STATE Update $P_\theta$ according to the loss $\mathcal{L}_\text{VAML}$. 
		\ENDWHILE
	\end{algorithmic}
"
526,1804.10974,"[t]
    \caption{ERAC Algorithm}
    \label{algo:ERAC}
    \begin{algorithmic}[1]
    \REQUIRE 
        A critic $Q_{\phi}(\mb{y}_1^{t-1}, y_t; \mb{y}^*)$
        and an actor $\pi_{\theta}(w \mid \mb{y}_1^{t})$ with
        weights $\phi$ and $\theta$ respectively, 
        and hyper-parameters $\tau$, $\beta$, $\lambda_\text{var}$, $\lambda_\text{mle}$
    \STATE Initialize delayed target critic $Q_{\bar{\phi}}$ with the same weights: $\bar{\phi} = \phi$.
    \WHILE{Not Converged}
    \STATE Receive a random example $(\mb{x}^*, \mb{y}^*)$.
    \STATE Generate a sequence $\mb{y}$ from $\pi_{\theta}$.
    \STATE Compute the rewards $r(\mb{y}_{1}^{t-1}, y_t; \mb{y}^*)$ for $t = 1, \dots, |\mb{y}|$.
    \STATE Compute targets for the critic
    \begin{equation*}
    \hat{Q}_{\bar{\phi}}(\mb{y}_{1}^{t-1}, y_t; \mb{y}^*) 
	= r(\mb{y}_{1}^{t-1}, y_t) + \tau \, \mathcal{H}(\pi_\theta(\cdot \mid \mb{y}_1^t)) 
	+ \sum_{w \in \mathcal{W}} \pi_\theta(w \mid \mb{y}_1^t) Q_{\bar{\phi}}(\mb{y}_1^t, w; \mb{y}^*).    
	\end{equation*}       
    \STATE Compute loss for critic
    \begin{align*}
    \mathcal{L}_\text{critic}
    &=\sum_{t=1}^{|\mb{y}|} \left[ Q_\phi(\mb{y}_{1}^{t-1}, y_t; \mb{y}^*) - \hat{Q}_{\bar{\phi}}(\mb{y}_{1}^{t-1}, y_t; \mb{y}^*) \right]^2 + \lambda_\text{var} \sum_{w \in \mathcal{W}} \left[ Q_\phi(\mb{y}_{1}^{t-1}, w; \mb{y}^*) - \bar{Q}_\phi(\mb{y}_{1}^{t-1}; \mb{y}^*) \right]^2, \\
    &\text{where}\quad \bar{Q}_\phi(\mb{y}_{1}^{t-1}; \mb{y}^*)=\frac{1}{|\mathcal{W}|} \sum_{w' \in \mathcal{W}} Q_\phi(\mb{y}_{1}^{t-1}, w'; \mb{y}^*)
    \end{align*}
    \STATE Compute loss for actor
    \begin{align*}
    \mathcal{L}_\text{actor} &= -\left[ \sum_{t=1}^{|\mb{y}|}  \sum_{w \in \mathcal{W}} \pi_{\theta}(w \mid \mb{y}_1^{t-1}) Q_\phi(\mb{y}_1^{t-1}, w; \mb{y}^*) \nonumber + \tau \mathcal{H}(\pi_{\theta}(\cdot \mid \mb{y}_1^{t-1})) + \lambda_\text{mle}\sum_{t=1}^{|\mb{y}^*|}\log \pi_{\theta}(y_t^* \mid {\mb{y}^*}_1^{t-1})\right]
    \end{align*}
    \STATE Update critic according to the loss $\mathcal{L}_\text{critic}$. 
    \STATE If actor is not fixed, update actor according to the loss $\mathcal{L}_\text{actor}$
    \STATE Update delayed target critic: $\bar{\phi} = \beta \phi + (1 - \beta) \bar{\phi}$
    \ENDWHILE
    \end{algorithmic}
"
527,1804.10922,"
% 		\SetKwInOut{Input}{Input}
% 		\SetKwInOut{Output}{Output}	
		
% 		\Input{Ontology$O$ defining a set of concepts $c$ (TBox), and set of Entity-Concept associations $c(e)$ (ABox)}
% 		\Output{Vector representation for every entity $e$, $V(e)$}
% 		\ Infer the deductive closure of the ontology $O$ using a semantic reasoner (e.g. Elk)\\
% 		\ Extract annotation properties (metadata) from ontology $O$\\
% 		\ Combine formal axioms from 1, ontology metadata from 2 and Entity-Concept associations  $c(E)$ in a text corpus $C$\\
% 		\ Pre-train Word2Vec on all PubMed abstracts\\
% 		\ Run Word2Vec model from  4 on corpus $C$ \\

% 		{
% 			return $V(e)$ for every entity $e$\;
% 		}
% 		\caption{\label{algo}OPA2Vec algorithm for producing vector
%                   representations of biological entities}
% 	"
528,1711.05869,"[h!]
  \caption{Predictive conditional independent test (PCIT)}
    \label{alg:indeptest}
  \begin{algorithmic}[1]
     \\Split data into training and test set
    \For{all variables y $\in$ Y}
\\ \hspace{0.1in} \textbf{on training data:}
\\ \hspace{0.18in} find optimal functional $f$ for predicting y from Z
\\ \hspace{0.18in} find optimal functional $g$ for predicting y from \{X,Z\}
\\
\\ \hspace{0.1in} \textbf{on test data:}
\\ \hspace{0.18in}calculate and store p-value for test that generalization loss of $g$ is lower than $f$
\\
\EndFor
\\
\If{symmetric test needed}
\\  \hspace{0.1in} exchange X and Y, repeat above process
\EndIf
\\
\Let{p\_values\_adjusted}{Apply FDR control to array of all calculated p-values}
\\
\Return p\_values\_adjusted
\end{algorithmic}
"
529,1711.05869,"
  \begin{algorithmic}[1]
    \Require{Set $\{p_{(i)}\}_{i=1}^m$ s.t. $p_j$: p-value for observing $X_j$ under $H_0^j$
    }
    \\
	Sort the p-values in ascending order, $p_{(1)} \leq ... \leq p_{(m)}$ \\
	Let $q = \alpha / (\sum_{i=1}^m{1 / i})$ for chosen confidence level $\alpha$ \\
	Find the \textbf{k} s.t. $k = max(i:p_{(i)}\leq \frac{i}{m}q)$ \\
	Reject $H_0^j$ for $j \in {1,...,k}$
  \end{algorithmic}
 \caption{The Benjamini-Hochberg-Yekuteli Procedure
 \label{alg:FDRcontrol}}
"
530,1711.05869,"
  \caption{Undirected graph structure estimation}
    \label{alg:undirstruct}
  \begin{algorithmic}[1]
    \For{any combination $X_i$, $X_j$ s.t. $i \neq j$}
    \Let{$X_-$}{$X \setminus \{X_i,X_j\}$}
    \Let{$p\_val_{i,j}$}{p-value for test $X_i \indep X_j |X_-$}
    \EndFor
    \Let{p\_val\_adj}{Apply FDR control on p\_val matrix}
    \\
\Return p\_val\_adj
\end{algorithmic}
"
531,1711.05869,"[ht]
\caption{Count association classifier, fitting. This is a theoretical counterexample to the permutation baseline, do not use in practice.\newline
\textit{Input:} Training data $\calD = \{(X_1,Y_1),\dots (X_N,Y_N)\}$, where $X_i$ and $Y_i$ take values in $\{-1,1\}$.
both trainable\newline
\textit{Output:} a classical predictor $f:\calX\rightarrow \calY$ \label{alg:countassoc}}
\begin{algorithmic}[1]
    \State Let $N_X\leftarrow \card\{X_i\;:\;X_i = 1\}$
    \State Let $N_Y\leftarrow \card\{Y_i\;:\;Y_i = 1\}$
    \State If $|N_X - N_Y| < |N- N_X + N_Y|$ output $f: x\mapsto x$
    \State Else output $f: x\mapsto -x$
\end{algorithmic}
"
532,1804.10834,"[t]
%	\caption{Domain Adaptation using Metric Learning}
%	%\small
%	Given: A source dataset of labeled points $x^s_i \in D_s$  with labels $L_s = \{y_i \}$, and an unlabeled target dataset $x^t_i \in D_t$,  with hyperparameters $t$, and $\gamma$. 
%	\begin{algorithmic}[1]
%		\STATE Define $X = \{x^s_1, \ldots, x^s_n, x^t_1, \ldots, x^t_m \}$.
%		\STATE Compute the source and target matrices $A_s$ and $A_t$ using Equations~\ref{covm} and \ref{covm-2}. 
%		\STATE Compute $A_m = A_s + X L X^T$.
%		\STATE Compute $A_{gs} = A_s + X (K + \mu L) X^T$.
%		\STATE {\bf Algorithm GCA1:} Compute the weighted geometric mean $A_1 = A^{-1}_s \ \sharp_t \ A_t$. (see Equation~\ref{gca-eqn-wt})
%		\STATE {\bf Algorithm Cascaded-GCA2:} Compute the cascaded weighted geometric mean taking into account the MMD metric $A_2 = A^{-1}_m \ \sharp_t \ A_t$. (see Equation~\ref{gca-mmd-wt}). 
%		\STATE {\bf Algorithm Cascaded-GCA3:} Compute the cascaded weighted geometric mean using source and target fiber bundle geometry $A_3 = A^{-1}_{gs} \ \sharp_t \ A_t$. (see Equation~\ref{gfab-eqn-wt})
%		\STATE Use the learned $A_i$ matrix to adapt source features to the target domain, and perform classification.
%	\end{algorithmic}
%	\label{alg:gca-basic}
%"
533,1804.10834,"[ht]
\caption{Algorithms for Domain Adaptation using Metric Learning}
\label{alg:gca-basic}
	\small
	Given: A source dataset of labeled points $x^s_i \in D_s$  with labels $L_s = \{y_i \}$, and an unlabeled target dataset $x^t_i \in D_t$,  with hyperparameters $t, \mu$,  and $\gamma$. 
	\begin{enumerate}
		\item Define $X = \{x^s_1, \ldots, x^s_n, x^t_1, \ldots, x^t_m \}$.
		\item Compute the source and target matrices $A_s$ and $A_t$ using Equations~(\ref{covm}) and (\ref{covm-2}). 
		%\item Compute $A_m = A_s + X L X^T$.
		%\item Compute $A_{gs} = A_s + X (K + \mu L) X^T$.
		\item {\bf Algorithm GCA1:} Compute the weighted geometric mean $A = A^{-1}_s \ \sharp_t \ A_t$ (see Equation~(\ref{gca-eqn-wt})).
		\item {\bf Algorithm GCA2:} Compute the weighted geometric mean  taking additionally into account the MMD metric $A = A^{-1}_m \ \sharp_t \ A_t$ (see Equation~(\ref{gca-mmd-wt})), where $A_m = A_s + X L X^T$. 
		\item {\bf Algorithm Cascaded-GCA2:} Compute the \emph{cascaded} weighted geometric mean  taking additionally into account the MMD metric $A = A^{-1}_m \ \sharp_t \ A_t$ (see Equation~(\ref{gca-mmd-wt})), $A_m = A_s \sharp_\gamma X L X^T$. 
		\item {\bf Algorithm GCA3:} Compute the weighted geometric mean taking additionally into account the source and target manifold geometry $A = A^{-1}_{gs} \ \sharp_t \ A_t$ (see Equation~(\ref{gfab-eqn-wt})), where $A_{gs} = A_s + X (K + \mu L) X^T$.
		\item {\bf Algorithm Cascaded-GCA3:} Compute the \emph{cascaded} weighted geometric mean taking additionally into account the source and target manifold geometry $A = A^{-1}_{gs} \ \sharp_t \ A_t$ (see Equation~(\ref{gfab-eqn-wt})), where $A_{gs} = A_s \sharp_\gamma (X (K + \mu L) X^T)$.
		\item Use the learned $A$ matrix to adapt source features to the target domain, and perform classification (e.g., using support vector machines). 
	\end{enumerate}
"
534,1710.07983,"[htb]
\caption{Counterexample-Guided Apprenticeship Learning (CEGAL)}
\begin{algorithmic}[1]
\State \textbf{Input}:
\\\qquad $M\gets$ A partially known $MDP\backslash R$; $f\gets$ A vector of feature functions
\\\qquad $\mu_E\gets$ The expected features of expert trajectories $\{\tau_0, \tau_1, \ldots,\tau_m\}$
\\\qquad $\Phi\gets$ Specification; $\epsilon\gets$ Error bound for the expected features;
\\\qquad $\sigma, \alpha\in(0, 1)\gets$ Error bound $\sigma$ and step length $\alpha$ for the parameter $k$; 
\State \textbf{Initialization}:
\\\qquad \textbf{If} $||\mu_E-\mu_{\pi_0}||_2\leq \epsilon$, {\bf then return} $\pi_0$ \Comment{$\pi_0$ is the \textbf{initial safe policy}}
\\\qquad $\Pi_S\leftarrow\{\pi_0\}$, $CEX\leftarrow\emptyset$ \Comment{Initialize candidate and counterexample set}
\\\qquad $inf\leftarrow0, sup\leftarrow1, k\leftarrow sup$ \Comment{Initialize multi-optimization parameter $k$}
\\\qquad $\pi_1\leftarrow$ Policy learnt from $\mu_E$ via apprenticeship learning
\State \textbf{Iteration $i\ (i\geq 1)$}:
\\\qquad\textbf{Verifier:}
\\\qquad\qquad status $\gets Model\_Checker(M,\pi_i, \Phi)$
\\\qquad\qquad \textbf{If} status = SAT, \textbf{then go to Learner}
\\\qquad\qquad \textbf{If} status = UNSAT
\\\qquad\qquad\qquad $cex_{\pi_i}\gets Counterexample\_Generator(M,\pi_i,\Phi)$
\\\qquad\qquad\qquad Add $cex_{\pi_i}$ to $CEX$ and solve $\mu_{cex_{\pi_i}}$, \textbf{go to Learner} 
\\\qquad\textbf{Learner:}
\\\qquad\qquad \textbf{If} status = SAT
\\\qquad\qquad\qquad \textbf{If} $||\mu_E-\mu_{\pi_i}||_2\leq \epsilon$, {\bf then return} $\pi^*\gets\pi_i$
\\\Comment{Terminate. $\pi_i$ is {$\epsilon$-close} to $\pi_E$}
\\\qquad\qquad\qquad Add $\pi_i$ to $\Pi_S$, $inf \leftarrow k, k \leftarrow sup$\Comment{Update $\Pi_S$, $inf$ and reset $k$}
\\\qquad\qquad \textbf{If} status = UNSAT
\\\qquad\qquad\qquad \textbf{If} $|k - inf|\leq\sigma$, {\bf then return} $\pi^*\leftarrow\underset{{\pi}\in \Pi_S}{argmin}||\mu_E - \mu_{{\pi}}||_2$\\\Comment{Terminate. $k$ is too close to its lower bound. }
\\\qquad\qquad\qquad $k \leftarrow \alpha\cdot inf + (1 - \alpha)k$ 
\Comment{Decrease k to learn for safety}
\\\qquad\qquad $\omega_{i+1} \leftarrow arg\underset{\omega}{max}\underset{\pi\in\Pi_S, \tilde{\pi}\in\Pi_S, cex\in CEX}{\min}\ \omega^T(k(\mu_E - \mu_{\pi})+(1-k)(\mu_{\tilde{\pi}}  - \mu_{cex}))$
\\\Comment{Note that the multi-objective optimization function recovers AL when $k=1$} 
\\\qquad\qquad $\pi_{i+1}, \mu_{\pi_{i+1}}\gets$ Compute the optimal policy $\pi_{i+1}$ and its expected features $\mu_{\pi_{i+1}}$ for the MDP $M$ with reward $R(s)=\omega_{i+1}^T f(s)$
\\\qquad{\bf Go to next iteration}
\end{algorithmic}
\label{algo1}
"
535,1804.10323,"[t]
% \KwIn{Pre-trained model $\phi(\cdot,\boldsymbol{\theta_o})$}
% \KwOut{Learnt FMN Model } 
\nl {\bf Initialization:} $\theta_e, \theta_d, \theta_{e'}, \theta_{d'} \leftarrow $ randomly initialize VAE and AE encoder, decoder respectively. \\
// Perform a total of $T$ training iterations \\
\For{$t = 1:T$ }{
 	{\bf VAE Training}\\
	\nl  $\mathbf{X} \leftarrow$ sample a random batch from $\mathcal{X}$ \\
    \nl $\mathbf{Z}_v \leftarrow \mathcal{F}_{vae-e}(\mathbf{X}; \theta_e)$\\
    \nl $\mathbf{Z}_g \sim N(\mathbf{0},\mathbf{I}) $ \\
    \nl $\mathbf{X}_{v,g} \leftarrow \mathcal{F}_{vae-d}(\mathbf{Z}_{v,g}; \theta_d)$ \\
    \nl $\mathbf{Z}'_{v,g,d} \leftarrow \mathcal{F}_{ae-e}(\mathbf{X}_{v,g,-}; \theta_{e'})$ \\
    \nl Calculate $\mathcal{L}_e$, $\mathcal{L}_n$ and $\mathcal{L}_s$ using Eqs.~\ref{eq:data_loss}, \ref{eq:l_n} and \ref{eq:l_s}\\
    \nl $\theta_e \leftarrow \nabla_{\theta_e} \mathcal{L}_{enc}$ (Eq.~\ref{eq:l_enc})\\
    {\bf Adversarial Training}\\
    
    \nl $\mathbf{X}'_{v,g,d} \leftarrow \mathcal{F}_{ae-d}(\mathbf{Z}'_{v,g,d}; \theta_{d'})$ \\
    \nl Calculate $\mathcal{L}_g, \mathcal{L}_d$ and $ \mathcal{L}_v$  using Eq.~\ref{eq:l_gvd}\\
    \nl $\theta_d \leftarrow \nabla_{\theta_d} \mathcal{L}_{gen}$ (Eq.~\ref{eq:l_gen})\\
    \nl Calculate $\eta, e_t$ and $k_t$ using Eqs.~\ref{eq:10}, \ref{eq:11} and \ref{eq:12}\\
    \nl $\theta_{e'}, \theta_{d'} \leftarrow \nabla_{\theta_{e'}, \theta_{d'}} \mathcal{L}_{dis}$ (Eq.~\ref{eq:l_dis})\\
    }
{\bf Return:} Updated parameters $\theta_e, \theta_d, \theta_{e'}, \theta_{d'}$
    
    \caption{{\bf Learning procedure for proposed model} \label{Algorithm}}

"
536,1803.09877,"[H]
%\begin{algorithmic}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\Input{$n$ items $I_1, I_2,\cdots, I_n$}
	\Output{The majority of the $n$ items}
	Initialize an element $\textit{Ma} = I_1$ and a counter $\textit{Counter} = 0$.\\
    \For{$i=1$ \KwTo $n$ }
    {
        \uIf{$\textit{Counter}==0$}
        {
             $Ma = I_i$.\\
             $\textit{Counter} = 1$.
        }
        \uElseIf{$Ma == I_i$}
        {
            $\textit{Counter} = \textit{Counter} + 1$.
        }
        \Else
        {
            $\textit{Counter} = \textit{Counter} - 1$.
        }
  }
  Return $Ma$.
	\caption{Streaming Majority Vote.}
	\label{Alg:MajorityVote.}
%\end{algorithmic}	
"
537,1803.09877,"[htbp]
		\SetKwInOut{Input}{Input}
		\SetKwInOut{Output}{Output}
		\Input{Received $d \times P$ matrix $\mathbf{R}^{Cyc}$}
		\Output{Desired gradient summation $\mathbf{u}^{Cyc}$}
		$V = \phi(\mathbf{R})$ // Locate the adversarial node indexes.\\
		$U = \{1,2,\cdots,P\} - V$. // Non-adversarial node indexes \\
		Find $\mathbf{b}$ by solving $ \mathbf{W}_{\cdot,U} \mathbf{b} = \mathbf{1}_P$\\
		Compute and return $\mathbf{u}^{Cyc} = \mathbf{R}_{\cdot, U} \mathbf{b} $	\caption{Decoder Function $D^{Cyc}$.}
		\label{Alg:DecoderFunction.}
"
538,1802.03680,"[t]
\caption{Iterative Graph Construction}
\label{alg:search}
\begin{algorithmic}
    \REQUIRE A starting location $v_0$ and the bounding box $B$
    \STATE{initialize graph $G$ and vertex stack $S$ with $v_0$}
    \WHILE{$S$ is not empty}
        \STATE{$action, \alpha := \text{decision\_func}(G, S_{\text{top}}, \mathit{Image}$)}
        \STATE{$u := S_{\text{top}} + (D \cos{\alpha}, D \sin{\alpha})$}
        \IF{$\mathit{action} = \text{stop}$ or $u$ is outside $B$}
            \STATE{pop $S_{\text{top}}$ from $S$}
        \ELSE
            \STATE{add vertex $u$ to $G$}
            \STATE{add an edge $(S_{\text{top}}, u)$ to $G$}
            \STATE{push $u$ onto $S$}
        \ENDIF
    \ENDWHILE
\end{algorithmic}
"
539,1708.03275,"[tbp]
	\caption{Edge aided 3D line segment fitting} 
	{\bf Input:}
	An edge segment which is a list of pixels: $es = \{p_1, p_2, ..., p_m\}$, where $p_i$ denotes the $i$-th pixel on the edge segment \\
	{\bf Output:}
	A set of fitted 3D line segments: $LS$
	\begin{algorithmic}[1]
	\State Initialize two empty set of pixels: $pixels$, $outliers$
	\For{\textbf{each} $p_i \in es$}
	\If{$pixels = \varnothing$}
	\State Move first $L$ pixels in $es$ to $pixels$
	\State Fit two 2D lines $l_{im}$ and $l_{depth}$ to $pixels$
	\EndIf
	\State Compute distance $d_{im}$ from $(p_i.x,p_i.y)$ to $l_{im}$ %to and $d_{depth}$ of $p_i$ to $l_{im}$ and $l_depth$
	\State Compute distance $d_{depth}$ from $(p_i.D,p_i.Z)$ to $l_{depth}$
	\If{$d_{im} < e_1 \ \& \ d_{depth} < e_2$}
	\State Move $p_i$ to $pixels$ 
	\Else 
	\State Move $p_i$ to $outliers$ 
	\EndIf
	\If{$es = \varnothing \parallel \left\vert{outliers}\right\vert > L$}
	\If{$\left\vert{pixels}\right\vert > L$}
	\State Fit $l_{im}$ and $l_{depth}$ to $pixels$
	\State Compute the 3D line segment and add to $LS$
	\EndIf
	\State Empty $pixles$ and $outliers$
	\EndIf
	\EndFor
	
% % 	\For each 
% 		%\State Store the fitted line segments $\rm LSs$
% % 		\For{$i=0;i<m;i$++}
%         % \For{\textbf{each} Edge Segment $\rm es_i$ in $\rm ES_k$ }
% 		\State line\_pixels = null, new\_line = true, outliers = 0 
% 		\For{$i=0;i<c;i$++}
% 		\If{new\_line}
% 		\For{$j=0;j<L;j$++}
% 		\State line\_pixels.insert($ p_{i+j}$) 
% 		%// Save temporary line segments
% 		\EndFor
% 		\State new\_line = false
% 		\EndIf
		
% 		\State $l_{im}$=LEAST\_SQUARE(line\_pixels.$x$, line\_pixels.$y$)
% 		\State $l_{depth}$=LEAST\_SQUARE(line\_pixels.$D$, line\_pixels.$Z$)
		
% 		\If{DIST($l_{im}$, point($ p_i.x,p_i.y)$) $< e_1$ \\ \hspace{0.2in} and DIST($l_{depth}$, point($ p_i.D,p_i.Z)$) $<e_2$}
% 		\State line\_pixels.insert($ p_i$), outliers = 0
% 		\Else
% 		\State outliers++
% 		\EndIf
		
% 		\If{outliers $>L$}
% 		\If{line\_pixels.size $>L$}
% 		\State $ls$.insertLine(line\_pixels)
% 		\EndIf
% 		\State line\_pixels = null, new\_line = true, outliers = 0
% 		\EndIf
% 		%\State tmpLS.push\_back($ES^i_j$)
		
% 		\EndFor
		
% 		\If{line\_pixels.size $>L$}
% 		\State $ ls$.insertLine(line\_pixels)
% 		\EndIf
		
% % 		\EndFor
% 		\State \Return $ ls$
	\end{algorithmic}
%Notes: 1) the similarity measure threshold $S_e = 0.9$; 2) $C_{i\bullet}$ are $(n-1)$ cycle candidates related to edge $e_i$.
	\label{alg:LineFitting}
"
540,1804.10028,"[t]
    \DontPrintSemicolon
     \caption{DELCO (training)}\label{al:train_algo}
     %\begin{algorithmic}[1]
     \KwData{$\mathcal{D}_\text{train}$, $n_\text{val}$, $\text{grid}_\lambda$ and $\left\{ \text{train-alg}_k \right\}_{k=1}^m$}
     Select $n_\text{val}$ data points from $\mathcal{D}_\text{train}$ to build $\mathcal{D}_\text{val}$ \;
     $\mathcal{D}'_\text{train} \leftarrow \mathcal{D}_\text{train} \setminus \mathcal{D}_\text{val}$\;
     \For{$k\in \left\{ 1,\hdots, m \right\}$}{
      Run $\text{train-alg}_k$ on $\mathcal{D}'_\text{train}$ to learn $\hat{c}_k$ \;
     }
     
     \For{$y\in \left\{ 1,\hdots, \ell \right\}$}{
        $\gamma_y \leftarrow \frac{1+\sum\limits_{i=1}^{n_{\textrm{val}}} \mathbb{I}_{y}\left(y^{(i)}\right)}{\ell+ n_{\textrm{val}}}$\;
        \For{$k\in \left\{ 1,\hdots, m \right\}$}{
          \For{$j\in \left\{ 1,\hdots, \ell \right\}$}{
            $\theta^{(k)}_{y,j} \leftarrow \frac{1+\sum\limits_{i=1}^{n_{\textrm{val}}} \mathbb{I}_{y}\left(y^{(i)}\right) \mathbb{I}_{j}\left(\hat{c}_k\left(\mathbf{x}^{(i)}\right)\right) }{\ell + \sum\limits_{i=1}^{n_{\textrm{val}}} \mathbb{I}_{y}\left(y^{(i)}\right)}$\;
            $F_{k,y} \left( j \right) \leftarrow \left[ 1 -  \mathbb{I}_0 \left( j \right) \right] \times  F_{k,y} \left( j-1 \right) + \theta^{(k)}_{y,j} $\;
          }
        
        }
     }
     
     \For{$\lambda \in \text{grid}_\lambda$}{
        Obtain $\hat{c}_{\text{ens}}$ by substituting \eqref{eq:joint_cop} in \eqref{eq:predictive} and then \eqref{eq:predictive} in \eqref{eq:c_ens}, and  using $\hat{c}_1,\hdots, \hat{c}_m, \boldsymbol\gamma, \boldsymbol\theta^{(1)}_1, \hdots, \boldsymbol\theta^{(m)}_\ell$ and $\lambda$\;
        $\text{Acc} \left( \lambda \right) \leftarrow \frac{\sum\limits_{i=1}^{n_{\textrm{val}}} \mathbb{I}_{y^{(i)}}\left( \hat{c}_{\text{ens}} \left(  \mathbf{x}^{(i)} \right)  \right) }{n_{\textrm{val}}}$ \;
     }
     $\hat{\lambda} \leftarrow \underset{\lambda \in \text{grid}_\lambda}{\arg\max}\; \text{Acc} \left( \lambda \right)$\;
     Obtain $\hat{c}_{\text{ens}}$ by substituting \eqref{eq:joint_cop} in \eqref{eq:predictive} and then \eqref{eq:predictive} in \eqref{eq:c_ens}, and using $\hat{c}_1,\hdots, \hat{c}_m, \boldsymbol\gamma, \boldsymbol\theta^{(1)}_1, \hdots, \boldsymbol\theta^{(m)}_\ell$ and $\hat{\lambda}$\;
     \Return{$\hat{c}_{\text{ens}}$}
     %\end{algorithmic}
     
     \label{algo}
     "
541,1709.00340,"
  \caption{Training Process}
  \label{trainingprocess}
  \begin{algorithmic}[1]
    \REQUIRE  
    The training images $I$ and their corresponding textual descriptions $T$.
    \ENSURE The model $M$.
    \STATE Set $M=\{M_{ori}, M_{object}, M_{part}\}$
    \STATE Use $I$ to fine-tune the CNN model, which is pre-trained on ImageNet, obtaining the model $M_{ori}$
    \STATE Conduct object localization as described in Section \ref{sec:objectlocalization}, to get the object regions $b$ of $I$
    \STATE Crop $b$ from $I$ and save as images $I_b$
    \STATE Use $I_b$ to fine-tune $M_{ori}$, obtaining the model $M_{object}$
    \STATE Follow \cite{reed2016generative} to train GAN-CLS using minibatch SGD with $I$ and $T$ as pairwise constraints
    \STATE Conduct selective search \cite{uijlings2013selective} on each image to get part proposals $S$
    \STATE Conduct textual pattern mining to obtain the discriminative textual patterns $P$ for each subcategory
    \FOR{$k = 1,...,n; j=1,...,d$}
    \STATE Take $k$-th part proposal $S_k$ and $j$-th textual pattern $P_j$ as the input of the generator $\mathcal{G}$ of GAN-CLS
    \STATE Perform a feed-forward pass, and output the correlation score of $S_k$ and $P_j$
    \STATE For $P_j$ we select one part proposal with the highest correlation score
    \ENDFOR
    \STATE Use the selected part proposals to  fine-tune $M_{object}$, obtaining the model $M_{part}$
    \RETURN $M$.
  \end{algorithmic}
"
542,1709.00340,"
  \caption{Training Process}
  \label{trainingprocess}
  \begin{algorithmic}[1]
    \REQUIRE  
    The input images $I$ and their corresponding textual descriptions $T$.
    \ENSURE The part model $M$.
    \STATE Follow \cite{reed2016generative} to train GAN-CLS using minibatch SGD with $I$ and $T$ as pairwise constraints
    \STATE Conduct selective search \cite{uijlings2013selective} on each image to get part proposals $S$
    \STATE Conduct textual pattern mining to obtain the discriminative textual patterns $P$ for each subcategory
    \FOR{$k = 1,...,n; j=1,...,d$}
    \STATE Take $k$-th part proposal $S_k$ and $j$-th textual pattern $P_j$ as the input of the generator $\mathcal{G}$ of GAN-CLS
    \STATE Perform a feed-forward pass, and output the correlation score of $S_k$ and $P_j$
    \STATE For $P_j$ we select one part proposal with the highest correlation score
    \ENDFOR
    \STATE Take  
    
    \RETURN $M$.
  \end{algorithmic}
"
543,1703.09971," \DontPrintSemicolon \SetAlgoLined
\tcp{Initialization}
  $\theta_0\leftarrow$ initialization value
  \\
  \tcp{Main loop}
  \For{$k=1$ to $K$}{ 
    \For{$i=1$ to $n$}{ 
      \For{$j=1$ to $N_{\mathrm{bridges}}$}{
        sample bridge $(\hat{\mathbf q}(\omega_{j}),\hat{\mathbf p}(\omega_{j}); \theta_{k-1}, \mathbf q^i)$\\
    compute $\log \mathbb P_{\theta_k}(\hat{\mathbf q}(\omega_{j}),\hat{\mathbf p}(\omega_{j}))$ and 
      $\varphi(\hat{\mathbf q}(\omega_{j}),\hat{\mathbf p}(\omega_{j}))$\\
      }
      set $C_{\mathbf q_i}=\mathrm{mean}_j\big(\varphi(\hat{\mathbf q}(\omega_{j}),\hat{\mathbf p}(\omega_{j}))\big)$\\
    set $\mathbb E_{(\mathbf q,\mathbf p)|\mathbf q^i}[\log \mathbb P_{ \theta_{k-1}}(\mathbf q,\mathbf p)]\approx
      C_{\mathbf q_i}^{-1}\mathrm{mean}_j\big(
    \log \mathbb P_\theta(\hat{\mathbf q}(\omega_{j}),\hat{\mathbf p}(\omega_{j}))
      \varphi(\hat{\mathbf q}(\omega_{j}),\hat{\mathbf p}(\omega_{j}))
      \big)$\\
    }
    set $Q(\theta|\theta_{k-1})=\mathrm{mean}_i\big(
    \mathbb E_{(\mathbf q,\mathbf p|\mathbf q_i)}[\log
    \mathbb P_{\theta_{k-1}}(\mathbf q,\mathbf p)]
      \big)
      $\\
      compute $\nabla_{\theta}Q(\theta|\theta_{k-1})$\\
      update $\theta$: $\theta_k=\theta_{k-1}+\epsilon\nabla_{\theta}Q(\theta|\theta_{k-1})$\\
  }
    \caption{\small Stochastic EM-estimation of parameters $\theta$.}
  \label{alg:em}
"
544,1804.09813,"[htbp]
\linespread{1.0}\selectfont
Initialize population with $\Pi_\textsc{max}$ individuals/solutions \\
\While{(number of iterations without improvement $< N_1$) $\wedge$ (number of iterations $< N_2$)}
	{
		Select parents $P_1$ and $P_2$ by binary tournament \\
		Apply crossover to $P_1$ and $P_2$ to generate an offspring $C$ \\
		Mutate $C$ to obtain $C'$ \\
		Apply local search (\textsc{K-means}) to $C'$ to obtain an individual $C''$ \\
		Add $C''$ to the population \\
		\If{the size of the population exceeds $\Pi_\textsc{max}$}
		{
			Eliminate clones and select $\Pi_\textsc{min}$ survivors \\
		}
}
Return best solution
 \caption{\textsc{HG-means} -- general structure} \label{genetic-algo}
"
545,1711.09576,"
\caption{L* Algorithm with explicit membership and equivalence queries.}\label{lstarAlgo} 
\begin{algorithmic}

\STATE $S \leftarrow \{ \epsilon \}, E \leftarrow \{ \epsilon \}$
\FOR{$(s \in  S)$, $(a \in \Sigma)$, and $(e \in E)$}
\STATE $T[s, e] \leftarrow \textbf{Member}(s \cdot e)$
\STATE $T[s \cdot a, e] \leftarrow \textbf{Member}(s \cdot a \cdot e)$
\ENDFOR
\WHILE{True}
\WHILE{$(s_{new} \leftarrow Closed(S, E, T) \neq \bot$)}
\STATE $Add(S, s_{new} )$
\FOR{$(a \in \Sigma, e \in E)$} 
\STATE  $T[s_{new} \cdot a, e] \leftarrow \textbf{Member} (s_{new} \cdot a \cdot e)$
\ENDFOR
\ENDWHILE
\STATE $\hypoaut \leftarrow MakeHypothesis (S, E, T)$
\STATE $cex \leftarrow \textbf{Equivalence}(\hypoaut)$
 \IF{$cex = \bot$}
 \RETURN{$\hypoaut$}
 \ELSE
\STATE  $e_{new} \leftarrow FindSuffix (cex )$
\STATE  $Add(E, e_{new})$
\FOR{$(s \in S, a \in \Sigma)$} 
\STATE     $T[s, e_{new} ] \leftarrow \textbf{Member} (s \cdot e_{new} )$
\STATE    $T[s \cdot a, e_{new} ] \leftarrow \textbf{Member} (s \cdot a \cdot e_{new} )$
\ENDFOR
\ENDIF
\ENDWHILE
\end{algorithmic}
"
546,1711.09576,"
   \caption{Pseudo-code for equivalence checking of an RNN $R$ 
   and minimal DFA $\hypoaut$, with initial 
   partitioning $p_0$. 
}
   \label{Equivalence}
\newcommand{\LIF}{\lIf}
\begin{algorithmic}
\SetEndCharOfAlgoLine{}

\STATE \textbf{method} update\_records($q,h,q_\hypoaut,w$):
	\STATE Visitors$(q)\leftarrow$ Visitors$(q)\cup\{h\}$, 
	\STATE Paths$(h)\leftarrow w$
	\STATE Association$(q) \leftarrow (q_\hypoaut)$
	\STATE Push(New,$\{ h\} $)
\STATE \textbf{end method}	
 
\STATE 

\STATE \textbf{method} handle\_cluster\_conf($q,q_{\hypoaut},q'_{\hypoaut}$):
		\STATE  \textbf{find} $s\in\Sigma^*$ \textbf{s.t.} $\fhypoaut(q_\hypoaut,s)\neq \fhypoaut(q'_\hypoaut,s)$
		\FOR{$h\in$Visitors($q$) }
				\STATE $w\leftarrow Paths(h)\cdot s$
				\STATE \LIF {$f_R(w)\neq f_\hypoaut(w)$} {
				\RETURN{Reject, $w$}\Indp}
		\ENDFOR
		\STATE $p\leftarrow \mathit{ref}(p,h',$Visitors($q'$)$\setminus\{h'\}$)
		\RETURN{ Restart\_Exploration}
\STATE \textbf{end method}	

\STATE 


\STATE \textbf{method} parallel\_explore($R,\hypoaut,p$):
\STATE empty all of: $Q,F,\delta$, New, Visitors, Paths,  Association
\STATE $q_0 \leftarrow p(h_0)$
\STATE update\_records($q_0,h_0,q_\hypoaut,_0,\varepsilon$)

\WHILE{New $\neq \emptyset$}
		\STATE $h \leftarrow$ Pop(New)
		\STATE $q \leftarrow p(h)$
		\STATE $q_\hypoaut \leftarrow$ Association$(q)$
		\STATE \LIF{$f_R(h)\neq \fhypoaut(q_\hypoaut)$} {
				\RETURN{Reject, (Paths($h$))} \Indp}
		\Indm
		\STATE \LIF{$q\in Q$} {\textbf{continue}}
			\STATE $Q \leftarrow Q\cup \{q\}$
			\STATE \LIF{$f_R(h)=Acc$} {$F \leftarrow F\cup \{q\}$}
			\FOR{$\sigma \in \Sigma$}
					\STATE $h'\leftarrow g_R(h,\sigma)$
					\STATE $q' \leftarrow p(h')$
					\STATE $\delta(q,\sigma)\leftarrow q'$
					\STATE \LIF{ $q'\in Q$ and \emph{Association}$(q')\neq \delta_\hypoaut(q_\hypoaut,\sigma)$} { \RETURN{ handle\_cluster\_conf($q,q_\hypoaut,\delta_\hypoaut(q_\hypoaut,\sigma)$)}\Indp}
					\Indm
					\STATE update\_records($q',h',\delta_\hypoaut(q_\hypoaut,\sigma)$,Paths$(h)\cdot\sigma$)
			\ENDFOR
\ENDWHILE
\RETURN{Accept}
\STATE \textbf{end method}

\STATE

\STATE \textbf{method} check\_equivalence($R,\hypoaut,p_0$):
\STATE $p\leftarrow p_0$
\STATE verdict $\leftarrow$ Restart\_Exploration
\WHILE{verdict = Restart\_Exploration}
\STATE verdict, $w \leftarrow$ parallel\_explore($R,\hypoaut,p$)
\ENDWHILE
\RETURN{verdict,$w$}
\STATE \textbf{end method}

\end{algorithmic}
"
547,1806.06553,"[!t]
	\caption{\acf{ISBOR}}
	\label{alg:sbor}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} $\mathbf{D} = \{\mathbf{x},\mathbf{y}\}, \theta$, maxIts and minDelta.
		\STATE \textbf{Output:} $\mathbf{w}$, $\mathbf{b}$ and $\sigma$.
		\STATE $\boldsymbol{\Phi}$ = basis($\mathbf{x}, \theta$);
		\STATE $\mathbf{w}, \boldsymbol{\phi}, \boldsymbol{\alpha}, \sigma,\mathbf{b,Q,S,f} = $ Initialize$(\Phi, \mathbf{y})$;
		\FOR{$i=1,2,\ldots,\text{maxIts}$ }
		\STATE  deltaML = $[g(\alpha_1), \ldots, g(\alpha_n)]$;
		\STATE $\boldsymbol{\phi_n}	 \leftarrow$ max(deltaML);
		\IF{$\boldsymbol{\phi_n}\in \boldsymbol{\phi}$ and $f_n < 0$}
		\STATE  $\{\mathbf{w},\boldsymbol{\alpha},\boldsymbol{\phi}\} \leftarrow \{\mathbf{w},\boldsymbol{\alpha},\boldsymbol{\phi} \}-\{w_n,\alpha_n,\phi_n\}$;
		\ELSIF{$f_n>0$}
		\STATE $\{\mathbf{w},\boldsymbol{\alpha},\boldsymbol{\phi}\} \leftarrow \{\mathbf{w},\boldsymbol{\alpha},\boldsymbol{\phi} \}\cup\{w_n,\alpha_n,\phi_n\}$;
		\ENDIF
		\STATE $\mathbf{w},\boldsymbol{\alpha},\mathbf{b},\mathbf{ml}=$ Estimate($\mathbf{w},\boldsymbol{\alpha},\mathbf{b},\boldsymbol{\Phi},\boldsymbol{\phi},\sigma$);
		\STATE compute $\sigma$ based on Eq.~\eqref{eq:noise};
		\STATE compute $\boldsymbol{Q,S}$ based on  Eq.~\eqref{eq:QS};
		\STATE compute $\boldsymbol{q,s, f}$;
		\IF{abs$(\mathbf{ml}-\mathbf{ml}_{old}) <$ minDelta}
		\STATE break;
		\ENDIF		
		\STATE $\mathbf{ml}_{old} = \mathbf{ml}$;
		\ENDFOR
	\end{algorithmic}	
"
548,1806.06457,"
\centerline{\caption{Cascade Net-Trim}}
\begin{algorithmic}
\STATE{$\hat{\mW}_{1}\gets \operatorname*{arg\,min}_{\mU}~\left\|\mU\right\|_{1} \quad \mbox{subject to} \quad \mU\in \mathcal{C}_{\epsilon}\left(\mX,\mX^{(1)},\boldsymbol{0} \right)$}

\STATE{$\hat\mX{}^{(1)} \gets \ReLU\left (\hat\mW_{ 1}{}^\top\mX\right )$}

\FOR{$\ell=2,\cdots,L$}
\STATE{
    $\Omega \gets \supp \mX^{(\ell)}$;
     $~~\epsilon_\ell \gets \gamma_\ell \left\|\left(\mW_\ell ^\top\hat\mX{}^{(\ell-1)}- \mX^{(\ell)}\right)_\Omega\right\|_F$}

\STATE{$\hat{\mW}_{\ell}\gets \operatorname*{arg\,min}_{\mU}~\left\|\mU\right\|_{1}\quad \mbox{subject to} \quad \mU\in \mathcal{C}_{\epsilon_\ell}\left(\hat\mX{}^{(\ell-1)},\mX^{(\ell)},\mW_{\ell}^\top\hat\mX{}^{(\ell-1)}\right)$}

 \STATE{$\hat\mX{}^{(\ell)} \gets \ReLU\left (\hat\mW_{ \ell}{}^\top\hat\mX{}^{(\ell-1)}\right)$  } 
\ENDFOR
    \end{algorithmic}
"
549,1806.06457,"[H]\small 
\centerline{\caption{Implementation of the Net-Trim Central Program}}

\begin{algorithmic}
\STATE{\textbf{input:} $\mX^{in}\in\mathbb{R}^{N\times P}$, $\mX^{out}\in\mathbb{R}^{M\times P}$, $\Omega$, $\mV_\Omega$, $\epsilon$, $\rho$}

\STATE{\textbf{initialize}: $\mU^{(1)}, \mU^{(2)}$ and $\mW^{(3)}$} \qquad\qquad\qquad \qquad   \texttt{\% all initializations can be with $\boldsymbol{0}$}\vspace{.04cm}

\STATE{$\mC\leftarrow \mX^{in}\mX^{in}{}^\top + \mI$\vspace{.04cm}}

\WHILE{not converged}
\STATE{$\mY\leftarrow \mW^{(3)}{}^\top\mX^{in}-\mU^{(1)}$}
 \IF{$\left \|\mY_\Omega - \mX^{out}_\Omega\right \|_F\leq \epsilon$} 
 \STATE {$\mW^{(1)}_\Omega\leftarrow \mY_\Omega$} 
 \ELSE
 \STATE{$\mW^{(1)}_\Omega\leftarrow \mX^{out}_\Omega+\epsilon\left\|\mY_\Omega - \mX^{out}_\Omega\right\|_F^{-1}\left(\mY_\Omega - \mX^{out}_\Omega\right)$} 
 \ENDIF
 \STATE{$\mW^{(1)}_{\Omega^c}\leftarrow  \mY_{\Omega^c} - (  \mY_{\Omega^c} -  \mV_{\Omega^c} )^+$}
\STATE{ $\mW^{(2)}\leftarrow S_{1/\rho}( \mW^{(3)} - \mU^{(2)} )$\quad  \texttt{~~~ \% $S_{1/\rho}$  applies to each element of the matrix}}\vspace{-.0cm}
\STATE{ $\mW^{(3)}\leftarrow \mC^{-1}(\mX^{in}( {\mW^{(1)}}+{\mU^{(1)}})^\top + \mW^{(2)}+\mU^{(2)})$}\vspace{-.0cm}
\STATE{$\mU^{(1)} \leftarrow \mU^{(1)} + \mW^{(1)} - \mW^{(3)}{}^\top\mX^{in}$}
\STATE{$\mU^{(2)} \leftarrow \mU^{(2)} + \mW^{(2)} - \mW^{(3)}$}
\ENDWHILE
\RETURN $\mW^{(3)}$

\end{algorithmic}
"
550,1806.06457,"[H]\small 
\centerline{\caption{Implementation of the Net-Trim for Convolutional Layers}}

\begin{algorithmic}
\STATE{\textbf{input:} $\mX^{in}\in\mathbb{T}_{in}$, $\mX^{out}\in\mathbb{T}_{out}$, $\Omega$, $\mV_\Omega$, $\epsilon$, $\rho$}

\STATE{\textbf{initialize}: $\mU^{(1)}\in\mathbb{T}_{out}, \mU^{(2)}\in\mathbb{T}_{w}$ and $\mW^{(3)}\in\mathbb{T}_{w}$} \qquad   \texttt{\% all initializations can be with $\boldsymbol{0}$}\vspace{.04cm}

\WHILE{not converged}
\STATE{$\mY\leftarrow \AX\left(\mW^{(3)}\right)-\mU^{(1)}$}
 \IF{$\left \|\mY_\Omega - \mX^{out}_\Omega\right \|_F\leq \epsilon$} 
 \STATE {$\mW^{(1)}_\Omega\leftarrow \mY_\Omega$} 
 \ELSE
 \STATE{$\mW^{(1)}_\Omega\leftarrow \mX^{out}_\Omega+\epsilon\left\|\mY_\Omega - \mX^{out}_\Omega\right\|_F^{-1}\left(\mY_\Omega - \mX^{out}_\Omega\right)$} 
 \ENDIF
 \STATE{$\mW^{(1)}_{\Omega^c}\leftarrow  \mY_{\Omega^c} - (  \mY_{\Omega^c} -  \mV_{\Omega^c} )^+$}
\STATE{ $\mW^{(2)}\leftarrow S_{1/\rho}( \mW^{(3)} - \mU^{(2)} )$\quad\qquad \qquad \texttt{~~~ \% $S_{1/\rho}$  applies to each element of the matrix}}\vspace{-.0cm}
\STATE{  $\mW^{(3)}\leftarrow \argmin_{\mW}~ \frac{1}{2}\left\|  \AX\left(\mW\right) - \left(\mW^{(1)} + \mU^{(1)}\right) \right\|^2_F + \frac{1}{2}\left\|\mW- \left(\mW^{(2)} + \mU^{(2)}\right) \right\|^2_F$
}\vspace{-.0cm}
\STATE{ $\mU^{(1)} \leftarrow \mU^{(1)} + \mW^{(1)} - \AX\left(\mW^{(3)}\right)$}
\STATE{$\mU^{(2)} \leftarrow \mU^{(2)} + \mW^{(2)} - \mW^{(3)}$}
\ENDWHILE
\RETURN $\mW^{(3)}$

\end{algorithmic}
"
551,1806.06457,"[H]\small 
\centerline{\caption{Least Squares Update in the Net-Trim Using Conjugate Gradient}}
\begin{algorithmic}





\STATE{\textbf{initialize}: $\mW_0=\boldsymbol{0}$; $~~\mR_0 = \AX^*\left(\mB\right) +\mC$; $~~\mP_0 = \mR_0$} \vspace{.04cm}

\FOR{$k = 1,\ldots,K_{\max}$} 
\STATE{$\mT_{k-1} = \AX\left(\mP_{k-1}\right)$}
\STATE{$\alpha_k = \frac{\left\| \mR_{k-1}\right\|_F^2}{\left\| \mT_{k-1} \right\|_F^2 + \left\|\mP_{k-1}  \right\|_F^2}$}\\[.1cm]
\STATE{$\mW_k = \mW_{k-1} + \alpha_k\mP_{k-1}$}
\STATE{$\mR_k = \mR_{k-1} - \alpha_k\left( \AX^*\left( \mT_{k-1}\right) + \mP_{k-1}\right)$}
\STATE{$\beta_k = \frac{\|\mR_k\|_F^2}{\|\mR_{k-1} \|_F^2}$}\\[.1cm]
\STATE{$\mP_{k} = \mR_k + \beta_k\mP_{k-1}$}
\ENDFOR
\STATE{\textbf{Output:} $\mW_{K_{\max}}$}

\end{algorithmic}
"
552,1806.06438,"[t!]
    \caption{Estimate $(\mu$, $\Sigma)$ for a distribution of optimal network weights $W^*$}
    \label{algo:DIP-musig}
    {
    \textbf{Input:} Set of optimal weights $W^*=\{w^*_1,w^*_2,\cdots,w^*_Q\}$ obtained from $L$-layer DCGAN run over $Q$ images; 
    number of samples $S$; number of iterations $T$. \newline
    \textbf{Output:} Mean vector $\mu\in\R^L$; covariance matrix $\Sigma\in \R^{L\times L}$. \newline
    \begin{algorithmic}[1]
    \FOR {$t = 1 \, \text{to} \, T$}
    \STATE Sample $q$ uniformly from $\{1, ..., Q\}$
    \FOR{$l = 1 \, \text{to} \, L$ \COMMENT{for each layer}}
    
    \STATE Get $v \in \R^S$, a vector of $S$ uniformly sampled weights from the $l^{th}$ layer of $w_{q}^*$
    \STATE $M_t[l,:] \leftarrow v^T$ where $M_t[l,:]$ is the $l^{th}$ row of matrix $M_t \in \R^{L\times S}$
    \STATE $\mu_t[l] \leftarrow \frac{1}{S}\sum_{i=1}^{S} v_i $
    \ENDFOR
    \STATE $\Sigma_t \leftarrow \frac{1}{S} M_t M_t^T - \mu_t \mu_t^T$
    \ENDFOR
    \STATE $\mu \leftarrow \frac{1}{T}\sum_{t=1}^{T} \mu_t$
    \STATE $\Sigma \leftarrow \frac{1}{T}\sum_{t=1}^{T} \Sigma_t$
    
    \end{algorithmic}
    }
"
553,1805.09001,"[H]
\begin{algorithmic}[1]
\Input{stimulus probability $x$, initial synaptic strength $s_0$, target strength function $\lambda$, strength step $\Delta_s$, and interations $I$.}
\Output{trajectory of strength $s$.}
\State initialize fire-together recorder ($10^4$-entries array): $recorder{\Leftarrow}0$.
\State initialize fire-together recorder pointer: $p{\leftarrow}0$.
\State initialize current strength: $s{\leftarrow}s_0$.
\State\textbf{for} $i{=}0$ \textbf{to} $I$ \textbf{do}
\State\hspace{0.5cm}preset current pointed entry of recorder: $recorder[p]{\leftarrow}0$.
\State\hspace{0.5cm}pick random $r1$ and $r2$ from uniform distribution $Unif(0, 1)$.
\State\hspace{0.5cm}\textbf{if} $x{>}r1$ \textbf{and} $s{>}r2$ \textbf{then}
\State\hspace{0.5cm}\hspace{0.5cm}neuron 1 and 2 fire together: $recorder[p]{\leftarrow}1$.
\State\hspace{0.5cm}\textbf{endif}
\State\hspace{0.5cm}\textbf{if} recorder has been traversed once ($i{\geq}10^4$) \textbf{then}
\State\hspace{0.5cm}\hspace{0.5cm}set $y$ with the proportion of $1$-entries in recorder.
\State\hspace{0.5cm}\hspace{0.5cm}set target strength: $s^*{\leftarrow}\lambda(y)$.
\State\hspace{0.5cm}\hspace{0.5cm}\textbf{if} $s^*{>}s$ \textbf{then}
\State\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}step-increase current strength: $s{\leftarrow}min(s{+}\Delta_s,1)$.
\State\hspace{0.5cm}\hspace{0.5cm}\textbf{end if}
\State\hspace{0.5cm}\hspace{0.5cm}\textbf{if} $s^*{<}s$ \textbf{then}
\State\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}step-decrease current strength: $s{\leftarrow}max(0,s{-}\Delta_s)$.
\State\hspace{0.5cm}\hspace{0.5cm}\textbf{end if}
\State\hspace{0.5cm}\textbf{end if}
\State\hspace{0.5cm}forward recorder pointer: $p{\leftarrow}(p{+}1){\bmod}10^4$.
\State\textbf{end for}
\end{algorithmic}
\caption{connection strength's tendency to fixed points}
\label{alg:connection_simulation_algorithm}
"
554,1806.06397," [!t]
	\caption{Training pipeline for MedGAN}\label{euclid}
	\label{a1}
	\begin{algorithmic}[1]
		\Require Paired training dataset $\{\left(x_l,y_l\right)\}_{l=1}^T$
		\Require Number of training epochs $N_{\textrm{epoch}} = 200$, number of training iterations for generator $N_\textrm{G} = 3$, $\lambda_1 = 20$ and $\lambda_2 = \lambda_3 = 0.0001$
		\Require Load pretrained VGG-19 network parameters
		\Ensure Weight parameters of generator and discriminator $\theta_G$, $\theta_D$
		\For{$n = 1,...,N_{\textrm{epoch}}$}
		\For{$l = 1,...,T$}
		\For{$t = 1, ..., N_G$}
		\State $\mathcal{L}_{\small\textrm{cGAN}} \gets -\textrm{log} \left(D\left(G(y_l),y_l\right) \right)$
		\State $\mathcal{L}_{\small\textrm{perceptual}} \gets \sum_{i} \lambda_{pi} P_i \left(G(y_l),x_l\right)$
		\State $\mathcal{L}_{\small\textrm{style}} \gets \sum_{j}  \frac{\lambda_{sj}}{4 d_j^2} \lVert{Gr_j\left(G(y_l)\right) - Gr_j\left(x_l\right)}\rVert_F^{2}$
		\State $\mathcal{L}_{\small\textrm{content}} \gets \sum_{j}  \frac{\lambda_{cj}}{h_j w_j d_j} \lVert{V_j\left(G(y_l)\right) - V_j\left(x_l\right)}\rVert_F^{2}$
		\State $\theta_G \overset{+}{\gets} - \nabla_{\theta_G} [\mathcal{L}_{\small\textrm{cGAN}} + \lambda_1 \mathcal{L}_{\small\textrm{perceptual}} + \lambda_2 \mathcal{L}_{\small\textrm{style}}$\newline 
		$\hspace*{30.5mm} + \lambda_3 \mathcal{L}_{\small\textrm{content}}]$ 
		\EndFor
		\State \small $\mathcal{L}_{\small\textrm{cGAN}} \gets \textrm{log} \left(D\left(x_l,y_l\right) \right) + \textrm{log} \left(1-D\left(G(y_l),y_l\right) \right)$
		\State $\theta_D \overset{+}{\gets} \nabla_{\theta_D} \left[\mathcal{L}_{\small\textrm{cGAN}}\right]$
		\State Spectral normalization: $\theta_{D,i} = \theta_{D,i} / \delta(\theta_{D,i})$
		\EndFor
		\EndFor
	\end{algorithmic}
"
555,1802.00168,"
\caption{DNNs with WNLL as Output Activation: Training Procedure.}\label{alg-Train}
\begin{algorithmic}
\State \textbf{Input: } Training set: (data, label) pairs $(\mathbf{X}, \mathbf{Y})$.
\State \textbf{Output: } An optimized DNNs with WNLL as output activation, denoted as ${\rm DNN}_{\rm WNLL}$.
\For {${\rm iter} = 1, $\dots$, N$ (where $N$ is the number of alternating steps.)}
% Train Fig 2(b) and then prune.
\State //Train the left branch: DNNs with linear activation.
\State Train DNN $+$ Linear blocks, and denote the learned model as ${\rm DNN}_{\rm Linear}$.
\State //Train the right branch: DNNs with WNLL activation.
\State Split $(\mathbf{X}, \mathbf{Y})$ into training data and template, i.e., $(\mathbf{X}, \mathbf{Y}) \doteq (\mathbf{X}^{\rm tr}, \mathbf{Y}^{\rm tr}) \bigcup (\mathbf{X}^{\rm te}, \mathbf{Y}^{\rm te})$.
\State Partition the training data into $M$ mini-batches, i.e., $(\mathbf{X}^{\rm tr}, \mathbf{Y}^{\rm tr}) = \bigcup_{i=1}^M (\mathbf{X}_i^{\rm tr}, \mathbf{Y}_i^{\rm tr})$.
\For {$i = 1, 2, \cdots, M$}
\State Transform $\mathbf{X}_i^{\rm tr}\bigcup \mathbf{X}^{\rm te}$ by ${\rm DNN}_{\rm Linear}$, i.e.,  $\tilde{\mathbf{X}}^{\rm tr} \bigcup \tilde{\mathbf{X}}^{\rm te} = {\rm DNN}_{\rm Linear}(\mathbf{X}_i^{\rm tr}\bigcup \mathbf{X}^{\rm te})$.
\State Apply WNLL (Eq.(\ref{WNLL})) on $\{\tilde{\mathbf{X}}^{\rm tr} \bigcup \tilde{\mathbf{X}}^{\rm te}, \mathbf{Y}^{\rm te}\}$ to interpolate label $\tilde{\mathbf{Y}}^{\rm tr}$.
%\State Freeze all layers except the buffer block $W_B$. Backpropagate via Eq. (\ref{eq:bp-wnll}) the error 
%\State between $\mathbf{Y}^{\rm tr}$ and $\tilde{\mathbf{Y}}^{\rm tr}$ \xmod{on the buffer block only}. 
\State Backpropagate the error between $\mathbf{Y}^{\rm tr}$ and $\tilde{\mathbf{Y}}^{\rm tr}$ via Eq.(\ref{eq:bp-wnll}) to update $\mathbf{W}_B$ only.
\EndFor
\EndFor
%\State Prune the linear function from the auxiliary network (Fig.\ref{fig:WNLL-DNN-Structure} (b)) to get the final ${\rm DNN}_{\rm WNLL}$.
%\xmod{(Comment: Why do we need to remove the final linear layer? We have already trained the weights of the DNN. What activation to choose at test time is not relevant. I think we can just remove this line.) }
\end{algorithmic}
"
556,1802.00168,"
\caption{DNNs with WNLL as Output Activation: Testing Procedure.}\label{alg-Test}
\begin{algorithmic}
\State \textbf{Input: } Testing data $\mathbf{X}$, template $(\mathbf{X}^{\rm te}, \mathbf{Y}^{\rm te})$. Optimized model ${\rm DNN}_{\rm WNLL}$.
\State \textbf{Output: } Predicted label $\tilde{\mathbf{Y}}$ for $\mathbf{X}$.
\State Apply the DNN block of ${\rm DNN}_{\rm WNLL}$ to $\mathbf{X}\bigcup \mathbf{X}^{\rm te}$ to get the representation $\tilde{\mathbf{X}}\bigcup \tilde{\mathbf{X}}^{\rm te}$.
\State Apply WNLL (Eq.(\ref{WNLL})) on $\{\tilde{\mathbf{X}} \bigcup \tilde{\mathbf{X}}^{\rm te}, \mathbf{Y}^{\rm te}\}$ to interpolate label $\tilde{\mathbf{Y}}$.
\end{algorithmic}
"
557,1806.06317,"[t]
\caption{LSSGD}\label{LSSGD-Pseudocode}
\begin{algorithmic}
\State \textbf{Input: } $f_i(\vw)$ for $i=1, 2, \cdots, n$. \\
$\vw^0$: initial guess of $\vw$, $T$: the total number of iterations, and $\eta_k$, $k=0, 1, \cdots, T$: the scheduled step size.
\State \textbf{Output: } The optimized weights $\vw^{\rm opt}$.
\For {$k=0, 1, \cdots, T$}
\State $\vw^{k+1} = \vw^k - \eta\A_\sigma^{-1}\left(\nabla f_{i_k}(\vw^k)\right)$.
\EndFor
\Return $\vw^T$
\end{algorithmic}
"
558,1803.05554,"[tb] 
   \caption{Minimal I-MAP MCMC}
\begin{algorithmic}
   \STATE {\bfseries Input:} \ Data $D$, number of iterations $T$, significance level $\alpha$, initial permutation $\pi_0$, sparsity strength $\gamma$, thinning rate $\kappa$
   \STATE {\bfseries Output:} $\hat{G}_{\pi_1}, \cdots, \hat{G}_{\pi_{\ceil{\kappa T}}}$
  % \STATE Empirical correlation matrix $\hat{\Sigma}^{-1} = \hat{\Sigma}^{-1}(D)$ 
   \STATE Construct $\hat{G}_{\pi_0}$ from $\hat{\mathcal{O}}_n(D,\alpha)$ via Fisher's z-transform
   \FOR{$i=1$ {\bfseries to} $T$}
   \STATE Sample $\pi_i \sim q(\pi_{i-1} \rightarrow \pi_{i})$%, for $q$ defined in Proposition \ref{prop:time_memory_ergodic}. 
   \STATE $\hat{G}_{\pi_i} = \textrm{UMI}(\pi_i, \pi_{i-1}, \hat{G}_{\pi_{i-1}}, \alpha, D)$ (Algorithm \ref{update_min_imap}, Appendix \ref{A:update_algo})
   \STATE $p_{i-1} = \log \mathbb{P}(D \mid \hat{G}_{\pi_{i-1}})\mbp(\hat{G}_{\pi_{i-1}})$
   \STATE $p_{i} = \log \mathbb{P}(D \mid \hat{G}_{\pi_{i}})\mbp(\hat{G}_{\pi_{i}})$
   \STATE $s_i = \min \big\{ 1, \exp(p_i - p_{i-1})\big\}$
   \STATE $z_i \sim \textrm{Bernoulli}(s_i)$
   \IF{$z_i = 0$}
   \STATE $\pi_{i} = \pi_{i-1}$ and $\hat{G}_{\pi_i}$ = $\hat{G}_{\pi_{i-1}}$ (chain does not move)
   \ENDIF
   \IF{$T$ is divisible by $\ceil{\frac{1}{\kappa}}$}
   \STATE Store $\hat{G}_{\pi_i}$
   \ENDIF
   \ENDFOR   
\end{algorithmic}
\label{algo}
"
559,1803.05554,"[tb] 
   \caption{Update Minimal I-MAP (\emph{UMI})}
\begin{algorithmic}
   \STATE {\bfseries Input:} Current permutation $\pi_i$, previous permutation $\pi_{i-1}$, previous minimal I-MAP $G_{\pi_{i-1}}$, significance level $\alpha$, data $D$
   %\IF{$\pi$ and $\pi_{i-1}$} do not differ by an adjacent transposition
   \STATE {\bfseries Output:} $G_{\pi_{i}}$
   \STATE $k$ = min index of adjacent transposition
   \IF{$k = 1$ (first and last element swapped)}
   \STATE Compute $\hat{G}_{\pi_i}$ from $\hat{\mathcal{O}}_n(D,\alpha)$
   \ELSE
   \STATE $G_{\pi_{i}}$ = $G_{\pi_{i-1}}$
   \STATE Reverse edge from $X_{\pi_{i}(k+1)}$ to $X_{\pi_{i}(k)}$ in $G_{\pi_{i}}$ if such an edge exists
   \FOR{$s=1$ {\bfseries to} $k-1$} 
   \FOR{$j=k$ {\bfseries to} $k+1$}
   \STATE $S = \{\pi(1), \cdots, \pi(j - 1) \} \setminus \{\pi(s) \}$
   \STATE Let $z = \hat{\mathcal{O}}^{(n)}_{i,j|S}(D,\alpha)$ 
   \STATE Update edge from $X_{\pi(s)}$ to $X_{\pi(j)}$ to $z$ in $G_{\pi_{i}}$
   \ENDFOR
   \ENDFOR  
   \ENDIF
\end{algorithmic}
\label{update_min_imap}
"
560,1806.06266,"[htb]
\centering
\caption{\BiparCATAlgo assignment}
\label{alg:assign}
\begin{algorithmic}[1]
\Require conflict graph $\conflictgraph$, parameters $\minreviewers, \maxpapers$, assignment algorithm $\genassalgo$
\Ensure an assignment of reviewers to papers
% \State partition the vertices of the conflict graph $\conflictgraph$ into two groups $(\rcx, \pcx)$ and $(\rcy, \pcy)$ such that there are no edges across the groups and that $\max\big\{ \frac{|\pcx|}{|\rcy|}, \frac{|\pcy|}{|\rcx|}\big\} \leq \frac{\maxpapers}{\minp}$~~(if impossible then return {\sc error})
\State $(\rcx, \pcx)$, $(\rcy, \pcy)\leftarrow\partition(\conflictgraph, \minreviewers, \maxpapers)$
\State use algorithm $\genassalgo$ to assign papers $\pcy$ to reviewers $\rcx$ 
\State use algorithm $\genassalgo$ to assign papers $\pcx$ to reviewers $\rcy$
\State \textbf{return} the union of assignments from step 2 and 3
\\\hrulefill
\Procedure{\partition}{conflict graph $\conflictgraph$, parameters $\minreviewers, \maxpapers$}
\State  run a BFS on $\conflictgraph$ to get connected $K$ components $\{(\setreviewers_k, \setpapers_k)\}_{k=1}^K$
\State  let $r_k = |\setreviewers_k|, p_k = |\setpapers_k|$, $\forall k\in[K]$
\State  initialize a table $T[\cdot, \cdot, \cdot]\in\{0, 1\}^{K\times (\numreviewers + 1)\times (\numpapers + 1)}$ so that $T[1, r_1, p_1] = T[1, 0, 0] = 1$, otherwise 0
\For {$k = 2$ to $K$}
    \State  $T[k, r, p] = T[k-1, r, p] \vee T[k-1, r-r_k, p-p_k],~\forall 0\leq r\leq \numreviewers, 0\leq p\leq \numpapers$ 
\EndFor
\State  for $0\leq r\leq \numreviewers, 0\leq p\leq \numpapers$, if there is no $T[K, r, p] = 1$ such that $\max\{\frac{p}{\numreviewers - r}, \frac{\numpapers - p}{r}\}\leq \frac{\maxpapers}{\minp}$, return \textsc{error}
\State  use the standard backtracking in the table $T[\cdot, \cdot, \cdot]$ to return $(\rcx, \pcx)$ and $(\rcy, \pcy)$
\EndProcedure
\end{algorithmic}
"
561,1806.06266,"[htb]
\centering
\caption{\BiparCATAlgo aggregation}
\label{alg:aggregate}
\begin{algorithmic}[1]
\Require profile $\profile = (\indexedpaperranking{\setpaperselem{1}}{1}, \ldots, \indexedpaperranking{\setpaperselem{\numreviewers}}{\numreviewers} )$, groups $(\rcx, \pcx), (\rcy, \pcy)$ with $|\pcx|\geq |\pcy|$, aggregation algorithm $\genaggalgo$
\Ensure total ranking of all papers
\State compute $\profile_C$ as the restriction of profile $\profile$ to only papers in $\pcx$, and $\profile_{\bar{C}}$ as the restriction of profile $\profile$ to only papers in $\pcy$
\State  $\rankingx \leftarrow \catname(\profile_C, \genaggalgo)$
\State  $\rankingy \leftarrow \catname(\profile_{\bar{C}}, \genaggalgo)$
\State define $I=\left(\left\lfloor \frac{\numpapers}{|\pcx|} \right\rfloor, \left\lfloor \frac{2\numpapers}{|\pcx|} \right\rfloor,...,\numpapers \right)$
\State {\bf return} total ranking obtained by filling papers in $\pcx$ into positions in $I$ in order given by $\rankingx$, and papers in $\pcy$ into positions in $[\numpapers] \backslash I$ in order given by $\rankingy$\label{step:interleaving}
\\\hrulefill
%
\Procedure{\catname}{profile $\widetilde{\profile}$, aggregation algorithm $\genaggalgo$}
    \State  build a directed graph $G_{\widetilde{\profile}}$ with the papers in $\widetilde{\profile}$ as its vertices and no edges
    \For {each $i \in [\numreviewers']$}    
        \State denoting $\pi^{(i)} = (\paper{i_1}\succ\ldots \succ \paper{i_{t_i}}$), add a directed edge from $\paper{i_j}$ to $\paper{i_{j+1}}$ in $G_{\widetilde{\profile}}$, $\forall j\in[t_i -1]$\label{step:build_edge}
    \EndFor
    \State  for every ordered pair $(\paper{j_1}, \paper{j_2})\in E_{G_{\widetilde{\profile}}}$, replace multiple edges from $\paper{j_1}$ to $\paper{j_2}$ with a single edge
    \State compute a topological ordering of the strongly connected components (SCCs) in $G_{\widetilde{\profile}}$
    \State for every SCC in $G_{\widetilde{\profile}}$, compute a permutation of the papers in the component using algorithm $\genaggalgo$
    \State \textbf{return} the permutation of all papers that is consistent with the topological ordering of the SCCs and the permutations within the SCCs
\EndProcedure
\end{algorithmic}
"
562,1806.06232,"
  \caption{HCBR (High level view)}\label{algo:HCBR}
  \begin{algorithmic}[1]
    \State Build $H$ and $\mathcal E$ from $\mathbf X$.
    \State Calculate $w$ and $\mu$ on $\mathcal E$.
    \State Adjust $\mu$ with training algorithm \ref{training} on $\mathbf X$ using rule \eqref{eqn:decision_rule}
    \For{each $\mathbf x$ in the test set}
        \State Calculate the projection $\pi(\mathbf x)$.
        \State Calculate the support $s(\mathbf x)$ using the projection.
        \State Predict using the refined rule \eqref{eqn:updated_cr}.
    \EndFor
  \end{algorithmic}
"
563,1806.06232,"
  \caption{Model training}\label{training}
  \begin{flushleft}
  \textbf{Input:} \\
    ~~- $\mathbf X$: training set \hfill\\
    ~~- $\mathbf y$: correct labels for $\mathbf X$\\
    ~~- $k$: number of training iterations\\
    ~~- $\mu^{(1)}, \mu^{(-1)}$: weights calculated with \eqref{intrinsic_strength}\\
    \textbf{Output:} \\
    ~~- Modified vectors $\mu^{(1)}, \mu^{(-1)}$
  \end{flushleft}
  \begin{algorithmic}[1]
      
      \For{$k$ \texttt{iterations}}
        \For{$\mathbf x_i \in \mathbf X$}
          \State $\bar y_i \gets \bar J(\mathbf x_i)$
          \If{$\bar y_i \neq y_i$}
            \For{$\mathbf e \in \pi(\mathbf x_i)$}
              \State $\mu^{(y_i)}(\mathbf e) \gets \mu^{(y_i)}(\mathbf e) + w(\mathbf e,\mathbf x_i) |\mu(\mathbf e)|$
              \State $\mu^{(\bar y_i)}(\mathbf e) \gets \mu^{(\bar y_i)}(\mathbf e) - w(\mathbf e,\mathbf x_i) |\mu(\mathbf e)|$
            \EndFor
          \EndIf
        \EndFor
      \EndFor
  \end{algorithmic}
"
564,1806.06228,"[!ht]
    \small
    \caption{Context-Aware Hierarchical Fusion Algorithm}\label{algorithm}
    \begin{algorithmic}[1]
        
        
        \vspace{2mm}
        \Procedure{TrainAndTestModel}{$U$, $V$}\Comment{\footnotesize{$U$ = train set, $V$ = test set}}
        \vspace{2mm}
        \State \textbf{Unimodal feature extraction:}
        \For{\texttt{i:[1,$N$]}}\Comment{\footnotesize{extract baseline features}}
            \State \texttt{$f_{A}^{i} \gets AudioFeatures(u_{i})$ }
            \State \texttt{$f_{V}^{i} \gets VideoFeatures(u_{i})$ }
            \State \texttt{$f_{T}^{i} \gets TextFeatures(u_{i})$ }
        \EndFor
        \For{\texttt{m $\in \{A, V, T\}$}}
            \State $F_m$ = $GRU_m$($f_m$)
        \EndFor
        \vspace{2mm}
        \State \textbf{Fusion}:
        \State \texttt{$g_{A} \gets MapToSpace(F_A)$ }\Comment{\footnotesize{dimensionality equalization}}
        \State \texttt{$g_{V} \gets MapToSpace(F_V)$ }
        \State \texttt{$g_{T} \gets MapToSpace(F_T)$ }

        \vspace{2mm}
        \State \texttt{$f_{VA} \gets BimodalFusion(g_V, g_A)$}\Comment{\footnotesize{bimodal fusion}}
        \State \texttt{$f_{AT} \gets BimodalFusion(g_A, g_T)$}
        \State \texttt{$f_{VT} \gets BimodalFusion(g_V, g_T)$}
        \For{\texttt{m $\in \{VA, AT, VT\}$}}
        \State $F_m$ = $GRU_m$($f_m$)
        \EndFor
        
        \vspace{2mm}
        \State $f_{AVT} \gets TrimodalFusion(F_{VA}, F_{AT}, F_{VT})$
        \Comment{\small{trimodal fusion}}
        \State $F_{AVT}$ = $GRU_{AVT}$($f_{AVT}$)

        \vspace{2mm}
        \For{\texttt{i:[1,$N$]}}\Comment{\footnotesize{softmax classification}}
            \State $\hat{y}^i =\underset{j}{\text{argmax}}(softmax(F_{AVT}^i)[j])$ 
        \EndFor
        

        \State $TestModel(V)$
        \EndProcedure
        
        \vspace{2mm}
        \Procedure{MapToSpace}{$x_z$} \Comment{\footnotesize{for modality $z$}}
        \State $ g_z \gets \tanh(W_zx_z+b_z)  $
        \State \textbf{return} $g_z$
        \EndProcedure

        \vspace{2mm}
        \Procedure{BimodalFusion}{$g_{z_1}$, $g_{z_2}$} \Comment{\footnotesize{for modality
        $z_1$ and $z_2$, where $z_1\neq z_2$}}
        \For{\texttt{i:[1,$D$]}}
        \State $f_{z_1z_2}^i \gets \tanh(w_i^{z_1z_2}.[g^i_{z_1},
        g^i_{z_2}]^\intercal+b_i^{z_1z_2})$
        \EndFor
        \State $f_{z_1z_2} \gets (f_{z_1z_2}^1, f_{z_1z_2}^2,\dots,f_{z_1z_2}^{D})$
        \State \textbf{return} $f_{z_1z_2}$
        \EndProcedure
        
        
        \vspace{2mm}
        \Procedure{TrimodalFusion}{$f_{z_1}$, $f_{z_2}$, $f_{z_3}$} \Comment{\footnotesize{for
                modality combination $z_1$, $z_2$, and $z_3$, where $z_1\neq z_2\neq z_3$}}
        \For{\texttt{i:[1,$D$]}}
        \State $f^i_{z_1z_2z_3} \gets \tanh(w_i.[f^i_{z_1}, f^i_{z_2}, f^i_{z_3}]^\intercal+b_i)$
        \EndFor
        \State $f_{z_1z_2z_3} \gets (f_{z_1z_2z_3}^1, f_{z_1z_2z_3}^2,\dots,f_{z_1z_2z_3}^{D})$
        \State \textbf{return} $f_{z_1z_2z_3}$
        \EndProcedure
        
        \vspace{2mm}
        \Procedure{TestModel}{$V$}
        \State \footnotesize{Similarly to training phase, $V$ is passed through the learnt models
        to get the features and classification outputs. \cref{training}
        mentions the trainable parameters ($\theta$).}
        \EndProcedure
    \end{algorithmic}
"
565,1801.03825,"

\SetKwInOut{Input}{input} \SetKwInOut{Output}{output}
\SetKwInOut{Fun}{function}
  %\textbf{funtion} ConnectionDensity( )
  \Fun{ConnectionDensity( )}
  \Input{$\mathcal{L}$ , with $n$ number of keywords \tcp{an array of arrays}}
  \Output{Hop-Count $\mathcal{H}$, Connection-Count $\mathcal{C}$}
 
  dConnectCounter = \{ \} \tcp{Count for connections from and to each node}
  dHopCounter = \{ \} \tcp{Similarly hop counts for each node}
  \ForEach{$L_a \in \mathcal{L}$}{
   \ForEach{$c_i^a \in L_a$}{ 
   	$dConnectCounter$[$c_i^a$] = 0  \tcp{Initialising the dictionary}
   	$dHopCounter$[$c_i^a$] = 0
   }
  }
  \ForEach{$(L_a,L_b) \in \mathcal{L}$}{
   % \tcp{label matching}
   \ForEach{$c_i^a \in L_a$}{
   	\ForEach{$c_j^b \in L_b$}{
    \If{$dKGhops(c_i^a$,$c_j^b$) <= 2} {
    $dConnectCounter$[$c_i^a$] += 1 \\
    $dConnectCounter$[$c_j^b$] += 1 \\
    }
    $dHopCounter$[$c_i^a$] += dKGhops($c_i^a$,$c_j^b$)\\
    $dHopCounter$[$c_j^b$] += dKGhops($c_i^a$,$c_j^b$)
  }
  }
}
\ForEach{$(c_i,score) \in dConnectCounter$}{
    $\mathcal{C} (c_i)$  = $dConnectCounter(c_i)/n$	  \tcp{Normalisation with respect to number of keywords spotted}
}
\ForEach{$(c_i,score) \in dHopCounter$}{
	$\mathcal{H} (c_i)$  = $dHopCounter(c_i)/n$
}
  \Return (Hop-Count $\mathcal{H}$, Connection-Count $\mathcal{C}$)
  %\tcp{Step 4: Retrieval}


\caption{Connection Density}

\label{alg:conn}

"
566,1806.06193,"[t]
% \label{alg:domain_selection}
% \KwIn{$\mathcal{S} = \{ (s_i, w_{s_i}) \}_{i=1}^m$, $\mathcal{T} = \{ (t_j, w_{t_j}) \}_{j=1}^n$, $k \leq m$}
% \KwOut{$\mathcal{D} \subset \mathcal{S}$, where $|\mathcal{D}| = k$}

% Initialization: $\mathcal{D} \leftarrow \emptyset$, d \leftarrow []\\
% \For{$i\leftarrow 1$ \KwTo $m$}{
% \text{EMD}(\{(s_i, 1)\}, \mathcal{T})\\
% }
% \caption{Source Domain selection}
% "
567,1806.04899,"[h]
\small
\caption{\small \fullgreedy{} (\greedy{})}
\label{greedy}
\begin{algorithmic}[1]
    \REQUIRE Set of an original ensemble $\mathcal{H}$, threshold $k$ as the size of the pruned sub-ensemble. 
    \ENSURE Set of the pruned sub-ensemble $\mathcal{P}$ satisfying that $\mathcal{P}\subset\mathcal{H}$ and $|\mathcal{P}|\leqslant k$. 
    \STATE $\mathcal{P}\gets$ an arbitrary individual classifier $h_i\in\mathcal{H}$. 
    \FOR{$2\leqslant i\leqslant k$}
        \STATE $h^*\gets \argmax_{h_i\in\mathcal{H}\setminus\mathcal{P}} \sum_{h_j\in\mathcal{P}} \dist(h_i,h_j)$. 
        \STATE Move $h^*$ from $\mathcal{H}$ to $\mathcal{P}$. 
    \ENDFOR
\end{algorithmic}
"
568,1806.04899,"[h]
\small
\caption{\small \fullddismi{} (\ddismi{})}
\label{ddismi}
\begin{algorithmic}[1]
    \REQUIRE Set of an original ensemble $\mathcal{H}$, threshold $k$ as the size of the pruned sub-ensemble, number of machines $m$. 
    \ENSURE Set of the pruned sub-ensemble $\mathcal{P}$ meeting that $\mathcal{P}\subset\mathcal{H}$ and $|\mathcal{P}|\leqslant k$. 
    \STATE Partition $\mathcal{H}$ randomly into $m$ groups as equally as possible, \ie{} 
    $\mathcal{H}_1, ..., \mathcal{H}_m$. 
    \label{alg2:s1}
    \FOR{$1\leqslant i\leqslant m$}
        \STATE $\mathcal{P}_i\gets$ \greedy{}($\mathcal{H}_i$, $k$). 
    \ENDFOR
    \STATE $\mathcal{P}'\gets$ \greedy{}($\cup_{i=1}^m\mathcal{P}_i$, $k$). 
    \label{alg2:s5}
    \STATE $\mathcal{P}\gets \argmax_{\mathcal{T}\in\{ \mathcal{P}_1,...,\mathcal{P}_m,\mathcal{P}' \}} \divs(\mathcal{T})$. 
    \label{alg2:s6}
\end{algorithmic}
"
569,1806.04899,"[h]
\small
\caption{\small \fullframe{} (\distributed{})}
\label{distributed}
\begin{algorithmic}[1]
    \REQUIRE Set of an original ensemble $\mathcal{H}$, number of machines $m$, a pruning method \algpru{}. 
    \ENSURE Set of the pruned sub-ensemble $\mathcal{P}$ meeting that $\mathcal{P}\subset\mathcal{H}$. 
    \STATE Partition $\mathcal{H}$ into $\{\mathcal{H}_i\}_{i=1}^m$ randomly. 
    \FOR{$1\leqslant i\leqslant m$}
        \STATE $\mathcal{P}_i\gets$ output from any pruning method \algpru{} on $\mathcal{H}_i$. 
        \label{alg3:s3}
    \ENDFOR
    \STATE $\mathcal{P}'\gets$ output from \algpru{} on $\cup_{i=1}^m \mathcal{P}_i$. 
    \label{alg3:s5}
    \STATE $\mathcal{P}\gets$ the best one among $\mathcal{P}_i,...,\mathcal{P}_m,$ and $\mathcal{P}'$ according to some certain criteria such as accuracy. 
    \label{alg3:s6} 
\end{algorithmic}
"
570,1806.06161,"[t]
\begin{algorithmic}
\small
\Require Hyperparameters $N_\text{new}, N_\text{old}, T, C_\text{pass}, N_\text{TP}, C_\text{select}$
\Function{BaRC}{$s_g$, $\rho_0$, $\hat{M}$}
\State $\pi \gets \pi_0$
\State starts $\gets$ $\{s_g\}$
\State oldstarts $\gets \{s_g\}$
\For{$i$ = 1, 2 \dots}
\State starts\_set $\gets$ \Call{ExpandBackwards}{starts, $\hat{M}$, $T$}
\State frac\_successful $\gets$ 0.0
\While{frac\_successful $< C_\text{pass}$}
\State $\rho_i \gets \mathrm{Unif}(\text{starts\_set}, N_\text{new}) \cup \mathrm{Unif}(\text{oldstarts}, N_\text{old})$ 
\State $\pi$, success\_map $\gets$ \Call{TrainPolicy}{$\rho_i$, $\pi$, $N_\text{TP}$}
\State starts $\gets$ \Call{Select}{success\_map, $C_\text{select}$}
\State oldstarts $\gets$ oldstarts $\cup$ starts
\State frac\_successful $\gets$ \Call{Evaluate}{$\rho_i$, $\pi$}
\EndWhile
\State iter\_result $\gets$ \Call{PerformanceMetric}{$\rho_0$, $\pi$}
\EndFor
\State \Return $\pi$
\EndFunction
\end{algorithmic}
\caption{The full BaRC algorithm.}
\label{alg:full_alg}
"
571,1806.06144,"[H]
\caption{Improved EigenPro iteration\\
(double coordinate block descent)}
\label{alg:epro}
\begin{algorithmic}

\STATE{\bf Input:} Kernel function $k(\bx, \bz)$,
EigenPro parameter $\epp$,
mini-batch size $m$, step size $\eta$,
size of fixed coordinate block $s$
\STATE{}
\vspace{-1.5mm}

\STATE{\bf initialize} model parameter $\balpha = (\alpha_1, \ldots, \alpha_n)^T \leftarrow 0$

\STATE{\bf subsample} $s$ coordinate indices $r_1, \ldots, r_s \in \left\{1, \ldots, n\right\}$ for constructing $\rmP_\epp$,
which form fixed coordinate block 
$\balpha_r \defeq (\alpha_{r_1}, \ldots, \alpha_{r_s})^T$

\STATE{\bf compute} top-$\epp$ eigenvalues
$\Sigma \defeq \text{diag}(\sigma_1, \ldots, \sigma_\epp)$ and corresponding eigenvectors
$V \defeq (\be_1, \ldots, \be_\epp)$ of
subsample kernel matrix
$K_s = [\rmk(\rx{i}, \rx{j})]_{i, j = 1}^s$

\STATE{}
\vspace{-1.5mm}
\FOR{$t = 1, \ldots$}
\STATE 1. sample a mini-batch $(\tx{1}, \ty{1}), \ldots, (\tx{m}, \ty{m})$

\STATE 2. calculate predictions on the mini-batch\\
\vspace{-6mm}
$$
f(\bx_{t_j}) = \sum_{i=1}^n \alpha_i \rmk(\bx_i,\bx_{t_j})
~~\textnormal{for}~~ j = 1, \ldots, m
$$
\vspace{-4mm}

\STATE 3. update {\bf sampled coordinate block} corresponding to the mini-batch
$\balpha_t \defeq (\alpha_{t_1}, \ldots, \alpha_{t_m})$,
\vspace{-2mm}
$$
\balpha_t \leftarrow \balpha_t - \eta \cdot
\frac{2}{m} (f(\tx{1}) - \ty{1}, \ldots, f(\tx{m}) - \ty{m})^T
$$
\vspace{-4mm}

\STATE 4. evaluate the following feature map $\phi(\cdot)$ on the mini-batch features $\bx_{t_1}, \ldots, \bx_{t_m}$:\\
\vspace{-2mm}
$$
\phi(\bx) \defeq (\rmk(\rx{1}, \bx), \ldots, \rmk(\rx{s}, \bx))^T
$$

\STATE 5. update {\bf fixed coordinate block}
$\balpha_r$ to apply $\rmP_\epp$,
\vspace{-2mm}
\begin{equation*}
\begin{split}
\balpha_r \leftarrow & \balpha_r + \eta \cdot \frac{2}{m}
\sum_{i=1}^m
(f(\tx{i}) - \ty{i}) \cdot
VDV^T \phi(\tx{i}) \\
&\textnormal{where}~~ D \defeq (1 - \sigma_\epp \cdot \Sigma^{-1})\Sigma^{-1}
\end{split}
\end{equation*}
\vspace{-4mm}
\ENDFOR

% Return{\textnormal{learned model parameter} $\balpha$}
\end{algorithmic}
"
572,1806.06123,"[H]
      \caption{Uncertainty Sampling}
      \label{alg:us}
      \begin{algorithmic}
        \STATE {\bfseries Input:} Probabilistic model $p_w(y|x)$, unlabeled $X_{\text{U}}$, $\nseed$
        \STATE Randomly sample $\nseed$ points without replacement from $\sX_{\text{U}}$ and call them $\sX_{\text{seed}}$.
        \STATE $\sX_{\text{U}} = \sX_{\text{U}} \setminus \sX_{\text{seed}}$
        \STATE $\sD = \emptyset$
        \FOR{each $x$ in $\sX_{\text{seed}}$}
          \STATE Query $x$ to get label $y$
          \STATE $\sD = \sD \cup \{(x,y)\}$
        \ENDFOR
        \FOR{$n - \nseed$ iterations}
          \STATE $\hat{w} = \argmin_w \sum_{(x,y) \in \sD} -\log p_w(y|x)$
          \STATE Choose $x = \argmin_{x \in \sX_{\text{U}}} |P_{ \hat{w}}(y | x) - \frac{1}{2}|$
          \STATE Query $x$ to get label $y$
          \STATE $\sX_{\text{U}} = \sX_{\text{U}} \setminus \{x\}$
          \STATE $\sD = \sD \cup \{(x,y)\}$          
        \ENDFOR
        \STATE $\hat{w} = \argmin_w \sum_{(x,y) \in \sD} -\log p_w(y|x)$ and return $\hat{w}$
      \end{algorithmic}
    "
573,1806.06123,"[tb]
   \caption{Bubble Sort}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $x_i$, size $m$
   \REPEAT
   \STATE Initialize $noChange = true$.
   \FOR{$i=1$ {\bfseries to} $m-1$}
   \IF{$x_i > x_{i+1}$}
   \STATE Swap $x_i$ and $x_{i+1}$
   \STATE $noChange = false$
   \ENDIF
   \ENDFOR
   \UNTIL{$noChange$ is $true$}
\end{algorithmic}
"
574,1806.05924,"[h]
\caption{Metropolis-Hastings within Gibbs sampler for sampling from $p(\boldsymbol{\theta}_{\geq i} | \mathscr{X}, \hat{\boldsymbol{\theta}}_{< i})$. \label{alg:propMHGibs}}
\begin{algorithmic}
\FOR {$t$ from 1 to $M$}
\FOR  {$j$ from $i$ to $k+1$}
	\STATE $\boldsymbol{\psi} := \{\hat{\boldsymbol{\theta}}_{<i}, \boldsymbol{\theta}_{i}^{t}, \ldots, \boldsymbol{\theta}_{j-1}^{t}, \boldsymbol{\theta}_{> j}^{t-1} \}$
	\STATE $\boldsymbol{\theta}_j^{t} := MH_j (\boldsymbol{\theta}_j^{t-1},  \boldsymbol{\psi})$
\ENDFOR
\ENDFOR
\end{algorithmic}
"
575,1806.05924,"
\caption{Spectral Clustering for variable clustering with the Gaussian graphical model. \label{alg:spectralVarClustering}}
\begin{algorithmic}
\STATE $J$ := set of regularization parameter values.
\STATE $K_{max}$ := maximum number of considered clusters.
\STATE $\mathscr{C}^* := \{ \}$
\FOR  {$\lambda \in J$}
\STATE $X^* := $ solve optimization problem from Equation \eqref{eq:finalOptProblem}.
\STATE $(\mathbf{e}_1,\ldots, \mathbf{e}_{K_{max}}) := $ determine the eigenvectors corresponding to the $K_{max}$ lowest eigenvalues of the Laplacian $L$ as defined in Equations \eqref{eq:laplacianDef}.
\FOR  {$k \in \{2,\ldots ,K_{max}\}$}
\STATE $\mathcal{C}_{\lambda, k} := $ cluster all variables into $k$ partitions using k-means with $(\mathbf{e}_1,\ldots, \mathbf{e}_k)$.
\STATE $\mathscr{C}^* := \mathscr{C}^* \cup \mathcal{C}_{\lambda, k}$
\ENDFOR
\ENDFOR
\RETURN restricted hypotheses space  $\mathscr{C}^*$
\end{algorithmic}
"
576,1806.05767,"[b]
\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
$\boldsymbol{Z} \gets \mathrm{Enet}(\boldsymbol{x_\mathrm{obs}})$\;
$\tau \gets \mathrm{NeuralPlanner}(\boldsymbol{x_\mathrm{init}},\boldsymbol{x_\mathrm{goal}}, \boldsymbol{Z});$\;
\If{$\tau$}
   {

$\tau \gets \mathrm{LazyStatesContraction}(\tau)$\;
 \If{$\mathrm{IsFeasible}(\tau)$}
   {
      \Return$\tau$\
   }
   \Else
   {
   	$\tau_\mathrm{new} \gets \mathrm{Replanning(\tau,\boldsymbol{Z})}$\;
   	$\tau_\mathrm{new} \gets \mathrm{LazyStatesContraction}(\tau_\mathrm{new})$\;
 \If{$\mathrm{IsFeasible}(\tau_\mathrm{new})$}
   {
      \Return$\tau_\mathrm{new}$\;
   }

   \Return$\varnothing$\;
   }
   
   }

\caption{MPNet($\boldsymbol{x_\mathrm{init}},\boldsymbol{x_\mathrm{goal}}, \boldsymbol{x_\mathrm{obs}}$)}
\label{algo:NPG}
"
577,1806.05767,"[b]
\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
$\tau^\mathrm{a} \gets \{\boldsymbol{x_\mathrm{start}}\}; \tau^\mathrm{b} \gets \{\boldsymbol{x_\mathrm{goal}}\};$\;

$\tau\gets \varnothing;$\;
$\mathrm{Reached} \gets \mathrm{False};$\;
\For{$i \gets 0$ \textbf{to} $N$} {
	
	$\boldsymbol{x_\mathrm{new}} \gets \mathrm{Pnet}\big(\boldsymbol{Z},\tau^\mathrm{a}(\mathrm{end}),\tau^\mathrm{b}(\mathrm{end})\big)$\;
	$\tau^\mathrm{a} \gets \tau^\mathrm{a} \cup \{\boldsymbol{x_\mathrm{new}}\}$\;
	$\mathrm{Connect} \gets \mathrm{steerTo}\big(\tau^\mathrm{a}(\mathrm{end}),\tau^\mathrm{b}(\mathrm{end})\big)$\;
   \If{$\mathrm{Connect}$}
   {
   	  $\tau \gets \mathrm{concatenate}(\tau^\mathrm{a},\tau^\mathrm{b})$\;
      \Return{$\tau$}\;
   }
   $\mathrm{SWAP}(\tau^\mathrm{a},\tau^\mathrm{b})$\;
   }


\Return{$\varnothing$}\;
\caption{NeuralPlanner($\boldsymbol{x_\mathrm{start}}, \boldsymbol{x_\mathrm{goal}}, \boldsymbol{Z}$)}
\label{algo:NP}
"
578,1806.05767,"[t]
\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
$\tau_\mathrm{new}\gets \varnothing;$\;
\For{$i \gets 0$ \textbf{to} $\tau.\mathrm{length}()$} {
   		\If{$\mathrm{steerTo}(\tau_i,\tau_{i+1})$}
   		{
   			$\tau_\mathrm{new} \gets \tau_\mathrm{new} \cup \{\tau_i,\tau_{i+1}\}  $\;
   		}
   		\Else
   		{
   		$\tau_\mathrm{mini} \gets \mathrm{Replanner}(\tau_i,\tau_{i+1},\boldsymbol{Z});$\;
   		\If{$\tau_\mathrm{mini}$}{
   			$\tau_\mathrm{new} \gets \tau_\mathrm{new} \cup \tau_\mathrm{mini}$\;
   		}
   		\Else
   		{
   		\Return$\varnothing$\; 
   		}
   		}
   		
   		
   	}
   	\Return$\tau_\mathrm{new}$\;
\caption{Replanning($\tau, \boldsymbol{Z}$)}
\label{algo:NPG}
"
579,1806.05722,"
%\caption{Least-squares for Markov parameters}\label{altopt}
%\begin{algorithmic}[1]
%\Procedure{MarkovLS}{}
%\item \emph{Input}: $(\ub_t,\y_t)_{t=1}^N$.
%{\For{$i\in \{1,2,\dots,K\}$}
%\State {
%$\Db_i\gets \text{OptOutput}(\A_{i-1},\B_{i-1},S_{2i-1})$}
%\State{
%$(\A_{i},\B_{i})\gets \text{OptStates}(\db_{i},S_{2i})$
%}
%\EndFor}
%\item \emph{Return}: $\hat{\Gb}$
%\EndProcedure
%\end{algorithmic}
%"
580,1806.05722," [!t]\caption{Ho-Kalman Algorithm to find a State-Space Realization.}\label{algo 1}
\begin{algorithmic}[1]
\Procedure{Ho-Kalman Minimum Realization}{}
\item {\bf{Inputs:}} \rrr{Length} $\TK$, Markov parameter matrix estimate $\cbe{}$, system order $n$,

\hspace{14pt}Hankel shape $(\TK_1,\TK_2+1)$ with $\TK_1+\TK_2+1=\TK$.
\item {\bf{Outputs:}} State-space realization $\hat{\A},\hat{\B},\hat{\Cb}$.
%\State 
\State Form the Hankel matrix $\hat{\Hb}\in\R^{m \TK_1\times p(\TK_2+1)}$ from $\cbe{}$.
\State $\Hbhm\in\R^{m \TK_1\times p\TK_2}\gets \text{first-$p\TK_2$-columns-of}(\hat{\Hb})$.
\State $\Hbhl\in \R^{m \TK_1\times p\TK_2}\gets \text{rank-$n$-approximation-of}(\Hbhm)$.
\State $\Ub,\bSi,\V=\text{SVD}(\Hbhl)$.
\State $\Obh\in \R^{m \TK_1\times n}\gets \Ub\bSi^{1/2}$.
\State $\Qbh\in \R^{n\times p \TK_2}\gets \bSi^{1/2}\V^*$.
\State $\Ch\gets \text{first-$m$-rows-of}(\Obh)$.
\State $\Bh\gets \text{first-$p$-columns-of}(\Qbh)$.
\State $\Hbhp\in\R^{m \TK_1\times p\TK_2}\gets \text{last-$p\TK_2$-columns-of}(\hat{\Hb})$.
\State $\Ah\gets \Obh^{\dagger}\Hbhp\Qbh^{\dagger}$.\\
\Return $\hat{\A}\in\R^{n\times n},\hat{\B}\in\R^{n\times p},\hat{\Cb} \in\R^{m\times n}$.
\EndProcedure
\end{algorithmic}
"
581,1806.05703,"[H]
 \Fn{\mgann{$l$}}{
 Train model $\mathcal{M}_l$ for $k$ batches, where each consists of:
 \begin{enumerate}
    \item Feed examples through the network in feed-forward mode;
    \item Compute error $E$ between network output and target;
    \item Use the classical backpropagation algorithm to compute the gradient of top-level parameter $\Theta_j$ w.r.t. this error;
    \item Use the appropriate $\text{Res}$ operations to compute the gradient of $E$ w.r.t. the parameters in $\mathcal{M}_l$, as described in Equation \ref{eqn:msann_res_equation}.
 \end{enumerate}
 \If{\emph{max\_depth} has not been reached}{
  \For{$1 \leq i \leq \gamma$}{
    \mgann{$l+1$};\\
    Train model $\mathcal{M}_l$ for $k$ batches, as above
  }
 }\;}
 \label{alg:mgannalg}
 \caption{One `cycle' of the MsANN procedure.}
"
582,1806.05618,"[tb]
	\caption{SVRG}
	\label{alg:svrg}
	\begin{algorithmic}
            \STATE {\bfseries Input:} a dataset $\mathcal{D}_N$, number of epochs $S$, epoch size $m$, step size $\alpha$, initial parameter $\vtheta_{m}^0:=\wt{\vtheta}^0$
		\FOR{$s=0$ {\bfseries to} $S-1$}
        \STATE $\vtheta_0^{s+1} := \wt{\vtheta}^{s} = \vtheta_{m}^s$
        \STATE $\wt{\mu} = \nabla f(\wt{\vtheta}^s)$
		\FOR{$t=0$ {\bfseries to} $m-1$}
%		\STATE Pick $i$ uniformly at random from $[1,N]$
        \STATE $x \sim \mathcal{U}\left(\mathcal{D}_N\right)$
		\STATE $v^{s+1}_t = 
%			\nabla f(\wt{\vtheta}^s) + 
			\wt{\mu} + 
			\nabla z(x|\vtheta_t^{s+1}) -
			\nabla z(x|\wt{\vtheta}^{s})
		$
        \STATE $\vtheta_{t+1}^{s+1} = \vtheta_t^{s+1} + \alpha v^{s+1}_t$
		\ENDFOR
		\ENDFOR
        \STATE \underline{Concave case:} \textbf{return} $\vtheta_{m}^S$
        \STATE \underline{Non-Concave case:} \textbf{return} $\vtheta_t^{s+1}$ with $(s,t)$ picked uniformly at random from $\{[0,S-1]\times[0,m-1]\}$
	\end{algorithmic}
"
583,1806.05618,"[tb]
	\caption{SVRPG}
	\label{alg:svrpg}
	\begin{algorithmic}
		\STATE {\bfseries Input:} number of epochs $S$, epoch size $m$, step size $\alpha$, batch size $N$, mini-batch size $B$, gradient estimator $g$, initial parameter $\vtheta_{m}^0 \coloneqq \wt{\vtheta}^0 \coloneqq \vtheta_0$
		\FOR{$s=0$ {\bfseries to} $S-1$}
		\STATE $\vtheta_0^{s+1} := \wt{\vtheta}^{s} = \vtheta_{m}^s$
		\STATE Sample $N$ trajectories $\{\tau_j\}$ from $p(\cdot\vert\wt{\vtheta}^{s})$
		\STATE $ \wt{\mu} = \gradApp{\wt{\vtheta}^{s}}{N}$ (see Eq.~\eqref{E:policygradient.estimate})% = \frac{1}{N}\sum_{j=0}^{N-1}g(\tau_j | \wt{\vtheta}^s)$
		\FOR{$t=0$ {\bfseries to} $m-1$}
		\STATE Sample $B$ trajectories $\{\tau_i\}$ from $p(\cdot\vert\vtheta_t^{s+1})$
		%        \STATE $c^{s+1}_t = \frac{1}{B} \sum\limits_{i=0}^{B-1} \left( g(\tau_i|\vtheta_t^{s+1}) - \omega(\tau_i|\vtheta^{s+1}_t, \wt{\vtheta}^s) g(\tau_i| \wt{\vtheta}^s) \right)$
		\STATE $c^{s+1}_t = \frac{1}{B} \sum\limits_{i=0}^{B-1}
		\begin{aligned}[t]
		\Big( & g(\tau_i|\vtheta_t^{s+1})\\ 
		& \quad{} - \omega(\tau_i|\vtheta^{s+1}_t, \wt{\vtheta}^s) g(\tau_i| \wt{\vtheta}^s) \Big)
		\end{aligned}$
		%        \STATE $\begin{aligned}[t]
		%                &v^{s+1}_t = \wt{\mu}\\ 
		%                & +\frac{1}{B} \sum\limits_{i=0}^{B-1} \left( g(\tau_i|\vtheta_t^{s+1}) - \omega(\tau_i|\vtheta^{s+1}_t, \wt{\vtheta}^s) g(\tau_i| \wt{\vtheta}^s) \right)
		%                \end{aligned}$
		\STATE $v^{s+1}_t = \wt{\mu} + c^{s+1}_t$ % \gradApp{\wt{\vtheta}^{s}}{N} + c^{s}_t$
		%%		\begin{align*}
		%%		\blacktriangledown J(\vtheta_t^{s+1}) = 
		%%		&\gradApp{\wt{\vtheta}^s}{N} \\
		%%		&+\frac{1}{B}\sum_{i=0}^{B-1}\left[ 
		%%		\score{}{\tau_i\vert\vtheta_t^{s+1}}\Reward(\tau_i)\right. \\
		%%		&\left. - \omega(\tau_i)\score{}{\tau_i \vert \wt{\vtheta}^{s}}\Reward(\tau_i)\right]
		%%		\end{align*}
		\STATE $\vtheta_{t+1}^{s+1} = \vtheta_t^{s+1} + \alpha v^{s+1}_t$
		\ENDFOR
		\ENDFOR
		\STATE {\bfseries return} $\vtheta_A\coloneqq\vtheta_t^{s+1}$ with $(s,t)$ picked uniformly at random from $\{[0,S-1]\times[0,m-1]\}$
	\end{algorithmic}
"
584,1806.05618,"[h]
	\begin{algorithmic}
		\STATE \textbf{Input:} A gradient estimate $g_t$ and parameters $\beta_1$, $\beta_2$, $\epsilon$ and $\alpha$.
		\STATE $\kappa_t = \beta_1 \kappa_{t-1} + (1 - \beta_1) g_t$
		\STATE $\nu_t = \beta_2 \nu_{t-1} + (1 - \beta_2) g_t \circ g_t$ ($\circ$ is the Hadamard (component-wise) product)
		\STATE $\hat{\kappa}_t = \dfrac{\kappa_t}{1 - \beta^t_1}$
		\STATE $\hat{\nu}_t = \dfrac{\nu_t}{1 - \beta^t_2}$
		\STATE $\Delta(g_t) = \dfrac{\alpha}{\sqrt{\hat{\nu}_t} + \epsilon} \hat{\kappa}_t$
		\STATE \textbf{Return:} The increment $\Delta(g_t)$  of the parameters.
	\end{algorithmic}
	\caption{
		\label{A:adam}
		Adam}
"
585,1705.09992,"[t]
\caption{Linearize and Project (LAP)} \label{LAPalgorithm}
\begin{algorithmic}[1]
\STATE Given $\bfx_0$ and $\bfw_0$ 
\STATE Compute active and inactive sets, $\mathcal{A}$ and $\mathcal{I}$
\STATE Evaluate $\Phi(\bfx_0,\bfw_0)$, $\bfr_0$, $\bfJ_x^{(0)}$ and $\bfJ_w^{(0)}$
\FOR{$k = 1,2,3,\ldots$}
	\STATE Compute the step the inactive set with LAP using \eqref{eq:inactProbX} and \eqref{eq:inactDeltaW} 
    	\STATE Compute the step on the active set using projected gradient descent \eqref{eq:projGradDes}
   	 \STATE Combine the steps using \eqref{eq:projGN} and \eqref{eq:gamma}
      	 \STATE Perform the projected Armijo line search satisfying \eqref{eq:projArmijo}, update $\bfx_k, \bfw_k$
       	 \STATE Update active and inactive sets, $\mathcal{A}$ and $\mathcal{I}$
    	 \STATE Evaluate $\Phi(\bfx_k, \bfw_k)$,  $\bfr_k$, and $\bfJ_x^{(k)}$ and $\bfJ_w^{(k)}$
    	 \IF{ Stopping criteria satisfied}
        		\RETURN{$\bfx_k, \bfw_k$} 
    	\ENDIF    
\ENDFOR
\end{algorithmic}
"
586,1803.05589,"[t]
%\caption{Update for SAN}
%\label{alg:updateSAN}
%\hspace*{\algorithmicindent} \textbf{Require: }  Parameter $\veta_\text{PGM}$ \eqref{eq:pgm_prior}, Function $\veta_{\theta}(\cdot)$ \eqref{eq:conjexp1}
%
%\begin{algorithmic}[1]
%
%    \FOR{Iteration $t=1,2,3,\ldots$,}
%	\STATE Randomly pick a data point $\vy_n$
%	
%	\STATE Sample global parameters $\vmodlGM$ and $\vdtrmc := \crl{\vmodlNN, \vinfe}$ \label{algstep:sampleglob}
%	
%		$\vtheta^{*}_\text{PGM} \sim q(\vtheta_{PGM} | \vlambda_\text{PGM})$
%		
%		$\vdtrmc^{*} \sim \gauss( \vdtrmc | \vmu_{\dtrmc}, \vsigma^2_{\dtrmc} ) $
%   
%   
%	\STATE Sample local latent variable from the inference network (as in VAE)
%	
%		$\vx_n^{*} \sim q(\vx_n|\vy_n, \vphi^{*} )$
%    
%    
%	\STATE Update $q(\vmodlGM|\vlambda_\text{PGM})$ using eq.\ref{eq:svi_update} (perform VI in PGM part of model using samples $\vx_n^{*}$)
%	
%%		$\vlambda_\text{PGM} \leftarrow  (1-\beta) \vlambda_\text{PGM} + \beta N   \sqr{ \veta_\text{PGM} + \veta_{\theta}( \vx_n^{*} ) }$    
%
%
%	\STATE Compute gradient w.r.t. $\vdtrmc$ (as in VAE)
%	
%		$\vg \leftarrow  \nabla_{\dtrmc } \log p(\vy_n|\vx^{*}_n, \vtheta^{*}_\text{NN} ) + \log \frac{ p(\vy_i|\vtheta_\text{PGM})}{ q(\vx_n|\vy_n, \vphi_\text{NN}) } \bigg\rvert_{\vdtrmc = \vdtrmc^{*}  }$
%

%	\STATE Update gradient (additional terms of our method compared to VAE)
%
%		$\vg \leftarrow  \vg + \nabla_{\dtrmc } \sqr{-\log p(\vx^{*}_n | \vphi_\text{PGM}) + \log \mathcal{Z}(\vphi) } \bigg\rvert_{\vdtrmc = \vdtrmc^{*}  } $
%
%	\STATE Perform adaptive-gradient update \eqref{eq:updateVAN1}-\eqref{eq:updateVAN2}

%		$\vsigma_\dtrmc^{-2} \leftarrow \vsigma_\dtrmc^{-2} + \beta \vg \odot \vg$
%
%		$\vmu_\dtrmc \leftarrow \vmu_\dtrmc - \beta \vsigma_\dtrmc^{2} \odot \vg $
%
%    \ENDFOR
%\end{algorithmic}
%"
587,1803.05589," 
%
%\caption{Update for SAN}
%\label{alg:updateSAN}
%\hspace*{\algorithmicindent} \textbf{Require: }  Parameter $\veta_\text{PGM}$ (eq.\ref{eq:pgm_prior}), Function $\veta_{\theta}(\cdot)$ (eq.\ref{eq:conjexp1}) 
%
%\begin{algorithmic}[1]
%
%    \FOR{Iteration $t=1,2,3,\ldots$,}
%	\STATE Randomly pick a data point $n$
%	
%   \STATE Sample $\vtheta_\text{NN}$, and $\vphi$
%   
%   $\vtheta^{*}_\text{NN} \sim \gauss( \vtheta_{\theta_\text{NN}} | \vmu_{\theta_\text{NN}}, \vsigma^2_{\theta_\text{NN}} )$
%
%   $\vphi^{*} \sim \gauss( \vphi | \vmu_{\phi}, \vsigma^2_{\phi} ) $
%   
%   \STATE Sample $\vtheta_\text{PGM}$
%   
%   $\vtheta^{*}_\text{PGM} \sim q(\vtheta_{PGM} | \vlambda_\text{PGM})$
%   
%   \STATE Sample from the inference network
%    
%    $\vx_n^{*} \sim q(\vx_n|\vy_n, \vphi^{*} )$
%    
% \STATE Update $q(\vtheta_\text{PGM}|\vlambda_\text{PGM})$ using eq.\ref{eq:svi_update}
%    
%$\vlambda_\text{PGM} \leftarrow  (1-\beta) \vlambda_\text{PGM} + \beta N   \sqr{ \veta_\text{PGM} + \veta_{\theta}( \vx_n^{*} ) }$    
%
%\STATE Compute gradient w.r.t. $\vtheta_\text{NN}$ using the same method as in VAE
%
%$\vg_{\theta_\text{NN}} \leftarrow  \nabla_{\theta_\text{NN}} \log p(\vy_i|\vx_i^{*}, \vtheta_\text{NN} ) \bigg\rvert_{\vtheta_\text{NN}= \vtheta^{*}_\text{NN} }$
%
%\STATE Compute gradient w.r.t. $\vphi$ using the same method as in VAE
%
%$\vg_{\phi} \leftarrow  \nabla_{\phi } \log p(\vy_n|\vx^{*}_n, \vtheta^{*}_\text{NN} ) + \log \frac{ p(\vy_i|\vtheta_\text{PGM})}{ q(\vx_n|\vy_n, \vphi_\text{NN}) } \bigg\rvert_{\vphi= \vphi^{*}  }  $
%
%\STATE Update gradient w.r.t. $\vphi$ due to additional terms in SIN
%
%$\vg_{\phi} \leftarrow  \nabla_{\phi } \sqr{-\log p(\vx^{*}_n | \vphi_\text{PGM}) + \log \mathcal{Z}(\vphi) } \bigg\rvert_{\vphi= \vphi^{*}  } $
%
%\STATE Perform adaptive-gradient update
%
%$\vg \leftarrow \sqr{  \vg_{ \theta_\text{NN} }, \vg_{\phi} }$,
%$\vmu \leftarrow \sqr{  \vmu_{ \theta_\text{NN} }, \vmu_{\phi} }$,
%$\vsigma^{-2} \leftarrow \sqr{  \vsigma^{-2}_{ \theta_\text{NN} }, \vsigma^{-2}_{\phi} }$
%
%$\vsigma^{-2} \leftarrow \vsigma^{-2} + \beta \vg \odot \vg$
%
%$\vmu \leftarrow \vmu + \beta \vsigma^{2} \odot \vg $
%
%    \ENDFOR
%\end{algorithmic}
%"
588,1803.05589,"[t]
   \caption{Structured, Amortized, and Natural-gradient (SAN) Variational Inference}
\label{alg:updateSAN}
   %
   \begin{algorithmic}[1]
   \REQUIRE Data $\vy$, Step-sizes $\beta_1,\beta_2,\beta_3$
   \STATE Initialize $\vlamGM, \vmodlNN, \vinfe$.
   \REPEAT
   \STATE Compute $q(\vlat|\vy,\vinfe)$ for SIN shown in \eqref{eq:vardist} either by using an exact expression or using VMP.
   \STATE Sample $\vlat^* \sim q(\vlat|\vy,\vinfe)$, and compute  $\nabla_\infe \Z$ and $\nabla_\infe \vlat^*$.
   %\STATE Gradient update for $\vinfeGM \leftarrow \vinfeGM + \beta_2 \{ [ \nabla_{x^*} \log \rnd{ p(\vlat^*|\vmodlGM^*) / q(\vlat^*|\vinfeGM^*) } ] \nabla_{\infeGM}\vx^*  + \nabla_{\infeGM} \Z \}$
   \STATE Update $\vlamGM$ using the natural-gradient step given in \eqref{eq:lamGMupdate}.
   %\STATE Update $\vinfeGM$ using the gradient given in \eqref{eq:grad3}.
   \STATE Update $\vmodlNN$ and $\vinfe$ using the gradients given in \eqref{eq:grad1}-\eqref{eq:grad2} with $\vmodlGM\sim q(\vmodlGM|\vlamGM)$.
   %\STATE Gradient update for $\vmodlNN \leftarrow \vmodlNN + \beta_3[$ {\small{\tt vae\_decoder\_grad}} $(\vy_n,\vx_n^*,\vmodlNN)] .$
   %\STATE Gradient update for $\vinfeNN \leftarrow \vinfeNN + \beta_4[$ {\small{\tt vae\_encoder\_grad}} $(\vy_n,\vx_n^*,\vmodlNN, \nabla_{\infeNN}\vx^*) + \nabla_{\infeNN} \Z ] .$ 
   \UNTIL{Convergence}
   \end{algorithmic}
"
589,1806.05437,"
% \SetKwData{ServiceDS}{Service_{db}}
% \SetKwData{SR}{sr}
% \SetKwData{TOPN}{N}

% \SetKwFunction{SelectTopNCategories}{selectTopNCategories}
% \SetKwFunction{ComputeImbalanceRate}{computeImbalanceRate}
% \SetKwFunction{CategoriesCounts}{categoriesCounts}
% \SetKwFunction{RankingCounts}{rankingCounts}
% \SetKwFunction{TotalCategories}{totalCategories}
% \SetKwFunction{DescriptionLengthCounts}{descriptionLengthCounts}
% \SetKwFunction{CategoriesFilter}{categoriesFilter}
% \SetKwFunction{DescriptionLengthFilter}{descriptionLengthFilter}
% \SetKwFunction{RandomSplitDataset}{randomSplitDataset}
% \SetKwFunction{ComputeKL}{computeKL}
% \SetKwFunction{CountRankCategories}{countRankCategories}


% \SetKwInOut{Input}{Input}
% \SetKwInOut{Output}{Output}

% \Input{$Service_{ds}$ - Service dataset \\
% \textit{rate} - Splitting rate \\
% \textit{N} - Minimum number of categories\\
% \textit{T} - Threshold of imbalance rate \\
% \textit{M} - Number of random selections}

% \Output{\textit{trainS} - Training Set \\ \textit{testS} - Test Set}
 
% \Begin{
% \BlankLine
% \tcp{Step 1: Category filter to reduce imbalance rate}
% \tcc{Counting and ranking categories on datasets}
% % \textit{cc} $\leftarrow$ \CategoriesCounts{ServiceDS}\;
% % \textit{rc} $\leftarrow$ \RankingCounts{$cc$}\;
% $rank_c$, $n$ $\leftarrow$ \CountRankCategories{$Service_{ds}$}\;

% \tcc{Compute imbalance rate $ir$ of all categories}
% $ir$ $\leftarrow$ \ComputeImbalanceRate{$rank_c$}\;

% \tcc{Select top $n-1$ categories and compute imbalance rate $ir_{n-1}$}
% \While{$n \ge N$ {\bf and} $|ir - ir_{n-1}| \ge T$}{

%     $n$ $\leftarrow$ $n - 1$\;
%     $top_{n-1}$ $\leftarrow$ \SelectTopNCategories{$rank_c$, $n$}\;
%     $ir_{n-1}$ $\leftarrow$ \ComputeImbalanceRate{$top_{n-1}$}\;

% }  

% $Service_{cf}$ $\leftarrow$ \CategoriesFilter{$Service_{ds}$, $top_{n-1}$}
% \BlankLine

% \tcp{Step 2: Description filter with 90\% length confidence level}

% $dlc$ $\leftarrow$ \DescriptionLengthCounts{$Service_{cf}$}\;
% $Service_{dl}$ $\leftarrow$ \DescriptionLengthFilter{$Service_{cf}$, $dlc$, 0.9}\;
% \BlankLine

% \tcp{Step 3: Minimum KL Selection}
% \tcc{Initialize training set and test set}
% $trainS$, $testS$ $\leftarrow$ \RandomSplitDataset{$Service_{dl}$, $rate$, $seed = 1$}\;
% $kls$ $\leftarrow$ \ComputeKL{$trainS$, $testS$}\;
% \tcc{Find training set and test set with minimum KL}
% \For{$i\leftarrow 2$ \KwTo $M$}{
%     $train$, $test$ $\leftarrow$ \RandomSplitDataset{$Service_{dl}$, $rate$, $seed = i$}\;
%     $kl$ $\leftarrow$ \ComputeKL{$train$, $test$}\;
    
%     \If{$kl \le kls$}{
%         $kls$ $\leftarrow$ $kl$\;
%         $trainS$, $testS$ $\leftarrow$ $train$, $test$\;
%     }
% }

% \Return{$trainS$, $testS$}\;

% }

% \caption{Service Splitting Algorithm}
% \label{splitting alogrithm}
% "
590,1802.04307,"[t]
       \caption{IPOT($\pmb{\mu},\pmb{\nu},\bm{C}$)}
	\label{algo1}       
	 \begin{algorithmic}
          \STATE \textbf{Input:} Probabilities $\{\pmb{\mu},\pmb{\nu}\}$ on support points $\{x_i\}_{i=1}^m$, $\{y_j\}_{j=1}^n$, cost matrix $\bm{C}=[\|x_i-y_j\|]$
	\STATE $\bm{b}\leftarrow \frac{1}{m}\pmb{1}_m$
	\STATE $G_{ij}\leftarrow e^{-\frac{C_{ij}}{\beta}}$
	\STATE $\bm{\Gamma}^{(1)} \leftarrow  \pmb{11}^T$
	\FOR {$t=1,2,3,...$}
	\STATE $\bm{Q} \leftarrow  \bm{G}\odot \bm{\Gamma}^{(t)}$
	\STATE \textbf{for} {$l=1,2,3,...,L$} \textbf{do}$\quad$ // Usually set $L=1$
	\STATE \quad $\bm{a}\leftarrow \frac{\pmb{\mu}}{\bm{Qb}}$, $\bm{b}\leftarrow \frac{\pmb{\nu}}{\bm{Q}^T \bm{a}}$
	\STATE \textbf{end for}
	\STATE $\bm{\Gamma}^{(t+1)} \leftarrow  \text{diag}(\bm{a}) \bm{Q} \text{diag}(\bm{b})$
	\ENDFOR
        \end{algorithmic}
      "
591,1802.04307,"[!t]
\label{alg:barycenter}
\caption{IPOT-WB($\{\bm{p}_k\}$)}
\begin{algorithmic}[1]\label{algo_proximal}
\STATE \textbf{Input:} The probability vector set $\{\bm{p}_k\}$ on grid $\{y_i\}_{i=1}^n$
\STATE $\bm{b}_k\leftarrow \frac{1}{n}\textbf{1}_n,\forall k=1,2,...,K$
\STATE $C_{ij}\leftarrow c(y_i,y_j):=||y_i-y_j||^2_2$
\STATE $G_{ij}\leftarrow e^{-\frac{C_{ij}}{\beta}}$
\STATE $\bm{\Gamma}_k \leftarrow  \mathbf{11}^T$
\FOR {$t=1,2,3,...$}
\STATE $\bm{H}_k \leftarrow  \bm{G} \odot \bm{\Gamma}_k,\forall k=1,2,...,K$
\FOR {$l=1,2,3,...,L$}
\STATE $\bm{a}_k\leftarrow \frac{\bm{q}}{\bm{H}_k \bm{b}_k},\forall k=1,2,...,K$, 
\STATE $\bm{b}_k\leftarrow \frac{\bm{p}_k}{\bm{H}_k^T \bm{a}_k},\forall k=1,2,...,K$
\STATE $\bm{q} \leftarrow  \prod_{k=1}^K (\bm{a}_k \odot (\bm{H}_k \bm{b}_k))^{\lambda_k}$
\ENDFOR
\STATE $\bm{\Gamma}_k \leftarrow  \text{diag}(\bm{a}_k) \bm{H}_k \text{diag}(\bm{b}_k),\forall k=1,2,...,K$
\ENDFOR
\STATE  \textbf{Return} $\bm{q}$
\end{algorithmic}
"
592,1802.04307,"[H]
       \caption{Learning generative networks}
	\label{algo_gan}       
	 \begin{algorithmic}
          \STATE \textbf{Input:} real data $\{x_i\}$, initialized generator $g_\theta$
	\WHILE{not converged}
\STATE Sample a batch of real data $\{x_i\}_{i=1}^n$
\STATE Sample a batch of noise data $\{z_j\}_{i=1}^n \sim q$
\STATE $C_{ij}:=c(x_i,g_{\theta}(z_j)):=||x_i-g_{\theta}(z_j)||^2_2$
\STATE $\bm{\Gamma}$ = IPOT($\frac{1}{n}\bm{1}_n,\frac{1}{n}\bm{1}_n,\bm{C}$)
\STATE Update $\theta$ with $\langle \bm{\Gamma},[2(x_i-g_{\theta}(z_j)){\frac {\partial g_{\theta}(z_j)}{\partial \theta}}]\rangle$
\ENDWHILE       
 \end{algorithmic}
      "
593,1802.04307,"[H]
%       \caption{Computing Wasserstein barycenter }
% 	\label{algo_wb}       
% 	 \begin{algorithmic}
%          \STATE \textbf{Input:} The probability vector set $\{\bm{p}_k\}$ on grid $\{y_i\}_{i=1}^n$
% \STATE $\bm{b}_k\leftarrow \frac{1}{n}\textbf{1}_n,\forall k=1,2,...,K$
% \STATE $C_{ij}\leftarrow c(y_i,y_j):=||y_i-y_j||^2_2$
% \STATE $G_{ij}\leftarrow e^{-\frac{C_{ij}}{\beta}}$
% \STATE $\bm{\Gamma}_k \leftarrow  \mathbf{11}^T$
% \FOR {$t=1,2,3,...$}
% \STATE $\bm{H}_k \leftarrow  \bm{G} \odot \bm{\Gamma}_k,\forall k=1,2,...,K$
% \FOR {$l=1,2,3,...,L$}
% \STATE $\bm{a}_k\leftarrow \frac{\bm{q}}{\bm{H}_k \bm{b}_k},\forall k=1,2,...,K$, 
% \STATE $\bm{b}_k\leftarrow \frac{\bm{p}_k}{\bm{H}_k^T \bm{a}_k},\forall k=1,2,...,K$
% \STATE $\bm{q} \leftarrow  \prod_{k=1}^K (\bm{a}_k \odot (\bm{H}_k \bm{b}_k))^{\lambda_k}$
% \ENDFOR
% \STATE $\bm{\Gamma}_k \leftarrow  \text{diag}(\bm{a}_k) \bm{H}_k \text{diag}(\bm{b}_k),\forall k=1,2,...,K$
% \ENDFOR
% \STATE  \textbf{Return} $\bm{q}$     
%  \end{algorithmic}
%       "
594,1806.01811,"
  \caption{Gradient Descent with Line Search Method}
  \begin{algorithmic}[1]
    \Function {line-search}{$x, b_0,\nabla F(x)$}
     \State $x_{new}\gets x-\frac{1}{b_0} \nabla F(x)$
      \While{$F(x_{new})> F(x)-\frac{b_0}{2} \| \nabla F(x)\|^2$}
        \State $b_0\gets 2b_0$
         \State $x_{new}\gets x-\frac{1}{b_0} \nabla F(x)$
      \EndWhile
      \State \textbf{return} $x_{new}$
    \EndFunction 
  \end{algorithmic}
"
595,1806.01811,"[H]
%  \caption{Gradient Descent with Line Search Method}
%\begin{algorithmic}
%  \STATE {\bfseries Input:} Tolerance $\varepsilon > 0$.  Initialize $x_0 \in \mathbb{R}^d, b_{0}>0,  j \leftarrow 0, beta=0.5$ \\
%    \REPEAT
%  \STATE{$j  \leftarrow j+1$}
%    \STATE  $\text{GD-Constant: } x_{j} \leftarrow x_{j-1} - \frac{1}{b_0} v_{j}  $
%    \UNTIL{$\| \nabla F(x_j) \|^2 \leq \varepsilon$}  
%\end{algorithmic}
%  "
596,1806.01811,"[H]
  \caption{AdaGrad-Norm with momentum in PyTorch}
\begin{algorithmic}[1]
  \State {\bfseries Input:}   Initialize $x_0 \in \mathbb{R}^d, b_{0}>0, v_0\leftarrow 0, j \leftarrow 0, \beta \leftarrow 0.9$,  and the total iterations $N$.
    \For{\texttt{ $j = 0,1, \ldots, N$ }}
    \State Generate  $\xi_{j}$ and $G_{j} = G(x_{j}, \xi_{j})$
    \State $v_{j+1} \leftarrow \beta v_{j} + (1-\beta)G_j$ 
 	 \State   $ x_{j+1} \leftarrow x_{j} - \frac{ v_{j+1} }{b_{j+1}}$  with $b_{j+1}^2 \leftarrow  b_{j}^2 +  \|G_{j} \|^2$ 
       \EndFor
\end{algorithmic}
  "
597,1806.01811,"
  \caption{Gradient Descent with Line Search Method}
  \begin{algorithmic}[1]
    \Function {line-search}{$x, b_0,\nabla F(x)$}
     \State $x_{new}\gets x-\frac{1}{b_0} \nabla F(x)$
      \While{$F(x_{new})> F(x)-\frac{b_0}{2} \| \nabla F(x)\|^2$}
        \State $b_0\gets 2b_0$
         \State $x_{new}\gets x-\frac{1}{b_0} \nabla F(x)$
      \EndWhile
      \State \textbf{return} $x_{new}$
    \EndFunction 
  \end{algorithmic}
"
598,1806.01811,"[H]
%  \caption{Gradient Descent with Line Search Method}
%\begin{algorithmic}
%  \STATE {\bfseries Input:} Tolerance $\varepsilon > 0$.  Initialize $x_0 \in \mathbb{R}^d, b_{0}>0,  j \leftarrow 0, beta=0.5$ \\
%    \REPEAT
%  \STATE{$j  \leftarrow j+1$}
%    \STATE  $\text{GD-Constant: } x_{j} \leftarrow x_{j-1} - \frac{1}{b_0} v_{j}  $
%    \UNTIL{$\| \nabla F(x_j) \|^2 \leq \varepsilon$}  
%\end{algorithmic}
%  "
599,1806.01811,"[H]
  \caption{AdaGrad-Norm with momentum in PyTorch}
\begin{algorithmic}[1]
  \State {\bfseries Input:}   Initialize $x_0 \in \mathbb{R}^d, b_{0}>0, v_0\leftarrow 0, j \leftarrow 0, \beta \leftarrow 0.9$,  and the total iterations $N$.
    \For{\texttt{ $j = 0,1, \ldots, N$ }}
    \State Generate  $\xi_{j}$ and $G_{j} = G(x_{j}, \xi_{j})$
    \State $v_{j+1} \leftarrow \beta v_{j} + (1-\beta)G_j$ 
 	 \State   $ x_{j+1} \leftarrow x_{j} - \frac{ v_{j+1} }{b_{j+1}}$  with $b_{j+1}^2 \leftarrow  b_{j}^2 +  \|G_{j} \|^2$ 
       \EndFor
\end{algorithmic}
  "
600,1806.01811,"[H]
  \caption{AdaGrad-Norm} \label{alg:Adagrad}
\begin{algorithmic}[1]
  \State {\bfseries Input:} 
   Initialize $x_0 \in \mathbb{R}^d, b_{0}>0, \eta>0$
    \For{\texttt{ $j = 1,2, \ldots$ }}
     \State Generate  $\xi_{j-1}$ and $G_{j-1} = G(x_{j-1}, \xi_{j-1})$
      \State${b}_{j}^2 \leftarrow  {b}_{j-1} ^2+ { \| G_{j-1} \|^2}$  
       \State $x_{j} \leftarrow x_{j-1} -  \frac{\eta}{{b}_{j}}G_{j-1} $
       \EndFor
\end{algorithmic}
  "
601,1806.01811,"[H]
  \caption{AdaGrad-Norm} \label{alg:Adagrad}
\begin{algorithmic}[1]
  \State {\bfseries Input:} 
   Initialize $x_0 \in \mathbb{R}^d, b_{0}>0, \eta>0$
    \For{\texttt{ $j = 1,2, \ldots$ }}
     \State Generate  $\xi_{j-1}$ and $G_{j-1} = G(x_{j-1}, \xi_{j-1})$
      \State${b}_{j}^2 \leftarrow  {b}_{j-1} ^2+ { \| G_{j-1} \|^2}$  
       \State $x_{j} \leftarrow x_{j-1} -  \frac{\eta}{{b}_{j}}G_{j-1} $
       \EndFor
\end{algorithmic}
  "
602,1806.05337,"[hbtp]
    \caption{Agglomeration algorithm.}  
    \label{algo:agglom}
    \small
    \textbf{ACD}(Example x, model, hyperparameter k, function CD(x, blob; model))    
    \begin{algorithmic}
        \CommentInline{initialize}
        \State tree = Tree() \Comment{tree to output}
        \State scoresQueue = PriorityQueue() \Comment{scores, sorted by importance}
        \For{feature in x} 
            \State scoresQueue.push(feature, priority=CD(x, feature; model))
        \EndFor
        
        \State 
        \CommentInline{iteratively build up tree}
        \While{scoresQueue is not empty}
            \State selectedGroups = scoresQueue.popTopKPercentile(k) \Comment{pop off top k elements}
            \State tree.add(selectedGroups) \Comment{Add top k elements to the tree}
            \State 
            \CommentInline{generate new groups of features based on current groups and add them to the queue}
            
            \For{selectedGroup in selectedGroups} 
                \State candidateGroups = getCandidateGroups(selectedGroup)
                \For{candidateGroup \:in candidateGroups} 
                    \State scoresQueue.add(candidateGroup, priority=CD(x, candidateGroup;model)-CD(x,selectedGroup; model))
                \EndFor
            \EndFor
            
        \EndWhile
        \item \textbf{return} tree
    \end{algorithmic}
"
603,1803.02400,"[t]
	\caption{Pseudo-Task MAML (PT-MAML)}
	\label{alg:maml}
	\begin{algorithmic}[1]
		% Given datasets
		% Use relevant function
		% Meta learning setup
		\REQUIRE Training Datapoints  $\mathcal{D}=\{\inp^{(j)}, \target^{(j)}\}$
%		$ p(\task)$: distribution over tasks
%		\REQUIRE $p(\task)$: distribution over tasks
		\REQUIRE $\alpha$, $\beta$: step size hyperparameters
		\REQUIRE $K$: support set size hyperparameter
		\STATE Construct a task $\task_j$ with training examples using a support set $\mathcal{S}^{(j)}_K$ and a test example $\mathcal{D}_j'=(\inp^{(j)}, \target^{(j)})$.
		\STATE Denote $ p(\task)$ as distribution over tasks
		\STATE Randomly initialize $\theta$
		\WHILE{not done}
		\STATE Sample batch of tasks $\task_i \sim p(\task)$
		\FORALL{$\task_i$}
		\STATE Evaluate $\nabla_\theta \lossi(\learner_\theta)$ using $\mathcal{S}^{(j)}_K$
		\STATE Compute adapted parameters with gradient descent: $\theta_i'=\theta-\alpha \nabla_\theta  \lossi(  \learner_\theta )$
%		\STATE Use the test example $\mathcal{D}_i'$ from $\task_i$ for the meta-update
		\ENDFOR
		 \STATE Update $\theta \leftarrow \theta - \beta \nabla_\theta \sum_{\task_i \sim p(\task)}  \lossi ( \learner_{\theta_i'})$ using each $\mathcal{D}_i'$ from $\task_i$ and $\lossi$ for the meta-update
		\ENDWHILE
		%\STATE while
	\end{algorithmic}
"
604,1806.05975,"[!h]
  \caption{Regularized HS-BNN Training}
  \label{alg:overall}
  \begin{algorithmic}[1]
    \STATE \textbf{Input} Model $p(\data, \theta)$, variational approximation  $q(\theta\mid\phi)$, number of iterations T.
    \STATE \textbf{Output}: Variational parameters $\phi$  
    \STATE Initialize variational parameters $\phi$.
    \FOR{\texttt{T iterations}}
    \STATE Update $\phi_c$, $\phi_\kappa$, $\phi_\gamma$, $\{\phi_{B_l}\}_l$, $\{\phi_{\taulayer}\}_l \leftarrow \text{ADAM}(\elbo(\phi))$.
    \FOR{\texttt{all hidden layers} $l$}
    \STATE Conditioned on $\phi_{B_l}$, $\phi_{\taulayer}$ update $\phi_{\lambdalayer}$, $\phi_{\lambdanode}$ using fixed point updates (Equation~\ref{eqn:fxp}).
    \ENDFOR
    \STATE Conditioned on $\phi_\kappa$ update $\phi_{\lambdakappa}$ via the corresponding fixed point update.
    \ENDFOR
\end{algorithmic}
"
605,1702.08420,"
	\caption{Importance Sampled Mixture of Experts (IS-MOE)}
	\label{alg:ISMOE}
	%	Generate $ J $ partitions from the posterior of Gaussian mixture model  with $ K $ clusters, $ P(Z|X)$.\\
	\For{$ j = 1 , \ldots , J $ in parallel}{
		Draw partition with $ K$ clusters  of data from $ P(Z|X)$\\
		Fit $ K $ independent GP models on the partitioned data.\\
		Predict new observations on each importance sample with \begin{equation*}P(f^{\ast}_{j} | Z_j, - ) = \sum_{k=1}^{K} P(f^{\ast}_{j} |  Z_j^{\ast}, - ) P(Z_j^{\ast}| -).\end{equation*}\\
		Obtain weights $ w_j = \prod_{k=1}^{K} P(Y_{k,j} | X_{k,j}, Z_j)$.
	} 
	Normalize weights, $ w_j := w_j/\sum_{j=1}^{J}w_j$.\\
	Average predictions using importance weights: $P(\bar{f}^{\ast} | - ) = \sum_{j=1}^{J} w_j	P(f^{\ast}_{j} | Z_j, - )$
"
606,1806.05228,"[t]
\caption{Algorithm for finding 3D shape correspondences}\label{alg:inference}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Reference shape $\groundtruth_\reference$ and target shape $\groundtruth_\target$}
\Output{Set of 3D point correspondences $\correspondences$}
\#Regression steps over latent code to find best reconstruction of $\groundtruth_\reference$ and $\groundtruth_\target$ \\
$\shapefeature_\reference\leftarrow\argmin_{\shapefeature} \lossrecon\left(\shapefeature; \groundtruth_\reference\right)$  \#detailed in equation~(\ref{eqn:atlas_loss})\\
$\shapefeature_\target\leftarrow\argmin_{\shapefeature} \lossrecon\left(\shapefeature; \groundtruth_\target\right)$ \#detailed in equation~(\ref{eqn:atlas_loss}) \\
$\correspondences\leftarrow\varnothing$ \\
\# Matching of $\surfacepoint_\reference\in\groundtruth_\reference$ to  $\surfacepoint_\target\in\groundtruth_\target$ \\
\ForEach{$\surfacepoint_\reference\in\groundtruth_\reference$}
{
    $\atlaspoint\leftarrow\argmin_{\atlaspoint^\prime\in\patchinput} |\mlp_{\parameters}\left(\atlaspoint^\prime; \shapefeature_\reference\right)-\surfacepoint_\reference|^2$ \\
    $\surfacepoint_\target\leftarrow\argmin_{\surfacepoint^\prime\in\groundtruth_\target} |\mlp_{\parameters}\left(\atlaspoint; \shapefeature_\target\right)-\surfacepoint^\prime|^2$ \\
    $\correspondences\leftarrow\correspondences\cup\left\{\left(\surfacepoint_\reference,\surfacepoint_\target\right)\right\}$
}
return $\correspondences$
"
607,1806.05134,"[H]
\caption{Computing $\cM_{d}(\alpha)$ for Angular Policy Gradient}
\label{alg:m_function}
\begin{algorithmic}[1]
\Input $d$, $\alpha$
\Output $\cM_d(\alpha)$
\State $M_0 \gets \Phi(\alpha)$
\State $M_1 \gets \alpha\Phi(\alpha) + \phi(\alpha)$
\If{$d > 1$}
    \For{$i = 2,\dots,d$}
        \State $M_i \gets \alpha M_{i-1} + d M_{i-2}$
    \EndFor
\EndIf
\State \Return $M_d$
\end{algorithmic}
"
608,1806.05117,"
%\caption{Pseudocode for the Sarsa($\lambda$) algorithm.}
%\begin{algorithmic}
%\FORALL{\emph{s}, \emph{a}}
%\STATE \emph{Q}(\emph{s}, \emph{a}) = 0
%\STATE \emph{e}(\emph{s}, \emph{a}) = 0
%\ENDFOR
%\REPEAT 
%\STATE Initialize \emph{s}, \emph{a}
%\REPEAT
%\STATE Take action \emph{a}, observe \emph{r}, \emph{s'}
%\STATE Choose \emph{a'} and \emph{s'} using policy derived from \emph{Q} 
%\STATE $\delta$ $\Leftarrow$ \emph{r} + $\gamma$\emph{Q}(\emph{s'}, \emph{a'}) - \emph{Q}(\emph{s}, \emph{a})
%\STATE \emph{e}(\emph{s}, \emph{a}) $\Leftarrow$ 1
%\FORALL{\emph{s}, \emph{a}}
%\STATE \emph{Q}(\emph{s}, \emph{a})  $\Leftarrow$ \emph{Q}(\emph{s}, \emph{a}) + $\alpha$$\delta$\emph{e}(\emph{s}, \emph{a})
%\STATE \emph{e}(\emph{s}, \emph{a}) $\Leftarrow$ $\gamma$$\lambda$\emph{e}(\emph{s}, \emph{a})
%\ENDFOR
%\STATE \emph{s} $\Leftarrow$ \emph{s'}; \emph{a} $\Leftarrow$ \emph{a'}
%\UNTIL{(steps of single episode have finished)}
%\UNTIL{(all episodes have finished)} 
%\end{algorithmic}
%\label{sarsaAlg}
%"
609,1806.05086,"
	\caption{Group capsule layer}
	\begin{algorithmic}\label{alg:routing}
	  \STATE{} \textbf{Input}: poses $\mathbf{P} = (\mathbf{p}_1, \ldots, \mathbf{p}_n) \in G^n$, activations $\mathbf{a} = (a_1, \ldots, a_n) \in \mathbb{R}^n$
		\STATE{} \textbf{Trainable parameters}: transformations $\mathbf{t}_{i,j}$
		\STATE{} \textbf{Output}: poses $\hat{\mathbf{P}} = (\hat{\mathbf{p}}_1, \ldots, \hat{\mathbf{p}}_m) \in G^m$, activations $\hat{\mathbf{a}} = (\hat{a}_1, \ldots, \hat{a}_m) \in \mathbb{R}^m$
		\STATE{} -------------------------------------------------------------------------------------------------------------------- %chktex 8
		\STATE{} $\mathbf{v}_{i,j} \leftarrow  \mathbf{p}_i \circ \mathbf{t}_{i,j}$ \hfill for all input capsules $i$ and output capsules $j$
		\STATE{} $\hat{\mathbf{p}}_j \leftarrow \mathcal{M}((\mathbf{v}_{1,j}, \ldots, \mathbf{v}_{n,j}),\mathbf{a})$ \hfill $\forall j$
		\FOR{$r$ iterations}
		\STATE{} $w_{i,j} \leftarrow \sigma(-\delta(\hat{\mathbf{p}}_j, \mathbf{v}_{i,j})) \cdot a_i$ \hfill $\forall  i , j$
		\STATE{} $\hat{\mathbf{p}}_j \leftarrow  \mathcal{M}((\mathbf{v}_{1,j}, \ldots, \mathbf{v}_{n,j}),\mathbf{w}_{:,j})$ \hspace{8.02cm} $\forall j $
		\ENDFOR%
		\STATE{} $\hat{a}_j \leftarrow \sigma(-\frac{1}{n}\sum_{i=1}^n \delta(\hat{\mathbf{p}}_j, \mathbf{v}_{i,j}))$ \hfill $\forall  j$
		\STATE{} Return $\hat{\mathbf{p}}_1, \ldots, \hat{\mathbf{p}}_m$,  $\hat{\mathbf{a}}$
	\end{algorithmic}
"
610,1806.05086,"[H]
	\caption{Group capsule layer}
	\begin{algorithmic}\label{alg:routing}
		\STATE{} \textbf{Input}: poses $\mathbf{P} = (\mathbf{p}_1, \ldots, \mathbf{p}_n) \in G^n$, activations $\mathbf{a} = (a_1, \ldots, a_n) \in \mathbb{R}^n$
		\STATE{} \textbf{Trainable parameters}: transformations $\mathbf{t}_{i,j}$
		\STATE{} \textbf{Output}: poses $\hat{\mathbf{P}} = (\hat{\mathbf{p}}_1, \ldots, \hat{\mathbf{p}}_m) \in G^m$, activations $\hat{\mathbf{a}} = (\hat{a}_1, \ldots, \hat{a}_m) \in \mathbb{R}^m$
		\STATE{} -------------------------------------------------------------------------------------------------------------------- %chktex 8
		\STATE{} $\mathbf{v}_{i,j} \leftarrow  \mathbf{p}_i \circ \mathbf{t}_{i,j}$ \hfill for all input capsules $i$ and output capsules $j$
		\STATE{} $\hat{\mathbf{p}}_j \leftarrow \mathcal{M}((\mathbf{v}_{1,j}, \ldots, \mathbf{v}_{n,j}),\mathbf{a})$ \hfill $\forall j$
		\FOR{$r$ iterations}
		\STATE{} $w_{i,j} \leftarrow \sigma(-\delta(\hat{\mathbf{p}}_j, \mathbf{v}_{i,j})) \cdot a_i$ \hfill $\forall  i , j$
		\STATE{} $\hat{\mathbf{p}}_j \leftarrow  \mathcal{M}((\mathbf{v}_{1,j}, \ldots, \mathbf{v}_{n,j}),\mathbf{w}_{:,j})$ \hspace{8.02cm} $\forall j $
		\ENDFOR%
		\STATE{} $\hat{a}_j \leftarrow \sigma(-\frac{1}{n}\sum_{i=1}^n \delta(\hat{\mathbf{p}}_j, \mathbf{v}_{i,j}))$ \hfill $\forall  j$
		\STATE{} Return $\hat{\mathbf{p}}_1, \ldots, \hat{\mathbf{p}}_m$,  $\hat{\mathbf{a}}$
	\end{algorithmic}
"
611,1806.05049,"[t]\small
  \caption{One pass of BCFW. Input: vectors $y\in\Y$, $\mu\in\Lambda$ and $\nu\in\mathbb R^d$ computed as in Prop.~\ref{prop:dual-of-decomposition-plus-trust-region}.
}\label{alg:BCFW}
\begin{algorithmic}[1]
  \STATE {\bf for each} $t\in T$ {\bf do} in a random order

  \STATE~~set $\lambda^t= c \cdot y^t_\star + \mu^t - \nu_{A_t}$ 

  \STATE~~call $t$-th oracle for $\lambda^t$: \ \ $z^t\leftarrow\argmin\limits_{z^t \in \SY_t} \langle z^t,[\lambda^t\; 1] \rangle$
\\ ~~ \big(i.e.\ let $x\leftarrow\argmin\limits_{x\in\SX_t}[f_t(x)+\langle\lambda^t,x\rangle]$ and $z^t=[x\;f_t(x)]$\big)
  \STATE~~interpolate $y(\gamma)^s \leftarrow \left\{ \begin{array}{ll} y^s, & s \neq t \\ (1-\gamma)y^t + \gamma z^t,& s = t \end{array} \right.$
  \STATE~~compute $\gamma\leftarrow \argmin_{\gamma\in[0,1]} f_{\mu,c}(y(\gamma))$: \\ ~~set 
$\gamma\leftarrow
\frac{\langle [\lambda^t\; 1] , z^t - y^t \rangle}
{c \norm{y^t_\star - z^t_\star}^2}
$
      and clip $\gamma$ to $[0,1]$ 
  \STATE~~set $\nu_i \leftarrow \nu_i + \frac{c}{|T_i|} (y(\gamma)^t_i - y^t_i)$ for $i \in A_t$ and $y^t \leftarrow y(\gamma)^t$ \!\!\!\!
  \STATE {\bf end for}
\end{algorithmic}
"
612,1607.02675,"[h!]
\caption{ADMM for solving \sdp{A+\lambda K}}
\label{alg:admm}
\begin{algorithmic}[1]
\REQUIRE Network $A$, node covariate matrix $Y$, tuning parameter $\lambda$, $\rho$.
\STATE Compute kernel matrix $K$ where $K(i,j)=f(\|Y_j-Y_j\|_2^2)$;
\WHILE{not converge}
\STATE $X^{(k+1)}=\Pi_{L}(\frac{1}{2}(Z^k-U^k+Y^k-V^k)+\frac{1}{\rho}(A+\lambda K))$;
\STATE $Z^{(k+1)} = \max(0,X^{k+1}+U^k)$, $Z^{(k+1)} = \min(Z^{(k+1)},1/\mmin)$; 
\STATE $Y^{(k+1)}=\Pi_{S^+}( X^{(k+1)}+V^k )$;
\STATE $U^{(k+1)}=U^{k}+X^{(k+1)}-Z^{(k+1)}$;
\STATE $V^{(k+1)}=V^{k}+X^{(k+1)}-Y^{(k+1)}$;
\ENDWHILE
\STATE Return $X^k$.
\end{algorithmic}
"
613,1804.09619," [!ht]
		%\ssmall
		\caption{\method for Detecting \& Alleviating Concept Drift}
		\label{Algorithm: Seek And Destroy}
		\begin{algorithmic} [1]
		\REQUIRE Tensor $\tensor{X}_{new}$ of size $I \times J \times K_{new}$, Factor matrices $\mathbf{A}_{old},\mathbf{B}_{old}, \mathbf{C}_{old}$ of size $I \times R$, $J \times R$ and $K_{old} \times R$ respectively, runningRank, mode.
		\ENSURE Factor matrices $\mathbf{A}_{updated}, \mathbf{B}_{updated}, \mathbf{C}_{updated}$ of size $I \times \rr$, $J \times \rr$ and $(K_{new}+K_{old}) \times \rr$, $\boldsymbol{\rho}$, $ \rr$.

		\STATE $batchRank \leftarrow getRankAutoten(\tensor{X}_{new}, runningRank)$
		\STATE $[\mathbf{A}, \mathbf{B}, \mathbf{C},\boldsymbol{\lambda}] = $ CP$\left( \tensor{X}_{new}, batchRank \right)$.
		\STATE $\mathbf{colA}, \mathbf{colB}, \mathbf{colC} \leftarrow$ Compute Column Normalization of $\mathbf{A}, \mathbf{B}, \mathbf{C}$.
		\STATE $\mathbf{normMatA}, \mathbf{normMatB}, \mathbf{normMatC} \leftarrow$ Absorb $\boldsymbol{\lambda}$ and Normalize $\mathbf{A}, \mathbf{B}, \mathbf{C}$.
		\STATE $rhoVal \leftarrow colA\ .*\ colB\ .*\ colC$
		\STATE $[\newcon, \conOverlap, overlapConceptOld] \leftarrow findConceptOverlap(\mathbf{A}_{old},\mathbf{normMatA})$
	    \IF{\newcon} 
	   \STATE  $\rr \leftarrow \rr + len(\newcon)$
	    \STATE $\mathbf{Aupdated} \leftarrow \begin{bmatrix}
	    \mathbf{A}_{old} \ \mathbf{normMatA}(:,\newcon)\\
	    \end{bmatrix}$ 
	    
	    \STATE  $\mathbf{Bupdated} \leftarrow \begin{bmatrix}
	    \mathbf{B}_{old} \ \mathbf{normMatB}(:,\newcon)\\
	    \end{bmatrix}$ 
	    
	    \STATE  $\mathbf{Cupdated} \leftarrow$  update $\mathbf{C}$  depending on the \newconcept, \\ \conceptoverlap, overlapConceptOld indices and $\rr$
	    \ELSE
	    \STATE $\mathbf{Aupdated} \leftarrow \mathbf{A}_{old}$
	    \STATE  $\mathbf{Bupdated} \leftarrow \mathbf{B}_{old}$ 
	    \STATE  $\mathbf{Cupdated} \leftarrow$  update $\mathbf{C}$ depending on the \conceptoverlap, overlapConceptOld indices and $\rr$
	    \ENDIF
	    \STATE Update $\rho$ depending on the \newconcept and \conceptoverlap indices
	    \IF{\newcon or $(len(\newcon) + len(\conOverlap) < \rr)$}
	    \STATE Concept Drift Detected
	    \ENDIF
		\end{algorithmic}
		\label{alg:method}
	"
614,1804.09619," [!ht]
		%\ssmall
		\caption{Find Concept Overlap}
		\label{Algorithm: Find concepts}
		\begin{algorithmic} [1]
			\REQUIRE Factor matrices $\mathbf{A}_{old},\mathbf{normMatA }$ of size $I \times R$, $I \times batchRank$ respectively.
			\ENSURE \newcon, \conOverlap, overlapConceptOld
			
			\STATE $THRESHOLD \leftarrow 0.6$
			\IF{$R == batchRank$}
			\STATE Generate all the permutations for [1:R]
			\STATE \ForEach{permutation}{Compute dot product of $\mathbf{A}_{old}\ and\ \mathbf{normMatA(:,permutation)}$}
			
			\ELSIF{$R > batchRank$}
			\STATE Generate all the permutations(1:R, batchRank)
			\STATE \ForEach{permutation}{Compute dot product of $\mathbf{A}_{old}(:,permutation)\ and\ \mathbf{normMatA}$}

			\ELSIF{$R < batchRank$}
			\STATE Generate all the permutations (1:batchRank, R)
			\STATE \ForEach{permutation}{Compute dot product of $\mathbf{A}_{old}\ and\ \mathbf{normMatA(:,permutation)}$}
			\ENDIF
			\STATE Select the best permutation based on the maximum sum.
			\STATE If dot product value of a column is less than threshold its a \newconcept 
			\STATE If dot product value of a column is more than threshold then its a \conceptoverlap. 
			\STATE Return column index's of \newconcept and \conceptoverlap  for both matrices
		\end{algorithmic}
		\label{alg:method}
	"
615,1804.00448,"
\SetKwProg{Fn}{function}{\string:}{}
\Fn(\tcc*[h]{train the network for one epoch}){\FRtrain{$S$, iterators}}{
\KwData{$S$: set of image sizes; \textbf{iterators}: list of data iterators, for each image size}

active $\leftarrow$ S \;
\While{active $\neq \emptyset$}{
  \For{$s \in active$}{
       \If{iterator[s].has\_next\_batch()}{
   	     	 mini\_batch $\leftarrow$ iterator[s].next\_batch() \;
	         train(mini\_batch) \;
       }
       \Else{
           active $\leftarrow$ active $\setminus$ s
       }
  }
}
}
\caption{Training algorithm for ``multiple sizes"", for one epoch.}
\label{alg:train}
"
616,1804.08951,"[!htb]
%	\KwRequire{A manipulator object, starting joint configuration $\textbf{\textit{q}}_0$, toleration $t$, starting lambda $\lambda$, maximum rejection steps $r_{max}$, and maximum iteration steps $i_{max}$} \;
%	\Fn{$\mathcal{K}^{-1}(\xi)$}{
%		$\lambda \leftarrow 0.1$ \;
%		$\textbf{\textit{e}}\leftarrow$ differential motion corresponding to $\xi$ and $\mathcal{K}(\textbf{\textit{q}}_0)$ \;
%		$\textbf{\textit{q}} \leftarrow q_i$ \;
%		$i \leftarrow 0$ \;
%		$r \leftarrow 0$ \;
%		\While{$i \leq i_{max}$}{
%			$i \leftarrow i+1$ \;
%	    	\If{$||\textbf{\textit{e}}|| \leq t$}
%	    	{\Return $\textbf{\textit{q}}$\;}
%	    	$ \textbf{\textit{J}} \leftarrow$ Jacobian of manipulator with joint angles of $\textbf{\textit{q}}$ \;
	    	%$ JtJ \leftarrow J^TJ$ \;
%	    	$ \textbf{\textit{dq}} \leftarrow (\textbf{\textit{J}}^{\top}\textbf{\textit{J}} + \lambda^2 \textbf{\textit{I}})^{-1}\textbf{\textit{J}}^{\top}\textbf{\textit{e}}$\;
%	    	$\textbf{\textit{q}}_n \leftarrow \textbf{\textit{q}}+\textbf{\textit{dq}}^{\top}$\;
%	    	$\textbf{\textit{e}}_n \leftarrow$ differential motion corresponding to $\xi$ and $\mathcal{K}(\textbf{\textit{q}})$\;
%	    	\eIf{$||\textbf{\textit{e}}_n||<||\textbf{\textit{e}}||$}{
%	    		 $\textbf{\textit{q}} \leftarrow \textbf{\textit{q}}_n$ \;
%	    		$\textbf{\textit{e}} \leftarrow \textbf{\textit{e}}_n$ \;
%	    		$ \lambda \leftarrow \frac{\lambda}{2}$ or other altruistic methods to decrease $\lambda$\;
%	    		$ r \leftarrow 0$ \;}
%   		{
%    			$ \lambda \leftarrow 2\lambda$ or other altruistic methods to increase $\lambda$\;
%    			$r \leftarrow r+1$ \;
%    			\If{$r>r_{max}$}{\Return $\emptyset$;}   			
%    	    }
%		}
%		\Return $\emptyset$ \;
%		\caption{Numerical inverse kinematics}
%	}
%"
617,1803.07164,"[htpb]
    \begin{algorithmic}[1]
  \STATE{\textbf{Hyperparameters}: Step sizes $\eta_m, \eta_c$, mini-batch sizes $B_m, B_c$, number of models $M$ to use for averaging, a test function generator $G$.}
  \STATE{\textbf{Input}: data $S=\{(z_1, x_1), \ldots, (z_n, x_n)\}$}
  \STATE{Generate set of test functions $\mcF = G(S)$}
  \FOR{$t = 1 ,\ldots , T$}
    \STATE{Randomly sample with replacement from $S$ three batches of samples $S_1, S_2, S_3$, with sizes $B_m, B_m, B_c$ corrspondingly.}
    
    \STATE{Let $E_S$ denote an expectation with respect to the empirical distribution in a sample $S$. Construct estimates of the gradient of the modeler:
    \begin{align*}
        \hat{\nabla}_{\theta, t} =~& 2 \sum_{f \in \mcF} \sigma_{f,t} \cdot  \E_{S_1}\left[\rho(z; h_{\theta_t}) f(x)\right] \cdot \\
        &~~~~~~~~~~~~~~~~~~~~~~~~\E_{S_2}\left[f(x)\nabla_{\theta}  \rho(z; h_\theta)\right]
     \end{align*}
      and the utility of the critic:
      \begin{align*}
        \hat{U}_{f,t} =~& \left(\E_{S_3}\left[ \rho(z; h_{\theta_t}) f(x)\right]\right)^2, ~\forall f\in\mcF
    \end{align*}}
    \STATE{{\bf Modeler Step.} Take a gradient step for the modeler using projected gradient descent (or any first order algorithm like Adam):
    \begin{align*}
            \theta_{t+1} =~& \Pi_{\Theta}\left(\theta_{t} - \eta_m \hat{\nabla}_{\theta, t}\right)
    \end{align*}}
    \STATE{{\bf Critic Step.} Take a Hedge step for the critic:
    \begin{align*}
            \sigma_{f, t+1} \propto~& \sigma_{f, t} \cdot \exp\left\{\eta_c \hat{U}_{f,t} \right\}, ~\forall f\in \mcF
    \end{align*}}
    \STATE{{\bf Critic Jitter.} If each $f\in \mcF$ is parameterized via some parameter $w_f \in W$, then take a gradient step for $w_f$:
    \begin{align*}
    \nabla_{w_f, t} =~& 2 \E_{S_1}\left[\rho(z; h_{\theta_t}) f(x; w_f)\right] \cdot \\
        &~~~~~~~~~~~~~~~~~~~~~~~~\E_{S_2}\left[\rho(z; h_\theta) \nabla_{w_f} f(x; w_f)\right]\\ 
    w_{f,t+1} =~& \Pi_{W}\left(w_{f, t} + \eta_w \nabla_{w_f, t}\right)
    \end{align*}}
    \ENDFOR
\STATE{Return the average model $h^* = \frac{1}{M}\sum_{t\in I} h_t$, where $I$ is a set of $M$ randomly chosen steps during training.}
\caption{AdversarialGMM}
  \label{alg:deepgmm}
\end{algorithmic}
"
618,1803.07164,"[htpb]
\begin{algorithmic}[1]
\STATE{\textbf{Hyperparameters}: Number of clusters $K$, minimum radius size $r$.}
  \STATE{\textbf{Input}: data $S=\{(z_1, x_1), \ldots, (z_n, x_n)\}$}
  \STATE{Cluster the data-points $(x_1,\ldots, x_n)$ into $K$ clusters using $K$-means.}
  \STATE{For each cluster $i\in [K]$ let $f_i:\mcX\rightarrow R$ be a Gaussian Kernel:
  \begin{equation}
      f_i(x) = \frac{1}{(2\pi\sigma^2)^{1/2d}} \exp\left\{-\frac{\|x - x_i\|_W^2}{2\sigma^2}\right\}
  \end{equation}
  with standard deviation $\sigma$ equals to twice the distance to the $r$-th closest data point to the centroid of cluster $i$, with respect to a $W$-matrix weight norm:
  \begin{equation}
  \|x\|_W^2 = x^T W x
  \end{equation}
  for some positive definite matrix $W=VV^T$.}
  \STATE{Return $\mcF = \{f_i: i\in [K]\}$}
\end{algorithmic}
\caption{KMeans based test function generation}\label{alg:test}
"
619,1803.07043,"[h!]
	\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\SetKwInOut{Parameters}{Parameters}
\Input{$(z^1,{\bf w}^1)\in \boldsymbol{\mathcal{H}}$, $(x_i^0,y_i^0)\in\mathcal{H}_i^2$ for $i=1,\ldots,n$.}
\Parameters{$\{I_k\}_{k\in\nN}$ where $I_k\subseteq\{1,\ldots,n\}$, $\{d(i,k)\}_{k\in\nN}$ for $i=1,\ldots,n$ where $1\leq d(i,k)\leq k$, $0<\underline{\beta}\leq\overline{\beta}<2$, $\gamma>0$.}

\For{$k=1,2,\ldots$}
{   
	%\If{$k\geq \hat{k}$}
	%{

		\For{$i\in I_k$}
		{
			\tcc{\col{these are the active operators to be processed}}
			\label{lineStartFor}
			\If{$i\in\Iback$}
			{
			    	$a = G_i z^{d(i,k)}+\rho_{i}^{d(i,k)} w_i^{d(i,k)}+e_i^k$\label{lineaupdate}
			    					\tcc*{\col{do a backward step}}
			    $x_i^k = \prox_{\rho_{i}^{d(i,k)} T_i}(a)
			    $\label{LinebackwardUpdate}\;
			    $
			    y_i^k = (\rho_{i}^{d(i,k)})^{-1}
			    \left(
			    a - x_i^k
			    \right)
			    $\label{lineBackwardUpdateY}\;

			    	          
		    }
		    \Else
			{
				\tcc{\col{do two forward steps}}            	            	
			    \label{LineForwardUpdate}
				$					
				x_i^k = G_i z^{d(i,k)}-\rho_{i}^{d(i,k)}
				( T_i G_i z^{d(i,k)} - w_{i}^{d(i,k)}),
				$\label{ForwardxUpdate} \;
				$
				y_i^k = T_i x_i^k.
				$
				\label{ForwardyUpdate}\;

		    }
        }
	    \For{$i\notin I_k$} 
		{
			\tcc{\col{These are the inactive operators}}
			$(x_i^k,y_i^k)=(x_i^{k-1},y_i^{k-1})$\label{lineLeave}
		}
	
	\tcc{\col{Beginning of projection procedure}} 
    $u_i^k = x_i^k - G_i x_n^k,\quad i=1,\ldots,n-1,$\label{lineCoordStart}\;
	$v^k = \sum_{i=1}^{n-1} G_i^* y_i^k+y_n^k$\label{lineVupdate}\;    
	$\pi_k = \|u^k\|^2+\gamma^{-1}\|v^k\|^2$  \label{linePiUpdate}\;    
	\eIf{$\pi_k>0$}{
		Choose some $\beta_k \in [\underline{\beta},\overline{\beta}]$\;  \label{lineHplane} 
		$\varphi_k(p^k) = 
		\langle z^k, v^k\rangle 
		+
		\sum_{i=1}^{n-1}
		\langle w_i^k,u_{i}^k\rangle 
		-
		\sum_{i=1}^{n}
		\langle x_i^k,y_i^k\rangle  
		$\label{lineComputeHplane}\;
		$\alpha_k = \frac{\beta_k}{\pi_k}\max\left\{0,\varphi_k(p^k)\right\}
		$\label{lineAlphaCompute}\;
	}
	{
%	    $\alpha_k = 0$\;
	    %\If{$y_i^k\in T_i x_i^k$ for $i=1,\ldots,n$}
	    \If{$\cup_{j=1}^k I_j=\{1,\ldots,n\}$ \label{allprocessed}}
	    {
		    \Return $z^{k+1}\leftarrow x_n^k, w_1^{k+1}\leftarrow y_1^k,\ldots,w_{n-1}^{k+1}\leftarrow y_{n-1}^k$\label{lineReturn}\;
	    }
        \Else
        {
           $\alpha_k = 0$\;
        }
	} 
$z^{k+1} = z^k - \gamma^{-1}\alpha_k v^k$\label{eqAlgproj1} \;
$w_i^{k+1} = w_i^k - \alpha_k u_{i}^k,\quad i=1,\ldots,n-1$,\label{eqAlgproj2} \;
$w_{n}^{k+1} = -\sum_{i=1}^{n-1} G_i^* w_{i}^{k+1}$\label{lineCoordEnd}\;
}
\caption{General Projective Splitting Algorithm for solving~\eqref{probCompMonoMany}.}
\label{AlgfullyAsync}
"
620,1803.07043,"[h!]\label{AlgGenericProject}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}
\Input{$p^1$, $0<\underline{\beta}\leq\overline{\beta}<2$}
	\caption{Generic linear separator-projection method for finding a point in a closed and convex set $\calS\subseteq\boldsymbol{\mathcal{H}}$.}
	\label{AlgAbstractProject}
	\For{$k=1,2,\ldots,$}{
	Find an affine function $\varphi_k$ such that $\nabla\varphi_k\neq 0$ and
	    $\varphi_k(p)\leq 0$ for all $p\in \calS$. \;
	Choose $\beta_k\in[\underline{\beta},\overline{\beta}]$\label{eqProjectUpdatem1}\;
	$p^{k+1} = p^k - 
	\frac{\beta_k\max\{0,\varphi_k(p^k)\}}
	{\|\nabla\varphi_k\|_{\bcalH}^2}
	\nabla\varphi_k$
	\label{eqProjectUpdate}\;
 }
"
621,1803.07043,"[h]\label{AlgBackTrack}
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\caption{Backtracking procedure for unknown Lipschitz constants}
	\label{AlgLineSearch}
	\Input{$i,k,z^{d(i,k)}$, $w_i^{d(i,k)}$, $\rho_i^{d(i,k)}$, $\Delta$}
	%\tcc{{\scriptsize $k$ is the iteration counter from Algorithm 
	% \ref{AlgfullyAsync} which is used here only to define the error sequences}}
	%$j = 1$, $t^1=1$, 
	% JE tried something simpler here without explicit increments and decrements of $j$
	$\rho_i^{(1,k)} = \rho_i^{d(i,k)}$\;
	$\theta_i^k = G_{i}z^{d(i,k)}$\;
	$\zeta_i^k = T_i \theta_i^k$\;
	\For{$j=1,2,\ldots$}{
	   $\tilde{x}_i^{(j,k)} 
	      = \theta_i^k - \rho_i^{(j,k)}(\zeta_i^k - w_i^{d(i,k)})$ \label{lineX}\;
       $\tilde{y}^{(j,k)}_i=T_i\tilde{x}^{(j,k)}_i$ \label{lineY}\;
       \If{\label{lineIf}$\Delta\|\theta_i^k-\tilde{x}_i^{(j,k)}\|^2 -
           \langle \theta_i^k-\tilde{x}_i^{(j,k)},\tilde{y}_i^{(j,k)}-w_i^{d(i,k)}\rangle
               \leq 0$}{
             \Return{ $J(i,k) \leftarrow j, \;
                       \hat{\rho}_i^{d(i,k)} \leftarrow \rho_i^{(j,k)}, \;
                       x_i^k \leftarrow \tilde{x}_i^{{(j,k)}}, \;
                       y_i^k \leftarrow \tilde{y}_i^{{(j,k)}}$}
                       \label{lineBTreturn}
           }
       $\rho_i^{(j+1,k)} = \rho_i^{(j,k)}/2$\;
	}
	% \While{$t^j>0$}
	% {
	% 	$\tilde{x}_i^{(j,k)} = \theta_i^k - \rho_i^{(j,k)}(\zeta_i^k - w_i^{d(i,k)})$\label{lineX}\;
	% 	$\tilde{y}^{(j,k)}_i=T_i\tilde{x}^{(j,k)}_i+\hat{e}^{(j,k)}_i$
	% 	\label{lineY}\;
 %        $\rho_i^{(j+1,k)} = \rho_i^{(j,k)}/2$\;
	% 	$t^{j+1} = \Delta\|\theta_i^k-\tilde{x}_i^{(j,k)}\|^2 -  \langle \theta_i^k-\tilde{x}_i^{(j,k)},\tilde{y}_i^{(j,k)}-w_i^{d(i,k)}\rangle$\;
 %        $j\leftarrow j+1$\;
	% }
	% \Output {$x_i^k \leftarrow \tilde{x}_i^{{(j-1,k)}},y_i^k \leftarrow \tilde{y}_i^{{(j-1,k)}}$}
"
622,1803.07043,"[h!]
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Parameters}{Parameters}
\col{ 
	\Input{$(z^1,{\bf w}^1)\in \boldsymbol{\mathcal{H}}$, $(x_i^0,y_i^0)\in\mathcal{H}_i^2$ for $i=1,\ldots,n$.}
	\Parameters{$\{I_k\}_{k\in\nN}$ where $I_k\subseteq\{1,\ldots,n\}$, $0<\underline{\beta}\leq\overline{\beta}<2$, $\gamma>0$.}
	
	\For{$k=1,2,\ldots$}
	{   		
		
			\For{$i\in I_k$}
			{ 
			  \tcc{Loop over the blocks chosen to be updated according to user-supplied rule $\{I_k\}$}
				\If{$i\in\Iback$}
				{
					$a = G_i z^{k}+\rho_{i}^{k} w_i^{k}+e_i^k$\label{lineaupdate2}
					\tcc*{do a backward step}
					$x_i^k = \prox_{\rho_{i}^{k} T_i}(a)
					$\label{LinebackwardUpdate2}\;
					$
					y_i^k = (\rho_{i}^{k})^{-1}
					\left(
					a - x_i^k
					\right)
					$\label{lineBackwardUpdateY2}\;
					
					
				}
				\Else
				{\label{LineForwardUpdate2}
					$					
					x_i^k = G_i z^{k}-\rho_{i}^{k}
					( T_i G_i z^{k} - w_{i}^{k}),
					$\label{ForwardxUpdate2}
					\tcc*{do two forward steps}            	
					$
					y_i^k = T_i x_i^k.
					$
					\label{ForwardyUpdate2}\;
					
				}
			}
			
			For $j\notin I_k$, set $(x_j^k,y_j^k)=(x_j^{k-1},y_j^{k-1})$\tcc*{other blocks unchanged}
			\tcc{The projection procedure is then the same as lines \ref{lineCoordStart}-\ref{lineCoordEnd} of Algorithm \ref{AlgfullyAsync}}
		
	}
}
	\caption{\col{Simplified Block-Iterative 
	              %Projective Splitting Scheme to update $(x_i^k,y_i^k)$
	              Algorithm.}}
	\label{AlgBlockIter}
"
623,1804.08748,"
\scriptsize
\DontPrintSemicolon
\KwIn{$\gamma$, $M$}
\KwOut{$S_{\gamma}$ \Comment{$S_{\gamma}$ is transformed version (signal) of $\gamma$}}
$S_{\gamma} \gets \langle\rangle$\;
\For {$i=1$  to  $n$-$1$}
{
    $\phi \gets ReturnState(M,\rho_i)$\;
    $\phi' \gets ReturnState(M,\rho_{i+1})$\;
    $v = 0$\;
    \If{$\phi\neq \phi'$}
    {
        $prob_{\phi\rightarrow \phi'} = ReturnProb(M,\phi,\phi')$\;
        $R \gets TransitionFrom(M, \phi)$\;
        \Comment{$R = \{r|$ $(\phi\rightarrow r)\in\Delta \}$}\;
        \For{$r \in R$}  
        {
            $prob_{\phi\rightarrow r} = ReturnProb(M,\phi,r)$\;
            $v$ += $Euclidean(\phi',r)\times prob_{\phi\rightarrow r}$\;
        }
        $v = \frac{v}{|R|}$\;
    }
    
    $S_{\gamma} \gets Append(S_{\gamma},v)$ \Comment{Appending $v$ at the end of $S_{\gamma}$}\;
}
\caption{{\small \tt Trajectory Transformation}}
\label{algo:transformation}
"
624,1804.08774,"[!t]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{\brane\ Framework}
\label{alg:framework}
\begin{algorithmic}[1]
\REQUIRE $G = (\mathcal{V}, \mathcal{E}, \mathbf{A})$, embedding dimensions $d_{1}$, $d_{2}$, batch size $b$, learning rate $\alpha$, regularization coefficient $\lambda$. 
\ENSURE Attributional embedding matrix $\mathbf{P}$ and neighborhood embedding matrix $\mathbf{P}^{\prime}$. \\
%\STATE Initialize $\mathcal{L}(\Theta)^{prev}$.
\STATE Initialize all model parameters $\Theta = \{\mathbf{P}, \mathbf{P}^{\prime}, \mathbf{W}, \mathbf{b}\}$ with $0$ mean and $0.01$ standard deviation from the Gaussian distribution.
\REPEAT
\STATE Construct the mini-batch of node-triples $(u,i,j)$.
\STATE Calculate $\mathbf{f}_{u},\mathbf{f}_{i},\mathbf{f}_{j}$ using Equations~\ref{eq:1},~\ref{eq:2},~\ref{eq:3},~\ref{eq:4},~\ref{eq:5}.
\STATE Calculate $\mathbf{h}_{u},\mathbf{h}_{i},\mathbf{h}_{j}$ based on the Equation~\ref{eq:6}.
\STATE Calculate $s_{ui} = \langle \mathbf{h}_{u}, \mathbf{h}_{i} \rangle$ and
$s_{uj} = \langle \mathbf{h}_{u}, \mathbf{h}_{j} \rangle$
\STATE Calculate $\mathcal{L}(\Theta)$ using Equation~\ref{eq:10}.
\STATE Update the gradients of $\Theta = \{\mathbf{P}, \mathbf{P}^{\prime}, \mathbf{W}, \mathbf{b}\}$ using the back-propagation.
%\STATE Calculate $\Delta \mathcal{L}(\Theta) \leftarrow |\mathcal{L}(\Theta) - \mathcal{L}(\Theta)^{prev}|$
%\STATE $\mathcal{L}(\Theta)^{prev} \leftarrow \mathcal{L}(\Theta)$
\UNTIL{Convergence} %$\Delta \mathcal{L}(\Theta) < \epsilon $.}
\RETURN $\mathbf{P}, \mathbf{P}^{\prime}$.
\end{algorithmic}
"
625,1802.10229,"[h]
  \small
  \setstretch{1.1}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \Input{input document $\mathbf{x}$, candidate sequences $\{ \mathbf{y} \}$, \newline joint scoring function $S(\mathbf{x}, \mathbf{y}_{t_1:t_2})$}
  \Output{beam sequence set $C$}
 $C \leftarrow \emptyset$\\
 \While{not converged}{
  // \texttt{forward beam search}\\
  \For{$t = 1, \cdots, T$}{
   $C^{(F)} \leftarrow \text{top-B}_{\mathbf{y}_{1:t}} [S(\mathbf{x}, \mathbf{y}_{1:t}) + S(\mathbf{x}, \mathbf{y}_{T:t})]$\\
   // \texttt{add gold subsequence}\\
   $C^{(F)} \leftarrow C^{(F)} \cup \{ \mathbf{y}_{1:t}^* \}$\\
   $C \leftarrow C \cup C^{(F)}$
   }
   // \texttt{backward beam search}\\
   \For{$t = T, \cdots, 1$}{
   $C^{(B)} \leftarrow \text{top-B}_{\mathbf{y}_{T:t}} [S(\mathbf{x}, \mathbf{y}_{T:t}) + S(\mathbf{x}, \mathbf{y}_{1:t})]$\\
   // \texttt{add gold subsequence}\\
   $C^{(B)} \leftarrow C^{(B)} \cup \{ \mathbf{y}_{T:t}^* \}$\\
   $C \leftarrow C \cup C^{(B)}$
  }
 }
 \caption{Bidirectional Beam Search with Gold path (BiBSG)}
 \label{eq:bibsg}
"
626,1804.03329,"[t]
% \caption{h-MDS}
\caption{ }
\begin{algorithmic}[1]
\STATE {\bfseries Input: Distance matrix $d_{i,j}$ and rank $r$}
\STATE Compute scaled distance matrix $Y_{i,j} = \cosh(d_{i,j})$
\STATE $X \rightarrow \text{PCA}(-Y,r)$
\STATE Project $X$ from hyperboloid model to Poincar\'{e} model: $x \to \frac{x}{1 + \sqrt{1 + \|x\|^2}}$
\STATE If desired, center $X$ at a different mean (e.g. the Karcher mean)
\STATE \textbf{return} $X$
\end{algorithmic}
\label{alg:new_hmds}
"
627,1804.03329,"[t]
\begin{algorithmic}[1]
\STATE \textbf{Input:} Node $a$ with parent $b$, children to place $c_1, c_2, \ldots, c_{\operatorname{deg}(a)-1}$, partial embedding $f$ containing an embedding for $a$ and $b$, scaling factor $\tau$
\STATE $(0, z) \leftarrow \operatorname{reflect}_{f(a) \rightarrow 0}(f(a),f(b))$ %\COMMENT{circle inversion}
\STATE $\theta \leftarrow \operatorname{arg}(z)$ \hspace{2em} \COMMENT{angle of $z$ from x-axis in the plane}
\FOR{$i \in \{1, \ldots, \operatorname{deg}(a)-1 \}$}
\STATE $y_i \leftarrow \left(\frac{e^\tau-1}{e^\tau+1} \cdot \cos\left(\theta + \frac{2\pi i}{\operatorname{deg}(a)} \right) , \frac{e^\tau-1}{e^\tau+1} \cdot \sin\left(\theta+\frac{2\pi i}{\operatorname{deg}(a)}\right) \right)$ % \label{alg:sarkar:step:circle}
\ENDFOR
\STATE $(f(a), f(b), f(c_1),\ldots,f(c_{\operatorname{deg}(a)-1})) \leftarrow \operatorname{reflect}_{0 \rightarrow f(a)}(0, z, y_1, \ldots, y_{\operatorname{deg}(x)-1})$
\STATE \textbf{Output:} Embedded $\mathbb{H}_2$ vectors $f(c_1), f(c_2), \ldots, f(c_{\operatorname{deg}(a)-1})$
\end{algorithmic}
\caption{Sarkar's Construction}
\label{alg:sarkar}
"
628,1804.08890,"[!t]
\caption{Cartoon+Texture Decomposition} \smallskip
\KwIn{Original image $f$, parameter $\sigma >0$}
{
  \begin{algorithmic}[1]
    \STATE Compute the LTV reduction rate using \eqref{eq: reduction} at each pixel.
    		%$$\lambda_{\sigma}(x) \gets \frac{LTV_{\sigma}(f)(x) - 
			%LTV_{\sigma}(L_{\sigma} \ast f)(x)}{LTV_{\sigma}(f)(x)}$$
    \STATE Obtain the cartoon component: 
   		\begin{align*}
   		\begin{split}
   		    u(x) &\gets w\big(\lambda_{\sigma}(x)\big)(L_{\sigma} \ast f)(x)\\&+ \big(1-w(\lambda_{\sigma}(x))\big)f(x)
   		    \end{split}
   		\end{align*}
    \STATE Obtain the texture component:
   		$$v(x) \gets f(x)-u(x)$$
  \end{algorithmic}
  }
\KwOut{Cartoon and texture components $u$, $v$}
\label{alg:NonLinear_CTD}
"
629,1804.08890,"[t]
\caption{MBO scheme for local MCV}\smallskip
\KwIn{Image $u_0$, parameters $\lambda,\mu,\beta,\;dt$}
  \begin{algorithmic}[1]
    \STATE Compute $$\tilde u_0 = g_k \ast u_0 - u_0$$ using the Gaussian filter, preferably imgaussfilt in MATLAB.
    \STATE Initialize $u_1$ and $u_2$ as in \eqref{eq:u-ini}.
   	 \FOR{$i = 1$ to $n$}
    		\STATE Compute the average intensities $\mathbf{c}^i$ as in \eqref{eq:cn}.
        		\STATE Compute the average intensities $\mathbf{d}^i$  as in \eqref{eq:dn}.
       	 	\STATE Compute the solution $\mathbf{u}^i$ using equations\\ \eqref{eq:ode1-n}-\eqref{eq:thre2-n}.
   	\ENDFOR
     \STATE Combine $u_1^n$ and $u_2^n$ to obtain multiphase image $\tilde{u}^n$,\\ i.e. $$\tilde{u}^n = u_1^n + 2u_2^n$$
  \end{algorithmic}
\KwOut{Segmented Image $\tilde{u}^n$}
\label{alg:LMCV}
"
630,1804.08890,"[th]
\caption{Merging Curvelet partition}\smallskip
\KwIn{Thresholded Fourier domain $T(\mathcal{F}_P(v),\tau)$, original boundaries $\{\theta_m\}_{m=1}^{N_\theta}$ and  
$\left\{\{\omega_n^m\}_{n=1}^{N_{s}^m}\right\}_{m=1}^{N_\theta}$}
  \begin{algorithmic}[1]
    \STATE Compute $M_n^m$ for $m=1\ldots N_\theta$ and $n=1\ldots N_s^m$ according to \eqref{eq:density}.
    \STATE Compute $\tilde{M} = \max_{m,n}M_n^m$.
    \FOR{$i = 1$ to $N_\theta$}
       \STATE Set $j := N_s^i$.
       \WHILE{$j \geq 3$}
           \IF{$M_{j-1}^i< 0.10\tilde{M}$}
	  \STATE Remove $\omega_{j-1}^i$ from $\left\{\{\omega_n^m\}_{n=1}^{N_{s}^m}\right\}_{m=1}^{N_\theta}$.
	  \STATE Set $N_s^i := N_s^i -1$.
	  \ENDIF
	  \STATE $j := j-1$.
      \ENDWHILE
    \ENDFOR
\end{algorithmic}
\KwOut{Updated boundaries $\left \{\{\theta_m\}_{m=1}^{N_\theta}, \left\{\{\omega_n^m\}_{n=1}^{N_{s}^m}\right\}_{m=1}^{N_\theta} \right \}$.}
\label{alg:merge}
"
631,1804.08890,"[th]
\caption{Modified Empirical Curvelet Transform}\smallskip
\KwIn{Image $v(x)$, Threshold Value $\tau$}
  \begin{algorithmic}[1]
    \STATE Compute the Pseudo-Polar FFT $\mathcal{F}_P(v)$.
    \STATE Threshold the Fourier coefficients to obtain $T(\mathcal{F}_P(v), \tau)$.
    \STATE Detect the original partition 
    	$ \left \{\{\theta_m\}_{m=1}^{N_\theta}, \left\{\{\omega_n^m\}_{n=1}^{N_{s}^m}\right\}_{m=1}^{N_\theta} \right \}$ 
	using Otsu's Method as described in \cite{GTO,SSHS}.
    \STATE Compute the updated set 
    	$ \left \{\{\theta_m\}_{m=1}^{N_\theta}, \left\{\{\omega_n^m\}_{n=1}^{N_{s}^m}\right\}_{m=1}^{N_\theta} \right \}$ 
	by applying the merging algorithm Algorithm~\ref{alg:merge}.
    \STATE Construct the corresponding curvelet filter bank 
    	$ \mathscr{B}^{\mathcal{MEC}} = \left\{\phi_1(x),
		 \{\psi_{m,n}(x)\}_{\substack{m=1,\ldots,N_{\theta}\\n=1,\ldots,N_{s}^m-1}}\right\}$ 
		 accordingly to \eqref{eq:lowpasscurvelet}-\eqref{eq:polarwedge}.
    \STATE Filter $v(x)$ using \eqref{eq:waveletdetail2}-\eqref{eq:waveletapproximation2} to obtain 
    	$ \mathcal{W}_{v}^{\mathcal{MEC}}= \left\{\mathcal{W}_{v}^{\mathcal{MEC}}(0,0,x), 
	\{ \mathcal{W}_{v}^{\mathcal{MEC}}(m,n,x)\}_{\substack{m=1,\ldots,N_{\theta}\\n=1,\ldots,N_s^m-1}} \right\}$
  \end{algorithmic}
\KwOut{Spectrum boundaries $\left \{\{\theta_m\}_{m=1}^{N_\theta}, 
	\left\{\{\omega_n^m\}_{n=1}^{N_{s}^m}\right\}_{m=1}^{N_\theta} \right \}$,
	empirical curvelet filter bank $\mathscr{B}^{\mathcal{MEC}}$,
	empirical curvelets coefficients $\mathcal{W}_{v}^{\mathcal{MEC}}$. }
\label{alg:MECT}
"
