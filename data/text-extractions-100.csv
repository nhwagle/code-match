,paper,link,code,text
0,Temporal coherence-based self-supervised learning for laparoscopic workflow analysis,http://arxiv.org/pdf/1806.06811v2.pdf,https://gitlab.com/nct_tso_public/pretrain_tc,"Temporalcoherence-basedself-supervised
learningforlaparoscopicwwanalysis
IsabelFunke
1
?
,AlexanderJenke
1
,orenTorgeMees
2
,JurgenWeitz
2
,Stefanie
Speidel
1
,andSebastianBodenstedt
1
?
1
DepartmentforTranslationalSurgicalOncology,NationalCenterforTumor
Diseases(NCT),PartnerSiteDresden,Dresden,Germany
Firstname.Lastname@nct-dresden.de
2
DepartmentofVisceral,ThoracicandVascularSurgery,FacultyofMedicineand
UniversityHospitalCarlGustavCarus,TUDresden,Dresden,Germany
Abstract.
Inordertoprovidetherighttypeofassistanceattheright
time,computer-assistedsurgerysystemsneedcontextawareness.To
achievethis,methodsforsurgicalwwanalysisarecrucial.Cur-
rently,convolutionalneuralnetworksprovidethebestperformancefor
video-basedwwanalysistasks.Fortrainingsuchnetworks,large
amountsofannotateddataarenecessary.However,collectingat
amountofdataisoftencostly,time-consuming,andnotalwaysfeasible.
Inthispaper,weaddressthisproblembypresentingandcomparingdif-
ferentapproachesforself-supervisedpretrainingofneuralnetworkson
unlabeledlaparoscopicvideosusingtemporalcoherence.Weevaluateour
pretrainednetworksonCholec80,apubliclyavailabledatasetforsurgical
phasesegmentation,onwhichamaximum
F
1
scoreof84.6wasreached.
Furthermore,wewereabletoachieveanincreaseofthe
F
1
scoreofup
to10pointswhencomparedtoanon-pretrainedneuralnetwork.
Keywords:
self-supervisedlearning

temporalcoherence

surgicalwork-
wanalysis

surgicalphaserecognition

pretraining

CNN-LSTM.
1Introduction
Theaimofacomputer-assistedsurgery(CAS)systemistoprovidethesurgeon
withtherighttypeofassistanceattherighttime.Toachievethis,context
awarenessiscrucial.Thismeansthatthesystemmustbeabletounderstand
theprocessescurrentlytakingplaceintheoperatingroom(OR)andadaptits
behavioraccordingly.
Surgicalanalysis
coversthechallengingtopicof
perceiving,understanding,anddescribingsurgicalprocesses[11].
Acommonapproachistoanalyzesurgicalprocessesbyinterpretingatime
seriesofsignalsthatarerecordedbysensors{insomecasesalsobyhumans
{intheOR.Aslaparoscopiesareperformedviacamera,methodsthatrequire
onlyvideoasinputsensordataareofspecialinterest,sincethevideocanbe
collectedslyduringsurgery.
?
Bothauthorscontributedequallytothiswork.
arXiv:1806.06811v2  [cs.CV]  7 Sep 20182I.Funkeetal.
State-of-the-artvideo-basedapproachesforwwanalysisrelyondeep
neuralnetworks[1,2,9,15,18].However,deeplearning-basedmethodsrequire
largeamountsoflabeleddatafortraining.Especiallyinsurgery,obtaininga
tamountofannotatedvideodataisandcostly.
Toalleviatetheproblemoflimitedtrainingdata,itiscommonto
pretrain
neuralnetworksandthemafterwards.Often,networksarepretrained
usinglabeleddatacomingfromanotherdomain,suchasImageNet[4].Another
wayistouseunlabeleddatafromthesamedomainandtrainonaproxytask
usinglabelsinherentinthedata,whichiscalled
self-supervised
learning.
Forself-supervisedlearningfromvideo,anumberofideashavebeenpro-
posed[5,8,12,13,16].Mostexploitthe
temporalcoherence
ofvideo,whichimplies
that(i)consecutiveframesareintemporalorder,(ii)frameschangeslowlyover
time,and(iii)frameschangesteadily,i.e.,abruptmotionsareunlikely.
Thestudies[12]and[13]proposeproxytasksbasedonthetemporalorder
betweenframes.Inlinewiththis,[2]usethetasktoorderpairsoflaparoscopic
imagesforpretraininganetworkforsurgicalphasesegmentation.
Surgicalphase
segmentation
[14]istheproblemofrecognizingthesurgicalphasebeingper-
formedbythesurgeonateachpointduringsurgery.Anotherproxytaskforthis
problemistopredicttheprogressandremainingdurationofasurgery[18].
Intuitively,thesetasksencouragethenetworktolearndiscriminativefeatures
thatareusefultoinfertheabsoluteorrelativetemporalpositionofavideoframe.
Incontrast,[5,8,16]aimatlearningfeaturesthatare
invariant
totypicalalter-
ationsoccurringbetweenadjacentframes,suchasslightrotationsordeforma-
tions.Tothisend,theyaimtoensurethattemporallycloseframes,whichmost
likelydepictthesamesemanticscene,aremappedtosimilarrepresentationsin
featurespace.Thisideagoesbackto
SlowFeatureAnalysis(SFA)
[17].
Inthispaper,wedescribeandcomparetapproachestoexploittem-
poralcoherencewhilepretrainingaconvolutionalneuralnetwork(CNN)for
surgicalphasesegmentation.WeassumethepretrainingencouragestheCNNto
learnfeaturesthatareinvarianttoirrelevantchangesbetweenadjacentframes,
suchasslightmovementsofinstrumentsoroftheendoscope,whilebeingdis-
criminativeenoughtodistinguishbetweensemanticallyderentframes.
Topromotereproducibilityandtofuelfutureresearch,wemadeourcode
availableat
https://gitlab.com/nct_tso_public/pretrain_tc
.
ExperimentsusingtheCholec80dataset[15]demonstratethataCNNpre-
trainedtoexploitthetemporalcoherenceofunlabeledlaparoscopicvideoout-
performsanon-pretrainedCNNafterbeingforsurgicalphaseseg-
mentation.Whenonly20labeledvideosareavailable,theproposedpretraining
achievesanincreasefrom67.8to78.6asmeasuredby
F
1
score.
2Methods
Thecoreofourneuralnetworkarchitectureforsurgicalphasesegmentationis
aResNet-50CNN[6].WeinitializeitwithImageNet[4]pretrainedweightsand
Temporalcoherence-basedself-supervisedlearningforlap.wwanalysis3
furthertrainitonunlabeledvideosoflaparoscopicsurgeries,usinganSFA-
basedapproachforself-supervisedlearning.ThisencouragestheCNNtomap
temporallyclosevideoframestosimilarrepresentationsinfeaturespace.
Moreformally,theCNNlearnsanembedding
f
:
R
3

h

w
!
R
d
,where
R
d
is
the
d
-dimensionalfeaturespaceand
R
3

h

w
isthespaceoflaparoscopicvideo
frameswithheight
h
,width
w
,andthreecolorchannels(RGB).Let
I
t
2
R
3

h

w
denotetheframeattimestep
t
.Totemporalcoherence,werequirethat
f
(
I
t
)
ˇ
f
(
I
t
+

)forasmall

with
j

j
<
.Tolearnanembeddingthatis
discriminativeandtoavoidtrivialsolutionssuchas
f
(
I
t
):=0,werequirethat
f
(
I
t
)and
f
(
I
t
+

)liefurtherapartinfeaturespacewhen

islarge,i.e.,
j

j
>
(seesubsection2.1fordetails).

and

arenonnegativereal-valuedparameters.
Toevaluatetheacyoftheproposedself-supervisedpretrainingapproach,
weextendtheCNNintoarecurrentneuralnetwork(RNN)andthe
CNN-RNNforsurgicalphasesegmentationusingannotatedlaparoscopicvideos
(seesubsection2.2).Wecanthencomparetheperformanceofthepretrained
CNN-RNNtotheperformanceofaCNN-RNNthathasbeentrainedsolelyfor
thesurgicalphasesegmentationtask(seesection3).
2.1Self-supervisedpretraining
Forself-supervisedpretraining,theoutputlayeroftheResNet-50CNNisre-
placedwithafullyconnectedlayerwith
d
=4096outputneurons(
FeatureNet
).
AstheCNNhasbeenpretrainedonImageNet,weonlyadjusttheweightsof
the
conv5
x
layersandofthenewlyaddedfullyconnectedlayerduringtraining.
Givenaframe
I
t
,wecalculatetheembedding
F
t
:=
f
(
I
t
)byforwardingthe
framethrough
FeatureNet
andtakingtheoutput(
o
1
;o
2
;:::;o
d
)
T
2
R
d
atthe
lastlayer.Wetrain
FeatureNet
tolearnatemporallycoherentvideoframeem-
beddingusingoneofthefollowingmethods.Throughoutthissection,
D
denotes
adistancefunction,inourcasetheL2norm.
(a)Trainingwith
contrastive
loss
Givenavideowith
T
frames,wecreateatuple(
I
t
;I
t
+

;I
t
+

)bysampling
t
from[0
;T

1],

from[

;
],and

from[

(
T

1)
;


]
[
[
;T

1]uniformly
atrandom.Regarding
FeatureNet
asaSiamesenetwork[3],wepropagatethe
temporallyclosepair(
I
t
;I
t
+

)throughtheCNNandcalculate
D
(
F
t
;F
t
+

).
Likewise,wepropagatethetemporallydistantpair(
I
t
;I
t
+

)andcalculate
D
(
F
t
;F
t
+

).Finally,wecalculatethecontrastiveloss[5]
L
c
(
F
t
;F
t
+

;F
t
+

)=
D
(
F
t
;F
t
+

)+
max
f
0
;m
c

D
(
F
t
;F
t
+

)
g
:
Thislossfunctionencourages
F
t
tobecloseto
F
t
+

,while
F
t
and
F
t
+

are
enforcedtobeseparatedbymargin
m
c
.
(b)Trainingwith
ranking
loss
Atrainingtuple(
I
t
;I
t
+

;I
t
+

)iscreatedthesamewayasinmethod(a).
Regarding
FeatureNet
asaTripletSiameseNetwork,wepropagatethetriplet
(
I
t
;I
t
+

;I
t
+

)throughtheCNNandcalculatetherankingloss[16]
L
r
(
F
t
;F
t
+

;F
t
+

)=
max
f
0
;D
(
F
t
;F
t
+

)

D
(
F
t
;F
t
+

)+
m
r
g
:
4I.Funkeetal.
Thislossfunctionconsidersthedistancebetween
F
t
and
F
t
+

relativeto
thedistancebetween
F
t
and
F
t
+

andencourages
F
t
and
F
t
+

tobecloser
togetherthan
F
t
and
F
t
+

byamarginof
m
r
.
(c)Trainingwith
1
st
&2
nd
ordercontrastive
loss
Whileorder)temporalcoherencerequirestheordertemporal
derivativesinthelearnedfeaturespacetobesmall,i.e.,
F
t
ˇ
F
t
+

,
second
ordertemporalcoherence
[8]requiresthesecondordertemporalderivatives
tobesmall,i.e.,
F
t

F
t
+

ˇ
F
t
+


F
t
+2

forasmallvalueof

.
Intuitively,ordertemporalcoherenceensuresthatembeddingsdonot
changequicklyovertime,whilesecondordertemporalcoherenceensuresthat
thechangesareconsistent,orsteady,acrossneighboringframes.Applying
thecontrastivelossfunctiontosecondordertemporalcoherenceyields
L
c
2
(
F
t
;F
t
+

;F
t
+2

;F
t
+

)=
L
c
(
F
t

F
t
+

;F
t
+


F
t
+2

;F
t
+


F
t
+

)
Inpractice,wecreateatrainingtuple(
I
t
;I
t
+

;I
t
+2

;I
t
+

)bysampling
t
,

,and

asdescribedinmethod(a).Regarding
FeatureNet
asaTripletSia-
meseNetwork,wepropagatethetriplets(
I
t
;I
t
+

;I
t
+2

)and(
I
t
;I
t
+

;I
t
+

)
throughthenetworkandcalculate
L
c
2
.Wethencombineitwiththe
ordercontrastiveloss
L
c
intoanoverallloss
L
c
+
c
2
=
L
c
+
!L
c
2
;
where
!
=0
:
5isanonnegativereal-valuedweightparameter.
2.2Supervisedforsurgicalphasesegmentation
Oncepretrained,wemodifytheCNNforsurgicalphasesegmentationbyextend-
ingitintoanRNNusingalongshort-termmemoryunit(LSTM)[7]with512
neurons.TheLSTMisfollowedbyafullyconnectedlayer,whichhasoneoutput
neuronpersurgicalphase.WerefertothisCNN-LSTMas
PhaseNet
.During
theweightsoftheCNNandtheLSTMarejointlyoptimized.How-
ever,theweightsoftheResNet-50layersbelow
conv5
x
stayfrozen.
3Evaluation
Forevaluation,weusedthepubliclyavailableCholec80dataset[15].Itconsists
of80videosfromlaparoscopiccholecystectomies,annotatedwithsurgicalphase
labels.WedividedthedatasetintofoursetsA,B,C,andDofequalsizeand
similaraverageprocedurelength.A,B,andCwereusedfortraining,whileD
waswithheldfortesting.Forpretraining,weextractedvideoframesat5Hz.
Trainingandtestingforphasesegmentationwasperformedat1Hz.Eachframe
wasdownsizedto384

216px.
Wetrainedthreetversionsof
FeatureNet
,onewitheachofthepre-
trainingvariantsdescribedinsection2.1.TheunionofsetsA,B,andC(i.e.,
60videosintotal)wasusedastrainingdata,ignoringthelabels.EachCNNwas
trainedfor25epochs.Perepoch,werandomlysampled250tuplespervideo,
whichwereprocessedinbatchesofsize64.

wassetto30sec(15secforvariant
Temporalcoherence-basedself-supervisedlearningforlap.wwanalysis5
#OPsAccuracyRecallPrecision
F
1
score
Nopretraining
2078
:
8

12
:
572
:
3

11
:
473
:
4

12
:
967
:
8

14
:
1
4088
:
8

7
:
783
:
2

8
:
483
:
8

9
:
280
:
4

10
:
3
6089
:
7

6
:
682
:
8

9
:
485
:
8

7
:
680
:
8

10
:
3
Contrastive
2084
:
4

10
:
677
:
2

8
:
478
:
8

5
:
373
:
9

8
:
9
4091
:
7

5
:
585
:
4

6
:
188
:
2

5
:
683
:
8

7
:
1
6092
:
0

4
:
586
:
2

4
:
285
:
5

4
:
883
:
6

4
:
9
Ranking
2086
:
1

7
:
279
:
4

6
:
582
:
9

5
:
977
:
2

7
:
4
4090
:
2

6
:
485
:
6

6
:
285
:
2

5
:
982
:
5

7
:
3
6090
:
3

5
:
485
:
2

6
:
286
:
1

5
:
282
:
9

6
:
9
1
st
&2
nd
order
contrastive
2088
:
1

5
:
880
:
7

5
:
783
:
8

5
:
678
:
6

6
:
1
4090
:
7

10
:
486
:
3

7
:
586
:
9

6
:
183
:
4

10
:
1
6092
:
7

4
:
387
:
0

4
:
087
:
6

5
:
384
:
6

5
:
4
Table1.
Performanceofthebaselinerow)andthepretrainedmodelsonthe
surgicalphasesegmentationtask.
#OPs
denoteshowmanylabeledOPswereused.
(c)),

to120secand
m
c
=
m
r
=2.WeusedtheAdamoptimizer[10]with
alearningrateof10

4
.Allnewlyaddedlayerswereinitializedwithrandom
valuesfromtherange(

1
p
n
;
1
p
n
),with
n
beingthenumberofneuronsinthelayer.
Toevaluatethesuitabilityoftheproposedpretrainingapproachforsurgical
phasesegmentation,eachofthepretrainedCNNs(
contrastive
,
ranking
,and
1
st
&2
nd
ordercontrastive
)wasextendedintoa
PhaseNet
and
usingthelabeledvideosfromeithersetA(#OPs=
20
),setsAandB(#OPs
=
40
),orsetsA,B,andC(#OPs=
60
).Asbaseline,a
PhaseNet
withoutself-
supervisedpretraining(
nopretraining
)wasinthesamemanner.
NotethattheunderlyingResNet-50CNNhadstillbeenpretrainedonImageNet.
Fore-tuningthenetworks,weusedtheAdamoptimizer[10]withalearning
rateof10

4
andabatchsizeof128.Aftereverybatch,thecontentofthe
LSTM'shiddenstatewassavedandrestoredforthenextbatch.Duetohardware
restraints,gradientswereonlyaccumulatedforthreebatchesbeforeapplyingthe
optimizer.Trainingwasstoppedoncetheaccuracyonthetrainingsetclimbed
above99.9%.Allnewlyaddedlayerswereinitializedasdescribedabove.
Theresultsofevaluatingeach
PhaseNet
ontestsetDcanbefoundintable1.
Wecalculatedthemetrics
accuracy
,
recall
,and
precision
asdenedin[14].The
F
1
scoreistheharmonicmeanofprecisionandrecall.Themetricswereaveraged
overalloperationsinthetestset.Table2presentsthephase-wiseresultsofthe
bestperformingpretrained
PhaseNet
(
1
st
&2
nd
ordercontrastive
)compared
tothe
PhaseNet
thatdidnotundergoself-supervisedpretraining.
4Discussion
Table1clearlyshowsthatallthreepretrainedmodelsoutperformthebaseline
whenbeingonthesamesetoflabeledtrainingdata.Theperformance
6I.Funkeetal.
#OPsP1P2P3P4P5P6P7
Nopre-
training
2064
:
5

35
:
783
:
4

15
:
959
:
0

33
:
480
:
8

14
:
262
:
0

24
:
062
:
8

20
:
262
:
4

18
:
3
4088
:
7

21
:
692
:
3

10
:
875
:
3

29
:
490
:
3

14
:
374
:
0

17
:
671
:
7

19
:
471
:
0

14
:
3
6082
:
4

25
:
394
:
9

5
:
781
:
1

18
:
592
:
0

10
:
976
:
1

15
:
373
:
7

17
:
965
:
4

24
:
6
1
st
&2
nd
order
contrastive
2079
:
3

25
:
792
:
2

7
:
681
:
9

14
:
491
:
3

8
:
072
:
4

17
:
572
:
8

17
:
560
:
0

24
:
4
4087
:
6

15
:
395
:
7

7
:
186
:
3

13
:
391
:
4

18
:
178
:
5

18
:
473
:
2

21
:
771
:
6

18
:
1
6090
:
2

14
:
797
:
6

2
:
789
:
3

9
:
995
:
9

3
:
775
:
4

19
:
376
:
9

18
:
267
:
8

16
:
6
Table2.
Comparisonofthebaselineandthebestperformingpretrainedmodel.We
reporttheaverage
F
1
scorescalculatedforeachofthephasesP1toP7.
boostisespeciallyapparentwhenonly20labeledvideosareavailable.Here,in
termsof
F
1
score,pretrainingachievesanincreasefrom67.8toupto78.6while
halvingthestandarddeviation.Pretrainingstillimprovesperformancewhen
morelabeledvideosareavailable.Notably,thepretrainedmodelson
only40labeledvideosoutperformthebaselinetrainedon60videos.Wecon-
cludethattheproposedSFA-basedpretrainingenablesaCNNtolearnfeature
representationsthatarebtothetaskofsurgicalphasesegmentation.
Comparingthethreepretrainingvariants,wedonotbigs.
Allinall,usingacombinationofandsecondordertemporalcoherencefor
pretrainingseemstothelargestboosttoperformance,especiallywhenonly
few(20)labeledvideosareused.
Lookingattheresultswithrespecttoeachsurgicalphase(table2),wesee
thatmostphasesbgreatlyfrompretraining(variant
1
st
&2
nd
order
contrastive
)whenonly20labeledvideosareavailable.Theofpretraining
diminisheswhenthenumberoflabeledvideosisincreased,butisstillnoticeable
inthemajorityofphases.OnlythebtophaseP7seemsnegligible.
P7containsvisualsimilaritieswithP5andP6,whichmakesthem
todistinguish.Sincethephaseisshort(about1to3min),framesthatwelabel
ascloseduringpretrainingmaybelongtopreviousphases.Likewise,frames
thatbelongtopreviousphasesbutaretemporallyclosearenotselectedas
distantpair.Hence,thenetworklearnsfeaturesthatareratherinvariantthan
discriminativewithregardtophaseP7andP6orP5.
Toshedsomelightonthefeatureslearnedduringpretraining,weinvestigated
whichimagesthenetworkconsiderssimilar.Weselectedqueryframes
f
I
q
g
from
avideousedduringpretraining.Then,foreachframe
I
q
andeachvideo
v
inthe
testset,weidentheframe
I
q;v
in
v
thatismostsimilarto
I
q
,i.e.,closest
to
I
q
infeaturespace.Moreformally,
I
q;v
=argmin
I
t
2
v
D
(
f
(
I
q
)
;f
(
I
t
)),where
DwaschosentobetheL2norm.Tocalculatetheembedding
f
,weusedthe
1
st
&2
nd
ordercontrastive
pretrained
FeatureNet
(before
Figure4presentsfourselectedqueries.Generally,itcanbeseenthatimages
thatarecloseinfeaturespaceshowsimilarsceneswithregardtoanatomical
structuresand/ortoolpresence.Theandsecondqueryframesdepictscenes
thatonlyintheamountofbloodvisible,atraitalsoobservedinthere-
trievedframes.Likewise,thethirdandfourthqueryframesshowsimilarscenes.
However,thethirdqueryframeisunusualasthespecimenbagisclosed.Ob-
servingthattheretrievedimagesaresemanticallynotcloselyrelatedtothe
Temporalcoherence-basedself-supervisedlearningforlap.wwanalysis7
Fig.1.
Imageretrievaltask.Eachrowrepresentsonequery.Left-most:Queryframe.
Right:Theframesclosestinfeaturespace,onepertestvideo.Numbersdenotedistance
toqueryframe.Thedepictedframesaresortedwithregardtothisdistance.
queryframe,weassumethatitsembeddingdoesnotthepresenceofthe
specimenbag.Forthefourthqueryframe,whichisvisuallysimilarbutmore
representative,semanticallysimilarframesareretrieved.
Werefrainfromcomparingtemporalcoherence-basedlearningtootherpre-
trainingmethodsforsurgicalphasesegmentation[2,18]sincethesestudieswere
conductedusingotherdatasets,namelyEndoVis2015(7cholecystectomies)in[2]
and120cholecystectomiesin[18].
5Summary
Inthispaper,weshowthatthetemporalcoherenceofunlabeledlaparoscopic
videocanbeexploitedforself-supervisedpretrainingbytrainingaCNNtomap
temporallyclosevideoframesontoembeddingsthatarecloseinfeaturespace.
WhenextendedintoaCNN-LSTMarchitectureforsurgicalphasesegmentation,
allpretrainedmodelsoutperformthenon-pretrainedbaselinewhenbeing
tunedonthesamelabeleddataset.Usingacombinationofandsecond
ordertemporalcoherence,thepretrainedmodelsevenperformsimilarlyorbetter
thanthebaselinewhenlesslabeleddataisused.Combiningourapproachwith
temporalorder-basedconceptsintoamoreholistictemporalcoherence-based
pretrainingmethodcouldpossiblyenhancethediscriminativepropertiesofthe
learnedembeddingandimproveperformanceevenfurther.
Futureworkwilladdressthequestionwhetherthelearnedembeddingscan
beusedforunsuperviseddetectionofmoree-grainedvideosegments,suchas
8I.Funkeetal.
surgicalactivitiesorsteps.Furthermore,wewillinvestigatewhetherthenotion
ofslowandsteadyfeaturesisbforregularizationduringsupervised
trainingcomparedtousingtheconceptduringaseparatepretrainingphase.
References
1.Aksamentov,I.,Twinanda,A.P.,Mutter,D.,Marescaux,J.,Padoy,N.:Deepneu-
ralnetworkspredictremainingsurgerydurationfromcholecystectomyvideos.In:
MICCAI.pp.586{593.Springer(2017)
2.Bodenstedt,S.,Wagner,M.,Katic,D.,Mietkowski,P.,Mayer,B.,Kenngott,H.,
Muller-Stich,B.,Dillmann,R.,Speidel,S.:Unsupervisedtemporalcontextlearn-
ingusingconvolutionalneuralnetworksforlaparoscopicwwanalysis.arXiv
preprintarXiv:1702.03684(2017)
3.Bromley,J.,Guyon,I.,LeCun,Y.,ackinger,E.,Shah,R.:Signaturev
usinga""siamese""timedelayneuralnetwork.In:NIPS.pp.737{744(1994)
4.Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:ImageNet:Alarge-scale
hierarchicalimagedatabase.In:CVPR.pp.248{255(2009)
5.Goroshin,R.,Bruna,J.,Tompson,J.,Eigen,D.,LeCun,Y.:Unsupervisedlearning
ofspatiotemporallycoherentmetrics.In:ICCV.pp.4086{4093(2015)
6.He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.
In:CVPR.pp.770{778(2016)
7.Hochreiter,S.,Schmidhuber,J.:Longshort-termmemory.Neuralcomputation
9
(8),1735{1780(1997)
8.Jayaraman,D.,Grauman,K.:Slowandsteadyfeatureanalysis:higherordertem-
poralcoherenceinvideo.In:CVPR.pp.3852{3861(2016)
9.Jin,Y.,Dou,Q.,Chen,H.,Yu,L.,Qin,J.,Fu,C.W.,Heng,P.A.:SV-RCNet:
Wwrecognitionfromsurgicalvideosusingrecurrentconvolutionalnetwork.
IEEEtransactionsonmedicalimaging
37
(5),1114{1126(2018)
10.Kingma,D.,Ba,J.:Adam:Amethodforstochasticoptimization.In:ICLR(2015)
11.Lalys,F.,Jannin,P.:Surgicalprocessmodelling:areview.Internationaljournalof
computerassistedradiologyandsurgery
9
(3),495{511(2014)
12.Lee,H.Y.,Huang,J.B.,Singh,M.,Yang,M.H.:Unsupervisedrepresentationlearn-
ingbysortingsequences.In:ICCV.pp.667{676(2017)
13.Misra,I.,Zitnick,C.L.,Hebert,M.:Shandlearn:unsupervisedlearningusing
temporalordervIn:ECCV.pp.527{544.Springer(2016)
14.Padoy,N.,Blum,T.,Ahmadi,S.A.,Feussner,H.,Berger,M.O.,Navab,N.:Statis-
ticalmodelingandrecognitionofsurgicalww.MedicalImageAnalysis
16
(3),
632{641(2012)
15.Twinanda,A.P.,Shehata,S.,Mutter,D.,Marescaux,J.,deMathelin,M.,Padoy,
N.:EndoNet:Adeeparchitectureforrecognitiontasksonlaparoscopicvideos.
IEEETransactionsonmedicalimaging
36
(1),86{97(2017)
16.Wang,X.,Gupta,A.:Unsupervisedlearningofvisualrepresentationsusingvideos.
In:ICCV.pp.2794{2802(2015)
17.Wiskott,L.,Sejnowski,T.J.:Slowfeatureanalysis:Unsupervisedlearningofin-
variances.Neuralcomputation
14
(4),715{770(2002)
18.Yengera,G.,Mutter,D.,Marescaux,J.,Padoy,N.:Lessismore:Surgicalphase
recognitionwithlessannotationsthroughself-supervisedpre-trainingofCNN-
LSTMnetworks.arXivpreprintarXiv:1805.08569(2018)
"
1,BinGAN: Learning Compact Binary Descriptors with a Regularized GAN,http://arxiv.org/pdf/1806.06778v5.pdf,https://github.com/maciejzieba/binGAN,"BinGAN:LearningCompactBinaryDescriptors
withaRegularizedGAN
MaciejZieba
WroclawUniversityof
ScienceandTechnology,Tooploox
maciej.zieba@pwr.edu.pl
PiotrSemberecki
WroclawUniversityof
ScienceandTechnology,Tooploox
piotr.semberecki@pwr.edu.pl
TarekEl-Gaaly
Voyage
tarek@voyage.auto
TomaszTrzcinski
WarsawUniversityofTechnology,
Tooploox
t.trzcinski@ii.pw.edu.pl
Abstract
Inthispaper,weproposeanovelregularizationmethodforGenerativeAdversarial
Networks,whichallowsthemodeltolearndiscriminativeyetcompactbinaryrep-
resentationsofimagepatches(
imagedescriptors
).Weemploythedimensionality
reductionthattakesplaceintheintermediatelayersofthediscriminatornetwork
andtrainbinarizedlow-dimensionalrepresentationofthepenultimatelayerto
mimicthedistributionofthehigher-dimensionalprecedinglayers.Toachievethis,
weintroducetwolosstermsthataimat:(i)reducingthecorrelationbetweenthe
dimensionsofthebinarizedlow-dimensionalrepresentationofthepenultimate
layer(
i
.
e
.maximizingjointentropy)and(ii)propagatingtherelationsbetween
thedimensionsinthehigh-dimensionalspacetothelow-dimensionalspace.We
evaluatetheresultingbinaryimagedescriptorsontwochallengingapplications,
imagematchingandretrieval,andachievestate-of-the-artresults.
1Introduction
Compactbinaryrepresentationsofimagesareinstrumentalforamultitudeofcomputervisionap-
plications,includingimageretrieval,simultaneouslocalizationandmapping,andlarge-scale3D
reconstruction.Typicalapproachestotheproblemoflearningdiscriminativeyetconciserepresen-
tationsincludesupervisedmachinelearningmethodssuchasboosting[
27
],hashing[
8
]and,more
recently,deeplearning[
23
].Althoughunsupervisedmethodshavealsobeenproposed[
14
,
16
,
6
],
theirperformanceistypicallylowerthanthecompetingsupervisedapproaches.
Thegoalofthisworkistobridgethisperformancegapbyusinganintermediatelayerrepresentation
ofaGenerativeAdversarialNetwork(GAN)[
9
]discriminatorasacompactbinaryimagedescriptor.
Recentstudiesshowthepowerfuldiscriminativecapabilitiesoffeaturesextractedfromthediscrimi-
natornetworksofGANs[
19
,
21
].Withagrowingnumberofhiddenunitsinintermediatelayers,the
qualityofvectorrepresentationsincrease,whenappliedtobothimagematchingandretrieval.This
iswhytypicalapproachesmakeuseofhigh-dimensionalintermediaterepresentationstogenerate
imagedescriptors,thereforeleadingtohighmemoryfootprintandcomputationallyexpensivematch-
ing.Weaddressthisshortcomingandbuildlow-dimensionalcompactdescriptorsbytrainingGAN
withanovelDistanceMatchingRegularizer(DMR).Thisregularizerisresponsibleforpropagating
theHammingdistancesbetweenbinaryvectorsinhigh-dimensionalfeaturespacesofintermediate
discriminatorlayerstothecompactfeaturespaceofthelow-dimensionaldeeperlayersinthesame
network.Moreprecisely,ourproposedmethodallowstoregularizetheoutputofanintermediate
32ndConferenceonNeuralInformationProcessingSystems(NIPS2018),Montréal,Canada.
arXiv:1806.06778v5  [cs.CV]  7 Nov 2018layer(withlownumberunits)ofthediscriminatorwiththehelpoftheoutputofpreviouslayers(with
highnumberofunits).Thisisachievedbypropagatingthecorrelationsbetweensamplepairsof
representationsbetweenthelayers.Moreover,tobetterallocatethecapacityofthelow-dimensional
featurerepresentationweextendourmodelwithanadjustedversionoftheBinarizationRepresenta-
tionEntropy(BRE)Regularizer[
5
].Thisregularizationtechniquewasinitiallyappliedtoincreasethe
diversityofintermediatelayersofthediscriminatorbymaximizingthejointentropyofthebinarized
outputsofthelayers.Weadjustthisregularizationtermsothatitconcentratesonincreasingthe
entropyoftheparticularpairsofbinaryvectorsthatarenotcorrelatedinhigh-dimensionalspace.
Asaconsequence,wekeepthebalancebetweenpropagatingtheHammingdistancesbetweenthe
layersforcorrelatedvectorsandincreasingthediversityofthebinaryvectorsinthelow-dimensional
featurespace.
Themaincontributionsofthispaperaretwo-fold.Firstly,webuildapowerfulyetcompactbinary
imagedescriptorusingaGANarchitecture.Secondly,weintroduceanovelregularizationmethod
thatpropagatestheHammingdistancesbetweencorrelatedpairsofvectorsinthehigh-dimensional
featuresofearlierlayerstothelow-dimensionalbinaryrepresentationofdeeperlayersduring
discriminatortraining.Abinaryimagedescriptorresultingfromourapproach,dubbedBinGAN,
outperformsstate-of-the-artmethodsintwochallengingtasks:imagematchingand
imageretrieval.Lastbutnotleast,wereleasethecodeofthemethodalongwiththeevaluationscripts
toenablereproducibleresearch
1
.
2RelatedWork
2.1BinaryDescriptors
Binarylocalfeaturedescriptorshavegainedaamountofattentionfromtheresearch
community,mainlyduetotheircompactnature,efyandmultitudeofapplicationsincomputer
vision[
4
,
13
,
1
,
25
,
28
,
27
,
7
].BRIEF[
4
],thewidelyadoptedbinaryfeaturedescriptor,sparkeda
newdomainofresearchonfeaturedescriptorsthatrelyonasetofhand-craftedintensitycomparisons
thatareusedtogeneratebinarystrings.Severalfollow-upworksproposeddifferentsampling
strategies,e.g.BRISK[
13
]andFREAK[
1
].Althoughtheseapproachesofferunprecedented
computationalefy,theirperformanceishighlysensitivetostandardimagetransformation,
suchasrotationorscaling,aswellasotherviewpointchanges.Toaddressthoselimitations,several
supervisedapproachestolearningbinarylocalfeaturedescriptorsfromthedatahavebeenproposed.
LDAHash[
25
]proposedtotraindiscriminativeprojectionsofSIFT[
17
]descriptorsandbinarize
themafterwardstoobtainahighlyrobustpatchdescriptor.D-Brief[
28
]extendsthisapproachby
increasingtheefyofthedescriptorwithbanksofsimpleelementsusedtoapproximate
theprojections.Tofurtherboosttheperformanceoflearnedbinarydescriptors,BinBoost[
27
]
proposestousegreedyboostingalgorithmfortrainingconsecutivebits,whileRDF[
7
]usesan
alternativegreedyalgorithmtoselectthemostdistinctivereceptiveusedtoconstructdimension
ofthedescriptor.Withthiskindofapproach,itispossibletoobtainmorepowerfuldescriptorsthan
byapplicationofhand-craftedmethods.However,thebinarydescriptorsaretrainedusingpair-wise
learningmethods,whichsubstantiallylimitstheirapplicabilitytonewtasks.
2.2HashingMethods
Ontheotherhand,binarydescriptorscanbelearnedwithhashingalgorithmsthataimatpreserving
originaldistancesbetweenimagesinbinaryspaces,suchasin[2,20,30,8].
LocalitySensitiveHashing(LSH)[
2
]binarizestheinputbythresholdingalow-dimensionalrepre-
sentationgeneratedwithrandomprojections.SemanticHashing(SH)[
20
]achievesthesamegoal
withamulti-layerRestrictedBoltzmannMachine.SpectralHashing(SpeH)[
30
]exploitsspectral
graphpartitioningtocreateefbinarycodes.IterativeQuantization(ITQ)[
8
]usesaniterative
approachtoasetofprojectionsthatminimizethebinarizationloss.Unlikemostrecentdeep
learningapproaches(discussednext),thesehashingalgorithmstypicallyoperateonhand-crafted
imagerepresentations,
e.g.
SIFTdescriptors[
25
],dramaticallyreducingtheireffectivenessand
limitingtheirperformance,ascanbeseenintheresultsofourexperiments.
1
Thecodeisavailableat:
github.com/maciejzieba/binGAN
2
2.3DeepLearningApproaches
Inspiredbythespectacularsuccessofdeepneuralnetworks,severalmethodshavebeenproposedthat
generatebinaryimagedescriptorsusingdeepneuralnetworks[
23
,
26
,
16
,
14
,
6
].Supervisedmethods,
suchas[
23
,
26
],achieveimpressiveresultsbyexploitingdatalabelingandtrainingadvancements
suchasSiamesearchitecture[
23
]orprogressivesampling[
26
].Nevertheless,theiroutstanding
performanceisoftenlimitedtotheoriginaltaskanditischallengingtoapplythemtootherdomains.
Unsuperviseddeeplearningmethods[
16
,
14
,
6
],ontheotherhand,aretypicallylessdom
anddonotrequireanydatalabeling,whichbecomesespeciallyimportantinthedomainswheresuch
labelingishardorimpossibletoobtain,
e.g.
medicalimaging.DeepHashing(DH)[
16
]usesneural
networkstoabinaryrepresentationthatreducesbinarizationlosswhilebalancingbitvalues
tomaximizeitsentropy.Asaninput,however,ittakesanintermediateimagerepresentation,such
astheGISTdescriptor.DeepBit[
14
]addressesthisshortcomingbyusingaconvolutionalneural
networkandfurtherimprovestheresultswithdataaugmentation.However,DeepBitreliesona
rigidsignfunctionwiththresholdatzerotobinarizetheoutputvalues,whichmay
leadtoquantizationlosses.DBD-MQ[
6
]overcomesthislimitationbymappingthis
problemasamulti-quantizationtaskandusingK-AutoEncodersnetworktosolveit.Inthispaper,we
followthislineofresearchandemployadifferentbinarizationtechnique,asin[
4
],togeneratebinary
descriptors.Furthermore,wealsorelyonrecentgenerativemodels,namelyGenerativeAdversarial
Networks[
9
],tobuildimagedescriptors.Inthisregard,ourworkisalsorelatedto[
18
]and[
24
],
whereGANsareusedtoaddressimageretrievalproblem.[
18
]learnsbinaryrepresentationsby
traininganend-to-endnetworktodistinguishsyntheticandrealimages.[
24
]proposestoemploy
GANstoenhancetheintermediaterepresentationofthegenerator.Contrarytoourapproach,however,
bothofthosemethodsusetanh-likeactivationforbinarizationandoptimizetheirperformancetoward
imageretrievaltask,whileourapproachisagnostictoapplicationandcanbeequallysuccessful
whenappliedtolocalfeaturedescriptorlearning,imagematchingorimageretrieval.
3BinGAN
Weproposeanovelapproachforlearningcompactbinarydescriptorsthatexploitsgoodcapabilities
oflearningdiscriminativefeatureswithGANmodels.Inordertoextractbinaryfeatureswemake
useofintermediatelayersofaGAN'sdiscriminator[
19
].Toenforcegoodbinaryrepresentationwe
incorporatetwoadditionallossesintrainingthediscriminator:adistancematchingregularizerthat
forcesthepropagationofdistancesfromhigh-dimensionalspacestothelow-dimensionalcompact
spaceandanadjustedbinarizationrepresentationentropy(BRE)regularizer[
5
]withweighted
correlation.
3.1GAN
ThemainideaofGAN[
9
]isbasedongametheoryandassumestrainingoftwocompetingnetworks:
generator
G
(
z
)
anddiscriminator
D
(
x
)
.ThegoalofGANsistotraingenerator
G
tosamplefrom
thedatadistribution
p
data
(
x
)
bytransformingthevectorofnoise
z
(ofwhich,thepriorisdenotedas
p
z
(
z
)
).Thediscriminator
D
istrainedtodistinguishthesamplesgeneratedby
G
fromthesamples
from
p
data
(
x
)
.Thetrainingproblemformulationisasfollows:
min
G
max
D
V
(
D;G
)=
E
x
˘
p
data
(
x
)
[log(
D
(
x
))]
+
E
z
˘
p
z
(
z
)
[log(1

D
(
G
(
z
)))]
:
(1)
Themodelisusuallytrainedwiththegradient-basedapproachesbytakingminibatchoffakeimages
generatedbytransformingrandomvectorssampledfrom
p
z
(
z
)
viathegeneratorandminibatchof
datasamplesfrom
p
data
(
x
)
.Theyareusedtomaximize
V
(
D;G
)
withrespecttoparametersof
D
by
assumingaconstant
G
,andthenminimizing
V
(
D;G
)
withrespecttoparametersof
G
byassuming
aconstant
D
.
However,toobtainmorediscriminativefeaturesontheintermediatelayerofdiscriminatorand
stabilityoftrainingprocessauthorsof[
21
]recommend,thatgenerator
G
shouldbetrainedusinga
featurematching
procedure.Theobjectivetotrainthegenerator
G
is:
L
G
=
jj
E
x
˘
p
data
(
x
)
f
(
x
)

E
z
˘
p
z
(
z
)
f
(
G
(
z
))
jj
2
2
;
(2)
3
where
f
(
x
)
denotestheintermediatelayerofthediscriminator.Inpracticalimplementationsitis
usuallythelayerjustbefore(
penultimatelayer
).
DespitethefactthatGANsareusedforgeneratingexamplesfromthedatadistribution,they
canbealsousedasfeatureembeddings.Thiswasinitiallydiscussedin[
19
]andfurtherextendedin
[
21
],wheretheauthorsthatbyincorporatingadiscriminatornetwork,inasemi-supervised
setting,theywereabletoobtaincompetitiveresults.Thereareacoupleofinusingadversarial
trainingforfeatureembeddings.First,duringtheadversarialtraining,thegeneratorproducesfake
imageswithincreasingqualityandthediscriminatoristrainedtodistinguishbetweentheseanddata
examples.Duringthisdiscriminativeprocedurethediscriminatorisforcedtotrainmore
featuresthatarecharacteristicforsomeregionsofthefeaturespacethatarestronglyassociatedwith
particularclasses.Second,theadversarialtrainingisdoneinanunsupervisedsettingwithoutthe
needfortediousdataannotation.Third,thefeaturematchingapproach(asin[
21
])thatisappliedto
trainthediscriminatorresultsingeneratingfakeimageswithsimilarfeaturecharacteristics,which
forcesthediscriminatortoextractmorediversefeatures.
Themostrecentapproachesforgeneratingbinaryimagedescriptorsaimatconstructingbinary
vectorsoflowdimensionality.However,itwasshownin[
19
]thatthebestperformingrepresentations
inGANscanbeobtainedfromhigh-dimensionalintermediatelayersofthediscriminator.Therefore,
inthiswork,weaimattransferringtheHammingdistancesfromthehigh-dimensionalspaceof
intermediatelayerstotheirbinarizedrepresentationsoflowdimensionalitytobuildourbinaryimage
descriptors.Tothatend,weproposearegularizationtechniquethatenforcesthistransfer,effectively
leadingtoaconstructionofacompactyetdiscriminativebinarydescriptor.InSec.4.1wethe
layersusedashighandlow-dimensionalrepresentationsforagivennetworkarchitecture.
3.2DistanceMatchingRegularizer
Inthissectionweintroducearegularizationlossfunctionthataimsatpropagatingthecorrelations
betweenpairsofexamplesfromhigh-dimensionalspacetolow-dimensionalrepresentation,whatis
equivalenttopropagatingHammingdistancesbetweentwolayersinthediscriminator.Weachieve
thisgoalbytakingapairofvectorsfromtwointermediatelayersofthesamenetwork(discriminator)
correspondingtotwoexamplesfromadatabatchandenforcingtheirbinarizedoutputstohavesimilar
normalizeddotproducts.
Let
f
(
x
)
and
h
(
x
)
denotethelowandhigh-dimensionalintermediatelayersofdiscriminatorwiththe
numbersofhiddenunitsequal
K
and
M
,respectively.Weassume,thatthenumberofhiddenunitsfor
f
(
x
)
ishigherthanthenumberoftheunitsfor
h
(
x
)
,
M
˛
K
.Thecorrespondingbinary
vectors
b
f
2
1
;
1
g
K
and
b
h
2
1
;
1
g
M
canbeobtainedusingasignfunction:
sign
(
a
)=
a=
j
a
j
.
Themainproblemwiththesignactivationsisthattheyarenotabletopropagatethegradient
backwards.Inordertoovercomethislimitationweusethefollowingquantizationtechniqueasin
[
5
]:
softsign
(
a
)=
a=
(
j
a
j
+

)
,where

isahyperparameterthatisresponsibleforsmoothingthe
sign
(

)
function.Wethevector
s
f
thatiscreatedbyapplyingthe
softsign
(

)
functionto
eachelementof
f
(
x
)
:
s
f;k
=
softsign
(
f
k
(
x
))
.
Hammingdistancebetweentwobinaryvectors,
b
1
and
b
2
canbeexpressedusingadotproduct:
d
H
(
b
1
;
b
2
)=

0
:
5

(
b
T
1
b
2

M
)
.Asaconsequence,distantvectorsarecharacterizedbylow-
valueddotproductsandclosevectorsarecharacterizedbyhighvalues.Consideringthisproperty,
weintroducetheDistanceMatchingRegularizer(DMR)thataimsatpropagatingthegoodcoding
propertiesofvectors
b
h
inhigh-dimensionalspacetothecompactspaceofbinaryvectors
b
f
(representedbytheirsoftproxyvectors
s
f
).WetheDMRinthefollowingmanner:
L
DMR
=
1
N
(
N

1)
N
X
k;j
=1
;k
6
=
j
j
b
T
h;k
b
h;j
M

s
T
f;k
s
f;j
K
j
;
(3)
Intermsofoptimizationprocedureweassumeconstantvaluesofhigh-dimensionalvectors
b
h
andoptimizetheparametersofthediscriminatorwithrespectto
s
f
.TomakeHammingdistances
comparablebetweenthehigh-dimensionalandthelow-dimensionalspaceswenormalizethemby
dividingbythecorrespondingvectordimensions.
4
The
L
DMR
functioncanbeinterpretedastheempiricalexpectedvalueofthelossfunction
l
(
d
h
;d
f
)=2
j
d
h

d
f
j
,where
d
h
isthenormalizedHammingdistanceinhigh-dimensional
spacethatisassumedtobeconstantand
d
f
isthenormalizeddistanceinthelow-dimensionalspace
calculatedonquantizedvectors.
Themotivationbehindusingthiskindofregularizationprocedureisasfollows.Ausualapproach
forlearninginformativeanddiscriminativefeatureembeddingsistotakeintermediatelayersofthe
network,concatenatethemandobtainhigh-dimensionalrepresentationthatprovidesbetterbenchmark
results.However,practicalapplicationssuchasimagematchingrequirebinary,shortandcompact
representationsforsakeofefy.Therefore,theroleof
L
DMR
regularizeristomapthegood
embeddingsfromhigh-dimensionaltothecompactbinaryspace.
3.3AdjustedBinarizationRepresentationEntropyRegularizer
Toincreasethediversityofbinaryvectorsinthelow-dimensionallayerweutilizeBREregularizer.
Itwasinitiallyappliedin[
5
]toguidethediscriminator
D
tobetterallocateitsmodelcapacity,by
encouragingthebinaryactivationpatternsonselectedintermediatelayersof
D
tomaximizethetotal
entropy.Toachievethis,featuresarebinarizedandtheexpectedvalueofeachofthe
binarydimensionsisenforcedtobeequalto
0
:
0
2
Forthatpurposethefollowingregularizerisused:
L
ME
=
1
K
K
X
k
=1
(
s
f;k
)
2
;
(4)
where

s
f;k
areelementsof
s
f
=
1
N
P
N
n
s
f;n
thatrepresenttheaverageof
N
quantizedbinaryvectors
s
f;n
.Topromotetheindependencebetweenthebinaryvariables,alossterm
L
AC
isproposedin[
5
]:
L
AC
=
1
N
(
N

1)
N
X
k;j
=1
;k
6
=
j
j
s
T
f;k

s
f;j
j
K
:
(5)
TheBREregularizerintroducedin[
5
]isasthesumof
L
ME
and
L
AC
losses.Effectively,we
wouldliketoincreasethediversityofthebinaryvectorswhosedotproductisequaltozero,
i.e.
their
distanceisclosertothemiddleoftherange,whileforthosevectorswithdotproductdifferentthen
zerotheimportanceofthediversityislower,henceitcanbedownweighed.Therefore,weproposeto
amendtheformulationoftheBREregularizerandreplace
L
AC
withitsweightedversionas
below:
L
MAC
=
N
X
k;j
=1
;k
6
=
j

k;j
Z
j
s
T
f;k

s
f;j
j
K
;
(6)
whereweights

k;j
areassociatedwithcorrespondingpairs
s
T
f;k
and
s
T
f;j
and
Z
=
P
N
k;j
=1
;k
6
=
j

k;j
isnormalizationconstant.
Itcanbeobservedthat
p
k;j
=

k;j
Z
(for

k;j

0
)constitutethediscretedistributionresponsible
fortakingpairsofvectorsforregularization.Practically,itwasshownin[
5
]that
L
AC
isthe
empiricalestimationof
E
[
b
T
b
0
K
]
,where
b
and
b
0
arezero-meanmultivariateBernoullivectorsthat
areindependent.The
L
MAC
criterioncanbeseenasempiricalestimationof
E
p
k;j
[
b
T
b
0
K
]
wherethe
pairs
b
and
b
0
arebindedbythe
p
k;j
distribution.
Weproposeto

k;j
inthefollowingmanner:

k;j
=exp
(

b
T
h;k
b
h;j
j


M
)
=exp
(

M

2

d
H
(
b
h;k
;
b
h;j
)
j


M
)
;
(7)
where
b
h;k
arebinaryvectorsfromthehigh-dimensionallayerand

isahyperparameterthatcontrols
thevarianceofdistances.Aswementionedbefore,wewouldliketopromotelow-dimensionalvectors
2
Weassume

1
;
1
g
andindependencebetweenthemisenforcedbyminimizingthecorrelation.
5
forregularizationthatarenotstronglycorrelatedinhigh-dimensionalspace
h
,thereforewepropose
thefunction
exp(

a
j

)
thattakesthehighestvaluesfor
a
closeto
0
.Asaconsequence,wepromote
thepairsofvectorsinthecriterion
L
MAC
forwhichdistancesarearound
M=
2
andputlessforceto
thepairsforwhichthedistancesinhigh-dimensionalspacearecloseto
0
and
M
.Whileoptimizing
L
MAC
ineachiterationofgradientmethodweassumethat
b
h;k
areconstantandcalculatedfromthe
h
(
x
)
layerofdiscriminatorbyapplicationofthe
sign
(

)
function.
3.4TrainingBinGAN
WetrainourBinGANmodelinatypicalunsupervisedGANscheme,byalternatingprocedureof
updatingthediscriminator
D
andgenerator
G
.Thediscriminator
D
istrainedusingthefollowing
learningobjective:
L
=
L
D
+

DMR

L
DMR
+

BRE

(
L
ME
+
L
MAC
)
(8)
where

DMR
;
BRE
areregularizationparameters.

DMR
theimpactoftheDMRreg-
ularizationtermand

BRE
theimpactofthetwoBREterms,
L
ME
and
L
MAC
.
L
D
=

E
x
˘
p
data
(
x
)
[log(
D
(
x
))]

E
z
˘
p
z
(
z
)
[log(1

D
(
G
(
z
)))]
isthelossfortrainingthediscriminator.
Thetrainingprocedureisperformedinthestandardmethodologyforthistypeofmodelsassuming
traininggenerator
G
anddiscriminator
D
inalternatingsteps.ThegeneratorinBinGANmodelis
trainedbyminimizingthefeaturematchingcriterionprovidedbyequation(2).Thediscriminatoris
updatedtominimizethelossfunctionthatisbyequation(8).Thealternatingprocedureof
updatinggeneratoranddiscriminatorisrepeatedforeachoftheminibatchesconsideredinthecurrent
epoch.
4Results
Weconductexperimentsontwobenchmarkdatasets,Browngray-scalepatches[
3
]andCIFAR-10
colorimages[
12
].Thesebenchmarksareusedtoevaluatethequalityofourapproachonimage
matchingandimageretrievaltasks,respectively.
4.1ModelArchitectureandParameterSettings
Forbothtasks,weusethesamegeneratorarchitectureandslightofthediscriminator.
Belowweoutlinethemainfeaturesofbothmodelsandtheirparameters.
Fortheimagematchingtaskthediscriminatoriscomposedof7convolutionallayers(3x3kernels,
3layerswith
96
kernelsand4layerswith
128
kernels),twonetwork-in-network(NiN)[
15
]layers
(with
256
and
128
unitsrespectively)anddiscriminativelayer.Forthelow-dimensionalfeaturespace
b
f
wetaketheaverage-pooledNiNlayercomposedof
256
units.Forthehigh-dimensionalspace
b
h
wetakethereshapedoutputofthelastconvolutionallayerthatiscomposedof
9216
units.
Forimageretrievalthediscriminatoriscomposedof:7convolutionallayers(3x3kernels,3layers
with96kernelsand4layerswith
192
kernels),twoNiNlayerswith
192
units,onefully-connected
layerwiththreevariantsof(16,32,64units)anddiscriminativelayer.Forthelow-dimensional
featurespace
b
f
wetakefully-connectedlayer,andforthehigh-dimensionalspace
b
h
wetake
average-pooledlastNiNlayer.
Thereare4hyperparametersinourmethod:

,

andregularizationparameters:

DMR
,

BRE
.Inall
ourexperiments,wetheparametersto:

DMR
=0
:
05
,

BRE
=0
:
01
,

=0
:
001
and

=0
:
5
.
Thevaluesofthehyperparametersweresetaccordingtothefollowingmotivations.Thehy-
perparameter

controlsthesoftnessofthe
sign
(

)
functionandthevaluewassetaccordingto
suggestionsprovidedin[
5
]thereforeadditionaltuningwasnotneeded.Thevalueofascaling
parameter

wassetaccordingtopriorassumptionsbasedontheanalysisoftheimpactof
scalingfactorfortheLaplacedistribution.Wescalethedistancesbythenumberoftheunits
(M),thereforethevalueof

canbeconstantamongvariousapplications.Thevaluesofreg-
ularizationterms

BRE
and

DMR
wereedempiricallyfollowingthemethodologyprovidedin[
6
].
6
Method
16bit32bit64bit
KHM
13.5913.9314.46
SphH
13.9814.5815.38
SpeH
12.5512.4212.56
SH
12.9514.0913.89
PCAH
12.9112.6012.10
LSH
12.5513.7615.07
PCA-ITQ
15.6716.2016.64
DH
16.1716.6216.96
DeepBit
19.4324.8627.73
DBD-MQ
21.5326.5031.85
BinGAN
30.0534.6536.77
Figure1:
(Left)
Performancecomparison(mAP,
%
)ofdifferentunsupervisedhashingalgorithmson
theCIFAR-10dataset.ThistableshowsthemeanAveragePrecision(mAP)oftop1000returned
imageswithrespecttodifferentnumberofhashbits.Wereporttheresultsforallthemethodsexcept
forBinGANafter[
6
].
(Right)
TopretrievedimagematchesfromCIFAR-10datasetforgivenquery
imagesfromtestset-column.
4.2ImageRetrieval
InthisexperimentweuseCIFAR-10datasettoevaluatethequalityofourapproachinimageretrieval.
CIFAR-10datasethas10categoriesandeachofthemiscomposedof6,000pictureswitharesolution
32

32
colorimages.Thewholedatasethas50,000trainingand10,000testingimages.
TocomparethebinarydescriptorgeneratedwithourBinGANmodelwiththecompetingapproaches,
weevaluateseveralunsupervisedstate-of-themethods,suchas:KMH[
10
],SphericalHashing
(SphH)[
11
],PCAH[
29
],SpectralHashing(SpeH)[
30
],SemanticHashing(SH)[
20
],LSH[
2
],
PCT-ITQ[
8
],DeepHashing(DH)[
16
],DeepBit[
14
],deepbinarydescriptorwithmultiquantization
(DBD-MQ)[
6
].ForallmethodsexceptDH,DeepBit,DBD-MQandours,wefollow[
16
]and
computehasheson512-dGISTdescriptors.ThetableinFig.1showstheCIFAR10retrievalresults
basedonthemeanAveragePrecision(mAP)ofthetop1000returnedimageswithrespecttodifferent
bitlengths.Fig.1showstop10imagesretrievedfromadatabaseforgivenqueryimagefromourtest
data.
OurmethodoutperformsDBD-MQmethod,theunsupervisedmethodpreviouslyreportingstate-of-
the-artresultsonthisdataset,for16,32and64bits.Theperformanceimprovementintermsofmean
AveragePrecisionreachesover40%,31%and15%,respectively.Themostperformance
boostcanbeobservedfortheshortestbinarystrings,asthankstothelosstermsintroducedinour
method,weexplicitlymodelthedistributionoftheinformationinalow-dimensionalbinaryspace.
4.3ImageMatching
Toevaluatetheperformanceofourapproachonimagematchingtask,weusetheBrowndataset[
3
]
andtrainbinarylocalfeaturedescriptorsusingourBinGANmethodandcompetingpreviousmethods,
applyingthemethodologydescribedin[
14
].TheBrowndatasetiscomposedofthreesubsetsof
patches:Yosemite,LibertyandNotredame.Theresolutionofthepatchesis
64

64
,althoughwe
subsamplethemto
32

32
toincreasetheprocessingefyandusethemethodtocreatebinary
descriptorsinpractice.Thedataissplitintotrainingandtestsetsaccordingtotheprovidedground
truth,with50,000trainingpairs(25,000matchedand25,000non-matchedpairs)and10,000test
pairs(5,000matched,and5,000non-matchedpairs),respectively.
Tab.1showsthefalsepositiveratesat
95%
truepositives(FPR@95%)forbinarydescriptors
generatedwithourBinGANapproachcomparedwithseveralstate-of-the-artdescriptors.Among
thecomparedapproaches,BoostedSSC[22],BRISK[13],BRIEF[4],DeepBit[14]andDBD-MQ
7
(a)Fakepatches
(b)Truepatches
Figure2:WepresentgenerativecapabilitiesofBinGANmodelfortheLibertydataset.Fakepatches
generatedbythemodelareshowninFig.2aandtruepatchesfromthedatainFig.2b.
Table1:Falsepositiveratesat
95%
truepositives(FPR@95%)obtainedforourBinGANdescriptor
comparedwiththestate-of-the-artbinarydescriptorsonBrowndataset(
%
).Real-valuedSIFT
featuresareprovidedforreference.Wereportalltheresultsfrom[
6
],exceptforL2-Netand
BinGAN.
Train
YosemiteNotreDameLiberty
Average
Test
NotreDameLibertyYosemiteLibertyNotreDameYosemite
FPR@95%
Supervised
LDAHash(16bytes)
51.5849.6652.9549.6651.5852.95
51.40
D-BRIEF(4bytes)
43.9653.3946.2251.3043.1047.29
47.54
BinBoost(8bytes)
14.5421.6718.9620.4916.9022.88
19.24
RFD(50-70bytes)
11.6819.4014.5019.3513.2316.99
15.86
BinaryL2-Net[26](32bytes)
2.516.654.044.011.95.61
4.12
Unsupervised
SIFT(128bytes)
28.0936.2729.1536.2728.0929.15
31.17
BRISK(64bytes)
74.8879.3673.2179.3674.8873.21
75.81
BRIEF(32bytes)
54.5759.1554.9659.1554.5754.96
56.23
DeepBit(32bytes)
29.6034.4163.6832.0626.6657.61
40.67
DBD-MQ(32bytes)
27.2033.1157.2431.10
25.78
57.15
38.59
BinGAN(32bytes)
16.8826.0840.8025.76
27.84
47.64
30.76
Table2:Ablationstudy.Falsepositiveratesat
95%
truepositives(FPR@95%)forthreesettingsof

parameterswhentrainingBinGANforimagematching.Optimizingallthreelosstermsleadstothe
bestperformanceontheBrowndataset.
Train
YosemiteNotreDameLiberty
Average
Test
NotreDameLibertyYosemiteLibertyNotreDameYosemite
FPR@95%

DMR
=

BRE
=0
32.7239.44
39.44
27.9227.2450.48
36.21

DMR
=0

BRE
=
0
:
01
30.1236.2844.2
24.2826.44
51.88
35.53

DMR
=
0
:
05

BRE
=0
24.6826.9640.1627.0027.28
45.28
31.90

DMR
=
0
:
05

BRE
=
0
:
01
16.8826.08
40.8025.7627.8447.64
30.76
[
6
]areunsupervisedbinarydescriptorswhileLDAHash[
25
],D-BRIEF[
28
],BinBoost[
27
]and
RFD[
7
]aresupervised.Thereal-valuedSIFT[
17
]isprovidedforreference.OurBinGANapproach
achievesthelowestFPR@95%valueofallunsupervisedbinarydescriptors.Theimprovementover
thestate-of-the-artcompetitor,DBD-MQ,isespeciallyvisiblewhentestingonYosemite.
Furthermore,weexaminetheofBinGAN'sregularizationtermsontheperformanceof
theresultingbinarydescriptor.Tab.2)showstheresultsofthisablationstudy.Usingbinarized
featuresfromaGANtrainedwithoutanyadditionallosstermsprovidesstate-of-the-artresultsin
termsofaverageFPR@95%.ByaddingDistanceMatchingRegularizer(

DMR
6
=0
)wecanobserve
improvementforalmostalltestingcases.Additionalperformanceboostcanbeobserved
8
(a)Notredame
(b)Yosemite
Figure3:Thesetofrandomlyselectedpatchesfromtheoriginaldata(oddcolumns)andcorrespond-
ingsyntheticallygeneratedpatches(evencolumns)thatareattheclosestHammingdistancetothe
truepatchinthebinarydescriptorspace.
whenaddingtheadjustedBREregularizer.WecanthereforeconcludethattheresultsofourBinGAN
approachcanbeattributedtoacombinationoftworegularizationtermsproposedinthiswork.
4.4GenerativeCapabilitiesofBinGAN
Contrarytopreviousmethodsforlearningbinaryimagedescriptors,ourapproachallowstosyntheti-
callygeneratenewimagepatchesthatcanbethenusedforsemi-supervisedlearning.Fig.2presents
theimagescreatedbyageneratortrainedontheLibertydataset.Fakeandtrueimagesaredifto
differentiate.Additionally,Fig.3presentspatchpairsthatconsistofatruepatchandasynthetically
generatedpatchwiththeclosestHammingdistancetothetruepatchinthebinarydescriptorspace.
Themajorityofgeneratedpatchesarefairlysimilartotheoriginalones,whichcanhintthatthose
patchescanbeusedforsemi-supervisedtrainingofmorepowerfulbinarydescriptors,althoughthis
remainsourfuturework.
5Conclusions
Inthiswork,wepresentedanovelapproachforlearningcompactbinaryimagedescriptorsthat
exploitregularizedGenerativeAdversarialNetworks.TheproposedBinGANarchitectureistrained
withtworegularizationtermsthatenableweightingtheimportanceofdimensionswiththecorrelation
matrixandpropagatethedistancesbetweenhigh-dimensionalandlow-dimensionalspacesofthe
discriminator.Theresultingbinarydescriptorishighlycompactyetdiscriminative,providingstate-
of-the-artresultsontwobenchmarkdatasetsforimagematchingandimageretrieval.
Acknowledgements
ThisresearchwaspartiallysupportedbythePolishNationalScienceCentregrantno.UMO-
2016/21/D/ST6/01946aswellasGoogleSponsorResearchAgreementundertheproject""Ef
visuallocalizationonmobiledevices"".
TheresearchconductedbyMaciejZiebahasbeenpartiallyco-bytheMinistryofScience
andHigherEducation,RepublicofPoland.
WewouldespeciallylinktothankKarolKurach,JanHosang,AdamBielskiandAleksanderHolynski
fortheirvaluableinsightsanddiscussion.
9
References
[1]
A.Alahi,R.Ortiz,andP.Vandergheynst.FREAK:Fastretinakeypoint.In
CVPR
,2012.
[2]
A.AndoniandP.Indyk.Near-optimalhashingalgorithmsforapproximatenearestneighborin
highdimensions.In
FOCS
,2006.
[3]
M.Brown,G.Hua,andS.Winder.Discriminativelearningoflocalimagedescriptors.
TPAMI
,
33(1):43Œ57,2011.
[4]
M.Calonder,V.Lepetit,C.Strecha,andP.Fua.BRIEF:Binaryrobustindependentelementary
features.In
ECCV
,2010.
[5]
Y.Cao,G.W.Ding,K.Y.-C.Lui,andR.Huang.ImprovingGANtrainingviabinarized
representationentropy(BRE)regularization.In
ICLR
,2018.
[6]
Y.Duan,J.Lu,Z.Wang,J.Feng,andJ.Zhou.Learningdeepbinarydescriptorwithmulti-
quantization.In
CVPR
,2017.
[7]
B.Fan,Q.Kong,T.Trzcinski,Z.Wang,C.Pan,andP.Fua.Receptiveselectionforbinary
featuredescription.
TIP
,23(6):2583Œ2595,2014.
[8]
Y.Gong,S.Lazebnik,A.Gordo,andF.Perronnin.Iterativequantization:Aprocrustean
approachtolearningbinarycodesforlarge-scaleimageretrieval.
TPAMI
,35(12):2916Œ2929,
2013.
[9]
I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley,S.Ozair,A.Courville,and
Y.Bengio.Generativeadversarialnets.In
NIPS
,2014.
[10]
K.He,F.Wen,andJ.Sun.K-meanshashing:Anafquantizationmethodfor
learningbinarycompactcodes.In
CVPR
,2013.
[11]
J.-P.Heo,Y.Lee,J.He,S.-F.Chang,andS.-E.Yoon.Sphericalhashing.In
CVPR
,2012.
[12]
A.KrizhevskyandG.Hinton.Learningmultiplelayersoffeaturesfromtinyimages.2009.
[13]
S.Leutenegger,M.Chli,andR.Y.Siegwart.BRISK:Binaryrobustinvariantscalablekeypoints.
In
ICCV
,2011.
[14]
K.Lin,J.Lu,C.-S.Chen,andJ.Zhou.Learningcompactbinarydescriptorswithunsupervised
deepneuralnetworks.In
CVPR
,2016.
[15]
M.Lin,Q.Chen,andS.Yan.Networkinnetwork.
arXivpreprintarXiv:1312.4400
,2013.
[16]
V.E.Liong,J.Lu,G.Wang,P.Moulin,J.Zhou,etal.Deephashingforcompactbinarycodes
learning.In
CVPR
,2015.
[17]
D.G.Lowe.Distinctiveimagefeaturesfromscale-invariantkeypoints.
IJCV
,60(2):91Œ110,
2004.
[18]
Z.Qiu,Y.Pan,T.Yao,andT.Mei.Deepsemantichashingwithgenerativeadversarialnetworks.
In
SIGIR
,2017.
[19]
A.Radford,L.Metz,andS.Chintala.Unsupervisedrepresentationlearningwithdeepconvolu-
tionalgenerativeadversarialnetworks.In
ICLR
,2016.
[20]
R.SalakhutdinovandG.Hinton.Semantichashing.
InternationalJournalofApproximate
Reasoning
,50(7):969Œ978,2009.
[21]
T.Salimans,I.Goodfellow,W.Zaremba,V.Cheung,A.Radford,andX.Chen.Improved
techniquesfortraininggans.In
NIPS
,2016.
[22]
G.Shakhnarovich.
Learningsimilarity
.PhDthesis,MIT,2005.
[23]
E.Simo-Serra,E.Trulls,L.Ferraz,I.Kokkinos,P.Fua,andF.Moreno-Noguer.Discriminative
learningofdeepconvolutionalfeaturepointdescriptors.In
ICCV
,2015.
10
[24]
J.Song,T.He,L.Gao,X.Xu,A.Hanjalic,andH.Shen.Binarygenerativeadversarialnetworks
forimageretrieval.In
AAAI
,2018.
[25]
C.Strecha,A.Bronstein,M.Bronstein,andP.Fua.LDAHash:Improvedmatchingwithsmaller
descriptors.
TPAMI
,34(1):66Œ78,2012.
[26]
Y.Tian,B.Fan,andF.Wu.L2-net:Deeplearningofdiscriminativepatchdescriptorineuclidean
space.In
CVPR
,2017.
[27]
T.Trzcinski,M.Christoudias,P.Fua,andV.Lepetit.Boostingbinarykeypointdescriptors.In
CVPR
,2013.
[28]
T.TrzcinskiandV.Lepetit.Efdiscriminativeprojectionsforcompactbinarydescriptors.
In
ECCV
,2012.
[29]
J.Wang,S.Kumar,andS.-F.Chang.Semi-supervisedhashingforscalableimageretrieval.In
CVPR
,2010.
[30]
Y.Weiss,A.Torralba,andR.Fergus.Spectralhashing.In
NIPS
,2009.
11
"
2,Multiscale Fisher's Independence Test for Multivariate Dependence,https://arxiv.org/pdf/1806.06777v7.pdf,https://github.com/MaStatLab/multiFit,"MultiscaleFisher'sIndependenceTestfor
MultivariateDependence
ShaiGorsky
DepartmentofMathematicsandStatistics,
UniversityofMassachusetts,Amherst
LiMa
DepartmentofStatisticalScience,
DukeUniversity
July8,2021
Abstract
Identifyingdependencyinmultivariatedataisacommoninferencetaskthat
arisesinnumerousapplications.However,existingnonparametricindependencetests
typicallyrequirecomputationthatscalesatleastquadraticallywiththesamplesize,
makingittoapplytheminthepresenceofmassivesamplesizes.Moreover,
resampling(e.g.,permutation)isusuallynecessarytoevaluatethestatisticalsignif-
icanceoftheresultingteststatisticsatsamplesizes,furtherworseningthe
computationalburden.Weintroduceascalable,resampling-freeapproachtotesting
theindependencebetweentworandomvectorsbybreakingdownthetaskintosimple
univariatetestsofindependenceonacollectionof2

2contingencytablesconstructed
throughsequentialdiscretizationofthesamplespace,transformingthe
inferencetaskintoamultipletestingproblemthatcanbecompletedwithalmostlinear
complexitywithrespecttothesamplesize.Toaddressincreasingdimensionality,
weintroduceasequentialadaptiveprocedurethatexploitsthespatial
featuresofdependencystructures.Wederiveatheorythatguarantees
theinferentialvalidityofouradaptiveprocedureatanygivensamplesize.Weshow
thatourapproachcanachievestrongcontrolofthelevelofthetestingprocedure
atanysamplesizewithoutresamplingorasymptoticapproximationandestablish
itslarge-sampleconsistency.Wedemonstratethroughanextensivesimulationstudy
itssubstantialcomputationaladvantageincomparisontoexistingapproacheswhile
achievingrobuststatisticalpowerundervariousdependencyscenarios,andillustrate
howthedivide-and-conquernaturecanbeexploitedtonotjusttestindependencebut
tolearnthenatureoftheunderlyingdependency.Finally,wedemonstratetheuseof
ourmethodthroughanalyzingadatasetfromawcytometryexperiment.
Keywords:
Nonparametricinference,multipletesting,unsupervisedlearning,scalable
inference,massivedata
1
arXiv:1806.06777v7  [stat.ME]  7 Jul 20211Introduction
Testingindependenceandlearningthedependencystructureinmultivariateproblems
hasbeenacentralinferencetasksincetheverybeginningofmodernstatistics,andthe
lasttwodecadeshavewitnessedasurgeofinterestinthisproblemamongstatisticians,
engineers,andcomputerscientists.Avarietyofrentmethodshavebeenproposedfor
testingindependencebetweentworandomvectors.Forexample,SzekelyandRizzo(2009)
generalizetheproduct-momentcovarianceandcorrelationtothedistancecovarianceand
correlation.Bakirovetal.(2006),Fanetal.(2017),andMeintanisandIliopoulos(2008)all
developednonparametrictestsofindependencebasedonthedistancebetweentheempirical
jointcharacteristicfunctionoftherandomvectorsandtheproductofthemarginalempirical
characteristicfunctionsofthetworandomvectors.SzekelyandRizzo(2013)furtherconsider
anasymptoticscenariowiththedimensionalityofthevectorsincreasingtoywhile
keepingthesamplesizeInatvein,Helleretal.(2013)formatestbasedon
univariatetestsofindependencebetweenthedistancesofeachoftherandomvectorsfrom
acentralpoint.Inmachinelearning,aclassofkernel-basedtestshasalsobecomepopular.
Forexample,Grettonetal.(2008)formatestbasedontheeigenspectrumofcovariance
operatorsinareproducingkernelHilbertspaces(RKHS).Morerecently,etal.(2018)
generalizedthisapproachtothemultivariatecasebyembeddingthejointdistribution
intoanRKHS.Weihsetal.(2018)deaclassofmultivariatenonparametricmeasures
whichleadstomultivariateextensionsoftheBergsma-Dassiossigncovariance.Leeetal.
(2019)proposedusingrandomprojectionstoreducemultivariateindependencetestingto
aunivariateproblem,andcompletethelatterusinganensembleapproachcombiningthe
distancecorrelationandabinaryexpansionteststatistic(Zhang,2019).
Theexistingmultivariateindependencetestsgenerallyrequirethecomputationof
statisticsatacomputationalcomplexitythatscalesatleastquadraticallyinthesamplesize,
makingthemimpracticalfordatasetswithsamplesizesgreaterthan,say,tensofthousands
ofobservations.Manyofthesemultivariatemethodsalsorequireresampling{intheform
ofeitherpermutationorbootstrap{toevaluatestatisticalsignThisadditional
computationalburdenmakesapplicationsofthesemethodscomputationallyexpensiveeven
forproblemswithmoderatesamplesizes.Toovercomethesechallenges,someappealto
2
asymptoticapproximations(eitherinlarge
n
orinlarge
p
)(SzekelyandRizzo,2013;
etal.,2018)toderiveproceduresthatwhentheasymptoticconditionsaredonot
requireresampling.However,becauseitishardtojudgewhethersuchconditionsaretrue
inmultivariatesettings,practitionersusuallystillresorttoresamplingtoensurevalidity.
Ascalabletestingstrategyfordatawithmassivesamplesizesshouldideallyachieve(i)
closetolinearcomputationalcomplexityinthesamplesizeand(ii)guarantees
withouttheneedforresamplingorasymptoticapproximation.Weaimtointroducea
frameworkthatachievesthesetwodesiderata.Spy,insteadofcalculatingasingletest
statisticforindependenceallatonce,wetakeamulti-scaledivide-and-conquerapproachthat
breaksapartthenonparametricmultivariatetestofindependenceintosimpleunivariate
independencetestsonacollectionof2

2contingencytablesedbysequentially
discretizingtheoriginalsamplespaceatacascadeofscales.Thisapproachtransformsa
complexnonparametrictestingproblemintoamultipletestingprobleminvolvingsimpletests
thatcanbecarriedouttly.WhilesuchanapproachwaspreviouslyadoptedinMa
andMao(2019)fortestingtheindependencebetweentwoscalarvariables,theincreasing
dimensionalityinthemultivariatesettingmakesabrute-force,exhaustiveapproachas
proposedthereincomputationallyprohibitiveandstatisticallyt.Assuchwe
incorporatedata-adaptivityintotheframeworkandintroduceasequential
adaptivetestingprocedurewhichexploitsthespatialcharacteristicsofdependencystructures
todrasticallyreducethenumberofunivariatetestscompletedintheprocedure.Atthesame
time,wederiveapletheoryshowingthatevenwiththeadditionaladaptivity,
exactinference(intermsofcontrollingthelevelofthetest)canbeachievedatanygiven
samplesizewithoutresortingtoeitherresamplingorlarge-sampleapproximation.
Asidefromtheseproperties,ourapproachalsoenjoysauniquefeatureofpractical
relevance|itsdivide-and-conquernatureallowslearningthestructureoftheunderlying
dependency.Byidentifyingandvisualizingthe2

2tablesonwhichtheunivariate
independencetestreturnsthemostt
p
-values,wecanidentifyusing
MultiFIT
interestingdependencyrelationshipsotherwisehiddenbythemultivariatenatureofthe
samplespaceandthecomplexityofthejointdistribution.
Wecarryoutextensivesimulationstudiesthatexaminethecomputationalscalability
3
andstatisticalpowerofourmethodinavarietyofdependencyscenariosandcompareour
methodtoanumberofstate-of-the-artapproaches.Wedemonstrateanapplicationofour
methodtoadatasetfromawcytometryexperimentwithamassivesamplesize.All
technicalproofsareprovidedintheOnlineSupplementaryMaterialsS1.
2Method
Ourstrategyistotransformnonparametrictestingofmultivariateindependenceintoa
multipletestingprobleminvolvingunivariateindependencetestsonacollectionof2

2
tablesconstructedbysequentiallypartitioningthesamplespace.InSection2.1,westartby
describingtheconstructionofthese2

2tablesandjustifythetestingstrategybyshowing
thattworandomvectorsareindependentifandonlyifunivariateindependenceholdson
allofthe2

2tablessoconstructed.InSection2.2,wepresentadata-adaptivesequential
testingprocedurethatcompletestheunivariatetestsononlyasubsetofthe2

2tables
toaccommodateincreasingdimensionalityoftherandomvectors.FinallyinSection2.3
wederiveatheorythatprovidesguaranteesforthevalidityofourprocedure
atanysamplesizewithoutappealingtoresamplingorasymptoticapproximations,and
establishthelarge-sampleconsistencyforourprocedure.
2.1Multi-scale
2

2
testingformultivariateindependence
Westartbyintroducingsomenotationsthatwillbeusedthroughoutthepaperaswell
assomeconceptsrelatedtonesteddyadicpartitioning(NDP),whichwillbeusedfor
constructingthe2

2tablesonwhichunivariateindependencetestsarecompleted.
Let


X


Y
denotea
D
-dimensionaljointsamplespaceoftworandomvectors
X
and
Y
where
X
and
Y
arerespectivelythemarginalsamplespaceof
X
and
Y
.For
simplicity,weassumethat
X
r
0
;
1
s
D
x
and
Y
r
0
;
1
s
D
y
{thatis,eachmarginalrandom
variableofthetworandomvectorsissupportedon
r
0
;
1
s
.Thiscostsnogeneralityasother
randomvariablescanbemappedontotheunitintervalthroughaCDFtransform.
A
partition
P
onaset
S
isacollectionofdisjointnon-emptysubsetsof
S
whoseunion
is
S
.A
nesteddyadicpartition
(NDP)on
S
isasequenceofpartitions,
P
0
;
P
1
;:::;
P
k
;:::
4
suchthat
P
0
t
S
u
,andforeach
k
¥
1,thesetsin
P
k
arethosegeneratedbydividing
eachsetin
P
k

1
intotwochildren.Forexample,ifweconsideranNDPon
r
0
;
1
s
generated
fromsequantiallydividingsetsintotwohalvesinthemiddleoftheinterval,thenwehave
anNDPsuchthatfor
k
¥
0,
P
k


l

1
2
k
;
l
2
k

l
Pt
1
;:::;
2
k
u
.WerefertothisparticularNDPas
the
canonical
NDP,andnotethat
ﬂ
P
k
generatestheBorel
˙
-algebra.Inthefollowing,we
shallconsideronlyNDPsthatgeneratetheBorel
˙
-algebra.Nowletusassumethateach
dimensionofhasacorrespondingNDP.Forourpurpose,theNDPforeachdimension
canbedistinct,butforeaseofillustrationletusassumethattheyareallthecanonical
NDPson
r
0
;
1
s
.Weconsiderthecross-productsofthesemarginalNDPsoneachdimension,
whichcreatesacascadeofpartitionsonthejointsamplespace.Sp,foranyvector
ofnon-negativeintegers
k
p
k
1
;:::;k
D
qP
N
D
0
,
P
k
1

P
k
D
formsapartitionofThe
elementsofthispartitionarerectanglesoftheform
A

A
1

A
2

A
D
;
with
A
d
P
P
k
d
forall
d

1
;
2
;:::;D
.
Notethatthevector
k
encodesthelevelof
A
intheNDPforeachdimensionofThat
is,
k
d
istheleveloftheNDPon
r
0
;
1
s
towhichthe
d
thmarginof
A
belongs.Fromnow
on,weshallrefertoaset
A
oftheaboveformasa
cuboid
.Werefertothesumofall
k
d
,
r

°
D
d

1
k
d
,asthe
resolution
of
A
.
Figure1
illustratesacuboid
A
ofresolution3ina
3-dimensionalsamplespacewithcanonicalNDPsonthemargins.
Wearenowreadytoconstructthe2

2tablesonwhichtocarryoutunivariatetests
ofindependence.Onecandivideacuboid
A
intofourblocksaccordingtotheNDPalong
anypairofitsmarginswhilekeepingallotherdimensionsintact.Forthedivisioninvolving
dimension
i
of
X
anddimension
j
of
Y
,weuse
A
00
ij
,
A
01
ij
,
A
10
ij
,and
A
11
ij
todenotethesefour
blocks.
Figure2
illustratesadivisiononthecuboiddemonstratedin
Figure1
.
Supposenowthat
F
isthejointsamplingdistributionof
p
X
;
Y
q
,thenforthe2

2
divisionof
A
alongthe
i
thdimensionof
X
and
j
thdimensionof
Y
,wecana
correspondingodds-ratiothatcharacterizesthedependencyin
F
onthe2

2division,

ij
p
A
q
F
p
A
10
ij
q
F
p
A
01
ij
q
F
p
A
00
ij
q
F
p
A
11
ij
q
:
Ani.i.d.samplefrom
F
willgiverisetoa2

2contingencytableformedbythenumberof
5
Figure1:Acuboidofresolution3ina3-dimensionalsamplespaceunderthecanonical
NDP.
Figure2:Thedivisionofthecuboid
A
inFigure1intofourblocksalongdimension1for
X
anddimension1for
Y
.
datapointslyinginthefourblocks
t
n
p
A
00
ij
q
;n
p
A
01
ij
q
;n
p
A
10
ij
q
;n
p
A
11
ij
qu
or
n
p
A
00
ij
q
n
p
A
01
ij
q
n
p
A
10
ij
q
n
p
A
11
ij
q
where
n
p
A
q
representsthenumberofdatapointsin
A
.
6
Onecantestwhether

ij
p
A
q
1basedonthiscontingencytable.Whileseveralstandard
testsareavailablefortestingindependenceona2

2table,weadoptFisher'sexacttest.
AswewillshowinSection2.3,itturnsoutthattheconditionalnatureofFisher'stestplays
acrucialroleinourtheory|itensuresthattheresultingtestingprocedure
obtainsexactvalidityatanysamplesizewithoutresamplingorasymptotics.
Figure3
illustratestwocontingencytablesonwhichFisher'stestisappliedforacuboid
A
.Inthe
following,wewilluse
p
ij
p
A
q
torepresenttheresulting
p
-valuefromthetestonthisparticular
2

2table.
(a)
(b)
Figure3:Illustrationofthetwo2

2contingencytablesonacuboid
A
arisingfroman
i.i.d.sampleinwhichdependencyexistsin
p
X
1
;Y
1
q
.
Howdoestestingthose\local""nulls

ij
p
A
q
1relatetoouroriginal\global""hypothesis
of
X
KK
Y
?Itisobviousthatif
X
KK
Y
thenindependencemusthold|thatis,

ij
p
A
q
1|
forany
A
andanypairof
X
-
Y
margins
i
and
j
.However,thereverseisnotobvious|does
independenceonthese2

2tablesformedunderthemarginalNDPsalsoimplythat
X
and
Y
areindependent?Ifthisisthecase,thenonecantestforindependencebetween
X
and
Y
bytestingwhether

ij
p
A
q
1onthe2

2tables.Thenexttheoremthat
thisisindeedthecase.
Theorem2.1.
X
KK
Y
ifandonlyif

ij
p
A
q
1
forallpairsofdimension
i
of
X
and
dimension
j
of
Y
onallcuboids
A
.
Thistheoremimpliesthatonecaninprincipletestforindependencebetweentworandom
vectors
X
and
Y
byexhaustivelytestingwhetherindependenceholdsoneachofthe2

2
7
tablesconstructedonallcuboidsuptosomemaximumresolution,aimedatidentifying
dependencystructuresuptoacertainlevelofdetail.Thisboilsdowntoamultipletesting
probleminvolvingacollectionof
p
-valuescomputedonallofthe2

2tablesuptothe
maximalresolution.However,suchabrute-forceexhaustivescanisnotpracticalwhenthe
dimensionalitygrows.Ifoneweretoexhaustivelytestindependenceonallpossible2

2
tablesofallcuboidsuptoevenjustamoderateresolution,thenumberoftestsrequired
wouldquicklybecomeprohibitive.Sp,thetotalnumberofteststobecompleted
uptoaresolutionof
R
is
°
R
ˆ

0
D
x

D
y

2
ˆ


ˆ

D

1
D

1

.
Formultivariateproblemsofmorethanahandfulofdimensionsthen,onemustbe
selectiveincarryingouttheunivariatetests.Beyondtheconsiderationofcomputational
practicality,reducingthenumberoftestsisalsodesirableforthesakeofstatisticalperfor-
mance.Everyadditionaltestcomeswithapriceinmultipletestingcontrol,andthusitis
importanttobediscreetinchoosingtheteststocomplete.
2.2MultiFIT:aadaptivetestingprocedure
Giventheaboveconsiderations,weproposeadata-adaptivestrategythatselectsineach
resolutionasubsetoftheavailabletablestotestbasedonthestatisticalevidenceattained
oncoarserresolutions.Inparticular,onlythe\children""oftablesinthepreviousresolution
whose
p
-valuesarebelowapre-spdthresholdareselectedfortesting.
Figure4
provides
anillustration.Supposethatcuboid
A
inresolution
r

p
ij
p
A
q€
p

,somepreset
threshold,thenthefourchildrencuboids,generatedbydividing
A
inthe
i
thorthe
j
th
dimensionsaretestedinresolution
r

1.Thisnetestingprocedureterminates
atamaximalresolution
R
max
orwhennocuboidsatthecurrentresolutionhave
p
-values
passingthethreshold.
Therationalebehindthisstrategyistoexploitthespatialsmoothnessofdependency
structures|when
X
and
Y
aredependent,adjacentandnestedcuboidstendtocontain
empiricalevidenceforthedependencyina\correlated""manner.(Itisworthnotingthathere
the\correlation""correspondstoourassumptionabouttheunderlyingsamplingdistribution
thatitsdependencystructureisspatiallysmooth,notthesamplingbehaviorofthedata
pointsgiventhesamplingdistribution.)Thususingthestatisticalevidenceatcoarser
8
Figure4:Theselectionoftablesfortestingbasedonthestatisticalevidenceontheirparent.
Thetworightchildrencorrespondtodividing
A
alongthemarginthatcorrespondstothe
i
thmarginof
X
,andthetwoleftchildrencorrespondtodividing
A
alongthemarginthat
correspondstothe
j
thmarginof
Y
.Thosefourchildrenaretestedinresolution
r

1if
theirparent
A
inresolution
A
producesa
p
-valuebelowthethreshold
p

.
resolutionstoinformwhichcuboidstotestinresolutionscanleadtoedetection
ofthedependencystructure.
Nextweformallypresenttheadaptivetestingprocedure.Welet
C
p
r
q
denotethe
collectionofcuboidsatresolution
r
onwhichwecarryoutindependencetestsoverallofthe
corresponding
D
x

D
y
2

2tables,oneforeach
p
i;j
q
pairofmargins,where
i
and
j
arethe
indicesforthe
X
and
Y
marginsrespectively.Theprocedureconsistsofthreecomponents:
0.
Initialization
:Let
C
p
0
q
be,andlet
C
p
r
q
H
for1
¤
r
¤
R
max
.
1.
scanning
:For
r

0
;
1
;
2
;:::;R
max
dothefollowing:
1a.
Independencetesting
:ApplyFisher'sexacttestofindependencetothe
D
x

D
y
2

2tablesofeachcuboid
A
P
C
p
r
q
andrecordthe
p
-values.
1b.
Selectionofcuboidstotestforthenextresolution
:When
r
€
R
max
,ifthe
p
i;j
q
-tableforacuboid
A
P
C
p
r
q
hasa
p
-valuemoretthanathreshold
p

,
addto
C
p
r

1
q
thefour
childcuboids
of
A
generatedfromdividing
A
alongthe
i
th
andthe
j
thdimensionsrespectively,eachgeneratingtwochildren.
9
2.
Multipletestingcontrol
:Applyanyvalidmultipletestingcontrolprocedureonthe
entiresetof
p
-valuesgeneratedbythealgorithm,therebycontrollingthelevelofthe
entiretestingprocedureat

.
Althoughthedata-adaptiveselectionin
Step1b
isdesignedtoovercometheexplosive
numberoftestsrequiredwhenthedimensionalityislarge,itisstilloftenfeasibletoapply
exhaustivetestinguptosomeresolution
R

€
R
max
.Inotherwords,onecanteston
allavailablecuboidsuptoresolution
R

,andlettheadaptiveselectionofthecuboidsin
Step1b
kickinforresolutionsbeyond
R

.Inoursoftware,weallowtheusertospecifya
resolution
R

belowwhichexhaustivetestingisadopted.Asmallervaluefor
R

willfavor
thedetectionofmoreglobalsignals,whilealarger
R

willfavorlocalizedsignals.
Inourimplementationofthetestingprocedure,weconsidertwotapproachesfor
achievingthemultipletestingcontrolin
Step2
.
StrategyI.Aholisticapproachtomultipletesting.
Underthisstrategy,oneapplies
multipletestingcontrolprocedureonthe
entiresetof
p
-values
generatedin
Step1
of
MultiFIT
allatonce
,regardlessoftheresolutionofthecorrespondingtable.Simplechoices
ofthemultipletestingdevicesincludeBonferroniareHolmcorrections.
StrategyII.Aresolution-speapproachtomultipletesting.
Underthisstrategy,one
appliesmultipletestingcontrolintwoonthe
p
-valueswithineachresolution
level,producinganintermediate,intra-resolutionlevelforeachresolution,
andtheninthesecondstagefurthercorrecttheseintra-resolution\
p
-values""overallthe
resolutions,whichwillproduceavalid,correctedoverall
p
-valuefortestingtheglobalnull
hypothesisofindependence.Thisstrategyhasthebthatonecannow\allocate""a
levelbudgettoeachresolution,andthusavoidsthepossibilityoflossinpowerdue
tohavingmanymoretablestestedinhighresolutionsthancoarseones.Thismethod
isgenerallymorepowerfulthantheholisticapproachabovefortestingtheglobalnull
hypothesiswhenadependencystructureexistsincoarserresolutions.
Anadditionalboftheresolution-spapproachisthatitcanbeimplemented
withearlystoppingsothatthe
MultiFIT
procedurecanterminateassoonasthereis
tevidenceforrejectingtheglobalnullinthefewresolutionswithoutcontinuing
intotestingonhigherresolutions.Thisispossiblebecauseinthisapproachweboundthe
10
oftablesinerresolutionsonthe(corrected)leveloftestsincoarser
resolutions.Oursoftwareimplementsthisearlystoppingstrategyfortheresolution-sp
approachtomultipletestingwhenHolm'smethodisusedforintra-resolutioncorrection
alongwithBonferroni'smethodforcross-resolutioncorrection.Earlystoppingcanreduce
thetimecomplexityntlyinthepresenceofaglobalsignal(see
Figure6
).
Detailedpseudo-codefortheprocedureisprovidedinSupplementS2.Wecallthis
testingprocedure
MultiFIT
,whichstandsforMulti-scaleFisher'sIndependenceTest.
2.3Finite-samplevalidityandlarge-sampleconsistency
Because
MultiFIT
formulatesthetestofindependenceasamultipletestingproblem,
itsinferentialvalidityrestsonwhetherthe
p
-valuesareindeedvalid,i.e.,thattheyare
stochasticallylargerthanauniformrandomvariableunderthenullhypothesis.Notethat
the
p
-valuesforthecuboidsselectedinthe
MultiFIT
procedurearecomputedaccordingto
the(central)hypergeometricnulldistributiononthe2

2tables.Atglance,these
nulldistributionsappeartoignorethedata-adaptiveselectionofacuboid
A
basedonthe
evidenceinitsancestralcuboids.Assuch,onemaysuspectthattheremightbeaselection
biasthatcausessuch
p
-valuestolosetheirfacevalues.
Thefollowingtheoremandcorollaryresolvethisconcernbyshowingthat,interestingly,
thedistributionofalltheselected2

2tablesgiventheirmarginaltotalsareindependent
oftheeventthattheyareselectedintheprocedure,andhencethe
p
-valuescomputedinthe
procedureareindeedstillvaliddespitetheadaptivesequentialselection.Consequently,one
canindeedcontroltheleveloftheentireprocedureusingmultipletestingmethodsbased
onthese
p
-values.
Theorem2.2.
Underthenullhypothesis
X
KK
Y
,
n
p
A
00
ij
qKK
1
p
A
P
C
p
r
q
q|
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
q
forallcuboids
A
ofresolution
r
andallpairs
p
i;j
q
ofthemargins,where
n
p
A
0

ij
q
n
p
A
00
ij
q
n
p
A
01
ij
q
and
n
p
A

0
ij
q
n
p
A
00
ij
q
n
p
A
10
ij
q
,and
1
p
A
P
C
p
r
q
q
istheindicatorfortheeventthat
A
isselectedtobetestedinthe
MultiFIT
procedure.
11
Inotherwords,foranycuboid
A
,theconditionaldistributionofthe2

2tableon
eachpairof
X
-
Y
marginsgiventhecorrespondingmarginaltotalsisthesamecentral
hypergeometricdistributionwhen
X
KK
Y
whetherornot
weconditionontheeventthat
cuboid
A
isselectedtobetestedinthe
MultiFIT
procedure.Assuch,the
p
-valuesfromthe
Fisher'sexacttestsappliedontheadaptivelyselectedtablesinourprocedurecanbetreated
atfacevalue,whichusingmultipletestingadjustmentbasedonthese
p
-valuesto
controlthelevel.
Corollary2.1.
The
p
-valuescomputedduring
Step1
ofthe
MultiFIT
procedurearevalid,
andthus
Step2
oftheprocedurecancontroltheleveloftheentiretestingprocedureatany
givenlevel

.
Theabovetheoremandcorollaryprovideastrongtheoreticalguarantee|unavailable
tootherexistingmethods|that
MultiFIT
attainsexactcontrolofthelevelatany
samplesize.Thisisanextremelyimportantpropertyinthatformultivariatesample
spacestraditionallarge
n
asymptoticcontrolsofthelevelcanoftenbeinaccurate,and
existingmethodstypicallyappealtoresamplingstrategiessuchaspermutationtoprovide
approximatecontrolofthelevel.Butpermutationisoftencomputationally
prohibitiveinthiscontextinthatevenjustasinglerunofatestcanbeexpensive,notto
mentionapplyingthesametesthundredstothousandsoftimes.Incontrast,
MultiFIT
achievesexactcontrolofthelevelbyasinglerunoftheprocedurewithoutresampling.We
anumericalvalidationoflevelcontrolthroughsimulationsinSectionS4.
WealsonotethattheproofofTheorem2.2turnsouttobeconceptuallyinteresting
andelucidateswhytheadoptionoftheFisher'sexacttestoneach2

2tableiscriticalto
ensuringtheexactsamplevalidityofthe
MultiFIT
procedure.Inparticular,theevent
thatacuboid
A
isselectedtobetestedin
MultiFIT
isinthe
˙
-algebrageneratedbythe
p
-valuesonallofitsancestralcuboids,whichcanbeshowntobeindependentofthecounts
inthe2

2tableon
A
underthenullhypothesisofindependenceoncethecorresponding
marginaltotalsareconditionedupon.ThisindependenceiselucidatedunderaBayesian
networkrepresentationofthemultivariatecentralhypergeometric(CHG)distribution(Ma
andMao,2019,Theorem3).Accordingly,conditioningontheselectionofacuboidunder
MultiFIT
doesnotalterthenulldistributionofthe
p
-valuesforthe2

2tablesonthat
12
cuboid,andthusthevalidityoftheprocedureismaintainedevenwiththeadaptiveselection
ofthetablestoteston.BelowweprovideasketchoftheproofforTheorem2.2forinterested
readersanddeferthetechnicaldetailstoSupplementS1.
SketchofProofforTheorem2.2
:Fortwonon-negativeintegers
a
and
b
,let
n
a;b
denote
a2
a

2
b
contingencytableformedbyacross-productofamarginalpartitionon
X
atdepth
a
andamarginalpartitionon
Y
atdepth
b
.Sp,itisthe2
a

2
b
contingenceytable
correspondingtoapartition
P
k
1

P
k
D
ofwhere
°
D
x
d

1
k
d

a
and
°
D
d

D
x

1
k
d

b
.
Underthenullhypothesisthat
X
KK
Y
,thesamplingdistributionofanysuchtable
n
a;b
givenallofitsrowtotalsandcolumntotalsisamultivariateCHGdistribution.
ByTheorem3inMaandMao(2019),adrawfromthecentralmultivariatehypergeo-
metricdistributionsuchas
n
a;b
canactuallybegeneratedinductivelyfromcoarse-t
resolutionsusingunivariateCHGdistributions.Sp,supposewehavealreadygener-
atedthetable
n
a

1
;b
and
n
a;b

1
,thentheconditionaldistributionof
n
a;b
givenitsrowand
columntotals,aswellasthetwo\parent""tables
n
a

1
;b
and
n
a;b

1
,aresimplyacollection
ofindependentunivariatecentralhypergeometricdistributions|oneforeachadjacent2

2
subtablesin
n
a;b
givenitsrowtotalsandcolumntotals,whichcorrespondtocellcountsin
n
a

1
;b
and
n
a;b

1
.
Let
A
beacuboidthatarisesfromdividingthe
X
marginsatotalof
r
x
timesandthe
Y
marginsatotalof
r
y
times.Theabovereasoningimpliesthatonecanshowbyconstruction
thatforany2

2tableonacuboid
A
,thereexistsaBayesiannetworkintheformpresented
in
Figure5
suchthatthetotalnumberofobservationsin
A
,
n
p
A
q
,isanelementinthe
contingencytable
n
r
x
;r
y
(thenodewithboldblackboundaryinFigure5),thecountsfor
thefourblocksofthe2

2table,
n
p
A
00
ij
q
,
n
p
A
01
ij
q
,
n
p
A
10
ij
q
,and
n
p
A
11
ij
q
,arein
n
r
x

1
;r
y

1
(the
nodewithbluedashedboundaryinFigure5),andthemarginaltotalsof
A
arein
n
r
x

1
;r
y
and
n
r
x
;r
y

1
(thetwonodeswithdottedredboundariesinFigure5).Inaddition,thecounts
ofallofthe2

2tablesonancestorsof
A
aremeasurablewithrespecttothe
˙
-algebra
generatedthegray-shadednodesintheBayesiannetwork,andthusareindependentof
the2

2tableon
A
giventhemarginaltotals.Thereforetheselectionofatabledoes
notthenulldistributiononcethemarginaltotalsareconditionedupon,assuch
conditioningblocksallthepathsfromtheseancestralnodestothebluedashednode.
13
Figure5:ABayesiannetworkaugmentationforthemultivariateCHGmodeloncontingency
tablesformedbycross-productsofsequentialmarginalpartitionson
X
andthoseon
Y
.
Nowthatwehaveestablishedthee-sampleexactvalidityofthe
MultiFIT
procedure,
ourlasttheoreticalresultshowsthatwhenthesamplesize
n
grows,undercertainconditions
MultiFIT
canconsistentlyrejectthenullhypothesisofindependence.
Theorem2.3
(Large-sampleconsistency)
.
Suppose
X
and
Y
arenotindependentunder
theirsamplingdistribution
F
.Let
p
X
1
;
Y
1
q
;
p
X
2
;
Y
2
q
;:::;
p
X
n
;
Y
n
q
bei.i.d.observations
from
F
.As
n
Ñ8
,supposeoneofthefollowingistrue
(i)
R

isdbutlargeenoughsuchthatthereexistsatleastonecuboid
A
ofresolution
r
¤
R

with

ij
p
A
q˘
1
forsomepairsofmargin
p
i;j
q
,
(ii)
R

Ñ8
anditis
o
p
log
n
q
.
Thenthepowerfor
MultiFIT
torejectthenullhypothesisthat
X
KK
Y
convergesto1.
14
2.4Practicalconsiderationsinapplying
MultiFIT
Weclosethissectionbydiscussingsomepracticalaspectsinapplyingthe
MultiFIT
procedure.
Wesetthedefaultvalueforthep-valuethreshold
p

inoursoftwareforresolutionshigher
than
R

at
p
D
x

D
y

log
2
p
n
qq

1
.Thiskeepsthenumberof2

2tablestestedconstant
(onaverage)underthenullhypothesisirrespectiveofthenumberofdimensions,whilealso
makingthethresholdmorestringentwithincreasingsamplesizeinsuchawaythatmakes
thetotalnumberoftablesscalesroughlylinearlywiththesamplesize,whichwewillco
numericallyinthenextsection.
Wenotethatunderthisstrategyofsetting
p

,wegetthatforcertainalternatives,in
particularthosethatarepervasiveoverthesamplespaceandinvolvealargenumberof
cuboids,thecomplexityofthe
MultiFIT
proceduremaybehigherthan
O
p
n
log
n
q
.Such
large-scale,globalalternatives,however,canusuallybedetectedincoarseresolutions,
andthusinpracticewhenthealgorithmisequippedwithearlystoppingitwillinfact
runfasterwithlarger
n
undersuchalternatives.Ifthepractitionerwishestoensurea
strict
O
p
n
log
n
q
boundonthecomputationalcomplexitywithorwithoutincorporating
earlystopping,asimpleapproximateversionofthe
MultiFIT
algorithmcanachievethis.
Spy,in
Step1b
of
MultiFIT
,insteadofincludingchildcuboidsof
all
cuboids
A
withp-valuelessthan
p

,wecanincludeonlychildcuboidswithp-valuelessthan
p

up
toamaximumnumberofcuboids
A
(e.g.,100)withthesmallestFisher'sp-values.This
alternativeconstraintensuresthatthecomputationalcostof
MultiFIT
algorithmisstrictly
boundedat
O
p
n
log
n
q
.Whileunderthisapproximationtheconditionsforensuringthe
guaranteesarenolongerwefoundinpracticethatitsstatistical
power(
Figure8
and
Figure9
)andlevel(
FigureS4
)hardlyfromthoseofthe
exact
MultiFIT
procedureinessentiallyallofthenumericalsettingswehaveencountered.
3NumericalExamples
3.1ComputationalScalability
Becausecomputationalscalabilityisakeymotivationforourapproach,westartbyevalu-
atingthecomputationalscalabilityof
MultiFIT
withthoseofthreeotherstate-of-the-art
15
methodswithwell-documentedsoftware|the(
HHG
)multivariatetest
ofassociationfromHelleretal.(2013),theDistanceCovariance(
DCov
)methodofSzekely
andRizzo(2009),andthekernel-basedmethod(
dHSIC
)ofetal.(2018).
Weapplythesemethodstodatasetssimulatedundersixscenariosdescribedin
TableS1
alongwitha\null""scenariowherethereisnodependence.Herewereporttheresults
fortwoscenariosastheyrepresentthebest-andworst-casecomputationalscenariosfor
MultiFIT
anddefertherestofthescenariosto
FigureS5
intheSupplementaryMaterials.
Thescenariowereportinvolvesdatageneratedunderthenullhypothesis,withall
marginsbeingdrawnindependentlyfromastandardnormaldistribution.Underthesecond
scenario,onedimensionof
Y
isstronglycorrelatedwithadimensionof
X
underthe\linear""
scenariofrom
TableS1
with
l

3.Whileinpracticenon-linearalternativesarethe
mainmotivationforthenonparametrictestsbeingconsideredhere,thelinearscenariois
essentiallytheworst-casescenariofor
MultiFIT
intermsofcomputationaltime.Thereason
isthatthestrongerthedependencyatcoarserlevels,themoretestswillbeperformedunder
MultiFIT
becausemoretestswillpassthe
p
-valuethresholdatcoarserlevels.Assuch,
thesetwoscenariosrepresentthetwoendsofthespectrumintheamountofcomputation
incurredunder
MultiFIT
.
Figure6
plotsthecomputationaltimeversusthesamplesize(inlog-logscale)at
tdimensionalities|2and10.Allmethodswererunonthesamedesktopcomputer
withasingleIntel
®
Core(TM)i7-3770CPUunitat3.40GHz,andthethreecompetitors
wereevaluateduptothemaximumsamplesizeallowedbytheavailable16GRAM.We
presenttheaveragedurationof10executionsofeachmethodundertdimensions,
d

2and
d

10.Itisworthnotingthattheresultsforthecompetitorsareforonlya
singlepermutationwhileatleasthundredsofresamplingrepetitionsarerequiredinorder
toperforminference.
Overall,thecomputationaladvantage
MultiFIT
issubstantial|itscalesapproximately
O
p
n
log
n
q
insamplesize,while
HHG
,
DCov
and
dHSIC
withouttheGammaapproximation
scaleapproximately
O
p
n
2
q
.TheGammaapproximationmethodof
dHSIC
makesthemethod
fasterinthepresenceofastrongsignal,butitstillcannothandlethelargersamplesizesdue
toitsmemoryrequirement.
MultiFIT
withearlystoppingachievedthebestcomputational
16
Figure6:Computationalscalability:acomparisonof
HHG
,
DCov
,
dHSIC
(asinglecomputation
oftheteststatistic)and
MultiFIT
with
D
x

D
y

d
,logruntimeversuslogsamplesize.
MultiFIT
wasrunwith
R


1and
p

p
D
x

D
y

log
2
p
n
qq

1
.Threevariantsof
MultiFIT
areinvestigated:thefullalgorithm,theapproximatealgorithmthatkeepsupto100most
t
p
-valuesateachresolution,andthefullalgorithmwithearlystopping.
MultiFIT
and
dHSIC
withGammaapproximationdoesnotrequirepermutation.Theothermethods
requirepermutationsforlevelcontrolandthereportedtimeisforasinglepermutation
.
atmoderatetolargesamplesizesuniformlyacrossnon-nullscenarios.Asexpected,
earlystoppingdoesnotreducecomputationunderthenull.Theapproximate
MultiFIT
withamaximumnumberofcuboidsperresolutionontheotherhandboundsthecomplexity
by
O
p
n
log
n
q
.
Wedoacknowldgethatthethreecompetitorsscalelinearlyindimensionalitywhile
MultiFIT
scalesquadraticallywiththenumberofdimensions.Assuch
MultiFIT
isnot
suitedforveryhigh-dimensionalproblems.Itismostsuitableforproblemsuptotensof
dimensionswithlargesamplesize.
3.2PowerComparison
Wenextexaminethestatisticalpowerofthecompetingmethodsunderseveralrepresentative
dependencyscenarios.Weconsidertwosetsofsimulationsettings.Inoneset,thedependency
17
existsonlyinasmallnumberofmargins,andthusisamenableto
MultiFIT
'ssearchover
pairsofaxes-alignedboundaries.Intheotherset,thedependencyisspreadoveralarge
numberofdimensionsandthusisparticularlyadversarialto
MultiFIT
.
Inthesetofsimulations,welet
X
1
and
Y
1
beindependentlynormallydistributed,
whereas
X
2
and
Y
2
aredependentaccordingtoseveraltscenarios,whichare
illustratedin
Figure7
inblackpointsintheupperrowofplotsanddetailedin
TableS1
.
MultiFIT
hasanaturaladvantagetodetectsuchmarginaldependenciesasitfocuseson
thetestingofpairsofmargins.
Inthesecondsetofdependencyscenarios,thetruesignalembodiesdependenciesof
the
Y
marginsonmultiple
X
marginsintermsoflinearcombinationsormixtures.This
dissipatesthestrengthofthedependencyovermanypairsofmarginsandthusishighly
unfavorableto
MultiFIT
.Thissetofscenariosisillustratedin
Figure7
ingreenpointsin
thetworowsofplotsanddetailedin
TableS2
and
TableS3
.
Forallscenariosexceptthe\local""scenarios,wesetthelevelofresolutionsuptowhich
exhaustivetestingisdone,
R


2,andforthe\local""scenarios,whereasignalisembedded
inasmallportionofthesamplespace,weset
R


4toensureexhaustivecoverageupto
resolution4.InSectionS6wepresentadetailedsensitivityanalysisontheofthe
tuningparameters
p

and
R

onthepowerofthetestunderthesimulationsettings.
Weperformed500simulationsforeachscenarioandat20erentnoiselevels,and
appliedthefourmethodsatthe5%level.Weappliedaranktransformtoeachofthe
D
marginsforthesimulateddataasthisisthedefaultunder
MultiFIT
andthecompetitors
HHG
,
DCov
and
dHSIC
alsoperformedmuchbetterwiththemarginalranktransform.
Figure8
reportstheresultforthesetofsimulations.
MultiFIT
outperforms
HHG
,
DCov
and
dHSIC
forthe\sine"",\circle"",\checkerboard""and\local""scenarios,thecases
thatarericherwithlocalstructures.Forthemore\global""dependencystructures|\linear""
and\parabolic""|
HHG
and
dHSIC
outperform
MultiFIT
,while
DCov
doessoonlyinthe
\linear""case.Thisisexplainedbythefactthatthesignalisobservablealmostentirelyin
thecoarsestlevel,andaswegointohigherresolutionswemerelyaddttests
thatreducetheoverallpower.Inthesecondsetofsimulations(
Figure9
),asexpected,
MultiFIT
losessomepowerrelativetothecompetitors.Nevertheless,itsoverallperformance
18
Figure7:Visualizationofthedependentmarginsofsixscenarioswithnoiselevel2.Note
thattheblack,\marginal""scenarioisonlyplottedinthetoprowasits
X
1
-
Y
1
marginsdo
notinvolveaninterestingdependency,whereasthe\spread""scenarioisplottedinboth.
Thedependencyinthemarginalscenarioismorenoticeableinthe
X
2
-
Y
2
marginsthanthe
spreadscenario.
Figure8:Powerversusnoiselevelfortmethods.Estimatedpowerat20noiselevels
forthetmethodsunderthesixscenariosfrom
TableS1
.
19
Figure9:Powerversusnoiselevelfortmethods.Estimatedpowerat20noiselevels
forthetmethodsunderthesixscenariosfrom
TableS2
and
TableS3
.
isstillrobustanditstilloutperformsallothermethodsinthe\sine""and\local""spread
scenarios.
Theresultsarelargelyconsistentwithourintuition.Duetoitsdivide-and-conquer
nature,MultiFITisparticularlygoodaidentifyingdependencystructuresthatconcentrates
withinasmallnumberofcuboids(i.e.,localfeatures),whileitspowerisweakerwhenthe
dependencystructureisspreadoveralargenumberofcuboids(i.e.,globalstructures).
Finally,weacknowledgthattheperformanceofsomeofthecompetitors,suchas
HSIC
,
couldbefurtherimprovedwithmoreexpertselectionoftuningparameters.Forexample,
theincorporationofamulti-scalebandwidthinto
HSIC
(LiandYuan,2019)couldfurther
improveitsperformance.
3.3Learningthenatureofthedependency
Sofarwehavefocusedonapplying
MultiFIT
fortestingthenullhypothesisofindependence.
Inpractice,especiallyinmultivariatesettings,thepractitionerisofteninterestedinnotjust
testingtheexistenceofdependencebuttohaveanunderstandingofitsnature.Aby-product
ofthedivide-and-conquerapproachistheabilitytoshedlightontheunderlyingdependency
structure.Inthissectionweprovidetwoexamplesthatillustrate
MultiFIT
'sabilityof
20
learningthenatureofthedependency.Intheexampleweconsideradependency
structureresultingfromhigherorderinteractions.Inthesecondexamplethedependency
consistsoftwosinewavesinthe
p
X
1
;Y
1
q
marginwitherentfrequencies,whileathird
margin,
X
2
,determinesthefrequency.Inbothexamplesitistovisualizethe
dependencyinlow-dimensionalmarginalvisualizations.Weshowthatafteridentifyingthe
2

2tablesthatcontainedstatisticallytevidencefordependency(aftermultiple
testingcorrection),byplottingthedatapointsinthosettables,onecanlearn
andvisualizetheunderlyingdependency.Inbothexamples,weusetheholisticapproachto
multipletestingandadoptHolm'scorrectiononthe
p
-values.
3.3.1Example1:Rotated3DCircle
Let
X
and
Y
eachbeofthreedimensions,andsimulateasamplewith800observations.
Wegeneratea\circle""scenariosothat
X
1
,
Y
1
,
X
2
,and
Y
2
arealli.i.d.standard
normals,whereas
X
3

cos
p

q

,
Y
3

sin
p

q

1
where

and

1
arei.i.dN
p
0
;
p
1
{
10
q
2
q
and


Uniform

ˇ;ˇ
q
.Wethenrotatethecircleby
ˇ
{
4degreesinthe
X
2
-
X
3
-
Y
3
spaceby
applying:





cos
p
ˇ
{
4
q
sin
p
ˇ
{
4
q
0
sin
p
ˇ
{
4
q
cos
p
ˇ
{
4
q
0
001
˝
˚
˚
˚
˛





|||
X
2
X
3
Y
3
|||
˝
˚
˚
˚
˛
:
Therotatedcircleisnolongervisiblebyexaminingthe2-dimensionalmargins.See
Figure10
forthemarginalviewsofthesamplebeforeandaftertherotation.
Figure11
plotsthedatapointsthatlieinthe2

2tablesidenasstatisticalt(at0.001
levelaftermultipletestingadjustmentwithmoHolm'sprocedure)undertherotated
setting.Theunderlyingdependencypatternisclearlyvisibleafterselectingthesetables.
Wefoundthatinvisualizingtheidentables,itisoftenusefultoplotthedatapoints
thatlieinthesamesliceofthattablebutwiththefullrangesoftheplottedmargins,
astheidentableoftencapturesaportionoftheinterestingdependency.
Figure11
demonstratesthistechniquebyplottingthoseadditionalobservations(inorange).Forthis
reason,wehaveincorporatedthisplottingfeatureinoursoftware.
21
(a)
(b)
Figure10:MarginalviewsofthedatasampleinSection3.3(a)beforeand(b)afterrotation.
Thedependencyiseasilyvisibleinthemarginalplotsbeforerotation.Oncerotated,the
signalisspreadamongthemarginsandnolongervisuallyobvious.
Figure11:Scatterplotsfortheobservationsinthethree2

2tablesidenasmost
tby
MultiFIT
fortherotatedcirclescenario.nttablesarethosewith
Holm'sadjusted
p
-valuesbelow0.001.)Thedependencystructureisagainvisibleinthe
marginalviews:redpointsareobservationsthatarewithinthecuboidthatistested,orange
pointsareobservationsthatareinacuboidformedbyexpandingthetestedcuboidsothat
theplottedmarginsarenotsubsetted.Noticehowtheleftplotcapturesthedependencyin
the
X
3
-
Y
3
planewhiletherightplotcapturesthedependencyinthe
X
2
-
Y
3
plane.
3.3.2Example2:MixedSineSignals
Hereweexamine
MultiFIT
'sabilitytodetectadependencystructureconsistingoftwosine
wavesintfrequencies.Let
X
p
X
1
;X
2
q
1
beatwo-dimensionalrandomvectorwith
22
independentmargins
X
1

U
p
0
;
1
q
and
X
2

Beta
p
0
:
3
;
0
:
3
q
,andlet
Y

$
'
&
'
%
sin
p
10

X
1
q

if
X
2
¡
0
:
5
sin
p
40

X
1
q

if
X
2
¤
0
:
5
Figure12
showsasimulateddatasetofsize800.Inthe
p
X
1
;Y
1
q
margin(theleftpanel)
wecanseethesuperimposedsinewaves.
Figure13
showsthreettablesiden
by
MultiFIT
usingthesamecolorcodingtechniquefromthepreviousexampleinwhichwe
canclearlydiscernbetweenthetfrequencywaves.
Figure12:Thetwopairsofmarginsofthesinemixture.Intheleftplotweseethe
superimposedsinesignals,intherightplotthemarginsthatdeterminethemixture.
Figure13:Scatterplotsfortheobservationsinthethree2

2tablesidenasmost
tby
MultiFIT
forthesinemixturescenario.Theredpointsareobservationsthat
arewithinthecuboidthatistested,orangepointsareobservationsthatareinacuboid
formedbyexpandingthetestedcuboidsothattheplottedmarginsarenotsubsetted.
23
4Applicationtoawcytometrydataset
Flowcytometryisthestandardbiologicalassayusedtomeasuresinglecellfeaturesknown
asmarkers,andiscommonlyusedtoquantifytherelativefrequenciesofcellsubsetsin
bloodordisaggregatedtissue.Thesefeaturesmaybegeneralphysical,chemicalorbiological
propertiesofacell.Suchdatainvolvecomplexdistributionalfeaturesandareofmassive
sizeswithtypicalsamplesizesintherangeofhundredsofthousands,whichpresents
computationalchallengestononparametricdataanalyticaltools.
Fortheevaluation,weusedwcytometrysamplesgeneratedbyanantibodypanel
designedtoidentifyactivatedTcellsubsets.Weshowtheresultsofthedependency
analysisonasingleillustrativesamplewith353,586cells.Fortheanalysis,weseparated
themarkersintoavectoroffour`basic'markers(dump,CD3,CD4,CD8)andavector
offour`functional'markers(IFN,TNF,IL-2andCD107).Thebasicmarkersareusedin
practicetoidentifyviableTcellsbyexclusionusingthe`dump'andCD3markers,and
thentofurtherpartitionTcellsintoCD4-positive(`helper')andCD8-positive(`cytotoxic')
subsets.ThefunctionalmarkersareusedtoidentifytheactivationstatusoftheseTcell
subsetsandtheirfunctionalcapabilities(IL-2isaTcellgrowthfactor,IFNand
TNFarecytokines,andCD107isacomponentofthemechanismusedbyT
cellstodirectlykillinfectedandcancercells).
Weapplied
MultiFIT
withHolm'smultipletestingadjustmenttothedatatoidentifying
dependencybetweenthebasicandfunctionalmarkers.Ouraimhereistodemonstrate
MultiFIT
'sabilitytohandlesuchlargedataandtoshedlightontheunderlyingdependency,
andsoweranthetestexhaustivelyuptothemaximalresolutionof4-testing102,416
2

2tables.Theexecutiontimeofthealgorithminthissettingisapproximately5minutes
onalaptopcomputerutilizingfour3.00GHzIntel
®
Xeon(R)E3-1505Mv6CPUcores.
Asthesamplesizeisverylargeandthedataclearlyhavestrongmarginaldependencies,
MultiFIT
idenhundredsofttestsaftermultipletestingadjustment.Interested
readerscanrunourcodeforthisexampleintheSupplementaryMaterialstovisualizethe
idendependencestructures.Noneof
HHG
,
DCov
and
dHSIC
wasabletohandlethis
amountofdataandallendedinovwerrors.
Figure14
presentsthevisualizationof
theobservationsinthe202

2tablewiththemostt
p
-valuesusingthestrategy
24
describedinSection3.3.
5Conclusion
Wehavepresentedascalableframeworkcalled
MultiFIT
fornonparametricallytesting
theindependenceofrandomvectorsthatachieveshighcomputationalscalability,decent
statisticalpower,andtheabilitytoshedlightontheunderlyingdependency.Weprovidea
theoreticalguaranteethat
MultiFIT
controlsthelevelexactlyatany
samplesizewithoutresortingtoresamplingorasymptoticapproximation.Theproposed
approachismostsuitableformultivariateproblemsuptotensofdimensions,canscaleup
tomassivesamplesizes,andthuscanbeusefulformanymoderndataanalyses.Wehave
publishedanRpackagecalled
MultiFit
onCRANthatimplementstheproposedmethod.
6Software
Forthe
MultiFIT
procedureweusedourRpackage
MultiFit
onCRAN.FortheHeller-
(
HHG
)test(Helleretal.,2013)weusedthe
HHG
packageonCRAN.For
DistanceCovariance(
DCov
)(SzekelyandRizzo,2009)weusedthe
energy
packageon
CRAN.For
dHSIC
etal.,2018)weusedthe
dHSIC
packageonCRAN.
7Acknowledgments
Theauthorswishtothanktheeditor,anAE,andtworefereesfortheirvaluablecomments
andsuggestions.LM'sresearchispartlysupportedbyNSFgrantsDMS-1749789and
DMS-2013930.ThewcytometrydataiscollectedthroughanEQAPOLcollaboration
withfederalfundsfromtheNationalInstituteofAllergyandInfectiousDiseases,National
InstitutesofHealth,ContractNumberHHSN272201700061C.
25
Figure14:Scatterplotsoftheobservationsidenbythe202

2tableswiththemost
t
p
-valuesforthewcytometrydataset.Redindicatesobservationsinthetested
cuboid.Orangeindicatesobservationsinthesamesliceofthesamplespace,determined
bythefourmarkersotherthanthetwomarginsplotted.Grayindicatestherestofthe
observations.
26
References
Agresti,A.andGottard,A.(2007).Nonconservativeexactsmall-sampleinferencefor
discretedata.
ComputationalStatistics&DataAnalysis
,51(12):6447{6458.
Bakirov,N.K.,Rizzo,M.L.,andSzekely,G.J.(2006).Amultivariatenonparametrictest
ofindependence.
JournalofMultivariateAnalysis
,97:1742{1756.
Fan,Y.,deMicheaux,P.L.,Penev,S.,andSalopek,D.(2017).Multivariatenonparametric
testofindependence.
JournalofMultivariateAnalysis
,153:189{210.
Gretton,A.,Fukumizu,K.,Teo,C.H.,Song,L.,Scolkopf,B.,andSmola,A.J.(2008).A
kernelstatisticaltestofindependence.InPlatt,J.C.,Koller,D.,Singer,Y.,andRoweis,
S.T.,editors,
AdvancesinNeuralInformationProcessingSystems20
,pages585{592.
CurranAssociates,Inc.
Heller,R.,Heller,Y.,andM.(2013).Aconsistentmultivariatetestofassociation
basedonranksofdistances.
Biometrika
,100:503{510.
Lee,D.,Zhang,K.,andKosorok,M.R.(2019).TestingIndependencewiththeBinary
ExpansionRandomizedEnsembleTest.
arXive-prints
,pagearXiv:1912.03662.
Li,T.andYuan,M.(2019).Ontheoptimalityofgaussiankernelbasednonparametric
testsagainstsmoothalternatives.
Ma,L.andMao,J.(2019).Fisherexactscanningfordependency.
JournaloftheAmerican
StatisticalAssociation
,114(525):245{258.
Meintanis,S.G.andIliopoulos,G.(2008).Fouriermethodsfortestingmultivariate
independence.
ComputationalStatistics&DataAnalysis
,52:1884{1895.
N.,Buhlmann,P.,Scolkopf,B.,andPeters,J.(2018).Kernel-basedtestsforjoint
independence.
JournaloftheRoyalStatisticalSociety:SeriesB(StatisticalMethodology)
,
80(1):5{31.
Szekely,G.J.andRizzo,M.L.(2009).Browniandistancecovariance.
TheAnnalsof
AppliedStatistics
,3(4):1236{1265.
27
Szekely,G.J.andRizzo,M.L.(2013).Thedistancecorrelationt-testofindependencein
highdimension.
JournalofMultivariateAnalysis
,117:193{213.
Weihs,L.,Drton,M.,andMeinshausen,N.(2018).Symmetricrankcovariances:a
generalizedframeworkfornonparametricmeasuresofdependence.
Biometrika
,105(3):547{
562.
Zhang,K.(2019).BETonindependence.
JournaloftheAmericanStatisticalAssociation
.
Accepted.
Zhou,Q.(2019).Asymptoticsofmultivariatecontingencytableswithmarginals.
JournalofStatisticalPlanningandInference
,198:165{170.
28
SupplementaryMaterial
S1Technicalproofs
Inthissection,weestablishproofsforTheorem2.1,Theorem2.2,andCorollary2.1inthe
mainpaper.Wewillintroduceanumberofdandlemmasalongthewaythatwill
beusedtocompletetheproofs.
S1.1.
Level-
k
CanonicalMarginalPartition
:
P
k
isa
level-
k
canonicalmarginalpartition
if
P
k


l

1
2
k
;
l
2
k

l
Pt
1
;:::;
2
k
u
:
Tosimplifythenotationsintheproofs,welet
Z
p
X
;
Y
q
.Moresp,weset
Z
1
:

X
1
;:::;Z
D
x
:

X
D
x
and
Z
D
x

1
:

Y
1
;:::;Z
D
:

Y
D
y
.Thus,
Z
isarandomvectorthat
distributedaccordingtothedistribution
F
,thejointsamplingdistributionof
p
X
;
Y
q
.
For
k
p
k
1
;:::;k
D
qP
N
D
0
,werefertothepartition
P
k
1

P
k
D
asthe
k
-stratum
of
Denotenowany
k
-stratum
as
A
k
.AsdescribedinSection2,weform
D
-dimensionalcuboids
bytakingtheCartesianproductofoneintervalfromeachofthe
D
canonicalmarginal
partitionsofGiventhen
k
p
k
1
;:::;k
D
qP
N
D
0
aspcuboidisdeterminedbysome
l
p
l
1
;:::;l
D
q
witheach1
¤
l
d
¤
2
k
d
suchthat
A

‚
d
Pt
1
;::;D
u

l
d

1
2
k
d
;
l
d
2
k
d

.
FigureS1
illustratestheseionsinathreedimensionalspace.
WenextadiscretizedformofindependencewhichwewillshowinLemmaS1.1
fullycharacterizesthemultivariateindependence:
S1.2.
k
-independence
:
Forany
A
P
A
k
,wecanwrite
A

A
x

A
y
where
A
x

D
x
¡
d

1

l
d

1
2
k
d
;
l
d
2
k
d

and
A
y

D
¡
d

D
x

1

l
d

1
2
k
d
;
l
d
2
k
d

:
(
FigureS2
illustratestheabovenotationsinthreedimensionalspace.)
Wesaythat
X
and
Y
are
k
-independent
andwriteitas
X
KK
k
Y
ifforany
A
P
A
k
,
P
p
X
P
A
x
;
Y
P
A
y
q
P
p
X
P
A
x
q
P
p
Y
P
A
y
q
:
29
FigureS1:Cuboidandstratum.
A3Dviewofa
k
-stratumandthecuboid
A
where
D

3and
k
p
1
;
0
;
2
q
.Thethin
bluelinesdelineateall
p
1
;
0
;
2
q
-cuboids,thatis,thestratum
A
p
1
;
0
;
2
q

P
1

P
0

P
2
.The
thickbluelinesdelineatethecuboid
A
forwhich
l
p
2
;
1
;
2
q
,i.e.,
A

A
1

A
2

A
3
where
A
1


2

1
2
1
;
2
2
1

r
0
:
5
;
1
qP
P
1
,
A
2


1

1
2
0
;
1
2
0

r
0
;
1
qP
P
0
,and
A
3


2

1
2
2
;
2
2
2


r
0
:
25
;
0
:
5
qP
P
2
.Theresolutionofthisstratumis3

1

0

2.
FigureS2:
A
x
and
A
y
.
A3Dviewofa
k
-stratumandthecuboid
A
where
D

3,
k
p
1
;
0
;
2
q
and
l
p
2
;
1
;
2
q
,the
sameasin
FigureS1
.Here
D
x

2with
X
1

Z
1
and
X
2

Z
2
;
D
y

1with
Y
1

Z
3
.The
cuboid
A
isrepresentednowas
A

A
x

A
y
where
A
x

A
1

A
2


2

1
2
1
;
2
2
1



1

1
2
0
;
1
2
0


r
0
:
5
;
1
qr
0
;
1
q
,and
A
y

A
3


2

1
2
2
;
2
2
2

r
0
:
25
;
0
:
5
q
.
30
S1.3.
p
i;j
q
-blocksofacuboid
:
Foreverycuboid
A
P
A
k
,
i
Pt
1
;::;D
x
u
and
j
Pt
1
;::;D
y
u
,onecanpartition
A
intofour
blocksbydividing
A
inthe
p
i;j
q
thface(thatis,thesideof
A
spannedbythe
i
thand
j
th
dimensions)whilekeepingtheotherdimensionsintact.
A

A
00
ij
Y
A
01
ij
Y
A
10
ij
Y
A
11
ij
;
wherefor
a;b
Pt
0
;
1
u
,
A
ab
ij

D
¡
d

1
$
'
'
'
'
&
'
'
'
'
%

2
l
d

2

a
2
k
d

1
;
2
l
d

1

a
2
k
d

1

if
d

i

2
l
d

2

b
2
k
d

1
;
2
l
d

1

b
2
k
d

1

if
d

D
x

j

l
d

1
2
k
d
;
l
d
2
k
d

if
d
Pt
1
;::;D
uzt
i;D
x

j
u
:
FigureS3
illustratesS1.3inthreedimensions.
FigureS3:(
i

1,
j

1)blocksof
A
.
A3Dviewofthe
k
-cuboid
A
where
D

3,
k
p
1
;
0
;
2
q
and
l
p
2
;
1
;
2
q
,thesameasin
FiguresS1
and
S2
.Notethatthe
i

1dimensionof
X
correspondstodimension1of
Z
the
j

1dimensionof
Y
correspondstodimension3of
Z
.Theblocksare:
A
00
11


2

2

2

0
2
1

1
;
2

2

1

0
2
1

1



1

1
2
0
;
1
2
0



2

2

2

0
2
2

1
;
2

2

1

0
2
2

1

r
0
:
5
;
0
:
75
qr
0
;
1
qr
0
:
25
;
0
:
375
q
,
A
01
11


2

2

2

0
2
1

1
;
2

2

1

0
2
1

1



1

1
2
0
;
1
2
0



2

2

2

1
2
2

1
;
2

2

1

1
2
2

1

r
0
:
5
;
0
:
75
qr
0
;
1
qr
0
:
375
;
0
:
5
q
,
A
10
11


2

2

2

1
2
1

1
;
2

2

1

1
2
1

1



1

1
2
0
;
1
2
0



2

2

2

0
2
2

1
;
2

2

1

0
2
2

1

r
0
:
75
;
1
qr
0
;
1
qr
0
:
25
;
0
:
375
q
,
A
11
11


2

2

2

1
2
1

1
;
2

2

1

1
2
1

1



1

1
2
0
;
1
2
0



2

2

2

1
2
2

1
;
2

2

1

1
2
2

1

r
0
:
75
;
1
qr
0
;
1
qr
0
:
375
;
0
:
5
q
.
LemmaS1.1establishesanequivalencebetweenmultivariateindependenceandacascade
ofdiscretizedmultivariateindependencerelations:
31
LemmaS1.1.
X
KK
Y
ô
X
KK
k
Y
forall
k
P
N
D
0
:
Proof.
ñ
:Immediate.
ð
:
Let
P
x

ﬂ
k
t
1
;:::D
x
u
P
N
D
x
0
P
1

:::
P
D
x
.Therefore
˙
p
P
x
q
B
pr
0
;
1
s
D
x
q
.
For
A
x
P
B
pr
0
;
1
s
D
x
q
let
r
X
P
A
x
st
!
:
X
p
!
qP
A
x
u
,then
˙
p
X
qtr
X
P
A
x
s
;A
x
P
B
pr
0
;
1
s
D
x
qu
.
Let
Q
x

ﬂ
k
t
1
;:::D
x
u

1
;
0
;
1
;
2
;:::
u
D
x
Q
k
1

:::

Q
k
D
x
sothat
Q

1
:
H
and
@
k
¥
0,
Q
k

P
k
.
Let
C
X
tr
X
P
B
x
s
;B
x
P
Q
x
u
.
Hence
˙
p
C
X
q
˙
p
X

1
p
B
x
q
;B
x
P
Q
x
q
˙
p
X

1
p
Q
x
qq
X

1
p
˙
p
Q
x
qq
X

1
p
˙
p
P
x
qq
˙
p
X
q
.
Notethat
C
X
isa
ˇ
-system:
E;E
1
P
C
X
ñ
E


X
P

H
or

l
1

1
2
k
1
;
l
1
2
k
1


:::


H
or

l
D
x

1
2
k
D
x
;
l
D
x
2
k
D
x

and
E
1


X
P

H
or

l
1
1

1
2
k
1
1
;
l
1
1
2
k
1
1


:::


H
or

l
1
D
x

1
2
k
1
D
x
;
l
1
D
x
2
k
1
D
x

forsome
l
;
l
1
;
k
;
k
1
.There-
fore:
E
X
E
1


X
P

H
or

l
1

1
2
k
1
;
l
1
2
k
1


:::


H
or

l
D
x

1
2
k
D
x
;
l
D
x
2
k
D
x

£

H
or

l
1
1

1
2
k
1
1
;
l
1
1
2
k
1
1


:::


H
or

l
1
D
x

1
2
k
1
D
x
;
l
1
D
x
2
k
1
D
x



X
P

H
or

l
1

1
2
k
1
;
l
1
2
k
1

X

l
1
1

1
2
k
1
1
;
l
1
1
2
k
1
1


:::


l
D
x

1
2
k
D
x
;
l
D
x
2
k
D
x

X

l
1
D
x

1
2
k
1
D
x
;
l
1
D
x
2
k
1
D
x

P
C
X
Similarly,
˙
p
Y
q
and
C
Y
,another
ˇ
-system,independentof
C
X
and
˙
p
Y
q
˙
p
C
Y
q
.
Bythebasiccriterion,then,
˙
p
X
qKK
˙
p
Y
q
.
Thefollowinglemmashowsthatif
X
and
Y
are
k
-independent,theyarealsoindependent
onallcoarserstrata.
LemmaS1.2.
If
X
KK
k
Y
then
X
KK
k
1
Y
forall
k
1
¤
k
.
Proof.
Let
F
X
betheprobabilitydistributionof
X
and
F
Y
betheprobabilitydistribution
of
Y
.Then
X
KK
k
Y
ô
F
p
A
q
F
X
p
A
x
q
F
Y
p
A
y
q
forall
A
P
A
k
.
32
Itisenoughtoshowthat
X
KK
k
Y
ñ
X
KK
k
1
Y
forany
k
1
suchthat(i)
k
1
i

k
i

1for
some
i
Pt
1
;:::;D
x
u
and(ii)
k
1
d

k
d
forall
d
Pt
1
;::;D
uzt
i
u
.
Hence,assuming
X
KK
k
Y
wegetfor
A
P
A
k
1
that:
F
p
A
q
F

A
1

:::

A
i

1

A
0
i

A
i

1

:::

A
D
x

A
y


F

A
1

:::

A
i

1

A
1
i

A
i

1

:::

A
D
x

A
y


F
X

A
1

:::

A
i

1

A
0
i

A
i

1

:::

A
D
x

F
Y
p
A
y
q

F
X

A
1

:::

A
i

1

A
1
i

A
i

1

:::

A
D
x

F
Y
p
A
y
q

F
X
p
A
x
q
F
Y
p
A
y
q
Asrequired.
InLemmaS1.6wewillshowhowtocharacterizethemultivariate
k
-independencewith
acollectionofunivariate
p
i;j
q
odds-ratios.However,beforewestateandproveLemmaS1.6
wedevelopadditionalnotationsandprovidediscretizedversionsofsomebasicresultsin
probability.Denote:
d
•t
1
;::;D
u
,
i

d
Xt
1
;::;D
x
u
and
j

t
j
:
j

D
x
P
d
Xt
D
x

1
;:::;D
u
u
And
X
i
t
X
i
:
i
P
i
u
;
X
p
i
q
t
X
i
1
:
i
1
Pt
1
;::;D
x
uz
i
u
Y
j
t
Y
j
:
j
P
j
u
;
Y
p
j
q
t
Y
j
1
:
j
1
Pt
1
;::;D
y
uz
j
u
Z
d

X
i

Y
j
t
Z
d
:
d
P
d
u
;
Z
p
d
q

X
p
i
q

Y
p
j
q
t
Z
d
1
:
d
1
Pt
1
;::;D
uz
d
u
A
x;
d

¡
d
P
d
Xt
1
;::;D
x
u
A
d
;A
x;
p
d
q

¡
d
1
Pt
1
;::;D
x
uz
d
A
d
1
A
y;
d

¡
d
P
d
Xt
D
x

1
;::;D
u
A
d
;A
y;
p
d
q

¡
d
1
Pt
D
x

1
;::;D
uz
d
A
d
1
A
d

A
x;
d

A
y;
d

¡
d
P
d
A
d
;A
p
d
q

A
x;
p
d
q

A
y;
p
d
q

¡
d
1
Pt
1
;::;D
uz
d
A
d
1
k
d
t
k
d
:
d
P
d
u
;
k
p
d
q
t
k
d
1
:
d
1
Pt
1
;::;D
uz
d
u
S1.4.
Conditional
k
-independence
:
Wesaythat
X
i
and
Y
j
are
k
-independentconditionalon
Z
p
d
q
andwriteitas
X
i
KK
k
Y
j
|
33
Z
p
d
q
ifforany
A
P
A
k
:
P
p
X
i
P
A
x;
d
;
Y
j
P
A
y;
d
|
Z
p
d
q
P
A
p
d
q
q
P
p
X
i
P
A
x;
d
|
Z
p
d
q
P
A
p
d
q
q
P
p
Y
j
P
A
y;
d
|
Z
p
d
q
P
A
p
d
q
q
Orequivalently:
P
p
X
i
P
A
x;
d
|
Y
j
P
A
y;
d
;
Z
p
d
q
P
A
p
d
q
q
P
p
X
i
P
A
x;
d
|
Z
p
d
q
P
A
p
d
q
q
When
k
d
1

0forsome
d
1
Pt
1
;::;D
uz
d
,
Z
d
1
r
0
;
1
s
forthoseindicesandhenceour
notationmaybecompacted.Forexample,if
k
d
1

0forall
d
1
Pt
1
;::;D
uz
d
:
X
i
KK
k
Y
j
|
Z
p
d
q
ô
X
i
KK
k
d
Y
j
Wenextprovideadiscretizedversionofsomebasicresultsinprobability:
LemmaS1.3.
Contraction:
For
d
•t
1
;::;D
u
suchthat
t
1
;::;D
x
u•
d
(i.e.
i
t
1
;::;D
x
u
,
X
i

X
and
Y
p
j
q

Z
p
d
q
):
$
'
&
'
%
X
KK
k
Y
j
|
Z
p
d
q
X
KK
k
p
d
q
Y
p
j
q
ñ
X
KK
k
Y
Proof.
Immediatefromde
LemmaS1.4.
Decomposition:
For
d
•t
1
;::;D
u
suchthat
t
1
;::;D
x
u•
d
(i.e.
i
t
1
;::;D
x
u
and
X
i

X
):
X
KK
k
Y
ñ
$
'
&
'
%
X
KK
k
d
Y
j
X
KK
k
p
d
q
Y
p
j
q
Proof.
Immediatefromde
LemmaS1.5.
WeakUnion:
For
d
•t
1
;::;D
u
suchthat
t
1
;::;D
x
u•
d
(i.e.
i
t
1
;::;D
x
u
,
X
i

X
,
Y
j

Z
d
and
Y
p
j
q

Z
p
d
q
):
X
KK
k
Y
ñ
$
'
&
'
%
X
KK
k
Y
j
|
Z
p
d
q
X
KK
k
Y
p
j
q
|
Z
d
34
Proof.
Immediatefromde
S1.5.
For
A
d
P
P
k
d

1
suchthat
A
d


l
d

1
2
k
d

1
;
l
d
2
k
d

1

,e
A
0
d


2
l
d

2
2
k
d
;
2
l
d

1
2
k
d

1

P
P
k
d
and
A
1
d


2
l
d

1
2
k
d

1
;
2
l
d
2
k
d

1

P
P
k
d
.
S1.6.
Given
k
P
N
D
0
and
i
Pt
1
;::;D
x
u
j
Pt
1
;::;D
y
u
,let
k
r
i;j
s
 
k
1
P
N
D
0
:
k
1
i
€
k
i
;k
1
D
x

j
€
k
D
x

j
and
k
1
d
¤
k
d
forall
d
Pt
1
;::;D
uzt
i;D
x

j
u
(
and
A
k
r
i;j
s

¤
k
1
P
k
r
i;j
s
A
k
1
:
S1.7.
Given
k
P
N
D
0
and
i
Pt
1
;::;D
x
u
,
j
Pt
1
;::;D
y
u
let
r
k
i
;k
j
sp
k
€
i;
€
j
qt
k
1
P
N
D
0
:
k
1
i
€
k
i
;k
1
D
x

j
€
k
D
x

j
and
k
1
d

k
d
forall
d
Pt
1
;:::;i

1
uYt
D
x

1
;:::;D
x

j

1
u
and
k
1
d

0forall
d
Pt
i

1
;:::;D
x
uYt
D
x

j

1
;:::;D
uu
Accordingly,let
A
r
k
i
;k
j
sp
k
€
i;
€
j
q

¤
k
1
Pr
k
i
;k
j
sp
k
€
i;
€
j
q
A
k
1
;
whichdenotesthetotalityofallcuboidsinstratathatarecoarserthanthestratum
A
k
alongthemargins
i
and
j
suchthatmargins
t
i

1
;:::;D
x
u
and
t
D
x

j

1
;:::;D
u
are
allowedanyvaluein
r
0
;
1
s
.
LemmaS1.6tiesthe
k
-independenceoftherandomvectors
X
and
Y
withthe
p
i;j
q
odds-ratios:
LemmaS1.6.
Forany
k
¡
0
D
X
KK
k
Y
ô

ij
p
A
q
1
@
i
Pt
1
;:::;D
x
u
;j
Pt
1
;:::;D
y
u
;A
P
A
k
1
@
k
1
P
N
D
0
suchthat
k
1
¤
k
with
k
1
i
€
k
i
and
k
1
D
x

j
€
k
D
x

j
.
Proof.
ñ
:
Assume
X
KK
k
Y
forsome
k
P
N
D
0
.
35
Let
i
Pt
1
;::;D
x
u
and
j
Pt
1
;::;D
y
u
.
Byapplyingtheweakunionlemmatwicewegetthat
X
i
KK
p
k
i
;k
D
x

j
q
Y
j
|
Z
pt
i;D
x

j
uq
.
Noticethatfor
k
1
suchthat
k
1
i

k
i

1,
k
1
D
x

j

k
D
x

j

1,
k
1
d

k
d
forall
d
P
t
1
;::;D
uzt
i;D
x

j
u
and
A
P
A
k
1
wegetthat
A
00
ij
;A
11
ij
;A
01
ij
;A
10
ij
P
A
k
.So

ij
p
A
q
1
bytheofconditionalindependenceandthe
p
i;j
q
odds-ratios.
ByLemmaS1.2wegetthatindeed

ij
p
A
q
1for
A
P
A
r
k
i
;k
j
s
.
ð
:
First,noticethatittoshowthat:
X
KK
k
Y
ð

ij
p
A
q
1
@
i
Pt
1
;
2
;:::;D
x
u
;j
Pt
D
x

1
;:::;D
u
;A
P
A
r
k
i
;k
j
sp
k
€
i;
€
j
q
Sincethenwemayrelyontheoppositedirectiontogetthat

ij
p
A
q
0forall
A
P
A
r
k
i
;k
j
s
.
Examine
X
i
KK
k
t
1
;:::;i;D
x

1
;:::;D
x

j
u
Y
j
|
X
t
1
;:::;i

1
u
;
Y
t
1
;:::;j

1
u
Toseethattheaboveistrue,let
k
1
P
N
D
0
suchthat
k
1
d

k
d
forall
d
Pt
1
;:::;i
uYt
D
x

1
;:::;D
x

j
u
and
k
1
d

0forall
d
Pt
i

1
;:::;D
x
uYt
D
x

j

1
;:::;D
u
.
Foragiven
A
P
A
k
1
let
x;y
beapairofunivariaterandomvariableswhosejointdistri-
butionisgivenby
G
:

F
X
i
;Y
j
|
X
t
1
;:::;i

1
u
P
A
x;
t
1
;:::;i

1
u
;
Y
t
1
;:::;j

1
u
P
A
y;
t
D
x

1
;:::;D
x

j

1
u
.Byassumption:
1


ij
p
A
q
F
p
A
00
ij
q
F
p
A
11
ij
q
F
p
A
01
ij
q
F
p
A
10
ij
q

G
p
A
0
i

A
0
D
x

j
q
G
p
A
1
i

A
1
D
x

j
q
G
p
A
0
i

A
1
D
x

j
q
G
p
A
1
i

A
0
D
x

j
q
Sincetheaboveholdsforevery
A
P
A
r
k
i
;k
j
sp
k
€
i;
€
j
q
wemayapplyTheorem2from
MaandMao(2019)andconcludethat
x
KK
p
k
i
;k
D
x

j
q
y
whichisequivalentto
X
i
KK
k
t
1
;:::;i;D
x

1
;:::;D
x

j
u
Y
j
|
X
t
1
;:::;i

1
u
;
Y
t
1
;:::;j

1
u
.
36
Next,examine:

$
'
'
'
'
'
'
'
'
'
'
'
'
'
'
'
&
'
'
'
'
'
'
'
'
'
'
'
'
'
'
'
%
X
1
KK
k
t
1
;D
x

1
u
Y
1
X
1
KK
k
t
1
;D
x

1
;D
x

2
u
Y
2
|
Y
1
X
1
KK
k
t
1
;D
x

1
;D
x

2
;D
x

3
u
Y
3
|
Y
t
1
;
2
u
.
.
.
X
1
KK
k
t
1
;D
x

1
:::;D

1
u
Y
D
y

1
|
Y
t
1
;:::;D
y

2
u
X
1
KK
k
t
1
;D
x

1
:::;D
u
Y
D
y
|
Y
t
1
;:::;D
y

1
u

$
'
'
'
'
'
'
'
'
&
'
'
'
'
'
'
'
'
%
X
2
KK
k
t
1
;
2
;D
x

1
u
Y
1
|
X
1
X
2
KK
k
t
1
;
2
;D
x

1
;D
x

2
u
Y
2
|
X
1
;Y
1
.
.
.
X
2
KK
k
t
1
;
2
;D
x

1
;:::D
u
Y
d
y
|
X
1
;
Y
t
1
;:::;D
y

1
u
.
.
.

$
'
'
'
'
'
'
'
'
&
'
'
'
'
'
'
'
'
%
X
D
x
KK
k
t
1
;:::;D
x
;D
x

1
u
Y
1
|
X
t
1
;:::;D
x

1
u
X
D
x
KK
k
t
1
;:::;D
x
;D
x

1
;D
x

2
u
Y
2
|
X
t
1
;:::;D
x

1
u
;Y
1
.
.
.
X
D
x
KK
k
Y
D
y
|
X
t
1
;:::;D
x

1
u
;
Y
t
1
;:::;D
y

1
u
Eachoftheaboverowsisobtainedbythepreviousargument.Applyingthecontraction
lemmarecursivelyfromtoptobottomtoeachoftherowsin

showsthat
X
1
KK
t
1
;D
x
;:::;D
u
Y
.
Furtherapplyingthecontractionlemmatothelatterresultandtherowsof

showsthat
X
t
1
;
2
u
KK
t
1
;
2
;D
x
;:::;D
u
Y
.Andasimilarapplicationofthecontractionlemmatotheprevious
resultsandalltherowsupto

showsthat
X
KK
k
Y
.
Note,forevery
i
Pt
1
;::;D
x
u
and
j
Pt
1
;::;D
y
u
,andforeachoftheaboveconditions
X
i
KK
k
t
1
;:::;i

1
;D
x

1
;:::;D
x

j
u
Y
j
|
X
t
1
;:::;i

1
u
;
Y
t
1
;:::;j

1
u
thereare
p
2
k
i

1
qp
2
k
j

1
q
onedegreesof
freedomtestsrequired,eachrepeatedduetotheconditioning
±
i

1
s

1
2
k
s

±
D

1
t

D
x

1
2
k
t
times.
Summingover
j
,wegetthatforeach
i
,thereare
p
2
k
i

1
q
±
i

1
s

1
2
k
s
p
2
°
D
t

D
x

1
k
t

1
q
one
37
degreesoffreedomtests.Summingthoseover
i
wegetthatoverallweneedtoperform
p
2
°
D
x
s

1
k
s

1
qp
2
°
D
t

D
x

1
k
t

1
q
onedegreesoffreedomtests.
Under
H
1
thereare2
°
D
x
s

1
k
s

2
°
D
t

D
x

1
k
t

1degreesoffreedom,under
H
0
thereare
p
2
°
D
x
s

1
k
s

1
qp
2
°
D
t

D
x

1
k
t

1
q
degreesoffreedom,andthusweneed
p
2
°
D
x
s

1
k
s

1
qp
2
°
D
t

D
x

1
k
t

1
q
degreesoffreedomtoidentifyabetweenthenullandthealternative.Itfollows
fromtheproofthatwemayindeeduse
p
2
°
D
x
s

1
k
s

1
qp
2
°
D
t

D
x

1
k
t

1
q
1-degreeoffreedom
independenceteststodoso.
ProofofTheorem2.1:
ImmediatefromLemmasS1.1andS1.6.
ProofofTheorem2.2:
Let
A
beacuboidinresolution
r
,
i
Pt
1
;:::;D
x
u
,
j
Pt
1
;:::;D
y
u
and
p
ij
p
A
q
the
p
-valuethatisdeterminedbythetable
t
n
p
A
00
ij
q
;n
p
A
01
ij
q
;n
p
A
10
ij
q
;n
p
A
11
ij
qu
.For
any
r
¡
R

,whetherornot
A
isselectedinthe
MultiFIT
procedurefortestingisdetermined
bythe
p
-valuesobservedonthecollectionofallpotentialancestralcuboidsof
A
.Without
lossofgeneralitywelet
R


0tosimplifynotation.Thegeneralcaserequirestrivial
changestotheproof.Acuboid
A
willbeselectedin
MultiFIT
ifthereexistsasequenceof
nestedcuboids,oralineage,
A
0
•
A
1
••
A
r
ofresolution0
;
1
;:::;r
respectivelysuch
thateach
A
k

1
isachildcuboidof
A
k
inthe
p
i
k
;j
k
q
-face,andmoreover,the
p
-valueofthe
p
i
k
;j
k
q
-tableof
A
k
islessthanthethreshold
p

.Assuch,theeventthatacuboid
A
isin
C
p
r
q
isinthe
˙
-algebrageneratedbythe2

2-tablecounts
n
p

A
ij
q
forall
p
i;j
q
pairsandall
sets

A
thatcanbeanancestorcuboidof
A
alongsomelineage.
Supposetheresolution-
r
cuboid
A
isinthe
A
k
stratumwith
|
k
|
°
D
d

1
k
d

r
.Also,
let
r
x

°
D
x
d

1
k
d
and
r
y

°
D
d

D
x

1
k
d
.Anypotentialancestorcuboidof
A
,denotedby

A
,
istheunionofseveralsetsin
A
k
,andthusthe
p
i;j
q
-tableof

A
,
n
p

A
ij
q
,forevery
p
i;j
q
pair,
isdeterminedexactlyifweknowthecountsinallsetsin
A
k
.
Foranypositiveinteger
ˆ
,wedenotethecollectionofalllevel-
ˆ
marginalpartitionsof

X
as
r
P
ˆ
x

#
P
k
1

:::

P
k
D
x
:
D
x
¸
d

1
k
d

ˆ
+
38
andthecollectionofalllevel-
ˆ
marginalpartitionsof
Y
as
r
P
ˆ
y

#
P
k
D
x

1

:::

P
k
D
:
D
¸
d

D
x

1
k
d

ˆ
+
Considerthefollowingsequenceofnestedmarginalpartitionson
X
,
r
P
1
x
•
r
P
2
x
•
•
r
P
r
x
x
•
r
P
r
x

1
x
•
r
P
r
x

2
x
•
r
P
r
x

D
x
x
(where
r
P
1
x
P
r
P
1
x
;

;
r
P
r
x

D
x
x
P
r
P
r
x

D
x
x
),such
thatwedivide
X
1
k
1
timestoget
r
P
1
x
;:::;
r
P
k
1
x
,followedbydividing
X
2
k
2
times
toget
r
P
k
1

1
x
;:::;
r
P
k
1

k
2
x
,andsoonansoforthuntildividing
X
D
x
k
D
x
timestoget
r
P
r
x

k
D
x

1
x
;:::;
r
P
r
x
x
.Thendivide
X
i
oncetoget
r
P
r
x

1
x
,anddivideeachoftheother
D
x

1dimensionsonceinanyordertoget
r
P
r
x

2
x
;:::;
r
P
r
x

D
x
x
.
Inexactlythesamemanner,wecanconstructasequenceofnestedmarginalpartitions
of
Y
,
r
P
1
y
•
r
P
2
y
••
r
P
r
y
y
•
r
P
r
y

1
y
•
r
P
r
y

2
y
•
r
P
r
y

D
y
x
(where
r
P
1
y
P
r
P
1
y
;

;
r
P
r
y

D
y
y
P
r
P
r
y

D
y
y
)suchthatwedivide
Y
1
k
D
x

1
timestoget
r
P
1
y
;:::;
r
P
k
D
x

1
y
,followedbydividing

Y
2
k
D
x

2
timestoget
r
P
k
D
x

1

1
y
;:::;
r
P
k
D
x

1

k
D
x

2
y
,andsoonansoforthuntildividing

Y
D
y
k
D
timestoget
r
P
r
y

k
D

1
y
;:::;
r
P
r
y
y
.Thendivide
Y
j
oncetoget
r
P
r
y

1
y
,and
divideeachoftheother
D
y

1dimensionsonceinanyordertoget
r
P
r
x

1
y
;:::;
r
P
r
x

D
y
x
.
Underthesetwomarginalpartitionsequences,wehave
A
P
r
P
r
x
x

r
P
r
y
y

A
k
,whereasthe
fourchildcuboidsof
A
withrespecttothe
p
i;j
q
-faceareinthetwostrata
r
P
r
x

1
x

r
P
r
y
y
and
r
P
r
x
x

r
P
r
y

1
y
.Moreover,any
p
i;j
q
-faceofanyancestralcuboidof
A
areformedbyunionsof
setsthatarenotinthestrata
r
P
r
x

i
1
x

r
P
r
y

j
1
y
for
i
1

1
;
2
;:::;D
x
and
j
1

1
;
2
;:::;D
y
.
NowbyTheorem3inMaandMao(2019),conditionalonthe
X
and
Y
marginalvalues
oftheobservations,thecountsinany
p
i;j
q
-tableof
A
giventhecorrespondingrowand
columntotalsareindependentofthe
˙
-algebrageneratedbyallcountsinthesetsthat
arenotoftheform
r
P
r
x

i
1
x

r
P
r
y

j
1
y
for
i
1

1
;
2
;:::;D
x
and
j
1

1
;
2
;:::;D
y
.Therefore,
thecountsinany
p
i;j
q
-tableof
A
arealsoindependentofthe
˙
-algebrageneratedbythe
2

2-tablecounts
n
p

A
ij
q
forallsets

A
thatcanbeancestorcuboidsof
A
alongsomelineage
andall
p
i;j
q
pairs,andhencearealsoindependentoftheselectionunderthe
MultiFIT
procedure.Thiscompletestheproof.
ProofofCorollary2.1:
Thiscorollaryfollowsimmediatelysincethe
p
-valueonthe
p
i;j
q
-tableofacuboid
A
isdeterminedfromthe(central)hypergeometricdistributiongiven
therowandcolumntotalsofthattable,whichduetoTheorem2.2,istheactualsampling
39
distributionofthetablegiventherowandcolumntotalsunderthenullhypothesisof
independencewhetherornotoneconditionsontheeventthat
A
isselectedfortestingin
the
MultiFIT
procedure.
ProofofTheorem2.3:Weadopthereasimilarstrategytowhatwasusedin
(MaandMao,2019,Theorem5).Werecallthatthenulldistributionofthe
2

2
contingencytableonacuboid
A
arisingfromani.i.d.sampleconditionalonits
marginsisthecentralhypergeometricdistribution(asdiscussedinSection2.3)
andthesametablewhenthenulldoesnotholdisdistributedaccordingto
thenoncentralhypergeometricdistribution.Zhou(2019)developedanormal
approximationforthenoncentralhypergeometricdistribution(andaccompa-
nyingresults)onwhichwerely.
Undertheconditionsinthetheorem'sstatement,wecantake
R
max

R

{thatis,
exhaustivetesting.AccordingthentoourStrategyIformultipletestingadjustment(the
holisticapproachtomultipletesting)withBonferroni'scorrection

ij
p
A
q

{

R
max
¸
ˆ

0
D
x

D
y

2
ˆ


ˆ

D

1
D

1


isthetable-spthreshold.AccordingtoourStrategyIIformultipletestingadjustment
(theresolution-spapproachtomultipletesting)withexhaustivetestinguptoresolution
R
max
andutilizingBonferroni'scorrectionwithinresolutionaswellasbetweenresolutions

ij
p
A
q

{

D
x

D
y

2
r


r

D

1
D

1


R
max

isthetable-spthreshold.TheselectionofBonferron'scorrectionhereensuresthatthe
proofisvalidforanycorrectionthatislessconservative{e.g.Holm'scorrectionthatwe
useinourimplementation.
Westartbyprovingthetheoreminacasewhereweassumethat
R


R
max
isedbutlargeenoughsuchthat
r
¤
R

.Ittonoticethatunderbothinstances

ij
p
A
q
isconstantwhen
R

isconstant.Letthen

ij
p
A
q

.Withoutlossofgenerality,let
40
usassumethat

¡
1.
ByTheorem2.2inZhou(2019),wehavethatgiven
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
q
,
Z
;i;j
p
A
q
n
p
A
00
ij
q
E

r
n
p
A
00
ij
q|
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
qs
Var
1
{
2

r
n
p
A
00
ij
q|
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
qs
Ñ
L
N
p
0
;
1
q
:
p
ij
p
A
q
,the
p
-valueforthetabledeterminedby
n
p
A
00
ij
q
whenthemargins
t
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
qu
aregiven,iscomputedforthetwosidedversionofFisher'sexacttestbysummingthe
probabilitiesofalltablesthataremoreextremethanthegivenone(thatis,thosewitha
smallerprobabilityofoccurrencecomparedtothetableforwhichthe
p
-valueiscomputed
accordingtothecentralhypergeometricdistribution).
Wecanutilizetheform
Z
;i;j
towritetheprobabilityofrejectionfortheteston
the
p
i;j
q
-tableof
A
.If
n
p
A
00
ij
q
islessthanorequaltothemodeofthehypergeometric
distributionwiththeparameters
t
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
qu
wehave
P
p
p
ij
p
A
q€

ij
p
A
q|

ij
p
A
q
;
n
r
x
;
0
;
n
0
;r
y
q
¡
P

Z
n;
1
;i;j
p
A
q¡
F

1
A;n
p

ij
p
A
qq|

ij
p
A
q
;
n
r
x
;
0
;
n
0
;r
y

where
F
A;n
denotestheexactcdfof
Z
n;
1
;i;j
giventhemarginaltotals.Else,if
n
p
A
00
ij
q
isgreater
thanthemodeofthehypergeometricdistributionwiththeparameters
t
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
qu
P
p
p
ij
p
A
q€

ij
p
A
q|

ij
p
A
q
;
n
r
x
;
0
;
n
0
;r
y
q
¡
P

Z
n;
1
;i;j
p
A
q¡
F

1
A;n
p
1


ij
p
A
qq|

ij
p
A
q
;
n
r
x
;
0
;
n
0
;r
y

:
Withoutlossofgeneralitywecontinuetoworkwiththesecondcase.
Assumenowthatindeed

ij
p
A
q

˘
1.Then
lim
n
Ñ8
P

Z
n;
1
;i;j
p
A
q¡
F

1
A;n
p
1


ij
p
A
qq|

ij
p
A
q
;
n
r
x
;
0
;
n
0
;r
y


lim
n
Ñ8
P

Z
;i;j
p
A
q¡
c
n
F

1
A;n
p
1


ij
p
A
qq
c
n
d
n




ij
p
A
q
;
n
r
x
;
0
;
n
0
;r
y

where
c
n

Var
1
{
2
1
r
n
p
A
00
ij
q|
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
qs
Var
1
{
2

r
n
p
A
00
ij
q|
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
qs
41
and
d
n

E

r
n
p
A
00
ij
q|
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
qs
E
1
r
n
p
A
00
ij
q|
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
qs
Var
1
{
2
1
r
n
p
A
00
ij
q|
n
p
A
0

ij
q
;n
p
A

0
ij
q
;n
p
A
qs
:
ByCorollary2.1inZhou(2019)wehave1
{
a
max
p
;

1
q¤
c
n
¤
1
{
a
min
p
;

1
q
forall
n
and
d
n

?
n
with
F
8
probability1.
Bytheabovenormalapproximation
F

1
A;n
p
1


ij
p
A
qqÑ


1
p
1


ij
p
A
qq
.
Therefore
c
n


1
p
1


ij
p
A
qq
c
n
d
n
Ñ
with
F
8
probability1,andsowith
F
8
probability1,
lim
n
Ñ8
P
p
p
ij
p
A
q€

ij
p
A
q|
n
r
x
;
0
;
n
0
;r
y
q
1
:
Thiscompletestheproofforthecasewhere
R

isconstantsincethenullofindependenceis
rejectedwhenever
p
ij
p
A
q€

ij
p
A
q
forsome
A
,
i
and
j
.
Assumenowthat
R
max

R


o
p
log
n
q
.Underboththeholisticandresolutionsp
strategiesformultipletestingwehave
?
n


ij;n
p
A
qÑ8
as
n
Ñ8
.
Since
c
n
d
n

O
p
?
n
q
with
F
8
probability1,weneedtoshowthat
c
n
F

1
A;n
p
1


ij
p
A
qq
o
p
?
n
q
with
F
8
probability1inordertoestablishconsistency.Rewrite:
c
n
F

1
A;n
p
1


ij;n
p
A
qq
c
n

F

1
A;n
p
1


ij;n
p
A
qq


1
p
1


ij;n
p
A
qq


c
n


1
p
1


ij;n
p
A
qq
Examine
c
n

F

1
A;n
p
1


ij;n
p
A
qq


1
p
1


ij;n
p
A
qq

:byTheorem2.3ofZhou(2019),we
havewith
F
8
probability1,
|

p
F

1
A;n
p
1


ij;n
p
A
qqp
1


ij;n
p
A
qq|€

{
?
n
forsomepositiveconstant

andlargeenough
n
.Therefore


1
p
1


ij;n
p
A
q

{
?
n
q€
F

1
A;n
p
1


ij;n
p
A
qq€


1
p
1


ij;n
p
A
q

{
?
n
q
:
Since1


p
x
q
e

x
2
{
2
{
x
as
x
Ñ8
and
?
n


ij;n
p
A
qÑ8
as
n
Ñ8
,wehave
|


1
p
1


ij;n
p
A
q

{
?
n
q


1
p
1


ij;n
p
A
qq|Ñ
0
42
and
|


1
p
1


ij;n
p
A
q

{
?
n
q


1
p
1


ij;n
p
A
qq|Ñ
0
Hence,
|
F

1
A;n
p
1


ij;n
p
A
qq


1
p
1


ij;n
p
A
qq|Ñ
0
:
Examine
c
n


1
p
1


ij;n
p
A
qq
:since1


p
x
q
e

x
2
{
2
{
x
as
x
Ñ8
and
?
n


ij;n
p
A
qÑ8
as
n
Ñ8
,wehave

1
p
1


ij;n
p
A
qq
o
p
?
n
q
.
Therefore,wehavewith
F
8
probability1that
c
n
F

1
A;n
p
1


ij
p
A
qq
o
p
?
n
q
.I.e.,we
havewithprobability1
p
ij
p
A
q€

ij
p
A
q
forsome
A
,
i
and
j
andthereforethenullof
independenceisrejectedwithprobability1.
43
S2Pseudo-codeforthe
MultiFIT
procedure
Algorithm1
MultiFIT
procedurefortestingmultivariateindependence
Let
C
p
r
q
bethecollectionofallcuboidsofresolution
r
for
r

0
;
1
;
2
;:::;R

,andlet
C
p
r
q
H
for
r

R


1
;:::;R
max
.
Ž
Step0:Initialization
for
r
in0
;
1
;
2
;:::;R
max
do
Ž
Foreachresolution
for
each
A
P
C
p
r
q
do
Ž
Foreachcuboidselectedfortesting
for
i
in1
;
2
;:::;D
x
do
for
j
in1
;
2
;:::;D
y
do
ApplyFisher'sexacttestonthe
p
i;j
q
-tableof
A
andrecordthe
p
-value
Ž
Step1a:Independencetesting
if
R

¤
r
€
R
max
then
if
the
p
i;j
q
-tableof
A
hasa
p
-valuesmallerthanathreshold
p

then
Addthefourhalfcuboidsof
A
into
C
p
r

1
q
Ž
Step1b:Selectcuboidsfortestinginthenextresolution
endif
endif
endfor
endfor
endfor
endfor
ApplyamultipletestingprocedurethatprovidesstrongFWERcontrolbasedonthe
recorded
p
-values.
Ž
Step2:Multipletestingcontrol
44
S3Scenariosforthepowerstudy
TableS1:SimulationScenarios
Scenario
#ofData
Points
Max
Res
SimulationSetting
Sine
300
4
X
1

Z
,
Y
1

Z
1
,
X
2

U
,
Y
2

sin
p
5
ˇ

X
2
q
4

Circular
300
4
X
1

Z
,
Y
1

Z
1
,


Uniform

ˇ;ˇ
q
X
2

cos
p

q

,
Y
2

sin
p

q

1
Checkerboard
1500
5
W

Multi-Bern
pt
1
;
2
;
3
;
4
;
5
u
;
p
1
{
5
;
1
{
5
;
1
{
5
;
1
{
5
;
1
{
5
qq
V
1

Multi-Bern
pt
1
;
3
;
5
u
;
p
1
{
3
;
1
{
3
;
1
{
3
qq
V
2

Multi-Bern
pt
2
;
4
u
;
p
1
{
2
;
1
{
2
qq
X
1

Z
,
Y
1

Z
1
,
X
2

W


,
Y
2

$
'
&
'
%
V
1


1
;
if
W
isodd
V
2


1
;
if
W
iseven
Linear
300
4
X
1

Z
,
Y
1

Z
1
,
X
2

U
,
Y
2

X
2

3

Parabolic
300
4
X
1

Z
,
Y
1

Z
1
,
X
2

U
,
Y
2
p
X
2

0
:
5
q
2

0
:
75

Local
1000
6
X
1

Z
,
Y
1

Z
1
,
X
2

Z
2
,
Y
2

$
'
&
'
%
X
2

1
{
6


if0
€
X
2
;Z
3
€
0
:
7
Z
3
;
otherwise
Sixsimulationscenarios.Inallcases,
Z;Z
1
;Z
2
;Z
3
arei.i.dN
p
0
;
1
q
.Ateachnoiselevel
l

1
;
2
;:::;
20,

1
and

2
arei.i.dN
p
0
;
p
l
{
20
q
2
q
,and
U

Uniform
p
0
;
1
q
.Themaximal
resolutionisthealgorithm'sdefault:
t
log
2
p
n
{
10
q
u
where
n
isthenumberofdatapoints.
45
TableS2:\Spread""SimulationScenarios(part1)
Scenario
#ofData
Points
Max
Res
SimulationSetting
Sine
300
4
X
1

U;X
2

U
Y
1

sin
p
5
ˇX
1
q
cos
p
5
ˇX
2
q
4

Y
2

sin
p
5
ˇX
2
q
cos
p
5
ˇX
1
q
4

1
Circular
300
4
Z
1
;Z
2

Beta
p
0
:
9
;
0
:
27
q
b
1
;b
2
;b
3
;b
4

Multi-Bern

1
;
1
u
;
p
0
:
5
;
0
:
5
qq
X
1

b
1
Z
1


,
X
2

b
2
Z
2


1
Y
1

b
3
a
1
p
0
:
2
b
1
Z
1

0
:
8
b
2
Z
2
q
2


2
Y
2

b
4
a
1
p
0
:
8
b
1
Z
1

0
:
2
b
2
Z
2
q
2


3
Checkerboard
1500
5
W;W
1

Multi-Bern
pt
1
;
2
;
3
;
4
;
5
u
;
p
0
:
2
;
0
:
2
;
0
:
2
;
0
:
2
;
0
:
2
qq
X
1

W

0
:
75
2

,
X
2

W
1

0
:
75
2

1
m
1

Multi-Bern
pt
1
;
2
u
;
p
0
:
2
;
0
:
8
qq
m
2

Multi-Bern
pt
1
;
2
u
;
p
0
:
8
;
0
:
2
qq
V
1
;V
1
1

Multi-Bern
pt
1
;
3
;
5
u
;
p
1
{
3
;
1
{
3
;
1
{
3
qq
V
2
;V
1
2

Multi-Bern
pt
2
;
4
u
;
p
0
:
5
;
0
:
5
qq
Y
1

$
'
&
'
%
V
1

0
:
75
2

2
;
if
x
m
1
isodd
V
2

0
:
75
2

2
;
if
x
m
1
iseven
Y
2

$
'
&
'
%
V
1
1

0
:
75
2

3
;
if
x
m
2
isodd
V
1
2

0
:
75
2

3
;
if
x
m
2
iseven

1
;
2
and

3
arei.i.dN
p
0
;
p
l
{
20
q
2
q
,and
U

Uniform
p
0
;
1
q
.Themaximalresolutionisthe
algorithm'sdefault:
t
log
2
p
n
{
10
q
u
where
n
isthenumberofdatapoints.
46
TableS3:\Spread""SimulationScenarios(part2)
Scenario
#ofData
Points
Max
Res
SimulationSetting
Linear
300
4
X
1
;X
2

U
p
0
;
1
q
Y
1

X
1

2
X
2

6

,
Y
2

X
1

X
2

6

1
Parabolic
300
4
X
1
;X
2

Uniform
p
0
;
1
q
Y
1
p
X
1

0
:
5
q
2
p
X
2

0
:
5
q
2

0
:
75

Y
2
p
X
2

0
:
5
q
2
p
X
1

0
:
5
q
2

0
:
75

1
Local
1000
6
Aspreadlinearsignalscaledto(0,0.7)forallmargins:
x
1
;x
2

U
p
0
;
0
:
7
q
y
1

0
:
7
{
2
:
1
p
x
1

x
2

1
:
4
q
1
{
6


y
2

0
:
7
{
1
:
4

x
1

x
2

0
:
7
q
1
{
6


1
Embeddedwithinasmallportionofthespace:
X
1

Z
,
X
2

Z
1
Y
1

$
'
&
'
%
y
1
if0
€
X
1
;Z
2
€
0
:
7
Z
2
;
otherwise
Y
2

$
'
&
'
%
y
3
if0
€
X
2
;Z
3
€
0
:
7
Z
2
;
otherwise
Z;Z
1
;Z
2
;Z
3
arei.i.dN
p
0
;
1
q
.Ateachnoiselevel
l

1
;
2
;:::;
20,

1
and

2
arei.i.d
N
p
0
;
p
l
{
20
q
2
q
,and
U

Uniform
p
0
;
1
q
.Themaximalresolutionisthealgorithm'sdefault:
t
log
2
p
n
{
10
q
u
where
n
isthenumberofdatapoints.
47
S4Numericalvalidationoflevelcontrolthroughsim-
ulations
Todemonstratethat
MultiFIT
properlycontrolsthelevelweexecuted500simulations
withthedefaulttuningparametersforvarioussamplesizes.Theunderlyingdata
t
X
1
i
u
,
t
X
2
i
u
,
t
Y
1
i
u
and
t
Y
2
i
u
aredrawnindependentlyfromastandardnormaldistributionfor
i
Pt
1
;:::;n
u
with
n
Pt
100
;
200
;:::;
2000
u
.
FigureS4
showstheestimatedlevelfor
MultiFIT
withtvariationsfortheindependencetestsoneachtableandmultiple
testingadjustmentoptionsonFisher'sexacttestwithmid-
p
corrected
p
-values(seeAgresti
andGottard(2007)foradiscussiononthemid-
p
correction).
FigureS4:Estimatedlevelversussamplesize
Theresultsthetheoreticalguaranteesthatthelevelcanbecontrolledatany
givenlevel

.Infact,theprocedureappearstobeabitconservativeincontrollingthe
level.Note,although
X
and
Y
areindependentunderthenullhypothesis,thedependency
structurebetweenthetmarginsof
X
isarbitrary,aswellasthedependencystructure
betweenthetmarginsof
Y
.Therefore,thissimulationisnotexhaustive.However,
werepeatedtheestimationofthelevelundervariousdependencystructuresforthemargins,
andtheresultsareconsistentwiththesethatareshownhere.
48
S5Scaling:comparisonofscenariosfor
MultiFIT
FigureS5:Computationalscalability:run-timevs.samplesizeforthesixsimulation
scenariosfrom
TableS1
whenwith
MultiFIT
intdimensionalitieswith
D
x

D
y

d
,
R


1and
l

3.Inallcasesthe\linear""scenariorequiresthemost
computationsandthe\null""scenariotheleast.
S6Powerstudy:sensitivityanalysisfortheparame-
ters
p

and
R

of
MultiFIT
In
FiguresS6
and
S7
wedemonstratetheofourmaintuningparameters{
p

and
R

{ontheestimatedpower.Weevaluatethepowerforthesimulationscenariosthat
aredetailedin
TableS1
overthegrid
p


R

where
p

t
0
:
005
;
0
:
01
;
0
:
05
;
0
:
1
;
0
:
5
u
and
R

t
1
;
2
;
3
;
4
u
.Ingeneral,higher
p

andhigher
R

valuesentailmoretesting.Under
somescenarios,wherethedependencystructureis\morelocal"",highervaluesareneeded
toensurebetterpower.Whenthedependencystructuresare\moreglobal""weseethatthe
selectionofthetuningparametersmakeslittletotheestimatedpower.
49
FigureS6:Powerversusnoiselevelfortmethods.Estimatedpowerat20noise
levelsfor
MultiFIT
witht
p

and
R

valuesunderthesixscenariosfrom
TableS1
.
50
FigureS7:Powerversusnoiselevelfortmethods.Estimatedpowerat20noise
levelsfor
MultiFIT
witht
p

and
R

valuesunderthesixscenariosfrom
TableS1
.
51
"
3,POTs: Protective Optimization Technologies,https://arxiv.org/pdf/1806.02711v6.pdf,https://github.com/spring-epfl/pots,"POTs:ProtectiveOptimizationTechnologies
BogdanKulynych
EPFL
bogdan.kulynych@ep˚.ch
RebekahOverdorf
EPFL
rebekah.overdorf@ep˚.ch
CarmelaTroncoso
EPFL
carmela.troncoso@ep˚.ch
SedaGürses
TUDelft/KULeuven
f.s.gurses@tudelft.nl
ABSTRACT
Algorithmicfairnessaimstoaddresstheeconomic,moral,social,
andpoliticalimpactthatdigitalsystemshaveonpopulationsthrough
solutionsthatcanbeappliedbyserviceproviders.Fairnessframe-
worksdoso,inpart,bymappingtheseproblemstoanarrowde˙-
nitionandassumingtheserviceproviderscanbetrustedtodeploy
countermeasures.Notsurprisingly,thesedecisionslimitfairness
frameworks'abilitytocaptureavarietyofharmscausedbysystems.
Wecharacterizefairnesslimitationsusingconceptsfromrequire-
mentsengineeringandfromsocialsciences.Weshowthatthefocus
onalgorithms'inputsandoutputsmissesharmsthatarisefrom
systemsinteractingwiththeworld;thatthefocusonbiasand
discriminationomitsbroaderharmsonpopulationsandtheirenvi-
ronments;andthatrelyingonserviceprovidersexcludesscenarios
wheretheyarenotcooperativeorintentionallyadversarial.
Wepropose
ProtectiveOptimizationTechnologies(POTs)
.POTs,
providemeansfora˛ectedpartiestoaddressthenegativeimpacts
ofsystemsintheenvironment,expandingavenuesforpoliticalcon-
testation.POTsintervenefromoutsidethesystem,donotrequire
serviceproviderstocooperate,andcanservetocorrect,shift,or
exposeharmsthatsystemsimposeonpopulationsandtheirenvi-
ronments.WeillustratethepotentialandlimitationsofPOTsintwo
casestudies:counteringroadcongestioncausedbytra˝c-beating
applications,andrecalibratingcreditscoringforloanapplicants.
1INTRODUCTION
Advancesincomputationalpower,softwareengineering,andma-
chinelearningalgorithmshavebeeninstrumentalintheriseof
digitalsystems.Theirubiquityinoureverydayactivitiesraisescon-
cernsregardingthecentralizationofdecisionalpower[
1
].These
concernsareampli˙edbytheopaqueandcomplexnatureofthese
systemswhichresultsinhard-to-explainoutputs[
2
]andunjust
outcomesforhistoricallymarginalizedpopulations
Computerscientistscountertheseinequitiesthroughframe-
worksstudiedundertherubricof
fairness
,usingavarietyofformal
fairnessnotions[
7
].Theirresultshavebeeninstrumentalinourun-
derstandingofthediscriminatorye˛ectsofalgorithmicdecisions.
Theframeworks,however,relyonnarrowingtheinequityproblem
toprimarilyconsiderthediscriminatoryimpactofalgorithms,and
assumingtrustworthyproviders.Thenarrowviewhasenabled
BogdanKulynychandRebekahOverdorfcontributedequallytothiswork.
valuablebreakthroughscenteredaroundtheserviceproviders'abil-
itytoaddresssomeinequities,butfailstocapturebroaderharms
orexploreotherwaystocontestservice-providerpower[8].
Inthisworkweinvestigatedigitalsystemsfromanewperspec-
tiveinordertounderstandhowtoaddressthebroaderclassof
harmsthattheycause.Toachievethis,wecharacterizethetypeof
systemsinwhichalgorithmsareintegratedanddeployed.These
systemstypicallybuildondistributedservicearchitecturesand
incorporatereal-timefeedbackfrombothusersandthird-party
serviceproviders[
9
,
10
].Thisfeedbackisleveragedforavarietyof
novelformsof
optimization
thataregearedtowardsextractionof
valuethroughthesystem.Typically,optimizationisusedfortech-
nicalperformanceandminimizingcosts,e.g.,byoptimizingcloud
usageorchestrationorallocationofhardwareresources.Ithasalso
becomepartandparcelofcontinuousdevstrategies
basedonexperimentationthatallowdeveloperstode˙nedynamic
objectivefunctionsandbuildadaptivesystems.Businessescannow
designforinteractionsandenvironmentsbyoptimizing
featureselection,behavioraloutcomes,andplanningthatisinline
withabusinessgrowthstrategy.Wearguethat
optimization-based
systems
aredevelopedtocaptureandmanipulatebehaviorand
environmentsfortheextractionofvalue.Assuch,theyintroduce
broaderrisksandharmsforusersandenvironmentsbeyondthe
outcomeofasinglealgorithmwithinthatsystem.Theseimpacts
gobeyondthebiasanddiscriminationstemmingfromalgorithmic
outputsfairnessframeworksconsider.
Borrowingconceptsandtechniquesfromsoftwareandrequire-
mentsengineering,andfromeconomicsandsocialsciences,we
characterizethelimitationsofcurrentfairnessapproachesincap-
turingandmitigatingharmsandrisksarisingfromoptimization.
Amongothers,weshowthatfocusingonthealgorithmsandtheir
outputsoverlooksthemany`externalities'causedbyoptimizing
everyaspectofasystem;thatdiscriminationisonlyoneofthein-
justicesthatcanarisewhensystemsaredesignedtomaximizegain;
andthatignoringserviceproviders'incentivesandcapabilitiesto
enforceproposedsolutionslimitsourunderstandingoftheiropera-
tionandfurtherconsolidatesthepowerprovidershaveregarding
decisionsandbehaviorsthathaveprofounde˛ectsinsociety.
Finally,wepropose
ProtectiveOptimizationTechnologies(POTs)
,
whichaimataddressingrisksandharmsthatcannotbecaptured
fromthefairnessperspectiveandcannotbeaddressed
without
a
cooperativeserviceprovider.TheultimategoalofPOTsistoelim-
inatetheharms,oratleastmitigatethem.Whenthesearenot
possible,POTscanshiftharmsawayfroma˛ectedusersorexpose
abusiveornon-socialpracticesbyserviceproviders.Weillustrate
thepotentialofPOTstoaddressexternalitiesofoptimization-based
arXiv:1806.02711v6  [cs.CY]  26 Jan 2020systemsintwocasestudies:tra˝c-beatingroutingapplications
andcreditscoring.Wealsoidentifynumeroustechniquesdevel-
opedbyresearchersinthe˙eldsofsecurityandprivacy,byartists,
andbyothers,that,thoughnotnecessarilydesignedtocounter
externalities,canbeframedasPOTs.
2FAIRNESSFROMASYSTEM'SPERSPECTIVE
WeintroduceMichaelA.Jackson'stheoryofrequirementsengi-
neering[
11
]todiscussthefocus,goals,andassumptionsbehind
fairnessframeworkfromasystems'perspective.Thistheoryargues
thatcomputerscientistsandengineerseconcernedbothwith
theworld,inwhichthe
machine
servesausefulpurpose,andwith
themachineitself[...]Thepurposeofthemachine[...]islocatedin
theworldinwhichthemachineistobeinstalledandused.Inother
words,ourobjectiveistodesignsystemsthatful˙llrequirements
intheworld
wherethesystemistobedeployed.
Moreprecisely,aportionoftheworldbecomesthe
environment
,
orthe
applicationdomain
,
*
ofa
machine
.Inthisworldthereare
phenomena
intheapplicationdomain(e.g.,events,behaviorof
people,activities)andinthemachine(e.g.,data,algorithms,state).
Themachine'sinputsandoutputs,i.e.,thethingsthemachinecan
senseora˛ect,aresharedphenomena:theyareobservablebothto
theapplicationdomainandthemachine.
Weintroducemachinesintoexistingapplicationdomainstoef-
fectchangeinthesedomains.Toachievethedesiredchange,we
needdescriptionsofthephenomenabeforethemachineisintro-
duced,knownas
domainassumptions
K
;andstatementsaboutthe
desiredsituationoncethemachineisintroducedtothedomain,
knownas
requirements
R
.A
speci˙cation
S
isasetofrequirements
providingenoughinformationforengineerstoimplementthema-
chine.
S
typicallydescribesphenomenasharedbetweenthemachine
andapplicationdomain.A
program
P
derivedfromthespeci˙cation
isadescriptionofamachine.Ifimplementedcorrectly,programs
satisfythespeci˙cation.Ifthespeci˙cationisderivedcorrectly,
programsgeneratephenomenathatattainthedesirede˛ectsinthe
applicationdomain,i.e.,theyful˙lltherequirements.
Jackson'sexplicittreatmentoftheapplicationdomainandits
interactionwiththemachinehelpsustoprojectknownproblems
withalgorithmstoasystemsview.Itenablesustodistinguish
problemsduetobadlyderivedrequirements(descriptionofthe
problem)fromthoseduetobadlyderivedspeci˙cations(description
ofthesolution)andthoseduetobadlyimplementedprograms(how
solutionsareimplemented)[12].
2.1Thefocusonalgorithmsisinsu˝cientfor
addressinginequitableoutcomesofsystems
InthelightofJackson'stheory,weclaimthat
thefocusonalgorithms
leavesoutthesystems'view.
Algorithmsareoftenapartofalarger
technicalsystem,whichisdeployedinanenvironment.Fairness
proposalsrarelyevaluatethesystems'environmentalconditions,
therebyleavingoutthepossibilitythat,
even
whenafairnessmetric
issatis˙edbythealgorithm,thesystem(environmentplusthe
machine)couldstillbeunfairorhaveothernegativesidee˛ects.
Suchfocusonthespeci˙cationofthemachinealsopromotesthe
*
Applicationdomaindoesnotrefertoaclassofapplications,likehealthorbanking
domain,buttoactorsandthingsintheenvironmentwherethemachineisintroduced.
ideathedi˝cultyinaddressing[unfairness]liesindevisinga
[11]andnotnecessarilyinrethinkingtheworld.
Mostfairnessframeworksfocusondescribing
S
independentof
K
and
R
andthenguaranteethatstatesofthemachine,anditsinput
andoutputs,havecertainproperties.However,amachinethathas
fairinputsandoutputsandful˙llsaspeci˙cation
S
fair
doesnot
guaranteetheful˙llmentofrequirementsoffairnessintheapplica-
tiondomain.Thiswouldrequireevaluating
K
,establishingwhat
phenomenathemachineisexpectedtochangeintheapplication
domain,andarticulatingrequirements
R
fair
withfairnessasagoal.
Onlythencouldweevaluatewhetheran
S
fair
ful˙lls
R
fair
.
Focusingon
S
fair
hasanumberofrepercussions.First,itdoesnot
re˚ect
howharmsmanifestthemselvesintheenvironment.
Without
anunderstandingof
K
and
R
fair
,aspeci˙cation
S
fair
maysimplynot
leadtoafairoutcome.Imagineahypotheticalpredictivepolic-
ingthatcanfairlydistributepoliceo˝cerstodi˛erent
neighborhoods.Ifthealgorithmdoesnotconsiderthatthepolicing
institutionisalreadycon˙guredtocontrolminorities[
13
]andthat
interactionswithpoliceposegreaterriskofharmsforminorities,a
allocanstilldisparatelyimpactthoseminorities[6].
Second,focusingonachievingfairnessforusersmight
leaveout
theimpactofthesystemonphenomenaintheapplicationdomainthat
isnotsharedwiththemachine
.Unjustoutcomescouldarisedueto
theoptimizationofcertainbehaviorintheapplicationdomain,and
notbecause
S
wasunfair.Forinstance,self-regulatedhousingmar-
ketssuchasAirbnb[
14
]maynotactivelydiscriminateagainsttheir
users,butreportshaveshownthattheycandisruptneighborhoods
bychangingrentdynamicsandneighborhoodcomposition[15].
Third,
thefocusonalgorithmsabstractsawaypotentialharmsof
phenomenainthemachine.
Muchofthefairnessliteraturefocuses
onwaysinwhichalgorithmscanbebiasedoronharmscausedby
decision-makingalgorithms.Thisoverlooksthat,whenthesystem
hostingthealgorithmsoptimizesitsoperation,itmaygathermore
inputsandoutputsthanthoseofthealgorithm.Therefore,focusing
onthealgorithmmaymisse˛ectsontheworldthatcangobeyond
thosegeneratedbytheoutputsofthealgorithmactions.
Whenphenomenainthemachinedomainaresubjecttoopti-
mization,unfairnesscanarisefromoptimizationprograms
P
opt
ful˙llingthespeci˙cation
S
fair
butnottherequirements
R
fair
inthe
applicationdomain.Forinstance,predictiontechniquestooptimize
targetedadvertisingcancreatediscriminatorye˛ects[
16
];andex-
plorationstrategiestooptimize
S
fair
maygatherinputsfromthe
applicationdomainthatputsomeusersunfairlyatrisk[17].
2.2Discriminatorye˛ectsarenottheonly
concernforbuildingjustsystems
InJackson'sterms,consideringonlydiscriminatorye˛ectscon-
strainstherequirements
R
fair
toaparticularclassofharms.This
approachrisks
missingotherharmscausedbythesystem
whenevalu-
atingtheperformanceofthespeci˙cation
S
fair
intheenvironment.
Weassumetheintroductionofamachineinanenvironment
aimstoimprovespeci˙cphenomena.Thefactthatthismachine
followsaspeci˙cation
S
fair
thatdoesnotdiscriminateaccording
to
R
fair
doesnotguaranteethatthismachinewillnotinduceother
harmstotheenvironmentinanyapplicationsdomain.Takeas
examplesunsafehousingorbadworkingconditions.Ifweusea
2
machinetodistributetheseresourcesmoree˝ciently,evenifit
doessofairly,
all
usersareharmed.Badhousingconditionsandlack
oflaborprotectionareproblemsinandofthemselvessouserswill
bebadlyservedregardlessoffairnessconditions.Whenthesystem
isbydesignunjust,orwhenthephenomenaarestructurallyunjust
orharmful,claimson
S
fair
aremeaningless.InJackson'sterms:the
requirements
R
areincompletewithrespecttojustoutcomes.
Sometimes,however,injusticesaretightlytiedtothemachine:
S
isnotthesolutionbutthesourceofproblems.Thewayrequire-
ments
R
areoptimizedmightleadtoexternalitiesfor(asubpopula-
tionof)users.Whenaspeci˙cation
S
optimizesanasocialoutcome,
suchasexcessiveuserengagementinsocialnetworks[
18
],itcan
exposeuserstoharmslikeaddiction.Solvingfairnessinthissystem
willnotresolvetheunderlyingproblem:thesystemisharmful.
Strivingtoful˙lltherequirements
R
fair
itself
mightbringnew
harmstotheapplicationdomain.Considerafairnesssolutionthat
alleviatesdistributionalshiftbasedonincreasingdiversityinthe
trainingset.Ifitsspeci˙cation
S
fair
requirescollectingdatafrom
moreindividualsorcollectsnewattributestoimplementthefair-
nessmeasure,itwillexacerbateprivacyissues.Theseissuesmight
resultinmanyotherharmsintheapplicationdomain.
Thisontologyassumesthat
S
isbuilttoeprand
oveintheworld.Thus,itdoesnotprovidethe
conceptualtoolstoaddressadversesituations.Thispositivevalence
hinderstheconsiderationofcasesinwhichamachineampli˙es
existinginjusticesorintroducenewones.Neitherthisontology
norfairnessframeworksaccountforpowerimbalancesoreco-
nomicincentives,andhowtheyimpacthowmachinerequirements
areconsideredandprioritized.Weconsiderthesemattersinthe
nextsectionbyaugmentingoursystemsviewtoconsidersocio-
economicaspectsoftheapplicationdomain.
3FAIRNESS,INCENTIVES,ANDPOWER
Inthissection,weextendouranalysistoproblemsrelatedtothe
politicaleconomyofsystems.Tomodelharms,weborrowthe
term
negativeexternalities
fromtheeconomicsliterature.Asystem
causesnegativeexternalitieswhenitsconsumption,production,
andinvestmentdecisionscausesigni˙cantrepercussionstousers,
non-users,ortheenvironment[
19
].Theintroductionofamachine
mightcauseexternalitiesintheapplicationdomain,independent
ofthecompletenessorcorrectnessofitsrequirementsandspec-
i˙cation.Forexample,theheavyuseoftra˝c-beatingappssuch
asWazecanworsencongestionfor
all
driversintheapplication
domain[
20
].Wearguethatvalidatingthespeci˙cation
S
against
therequirements
R
isnotenough.Tobuildjustsystemsonemust
considerexternalitiesofthemachineintheapplicationdomain.
Congruentwithmodelsinfairoptimizationandeconomics,to
expressexternalitiesandincentivesweintroducetwoutilityfunc-
tionsthatcapturethemachine'simpactontheapplicationdomain:
the
serviceprovider'sutility
,whichmeasureshowmuchvaluethe
providerextractsfromintroducingthemachine,andthe
socialutil-
ity
,whichmeasuresthemachine'sutilityfortheenvironmentand
people.Wede˙netwoversionsofsocialutility:onewithad's
vieoftheapplicationdomainandonefromthespeci˙cation's
perspective.Theseutilityfunctionsenableustocapture
injustices
duetotheintroductionofthemachineintotheenvironment.
3.1IdealizedFair-by-DesignServiceProvider
We˙rstconsideranidealized
fair-by-designserviceprovider
that
iswillingtoaddresstheexternalitiesofthemachine.Thatis,this
provideraimstomaximizeboththeirownutilityandthesocial
utility.Usingthissetting,weshowthewaysinwhichfairness
modelsfailtoaddressabroadclassofsystems'externalities.
Weconsiderthatasystemisparametrizedbyavectorofinternal
parameters

2

forsomeconvexsetofpossibleparameters

.
Let
P
bea
population
,asetofindividualsorotherenvironmental
entitiesthatmightbea˛ectedbythesystem.Let
U
¹

º
:

!
R
betheutilityfunctionoftheproviderwhentheyuseparameters

.
Let
B
¹

º
:

!
R
denoteahypotheticalsocial-utilityfunction,or
bene˙t
,de˙nedintherequirements
R
.Let
^
B
¹

º
bethesocialutility
intheprovider'sspeci˙cation.Theprovideroptimizesitsoperation
bysolvingthefollowingmulti-objectiveoptimizationproblem:
max

2

f
U
¹

º
;
^
B
¹

ºg
(1)
Thisproblemisconsideredinfairlearningliteratureinitsscalarized
formorconstraint-form[
21

25
],withthesocialutilitymodeling
anotionoffairness.Weassumeanidealsituationinwhichthe
chosenparametervector


isaPareto-optimalsolution[
26
],that
is,itcannotimproveanyoftheobjectiveswithouthurtingatleast
oneofthem.Pareto-optimality,however,isnotsu˝cienttoguaran-
teefairnessorequityasdi˛erenttrade-o˛sbetweentheobjectives
arepossible[
27
].WeassumethatoutofthepossibleParetoso-
lutions,theproviderchoosesonethatmaximizessocialutility:


,
argmax

2


^
B
¹

º
;
where


isthesetofallPareto-optimal
solutions.
3.1.1LimitationsintheFaceofExternalities.
Consider


,thesys-
temobtainedwhenusingthed'svievaluesofsocialutility:


,
argmax

2


B
¹

º
;
with


beingthesetofthecorresponding
Pareto-optimalsolutionswhenusing
B
inEq.1.
Let

B
,
B
¹


º
B
¹


º
.Wesaythatasystemwithparameters

induces
externalities
onitsenvironmentwhenthesocialutility
of

isnotequaltothatofthesystem


:

B
,
0
.
Thisde˙nitionisanalogoustotheneoclassicaleconomicinter-
pretationofexternalities:duetoan,apartialoptimum
(


)isachieved,thatisdi˛erentfromtheoptimumifnoine˝cien-
cieswerepresent(


)[
28
].Onecanalsoparallelthedivergence
betweensocial-utilityvaluesin

B
tothePigouviandivergenceof
socialandprivate[
29
].Becauseweareanalyzingharms,we
focusonnegativeexternalities:

B
<
0
.
Clearly,

B
=
0
when
B
=
^
B
.If
B
doesnotpreciselymatch
^
B
,
however,therewilllikelybeexternalitiespresent,asillustratedin
Fig.1.Ingeneral,itisnotknownhowexactly

B
isimpactedby
deviationsof
^
B
from
B
.InAppendixA,foraclassofstrictlyconcave
utilityfunctionsweshowthatthe
sensitivity
of

B
toin˙nitisemal
errorof
^
B
isapproximatelyquadraticinthemagnitudeoftheerror.
Intuitively,

B
growsquadraticallyfastas
^
B
diverges.
IncompleteInformationofSocialUtilities.
We˙rststudythecase
inwhichtheprovider'sviewofthesocialutility,
^
B
,doesnotfully
re˚ecttherequirementsandthecontextoftheapplicationdomain.
Socialbene˙tisoftenmodeledasafunctionincorporatingin-
dividualmodelsofsocialutility.Acommonassumptioninneo-
classicaleconomicsisthatsocialutilityisthesumofindividual
3




j

B
j
B
¹

º
U
¹

º
¹
U
;
^
B
º
¹
U
;
B
º
Figure1:Paretofrontierswithreal
¹
U
;
B
º
,andprovider's
version
¹
U
;
^
B
º
.Misspeci˙cationof
B
resultsinexternalities

B
,di˛erenceinvaluesofthebene˙tfunctionbetween
provider'ssystem
B
¹


º
and
B
¹


º
.
utilities[
30
]:
^
B
¹

º
=
Í
i
2
P
^
B
i
¹

º
;
where
^
B
i
:

!
R
isautilityof
individual
i
2
P
.Inpractice,however,thefair-by-designprovider
has
incompleteinformation
:theyareawareoftheusers,yetlack
thefullknowledgeoftheirneeds(similartoerfectknowledge
ineconomicsandgametheory[
31
]).Thatis,theycouldmisspec-
ify
^
B
i
¹

º
,
B
i
¹

º
forsomeuser
i
2
P
,inadvertentlyignoringthe
needsandwell-beingofthisindividual.Considerahypothetical
fair-by-designpredictivepolicing[
32
]applicationthatassumesthat
^
B
ismaximizedwhenthereisequalityoffalsepositivesinpatrol
dispatchesacrossregions.Theresultingdispatchingratesmight
befairforthisde˙nition,buttheoverallsocialbene˙t
B
mightbe
unchangedasminoritiescouldstillbeover-policedintermsofthe
numberofdispatches.
OmittingImpactonNon-UsersandEnvironmentalImpact.
Another
caseiswhenthefair-by-designproviderhasstructurallackof
knowledgeoftheapplicationdomain,e.g.,knowstheutilityof
theirusers,butnottheutilityofanythingelse(i.e.,ofphenomena
notsharedwiththemachine).Thus,afairsolutionfortheusers
couldharmnon-usersastheirbene˙tswereneverspeci˙edinthe
model
^
B
¹

º
.Thisisexempli˙edinthecaseofself-regulatinghous-
ingmarkets,whichcouldhypotheticallybeoptimizedforfairness
inacceptanceratesforguests[
33
],butdisruptneighborhoodsim-
pactinghousingconditions,especiallyforpeoplewithlowincomes.
3.1.2LimitationsUnderCompleteKnowledge.
Eveninthepres-
enceofcompleteinformationregarding
U
and
B
,therecanexist
externalitiesthattheprovidercannotmitigate.
First,theprovidercanmaximize
B
andyetcauseexternalitiesif
thesystem'sgoal
itself
isharmful[
34
].Forexample,afacialrecog-
nitionsurveillancesystemmightbecompletelyfairwithrespectto
skincolor,butitstillcausesrisksandharmstothepopulationasa
wholeintermsofprivacyloss.
Second,thefair-by-designmodelhasaspremisethatthereexists
asolution


that,ifnotmaximizes,thenatleastsatis˙esminimum
standardsofeveryone'sbene˙t.However,thismaynotbethecase.
RecalltheexamplesaboutunsafehousingorbadjobsinSec.2.2.In
thosecases,thefairoutcomemightstillbeharmfulforallusers.
3.2LimitationsonIdeals
Mitigatingexternalitiesbecomesagreaterissuewhenincentives,
capacities,andpowerstructuresarenotaligned.Onewaythis
manifestsitselfisinthemodelingofsocialbene˙tasafunction.
Indeed,afunctioncannotencodeallthenuanceregardinghuman
needs.Moreover,neithertheprovidernortheusers'utilityfunc-
tionsmodelthepoliticalcontextorthepowerasymmetriesinthe
environmentofthemachine[
35
].Thesecanheavilyin˚uenceor
skewwhatweassumetobethegroundtruth
B
.Thus,powerand
politicsmaycometorenderthesystemspeci˙cationunfaireven
whenitwasdesignedconsideringtheperfectbene˙tfunctionfor
users.
Furthermore,thefairness-by-designapproachinherentlyas-
sumesthattheprovideralwayshasenoughresourcestoimplement
thefairsolutionthatmaximizessocialutility.However,itisunrea-
sonabletobelievethatsuchanassumptionwillholdinpractice.For
instance,evenifWazewascognizantoftheneedsofallindividuals
andallthepeculiaritiesintheirstreets,itisunlikelythattheycould
a˛ordanoperationsinwhichallofthoseconstraintsaretakeninto
accountorbecapableofmitigatingtheimpactofexternalitiesdue
totheinteractionofmultipletra˝cbeatingapplications.
Finally,notallserviceprovidershavetheincentivestoimplement
fair-by-designsolutions.Theprovider
mightnotbeincentivized
to
careaboutsocialutilityorcouldbene˙tfromexternalizingcertain
costs.Insuchcases,themagnitudeoftheexternalities

B
islikely
tobemorepronouncedthaninthecaseofincompleteknowledge.
4PROTECTIVEOPTIMIZATION
TECHNOLOGIES
Thesystemsviewonalgorithmsinprevioussectionsenablesusto
systematicallyexploreaproblemspacethathadnotbeenformal-
izedbefore.Amajorsourceofproblemsistheuseofoptimization
techniquesthathelptocaptureandmanipulatephenomenainthe
applicationdomainsforthe
extractionofvalue
.Thispracticecauses
intentionalorunforeseenchangestotheenvironmentwhichresult
in(oftenneglected)harmstotheenvironment.
Weshowedthatexistingfairnessframeworksproducesolutions
thatcanonlyaddressasubsetoftheseharms,atthediscretionof
aserviceprovider.Theseframeworkshavelimitedcapabilityto
mitigateharmsarisingfrominequitiesinherenttotheapplication
domainandfromharmfulimpactsintroducedbythemachine.By
focusingsolelyonactionsthatcanbetakenbyserviceproviders,
whohaveoppositeincentives,fairnessframeworksnarrowboth
politicsandcontestationtothere-designofthealgorithmwhich
maynotalwaysbethesiteofeithertheproblemorthesolution.
Inthissection,wediscussmeanstoaddresstheseissues.Al-
thoughthesemeanscouldbeofsocio-legalnature,wefocuson
technologicalapproachesthatwecall
ProtectiveOptimizationTech-
nologies(POTs)
.Theirgoalistoshapetheapplicationdomainin
ordertorecon˙gurethephenomenasharedwiththemachineto
addressharmsintheenvironment.POTsaredesignedtobede-
ployedbyactorsa˛ectedbytheoptimizationsystem.Asthese
actorsdirectlyexperiencetheexternalities,a)theyhaveintimate
knowledgeofthesystem'snegativee˛ects,b)theyareinposition
tohaveabetterviewoftheirsocialutilitythanasystemprovider
canmoecauseitistheirownutility[
36
].Lastly,c)POTsdo
notrelyontheincentivesoftheprovider.
POTsseektoequipindividualsandcollectiveswithtoolsto
counterorcontesttheexternalitiesofthesystem.Theirgoalisnot
tomaximizebene˙tsforbothusersandserviceproviders,norto
˙ndthebeststrategytoenableoptimizationsystemstoextractvalue
4
Figure2:Uber:acomplexoptimizationsystem.Inputsto
Uberoptimization(black),e˛ectsontheenvironment(blue),
andalternativestodeployPOTs(red).
atminimumdamage.POTsareintendedto
eliminate
theharms
inducedbytheoptimizationsystem,oratleast
mitigate
them.In
othercases,POTsmay
shift
theharmstoanotherpartoftheproblem
spacewheretheharmcanbelessdamagingfortheenvironment
orcanbeeasiertodealwith.Finally,whenserviceprovidersreact
toreducethee˛ectivenessofPOTs,thisveryaction
exposes
the
serviceproviders'needtomaintainthepowerrelationshipandtheir
capabilitytomanipulatetheenvironmenttotheirownbene˙t.
POTsandOptimizationSystems.
Toextractvaluethroughoptimiza-
tion,serviceprovidersobtaininputsfromtheirenvironmentsthat
helpthemmakedecisions.Weconsiderthreekindsofinputs:i)
inputsthatusersgeneratewheninteractingwiththesystem,ii)
inputsaboutindividualsandenvironmentsreceivedfromthirdpar-
ties[
37
],andiii)inputsfromregulationsandmarketsthatde˙ne
thepoliticalandeconomiccontextinwhichthesystemoperates.
Notethatthirdpartiescanbepublicorprivate,andthedatathey
providecanbegatheredonlineoro˜ine.WeuseUberride-hailing
service[38]asanexampletoshowcasethiscomplexity(Fig.2).
Inordertomaximizeitspro˙t,Uberoptimizesthepriceso˛ered
toridersandthewageso˛eredtodrivers.Uberusesthefollowing
inputs(blackarrows).First,itusesthedirectinputsitreceivesfrom
bothridersanddrivers.Second,itusesdatafromothersources
suchasonlineserviceproviders,e.g.,Googleformapsanddata
thattheymightcollectusingcookiesorpixelsonothersitestheir
usersvisit[
39
].Uberalsoreceiveso˜inedatafrompartieslike
municipalitiesinterestedinpromotingtheuseofUbertoreduce
costsofpublictransport[
40
].Lastly,Uberusesinputsfromthe
marketandregulatorstoevaluatetheeconomiccontextinorderto
adjustwagesandrideprices.
Uberultimatelyusestheseinputsandallthepoliticalandeco-
nomiccontextinacombinationofmanagerialandmathematical
optimizationtodeliveroutcomestotheenvironment:matchrid-
erstodriversandsetrideprices.Reportsandstudiesdemonstrate
thattheseoutcomescauseexternalities:Uber'sactivityincreases
congestionandmisuseofbikelanes[
41
],increasespollution,and
decreasespublicsupportfortransit[42,43].
Intheprevioussections'vocabulary,Uberanditsoptimization
techniquesarethemachine;andtheapplicationdomaincomprises
Uberdriversandriders,non-users,theonlineando˜ineenviron-
ments,andthemarketandregulatoryframeworksinwhichUber
operates.Roughlyspeaking,Uber'srequirements
R
aretomatch
driverstoriderswithassociateddynamicpricing.Thespeci˙cation
S
de˙neswhatUberapplicationsshoulddotoful˙ll
R
.
Uberisknowntohaveunfairnessproblems.Forexample,Rosen-
blatetal.showthatcustomer-basedreviewsarebiasedagainst
minoritydrivers.Asgettingblockedfromthesystemdependson
thesereviews,eveniftheUber'salgorithmsdonotdiscriminate
driversontheirattributesperse,therulesofthesystemresultin
disparateimpactonblockeddrivers[
44
].Further,evenwhenUber
algorithmsarefair,e.g.,theyrewardalldriversthesameirrespective
oftheirprotectedattributes,theoptimizationprocessesunderlying
Uber'soperationresultinunjustoutcomes:lowwages[
45
].The
formerisanexternalitystemmingfromstructuralbiasesinthe
applicationdomain(thusahypothetical
S
fair
doesnotresulton
R
fair
);whereasthelatterisaproblemofincentivesmisalignment.
IntheUberscenario,POTscanbedeployedbyusersandnon-
userswiththegoalofchangingthephenomenacapturedbyUber.
Thesecancomeinthreeforms(redlinesinFig.2):bychangingthe
inputsoftheuserstothesystem(e.g.thesurge-inductionPOT[
46
]
aswedescribeshortlyinSec.4.1),bychangingtheonlineoro˜ine
signalsgatheredbyUber(e.g.,mayorschangingthecityurban
planning),orbya˛ectingthemarket(e.g.,bychangingregulations
ormandatingsalaryincreases[44]).
4.1ExamplesofPOTs
OurvisionforPOTssystematizestheuseoftechnologiesastools
tobothexplorethee˛ectsthatalgorithmsandoptimizationsys-
temshaveonoursociety,andthedesignofcountermeasuresto
contesttheirnegativee˛ectsintheabsenceofregulatoryorother
accountabilitymeasures.
Wenowrevisitrecentacademictechnologies,artisticinterven-
tions,anddeployedtools,thatcanbereframedasPOTs(Table1
providesasummary).POTsformalizesuchtechnologiesandinter-
ventions,enablingthesystematicstudyanddesignofsuchsolutions.
WeillustratethisinSec.5,wherewedesigntwoPOTsfromscratch.
Thesetechnologieshavedi˛erentorigins.First,weobservethat
therearetechnologiesproposedintheacademiathatcanbere-
purposedasPOTs.Forinstance,inthe˙eldofcomputersecurity,
researchthataimstoprotectagainstattackersgamingtheYouTube
algorithm[
51
]canberepurposedbyusersto˙ghtagainst˙lterbub-
bles;inthe˙eldofadversarialmachinelearning,toolsdevelopedto
evadecopyrightdetection[
48
],originallydevelopedtostrengthen
DRM,canbereframedasawaytopreventfair-usetakedowns[
64
].
Second,wedrawfromworksproducedbyartistslookingintothe
impactoftechnologyonsociety.Forinstance,counter-surveillance
fashionthattricksfacialrecognitiontechnologies[
62
]canbere-
purposedtoevadediscriminatoryfacialrecognitionalgorithms
ordiscriminatoryusesoffacialrecognition.Finally,welookto
deployedtechnologiesthatarealreadycounteringoptimization,
eitherintentionallyorasasidee˛ect.Forinstance,Jobscan[
56
]
assistsjobapplicantsingettingpasttheautomatedsortingimple-
mentedbylargecompaniesandjob-postingsites.Thistoolcould
berepurposedtoreducethegenderorracialbiasreportedforthese
tools[65].
5
Table1:RepurposingtechnologiesasPOTs.Wedetailtheoriginofthetechnology;theexternalityitaddresses,theoptimiza-
tionsystemcausingtheexternalityandthedesiredoutcomeofthePOT;deploymentrequirements(individualorcooperative
action);andtheunderlyingdesigntechniques(counter-optimizationorheuristicstodecidehowtoshapetheenvironment).
OriginOptimizationSystemExternalityPOTDesiredOutcomeDeploymentTechnique
AcademicFaceRecognitionPrivacy,discriminationWearprintedeyeglasses[47]EvadefacedetectionIndividualOptimization
AcademicCopyrightInfringementDetectionFairusetakedownsAdversarialexamples[48]AvoidafairusetakedownIndividualOptimization
AcademicPsychometricPro˙lingPrivacy,manipulationsTextstyletransfer[49,50]PreventfromattributeinferenceIndividualOptimization
AcademicYouTubeRecommendationsManipulationPoisoning[51]BreakingoutofcontentbubblesIndividualOptimization
AcademicWazeRoutingLocaltra˝ccongestionSybildevicessimulatetra˝c[52]PreventroutingintotownsIndividualHeuristic
AcademicGREScorerBiasedgradingsystemGenerateessaytopassGRE[53]HighertestscoreIndividualHeuristic
DeployedAdNetworkPrivacy,manipulationsClickonallads[54]AdNetworkdestroyedCollectiveHeuristic
DeployedUberPricingSystemLowwagesShuto˛app,turnitbackon[46]InducesurgeCollectiveHeuristic
DeployedInstacartPricingLowwagesTip22
g
inapp,cashatdoor[55]FairpayforjobsCollectiveHeuristic
DeployedAutomatedHiringBias,discriminationEditresume[56]FlipautomatedhiringdecisionIndividualOptimization
DeployedPokemonGoResourceSpawnUnfairnessEditOpenStreetMaps[57]EncourageresourcestospawnIndividualHeuristic
DeployedFitBitforInsurancePremiumPrivacy,surveillanceSpoofdevicelocation[58]Getinsurancebene˙tsIndividualHeuristic
DeployedPharmaOptimizingPatentsEndofhumanityFindpotentialdrugsusingML[59]GetdrugsinthepublicdomainIndividualMixed
DeployedInsuranceCoverageOptimizationHighcostsoftreatmentDoctorschangingclaimcodes[60]GethigherreimbursementsIndividualHeuristic
ArtisticFaceRecognitionPrivacy,surveillanceScarfthatisclassi˙edasaface[61]EvadefacedetectionIndividualOptimization
ArtisticFaceRecognitionPrivacy,surveillanceCamou˚agetocoverfeatures[62]EvadefacedetectionIndividualHeuristic
ArtisticAutonomousCarsExplorationrisksGroundmarkings[63]TrapautonomouscarsIndividualHeuristic
Theseexamplescanalsobecategorizedbasedonthemeans
usedtodesignthePOT:thosebasedrelyingonadversarialma-
chinelearning[
47
,
49

51
,
56
,
61
],andthosethatuseheuristicsto
exploitthetargetoptimizationprocess.Forinstance,asaresponse
tolowwages,Uberdrivershavedevelopedheuristicsforinducing
surges[46].
Finally,sometechnologiescanbedeployedindividuallyandoth-
ersrequirecollectiveaction.AdNauseam[
54
],forexample,reduces
theutilityofadnetworksbyclickingalladsservedtotheuser,˚ood-
ingthenetworkwithfalseinformation.Withoutacriticalmassof
users,however,AdNauseam'se˛ectwouldnotbeobservable.
5DESIGNINGPROTECTIVEOPTIMIZATION
TECHNOLOGIES:CASESTUDIES
Inthissection,weshowhow,givenanoptimizationsystemanda
negativeexternality,onecandesignnewPOTsfromscratch.
LikemanyofthetechnologiesandinterventionsinTable1,we
makeuseofoptimizationtechniquestodesigntheoperationof
POTs.StartingfromthemodelintroducedinSec.3,wemodelthe
optimizationsystem'sobjectiveasafunction
U
¹
x
;

º
:
X
m


!
R
;
where
X
m
denotesthespaceofenvironmentinputstothe
system(i.e.,phenomenathataresensedbythemachine),andthe
vector
x
2
X
m
isasetofinputscomingtothesystem.Each
concreteinput
x
i
cancomefromadi˛erentsource(users,non-
users,orotheractorsintheenvironment).Forsimplicity,wemodel
thetimedimensionthroughdiscretetimesteps,andweassume
thattherearetwopossibletimesteps:
0
and
1
.
Tomaximizeitsgain,theoptimizationsystemstrivestosolvea
mathematicaloptimizationproblem


t
=
argmax

U
¹
x
t
;

º
fora
setofinputs
x
t
ateachpointintime
t
.Giventhissystem,the
goalofaPOTis˙ndingandinexpensiv
modi˙cationstotheinputsoftheoptimizationsystemsoasto
maximizesocialutilitythatwedenoteas
B
pot
¹

º
.Notethatthis
de˙nitionofsocialutilityneedsnottocorrespondtothesocial
utilityconsideredbytheoptimizationsystem(
^
B
¹

º
inSection3).
Weconsiderthateachinputhasanassociatedmodi˙cationcost
C
¹
x
i
!
x
0
i
º
:
X

X
!
R
+
,thatrepresentshowharditisto
modifyit.Thecostofchangingasetofinputs
C
¹
x
!
x
0
º
canbe
anyfunctionoftheindividualcosts.
Inthismodel,wede˙nethePOTdesignasabi-levelmulti-
objectiveoptimizationproblem:
min
x
0
f
C
¹
x
!
x
0
º
;

B
pot
¹


t
+
1
ºg
s.t.


t
+
1
=
argmax

t
+
1
U
¹
x
0
;

t
+
1
º
(2)
where
x
isthevectorofinputsifnointerventionhappened,and
x
0
arepossiblevectorsofmodi˙edinputs.Parameter

t
+
1
represents
thesystem'sstateinthenextstep,afterthePOThasbeendeployed
throughthemodi˙edinputs
x
0
.
Weinstantiatethisproblemfortwodi˛erentusecases,one
wherethePOTcanbedeployedbyanindividual,andonewhere
e˛ectivedeploymentrequiresacollective.Thecodeforbothcase
studiesisavailableonline.
—
5.1ThwartingTra˝cfromRoutingApps
Inour˙rstcasestudy,welookatWaze,acrowdsourcedmobile
routingapplicationthatoptimizestheroutesofitsusersbased
oninformationabouttra˝c,roadclosures,etc.[
66
].Wazecauses
negativeexternalitiesforresidentsoftownsandneighbourhoods
thatareadjacenttobusyroutes.Forexample,takethetownof
Leonia,NewJersey,USA,whichliesjustoutsideoneofthebridges
intoNewYorkCity.AsWazeroseinpopularityanddirectedan
increasingamountofusersthroughthetownwhenthehighway
wasbusy,thetownbecamecrowdedduringrushhours.Toprevent
Wazetra˝c,thetownwasbrie˚yclosedo˛tonon-localtra˝c,
whichwasdeterminedillegal[
67
].Inthissectionweproposea
solutionfordiscouragingWazefromselectingroutesthroughthe
townwhileminimizingtheimpactonitsinhabitants.
ProblemSetup.
Wesetupthisproblemasaplanningproblemin
whichthetown'stra˝cnetworkismodeledasaweighteddirected
graph,andthegoalistoincreasethecostofpathsbetweenthe
highwayramps.Wede˙neWaze'sutility
U
asthecapabilitytopro-
videfastestroutesforitsusers.Routingthroughtowncanincrease
—
https://github.com/spring-ep˚/pots
6
thisutilitywhenittakeslesstimethantravelingviathehighway.
ThePOTisdesignedtoincreasetheminimumtimethroughtown
sothatWazeusersarenotroutedthroughit.Wede˙nethesocial
utility
B
pot
asabinaryvariablethattakesvalue1whennovehicle
isroutedthroughthetown(i.e.,thecostintimeoftraversingthe
townisgreaterthantraversingthehighway)and0otherwise.
Let
G
=
¹
V
;
E
;
t
º
beaweighteddirectedgraphrepresentingthe
tra˝cnetworkwithinthetown.Eachedge
¹
x
;
y
º2
E
ˆ
V

V
representsaroadsegment,andeachvertexrepresentsajunctionof
segments.Theedgeshaveassociatedtimetopassthesegmentgiven
bythefunction
t
¹
x
;
y
º
.Wede˙nethe
timecost
oftraversingapathin
thegraphasthetotaltimetopassitsedges:
t
¹
e
º
,
Í
¹
x
;
y
º2
e
t
¹
x
;
y
º
.
Let
a
;
b
2
V
bethesourceandsinkverticesthatrepresentthe
entrytothetownfromthehighway
¹
a
º
,andtheexittoreturnto
thehighway
¹
b
º
.Letthetimetotravelfrompoint
a
to
b
viathe
highwaybe
t

.Whilewedonotknowtheroutingalgorithmused
byWaze,weassumethatWazewillsendusersthroughthetown
whenthepaththroughtownisquickerthanthehighway.Thatis,
ifthereisapath
e
from
a
to
b
in
G
withcost
t
¹
e
º
<
t

,Wazeroutes
usersthroughthetown.
5.1.1AvoidingRoutingviaPlanning.
Weaimtotransformthe
graph
G
intoagraph
G
0
suchthatthetimecostofanypath
e
0
from
a
to
b
in
G
0
is
t
¹
e
0
º
t

.Wefocusonwhatthetowncan
control:timetotraversearoadsegment.Weexpressthesechanges
astheincreaseintimethatittakestotraverseasegment,

t
¹
x
;
y
º
.
Weabstracttheexactmethodforaddingtimetothesegment,which
couldbechangestospeedlimits,tra˝clights,stopsigns,speed
bumpsetc.Weconstruct
G
0
bymodifyingthetimevaluesinthe
originalgraph
G
:
t
0
¹
x
;
y
º
=
t
¹
x
;
y
º
+

t
¹
x
;
y
º
;
where
t
0
isafunction
representingtheedgetimein
G
0
.
Weacknowledgethatsomeroadsaremorecostlytochangethan
othersbyassociatingamodi˙cationcosttoeveryroad.Inpractice,
thiscostwillbespeci˙edbythetown.Weuse
length
ofaroad
segmentassuchacost,capturingthatchanginglongerroadswill
likelyhavemoreimpactonthetown.Let

¹
x
;
y
º2f
0
;
1
g
bebi-
nary
interdictionvariables
thatrepresentwhetherwearemodifying
theedge
¹
x
;
y
º
ingraph
G
0
.Toexpressthecost
C
ofmodifyinga
graph
G
into
G
0
weusethefollowingmodi˙cation-costfunction:
Í
¹
x
;
y
º2
E
c
¹
x
;
y
º

¹
x
;
y
º
;
where
c
:
E
!
R
+
representsthecostof
modifyingtheedge
¹
x
;
y
º
.
WenowformalizethePOTthroughposingthemulti-objective
optimizationprobleminEq.2inaconstraintform:minimize
C
subjecttothe
B
pot
constraint,giventhatWazeismaximizing
U
:
min

¹º2f
0
;
1
g
Õ
¹
x
;
y
º2
E
c
¹
x
;
y
º

¹
x
;
y
º
s.t.
t
¹
e
º
t

foranypath
e
from
a
to
b
(3)
Thisisequivalenttotheproblemknownas
shortest-pathnetwork
interdiction
[
68

71
].Intheformequivalenttooursitcanbesolved
asamixed-integerlinearprogram(MILP)[
70
,
71
].Wereferto
AppendixBfortheexactformulationoftheMILPinourcontext.
Weuseortools[
72
]forspecifyingthisMILPinPython,andthe
CBCopen-sourcesolver[73]forsolving.
5.1.2EmpiricalEvaluation.
WeapplythisPOTtothreetowns,all
ofwhichreportedissueswithWaze:Leonia,NJ,USA;Lieusaint,
France;andFremont,CA,USA.Weretrievethemapdataforeach
viatheOpenStreetMapsAPI[74].
Toassign
t
¹
x
;
y
º
toeachsegment
¹
x
;
y
º
,weestimatethetimeit
takestotraversethesegmentusingitslengthandtype(e.g.,we
considerresidentialstreetstohaveaspeedof25mph)andaddextra
timefortraversingtheintersection.Weinfertheintersectiontime
usingthetraveltimefromGoogleMapsforaccuracy.Foreach
city,werunthesolverfordi˛erent
t

values,correspondingto
di˛erenttraveltimesonthehighway.Largervaluesof
t

require
morechangestoroadsinthetown.
Figure3:SolutionsforLeonia,NJ(left)andLieusaint,France
(right),whenthetimeofroadsegmentsisallowedtobein-
creasedby75%.Townstreetsaremarkedinblue,highways
inorange,andsurroundingsingrey.Thereddotssignify
a
and
b
,theclosestpointstothehighwayinthetown.The
roadsmarkedbythick,blacklinesaretheoptimalsetofseg-
mentsinwhichthetimeshouldbeincreased.
Forsimplicityofimplementation,wechoosetwopointsatthe
edgeofthetown(reddotsinFig.3)andnotthesink/sourcepoints,
whichwouldbeonthehighway,forthesolver.Wethenaddthetime
thatittakestotravelfromtheactualsourcepointonhighwaytothe
˙rstpointandfromthesecondpointbacktothesinkpointinorder
thecalculatetheresults.ForLeonia,forinstance,weapproximated
thisas30secondsoneachsideofthetown.
Weconsiderscenariosinwhichthetownisabletoincreasethe
timethatittakestotraverseeachroadsegmentby25%,50%,and
75%.Foreachtownwefoundthevalueof
t

forwhichWazebegins
sendingcarsthroughthetownandthevalueof
t

inwhichno
furtherchangestotheroadcanpreventWazefromsendingits
usersthroughthetown.
ThegraphforLeonia,NJcontains251verticesand661edges.
Giventheparameterswechoose,withoutanychangestothetown,
WazebeginstosenditsusersthroughLeoniawhen
t

=
4
:
0
.That
is,itnormallytakes4minutestotravelthroughLeonia,sowhenit
takeslongerthan4minutestotraversethehighway,Wazeroutes
usersthroughtown.Thiscorrespondstotra˝ctravelingatabout
31mphonthehighway.Ifwelimittheamountoftimethatcan
beaddedtotraversearoadsegmentto25%oftheoriginaltime,
wecanpreventcarsfrombeingroutedthroughLeoniauntilthe
averagehighwayspeedhasreached26mph(
t

=
5
:
1
).Thatis,we
can˙ndasolutionthatpreventsroutingthroughtownaslongas
thehighwayspeedisgreaterthan26mph.For75%timeincrease,
wecanchangethetownsuchthattra˝cwillnotberoutedthrough
Leoniauntiltheaveragespeedonthehighwayhasfallento19mph.
ThissolutionforLeoniaisshowninFigure3.
7
LieusaintislargerthanLeonia,with645verticesand1,248edges.
Giventheparameterswechoose,Wazeroutesitsusersthrough
Lieusaintwhen
t

=
7
:
0
,whichcorrespondstothespeedonthe
highwaydroppingto16mph.Allowingtheroadsegmentstobe
loweredby75%,wecanpreventtra˝cfrombeingroutedthrough
Lieusaintuntilthehighwayspeedhasdroppedbelow10mph(
t

=
12
:
0
)(SeeLieusaintinFigure3).WereportthesolutionforFremont,
CA,asigni˙cantlylargertown,inAppendixB.
Finally,wemeasurethecostofimplementingthesesolutions
(Fig.4).Foreachtown,weconsidertheimpacttothetowntobe
howmuchlonger,onaverage,ittakestotravelbetweenanytwo
pointsintown.Wecomputetheshortestpathbetweeneverypair
ofpointsinthetownandaveragethesetimesbeforeandafter
thePOTsolution.Wethencomputethepercentageincreasefrom
theinitialaveragetimetothepost-POTaveragetime.Thehigher
impactsolutionsarethosewhichwilltoleratealowerhighway
speed.Thatis,theywillpreventcarsfrombeingroutedthroughthe
townatlowerhighwayspeeds.Weseethateventhoughallowing
roadsegmentstotake75%longertotraversecanpreventcarsfrom
enteringthetownatalowerhighwayspeed,theimpacttothetown
ismuchhigher.TheinhabitantsofLieusaintalsosu˛ermorefrom
thechangesthantheresidentsofthemuchsmallerLeonia.
Figure4:E˛ectofChangesonIn-TownTravel.
5.1.3POTImpactandLimitations.
Theinterventionweproposein
thissectionfocusesonalleviatingtheproblemsroutingapplications
causein
one
town.ThePOTwouldsupportthistown'sgovernment
todecidewhichchangesshouldbeimplementedinthecitylayout
andtra˝crulessothatthecostoftraversingthecitybecomes
undesirableforpassthroughdrivers.Whilethisinterventionindeed
mitigatesthee˛ectofexternaltra˝conthetargettown,itislikely
thatthenvehiclesareroutedelsewhere,i.e.,thePOT
shifts
the
harmsfromthisparticulartowntootherregions.Moreover,we
acknowledgethatourPOTcanonlyhelpincaseswhenvehiclescan
stillcirculateonthehighway.Themomentthecongestionforces
vehiclestohalt,thetownbecomesthebetteroptionregardlessof
anymodi˙cationonitsroadnetwork.
5.2ReducingFalseNegativesinCreditScoring
Inthiscasestudy,weexploresolutionsforcounteringharmful
e˛ectsofamachine-learning
creditscoring
system:Asystemthata
bankusestodecidehowtopricealoanapplicationaccordingto
thepredictedrisk.Theunderlyingalgorithmsthatsupportsuch
decisionsaredesignedtomaximizebanks'pro˙tsandminimize
theirrisks.Thesealgorithmscanbediscriminatory[
75
],orcause
feedbackloopsforpopulationsdisadvantagedbythe˙nancialsys-
tem[
76
].Theseharmsareoftencausedbyinputsthatrepresent
unjustrealitieswhicharepropagatedtothemodel'sdecisions.
ProblemSetup.
Wemodelthecreditscoringsystemasaclassi˙er
f

¹
x
º
thattakesasinputtheinformationabouttheloanapplicant
andtheloandetails,
x
,andoutputsacon˙dencescoreforwhether
theapplicantwouldrepaytheloanornot.Thisfunctionoptimizes
thebank'sutilitythatwemodelhereasthenegativeempiricalloss
overhistoricaldata:


,
argmin

2

Í
¹
x
;
y
º2
X
L
¹
x
;
y
;

º
;
where

istheparametersoftheclassi˙er,
L
isthelossfunction,and
X
is
thebank'sdatasetofhistoricalloans(
x
)andtheirrepaymentor
defaultoutcomes(
y
).Weassumethattheclassi˙erisretrainedas
newdatapointsarrive.
Wemodeltheharmsofthesystemastherateoffalsenegatives
(wrongdefaultpredictions)foreconomicallydisadvantagedpop-
ulations.InthisPOT,wecounterthisproblemusingadversarial
machinelearningtechniquesdeployedbyacollectiveofindividuals
G
pot
withthemeanstotakeandrepayloans,asexplainedshortly.
ThisPOTaimstoincreasethesocialutilityde˙nedasthenegative
lossona
targetgroup
G
:
B
pot
¹

º
,

Í
¹
x
;
y
º2
G
L
¹
x
;
y
;

º
;
where
G
isthedisadvantagedsubsetofapplicants(inthiscasestudy,we
de˙nedisadvantagedashavinglittlefundsinthebankaccount)
whowerewrongfullydeniedaloan.ThisPOTcanbethoughtas
promotingequalityoffalse-negativeratesbetweenthetargetgroup
andeveryoneelse[
77
,
78
],withthedi˛erencethatwedonotlimit
ourviewofexternalitiestoacommonlyprotectedsubgroup,i.e.,
economicaldisadvantageisnotcommonlyconsideredasprotected.
5.2.1ReducingFalseNegativeswithAdversarialMachineLearning.
We˙rstidentifywhatinputs(
x
intheabstractmodel)canbemodi-
˙edbythecollective
G
pot
deployingthePOT.First,thedeployers
canonlyaddnewinputstothedatasetbytakingandrepaying
loans.Second,thedemographicattributesoftheseaddedloanap-
plicationshavetobesimilartothoseofindividualsin
G
.Thus,the
POTmustinformthecollective
G
pot
aboutwhoandforwhichloans
theyshouldapplyforandrepay,insuchawaythattheyreduce
thefalse-negativerateonthetargetgroupafterretraining.This
POTisidealisticinthatitassumesthatthiscollectivewillinclude
applicantsofdiversebackgroundstobeabletoprovidedi˛erent
inputstotheclassi˙er.However,intheabsenceofothermeansof
feedback,communication,andaccountability,itrepresentstheonly
meanstoin˚uenceanunjustsystem.Itisalsoconsistentwiththe
existingpracticespeopleresorttointhissetting[79,80].
Findingwhichinputstoinjectintoatrainingdatasettomodifyits
outputsisknownas
poisoning
inadversarialmachinelearning[
81
].
8
Typically,poisoningattacksaimtoincreasetheaverageerrorof
theclassi˙erorincreasetheerroronspeci˙cinputs[
82
].Wenote
thatouruseofpoisoningisdi˛erent.First,wepoisonto
decrease
theerror
foragiventargetgroup.Second,ouruseofpoisoningis
notadversarial.Onthecontrary,weuseittoprotectusersagainst
harmfule˛ectsofthemodel.
Withthisinmind,wedesignthePOTusingthefollowingbi-level
optimizationproblem:
min
J
¹


º
=
Õ
¹
x
;
y
º2
G
L
¹
x
;
y
;


º
+

R
¹


º
s.t.


=
argmin

2

Õ
¹
x
;
y
º2
X
[
X
pot
L
¹
x
;
y
;

º
f

0
¹
x
º
=
`accept'forall
¹
x
;
y
º2
X
pot
X
pot
ˆ
X
pool
;
j
X
pot
j
n
where

0
isthecurrentparametersoftheclassi˙er,
X
pot
isthe
set
ofpoisonedapplications
,
n
isthemaximumnumberofpoisoned
applications,and
X
pool
isa
setoffeasibleloanapplications
.That
is,weminimizetheclassi˙er'slossforthetargetgroup,where
theclassi˙er


istrainedusingthepoisonedexamples
X
pot
.In
ourevaluationbelow,weadditionallymakeuseofaregularizer
R
tominimizethee˛ectofpoisoningonanyotherapplications:
R
¹

º
,

Í
x
;
y
2
X
L
¹
x
;
f
¹
x
º
;

º
:
Thisformulationmakestwoassumptions.First,weassumethat
whendesigningthePOTwehaveaccesstothetrainingdatasetof
theprovider
X
,andwecanobtainretrainedloan-approvalmodels.
Weconsiderthisassumptionreasonableaspoisoningattackstend
totransferevenifthedatasetandmodeldonotmatchtherealones,
especiallywhenthemodelshavelowcomplexity[
83
].Second,the
addedloanapplicationsmustbefeasible:therehastoexistaperson
inthe
G
pot
withdemographicsrequiredforthisloanapplication.
Wesolvethisproblembyscoringeachexamplein
X
pool
accord-
ingtothevalueofouroptimizationobjective
J
¹


º
,retraining


foreachexample,andthenemployingagreedyalgorithmtoassem-
ble
X
pot
thatsatis˙estheconstraints.WerefertotheAppendixC
forthedetailsofthealgorithm.
5.2.2EmpiricalEvaluation.
Wecreateasimulatedloanapproval
systemusingtheGermancreditriskdatasetfromtheUCIMachine
LearningRepository[
84
].Thisdatasetcontains1,000featurevec-
torsrepresentingloanapplications,includingapplicants'jobtype,
bankaccountdetails,gender,etc.,andloandetails:amount,dura-
tion,andpurpose.Eachhasabinarylabelencodingwhetherthe
loanwasrepaid(70%)ornot(30%).Weimplementtheloanapproval
systemasalogisticregressionclassi˙er.Foroursimulation,we
splitthedatasetintothebank'strainingdataset
X
(800examples),
andthetestset(200).Theclassi˙erachieves75.5%testaccuracy.
Weobtainedthebestresultswiththeuseof

=
0
:
5
asthetrade-o˛
parameterfortheregularizationterm(outof0.25,0.5,0.1)in
J
¹


º
.
Wesimulatethesetoffeasibleapplications
X
pool
usingasubsetof
successfullyrepaid
applicationsfromusersthatarenotinthetarget
groupbygeneratingallpossiblechangesofmodi˙ableattributes.
WeevaluateourPOTintwosettings:asettinginwhich
theonlyapplicationsreceivedbythebankarethosefromthecol-
lective(
X
[
X
pot
);andasettinginwhichotherpeopletake
loansbetweenthetimewhenthePOTisdeployedandthetime
Figure5:E˛ectsofthePOTontheerrorratesoftheclas-
si˙erforthetargetgroup.Top:decreaseinthenumberof
falsenegatives(wronglydeniedloans).Bottom:increasein
thenumberoffalsepositives(wronglygrantedloans).
whentheclassi˙erisretrained.Wereporttheresultsinthenoisy
settingfor10and50additionalloans(1.25%and6.25%oftheorigi-
naldataset,respectively).Werepeatthenoisyexperiments10times
inwhichwedrawtheloanapplicationsatrandom.
Wepresentthee˛ectofpoisoningonthetargetgroupandev-
eryoneelseinFig.5,top.Unsurprisingly,themorepoisoningappli-
cationsthereare,themoresigni˙cantthee˛ect.With10poisoning
applications(1.25%ofthetrainingdataset),ouralgorithmreduces
thenumberoffalsenegativesinthetargetgroupby9;inthepres-
enceofnoisethisdecreasesto7.Thefalsenegativesintherestof
thedatasetareonaveragenotimpactedbyourPOT.
5.2.3POTImpactandLimitations.
Unfortunately,thePOTincreases
thefalsepositives(Fig.5,bottom).Thatis,it
shifts
theharmtothe
bank,whichwouldgivemoreloanstopeoplewhocannotrepay,
increasingitsrisk.Thise˛ectsharplyincreaseswiththeseventh
application(outof10).Thisisduetothepoisoninginputsstarting
tochangethemodelparametercorrespondingtotheloanpurpose.
Inturn,thisincreaseinfalsepositivescouldleadtoadversesocial
impactovertimeifbankstrytocounterthePOTe˛ect[
85
].The
POTdeployerscouldadjustthetrade-o˛parameter

tocontrolfor
theside-e˛ects.
Tobeconsistentwiththefairnessliterature[
77
,
86
],weassume
thatthetargetgroupisinterestedingettinghighercreditratings
(throughdecreaseinfalse-negatives).Thismodelthatpostulates
theaccesstocreditasbene˙cial,however,isnaïve.Eventhoughthe
loan-makingprocessthatreliesonrisk-basedpricinghaseconomic
backing,byde˙nitionitimpliesthattheless˙nanicallyadvantaged
peoplewillgetmoreexpensiveloans.Hence,aninterventionthat
aimsatincreasinginclusionofdisadvantagedpopulationsinthe
lendingprocesscanbeseenasaninstanceof
predatoryinclusion
[
76
,
87
,
88
].Evenifitresultsinlowerloanpricesintheshortterm,it
canleadtodispossessioninthelongrun.Whenharmsareviewed
9
throughthislens,Itisnotclearifanytechnologicalinterventionis
capableofcounteractingsuchsystems.
6DISCUSSION
Ourinspirationtoprovideamoreholisticviewonoptimization
systemsandtheirharmscomesfromworksthatpointtothelogic
andpotentialimpactofoptimizationsystems.Inparticular,Poon
hasdrawnattentiontothewaysinwhichoptimizationsystemsare
drivenbyoutcomes,asexempli˙edinourutilityfunctionsinSec.3.
Thisallowsfortechniqueslikeoperationalcontrolandstatistical
managementtobetheprimarymodewithwhichmachinesinteract
withphenomenaintheworld[
76
].Asaresult,thesetechniques
functionbothasameansforengineeringandasmathematical
statethatposesasasolutiontopolitical[
89
].Opti-
mizationisatechniquelongestablishedin,forexample,resource
allocation.However,itsincreasingsupremacyinsystemsthatre-
con˙gureeveryaspectoflife(education,care,health,love,work
etc.)forextractionofvalueisnewandrefashionsallsocial,political,
andgovernancequestionsintoeconomicones.Thisshiftallows
companiestocommodifyaspectsoflifeinawaythatcon˚ateshard
questionsaroundresourceallocationwithmaximizationofpro˙t
andmanagementofrisk[
90
].Theimpactissubtlebutfundamental,
asevidentinthewayevenwestartframingcomplexandhistorical
questionsofjusticeintermsofutilityfunctions.
Togroundourselvesintheworld,weusedtherequirements
engineeringmodelofMichaelA.Jackson,aligningitwithcallsfor
decenteringtechnologyandcenteringcommunitiesinthedesign
ofsystems[
91
].Weextendedthemodeltoensurethatwearenot
suggestingthatproblemsandsolutionsareneatlyseparableand
voidofpowerandpoliticaleconomy[
92
].However,manyelements
thatmightbecrucialforconceptualizingoptimizationsystemsare
stillmissing.Theontologyo˛ersfewconceptstocapturematters
arounddata,machinelearning,orservices,anditdoesnotpro-
videdeeperinsightsintoaddressingissueslikefairnessorjustice.
Itis,however,anodtotheimportanceofontologicalworkfor
systematizingre˚ectionsonourframeworks[93,94].
Tocapturethepoliticaleconomyofoptimizationsystems,we
turnedtoutilitarianmodelsandcalculationsofexternalities.Such
modelsarecommonlyusedbothinmathematicalandmanagerial
formsofoptimizationandarethecornerstoneofneoclassicaleco-
nomics.However,utilitarianmodelshavebeenthoroughlycritiqued
for,amongothers,perpetuatinginequalities[
35
].Mostprominently,
Senhashighlightedthelimitationsofassessingvaluethroughcon-
sequences,assessingvaluethroughsubjectiveutility,maximizing
welfarewithoutregardforitsdistribution,andfetishizingresources
overrelationsbetweenresourcesandpeople[
95
].Overall,utilitar-
ianapproachesareweakincapturingcollectiveinterests,social
well-being,formsofpower,andsubjugation.Giventhesecritiques,
thecentralrolethatthesemodelsplayindesigninglarge-scale
optimizationsystemsisaprobleminandofitself.
Onepossiblewayforwardistoconsideralternativeeconomic
modelsforthedesignandevaluationofsystems,e.g.,[
35
,
96

98
].
POTsdependandbuildontheexistenceofsuchalternativeeco-
nomicmodelsandtheavailabilityofcollectivity,altruismandreci-
procity.Theyassumethereissomethingtobegainedbothindi-
viduallyandcollectively,dismissingthesel˙shagentspresumed
inutilitarianapproaches.Infact,westruggledtoexpressPOTsin
theutilitarianlogic:ifweoptimizefortheutilityoftheservice
provideritishardtojustifyanyPOTthatmayreducetheutilityof
theserviceprovider.
Beyondeconomicgains,POTsstrategicallysupportpeople's
agency.Optimizationsystemso˛erlittleagencytoe˛ectivelycon-
testtheirvalueproposition[
75
,
99

101
]ando˛ermoreoptimiza-
tionassolutionsforexternalities[
102
].POTscanbeusedtoexercise
someagencytowardsanunaccountable[
103
]ande
[
104
].Nevertheless,serviceprovidersmayarguethatPOTs
aregamingthesystem.Ourfocusisonsocial-justicecontexts,
inwhichPOTscanbecastaseaponsofthegefortheleast
equippedtodealwiththeconsequencesofoptimization[
105
].POTs
canalsoservetoexposesystems'injustices,achievingtransparency
andaccountabilitygoals.Inthatsense,theyalsocancometoact
like
rhetoricalsoftware
turnthelogicofasystemagainstit-
self[...]todemonstrateits[
106
].Thisincludesmaking
apparentthedamagingresultsofutilitarianformsofgovernance
prominentinoptimizationsystems.
Despitetheirpositivepotential,designinganddeployingPOTs
isnottrivial.Byvirtueofmodifying,subverting,orsabotaging
anoptimizationsystem,POTsmayelicittransitionsinthesystem
statethatresultinexternalities.IfseveralPOTsaredeployedand
enterinanarmsrace,thoseagentswiththemostknowledgeand
resourcesarelikelytodeploythemostaggressiveande˛ective
POTsandhavethemostleverage.Thisunderminestheabilityof
lesspowerfulpopulations,whomayneedPOTsthemost,tohave
anye˛ectonthesystem.Thissignalsthatwell-thoughtPOTsmust
bebuilttoprovidelesspowerfulactorswiththemeanstorespond
tothepotentialabuseofpowerbythosethathavemorecapabilities.
Justasmuch,themulti-inputformofmostoptimizationsystems
posesaseriouschallenge:whenoptimizationisbasedoncontinu-
oustrackingacrossmanychannels,POTscannotbebuiltshortof
creatingoptimizedofentitiesintheenvironments[
107
].
Thefactthatawholeinfrastructureforoptimizingpopulationsand
environmentsisbuiltinawaythatminimizestheirabilitytoobject
toorcontesttheuseoftheirinputsforoptimizationisofgreat
concerntotheshouldbealsotoanyonewhobelieves
inchoice,subjectivity,anddemocraticformsofgovernance.
ACKNOWLEDGEMENTS
WeareindebtedtoMarthaPoonforheroriginalframingofthe
optimizationproblem.WealsothankEroBalsaforhiscollaboration
onthepreviousversionsofthiswork,JakubTarnawskiforhis
invaluablehelpwith˙ndingtherighttoolforthetra˝c-routing
casestudy,andMuhammadBilalZafarforhishelpfulcomments.
ThisworkwassupportedinpartbytheResearchCouncilKULeu-
ven:C16/15/058,andtheEuropeanCommissionthroughKULeuven
BOFOT/13/070,H2020-DS-2014-653497PANORAMIX,andH2020-
ICT-2015-688722NEXTLEAP.SedaGürsesissupportedbyaRe-
searchlanders(FWO)Fellowship.RebekahOverdorf
issupportedbytheOpenTechnologyControls
Fellowship.
10
REFERENCES
[1]
KarenYeung.Astudyoftheimplicationsofadvanceddigitaltechnologies
(includingAIsystems)fortheconceptofresponsibilitywithinahumanrights
framework.
MSI-AUT
,2018.
[2]
BrentMittelstadt,ChrisRussell,andSandraWachter.Explainingexplanations
inAI.In
ConferenceonFairness,Accountability,andTransparency(FAT*)
,2019.
[3]
SimoneBrowne.
Darkmatters:Onthesurveillanceofblackness
.DukeUniversity
Press,2015.
[4]
OscarHGandy.
Comingtotermswithchance:Engagingrationaldiscrimination
andcumulativedisadvantage
.Routledge,2016.
[5]
JuliaAngwin,Je˛Larson,SuryaMattu,andLaurenKirchner.Machinebias:
There'ssoftwareusedacrossthecountrytopredictfuturecriminals.andit's
biasedagainstblacks.
ProPublica
,2016.https://www.propublica.org/article/
machine-bias-risk-assessments-in-criminal-sentencing,visited2019-12-05.
[6]
DanielleEnsign,SorelleA.Friedler,ScottNeville,CarlosScheidegger,andSuresh
Venkatasubramanian.Runawayfeedbackloopsinpredictivepolicing.In
Con-
ferenceonFairness,AccountabilityandTransparency,FAT*
,2018.
[7]
SolonBarocas,MoritzHardt,andArvindNarayanan.
FairnessandMachine
Learning
.fairmlbook.org,2018.http://www.fairmlbook.org,visited2019-12-05.
[8]
RebekahOverdorf,BogdanKulynych,EroBalsa,CarmelaTroncoso,andSedaF.
Gürses.Questioningtheassumptionsbehindfairnesssolutions.In
Critiquing
andCorrectingTrendsinMachineLearning
,2018.
[9]
SedaGursesandJorisvanHoboken.Privacyaftertheagileturn.InEvan
Selinger,JulesPolonetsky,andOmerTene,editors,
TheCambridgeHandbookof
ConsumerPrivacy
,pagesCambridgeUniversityPress,2018.
[10]
IrinaKaldrackandMartinaLeeker.
ThereIsNoSoftware,thereAreJustServices
,
pagesmesonpress,2015.
[11]
MichaelJackson.Theworldandthemachine.In
InternationalConferenceon
SoftwareEngineering(ICSE)
,1995.
[12]
MichaelJackson.
SoftwareRequirementsandSpeci˙cations:Alexiconofpractice,
principlesandprejudices
.Addison-Wesley,1995.
[13]
SamLavigne,BrianClifton,andFrancisTseng.Predicting˙nancialcrime:
Augmentingthepredictivepolicingarsenal.
CoRR
,2017.
[14]
AirBnB.https://airbnb.com,visited2019-12-05.
[15]
GabyHinsli˛.Airbnbandtheso-calledsharingeconomyishollowingout
ourcities.
TheGuardian
,2018.https://www.theguardian.com/commentisfree/
2018/aug/31/airbnb-sharing-economy-cities-barcelona-inequality-locals,vis-
ited2019-12-05.
[16]
MuhammadAli,PiotrSapiezynski,MirandaBogen,AleksandraKorolova,Alan
Mislove,andAaronRieke.Discriminationthroughoptimization:HowFace-
book'saddeliverycanleadtoskewedoutcomes.
CoRR
,2019.
[17]
SarahBird,SolonBarocas,KateCrawford,andHannaWallach.Exploringor
exploiting?socialandethicalimplicationsofautonomousexperimentationinAI.
In
WorkshoponFairness,Accountability,andTransparencyinMachineLearning
(FAT-ML)
,2016.
[18]
ZeynepTufekci.Youtube,thegreatradicalizer.
TheNewYorkTimes
,10,2018.
[19]
DavidA.Starrett.Economicexternalities.In
FundamentalEconomicsVolI.
EOLSS,2011.
[20]
TheophileCabannes,FrankShyu,EmilyPorter,ShuaiYao,YexinWang,Marco
AntonioSangiovanniVincentelli,StefanusHinardi,MichaelZhao,andAlexan-
dreM.Bayen.Measuringregretinrouting:Assessingtheimpactofincreased
appusage.In
InternationalConferenceonIntelligentTransportationSystems,
ITSC
,2018.
[21]
ToshihiroKamishima,ShotaroAkaho,andJunSakuma.Fairness-awarelearning
throughregularizationapproach.In
IEEEInternationalConferenceonData
MiningWorkshops
,2011.
[22]
MuhammadBilalZafar,IsabelValera,ManuelGomezRodriguez,andKrishnaP
Gummadi.Fairnessbeyonddisparatetreatment&disparateimpact:Learning
classi˙cationwithoutdisparatemistreatment.In
InternationalConferenceon
WorldWideWeb(WWW)
,2017.
[23]
MuhammadBilalZafar,IsabelValera,ManuelGomez-Rodriguez,andKrishnaP.
Gummadi.Fairnessconstraints:Mechanismsforfairclassi˙cation.In
Interna-
tionalConferenceonArti˙cialIntelligenceandStatistics,AISTATS
,2017.
[24]
L.ElisaCelis,DamianStraszak,andNisheethK.Vishnoi.Rankingwithfairness
constraints.In
ICALP
,2017.
[25]
MicheleDonini,LucaOneto,ShaiBen-David,JohnSShawe-Taylor,andMas-
similianoPontil.Empiricalriskminimizationunderfairnessconstraints.In
AdvancesinNeuralInformationProcessingSystems(NeurIPS)
,2018.
[26]
KaisaMiettinen.
Nonlinearmultiobjectiveoptimization
.SpringerScience&
BusinessMedia,2012.
[27]
JulianLeGrand.Equityversuse˝ciency:theelusivetrade-o˛.
Ethics
,1990.
[28]
CarlJDahlman.Theproblemofexternality.
Thejournaloflawandeconomics
,
1979.
[29]
ArthurPigou.
Theeconomicsofwelfare
.London:McMillanandCo.,1920.
[30]
NgYew-Kwang.
WelfareEconomics:IntroductionandDevelopmentofBasic
Concepts
.Springer,1983.
[31]
LouisPhlips.
Theeconomicsofimperfectinformation
.CambridgeUniversity
Press,1988.
[32]
KristianLumandWilliamIsaac.Topredictandserve?
Signi˙cance
,2016.
[33]
KatieBenner.Airbnbvowsto˙ghtracism,butits
userscan'tsuetopromptfairness.
TheNewYorkTimes
,
2016.https://www.nytimes.com/2016/06/20/technology/
airbnb-vows-to-˙ght-racism-but-its-users-cant-sue-to-prompt-fairness.
html,visited2019-12-05.
[34]
OsKeyes,JevanHutson,andMeredithDurbin.Amulchingproposal:Analysing
andimprovinganalgorithmicsystemforturningtheelderlyintohigh-nutrient
slurry.In
ExtendedAbstractsofthe2019CHIConferenceonHumanFactorsin
ComputingSystems
,2019.
[35]
MarilynPower.Socialprovisioningasastartingpointforfeministeconomics.
Feministeconomics
,2004.
[36]
PaulDolan.Themeasurementofindividualutilityandsocialwelfare.
Journal
ofHealthEconomics
,1998.
[37]
CarlosBarrenecheandRowanWilken.Platformspeci˙cityandthepoliticsof
locationdataextraction.
EuropeanJournalofCulturalStudies
,2015.
[38]
uber.https://uber.com,visited2019-12-05.
[39]
uber.Uber:Cookienotice.https://www.uber.com/legal/privacy/cookies/en/,
visited2019-12-05.
[40]
Uber.ThestoryofInnis˙l.2019.https://www.uber.com/en-CA/blog/
the-story-of-innis˙l/,visited2019-12-05.
[41]
FredrickKunkle.SanFranciscopolicesaymosttra˝cticketsgotoUberand
Lyftdrivers.
WashingtonPost
,2017.
[42]
MichaelGraehler,AlexMucci,andGregoryErhardt.Understandingtherecent
transitridershipdeclineinmajoruscities:Servicecutsoremergingmodes?In
TransportationResearchBoard98thAnnualMeeting
,2019.
[43]
AngieSchmitt.AllthebadthingsaboutUberandLyftin
onesimplelist.2019.https://usa.streetsblog.org/2019/02/04/
all-the-bad-things-about-uber-and-lyft-in-one-simple-list/,visited2019-12-05.
[44]
AlexRosenblat,KarenECLevy,SolonBarocas,andTimHwang.Discriminating
tastes:Uber'scustomerratingsasvehiclesforworkplacediscrimination.
Policy
&Internet
,2017.
[45]
MichaelSainato.Imade$3.75anhour:LyftandUberdriverspushtounionizefor
betterpay.
TheGuardian
,2019.https://www.theguardian.com/us-news/2019/
mar/22/uber-lyft-ipo-drivers-unionize-low-pay-expenses,visited2019-12-05.
[46]
SamSweeney.Uber,Lyftdriversmanipulatefaresatreagannational
causingarti˙cialpricesurges.
WLJA
,2019.https://wjla.com/news/local/
uber-and-lyft-drivers-fares-at-reagan-national,visited2019-12-05.
[47]
MahmoodSharif,SrutiBhagavatula,LujoBauer,andMichaelK.Reiter.Acces-
sorizetoacrime:Realandstealthyattacksonstate-of-the-artfacerecognition.
In
ConferenceonComputerandCommunicationsSecurity(CCS)
,2016.
[48]
ParsaSaadatpanah,AliShafahi,andTomGoldstein.Adversarialattackson
copyrightdetectionsystems.
CoRR
,2019.
[49]
ZhenxinFu,XiaoyeTan,NanyunPeng,DongyanZhao,andRuiYan.Style
TransferinText:ExplorationandEvaluation.In
AAAIConferenceonArti˙cial
Intelligence
,2018.
[50]
CíceroNogueiradosSantos,IgorMelnyk,andInkitPadhi.Fightingo˛ensive
languageonsocialmediawithunsupervisedtextstyletransfer.
CoRR
,2018.
[51]
MinghongFang,GuoleiYang,NeilZhenqiangGong,andJiaLiu.Poisoning
attackstograph-basedrecommendersystems.2018.
[52]
GangWang,BolunWang,TianyiWang,AnaNika,HaitaoZheng,andBenY.
Zhao.Ghostriders:Sybilattacksoncrowdsourcedmobilemappingservices.
IEEE/ACMTrans.Netw.
,2018.
[53]
LesPerelman.Babelgenerator.http://lesperelman.com/
writing-assessment-robo-grading/babel-generator/,visited2019-12-05.
[54]
DanielC.HoweandHelenNissenbaum.https://adnauseam.io,visited2019-12-
05.
[55]
JoshEidelson.Theseinstacartworkerswantyoutoleavethema22-cent
tip.
Bloomberg
,2019.https://www.bloomberg.com/news/articles/2019-01-17/
these-instacart-workers-want-you-to-leave-them-a-22-cent-tip,visited2019-
12-05.
[56]
Jobscan.https://jobscan.co,visited2019-12-05.
[57]
AllanaAkhtar.IsPokémonGoracist?howtheapp
mayberedliningcommunitiesofcolor.
USAToday
,
2016.https://usatoday.com/story/tech/news/2016/08/09/
pokemon-go-racist-app-redlining-communities-color-racist-pokestops-gyms/
87732734/,visited2019-12-05.
[58]
TegaBrainandSuryaMattu.https://www.un˙tbits.com,visited2019-12-05.
[59]
FrancisTsengandSeanRaspet.https://matter.farm/about/,visited2019-12-05.
[60]
MatthewKWynia,DeborahSCummins,JonathanBVanGeest,andIraBWilson.
Physicianmanipulationofreimbursementrulesforpatients:betweenarock
andahardplace.
Jama
,2000.
[61]
AdamHarvey.https://ahprojects.com/hyperface/,visited2019-12-05.
[62]
AdamHarvey.https://cvdazzle.com/,visited2019-12-05.
[63]
JamesBridle.https://jamesbridle.com/works/autonomous-trap-001,visited
2019-12-05.
11
[64]
ElectronicFrontierFoundation.Aguidetoyoutuberemovals.https://www.e˛.
org/issues/intellectual-property/guide-to-youtube-removals,visited2019-12-
05.
[65]
Je˛reyDastin.AmazonscrapssecretAIrecruitingtoolthatshowedbiasagainst
women.
Reuters
,2018.
[66]
Waze.https://www.waze.com,visited2019-12-05.
[67]
SvetlanaShkolnikova.Leonia'sroadclosureordinanceruledinvalid.
North
JerseyRecord
,2018.https://northjersey.com/story/news/bergen/leonia/2018/08/
30/leonia-nj-road-closure-ordinance-ruled-invalid/1145418002/,visited2019-
12-05.
[68]
BruceGolden.Aprobleminnetworkinterdiction.
NavalResearchLogistics
Quarterly
,1978.
[69]
DelbertRayFulkersonandGaryCHarding.Maximizingtheminimumsource-
sinkpathsubjecttoabudgetconstraint.
MathematicalProgramming
,1977.
[70]
EitanIsraeliandRKevinWood.Shortest-pathnetworkinterdiction.
Networks:
AnInternationalJournal
,2002.
[71]
XiangyuWei,ChengZhu,KaimingXiao,QuanjunYin,andYabingZha.Shortest
pathnetworkinterdictionwithgoalthreshold.
IEEEAccess
,2018.
[72]
ortools.https://developers.google.com/optimization,visited2019-12-05.
[73]
JohnForrest,StefanVigerske,TedRalphs,HaroldoGambiniSantos,LouHafer,
BjarniKristjansson,JPFasano,EdwinStraver,MilesLubin,rlougee,jpgoncal1,
h-igassmann,andMatthewSaltzman.coin-or/cbc:Version2.10.3,2019.
[74]
OpenStreetMap.https://openstreetmap.org,visited2019-12-05.
[75]
DanielleKeatsCitronandFrankPasquale.Thescoredsociety:dueprocessfor
automatedpredictions.
WashingtonLawReview
,2014.
[76]
MarthaPoon.Corporatecapitalismandthegrowingpowerofbigdata:Review
essay.
Science,Technology,&HumanValues
,2016.
[77]
MoritzHardt,EricPrice,andNatiSrebro.Equalityofopportunityinsupervised
learning.In
AdvancesinNeuralInformationProcessingSystems(NeurIPS)
,2016.
[78]
AlexandraChouldechova.Fairpredictionwithdisparateimpact:Astudyof
biasinrecidivismpredictioninstruments.
Bigdata
,2017.
[79]
ZackFriedman.Howpersonalloanscanboostyourcreditscore.
Forbes
,May2017.https://www.forbes.com/sites/zackfriedman/2017/05/25/
personal-loans-credit-score/#79bce94175ba,visited2019-12-05.
[80]
BevO'Shea.Whatisacredit-builderloan?
Forbes
,April2018.https://www.
nerdwallet.com/blog/˙nance/what-is-credit-builder-loan/,visited2019-12-05.
[81]
NicolasPapernot,PatrickD.McDaniel,AruneshSinha,andMichaelP.Wellman.
Towardsthescienceofsecurityandprivacyinmachinelearning.
CoRR
,2016.
[82]
BattistaBiggioandFabioRoli.Wildpatterns:Tenyearsaftertheriseofadver-
sarialmachinelearning.
PatternRecognition
,2018.
[83]
AmbraDemontis,MarcoMelis,MauraPintor,MatthewJagielski,BattistaBiggio,
AlinaOprea,CristinaNita-Rotaru,andFabioRoli.Whydoadversarialattacks
transfer?Explainingtransferabilityofevasionandpoisoningattacks.In
USENIX
SecuritySymposium
,2019.
[84]
DuaDheeruandE˙KarraTaniskidou.UCImachinelearningrepository,2017.
=http://archive.ics.uci.edu/ml,visited2019-12-05.
[85]
LydiaT.Liu,SarahDean,EstherRolf,MaxSimchowitz,andMoritzHardt.
Delayedimpactoffairmachinelearning.In
InternationalJointConferenceon
Arti˙cialIntelligence,IJCAI
,2019.
[86]
KeYangandJuliaStoyanovich.Measuringfairnessinrankedoutputs.In
Pro-
ceedingsofthe29thInternationalConferenceonScienti˙candStatisticalDatabase
Management
,2017.
[87]
LouiseSeamsterandRaphaëlCharron-Chénier.Predatoryinclusionandeduca-
tiondebt:Rethinkingtheracialwealthgap.
SocialCurrents
,2017.
[88]
Keeanga-YamahttaTaylor.
RaceforPro˙t:HowBanksandtheRealEstateIndustry
UnderminedBlackHomeownership
.UNCPressBooks,2019.
[89]
FenwickMcKelvey.
Internetdaemons:Digitalcommunicationspossessed
.Uof
MinnesotaPress,2018.
[90]
OscarHGandy.Engagingrationaldiscrimination:exploringreasonsforplacing
regulatoryconstraintsondecisionsupportsystems.
EthicsandInformation
Technology
,2010.
[91]
SeetaPeñaGangadharanandJ¦drzejNiklas.Decenteringtechnologyindis-
courseondiscrimination.
Information,Communication&Society
,2019.
[92]
JuliaPowlesandHelenNissenbaum.Theseductivediversionof`solv-
ing'biasinarti˙cialintelligence,2018.https://medium.com/s/story/
the-seductive-diversion-of-solving-bias-in-arti˙cial-intelligence-890df5e5ef53,
visited2019-12-05.
[93]
SamCorbett-DaviesandSharadGoel.Themeasureandmismeasureoffairness:
Acriticalreviewoffairmachinelearning.
CoRR
,2018.
[94]
BenGreenandLilyHu.Themythinthemethodology:Towardsarecontextu-
alizationoffairnessinmachinelearning.In
theICML2018DebatesWorkshop
,
2018.
[95]
AmartyaSen.Utilitarianismandwelfarism.
TheJournalofPhilosophy
,1979.
[96]
AmartyaSen.Resources,valuesanddevelopment.
HarvardUniversityPress
,
1984.
[97]
NataliaQuirogaDiaz.Decolonialfeministeconomics:Anecessaryviewfor
strengtheningsocialandpopulareconomy.
ViewPointMagazine
,October2015.
https://doi.org/10.6084/m9.˙gshare.11336048.
[98]
MarcFleurbaey,MaddalenaFerranna,MarkBudolfson,FrancisDennig,Kian
Mintz-Woo,RobertSocolow,DeanSpears,andStéphaneZuber.Thesocialcost
ofcarbon:Valuinginequality,risk,andpopulationforclimatepolicy.
TheMonist
,
2018.
[99]
EliseThomas.FacebookkeepsfailinginMyanmar.
For-
eignPolicy
,21.June2019.https://foreignpolicy.com/2019/06/21/
facebook-keeps-failing-in-myanmar-zuckerberg-arakan-army-rakhine/,
visited2019-12-05.
[100]
MyanmarCivilSocietyGroups(Phandeeyar,Mido,BurmaMonitor,Center
forSocialIntegrity,EqualityMyanmar,MyanmarHumanRightsEducation
Network).OpenlettertoMarkZuckerberg.https://drive.google.com/˙le/d/
1Rs02G96Y9w5dpX0Vf1LjWp6B9mp32VY-/view,visited2019-12-05.
[101]
VeronicaMiracle.LosAngelescouncilmantriestoworkwithmapappsto
alleviatetra˝cinneighborhoods.
ABC7
,April2018.https://abc7.com/tra˝c/
la-councilman-tries-to-work-with-map-apps/3329737/,visited2019-12-05.
[102]
JamesVincent.WhyAIisn'tgoingtosolveFacebook'sfakenewsprob-
lem.
TheVerge
,April2018.https://www.theverge.com/2018/4/5/17202886/
facebook-fake-news-moderation-ai-challenges,visited2019-12-05.
[103]
FrankPasquale.Oddnumbers.algorithmsalonecan'tmeaningfullyhold
otheralgorithmsaccountable.
RealLife
,August2018.http://reallifemag.com/
odd-numbers/,visited2019-12-05.
[104]
MalteZiewitz.Rethinkinggaming:Theethicalworkofoptimizationinweb
searchengines.
Socialstudiesofscience
,2019.
[105]
GabriellaColeman.Frominternetfarmingtoweaponsofthegeek.
Current
Anthropology
,2017.
[106]
FrancisTseng.Sabotage,heresyandtraps.ConferenceTalk,June2019.
[107]
MinnaRuckenstein.Visualizedandinteractedlife:Personalanalyticsand
engagementswithdatadoubles.
Societies
,2014.
[108]
PeterJHuber.
Robuststatistics
.Springer,2011.
[109]
HaroldKuhnandAlbertWTucker.Nonlinearprogramming.In
Tracesand
emergenceofnonlinearprogramming
,pages2014.
[110]
PangWeiKohandPercyLiang.Understandingblack-boxpredictionsvia
in˚uencefunctions.In
InternationalConferenceonMachineLearning(ICML)
,
2017.
[111]
StephenBoydandLievenVandenberghe.
Convexoptimization
.Cambridge
universitypress,2004.
12
ASENSITIVITYTOMISSPECIFICATIONS
Wetheoreticallyestimatetheimpactofmisspeci˙cationsonthe
severityofexternalities.Forthat,weusein˚uencefunctionsfrom
thetoolkitofrobuststatistics[108].
Weassumethattheutilityfunctions
U
,
B
,and
^
B
arestrictlycon-
caveandtwice-di˛erentiable;andwestrengthentheidealproperty
ofthefair-by-designprovider.BesidespickingthePareto-optimal
solutionthatmaximizestheirmodelofsocialutility,wenowassume
thenear-optimalityofthesocial-utilityobjectiveforthissolution:
r
^
B
¹


ºˇ
0
.Thatis,thesystem


isclosetothe
idealsolution
for
thesocial-utilityobjective
^
B
¹

º
[26].
Considera
partiallycorrected
optimizationobjective:
max

2

f
U
¹

º
;
^
B
¹

º
""
^
B
¹

º
+
""
B
¹

ºg
;
wherewemoveapointmass
""
awayfromtheprovider'smodelof
thesocialutilitytoitsd'svievalue.LetustakeitsaPareto-
optimalsolution


""
thathashighestbene˙t.Wede˙nethe
in˚uence
function
of

B
asfollows:
IF
""
,
d
d
""
»
B
¹


""
º
B
¹


º¼
:
Thismodelshowfastthemagnitudeoftheexternalitygrowsas
moreweightisgiventothecorrected
B
intheoptimizationproblem.
LetusrestateaknownpropertyofPareto-optimalsolutionsdue
toKuhnandTucker[109]:
TheoremA.1([
109
]).
Let


beaPareto-optimalsolutiontothe
optimizationproblemoftheform:
max

2

f
U
¹

º
;
B
¹

ºg
Thenthereexists

2»
0
;
1
¼
,suchthatthefollowingholds:

r
U
¹


º
+
¹
1


ºr
B
¹


º
=
0
Letusdenoteby
H
f
¹
x
º
theHessianmatrixof
f
at
x
,andfor
convenienceset
H

;

:
=

H
U
¹

º
+
¹
1


º
H
^
B
¹

º
.Additionally,
denoteby


:
=


""



thedi˛erenceinsystemparameterscoming
fromthecorrectedandtheoriginaloptimizationproblems.Wecan
nowpresentourestimateforthein˚uencefunction.
StatementA.1.
Usinglinearizationtechniques,wecanobtainthe
followingapproximationforthein˚uencefunctionforsome

2»
0
;
1
¼
:
IF
""
ˇr
B
¹


º
|
»
H


;

¼

1
r
B
¹


º
Derivation.
UsingTheoremA.1,wecansaythereexists

2»
0
;
1
¼
suchthat:

r
U
¹


""
º
+
¹
1


º

r
^
B
¹


""
º
""
r
^
B
¹


""
º
+
""
r
B
¹


""
º

=
0
(A.1)
WenowrewriteEq.A.1intermsoftheoriginalsystemparame-
ters


usinga˙rst-orderTaylorapproximation:
0
=

r
U
¹


º
+
¹
1


º

r
^
B
¹


º
""
r
^
B
¹


º
+
""
r
B
¹


º

+
h

H
U
¹


º
+
¹
1


º

H
^
B
¹


º
""
H
^
B
¹


º
+
""
H
B
¹


º
i



WecanrearrangeandfurtherapproximatefollowingKohand
Liang[110],keepinginmindthat
""
issmall:


ˇ
""
»
H


;

¼

1
r»
B
¹


º
^
B
¹


º¼
ˇ
""
»
H


;

¼

1
r
B
¹


º
Wenowapproximatethein˚uencefunctionusingits˙rst-order
Taylorexpansionandtheobtainedexpressionfor


:
IF
""
=
d
d
""
»
B
¹


""
º
B
¹


º¼ˇ
d
d
""

B
¹


º
+
r
B
¹


º



=
r
B
¹


º
d
d
""


=
r
B
¹


º
|
»
H


;

¼

1
r
B
¹


º

StatementA.2.
Giventheassumptionson
U
;
B
;
^
B
and


,ourlin-
earapproximationforthein˚uencefunctionof

B
isasymptotically
lowerboundedasfollows:
IF
""
=

¹kr
B
¹


ºk
2
º
Proof.
As
H


;

isnegative-de˙nitebytheconcavityassump-
tionandthefactthatconvexcombinationspreserveconcavity,so
isitsinverse.Hence,

H


;

¼

1
ispositive-de˙nite.Byalower
boundofasymmetricpositive-de˙nitequadraticformwehave:
d
d
""

B
ˇr
B
¹


º
|
»
H
J
¹


º¼

1
r
B
¹


º
=

¹kr
B
¹


ºk
2
º

Forconcavefunctions,
kr
B
¹


ºk
canserveasameasureof
errorofthesolution


[111],whichcon˙rmsourintuition.
BDETAILSFORTHETRAFFICTHWARTING
CASESTUDY
B.1MILPFormulation
Withasimplereparameterization,itispossibletoformulatethe
optimizationproblemasfollows:
min

¹º2f
0
;
1
g
Õ
¹
x
;
y
º2
E
c
¹
x
;
y
º

¹
x
;
y
º
s.t.
Õ
¹
x
;
y
º2
e
»
t
¹
x
;
y
º
+

¹
x
;
y
º

t
¹
x
;
y
º¼
t

foranypath
e
from
a
to
b
;
(B.1)
Inthisform,theoptimizationproblemistheshortest-pathinter-
dictionproblem[68,69],andcanbesolvedasanMILP[70,71]:
min

¹º2f
0
;
1
g
Õ
¹
x
;
y
º2
E
c
¹
x
;
y
º

¹
x
;
y
º
s.t.
ˇ
¹
t
º
ˇ
¹
s
º
t

ˇ
¹
y
º
ˇ
¹
x
º
t
¹
x
;
y
º
+

¹
x
;
y
º

t
¹
x
;
y
º
8
¹
x
;
y
º2
E
ˇ
¹
v
º2
R
8
v
2
V
;
(B.2)
where
ˇ
¹
v
º
areadditional
vertex-potentialvariables
thatrepresent
thesmallesttimecostforgettingfrom
a
to
v
ingraph
G
0
.
Assumethateachedge
¹
x
;
y
º
isassociatedwithalengthde˙ned
by
s
¹
x
;
y
º
,andaspeedlimit
v
¹
x
;
y
º
.Inthecaseofchangingthe
speedlimitsthrough

v
,

t
¹
x
;
y
º
canbeobtainedfrom
s
,
v
and

v
asfollows:

t
¹
x
;
y
º
=
s
¹
x
;
y
º

v
¹
x
;
y
º
v
¹
x
;
y
º
2

v
¹
x
;
y
º

v
¹
x
;
y
º
(B.3)
13
B.2EvaluationDetailsforFremont,California
ThegraphforFremont,CA,USA,ismuchlargerthanfortheother
townsconsideredinourevaluation,withatotalof9,215nodes
and19,313edges.Thenormaltimefrom
a
to
b
inthetownis8.5
minutes.Figure6showstheoptimalsetofroadstolowerthespeed
limitonfor
t

=
15
:
25
,usinga75%decreaseintimefortheallowed
roadchanges.
CDETAILSFORTHECREDIT-SCORING
CASESTUDY
Wedetailtheheuristicalgorithmweusetosolvetheoptimization
problemofthePOTinAlgorithm1.Tocomputethescores,we
retrainaclassi˙erforeachexample
¹
x
;
y
º2
X
pool
.Inourcaseof
thelogisticregressionasthebank'smodel,retrainingisinexpen-
sive.Formorecomplexmodels,approximationtechniquescanbe
used[83].
Algorithm1
Algorithmforselectingpoisoningloanapplications
inordertoreducethefalse-negativerateonthetargetgroup.
(1)
S
=
Priority˚eue
¹º
(2)
for
¹
x
;
y
º2
X
pool
:
(3)
continueif
f
¹
x
;

0
º
,
`accept'
(4)
simulate


¹
x
;
y
º
=
argmin
Í
X
[f¹
x
;
y
ºg
L
¹
x
0
;
y
0
;

º
(5)
computethescore
J
¹


¹
x
;
y
º
º
(6)
add
¹
x
;
y
º
to
S
alongwiththecomputedscore
(7)
X
pot
:
=
fg
(8)
while
j
X
pot
j
n
:
(9)
remove
¹
x

;
y

º
withthelowestscorefrom
S
(10)
add
¹
x

;
y

º
toR.
14
Figure6:SolutionforFremont,California(inblack)
15
"
4,Surface Networks,http://arxiv.org/pdf/1705.10819v2.pdf,https://github.com/jiangzhongshi/SurfaceNetworks,"SurfaceNetworks
IlyaKostrikov
1
,ZhongshiJiang
1
,DanielePanozzo

1
,DenisZorin
y
1
,andJoanBruna
z
1,2
1
CourantInstituteofMathematicalSciences,NewYorkUniversity
2
CenterforDataScience,NewYorkUniversity
Abstract
Westudydata-drivenrepresentationsforthree-dimensionaltrianglemeshes,whichareoneoftheprevalentobjectsused
torepresent3Dgeometry.Recentworkshavedevelopedmodelsthatexploittheintrinsicgeometryofmanifoldsandgraphs,
namelytheGraphNeuralNetworks(GNNs)anditsspectralvariants,whichlearnfromthelocalmetrictensorviathe
Laplacianoperator.
Despiteofferingexcellentsamplecomplexityandbuilt-ininvariances,intrinsicgeometryaloneisinvarianttoisometric
deformations,makingitunsuitableformanyapplications.Toovercomethislimitation,weproposeseveralupgradesto
GNNstoleverageextrinsicdifferentialgeometrypropertiesofthree-dimensionalsurfaces,increasingitsmodelingpower.In
particular,weproposetoexploittheDiracoperator,whosespectrumdetectsprincipalcurvaturedirectionsŠthisisinstark
contrastwiththeclassicalLaplaceoperator,whichdirectlymeasuresmeancurvature.Wecointheresultingmodels
Surface
Networks(SN)
.
Weprovethatthesemodelsshaperepresentationsthatarestabletodeformationandtodiscretization,andwe
demonstratetheefandversatilityofSNsontwochallengingtasks:temporalpredictionofmeshdeformationsunder
non-lineardynamicsandgenerativemodelsusingavariationalautoencoderframeworkwithencoders/decodersgivenby
SNs.
1.Introduction
3Dgeometryanalysis,manipulationandsynthesisplaysanimportantroleinavarietyofapplicationsfromengineering
tocomputeranimationtomedicalimaging.Despitethevastamountofhigh-quality3Dgeometricdataavailable,data-
drivenapproachestoproblemsinvolvingcomplexgeometryhaveyettobecomemainstream,inpartduetothelackofdata
representationregularitywhichisrequiredfortraditionalconvolutionalneuralnetworkapproaches.Whileincomputervision
problemsinputsaretypicallysampledonregulartwoorthree-dimensionalgrids,surfacegeometryisrepresentedinamore
complexformand,ingeneral,cannotbeconvertedtoanimage-likeformatbyparametrizingtheshapeusingasingleplanar
chart.Mostcommonlyanirregulartrianglemeshisusedtorepresentshapes,capturingitsmaintopologicalandgeometrical
properties.
Similarlytotheregulargridcase(usedforimagesorvideos),weareinterestedindata-drivenrepresentationsthatstrike
therightbalancebetweenexpressivepowerandsamplecomplexity.InthecaseofCNNs,thisisachievedbyexploitingthe
inductivebiasthatmostcomputervisiontasksarelocallystabletodeformations,leadingtolocalized,multiscale,stationary
features.Inthecaseofsurfaces,wefaceafundamentalmodelingchoicebetween
extrinsic
versus
intrinsic
representations.
Extrinsicrepresentationsrelyontheembeddingofsurfaceswithinathree-dimensionalambientspace,whereas
intrinsicrepresentationsonlycapturegeometricpropertiestothesurface,irrespectiveofitsparametrization.Whereas
theformerofferarbitraryrepresentationpower,theyareunabletoeasilyexploitinductivepriorssuchasstabilitytolocal
deformationsandinvariancetoglobaltransformations.

DPwassupportedinpartbytheNSFCAREERawardIIS-1652515,agiftfromAdobe,andagiftfromnTopology.
y
DZwassupportedinpartbytheNSFawardsDMS-1436591andIIS-1320635.
z
JBwaspartiallysupportedbySamsungElectronics(ImprovingDeepLearningusingLatentStructure)andDOAW911NF-17-1-0438.Corresponding
author:
bruna@cims.nyu.edu
1
arXiv:1705.10819v2  [stat.ML]  18 Jun 2018Aparticularlysimpleandpopularextrinsicmethod[
36
,
37
]representsshapesaspointcloudsin
R
3
ofvariablesize,
andleveragesrecentdeeplearningmodelsthatoperateoninputsets[
44
,
43
].Despiteitsadvantagesintermsofease
ofdataacquisition(theynolongerrequireameshtriangulation)andgoodempiricalperformanceonshape
andsegmentationtasks,onemaywonderwhetherthiscomesatalossofprecisionasoneconsidersmore
challengingpredictiontasks.
Inthispaper,wedevelopanalternativepipelinethatappliesneuralnetworksdirectlyontrianglemeshes,buildingon
geometricdeeplearning
.Thesemodelsprovidedata-drivenintrinsicgraphandmanifoldrepresentationswithinductive
biasesanalogoustoCNNsonnaturalimages.ModelsbasedonGraphNeuralNetworks[
40
]andtheirspectralvariants
[
6
,
11
,
26
]havebeensuccessfullyappliedtogeometryprocessingtaskssuchasshapecorrespondence[
32
].Intheirbasic
form,thesemodelslearnadeeprepresentationoverthediscretizedsurfacebycombiningalatentrepresentationatagiven
nodewithalocallinearcombinationofitsneighbors'latentrepresentations,andapoint-wisenonlinearity.Differentmodels
varyintheirchoiceoflinearoperatorandpoint-wisenonlinearity,whichnotablyincludesthegraphLaplacian,leadingto
spectralinterpretationsofthosemodels.
Ourcontributionsarethree-fold.First,weextendthemodeltosupportextrinsicfeatures.More,weexploitthe
factthatsurfacesin
R
3
admitadifferentialoperator,the
Dirac
operator,thatisstabletodiscretization,providesa
directgeneralizationofLaplacian-basedpropagationmodels,andisabletodetectprincipalcurvaturedirections[
9
,
18
].Next,
weprovethatthemodelsresultingfromeitherLaplaceorDiracoperatorsarestabletodeformationsandtodiscretization,
twomajorsourcesofvariabilityinpracticalapplications.Last,weintroduceagenerativemodelforsurfacesbasedonthe
variationalautoencoderframework[
25
,
39
],thatisabletoexploitnon-Euclideangeometricregularity.
BycombiningtheDiracoperatorwithinputcoordinates,weobtainafullydifferentiable,end-to-endfeaturerepresentation
thatweapplytoseveralchallengingtasks.TheresultingSurfaceNetworksŒusingeithertheDiracortheLaplacian,inherit
thestabilityandinvariancepropertiesoftheseoperators,thusprovidingdata-drivenrepresentationswithstabilityto
deformations.Wedemonstratethemodelefyonatemporalpredictiontaskofcomplexdynamics,basedonaphysical
simulationofelasticshells,whichthatwhenevergeometricinformation(intheformofamesh)isavailable,itcan
beleveragedtooutperformpoint-cloudbasedmodels.
Ourmaincontributionsaresummarizedasfollows:

WedemonstratethatSurfaceNetworksprovideaccuratetemporalpredictionofsurfacesundercomplexnon-linear
dynamics,motivatingtheuseofgeometricshapeinformation.

WeprovethatSurfaceNetworksshaperepresentationsthatarestabletodeformationandtodiscretization.

Weintroduceagenerativemodelfor3Dsurfacesbasedonthevariationalautoencoder.
Areferenceimplementationofouralgorithmisavailableat
https://github.com/jiangzhongshi/SurfaceNetworks
.
2.RelatedWork
Learningend-to-endrepresentationsonirregularandnon-Euclideandomainsisanactiveandongoingareaofresearch.
[
40
]introducedgraphneuralnetworksasrecursiveneuralnetworksongraphs,whosestationarydistributionscouldbetrained
bybackpropagation.Subsequentworks[
27
,
43
]haverelaxedthemodelbyuntyingtherecurrentlayerweightsandproposed
severalnonlinearupdatesthroughgatingmechanisms.Graphneuralnetworksareinfactnaturalgeneralizationsofconvo-
lutionalnetworkstonon-Euclideangraphs.[
6
,
17
]proposedtolearnsmoothspectralmultipliersofthegraphLaplacian,
albeitwithhighcomputationalcost,and[
11
,
26
]resolvedthecomputationalbottleneckbylearningpolynomialsofthegraph
Laplacian,thusavoidingthecomputationofeigenvectorsandcompletingtheconnectionwithGNNs.Wereferthereaderto
[
5
]foranexhaustiveliteraturereviewonthetopic.GNNsareapplicationinmanydifferentdomains.[
2
,
7
]develop
graphinteractionnetworksthatlearnpairwiseparticleinteractionsandapplythemtodiscreteparticlephysicaldynamics.
[
12
,
22
]studymolecularusingvariantsoftheGNNarchitecture,and[
14
]furtherdevelopthemodelbycombin-
ingitwithsetrepresentations[
44
],showingstate-of-the-artresultsonmolecularprediction.Theresultingmodels,so-called
Message-PassingNeuralNetworks,alsolearnthediffusionoperator,whichcanbeseenasgeneralizationsoftheDiracmodel
ongeneralgraphs.
Inthecontextofcomputergraphics,[
30
]developedtheCNNmodelonmeshedsurfacesusingintrinsicpatchrep-
resentations,andfurthergeneralizedin[
4
]and[
32
].Thislastworkallowsforxiblerepresentationsviatheso-called
pseudo-coordinatesandobtainsstate-of-the-artresultson3Dshapecorrespondence,althoughitdoesnoteasilyencode
orderdifferentialinformation.TheseintrinsicmodelscontrastwithEuclideanmodelssuchas[
48
,
46
],thathavehigher
samplecomplexity,sincetheyneedtolearntheunderlyinginvarianceofthesurfaceembedding.Point-cloudbasedmodels
areincreasinglypopulartomodel3dobjectsduetotheirsimplicityandversatility.[
36
,
37
]useset-invariantrepresentations
from[
44
,
43
]tosolveshapesegmentationandtasks.Morerecently,[
29
]proposestolearnsurfaceconvolutional
networkfromacanonicalrepresentationofplanarwithexcellentperformanceonshapesegmentationandclassi-
althoughsuchcanonicalrepresentationsmayintroduceexponentialscalechangesthatcanintroduceinstabilities.
Finally,[
13
]proposesapoint-cloudgenerativemodelfor3Dshapes,thatincorporatesinvariancetopointpermutations,but
doesnotencodegeometricalinformationasourshapegenerativemodel.Learningvariationaldeformationsisanimportant
problemforgraphicsapplications,sinceitenablesnegligibleandedper-framecost[
35
],butitiscurrentlylimitedto2D
deformationsusingpointhandles.Inconstrast,ourmethodeasilygeneralizesto3Dandlearnsdynamicbehaviours.
3.SurfaceNetworks
Thissectionpresentsoursurfaceneuralnetworkmodelanditsbasicproperties.Westartbyintroducingtheproblemsetup
andnotationsusingtheLaplacianformalism(Section
3.1
),andthenintroduceourmodelbasedontheDiracoperator(Section
3.2
).
3.1.LaplacianSurfaceNetworks
Ourgoalistoatrainablerepresentationofdiscretesurfaces.Let
M
=
f
V;E;F
g
beatriangularmesh,where
V
=(
v
i
2
R
3
)
i

N
containsthenodecoordinates,
E
=(
e
i;j
)
correspondstoedges,and
F
isthesetoftriangularfaces.We
denoteas

thediscreteLaplace-Beltramioperator(weusethepopularcotangentweightsformulation,see[
5
]fordetails).
Thisoperatorcanbeinterpretedasalocal,linearhigh-passin
M
thatactsonsignals
x
2
R
d

V
j
onthe
verticesasasimplematrixmultiplication
~
x
=
x
.Bycombining

withan
all-pass
andlearninggenericlinear
combinationsfollowedbyapoint-wisenonlinearity,weobtainasimplegeneralizationoflocalizedconvolutionaloperators
in
M
thatupdateafeaturemapfromlayer
k
tolayer
k
+1
usingtrainableparameters
A
k
and
B
k
:
x
k
+1
=
ˆ

A
k

x
k
+
B
k
x
k

;A
k
;B
k
2
R
d
k
+1

d
k
:
(1)
ByobservingthattheLaplacianitselfcanbewrittenintermsofthegraphweightsimilaritybydiagonalrenormalization,
thismodelisainstanceofthegraphneuralnetwork[
40
,
5
,
26
]andageneralizationofthespectrum-freeLaplacian
networksfrom[
11
].Asshowninthesepreviousworks,convolutional-likelayers(
1
)canbecombinedwithgraphcoarsening
orpoolinglayers.
Incontrasttogeneralgraphs,meshescontainalow-dimensionalEuclideanembeddingthatcontainspotentiallyuseful
informationinmanypracticaltasks,despitebeingextrinsicandthusnotinvarianttotheglobalpositionofthesurface.A
simplestrategytostrikeagoodbalancebetweenexpressivityandinvarianceistoincludethenodecanonicalcoordinatesas
inputchannelstothenetwork:
x
1
:=
V
2
R
j
V

3
.ThemeancurvaturecanbecomputedbyapplyingtheLaplaceoperator
tothecoordinatesofthevertices:

x
1
=

2
H
n
;
(2)
where
H
isthemeancurvaturefunctionand
n
(
u
)
isthenormalvectorofthesurfaceatpoint
u
.Asaresult,theLaplacian
neuralmodel(
1
)hasaccesstomeancurvatureandnormalinformation.FeedingEuclideanembeddingcoordinatesintograph
neuralnetworkmodelsisrelatedtotheuseofgeneralizedcoordinatesfrom[
32
].Bycascading
K
layersoftheform(
1
)we
obtainarepresentation


(
M
)
thatcontainsgenericfeaturesateachnodelocation.Whenthenumberoflayers
K
isof
theorderofdiam
(
M
)
,thediameterofthegraphdeterminedby
M
,thenthenetworkisabletopropagateandaggregate
informationacrossthewholesurface.
Equation(
2
)illustratesthataLaplacianlayerisonlyabletoextractisotropichigh-frequencyinformation,correspondingto
themeanvariationsacrossalldirections.Althoughingeneralgraphsthereisnoproceduretorecoveranisotropic
localvariations,inthecaseofsurfacessomeauthors([
4
,
1
,
32
])haveconsideredanisotropicextensions.Wedescribenext
aparticularlysimpleproceduretoincreasetheexpressivepowerofthenetworkusingarelatedoperatorfromquantum
mechanics:theDiracoperator,thathasbeenpreviouslyusedsuccessfullyinthecontextofsurfacedeformation[
9
]andshape
analysis[
18
].
3.2.DiracSurfaceNetworks
TheLaplace-Beltramioperator

isasecond-orderdifferentialoperator,constructedas
=

div
r
bycombiningthe
gradient(adifferentialoperator)withitsadjoint,thedivergenceoperator.InanEuclideanspace,onehasaccessto
thesedifferentialoperatorsseparately,enablingorientedhigh-pass
Forconvenience,weembed
R
3
totheimaginaryquaternionspaceIm
(
H
)
(seeAppendixAintheSuppl.Materialfor
details).TheDiracoperatoristhenasamatrix
D
2
H
j
F

V
j
thatmaps(quaternion)signalsonthenodestosignals
onthefaces.Incoordinates,
D
f;j
=

1
2
jA
f
j
e
j
;f
2
F;j
2
V;
where
e
j
istheopposingedgevectorofnode
j
intheface
f
,and
A
f
isthearea(seeAppendixA)usingcounter-clockwise
orientationsonallfaces.
ToapplytheDiracoperatorinquaternionstosignalsinverticesandfacesinrealnumbers,wewritethe
featurevectorsasquaternionsbysplittingthemintochunksof4realnumbersrepresentingtherealandimaginarypartsofa
quaternion;seeAppendixA.Thus,wealwaysworkwithfeaturevectorswithdimensionalitiesthataremultiplesof
4
.The
Diracoperatorprovidesdifferentialinformationandissensitivetolocalorientations.Moreover,onecanverify[
9
]
that
Re
D

D
=
;
where
D

istheadjointoperatorof
D
inthequaternionspace(seeAppendixA).Theadjointmatrixcanbecomputedas
D

=
M

1
V
D
H
M
F
where
D
H
isaconjugatetransposeof
D
and
M
V
,
M
F
arediagonalmassmatriceswithonethirdof
areasoftrianglesincidenttoavertexandfaceareasrespectively.
TheDiracoperatorcanbeusedtoanewneuralsurfacerepresentationthatalternateslayerswithsignals
overnodeswithlayersoverfaces.Givena
d
-dimensionalfeaturerepresentationoverthenodes
x
k
2
R
d

V
j
,andthe
facesofthemesh,
y
k
2
R
d

F
j
,wea
d
0
-dimensionalmappingtoafacerepresentationas
y
k
+1
=
ˆ

C
k
Dx
k
+
E
k
y
k

;C
k
;E
k
2
R
d
k
+1

d
k
;
(3)
where
C
k
;E
k
aretrainableparameters.Similarly,wetheadjointlayerthatmapsbacktoa
~
d
-dimensionalsignalover
nodesas
x
k
+1
=
ˆ

A
k
D

y
k
+1
+
B
k
x
k

;A
k
;B
k
2
R
d
k
+1

d
k
;
(4)
where
A
k
;B
k
aretrainableparameters.Asurfaceneuralnetworklayeristhusdeterminedbyparameters
f
A;B;C;E
g
using
equations(
3
)and(
4
)to
x
k
+1
2
R
d
k
+1

V
j
.Wedenoteby

D
(
M
)
themeshrepresentationresultingfromapplying
K
suchlayers(thatweassumeedforthepurposeofexposition).
TheDirac-basedsurfacenetworkisrelatedtoedgefeaturetransformsproposedongeneralgraphsin[
14
],althoughthese
edgemeasurementscannotbeassociatedwithderivativesduetolackofproperorientation.Ingeneralgraphs,thereisno
notionofsquarerootof

thatrecoversorientedderivatives.
4.StabilityofSurfaceNetworks
HerewedescribehowSurfaceNetworksaregeometricallystable,becausesurfacedeformationsbecomeadditivenoise
underthemodel.Givenacontinuoussurface
S
ˆ
R
3
oradiscretemesh
M
,andasmoothdeformation
˝
:
R
3
!
R
3
,
weareparticularlyinterestedintwoformsofstability:

Givenadiscretemesh
M
andacertainnon-rigiddeformation
˝
actingon
M
,wewanttocertifythat
k

M
)


˝
(
M
))
k
issmallif
kr
˝
(
r
˝
)


I
k
issmall,i.ewhenthedeformationisnearlyrigid;seeTheorem
4.1
.

Giventwodiscretizations
M
1
and
M
2
ofthesameunderlyingsurface
S
,wewouldliketocontrol
k

M
1
)


M
2
)
k
intermsoftheresolutionofthemeshes;seeTheorem
4.2
.
Thesestabilitypropertiesareimportantinapplications,sincemosttasksweareinterestedinarestabletodeformationand
todiscretization.WeshallseethatthepropertyisasimpleconsequenceofthefactthatthemeshLaplacianandDirac
operatorsarethemselvesstabletodeformations.Thesecondpropertywillrequireustospecifyunderwhichconditionsthe
discretemeshLaplacian

M
convergestotheLaplace-Beltramioperator

S
on
S
.Unlessitisclearfromthecontext,inthe
following

willdenotethediscreteLaplacian.
Theorem4.1
Let
M
bea
N
-nodemeshand
x;x
0
2
R
j
V

d
beinputsignalsonthenodes.Assumethenonlinearity
ˆ
(

)
isnon-expansive(
j
ˆ
(
z
)

ˆ
(
z
0
)
jj
z

z
0
j
).Then
(a)
k


(
M
;
x
)



(
M
;
x
0
)
k


k
x

x
0
k
;
where


dependsonlyonthetrainedweightsandthemesh.
(b)
k

D
(
M
;
x
)


D
(
M
;
x
0
)
k

D
k
x

x
0
k
;
where

D
dependsonlyonthetrainedweightsandthemesh.
(c)
Let
jr
˝
j
1
:=sup
u
kr
˝
(
u
)(
r
˝
(
u
))


1
k
,where
r
˝
(
u
)
istheJacobianmatrixof
u
7!
˝
(
u
)
.Then
k


(
M
;
x
)



(
˝
(
M
);
x
)
k


jr
˝
j
1
k
x
k
;
where


isindependentof
˝
and
x
.
(d)
Denoteby
g
jr
˝
j
1
:=sup
u
kr
˝
(
u
)

1
k
.Then
k

D
(
M
;
x
)


D
(
˝
(
M
);
x
)
k

D
g
jr
˝
j
1
k
x
k
;
where

D
is
independentof
˝
and
x
.
Properties(a)and(b)arenottosurfacerepresentations,andareasimpleconsequenceofthenon-expansiveproperty
ofourchosennonlinearities.Theconstant

iscontrolledbytheproductof
`
2
normsofthenetworkweightsateachlayerand
thenormofthediscreteLaplacianoperator.Properties(c)and(d)arebasedonthefactthattheLaplacianandDiracoperators
arethemselvesstabletodeformations,apropertythatdependsontwokeyaspects:theLaplacian/Diracislocalizedin
space,andnext,thatitisahigh-passandthereforeonlydependsonrelativechangesinposition.
OnecaveatofTheorem
4.1
isthattheconstantsappearingintheboundsdependuponabandwidthparametergivenbythe
reciprocaloftriangleareas,whichincreasesasthesizeofthemeshincreases.Thiscorrespondstothefactthatthespectral
radiusof

M
divergesasthemeshsize
N
increases.
Inordertoovercomethisproblematicasymptoticbehavior,itisnecessarytoexploitthesmoothnessofthesignalsincom-
ingtothesurfacenetwork.ThiscanbemeasuredwithSobolevnormsusingthespectrumoftheLaplacianoperator.
Givenamesh
M
of
N
nodesapproximatinganunderlyingsurface
S
,anditsassociatedcotangentLaplacian

M
,consider
thespectraldecompositionof

M
(asymmetric,positiveoperator):

M
=
X
k

N

k
e
k
e
T
k
;e
k
2
R
N
;
0


1


2


N
:
Undernormaluniformconvergence
1
[
45
],thespectrumof

M
convergestothespectrumoftheLaplace-Beltramioperator

S
of
S
.If
S
isbounded,itisknownfromtheWeyllaw[
47
]thatthereexists
>
0
suchthat
k


(
S
)
.


1
k
,sothe
eigenvalues

k
donotgrowtoofast.Thesmoothnessofasignal
x
2
R
j
V

d
in
M
iscapturedbyhowfastits
spectraldecomposition
^
x
(
k
)=
e
T
k
x
2
R
d
decays[
42
].We
k
x
k
2
H
:=
P
k

(
k
)
2
k
^
x
(
k
)
k
2
isSobolevnorm,and

(
x;S
)
>
1
asthelargestratesuchthatitsspectraldecompositioncoefsatisfy
k
^
x
(
k
)
k
.
k


;
(
k
!1
)
:
(5)
If
x
2
R
j
V

d
istheinputtotheLaplaceSurfaceNetworkof
R
layers,wedenoteby
(

0
;
1
;:::;
R

1
)
thesmoothness
ratesofthefeaturemaps
x
(
r
)
ateachlayer
r

R
.
Theorem4.2
Considerasurface
S
andaapproximation
M
N
of
N
points,and


aLaplaceSurfaceNetwork
withparameters
f
(
A
r
;B
r
)
g
r

R
.Denoteby
d
(
S;
M
N
)
theuniformnormaldistance,andlet
x
1
;x
2
bepiece-wisepolyhedral
approximationsof

x
(
t
)
,
t
2
S
in
M
N
,with
k

x
k
H
(
S
)
<
1
.Assume
k

x
(
r
)
k
H
(
S
)
<
1
for
r

R
.
(a)
If
x
1
;x
2
aretwofunctionssuchthatthe
R
featuremaps
x
(
r
)
l
haverates
(

0
;
1
;:::;
R

1
)
,then
k


(
x
1
;
M
N
)



(
x
2
;
M
N
)
k
2

C
(

)
k
x
1

x
2
k
h
(

)
;
(6)
with
h
(

)=
Q
R
r
=1

r

1

r

1
=
2
,andwhere
C
(

)
doesnotdependupon
N
.
(b)
If
˝
isasmoothdeformationthen
k


(
x
;
M
N
)



(
x
;
˝
(
M
N
))
k
C
jr
˝
j
1
h
(

)
;
where
C
doesnotdepend
upon
N
.
(c)
Let
M
and
M
0
be
N
-pointdiscretizationsof
S
,If
max(
d
(
M
;S
)
;d
(
M
0
;S
))


,then
k


(
M
;
x
)



(
M
0
;x
0
)
k
C
h
(

)
;
where
C
isindependentof
N
.
ThisresultensuresthatifweuseasgeneratoroftheSNanoperatorthatisconsistentasthemeshresolutionincreases,
theresultingsurfacerepresentationisalsoconsistent.AlthoughourpresentresultonlyconcernstheLaplacian,theDirac
operatoralsohasacontinuouscounterpart[
9
]thatgeneralizesthegradientoperatorinquaternionspace.Also,
ourcurrentboundsdependexplicitlyuponthesmoothnessoffeaturemapsacrossdifferentlayers,whichmaybecontrolledin
termsoftheoriginalsignalifoneconsidersnonlinearitiesthatdemodulatethesignal,suchas
ˆ
(
x
)=
j
x
j
or
ˆ
(
x
)=
ReLU
(
x
)
.
Theseextensionsareleftforfuturework.Finally,asetupthatweuseinexperimentsistouseasinputsignalthe
canonicalcoordinatesofthemesh
M
.Inthatcase,animmediateapplicationoftheprevioustheoremyields
Corollary4.3
Denote

M
):=
M
(
V
)
,where
V
arethenodecoordinatesof
M
.Then,if
A
1
=0
,
k

M
)


˝
(
M
))
k

max(
jr
˝
j
1
;
kr
2
˝
k
)
h
(

)
:
(7)
1
whichcontrolshowthenormalsofthemeshalignwiththesurfacenormals;see[
45
].
Figure1.Height-FieldRepresentationofsurfaces.A3Dmesh
Mˆ
R
3
(right)isexpressedintermsofaﬁsamplingﬂ2Dirregularmesh
~
Mˆ
R
2
(left)andadepthscalar
f
:
~
M!
R
over
~
M
(center).
5.GenerativeSurfaceModels
State-of-the-artgenerativemodelsforimages,suchasgenerativeadversarialnetworks[
38
],pixelautoregressivenetworks
[
34
],orvariationalautoencoders[
25
],exploitthelocalityandstationarityofnaturalimagesintheirprobabilisticmodels,in
thesensethatthemodel
p

(
x
)
ˇ
p

(
x
˝
)
byconstruction,where
x
˝
isasmalldeformationofagiveninput
x
.This
propertyisobtainedviaencodersanddecoderswithadeepconvolutionalstructure.Weintendtoexploitsimilargeometric
stabilitypriorswithSNs,owingtotheirstabilitypropertiesdescribedinSection
4
.Ameshgenerativemodelcontains
twodistinctsourcesofrandomness:ontheonehand,therandomnessassociatedwiththeunderlyingcontinuoussurface,
whichcorrespondstoshapevariability;ontheotherhand,therandomnessofthediscretizationofthesurface.Whereas
theformercontainstheessentialsemanticinformation,thelatterisnotinformative,andtosomeextentindependentofthe
shapeidentity.Wefocusinitiallyonmeshesthatcanberepresentedasadepthmapoveran(irregular)2Dmesh,referredas

meshesintheliterature.Thatis,amesh
M
=(
V;E;F
)
isexpressedas
(
~
M
;f
(
~
M
))
,where
~
M
=(
~
V;
~
E;
~
F
)
is
nowa2Dmeshand
f
:
~
V
!
R
isa
depth
-mapencodingtheoriginalnodelocations
V
,asshowninFigure
1
.
Inthiswork,weconsiderthevariationalautoencoderframework[
25
,
39
].Itconsidersamixturemodeloftheform
p
(
M
)=
R
p

(
Mj
h
)
p
0
(
h
)
dh
,where
h
2
R
j
S
j
isavectoroflatentvariables.Wetrainthismodelbyoptimizingthe
variationallowerboundofthedatalog-likelihood:
min
; 
1
L
X
l

L

E
h
˘
q
 
(
h
jM
l
)
log
p

(
M
l
j
h
)+
D
KL
(
q
 
(
h
jM
l
)
jj
p
0
(
h
))
:
(8)
Wethusneedtospecifyaconditionalgenerativemodel
p

(
Mj
h
)
,apriordistribution
p
0
(
h
)
andavariationalapproximation
totheposterior
q
 
(
h
jM
)
,where

and
 
denoterespectivelygenerativeandvariationaltrainableparameters.Basedonthe
representation,wechooseforsimplicityaseparablemodeloftheform
p

(
Mj
h
)=
p

(
f
j
h;
~
M
)

p
(
~
M
)
;
where
~
M˘
p
(
~
M
)
isahomogeneousPoissonpointprocess,and
f
˘
p

(
f
j
h;
~
M
)
isanormaldistributionwithmeanand
isotropiccovarianceparametersgivenbyaSN:
p

(
f
j
h;
~
M
)=
N
(

(
h;
~
M
)
;˙
2
(
h;
~
M
)
1
)
;
with
[

(
h;
~
M
)
;˙
2
(
h;
~
M
)]=
D
(
~
M
;
h
)
:
Thegenerationstepthusproceedsasfollows.Wesamplea2Dmesh
~
M
independentofthelatentvariable
h
,andthensampleadepthover
~
M
conditionedon
h
fromtheoutputofadecoder
network

D
(
~
M
;
h
)
.Finally,thevariationalfamily
q
 
isalsoaNormaldistributionwhoseparametersareobtainedfroman
encoderSurfaceNeuralNetworkwhoselastlayerisaglobalpoolingthatremovesthespatiallocalization:
q
 
(
h
jM
)=
N
(


˙
2
1
)
;
with
[


˙
]=


D
(
M
)
:
6.Experiments
Forexperimentalevaluation,wecomparemodelsbuiltusingResNet-v2blocks[
16
],whereconvolutionsarereplacedwith
theappropriateoperators(seeFig.
2
):
(i)
apointcloudbasedmodelfrom[
43
]thataggregatesglobalinformationbyaveraging
featuresintheintermediatelayersanddistributingthemtoallnodes;
(ii)
aLaplacianSurfacenetworkwithinputcanonical
coordinates;
(iii)
aDiracSurfaceNetworkmodel.Wereportexperimentsongenerativemodelsusinganunstructuredvariant
ofMNISTdigits(Section
6.1
),andontemporalpredictionundernon-rigiddeformationmodels(Section
6.2
).
6.1.MeshMNIST
Forthistask,weconstructaMeshMNISTdatabasewithonly

meshes(Sec.
5
).First,wesamplepointson
a2Dplane
([0
;
27]

[0
;
27])
withPoissondisksamplingwith
r
=1
:
0
,whichroughlygenerates
500
points,andapplya
Delaunaytriangulationtothesepoints.WethenoverlaythetriangulationwiththeoriginalMNISTimagesandassigntoeach
Figure2.AsingleResNet-v2blockusedforLaplace,AveragePooling(top)andDiracmodels(bottom).Thegreenboxescorrespondto
thelinearoperatorsreplacingconvolutionsinregulardomains.WeconsiderExponentialLinearUnits(ELU)activations(orange),Batch
Normalization(blue)and`
1

1
'convolutions(red)containingthetrainableparameters;seeEqs(
1
,
3
and
4
).Weslightlyabuselanguage
anddenoteby
x
k
+1
theoutputofthis2-layerblock.
Model
ReceptiveNumberofparameters
SmoothL1-loss(meanpersequence(std))
MLP
1519672
64.56(0.62)
PointCloud
-1018872
23.64(0.21)
Laplace
161018872
17.34(0.52)
Dirac
81018872
16.84(0.16)
Table1.Evaluationofdifferentmodelsonthetemporaltask
pointa
z
coordinatebilinearlyinterpolatingthegrey-scalevalue.Thus,theprocedureallowsustoasamplingprocess
over3Dmeshes.
WeusedVAEmodelswithdecodersandencodersbuiltusing10ResNet-v2blockswith128features.Theencoder
convertsameshintoalatentvectorbyaveragingoutputofthelastResNet-v2blockandapplyinglineartransformationsto
obtainmeanandvariance,whilethedecodertakesalatentvectoranda2Dmeshasinput(correspondingtoa3D
mesh)andpredictsoffsetsforthecorrespondinglocations.Wekeepvarianceofthedecoderasatrainableparameterthatdoes
notdependoninputdata.Wetrainedthemodelfor
75
epochsusingAdamoptimizer[
24
]withlearningrate
10

3
,weight
decay
10

5
andbatchsize
32
.Figures
3
,
4
illustratesamplesfromthemodel.Thegeometricencoderisabletoleveragethe
localtranslationinvarianceofthedatadespitetheirregularsampling,whereasthegeometricdecoderautomaticallyadaptsto
thesampledgrid,asopposedtoset-basedgenerativemodels.
Figure3.Samplesgeneratedforthesamelatentvariableanddifferenttriangulations.Thelearnedrepresentationisindependentofdis-
cretization/triangulation(Poissondisksamplingwithp=1.5).
Figure4.Meshesfromthedatasetve).Andmeshesgeneratedbyourmodel(lastve).
6.2.emporalPredictions
Onetaskweconsideristemporalpredictionsofnon-lineardynamics.Givenasequenceofframes
X
=
X
1
;X
2
;:::;X
n
,
thetaskistopredictthefollowingframes
Y
=
Y
1
=
X
n
+1
;Y
2
;:::;Y
m
=
X
n
+
m
.Asin[
31
],weuseasimplenon-
recurrentmodelthattakesaconcatenationofinputframes
X
andpredictsaconcatenationofframes
Y
.Weconditionon
n
=2
framesandpredictthenext
m
=40
frames.Inordertogeneratedata,weextracted10kpatchesfromthe
MPI-Faustdataset[
3
],byselectingarandompointandgrowingatopologicalsphereofradius
15
edges(i.e.the15-ringof
thepoint).Foreachpatch,wegenerateasequenceof
50
framesbyrandomlyrotatingitandlettingitfalltotheground.
Weconsiderthemeshathinelasticshell,andwesimulateitusingtheAs-Rigid-As-Possibletechnique[
41
],withadditional
gravitationalforces[
20
].Libigl[
21
]hasbeenusedforthemeshprocessingtasks.Sequenceswithpatchesfromthe80
subjectswereusedintraining,whilethe20lastsubjectswereusedfortesting.Thedatasetandthecodeareavailableon
request.Werestrictourexperimentstotemporalpredictiontasksthataredeterministicwhenconditionedonseveralinitial
frames.Thus,wecantrainmodelsbyminimizingsmooth-L1loss[
15
]betweentargetframesandoutputofourmodels.
GroundTruthMLPPointCloudLaplaceDirac
Figure5.Qualitativecomparisonofdifferentmodels.Weplot30thpredictedframescorrespondinglyfortwosequencesinthetestset.
Boxesindicatedistinctivefeatures.Forlargercrops,seeFigure
6
.
Weusedmodelswith15ResNet-v2blockswith
128
outputfeatureseach.InordertocoverlargercontextforDiracand
Laplacebasedmodels,wealternatetheseblockswithAveragePoolingblocks.Wepredictoffsetstothelastconditioned
frameandusethecorrespondingLaplaceandDiracoperators.Thus,themodelstake6-dimensionalinputsandproduce
120-dimensionaloutputs.WetrainedallmodelsusingtheAdamoptimizer[
24
]withlearningrate
10

3
,weightdecay
10

5
,
andbatchsize32.After60kstepswedecreasedthelearningratebyafactorof2every10ksteps.Themodelsweretrained
for110kstepsinoverall.
GroundTruthLaplaceDirac
Figure6.Dirac-basedmodelvisuallyoutperformsLaplace-basedmodelsintheregionsofhighmeancurvature.
Table
1
reportsquantitativepredictionperformanceofdifferentmodels,andFigure
5
displayssamplesfromtheprediction
Figure7.Fromlefttoright:PointCloud(set2set),groundtruthandDiracbasedmodel.Colorcorrespondstomeansquarederrorbetween
groundtruthandprediction:green-smallererror,red-largererror.
Figure8.Fromlefttoright:Laplace,groundtruthandDiracbasedmodel.Colorcorrespondstomeansquarederrorbetweengroundtruth
andprediction:green-smallererror,red-largererror.
modelsatframes.Theset-to-setmodel[
44
,
43
],correspondingtoapoint-cloudrepresentationusedalsoin[
36
],
alreadyperformsreasonablywellonthetask,evenifthevisualdifferenceisnoticeable.Nevertheless,thegapbetween
thismodelandLaplace-/Dirac-basedmodelsissbothvisuallyandquantitatively.Dirac-basedmodeloutperforms
Laplace-basedmodeldespitethesmallerreceptived.Videoscomparingtheperformanceofdifferentmodelsareavailable
intheadditionalmaterial.
Figure
6
illustratestheeffectofreplacingLaplacebyDiracintheformulationoftheSN.Laplacian-basedmodels,since
theypropagateinformationusinganisotropicoperator,havemoredifultiesatresolvingcornersandpointystructuresthan
theDiracoperator,thatissensitivetoprincipalcurvaturedirections.However,thecapacityofLaplacemodelstoexploit
theextrinsicinformationonlyviatheinputcoordinatesisremarkableandmorecomputationallyefthantheDirac
counterpart.Figures
7
and
8
overlaythepredictionerrorandcompareLaplaceagainstDiracandPointCloudagainstDirac
respectively.TheythatSNsoutperformthepoint-cloudbasedmodel,whichoftenproduceexcessive
andlargedeformations,andnextthatDiracoperatorshelpresolveareaswithhighdirectionalcurvature.Werefer
tothesupplementarymaterialforadditionalqualitativeresults.
7.Conclusions
WehaveintroducedSurfaceNetworks,adeepneuralnetworkthatisdesignedtonaturallyexploitthenon-Euclidean
geometryofsurfaces.Wehaveshownhowadifferentialoperator(theDiracoperator)candetectandadaptto
geometricfeaturesbeyondthelocalmeancurvature,thelimitofwhatLaplacian-basedmethodscanexploit.Thisdistinction
isimportantinpractice,sinceareaswithhighdirectionalcurvatureareperceptuallyimportant,asshownintheexperiments.
Thatsaid,theDiracoperatorcomesatincreasedcomputationalcostduetothequaternioncalculus,anditwouldbeinteresting
toinstead
learn
theoperator,akintorecentMessage-PassingNNs[
14
]andexplorewhetherDiracisrecovered.
Wheneverthedatacontainsgood-qualitymeshes,ourexperimentsdemonstratethatusingintrinsicgeometryoffersvastly
superiorperformancetopoint-cloudbasedmodels.Whiletherearenotmanysuchdatasetscurrentlyavailable,weexpect
themtobecomecommoninthenextyears,asscanningandreconstructiontechnologyadvancesand3Dsensorsareintegrated
inconsumerdevices.SNsprovideefinference,withpredictableruntime,whichmakesthemappealingacrossmany
areasofcomputergraphics,whereaed,per-framecostisrequiredtoensureastableframerate,especiallyinVRapplica-
tions.OurfutureplansincludeapplyingSurfaceNetworkspreciselytohavingautomated,data-drivenmeshprocessing,and
generalizingthegenerativemodeltoarbitrarymeshes,whichwillrequireanappropriatemulti-resolutionpipeline.
References
[1]
M.Andreux,E.Rodol
˚
a,M.Aubry,andD.Cremers.AnisotropicLaplace-Beltramioperatorsforshapeanalysis.In
Proc.NORDIA
,
2014.
3
[2]
P.Battaglia,R.Pascanu,M.Lai,D.J.Rezende,etal.Interactionnetworksforlearningaboutobjects,relationsandphysics.In
AdvancesinNeuralInformationProcessingSystems
,pages4502Œ4510,2016.
2
[3]
F.Bogo,J.Romero,M.Loper,andM.J.Black.Faust:Datasetandevaluationfor3dmeshregistration.In
ProceedingsoftheIEEE
ConferenceonComputerVisionandPatternRecognition
,pages3794Œ3801,2014.
7
,
18
,
20
[4]
D.Boscaini,J.Masci,E.Rodol
˚
a,andM.Bronstein.Learningshapecorrespondencewithanisotropicconvolutionalneuralnetworks.
In
AdvancesinNeuralInformationProcessingSystems
,pages3189Œ3197,2016.
2
,
3
[5]
M.M.Bronstein,J.Bruna,Y.LeCun,A.Szlam,andP.Vandergheynst.Geometricdeeplearning:goingbeyondeuclideandata.
arXiv
preprintarXiv:1611.08097
,2016.
2
,
3
,
14
[6]
J.Bruna,W.Zaremba,A.Szlam,andY.LeCun.Spectralnetworksandlocallyconnectednetworksongraphs.
Proc.ICLR
,2013.
2
[7]
M.B.Chang,T.Ullman,A.Torralba,andJ.B.Tenenbaum.Acompositionalobject-basedapproachtolearningphysicaldynamics.
ICLR
,2016.
2
[8]
D.ChenandJ.R.Gilbert.Obtainingboundsonthetwonormofamatrixfromthesplittinglemma.
ElectronicTransactionson
NumericalAnalysis
,21:28Œ46,2005.
14
[9]
K.Crane,U.Pinkall,andP.Schr
¨
oder.Spintransformationsofdiscretesurfaces.In
ACMTransactionsonGraphics(TOG)
.ACM,
2011.
2
,
3
,
4
,
5
[10]
K.C.Das.Extremalgraphcharacterizationfromtheupperboundofthelaplacianspectralradiusofweightedgraphs.
LinearAlgebra
andItsApplications
,427(1):55Œ69,2007.
12
[11]
M.Defferrard,X.Bresson,andP.Vandergheynst.ConvolutionalneuralnetworksongraphswithfastlocalizedspectralIn
AdvancesinNeuralInformationProcessingSystems
,pages3837Œ3845,2016.
2
,
3
[12]
D.Duvenaud,D.Maclaurin,J.Aguilera-Iparraguirre,R.G
´
omez-Bombarelli,T.Hirzel,A.Aspuru-Guzik,andR.P.Adams.Convo-
lutionalnetworksongraphsforlearningmolecularIn
NeuralInformationProcessingSystems
,2015.
2
[13]
H.Fan,H.Su,andL.Guibas.Apointsetgenerationnetworkfor3dobjectreconstructionfromasingleimage.
arXivpreprint
arXiv:1612.00603
,2016.
3
[14]
J.Gilmer,S.S.Schoenholz,P.F.Riley,O.Vinyals,andG.E.Dahl.Neuralmessagepassingforquantumchemistry.
arXivpreprint
arXiv:1704.01212
,2017.
2
,
4
,
10
[15]
R.Girshick.Fastr-cnn.In
ProceedingsoftheIEEEInternationalConferenceonComputerVision
,pages1440Œ1448,2015.
7
[16]
K.He,X.Zhang,S.Ren,andJ.Sun.Identitymappingsindeepresidualnetworks.In
EuropeanConferenceonComputerVision
,
pages630Œ645.Springer,2016.
6
[17]
M.Henaff,J.Bruna,andY.LeCun.Deepconvolutionalnetworksongraph-structureddata.
arXiv:1506.05163
,2015.
2
[18]
K.C.Hsueh-TiDerekLiu,AlecJacobson.Adiracoperatorforextrinsicshapeanalysis.
ComputerGraphicsForum
,2017.
2
,
3
[19]
Y.Hu,Q.Zhou,X.Gao,A.Jacobson,D.Zorin,andD.Panozzo.Tetrahedralmeshinginthewild.
SubmittedtoACMTransactionon
Graphics
,2018.
18
[20]
A.Jacobson.
AlgorithmsandInterfacesforReal-TimeDeformationof2Dand3DShapes
.PhDthesis,ETH,Z
¨
urich,2013.
7
[21]
A.Jacobson,D.Panozzo,etal.libigl:AsimpleC++geometryprocessinglibrary,2016.http://libigl.github.io/libigl/.
7
[22]
S.Kearnes,K.McCloskey,M.Berndl,V.Pande,andP.Riley.Moleculargraphconvolutions:movingbeyond
Journal
ofcomputer-aidedmoleculardesign
,2016.
2
[23]
V.G.Kim,Y.Lipman,andT.Funkhouser.Blendedintrinsicmaps.In
ACMTransactionsonGraphics(TOG)
,volume30,page79.
ACM,2011.
19
[24]
D.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization.In
InternationalConferenceonLearningRepresentation
,2015.
7
,
8
[25]
D.P.KingmaandM.Welling.Auto-encodingvariationalbayes.
arXivpreprintarXiv:1312.6114
,2013.
2
,
6
[26]
T.N.KipfandM.Welling.Semi-supervisedwithgraphconvolutionalnetworks.
arXivpreprintarXiv:1609.02907
,
2016.
2
,
3
[27]
Y.Li,D.Tarlow,M.Brockschmidt,andR.Zemel.Gatedgraphsequenceneuralnetworks.
arXivpreprintarXiv:1511.05493
,2015.
2
[28]
O.Litany,T.Remez,E.Rodol
˚
a,A.M.Bronstein,andM.M.Bronstein.Deepfunctionalmaps:Structuredpredictionfordenseshape
correspondence.
2017IEEEInternationalConferenceonComputerVision(ICCV)
,pages5660Œ5668,2017.
21
[29]
H.Maron,M.Galun,N.Aigerman,M.Trope,N.Dym,E.Yumer,V.Kim,andY.Lipman.Convolutionalneuralnetworksonsurfaces
viaseamlesstoriccovers.In
SIGGRAPH
,2017.
3
[30]
J.Masci,D.Boscaini,M.Bronstein,andP.Vandergheynst.Geodesicconvolutionalneuralnetworksonriemannianmanifolds.In
ProceedingsoftheIEEEinternationalconferenceoncomputervisionworkshops
,pages37Œ45,2015.
2
[31]
M.Mathieu,C.Couprie,andY.LeCun.Deepmulti-scalevideopredictionbeyondmeansquareerror.
arXivpreprint
arXiv:1511.05440
,2015.
7
[32]
F.Monti,D.Boscaini,J.Masci,E.Rodol
˚
a,J.Svoboda,andM.M.Bronstein.Geometricdeeplearningongraphsandmanifolds
usingmixturemodelcnns.
arXivpreprintarXiv:1611.08402
,2016.
2
,
3
[33]
A.Nowak,S.Villar,A.S.Bandeira,andJ.Bruna.Anoteonlearningalgorithmsforquadraticassignmentwithgraphneuralnetworks.
arXivpreprintarXiv:1706.07450
,2017.
18
[34]
A.v.d.Oord,N.Kalchbrenner,andK.Kavukcuoglu.Pixelrecurrentneuralnetworks.
arXivpreprintarXiv:1601.06759
,2016.
6
[35]
R.PoranneandY.Lipman.Simpleapproximationsofplanardeformationoperators.Technicalreport,ETHZ,2015.
3
[36]
C.R.Qi,H.Su,K.Mo,andL.J.Guibas.Pointnet:Deeplearningonpointsetsfor3dandsegmentation.
arXivpreprint
arXiv:1612.00593
,2016.
2
,
3
,
9
[37]
C.R.Qi,L.Yi,H.Su,andL.J.Guibas.Pointnet++:Deephierarchicalfeaturelearningonpointsetsinametricspace.
arXivpreprint
arXiv:1706.02413
,2017.
2
,
3
[38]
A.Radford,L.Metz,andS.Chintala.Unsupervisedrepresentationlearningwithdeepconvolutionalgenerativeadversarialnetworks.
arXivpreprintarXiv:1511.06434
,2015.
6
[39]
D.J.RezendeandS.Mohamed.Variationalinferencewithnormalizingws.
arXivpreprintarXiv:1505.05770
,2015.
2
,
6
[40]
F.Scarselli,M.Gori,A.C.Tsoi,M.Hagenbuchner,andG.Monfardini.Thegraphneuralnetworkmodel.
IEEETransactionson
NeuralNetworks
,20(1):61Œ80,2009.
2
,
3
[41]
O.SorkineandM.Alexa.As-rigid-as-possiblesurfacemodeling.In
SymposiumonGeometryprocessing
,volume4,2007.
7
[42]
D.A.Spielman.Spectralgraphtheoryanditsapplications.In
FoundationsofComputerScience,2007.FOCS'07.48thAnnualIEEE
Symposiumon
,pages29Œ38.IEEE,2007.
5
[43]
S.Sukhbaatar,R.Fergus,etal.Learningmultiagentcommunicationwithbackpropagation.In
AdvancesinNeuralInformation
ProcessingSystems
,pages2244Œ2252,2016.
2
,
3
,
6
,
9
[44]
O.Vinyals,S.Bengio,andM.Kudlur.Ordermatters:Sequencetosequenceforsets.
arXivpreprintarXiv:1511.06391
,2015.
2
,
3
,
9
[45]
M.Wardetzky.Convergenceofthecotangentformula:Anoverview.In
DiscreteDifferentialGeometry
,pages275Œ286.2008.
5
,
12
,
17
[46]
L.Wei,Q.Huang,D.Ceylan,E.Vouga,andH.Li.Densehumanbodycorrespondencesusingconvolutionalnetworks.In
Proceedings
oftheIEEEConferenceonComputerVisionandPatternRecognition
,pages1544Œ1553,2016.
2
[47]
H.Weyl.
¨
Uberdieasymptotischeverteilungdereigenwerte.
NachrichtenvonderGesellschaftderWissenschaftenzuG
¨
ottingen,
Mathematisch-PhysikalischeKlasse
,1911:110Œ117,1911.
5
[48]
Z.Wu,S.Song,A.Khosla,F.Yu,L.Zhang,X.Tang,andJ.Xiao.3dshapenets:Adeeprepresentationforvolumetricshapes.In
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
,pages1912Œ1920,2015.
2
A.TheDiracOperator
Thequaternions
H
isanextensionofcomplexnumbers.Aquaternion
q
2
H
canberepresentedinaform
q
=
a
+
bi
+
cj
+
dk
where
a;b;c;d
arerealnumbersand
i;j;k
arequaternionunitsthatsatisfytherelationship
i
2
=
j
2
=
k
2
=
ijk
=

1
.
AsmentionedinSection3.1,theDiracoperatorusedinthemodelcanbeconvenientlyrepresentedasaquaternionmatrix:
D
f;j
=

1
2
jA
f
j
e
j
;f
2
F;j
2
V;
where
e
j
istheopposingedgevectorofnode
j
intheface
f
,and
A
f
isthearea,asillustratedinFig.
A
,usingcounter-
clockwiseorientationsonallfaces.
v
j
e
j
f
TheDeepLearninglibraryPyTorchthatweusedtoimplementthemodelsdoesnotsupportquaternions.Nevertheless,
quaternion-valuedmatrixmultiplicationcanbereplacedwithreal-valuedmatrixmultiplicationwhereeachentry
q
=
a
+
bi
+
cj
+
dk
isrepresentedasa
4

4
block
2
6
6
4
a

b

c

d
ba

dc
cda

b
d

cba
3
7
7
5
andtheconjugate
q

=
a

bi

cj

dk
isatransposeofthisreal-valuedmatrix:
2
6
6
4
abcd

bad

c

c

dab

dc

ba
3
7
7
5
:
B.Theorem4.1
B.1.Proofof(a)
Weshowtheresultforthemapping
x
7!
ˆ
(
Ax
+
B

x
)
,correspondingtoonelayerof


.Bythe
Laplacian

of
M
is
=
diag
(

A
)

1
(
U

W
)
;
where

A
j
isonethirdofthetotalareaoftrianglesincidenttonode
j
,and
W
=(
w
i;j
)
containsthecotangentweights[
45
],
and
U
=
diag
(
W
1
)
containsthenodeaggregatedweightsinitsdiagonal.
From[
10
]weverifythat
k
U

W
k
p
2max
i
8
<
:
s
U
2
i
+
U
i
X
i
˘
j
U
j
w
i;j
9
=
;
(9)

2
p
2sup
i;j
w
i;j
sup
j
d
j

2
p
2cot(

min
)
d
max
;
where
d
j
denotesthedegree(numberofneighbors)ofnode
j
,

min
isthesmallestangleinthetriangulationof
M
and
S
max
thelargestnumberofincidenttriangles.Itresultsthat
k

k
C
cot(

min
)
S
max
inf
j

A
j
:=
L
M
;
whichdependsuniquelyonthemesh
M
andisfornon-degeneratemeshes.Moreover,since
ˆ
(

)
isnon-expansive,we
have
k
ˆ
(
Ax
+
B

x
)

ˆ
(
Ax
0
+
B

x
0
)
kk
A
(
x

x
0
)+
B

x

x
0
)
k
(10)

(
k
A
k
+
k
B
k
L
M
)
k
x

x
0
k
:
Bycascading(
10
)acrossthe
K
layersofthenetwork,weobtain
k

M
;
x
)


M
;
x
0
)
k
0
@
Y
k

K
(
k
A
k
k
+
k
B
k
k
L
M
)
1
A
k
x

x
0
k
;
whichproves(a).

B.2.Proofof(b)
Theproofisanalogous,byobservingthat
k
D
k
=
p
k

k
andtherefore
k
D
k
p
L
M
:

B.3.Proofof(c)
Toestablish(c)weobservethatgiventhreepoints
p;q;r
2
R
3
forminganyofthetrianglesof
M
,
k
p

q
k
2
(1
jr
˝
j
1
)
2
k
˝
(
p
)

˝
(
q
)
k
2
k
p

q
k
2
(1+
jr
˝
j
1
)
2
(11)
A
(
p;q;r
)
2
(1
jr
˝
j
1
C

2
min

o
(
jr
˝
j
1
2
)
A
(
˝
(
p
)
;˝
(
q
)
;˝
(
r
))
2
A
(
p;q;r
)
2
(1+
jr
˝
j
1
C

2
min
+
o
(
jr
˝
j
1
2
))
:
(12)
Indeed,(
11
)isadirectconsequenceofthelowerandupperLipschitzconstantsof
˝
(
u
)
,whichareboundedrespectivelyby
1
jr
˝
j
1
and
1+
jr
˝
j
1
.Asfor(
12
),weusetheHeronformula
A
(
p;q;r
)
2
=
s
(
s
k
p

q
k
)(
s
k
p

r
k
)(
s
k
r

q
k
)
;
with
s
=
1
2
(
k
p

q
k
+
k
p

r
k
+
k
r

q
k
)
beingthehalf-perimeter.Bydenoting
s
˝
thecorrespondinghalf-perimeter
determinedbythedeformedpoints
˝
(
p
)
;˝
(
q
)
;˝
(
r
)
,wehavethat
s
˝
k
˝
(
p
)

˝
(
q
)
k
s
(1+
jr
˝
j
1
)
k
p

q
k
(1
jr
˝
j
1
)=
s
k
p

q
k
+
jr
˝
j
1
(
s
+
k
p

q
k
)
and
s
˝
k
˝
(
p
)

˝
(
q
)
k
s
(1
jr
˝
j
1
)
k
p

q
k
(1+
jr
˝
j
1
)=
s
k
p

q
kjr
˝
j
1
(
s
+
k
p

q
k
)
;
andsimilarlyforthe
k
r

q
k
and
k
r

p
k
terms.Itresultsin
A
(
˝
(
p
)
;˝
(
q
)
;˝
(
r
))
2
A
(
p;q;r
)
2

1
jr
˝
j
1

1+
s
+
k
p

q
k
s
k
p

q
k
+
s
+
k
p

r
k
s
k
p

r
k
+
s
+
k
r

q
k
s
k
r

q
k


o
(
jr
˝
j
1
2
)

A
(
p;q;r
)
2
h
1

C
jr
˝
j
1


2
min

o
(
jr
˝
j
1
2
)
i
;
andsimilarly
A
(
˝
(
p
)
;˝
(
q
)
;˝
(
r
))
2
A
(
p;q;r
)
2
h
1+
C
jr
˝
j
1


2
min

o
(
jr
˝
j
1
2
)
i
:
BynotingthatthecotangentLaplacianweightscanbewritten(seeFig.
9
)as
w
i;j
=

`
2
ij
+
`
2
jk
+
`
2
ik
A
(
i;j;k
)
+

`
2
ij
+
`
2
jh
+
`
2
ih
A
(
i;j;h
)
;
wehavefromthepreviousBilipschitzboundsthat
˝
(
w
i;j
)

w
i;j

1

C
jr
˝
j
1


2
min


1
+2
jr
˝
j
1

1

C
jr
˝
j
1


2
min


1
 
`
2
ij
+
`
2
jk
+
`
2
ik
A
(
i;j;k
)
+
`
2
ij
+
`
2
jh
+
`
2
ih
A
(
i;j;h
)
!
;
˝
(
w
i;j
)

w
i;j

1+
C
jr
˝
j
1


2
min


1

2
jr
˝
j
1

1+
C
jr
˝
j
1


2
min


1
 
`
2
ij
+
`
2
jk
+
`
2
ik
A
(
i;j;k
)
+
`
2
ij
+
`
2
jh
+
`
2
ih
A
(
i;j;h
)
!
;
whichprovesthat,uptosecondorderterms,thecotangentweightsareLipschitzcontinuoustodeformations.
Finally,sincethemeshLaplacianoperatorisconstructedasdiag
(

A
)

1
(
U

W
)
,with

A
i;i
=
1
3
P
j;k
;(
i;j;k
)
2
F
A
(
i;j;k
)
,
and
U
=
diag
(
W
1
)
,letusshowhowtobound
k


˝

k
from

A
i;i
(1


M
jr
˝
j
1

o
(
jr
˝
j
1
2
))

˝
(

A
i;i
)


A
i;i
(1+

M
jr
˝
j
1
+
o
(
jr
˝
j
1
2
))
(13)
and
w
i;j
(1


M
jr
˝
j
1

o
(
jr
˝
j
1
2
))

˝
(
w
i;j
)

w
i;j
(1+

M
jr
˝
j
1
+
o
(
jr
˝
j
1
2
))
:
(14)
j
i
k
h

ij

ij
a
i
a
ijk
`
ij
Figure9.TriangularmeshandCotangentLaplacianreproducedfrom[
5
])
Usingthefactthat

A
,
˝
(

A
)
arediagonal,andusingthespectralboundfor
k

m
sparsematricesfrom[
8
],Lemma5.12,
k
Y
k
2

max
i
X
j
;
Y
i;j
6
=0
j
Y
i;j
j
 
l
X
r
=1
j
Y
r;j
j
!
;
thebounds(
13
)and(
14
)yieldrespectively
˝
(

A
)=

A
(
1
+

˝
)
;
with
k

˝
k
=
o
(
jr
˝
j
1
)
;
and
˝
(
U

W
)=
U

W
+

˝
;
with
k

˝
k
=
o
(
jr
˝
j
1
)
:
Itresultsthat,uptosecondorderterms,
k


˝

k
=


˝
(

A
)

1
(
˝
(
U
)

˝
(
W
))


A

1
(
U

W
)


=





A
[
1
+

˝
]


1
[
U

W
+

˝
]


A

1
(
U

W
)



=




1


˝
+
o
(
jr
˝
j
1
2
)


A

1
(
U

W
+

˝
)


A

1
(
U

W
)



=



˝
+

A

1

˝


+
o
(
jr
˝
j
1
2
)
=
o
(
j
˝
j
1
)
;
whichshowsthattheLaplacianisstabletodeformationsinoperatornorm.Finally,bydenoting
~
x
˝
alayerofthedeformed
Laplaciannetwork
~
x
˝
=
ˆ
(
Ax
+
B˝

x
)
;
itfollowsthat
k
~
x

~
x
˝
kk
B


˝

x
k
(15)

C
k
B
kjr
˝
j
1
k
x
k
:
(16)
Also,
k
~
x

~
y
˝
kk
A
(
x

y
)+
B

x

˝

y
)
k

(
k
A
k
+
k
B
kk

k
)
k
x

y
k
+
k


˝

kk
x
k

(
k
A
k
+
k
B
kk

k
)
|
{z
}

1
k
x

y
k
+
C
jr
˝
j
1
|
{z
}

2
k
x
k
;
(17)
andtherefore,byplugging(
17
)with
y
=~
x
˝
,
K
layersoftheLaplaciannetworksatisfy
k

x
;


x
;
˝

k
0
@
Y
j

K

1

1
(
j
)
1
A
k
~
x

~
x
˝
k
+
0
@
X
j<K

1
Y
j
0

j

1
(
j
0
)

2
(
j
)
1
A
jr
˝
j
1
k
x
k

2
4
C
0
@
Y
j

K

1

1
(
j
)
1
A
k
B
k
+
0
@
X
j<K

1
Y
j
0

j

1
(
j
0
)

2
(
j
)
1
A
3
5
jr
˝
j
1
k
x
k
:

:
B.4.Proofof(d)
Theproofisalsoanalogoustotheproofof(c),withthedifferencethatnowtheDiracoperatorisnolongerinvariantto
orthogonaltransformations,onlytotranslations.Giventwopoints
p
,
q
,weverifythat
k
p

q

˝
(
p
)

˝
(
q
)
k
f
j
˝
j
1
k
p

q
k
;
which,followingthepreviousargument,leadsto
k
D

˝
(
D
)
k
=
o
(
f
j
˝
j
1
)
:
(18)
C.Theorem4.2
C.1.Proofofpart(a)
Theproofisbasedonthefollowinglemma:
LemmaC.1
Let
x
N
;y
N
2H
(
M
N
)
suchthat
8
N
,
k
x
N
k
H

c
,
k
y
N
k
H

c
.Let
^
x
N
=
E
N
(
x
N
)
,where
E
N
isthe
eigendecompositionoftheLaplacianoperator

N
on
M
N
,,withassociatedeigenvalues

1
:::
N
inincreasingorder.Let
>
0
and

beasin(
5
)for
x
N
and
y
N
.If
>
1
and
k
x
N

y
N
k

forall
N
,
k

N
(
x
N

y
N
)
k
2

C
2

1


1
=
2
;
(19)
where
C
isaconstantindependentof

and
N
.
Onelayerofthenetworkwilltransformthedifference
x
1

x
2
into
ˆ
(
Ax
1
+
B

x
1
)

ˆ
(
Ax
2
+
B

x
2
)
.Weverifythat
k
ˆ
(
Ax
1
+
B

x
1
)

ˆ
(
Ax
2
+
B

x
2
)
kk
A
kk
x
1

x
2
k
+
k
B
kk

x
1

x
2
)
k
:
WenowapplyLemma
C.1
toobtain
k
ˆ
(
Ax
1
+
B

x
1
)

ˆ
(
Ax
2
+
B

x
2
)
kk
A
kk
x
1

x
2
k
+
C
k
B
kk
x
1

x
2
k


1


1
=
2
k
x
1

x
2
k


1


1
=
2

k
A
kk
x
1

x
2
k
(2


1)

1
+
C
k
B
k


C
(
k
A
k
+
k
B
k
)
k
x
1

x
2
k


1


1
=
2
;
wherewe
C
toaccountforthefactthat
k
x
1

x
2
k
(2


1)

1
isbounded.Wehavejustshowedthat
k
x
(
r
+1)
1

x
(
r
+1)
2
k
f
r
k
x
(
r
)
1

x
(
r
)
2
k
g
r
(20)
with
f
r
=
C
(
k
A
r
k
+
k
B
r
k
)
and
g
r
=

r

1

r

1
=
2
.Bycascading(
20
)foreachofthe
R
layerswethusobtain
k


(
x
1
)



(
x
2
)
k
""
R
Y
r
=1
f
Q
r
0
>r
g
r
0
r
#
k
x
1

x
2
k
Q
R
r
=1
g
r
;
(21)
whichproves(
6
)

.
Proofof(
19
):
Let
f
e
1
;:::;e
N
g
betheeigendecompositionof

N
.Forsimplicity,wedropthesubindex
N
inthesignals
fromnowon.Let
^
x
(
k
)=
h
x;e
k
i
and
~
x
(
k
)=

k
^
x
(
k
)
;andanalogouslyfor
y
.FromtheParsevalidentitywehavethat
k
x
k
2
=
k
^
x
k
2
.Weexpress
k

x

y
)
k
as
k

x

y
)
k
2
=
X
k

N

2
k
(^
x
(
k
)

^
y
(
k
))
2
:
(22)
Thebasicprincipleoftheproofistocutthespectralsum(
22
)intwoparts,chosentoexploitthedecayof
~
x
(
k
)
.Let
F
(
x
)(
k
)=
P
k
0

k
~
x
(
k
)
2
k
x
k
2
H
=
P
k
0

k
~
x
(
k
)
2
P
k
0
~
x
(
k
)
2
=
P
k
0

k

2
k
^
x
(
k
)
2
P
k
0

2
k
^
x
(
k
)
2

1
;
andanalogouslyfor
y
.Foranycutoff
k


N
wehave
k

x

y
)
k
2
=
X
k

k


2
k
(^
x
(
k
)

^
y
(
k
))
2
+
X
k>k


2
k
(^
x
(
k
)

^
y
(
k
))
2


2
k


2
+2(
F
(
x
)(
k

)
k
x
k
2
H
+
F
(
y
)(
k

)
k
y
k
2
H
)


2
k


2
+2
F
(
k

)(
k
x
k
2
H
+
k
y
k
2
H
)


2
k


2
+4
F
(
k

)
D
2
;
(23)
wherewedenoteforsimplicity
F
(
k

)=max(
F
(
x
)(
k

)
;F
(
y
)(
k

))
.Byassumption,wehave

2
k
.
k
2

and
F
(
k
)
.
X
k
0

k
k
2(



)
'
k
1+2(



)
:
Bydenoting
~

=




1
=
2
,itfollowsthat
k

x

y
)
k
2
.

2
k
2


+4
D
2
k

2
~


(24)
Optimizingfor
k

yields

2
2
k
2


1

2
~

4
D
2
k

2
~


1
=0
;
thus
k

=

4
D
2

2

1
2

+2
~

:
(25)
Byplugging(
25
)backinto(
24
)anddroppingallconstantsindependentof
N
and

,thisleadsto
k

x

y
)
k
2
.

2

1

+
~

=

2

1


1
=
2
;
whichprovespart(a)

.
C.2.Proofofpart(b)
Wewillusethefollowinglemma:
LemmaC.2
Let
M
=(
V;E;F
)
isanon-degeneratemesh,and

1
(
M
)=sup
(
i;j
)
2
E

A
i

A
j
;
2
(
M
)=sup
(
i;j;k
)
2
F
`
2
ij
+
`
2
jk
+
`
2
ik
A
(
i;j;k
)
;
3
(
M
)=

min
:
(26)
Then,givenasmoothdeformation
˝
and
x
in
M
,wehave
k


˝

x
k
C
jr
˝
j
1
k

x
k
;
(27)
where
C
dependsonlyupon

1
,

2
and

3
.
Inthatcase,weneedtocontrolthedifference
ˆ
(
Ax
+
B

x
)

ˆ
(
Ax
+
B˝

x
)
.Weverifythat
k
ˆ
(
Ax
+
B

x
)

ˆ
(
Ax
+
B˝

x
)
kk
B
kk


˝

x
k
:
ByLemma
C.2
itfollowsthat
k


˝

x
k
C
jr
˝
j
1
k

x
k
andtherefore,bydenoting
x
(1)
1
=
ˆ
(
Ax
+
B

x
)
and
x
(1)
2
=
ˆ
(
Ax
+
B˝

x
)
,wehave
k
x
(1)
1

x
(1)
2
k
C
jr
˝
j
1
k

x
k
=
C
jr
˝
j
1
k
x
k
H
:
(28)
ByapplyingagainLemma
C.1
,wealsohavethat
k

x
(1)
1

˝

x
(1)
2
k
=
k

x
(1)
1

+
˝



x
(1)
2
k
=
k

x
(1)
1

x
(1)
2
)+(
˝



x
(1)
2
k

C
k
x
(1)
1

x
(1)
2
k

1

1

1

1
=
2
+
jr
˝
j
1
k
x
(1)
2
k
H
.
C
jr
˝
j
1

1

1

1

1
=
2
;
which,bycombiningitwith(
28
)andrepeatingthroughthe
R
layersyields
k


(
x;
M
)



(
x;˝
(
M
)
k
C
jr
˝
j
1
Q
R
r
=1

r

1

r

1
=
2
;
(29)
whichconcludestheproof

.
Proofof(
27
):
TheprooffollowscloselytheproofofTheorem
4.1
,part(c).From(
13
)and(
14
)wehavethat
˝
(

A
)=

A
(
I
+
G
˝
)
;
with
j
G
˝
j
1

C
(

2
;
3
)
jr
˝
j
1
;
and
˝
(
U

W
)=(
I
+
H
˝
)(
U

W
)
;
with
j
H
˝
j
1

C
(

2
;
3
)
jr
˝
j
1
:
Itfollowsthat,uptosecondorder
o
(
jr
˝
j
1
2
)
terms,
˝


=
˝
(

A
)

1
(
˝
(
U
)

˝
(
W
))


A

1
(
U

W
)
=


A
[
1
+
G
˝
]


1
[(
I
+
H
˝
)(
U

W
)]


A

1
(
U

W
)
'

A

1
H
˝
(
U

W
)+
G
˝

:
(30)
Bywriting

A

1
H
˝
=
f
H
˝

A

1
,andsince

A
isdiagonal,weverifythat
(
f
H
˝
)
i;j
=(
H
˝
)
i;j
A
i;i
A
j;j
;
with
A
i;i
A
j;j


1
,andhencethat

A

1
H
˝
(
U

W
)=
f
H
˝

;
with
j
f
H
˝
j
1

C
(

1
;
2
;
3
)
jr
˝
j
1
:
(31)
Weconcludebycombining(
30
)and(
31
)into
k


˝

x
k
=
k
(
G
˝
+
f
H
˝

x
k

C
0
(

1
;
2
;
3
)
jr
˝
j
1
k

x
k
;
whichproves(
27
)

C.3.Proofofpart(c)
ThisresultisaconsequenceoftheconsistencyofthecotangentLaplaciantotheLaplace-Beltramioperatoron
S
[
45
]:
TheoremC.3([
45
],Thm3.4)
Let
M
beacompactpolyhedralsurfacewhichisanormalgraphoverasmoothsurface
S
withdistortiontensor
T
,andlet

T
=(det
T
)
1
=
2
T

1
.Ifthenormaluniformdistance
d
(
T
;
1
)=
k

T
1
k
1

d
(
T
;
1
)


,then
k

M


S
k
:
(32)
If

M
convergesuniformlyto

S
,inparticularweverifythat
k
x
k
H
(
M
)
!k
x
k
H
(
S
)
:
Thus,giventwomeshes
M
,
M
0
approximatingasmoothsurface
S
intermsofuniformnormaldistance,andthecorre-
spondingirregularsampling
x
and
x
0
ofanunderlyingfunction

x
:
S
!
R
,wehave
k
ˆ
(
Ax
+
B

M
x
)

ˆ
(
Ax
0
+
B

M
0
x
0
)
kk
A
kk
x

x
0
k
+
k
B
kk

M
x


M
0
x
0
k
:
(33)
Since
M
and
M
0
bothconvergeuniformlynormallyto
S
and

x
isLipschitzon
S
,itresultsthat
k
x


x
k
;
and
k
x
0


x
k
;
thus
k
x

x
0
k
2

.Also,thankstotheuniformnormalconvergence,wealsohaveconvergenceintheSobolevsense:
k
x


x
k
H
.
;
k
x
0


x
k
H
.
;
whichimpliesinparticularthat
k
x

x
0
k
H
.
:
(34)
From(
33
)and(
34
)itfollowsthat
k
ˆ
(
Ax
+
B

M
x
)

ˆ
(
Ax
0
+
B

M
0
x
0
)
k
2
k
A
k

+
(35)
+
k
B
kk

M
x


S

x
+
S

x


M
0
x
0
k

2

(
k
A
k
L
+
k
B
k
)
:
ByapplyingagainLemma
C.1
to
~
x
=
ˆ
(
Ax
+
B

M
x
)
,
~
x
0
=
ˆ
(
Ax
0
+
B

M
0
x
0
)
,wehave
k
~
x

~
x
0
k
H

C
k
~
x

~
x
0
k

1

1

1

1
=
2
.


1

1

1

1
=
2
:
Weconcludebyretracingthesameargumentasbefore,reapplyingLemma
C.1
ateachlayertoobtain
k

M
(
x
)


M
0
(
x
0
)
k
C
Q
R
r
=1

r

1

r

1
=
2
:

:
D.ProofofCorollary4.3
Weverifythat
k
ˆ
(
B

x
)

ˆ
(
B˝

˝
(
x
))
kk
B
kk

x

˝

˝
(
x
)
k
k
B
kk

x

˝
(
x
))+

˝

˝
(
x
))
k
k
B
k
(
k

x

˝
(
x
))
k
+
k


˝

˝
(
x
))
k
:
Thesecondtermis
o
(
jr
˝
j
1
)
fromLemma
C.2
.Thetermis
k
x

˝
(
x
)
k
H
k

I

˝
)
kk
x
kkr
2
˝
kk
x
k
;
where
kr
2
˝
k
istheuniformHessiannormof
˝
.Theresultfollowsfromapplyingthecascadingargumentfromlastsection.

E.PreliminaryStudy:MetricLearningforDenseCorrespondence
Asaninterestingextension,weapplythearchitecturewebuiltinExperiments6.2directlytoadenseshapecorrespondence
problem.
Similarlyasthegraphcorrespondencemodelfrom[
33
],weconsideraSiameseSurfaceNetwork,consistingoftwo
identicalmodelswiththesamearchitectureandsharingparameters.Forapairofinputsurfaces
M
1
;
M
2
of
N
1
,
N
2
points
respectively,thenetworkproducesembeddings
E
1
2
R
N
1

d
and
E
2
2
R
N
2

d
.Theseembeddingsatrainable
similaritybetweenpointsgivenby
s
i;j
=
e
h
E
1
;i
;E
2
;j
i
P
j
0
e
h
E
1
;i
;E
2
;j
0
i
;
(36)
whichcanbetrainedbyminimizingthecross-entropyrelativetogroundtruthpairs.Adiagramofthearchitectureis
providedinFigure
10
.
Ingeneral,denseshapecorrespondenceisataskthatrequiresablendofintrinsicandextrinsicinformation,motivating
theuseofdata-drivenmodelsthatcanobtainsuchtradeoffsautomatically.FollowingthesetupinExperiment6.2,weuse
modelswith15ResNet-v2blockswith128outputfeatureseach,andalternateLaplaceandDiracbasedmodelswithAverage
Poolingblockstocoveralargercontext:Theinputtoournetworkconsistsofvertexpositionsonly.
Wetestedourarchitectureonareconstructed(i.e.changingthemeshconnectivity)versionoftherealscanofFAUST
dataset[
3
].TheFAUSTdatasetcontains100realscansandtheircorrespondinggroundtruthregistrations.Thegroundtruth
isbasedonadeformabletemplatemeshwiththesameorderingandconnectivity,whichistothescans.Inorderto
eliminatethebiasofusingthesametemplateconnectivity,aswellastheneedofasingleconnectedcomponent,thescans
arereconstructedagainwith[
19
].Tofosterreplicability,wereleasetheprocesseddatasetintheadditionalmaterial.Inour
experiment,weuse80modelsfortrainingand20modelsfortesting.
Figure10.Siamesenetworkpipeline:thetwonetworkstakevertexcoordinatesoftheinputmodelsandgenerateahighdimensionalfeature
vector,whicharethenusedtoamapfrom
M
1
to
M
2
.Here,themapisvisualizedbytakingacolormapon
M
2
,andtransferring
iton
M
1
Figure11.Additionalresultsfromoursetup.Plotinthemiddleshowsrateofcorrectcorrespondencewithrespecttogeodesicerror[
23
].
WeobservethatLaplaceisperformingsimilarlytoDiracinthisscenario.WebelievethatthereasonisthattheFAUSTdatasetcontains
onlyisometricdeformations,andthusthetwooperatorshaveaccesstothesameinformation.Wealsoprovidevisualcomparison,withthe
transferofahigherfrequencycolormapfromthereferenceshapetoanotherpose.
Sincethegroundtruthcorrespondenceisimpliedonlythroughthecommontemplatemesh,wecomputethecorrespon-
dencebetweenourmesheswithanearestneighborsearchbetweenthepointcloudandthereconstructedmesh.Consequently,
Figure12.Heatmapillustratingthepoint-wisegeodesicdifferencebetweenpredictedcorrespondencepointandthegroundtruth.Theunit
isproportionaltothegeodesicdiameter,andsaturatedat10%.
Figure13.AfailurecaseofapplyingtheLaplacenetworktoanewposeintheFAUSTbenchmarkdataset.Thenetworkconfusesbetween
leftandrightarms.Weshowthecorrespondencevisualizationforfrontandbackofthispair.
duetothedrasticchangeinvertexreplacementaftertheremeshing,only60-70percentoflabeledmatchesareused.Although
makingitmorechallenging,webelievethissetupisclosetoarealcasescenario,whereacquisitionnoiseandocclusionsare
unavoidable.
OurpreliminaryresultsarereportedinFigure
11
.Forsimplicity,wegeneratepredictedcorrespondencesbysimplytaking
themodeofthesoftmaxdistributionforeachreferencenode
i
:
^
j
(
i
)=argmax
j
s
i;j
,thusavoidingastepthatis
standardinothershapecorrespondencepipelines.TheMLPmodelusesnocontextwhatsoeverandprovidesabaselinethat
capturesthepriorinformationfrominputcoordinatesalone.Usingcontextualinformation(evenextrinsicallyasinpoint-
cloudmodel)bringsveimprovments,buttheseresultsmaybesubstantiallyimprovedbyencodingfurtherprior
knowledge.AnexampleofthecurrentfailureofourmodelisdepitctedinFigure
13
,illustratingthatourcurrentarchitecture
doesnothavesuflargespatialcontexttodisambiguatebetweenlocallysimilar(butgloballyinconsistent)parts.
WepostulatethattheFAUSTdataset[
3
]isnotanidealforourcontributionfortworeasons:(1)itissmall(100
models),and(2)itcontainsonlynear-isometricdeformations,whichdonotrequirethegeneralityofferedbyournetwork.As
demonstratedin[
28
],thecorrespondenceperformancescanbedramaticallyimprovedbyconstructingbasisthatareinvariant
tothedeformations.Welookforwardtotheemergenceofnewgeometricdatasets,andwearecurrentlydevelopingacapture
setupthatwillallowustoacquireamorechallengingdatasetforthistask.
F.FurtherNumericalExperiments
GroundTruthMLPAvgPoolLaplaceDirac
Figure14.Qualitativecomparisonofdifferentmodels.Weplot1th,10th,20th,30thand40thpredictedframecorrespondingly.
GroundTruthMLPAvgPoolLaplaceDirac
Figure15.Qualitativecomparisonofdifferentmodels.Weplot1th,10th,20th,30thand40thpredictedframecorrespondingly.
GroundTruthMLPAvgPoolLaplaceDirac
Figure16.Qualitativecomparisonofdifferentmodels.Weplot1th,10th,20th,30thand40thpredictedframecorrespondingly.
GroundTruthMLPAvgPoolLaplaceDirac
Figure17.Qualitativecomparisonofdifferentmodels.Weplot1th,10th,20th,30thand40thpredictedframecorrespondingly.
GroundTruthLaplaceDirac
Figure18.Dirac-basedmodelvisuallyoutperformsLaplace-basedmodelsintheregionsofhighmeancurvature.
Figure19.Fromlefttoright:Laplace,groundtruthandDiracbasedmodel.Colorcorrespondstomeansquarederrorbetweengroundtruth
andprediction:green-smallererror,red-largererror.
Figure20.Fromlefttoright:set-to-set,groundtruthandDiracbasedmodel.Colorcorrespondstomeansquarederrorbetweenground
truthandprediction:green-smallererror,red-largererror.
"
5,Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples,https://arxiv.org/pdf/1711.09576v4.pdf,https://github.com/tech-srl/lstar_extraction,"ExtractingAutomatafromRecurrentNeuralNetworks
UsingQueriesandCounterexamples
GailWeiss
1
YoavGoldberg
2
EranYahav
1
Abstract
Wepresentanovelalgorithmthatusesexactlearn-
ingandabstractiontoextractadeterministic
niteautomatondescribingthestatedynamicsofa
giventrainedRNN.WedothisusingAngluin'sL

algorithmasalearnerandthetrainedRNNasan
oracle.Ourtechniqueefextractsaccurate
automatafromtrainedRNNs,evenwhenthestate
vectorsarelargeandrequiredifferentiation.
1.Introduction
RecurrentNeuralNetworks(RNNs)areaclassofneural
networksusedtoprocesssequencesofarbitrarylengths.An
RNNreceivesaninputsequencetimestepbytimestep,re-
turninganew
statevector
aftereachstep.For
tasks,thisisfollowedbypassingthestatevectorstoamulti-
classcomponent,whichistrainedalongside
theRNNandreturnsaforthesequence.We
callacombinationofanRNNandabinary
componentan
RNN-acceptor
.
RNNsarecentraltodeeplearning,andnaturallanguage
processinginparticular.However,whiletheyhavebeen
showntoreasonablyapproximateavarietyoflanguages,
whattheyeventuallylearnisunclear.Indeed,severallines
ofworkattempttoextractclearrulesfortheirdecisions
(Jacobsson,2005;Omlin&Giles,1996;Cechinetal.,2003).
Motivation
GivenanRNN-acceptor
R
trainedovera
alphabet

,ourgoalistoextractadeterministic
automaton(DFA)
A
thatsequencesinamanner
observablyequivalentto
R
.(Ideally,wewouldliketoobtain
aDFAthataccepts
exactly
thesamelanguageasthenetwork,
butthisisamuchmorediftask.)
Weapproachthistaskusing
exactlearning
.
1
Technion,Haifa,Israel
2
BarIlanUniversity,RamatGan,Israel.
Correspondenceto:GailWeiss
<
sgailw@cs.technion.ac.il
>
.
Proceedingsofthe
35
th
InternationalConferenceonMachine
Learning
,Stockholm,Sweden,PMLR80,2018.Copyright2018
bytheauthor(s).
ExactLearning
Intheofexactlearning,
concepts
(sets
ofinstances)canbelearnedpreciselyfroma
minimallyade-
quateteacher
Šanoraclecapableofansweringtwoquery
types(Goldman&Kearns,1995):

membershipqueries
:labelagiveninstance

equivalencequeries
:statewhetheragivenhypothesis
(setofinstances)isequaltotheconceptheldbythe
teacher.Ifnot,returnaninstanceonwhichthehypoth-
esisandtheconceptdisagree(a
counterexample
).
TheL

algorithm(Angluin,1987)isanexactlearningalgo-
rithmforlearningaDFAfromaminimallyadequateteacher
forsomeregularlanguage
L
.Inthiscontext,theconceptis
L
,andtheinstancesarewordsoveritsalphabet.
WedesignateatrainedRNN
1
asteacherfortheL

algorithm,
inordertoextractaDFArepresentingitsbehavior.The
RNNisusedtriviallytoanswermembershipqueries:input
sequencesarefedtothenetworkforThemain
challengeinthissettingisansweringequivalencequeries.
ProblemEquivalenceQuery
GivenanRNN-
acceptor
R
trainedoveraalphabet

,andaDFA
A
over

,determinewhether
R
and
A
areequivalent,and
returnacounterexample
w
2


ifnot.
Asthisproblemislikelytobeintractable,weuseanap-
proximation.Oneapproachwouldberandomsampling;
however,should
R
and
A
besimilar,thismaytaketime.
OurApproach
Weuse
abstraction
oftheRNN
R
to
answerequivalencequeries.Theabstractionandthe
L

DFA
A
actastwohypothesesfortheRNNgroundtruth,
andmustatleastbeequivalenttoeachotherinordertobe
equivalentto
R
.Wheneverthetwodisagreeonasample,we
itstruein
R
,obtainingthroughthiseither
acounterexampleto
A
oratotheabstraction.
Ourapproachisguaranteednevertoreturnanincorrect
counterexamplenorinvokeanunnecessaryi.e.,
ityieldsnofalsenegatives
.Asfarasweknow,thisisthe
attempttoapplyexactlearningtoagivenRNN.
1
Inwhatfollows,whenunderstoodfromcontext,weusethe
termRNNtomeanRNN-acceptor.
arXiv:1711.09576v4  [cs.LG]  27 Feb 2020ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
MainContributions

Wepresentanovelandgeneralframeworkforextract-
ingautomatafromtrainedRNNs.WeusetheRNNsas
teachersinanexactlearningsetting.

Weimplement
2
thetechniqueandshowitsabilityto
extractdescriptiveautomatainsettingswhereprevious
approachesfail.Wedemonstrateitseffectivenesson
modernRNNarchitectures.

WeapplyourtechniquetoRNNstrainedto
100%
train
andtestaccuracyonsimplelanguages,anddiscover
indoingsothatsomeRNNshavenotgeneralizedto
theintendedconcept.Ourmethodeasilyrevealsand
produces
adversarialinputs
Šwordsby
thetrainedRNNandnotpresentinthetrainortestset.
2.RelatedWork
DFAextractionfromRNNswasextensivelyexploredby
Gilesandcolleagues;seeWangetal.(2017)andJacobsson
(2005)forapartialsurvey.
Broadly,theapproachesworkbyapartition-
ingofthereal-valuedRNNstatespaceandthenexploring
thenetworktransitionsinthepartitionedspace,usingtech-
niquessuchasBFSexploration(Omlin&Giles,1996)and
othertransition-samplingapproaches.Theapproachesdiffer
mainlyintheirchoiceandofpartitioning.
TheseworksgenerallyusesecondorderRNNs(Gilesetal.,
1990),whichareshowntobettermapDFAsthan
orderElmanRNNs(Elman,1990;Goudreauetal.,1994;
Wangetal.,2018).Inthiswork,however,wewillfo-
cusonGRUs(Choetal.,2014;Chungetal.,2014)and
LSTMs(Hochreiter&Schmidhuber,1997),astheyare
morewidelyusedinpractice.
Oneapproachtostatespacepartitioningistodivideeach
dimensioninto
q
equalintervals,with
q
beingthe
quanti-
zationlevel
(Omlin&Giles,1996).Thisapproachsuffers
frominherentstatespaceexplosionanddoesnotscaleto
thenetworksusedinpracticetoday.(Theoriginalpaper
demonstratesthetechniqueonnetworkswith8hiddenval-
ues,whereastoday'scanhavehundredstothousands).
Anotherapproachistoanunsupervisedsuchas
k-meanstoalargesamplesetofreachablenetworkstates
(Cechinetal.,2003;Zengetal.,1993).Thenumberof
clusters
k
generatedwiththeseisaparameterthat
mightgreatlyaffectextractionresults,especiallyifitistoo
small.ThesamplestatescanbefoundbyasimpleBFS
explorationofthenetworkstatespacetoacertaindepth,or
byrecordingallstatevectorsreachedbythenetworkwhen
appliedtoitstrainset(ifavailable).
2
www.github.com/tech-srl/lstar
extraction
Aninherentweaknessofboththeseapproachesisthatthe
partitioningissetbeforetheextractionbegins,withnomech-
anismforrecognizingandovercomingoverlycoarsebehav-
ior.Bothmethodsthusfacethechallengeofchoosingthe
bestparametervalueforextraction.Theyaregenerallyap-
pliedseveraltimeswithdifferentparametervalues,after
whichthe`best'DFAischosenaccordingtoaheuristic.
CurrenttechniquestreatallthedimensionsofanRNNasa
singlestate.Infuturework,itmaybeinterestingtomake
thedistinctionbetween`fast'and`slow'internalstatesasin-
troducedinthedifferentialstateframeworkunifyingGRUs
andLSTMs(OrorbiaIIetal.,2017).
3.Background
RecurrentNeuralNetworksandRNNacceptors
AnRNN
isaparameterizedfunction
g
R
(
h;x
)
thattakesasinputa
state-vector
h
t
2
R
d
s
andaninputvector
x
t
+1
2
R
d
i
and
returnsastate-vector
h
t
+1
2
R
d
s
.AnRNNcanbeapplied
toasequence
x
1
;:::;x
n
byrecursiveapplicationofthefunc-
tion
g
R
tothevectors
x
i
.Touseasetofdiscretesymbolsas
aninputalphabet,eachsymbolisdeterministicallymapped
toaninputvectorusingeitheraone-hotencodingoran
embeddingmatrix.Asweareonlyinterestedintheinternal
networktransitions,weuseone-hotencodinginthiswork.
Forconvenience,werefertoinputsymbolsandtheircor-
respondinginputvectorsinterchangeably.Wedenotethe
statespaceofanetwork
R
by
S
R
=
R
d
s
.Formulti-layered
RNNs,whereseverallayerseachhavetheirownstatevector,
weconsidertheconcatenationofthesevectorsasthestate
vectoroftheentirenetwork.Inabinary
RNN-acceptor
,
thereisanadditionalfunction
f
R
:
S
R
!f
Acc;Rej
g
that
theRNN'sstatevectors.AnRNN-acceptor
R
is
bythepairoffunctions
g
R
;f
R
.
NetworkAbstraction
Givenaneuralnetwork
R
withstate
space
S
andalphabet

,andapartitioningfunction
p
:
S
!
N
,OmlinandGiles(1996)presentedamethodforextracting
aDFAforwhicheverystateisapartitionfrom
p
,andthe
statetransitionsandarebyasingle
samplefromeachpartition.ThemethodiseffectivelyaBFS
explorationofthepartitionsby
p
,beginningwith
p
(
h
0
)
,where
h
0
isthenetwork'sinitialstate,andcontinuing
accordingtothenetwork'stransitionfunction
g
R
.
Wedenoteby
A
R;p
theDFAextractedbythismethodfrom
anetwork
R
andpartitioning
p
,anddenoteallitsrelated
setsandfunctionsbysubscript
R;p
.
TheL

Algorithm
TheL

algorithmisanexactlearning
algorithmforextractingaDFAfromanyteacherthatcan
answer
membershipqueries
(labelagivenword)and
equiv-
alencequeries
(acceptorrejectagivenDFA,withacoun-
terexampleifrejecting).WeknowthatL

alwaysproposes
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
aminimalDFAinequivalencequeriesandutilizethisinour
work.Beyondthis,wetreatthealgorithmasablackbox.A
shortreviewisprovidedinthesupplementarymaterial.
4.LearningAutomatafromRNNsusingL*
WebuildanRNN-basedteacherforL

asfollows:
Formembershipqueries
,weusetheRNNdi-
rectly,checkingwhetheritacceptsorrejectsthegivenword.
Forequivalencequeries:
GivenaproposedDFA
A
,we
compareittoabstractions
A
R;p
ofthenetwork
R
,begin-
ningwithsomeinitialpartitioning
p
of
S
R
.Ifwea
disagreementbetween
A
andanabstraction
A
R;p
,weuse
R
todeterminewhethertoreturnitasacounterexampleor
to
p
andrestartthecomparison.
Intheorythiscontinuesuntil
A
and
A
R;p
converge,i.e.,are
equivalent.Inpractice,forsomeRNNsthismaytakealong
timeandyieldalargeDFA(
>
30,000states).Tocounter
this,weplacetimeorsizelimitsontheinteraction,after
whichthelastL

DFA,
A
,isreturned.Weseethatthese
DFAsstillgeneralizewelltotheirrespectivenetworks.
3
Note
Convergenceof
A
R;p
and
A
doesnotguaranteethat
R
and
A
areequivalent.Providingsuchaguaranteewould
beaninterestingdirectionforfuturework.
5.Notations
AutomatonandFunction
Foradeterminis-
ticautomaton
A
=
h

;Q;q
0
;F;
i
,

isitsalphabet,
Q
thesetofautomatonstates,
F

Q
thesetofaccepting
states,
q
0
2
Q
theinitialstate,and

:
Q


!
Q
itstransitionfunction.Wedenoteby
^

:
Q



!
Q
therecursiveapplicationof

toasequence,i.e.,forevery
q
2
Q
,
^

(
q;""
)=
q
,andforevery
w
2


and
˙
2

,
^

(
q;w

˙
)=

(
^

(
q;w
)
;˙
)
.Forconvenience,weaddthe
notation
f
:
Q
!f
Acc;Rej
g
asthefunctiongivingthe
ofeachstate,i.e.,
f
(
q
)=
Acc
()
q
2
F
.
BinaryRNN-acceptor
ForabinaryRNN-acceptor,we
denoteby
h
0
;R
theinitialstateofthenetwork,andby
^
g
R
:
S
R



!
S
R
therecursiveapplicationof
g
R
to
asequence,i.e.,forevery
h
2
S
R
,
^
g
R
(
h;""
)=
h
,andfor
every
w
2


and
˙
2

,
^
g
R
(
h;w

˙
)=
g
R
(^
g
R
(
h;w
)
;˙
)
.
Wedropthesubscript
R
whenitisclearfromcontext.
WenotethatagivenRNN-acceptorcanbeinterpretedasa
deterministic,thoughpossiblystatemachine.
3
Wecouldalsoreturnthelastabstraction,
A
R;p
,andfocuson

p
overreturningcounterexamples.Buttheabstractionsare
oftenlessaccurate.Wesuspectthisisduetothelackof`foresight'
A
R;p
hasincomparisontoL

'smanyseparatingsufstrings.
Shorthand
Asanabuseofnotation,foranyDFAorRNN

C
withstatetransitionfunction
t
C
,state
cationfunction
f
C
,andinitialstate
q
C;
0
,weuse
^
t
C
(
w
)
to
denote
^
t
C
(
q
C;
0
;w
)
,
f
C
(
q;w
)
todenote
f
C
(
^
t
C
(
q;w
))
,and
f
C
(
w
)
todenote
f
C
(
^
t
C
(
q
C;
0
;w
))
.Withinthisnotation,
theofaword
w
2


byanautomaton
A
andabinaryRNN-acceptor
R
withrespective
functions
f
A
and
f
R
aregivenby
f
A
(
w
)
and
f
R
(
w
)
.
6.AnsweringEquivalenceQueries
6.1.Overview
Givenanetwork
R
,apartitioningfunction
p
:
S
!
N
over
itsstatespace
S
,andaproposedminimalautomaton
A
,
wewishtocheckwhether
R
isequivalentto
A
,preferably
exploringaslittleof
R
'sbehaviorasnecessarytorespond.
Wesearchforadisagreeingexample
w
between
A
andthe
abstraction
A
R;p
,byparalleltraversalofthetwo.Ifoneis
found,wecheckitstruein
R
.Ifthisdisagrees
with
A
,
w
isreturnedasacounterexample;otherwise,
p
is
(Section7)andthetraversalbeginsagain.
4
Everycounterexample
w
returnedbyourmethodisinher-
entlytrue,i.e.,
f
A
(
w
)
6
=
f
R
(
w
)
.Fromthisandthe
minimalityofL

equivalencequeries,weobtain:
Property1
Everyseparatestateintheextractedau-
tomaton
A
isbyconcreteinputtothenetwork.
Inotherwords,allcomplexityinaDFAextractedfroma
givenRNN
R
isaresultoftheinherentcomplexityof
R
.
Thisisincontrasttoothermethods,inwhichincorrectpar-
titioningofthenetworkstatespacemayleadtounnecessary
complexityintheextractedDFA,evenafterminimization.
Moreover,ourmethodthepartitioningonlywhenit
isproventoocoarsetocorrectlyrepresentthenetwork:
Property2
Everyrtothepartitioningfunction
p
:
S
!
N
isbyconcreteinputtothenetwork.
Thisisimportant,asthesearchforcounterexamplesruns
atopanextractionoftheabstraction
A
R;p
,andsounneces-
sarymayleadtostatespaceexplosionŠ
canmakethesearchsoslowastobeimpractical.
Forclarity,wehenceforthrefertothecontinuousnetwork
states
h
2
S
asR-states,theabstractedstatesin
A
R;p
as
A-states,andthestatesoftheL

DFAsasL-states.
6.2.ParallelExploration
Thekeyintuitiontoourapproachisthat
A
isminimal,and
soeachA-stateshouldŠifthetwoDFAsareequivalentŠbe
4
Ifthenementdoesnotaffectanystatestraversedsofar,this
isequivalenttothecurrentstate'sabstractionandcontinuing.
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
equivalenttoexactlyoneL-state,w.r.t.and
projectionoftransitionfunctions.Theextractionof
A
R;p
is
effectivelyaBFStraversalof
A
R;p
,allowingustoassociate
betweenstatesinthetwoDFAsduringitsextraction.
Werefertobadassociations,inwhichanacceptingA-state
isassociatedwitharejectingL-stateorviceversa,as
ab-
stract
,andtomultiplebutdisagreeing
associations,inwhichoneA-stateisassociatedwithtwodif-
ferentL-states,as
clustering
.(Theinversecase,in
whichoneL-stateisassociatedwithseveralA-states,isnot
necessarilyaproblem,as
A
R;p
isnotnecessarilyminimal.)
WemayalsoassertthattheofeachR-state
h
encounteredwhileextracting
A
R;p
isidenticaltothatof
theL-state
q
A
2
Q
A
thattheparalleltraversalof
A
reaches
duringtheexploration.AstheofanA-stateis
determinedbytheR-statewithwhichitwasreached,
thisalsocoversallabstractWerefer
tofailuresofthisassertionas

,and
checkonlyforthemandforclustering
6.3.ResolutionandCounterexample
Generation
Weassumeaninitialpartitioning
p
:
S
!
N
oftheR-state
spaceandaoperation
ref
:
p;h;H
7!
p
0
which
receivesapartitioning
p
,anR-state
h
,andasetofR-states
H

S
nf
h
g
,andreturnsanewpartitioning
p
0
satisfying:
1.
8
h
1
2
H
,
p
0
(
h
)
6
=
p
0
(
h
1
)
,and
2.
8
h
1
;h
2
2
S
,
p
(
h
1
)
6
=
p
(
h
2
)
)
p
0
(
h
1
)
6
=
p
0
(
h
2
)
.
(Inpractice,condition1mayberelaxedtoseparatingat
leastoneofthevectorsin
H
from
h
,andourmethodcan
andhasovercomeimperfectsplits.)

occurwhensome
w
2


forwhich
f
R
(
w
)
6
=
f
A
(
w
)
hasbeentraversedduring
A
R;p
'sextrac-
tion.Weresolvethembyreturning
w
asacounterexample.
Clustering
occurwhentheparallelexplorationas-
sociatesanA-state
q
2
Q
R;p
withanL-state
q
2
,after
q
has
alreadybeenassociatedwithanL-state
q
1
6
=
q
2
.As
A
is
minimal,
q
1
and
q
2
cannotbeequivalent.Itfollowsthatif
w
1
;w
2
2


aretheBFStraversalpathsthroughwhich
q
wasassociatedwith
q
1
;q
2
2
Q
A
,thenthereexistssome
differentiatingsequence
s
2


forwhich
f
A
(
q
1
;s
)
6
=
f
A
(
q
2
;s
)
,i.e.,forwhich
f
A
(
w
1

s
)
6
=
f
A
(
w
2

s
)
.Con-
versely,thearrivalof
w
1
and
w
2
atthesameA-state
q
2
A
R;p
gives
f
R;p
(
w
1

s
)=
f
R;p
(
q;s
)=
f
R;p
(
w
2

s
)
.
Itfollowsthat
A
and
A
R;p
disagreeontheof
either
w
1

s
or
w
2

s
,andsonecessarilyatleastoneisnot
equivalentto
R
.Wepass
w
1

s
and
w
2

s
through
R
fortheir
trueIf
A
isatfault,thesequenceonwhich
A
and
R
disagreeisreturnedasacounterexample.Otherwise,
necessarily,
f
R
(
w
1

s
)
6
=
f
R
(
w
2

s
)
,andso
A
R;p
should
satisfy
^

R;p
(
w
1
)
6
=
^

R;p
(
w
2
)
.TheR-states
h
1
=^
g
(
w
1
)
and
h
2
=^
g
(
w
2
)
arepassed,alongwith
p
,to
ref
,toyielda
new,,partitioning
p
0
forwhich
^

R;p
0
(
w
1
)
6
=
^

R;p
0
(
w
2
)
.
Thisreasoningappliesto
w
2
with
all
paths
w
0
thathave
reached
q
withoutbefore
w
2
.Assuch,the
cationsof
all
words
w
0

s
aretestedagainst
R
,prioritizing
returningacounterexampleover
p
.Ifa
istriggered,then
h
=^
g
(
w
2
)
issplitfromthesetofR-states
h
0
=^
g
(
w
0
)
.
Algorithm1showspseudocodeforthisequivalencecheck-
ing.Init,allmappingsexceptoneareuniqueand
beforetheyareaccessed.Theexceptionis
Paths
,aswe
mightreachthesameR-state
h
2
S
R
morethanonce,by
differentpaths.Thiscanberemediedbymaintainingin
Paths
notsinglepathsbutlistsofpaths.
Ourexperimentsshowedthatlongcounterexamplesoften
caused
A
toblowup,withoutgeneralizingwell.Thus,we
alwaysreturntheshortestavailablecounterexample.
7.Abstractionand
Givenapartitioning
p
,anR-state
h
,andasetofR-states
H

S
nf
h
g
,we
p
inaccordancewiththerequire-
mentsdescribedinSection6.3.Wewanttogeneralizethe
informationgivenby
h
and
H
well,soasnottoinvoke
excessiveWealsoneedaninitialpartitioning
p
0
fromwhichtostart.
OurmethodisunaffectedbythelengthoftheR-states,and
veryconservative:eachincreasesthenumberof
A-statesbyexactlyone.Ourexperimentsshowthatitisfast
enoughtoquicklycounterexamplestoproposedDFAs.
7.1.InitialPartitioning
Aswewishtokeeptheabstractionassmallaspossible,we
beginwithnostateseparationatall:
p
0
:
h
7!
0
.
7.2.Support-Vectorbased
Inthissectionweassume
p
(
h
0
)=
p
(
h
)
forevery
h
0
2
H
,
whichistrueforourcase.Themethodgeneralizestrivially
tocaseswherethisisnottrue.
5
WewouldliketoallocatearegionaroundtheR-state
h
thatislargeenoughtocontainotherR-statesthatbehave
similarly,butseparatefromneighboringR-statesthatdo
not.WeachievethisbyanSVM(Boseretal.,1992)
withanRBFkernel
6
toseparate
h
from
H
.The
5
Byremovingfrom
H
anyvectors
h
0
forwhich
p
(
h
0
)
6
=
p
(
h
)
.
6
Whileweseethisasanaturalchoice,otherkernelsorclassi-
mayyieldsimilarresults.Wedonotexploresuchvariations
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
Algorithm1
Pseudo-codeforequivalencecheckingofan
RNN
R
andminimalDFA
A
,withinitialpartitioning
p
0
.
method
update
records(
q;h;q
A
;w
):
Visitors
(
q
)
 
Visitors
(
q
)
[f
h
g
,
Paths
(
h
)
 
w
Association
(
q
)
 
(
q
A
)
Push(New,
f
h
g
)
endmethod
method
handle
cluster
conf(
q;q
A
;q
0
A
):

s
2


s.t.
f
A
(
q
A
;s
)
6
=
f
A
(
q
0
A
;s
)
for
h
2
Visitors(
q
)
do
w
 
Paths
(
h
)

s
if
f
R
(
w
)
6
=
f
A
(
w
)
then
return
Reject,
w
endfor
p
 
ref
(
p;h
0
;
Visitors(
q
0
)
nf
h
0
g
)
return
Restart
Exploration
endmethod
method
parallel
explore(
R;
A
;p
):
emptyallof:
Q;F;
,New,Visitors,Paths,Association
q
0
 
p
(
h
0
)
update
records(
q
0
;h
0
;q
A
;
0
;""
)
while
New
6
=
;
do
h
 
Pop(New)
q
 
p
(
h
)
q
A
 
Association
(
q
)
if
f
R
(
h
)
6
=
f
A
(
q
A
)
then
return
Reject,(Paths(
h
))
if
q
2
Q
thencontinue
Q
 
Q
[f
q
g
if
f
R
(
h
)=
Acc
then
F
 
F
[f
q
g
for
˙
2

do
h
0
 
g
R
(
h;˙
)
q
0
 
p
(
h
0
)

(
q;˙
)
 
q
0
if
q
0
2
Q
and
Association
(
q
0
)
6
=

A
(
q
A
;˙
)
then
return
handle
cluster
conf(
q;q
A
;
A
(
q
A
;˙
)
)
update
records(
q
0
;h
0
;
A
(
q
A
;˙
)
,Paths
(
h
)

˙
)
endfor
endwhile
return
Accept
endmethod
method
check
equivalence(
R;
A
;p
0
):
p
 
p
0
verdict
 
Restart
Exploration
while
verdict=Restart
Exploration
do
verdict,
w
 
parallel
explore(
R;
A
;p
)
endwhile
return
verdict,
w
endmethod
max-marginpropertyoftheSVMensuresalargespace
around
h
,whiletheGaussianRBFkernelallowsforanon-
linearpartitioningofthespace.
WeusethistosplittheA-state
p
(
h
)
,yieldinganew
partitioning
p
0
withexactlyonemoreA-statethan
p
.We
trackthebyarrangingtheobtainedSVMsina
decisiontree,whereeachnode'sdecisionisthecorrespond-
ingSVM,andtheleavesrepresentthecurrentA-states.
BarringfailureoftheSVM,thisapproachthere-
quirementsofoperations,andavoidsstateexplo-
sionbyaddingonlyoneA-stateperOtherwise,
themethodfailstosatisfyrequirement1.Nevertheless,at
leastoneoftheR-states
h
0
2
H
isseparatedfrom
h
,and
laterexplorationscaninvokefurtherifneces-
sary.Inpracticethisdoesnothinderthegoaloftheabstrac-
tion:counterexamplestoequivalencequeries.
Theabstraction'sstorageislinearinthenumberofA-states
itcanmapto;andcomputinganR-state'sassociatedA-state
maybelinearinthisnumberaswell.However,asthis
numberofA-statesalsogrowsveryslowly(linearlyinthe
numberofthisdoesnotbecomeaproblem.
7.3.PracticalConsiderations
Astheinitialpartitioningandtheoperationare
verycoarse,themethodmayacceptverysmallbutwrong
DFAs.Tocounterthis,twomeasuresaretaken:
1.
Oneacceptingandonerejectingsequenceareprovided
totheteacheraspotentialcounterexamplestobecon-
sideredateveryequivalencequery.
2.
Theusesanaggressiveapproachthat
generatesagreat(butmanageable)numberofA-states.
Themeasure,necessarytopreventterminationona
singlestateautomaton,requiresonlytwosamples.These
canbefoundbyrandomsampling,ortakenfromthetraining
set.
7
InkeepingwiththeobservationmadeinSection6.3,
wetaketheshortestavailablesamples.Thesecondmeasure
preventstheextractionfromtooreadilyterminatingonsmall
DFAs.OurmethodforitispresentedinSection7.3.1.
7.3.1.A
GGRESSIVE
D
IFFERENCE
-
BASED
R
EFINEMENT
Wesplit
h
fromatleastoneofthevectors
h
0
2
H
bysplit-
ting
S
alongthe
d
dimensionswiththelargestgapbetween
h
andthemean
h
m
of
H
,downthemiddleofthatgap.This
canbecomfortablymaintainedinadecisiontree,
generatingatthesplitpointatreeofdepth
d
forwhich,
oneachlayer
i
=1
;
2
;:::;d
,eachnodeissplitalongthe
dimensionwiththe
i
-thlargestgap.Thisfollows
intuitivelyfromthequantizationsuggestedbyOmlinand
inthiswork.
7
Ifnosuchsamplesexist,asinglestateDFAmaybecorrect.
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
Giles,butfocusesonlyonthedimensionswiththegreatest
deviationofvaluesbetweenthestatesbeingsplitandsplits
the`active'rangeofvalues.
Thevalue
d
maybesetbytheuser,andincreasediftheex-
tractionissuspectedtohaveconvergedtoosoon.Wefound
thatvaluesofaround7-10generallyprovideastrongenough
initialpartitioningof
S
,withoutmakingtheabstractiontoo
largeforfeasibleexploration.
8.ExperimentalResults
Wedemonstratetheeffectivenessofourmethodonnet-
workstrainedontheTomitagrammars(1982),
8
usedas
benchmarksinpreviousautomata-extractionwork(Wang
etal.,2017),andonsubstantiallymorecomplicatedlan-
guages.Weshowtheeffectivenessofourequivalencequery
approachoversimplerandomsamplingandpresentcasesin
whichourmethodextractsinformativeDFAswhereasother
approachesfail.Inaddition,forsomeseeminglyperfect
networks,wethatourmethodquicklyreturnscounterex-
amplesrepresentingdeviationsfromthetargetlanguage.
Onallnetworks,weappliedourmethodwithinitialre-
depth
10
.Unlikeotherextractionmethods,where
parametersmustbetunedtothebestDFA,noparameter
tuningwasrequiredtoachieveourresults.
Weclarifythatwhenwerefertoextractiontimeforany
method,weconsiderthe
entire
process:fromthemoment
theextractionbegins,tothemomentaDFAisreturned.
9
PrototypeImplementationandSettings
Weimple-
mentedallmethodsinPython,usingPyTorch(Paszkeetal.,
2017)andscikit-learn(Pedregosaetal.,2011).FortheSVM
weusedtheSVCvariant,withregularizationfac-
tor
C
=10
4
toencourageperfectsplitsandotherwisede-
faultparametersŠinparticular,theRBFkernelwithgamma
value
1
=
(
numfeatures
)
.
TrainingSetup
Asourfocuswasextraction,wetrained
allnetworksto
100%
accuracyontheirtrainsets,andof
theseweconsideredonlythosethatreached
99
:
9+%
accu-
racyonadevsetconsistingofupto1000uniformlysampled
wordsofeachofthelengths
n
2
1
;
4
;
7
;:::;
28
.Thepositive
tonegativesampleratiosinthedevsetswerenotcontrolled.
Thetrainsetscontainedsamplesofvariouslengths,witha
8
TheTomitagrammarsarethefollowing7languagesover
thealphabet
f
0
;
1
g
:[1]
1

,[2]
(10)

,[3]thecomplement
of
((0|1)

0)

1(11)

(0(0|1)

1)

0(00)

(1(0|1)

)

,
[4]allwords
w
notcontaining
000
,[5]all
w
forwhich
#
0
(
w
)
and
#
1
(
w
)
areeven(where
#
a
(
w
)
isthenumberof
a
'sin
w
),[6]
all
w
forwhich(
#
0
(
w
)

#
1
(
w
))

3
0
,and[7]
0

1

0

1

.
9
Measuredusing
clock()
,ofPython's
time
module,and
coveringamongothers:abstractionexploration,abstraction
ments(includingtrainingSVMandL


Table1.
AccuracyofDFAsextractedfromGRUnetworksrep-
resentingsmallregularlanguages.Singlevaluesrepresentthe
averageof3experiments,multiplevalueslisttheresultforeach
experiment.Extractiontimeof30secondsisatimeout.
Hidden
DFA
AverageAccuracyonLength
Size
Time(s)
Size
10
50
100
1000
Train
50
30,30,30
11,11,155
99.9
99.8
99.9
99.9
99.9
100
11.0
11,10,11
100
99.9
99.9
99.9
100
500
30,30,30
10,10,10
100
99.9
100
99.9
100.0
1:1ratiobetweenthepositiveandnegativesamplesfrom
eachlengthwherepossible.Toachievethis,alargenumber
ofwordswereuniformlysampledforeachlength.When
notenoughsamplesofoneclasswerefound,welimited
theratioto50:1,ortookatmost50samplesifallwere
identically.Thetrainsetsizes,andthelengthsof
thesamplesinthem,arelistedforeverylanguageinthis
paperinthesupplementarymaterial.
Forlanguageswherethepositiveclasswasunlikelytobe
foundbyrandomsamplingŠe.g.balancedparenthesesor
emailsŠwegeneratedpositivesamplesusingtailoredfunc-
tions.
10
Inthesecaseswealsogeneratednegativesamples
bymutatingthepositiveexamples.
11
Whereveratestsetis
mentioned,itwastakenasa1:1samplesetfromthesame
distributiongeneratingthepositiveandnegativesamples.
EffectivenessonRandomRegularLanguages
We
evaluatedourmethodonthe7Tomitagrammars.Wetrained
one2-layerGRUnetworkwithhiddensize
100
foreach
grammar(7RNNsintotal).AllbutoneRNNreached
100%
devaccuracy;theonetrainedonthe6thTomitagrammar
reached
99
:
94%
.ForeachRNN,ourmethodcorrectlyex-
tractedandacceptedthetargetgrammarinunder2seconds.
ThelargestTomitagrammarshave5-stateDFAsovera
2-letteralphabet.Wealsoexploredsubstantiallymorecom-
plexgrammars:wetrained2-layerGRUnetworkswith
varyinghidden-statesizeson10-stateminimalDFAsgen-
eratedrandomlyovera3-letteralphabet.Weappliedour
methodtothesenetworkswitha30secondtimelimitŠ
thoughmostreachedequivalencesooner.ExtractedDFAs
werecomparedagainsttheirnetworksontheirtrainsetsand
on
1000
randomsamplesforeachofseveralword-lengths.
Table1showstheresults.Eachrowrepresents3experi-
ments:
9
randomDFAsweregenerated,trainedon,andex-
tracted.TheextractedDFAsaresmall,andhighlyaccurate
evenonlongsequences(length
1000
).Additionalresults
showingsimilartrends,includingexperimentsonLSTM
networks,areavailableinthesupplementarymaterial.
10
Forinstance,afunctionthatcreatesemailsbyuniformlysam-
pling2sequencesoflength
2

8
,choosinguniformlyfromthe
options
.com
,
.net
,andall
.co.XY
for
X,Y
lowercasecharac-
ters,andthenconcatenatingthethreewithanadditional
@
.
11
Byadding,removing,changing,ormovingupto9timechar-
acters.
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
Comparisonwitha-prioriQuantization
Intheir1996
paper,OmlinandGilessuggestpartitioningthenetwork
statespacebydividingeachstatedimensioninto
q
equal
intervals,with
q
beingthe
quantizationlevel
.Wetestedthis
methodoneachofournetworks,with
q
=2
andatimelimit
of
1000
secondstoavoidexcessivememoryconsumption.
Incontrasttoourmethod,whichextractedonthesesame
networkssmallandaccurateDFAswithin
30
seconds,we
foundthatforthismethodthiswasnotenoughtimetoex-
tractacompleteDFA.TheextractedDFAswerealsovery
largeŠoftenwithover
60
;
000
statesŠandtheircoverage
ofsequencesoflength
1000
tendedtozero.Forthecovered
sequenceshowever,theextractedDFA'saccuracywasof-
tenveryhigh(
99
+
%
),suggestingthatquantizationŠwhile
impracticalŠissufexpressivetodescribeanet-
work'sstatespace.However,itisalsopossiblethatthe
sheersizeofthequantization(
2
50
foroursmallestRNNs)
simplyallowedeachexploredR-stateitsownA-state,giving
highaccuracybyobservationbias.
Thishighlightsthekeystrengthofourmethod:incontrast
toothermethods,ourmethodisabletosmalland
accurateDFAsrepresentingagivenRNN,whensuchDFAs
areavailable.Itdoesthisinafractionofthetimerequiredby
othermethodstocompletetheirextractions.Thisisbecause,
unlikeothermethods,itmaintainsfromaveryearlypointin
extractionacompleteDFAthatconstitutesacontinuously
improvingapproximationof
R
.
ComparisonwithRandomSamplingForCounterexam-
pleGeneration
Weshowthatthereismerittoourap-
proachtoequivalencequeriesoversimplerandomsampling.
Networks
R
forwhichtheratiobetweenacceptingand
rejectingsequencesisveryunevenmaybecloselyapprox-
imatedbysimpleDFAsŠmakingithardtodifferentiate
betweenthemandtheirL

proposedautomatabyran-
domsampling.Wetrainedtwonetworksononesuchlan-
guage:balancedparentheses(BP)overthe28-letteralpha-
bet
f
a
;
b
;:::;
z
;
(
;
)
g
(thelanguageofallsequences
w
over
a-z()
inwhicheveryopeningparenthesisiseventually
followedbyasinglecorrespondingclosingparenthesis,and
viceversa).Thenetworksweretrainedto
100%
accuracyon
trainsetsofsize
˘
44600
,containingsampleswithbalanced
parenthesesuptodepth
11
.Thetwotrainsetshad
36%
and
43%
negativesamples,whichwerecreatedbyslightly
mutatingthepositivesamples.Thenetworkswerea2-layer
GRUanda2-layerLSTM,bothwithhiddensize50percell.
WeextractedfromthesenetworksusingL

,approaching
equivalencequerieseitherwithourmethodorbyrandom
sampling.Weimplementedtherandomsamplingteacher
tosampleupto1000wordsofeachlengthinincreasing
order.Forfairness,wealsoprovideditwiththesametwo
initialsamplesourteacherwasgiven,allowingittocheck
Table2.
Accuracyandmaximumnestingdepthofextractedau-
tomatafornetworkstrainedonBP,usingeitherabstractions(ﬁAb-
strﬂ)orrandomsampling(ﬁRSﬂ)forequivalencequeries.Accuracy
ismeasuredwithrespecttothetrainedRNN.
TrainSetAccuracy
MaxNest.Depth
Network
Abstr
RS
Abstr
RS
GRU
99.98
87.12
8
2
LSTM
99.98
94.19
8
3
Table3.
Counterexamplesgeneratedduringextractionofautomata
fromaGRUnetworktrainedonBP.
Based
BruteForce
example
Time(s)
example
Time(s)
))
1.1
))
0.4
(())
1.2
(()i)ma
32.6
((()))
2.1
(((())))
3.1
((((()))))
3.8
(((((())))))
4.4
((((((()))))))
6.6
(((((((())))))))
9.2
((((((((v())))))))
10.7
((((((((a()z)))))))))
8.3
andpossiblyreturnthemateveryequivalencequery.
Weraneachextractionwithatimelimitof
400
secondsand
foundanicepattern:everyDFAproposedbyL

represented
BPtosomeboundednestingdepth,andeverycounterexam-
pletaughtittoincreasethatdepthby1.
TheaccuracyoftheextractedDFAsonthenetworktrain
setsisshowninTable2,alongwiththemaximumdepth
theL

DFAsreachedwhilestillmimickingBP.Forthe
GRUextractions,thecounterexamplesandtheirgeneration
timesarelistedinTable3.Notethespeedandsuccinct-
nessofthosegeneratedbyourmethodasopposedtothose
generatedbyrandomsampling.
AdversarialInputs
Excitingly,thepenultimatecoun-
terexamplereturnedbyourmethodisanadversarialinput:
asequencewithunbalancedparenthesesthatthenetwork
(incorrectly)accepts.Thisinputisfoundinspiteofthe
network'sseeminglyperfectbehavioronits
44000
+sample
trainset.Westressthattherandomsamplerdidnotmanage
tosuchsamples.
Inspectingtheextractedautomataindeedrevealsanalmost-
but-not-quitecorrectDFAfortheBPlanguage(theautomata
aswellasthecounterexamplesareavailableinthesupple-
mentarymaterial).TheRNNovtorandompeculiarities
inthetrainingdataanddidnotlearntheintendedlanguage.
k-MeansClustering
Wealsoimplementedasimplek-
meansclusteringandextractionapproachandappliedit
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
totheBPnetworkswithavarietyof
k
values,allowing
ittodividethestatespaceintoupto
100
clustersbased
onthestatesobservedwiththenetworks'trainsets.This
failedtolearnanyBPtoanydepthforeithernetwork:for
bothnetworks,itonlymanagedtoextractDFAsalmost
resemblingBPtonestingdepth3(acceptingalsosome
unbalancedsequences).
Limitations
DuetoL

'spolynomialcomplexityandin-
tolerancetonoise,fornetworkswithcomplicatedbehavior,
extractionbecomesextremelyslowandreturnslargeDFAs.
WheneverappliedtoanRNNthathasfailedtogeneralize
properlytoitstargetlanguage,ourmethodsoonsev-
eraladversarialinputs,buildsalargeDFA,andtimesout
whileit.
12
Thisdoeshoweverdemonstratetheeasewithwhichthe
methodincorrectlytrainednetworks.Thesecases
areannoyinglyfrequent:formanyRNN-acceptorswith
100%trainandtestaccuracyonlargetestsets,ourmethod
wasabletomanysimpleexamples.
Forinstance,foraseeminglyperfectLSTMnetworktrained
ontheregularexpression
[a-z][a-z0-9]*@[a-z0-9]+.(com
j
net
j
co.[a-z][a-z])$
(simpleemailaddressesoverthe38letteralphabet
f
a-z
;
0-9
;
@
;
.
g
)to100%accuracyona40,000sample
trainsetanda2,000sampletestset,ourmethodquicklyre-
turnedthecounterexamplesseeninTable4,showingclearly
wordsthatthenetwork(e.g.,
25.net
).We
ranextractiononthisnetworkfor400seconds,andwhile
wecouldnotextractarepresentativeDFAinthistime,
13
ourmethoddidshowthatthenetworklearnedafarmore
elaborate(andincorrect)functionthanneeded.Incontrast,
givena400secondoveralltimelimit,therandomsampler
didnotanycounterexamplebeyondtheprovidedone.
Wenotethatourimplementationofkmeansclusteringand
extractionhadnosuccesswiththisnetwork,returninga
completelyrejectingautomaton(representingtheempty
language),despitetrying
k
valuesofupto
100
andusingall
ofthenetworkstatesreachedusingatrainsetwith50/50
ratiobetweenpositiveandnegativesamples.
Beyonddemonstratingthecapabilitiesofourmethod,these
resultsalsohighlightthebrittlenessingeneralizationof
trainedRNNs,andsuggestthatevidencebasedontest-set
performanceshouldbeinterpretedwithextremecaution.
12
ThishappenedalsotoourBPLSTMnetwork,whichtimed
outduringL

afterthelastcounterexample.
13
A134-stateDFA
A
wasproposedbyL

after178seconds,
andthenextto
A
(4.43secondslater)timedout.The
accuracyofthe134-stateDFAonthetrainsetwasnearlyrandom.
Wesuspectthatthenetworklearnedsuchacomplicatedbehavior
thatitsimplycouldnotberepresentedbyanysmallDFA.
Table4.
Counterexamplesgeneratedduringextractionfroman
LSTMemailnetworkwith
100%
trainandtestaccuracy.Examples
ofthenetworkdeviatingfromitstargetlanguageareshowninbold.
Counter-
Network
Target
example
Time(s)


0@m.com
provided
p
p
@@y.net
2.93


25.net
1.60
p

5x.nem
2.34
p

0ch.nom
8.01


9s.not
3.29


2hs.net
3.56
p

@cp.net
4.43


ThisreverberatestheresultsofGormanandSproat(2016),
whotrainedaneuralarchitecturebasedonamulti-layer
LSTMtomimicastatetransducer(FST)fornumber
normalization.TheyshowedthattheRNN-basednetwork,
trainedon22Msamplesandvalidatedona2.2Msample
developmentsetto0%erroronboth,stillhadoccasional
errors(thoughwitherrorrate
<
0.0001)whenappliedtoa
240,000sampleblindtestset.
9.Conclusions
Wepresentanoveltechniqueforextractingdeterministic
automatafromrecurrentneuralnetworkswithroots
inexactlearning.Asourmethodmakesnoassumptions
astotheinternalofthenetwork,itiseasily
applicabletoanyRNNarchitecture,includingthepopular
LSTMandGRUmodels.
Incontrasttopreviousmethods,ourmethodisnotaffected
byhiddenstate-size,andsuccessfullyextractsrepresentative
DFAsforanynetworksthatcanindeedberepresentedas
such.Unlikeotherextractionapproaches,ourtechnique
workswithlittletonoparametertuning,andrequiresvery
littlepriorinformationtogetstarted(theinputalphabet,and
2labeledexamples).
OurmethodisguaranteedtoneverextractaDFAmore
complicatedthanthelanguageoftheRNNbeingconsid-
ered.Moreover,thecounterexamplesreturnedduringour
extractioncanpointustoincorrectpatternsthenetworkhas
learnedwithoutourawareness.
Beyondscalabilityandeaseofuse,ourmethodcanreturn
reasonablyaccurateDFAsevenifextractioniscutshort.
Moreover,wehaveshownthatfornetworksthatdocor-
respondtosuccinctautomata,ourmethodgetsverygood
resultsŠgenerallyextractingsmall,succinctDFAswith
accuraciesofover
99%
withrespecttotheirnetworks,in
secondsortensofseconds.Thisisincontrasttoexisting
methods,whichrequireordersofmagnitudemoretimeto
complete,andoftenreturnlargeandcumbersomeDFAs
(withtensofthousandsofstates).
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
SupplementaryMaterial
Thissupplementarymaterialcontainsadescriptionofthe
L

algorithm(AppendixA),andadditionalexperimental
resultsanddetails(AppendixB).
A.Angluin'sL*Algorithm
Algorithm2
L*Algorithmwithexplicitmembershipand
equivalencequeries.
S
 f

g
;E
 f

g
for
(
s
2
S
)
,
(
a
2

,and
(
e
2
E
)
do
T
[
s;e
]
 
Member
(
s

e
)
T
[
s

a;e
]
 
Member
(
s

a

e
)
endfor
while
True
do
while
(
s
new
 
Closed
(
S;E;T
)
6
=
?
)
do
Add
(
S;s
new
)
for
(
a
2

;e
2
E
)
do
T
[
s
new

a;e
]
 
Member
(
s
new

a

e
)
endfor
endwhile
A 
MakeHypothesis
(
S;E;T
)
cex
 
Equivalence
(
A
)
if
cex
=
?
then
return
A
else
e
new
 
FindSuffix
(
cex
)
Add
(
E;e
new
)
for
(
s
2
S;a
2

do
T
[
s;e
new
]
 
Member
(
s

e
new
)
T
[
s

a;e
new
]
 
Member
(
s

a

e
new
)
endfor
endif
endwhile
Angluin'sL

algorithm(1987)isanexactlearningalgorithm
forregularlanguages.Thealgorithmlearnsanunknown
regularlanguage
U
overanalphabet

,generatingaDFA
thataccepts
U
asoutput.Weonlyprovideabriefand
informaldescriptionofthealgorithm;forfurtherdetails
see(Angluin,1987;Bergetal.,2005).
Algorithm2showstheL

algorithm.Thisversionisadapted
fromAluretal.(2005),wherethemembershipandequiva-
lencequerieshavebeenmademoreexplicitthantheyappear
inAngluin(1987).
Thealgorithmmaintainsan
observationtable
(
S;E;T
)
that
recordswhetherstringsbelongto
U
.InAlgorithm2,this
tableisrepresentedbythetwo-dimensionalarray
T
,with
dimensions
j
S
jj
E
j
,where,informally,wecanview
S
as
asetofwordsthatleadfromtheinitialstatetostatesofthe
hypothesizedautomaton,and
E
asasetofwordsserving
asexperimentstoseparatestates.Thetable
T
itselfmapsa
word
w
2
(
S
[
S



E
to
True
if
w
2
U
and
False
otherwise.
Thetableisupdatedbyinvokingmembershipqueriestothe
teacher.Whenthealgorithmreachesaconsistentandclosed
observationtable(meaningthatallstateshaveoutgoing
transitionsforallletters,withoutcontradictions),thealgo-
rithmconstructsahypothesizedautomaton
A
,andinvokes
an
equivalencequery
tocheckwhether
A
isequivalentto
theautomatonknowntotheteacher.Ifthehypothesized
automatonacceptsexactly
U
,thenthealgorithmterminates.
Ifitisnotequivalent,thentheteacherproducesacounterex-
ampleshowingadifferencebetween
U
andthelanguage
acceptedby
A
.
Arunthroughofthealgorithmisasfollows:
thelearnerstartswithanautomatonwithonestateŠthe
initialstateŠwhichisacceptingorrejectingaccordingto
theoftheemptyword.Then,foreverystate
intheautomaton,foreveryletterinthealphabet,thelearner
vbywayofmembershipqueriesthatforeveryshortest
sequencereachingthatstate,thecontinuationfromthat
withthatletteriscorrectlyAslongas
aninconsistencyexists,theautomatonisEvery
timetheautomatonreachesaconsistentstate(acomplete
transitionfunction,withnoinconsistenciesbysingle-letter
extensions),thatautomatonispresentedtotheteacherasan
equivalencequery.Ifitisaccepted,thealgorithmcompletes;
otherwise,itusestheteacher-providedcounterexampleto
furthertheautomaton.
B.AdditionalResults
B.1.RandomRegularLanguages
WeshowresultsforextractionfromGRUandLSTMnet-
workswithvaryinghiddensizes,trainedonsmallregular
languagesofvaryingalphabetsizeandDFAsize.Each
extractionwaslimitedto30seconds,andhadinitial
mentdepth10.Foreachofthecombinationsofstatesize
andtargetlanguagecomplexity,3networksofeachtype
weretrained,eachonadifferent(randomlygenerated)lan-
guage.Thefullresultsoftheseexperimentsareshownin
Table5.Notethateachrowineachofthetablesrepresents
3experiments,i.e.intotal
9

2

3=54
randomDFAs
weregenerated,trainedon,andre-extracted.
Wenotewithsatisfactionthatin36ofthe54experiments,
theextractionprocessreachedequivalenceonaregular
languageidenticaltothetargetlanguagethenetworkhad
beentrainedon.Wealsonotethatononeoccasioninthe
LSTMexperimentstheextractionreachedequivalencetoo
easily,acceptinganautomatonofsize2thatultimatelywas
notagreatmatchforthenetwork.Suchaproblemcouldbe
counteredbyincreasingtheinitialsplitdepth,forinstance
whensamplingshowsthatatoo-simpleautomatonhasbeen
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
Table5.
ResultsforDFAextractedusingourmethodfrom2-layerGRUandLSTMnetworkswithvariousstatesizes,trainedonrandom
regularlanguagesofvaryingsizesandalphabets.Eachrowineachtablerepresents3experimentswiththesameparameters(network
hidden-statesize,alphabetsize,andminimaltargetDFAsize).Singlevaluesrepresenttheaverageofthe3experiments,multiplevalues
listtheresultforeachexperiment.Anextractiontimeof30secondssignalsatimedoutextraction(forwhichthelastautomatonproposed
byL

istakenastheextractedautomaton).
ExtractionfromLSTMNetworksŠOurMethod
Hidden
Alphabet
Language/
Extraction
Extracted
AverageExtractedDFAAccuracy
Size
Size
TargetDFASize
Time(s)
DFASize
l
=10
l
=50
l
=100
l
=1000
Training
50
3
5
2.9
5,5,6
100.0
99.96
99.86
99.90
100.0
100
3
5
2.9
5,5,2
92.96
92.96
93.73
93.46
91.06
500
3
5
11.8
5,5,5
100.0
100.0
100.0
99.96
100.0
50
5
5
30,30,30
68,59,115
99.96
99.93
99.76
99.93
99.99
100
5
5
30,7.7,30
57,5,38
99.96
99.96
99.96
99.90
100.0
500
5
5
30,20.7,19.0
5,5,5
100.0
100.0
99.93
99.90
100.0
50
3
10
30,30,11.1
10,10,10
99.96
99.96
99.90
99.90
100.0
100
3
10
7.6,30,7.7
10,10,11
99.96
99.93
99.96
99.96
100.0
500
3
10
30,30,30
10,9,10
92.30
92.80
93.70
93.43
92.30
ExtractionfromGRUNetworksŠOurMethod
Hidden
Alphabet
Language/
Extraction
Extracted
AverageExtractedDFAAccuracy
Size
Size
TargetDFASize
Time(s)
DFASize
l
=10
l
=50
l
=100
l
=1000
Training
50
3
5
1.7
5,5,6
100.0
100.0
99.86
99.96
100.0
100
3
5
4.1
5,5,5
100.0
100.0
100.0
99.96
100.0
500
3
5
7.0
5,5,5
100.0
100.0
100.0
100.0
100.0
50
5
5
30,30,8.2
150,93,5
100.0
99.90
99.93
99.86
100.0
100
5
5
9.0,8.0,30
5,5,16
100.0
100.0
99.96
99.96
99.99
500
5
5
15.5,30,25.6
5,5,5
100.0
100.0
99.96
100.0
100.0
50
3
10
30,30,30
11,11,155
99.96
99.83
99.93
99.93
99.99
100
3
10
11.0
11,10,11
100.0
99.93
99.96
99.93
100.0
500
3
10
30,30,30
10,10,10
100.0
99.93
100.0
99.90
100.0
Table6.
ResultsforautomataextractedusingOmlin&Giles'a-prioriquantizationasdescribedintheir1996paper,withquantization
level2,fromthesamenetworksusedinTable5(3networksforeachsetofparametersandnetworktype).
ExtractionfromLSTMNetworksŠO&GQuantization
Hidden
Alphabet
Language/
Extracted
Coverage/Accuracy(%)
Size
Size
TargetDFASize
DFASizes
l
=1
l
=5
l
=10
l
=15
l
=50
Training
50
3
5
310931073107
100100
100100
30.6683.65
3.8781.53
0.0NA
27.4488.0
100
3
5
222522522275
100100
100100
7.5780.50
0.0750.0
0.0NA
19.3184.57
500
3
5
585601584
100100
100100
0.0NA
0.0NA
0.0NA
8.8071.71
50
5
5
195619731962
100100
10073.93
0.03100
0.0NA
0.0NA
12.3978.34
100
5
5
139214001400
100100
10064.3
0.0NA
0.0NA
0.0NA
11.1974.80
500
5
5
359366366
100100
33.4370.60
0.0NA
0.0NA
0.0NA
6.2473.92
50
3
10
313532383228
100100
100100
29.4383.72
4.5794.19
0.0NA
27.7088.80
100
3
10
229422822272
100100
100100
0.9091.30
0.0NA
0.0NA
16.8381.07
500
3
10
586589589
100100
100100
0.0NA
0.0NA
0.0NA
8.4874.77
ExtractionfromGRUNetworksŠO&GQuantization
Hidden
Alphabet
Language/
Extracted
Coverage/Accuracy(%)
Size
Size
TargetDFASize
DFASizes
l
=1
l
=5
l
=10
l
=15
l
=50
Training
50
3
5
449745584485
100100
100100
50.7390.52
25.2691.95
2.6696.25
42.2891.08
100
3
5
318831843197
100100
100100
3.5066.66
0.0750.0
0.0NA
19.1483.63
500
3
5
120012211225
100100
100100
0.0NA
0.0NA
0.0NA
8.9874.45
50
5
5
281027962802
100100
10087.97
0.10100
0.0NA
0.0NA
14.5680.61
100
5
5
193519411936
100100
10073.17
0.0NA
0.0NA
0.0NA
12.0576.39
500
5
5
721706749
100100
91.0355.94
0.0NA
0.0NA
0.0NA
9.5271.53
50
3
10
459845824586
100100
100100
15.7379.76
1.2370.71
0.0NA
24.3286.93
100
3
10
320331923194
100100
100100
0.381.25
0.0NA
0.0NA
19.1883.84
500
3
10
122612091209
100100
100100
0.0NA
0.0NA
0.0NA
13.3976.64
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
extracted.
WealsoranextractionwithOmlinandGiles'a-prioriquan-
tizationoneachofthesenetworks,withquantizationlevel
2andatimelimitof50seconds.Theextractionsdidnot
completeinthistime.Forcompleteness,wepresentthe
size,coverage,andaccuraciesofthesepartiallyextracted
DFAsinTable6.ThesmallersizeoftheextractedDFAs
fornetworkswithlargerhiddensizeisaresultofthetransi-
tionfunctioncomputationtakinglonger:thisslowstheBFS
explorationandthustheextractionvisitslessstatesinthe
allottedtime.
B.2.BalancedParentheses
WetrainedaGRUandanLSTMnetworkontheirregular
languageofbalancedparentheses,andthenattemptedto
extractautomatafromthesenetworks.Forboth,L

at
proposedaseriesofautomataeachrepresentingthelan-
guageofbalancedparenthesestoincreasingnestingdepth.
SomeoftheseareshowninFig.1.FortheGRUnetwork,
afteracertainpointintheextraction,weevenfoundacoun-
terexamplewhichshowedthenetworkhadnotgeneralized
correctlytothelanguageofbalancedparentheses,andthe
nextautomatonreturnedresembledŠbutwasnotquiteŠan
automatonforbalancedparenthesesnestedtosomedepth.
WeshowthisautomatoninFig.2.
Inourmainsubmission,weshowthecounterexamplesre-
turnedduringa400-secondextractionfromtheGRUnet-
work,asgeneratedeitherbyrandomsamplingorbyour
method.Forcompleteness,wepresentnowinTable7the
counterexamplesfortheLSTMextraction.
B.3.OtherInterestingExamples
B.3.1.C
OUNTING
WetrainedanLSTMnetworkwith2layersandhiddensize
100(givingoverallstatesize
d
s
=2

2

100=400
)on
theregularlanguage
[a-z]*1[a-z1]*2[a-z2]*3[a-z3]*4[a-z4]*5[a-z5]*$
overthe31-letteralphabet
f
a
;
b
;:::;
z
;
1
;
2
;:::;
5
g
,i.e.the
regularlanguageofallsequences
1+2+3+4+5+
withlow-
ercaseletters
a-z
scatteredinsidethem.Wetrainedthis
networkonatrainsetofsize20000andtesteditonatest
setofsize2000(bothevenlysplitonpositiveandnegative
examples),andsawthatitreached100%accuracyonboth.
Weextractedfromthisnetworkusingourmethod.Within2
counterexamples(theprovidedcounterexample
12345
,and
anothergeneratedbyourmethod),andafteratotalof9.5
seconds,L

proposedtheautomatonrepresentativeofthe
network'stargetlanguage,showninFig.3.However,our
methoddidnotacceptthisDFAasthecorrectDFAforthe
network.Instead,afterafurther85.4secondsofexploration
Figure1.
Selectautomataofincreasingsizeforrecognizingbal-
ancedparenthesesoverthe28letteralphabet
a-z
;
(
;
)
,uptonest-
ingdepths1wed),1(correct),2,and5,respectively.
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
Table7.
ExtractionofautomatafromanLSTMnetworktrainedto100%accuracyonthetrainingsetforthelanguageofbalanced
parenthesesoverthe28-letteralphabet
a-z
;
(
;
)
.Thetableshowsthecounterexamplesandthecounterexamplegenerationtimesforeach
ofthesuccessiveequivalencequeriesposedbyL

duringextraction,forbothourmethodandabruteforceapproach.Eachsuccessive
equivalencequeryfromL

wasanautomatonclassifyingthelanguageofallwordswithbalancedparenthesesuptonestingdepth
n
,with
increasing
n
.
Based
BruteForce
Counterexample
Time(seconds)
Counterexample
Time(seconds)
))
1.4
))
1.5
(())
1.6
tg(gu()uh)
57.5
((()))
3.1
((wviw(iac)r)mrsnqqb)iew
231.5
(((())))
3.1
((((()))))
3.4
(((((())))))
4.7
((((((()))))))
6.3
(((((((())))))))
9.2
((((((((()))))))))
14.0
Table8.
CounterexamplesreturnedtotheequivalencequeriesmadebyL

duringextractionofaDFAfromanetworktrainedto100%
accuracyonbothtrainandtestsetsontheregularlanguage
[a-z]*1[a-z1]*2[a-z2]*3[a-z3]*4[a-z4]*5[a-z5]*$
overthe31-letteralphabet
f
a
;
b
;:::;
d
;
1
;
2
;:::;
5
g
.Counterexampleshighlightingthediscrepanciesbetweenthenetworkbehaviorandthetargetbehaviorareshown
inbold.
CounterexampleGenerationfortheCountingLanguage
Counterexample
GenerationTime(seconds)
Network
Target
12345
provided
True
True
512345
8.18
False
False
aca11
85.41
True
False
blw11
0.50
True
False
dnm11
0.96
False
False
bzm11
0.90
False
False
drxr11
0.911
True
False
brdb11
0.90
False
False
elrs11
1.16
True
False
hu11
1.93
False
False
ku11
2.59
False
False
ebj11
2.77
False
False
pgl11
3.77
True
False
reeg11
4.16
False
False
eipn11
5.66
False
False
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
Figure2.
Automatonnolongerrepresentingalanguageofbalancedparenthesesuptoacertaindepth.(Showinghowatrainednetwork
maybeovpastacertainsamplecomplexity.)
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
Figure3.
DFArepresentingtheregularlanguage
[a-z]*1[a-z1]*2[a-z2]*3[a-z3]*4[a-z4]*5[a-z5]*$
overthe
alphabet
f
a
;
b
;:::;
z
;
1
;
2
;:::;
5
g
.
andthecounterexample
aca11
wasfoundand
returnedtoL

,meaning:ourmethodfoundthatthenetwork
acceptedtheword
aca11
Šdespitethiswordnotbeingin
thetargetlanguageofthenetworkandthenetworkhaving
100%accuracyonbothitstrainandtestset.
Ultimately,after400secondsourmethodextractedfromthe
network(butdidnotreachequivalenceon)aDFAwith118
states,returningthecounterexampleslistedinTable8and
achieving100%accuracyagainstthenetworkonitstrainset,
and99.9+%accuracyonallsampledsequencelengths.We
notethatbythenatureofourmethod,thecomplexityofthis
DFAisnecessarilyanindicatoroftheinherentcomplexity
oftheconcepttowhichthetrainednetworkhasgeneralized.
B.3.2.T
OKENIZED
JSONL
ISTS
WetrainedaGRUnetworkwith2layersandhiddensize
100ontheregularlanguagerepresentingasimpletokenized
JSONlistwithnonesting,
(
n
[
n
])
j
(
n
[[S0NTF](,[S0NTF])*
n
])$
overthe8-letteralphabet
f
[
;
]
;
S
;
0
;
N
;
T
;
F
;
,
g
,toaccuracy
100%onatrainingsetofsize20000andatestsetofsize
2000,bothevenlysplitbetweenpositiveandnegativeexam-
ples.Asbefore,weextractedfromthisnetworkusingour
method.
Within2counterexamples(1providedand1generated)and
atotalof3.8seconds,ourmethodextractedtheautomaton
showninFig.4a,whichisalmostbutnotquiterepresenta-
tiveofthetargetlanguage.7.12secondslateritreturned
acounterexampletothisDFAwhichpushedL

to
furtherandreturntheDFAshowninFig.4b,whichisalso
almostbutnotquiterepresentativeofzero-nestingtokenized
JSONlists.
Ultimately,after400seconds,ourmethodextracted(but
didnotreachequivalenceon)anautomatonofsize441,re-
turningthecounterexampleslistedinTable9andachieving
100%accuracyagainstthenetworkonbothitstrainsetand
allsampledsequencelengths.Asbefore,wenotethateach
statesplitbythemethodisbyconcreteinputsto
thenetwork,andsotheextractionofalargeDFAisasign
oftheinherentcomplexityofthelearnednetworkbehavior.
B.4.TrainSetDetails
Thesizes,samplelengths,andpositivetonegativeratiosof
thesamplesinthetrainsetsarelistedhere(Table10)for
eachofthenetworksusedinourmainexperiments,aswell
asfortheJSONandCountinglanguages.Notethatforsome
languages(suchastheTomitagrammar,
1

),thereare
veryfewpositive/negativesamples.Fortheselanguages,
thetestsetsarelessbalancedbetweenpositiveandnegative
samples.
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
(a)
(b)
Figure4.
TwoDFAsresembling,butnotperfectly,thecorrectDFAfortheregularlanguageoftokenizedJSONlists,
(
n
[
n
])
j
(
n
[[S0NTF](,[S0NTF])*
n
])$
.DFA4aisalmostcorrect,butacceptsalsolist-likesequencesinwhichthelastitemismiss-
ing,i.e.,thereisacommafollowedbyaclosingbracket.DFA4bisreturnedbyL

aftertheteacher(network)rejects4a,butisalsonota
correctrepresentationofthetargetlanguageŠtreatingthesequence
[,
asalegitimatelistitemequivalenttothecharacters
S
;
0
;
N
;
T
;
F
.
Table9.
CounterexamplesreturnedtotheequivalencequeriesmadebyL

duringextractionofaDFAfromanetworktrainedto100%accu-
racyonbothtrainandtestsetsontheregularlanguage
(
n
[
n
])
j
(
n
[[S0NTF](,[S0NTF])*
n
])$
overthe8-letteralphabet
f
[
;
]
;
S
;
0
;
N
;
T
;
F
;
,
g
.
Counterexampleshighlightingthediscrepanciesbetweenthenetworkbehaviourandthetargetbehaviourareshowninbold.
CounterexampleGenerationfortheNon-NestedTokenizedJSON-listsLanguage
Counterexample
GenerationTime(seconds)
Network
Target
[]
provided
True
True
[SS]
3.49
False
False
[[,]
7.12
True
False
[S,,
8.61
True
False
[0,F
8.38
True
False
[N,0,
8.07
False
False
[S,N,0,
9.43
True
False
[T,S,
9.56
False
False
[S,S,T,[]
15.15
False
False
[F,T,[
3.23
False
False
[N,F,S,0
10.04
True
False
[S,N,[,,,,
27.79
True
False
[T,0,T,
28.06
True
False
[S,T,0,],
26.63
True
False
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
Table10.
Trainsetstatisticsfornetworksusedinthiswork.Therandomregularlanguagenetworksusedinthemainworkwerebasedon
minimalDFAsofsize10overalphabetsofsize3,with3languagesperhiddenstatesize.Weliststatisticsfortheirtrainsetsingrouped
bythehiddensize.Thetrainsetsizesandlengthswerethesameforeachoftheserandomlanguages,butthenumberofpositive/negative
samplesfoundeachtimevariedslightly.
TrainSetStats
Language
Architecture
HiddenSize
TrainSetSize
OfWhichPositiveSamples
LengthsinTrainSet
Tomita1
GRU
100
613
14
0-13,16,19,22
Tomita2
GRU
100
613
8
0-13,16,19,22
Tomita3
GRU
100
2911
1418
0-13,16,19,22
Tomita4
GRU
100
2911
1525
0-13,16,19,22
Tomita5
GRU
100
1833
771
0-13,16,19,22
Tomita6
GRU
100
3511
1671
0-13,15-20
Tomita7
GRU
100
2583
1176
0-13,16,19,22
Random1-3
GRU
50
16092
8038,7768,8050
1-15,16,18,...,26
Random4-6
GRU
100
16092
7783,7842,8167
1-15,16,18,...,26
Random7-9
GRU
500
16092
8080,8143,7943
1-15,16,18,...,26
BalancedParentheses
GRU
50
44697
25243
0-73
BalancedParentheses
LSTM
50
44816
28781
0-81
emails
LSTM
100
40000
20000
0-34
JSONLists
GRU
100
20000
10000
0-74
Counting
LSTM
100
20000
10000
0-43
Wereiteratethatallnetworksusedinthisworkweretrained
to
100%
trainsetaccuracyandreachedatleast
99
:
9%
ona
setof1000samplesfromeachofthelengths
4
;
7
;
10
;:::;
28
.
AnexplicitdescriptionoftheTomitagrammarscanbefound
in(Tomita,1982).
Acknowledgments
Theresearchleadingtotheresultspresentedinthispaperis
supportedbytheEuropeanUnion'sSeventhFrameworkPro-
gramme(FP7)undergrantagreementno.615688(PRIME),
TheIsraeliScienceFoundation(grantnumber1555/15),The
AllenInstituteforIntelligence,andTheIntelCol-
laborativeResearchInstituteforComputationalIntelligence
(ICRI-CI)
References
Alur,R.,

Cern
´
y,P.,Madhusudan,P.,andNam,W.Synthesis
ofinterfaceforjavaclasses.In
Proceed-
ingsofthe32NdACMSIGPLAN-SIGACTSymposiumon
PrinciplesofProgrammingLanguages
,POPL'05,pp.98Œ
109,NewYork,NY,USA,2005.ACM.ISBN1-58113-
830-X.doi:10.1145/1040305.1040314.URL
http:
//doi.acm.org/10.1145/1040305.1040314
.
Angluin,D.Learningregularsetsfromqueriesandcoun-
terexamples.
Inf.Comput.
,75(2):87Œ106,1987.doi:
10.1016/0890-5401(87)90052-6.URL
https://doi.
org/10.1016/0890-5401(87)90052-6
.
Berg,T.,Jonsson,B.,Leucker,M.,andSaksena,M.
Insightstoangluin'slearning.
Electr.NotesTheor.
Comput.Sci.
,118:3Œ18,2005.doi:10.1016/j.entcs.
2004.12.015.URL
https://doi.org/10.1016/
j.entcs.2004.12.015
.
Boser,B.E.,Guyon,I.M.,andVapnik,V.N.Atraining
algorithmforoptimalmarginIn
Proceedings
oftheFifthAnnualWorkshoponComputationalLearning
Theory
,COLT'92,pp.144Œ152,NewYork,NY,USA,
1992.ACM.ISBN0-89791-497-X.doi:10.1145/130385.
130401.URL
http://doi.acm.org/10.1145/
130385.130401
.
Cechin,A.L.,Simon,D.R.P.,andStertz,K.Stateau-
tomataextractionfromrecurrentneuralnetsusingk-
meansandfuzzyclustering.In
ProceedingsoftheXXIII
InternationalConferenceoftheChileanComputerSci-
enceSociety
,SCCC'03,pp.73Œ78,Washington,DC,
USA,2003.IEEEComputerSociety.ISBN0-7695-2008-
1.URL
http://dl.acm.org/citation.cfm?
id=950790.951318
.
Cho,K.,vanMerrienboer,B.,Bahdanau,D.,andBengio,Y.
Onthepropertiesofneuralmachinetranslation:Encoder-
decoderapproaches.
CoRR
,abs/1409.1259,2014.URL
http://arxiv.org/abs/1409.1259
.
Chung,J.,G
¨
ul
c¸
ehre,
C¸
.,Cho,K.,andBengio,Y.Empirical
evaluationofgatedrecurrentneuralnetworksonsequence
modeling.
CoRR
,abs/1412.3555,2014.URL
http:
//arxiv.org/abs/1412.3555
.
ExtractingAutomatafromRecurrentNeuralNetworksUsingQueriesandCounterexamples
Elman,J.L.Findingstructureintime.
CognitiveScience
,
14(2):179Œ211,1990.ISSN1551-6709.doi:10.1207/
s15516709cog1402
1.URL
http://dx.doi.org/
10.1207/s15516709cog1402_1
.
Giles,C.L.,Sun,G.-Z.,Chen,H.-H.,Lee,Y.-C.,andChen,
D.Higherorderrecurrentnetworksandgrammatical
inference.InTouretzky,D.S.(ed.),
AdvancesinNeural
InformationProcessingSystems2
,pp.380Œ387.Morgan-
Kaufmann,1990.
Goldman,S.A.andKearns,M.J.Onthecomplexityof
teaching.
J.Comput.Syst.Sci.
,50(1):20Œ31,1995.doi:
10.1006/jcss.1995.1003.URL
https://doi.org/
10.1006/jcss.1995.1003
.
Gorman,K.andSproat,R.Minimallysupervisednum-
bernormalization.
TransactionsoftheAssociationfor
ComputationalLinguistics
,4:507Œ519,2016.ISSN2307-
387X.URL
https://www.transacl.org/ojs/
index.php/tacl/article/view/897
.
Goudreau,M.W.,Giles,C.L.,Chakradhar,S.T.,andChen,
D.First-orderversussecond-ordersingle-layerrecurrent
neuralnetworks.
IEEETrans.NeuralNetworks
,5(3):
511Œ513,1994.doi:10.1109/72.286928.URL
https:
//doi.org/10.1109/72.286928
.
Hochreiter,S.andSchmidhuber,J.Longshort-termmemory.
NeuralComputation
,9(8):1735Œ1780,1997.doi:10.
1162/neco.1997.9.8.1735.URL
https://doi.org/
10.1162/neco.1997.9.8.1735
.
Jacobsson,H.Ruleextractionfromrecurrentneuralnet-
works:Ataxonomyandreview.
NeuralComput.
,17
(6):1223Œ1263,June2005.ISSN0899-7667.doi:
10.1162/0899766053630350.URL
http://dx.doi.
org/10.1162/0899766053630350
.
Omlin,C.W.andGiles,C.L.Extractionofrules
fromdiscrete-timerecurrentneuralnetworks.
Neu-
ralNetworks
,9(1):41Œ52,1996.doi:10.1016/
0893-6080(95)00086-0.URL
https://doi.org/
10.1016/0893-6080(95)00086-0
.
OrorbiaII,A.G.,Mikolov,T.,andReitter,D.Learning
simplerlanguagemodelswiththedeltarecurrentneural
networkframework.
CoRR
,abs/1703.08864,2017.URL
http://arxiv.org/abs/1703.08864
.
Paszke,A.,Gross,S.,Chintala,S.,Chanan,G.,Yang,E.,
DeVito,Z.,Lin,Z.,Desmaison,A.,Antiga,L.,andLerer,
A.Automaticdifferentiationinpytorch.In
NIPS-W
,
2017.
Pedregosa,F.,Varoquaux,G.,Gramfort,A.,Michel,V.,
Thirion,B.,Grisel,O.,Blondel,M.,Prettenhofer,P.,
Weiss,R.,Dubourg,V.,Vanderplas,J.,Passos,A.,Cour-
napeau,D.,Brucher,M.,Perrot,M.,andDuchesnay,E.
Scikit-learn:MachinelearninginPython.
Journalof
MachineLearningResearch
,12:2825Œ2830,2011.
Tomita,M.Dynamicconstructionofautomatafrom
examplesusinghill-climbing.In
Proceedingsofthe
FourthAnnualConferenceoftheCognitiveScienceSoci-
ety
,pp.105Œ108,AnnArbor,Michigan,1982.
Wang,Q.,Zhang,K.,OrorbiaII,A.G.,Xing,X.,Liu,X.,
andGiles,C.L.Anempiricalevaluationofrecurrentneu-
ralnetworkruleextraction.
CoRR
,abs/1709.10380,2017.
URL
http://arxiv.org/abs/1709.10380
.
Wang,Q.,Zhang,K.,OrorbiaII,A.G.,Xing,X.,Liu,X.,
andGiles,C.L.Acomparisonofruleextractionfor
differentrecurrentneuralnetworkmodelsandgrammat-
icalcomplexity.
CoRR
,abs/1801.05420,2018.URL
http://arxiv.org/abs/1801.05420
.
Zeng,Z.,Goodman,R.M.,andSmyth,P.Learning
nitestatemachineswithself-clusteringrecurrentnet-
works.
NeuralComputation
,5(6):976Œ990,1993.doi:10.
1162/neco.1993.5.6.976.URL
https://doi.org/
10.1162/neco.1993.5.6.976
.
"
6,Towards multi-instrument drum transcription,http://arxiv.org/pdf/1806.06676v2.pdf,https://github.com/keunwoochoi/DrummerNet,"Proceedingsofthe21
st
InternationalConferenceonDigitalAudioEffects(DAFx-18),Aveiro,Portugal,September4Œ8,2018
TOWARDSMULTI-INSTRUMENTDRUMTRANSCRIPTION
RichardVogl
1,2
GerhardWidmer
2
PeterKnees
1
richard.vogl@tuwien.ac.atgerhard.widmer@jku.atpeter.knees@tuwien.ac.at
1
FacultyofInformatics
TUWien
Vienna,Austria
2
InstituteofComputationalPerception
JohannesKeplerUniversity
Linz,Austria
ABSTRACT
Automaticdrumtranscription,asubtaskofthemoregeneralauto-
maticmusictranscription,dealswithextractingdruminstrument
noteonsetsfromanaudiosource.Recently,progressintranscrip-
tionperformancehasbeenmadeusingnon-negativematrixfac-
torizationaswellasdeeplearningmethods.However,theseworks
primarilyfocusontranscribingthreedruminstrumentsonly:snare
drum,bassdrum,andhi-hat.Yet,formanyapplications,theabil-
itytotranscribemoredruminstrumentswhichmakeupstandard
drumkitsusedinwesternpopularmusicwouldbedesirable.In
thiswork,convolutionalandconvolutionalrecurrentneuralnet-
worksaretrainedtotranscribeawiderrangeofdruminstruments.
First,theshortcomingsofpubliclyavailabledatasetsinthiscon-
textarediscussed.Toovercometheselimitations,alargersyn-
theticdatasetisintroduced.Then,methodstotrainmodelsusing
thenewdatasetfocusingongeneralizationtorealworlddataare
investigated.Finally,thetrainedmodelsareevaluatedonpublicly
availabledatasetsandresultsarediscussed.Thecontributionsof
thisworkcomprise:
(i.)
alarge-scalesyntheticdatasetfordrum
transcription,
(ii.)
stepstowardsanautomaticdrumtranscrip-
tionsystemthatsupportsalargerrangeofinstrumentsbyeval-
uatinganddiscussingtrainingsetupsandtheimpactofdatasets
inthiscontext,and
(iii.)
apubliclyavailablesetoftrainedmod-
elsfordrumtranscription.Additionalmaterialsareavailableat
http://ifs.tuwien.ac.at/~vogl/dafx2018
.
1.INTRODUCTION
Automaticdrumtranscription(ADT)focusesonextractingasym-
bolicnotationfortheonsetsofdruminstrumentsfromanaudio
source.Asasubtaskofautomaticmusictranscription,ADThas
awidevarietyofapplications,bothinanacademicaswellasin
acommercialcontext.Whilestate-of-the-artapproachesachieve
reasonableperformanceonpubliclyavailabledatasets,thereare
stillseveralopenproblemsforthistask.Inpriorwork[1]weiden-
tifyadditionalinformationŠsuchasbarboundaries,localtempo,
ordynamicsŠrequiredforacompletetranscriptandproposea
systemtrainedtodetectbeatsalongsidedrums.Whilethisadds
someofthemissinginformation,furtherworkinthisdirectionis
stillrequired.
Anothermajorshortcomingofcurrentapproachesisthelim-
itationtoonlythreedruminstruments.Thefocusonsnaredrum
(SD),bassdrum(BD),andhi-hat(HH)ismotivatedbythefacts
thatthesearetheinstruments
(i.)
mostcommonlyusedandthus
withthehighestnumberofonsetsinthepubliclyavailabledatasets;
and
(ii.)
whichoftenthemainrhythmicaltheme.Neverthe-
less,formanyapplicationsitisdesirabletobeabletotranscribea
widervarietyofthedruminstrumentswhicharepartofastandard
drumkitinwesternpopularmusic,e.g.,forextractingfulltran-
scriptsforfurtherprocessinginmusicproductionoreducational
scenarios.Oneofthemainissueswithbuildingandevaluating
suchasystemistherelativeunderrepresentationoftheseclasses
inavailabledatasets(seesection2).
Inthisworkwefocusonincreasingthenumberofinstru-
mentstobetranscribed.Moreprecisely,insteadofthreeinstru-
mentclasses,weaimattranscribingdrumsatalevelofgranu-
larityaswellasadditionaltypesofdrums,leadingto
schemasconsistingofeightand18differentinstruments(seeta-
ble1).Inordertomaketrainingforalargenumberofinstruments
feasible,weoptforasinglemodeltosimultaneouslytranscribeall
instrumentsofinterest,basedonconvolutionalandconvolutional
recurrentneuralnetworks.Especiallyinthecaseofdeeplearn-
ing,aconsiderableamountofprocessingpowerisneededtotrain
themodels.Althoughotherapproachestrainseparatemodelsfor
eachinstrumentinthethree-instrument-scenario[2,3],for18in-
strumentsitismorefeasibletotrainasinglemodelinamulti-task
fashion(cf.[4]).Toaccountfortheneedoflargevolumesofdata
inordertotrainthechosennetworkarchitectures,alargesynthetic
datasetisintroduced,consistingof4197tracksandanoveralldu-
rationofabout259h.
Theremainderofthispaperisorganizedasfollows.Insec-
tion2wediscussrelatedwork,followedbyadescriptionofour
proposedmethodinsection3.Section4providesareviewofex-
istingdatasetsusedforevaluation,aswellasadescriptionofthe
new,largesyntheticdataset.Sections5and6describethecon-
ductedexperimentsanddiscusstheresults,respectively.Finally,
wedrawconclusionsinsection7.
2.RELATEDWORK
TherehasbeenaconsiderableamountofworkpublishedonADT
inrecentyears,e.g.,[5,6,7,8,9].Inthepast,differentcombi-
nationsofsignalprocessingandinformationretrievaltechniques
havenbeenappliedtoADT.Forexample:onsetdetectionincom-
binationwith
(i.)
bandpass[10,11],and
(ii.)
instrument
[5,6,7];aswellasprobabilisticmodels[8,12].
Anothergroupofmethodsfocusonextractinganonset-pseudo-
probabilityfunction(activationfunction)foreachinstrumentun-
derobservation.Thesemethodsutilizesourceseparationtech-
niqueslikeIndependentSubspaceAnalysis(ISA)[13],PriorSub-
spaceAnalysis(PSA)[14],andNon-NegativeIndependentCom-
ponentAnalysis(NNICA)[15].Morerecently,theseapproaches
havebeenfurtherdevelopedusingNon-NegativeMatrixFactor-
ization(NMF)variantsaswellasdeeplearning[1,3,16,17].
TheworkofWuetal.[18]providesacomprehensiveoverview
ofthepublicationsforthistask,andadditionallyperformsin-depth
evaluationofcurrentstate-of-the-artmethods.Duetothelarge
DAFX-1
arXiv:1806.06676v2  [cs.SD]  3 Oct 2018Proceedingsofthe21
st
InternationalConferenceonDigitalAudioEffects(DAFx-18),Aveiro,Portugal,September4Œ8,2018
Table1:Classesusedinthedifferentdruminstrument
tionsystems.LabelsmaptoGeneralMIDIdruminstruments:e.g.
bassdrum:35,36;sidestick:37;etc.Themappingisavailableon
theaccompanyingwebsite.
numberofclasses
instrumentname
3818
BDBDBDbassdrum
SDSDSDsnaredrum
SSsidestick
CLPhandclap
TT
HThightom
MTmidtom
LTlowtom
HHHH
CHHclosedhi-hat
PHHpedalhi-hat
OHHopenhi-hat
TBtambourine
RDRDridecymbal
BE
RBridebell
CBcowbell
CY
CRCcrashcymbal
SPCsplashcymbal
CHCChinesecymbal
CLCLclave/sticks
numberofworksandgiventhespacelimitations,intheremainder
ofthissection,wewillfocusonworkthatisdirectlyrelevantwith
respecttothecurrentstateoftheartandmethodsfocusingonmore
thanthreedruminstrumentclasses.
Asmentioned,thestateoftheartforthistaskiscurrentlyde-
byend-to-endactivationfunctionbasedmethods.Inthiscon-
text,end-to-endimpliesusingonlyoneprocessingsteptoextract
theactivationfunctionforeachinstrumentunderobservationfrom
adigitalrepresentationoftheaudiosignal(usuallyspectrogram
representations).Activationfunctionscanbeinterpretedasproba-
bilityestimatesforacertaininstrumentonsetateachpointintime.
Toobtainthepositionsofthemostprobableinstrumentonsets,
simplepeakpicking[19,20,1,3,2,16,15]oralanguage-model-
styledecisionprocesslikedynamicBayesiannetworks[21]canbe
used.ThesemethodscanbefurtherdividedintoNMFbasedand
deepneuralnetwork(DNN)basedapproaches.
Wuetal.[16]introducepartiallyedNMF(PFNMF)and
furthertoextractthedruminstrumentonsettimes
fromanaudiosignal.Dittmaretal.[17]useanother
ofNMF,namelysemiadaptiveNMF(SANMF)totranscribedrum
solotracksinrealtime,whilerequiringsamplesoftheindividual
druminstrumentsfortraining.Morerecently,recurrentneuralnet-
works(RNNs)havesuccessfullybeenusedtoextracttheactivation
functionsfordruminstruments[19,20,2].Ithasalsobeenshown
thatconvolutional(CNNs)[1,3]andconvolutionalrecurrentneu-
ralnetworks(CRNNs)[1]havethepotentialtoevensurpassthe
performanceofRNNs.
ThemajorityofworksonADT,especiallythemorerecent
ones,focussolelyontranscribingthreedruminstrument(SD,BD,
HH)[9,19,20,1,2,3,16,8,17,7,8].Insomeworksmulti-
pledruminstrumentsaregroupedintocategoriesfortranscription
[5]andeffortshavebeenmadetoclassifyspecialdrumplaying
techniqueswithininstrumentgroups[22].However,onlylittle
workexistswhichapproachtheproblemoftranscribingmorethan
Figure1:OverviewofimplementedADTsystemusingDNNs.
threeindividualdruminstruments[15],furthermore,suchasys-
temhasŠtoourknowledgeŠneverbeenevaluatedoncurrently
availablepublicdrumtranscriptiondatasets.
In[6],asetofMIDIdrumloopsrenderedwithdifferentdrum
samplesareusedtocreatesyntheticdatainthecontextofADT.
Usingsyntheticdatawasanecessityintheearlyyearsofmusicin-
formationretrieval(MIR),butduetothecontinuouseffortsofcre-
atingdatasets,thishasdeclinedinrecentyears.However,machine
learningmethodslikedeeplearning,oftenrequirerlargeamounts
ofdata,andmanualannotationinlargevolumesisunfeasiblefor
manyMIRtasks.Inotherlikespeechrecognitionorim-
ageprocessing,creatingannotationsiseasier,andlargeamounts
ofdataarecommonlyavailable.Usingdataaugmentationcan,to
acertaindegree,beusedtoovercomelackofdata,ashasbeen
demonstratedinthecontextofADT[20].In[23]anapproachto
resynthesizessolotracksusingautomaticallyannotatedf0trajec-
tories,tocreateperfectannotations,isintroduced.Thisapproach
couldbeapplicableforADT,onceasatisfactorymodelforthefull
rangeofdruminstrumentsisavailable.Atthemomentsuchanno-
tationswouldbelimitedtothethreedruminstrumentclassesused
instate-of-the-artmethods.
3.METHOD
Inthiswork,weuseanapproachsimilartotheonesintroducedin
[2]and[19],fordrumtranscription.Asmentionedintheintroduc-
tion,asinglemodeltrainedinamulti-taskfashionwillbeused.
Creatingindividualmodelsforeachinstrumentisanoption[2,3],
however,inthecontextofthisworkithastwodownsides:First,
trainingtimewillscalelinearlywiththeamountofmodels,which
isproblematicwhenincreasingthenumberofinstrumentsunder
observation.Second,trainingmulti-taskmodelsinthecontext
ofADTcanimprovetheperformance[1].Otherstate-of-the-art
methodsbasedonNMF[16,17]arelesssuitableforamulti-task
approach,sincetheperformanceofNMFmethodsispronetode-
gradeforbasismatriceswithhigherrank.
Thus,themethodproposedin[1]seemsmostpromisingfor
thegoalofthiswork.WewillonlyuseCNNsandCRNNs,since
simpleRNNsdonothaveanyadvantageinthiscontext.Theim-
plementedADTsystemconsistsofthreestages:asignalprepro-
cessingstage,aDNNactivationfunctionextractionstage,anda
peakpickingpostprocessingstage,identifyingthenoteonset.The
systemoverviewisvisualizedin1,andthesinglestageswill
bediscussedindetailinthefollowingsubsections.
3.1.Preprocessing
Duringsignalpreprocessing,alogarithmicmagnitudespectrogram
iscalculatedusingawindowsizeof2048samples(@44.1kHzin-
putaudioframerate)andchoosing441samplesashopsizefora
DAFX-2
Proceedingsofthe21
st
InternationalConferenceonDigitalAudioEffects(DAFx-18),Aveiro,Portugal,September4Œ8,2018
Figure2:ArchitecturecomparisonbetweentheCNNandCRNN
usedforactivationfunctionextraction.
100Hztargetframerateofthespectrogram.Thefrequencybins
aretransformedtoalogarithmicscaleusingtriangularin
arangefrom20to20,000Hz,using12frequencybinsperoc-
tave.Finally,thepositive-differentialovertimeofthis
spectrogramiscalculatedandstackedontopoftheoriginalspec-
trogram.Theresultingfeaturevectorshavealengthof168values
(2x84frequencybins).
3.2.ActivationFunctionExtraction
Theactivationfunctionextractionstageisrealizedusingoneof
twodifferentDNNsarchitectures.Figure2visualizesandcom-
paresthetwoimplementedarchitectures.Theconvolutionalparts
areequivalentforbotharchitectures,however,thedenseoutput
layersaredifferent:whilefortheCNNtwonormaldenselayers
areused(ReLUs),incaseoftheCRNNtwobidirectionalRNN
layersconsistingofgatedrecurrentunits(GRUs)[24]areused.As
alreadynotedin[1],GRUsexhibitsimilarcapabilitiesasLSTMs
[25],whilebeingmoreeasytotrain.
Thecombinationofconvolutionallayerswhichfocusonlocal
spectralfeatures,andrecurrentlayerswhichmodelmid-andlong-
termrelationships,hasbeenfoundtobeoneofthebestperforming
modelsforADT[1].
3.3.PeakPicking
Toidentifythedruminstrumentonsets,astandardpeakpicking
methodintroducedforonsetdetectionin[26]isused.Apeakat
point
n
intheactivationfunction
f
a
(
n
)
mustbethemaximum
valuewithinawindowofsize
m
+1
(i.e.:
f
a
(
n
)=
max
(
f
a
(
n

m
)
;

;f
a
(
n
))
),andexceedingthemeanvalueplusathreshold

withinawindowofsize
a
+1
(i.e.:
f
a
(
n
)

mean
(
f
a
(
n

a
)
;

;f
a
(
n
))+

).Additionally,apeakmusthaveatleastadis-
tanceof
w
+1
tothelastdetectedpeak
n
lp
(i.e.:
n

n
lp
>w;
).
Theparametersforpeakpickingarethesameasusedin[1]:
m
=
a
=
w
=2
.Thebestthresholdforpeakpickingisdeterminedon
thevalidationset.Asobservedin[3,20,1],appropriatelytrained
DNNsproducespikyactivationfunctions,therefore,lowthresh-
olds(
0
:
1

0
:
2
)givebestresults.
3.4.TrainingandEvaluation
Trainingofthemodelsisperformedusing
Adam
optimization[27]
withmini-batchesofsize100and8fortheCNNsandCRNNsre-
spectively.ThetraininginstancesfortheCNNhaveaspectralcon-
textof25samples.IncaseoftheCRNN,thetrainingsequences
consistof400instanceswithaspectralcontextof13samples.The
DNNsaretrainedusingaedlearningrate(
l
r
=0
:
001
)with
KD
SD
HH
0.0
0.2
0.4
0.6
BD
SD
TT
HH
CY
RD
CB
CL
0.0
0.2
0.4
relativefrequency
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
instrumentclasses
0.0
0.1
0.2
0.3
0.4
ENST
MDB
RBMA13
MIDI
MIDIbal.
Figure3:Labeldistributionsofthedifferentdatasetsusedinthis
work.
additionalifnoimprovementonthevalidationsetis
achievedfor10epochs.Duringthelearningrateisre-
duced(
l
r
=
l
r

0
:
2
)andtrainingcontinuesusingtheparameters
ofthebestperformingmodelsofar.
Athree-foldcross-validationstrategyisemployed,usingtwo
splitsduringtraining,while15%ofthetrainingdataisseparated
andusedforvalidationaftereachepoch(0.5%incaseofthelarge
datasets,toreducevalidationtime).Testingisdoneonthethird,
duringtrainingunseen,split.Wheneveravailable,drumsolover-
sionsofthetracksareusedasadditionaltrainingmaterial,but
notfortesting/evaluation.Thesoloversionsarealwaysputinto
thesamesplitsastheirmixedcounterparts,tocounterov
Thissetupisconsistentlyusedthroughallexperiments,when-
everdatasetsaremixedorcross-validated,correspondingsplitsare
used.
Foraudiopreprocessing,peakpicking,andcalculationofeval-
uationmetrics,themadmom
1
pythonframeworkwasused.DNN
trainingwasperformedusingTheano
2
andLasagne
3
.Foramore
detailsonC(R)NNtrainingandacomparisonoftheirworking
principlesinthecontextofADT,wekindlyreferthereadertoour
previouswork[1]duetospacelimitationsandadifferentfocusof
thiswork.
4.DATASETS
ThereareanumberofpubliclyavailabledatasetsforADTwith
varyingsize,degreeofdetail,andnumberofclassesregardingthe
druminstrumentannotations.Asnotedintheintroduction,current
state-of-the-artapproacheslimittheinstrumentsunderobservation
tothethreemostcommonones(SD,BD,HH).Thisisdoneby
ignoringotherinstrumentsliketom-tomsandcymbals,aswellas
1
https://github.com/CPJKU/madmom
2
https://github.com/Theano/Theano
3
https://github.com/Lasagne/Lasagne
DAFX-3
Proceedingsofthe21
st
InternationalConferenceonDigitalAudioEffects(DAFx-18),Aveiro,Portugal,September4Œ8,2018
Table2:F-measure(
mean
/
sum
)resultsofimplementedADT
methodsonpublicdatasetsfordifferentclasssystems.The
lineindicatesstate-of-the-artF-measureresultsinpreviouswork
usingCNNandCRNNADTsystemsinathree-classscenario.
CLmodel
ENSTMDBRBMA13
3SotA
[1]
Š/0.78Š/ŠŠ/0.67
3
CNN0.75/0.770.65/0.720.53/0.63
CRNN0.74/0.760.64/0.700.55/0.64
8
CNN0.59/0.630.68/0.650.55/0.44
CRNN0.65/0.700.68/0.630.55/0.50
18
CNN0.69/0.490.76/0.470.62/0.31
CRNN0.75/0.670.77/0.550.64/0.39
groupingdifferentplaystyleslikeclosed,opened,andpedalhi-
hatstrokes.Inordertoinvestigatewaysofgeneratingamodel
whichiscapabletotranscribemorethanthesethreeinstruments,
twosystems,i.e.,amediumandalargeone,fordrum
instrumentsofastandarddrumkitareTable1showsthe
twosetsofclasses,whichcontaineightand18labelsrespectively,
alongsidewiththeclassicthree-classsetusedinstate-of-the-art
worksandthemappingusedbetweentheseclasses.
InthefollowingwediscusspubliclyavailableADTdatasets
andtheirlimitations,leadingtothedescriptionofthelargevolume
syntheticdatasetintroducedfortrainingofourmodels.
4.1.ENSTDrums(
ENST
)
TheENSTDrums
4
datasetpublishedbyGilletandRichard[28]
in2005,iscommonlyusedinADTevaluations.Thefreelyavail-
ablepartofthedatasetconsistsofsingletrackaudiorecordings
andmixes,performedbythreedrummersondifferentdrumkits.
Itcontainsrecordingsofsinglestrokesforeachinstrument,short
sequencesofdrumpatterns,aswellasdrumtrackswithadditional
accompaniment(
minus-one
tracks).Theannotationscontainla-
belsfor20differentinstrumentclasses.
Forevaluation,the
wetmixes
(containstandardpost-processing
likecompressionandequalizing)ofthe
minus-onetracks
were
used.Theymakeup64tracksof61saveragedurationandatotal
durationof1h.Therestofthedataset(singlestrokes,patterns)
wasusedasadditionaltrainingdata.
4.2.MDB-Drums(
MDB
)
TheMDB-Drumsdataset
5
waspublishedin[29]andprovidesdrum
annotationsfor23tracksoftheMedleyDBdataset
6
[30].The
tracksareavailableasdrumsolotrackswithadditionalaccompa-
niment.Again,onlythefullmixesareusedforevaluation,while
thedrumsolotracksareusedasadditionaltrainingdata.Thereare
twolevelsofdruminstrumentannotations,thesecondproviding
multipledruminstrumentsandadditionaldrumplayingtechnique
detailsin21classes.Trackshaveanaveragedurationof54sec-
ondsandthetotaldurationis20m42s.
4
http://perso.telecom-paristech.fr/~grichard/ENST-drums/
5
https://github.com/CarlSouthall/MDBDrums
6
http://medleydb.weebly.com/
Table3:F-measureresults(
mean
/
sum
)oftheimplementednet-
worksonsyntheticdatasets.
CLmodel
MIDIMIDI1%MIDIbal.
3
CNN0.74/
0.84
0.70/0.79Š/Š
CRNN0.74/
0.84
0.68/0.77Š/Š
8
CNN0.64/0.630.63/0.690.54/0.58
CRNN0.74/
0.82
0.69/0.730.58/0.70
18
CNN0.66/0.390.65/0.390.59/0.18
CRNN0.73/
0.70
0.69/0.620.63/0.52
4.3.RBMA13(
RBMA13
)
TheRBMA13datasets
7
waspublishedalongside[1].Itconsists
of30tracksofthefreelyavailable2013RedBullMusicAcademy
VariousAssetssampler.
8
Thetracks'genresanddrumsoundsof
thissetaremorediversecomparedtotheprevioussets,making
itaparticularlydifset.Itprovidesannotationsfor23drum
instrumentsaswellasbeatanddownbeats.Tracksinthissethave
anaveragedurationof3m50sandatotalof1h43m.
4.4.Limitationsofcurrentdatasets
AmajorproblemofpubliclyavailableADTdatasetsinthecontext
ofdeeplearningisthevolumeofdata.TobeabletotrainDNNs
ef,usuallylargeamountsofdiversedataareused(e.g.in
speechandimageprocessing).Onewaytocounterthelackofdata
istousedataaugmentation(asdonein[20]forADT).However,
dataaugmentationisonlyhelpfultoacertaindegree,depending
ontheapplicableaugmentationmethodsandthediversityofthe
originaldata.
Giventhenatureofdrumrhythmsfoundinwesternpopular
music,anotherissueofADTdatasetsistheunevendistribution
ofonsetsbetweeninstrumentclasses.Incaseoftheavailable
datasets,thisimbalancecanbeobservedin3.Whileitis
advantageousforthemodeltoadapttothisbias,intermsofover-
allperformance,thisoftenresultsinthetrainedmodelstonever
predictonsetsforsparseclasses.Thisisduetothenumberofpo-
tentialfalsenegativesbeingnegligible,comparedtotheamountof
falsepositivesproducedintheearlystagesoftraining.Tocounter
arelatedeffectonslightlyimbalancedclasses(BD,SD,HHinthe
three-classscenario),aweightingofthelossfunctionsforthedif-
ferentclassescanbehelpful[20].Nevertheless,alossfunction
weightingcannotcompensatefortheprobleminthecaseofvery
sparseclasses.
SincemanualannotationforADTisaveryresourceintensive
task,afeasibleapproachtotackletheseproblemsistocreatea
syntheticdatasetusingthecombinationofsymbolictracks,e.g.
MIDItracks,drumsynthesizersand/orsamplersoftware.
4.5.Syntheticdataset(
MIDI
)
Forgeneratingthesyntheticdataset,asimilarapproachasin[6]
wasemployed.Sincethefocusofthisworkisthetranscription
ofmultipledruminstrumentsfrompolyphonicmusic,fullMIDI
tracksofwesternpopularmusicwereusedinsteadofMIDIdrum
loops.First,everyMIDItrackfromafreelyavailableonlinecol-
lection
9
wassplitintoadrumandaccompanimenttrack.Using
7
http://ifs.tuwien.ac.at/~vogl/datasets/
8
https://rbma.bandcamp.com/album/
9
http://www.midiworld.com
DAFX-4
Proceedingsofthe21
st
InternationalConferenceonDigitalAudioEffects(DAFx-18),Aveiro,Portugal,September4Œ8,2018
SUM
BD
SD
TT
HH
CY
RD
CB
CL
0.0
0.2
0.4
0.6
0.8
1.0
F-measure
SUM
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
instrumentclasses
0.0
0.2
0.4
0.6
0.8
1.0
F-measure
MIDI
MIDIbal.
Figure4:Instrumentclassdetailsforevaluationresultson
MIDI
and
MIDIbal.
for8and18instrumentclassesusingtheCRNN.
Firstvalue(SUM)representstheoverallsumF-measureresults.
timidity++
10
,thedrumtrackswererenderedutilizing57different
drumSoundFonts
11
.TheusedSoundFontswerecollectedfrom
differentonlinesources,andgreatcarewastakentomanually
checkandcorrecttheinstrumentmappingsandoverallsuitabil-
ity.Theycoverawiderangeofdrumsoundsfromelectronicdrum
machines(e.g.TR808),acoustickits,andcommonlyusedcom-
binations.TheSoundFontsweredividedintothreegroupsforthe
threeevaluationsplits,tocounterovtodrumkits.The
accompanimenttrackswererenderedusingafullGeneralMIDI
SoundFont.UsingtheMIDItracks,drumannotationsaswellas
beatanddownbeatannotationsweregenerated.Afterremoving
brokenMIDIveryshort(<30s)aswellasverylong(>15m)
tracks,thesetcontains4197trackswithanaveragedurationof3m
41sandatotaldurationofabout259h.Aswiththeotherdatasets,
weonlyusethemixesforevaluation,whilethedrumsolotracks
areusedasadditionaltrain-onlydata.
Figure3showsthatthegeneraltrendofthedruminstrument
classdistributionissimilartothesmallerdatasets.Thisisnotsur-
prisingsincethemusicisofthesamebroadorigin(westernpop-
ularmusic).Sinceoneofthegoalsofcreatingthisdatasetwas
toachieveamorebalanceddistribution,someadditionalprocess-
ingisnecessary.Duetothefactthatwecaneasilymanipulatethe
sourceMIDIdrumwecanchangeacertainamountofin-
strumentsforseveraltrackstobalancetheclasses.We
didthisforthe18classesaswellasforthe8classesandgen-
eratedtwomoresyntheticdatasetsconsistingofthesametracks,
butwithdruminstrumentschangessothattheclassesarebalanced
withintheirrespectivedruminstrumentclasssystem.Thiswas
doneinawaytoswitchinstrumentswhichhaveasimilarexpected
usagefrequencywithinatrack,whilekeepingmusicalityinmind.
IdealcandidatesforthisareCHHandRD:exchangingthemmakes
sensefromamusicalstandpoint,aswellintermsofusagefre-
quency.Ontheotherhand,BDandCRCarecloseinexpected
usagefrequencybutswitchingthemcanbequestionablefroma
musicalstandpoint,dependingonthemusicgenre.Afulllistof
performedswitchesforthebalancedversionscanbefoundonthe
accompanyingwebpage.
10
http://timidity.sourceforge.net/
11
https://en.wikipedia.org/wiki/SoundFont
Table4:F-measureresults(
mean
/
sum
)fortheCRNNmodelon
publicdatasetswhentrainedondifferentdatasetcombinations.
Thetoppartshowsresultsforthe8classscenario,whilethebot-
tompartshowsresultsforthe18classscenario.Wheneverthe
MIDI
setismixedwithrealworlddatasets,onlythe1%subsetis
used,tokeepabalancebetweendifferentdatatypes.
8instrumentclasses
trainset
ENSTMDBRBMA13
all
0.61/0.640.68/0.640.57/0.52
MIDI
0.65/0.680.70/0.610.57/0.51
MIDIbal.
0.61/0.570.66/0.520.56/0.47
all
+
MIDI
0.58/0.620.67/0.570.57/0.52
all
+
MIDIbal.
0.61/0.640.68/0.560.56/0.51
ptMIDI
0.64/
0.69
0.72/
0.68
0.58/
0.56
ptMIDIbal.
0.61/0.630.72/0.670.58/
0.56
18instrumentclasses
trainset
ENSTMDBRBMA13
all
0.71/0.580.77/0.550.63/0.41
MIDI
0.73/0.610.77/0.530.64/0.39
MIDIbal.
0.70/0.520.76/0.450.63/0.35
all
+
MIDI
0.73/0.620.77/0.540.64/0.41
all
+
MIDIbal.
0.72/0.570.76/0.470.64/0.37
ptMIDI
0.74/
0.67
0.78/
0.60
0.64/
0.47
ptMIDIbal.
0.74/0.650.78/0.580.64/0.45
Adownsideofthisapproachisthattheinstrumentswitches
maycreateialdrumpatternswhichareatypicalforwestern
popularmusic.Thiscanbeproblematiciftherecurrentpartsofthe
usedCRNNarchitecturestarttolearnstructuresoftypicaldrum
patterns.Sincetheseeffectsarediftomeasureandinorder
tobeabletobuildalarge,balanceddataset,thisconsequencewas
consideredacceptable.
5.EXPERIMENTS
ThesetofexperimentsevaluatestheimplementedADTmeth-
odsontheavailablepublicdatasets,usingtheclassicthreedrum
instrumentclasslabels,aswellasthetwonewdrum
schemaswith8and18classes,asabaseline.Asevaluationmea-
sureprimarilytheF-measureoftheindividualdruminstrument
onsetsisused.TocalculatetheoverallF-measureoverallinstru-
mentsandalltracksofadataset,twomethodsareused:First,the
meanoverallinstruments'F-measure(=F-measureoftrack),as
wellasthemeanoveralltracks'F-measureiscalculated(
mean
).
Second,allfalsepositives,falsenegatives,andtruepositivesfor
allinstrumentsandtracksareusedtocalculateaglobalF-measure
(
sum
).Thesetwovaluesgiveinsightintodifferentaspects.While
the
mean
valueismoreconservativeforonlyslightlyimbalanced
classes,itisproblematicwhenappliedtosetscontainingonly
sparselypopulatedclasses.Inthiscase,sometracksmayhave
zerooccurrencesofaninstrument,thusresultinginaF-measure
of1.0whennoinstrumentisdetectedbytheADTsystem.Inthat
case,theoverall
mean
F-measurevalueforthisinstrumentisclose
to1.0ifitonlyoccursinasmallfractionoftracksandthesystem
neverpredictsit.Ontheotherhand,the
sum
valuewillgiveaF-
measureclosetozeroifthesystemneverpredictsaninstrument,
evenforsparseclassesŠwhichismoredesirableinthiscontext.
Thesecondsetofexperimentsevaluatestheperformanceof
theADTmethodsonthesyntheticdatasets,aswellasa1%subset
DAFX-5
Proceedingsofthe21
st
InternationalConferenceonDigitalAudioEffects(DAFx-18),Aveiro,Portugal,September4Œ8,2018
ENST
SUM
BD
SD
TT
HH
CY
RD
CB
CL
0.0
0.2
0.4
0.6
0.8
1.0
F-measure
SUM
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
0.0
0.2
0.4
0.6
0.8
1.0
F-measure
all
MIDI
MIDIbal.
all+MIDI
all+MIDIbal.
ptMIDI
ptMIDIbal.
Figure5:ThisshowsF-measureresultsforeachinstrument,forboththe8class(top)aswellasthe18class(bottom)scenarios,
exemplaryforthe
ENST
dataset.Figuresforothersetsarefoundontheaccompanyingwebpage(seesec.7).Thecolorofbarsindicatesthe
datasetorcombinationstrainedon:
all
Šthreepublicdatasets;
MIDI
Šsyntheticdataset;
MIDIbal.
Šsyntheticsetwithbalancedclasses;
all+MIDI
Šthreepublicdatasetsplus1%splitofsyntheticdataset;
all+MIDIbal.
Šthreepublicdatasetsplusthe1%splitofthebalanced
syntheticdataset;
ptMIDI
and
ptMIDIbal.
Špre-trainedonthe
MIDI
and
MIDIbal.
datasetsrespectivelyandtunedon
all
.The
setofbarsontheleft(SUM)showstheoverall
sum
F-measurevalue.
foreachoftheinstrumentschemas.Thiswillgive
insightinhowthesystemsperformonthesyntheticdatasetand
howrelevantthedatavolumeisforeachoftheschemas.
Inthesetofexperiments,modelstrainedwithdifferent
combinationsofsyntheticandrealdatawillbeevaluated.The
evaluationwillshowhowwellmodelstrainedonsyntheticdata
cangeneralizeonrealworlddata.Mixingtherealworlddatasets
withthesymbolicdataisasimpleapproachofleveraginga
balanceddatasettoimprovedetectionperformanceofunderrep-
resenteddruminstrumentclassesincurrentlyavailabledatasets.
Tobeabletocomparetheresults,modelsaretrainedonallofthe
publicdatasets(
all
),thefullsyntheticdataset(
MIDI
),thebalanced
versionsofthesyntheticdataset(
MIDIbal.
),amixofthepublic
datasetsandthe1%subsetofthesyntheticdataset(
all
+
MIDI
),and
amixofthepublicdatasetsanda1%subsetofthebalancedsyn-
theticdatasets(
all
+
MIDIbal.
).Additionally,modelspre-trained
onthe
MIDI
and
MIDIbal.
datasetswithadditional
onthe
all
datasetwereincluded.Weonlycompareamixofthe
smallerpublicdatasetstotheothersets,sincemodelstrainedon
onlyonesmalldatasethavethetendencytoovandthusgen-
eralizenotwellŠwhichmakescomparisonproblematic.
6.RESULTSANDDISCUSSION
TheresultsofthesetofexperimentsisvisualizedinTable2,
whichshowsthe3-foldcross-validationresultsformodelstrained
onpublicdatasetswith3,8,and18labels.TheresultingF-measure
valuesarenotsurprising:forthe3-classscenariothevaluesare
closetothereportedvaluesintherelatedwork.Differencesare
duetoslightlydifferentmodelsandhyper-parametersettingsfor
training.Asexpected,especiallythe
sum
valuesdropforthecases
of8and18classes.Itcanbeobserved,thattheCRNNperforms
bestforallsetsin18classscenarioandfortwooutofthreesets
fortheeightclassscenario.
Table3showstheresultsformodelstrainedonsyntheticdata-
setswith3,8,and18labels.Asexpected,thereisatendencyfor
themodelstrainedonthe1%subsettoperformworse,especially
fortheCRNN.However,thiseffectisnotassevereassuspected.
Thismightbeduetothefactthat,whiledifferentdrumkitswere
used,thesyntheticsetisstillquiteuniform,givenitssize.The
overallresultsforthebalancedsetsareworsethanforthenormal
set.Thisisexpected,sincethedifofthebalancedsetsis
muchgreaterthanfortheimbalancedone(sparseclassescanbe
ignoredbythemodelswithoutmuchpenalty).Figure4showsa
comparisonofF-measurevaluesforindividualinstrumentsclasses
whentrainingon
MIDI
and
MIDIbal.
sets.Theplotshows,that
performanceforunderrepresentedclassesimprovesforthebal-
ancedset,whichwasthegoalofbalancingtheset.Adownside
isthattheperformanceforclasseswhichhaveahigherfrequency
ofoccurrenceinthe
MIDI
datasetdecreasesinmostcases,which
contributestotheoveralldecrease.However,thiseffectislessse-
vereinthe8classcase.
Ageneraltrendwhichcanbeobserved,especiallyinthesce-
narioswithmoreinstrumentclasslabels,isthatCRNNsconsis-
tentlyoutperformCNNs.Sincethisistrueforallotherexperi-
mentsaswell,andforreasonsofclarity,wewilllimittheresults
forthenextplotsandtablestothoseoftheCRNNmodel.
Table4showstheF-measureresultsfortheCRNNmodel
trainedondifferentdatasetcombinationsandevaluatedonpublic
datasets.In5,adetailedlookinthecontextofcross-datasets
evaluationoninstrumentclassbasisforthe
ENST
datasetispro-
vided.Asmentionedinsection5,resultsformodelstrainedon
onlyonepublicdatasetarenotincludedinthischart.Whilethe
performanceforthoseishigher,theyareslightlyovtothe
individualdatasetsanddonotgeneralizewelltootherdatasets,
thereforeacomparisonwouldnotbemeaningful.Althoughan
overallbigperformanceimprovementforpreviouslyunderrepre-
sentedclassescannotbeobserved,severalinterestingthingsare
visible:
(i.)
boththemodelstrainedsolelyonthe
MIDI
andthe
MIDIbal.
datasetsgeneralizesurprisinglywelltotherealworld
dataset;
(ii.)
insomecases,performanceimprovementsforun-
derrepresentedclassescanbeobserved(e.g.for18classes:LT,
MT,RD,CRC,CHC),whenusingthesyntheticdata;
(iii.)
bal-
DAFX-6
Proceedingsofthe21
st
InternationalConferenceonDigitalAudioEffects(DAFx-18),Aveiro,Portugal,September4Œ8,2018
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
detected(fp)
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
missing(fn)
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
detected(fp)
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
missing(fn)
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
detected(tp)
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
missing(fn)
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
detected(tp)
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
missing(fn)
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
detected(tp)
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
additional(fp)
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
detected(tp)
BD
SD
SS
CLP
LT
MT
HT
CHH
PHH
OHH
TB
RD
RB
CRC
SPC
CHC
CB
CL
additional(fp)
Figure6:Leftcolumnshowsmatricesfor
MIDI
set,rightcol-
umnshowsmatricesfor
MIDIbal.
set,bothforthe18classessce-
nario.Fromtoptobottom,thematricesdisplay:classicconfu-
sions(fn/fp),maskingbytruepositives(fn/tp),andpositivemask-
ing(excitementŠfp/tp).
ancingtheinstruments,whileeffectivewithintheevaluationfor
thesyntheticdataset,seemsnottohaveapositiveeffectinthe
cross-datasetscenarioandwhenmixingdataset;and
(iv.)
using
pre-trainingonthe
MIDI
setwithonthe
all
set,seems
toproducemodelswhicharebettersuitedtodetectunderrepre-
sentedclasseswhilestillperformingwellonotherclasses.
Togainmoreinsightintowhicherrorsthesystemsmakewhen
classifyingwithinthe8and18classsystems,threesetsofpseudo
confusionmatriceswerecreated.Wetermthem
pseudo
confu-
sionmatricesbecauseoneonsetinstancecanhavemultipleclasses,
whichisusuallynotthecaseforproblems.These
threepseudoconfusionmatricesindicatehowoften
(i.)
afalsepos-
itiveforanotherinstrumentwasfoundforfalsenegatives(classic
confusions);
(ii.)
atruepositiveforanotherinstrumentwasfound
forfalsenegatives(onsetmaskedorhidden);and
(iii)
atrueposi-
tiveforanotherinstrumentwasfoundforafalsepositive(positive
maskingorexcitement).Figure6showsexamplesofthesematri-
cesforthe
MIDI
and
MIDIbal.
setsinthe18classscenario.The
imagesleadtointuitiveconclusions:similarsoundinginstruments
maygetconfused(BD/LT,CHH/PHH),instrumentswithenergy
overawidefrequencyrangemaskmoredelicateinstrumentsas
wellassimilarsounds(HT/BD,CLP/SD),andsimilarsounding
instrumentsleadtofalsepositives(LT/MT/HT,RB/RD).Manyof
theseerrorsmayverywellbemadebyhumantranscribersaswell.
Thisalsostrengthenstheassumptionthatinstrumentmappingsare
notwellboundariesofthefrequencyrangebetweenbass
drum,low,midandhightomsarenotwellthedistinc-
tionbetweencertaincymbalsissometimesdifevenforhu-
mans,anddifferenthi-hatsoundsaresometimesonlydistinguish-
ablegivenmorecontext,likegenreorlongtermrelationswithin
thepiece.
Tofurtherimproveperformance,anensembleofmodelstrained
ondifferentdatasets(syntheticandreal,includingbalancedvari-
ants)canbeused.However,experienceshowsthatwhilethese
systemsoftenperformbestinrealworldscenariosandincompeti-
tions(e.g.MIREX),theygivenotsomuchinsightinanevaluation
scenario.
7.CONCLUSION
Inthisworkwediscussedashortcomingofcurrentstate-of-the
artautomaticdrumtranscriptionsystems:thelimitationtothree
druminstruments.Whilethischoicemakessenseinthecontext
ofcurrentlyavailabledatasets,somerealworldapplicationsre-
quiretranscriptionofmoreinstrumentclasses.Toapproachthis
shortcoming,weintroducedanewandpubliclyavailablelarge
scalesyntheticdatasetwithbalancedinstrumentdistributionand
showedthatmodelstrainedonthisdatasetgeneralizewelltoreal
worlddata.Wefurthershowedthatbalancingcanimproveperfor-
manceforusuallyunderrepresentedclassesincertaincases,while
overallperformancemaydecline.Ananalysisofmistakesmade
bysuchsystemswasprovidedandfurtherstepsintothisdirections
werediscussed.Thedataset,trainedmodelsandfurthermaterial
areavailableontheaccompanyingwebpage.
12
8.ACKNOWLEDGEMENTS
ThisworkhasbeenpartlyfundedbytheAustrianFFGunderthe
BRIDGE1project
SmarterJam
(858514),aswellasbytheEuro-
peanResearchCouncil(ERC)undertheEuropeanUnion'sHori-
zon2020researchandinnovationprogramme(ERCGrantAgree-
mentNo.670035,project
CONESPRESSIONE
).
9.REFERENCES
[1]
RichardVogl,MatthiasDorfer,GerhardWidmer,andPeter
Knees,ﬁDrumtranscriptionviajointbeatanddrummodel-
ingusingconvolutionalrecurrentneuralnetworks,ﬂin
Proc.
ofthe18thIntl.Soc.forMusicInformationRetrievalConf.
,
Suzhou,China,Oct.2017.
[2]
CarlSouthall,RyanStables,andJasonHockman,ﬁAuto-
maticdrumtranscriptionusingbidirectionalrecurrentneural
networks,ﬂin
Proc.ofthe17thIntl.Soc.forMusicInforma-
tionRetrievalConf.
,NewYork,NY,USA,Aug.2016.
[3]
CarlSouthall,RyanStables,andJasonHockman,ﬁAu-
tomaticdrumtranscriptionforpolyphonicrecordingsus-
ingsoftattentionmechanismsandconvolutionalneuralnet-
12
http://ifs.tuwien.ac.at/~vogl/dafx2018
DAFX-7
Proceedingsofthe21
st
InternationalConferenceonDigitalAudioEffects(DAFx-18),Aveiro,Portugal,September4Œ8,2018
works,ﬂin
Proc.ofthe18thIntl.Soc.forMusicInformation
RetrievalConf.
,Suzhou,China,Oct.2017.
[4]
RichCaruana,ﬁMultitasklearning,ﬂin
Learningtolearn
,
pp.95Œ133.Springer,1998.
[5]
OlivierGilletandGaëlRichard,ﬁAutomatictranscriptionof
drumloops,ﬂin
Proc.ofthe29thIEEEIntl.Conf.onAcous-
tics,Speech,andSignalProcessing
,Montreal,QC,Canada,
May2004.
[6]
MariusMiron,MatthewEPDavies,andFabienGouyon,ﬁAn
open-sourcedrumtranscriptionsystemforpuredataandmax
msp,ﬂin
Proc.ofthe38thIEEEIntl.Conf.onAcoustics,
SpeechandSignalProcessing
,Vancouver,BC,Canada,May
2013.
[7]
KazuyoshiYoshii,MasatakaGoto,andHiroshiGOkuno,
ﬁDrumsoundrecognitionforpolyphonicaudiosignalsby
adaptationandmatchingofspectrogramtemplateswithhar-
monicstructuresuppression,ﬂ
IEEETrans.onAudio,Speech,
andLanguageProcessing
,vol.15,no.1,pp.333Œ345,2007.
[8]
JouniPaulusandAnssiKlapuri,ﬁDrumsounddetectionin
polyphonicmusicwithhiddenmarkovmodels,ﬂ
EURASIP
JournalonAudio,Speech,andMusicProcessing
,2009.
[9]
Chih-WeiWuandAlexanderLerch,ﬁAutomaticdrumtran-
scriptionusingthestudent-teacherlearningparadigmwith
unlabeledmusicdata,ﬂin
Proc.ofthe18thIntl.Soc.forMu-
sicInformationRetrievalConf.
,Suzhou,China,Oct.2017.
[10]
GeorgeTzanetakis,AjayKapur,andRichardIMcWalter,
ﬁSubband-baseddrumtranscriptionforaudiosignals,ﬂin
Proc.ofthe7thIEEEWorkshoponMultimediaSignalPro-
cessing
,Shanghai,China,Oct.2005.
[11]
MaximosA.Kaliakatsos-Papakostas,AndreasFloros,
MichaelN.Vrahatis,andNikolaosKanellopoulos,ﬁReal-
timedrumstranscriptionwithcharacteristicbandpass-
ing,ﬂin
Proc.AudioMostly:AConf.onInteractionwith
Sound
,Corfu,Greece,2012.
[12]
OlivierGilletandGaëlRichard,ﬁSupervisedandunsuper-
visedsequencemodellingfordrumtranscription,ﬂin
Proc.
ofthe8thIntl.Conf.onMusicInformationRetrieval
,Vienna,
Austria,Sept.2007.
[13]
DerryFitzGerald,BobLawlor,andEugeneCoyle,ﬁSub-
bandindependentsubspaceanalysisfordrumtranscription,ﬂ
in
Proc.Intl.Conf.onDigitalAudioEffects
,Hamburg,Ger-
many,2002.
[14]
AndrioSpich,MassimilianoZanoni,AugustoSarti,andSte-
fanoTubaro,ﬁDrummusictranscriptionusingpriorsub-
spaceanalysisandpatternrecognition,ﬂin
Proc.Intl.Conf.
onDigitalAudioEffects
,Graz,Austria,2010.
[15]
ChristianDittmarandChristianUhle,ﬁFurtherstepstowards
drumtranscriptionofpolyphonicmusic,ﬂin
Proc.ofthe
116thAudioEngineeringSoc.Conv.
,Berlin,Germany,May
2004.
[16]
Chih-WeiWuandAlexanderLerch,ﬁDrumtranscription
usingpartiallyednon-negativematrixfactorizationwith
templateadaptation,ﬂin
Proc.ofthe16thIntl.Soc.forMusic
InformationRetrievalConf.
,Málaga,Spain,Oct.2015.
[17]
ChristianDittmarandDanielGärtner,ﬁReal-timetranscrip-
tionandseparationofdrumrecordingsbasedonnmfdecom-
position,ﬂin
Proc.ofthe17thIntl.Conf.onDigitalAudio
Effects
,Erlangen,Germany,Sept.2014.
[18]
Chih-WeiWu,ChristianDittmar,CarlSouthall,Richard
Vogl,GerhardWidmer,JasonHockman,MeinhardMüller,
andAlexanderLerch,ﬁAnoverviewofautomaticdrumtran-
scription,ﬂ
IEEETrans.onAudio,Speech,andLanguage
Processing
,vol.26,no.9,pp.1457Œ1483,2018.
[19]
RichardVogl,MatthiasDorfer,andPeterKnees,ﬁRecurrent
neuralnetworksfordrumtranscription,ﬂin
Proc.ofthe17th
Intl.Soc.forMusicInformationRetrievalConf.
,NewYork,
NY,USA,Aug.2016.
[20]
RichardVogl,MatthiasDorfer,andPeterKnees,ﬁDrumtran-
scriptionfrompolyphonicmusicwithrecurrentneuralnet-
works,ﬂin
Proc.ofthe42ndIEEEIntl.Conf.onAcoustics,
SpeechandSignalProcessing
,NewOrleans,LA,USA,Mar.
2017.
[21]
SebastianBöck,FlorianKrebs,andGerhardWidmer,ﬁJoint
beatanddownbeattrackingwithrecurrentneuralnetworks,ﬂ
in
Proc.ofthe17thIntl.Soc.forMusicInformationRetrieval
Conf.
,NewYork,NY,USA,2016.
[22]
Chih-WeiWuandAlexanderLerch,ﬁOndrumplayingtech-
niquedetectioninpolyphonicmixtures,ﬂin
Proc.ofthe17th
Intl.Soc.forMusicInformationRetrievalConf.
,NewYork
City,UnitedStates,August2016.
[23]
JustinSalamon,RachelMBittner,JordiBonada,Juan
JoséBoschVicente,EmiliaGómezGutiérrez,and
JuanPabloBello,ﬁAnanalysis/synthesisframeworkforau-
tomaticf0annotationofmultitrackdatasets,ﬂin
Proc.of
the18thIntl.Soc.forMusicInformationRetrievalConf.
,
Suzhou,China,Oct.2017.
[24]
KyunghyunCho,BartvanMerriënboer,DzmitryBahdanau,
andYoshuaBengio,ﬁLearningphraserepresentationsusing
rnnencoderâ

A¸Sdecoderforstatisticalmachinetranslation,ﬂ
in
Proc.oftheConf.onEmpiricalMethodsinNaturalLan-
guageProcessing
,Doha,Qatar,Oct.2014.
[25]
SeppHochreiterandJürgenSchmidhuber,ﬁLongshort-term
memory,ﬂ
NeuralComputation
,vol.9,no.8,pp.1735Œ1780,
Nov.1997.
[26]
SebastianBöckandGerhardWidmer,ﬁMaximumvi-
bratosuppressionforonsetdetection,ﬂin
Proc16thIntlConf
onDigitalAudioEffects
,Maynooth,Ireland,Sept.2013.
[27]
DiederikP.KingmaandJimmyBa,ﬁAdam:Amethodfor
stochasticoptimization,ﬂ
arXivpreprintarXiv:1412.6980
,
2014.
[28]
OlivierGilletandGaëlRichard,ﬁENST-drums:anexten-
siveaudio-visualdatabasefordrumsignalsprocessing,ﬂin
Proc.ofthe7thIntl.Conf.onMusicInformationRetrieval
,
Victoria,BC,Canada,Oct.2006.
[29]
CarlSouthall,Chih-WeiWu,AlexanderLerch,andJason
Hockman,ﬁMDBdrumsâ

A¸Sanannotatedsubsetofmed-
leydbforautomaticdrumtranscription,ﬂin
LateBreak-
ing/Demos,18thIntl.Soc.forMusicInformationRetrieval
Conf.
,Suzhou,China,Oct.2017.
[30]
RachelMBittner,JustinSalamon,MikeTierney,Matthias
Mauch,ChrisCannam,andJuanPabloBello,ﬁMedleyDB:
Amultitrackdatasetforannotation-intensivemirresearch.,ﬂ
in
Proc.ofthe15thIntl.Soc.forMusicInformationRetrieval
Conf.
,Taipei,Taiwan,Oct.2014,vol.14.
DAFX-8
"
7,Learning Asymmetric and Local Features in Multi-Dimensional Data through Wavelets with Recursive Partitioning,https://arxiv.org/pdf/1711.00789v5.pdf,https://github.com/MaStatLab/WARP,"1
LearningAsymmetricandLocalFeaturesin
Multi-DimensionalDatathroughWaveletswith
RecursivePartitioning
MengLiandLiMa
Abstract
ŠEffectivelearningofasymmetricandlocalfeaturesinimagesandotherdataobservedonmulti-dimensionalgridsisa
challengingobjectivecriticalforawiderangeofimageprocessingapplicationsinvolvingbiomedicalandnaturalimages.Itrequires
methodsthataresensitivetolocaldetailswhilefastenoughtohandlemassivenumbersofimagesofeverincreasingsizes.We
introduceaprobabilisticmodel-basedframeworkthatachievestheseobjectivesbyincorporatingadaptivityintodiscretewavelet
transforms(DWT)throughBayesianhierarchicalmodeling,therebyallowingwaveletbasestoadapttothegeometricstructureofthe
datawhilemaintainingthehighcomputationalscalabilityofwaveletmethodsŠlinearinthesamplesize(e.g.,theresolutionofan
image).WederivearecursiverepresentationoftheBayesianposteriormodelwhichleadstoanexactmessagepassingalgorithmto
completelearningandinference.Whileourframeworkisapplicabletoarangeofproblemsincludingmulti-dimensionalsignal
processing,compression,andstructurallearning,weillustrateitsworkandevaluateitsperformanceinthecontextofimage
reconstructionusingrealimagesfromtheImageNetdatabase,twowidelyusedbenchmarkdatasets,andadatasetfromretinaloptical
coherencetomographyandcompareitsperformancetostate-of-the-artmethodsbasedonbasistransformsanddeeplearning.
F
1I
NTRODUCTION
E
FFECTIVE
learningofasymmetricandlocalfeaturesin
imagesandotherdataobservedonmulti-dimensional
gridsplaysacriticalroleinawiderangeofapplica-
tions.Onesuchapplicationisopticalcoherencetomography
(OCT).OCTisanon-invasiveimagingmodalitywidely
usedinophthalmologytovisualizecross-sectionsoftissue
layers.ThesetissuelayersŠsuchastheinnernuclearlayer
andouternuclearlayerŠareoftenmostlyhomogeneous
horizontallywhileinvolvinglargeverticalcontrasts.These
contrastsacrosslayersarekeyforophthalmologiststomake
adiagnosisbasedonthe(algorithmicallyreconstructed)
image.Furthermore,localstructuresinsuchimagescan
indicateoculardiseases,andtheirproperquantitativeas-
sessmentisanimportantreferenceformonitoringthepro-
gressionofthediseaseinclinicalpractice[1],[2],[3],[4],[5],
[6].Manyotherapplicationsof2Dand3Dimageanalysesin
biomedicineandbeyondalsoinvolveasymmetricandlocal
featurestovariousextents.Theeffectiveanalysisofsuch
multi-dimensionalobservationscanbegreatlyenhancedby
incorporatingadaptivityintothealgorithmormethodto
takeintoaccountsuchfeatures.
Afurtherchallengeinmodernapplicationsinvolving
multi-dimensionalobservationsistheeverincreasingsize
ofthedatasets.Forexample,boththenumberofimages
analyzedaswellastheresolutionŠi.e.,thetotalnumberof
pixelsŠofeachimagehavebeenexpandingrapidly.Many
traditionalmethodsandmodelsbecomecomputationalim-
practicalformoderndataastheyscalepolynomiallywith
theresolution.Effectivemethodsforanalyzingsuchdata

M.LiiswiththeDepartmentofStatistics,RiceUniversity,Houston,TX,
77025.E-mail:meng@rice.edu.

L.MaiswiththeDepartmentofStatisticalScience,DukeUniversity,
Durham,NC,27708.E-mail:li.ma@duke.edu.
mustscalewellwithboththeresolutionofeachimageand
thenumberofimages.
Theprimaryaimofthisworkistopresentageneral-
purposegenerativeprobabilisticmodelfordataonmulti-
dimensionalgridsthatcanbeusedtoaddressthesechal-
lengesininferenceandlearningŠbeingabletoeffectively
adapttotheasymmetricandlocalnatureofinteresting
features,whileachievingahighlyeflinearcompu-
tationalbudget.
Ourstartingpointisawell-knownstrategyfor
representingfunctionsŠamulti-resolutionrepresentation
throughthediscretewavelettransform(DWT).Wavelet
analysisishardlyanewtopic[7],[8],[9]andithasplayedan
importantroleinthecontextofsignalprocessingandimage
analysis.Itslinearcomputationalscalabilityiswell-suited
foranalyzingmassivedata.However,traditionalstatistical
waveletanalyseshavemostlybeenfocusingoneffective
modelingandinferenceonthewaveletcoef
given
a

wavelettransformoftheoriginaldata[10],[11],[12],
[13],[14],[15].Apredeterminedwavelettransform,
however,cannotadapttothestructureofthedataand
consequentlysuffersinitsabilitytoeffectivelymaintain
thelocalstructuresintheoriginalobservation.Also,clas-
sicalwavelettransformsonmulti-dimensionalgridsare
generallysymmetricwithrespecttothedimensions,ren-
deringthemineffectiveforpreservingasymmetricfeatures.
Nodownstreamstatisticalanalysescanrecoverwhathas
alreadybeenlostattheupstreamwavelettransformstage.
Inthiswork,weshowthatitispossibletoincorporate
thedesiredadaptivityintothewavelettransformstage
whilemaintainingthecomputationalscalabilityofthesta-
tisticalanalysisthroughaverysimplehierarchicalmodeling
strategyŠstartingthemodelﬁonelevelupﬂ,thatis,by
incorporatingthewavelettransformitselfasanunknown
arXiv:1711.00789v5  [stat.ME]  6 Nov 20202
quantityofinterestintotheprobabilisticmodel,andlearn
itbasedonthedata.,weconsiderlatent(1D)
wavelettransformsthatcanﬁtwistandturnﬂ(orﬁwarpﬂ)
overthemulti-dimensionalgrid,orthe
indexspace
,and
adoptaBayesianprioronthepathofitstwistingand
turning.Inotherwords,weplaceaprioronthelocal
directionalityofthe1Dtransformtoallowtheﬁwarpingﬂto
adapttothegeometricstructureoftheunderlyingfunction,
e.g.,thetrueimage,throughtheBayesianmachinery.
Indesigninganappropriatepriorforthelocaldirec-
tionality,wenotethatﬁwarpingﬂa1Dwavelettrans-
formthroughthegridpointsisequivalenttothe
1Dwavelettransformwhileshufgridpointsinthe
multivariateindexspaceoftheobservationŠi.e.,through
applyingagiven1Dwavelettransformtoapermuted
versionoftheobservation.Thisconnectionimpliesthat
probabilisticmodelsonﬁwarpingﬂcanbeinducedfrom
distributionsonthespaceofpermutationsoftheindex
pointsorlocations.Moreover,wedrawafurtherconnection
betweenpermutationsandrecursivedyadicpartitioning
ontheindexspacetoconstructaprioronpermutations
inducedbyrandomrecursivepartitioningovertheindex
space.Thispriortakesadvantageofthefactthatmulti-
dimensionalimagestendtobepiecewisesmoothtostrike
abalancebetweenandcomputationaltractability,
allowingustocompleteexactBayesianinferencethrougha
recursivemessagepassingalgorithmwithacomputational
budgetlinearintheresolutionandsamplesize.
Duetotheconnectiontorecursivepartitioning,weshall
refertoourapproachasWARP,or
WAveletswithRecursive
Partitioning
.Throughextensivenumericalstudiesinvolv-
ingalargenumberofnaturalimagesfromtheImageNet
database,twoadditionalbenchmarkdatasets,andanOCT
dataset,weshowthatWARPoftenoutperformstheex-
istingstate-of-the-artapproachesbyasubstantialmargin
whilemaintainingthecomputationalefofclassical
waveletanalyseswithwavelettransforms.Whilewe
focuson2Dand3Dimagesinourmotivationandnumerical
examples,ourframeworkisreadilyapplicabletoobserva-
tionsofmorethanthreedimensionswithout
Therestofthepaperisorganizedasfollows.Section2
introducestheWARPframework.InSection2.1were-
viewthekeycomponentsofBayesianwaveletregression
models,introducepermutationoftheindexspaceasa
waytoincorporateadaptivityintowaveletanalysis,and
constructaclassofpriorsonpermutationsinducedby
recursivedyadicpartitioningontheindexspace.Wederive
thecorrespondingposteriormodelandprovidecomputa-
tionalrecipesforexactBayesianinferenceundertheWARP
modelwithHaarwaveletsinSection2.2.InSection3,we
carryoutanextensivenumericalstudyandcompareour
methodtoexistingstate-of-the-artwaveletandnon-wavelet
methodsincludingadeeplearningmethodonavariety
ofrealimages.InSection5wecarryoutacasestudyby
applyingWARPtoanalyzeanOCTdataset,andcompares
itsperformancetoanumberofstate-of-the-artapproaches.
Section6concludeswithsomebriefremarks.TheC++
sourcecodealongwithaMatlabtoolboxandRpackage
toimplementtheproposedmethodisavailableonlineat
https://github.com/MaStatLab/WARP.
2M
ETHOD
2.1ABayesianhierarchicalwaveletregressionmodel
withrecursivedyadicpartitions
2.1.1Backgroundandoverview
Weuse

todenoteaspaceofindicesorlocations(e.g.,
pixelsinimages)whereweobtainnumericalmeasure-
ments(e.g.,intensitiesofpixels).Throughoutthiswork,
weassume

tobean
m
-dimensionalrectangulartube
consistingof
n
i
=2
J
i
gridpointsinthe
i
thdimensionfor
i
=1
;
2
;:::;m
,thatis,thefunctionvaluesareobservedon
amulti-dimensionalequidistantgrid.Tosimplifynotation,
weshalluse
[
a;b
]
torepresenttheset
f
a;a
+1
;:::;b
g
for
twointegers
a
and
b
with
a

b
.Thentheindexspace

is
oftheform
=[0
;
2
J
1

1]

[0
;
2
J
2

1]

[0
;
2
J
m

1]
:
Thelocationsin

canbeplacedintoavectoroflength
n
=2
J
.Forexample,wecanmapthelocation
s
=
(
s
1
;s
2
;:::;s
m
)
2

tothe
t
thelementinthevector,where
t
=
s
1
+
P
m
l
=2
(
Q
l

1
i
=1
n
i
)
s
l
.Correspondingly,anyfunction
f
:
!
R
canberepresentedasavector
f
oflength
n
=2
J
whose
t
thelementis
f
(
s
)
.
Now,weconsideraregressionmodel
y
=
f
+

with

˘
N
(
0
;


)
;
(1)
where
y
=(
y
0
;y
1
;:::;y
2
J

1
)
0
aretheobservationson

,
f
=(
f
0
;f
1
;:::;f
2
J

1
)
0
theunderlyingunknownfunction
mean(orthesignal),and

=(

0
;
1
;:::;
2
J

1
)
0
thenoise.
Foreaseofillustration,weassumehomogeneouswhite
noise,i.e.,


=
˙
2
I
n
,thoughourmodelandinference
algorithmsdonotrelyonthisassumptionatallandcan
bereadilyapplytomodelswithheterogeneousvariance;
seeSection6forfurtherdiscussion.
Onecanapplya1Ddiscretewavelettransform(DWT)
totheobservationvector
y
throughmultiplyingthecorre-
spondingorthonormalmatrix
W
tobothsidesofEq.(1),
obtaining
w
=
z
+
u
where
w
=
W
y
isthevectorof
empiricalwaveletcoef
z
=
W
f
themeanvector
forwaveletcoefand
u
=
W

thenoisevectorinthe
waveletdomain.Thismodelcanberewritteninalocation-
scaleform:
w
j;k
=
z
j;k
+
u
j;k
for
j
=0
;
1
;:::;J

1
and
k
=0
;
1
;:::;
2
j

1
,where
w
j;k
,
z
j;k
,
u
j;k
arethe
k
thwavelet
coefsignal,andnoiseatthe
j
thscaleinthewavelet
(i.e.,location-scale)domain,respectively.
Itwillgenerallybeunreasonabletotreatmulti-
dimensionalobservationssimplyasavectorwithanarbi-
traryorderingofthelocations;see[16],[17],[18].Sucha
vectorizationignoresthestructureoftheunderlyingfunc-
tion,andthuswillresultinlesseffectiveﬁenergyconcen-
trationﬂ,i.e.,producingawaveletdecompositionof
f
that
isnotverysparseŠwithmanynon-zero
z
j;k
'sofsmallto
moderatesizes,reducingthesignal-to-noiseratioatthose
(
j;k
)
combinations.
Foreachdatasetathand,however,theretypi-
callyexistssomeorderingsofthelocationsthateffectively
reorganizethedatasothatthecorrespondingvectorization
ofthedataprovidesanefrepresentationoftheunder-
lyingfunction;seeFigure1foranillustration.Adoptinga
Bayesianmodelingperspective,onecanthinkoftheunder-
lyingﬁgoodﬂvectorizationsaslatentstructuresofinterest.
3
Also,onecanviewthewaveletregressionmodelunder
eachgivenindexpermutationasacompetinggenerative
modelfortheobserveddatagiventhelatentstructure.
Thisperspectiveinspiresustoincorporateaprioronthe
permutations,therebyallowingustocomputeaposterior
onthespaceofcompetingwaveletregressionmodels,and
thenbasedonthegoaloftheanalysisproceedwiththe
commondevicesforBayesianinference.Twoparticular
usefultoolsare(i)Bayesianmodelselection[19]Šlearning
agoodpermutationforrepresentingtheimagebasedonits
posteriorprobability;and(ii)BayesianmodelaveragingŠ
estimatingtheunderlyingfunctionbasedonaveragingover
thedifferentpermutationsusingtheirposteriorprobabili-
ties[20].
ThisBayesianapproachdoesincurapracticalchallenge
commonlyarisinginhigh-dimensionalproblemsŠthespace
ofallpermutationsissomassivethatbrute-forceenumera-
tionoverthespaceiscomputationallyprohibitive.Inthe
currentcontext,effectiveexplorationofthemodel(i.e.,
permutation)spacebecomespossible,however,oncewe
realizethatthevastmajorityofthepermutationswilllead
towaveletregressionmodelsthatignorethespatialsmooth-
nessoftheunderlyingfunctionŠi.e.,closelocationsin

oftencorrespondtosimilarvaluesin
f
.Inparticular,wecan
focusattentiononasubclassofpermutationsthattovarious
extentspreservespatialsmoothness,anddesignamodel
spacepriorsupportedonthismanageablesubclass.Tothis
end,weappealtoarelationshipbetweenrecursivedyadic
partitioning(RDP)andpermutations,andshallconsiderthe
collectionofpermutationsinducedbyRDPson

.
NextweintroducesomebasicnotionsregardingRDPs
on

,whicharethenusedtoconstructaprioronpermuta-
tions.Inreadingthenexttwosubsections,thereadermay
refertoFigure1foranillustrationofthekeynotionsand
notations.
2.1.2Recursivedyadicpartitioningonthelocationspace
A
partition
of

isacollectionofnonemptysets
f
A
1
;A
2
;:::;A
H
g
suchthat
=
[
H
h
=1
A
h
and
A
h
1
\
A
h
2
=
;
forany
h
1
6
=
h
2
.Nowlet
T
0
;
T
1
;
T
2
;:::;
T
j
;:::
bea
sequenceofpartitions
of

.Wesaythatthissequenceisa
recursivedyadicpartition
(RDP)ifitthefollowing
twoconditions:(i)
T
j
consistsof
2
j
blocks:
T
j
=
f
A
j;k
:
k
=0
;
1
;:::;
2
j

1
g
;(ii)
T
j
+1
isobtainedbydividingeach
setin
T
j
intotwopieces,i.e.,
A
j;k
=
A
j
+1
;
2
k
[
A
j
+1
;
2
k
+1
forall
j

0
and
k
=0
;
1
;:::;
2
j

1
.
WecallanRDP
canonical
ifthesequenceofpartitions
satisfytwoadditionalconditions:(iii)ifthepartitionblocks
A
j;k
arerectanglesoftheform
A
j;k
=[
a
(1)
j;k
;b
(1)
j;k
]

[
a
(2)
j;k
;b
(2)
j;k
]

[
a
(
m
)
j;k
;b
(
m
)
j;k
]
:
and(iv)
A
j
+1
;
2
k
and
A
j
+1
;
2
k
+1
areproducedbydividing
A
j;k
intotwohalvesatthemiddleofoneof
A
j;k
's
divisible
dimensions.
Arectangularpartitionblock
A
j;k
is
divisible
indimen-
sion
d
if
A
j;k
issupportedonatleasttwovaluesinthat
dimension,i.e.,
a
(
d
)
j;k
<b
(
d
)
j;k
.Inthiscase,if
A
j;k
isdivided
indimension
d
,thenitschildren
A
j
+1
;
2
k
and
A
j
+1
;
2
k
+1
are
givenby
[
a
(
d
)
j
+1
;
2
k
;b
(
d
)
j
+1
;
2
k
]=[
a
(
d
)
j;k
;
(
a
(
d
)
j;k
+
b
(
d
)
j;k
)
=
2]
and
[
a
(
d
)
j
+1
;
2
k
+1
;b
(
d
)
j
+1
;
2
k
+1
]=[(
a
(
d
)
j;k
+
b
(
d
)
j;k
)
=
2+1
;b
(
d
)
j;k
]
;
while
[
a
(
d
0
)
j
+1
;
2
k
;b
(
d
0
)
j
+1
;
2
k
]=[
a
(
d
0
)
j
+1
;
2
k
+1
;b
(
d
0
)
j
+1
;
2
k
+1
]=[
a
(
d
0
)
j;k
;b
(
d
0
)
j;k
]
forall
d
0
6
=
d
.
AnycanonicalRDPon

willhaveexactly
J
+1
levels,
i.e.,
T
0
;
T
1
;:::;
T
J
.The
j
thlevelpartition
T
j
consistsof
2
j
rectangularpiecesofequalsize,eachcovering
n=
2
j
locationsin

.Fromnowon,wesimplyuseRDPtorefer
tocanonicaloneswhenthiscausesnoconfusion.
2.1.3RDPsandpermutations
EachRDPcanberepresentedbya
J
levelbifurcatingtree
withthepartitionblocksin
T
j
formingthe
2
j
nodesinthe
j
thlevelofthetree.Assuch,wecanuse
T
=
[
J
j
=0
T
j
to
representtheRDP.Eachnodeinthe
J
thlevelcorrespondsto
auniquelocationin

,andiscalledﬁatomicﬂasitcontains
asingleelement.WeshallinterchangeablyrefertoanRDP
asaﬁtreeﬂ,andtothepartitionblocksasﬁnodesﬂ.
GiventheRDP
T
,eachlocation
s
2

fallsintoaunique
branchof
T
,thatis,
=
A
0
;
0
˙
A
1
;k
1
(
s
)
˙
A
2
;k
2
(
s
)
˙
˙
A
J;k
J
(
s
)
=
f
s
g
,with
A
j;k
j
(
s
)
beingthenodein
the
j
thleveltowhich
s
belongs.Accordingly,theRDP
T
inducesauniquevectorizationofthelocationsin

such
that
s
correspondstothe
t
(
s
)
thelementofthevectorwhere
t
(
s
)=
P
J
l
=1
2
J

l

e
l
(
s
)
with
e
l
=
k
l
(
s
)mod2
,indicating
thebranchofthetree
s
fallsintoatlevel
l
.Assuch,
T
inducesapermutationofthe
n
locations,andwelet
ˇ
T
denotethispermutation.
Asanillustration,Figure1presentsanRDPandthe
inducedpermutationusingatoy
4

4
image(so
m
=2
and
J
1
=
J
2
=2
).Weindexpixelsinthetrueimagefrom
0to15.Inaddition,weassumethattheunderlyingfunc-
tiontakesonlytwovaluesŠ1and2Šonthe16locations,
representedbythewhiteandtheredcolors,respectively.
ThedemonstratedRDPcorrespondswelltothestructureof
theunderlyingsignal,whichwouldresultinaneffective1D
waveletanalysisonthevectorizedobservation.
WeshallnowutilizetherelationshipbetweenRDPsand
permutationstoconstructaprioronthelatter.Beforethat,
weshallsimplifyournotationsalittle.Notethatwhilewhat
the
(
j;k
)
thnode
A
j;k
isdependsontheRDP
T
,different
RDPscansharecommonnodesŠthe
(
j;k
)
thnodeinone
T
maybethesameasthe
(
j;k
0
)
thnodeinanother.(Notethat
thelevelofthenodemustbethesameineitherRDP.)In
thefollowing,wewillneedtospecifyquantitiesthatonly
dependonthenoderegardlessoftheRDPtree
T
itarises
from.Asuccinctwayforexpressingsuchquantitiesisto
writethemasamappingfrom
A
to
R
,where
A
denotes
thecollectionofallsetsthat
could
benodesin
some
RDP,
orequivalently,
A
isthetotalityofnodesinallRDPs.(This
istobedistinguishedfromthecollectionofnodesinany
particularRDP,whichisdenotedby
T
.)Itisworthnoting
A
isaset.
Nowwemay
ˆ
j;k
insuchawaythatitsvalue
onlydependsonwhattheset
A
j;k
is,regardlessoftheRDP
T
towhichitbelongs.Inthiscasewecanlet
ˆ
j;k
=
ˆ
(
A
j;k
)
,
where
ˆ
(

)
isamappingform
A
to
[0
;
1]
.Whileaset
A
2A
4
'
&
$
%
0
4
8
12
1
5
9
13
2
6
10
14
3
7
11
15
True(
f
)
A
0
;
0
Level0
A
1
;
0
A
1
;
1
Level1
A
2
;
0
A
2
;
1
A
2
;
2
A
2
;
3
Level2
A
3
;
0
A
3
;
1
A
3
;
2
A
3
;
3
A
3
;
5
A
3
;
4
A
3
;
7
A
3
;
6
Level3
A
4
;
0
A
4
;
1
A
4
;
2
A
4
;
3
A
4
;
4
A
4
;
5
A
4
;
6
A
4
;
7
A
4
;
8
A
4
;
9
A
4
;
10
A
4
;
11
A
4
;
12
A
4
;
13
A
4
;
14
A
4
;
15
Level4
0
1
4
5
2
6
3
7
8
9
10
11
12
13
14
15
=[0
;
15]
A
1
;
0
=[0
;
7]
A
2
;
0
=
f
0
;
1
;
4
;
5
g
A
3
;
0
=
f
0
;
1
g
A
4
;
0
=
f
0
g
A
4
;
1
=
f
1
g
A
3
;
1
=
f
4
;
5
g
A
4
;
2
=
f
4
g
A
4
;
3
=
f
5
g
A
2
;
1
=
f
2
;
3
;
6
;
7
g
A
3
;
2
=
f
2
;
6
g
A
4
;
4
=
f
2
g
A
4
;
5
=
f
6
g
A
3
;
3
=
f
3
;
7
g
A
4
;
6
=
f
3
g
A
4
;
7
=
f
7
g
A
1
;
1
=[8
;
15]
A
2
;
2
=
f
8
;
9
;
10
;
11
g
A
3
;
4
=
f
8
;
9
g
A
4
;
8
=
f
8
g
A
4
;
9
=
f
9
g
A
3
;
5
=
f
10
;
11
g
A
4
;
10
=
f
10
g
A
4
;
11
=
f
11
g
A
2
;
3
=
f
12
;
13
;
14
;
15
g
A
3
;
6
=
f
12
;
13
g
A
4
;
12
=
f
12
g
A
4
;
13
=
f
13
g
A
3
;
7
=
f
14
;
15
g
A
4
;
14
=
f
14
g
A
4
;
15
=
f
15
g
Permutation/vectorizationinducedbyRDP
Fig.1.IllustrationofthecorrespondencebetweenRDPsandpermutations.Inthetreerepresentation,
A
2
;
0
=
f
0
;
1
;
4
;
5
g
meansthenode
A
2
;
0
containsthe(0,1,4,5)thelementsof

.Thecoloringcodefortheobservationsisredfor2andwhitefor1.Fromlevel0tolevel3,edgesthatare
thickerthanothersarethepartitionsofthecurrentlevel;nodesatthelastlevelareallatomic.
mightbethe
A
j;k
inoneRDPand
A
j;k
0
inanother,thecorre-
sponding
ˆ
(
A
)
valuewillthenbethesameunderthismap-
pingbasedThemapping-basednotationsuch
as
ˆ
(

)
allowsvariousparameterstobeinanode-
(ratherthanmanner.Thisobservation
hasextremelyimportantcomputationalimplicationsŠas
wewillshowlater,thespaceofnodes
A
forallcanonical
RDPsisofacardinalitylinearinthesizeof

,whilethatof
canonicalRDPsisexponentialin
n
.(SeeProposition1inthe
SupplementaryMaterials.)Thereforeitisexactlytheability
tocarryingoutthecomputationfortheposteriorinanode-
mannerthatallowsustoachievelinearcomplexity
inourinferencealgorithm.Moreover,thisnotationwillalso
helpelucidatederivationsontheposterior.
2.1.4PriorsonRDPs:randomRDP
Ourstrategyofrepresentingmulti-dimensionalfunctions
usingvectorswillonlypayoffifthevectorizationof

can
resultinanefcharacterizationofthedata,thereby
leadingtostrongerenergyconcentrationunderwavelet
transforms.Forexample,theRDPillustratedinFigure1will
leadtoparticularlyefinferenceofthecorresponding
function.Ingeneral,thetrueoptimalvectorizationŠorthe
correspondingRDPŠisunknown,andoneshallrelyonthe
datatolearntheRDPsthatinduceﬁgoodﬂvectorizations.
WeaimtoachievethisinahierarchicalBayesianap-
proachbytreatingtheRDPasalatentstructureandplacing
apriorontheRDP.Weconsiderthefollowingprioronthe
RDPoriginallyproposedinthecontextofdensityestima-
tion[21],[22],whichisinafashion
andleadstoveryefnode-basedposteriorinference
algorithmsthatscalelinearlyin
n
,thesizeof

.
Wedescribethepriorasasimplegenerativeprocedure
foranRDPinaninductivemanner.First,
T
0
=
f

g
by
Nowsupposewehavegenerated
T
0
;
T
1
;

;
T
j
forsome
0

j

J

1
,then
T
j
+1
isgeneratedasfollows.
Foreach
A
j;k
2T
j
,let
D
(
A
j;k
)
ˆf
1
;
2
;:::;m
g
bethe
collectionofitsdivisibledimensions.Werandomlydraw
adimensionin
D
(
A
j;k
)
,anddivide
A
j;k
inthatdimension
toget
A
j
+1
;
2
k
and
A
j
+1
;
2
k
+1
.Inparticular,welet

d
(
A
j;k
)
betheprobabilityfordrawingthe
d
thdimension,where
P
m
d
=1

d
(
A
j;k
)=1
and

d
(
A
j;k
)=0
for
d
62D
(
A
j;k
)
.In
manyproblems,
apriori
onehasnoreasontofavordividing
anyparticulardimensionoveranother,andadefaultspeci-
istoset

d
(
A
j;k
)=1
=
jD
(
A
j;k
)
j
1
f
d
2D
(
A
j;k
)
g
;
where
1
E
istheindicatorfunctionofwhether
E
holdsor
not.Thiscompletestheinductivegenerationof
T
j
+1
.The
procedurewillterminateafter
T
J
isgeneratedasallnodes
in
T
J
areatomicwithnodivisibledimensions.
Theabovegenerativemechanismformsaprobability
distributiononthespaceofRDPs,whichiscalledthe
randomrecursivedyadicpartition
(RRDP)distribution,andit
isbythecollectionofselectionprobabilities

d
(

)
onall
potential
nodes.Wewrite
T˘
RRDP
(

)
;
where
f

(
A
):
A
2Ag
;
and

(
A
)=
(

1
(
A
)
;
2
(
A
)
;:::;
m
(
A
))
0
,thatis,

isamapping
from
A
tothe
(
m

1)
-dimensionalsimplex.
ItisworthnotingthattheRRDPisarestrictedversion
ofthemoregeneralBayesianandregression
tree(CART)prior[23],[24].ThemainconstraintinRRDP
comparedtothegeneralBayesianCARTisthattheformer
5
issupportedoncanonicalRDPsonlyŠthatis,eachdyadic
partitionmustbeanevensplit,occurringatthemiddleof
therangeinoneofthedivisibledimensions.Thisadditional
restrictionensuresthatthecardinalityof
A
islinearin
n
,
therebyreducingthecomputationalcomplexityrequiredfor
inferenceto
O
(
n
)
.
2.2RecipesforBayesianinference
Inthissection,wepresentrecipesforderivingandsampling
fromtheposteriorofourBayesianmodel,andforevaluating
posteriorsummariessuchastheposteriormeanof
f
.We
notethatthemarginalposterioroftheRDP
T
isthekey
componentforposteriorinference,becauseonceconditional
on
T
,ourmodelreducestoastandardBayesianwavelet
regressionforwhichclosed-formconditionalposteriorsare
readilyavailableundercommonprior
Interestingly,whenaHaarbasisisadoptedinthe
waveletregressionmodel,themarginalposteriorof
T
can
becalculatedanalyticallyinclosedformthrougharecursive
algorithmthatisoperationallysimilartoMallat'spyra-
midalgorithm,achievingalinearcomputationalcomplexity
O
(
n
)
.
2.2.1ExactBayesianinferenceunderHaarbasis
TheHaarwaveletbasisisuniqueinthatthe
(
j;k
)
thwavelet
coefunderthevectorizationinducedbyanyRDP
T
isdeterminedbyonlythelocationsinsidethenode
A
j;k
.
WecallthispropertyoftheHaarbasis
node-autonomy
and
saythatinferenceundertheHaarbasisis
node-autonomous
.
,forallRDPsinwhich
A
isanodeandisdi-
videdinthe
d
thdirection,thecorrespondingHaarwavelet
coefassociatedwiththenode
A
isgivenby
w
d
(
A
)=1
=
q
j
A
j
0
B
@
X
x
2
A
(
d
)
l
y
(
x
)

X
x
2
A
(
d
)
r
y
(
x
)
1
C
A
where
A
(
d
)
l
and
A
(
d
)
r
representthetwochildrennodesif
A
isdividedinthe
d
thdimensionand
j
A
j
=2
J

j
isthetotal
numberoflocationsin
A
.Incontrast,waveletcoef
fromwaveletbaseswithlongersupportthanHaararenot
node-autonomousŠnotonlydoesthecoefassociated
with
A
dependontheobservationswithin
A
butonthose
inother(oftenbutnotalwaysadjacent)nodesin
T
aswell.
Node-autonomyenablestheposteriortobecomputed
inafashion,avoidingintegrationinthemuch
largerspaceofRDPs.Consequently,exactinferencecanbe
completedinacomputationalcomplexityofthesamescale
asthetotalnumberofallpotentialnodesinRDPs,whichis
equalto
Q
m
i
=1
(2
n
i

1)=
O
(2
m
n
)
.
Nextwelayoutthegeneralstrategyforinference.We
showthroughtwotheoremsthatthemarginalposteriorof
theRDP
T
iscomputableinanalyticallythrougharecur-
sivealgorithmthatresemblesMallat'spyramidalgorithm
fortwoverypopularclassesofBayeswaveletregression
modelsŠ(i)thosethatmodeleachwaveletcoefientinde-
pendentlygiven
T
(Theorem1);and(ii)thosethatinduce
ahiddenMarkovmodel(HMM)forincorporatingdepen-
dencyamongthewaveletcoefgiven
T
(Theorem2).
Theorem1.
Suppose
T˘
RRDP(

)
andgiventheHaarDWT
under
T
,onemodelsthewaveletindependently,i.e.,
(
w
j;k
;z
j;k
)
ind
˘
p
j;k
(
w;z
j
˚
)
forall
(
j;k
)
,where
˚
represents
thehyperparametersoftheBayesianwaveletregressionmodel.
Thenthemarginalposteriorof
T
isstillanRRDP.,
Tj
y
˘
RRDP(
~

)
wheretheposteriorselectionprobability
mapping
~

isgivenas
~

d
(
A
)=

d
(
A
)
M
d
(
A

A
(
d
)
l

A
(
d
)
r
)
=

A
)
foranynon-atomic
A
2A
where
M
d
(
A
)
isthemarginal
likelihoodcontributionfromthewaveletonnode
A
if
itisanodein
T
anddividedindimension
d
,i.e.,
M
d
(
A
)=
R
p
j;k
(
w
d
(
A
)
;z
j
˚
)
dz
and
:
A!
[0
;
1
)
isamapping
recursively(i.e.,itsvalueon
A
dependsonitsvalueson
A
'schildren)as

A
)=
X
d
2D
(
A
)

d
(
A
)
M
d
(
A

A
(
d
)
l

A
(
d
)
r
)
if
A
isnotatomic,and

A
)=1
if
A
isatomic.
Remark:

istheoverallmarginallikelihood.Itisa
functionofthehyperparameters
˚
,andcanbeusedfor
specifyingthehyperparameters
˚
inanempiricalBayes
strategyusingmaximummarginallikelihoodestimation
(MMLE).
Theorem2.
Suppose
T˘
RRDP(

)
andgiven
T
under
aHaarDWT,onemodelsthewaveletconditionally
independentlygivenasetoflatentstatevariables
S
=
f
S
j;k
:
j
=0
;
1
;
2
:::;J;k
=0
;
1
;:::;
2
j

1
g
(
w
j;k
;z
j;k
)
j
S
j;k
=
s
ind
˘
p
(
s
)
j;k
(
w;z
j
˚
)
forall
(
j;k
)
where
S
j;k
2f
1
;
2
;:::;K
g
isalatentstatevariableassociated
with
(
j;k
)
.Also,supposethecollectionofalllatentvariablesis
modeledasatop-downMarkovtree(MT)withtransitionkernel
ˆ
,
S˘
MT(
ˆ
)
,i.e.,
P(
S
j;k
=
s
0
j
S
j

1
;
b
k=
2
c
=
s
)=
ˆ
j
(
s;s
0
)
where
ˆ
j
(

;

)
isthetransitionkerneloftheMarkovmodelwhichis
allowedtobedifferentover
j
.Thenthejointmarginalposteriorof
(
T
;
S
)
canbefullyasthefollowingsequentialgenerative
process.Suppose
T
0
;
T
1
;:::;
T
j
andthelatentvariablesupto
level
j

1
havebeengenerated.(Tobegin,wehave
j
=0
and
T
0
=
f

g
.)Thenthestatevariablesinlevel
j
,aregeneratedfrom
thefollowingposteriortransitionprobabilities
P(
S
j;k
=
s
0
j
S
j

1
;
b
k=
2
c
=
s;
T
(
j
)
;
y
)
=
ˆ
j
(
s;s
0
)
X
d
2D
(
A
)

d
(
A
)
M
(
s
0
)
d
(
A

s
0
(
A
(
d
)
l

s
0
(
A
(
d
)
r
)
=

s
(
A
)
;
where
A
isthenode
A
j;k
in
T
j
.Given
S
j;k
=
s
0
,suppose
j<J
,
then
T
j
+1
isgeneratedbydrawing
D
j;k
fromamultinomialwith
probabilities
~

(
A
)
suchthat
P(
D
j;k
=
d
j
S
j;k
=
s
0
;
T
(
j
)
;
y
)
=

d
(
A
)
M
(
s
0
)
d
(
A

s
0
(
A
(
d
)
l

s
0
(
A
(
d
)
r
)
P
d
0
2D
(
A
)

d
0
(
A
)
M
(
s
0
)
d
0
(
A

s
0
(
A
(
d
0
)
l

s
0
(
A
(
d
0
)
r
)
;
where
M
(
s
)
d
(
A
)
isthemarginallikelihoodcontributionfrom
thewaveletonnode
A
ifitisanodein
6
T
,isdividedindimension
d
in
T
,anditslatent
stateis
s
.Thatis,
M
(
s
)
d
(
A
)=
R
p
(
s
)
j;k
(
w
d
(
A
)
;z
j
˚
)
dz
and

=
1
;

2
;:::;

K
):
A!
[0
;
1
)
K
is
avector-valuedmappingrecursivelyas

s
(
A
)=
P
s
0
ˆ
j
(
s;s
0
)
P
d
2D
(
A
)

d
(
A
)
M
(
s
0
)
d
(
A

s
0
(
A
(
d
)
l

s
0
(
A
(
d
)
r
)
if
A
isnotatomic,and

s
(
A
)=1
if
A
isatomic,forall
s
2f
1
;
2
;:::;K
g
,where
j
isthelevelof
A
.
Oncethemarginalposteriorof
T
iscomputedthrough
Theorem1orTheorem2,thefulljointposteriorisavailable
astheconditionalposterioroftherestofourmodelgiven
T
isavailableforcommonBayesianwaveletregressions.
(MoredetailsaregiveninSection2.2.2.)Thenstandard
Bayesianinferencecanproceed.
Inparticular,onecandrawsamplesfor
(
T
;
S
)
fromtheir
marginalposteriorgiveninTheorem2.Thengiven
(
T
;
S
)
,
onecanfurthersample
z
fromtheconditionalposterior
correspondingtothechosenwaveletregressionmodel,and
Bayesianinferencecanproceedintheusualmanner.For
example,onecanobtainposteriorsamplesoftheunderlying
function
f
bydrawingsamples
(
T
(1)
;
S
(1)
;
z
(1)
)
;
(
T
(2)
;
S
(2)
;
z
(2)
)
;:::;
(
T
(
B
)
;
S
(
B
)
;
z
(
B
)
)
:
Thenforthe
b
thdraw,wecancomputethecorresponding
function
f
(
b
)
usingtheinverseDWT
f
(
b
)
=
ˇ

1
T
(
b
)

W

1
z
(
b
)

;
where
ˇ

1
T
denotestheinversepermutationcorresponding
toanRDP
T
.Basedontheposteriorsamplesof
f
,we
canconstructpointwisecrediblebandsandestimatethe
posteriormean
E(
f
j
y
)
.WecanapplyRao-Blackwellization
andobtainthefollowingestimatefortheposteriormean
E(
f
j
y
)
ˇ
1
B
B
X
b
=1
ˇ

1
T
(
b
)

W

1
E
(
z
(
b
)
jT
(
b
)
i
;
y
)

:
ForseveralpopularBayesianwaveletregressionmodels,
theposteriormeancanactuallybecomputedanalytically
throughmessagepassing(MP)withoutposteriorsampling
whentheHaarbasisisadopted.Wenextturnto
reviewingthesewaveletregressionmodelsinSection2.2.2,
anddefertheMPalgorithm(Theorem3)toSupplementary
Materials.
2.2.2ExamplesofcompatibleBayesianwaveletregression
models
SofarwehavekeptthedescriptionoftheBayesianwavelet
regressionmodelgeneral,usinggenericnotationssuchas
p
(
w
j;k
;z
j;k
j
˚
)
and
p
(
w
j;k
;z
j;k
j
S
j;k
;
˚
)
withoutspelling
outthedetails.Nextwedescribesomeofthemostpopular
Bayesianwaveletregressionmodels.Theyindeedtakethese
generalformsandthereforeourframeworkisapplicableto
them.
ApopularclassofBayesianwaveletregressionmodels
forachievingadaptiveshrinkageof
z
utilizetheso-called
spike-and-slabprior,whichintroducesalatentbinaryran-
domvariable
S
j;k
foreach
(
j;k
)
suchthat
z
j;k
j
S
j;k
ind
˘
(1

S
j;k
)

0
(
z
j;k
)+
S
j;k

(
z
j;k
j
˝
j
;˙
)
where

0
(

)
isapointmassat0,and

(

˝
j
;˙
)
isauni-
modalsymmetricdensitythatpossiblydependson
˙
and
anotherscaleparameter
˝
j
.Acommonchoiceof

(

˝
j
;˙
)
is
thenormaldistributionwithmean0andvariance
˝
j
˙
2
,de-
notedby
˚
(

0
;
p
˝
j
˙
)
,whileheavy-tailedpriorsincluding
theLaplaceandquasi-Cauchydistributions[25]alsoenjoy
desirabletheoreticalproperties.,thefunction

(
x
j
˝
j
;˙
)
is

(
x
j
˝
j
;˙
)=
a
exp(

a
j
x=˙
j
)
=
(2
˙
)
forLaplacepriorswhere
a
=
p
2
=˝
j
,and

(
x
j
˝
j
;˙
)=(2
ˇ
)

1
=
2
f
1
j
x=˙
j
~

j
x=˙
j
)
=˚
(
x=˙
)
g
=˙
forquasi-Cauchypriorswith
~

x
)=
R
1
x
˚
(
t
j
0
;
1)
dt
.
Manyauthors[12],[13],[15],[26]adoptindependent
priorsonthelatentshrinkagestatevariable
S
j;k
S
j;k
ind
˘
Bern(
ˆ
j;k
)
:
Onewaytospecify
ˆ
=
f
ˆ
j;k
;
0

k<
2
j
;
0

j

J

1
g
thatproperlycontrolsformultiplicityis
ˆ
j;k
/
2

j
.The
of
˝
=
f
˝
j
;
0

j

J

1
g
ofcourse
dependsonthechoiceof

(

˝
j
;˙
)
.Forinstance,ifone
uses
˝
j
=2


˝
0
forthenormalandLaplaceprior,this
leadstothereducedparameter
˝
=(
;˝
0
)
.Onecanuse
˝
j

1
forthequasi-Cauchyprior.Otherauthors[11],
[27]showthatintroducingMarkovdependencyintothe
latentshrinkagestatescansubstantiallyimproveinference
byallowingeffectiveborrowingofinformationacrossthe
locationandscale.
CarryingoutinferenceunderWARPrequiresthecondi-
tionalposteriorof
z
j;k
given
(
T
;
S
)
.Fortheabovepopular
models,thisposteriorisgivenby
z
j;k
j
S
j;k
;
y
ind
˘
(1

S
j;k
)

0
(
z
j;k
)+
S
j;k
f
1
(
z
j;k
j
w
j;k
;˝
j
;˙
)
;
where
f
1
(
z
j;k
j
w
j;k
;˝
j
;˙
)
/
˚
(
w
j;k
j
z
j;k
;˙
)


(
z
j;k
j
˝
j
;˙
)
.
Thefunction
f
1
(
z
j;k
j
w
j;k
;˝
j
;˙
)
isanalyticallyavailable
if

(
j
˝
j
;˙
)
isthedensityofnormal,Laplace,orquasi-
Cauchydistributions.Forthenormalpriorwhere

(
j
˝
j
;˙
)=
˚
(
j
0
;
p
˝
j
˙
)
,
f
1
(
j
w
j;k
;˝
j
;˙
)
isthedensityof
N(
w
j;k
=
(1+
˝

1
j
)
;˙
2
=
(1+
˝

1
j
))
.ForLaplaceandquasi-
Cauchypriors,analyticalformsof
f
1
(
j
˝
j
;˙
)
areavailable
in[25,Sec.2.3].Asitisoftenthemeancorrespondingto
f
1
thatisneededforposteriorestimation,weheregive
theclosedformsofthemeansbyintegratingout
z
j;k
with
respecttoitsposteriordistribution.Letthecorresponding
meanfunctionbe

1
(
w
j;k
;˝
j
;˙
)
,whichisgivenby
w
j;k
=
(1+
˝

1
j
)
fornormalpriors,
w
j;k

˙
a
f
e

aw
j;k
=˙

w
j;k
=˙

a
)

e
aw
j;k
=˙
~

w
j;k
=˙
+
a
)
g
e

aw
j;k
=˙

w
j;k
=˙

a
)+
e
aw
j;k
=˙
~

w
j;k
=˙
+
a
)
forLaplacepriors,and
w
j;k
(
1

exp
 

w
2
j;k
2
˙
2
!)

1

2

w
j;k
˙


1
forquasi-Cauchypriors.
Forthesewaveletregressionmodelsthatadoptthe
spike-and-slabsetup,byTheorem2wecanderiveafully
7
conjugateposteriorthattakesthesameformastheprior.
Inparticular,foreach
A
2A
,underthenormalpriorfor

(
j
˝
j
;˙
)
,applyingTheorem2showsthat

Themarginallikelihoodcontributionfromthedata
withinnode
A
if
A
isdividedindimension
d
is:
M
(
s
)
d
(
A
)=
1
p
2
ˇ
(1+
s˝
j
)
˙
2
exp
ˆ

w
d
(
A
)
2
2
˙
2
(1+
s˝
j
)
˙
for
s
=0
;
1
:

Theposteriorspikeprobabilityon
A
if
A
isdivided
indimension
d
is:
~
ˆ
d
(
A
)=
ˆ
(
A
)
M
(1)
d
(
A
)
=M
d
(
A
)
;
where
M
d
(
A
)=
ˆ
(
A
)
M
(1)
d
(
A
)+(1

ˆ
(
A
))
M
(0)
d
(
A
)
:
Inmostpracticalproblems,thevariationinthefunction
valuewithineachpartitionblockwilleventuallybecome
negligiblewithrespecttothenoiselevel,andsofurther
divisionwithinsuchhomogeneousblockswillnotimprove
statisticalefandcouldleadtoForex-
ample,inFigure1thepartitionintheupperleftblock
(Level3)alongwithitsdescendantsisnotnecessary.Thus
itisalsodesirabletoincorporateadaptivityinthedepth
ofthewavelettreealongeachsubbranchandallowitto
beterminatedearlierthanreachinglevel
J
dependingon
howsmooththefunctionisacrosstheindexspace.This
considerationiscloselyrelatedtotheideaofadaptiveblock
shrinkage[28]inthefrequentistwaveletregressionanalysis.
Oncethereislittleevidenceforanyinterestingstructure
withinasubsetoftheindexspace,thenthefunctionvalue
withinthatsubsetcanbeshrunktoaconstant.Thatis,the
wavelettreeisﬁprunedﬂthere.Remarkably,waveletmodels
withsuchpruningarealsocompatiblewithourWARP
frameworkandcanbereadilyachievedbyintroducinga
pruningindicatortoaccompany
S
j;k
.Wereferinterested
readerstoSupplementaryMaterialsforadditionaltechnical
detailsonhowtoincorporatepruning.
FortheHaarbasis,theposteriormean
E(
f
j
y
)
forthe
abovewaveletmodelscanbeevaluatedanalyticallythrough
recursivemessagepassingwithoutanyMonteCarlosam-
plingforBayesianwaveletregressionmodelsthatadopt
thespike-and-slabsetupalongwithoptionalpruningofthe
wavelettree,whichcontainsthemodelswithoutoptional
pruningasspecialcaseswithzeropruningprobabilities.
Forcompleteness,wedescribethisstrategyintheSupple-
mentaryMaterialsandwilluseittocompute
E(
f
j
y
)
inour
numericalexamples.
3E
XPERIMENTS
Inthissection,weconductextensiveexperimentstoeval-
uatetheperformanceofourproposedframeworkinthe
imagereconstructiontaskintermsofbothestimationac-
curacyandcomputationalscalability.Applicationstoother
imageprocessingtasksarediscussedinSection4.Wecom-
pareWARPtoanumberofstate-of-the-artwavelet,non-
wavelet,anddeepneuralnetwork-basedmethodsavailable
intheliteraturefordenoising2Dimages.Weprovideresults
ondenoising3DimagesintheSupplementaryMaterials.
ThroughouttheseexperimentsweapplyWARPwithinde-
pendentspike-and-slabBayesianwaveletregressionmodels
undertheHaarbasisalongwithoptionalpruning.
Ourpriorisasfollows:
ˆ
(
A
)=
min(1
;
2

j
C
)
for
A
inthe
j
thresolution(for
j<J
),
˝
j
=2


˝
0
,and

(
A
)=

0
forall
A
;weset
˙
2
toan
estimatebasedonthescalewaveletcoef[8];
allotherparametersin
˚
=(
;;˙
2
;˝;C;
0
)
areesti-
matedbymaximizingthemarginallikelihood(availablein
aclosedformas

fromourrecursivemessagepassing
algorithm)atasetofgridpoints.SupplementaryMaterials
containasensitivityanalysisshowingthatWARPisgener-
allyrobusttothevaluesofitshyperparameters.Therefore
werecommendagridsearchonasmallsetratherthana
fulloptimizationasthedefaulttuningmethod.Gaussian
noisewithstandarddeviation
˙
isaddedtothetrueimages
andweapplyallmethodstothenoisyobservationsfor
imagereconstruction.ForWARP,weusetheposteriormean
asthereconstructedimage,whichisanalyticallyattainable
throughTheorem3.
3.1ImagereconstructionusingImageNetdata
Weuse100testimagesrandomlychosenfromtheImageNet
dataset[29]toevaluateselectedmethodsinreconstructing
imagesofvariousstructures.ImageNetisoriginallyusedfor
large-scalevisualrecognitioninthecommunityofcomputer
vision,andwehereuseitsFall2011release(consisting
of14,197,122urls).Wecompareourmethodwitheight
existingwaveletandnon-waveletapproacheswithavailable
software:1-dimensionalHaardenoisingoperatedonvector-
izedobservation[25]or1D-Haar,translation-invariant2D
Haarestimation[14]orTI-2D-Haar,shape-adaptiveHaar
wavelets[30]orSHAH,adaptiveweightssmoothing[31]
orAWS,BayesiansmoothingmethodusingtheChinese
restaurantprocess[32]orCRP,wedgelet[33]
orWedgelet,nonparametricBayesiandictionarylearning
proposedby[34]orBPFA,andtheconventionalrunning
medianmethodorRM.Weapplythecyclespinningtech-
niquetoremovevisualartifactsinimagereconstruction[35],
[36]forthemethodsofWARP,1D-Haar,SHAH,AWS,CRP,
WedgeletandRM,byaveraging121localshifts(astep
sizeupto5pixelsineachdirection).TI-2D-Haaristrans-
lationinvariantandBPFAincludescyclespinningbasedon
patches,andthusnoadditionalcyclespinningisneeded
forthesetwomethods.Foreachmethod,wecalculatethe
meansquarederror(MSE)andmeanabsoluteerror(MAE)
tomeasureitsaccuracy,andtimeeachmethodbasedon
onereplicationranonMacBookProwith2.7GHzIntelcore
i7CPUand16GBRAM.Weimplementthemethodsusing
publiclyavailablecode,eitherinR(1D-Haar,SHAH,and
AWS)orMatlab(TI-2D-Haar,CRP,Wedgelet,BPFA,and
RM).WARPisavailableinbothRandMatlab,andweuse
theRversiontotimeit.
Figure2presentstheaverageMSEsandMAEsofall
methodswhere
˙
variesfrom0.1to0.7.Wecansee
thattheproposedhierarchicaladaptivepartitionimproved
thebasicwaveletregression(compare1D-Haar
vs.WARP)forallscenarios.Infact,WARPisuniformlythe
bestmethodunderbothmetricsforallscenarios,withthe
performanceleadoverothermethodswideningasthenoise
8
(a)MSE(

10

3
)(b)MAE(

10

2
)
Fig.2.Comparisonofvariousmethodsbasedon100randomlyselected
512

512
imagesfromImageNet.Themethodofrunningmedianis
offthechart(notplottedhere).Themaximumstandarderrorsateach
˙
amongallmethodsare
(0
:
001
;
0
:
042
;
0
:
071
;
0
:
058)

10

3
forMSE,
(0
:
002
;
0
:
062
;
0
:
065
;
0
:
058)

10

2
forMAE,respectively.Therunning
timeofeachmethodinsecondsis7.2(WARP),76.9(SHAH),7.9
(AWS),10.7(CRP),8.7(Wedgelet),
2
:
1

10
3
(BPFA),andlessthan
1(1D-Haar,TI-2D-Haar,RM),basedononetestimagewithoutcycle
spinningat
˙
=0.3includingbothtuningandestimationsteps.
levelincreases.ThesensitivityanalysisintheSupplemen-
taryMaterialsindicatesthatthemethodofWARPisrobust
tohyperparametersandchoicesof

.
WARPiscomputationallyeffromthe
conjugacyofrandomrecursivepartitionandclosedform
expressioninTheorem3.WARPisthefastestadaptive
approachamongSHAH,AWS,CRP,Wedgelet,andBPFA.
(ThecomputingtimesaregiveninthecaptionofFigure2.)
Section3.2furthercomparesthescalabilityofselectedmeth-
odsusingimagesofvarioussizes.
3.2Scalability
NextweverifythelinearcomplexityoftheWARPframe-
workusingboth2Dand3Dimages.Usuallythereare
variouswaystotuneeachmethod,andwefocusonthe
estimationstepgiventuningparametersforallmethods
tomakeafaircomparison.ForWARP,oneactuallymay
choosethetuningparametersfromasmallerimageby
downsamplingwithoutlossofmuchaccuracy,inviewof
itsinsensitivitytohyperparameters(SectionDintheSup-
plementaryMaterials).
Figure4(a)comparesthescalabilityofselectedmethods
(a)true(b)WARPvs1D-DWT(c)WARPvs2D-DWT
Fig.3.ComparisonofenergyconcentrationforthreemethodsŠWARP,1DHaar,and2DHaarŠonImageNetimages.Column(a)plotsthetrue
image,Column(b)comparesWARPversus1DDWT,andColumn(c)comparesWARPversus2DDWT.InColumns(b)and(c),theredandblue
linescorrespondtotheright
y
axis,plottingthenumberoftoattainaenergylevel(
x
axis)bydeterministicDWTandWARP,
respectively.Theblackcurvecorrespondstotheleft
y
axisandis100%lesstheratiooftheblueandredcurves,indicatingthepercentagereduction
inthenumberofwavelettoachievethesamesumofsquaresbyWARP.
9
TABLE1
MSE(

10

3
)ofWARPandDnCNNon12widelyusedtestimagesandBSD68.
˙
12widelyusedtestimagesBSD68
123456789101112
0
:
2
WARP2.891.422.494.073.653.393.171.694.332.552.502.593.75
DnCNN2.771.652.553.372.903.492.881.733.982.442.332.683.35
0
:
4
WARP5.693.155.577.978.216.016.643.156.384.464.164.676.10
DnCNN15.6414.1415.3915.9315.8516.5615.5313.2815.3714.5713.4914.6914.90
0
:
6
WARP8.234.318.1210.8612.268.369.574.407.966.065.666.227.88
DnCNN75.8371.1273.3473.4971.7377.0675.1670.3171.3172.7569.9671.4671.65
(a)2Dimages(b)3Dimages
Fig.4.Scalabilityofvariousmethodsusing2Dand3Dimages.Each
lineistherunningtimetakenbytheestimationstep(
y
-axis)usingthe
correspondingmethodversusthenumberoflocationsintheimage(
x
-
axis).
inFigure2;weexclude1D-HaarandRMastheirreconstruc-
tionsarehighlyinaccurate,andBPFAasitscalespoorly
evenat
512

512
images.Wecanseethattheempirical
runningtimeapproximatelyfollowsalinearfunctionofthe
numberoflocations.Infact,WARPtakesonlyabout2min-
utesforalargeimageof
4096

4096
thatcontains17million
pixels,and5.3secondsforanimageof1024by1024.Figure4
suggeststhatWedgeletandSHAHtakequadratictimeor
evenmore,whileTI-2D-Haar,AWS,andCRPtakeslinear
time,buttheirperformancesaresubstantiallyinferiortothat
ofWARPasshowninFigure2.CRPseemstohaveasmaller
slopethanWARP,butitrequiresconsiderablylongertuning
timethanWARPaccordingtothetotalrunningtimewith
thetuningstepincludedinthecaptionofFigure2,atleast
basedonitslatestversionofimplementationtodate.
Itisworthnotingthatwhilemanystate-of-the-artmeth-
odsdesignedfor2DimagessuchasWedgelet,TI-2D-Haar,
andBPFArequiresubstantialforanewdi-
mensionalsetting,suchas3Dimages,theproposedWARP
frameworkisdirectlyapplicableto
m
-dimensionaldata
withoutwiththesamelinearscalabilityas
suggestedbyFigure4(b).
3.3Comparisonwithdeepneuralnetworks
Inthissection,wecomparetheproposedmethodWARP
withconvolutionalneuralnetworks.Inparticular,weapply
WARPandthedenoisingconvolutionalneuralnetworks
(DnCNN)proposedin[37]totwopopularbenchmark
datasets:thetwelvewidelyusedtestimages(Figure7in
SupplementaryMaterials)andtheBSD68data[38]which
contains68naturalimagesfromtheBerkeleysegmentation
dataset.DnCNNshavebeenreportedtoachievethestate-
of-the-artperformanceinvariousimageprocessingtasks
[37].Weadoptapre-trainedmodelavailableinMatlabfor
DnCNN.
Table1reportstheMSEsofWARPandDnCNNonthe
12widelyusedtestimagesandBSD68(averaged)atthree
noiselevelswhen
˙
=0
:
2
;
0
:
4
;
0
:
6
.Wecanseethatfor
lightnoisewhen
˙
=0
:
2
,WARPleadstosmallerMSEs
inoutoftwelveimages(i.e.,Image2,3,6,8,12)
andgivescomparableperformanceinotherimages.WARP
givesuniformlysmallerMSEswhen
˙
=0
:
4
(intermediate
noise)andconstantlyoutperformsDnCNNbyoneorderof
magnitudewhen
˙
=0
:
6
(largenoise),whichisconsistent
withourobservationsintheImageNetexperiment.Besides
theexcellentperformanceofWARP,itisworthmention-
ingthatunlikeDnCNNwhichrequiressubstantiallymore
extensivepre-trainingandtuning,WARPdoesnotrequire
pre-trainingatall,anditssmallamountoftuningcanbe
completelyautomatedwithoutuserintervention.Wedoac-
knowledgethattheperformanceofthepre-trainedDnCNN
mightbeimprovedwithmoreextensivetraining.
4E
NHANCEDENERGYCONCENTRATIONANDBE
-
YOND
2D
IMAGERECONSTRUCTION
TheexcellentperformanceofWARPinimagereconstruction
suggeststhatthemodeliscapableofidentifyingef
representationoftheunderlyingstructureinavarietyof
realimagesasitisdesignedtoachieve.Thisalsosuggests
thatextractingtheunderlyingrepresentationcanpotentially
avarietyofotherdownstreamprocessingtasks.In
thissectionweuseaconceptofﬁenergyconcentrationﬂ
toexaminehowsuchefisachievedandthendiscuss
thepotentialapplicabilityinotherimageprocessingtasks
suchascompression.
Energy(orinformation)concentrationunderwavelet
transformscanbebythenumberofwavelet
coefneededtoretainagivenproportionofthesum
ofsquaresoftheunderlyingfunction.Anefwavelet
representationwillonlyneedasmallnumberofcoef
tocapturemostoftheinformationcontainedinthefunction
(asmeasuredintermsofitssumofsquares).Sucharep-
resentationleadstohighsignal-to-noiseratiosonasmall
numberofcoefthatwillfacilitatealldownstream
processingtasks.
NextwecompareenergyconcentrationunderWARPto
thatunderclassical1Dand2Dwaveletrepresentationsto
quantifytheimprovementinenergyconcentrationWARP
achievesthroughadaptivelyidentifyinggoodpermutations.
Tothisend,weusethesameImageNetdataasusedinSec-
tion3.Foreachimage,wedrawasamplefromtheposterior
distributionofpartitiontreesproducedbyWARP,andcom-
putethenumberofcoefrequiredtoattainarange
10
ofenergylevels(i.e.,thetotalsumofsquares)onanoisy
observationat
˙
=0
:
1
andcomparethemtothoserequired
understandard1Dand2Dwavelettransforms.Figure3
presentsthenumbersofwaveletcoefrequiredover
theproportionofthesumofsquaresforthreerepresentative
images.
Focusingontheproportionofthesumofsquaresfrom
0.85to0.95,wecanseethattheadaptiverepresentation
achievedbyWARPrequiressubstantiallyfewernumbersof
waveletcoef(theredsolidlineswithscalesonthe
rightofeachplot)toattainthesameenergylevelthantra-
ditional1Dand2DHaarDWT(thebluedashedlineswith
scalesontherightofeachplot).InFigure3,wealsoplotted
thepercentagereductioninthenumberofcoef(the
blacklinewithscalesontheleftofeachplot)ateachenergy
level.ThelargestcoefsavingofWARPis(80%,70%,
70%)comparedto2DDWT,andthissavingbecomes(97%,
99%,90%)whencomparedto1DDWT.Enhancedenergy
concentrationofWARPisobservedinawiderangeoftest
imagesinthedatabase,andtheextentofimprovementin
energyconcentrationvariesaccordingtotheabundanceof
asymmetricstructurespresentintheunderlyingimage.
TheimprovedenergyconcentrationofWARPisex-
pectedtoavarietyofdownstreamprocessingtasks
beyondimagedenoising.Forexample,efimagecom-
pressioncanbeachievedusingtheposteriormodeofthe
WARPmodel,whichprovidesasparsecodingoftheimage.
Couplingthisideawithapairofencoderanddecoder,
weintroduceanalgorithmforefimageandvideo
compressioninafollow-uppaper[39].Interestedreaders
mayrefertothatpaperforadditionalnumericalexperi-
mentsinvolvingavarietyofdatasets,including2DIm-
ageNet,3Dmedicalimage,real-lifeYouTubevideos,and
surveillancevideos,inwhichWARP-basedcompression
substantiallyoutperformsseveralstate-of-the-artcompres-
sionapproaches.
5A
PPLICATIONTORETINALOPTICALCOHERENCE
TOMOGRAPHY
Weapplytheproposedmethodtoadatasetofopticalco-
herencetomography(OCT)volumes.OCTprovidesanon-
invasiveimagingmodalitytovisualizecross-sectionsoftis-
suelayersatmicrometerresolution,andthusisinstrumental
invariousmedicalapplicationsespeciallyforthediagnosis
andmonitoringofpatientswithoculardiseases[40],[41],
[42],[43].TheaccurateinterpretationofOCTimagesmay
requiretheinvolvementofbothretinaspecialistsandcom-
prehensiveophthalmologists,andthistaskiscompounded
byheavilynoisedobservationsatalowsignal-to-noiseratio
duetosample-basedspeckleanddetectornoise[44],[45],
[46].Therefore,reconstructionofOCTimagesisnecessaryto
improvebothmanualandautomatedOCTimageanalysis,
andisincreasinglyimportantwhenOCTimagesareused
toextractobjectiveandquantitativeassessmentinophthal-
mologywhichistoutedasoneadvantageofOCTinclinical
practice[42].
WeusetheOCTdataavailableathttp://people.duke.
edu/
˘
sf59/Fang
TMI
2013.htm,acquiredbyaBioptigen
SDOCTsystem(Durham,NC,USA)atanaxialresolution
of
˘
4.5

m.WeapplythemethodsofTI-2D-Haar,SHAH,
TABLE2
MeanPSNRfor18fovealimagesreconstructedbyBRFOE,K-SVD,
PGPD,BM3D,MSBTD,SSR,andWARP.Resultsforthemethodsother
thanWARParefrom[46].
BRFOEK-SVDPGPDBM3DMSBTDSSRWARP
25.3227.0327.0127.0427.0828.1028.18
AWS,CRP,Wedgelet,BPFA,andWARP,totwonoisyslices
(plottedasﬁObs.ﬂinFigure5).Wealsohaveaccesstoa
registeredandaveragedimagebyaveraging40repeatedly
sampledscans[46],whichisreferredtoastheﬁnoiselessﬂ
referenceimageandisusedtocomparethequalityofre-
constructedimages.FromtheresultsinFigure5,weclearly
seethatWARPgivesthebestglobalqualitativemetricusing
MSEandMAEamongallmethodsincomparison.
Visualcomparisonprovidesadetailedassessmentofre-
constructedimagesonlocalfeaturesthatmightbeclinically
relevant.FortheobservationinFigure5,wecansee
WARPdistinguishesalllayerswell(theboxedregionin
thenoiselessimage),especiallycomparedtoTI-2D-Haar
andAWSwhosereconstructionsareblurredacrosslayers.
Forthesecondobservation,weobserveaseparationof
theposteriorcorticalvitreousfromtheinternallimiting
membraneinthenoiselessimage,whichshowsthepotential
toprogresstovitreomaculartraction(VMT)[47].Thissep-
arationbecomeslessclearifusingTI-2D-Haar(especially
theleftproportion),althoughTI-2D-HaargivesMSEand
MAEthatareclosertoWARPthantheothermethods.For
bothobservations,thereisstillsubstantialnoiseleftinthe
denoisedimagesbySHAH,andAWSgivesareconstruction
exhibitingundesirablepatches.Thisstudythat
WARPiscapabletodenoiseimageswhilekeepingimpor-
tantfeaturespresentintheimage,duetoitsabilitytoadapt
tothegeometryoftheunderlyingstructures.
WefurthercompareWARPwithastudyconducted
in[46],whichconsidersanothersixmethod:BRFOE[48],K-
SVD[49],PGPD[50],BM3D[51],MSBTD[52],andSSR[46].
Thesesixmethodshavebeenappliedto18fovealimages
from18subjects,usingfourslicesnearbytheoriginalobser-
vationatvariousstagesoftheirimplementation.Although
WARPdoesnotrequirenearbyinformationandcaneven
processa3Dvolumeifsuchdataexist,weapplyWARP
totheobservationthataveragestheoriginalobservation
andthefournearbyslicesonlytomakeafaircomparison.
InTable2,weadoptthemeanofpeaksignal-to-noise
ratio(PSNR)forallmethodstoalignwith[46],whichis
calculatedas

10log
10
(
MSE
)
(notingthatwerescaleall
observationsandnoiselessgray-scaleimagesby255).We
canseethatWARPgivesthelargestmeanofPSNR,thus
achievesexcellentperformancescomparedtoawiderange
ofexistingmethodsinthisapplicationsetting.Wechoose
thetwosubjectsconsideredinFigure5,andplottherecon-
structedimagesbyWARPutilizingthenearbyfourslices
inFigure6.ItsuggeststhatWARPevenhasanenhanced
displaycomparedtotheﬁnoiselessﬂimage,especiallyinthe
lowerhalfoftheimage.
11
6D
ISCUSSION
WehaveintroducedtheWARPframeworkthatusesrandom
recursivepartitioningtoinduceaprioronthepermutations
oftheindexspace,therebyachievingefinferenceon
multi-dimensionalfunctionsbyconvertingitintoaBayesian
modelchoiceprobleminvolvingone-dimensionalcompeti-
tivegenerativemodels.WhileourapproachisBayesian,one
mayconsiderothermethodssuchasfrequentistadaptive
partitioningandshrinkagemethodsthatincorporatethe
sameidea.Wedosatisfyingthefullyprincipledproba-
bilisticinferentialrecipesthatariseunderourapproach.
TheproposedframeworkWARPcanbeappliedalong
withawiderrangeofBayeswaveletregressionmodels,
includingthosethatallowheterogeneousnoiselevels.Ifthe
error

inModel(1)hasgeneralcovariancematrix


,it
oftenstillmakessensetoassumethatthecovarianceofthe
error
u
inthewaveletdomain,i.e.
W


W
0
,isdiagonal,due
totheso-calledwhiteningpropertyofwavelettransforms
discussedin[53].Inthiscase,let
˙
2
j
=
Var
(
u
j;k
)
foreach
j
.Thenonemayestimate
˙
2
j
usingarobustestimator
ofthescalebasedon
f
w
j;k
;
0

k

2
j

1
g
givena
tree,forexample,usingthemedianabsolutedeviationof
f
w
j;k
;
0

k

2
j

1
g
rescaledby0.6745.Alternatively,
onecanadoptahyperprioronlocation-basedunknown
variance
˙
2
j
˘
IG
(

+1
;˙
2
0
)
,whichisaninversegamma
priorwithshape

+1
andscale
˙
2
0
(thusthepriormean
is
˙
2
0
).Thehyperparameters
(
;˙
2
0
)
areeitherby
usersorestimatedusingdata,forinstance,wemayestimate
˙
2
0
bythemedianestimatebasedonthescalewavelet
coef[8].
Finally,whileweintroducetheWARPframeworkinthe
contextofimagedenoising,webelievethattheadaptive
waveletrepresentationisapplicabletoawiderangeofother
tasksinvolvingmulti-dimensionalsignalprocessing.
A
CKNOWLEDGMENTS
WeareverygratefultoanAEandthreereviewersfor
providingextremelyhelpfulcommentsandsuggestions.We
alsothankDanielBourgeoisforhishelpinportingourC++
codetoR.MengLi'sresearchispartlysupportedbyNSF
grantDMS-2015569andanORAURalphE.PoweJunior
FacultyEnhancementAward.LiMa'sresearchispartly
supportedbyNSFgrantsDMS-1749789andDMS-2013930.
S
UPPLEMENTARY
M
ATERIALS
SupplementarymaterialscontainProposition1andits
proof;descriptionsofWARPwithlocalblockshrinkage;
detailsoftherecursivemessagepassingalgorithm;proofs
ofalltheorems;asensitivityanalysisfortheproposed
framework;plotsofthe12widelyusedtestimagesusedin
Section3.3;andcomparisonofWARPandselectedmethods
usingexperimentsof3Dimagereconstruction.
R
EFERENCES
[1]
T.Alasil,P.A.Keane,J.F.Updike,L.Dustin,Y.Ouyang,A.C.
Walsh,andS.R.Sadda,ﬁRelationshipbetweenopticalcoherence
tomographyretinalparametersandvisualacuityindiabeticmac-
ularedema,ﬂ
Ophthalmology
,vol.117,no.12,pp.2379Œ2386,2010.
[2]
I.I.Bussel,G.Wollstein,andJ.S.Schuman,ﬁOctforglaucomadi-
agnosis,screeninganddetectionofglaucomaprogression,ﬂ
British
JournalofOphthalmology
,pp.bjophthalmolŒ2013,2013.
[3]
W.C.Huang,A.V.Cideciyan,A.J.Roman,A.Sumaroka,R.She-
plock,S.B.Schwartz,E.M.Stone,andS.G.Jacobson,ﬁInner
andouterretinalchangesinretinaldegenerationsassociated
withabca4mutations,ﬂ
Investigativeophthalmology&visualscience
,
vol.55,no.3,pp.1810Œ1822,2014.
[4]
J.K.Sun,M.M.Lin,J.Lammer,S.Prager,R.Sarangi,P.S.Silva,
andL.P.Aiello,ﬁDisorganizationoftheretinalinnerlayersas
apredictorofvisualacuityineyeswithcenter-involveddiabetic
macularedema,ﬂ
JAMAophthalmology
,vol.132,no.11,pp.1309Œ
1316,2014.
[5]
R.H.Rabbani,F.Hajizadeh,M.D.Abramoff,and
M.Sonka,ﬁThicknessmappingofelevenretinallayerssegmented
usingthediffusionmapsmethodinnormaleyes,ﬂ
Journalof
ophthalmology
,vol.2015,2015.
[6]
A.Oishi,P.P.Fang,S.Thiele,F.G.Holz,andT.U.Krohne,
ﬁLongitudinalchangeofouternuclearlayerafterretinalpigment
epithelialtearsecondarytoage-relatedmaculardegeneration,ﬂ
Retina
,vol.38,no.7,pp.1331Œ1337,2018.
[7]
D.L.DonohoandI.M.Johnstone,ﬁIdealspatialadapatationby
waveletshrinkage,ﬂ
Biometrika
,vol.81,no.3,pp.425Œ455,1994.
[8]
ŠŠ,ﬁAdaptingtounknownsmoothnessviawaveletshrinkage,ﬂ
J.Am.Statist.Ass.
,vol.90,no.432,pp.1200Œ1224,1995.
[9]
S.Mallat,
AWaveletTourofSignalProcessing:TheSparseWay
,
3rded.Academicpress,2008.
[10]
F.Abramovich,T.Sapatinas,andB.W.Silverman,ﬁWavelet
thresholdingviaaBayesianapproach,ﬂ
J.R.Statist.Soc.B
,vol.60,
no.4,pp.725Œ749,1998.
[11]
M.S.Crouse,R.D.Nowak,andR.G.Baraniuk,ﬁWavelet-based
statisticalsignalprocessingusinghiddenMarkovmodels,ﬂ
IEEE
TransactionsonSignalProcessing
,vol.46,no.4,pp.886Œ902,1998.
[12]
M.ClydeandE.I.George,ﬁFlexibleempiricalBayesestimation
forwavelets,ﬂ
J.R.Statist.Soc.B
,vol.62,no.4,pp.681Œ698,2000.
[13]
P.J.Brown,T.Fearn,andM.Vannucci,ﬁBayesianwavelet
regressiononcurveswithapplicationtoaspectroscopic
calibrationproblem,ﬂ
J.Am.Statist.Ass.
,vol.96,no.454,pp.
398Œ408,jun2001.
[14]
R.WillettandR.Nowak,ﬁFastmultiresolutionphoton-limitedim-
agereconstruction,ﬂin
IEEEInternationalSymposiumonBiomedical
Imaging:NanotoMacro,2004
,2004,pp.1192Œ1195.
[15]
J.S.MorrisandR.J.Carroll,ﬁWavelet-basedfunctionalmixed
models,ﬂ
J.R.Statist.Soc.B
,vol.68,no.2,pp.179Œ199,apr2006.
[16]
D.L.Donoho,ﬁWedgelets:Nearlyminimaxestimationofedges,ﬂ
Ann.Statist.
,vol.27,no.3,pp.859Œ897,1999.
[17]
L.Jacques,L.Duval,C.Chaux,andG.Peyr
´
e,ﬁApanoramaon
multiscalegeometricrepresentations,intertwiningspatial,direc-
tionalandfrequencyselectivity,ﬂ
SignalProcessing
,vol.91,no.12,
pp.2699Œ2730,2011.
[18]
S.T.Ali,J.-P.Antoine,andJ.-P.Gazeau,
CoherentStates,Wavelets
andTheirGeneralizations
,2nded.Springer,2014.
[19]
A.E.Raftery,ﬁBayesianmodelselectioninsocialresearch,ﬂ
Socio-
logicalMethodology
,vol.25,pp.111Œ163,1995.
[20]
J.A.Hoeting,D.Madigan,A.E.Raftery,andC.T.Volinsky,
ﬁBayesianmodelaveraging:atutorial,ﬂ
Statist.Sci.
,vol.14,no.4,
pp.382Œ417,111999.
[21]
W.H.WongandL.Ma,ﬁOptionalP
´
olyatreeandBayesian
inference,ﬂ
Ann.Statist.
,vol.38,no.3,pp.1433Œ1459,2010.
[22]
L.Ma,ﬁAdaptivetestingofconditionalassociationthroughrecur-
sivemixturemodeling,ﬂ
J.Am.Statist.Ass.
,vol.108,no.504,pp.
1493Œ1505,2013.
[23]
H.A.Chipman,E.I.George,andR.E.McCulloch,ﬁBayesian
CARTmodelsearch,ﬂ
J.Am.Statist.Ass.
,vol.93,no.443,pp.
935Œ948,sep1998.
[24]
D.G.T.Denison,B.K.Mallick,andA.F.M.Smith,ﬁABayesian
CARTalgorithm,ﬂ
Biometrika
,vol.85,no.2,pp.363Œ377,1998.
[25]
I.M.JohnstoneandB.W.Silverman,ﬁEmpiricalBayesselection
ofwaveletthresholds,ﬂ
Ann.Statist.
,vol.33,no.4,pp.1700Œ1752,
2005.
[26]
H.A.Chipman,E.D.Kolaczyk,andR.E.McCulloch,ﬁAdaptive
Bayesianwaveletshrinkage,ﬂ
J.Am.Statist.Ass.
,vol.92,no.440,
pp.1413Œ1421,dec1997.
[27]
L.MaandJ.Soriano,ﬁEffunctionalANOVAthrough
wavelet-domainMarkovgroves,ﬂ
J.Am.Statist.Ass.
,2017,to
appear,DOI:10.1080/01621459.2017.1286241.
12
[28]
T.T.Cai,ﬁAdaptivewaveletestimation:Ablockthresholding
andoracleinequalityapproach,ﬂ
Ann.Statist.
,vol.27,no.3,pp.
898Œ924,jun1999.
[29]
J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei,ﬁIm-
agenet:Alarge-scalehierarchicalimagedatabase,ﬂin
2009IEEE
ConferenceonComputerVisionandPatternRecognition
,2009,pp.
248Œ255.
[30]
P.FryzlewiczandC.Timmermans,ﬁSHAH:SHape-AdaptiveHaar
waveletsforimageprocessing,ﬂ
JournalofComputationalandGraph-
icalStatistics
,vol.25,no.3,pp.879Œ898,2016.
[31]
J.PolzehlandV.G.Spokoiny,ﬁAdaptiveweightssmoothingwith
applicationstoimagerestoration,ﬂ
J.R.Statist.Soc.B
,vol.62,no.2,
pp.335Œ354,2000.
[32]
M.LiandS.Ghosal,ﬁBayesianmultiscalesmoothingofGaussian
noisedimages,ﬂ
BayesianAnalysis
,vol.9,no.3,pp.733Œ758,2014.
[33]
R.Castro,R.Willett,andR.Nowak,manifold
learning,ﬂin
IEEEInternationalConferenceonAcoustics,Speech,and
SignalProcessing,2004(ICASSP'04)
,2004.
[34]
M.Zhou,H.Chen,J.Paisley,L.Ren,L.Li,Z.Xing,D.Dunson,
G.Sapiro,andL.Carin,ﬁNonparametricBayesiandictionary
learningforanalysisofnoisyandincompleteimages,ﬂ
IEEE
TransactionsonImageProcessing
,vol.21,no.1,pp.130Œ144,2012.
[35]
R.CoifmanandD.Donoho,ﬁTranslation-invariantde-noising,ﬂ
in
WaveletsandStatistics
,ser.LectureNotesinStatistics,A.Anto-
niadisandG.Oppenheim,Eds.SpringerNewYork,1995,vol.
103,pp.125Œ150.
[36]
M.LiandS.Ghosal,ﬁFasttranslationinvariantmultiscaleimage
denoising,ﬂ
IEEETransactionsonImageProcessing
,vol.24,no.12,
pp.4876Œ4887,2015.
[37]
K.Zhang,W.Zuo,Y.Chen,D.Meng,andL.Zhang,ﬁBeyond
aGaussiandenoiser:ResiduallearningofdeepCNNforimage
denoising,ﬂ
IEEETransactionsonImageProcessing
,vol.26,no.7,
pp.3142Œ3155,2017.
[38]
D.Martin,C.Fowlkes,D.Tal,andJ.Malik,ﬁAdatabaseofhu-
mansegmentednaturalimagesanditsapplicationtoevaluating
segmentationalgorithmsandmeasuringecologicalstatistics,ﬂin
Proc.8thInt'lConf.ComputerVision
,vol.2,July2001,pp.416Œ423.
[39]
R.Liu,M.Li,andL.Ma,ﬁCARP:CompressionThroughAdaptive
RecursivePartitioningforMulti-DimensionalImages,ﬂin
2020
IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR)
.IEEE,jun2020,pp.14294Œ14302.
[40]
D.Huang,E.A.Swanson,C.P.Lin,J.S.Schuman,W.G.Stinson,
W.Chang,M.R.Hee,T.Flotte,K.Gregory,C.A.
etal.
,
ﬁOpticalcoherencetomography,ﬂ
Science
,vol.254,no.5035,pp.
1178Œ1181,1991.
[41]
D.S.GrewalandA.P.Tanna,ﬁDiagnosisofglaucomaand
detectionofglaucomaprogressionusingspectraldomainoptical
coherencetomography,ﬂ
Currentopinioninophthalmology
,vol.24,
no.2,pp.150Œ161,2013.
[42]
G.Virgili,F.Menchini,G.Casazza,R.Hogg,R.R.Das,X.Wang,
andM.Michelessi,ﬁOpticalcoherencetomography(oct)fordetec-
tionofmacularoedemainpatientswithdiabeticretinopathy,ﬂ
The
Cochranedatabaseofsystematicreviews
,vol.1,p.CD008081,2015.
[43]
N.Cuenca,I.Ortu
Ÿ
no-Lizar
´
an,andI.Pinilla,ﬁCellularcharacteri-
zationofoctandouterretinalbandsusingimmunohisto-
chemistrymarkersandclinicalimplications,ﬂ
Ophthalmology
,vol.
125,no.3,pp.407Œ422,2018.
[44]
P.A.Keane,P.J.Patel,S.Liakopoulos,F.M.Heussen,S.R.
Sadda,andA.Tufail,ﬁEvaluationofage-relatedmaculardegener-
ationwithopticalcoherencetomography,ﬂ
Surveyofophthalmology
,
vol.57,no.5,pp.389Œ414,2012.
[45]
F.Shi,X.Chen,H.Zhao,W.Zhu,D.Xiang,E.Gao,M.Sonka,
andH.Chen,ﬁAutomated3-dretinallayersegmentationofmac-
ularopticalcoherencetomographyimageswithserouspigment
epithelialdetachments.ﬂ
IEEETrans.Med.Imaging
,vol.34,no.2,
pp.441Œ452,2015.
[46]
L.Fang,S.Li,D.Cunefare,andS.Farsiu,ﬁSegmentation
BasedSparseReconstructionofOpticalCoherenceTomography
Images,ﬂ
IEEETransactionsonMedicalImaging
,vol.36,no.2,pp.
407Œ421,feb2017.
[47]
J.S.Duker,P.K.Kaiser,S.Binder,M.D.DeSmet,A.Gaudric,
E.Reichel,S.R.Sadda,J.Sebag,R.F.Spaide,andP.Stalmans,ﬁThe
internationalvitreomaculartractionstudygroupof
vitreomacularadhesion,traction,andmacularhole,ﬂ
Ophthalmol-
ogy
,vol.120,no.12,pp.2611Œ2619,2013.
[48]
Y.WeissandW.T.Freeman,ﬁWhatmakesagoodmodelofnatural
images?ﬂin
2007IEEEConferenceonComputerVisionandPattern
Recognition
.IEEE,2007,pp.1Œ8.
[49]
M.EladandM.Aharon,ﬁImagedenoisingviasparseandredun-
dantrepresentationsoverlearneddictionaries,ﬂ
IEEETransactions
onImageprocessing
,vol.15,no.12,pp.3736Œ3745,2006.
[50]
J.Xu,L.Zhang,W.Zuo,D.Zhang,andX.Feng,ﬁPatchgroup
basednonlocalself-similaritypriorlearningforimagedenoising,ﬂ
in
ProceedingsoftheIEEEinternationalconferenceoncomputervision
,
2015,pp.244Œ252.
[51]
K.Dabov,A.Foi,V.Katkovnik,andK.Egiazarian,ﬁImagedenois-
ingbysparse3-dtransform-domaincollaborative
IEEE
Transactionsonimageprocessing
,vol.16,no.8,pp.2080Œ2095,2007.
[52]
L.Fang,S.Li,Q.Nie,J.A.Izatt,C.A.Toth,andS.Farsiu,ﬁSparsity
baseddenoisingofspectraldomainopticalcoherencetomography
images,ﬂ
Biomedicalopticsexpress
,vol.3,no.5,pp.927Œ942,2012.
[53]
I.M.JohnstoneandB.W.Silverman,ﬁWaveletthreshold
estimatorsfordatawithcorrelatednoise,ﬂ
J.R.Statist.Soc.B
,
vol.59,no.2,pp.319Œ351,1997.
[54]
A.V.AhoandN.J.Sloane,ﬁSomedoublyexponentialsequences,ﬂ
FibonacciQuart
,vol.11,no.4,pp.429Œ437,1973.
[55]
P.MukherjeeandP.Qiu,ﬁ3-dimagedenoisingbylocalsmoothing
andnonparametricregression,ﬂ
Technometrics
,vol.53,no.2,pp.
196Œ208,2011.
[56]
P.PeronaandJ.Malik,ﬁScale-spaceandedgedetectionusing
anisotropicdiffusion,ﬂ
IEEETransactionsonPatternAnalysisand
MachineIntelligence
,vol.12,no.7,pp.629Œ639,1990.
[57]
L.Rudin,S.Osher,andE.Fatemi,ﬁNonlineartotalvariation
basednoiseremovalalgorithms,ﬂ
PhysicaD:NonlinearPhenomena
,
vol.60,no.1,pp.259Œ268,1992.
[58]
P.Coup
´
e,P.Yger,S.Prima,P.Hellier,C.Kervrann,andC.Barillot,
ﬁAnoptimizedblockwisenonlocalmeansdenoisingfor3-d
magneticresonanceimages,ﬂ
IEEETransactionsonMedicalImaging
,
vol.27,no.4,pp.425Œ441,2008.
13
Fig.5.TworetinalOCTdatasets(titledﬁObs.ﬂ)andreconstructedimagesusingTI-2D-Haar,SHAH,AWS,CRP,Wedgelet,BPFA,andWARP.The
twometricsfollowingeachmethodaretheMSE(

10

4
)
andMAE(

10

2
)
respectively.Theﬁnoiselessﬂreferenceisanregisteredandaveraged
image.
Obs.(152.4,9.9)TI-2D-Haar(7.3,2.0)SHAH(15.7,2.7)
AWS(9.4,2.2)CRP(7.4,2.1)Wedgelet(7.7,2.1)
BPFA(7.8,2.2)WARP(6.5,1.9)ﬁnoiselessﬂ
Obs.(159.4,10.1)TI-2D-Haar(10.9,2.4)SHAH(18.4,3.0)
AWS(12.9,2.5)CRP(11.5,2.5)Wedgelet(11.1,2.4)
BPFA(11.7,2.6)WARP(10.2,2.3)ﬁnoiselessﬂ
14
Fig.6.ReconstructedimagesusingWARPbasedonthenoisyobservationanditsfournearbyslices.Thetwometricsfollowingeachmethodare
theMSE(

10

4
)
andMAE(

10

2
)
respectively.Theﬁnoiselessﬂreferenceisanregisteredandaveragedimage.
WARP(6.6,2.0)ﬁnoiselessﬂ
WARP(10.0,2.3)ﬁnoiselessﬂ
S-1
SupplementaryMaterialsforﬁLearning
AsymmetricandLocalFeaturesin
Multi-DimensionalDatathrough
WaveletswithRecursivePartitioningﬂ
Supplementarymaterialscontain(A)Proposition1and
itsproof,(B)descriptionsofWARPwithlocalblockshrink-
age,(C)detailsoftherecursivemessagepassingalgorithm,
(D)proofsofalltheorems,(E)asensitivityanalysisfor
theproposedframework,(F)plotsofthe12widelyused
testimagesusedinSection3.3,and(G)comparisonof
WARPandselectedmethodsusingexperimentsof3Dimage
reconstruction.
AC
ARDINALITYOFTHESPACEOF
RDP
S
Proposition1.
Thelogcardinalityofthetreespaceinducedby
RDPsis
O
(
n
)
when
m
=2
.
ProofofProposition1.
Let
c
(
a;b
)
bethecardinalityofthetree
spaceinducedbyRDPsforan
2
a
by
2
b
image.Wecanobtain
thefollowingrecursiveformula
c
(
a;b
)=
(
c
2
(
a

1
;b
)+
c
2
(
a;b

1)
;
if
a;b

1
1
if
a
=0
or
b
=0
:
Weassertthatthereexisttwoconstants
(
k
1
;k
2
)
suchthat
k
2

k
1
>
1
and
c
(
a;b
)
2

1
2
k
2
a
+
b
1
;
1
2
k
2
a
+
b
2

;
forany
a

1
and
b

1
.
Firstconsider
a
=1
and
b

1
.Wehave
c
(1
;b
)=
c
2
(1
;b

1)+1
when
b

1
and
c
(1
;
0)=1
when
b
=0
.The
quantity
c
(1
;b
)
isactuallythenumberofﬁstronglyﬂbinary
treesofheight

b
,whichpossessesananalyticalform
c
(1
;b
)=
b
k
2
b
c
;
accordingto[54],where
k
=exp
8
<
:
1
X
j
=0
2

j

1
log(1+
c

2
(1
;j
))
9
=
;
ˇ
1
:
503
:
Letting
k
1
=
p
k
and
k
2
=
k
andnoting
k
2
b

2
forall
b

1
,weobtainthat
1
2
k
2
1+
b
1
=
1
2
k
2
b

k
2
b

1
b
k
2
b
c
k
2
b

1
2
k
2
1+
b
;
forall
b

1
.Therefore,theassertionholdsforall
a
=1
and
b

1
.Since
c
(
a;b
)=
c
(
b;a
)
,theassertionalsoholdsforall
a

1
and
b
=1
.
Forany
a

1
and
b

1
,itiseasytoverifythatifthe
assertionholdsfor
(
a;b

1)
and
(
a

1
;b
)
,thenitholdsfor
(
a;b
)
.Wethencompletetheproofbyinduction.
BWARP
WITHLOCALBLOCKSHRINKAGE
Traditionalwaveletanalysisisdonebythemaximum
depthofthewavelettreeat
J
.Thatis,onepartitionsthe
indexspaceallthewaydowntothelevelofﬁatomicﬂ
blocks.Inmostpracticalproblems,oncetheblocksaresmall
enough,thefunctionvaluewithintheblockbecomesessen-
tiallyconstantwithrespecttothenoiselevel,andsofurther
divisionwithinsuchhomogeneousblockswillbewasteful
andwillreducestatisticalef.Forexample,inFigure1
thepartitionintheupperleftblock(Level3)alongwith
itsdescendantsisnotnecessary.Thusitisoftendesirable
toincorporateadaptivityinthedepthofthewavelettree
andallowittobeterminatedearlierthanreachinglevel
J
.
Inpracticetheoptimalmaximumdepthvariesacross

.
Forexample,somepartsofanimagemaycontainmany
interestingdetails,whiletherestdonotŠe.g.,animageof
apaintinghungonagraywall.Ahighresolutionwillbe
neededtocapturethedetailsinthepainting,butwould
beunnecessaryandintroduceadditionalvariabilityinthe
estimationforthewall.
Thisconsiderationiscloselyrelatedtotheideaofadap-
tiveblockshrinkage[28]inthefrequentistwaveletregres-
sionanalysis.Oncethereislittleevidenceforanyinteresting
structurewithinasubsetoftheindexspace,thenthefunc-
tionvaluewithinthatsubsetcanbeshrunktoaconstant.
Thatis,thewavelettreeisﬁprunedﬂthere.Nextweshow
thatsuchpruningcanbeachievedinahierarchicalmodel-
ingmanner,andtheresultingBayesianwaveletregression
modelisagaincompatiblewithourWARPframework.
Toachievesuchpruning,weintroduceanothersetof
latentvariables
R
=
f
R
j;k
:
j
=0
;
1
;:::;J

1
;k
=
0
;
1
;:::;
2
j

1
g
,where
R
j;k
=1
indicatesthatthetreeis
prunedatnode
(
j;k
)
.Nextwedescribeagenerativeprior
on
R
thatwillblendwellwiththeWARPframework.To
start,let
R
0
;
0
ind
˘
Bern(

0
;
0
)
andforall
j

1
,and
R
j;k
j
R
j

1
;
b
k=
2
c
ind
˘
(
Bern(

j;k
)
if
R
j

1
;
b
k=
2
c
=0
Bern(1)
if
R
j

1
;
b
k=
2
c
=1
:
Thatis,ifanode'sparenthasbeenpruned,thenitschildren
arealsoprunedbyconstruction.Fromnowon,weshall
refertothispriormodelon
R
asan
optionalpruning
(OP)
model[22],whichisbyasetof
pruningprobabilities

j;k
2
[0
;
1]
.Wewrite
R˘
OP(

)
.
Given
R
,wecanmodifyourprioron
S
torthe
effectofpruning.Forexample,insteadofanindependent
prioron
S
,wecannowgeneratethemasfollows
S
j;k
jR
ind
˘
(
Bern(
ˆ
j;k
)
if
R
j;k
=0
Bern(0)
if
R
j;k
=1
.
Thatis,ifthenodehasnotbeenpruned,thenwegenerate
S
j;k
fromtheindependentBernoulliasinthestandard
spike-and-slabsetup,butifthenodehasbeenpruned,then
byconstruction,wemusthave
S
j;k
=0
duetopruning.
Itisoftenreasonabletospecifythepriorshrinkageand
pruningprobabilitiesasfunctionsofthelevelintheRDP.
Thatis,
ˆ
j;k
=
ˆ
j
and

j;k
=

j
forall
k
.Inthe
notation,
ˆ
(
A
)=
ˆ
j
and

(
A
)=

j
forall
j
thnode
A
2A
.
Inthiscase,onecanshowthatthisjointmodelon
(
S
;
R
)
is
equivalenttoaMarkovtreemodelwiththreestates
intermsofthecombinationsof
(
S
j;k
;R
j;k
)=(1
;
0)
,(0,0),
S-2
or(0,1),andwiththecorrespondingtransitionmatrixfor
S
j;k
givenby
ˆ
j
=
2
4
ˆ
j
(1


j
)(1

ˆ
j
)(1


j
)

j
ˆ
j
(1


j
)(1

ˆ
j
)(1


j
)

j
001
3
5
:
ThisallowsustoderivetheposteriorfromTheorem2,and
carryoutinferenceaccordingly.,foreach
A
2A
,
let
p
0
(
A
)
bethemarginallikelihoodcontributedfromthe
waveletcoefin
A
anditsdescendantsif
A
ispruned,
i.e.,
p
0
(
A
)=
1
(
p
2
ˇ˙
2
)
j
A

1
exp
(

P
x
2
A
(
y
(
x
)


y
(
A
))
2
2
˙
2
)
;
where

y
(
A
)=
P
x
2
A
y
(
x
)
=
j
A
j
.If
A
2T
,thefollowingmaps
aredirectlyavailablefromTheorem2:

Themarginallikelihoodcontributionfromthedata
withinnode
A
if
A
isdividedindimension
d
:
M
d
(
A
)=
ˆ
(
A
)
M
(1)
d
(
A
)+(1

ˆ
(
A
))
M
(0)
d
(
A
);

Theposteriorspikeprobability
~
ˆ
d
of
A
if
A
isdivided
indimension
d
:
~
ˆ
d
(
A
)=
ˆ
(
A
)
M
(1)
d
(
A
)
=M
d
(
A
);

Themarginallikelihoodfromdataon
A
anditsdescendants:

A
)=(1


(
A
))
P
d
2D
(
A
)

d
(
A
)
M
d
(
A

A
(
d
)
l

A
(
d
)
r
)+

(
A
)
p
0
(
A
)
if
A
isnon-atomic;

A
)=1
if
A
isatomic.

Theposteriorprobabilityofpruning
A
:
~

(
A
)=

(
A
)
p
0
(
A
)
=

A
);

Theposteriorprobabilityfor
A
tobedividedin
dimension
d
given
A
isnotpruned:
~

d
(
A
)=(1


(
A
))

d
(
A
)
M
d
(
A

A
(
d
)
l

A
(
d
)
r
)

A
)


(
A
)
p
0
(
A
)
:
CR
ECURSIVEMESSAGEPASSINGALGORITHM
FortheHaarbasis,theposteriormean
E(
f
j
y
)
canbe
evaluatedanalyticallythroughrecursivemessagepassing
withoutanyMonteCarlosamplingforBayesianwavelet
regressionmodelsthatadoptthespike-and-slabsetupalong
withoptionalpruningofthewavelettree,whichcontains
themodelswithoutoptionalpruningasspecialcaseswith
zeropruningprobabilities.Wedescribethestrategynext
andwilluseittocompute
E(
f
j
y
)
inournumericalexam-
ples.
Foreach
A
2A
,let
c
(
A
)
bethescale(fatherwavelet)co-
efon
A
if
A
2T
,andlet
'
(
A
)=E(
c
(
A
)
1
f
A
2Tg
j
y
)
.
Notethat
E(
f
j
y
)
isgivenby
'
(
A
)
forallatomic
A
.To
computethemapping
'
,weintroducetwoauxiliarymap-
pings
 
0
(
A
)=P(
A
2T
;R
(
A
)=0
j
y
)
and
'
0
(
A
)=
E(
c
(
A
)
1
f
A
2T
;R
(
A
)=0
g
j
y
)
:
Let

A
(
d
)
denotetheparentof
A
in
T
if
A
isachildnodeafterdividingitsparentinthe
d
th
dimension,andlet
P
(
A
)
ˆf
1
;
2
;:::;m
g
bethecollectionof
dimensionsof
A
thatdonothavefullsupport
[0
;
2
J
i

1]
,i.e.,
thosethathavebeenpartitionedatleastonceinprevious
levels.utingthetri-variatemapping
(
˚
0
;'
0
;'
):
A!
R
3
.
Theorem3.
Toinitiatetherecursion,for
A
=
,welet
 
0
(
A
)=1

~

(
A
)
;'
0
(
A
)=(1

~

(
A
))
j
A
j
=
p
n
,and
'
(
A
)=
j
A
j
=
p
n
.Supposewehaveevaluatedthesemappings
uptolevel
j

1
,forlevel
j
=1
;:::;J
,wehave
 
0
(
A
)=
X
d
2P
(
A
)
 
0
(

A
(
d
)
)
~

d
(

A
(
d
)
)(1

~

(
A
));
'
0
(
A
)=(1

~

(
A
))

X
d
2P
(
A
)
~

d
(

A
(
d
)
)
p
2

'
0
(

A
(
d
)
)

~
ˆ
d
(

A
(
d
)
)

1
(
w
d
(

A
(
d
)
))
 
0
(

A
(
d
)
)(

1)
1
(
A
istheleftchildof

A
(
d
)
)

;
'
(
A
)=
'
0
(
A
)
1

~

(
A
)
+
1
p
2
X
d
2P
(
A
)
f
'
(

A
(
d
)
)

'
0
(

A
(
d
)
)
g

d
(

A
(
d
)
)
:
Remark:Notethatthisrecursionistop-down(fromlowto
highresolutions),whereasthatforcomputing

isbottom-
up(fromhightolowresolutions).Thetwo-directionalre-
cursionsharesthespiritoftheforward-backwardalgorithm
forHMMs.
Oncewehavecomputedthemapping
(
'
0
; 
0
;'
):
A!
R
3
,theposteriormean
E(
f
j
y
)
isthengivenby
'
applied
ontheatomicnodes.Notethatthistheoremappliestothe
specialcasewithnopruningaswell.
DP
ROOFSOF
T
HEOREMS
ProofofTheorem1.
BecauseTheorem1canbeconsidereda
specialcasewithasinglelatentstate,itsprooffollowsimme-
diatelyfromthelattertheorem,whichweprovebelow.
ProofofTheorem2.
Firstweverifythatthemapping

s
(
A
)
isthemarginallikelihoodcontributedfromdatawithloca-
tionsin
A
,giventhat
A
2T
andthatthelatentstatevariable
associatedwiththeparentof
A
in
T
is
s
.Weshowthisby
induction.Firstnotethatif
A
isatomic,then

s
(
A
)=P(
y
(
A
)
j
A
2T
;S
(
A
p
)=
s
)=1
bydesignastherearenowaveletcoefassociated
withatomicnodes.Now,supposewehaveshownthat

s
(
A
)=P(
y
(
A
)
j
A
2T
;S
(
A
p
)=
s
)
forall
A
withlevel
higherthan
j
.Thenif
A
isoflevel
j
,itfollowsthat
P(
y
(
A
)
j
A
2T
;S
(
A
p
)=
s
)
=
X
s
0
X
d
P(
y
(
A
)
j
A
2T
;S
(
A
)=
s
0
;S
(
A
p
)=
s;D
(
A
)=
d
)

P(
S
(
A
)=
s
0
j
A
2T
;S
(
A
p
)=
s
)

P(
D
(
A
)=
d
j
A
2T
;S
(
A
p
)=
s
)
=
X
s
0
ˆ
j
(
s;s
0
)
X
d
2D
(
A
)

d
M
(
s
0
)
d
(
A

s
0
(
A
d
l

s
0
(
A
(
d
)
r
)
;
whichleadstotheof

s
(
A
)
inTheorem2.
Nextletusderivethejointmarginalposteriorof
(
T
;
S
)
.
Notethat
P(
S
j;k
=
s
0
j
S
j

1
;
b
k=
2
c
=
s;
T
(
j
)
;
y
)
=
P(
S
j;k
=
s
0
;S
j

1
;
b
k=
2
c
=
s;
y
(
A
)
jT
(
j
)
)
P(
S
j

1
;
b
k=
2
c
=
s;
y
(
A
)
jT
(
j
)
)
:
S-3
Nowwehave
P(
S
j;k
=
s
0
;D
j;k
=
d;
y
(
A
)
jT
(
j
)
;S
j

1
;
b
k=
2
c
=
s
)
=
ˆ
j
(
s;s
0
)

d
(
A
)
M
(
s
0
)
d
(
A

s
0
(
A
(
d
)
l

s
0
(
A
(
d
)
r
)
;
whichleadsto
P(
S
j;k
=
s
0
;
y
(
A
)
jT
(
j
)
;S
j

1
;
b
k=
2
c
=
s
)
=
ˆ
j
(
s;s
0
)
X
d

d
(
A
)
M
(
s
0
)
d
(
A

s
0
(
A
(
d
)
l

s
0
(
A
(
d
)
r
)
andfurthermore,
P(
S
j;k
=
s
0
j
S
j

1
;
b
k=
2
c
=
s;
T
(
j
)
;
y
)
=
ˆ
j
(
s;s
0
)
P
d

d
(
A
)
M
(
s
0
)
d
(
A

s
0
(
A
(
d
)
l

s
0
(
A
(
d
)
r
)
P
s
00
ˆ
j
(
s;s
00
)
P
d

d
(
A
)
M
(
s
00
)
d
(
A

s
00
(
A
(
d
)
l

s
00
(
A
(
d
)
r
)
;
wherethedenominatorisjust

s
(
A
)
.
Finally,
P(
D
j;k
=
d
j
S
j;k
=
s
0
;
T
(
j
)
;
y
)
=
P(
D
j;k
=
d;
y
(
A
)
j
S
j;k
=
s
0
;
T
(
j
)
)
P(
y
(
A
)
j
S
j;k
=
s
0
;
T
(
j
)
)
=

d
(
A
)
M
(
s
0
)
d
(
A

s
0
(
A
(
d
)
l

s
0
(
A
(
d
)
r
)
P
d
0

d
0
(
A
)
M
(
s
0
)
d
0
(
A

s
0
(
A
(
d
0
)
l

s
0
(
A
(
d
)
r
)
:
Thiscompletestheproof.
ProofofTheorem3.
Weobtaintherecursiverecipefor
computingthemaps
(
 
0
;'
0
)
followingTheorem1:
 
0
(
A
)
=
X
d
2P
(
A
)
P(

A
(
d
)
2T
;R
(

A
(
d
)
)=0
j
y
)
~

d
(

A
(
d
)
)(1

~

(
A
))
=
X
d
2P
(
A
)
 
0
(

A
(
d
)
)
~

d
(

A
(
d
)
)(1

~

(
A
))
;
and
'
0
(
A
)=E

c
(
A
)
1
f
A
2T
;R
(
A
)=0
g
j
y

=
X
d
2P
(
A
)
E

c
(
A
)
1
f

A
(
d
)
2T
;D
(

A
(
d
)
)=
d;R
(
A
)=0
g
j
y

=
X
d
2P
(
A
)
E

c
(
A
)
j

A
(
d
)
2T
;D
(

A
(
d
)
)=
d;R
(
A
)=0
;
y


P


A
(
d
)
2T
;D
(

A
(
d
)
)=
d;R
(
A
)=0
j
y

=(1

~

(
A
))
X
d
2P
(
A
)
~

d
(

A
(
d
)
)
p
2

'
0
(

A
(
d
)
)

~
ˆ
d
(

A
(
d
)
)

1
(
w
d
(

A
(
d
)
))
 
0
(

A
(
d
)
)

(

1)
1
(
A
istheleftchildof

A
(
d
)
)

:
(2)
Wenextderivetherecursiveformulafor
'
(
A
)
.Let
'
1
(
A
)=E(
c
(
A
)
1
f
A
2T
;R
(
A
)=1
g
j
y
)
,thenwehave
'
(
A
)=
E(
c
(
A
)
1
f
A
2Tg
j
y
)=
'
0
(
A
)+
'
1
(
A
)
.Notethat
'
(
A
)=
X
d
2P
(
A
)
E

c
(
A
)
1
f

A
(
d
)
2T
;D
(

A
(
d
)
)=
d
g
j
y

;
(3)
andforeach
d
2P
(
A
)
,wehave
E

c
(
A
)
1
f

A
(
d
)
2T
;D
(

A
(
d
)
)=
d
g
j
y

=
X
r
=0
;
1
E

c
(
A
)
1
f

A
(
d
)
2T
;D
(

A
(
d
)
)=
d;R
(

A
(
d
)
)=
r
g
j
y

=
X
r
=0
;
1
E

c
(
A
)
j

A
(
d
)
2T
;D
(

A
(
d
)
)=
d;R
(

A
(
d
)
)=
r;
y


P(

A
(
d
)
2T
;D
(

A
(
d
)
)=
d;R
(

A
(
d
)
)=
r
j
y
)
:
(4)
Forthesecondtermin(4),wehave
P(

A
(
d
)
2T
;D
(

A
(
d
)
)=
d;R
(

A
(
d
)
)=
r
j
y
)
=P(
D
(

A
(
d
)
)=
d
j

A
(
d
)
2T
;R
(

A
(
d
)
)=
r;
y
)

P(

A
(
d
)
2T
;R
(

A
(
d
)
)=
r
j
y
)
=
~

d
(

A
(
d
)
)
1

r

d
(

A
(
d
)
)
r
 
r
(

A
(
d
)
)
:
Forthetermin(4),itiseasytocheckthat
E

c
(
A
)
j

A
(
d
)
2T
;D
(

A
(
d
)
)=
d;R
(

A
(
d
)
)=
r;
y

=
8
>
>
>
>
>
<
>
>
>
>
>
:
1
p
2

'
0
(

A
(
d
)
)
 
0
(

A
(
d
)
)

~
ˆ
d
(

A
(
d
)
)

1
(
w
d
(

A
(
d
)
))

(

1)
1
(
A
istheleftchildof

A
(
d
)
)

if
r
=0
1
p
2
'
1
(

A
(
d
)
)
= 
1
(

A
(
d
)
)
if
r
=1
;
whereweusetheindependencebetween
c
(
A
)
and
D
(
A
)
given
A
2T
.Pluggingthetwotermsinto(4),weobtain
that
E

c
(
A
)
1
f

A
(
d
)
2T
;D
(

A
(
d
)
)=
d
g
j
y

=
1
p
2

'
0
(

A
(
d
)
)

~
ˆ
d
(

A
(
d
)
)
w
d
(

A
(
d
)
)
=
(1+
˝

1
j

1
)

(

1)
1
(
A
istheleftchildof

A
(
d
)
)

 
0
(

A
(
d
)
)

~

d
(

A
(
d
)
)
+
1
p
2
'
1
(

A
(
d
)
)

d
(

A
(
d
)
)
:
(5)
Combiningtheresultin(3)and(5),andcomparingit
with
'
0
(
A
)
in(2),weobtainthat
'
(
A
)=
'
0
(
A
)
=
(1

~

(
A
))+
1
p
2
X
d
2P
(
A
)
'
1
(

A
(
d
)
)

d
(

A
(
d
)
)
;
whichconcludestheproofbypluggingin
'
1
(

)=
'
(

)

'
0
(

)
.
ES
ENSITIVITYANALYSIS
Inthissection,weconductasensitivityanalysisforthe
proposedWARPframeworkatvariouschoicesofhyperpa-
rameters.
WeimplementthemethodofﬁWARP-fullﬂwhich
chooses
˚
byafulloptimizationofthemarginallikeli-
hoodusingtwosimulatedimages
(
f
1
;f
2
)
explicitlygiven
inSectionG.RecallthattherowofWARPselects
˚
ata
limitedsetofgridpoints.Table3showsthattheMSEs
ofWARP-fullarealmostidenticaltotherowofWARP.
Thisobservationisconsistentacrossmanyscenarioswe
havetested.Therefore,themethodofWARPseemsrobust
intermsofhyperparameters,andweshallrecommenda
S-4
TABLE3
AverageMSEs(

10

2
)ofWARP-fullandWARPbasedon100replicationsunderthesettingofTable5.
n
=64
n
=128
Method
f
=
f
1
f
=
f
2
f
=
f
1
f
=
f
2
˙
=0
:
10
:
2
˙
=0
:
10
:
2
˙
=0
:
10
:
2
˙
=0
:
10
:
2
WARP-full0.020.040.040.120.010.020.020.05
WARP0.020.040.040.110.010.020.020.05
TABLE4
SensitivityanalysisofWARPwhenhyperparametersareselected
differentlyusingtheShepp-Loganphantomtestimage
(256

256)
in
Matlabatvarious
˙
.TheaverageMSEs(

10

2
)
arereportedbased
on5replications.
˝
0.10.30.50.7
functionconstant0.030.270.570.89
functionmix0.030.270.580.88
functionfull0.030.270.570.87
mixconstant0.030.260.570.94
mixmix0.030.260.570.91
mixfull0.030.270.570.91
fullconstant0.030.270.580.86
fullmix0.030.270.560.87
fullfull0.030.270.570.88
maximizationoverasmallsetofgridpointsasdefault.
Inaddition,weinvestigatetheperformancesofWARPat
variouschoicesof

in
B
0
includingLaplaceandquasi-
Cauchypriors.Weoutthese
B
0
leadtoalmostexactly
thesameMSEsasnormalpriors(resultsnotshownhere).
WefurtherinvestigatethesensitivityofWARPbycon-
sideringthefollowingwaystoselecthyperparameters
˝
and

:

˝
:ﬁfunctionﬂ(weuse
˝
j
=2


˝
0
asinSection3);
ﬁmixﬂ(weuseseparate
˝
j
onlyforthelasttwolevels
andaconstantforotherlevels,thereforewehave
threefreeparametersfor
˝
);ﬁfullﬂ(weuseseparate
˝
j
'sforalllevels
j
)


:ﬁconstantﬂ(weuse

(
A
)=

0
forall
A
asinSec-
tion3);ﬁmixﬂ(weuse

j
forthelasttwolevelsanda
constantforotherlevels,thereforewehavethreefree
parametersfor

);ﬁfullﬂ(weuseseparate

j
'sforall
levels
j
).
Table4showsthattheMSEsonlyexhibitminimaldiffer-
encesacrossvariouscombinationsoftuningapproaches.
Thisthepreviousthattheproposed
frameworkisnotsensitivetohyperparameters.
F12
WIDELYUSEDTESTIMAGES
The12widelyusedtestimagesusedinSection3.3areploted
inFigure7.
G3D
IMAGES
UnlikeWARPwhichisdirectlyapplicableto
m
-dimensional
datafor
m>
2
,othermethodscomparedin3.1suchas
Wedgelet,TI-2D-Haar,andBPFAmayrequiresubstantial
foranewdimensionalsetting.SHAHiscon-
ceptuallyapplicablefor3Ddata,buttheexistingsoftware
takeshourstodaysinthetuningstepfor3Dimagesof
Fig.7.Thewidelyused12testimages.
intermediatesizewhileitsperformancein2Dsettingsis
notamongtoptwo.Therefore,wecompareWARPwith
RMandacollectionofotherapproaches,includinga3D
imagedenoisingmethodvialocalsmoothingandnon-
parametricregression(LSNR)proposedby[55],anisotropic
diffusion(AD)method[56],totalvariationminimization
(TV)method[57]andoptimizednon-localmeans(ONLM)
method[58].TheTVmethodisby[55]bymin-
imizinga3D-versionoftheTVcriterion.Weadoptsimu-
lationsettingsin[55],whichusestwo3Dimages
withthefollowingtrueintensityfunctions:
f
1
(
x;y;z
)=

(
x

1
2
)
2

(
y

1
2
)
2

(
z

1
2
)
2
+
1
f
(
x;y;z
)
2
R
1
[
R
2
g
;
where
R
1
=
f
(
x;y;z
):
j
x

1
2
j
1
4
;
j
y

1
2
j
1
4
;
j
z

1
2
j
1
4
g
and
R
2
=
f
(
x;y;z
):(
x

1
2
)
2
+(
y

1
2
)
2

0
:
15
2
;
j
z

1
2
j
0
:
35
g
;
f
2
(
x;y;z
)=
1
4
sin(2
ˇ
(
x
+
y
+
z
)+1)+
1
4
+
1
f
(
x;y;z
)
2
S
1
[
S
2
g
;
where
S
1
=
f
(
x;y;z
):(
x

1
2
)
2
+(
y

1
2
)
2

1
4
(
z

1
2
)
2
;
0
:
2

z

0
:
5
g
and
S
2
=
f
(
x;y;z
):0
:
2
2

(
x

1
2
)
2
+(
y

1
2
)
2
+
(
z

1
2
)
2

0
:
4
2
;z<
0
:
45
g
.
Table5showsthecomparisonofvariousmethodsusing
MSE.Itisworthmentioningthatthenumericalrecordsfor
theothermethodstoestimate
f
1
and
f
2
arefrom[55]
asthecodeisnotimmediatelyavailableandtherunning
timeforsomemethodsuchasLSNRcantakehourstodays
(includingthetuningstep).WARPisuniformlythebest
approachamongalltheselectedmethodsatleastunderthe
simulationsetting.
S-5
TABLE5
3Ddenoisingfortwoimages
f
1
,
f
2
intermsofMSE
(

10

2
)
.WARPuses
5

5

5
localshiftsandarebasedon100replications.Themeanof
100MSEsisreported,andthemaximumstandarderroris0.00.
Method
n
=64
n
=128
f
=
f
1
f
=
f
2
f
=
f
1
f
=
f
2
˙
=0
:
10
:
2
˙
=0
:
10
:
2
˙
=0
:
10
:
2
˙
=0
:
10
:
2
WARP
0.020.040.040.110.010.020.020.05
LSNR0.030.080.060.13
0.010.030.02
0.06
TV0.030.090.060.15
0.01
0.040.030.06
AD0.060.350.070.380.030.200.040.22
ONLM0.030.120.060.14
0.01
0.060.030.06
RM0.220.330.110.260.080.190.060.14
"
8,Self-Attentional Acoustic Models,http://arxiv.org/pdf/1803.09519v2.pdf,https://github.com/bagequan/tencent-transformer-with-disagreement,"Self-AttentionalAcousticModels
MatthiasSperber
1
,JanNiehues
1
,GrahamNeubig
2
,SebastianSt
¨
uker
1
,AlexWaibel
12
1
KarlsruheInstituteofTechnology
2
CarnegieMellonUniversity
f
first
g
.
f
last
g
@kit.edu,gneubig@cs.cmu.edu
Abstract
Self-attentionisamethodofencodingsequencesofvectors
byrelatingthesevectorstoeach-otherbasedonpairwisesimi-
larities.Thesemodelshaverecentlyshownpromisingresults
formodelingdiscretesequences,buttheyarenon-trivialto
applytoacousticmodelingduetocomputationalandmodel-
ingissues.Inthispaper,weapplyself-attentiontoacoustic
modeling,proposingseveralimprovementstomitigatetheseis-
sues:First,self-attentionmemorygrowsquadraticallyinthese-
quencelength,whichweaddressthroughadownsamplingtech-
nique.Second,wethatpreviousapproachestoincorporate
positioninformationintothemodelareunsuitableandexplore
otherrepresentationsandhybridmodelstothisend.Third,to
stresstheimportanceoflocalcontextintheacousticsignal,we
proposeaGaussianbiasingapproachthatallowsexplicitcon-
troloverthecontextrange.Experimentsthatourmodel
approachesastrongbaselinebasedonLSTMswithnetwork-
in-networkconnectionswhilebeingmuchfastertocompute.
Besidesspeed,wethatinterpretabilityisastrengthofself-
attentionalacousticmodels,anddemonstratethatself-attention
headslearnalinguisticallyplausibledivisionoflabor.
1
IndexTerms
:speechrecognition,acousticmodel,self-
attention
1.Introduction
Inordertotransformanacousticsignalintoausefulabstract
representation,acousticmodelsmusttakeintoaccountthecom-
plexinterplayoflocalandglobaldependenciesinanacoustic
signal.Atalocal,temporallyconstrainedlevel,weobservecon-
cretelinguisticevents(phonemes),whileatagloballevelthe
signalisbyfactorssuchaschannelandvoiceprop-
erties.Traditionalacousticmodelsthisintuitionabout
globalandlocaldependenciesbyapplyinganormalization
phase,aglobaloperationthataimsatproducinginvariancewith
respecttochannelandspeakercharacteristics.Afterthis,tra-
ditionallyahiddenMarkovmodelisappliedoverpolyphones,
modelingonlylocaldependencies(beads-on-a-stringview[1]).
Thisrestrictionhasinpartbeenmotivatedbytheintuitionthat
globaleffectsshouldberemovedfromthesignalatthisstage.
However,theempiricalsuccessofrecurrentneuralnet-
works(RNNs)foracousticmodeling[2]haschallengedthisin-
tuitionandindicatedthatconsideringtheglobalcontextisstill
atthisstage.Unfortunately,RNNssufferfromslow
computationspeedandmaynotbeabletooptimallyexploit
long-rangecontext.Self-attentionalarchitectures[3,4,5]have
recentlyshownpromisingresultsasanalternativetoRNNsfor
modelingdiscretesequences[6].Thesemodelsrelatedifferent
positionsinasequencebycomputingpairwisesimilarities,in
ordertocomputeahigherlevelrepresentationofthesequence.
1
Codeat
http://msperber.com/research/self-att
Self-attentionisattractive(1)computationallybecauseitcanbe
efimplementedthroughbatchedtensormultiplication,
and(2)fromamodelingperspectivebecauseitallowsdirect
conditioningonbothshortrange-contextandlong-rangecon-
text,withouttheneedtopassinformationthroughmanyinter-
mediatestatesasisthecasewithRNNs.
Inthispaper,weexploreself-attentionalarchitecturesfor
acousticmodeling,byusingthelisten-attend-spellmodel[7]
andreplacingitspyramidalencodercomponentwithself-
attention.
2
Inordertomakeself-attentionalarchitectureswork
foracousticmodeling,severalchallengesmustbeaddressed.
First,self-attentioncomputesthesimilarityofeachpairofin-
puts,sotheamountofmemorygrowsquadraticallywithre-
specttothesequencelength.Thisisproblematicformodeling
acousticsequences,becausethesecangetverylong,e.g.our
trainingutterancescontainupto2026frames(800onaverage).
Toaddressthisissue,weapplydownsamplingbyreshapingthe
sequencebeforeself-attentionallayers.
Thesecondchallengeisincorporatingpositionalinforma-
tionintothemodel.UnlikeanRNN,self-attentionhasnoinher-
entmechanismofmodelingsequenceposition.Vaswanietal.
[6]proposeanadditivetrigonometricpositionencoding,which
isproblematicinthecaseofacousticmodelingbecauseourin-
putsareedspeechfeaturesratherthanxiblylearnedword
embeddings.Whileconcatenatingpositionalembeddingsin-
steadprovidessomeremedy,weitnecessarytodesigna
hybridself-attention/RNNarchitecturetoobtaingoodresults.
Thethirdchallengeiseffectivemodelingofcontextrel-
evance.Speechframescontainmuchlessinformationthan
wordsanditisthereforemorediftoestimatetheimpor-
tanceofpairsofframeswithrespecttoeachother.Basedonthe
intuitionthatlocalityofcontextplaysaspecialroleinacoustic
modeling,weproposetoapplydiagonalGaussianmaskswith
learnablevariancetoattentionheads.Thisgivesattentionheads
morecontrolovercontextrelevanceandimprovesworderror
ratesconsistently,byupto1.59%.Weobservethatwhilebot-
tomlayerattentionheadsconvergetowarddiversityincontext
range,higherlayersuselong-rangecontext.
Attentionmechanismsimprovetheoftencriticizedpoorin-
terpretabilityofneuralend-to-endmodelsbecausetheyenforce
anexplicitexpressionofdependencies.Self-attentionbrings
thisinterpretabilityinsidetheencoder,makingitpossibletoex-
aminehowspeechisencodedbeforemakingtherecog-
nitiondecisions.Ouranalysisrevealsthatdifferentattention
headsmeasuresimilarityalongdifferentlinguisticallyplausible
dimensionssuchasphonemeclusters,indicatingthattheyfunc-
tioninparttoreduceacousticvariabilitybyestablishingaver-
agedversionsofmatchingacousticeventsacrosstheutterance.
2
Wealsorefertoindependentworkthathasconcurrentlyaddressed
similarquestions[8].
arXiv:1803.09519v2  [cs.CL]  18 Jun 2018Figure1:
Blockdiagramsofbaselinesandthecoremodel.
2.AttentionalModelsforASR
2.1.Listen,Attend,Spell
OurASRmodelisbasedonthelisten-attend-spellmodel[7],
anattentionalencoder-decodermodel[9,10,11]wheretheen-
codercomponentservesasanacousticmodel,takingspeech
featuresasinput.Becauseacousticsequencesareverylong,
theencoderperformsdownsamplingtomakememoryandrun-
timemanageable.ThisisachievedthroughapyramidalLSTM
(Fig.1a),astackofLSTMlayerswherepairsofconsecutive
outputsofalayerareconcatenatedbeforebeingfedtothenext
layer,suchthatthenumberofstatesishalvedbetweenlayers.
2.2.ExistingEncodersforSpeech
SeveralimprovementsoverthepyramidalLSTMencoderhave
beenproposed[12].Asasecondbaseline,weemployastate-
of-the-artmodel[12]thatstacksblocksconsistingofanLSTM,
anetwork-in-network(NiN)projection,andbatchnormaliza-
tion(Fig.1b).ThetopLSTM/NiNblockisextendedbya
LSTMlayer.NiNdenotesasimplelinearprojectionapplied
ateverytimestep,possiblyperformingdownsamplingbycon-
catenatingpairsofadjacentprojectioninputs.
3.Self-AttentionalAcousticModels
Self-attentionisappliedtoasequenceofstatevectorsandtrans-
formseachstateintoaweightedaverageoverallthestatesinthe
sequence,withmorerelevantstatesbeinggivenmore
Theunderlyingintuitionisthatstatesateachtimestepshould
beconditionedonthemostrelevantstatesacrossthewholese-
quence.Ourbasicformofself-attentionfollowsVaswaniet
al.[6],whererelevanceismeasuredbycomputingdotproduct
similarityafterapplyingalinearprojectiontobothvectors.For
acousticsequences,neighboringframesarenaturallysimilarif
theyrepresentpartsofthesameacousticevent.Whenanevent
withsimilaracousticcharacteristicsappearsatdifferentplaces
inanutterance,thoseoccurrenceswouldbedeemedrelevant,as
well.Following[6]weuse8attentionheadswhereeachhead
cancomputethissimilarityindependently.
Ourmodeliscomputedasfollows(Fig.1c):
Q
i
=
XW
Q
i
;K
i
=
XW
K
i
;V
i
=
XW
V
i
(1)
head
i
=softmax(
Q
i
K
T
i
p
d
)
V
i
8
i
(2)
MultiHeadAtt
=concat(
head
1
;
head
2
;:::
)
(3)
MidLayer
=LayerNorm[
MultiHeadAtt
+
X
]
(4)
SAL
=LayerNorm[FF(
MidLayer
)+
MidLayer
]
(5)
Here,
X
2
R
l

d
;Q
i
;K
i
;V
i
2
R
l

d=n
denoteinputsandtheir
query/key/valuetransformationsforattentionheadsindexed
by
i
2f
1
;

;
8
g
,sequencelength
l
,andhiddendimen-
sion
d
.SALdenotestheoutputoftheselfatten-
tionlayer.
W
Q
i
;W
K
i
;W
V
i
2
R
d

d=n
areparametermatri-
ces.FFisaposition-wisefeed-forwardnetworkintended
tointroduceadditionaldepthandnonlinearities,as
FF
(
x
)=max(0
;xW
1
+
b
1
)
W
2
+
b
2
.LayerNormisaccord-
ingto[13].
4.TailoringSelf-AttentiontoSpeech
4.1.Downsampling
Tointroducedownsamplingsothatthemodeldescribedin§3
inmemory,weapplyareshapingoperationbeforeevery
self-attentionblock.Thisreducesthesequencelengthbyafac-
tor
a
andincreasesthevectorstatedimensionaccordingly:
X
2
R
l

d
!
reshape
^
X
2
R
l
a

ad
Wethencompute(1)through(5)asbefore,withtheshape
ofweightmatricesadjustedto
W
Q
i
;W
K
i
;W
V
i
2
R
da

d=n
.This
reducesthememoryconsumptionoftheattentionmatrixbyfac-
tor
a
2
.Itiscrucialtoapplyreshapesalsobeforethebottom
layersothatthelargebottomattentionmatrixisscaleddown.
Notethatthisapproachisverysimilartodownsamplingasin
thepyramidalLSTM,exceptthatitisappliedtoasequence
featurematrixinsteadofper-timestep,andalsoappliedbefore
thebottomlayer.
4.2.PositionModeling
Positioninformationiscrucialinsequence-to-sequencemod-
els,butself-attentioniscompletelyagnostictosequenceposi-
tions.Priorworksaddedtrigonometricpositionencodings[6]
orlearnedpositionembeddings[14]toinputvectors,butwe
foundthatthisapproachdoesnotworkwellforacousticse-
quences.Thisisintuitive,astheinputsareedfeaturevectors
ratherthantrainablewordembeddings,makingitdiffor
themodeltoseparatepositionandcontentforeachstate.
4.2.1.ConcatenatedPositionRepresentation
Astraight-forwardsolutiontoenableseparationofpositionand
contentforedinputsistoconcatenatepositionrepresentation
insteadofusingasum.Weexplorethreevariants:First,con-
catenatingtrigonometricencodings[6]totheinputfeaturevec-
tors.Second,concatenatinglearnedembeddings[14]toinputs.
Third,concatenatingseparatelylearnedpositionembeddingsto
thequeriesandkeys(
Q
,
K
inEquation1)sothatthekeyand
querypositioncanbetakenintoaccountwhencomputingrele-
vanceateachlayer.
4.2.2.HybridModels
RNNsareeffectiveatkeepingtrackofpositionalinformation.
Wecanexploitthisbyintroducingrecurrentlayersintoouren-
coder.Weexploretwoalternatives:
Stackedhybridmodel
.Here,westack2LSTM/NiN
blocks(Fig.1b)withoutdownsampling,followedbya
LSTM,ontopofourself-attentionlayers.Thisapproachdoes
notmaketheself-attentionlayersthemselvesposition-aware,
buttheencoderstatesareposition-aware.Reversingthe
orderofself-attentionandLSTM/NiNisalsoconceivablebut
wouldcompromisespeedbecauseslowrecurrentcomputations
areappliedbeforedownsampling.
Interleavedhybridmodel
.Anotheroptionistoreplacethe
feed-forwardoperation(FFinEquation5)byanLSTM.Note
thatthisintroducesLSTMsbeforethesequenceisfullydown-
sampledandthereforecompromisessomeofthespeedgains.
Ontheotherhand,itallowsthehigherself-attentionlayersto
takeadvantageofpositioninformationencodedbylowerinter-
leavedLSTMs.
4.3.AttentionBiasing
Self-attentionallowsdirectconditioningonthewholesequence,
butitisuncleartowhatextentthisisforouracous-
ticmodel.Whilecontextrequiredtomodelpolyphonesmay
spanonlyarelativelysmalltemporalwindow,remainingchan-
nelandspeakerpropertiesmayrequirelong-rangecontext.To
accountforthespecialroleofcontextlocalityinacousticmod-
eling,weintroduceanexplicitwayofcontrollingthecon-
textrangebyusingabiasmatrix
M
2
R
l

l
andcomputing
head
i
=softmax(
Q
i
K
T
i
p
d
+
M
)
V
i
.Bysettingvaluesaround
thediagonalofthismasktoahighervalue,wecanbiastheself-
attentiontowardattendinginalocalrangearoundeachframe.
4.3.1.LocalMasking
Wecanapplyhardmaskingbysetting
M
asaninverselybanded
matrixofbandwith
b
2
N
odd
with
M
jk
=
(
0
j
j

k
j
<
b
2
-
1
else
:
Asaresult,allattentionweightsoutsidethebandaresetto
0,sothattheself-attentionisrestrictedtoalocalregionofsize
b
.Thehyperparameter
b
canbesetpriortotrainingsuchthatthe
modeleffectivelyattendstoarangesimilartopolyphonecon-
textinhiddenMarkovmodels.Noticethesimilarideaexplored
concurrentlyinindependentwork[8].
4.3.2.GaussianBias
Formorexibility,weuseasoftGaussianmaskby
M
jk
=

(
j

k
)
2
2
˙
2
:
˙
isatrainablestandarddeviationparameter.Itislearnedsepa-
ratelyforeachattentionheadsothatthecontextrangecandiffer
betweenattentionheads.Besidesmoremodelingexpressive-
ness,thelearnedvariancescanalsobeinspectedandmayhelp
ustounderstandandinterpretthemodel.
3
Notethatthisbears
someresemblancetopriorwork[15]whousea
linear
distance
mapinsteadofaGaussiananddonotincludetrainableparame-
ters,makingtheirmodellessxibleandlessinterpretable.
5.ExperimentalSetup
WefocusourexperimentsontheTEDLIUMcorpus[16],a
widelyusedcorpusof200hofrecordedTEDtalks,withthe
developmentsplitusedasvalidationdata.Ourimplementation
isbasedontheXNMTtoolkit,withwhichwehavepreviously
demonstratedcompetitiveASRresultsontwobenchmarks[17].
Thetrainingsettingsfollow[17]whererelevant.Weextract
40-dimensionalMelfeatureswithper-speakermean
3
Toovercometrainabilityissuesandencouragetheoptimizertoad-
justthevarianceparameter,wefounditnecessarytore-parametrizeit
using
˝
2
=
˙
andoptimize
˝
viaback-propagation.
Table1:
Comparisontobaselines.Trainingspeed(char/sec)
wasmeasuredonaGTX1080TiGPU.
model
devWER
testWER
char/sec
pyramidal
15.83
16.16
1.1k
LSTM/NiN
14.57
14.70
1.1k
stackedhybrid
16.38
17.48
2.4k
interleavedhybrid
15.29
16.71
1.5k
andvariancenormalizationusingKaldi[18].Weexcludeut-
teranceslongerthan1500framestokeepmemoryrequirements
manageable.Theencoder-decoderattentionisMLP-based,and
thedecoderusesasingleLSTMlayer.Thenumberofhidden
unitsis128fortheencoder-decoderattentionMLP,64fortar-
getcharacterembeddings,and512elsewhereunlessotherwise
noted.Themodelusesinputfeeding[19],variationalrecur-
rentdropoutwithprobability0.2andtargetcharacterdropout
withprobability0.1[20].Weapplylabelsmoothing[21]and
thetargetembeddingnormto1[22].Forinference,weuse
abeamsizeof20andlengthnormalizationwithexponent1.5.
Self-attentionlayersuseahiddendimensionof256andfeed-
forwarddimensionof256,andattentiondropoutwithproba-
bility0.2.WhenLSTMsarepartoftheencoder,weusebidi-
rectionalLSTMswith256hiddenunitsperdirection.Concate-
natedpositionrepresentationvectorsareofsize40.
Thevocabularyconsistsofthe26Englishcharacters,apos-
trophe,whitespace,andspecialstart-of-sequenceandunknown-
charactertokens.Wesetthebatchsizedynamicallydepending
ontheinputsequencesizesuchthattheaveragebatchsizewas
24(18forLSTM-freemodels).WeuseAdam[23]withinitial
learningrateof0.0003,decayedby0.5whenvalidationWER
didnotimproveover10epochsinitiallyand5epochsafterthe
decay.
6.QuantitativeResults
6.1.ComparisontoBaselines
Thesetofexperimentscomparestheproposedhybridmod-
elstothebaselines.TheresultsaresummarizedinTable1.We
observesimilarworderrorrates,withtheinterleavedmodelout-
performingthestackedmodelandoutperformingthepyramidal
LSTMbaselineonthedevelopmentdatabutnotthetestdata.
TheLSTM/NiNbaselinewasstrongest.Intermsoftraining
speed,thestackedmodelisfastestbyalargemargin,followed
bytheinterleavedmodelandtheLSTM/NiNmodel.Tocon-
thattheattentionmechanismisactuallycontributingtothe
hybridmodelandnotjustpassingonactivations,weperformed
asanitycheckbytrainingastackedhybridmodelwithatten-
tionscoresoffthediagonalsetto

,andobservedadropof
1.25%absoluteWER.
6.2.PositionModeling
Next,weevaluatethedifferentapproachestopositionmodel-
ing(§4.2).TheresultsaresummarizedinTable2.Whenusing
additivepositionalencodingsthemodeldiverged,whilecon-
catenatingembeddingsconverged,albeittoratherpooroptima.
Thekey/querypositionalembeddingsinisolationdiverged,and
combinationwithconcatenatedinputembeddingsdidnotim-
proveresults.Onlythehybridmodelswereabletoobtainre-
sultscomparabletothebaselines.Wealsotriedcombininghy-
bridmodelswithpositionalembeddings,butdidnotseeim-
provementsoverthemodelwithoutpositionalembeddings.
Table2:
WERresultsonpositionmodeling.
model
dev
test
add(trig.)
diverged
concat(trig.)
30.27
38.60
concat(emb.)
29.81
31.74
stackedhybrid
16.38
17.48
interleavedhybrid
15.29
16.71
Table3:
WERresultsonattentionbiasing.
model
dev
test
stackedhybrid
16.38
17.48
+localmasking
15.42
16.17
+Gaussmask(init.small)
16.05
16.96
+Gaussmask(init.large)
14.90
15.89
interleavedhybrid
15.29
16.71
+localmasking
15.44
16.19
+Gaussmask(init.small)
16.43
16.89
+Gaussmask(init.large)
15.00
15.82
6.3.AttentionBiasing
Thissetofexperimentsteststheeffectofintroducingexplicitat-
tentionbiasesthatenablethemodeltocontrolitscontextrange
(§4.3).Thelocaldiagonalmaskwassettoconstrainthecon-
texttoawindowof5timesteps,andGaussianbiasingvariances
wereinitializedto9(smallsetting)or100(largesetting).Re-
sultsaresummarizedinTable3.Forthestackedmodel,itcan
beseenthatthebiasinghelpsingeneral.Thestrongestmodel
variantwasthelearnableGaussianmask.Interestingly,itwas
importanttoinitializetheGaussiantopossessalargevariance.
Wehypothesizethatthisimprovesgradientwearlyoninthe
modeltraining,similartohowinitializingLSTMforgetgatebi-
asesto1(noforgetting)improvesresults[24].Theinterleaved
hybridmodelshowssimilartrends.Notethatthesometimes
inconsistentorderingbetweendevandtestresultscanbeex-
plainedbythefactthattheTEDLIUMdevsetisrelativelysmall
withonly500utterances.
TheGaussianmaskallowsinspectingitstrainablevariance
parameter.Fig.2showshowtheparameterevolveswhenini-
tializedtoalargevalue.Itcanbeseenthatinthelayer,
diversityseemstobedesirable,withsomeattentionheadsfo-
cusingonasmalllocalcontext,andothersonlargercontexts.
Incontrast,thesecondlayerdoesnotappeartofrom
limitingitscontext.Thispartlytheideaofhierarchi-
calmodeling,wherethemodelinggranularityincreasesacross
layers,butalsoshowsthatevenatthebottomlayeracontrolled
amountoflong-rangecontextisdesirable.
7.InterpretabilityofAttentionHeads
Wehypothesizethatcertainattentionheadsrespondtocertain
typesofacousticevents.Totestthishypothesis,wecorrelate
theaverageattentionthateachattentionheadplacesonframes
withthecorrespondingphonemelabelsobtainedviaforcedde-
coding.Were-trainthestackedhybridmodelwithphonemes
insteadofcharactersastargets,anduseencoder-decoderat-
tentionscores,summedoverphonemetypes,toobtainasoft
alignmentofphonemelabelsforeachframe.Thisgivesusa
measureforhowmucheachframeinthesequencecorresponds
tothephonemetypeunderinspection.
Figure2:
Evolutionofthevarianceparametersforeachofthe
8attentionheadsovercourseoftraining(left:stlayer,right:
secondlayer).
Table4:
Analysisoffunctionofattentionheads.Notethatwe
conductedasmallamountofcherrypickingbyremoving4out-
liersthatdidnotseemtocategories(OYfromhead1,ZH
fromhead3,EHandERfromhead7).Entropyiscomputed
overthecorrelationscores,truncatedbelow0.
i
topphonemes
entropy
comments
1
S,TH,Z
3.7
sibilants
2
<
/s
>
1.9
silence
3
UW,Y,IY,IX
3.6
ﬂyouﬂdiphthong
B,G,D
voicedplosives
M,NG,N
nasals
4
XM,AW,AA,AY,
3.2
A,schwa
L,AO,AH
5
ZH,AXR,R
3.5
R,ZH
6
ZH,Z,S
3.2
sibilants
IY,IH,Y,UW
ﬂyouﬂdiphthong
7
S,
<
/s
>
,TH
3.4
fricative,noise
CH,SH,F
8
mixed
3.7
unfocused
Wenowcorrelatethesephonemeactivationstoeachofthe
layer's8attentionheads.Weaveragethematricesacross
rowstoobtaintheoverallattentionthateachframereceives.We
thencomputethePearsoncorrelationcoefofthesumma-
rizedself-attentionandencoder-decoderattentionsequences,
concatenatedoverutterances.
Table4showsthemosthighlycorrelatedphonemesfor
eachattentionhead,alongwithanattempttoclassifytheseman-
uallyaccordingtolinguisticcategories.Thisworksremarkably
wellandwecanclearlyseealinguisticallyplausibledivision
oflabor,eventhoughcategoriesareneitherexhaustivenordis-
junct.Noticethathead2seemstoalwaysfocusontheutter-
anceendwhereweusuallyexpectsilence,andhead8ismostly
unfocused,whichwemayinterpretastheseheadsestablishing
channelandspeakercontext.
8.Conclusion
Applyingself-attentiontoacousticmodelingischallengingfor
computationalandmodelingreasons.Weinvestigatewaysto
addressthesechallengesandobtainourbestresultswhenus-
ingahybridmodelarchitectureandGaussianbiasesthatal-
lowcontrollingcontextrange.Thismodelisalmostasgood
asastrongLSTM-basedbaselineatmuchfastercomputation
speed.Wehighlightinterpretabilityasanadvantageovercon-
ventionalmodels.Futureworkincludesinvestigationofself-
attentionwithothersequencesoflow-informationstatessuch
ascharacters,andoftransferringresultsoncontrollingcontext
rangeandinterpretabilitytotextmodeling.
9.References
[1]
M.Ostendorf,ﬁMovingbeyondthebeads-on-a-string'modelof
speech,ﬂin
AutomaticSpeechRecognition&Understanding
(ASRU)
,1999,pp.79Œ84.
[2]
H.Sak,A.Senior,andF.Beaufays,ﬁLongshort-termmemoryre-
currentneuralnetworkarchitecturesforlargescaleacousticmod-
eling,ﬂin
AnnualConferenceoftheInternationalSpeechCommu-
nicationAssociation(InterSpeech)
,2015.
[3]
J.Cheng,L.Dong,andM.Lapata,ﬁLongshort-termmemory-
networksformachinereading,ﬂin
EmpiricalMethodsinNatural
LanguageProcessing(EMNLP)
,2016.
[4]
A.Parikh,O.T
¨
ackstr
¨
om,D.Das,andJ.Uszkoreit,ﬁADecompos-
ableAttentionModelforNaturalLanguageInference,ﬂin
Empir-
icalMethodsinNaturalLanguageProcessing(EMNLP)
,Austin,
Texas,USA,2016,pp.2249Œ2255.
[5]
Z.Lin,M.Feng,C.N.dosSantos,M.Yu,B.Xiang,B.Zhou,and
Y.Bengio,ﬁAStructuredSelf-attentiveSentenceEmbedding,ﬂ
in
InternationalConferenceonRepresentationLearning(ICLR)
,
2017,pp.1Œ15.
[6]
A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.
Gomez,L.Kaiser,andI.Polosukhin,ﬁAttentionIsAllYouNeed,ﬂ
in
NeuralInformationProcessingSystemsConference(NIPS)
,
2017,pp.5998Œ6008.
[7]
W.Chan,N.Jaitly,Q.V.Le,andO.Vinyals,ﬁListen,attend
andspell:Aneuralnetworkforlargevocabularyconversational
speechrecognition,ﬂin
Acoustics,SpeechandSignalProcessing
(ICASSP)
,2016.
[8]
D.Povey,H.Hadian,P.Ghahremani,K.Li,andS.Khudanpur,
ﬁATime-RestrictedSelf-AttentionLayerforASR,ﬂin
Interna-
tionalConferenceonAcoustics,SpeechandSignalProcessing
(ICASSP)
,2018.
[9]
N.KalchbrennerandP.Blunsom,ﬁRecurrentContinuousTrans-
lationModels,ﬂin
EmpiricalMethodsinNaturalLanguagePro-
cessing(EMNLP)
,Seattle,Washington,USA,2013,pp.1700Œ
1709.
[10]
I.Sutskever,O.Vinyals,andQ.V.Le,ﬁSequencetoSequence
LearningwithNeuralNetworks,ﬂin
AdvancesinNeuralInfor-
mationProcessingSystems(NIPS)
,Montr
´
eal,Canada,2014,pp.
3104Œ3112.
[11]
D.Bahdanau,K.Cho,andY.Bengio,ﬁNeuralMachineTransla-
tionbyJointlyLearningtoAlignandTranslate,ﬂin
International
ConferenceonRepresentationLearning(ICLR)
,SanDiego,USA,
2015.
[12]
Y.Zhang,W.Chan,andN.Jaitly,ﬁVeryDeepConvolutionalNet-
worksforEnd-to-EndSpeechRecognition,ﬂin
InternationalCon-
ferenceonAcoustics,SpeechandSignalProcessing(ICASSP)
,
2017.
[13]
J.L.Ba,J.R.Kiros,andG.E.Hinton,ﬁLayerNormalization,ﬂ
arXiv:1607.06450
,2016.
[14]
J.Gehring,M.Auli,D.Grangier,D.Yarats,andY.N.
Dauphin,ﬁConvolutionalSequencetoSequenceLearning,ﬂ
arXiv:1705.03122
,2017.
[15]
J.ImandS.Cho,ﬁDistance-basedSelf-AttentionNetworkfor
NaturalLanguageInference,ﬂ
arXiv:1712.02047
,2017.
[16]
A.Rousseau,P.Del
´
eglise,andY.Est
˚
eve,ﬁEnhancingtheTED-
LIUMCorpuswithSelectedDataforLanguageModelingand
MoreTEDTalks,ﬂin
InternationalConferenceonLanguageRe-
sourcesandEvaluation(LREC)
,2014,pp.3935Œ3939.
[17]
G.Neubig,M.Sperber,X.Wang,M.Felix,A.Matthews,S.Pad-
manabhan,Y.Qi,D.S.Sachan,P.Arthur,P.Godard,J.Hewitt,
R.Riad,andL.Wang,ﬁXNMT:TheeXtensibleNeuralMachine
TranslationToolkit,ﬂin
ConferenceoftheAssociationforMa-
chineTranslationintheAmericas(AMTA)OpenSourceSoftware
Showcase
,Boston,2018.
[18]
D.Povey,A.Ghoshal,G.Boulianne,L.Burget,O.Glembek,
N.Goel,M.Hannemann,P.Motlicek,Y.Qian,P.Schwarz,and
Others,ﬁTheKaldispeechrecognitiontoolkit,ﬂin
Workshopon
AutomaticSpeechRecognition&Understanding(ASRU)
,2011.
[19]
M.-T.Luong,H.Pham,andC.D.Manning,ﬁEffectiveAp-
proachestoAttention-basedNeuralMachineTranslation,ﬂin
Em-
piricalMethodsinNaturalLanguageProcessing(EMNLP)
,Lis-
bon,Portugal,2015,pp.1412Œ1421.
[20]
Y.GalandZ.Ghahramani,ﬁATheoreticallyGroundedApplica-
tionofDropoutinRecurrentNeuralNetworks,ﬂin
NeuralInfor-
mationProcessingSystemsConference(NIPS)
,Barcelona,Spain,
2016,pp.1019Œ1027.
[21]
C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna,
ﬁRethinkingtheInceptionArchitectureforComputerVision,ﬂin
ComputerVisionandPatternRecognition
,2016.
[22]
T.Q.NguyenandD.Chiang,ﬁImprovingLexicalChoiceinNeu-
ralMachineTranslation,ﬂin
NorthAmericanChapteroftheAs-
sociationforComputationalLinguistics(NAACL)
,NewOrleans,
USA,2018.
[23]
D.P.KingmaandJ.Ba,ﬁAdam:AMethodforStochasticOpti-
mization,ﬂin
InternationalConferenceonLearningRepresenta-
tions(ICLR)
,Banff,Canada,2014.
[24]
R.Jozefowicz,W.Zaremba,andI.Sutskever,ﬁAnEmpiricalEx-
plorationofRecurrentNetworkArchitectures,ﬂin
International
ConferenceonMachineLearning(ICML)
,Lille,France,2015.
"
9,Multilingual bottleneck features for subword modeling in zero-resource languages,http://arxiv.org/pdf/1803.08863v2.pdf,https://github.com/eginhard/cae-utd-utils,"Multilingualbottleneckfeaturesforsubwordmodeling
inzero-resourcelanguages
EnnoHermann
1
,SharonGoldwater
1
1
ILCC,SchoolofInformatics,UniversityofEdinburgh,UK
ehermann@inf.ed.ac.uk,sgwater@inf.ed.ac.uk
Abstract
Howcanweeffectivelydevelopspeechtechnologyforlan-
guageswherenotranscribeddataisavailable?Manyexisting
approachesusenoannotatedresourcesatall,yetitmakessense
toleverageinformationfromlargeannotatedcorporainother
languages,forexampleintheformofmultilingualbottleneck
features(BNFs)obtainedfromasupervisedspeechrecognition
system.Inthiswork,weevaluatetheofBNFsforsub-
wordmodeling(featureextraction)insixunseenlanguageson
aworddiscriminationtask.Firstweestablishastrongunsu-
pervisedbaselinebycombiningtwoexistingmethods:vocal
tractlengthnormalisation(VTLN)andthecorrespondenceau-
toencoder(cAE).WethenshowthatBNFstrainedonasingle
languagealreadybeatthisbaseline;includingupto10languages
resultsinadditionalimprovementswhichcannotbematchedby
justaddingmoredatafromasinglelanguage.Finally,weshow
thatthecAEcanimprovefurtherontheBNFsifhigh-quality
same-wordpairsareavailable.
IndexTerms
:multilingualbottleneckfeatures,subwordmodel-
ing,unsupervisedfeatureextraction,zero-resourcespeechtech-
nology
1.Introduction
Recentyearshaveseenincreasinginterestinﬁzero-resourceﬂ
speechtechnology:systemsdevelopedforatargetlanguage
withouttranscribeddataorotherhand-curatedresources.One
challengeforthesesystems,highlightedbytheZeroResource
SpeechChallenge(ZRSC)of2015[1]and2017[2],istoim-
provesubwordmodeling,i.e.,toextractspeechfeaturesfrom
thetargetlanguageaudiothatworkwellforworddiscrimination
ordownstreamtaskssuchasquery-by-example.
TheZRSCsweremotivatedlargelybyquestionsin
intelligenceandhumanperceptuallearning,andfocusedon
approacheswherenotranscribeddatafrom
any
languageis
used.Yetfromanengineeringperspectiveitalsomakessenseto
explorehowtrainingdatafromhigher-resourcelanguagescan
beusedtoimprovespeechfeaturesinazero-resourcelanguage.
Thereisconsiderableevidencethatbottleneckfeatures
(BNFs)extractedusingamultilinguallytraineddeepneural
network(DNN)canimproveASRfortargetlanguageswithjust
afewhoursoftranscribeddata[3
Œ
7].However,therehasbeen
littleworksofarexploringsupervisedmultilingualBNFsfor
targetlanguageswithnotranscribeddataatall.[8,9]trained
monolingual
BNFextractorsandshowedthatapplyingthem
cross-linguallyimprovesworddiscriminationinazero-resource
setting.[10,11]trainedamultilingualDNNtoextractBNFsfor
azero-resourcetask,buttheDNNitselfwastrainedonuntran-
scribedspeech:anunsupervisedclusteringmethodwasapplied
toeachlanguagetoobtainphone-likeunits,andtheDNNwas
trainedontheseunsupervisedphonelabels.
Weknowofonlytwopreviousstudiesofsupervisedmulti-
lingualBNFsforzero-resourcespeechtasks.In[12],theauthors
trainedBNFsoneitherMandarin,Spanishorboth,andused
thetrainedDNNstoextractfeaturesfromEnglish(simulating
azero-resourcelanguage).Onaquery-by-exampletask,they
showedthatBNFsalwaysperformedbetterthanMFCCs,and
thatbilingualBNFsperformedaswellorbetterthanmonolin-
gualones.Furtherimprovementswereachievedbyapplying
weaksupervisioninthetargetlanguageusingacorrespondence
autoencoder[13]trainedonEnglishwordpairs.However,theau-
thorsdidnotexperimentwithmorethantwotraininglanguages,
andonlyevaluatedonEnglish.
Inthesecondstudy[14],theauthorsbuiltmultilingualsys-
temsusingeithersevenortenhigh-resourcelanguages,and
evaluatedonthethreeﬁdevelopmentﬂandtwoﬁsurpriseﬂlan-
guagesoftheZRSC2017.However,theyincludedtranscribed
trainingdatafromfouroutoftheveevaluationlanguages,so
onlyonelanguage'sresults(Wolof)aretrulyzero-resource.
Thispaperpresentsamorethoroughevaluationofmultilin-
gualBNFs,trainedonbetweenoneandtenlanguagesfromthe
GlobalPhonecollectionandevaluatedonsixothers.Weshow
thattrainingonmorelanguagesconsistentlyimprovesperfor-
manceonworddiscrimination,andthattheimprovementisnot
simplyduetomoretrainingdata:anequivalentamountofdata
fromonelanguagefailstogivethesame
SinceBNFtrainingusesnotargetlanguagedataatall,we
alsocomparetomethodsthattrainunsupervisedonthetarget
language,eitheraloneorincombinationwiththemultilingual
training.Weuseacorrespondenceautoencoder(cAE)[13],
whichlearnstoabstractawayfromsignalnoiseandvariability
bytrainingonpairsofspeechsegmentsextractedusinganunsu-
pervisedtermdiscovery(UTD)systemŠi.e.,pairsthatarelikely
tobeinstancesofthesamewordorphrase.Inthesettingwith
targetlanguagedataonly,wethatapplyingvocaltractlength
normalisation(VTLN)totheinputofboththeUTDandcAE
systemsimprovesthelearnedfeaturesconsiderably,suggesting
thatcAEandVTLNabstractoverdifferentaspectsofthesignal.
Nevertheless,BNFstrainedonjustasingleotherlanguageal-
readyoutperformthecAE-onlytraining,withmultilingualBNFs
doingbetterbyawidemargin.
WethentriedthemultilingualBNFstothetarget
languagebyusingthemasinputtothecAE.Whentrainedwith
UTDwordpairs,wefoundnotothisHow-
ever,trainingwithmanuallylabeledwordpairsdidyield
suggestingthatthistypeofsupervisioncanhelptunethe
BNFsifthewordpairsaresufhigh-quality.
2.Experimentalsetup
2.1.Dataset
Weuse16languagesfromtheGlobalPhonecorpusofspeech
readfromnewsarticles[15].Theselectedlanguagesanddataset
arXiv:1803.08863v2  [cs.CL]  18 Jun 2018sizesareshowninTable1.Weconsiderthe10languagesin
thetopsectionwithacombined198.3hoursofspeechashigh-
resourcelanguages,wheretranscriptionsareavailabletotraina
supervisedautomaticspeechrecognition(ASR)system.Wetreat
the6languagesinthebottomsectionaszero-resourcelanguages
onwhichweevaluatethenewfeaturerepresentations.
InadditionweusetheEnglishWallStreetJournal(WSJ)
corpus[16]whichiscomparabletotheGlobalPhonecorpus.We
eitherusetheentire81hoursoronlya15hoursubset,sothatwe
cancomparetheeffectofincreasingtheamountofdataforone
languagewithtrainingondatafrom4GlobalPhonelanguages.
Table1:
Datasetsizes(hours).About100speakersperlanguage
with80%oftheseinthetrainingsetandnospeakeroverlap.
LanguageTrainDevTest
High-resource
Bulgarian(BG)17.12.32.0
Czech(CS)26.82.42.7
French(FR)22.82.12.0
German(DE)14.92.01.5
Korean(KO)16.62.22.1
Polish(PL)19.42.82.3
Portuguese(PT)22.81.61.8
Russian(RU)19.82.52.4
Thai(TH)21.21.50.4
Vietnamese(VI)16.91.41.5
English81WSJ(EN)81.31.10.7
English15WSJ15.1--
Zero-resource
Croatian(HR)12.12.01.8
Hausa(HA)6.61.01.1
Mandarin(ZH)26.62.02.4
Spanish(ES)17.62.11.7
Swedish(SV)17.42.12.2
Turkish(TR)13.32.01.9
2.2.Baselinefeatures
Forbaselinefeatures,weuseKaldi[17]toextract
MFCCs+

+

andPLPs+

+

withawindowsizeof
25msandashiftof10ms,andweapplyper-speakercepstral
meannormalization.WealsoevaluatedMFCCsandPLPswith
vocaltractlengthnormalisation(VTLN),asimplefeature-space
speakeradaptationtechniquethatnormalizesaspeaker'sspeech
bywarpingthefrequency-axisofthespectra.VTLNmodels
aretrainedusingmaximumlikelihoodestimationunderagiven
acousticmodelŠhere,adiagonal-covarianceuniversalback-
groundmodelwith1024componentstrainedoneachlanguage's
trainingdata.Warpfactorscanthenbeextractedforboththe
trainingandforunseendata.
2.3.Bottleneckfeatures
Formonolingualtrainingofthehigh-resourcelanguages,wefol-
lowtheKaldirecipesfortheGlobalPhoneandWSJcorporaand
trainasubspaceGaussianmixturemodel(SGMM)systemfor
eachlanguagetogetinitialcontext-dependentstatealignments;
thesestatesserveastargetsforDNNtraining.
Formultilingualtraining,wecloselyfollowtheexisting
KaldirecipefortheBabelcorpus.Wetrainatime-delayneural
network(TDNN)[18]withblocksoftmax[19],i.e.allhidden
Figure1:
Correspondenceautoencodertrainingprocedure(see
section2.4).PartsofthiseduetoHermanKamper,used
withpermission.
layersaresharedbetweenlanguages,butthereisaseparateout-
putlayerforeachlanguageandforeachtraininginstanceonly
theerroratthecorrespondinglanguage'soutputlayerisused
toupdatetheweights.TheTDNNhassix625-dimensionalhid-
denlayers
1
followedbya39-dimensionalbottlenecklayerwith
ReLUactivationsandbatchnormalization.Eachlanguagethen
hasitsown625-dimensionalafandasoftmaxlayer.Thein-
putstothenetworkare40-dimensionalMFCCswithallcepstral
coefcientstowhichweappendi-vectorsforspeakeradapta-
tion.Thenetworkistrainedwithstochasticgradientdescentfor
2epochs.
Inpreliminaryexperimentswetrainedaseparatei-vector
extractorforeachdifferentsizedsubsetoftraininglanguages.
However,resultsweresimilartotrainingonthepooledsetofall
10high-resourcelanguages,soforexpedienceweusedthe100-
dimensionali-vectorsfromthispooledtrainingforallreported
experiments.Includingi-vectorsyieldedasmallperformance
gainovernotdoingso;wealsotriedapplyingVTLNtothe
MFCCsforTDNNtraining,butfoundnoadditional
2.4.Correspondenceautoencoder
Inseveralexperimentswefurtheradaptthebaselinefeaturesor
BNFsusingacAEnetwork.ThecAEattemptstonormalizeout
non-linguisticfactorssuchasspeaker,channel,gender,etc.,us-
ingtop-downinformationfrompairsofsimilarspeechsegments.
ExtractingcAEfeaturesrequiresthreesteps,asillustratedin
Figure1.First,anunsupervisedtermdiscovery(UTD)system
isappliedtothetargetlanguagetoextractpairsofspeechseg-
mentsthatarelikelytobeinstancesofthesamewordorphrase.
Eachpairisthenalignedattheframelevelusingdynamictime
warping(DTW),andpairsofalignedframesarepresentedasthe
input
x
andtargetoutput
x
0
ofaDNN.Aftertraining,amiddle
layer
y
isusedasthelearnedfeaturerepresentation.
ToobtaintheUTDpairs,weusedafreelyavailableUTD
system
2
[20]andextracted36kwordpairsforeachtargetlan-
guage.PublishedresultswiththissystemusePLPfeaturesas
input,andindeedourpreliminaryexperimentsthat
MFCCsdidnotworkaswell.Wethereforereportresultsusing
onlyPLPorPLP+VTLNfeaturesasinputtoUTD.
ToprovideanupperboundoncAEperformance,wealso
reportresultsusing
goldstandard
same-wordpairsforcAEtrain-
ing.Asin[12,13,21],weforce-alignthetargetlanguagedata
andextractallthesame-wordpairsthatareatleast5charac-
tersand0.5secondslong(between89kand102kpairsforeach
language).
1
Thesplicingindexesare
-1,0,1-1,0,1-1,0,1-3,0,3
-3,0,3-6,-3,00
.
2
https://github.com/arenjansen/ZRTools
Following[9,13],wetrainthecAEmodel
3
bypre-
traininganautoencoderwitheight100-dimensionallayersand
alayerofsize39layer-wiseontheentiretrainingdata
for5epochswithalearningrateof
2
:
5

10

4
.Wethen
tunethenetworkwithsame-wordpairsasweaksupervisionfor
60epochswithalearningrateof
2
:
5

10

5
.Framepairsare
presentedtothecAEusingeitherMFCC,MFCC+VTLN,or
BNFrepresentation,dependingontheexperiment(preliminary
experimentsindicatedthatPLPsperformedworsethanMFCCs,
soMFCCsareusedasthestrongerbaseline).Featuresare
extractedfromthehiddenlayerofthecAE.
2.5.Evaluation
Weevaluateallspeechfeaturesonthesame-differenttask[22]
whichtestswhetheragivenspeechrepresentationcancorrectly
classifytwospeechsegmentsashavingthesamewordtypeor
not.Foreachwordpairinaset
S
theDTWcost
betweentheacousticfeaturevectorsunderagivenrepresentation
iscomputed.Twosegmentsarethenconsideredamatchifthe
costisbelowathreshold.Precisionandrecallatagiventhreshold
˝
areas
P
(
˝
)=
M
SW
(
˝
)
M
all
(
˝
)
;R
(
˝
)=
M
SWDP
(
˝
)
j
S
SWDP
j
where
M
isthenumberofsame-word(SW),same-word
different-speaker(SWDP)oralldiscoveredmatchesatthat
thresholdand
j
S
SWDP
j
isthenumberofactualSWDPpairs
in
S
.Byvaryingthethresholdaprecision-recallcurvecanbe
computed,wheretheevaluationmetricistheaveragepreci-
sion(AP)ortheareaunderthatcurve.Wegenerateevaluation
setsofwordpairsfortheGlobalPhonedevelopmentandtest
setsasabove,fromallwordsthatareatleast5charactersand
0.5secondslong,exceptthatwenowalsoincludedifferent-word
pairs.
Wenotethatpreviouswork[13,22]computedrecallwithall
SWpairsforeasiercomputationbecausetheirtestsetsincluded
anegligiblenumberofsame-wordsame-speaker(SWSP)pairs.
InourcasethesmallernumberofspeakersintheGlobalPhone
corporaresultsinupto60%ofSWpairsbeingfromthesame
speaker.Wethereforeexplicitlycomputetherecallonlyfor
SWDPpairstofocustheevaluationoffeaturesontheirspeaker
invariance.
Asasanitycheck,wealsoprovideworderrorrates(WER)
fortheASRsystemstrainedonthehigh-resourcelanguages.
3.Results
3.1.Usingtargetlanguagedataonly
Oursetofexperimentsaimstothebestfeaturesthat
canbeextractedusingtargetlanguagedataonly.Previouswork
hasshownthatcAEfeaturesarebetterthanMFCCs,especially
forcross-speakerworddiscrimination[9],butweknowofno
directcomparisonbetweencAEfeaturesandVTLN,whichcan
alsobetrainedwithouttranscriptions.
Table2showsAPresultsonalltargetlanguagesforbaseline
features,cAEfeatureslearnedusingrawfeaturesasinput(asin
previouswork),andcAEfeatureslearnedusingVTLN-adapted
featuresasinputtoeithertheUTDsystem,thecAE,orboth.We
thatcAEfeaturesastrainedpreviouslyareslightlybetter
thanMFCC+VTLN,butcanbeimprovedconsiderablybyapply-
ingVTLNtotheinputofbothUTDandcAEtrainingŠindeed,
3
https://github.com/kamperh/speech
correspondence
Table2:
Averageprecisionscoresonthesame-differenttask
(devsets),showingtheeffectsofapplyingVTLNtotheinput
featuresfortheUTDand/orcAEsystems.cAEinputiseither
MFCCorMFCC+VTLN.Toplineresults(rows5-6)traincAEon
goldstandardpairs,ratherthanUTDoutput.Baselineresults
rows)directlyevaluateacousticfeatureswithoutUTD/cAE
training.Bestunsupervisedresultinbold.
UTD
input
cAE
input
ESHAHRSVTRZH
PLP28.639.926.922.225.220.4
PLP+VTLN46.248.236.337.931.435.7
PLP+VTLN40.445.735.825.825.926.9
PLP+VTLN+VTLN
51.552.939.642.933.444.4
Goldpairs
65.365.255.652.950.660.5
Goldpairs
+VTLN68.970.157.856.956.369.5
Baseline:
MFCC18.319.617.612.316.818.3
Baseline:
MFCC+VTLN27.428.423.220.421.327.7
evenusinggoldpairsascAEinputapplyingVTLNis
ThissuggeststhatcAEtrainingandVTLNabstractoverdifferent
aspectsofthespeechsignal,andthatbothshouldbeusedwhen
onlytargetlanguagedataisavailable.
3.2.Multilingualtraining
Table3comparestheWERofthemonolingualSGMMsystems
whichprovidethetargetsforTDNNtrainingtotheWERof
themodeltrainedonall10high-resourcelanguages.The
multilingualmodelshowssmallbutconsistentimprovements
foralllanguagesexceptVietnamese.Ultimatelythough,we
arenotsomuchinterestedintheperformanceontypicalASR
tasks,butinwhetherBNFsfromthismodelalsogeneralizeto
zero-resourceapplicationsonunseenlanguages.
Figure2showsAPonthesame-differenttaskofmultilingual
BNFstrainedfromscratchonanincreasingnumberoflanguages
intworandomlychosenorders.Weprovidetwobaselinesfor
comparison,drawnfromourresultsinTable2.Firstly,ourbest
cAEfeaturestrainedwithUTDpairs(fromrow4ofTable2)
areareferenceforafullyunsupervisedsystem.Secondly,the
bestcAEfeaturestrainedwithgoldstandardpairs(fromrow6
ofTable2)giveanupperboundonthecAEperformance.
Inall6languages,evenBNFsfromamonolingualTDNN
alreadyconsiderablyoutperformthecAEtrainedwithUTD
pairs.Addinganotherlanguageusuallyleadstoanincreasein
AP,withtheBNFstrainedon8Œ10high-resourcelanguages
performingthebest,alsoalwaysbeatingthegoldcAE.However,
thebiggestperformancegainisfromaddingasecondtraining
languageŠfurtherincreasesaremostlysmaller.Theorderof
languageshasonlyasmalleffect,althoughforexampleadding
Table3:
WorderrorratesofmonolingualSGMMand10-lingual
TDNNASRsystemevaluatedonthedevelopmentsets.
LanguageMonoMulti
BG17.516.9
CS17.115.7
DE9.69.3
FR24.524.0
KO20.319.3
MonoMulti
PL16.515.1
PT20.519.9
RU27.526.9
TH34.333.3
VI11.311.6
Figure2:
Same-differenttaskevaluationonthedevelopmentsetsforBNFstrainedondifferentamountsofdata.Wecomparetrainingon
upto10differentlanguageswithadditionaldatainonelanguage(English).Formultilingualtraining,languageswereaddedintwo
differentorders:FR-PT-DE-TH-PL-KO-CS-BG-RU-VI(BNFs1)andRU-CZ-VI-PL-KO-TH-BG-PT-DE-FR(BNFs2).Eachdatapoint
showstheresultofaddinganadditionallanguage.AsbaselinesweincludethebestunsupervisedcAEandthecAEtrainedongold
standardpairsfromrows4and6ofTable2.
Table4:
APonthesame-differenttaskwhentrainingcAEonthe
10-lingualBNFsfromabove(cAE-BNF)withUTDandgoldstan-
dardwordpairs(testsetresults).BaselinesareMFCC+VTLN
andthecAEmodelsfromrows4and6ofTable2thatuse
MFCC+VTLNasinputfeatures.Bestresultwithouttargetlan-
guagesupervisioninbold.
FeaturesESHAHRSVTRZH
MFCC+VTLN44.122.325.034.317.933.4
cAEUTD72.141.641.653.229.352.8
cAEgold85.166.358.967.147.970.8
10-lingualBNFs
85.371.056.8
72.0
65.3
77.5
cAE-BNFUTD85.067.440.3
74.3
64.6
78.8
cAE-BNFgold89.279.060.879.969.581.6
otherSlaviclanguagesisgenerallyassociatedwithanincrease
inAPonCroatian,suggestingthatitmaybetotrain
onlanguagesrelatedtothezero-resourcelanguage.
Todeterminewhetherthesegainscomefromthediversityof
traininglanguagesorjustthelargeramountoftrainingdata,we
trainedmodelsonthe15hoursubsetandthefull81hoursofthe
EnglishWSJcorpus,whichcorrespondstotheamountofdata
offourGlobalPhonelanguages.Moredatadoeshelptosome
degree,asFigure2shows,butexceptforMandarintrainingon
justtwolanguages(46hours)alreadyworksbetter.
3.3.cAEresults
Previouswork[13]andourbaselinesinTable2showthata
fullyunsupervisedsystemlikeacAEgeneratesfeaturesthatcan
discriminatebetweenwordsmuchbetterthanstandardacoustic
featureslikeMFCCs.IsthecAEalsoabletofurtherimproveon
multilingualBNFswhichalreadyhaveamuchhigherbaseline
performance?
WetrainedthecAEwiththesamesetsofsame-wordpairsas
before,butreplacedVTLN-adaptedMFCCswiththe10-lingual
BNFsasinputfeatureswithoutanyotherchangesinthetraining
procedure.Table4showsthatthecAEtrainedwithUTDpairs
isabletoslightlyimproveontheBNFsinsomecases,butthis
isnotconsistentacrossalllanguagesandforCroatianthecAE
featuresaremuchworse.Thelimitingfactorappearstobethe
qualityoftheUTDpairs.Withgoldstandardpairs,thecAE
featuresimproveinalllanguages.
4.Conclusions
WeevaluatedmultilingualBNFstrainedonupto10high-
resourcelanguagesonaworddiscriminationtaskin6zero-
resourcelanguages.TheseBNFsoutperformbothstandard
acousticfeatureslikeMFCCsandcAEfeaturestrainedina
fullyunsupervisedway.Weshowedthattrainingonmultiple
languageshelpstheBNFsandthatjusttrainingonmoredata
inasinglelanguagedoesnotworkaswell.WhilethecAEis
theoreticallyabletofurtherimproveontheBNFs,thisdoesnot
workinpracticeifonlywordpairsdiscoveredbyaUTDsystem
areavailable.Infutureworkwewouldliketofurtheranalyze
thecomplementarynatureofVTLNandcAEtrainingandex-
ploretheofthesemultilingualBNFsfordown-stream
zero-resourceapplicationslikespeech-to-texttranslation.
5.Acknowledgements
WethankAndreaCarmantiniforhelpingtosetupmultilingual
trainingfortheGlobalPhonecorpusinKaldiandHermanKam-
perforhelpfulfeedback.Theresearchwasfundedinpartbya
JamesS.McDonnellFoundationScholarAward.
6.References
[1]
M.Versteegh,R.Thiolliere,T.Schatz,X.N.Cao,X.Anguera,
A.Jansen,andE.Dupoux,ﬁThezeroresourcespeechchallenge
2015,ﬂin
Proc.Interspeech
,2015,pp.3169Œ3173.
[2]
E.Dunbar,X.N.Cao,J.Benjumea,J.Karadayi,M.Bernard,
L.Besacier,X.Anguera,andE.Dupoux,ﬁThezeroresource
speechchallenge2017,ﬂin
Proc.ASRU
,2017,pp.323Œ330.
[3]
K.Vesel
´
y,M.
´
at,F.Gr
´
ezl,M.Janda,andE.Egorova,ﬁThe
Language-independentBottleneckFeatures,ﬂin
Proc.SLT
,2012,
pp.336Œ341.
[4]
N.T.Vu,W.Breiter,F.Metze,andT.Schultz,ﬁAninvestigation
oninitializationschemesformultilayerperceptrontrainingusing
multilingualdataandtheireffectonASRperformance,ﬂin
Proc.
Interspeech
,2012,pp.2586Œ2589.
[5]
S.Thomas,S.Ganapathy,andH.Hermansky,ﬁMultilingualMLP
featuresforlow-resourceLVCSRsystems,ﬂin
Proc.ICASSP
,2012,
pp.4269Œ4272.
[6]
J.Cui,B.Kingsbury,B.Ramabhadran,A.Sethy,K.Audhkhasi
etal.
,ﬁMultilingualrepresentationsforlowresourcespeechrecog-
nitionandkeywordsearch,ﬂin
Proc.ASRU
,2015,pp.259Œ266.
[7]
T.Alum
¨
ae,S.Tsakalidis,andR.M.Schwartz,ﬁImprovedmulti-
lingualtrainingofstackedneuralnetworkacousticmodelsforlow
resourcelanguages.ﬂin
Proc.Interspeech
,2016,pp.3883Œ3887.
[8]
Y.Yuan,C.-C.Leung,L.Xie,B.Ma,andH.Li,ﬁLearningneural
networkrepresentationsusingcross-lingualbottleneckfeatures
withword-pairinformation,ﬂin
Proc.Interspeech
,2016,pp.788Œ
792.
[9]
D.Renshaw,H.Kamper,A.Jansen,andS.Goldwater,ﬁACompar-
isonofNeuralNetworkMethodsforUnsupervisedRepresentation
LearningontheZeroResourceSpeechChallenge,ﬂin
Proc.Inter-
speech
,2015,pp.3199Œ3203.
[10]
Y.Yuan,C.-C.Leung,L.Xie,H.Chen,B.Ma,andH.Li,ﬁExtract-
ingBottleneckFeaturesandWord-LikePairsfromUntranscribed
SpeechforFeatureRepresentation,ﬂin
Proc.ASRU
,2017,pp.
734Œ739.
[11]
H.Chen,C.-C.Leung,L.Xie,B.Ma,andH.Li,ﬁMultilingual
Bottle-NeckFeatureLearningfromUntranscribedSpeech,ﬂin
Proc.ASRU
,2017,pp.727Œ733.
[12]
Y.Yuan,C.-c.Leung,L.Xie,H.Chen,B.Ma,andH.Li,ﬁPair-
wiseLearningUsingMulti-lingualBottleneckFeaturesforLow-
ResourceQuery-by-ExampleSpokenTermDetection,ﬂin
Proc.
ICASSP
,2017,pp.5645Œ5649.
[13]
H.Kamper,M.Elsner,A.Jansen,andS.Goldwater,ﬁUnsupervised
NeuralNetworkBasedFeatureExtractionUsingWeakTop-Down
Constraints,ﬂin
Proc.ICASSP
,2015,pp.5818Œ5822.
[14]
H.Shibata,T.Kato,T.Shinozaki,andS.Watanabe,ﬁComposite
EmbeddingSystemsforZerospeech2017Track1,ﬂin
Proc.ASRU
,
2017,pp.747Œ753.
[15]
T.Schultz,N.T.Vu,andT.Schlippe,ﬁGlobalPhone:AMultilin-
gualText&SpeechDatabasein20Languages,ﬂin
Proc.ICASSP
,
2013,pp.8126Œ8130.
[16]
D.B.PaulandJ.M.Baker,ﬁTheDesignfortheWallStreetJournal-
basedCSRCorpus,ﬂin
Proc.HLT
,1992,pp.357Œ362.
[17]
D.Povey,A.Ghoshal,G.Boulianne,L.Burget,O.Glembek,
N.Goel,M.Hannemann,P.Motl
´


cek,Y.Qian,P.Schwarz,
J.Silovsk
´
y,G.Stemmer,andK.Vesel
´
y,ﬁTheKaldiSpeechRecog-
nitionToolkit,ﬂin
Proc.ASRU
,2011.
[18]
V.Peddinti,D.Povey,andS.Khudanpur,ﬁAtimedelayneuralnet-
workarchitectureforefmodelingoflongtemporalcontexts,ﬂ
in
Proc.Interspeech
,2015,pp.3214Œ3218.
[19]
F.Gr
´
ezl,M.
´
at,andK.Vesel
´
y,ﬁAdaptationofMultilingual
StackedBottle-neckNeuralNetworkStructureforNewLanguage,ﬂ
in
Proc.ICASSP
,2014,pp.7704Œ7708.
[20]
A.JansenandB.VanDurme,ﬁEfspokentermdiscovery
usingrandomizedalgorithms,ﬂin
Proc.ASRU
,2011,pp.401Œ406.
[21]
A.Jansen,S.Thomas,andH.Hermansky,ﬁWeaktop-downcon-
straintsforunsupervisedacousticmodeltraining,ﬂin
Proc.ICASSP
,
2013,pp.8091Œ8095.
[22]
M.A.Carlin,S.Thomas,A.Jansen,andH.Hermansky,ﬁRapid
EvaluationofSpeechRepresentationsforSpokenTermDiscovery,ﬂ
in
Proc.Interspeech
,2011,pp.828Œ831.
"
10,WSD algorithm based on a new method of vector-word contexts proximity calculation via epsilon-filtration,http://arxiv.org/pdf/1805.09559v2.pdf,https://github.com/componavt/wcorpus,"TransactionsofKarelianResearchCentreRASÒðóäûÊàðåëüñêîãîíàó÷íîãîöåíòðàÐÀÍ
No.7.2018.P.š7.2018.Ñ.
DOI:10.17076/mat829
ÓÄÊ81.32
WSDALGORITHMBASEDONANEWMETHODOF
VECTOR-WORDCONTEXTSPROXIMITYCALCULATION
VIA
""
-FILTRATION
A.N.Kirillov,N.B.Krizhanovskaya,A.A.Krizhanovsky
InstituteofAppliedMathematicalResearchoftheKarelianResearchCentre
oftheRussianAcademyofSciences
Theproblemofwordsensedisambiguation(WSD)isconsideredinthearticle.Set
ofsynonyms(synsets)andsentenceswiththesesynonymsaretaken.Itisnecessary
toautomaticallyselectthemeaningofthewordinthesentence.1285sentences
weretaggedbyexperts,namely,oneofthedictionarymeaningswasselectedby
expertsfortargetwords.TosolvetheWSDproblem,analgorithmbasedonanew
methodofvector-wordcontextsproximitycalculationisproposed.Apreliminary
""
-˝lteringofwordsisperformed,bothinthesentenceandinthesetofsynonyms,in
ordertoachievehigheraccuracy.Anextensiveprogramofexperimentswascarried
out.Fouralgorithmsareimplemented,includingthenewalgorithm.Experiments
haveshownthatinsomecasesthenewalgorithmproducesbetterresults.The
developedsoftwareandthetaggedcorpushaveanopenlicenseandareavailable
online.WiktionaryandWikisourceareused.Abriefdescriptionofthisworkcan
beviewedasslides(https://goo.gl/9ak6Gt).AvideolectureinRussianaboutthis
researchisavailableonline(https://youtu.be/-DLmRkepf58).
Keywords:synonym;synset;corpuslinguistics;word2vec;Wikisource;WSD;
RusVectores;Wiktionary.
À.Í.Êèðèëëîâ,Í.Á.Êðèæàíîâñêàÿ,À.À.Êðèæàíîâñêèé.
ÀËÃÎÐÈÒÌÐÅØÅÍÈßWSD-ÇÀÄÀ×ÈÍÀÎÑÍÎÂÅ
ÍÎÂÎÃÎÑÏÎÑÎÁÀÂÛ×ÈÑËÅÍÈßÁËÈÇÎÑÒÈ
ÊÎÍÒÅÊÑÒÎÂÑÓ×ÅÒÎÌ
""
-ÔÈËÜÒÐÀÖÈÈÑËÎÂ
Ðàññìîòðåíàçàäà÷àðàçðåøåíèÿëåêñè÷åñêîéìíîãîçíà÷íîñòè(WSD),àèìåí-
íî:ïîèçâåñòíûìíàáîðàìñèíîíèìîâ(ñèíñåòû)èïðåäëîæåíèéñýòèìèñèíî-
íèìàìèòðåáóåòñÿàâòîìàòè÷åñêèîïðåäåëèòü,âêàêîìçíà÷åíèèèñïîëüçîâàíî
ñëîâîâïðåäëîæåíèè.Ýêñïåðòàìèáûëèðàçìå÷åíû1285ïðåäëîæåíèé,âûáðà-
íîîäíîèççàðàíååèçâåñòíûõçíà÷åíèé(ñèíñåòîâ).ÄëÿðåøåíèÿWSD-çàäà÷è
ïðåäëîæåíàëãîðèòì,îñíîâàííûéíàíîâîìñïîñîáåâû÷èñëåíèÿáëèçîñòèêîí-
òåêñòîâ.Ïðèýòîìäëÿáîëååâûñîêîéòî÷íîñòèâûïîëíÿåòñÿïðåäâàðèòåëüíàÿ
""
-ôèëüòðàöèÿñëîâ,êàêâïðåäëîæåíèè,òàêèâíàáîðåñèíîíèìîâ.Ïðîâåäåíà
îáøèðíàÿïðîãðàììàýêñïåðèìåíòîâ.Ðåàëèçîâàíî÷åòûðåàëãîðèòìà,âêëþ÷àÿ
ïðåäëîæåííûé.Ýêñïåðèìåíòûïîêàçàëè,÷òîâðÿäåñëó÷àåâíîâûéàëãîðèòì
ïîêàçûâàåòëó÷øèåðåçóëüòàòû.Ðàçðàáîòàííîåïðîãðàììíîåîáåñïå÷åíèåèðàç-
ìå÷åííûéêîðïóññîòêðûòîéëèöåíçèåéäîñòóïíûîíëàéí.Èñïîëüçîâàíûñèíñå-
òûÂèêèñëîâàðÿèòåêñòûÂèêèòåêè.Êðàòêîåîïèñàíèåðàáîòûââèäåñëàéäîâ
äîñòóïíîïîññûëêå(https://goo.gl/9ak6Gt),âèäåîñäîêëàäîìòàêæåäîñòóïíî
îíëàéí(https://youtu.be/-DLmRkepf58).
Êëþ÷åâûåcëîâà:ñèíîíèì;ñèíñåò;êîðïóñíàÿëèíãâèñòèêà;word2vec;Âè-
êèòåêà;WSD;RusVectores;Âèêèñëîâàðü.


149
arXiv:1805.09559v2  [cs.IR]  18 Jun 2018Introduction
Theproblemofwordsensedisambiguation
(WSD)isarealchallengetocomputerscientists
andlinguists.Lexicalambiguityiswidespread
andisoneoftheobstructionsinnaturallanguage
processing.
Inourpreviousworkattributes
ofsynonym[6],wehaveproposedthe
geometricapproachtomathematicalmodeling
ofsynonymset(synset)usingthewordvector
representation.Severalgeometriccharacteristics
ofthesynsetwordsweresuggested(synset
interior,synsetwordrankandcentrality).They
areusedtoselectthemostsigni˝cantsynset
words,i.e.thewordswhosesensesarethenearest
tothesenseofthesynset.
Thetopicrelatedtopolysemy,synonyms,
˝lteringandWSDiscontinuedinthisarticle.Let
usformulatethemathematicalfoundationsfor
solvingtheproblemsofcomputationallinguistics
inthisarticle.
Usingtheapproachproposedinthepaper[2],
wepresenttheWSDalgorithmbasedona
newcontextdistance(proximity)calculation
via
""
-˝ltration.Theexperimentsshowthe
advantagesoftheproposeddistanceoverthe
traditionalaveragevectorssimilaritymeasureof
distancebetweencontexts.
New
""
-proximitybetweenfinitesets
Itisquiteevidentthatthecontextdistance
choiceisoneofthecrucialfactorsin˛uencing
WSDalgorithms.Here,inordertoclassify
discretestructures,namelycontexts,wepropose
anewapproachtocontextproximitybasedon
Hausdor˙metricandsymmetricdi˙erenceof
sets:
A
4
B
=(
A
[
B
)
n
(
A
\
B
)
.
Fig.1.
Theset
A
4
B
istheshadedpartofcircles
RecallthenotionofHausdor˙metric.
Considerametricspace
(
X;%
)
where
X
isaset,
%
isametricin
X
.De˝nethe
""
-dilatation
A
+
""
ofaset
A
ˆ
X
A
+
""
=
[f
B
""
(
x
):
x
2
A
g
;
where
B
""
(
x
)
isaclosedballcenteredat
x
with
theradius
""
.
TheHausdor˙distance
%
H
(
A;B
)
between
compactnonemptysets
A
and
B
is
%
H
(
A;B
)=min
f
"">
0:(
A
ˆ
B
+
""
)
^
(
B
ˆ
A
+
""
)
g
;
where
A
+
""
,
B
+
""
arethe
""
-dilatationsof
A
and
B
.Considerthefollowingsets(Fig.2):
A
(
""
)=
A
\
(
B
+
""
)
;B
(
""
)=
B
\
(
A
+
""
)
:
Fig.2.
Twosets
A
+
""
and
B
+
""
arethe
""
-dilatations
ofsegments
A
and
B
,andtwonewproposed
set-valuedmaps
A
(
""
)
and
B
(
""
)
wereinspiredby
Hausdor˙distance
Then
%
H
(
A;B
)=min
f
"">
0:
A
(
""
)
[
B
(
""
)=
A
[
B
g
:
Considertwocontexts
W
1
=
f
w
11
;:::;w
1
m
g
,
W
2
=
f
w
21
;:::;w
2
n
g
,where
w
1
i
;w
2
j
arewords
inthecontexts,
i
=1
;::;m;j
=1
;:::;n
.
Denoteby
V
1
=
f
v
11
;:::;v
1
m
g
,
V
2
=
f
v
21
;:::;v
2
n
g
thesetsofvectors
v
1
i
;v
2
j
correspondingto
thewords
w
1
i
;w
2
j
.Recallthatgenerallyin
WSDprocedures,thedistancebetweenwords
ismeasuredbysimilarityfunction,whichis
acosineofanglebetweenvectorsrepresenting
words:
sim
(
v
1
;v
2
)=
(
v
1
;v
2
)
jj
v
1
jjjj
v
2
jj
,where
(
v
1
;v
2
)
is
ascalar(inner)productofvectors
v
1
;v
2
,and
jj
v
i
jj
isanormofvector,
i
=1
;
2
.Inwhatfollows,
sim
(
v
1
;v
2
)
2
[

1
;
1]
.Thus,thelessdistancethe
moresimilarity.Keepinginmindthelatter
remark,weintroducethefollowing
""
-proximity


150
ofvectorcontexts
V
1
;V
2
.Given
""
>
0
,construct
thesets
C
(
V
1
;V
2
;""
)=
f
u;v
:
u
2
V
1
;v
2
V
2
;sim
(
u;v
)
>
""
g
:
D
(
V
1
;V
2
;""
)=(
V
1
[
V
2
)
n
C
(
V
1
;V
2
)
:
Supposingthat
sim
playstheroleofametric,
then
C
(
V
1
;V
2
;""
)
isanalogoustotheexpression
A
(
""
)
[
B
(
""
)
inthede˝nitionoftheHausdor˙
distance.
Denoteby
j
Y
j
thepowerofaset
Y
ˆ
X
,
R
+
=
f
x
:
x
>
0
;x
2
R
g
.
De˝nition1.
The
K
-proximityofcontexts
V
1
;V
2
isthefunction
K
(
V
1
;V
2
;""
)=
j
C
(
V
1
;V
2
;""
)
j
j
V
1
[
V
2
j
:
Itisclearthat
K
(
V
1
;V
2
;""
)
2
[0
;
1]
.Wealso
de˝nethefollowingfunction.
De˝nition2.
The
~
K
-proximityofcontexts
V
1
;V
2
isthefunction
~
K
(
V
1
;V
2
;""
)=
j
C
(
V
1
;V
2
;""
)
j
1+
j
D
(
V
1
;V
2
;""
)
j
;
describingtheratioofand
elementsofsets.
Thede˝nitionimpliesthat
min
~
K
(
V
1
;V
2
;""
)=0
,
max
~
K
(
V
1
;V
2
;""
)=
j
V
1
[
V
2
j
.Thepresenceof
1inthedenominatorpermitstoavoidzero
denominatorwhen
j
D
(
V
1
;V
2
;""
)
j
=0
.
Theubiquitousdistance
%
betweencontexts
V
1
;V
2
isbasedonthesimilarityofaverage
vectors:
%
(
V
1
;V
2
)=
sim
(
V
1
;
V
2
)
.But
theexample(Fig.3)showsthatfortwo
geometricallydistantandnottoosimilar
structures
%
(
V
1
;V
2
)=1
,thatisthesimilarity
%
takesthemaximumvalue.
Example
Considerthesets
A
=
f
a
1
;a
2
;a
3
g
,
B
=
f
b
1
g
picturedinFig.3,where
a
1
+
a
3
=
!
0
,
a
2
=
b
1
.
Then,
sim
(
A;B
)=
sim
(
1
3
(
a
1
+
a
2
+
a
3
)
;b
1
)=
sim
(
a
2
;b
1
)=1
,
~
K
(
A;B;""
)=
2
3
,
K
(
A;B;""
)=
1
2
.
Theequalityofaveragevectorsdoesnotmean
thecoincidenceof
A
and
B
,whicharerather
di˙erent(Fig.3).
Fig.3.
Anexampleofsimilaraveragevectors(
A
=
a
2
=
b
1
=
B
)andtotallydi˙erentsetsofvectors:
f
a
1
;a
2
;a
3
g
and
f
b
1
g
Averagealgorithmwithsynonyms
""
-filtration
Considerasentence
S
w
=(
w
1
:::w

i
:::w
n
)
containingatargetword
w

i
(denoteitas
w

).
andavectorrepresentation
S
=(
v
1
:::v

i
:::v
n
)
of
S
w
,where
w
j
isaword,
v
j
isavector
representationof
w
j
.Denote
v

i
as
v

.Suppose
thetargetword
w

has
l
senses.Denoteby
syn
w
k
asynsetcorrespondingto
k
-thsense,
k
=
1
;:::;l
,
syn
w
k
=
f
w
k
1
;:::;w
ki
k
g
,where
w
kp
are
synonyms.Let
syn
k
=
f
v
k
1
;:::;v
ki
k
g
beaset
ofvectorrepresentationsofsynonyms
w
kp
,
p
=
1
;:::;i
k
:
Inwhatfollows,weintroduceaprocedureof
""
-˝ltration,theideaofwhichisborrowedfrom
thepaper[2].
Thesynset˝ltrationistheformationofa
socalledcandidatesetwhichconsistsofthose
synonymswhosesimilaritywiththewordsfrom
asentenceishigherthanasimilaritythreshold
""
.
The˝rstaveragealgorithm1,described
below,usesaveragevectorsofwordsofsentences
andaveragevectorsofthecandidatesetof
synonymsinsynsets.
Thisalgorithmcontainsthefollowinglines.
Line1.Calculatetheaveragevectorofwords
ofthesentence
S
S
=
1
n
n
X
j
=1
v
j
LinesGiven
"">
0
,letusconstructthe
˝lteredsetofsynonymsforeachsynset
cand
k
(
""
)=
f
u
2
syn
k
:
u
6
=
v

;sim
(
u;v

)
>""
g
:
Denoteby
s
k
(
""
)=
j
(
cand
k
(
""
))
j
thepowerofaset
cand
k
(
""
)
.
Line7.Calculatefor
s
k
(
""
)
>
0
theaverage
vectorofthesynsetcandidates
syn
k
(
""
)=
1
s
k
(
""
)
X
u
2
cand
k
(
""
)
u:
If
s
k
(
""
)=0
,thenlet
syn
k
(
""
)
beequaltothezero
vector.
Line8.Calculatethesimilarityoftheaverage
vectorsofthesentenceandthe
k
-th˝ltered
synset
sim
k
(
""
)=
sim
(
syn
k
(
""
)
;
S
)
:
LineSuppose
max
k
=1
;:::;l
f
sim
k
(
""
)
g
=
sim
k

(
""
)
,i.e.
k

2f
1
;:::;l
g
isthenumberofthe
largest
sim
k
(
""
)
.If
k

isnotunique,thentake


151
Algorithm1:
Averagealgorithmwithsynonyms
""
-˝ltration
Data
:
v

vectorofthetargetword
w

with
l
senses(synsets),
v
i
2
S
,
S
sentencewiththetargetword
w

,
v

2
S
,
f
syn
k
g
synsetsofthetargetword,thatis
syn
k
3
v

,
k
=
1
;l
.
Result
:
k

2f
1
;:::;l
g
isthenumberofthesenseoftheword
w

inthesentence
S
.
1
S
=
1
n
n
P
j
=1
v
j
,
theaveragevectorofwordsofthesentence
S
2
do
3
take
"">
0
foreachsynsetofthetargetword
4
foreach
syn
k
3
v

do
constructthefilteredset
cand
k
(
""
)
ofthesynset
syn
k
:
5
cand
k
(
""
)=
f
u
2
syn
k
:
u
6
=
v

;sim
(
u;v

)
>""
g
6
s
k
(
""
)=
j
cand
k
(
""
)
j
,
numberofcandidatesofsynonyms
theaveragevectorofsynsetcandidates:
7
syn
k
(
""
)=
8
<
:
1
s
k
(
""
)
P
u
2
cand
k
(
""
)
u;
if
s
k
(
""
)
>
0
!
0
;
if
s
k
(
""
)=0
thesimilarityofaveragevectorsofthesentenceandthe
k
-thfilteredsynset:
8
sim
k
(
""
)=
sim
(
syn
k
(
""
)
;
S
)
9
end
10
sim
k

(
""
)=max
k
=1
;:::;l
f
sim
k
(
""
)
g)
k

2f
1
;:::;l
g
,
k

isthenumberofthelargest
sim
k
(
""
)
11
while
k

isnotunique
another
"">
0
andrepeattheprocedurefrom
line3.
Result:thetargetword
w

hasthesense
correspondingtothe
k

-thsynset
syn
w
k

.
Remark:inthecase
""
=0
,wedenotethis
algorithmas
A
0
-algorithm.Inthiscase,the
traditionalaveragingofsimilarityisused.
Note
.
A
0
-algorithmwasusedinour
experiments,itwasimplementedinPython.
1
A
0
-algorithmexample
Asimpleexampleand˝gures4willhelpto
understandhowthis
A
0
-algorithmworks.
Takesomedictionaryword
w
2
withseveral
sensesandseveralsynonymsets(forexample,
syn
1
and
syn
2
)andthesentence
S
withthisword
(Fig.4).Thetaskistoselectameaning(synset)
of
w
2
(thatisthetargetwordis
w

2
)usedinthe
sentence
S
viathe
A
0
-algorithm.
Letusmatchtheinputdataandthesymbols
usedinthe
A
0
-algorithm.Theword
(sluzhit')correspondstothevector
v
2
.
Fig.4.
DigestoftheWiktionaryentry
(sluzhit')andmeanvectors
syn
1
and
syn
2
ofthe
synonymssets
syn
1
,
syn
2
andthesentence
S
with
thisword
w

2
1
Seethefunction
selectSynsetForSentenceByAverageSimilarity
inthe˝le
https://github.com/componavt/
wcorpus.py/blob/master/src/test_synset_for_sentence/lib_sfors/synset_selector.py


152
Fig.5.
Samplesourcedataare(1)vertices
v
1
:::v
5
correspondingtowordsofthesentence
S
,the
vertex
v
2
wasexcludedsinceitcorrespondstothe
targetword
w

2
,and(2)thetargetword
w

2
with
twosynsets
syn
1
and
syn
2
(Fig.4),(3)vertices
(vectorscorrespondtowords)ofthe˝rstsynsetare
f
v
1
syn
1
;v
2
syn
1
g
andthesecondsynset
f
v
1
syn
2
;v
2
syn
2
g
Fig.6.
Similaritybetweenthemeanvalueofvectors
ofthesentenceandthe˝rstsynonymsetislower
thanthesimilaritywiththesecondsynset,thatis
sim
(
syn
1
;
S
)
<sim
(
syn
2
;
S
)
.Thus,thesecondsense
ofthetargetword
w

2
(thesecondsynset
syn
2
)will
beselectedinthesentence
S
by
A
0
-algorithm
Thereisadictionaryarticleaboutthisword
intheWiktionary,seeFig.4(aparseddatabase
ofWiktionaryisusedinourprojects).
2
TwosynonymsetsofthisWiktionaryentry
aredenotedby
syn
1
and
syn
2
.
Meanvaluesofthevectorscorrespondingto
synonymsinthesesynsetswillbedenotedas
syn
1
and
syn
2
,and
S
isthemeanvectorofall
vectorscorrespondingtowordsinthesentence
S
containingtheword(sluzhit').
Averagealgorithmwithsentence
andsynonyms
""
-filtration(
A
""
)
Thisalgorithm2isamodi˝cationof
algorithm1.The˝ltrationofasentenceisadded
tosynset˝ltration.Namely,weselectaword
fromthesentenceforwhichthesimilaritywithat
leastonesynonymfromthesynsetishigherthan
thesimilaritythreshold
""
.Then,weaveragethe
setofselectedwordsformingthesetofcandidates
fromthesentence.Letusexplainalgorithm2line
byline.
LinesGiven
"">
0
,letusconstructtheset
ofwordsofthesentence
S
˝lteredbysynonyms
ofthe
k
-thsynset
syn
k
cand
k
S
(
""
)=
f
v
2
S
:
9
u
2
syn
k
;sim
(
v;u
)
>"";
v
6
=
v

;u
6
=
v

g
Denoteby
S
k
(
""
)=
j
cand
k
S
(
""
)
j
thepowerof
theset
cand
k
S
(
""
)
.
Line6.Calculatetheaveragevectorofwords
ofthe˝lteredsentence
cand
k
S
(
""
)=
1
S
k
(
""
)
X
v
2
cand
k
S
(
""
)
v
If
S
k
(
""
)=0
,thenlet
cand
k
S
(
""
)
beequaltothe
zerovector.
LinesConstruct˝lteredsetsofsynonyms
candsyn
k
(
""
)=
f
u
2
syn
k
:
9
v
2
S;sim
(
u;v
)
>"";
u
6
=
v

;v
6
=
v

g
:
Denoteby
s
k
(
""
)=
j
candsyn
k
(
""
)
j
thepower
ofthe
k
-th˝lteredsynonymset.
Line9.Calculatefor
s
k
(
""
)
>
0
theaverage
vectorofthe
k
-thsynsetofcandidates
candsyn
k
(
""
)=
1
s
k
(
""
)
X
u
2
candsyn
k
(
""
)
u:
If
s
k
(
""
)=0
,then
candsyn
k
(
""
)
equalstothezero
vector.
Line10.Calculatethesimilarityofthe
averagevectorsofthe˝lteredsentenceandthe
k
-th˝lteredsynset
sim
k
(
""
)=
sim
(
cand
k
S
(
""
)
;
candsyn
k
(
""
))
:
LinesSuppose
max
k
=1
;:::;l
f
sim
k
(
""
)
g
=
sim
k

(
""
)
,i.e.
k

2f
1
;:::;l
g
isthenumberofthe
largest
sim
k
(
""
)
.If
k

isnotuniquethentake
another
"">
0
andrepeattheprocedurefrom
line2.
Result:thetargetword
w

inthesentence
S
hasthesensecorrespondingtothe
k

-thsynset
syn
w
k

.
ThisalgorithmwasimplementedinPython.
3
2
Seesectioneboftoolsandonpage156.
3
Seethefunction
selectSynsetForSentenceByAverageSimilarityModi˝ed
inthe˝le
https://github.com/
componavt/wcorpus.py/blob/master/src/test_synset_for_sentence/lib_sfors/synset_selector.py


153
Algorithm2:
Averagealgorithmwithsentenceandsynonyms
""
-˝ltration(
A
""
)
Data
:
v

vectorofthetargetword
w

with
l
senses(synsets),
v
i
2
S
,
S
sentencewiththetargetword
w

,
v

2
S
,
f
syn
k
g
synsetsofthetargetword,thatis
syn
k
3
v

,
k
=
1
;l
.
Result
:
k

2f
1
;:::;l
g
isthenumberofthesenseoftheword
w

inthesentence
S
.
1
do
2
take
"">
0
foreachsynsetofthetargetword
3
foreach
syn
k
3
v

do
constructthesetofwordsofthesentence
S
filteredbysynonymsofthe
k
-thsynset
syn
k
:
4
cand
k
S
(
""
)=
f
v
2
S
:
9
u
2
syn
k
;sim
(
v;u
)
>"";v
6
=
v

;u
6
=
v

g
5
S
k
(
""
)=
j
cand
k
S
(
""
)
j
,
numberofcandidatesofthesentence;
theaveragevectorofsentencecandidates:
6
cand
k
S
(
""
)=
8
<
:
1
S
k
(
""
)
P
v
2
cand
k
S
(
""
)
v;
if
S
k
(
""
)
>
0
!
0
;
if
S
k
(
""
)=0
""
-filtrationofthesynset
syn
k
bythesentence
S
:
7
candsyn
k
(
""
)=
f
u
2
syn
k
:
9
v
2
S;sim
(
u;v
)
>"";u
6
=
v

;v
6
=
v

g
8
s
k
(
""
)=
j
candsyn
k
(
""
)
j
,
numberofcandidatesofsynonyms
theaveragevectorofsynsetcandidates:
9
candsyn
k
(
""
)=
8
<
:
1
s
k
(
""
)
P
u
2
candsyn
k
(
""
)
u;
if
s
k
(
""
)
>
0
!
0
;
if
s
k
(
""
)=0
thesimilarityoftheaveragevectorsofthesentenceandthe
k
-thfilteredsynset:
10
sim
k
(
""
)=
sim
(
cand
k
S
(
""
)
;
candsyn
k
(
""
))
11
end
12
sim
k

(
""
)=max
k
=1
;:::;l
f
sim
k
(
""
)
g)
k

2f
1
;:::;l
g
,
k

isthenumberofthelargest
sim
k
(
""
)
13
while
k

isnotunique
K
-algorithmbasedon
""
-dilatation
Thealgorithm3(
K
-algorithm)isbasedon
thefunction
~
K
(
A;B;""
)
(seeprevioussection

""
-proximitybetween˝niteon
page150),where
A
=
syn
k
,thatis
k
-thsynset,
and
B
=
S
,where
S
isasentence.Thealgorithm
includesthefollowingsteps.
LinesGiven
"">
0
,letusconstructthe
C
k
(
""
)
setofwordsofthe
k
-thsynsetand
thesentence
S
.
Line5.Denoteby
D
k
(
""
)
thesetofstan
words
D
k
(
""
)=(
S
[
syn
k
)
n
C
k
(
""
)
:
Line6.Calculate
~
K
k
(
""
)
astheratioof
andelementsofthesets
~
K
k
(
""
)=
j
C
k
(
""
)
j
1+
j
D
k
(
""
)
j
:
LinesSuppose
max
k
=1
;:::;l
~
K
k
(
""
)=
~
K
k

(
""
)
.
If
k

isnotunique,thentakeanother
"">
0
and
repeattheprocedurefromline2.
Algorithm3:
K
-algorithmbasedon
""
-dilatation
Data
:
v

vectoroftargetword
w

with
l
senses(synsets),
v
i
2
S
,
v

2
S
,
f
syn
k
g
synsetsof
v

,
k
=
1
;l
.
Result
:
k

2f
1
;:::;l
g
isthenumberof
thesenseoftheword
w

inthe
sentence
S
.
1
do
2
take
"">
0
foreachsynsetofthetargetword
3
foreach
syn
k
3
v

do
setofnearwords:
4
C
k
(
""
)=
f
u;v
:
u
2
syn
k
;v
2
S;sim
(
u;v
)
>""
g
setofdistantwords:
5
D
k
(
""
)=(
S
[
syn
k
)
n
C
k
(
""
)
ratioofand
elementsofthesets:
6
~
K
k
(
""
)=
j
C
k
(
""
)
j
1+
j
D
k
(
""
)
j
7
end
getthenumberofthelargestratio
k

8
~
K
k

(
""
)=max
k
=1
;:::;l
~
K
k
(
""
)
9
while
k

isnotunique


154
Result:thetargetword
w

hasthesense
correspondingtothe
k

-thsynset
syn
w
k

.
AnexampleofconstructingCandDsetsis
presentedinFig.7andTable.Itusesthesame
sourcedataasforthe
A
0
-algorithm,seeFig.5.
Remark.Thisalgorithmisapplicabletothe
K
-functiondescribedintheprevioussection
3
aswell.Thisalgorithmwasimplementedin
Python.
4
Moredetailsforthisexample(Fig.7)are
presentedinTable,whichshows
C
and
D
sets
withdi˙erent
""
andvaluesofthe
~
K
-function.
Boldtypeofword-verticesinTableindicates
newvertices.Thesenewverticesarecapturedby
asetofvertices
C
andtheseverticesare
excludedfromthesetofvertices
D
with
eachsubsequentdilatationextensionwitheach
subsequent
""
.Forexample,inthetransitionfrom
""
1
to
""
2
theset
D
2
(
""
1
)
losesthevertex
v
3
.During
thistransition
""
1
!
""
2
theset
C
2
(
""
2
)
getsthe
samevertex
v
3
incomparisonwiththeset
C
2
(
""
1
)
.
InFig.8,thefunction
~
K
1
(
""
)
showsthe
proximityofthesentence
S
andthesynset
syn
1
,
thefunction
~
K
2
(
""
)
theproximityof
S
andthe
synset
syn
2
.ItcanbeseeninFigure8thatwith
decreasing
""
,thevalueof
~
K
2
(
""
)
growsfasterthan
~
K
1
(
""
)
.
Therefore,thesentence
S
isclosertothe
secondsynset
syn
2
.Thesameresultcanbeseen
inthepreviousFig.7.
Fig.7.
Anexampleofseriesof
C
k
(
""
)
(setsofwordsof
k
-thsynsetwhicharecloseandneartothesentence
S
)inthe
K
-algorithmbasedon
""
-dilatation.The
growthofthedilationoftheverticesofthesecond
synset
f
v
1
syn
2
;v
2
syn
2
g
capturestheverticesofthe
sentence
S
=
f
v
1
;v
3
;v
4
;v
5
g
fasterthanthedilation
oftheverticesofthe˝rstsynset.Inothersymbols:
(
syn
2
+
""
)
\
S
˙
(
syn
1
+
""
)
\
S
.Thatis,accordingto
the
K
-algorithm,thesecondvalueoftheword-vector
v
2
,representedbythesynset
syn
2
,willbeselected
forthesentence
S
Fig.8.
Left-continuousstepfunctions
~
K
1
(
""
)
,
~
K
2
(
""
)
showthatthesentence
S
isclosertothesecondsynset
syn
2
4
Seethefunction
selectSynsetForSentenceByAlienDegree
inthe˝le
https://github.com/componavt/wcorpus.py/
blob/master/src/test_synset_for_sentence/lib_sfors/synset_selector.py


155
Anexampleofthe
K
-algorithmtreatingtheword
w
2
,whichhastwosynsets
syn
1
,
syn
2
andthesentence
S
,
where
w
2
2
S
,seeFig.4.Thenumberofthealgorithmiterationcorrespondstotheindexof
""
.Lettheseries
of
""
beorderedsothat
1=
""
0
>""
1
>""
2
>:::>""
7
=

1
.Itisknownthat
j
C
1
[
D
1
n
v
2
j
=
j
S
n
v
2
j
=6
,that
isthetotalnumberofwordsinthesynsetsandinthesentenceareconstants.
""C
2
(
""
)
D
2
(
""
)
j
C
2
jj
D
2
j
~
K
2
(
""
)
~
K
k
(
""
)=
j
C
k
(
""
)
j
1+
j
D
k
(
""
)
j
""
0
?
v
1
,
v
3
,
v
4
,
v
5
,
v
1
syn
2
,
v
2
syn
2
060.0
""
1
v
1
,
v
2
syn
2
v
3
,
v
4
,
v
5
,
v
1
syn
2
24
2
5
""
2
v
1
,
v
2
syn
2
,
v
3
v
4
,
v
5
,
v
1
syn
2
33
3
4
""
3
v
1
,
v
2
syn
2
,
v
3
,
v
1
syn
2
v
4
,
v
5
42
4
3
C
1
(
""
)
D
1
(
""
)
j
C
1
jj
D
1
j
~
K
1
(
""
)
""
4
v
2
syn
1
,
v
4
v
1
syn
1
,
v
1
,
v
3
,
v
5
24
2
5
C
2
(
""
)
D
2
(
""
)
j
C
2
jj
D
2
j
~
K
2
(
""
)
""
5
v
1
,
v
2
syn
2
,
v
3
,
v
1
syn
2
,
v
4
,
v
5
,
?
606
C
1
(
""
)
D
1
(
""
)
j
C
1
jj
D
1
j
~
K
1
(
""
)
""
6
v
2
syn
1
,
v
4
,
v
1
syn
1
v
1
,
v
3
,
v
5
33
3
4
Experiments
Weboftoolsandresources
Thissectiondescribestheresourcesusedin
ourresearch,namely:Wikisource,Wiktionary,
WCorpusandRusVectores.
ThedevelopedWCorpus
5
systemincludes
textsextractedfromWikisourceandprovides
theuserwithatextcorpusanalysistool.This
systemisbasedontheLaravelframework(PHP
programminglanguage).MySQLdatabaseis
used.
6
Wikisource
.ThetextsofWikipediahave
beenusedasabasisforseveralcontemporary
corpora[5].Butthereisnomentionofusingtexts
fromWikisourceintextprocessing.Wikisourceis
anopenonlinedigitallibrarywithtextsinmany
languages.Wikisourcesitescontains10millions
oftexts
7
inmorethan38languages.
8
Russian
Wikisource(thedatabasedumpasofFebruary
2017)wasusedinourresearch.
Textsparsing
.ThetextsofWikisourcewere
parsed,analysedandstoredtotheWCorpus
database.Letusdescribethisprocessindetail.
Thedatabasedumpcontainingalltextsof
RussianWikisourcewastakenfrom
Dosite.
9
TheseWikisourcedatabase
˝leswereimportedintothelocalMySQL
databasetitledinFig.9,
whereCorpusPisthesetofWCorpus
PHP-scriptswhichanalyseandparsethetextsin
thefollowingthreesteps.
1.
First,thetitleandthetextofan
articlefromtheWikisourcedatabaseare
extracted(560thousandsoftexts).Onetext
correspondstoonepageonWikisourcesite.
Itmaybesmall(forexample,severallinesof
apoem),medium(chapterorshortstory),
orhugesize(e.g.thesizeofthepagewith
thenovellaEternalwritten
byFyodorDostoyevskyis500KB).Text
preprocessingincludesthefollowingsteps:

TextswritteninEnglishandtextsin
Russianorthographybefore1918were
excluded;about12thousandstexts
wereexcluded.

Serviceinformation(wikimarkup,
references,categoriesandsoon)was
removedfromthetext.

Veryshorttextswereexcluded.As
aresult,377thousandtextswere
extracted.

Textssplittingintosentencesproduced
6millionsofsentences.

Sentencesweresplitintowords(1.5
millionsofuniquewords).
5
https://github.com/componavt/wcorpus
6
SeeWCorpusdatabasescheme:
https://github.com/componavt/wcorpus/blob/master/doc/workbench/db_
scheme.png
7
https://stats.wikimedia.org/wikisource/EN/TablesWikipediaZZ.htm
8
https://stats.wikimedia.org/wikisource/EN/Sitemap.htm
9
https://dumps.wikimedia.org/backup-index.html


156
Fig.9.
ThearchitectureofWCorpussystemandtheuseofotherresources
3.
Secondly,wordformswerelemmatized
usingphpMorphy
10
program(0.9million
lemmas).
4.
Lastly,lemmas,wordforms,sentencesand
relationsbetweenwordsandsentenceswere
storedtoWCorpusdatabase(Fig.9).
Inourpreviousworkattributes
ofsynonym[6]wealsousedneuralnetwork
modelsofthegreatprojectRusVectores
11
,which
isakindofaword2vectoolbasedonRussian
texts[9].
Contextsimilarityalgorithmsevaluation
InordertoevaluatetheproposedWSD
algorithms,severalwordswereselectedfroma
dictionary,thensentenceswiththesewordswere
extractedfromthecorpusandtaggedbyexperts.
Ninewords
Onlypolysemouswordswhichhaveatleast
twomeaningswithdi˙erentsetsofsynonymsare
suitableforourevaluationofWSDalgorithms.
Thefollowingcriteriafortheselectionof
synonymsandsetsofsynonymsfromRussian
Wiktionarywereused:
1.
Onlysingle-wordsynonymsareextracted
fromWiktionary.Thisisduetothe
factthattheRusVectoresneuralnetwork
modelora_20used
inourresearchdoesnotsupportmultiword
expressions.
2.
Ifawordhasmeaningswithequalsets
ofsynonyms,thenthesesetswereskipped
becauseitisnotpossibletodiscerndi˙erent
meaningsofthewordusingonlythese
synonymswithoutadditionalinformation.
10
https://packagist.org/packages/componavt/phpmorphy
11
http://rusvectores.org/en/
12
http://whinger.krc.karelia.ru/soft/wikokit/index.html
13
https://github.com/componavt/piwidict
14
SeeinformationaboutthesubcorpusinthesectiontencesofthreeRussianonpage158.


157
Alistofpolysemouswordswasextractedfrom
theparsedRussianWiktionary
12
usingPHPAPI
piwidict
13
(Fig.9).
Thus,9polysemousRussianwords(presented
inthesubcorpus
14
)wereselectedbyexpertsfrom
thisWiktionarylist,namely:(bezdna),
(brosat'),(vidnyy),
(donesti),(donosit'),
(zanyatiye),î(likhoy),
(otsyuda),äà(udachno).Thetenth
word(sluzhit')wasleftoutof
consideration,becausethereare1259of1308
sentenceswiththisfrequentwordtobetagged
byexpertsinthefuture(Fig.10).
Fig.10.
Russianverb(sluzhit')hassevenmeaningsandsevensynsetsinthedevelopedsystem
WCorpus.49sentencesarealreadylinkedtorelevantsensesofthisverb.1259sentencesremaintobetagged
byexperts
SentencesofthreeRussianwriters
Thesentenceswhichcontainpreviously
de˝ned9wordsweretobeselectedfrom
thecorpusandtaggedbyexperts.Butthe
Wikisourcecorpuswastoohugeforthispurpose.
So,inourresearchasubcorpusofWikisource
textswasused.Thesearethetextswritten
byFyodorDostoevsky,LeoTolstoyandAnton
Chekhov.
AnalysisofthecreatedWCorpusdatabase
withtextsofthreewritersshowsthatthe
subcorpuscontains:
15

2635texts;

333thousandsentences;

215thousandwordforms;

76thousandlemmas;

4.3millionwordform-sentencelinks;
Textsofthissubcorpuscontain1285sentences
withthese9words,wherein9wordshaveintotal
42synsets(senses).ItwasdevelopedAgraphical
userinterface(webform)oftheWCorpussystem
(Fig.10)wasdeveloped,whereexpertsselected
oneofthesensesofthetargetwordforeachof
the1285sentences.
Thissubcorpusdatabasewithtagged
sentencesandlinkedsynsetsisavailable
online[7].
Textprocessingandcalculations
These1285sentenceswereextractedfromthe
corpus.Sentencesweresplitintotokens.Then
wordformswereextracted.Allthewordforms
werelowercaseandlemmatized.Therefore,
asentenceisabagofwords.Sentenceswithonly
onewordwereskipped.
ThephpMorpylemmatizertakesa
wordformandyieldspossiblelemmaswiththe
correspondingpartofspeech(POS).Information
onPOSofawordisneededtoworkwiththe
RusVectores'predictionneuralnetworkmodel
becausetogeta
vectoritisnecessarytoaskforawordandPOS,
forexampleOnlynouns,verbs,
adjectivesandadverbsremaininasentencebag
ofwords,otherwordswereskipped.
Thecomputerprogram(Pythonscripts)
whichworkswiththeWCorpusdatabaseand
RusVectoreswaswrittenandpresentedinthe
15
SeeSQL-queriesappliedtothesubcorpus
https://github.com/componavt/wcorpus/wiki/SQL
16
https://github.com/componavt/wcorpus.py


158
formoftheproject
wcorpus.py
atGitHub.
16
Thesourcecodeinthe˝le
synset_selector.py
17
implementsthreealgorithmsdescribedinthe
article,namely:

A
0
-algorithmimplementedinthefunction
selectSynsetForSentenceByAverageSimila-
rity()
;

K
-algorithmfunction
selectSynsetForSen-
tenceByAlienDegree()
;

A
""
-algorithmfunction
selectSynsetForSen-
tenceByAverageSimilarityModi˝ed()
.
Thesethreealgorithmscalculatedand
selectedoneofthepossiblesynsetsforeachof
1285sentences.
Twoalgorithms(
K
and
A
""
)haveaninput
parameterof
""
,therefore,acyclewithastepof
0.01from0to1wasadded,whichresultedin100
iterationsforeachsentence.
Then,answersgeneratedbythealgorithms
werecomparedwiththesynsetsselectedby
experts.
Thenumberofsentenceswiththesense
correctlytaggedbythe
K
-algorithmfornine
RussianwordspresentedinFig.11.
Thelegendofthis˝gureliststargetwords
withnumbersinbrackets
(
X;Y
)
,where
X
isthe
numberofsentenceswiththesewords,
Y
isthe
numberofsenses.
Thecurvesforthewords
Acyansolidlinewithstarpoints)
andgreensolidline
withtrianglepoints)arequitehighforsome
""
,
because(1)therearemanysentenceswiththese
words(352and308)inoursubcorpus,(2)these
wordshavefewmeanings(3and2).
Fig.11.
NumberofsentenceswiththecorrecttaggedsensefornineRussianwordsbythe
K
-algorithm
17
https://github.com/componavt/wcorpus.py/blob/master/src/test_synset_for_sentence/lib_sfors/
synset_selector.py


159
Fig.12.
NormaliseddatawiththefractionofsentenceswithcorrectlytaggedsensefornineRussianwords
Moremeanings,poorerresults.
Ifawordhasmoremeanings,thenthe
algorithmyieldsevenpoorerresults.Itisvisible
inthenormaliseddata(Fig.12),whereexamples
withgoodresultsare(OTSYUDA)
and(LIKHOY,pinkdashdotline
withdiamondpoints)with2meanings;the
example(BROSAT',redbold
dottedline)with9meaningshastheworstresult
(thelowestdottedcurve).
Comparisonofthreealgorithms
Letuscomparethreealgorithmsbysumming
theresultsforallninewords.Fig.13containsthe
followingcurves:

A
0
-algorithmlongdashblueline;

K
-algorithmsolidredline;

A
""
-algorithmdashyellowline.
The
A
0
-algorithmdoesnotdependon
""
.It
showedmediocreresults.
The
K
-algorithmyieldsbetterresultsthan
A
""
-algorithmwhen
"">
0
:
15
.
The
K
-algorithmshowedthebestresultson
theinterval[0.15;0.35].Namely,morethan700
sentences(outof1285human-taggedsentences)
wereproperlytaggedwiththe
K
-algorithmon
thisinterval(Fig.13).
Fig.13.
Comparisonof
A
0
-algorithm,
K
-algorithm,
A
""
-algorithm


160
Comparisonoffouralgorithmsasapplied
toninewords
Letuscomparetheresultsofrunningfour
algorithmsforeachwordseparately(Fig.14):

A
0
-algorithmlongdashbluelinewith
trianglepoints;

K
-algorithmsolidredlinewithsquare
points;

A
""
-algorithmdashyellowlinewithcircle
points;

frequentgreendashed
linewithXmarks.
Thesimpletfrequent
algorithmwasaddedtocomparetheresults.This
algorithmdoesnotdependonthevariable
""
,it
selectsthemeaning(synset)thatisthemost
frequentinourcorpusoftexts.InFig.14this
algorithmcorrespondstoagreendashedline
withXmarks.
Theresultsofthefrequent
algorithmand
A
0
-algorithmaresimilar(Fig.14).
The
K
-algorithmistheabsolutechampion
inthiscompetition,thatisforeachword
thereexistsan
""
suchthatthe
K
-algorithm
outperformsotheralgorithms(Fig.14).
Letusexplainthecalculationofthecurvesin
Fig.14.
Forthe
A
0
-algorithmandthefrequent
algorithm,themeaning(synset)is
calculatedforeachoftheninewordsonthesetof
1285sentences.Thus,
1285

2
calculationswere
performed.
Andagain,the
A
""
-algorithmandthe
K
-algorithmdependonthevariable
""
.Buthow
cantheresultsbeshownwithoutthe
""
axis?Ifat
leastonevalueof
""
givesapositiveresult,then
wesupposethattheWSDproblemwascorrectly
solvedforthissentencebythealgorithm.
ThevalueontheYaxisfortheselectedword
(for
A
""
-algorithmand
K
-algorithm)isequalto
thesumofsuchcorrectlydeterminedsentences
(withdi˙erentvaluesof
""
)inFig.14.
Fig.14.
Comparisonof
A
0
-algorithm,
K
-algorithm,
A
""
-algorithmandthemostfrequentmeaning
Perhapsitwouldbemorecorrectto˝x
""
correspondingtothemaximumnumberof
correctlydeterminedsentences.Then,theresult
willnotbesooptimistic.
Toshowthecomplexityofcomparingand
evaluating
""
-algorithms(thatis,algorithmsthat
dependon
""
),letustrytoanalyzetheresultsof
the
K
-algorithm,showninFig15.
Thepercentage(proportion)ofcorrectly
determined1285sentencesfor9wordsbythe
K
-algorithm,wherethe
""
variablechangesfrom0
to1inincrementsof0.01,ispresentedinFig.15.
Thus,
1285

100
calculationswereperformed.


161
Theseproportionsaredistributedoverasetof
possiblecalculatedresultsfrom0%(nosentence
isguessed)to100%(allsentencesareguessed)
foreachofninewords.
ThisFigure15doesnotshowwhich
""
-values
producebetterorpoorerresults,althoughit
couldbeseeninFigures13.ButtheFigure
doesshowthesetandthequalityoftheresults
obtainedwiththehelpofthe
K
-algorithm.For
example,theword(likhoy)with22
sentencesand100di˙erent
""
hasonly8di˙erent
outcomesofthe
K
-algorithm,sevenofwhichlie
intheregionabove50%,thatis,morethaneleven
sentencesareguessedatany
""
.
Forexample,theword(brosat')
hasthelargestnumberofmeaningsinourdata
set,ithas9synonymsetsinourdictionary
and11meaningsinRussianWiktionary.
18
Àll
possibleresultsofthe
K
-algorithmforthisword
aredistributedintherangeofThe
maximumshareofguessedsentencesis30.61%.
Notethatthisvalueisachievedwhen
""
=0
:
39
,
andthisisclearlyshowninFigure12,seethe
thickdottedline.
Allcalculations,chartsdrawnfrom
experimentaldataandresultsoftheexperiments
areavailableonlineinGoogleSheets[8].
Fig.15.
Proportionsofcorrectlyguessedsentencesdistributedoverasetofpossiblecalculatedresults
Conclusions
Thedevelopmentofthecorpusanalysis
systemWCorpus
19
wasstarted.377thousand
textswereextractedfromRussianWikisource,
processedanduploadedtothiscorpus.
Context-predictivemodelsoftheRusVectores
projectareusedtocalculatethedistancebetween
lemmas.ScriptsinPythonweredevelopedto
processRusVectoresdata,seethe
wcorpus.py
projectontheGitHubwebsite.
TheWSDalgorithmbasedonanewmethod
ofvector-wordcontextsproximitycalculationis
proposedandimplemented.Experimentshave
shownthatinanumberofcasesthenew
algorithmshowsbetterresults.
ThefutureworkismatchingRussianlexical
resources(Wiktionary,WCorpus)toWikidata
objects[11].
ThestudywassupportedbytheRussian
FoundationforBasicResearch,grant
No.18-012-00117
.
References
1.
AroraS.,LiangY.,MaT.
Asimplebut
tough-to-beatbaselineforsentenceembeddings.
InProceedingsoftheICLR
,2017.P.
URL:
https://pdfs.semanticscholar.org/
3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.
pdf
(accessdate:3.04.2018).
2.
ChenX.,LiuZ.,SunM.
Auni˝edmodel
forwordsenserepresentationanddisambiguation.
InProceedingsoftheEMNLP
,2014.P.
1035.doi:10.3115/v1/d14-1110.URL:
http:
//www.aclweb.org/anthology/D14-1110
(access
date:3.04.2018).
18
https://ru.wiktionary.org/wiki/áðîñàòü
19
https://github.com/componavt/wcorpus


162
3.
ChoiS.S.,ChaS.H.,TappertC.C.
Asurvey
ofbinarysimilarityanddistancemeasures.
Journal
ofSystemics,CyberneticsandInformatics
.2010.
Vol.8.no.1.P.URL:
http://citeseerx.
ist.psu.edu/viewdoc/download?doi=10.1.1.
352.6123&rep=rep1&type=pdf
(accessdate:
3.04.2018).
4.
HausslerD.
Convolutionkernelsondiscrete
structures.
Technicalreport,Departmentof
ComputerScience,UniversityofCaliforniaat
SantaCruz
.1999.URL:
https://www.soe.ucsc.
edu/sites/default/files/technical-reports/
UCSC-CRL-99-10.pdf
(accessdate:3.04.2018).
5.
JurczykT.,DeshmaneA.,ChoiJ.
Analysisof
Wikipedia-basedcorporaforquestionanswering.
arXivpreprintarXiv:1801.02073
.2018.URL:
http://arxiv.org/abs/1801.02073
(accessdate:
3.04.2018).
6.
KrizhanovskyA.,KirillovA.
Calculated
attributesofsynonymsets.
arXivpreprint
arXiv:1803.01580
.2018.URL:
http://arxiv.org/
abs/1803.01580
(accessdate:3.04.2018).
7.
KrizhanovskyA.,KirillovA.,KrizhanovskayaN.
WCorpusmysqldatabasewithtextsof3writers.
˝gshare
.2018.URL:
https://doi.org/10.6084/
m9.figshare.5938150.v1
(accessdate:3.04.2018).
8.
KrizhanovskyA.,KirillovA.,KrizhanovskayaN.
Assignsensestosentencesof3writers.
Google
Sheets
.2018.URL:
http://bit.ly/2I14QIT
(accessdate:27.04.2018).
9.
KutuzovA.,KuzmenkoE.
Textsin,meaning
out:neurallanguagemodelsinsemanticsimilarity
taskforRussian.
arXivpreprintarXiv:1504.08183
.
2015.URL:
https://arxiv.org/abs/1504.08183
(accessdate:3.04.2018).
10.
LesotM-J.,RifqiM.,BenhaddaH.
Similarity
measuresforbinaryandnumericaldata:a
survey.
InternationalJournalofKnowledge
EngineeringandSoftDataParadigms
.2009.Vol.1.
no.1.P.doi:10.1504/ijkesdp.2009.021985.
URL:
http://citeseerx.ist.psu.edu/viewdoc/
download?doi=10.1.1.212.6533&rep=rep1&
type=pdf
(accessdate:3.04.2018).
11.
NielsenF.
LinkingImageNetWordNetSynsets
withWikidata.
InWWW'18Companion:The
2018WebConferenceCompanion
.2018.URL:
https://arxiv.org/pdf/1803.04349.pdf
(access
date:18.04.2018).
ReceivedMarch31,2018
ÑÂÅÄÅÍÈßÎÁÀÂÒÎÐÀÕ:CONTRIBUTORS:
ÊèðèëëîâÀëåêñàíäðÍèêîëàåâè÷
âåäóùèéíàó÷íûéñîòðóäíèê,ä.ô.-ì.í.
Èíñòèòóòïðèêëàäíûõìàòåìàòè÷åñêèõèññëåäîâàíèé
ÊàðÍÖÐÀÍ,Ôåäåðàëüíûéèññëåäîâàòåëüñêèéöåíòð
¾Êàðåëüñêèéíàó÷íûéöåíòðÐÀÍ¿
óë.Ïóøêèíñêàÿ,11,Ïåòðîçàâîäñê,
ÐåñïóáëèêàÊàðåëèÿ,Ðîññèÿ,185910
ýë.ïî÷òà:kirillov@krc.karelia.ru
òåë.:(8142)766312
Kirillov,Alexander
InstituteofAppliedMathematicalResearch,
KarelianResearchCentre,
RussianAcademyofSciences
11PushkinskayaSt.,185910Petrozavodsk,
Karelia,Russia
e-mail:kirillov@krc.karelia.ru
tel.:(8142)766312
ÊðèæàíîâñêàÿÍàòàëüÿÁîðèñîâíà
âåäóùèéèíæåíåð-èññëåäîâàòåëü
Èíñòèòóòïðèêëàäíûõìàòåìàòè÷åñêèõèññëåäîâàíèé
ÊàðÍÖÐÀÍ,Ôåäåðàëüíûéèññëåäîâàòåëüñêèéöåíòð
¾Êàðåëüñêèéíàó÷íûéöåíòðÐÀÍ¿
óë.Ïóøêèíñêàÿ,11,Ïåòðîçàâîäñê,
ÐåñïóáëèêàÊàðåëèÿ,Ðîññèÿ,185910
ýë.ïî÷òà:nataly@krc.karelia.ru
òåë.:(8142)766312
Krizhanovskaya,Natalia
InstituteofAppliedMathematicalResearch,
KarelianResearchCentre,
RussianAcademyofSciences
11PushkinskayaSt.,185910Petrozavodsk,
Karelia,Russia
e-mail:nataly@krc.karelia.ru
tel.:(8142)766312
ÊðèæàíîâñêèéÀíäðåéÀíàòîëüåâè÷
ðóê.ëàá.èíôîðìàöèîííûõêîìïüþòåðíûõ
òåõíîëîãèé,ê.ò.í.
Èíñòèòóòïðèêëàäíûõìàòåìàòè÷åñêèõèññëåäîâàíèé
ÊàðÍÖÐÀÍ,Ôåäåðàëüíûéèññëåäîâàòåëüñêèéöåíòð
¾Êàðåëüñêèéíàó÷íûéöåíòðÐÀÍ¿
óë.Ïóøêèíñêàÿ,11,Ïåòðîçàâîäñê,
ÐåñïóáëèêàÊàðåëèÿ,Ðîññèÿ,185910
ýë.ïî÷òà:andew.krizhanovsky@gmail.com
òåë.:(8142)766312
Krizhanovsky,Andrew
InstituteofAppliedMathematicalResearch,
KarelianResearchCentre,
RussianAcademyofSciences
11PushkinskayaSt.,185910Petrozavodsk,
Karelia,Russia
e-mail:andew.krizhanovsky@gmail.com
tel.:(8142)766312
"
11,RenderNet: A deep convolutional network for differentiable rendering from 3D shapes,http://arxiv.org/pdf/1806.06575v3.pdf,https://github.com/thunguyenphuoc/RenderNet,"RenderNet:Adeepconvolutionalnetworkfor
differentiablerenderingfrom3Dshapes
ThuNguyen-Phuoc
UniversityofBath
T.Nguyen.Phuoc@bath.ac.uk
ChuanLi
LambdaLabs
c@lambdalabs.com
StephenBalaban
LambdaLabs
s@lambdalabs.com
Yong-LiangYang
UniversityofBath
Y.Yang@cs.bath.ac.uk
Abstract
Traditionalcomputergraphicsrenderingpipelinesaredesignedforprocedu-
rallygenerating2Dimagesfrom3Dshapeswithhighperformance.Thenon-
differentiabilityduetodiscreteoperations(suchasvisibilitycomputation)makesit
hardtoexplicitlycorrelaterenderingparametersandtheresultingimage,posing
achallengeforinverserenderingtasks.Recentworkondifferentiable
renderingachievesdifferentiabilityeitherbydesigningsurrogategradientsfor
non-differentiableoperationsorviaanapproximatebutdifferentiablerenderer.
Thesemethods,however,arestilllimitedwhenitcomestohandlingocclusion,and
restrictedtoparticularrenderingeffects.WepresentRenderNet,adifferentiable
renderingconvolutionalnetworkwithanovelprojectionunitthatcanrender2Dim-
agesfrom3Dshapes.Spatialocclusionandshadingcalculationareautomatically
encodedinthenetwork.OurexperimentsshowthatRenderNetcansuccessfully
learntoimplementdifferentshaders,andcanbeusedininverserenderingtasksto
estimateshape,pose,lightingandtexturefromasingleimage.
1Introduction
Renderingreferstotheprocessofformingarealisticorstylizedimagefromadescriptionofthe3D
virtualobject(e.g.,shape,pose,material,texture),andtheilluminationconditionofthesurrounding
scene(e.g.,lightposition,distribution,intensity).Ontheotherhand,inverserendering(graphics)
aimsatestimatingthesepropertiesfromasingleimage.Thetwomostpopularrenderingmethods,
rasterization-basedrenderingandraytracing,aredesignedtoachievefastperformanceandrealism
respectively,butnotforinversegraphics.Thesetwomethodsrelyondiscreteoperations,such
asz-bufferingandray-objectintersection,toidentifypointvisibilityinarenderingscene,which
makesthesetechniquesnon-differentiable.Althoughitispossibletotreatthemasnon-differentiable
renderersincomputervisiontasks[
1
],inferringparameters,suchasshapesorposes,fromthe
renderedimagesusingtraditionalgraphicspipelinesisstillachallengingtask.Adifferentiable
rendererthatcancorrelatethechangeinarenderedimagewiththechangeinrenderingparameters
thereforewillfacilitatearangeofapplications,suchasvision-as-inverse-graphicstasksorimage-
based3Dmodellingandediting.
Recentworkindifferentiablerenderingachievesdifferentiabilityinvariousways.LoperandBlack
[
2
]
proposeanapproximaterendererwhichisdifferentiable.Katoetal.
[
3
]
achievedifferentiability
byproposinganapproximategradientfortherasterizationoperation.Recentworkonimage-based
reconstructionusesdifferentiableprojectionsof3Dobjectsontosilhouettemasksasasurrogatefora
renderedimageoftheobjects[
4
,
5
].Wuetal.
[
6
]
andTulsianietal.
[
7
]
derivedifferentiableprojective
32ndConferenceonNeuralInformationProcessingSystems(NeurIPS2018),Montréal,Canada.
arXiv:1806.06575v3  [cs.CV]  1 Apr 2019functionsfromnormal,depth,andsilhouettemaps,butrespectivelycanonlyhandleorthographic
projection,orneedsmultipleinputimages.Theseprojectionscanthenbeusedtoconstructanerror
signalforthereconstructionprocess.Alloftheseapproaches,however,arerestrictedto
renderingstyles(rasterization)[
2
,
3
,
8
],inputgeometrytypes[
9
,
10
],orlimitingoutputformatssuch
asdepthorsilhouettemaps[
4
,
5
,
6
,
7
,
11
,
12
].Moreover,noneoftheseapproachestrytosolvethe
problemfromthenetworkarchitecturedesignpointofview.Recentprogressinmachinelearning
showsthatnetworkarchitectureplaysanimportantroleforimprovingtheperformanceofmany
tasks.Forexample,inResNet[
13
]andDenseNet[
14
]havecontributed
performancegains.Insegmentationtasks,U-Net[
15
]provesthathavingshort-cutconnectionscan
greatlyimprovethedetaillevelofthesegmentationmasks.Inthispaper,wethereforefocuson
designinganeuralnetworkarchitecturesuitableforthetaskofrenderingandinverserendering.
WeproposeRenderNet,aconvolutionalneuralnetwork(CNN)architecturethatcanbetrainedend-
to-endforrendering3Dobjects,includingobjectvisibilitycomputationandpixelcolorcalculation
(shading).OurmethodexploresthenovelideaofcombiningtheabilityofCNNswithinductive
biasesaboutthe3Dworldforgeometry-basedimagesynthesis.Thisisdifferentfromrecentimage-
generatingCNNsdrivenbyobjectattributes[
16
],noise[
17
],semanticmaps[
18
],orpixelattributes
[
19
],whichmakeveryfewassumptionaboutthe3Dworldandtheimageformationprocess.Inspired
bytheliteraturefromcomputergraphics,weproposetheprojectionunitthatincorporatesprior
knowledgeaboutthe3Dworld,andhowitisrendered,intoRenderNet.Theprojectionunit,through
learning,isadifferentiableapproximationofthenon-differentiablevisibilitycomputationstep,
makingRenderNetanend-to-endsystem.Unlikenon-learntapproachesinpreviouswork,alearnt
projectionunitusesdeepfeaturesinsteadoflow-levelprimitives,makingRenderNetgeneralizewell
toavarietyofinputgeometries,robusttoerroneousorlow-resolutioninput,aswellasenabling
learningmulti-stylerenderingwiththesamenetworkarchitecture.RenderNetisdifferentiableand
canbeeasilyintegratedtootherneuralnetworks,variousinverserenderingtasks,such
asnovel-viewsynthesis,poseprediction,orimage-based3Dshapereconstruction,unlikeprevious
image-basedinverserenderingworkthatcanrecoveronlypartofthefull3Dshapes[
20
,
21
].
Wechoosethevoxelpresentationof3Dshapesforitsregularityandxibility,anditsapplicationin
visualizingvolumetricdatasuchasmedicalimages.Althoughvoxelgridsaretraditionallymemory
inefcomputersarebecomingmorepowerful,andrecentworkalsoaddressesthisinefy
usingoctrees[
22
,
23
],enablinghigh-resolutionvoxelgrids.Inthispaper,wefocusonvoxeldata,
andleaveotherdataformatssuchaspolygonmeshesandunstructuredpointcloudsaspossiblefuture
extensions.WedemonstratethatRenderNetcangeneraterenderingsofhighquality,evenfrom
low-resolutionandnoisyvoxelgrids.Thisisaadvantagecomparedtomeshrenderers,
includingmorerecentworkindifferentiablerendering,whichdonothandleerroneousinputswell.
Byframingtherenderingprocessasafeed-forwardCNN,RenderNethastheabilitytolearnto
expressdifferentshaderswiththesamenetworkarchitecture.Wedemonstrateanumberofrendering
stylesrangingfromsimpleshaderssuchasPhongshading[
24
],suggestivecontourshading[
25
],to
morecomplexshaderssuchasacompositeofcontourshadingandcartoonshading[
26
]orambient
occlusion[
27
],someofwhicharetime-consumingandcomputationallyexpensive.RenderNetalso
hasthepotentialtobecombinedwithneuralstyletransfertoimprovethesynthesizedresults,orother
complexshadersthatarehardtoexplicitly.
Insummary,theproposedRenderNetcanbothrenderingandinverserendering:RenderNetcan
learntogenerateimageswithdifferentappearance,andcanalsobeusedforvision-as-inverse-graphics
tasks.Ourmaincontributionsarethreefold.

Anovelconvolutionalneuralnetworkarchitecturethatlearnstorenderindifferentstyles
froma3Dvoxelgridinput.Toourknowledge,wearethetoproposeaneuralrenderer
for3Dshapeswiththeprojectionunitthatenablesbothrenderingandinverserendering.

WeshowthatRenderNetgeneralizeswelltoobjectsofunseencategoryandmorecomplex
scenegeometry.RenderNetcanalsoproducetexturedimagesfromtexturedvoxelgrids,
wheretheinputtexturescanbeRGBcolorsordeepfeaturescomputedfromsemanticinputs.

Weshowthatourmodelcanbeintegratedintoothermodulesforapplications,suchas
texturingorimage-basedreconstruction.
2
2Relatedwork
Ourworkisrelatedtothreecategoriesoflearning-basedworks:image-basedrendering,geometry-
basedrenderingandimage-basedshapereconstruction.Inthissection,wereviewsomelandmark
methodsthatarecloselyrelatedtoourwork.Inparticular,wefocusonneural-network-based
methods.
Image-basedrendering
ThereisarichliteratureofCNN-basedrenderingbylearningfromimages.
Dosovitskiyetal.[
16
]create2Dimagesfromlow-dimensionalvectorsandattributesof3Dobjects.
Cascadednetworks[
18
],andPix2Pix[
28
]additionallyconditiononsemanticmapsor
sketchesasinputs.Usingamodelthatismoredeeplygroundedincomputergraphics,DeepShading
[
19
]learnstocreateimageswithhighandcomplexvisualeffectsfromper-pixelattributes.
DC-IGN[
29
]learnsdisentangledrepresentationofimageswithrespecttotransformations,suchas
out-of-planerotationsandlightingvariations,andthusisabletoeditimageswithrespecttothese
factors.Relevantworksonnovel3Dviewsynthesis[
30
]leveragecategorshapepriorsand
opticalwtodealwithocclusion/disocclusion.Whilethesemethodsyieldimpressiveresults,we
arguethatgeometry-basedmethods,whichmakestrongerassumptionsaboutthe3Dworldandhow
itproduces2Dimages,willbeabletoperformbetterincertaintasks,suchasout-of-planerotation,
imagerelighting,andshapetexturing.ThisalsocoincideswithRematasetal.
[
31
]
,Yangetal.
[
32
]
andSuetal.[
33
]whousestrong3Dpriorstoassistthenovel-viewsynthesistask.
Geometry-basedrendering
Despitetherichliteratureinrenderingincomputergraphics,thereis
alotlessworkusingdifferentiablerenderingtechniques.OpenDR[
2
]hasbeenapopularframework
fordifferentiablerendering.However,beingamoregeneralmethod,itismorestrenuoustobe
integratedintootherneuralnetworksandmachinelearningframeworks.Katoetal.
[
3
]
approximate
thegradientoftherasterizationoperationtomaketherenderingdifferentiable.However,thismethod
islimitedtorasterization-basedrendering,makingitdiftorepresentmorecomplexeffectsthat
areusuallyachievedbyraytracingsuchasglobalillumination,orrefraction.
Image-based3Dshapereconstruction
Reconstructing3Dshapefrom2Dimagecanbetreated
asestimatingtheposteriorofthe3Dshapeconditionedonthe2Dinformation.Theprioroftheshape
couldbeasimplesmoothnesspriororapriorlearnedfrom3Dshapedatasets.Thelikelihoodterm,
ontheotherhand,requiresestimatingthedistributionof2Dimagesgiventhe3Dshape.Recent
workhasbeenusing2Dsilhouettemapsoftheimages[
4
,
5
].Whilethisproveseffective,silhouette
imagescontainlittleinformationabouttheshape.Hencealargenumberofimagesorviewsofthe
objectisrequiredforthereconstructiontask.Fornormalmapsanddepthmapsoftheshape,Wuetal.
[
6
]
derivedifferentiableprojectivefunctionsassumingorthographicprojection.Similarly,Tulsiani
etal.
[
7
]
proposeadifferentiableformulationthatenablescomputinggradientsofthe3Dshapegiven
multipleobservationsofdepth,normalorpixelcolormapsfromarbitraryviews.Inourwork,we
proposeRenderNetasapowerfulmodelforthelikelihoodterm.Toreconstruct3Dshapesfrom2D
images,wedoMAPestimationusingourtrainedrenderingnetworkasthelikelihoodfunction,in
additiontoashapepriorthatislearnedfroma3Dshapedataset.Weshowthatwecanrecovernot
onlytheposeandshape,butalsolightingandtexturefromasingleimage.
3Model
Thetraditionalcomputergraphicspipelinerendersimagesfromtheviewpointofavirtualpin-hole
camerausingacommonperspectiveprojection.Theviewingdirectionisassumedtobealong
thenegativez-axisinthecameracoordinatesystem.Therefore,the3Dcontentinthe
worldcoordinatesystemneedstobetransformedintothecameracoordinatesystembeforebeing
rendered.Thetwocurrentlypopularrenderingmethods,rasterization-basedrenderingandraytracing,
procedurallycomputethecolorofeachpixelintheimagewithtwomajorsteps:testingvisibilityin
thescene,andcomputingshadedcolorvalueunderanilluminationmodel.
RenderNetjointlylearnsbothstepsoftherenderingprocessfromtrainingdata,whichcanbe
generatedusingeitherrasterizationorraytracing.Inspiredbythetraditionalrenderingpipeline,we
alsoadopttheworld-space-to-camera-spacecoordinatetransformationstrategy,andassumethatthe
cameraisaxis-alignedandlooksalongthenegativez-axisofthevolumetricgridthatdiscretizes
theinputshape.Insteadofhavingthenetworklearnoperationswhicharedifferentiableandeasy
toimplement,suchasrigid-bodycoordinatetransformationortheinteractionoflightwithsurface
3
Figure1:
Networkarchitecture.
SeeSection2inthesupplementarydocumentfordetails.
normals(e.g.assumingaPhongilluminationmodel[
24
]),weprovidemostofthemexplicitlytothe
network.ThisallowsRenderNettofocusitscapacityonmorecomplexaspectsoftherenderingtask,
suchasrecognizingvisibilityandproducingshadedcolor.
RenderNetreceivesavoxelgridasinput,andappliesarigid-bodytransformationtoconvertfrom
theworldcoordinatesystemtothecameracoordinatesystem.Thetranformedinput,afterbeing
trilinearlysampled,isthenfedtoaCNNwithaprojectionunittoproducearendered2Dimage.
RenderNetconsistsof3Dconvolutions,aprojectionunitthatcomputesvisibilityofobjectsinthe
sceneandprojectsthemonto2Dfeaturemaps,followedby2Dconvolutionstocomputeshading.
WetrainRenderNetusingapixel-spacelossbetweenthetargetimageandtheoutput.Optionally,
thenetworkcanproducenormalmapsofthe3Dinputwhichcanbecombinedwithlightsourcesto
illuminatethescene.Whiletheprojectionunitcaneasilyincorporateorthographicprojections,the
3Dconvolutionscanmorphthesceneandallowsforperspectivecameraviews.Infutureversionsof
RenderNet,perspectivetransformationmayalsobeexplicitlyincorporatedintothenetwork.
3.1Rotationandresampling
Thetransformedinputviarigidbodymotionensuresthatthecameraisalwaysinthesamecanonical
poserelativetothevoxelgridbeingrendered.Thetransformationisparameterizedbytherotation
aroundthey-axisandz-axis,whichcorrespondstotheazimuthandelevation,andadistance
R
that
determinesthescalingfactor,i.e.,howclosetheobjectistothecamera.Weembeddedtheinputvoxel
gridintoalargergridtomakesuretheobjectisnotcutoffafterrotation.Thetotaltransformation
thereforeincludesscaling,rotation,translation,andtrilinearresampling.
3.2Projectionunit
TheinputofRenderNetisavoxelgrid
V
ofdimension
H
V

W
V

D
V

C
V
(correspondingtoheight,
width,depth,andchannel),andtheoutputisanimage
I
ofdimension
H
I

W
I

C
I
(corresponding
toheight,widthandchannel).Tobridgethedisparitybetweenthe3Dinputand2Doutput,wedevise
anovelprojectionunit.Thedesignofthisunitisstraightforward:itconsistsofareshapinglayer,and
amultilayerperceptron(MLP).Maxpoolingisoftenusedtothe3Dinputacrossthedepth
dimension[
4
,
5
],butthiscanonlycreatethesilhouettemapofthe3Dshape.Theprojectionunit,on
theotherhand,learnsnotonlytoperformprojection,butalsotodeterminevisibilityofdifferentparts
ofthe3Dinputalongthedepthdimensionafterprojection.
Forthereshapingstepoftheunit,wecollapsethedepthdimensionwiththefeaturemapstomapthe
incoming4Dtensortoa3Dsqueezedtensor
V
0
withdimension
W

H

(
D

C
)
.Thisisimmediately
followedbyanMLP,whichiscapableoflearningmorecomplexstructurewithinthelocalreceptive
thanaconventionallinear[
13
].WeapplytheMLPoneach
(
D

C
)
vector,whichwe
implementusinga1

1convolutioninthisproject.ThereshapingstepallowseachunitoftheMLP
toaccessthefeaturesacrossdifferentchannelsandthedepthdimensionoftheinput,enablingthe
4
networktolearntheprojectionoperationandvisibilitycomputationalongthedepthaxis.Given
thesqueezed3Dtensor
V
0
with
(
D

C
)
channels,theprojectionunitproducesa3Dtensorwith
K
channelsasfollows:
I
i;j;k
=
f
 
X
dc
w
k;dc

V
0
i;j;dc
+
b
k
!
(1)
where
i
,
j
arepixelcoordinates,
k
istheimagechannel,
dc
isthesqueezeddepthchannel,where
d
and
c
arethedepthandchanneldimensionoftheoriginal4Dtensorrespectively,and
f
issome
non-linearfunction(parametricReLUinourexperiments).
3.3ExtendingRenderNet
WecancombineRenderNetwithothernetworkstohandlemorerenderingparametersandperform
morecomplextaskssuchasshadowrenderingortexturemapping.Wemodelaconditionalrenderer
p
(
I
j
V;h
)
where
h
canbeextrarenderingparametersuchaslights,orspatially-varyingparameters
suchastexture.
HerewedemonstratetheextensibilityofRenderNetusingtheexampleofthePhongillumination
model[
24
].Theper-pixelshadedcolorfortheimagesiscalculatedby
S
=max(0
;
~
l

~n
+
a
)
,where
~
l
istheunitlightdirectionvector,
~n
isthenormalvector,whosecomponentsareencodedbytheRGB
channelsofthenormalmap,and
a
isanambientconstant.Shading
S
andalbedomap
A
arefurther
combinedtocreatetheimage
I
basedon
I
=
A

S
[
34
].ThisisillustratedinSection
4.1
,
wherewecombinethealbedomapandnormalmaprenderedbythecombinationofatexture-mapping
networkandRenderNettorendershadedimagesoffaces.
4Experiments
ToexplorethegeneralityofRenderNet,wetestourmethodonbothcomputergraphicsandvision
tasks.First,weexperimentwithdifferentrenderingtaskswithvaryingdegreeofcomplexity,including
challengingcasessuchastexturemappingandsurfacerelighting.Second,weexperimentwithvision
applicationssuchasimage-basedposeandshapereconstruction.
Datasets
WeusethechairdatasetfromShapeNetCore[
35
].Apartfrombeingoneofthecategories
withthelargestnumberofdatapoints(6778objects),thechaircategoryalsohaslargeintra-class
variation.WeconverttheShapeNetDatasetto64

64

64voxelgridsusingvolumetricconvolution
[
36
].Werandomlysampled120viewsofeachobjecttorendertrainingimagesat512

512resolution.
Theelevationandazimuthareuniformlysampledbetween
[10
;
170]
degreesand
[0
;
359]
degrees,
respectively.Cameraradiusaresetat3to6.3unitsfromtheorigin,withtheobject'saxis-aligned
boundingboxnormalizedto1unitlength.Forthetexturemappingtasks,wegenerate100,000faces
fromtheBaselFaceDataset[
37
],andrenderthemwithdifferentazimuthsbetween
[220
;
320]
degrees
andelevationsbetween
[70
;
110]
degrees.WeuseBlender3DtogeneratetheAmbientOcclusion(AO)
dataset,andVTKfortheotherdatasets.Forthecontourdataset,weimplementedthepixel-based
suggestivecontour[
25
]algorithminVTK.
Training
Weadoptthepatchtrainingstrategytospeedupthetrainingprocessinourmodel.We
trainthenetworkusingrandomspatiallycroppedsamples(alongthewidthandheightdimensions)
fromthetrainingvoxelgrids,whilekeepingthedepthandchanneldimensionsintact.Weonlyuse
thefull-sizedvoxelgridinputduringinference.Thepatchsizestartsassmallas1/8ofthefull-sized
grid,andprogressivelyincreasestowards1/2ofthefull-sizedgridattheendofthetraining.
WetrainRenderNetusingapixel-spaceregressionloss.Weusemeansquarederrorlossforcolored
images,andbinarycrossentropyforgrayscaleimages.WeusetheAdamoptimizer[
38
],witha
learningrateof0.00001.
Code,dataandtrainedmodelswillbeavailableat:
https://github.com/thunguyenphuoc/
RenderNet
.
4.1Learningtorenderandapplytexture
Figure
2
showsthatRenderNetisabletolearndifferenttypesofshaders,includingPhongshading,
contourlineshading,complexmulti-passshading(cartoonshading),andaray-tracingeffect(Ambient
5
Figure2:
Left:
DifferenttypesofshadersgeneratedbyRenderNet(intputatthetop).
Right:
ComparingPhongshadingbetweenRenderNet,astandardOpenGLmeshrenderer,andastandard
MarchingCubesalgorithm.RenderNetproducescompetitiveresultswiththeOpenGLmeshrenderer
withoutsufferingfrommeshartefacts(noticetheseatingpadofchair(c)orthelegofchair(d)in
Meshrenderer),anddoesnotsufferfromlow-resolutioninputlikeMarchingcubes.
Occlusion)withthesamenetworkarchitecture.RenderNetwastrainedondatasetsforeachofthese
shaders,andtheshowsoutputsgeneratedforunseentest3Dshapes.WereportthePSNRscore
foreachshaderinFigure
5
.
RenderNetgeneralizeswelltoshapesofunseencategories.Whileitwastrainedonchairs,itcanalso
rendernon-man-madeobjectssuchastheStanfordBunnyandMonkey(Figure
3
).Themethodalso
worksverywellwhentherearemultipleobjectsinthescene,suggestingthenetworkrecognizesthe
visibilityoftheobjectsinthescene.
RenderNetcanalsohandlecorruptedorlow-resolutionvolumetricdata.Forexample,Figure
3
shows
thatthenetworkisabletoproduceplausiblerenderingsfortheBunnywhentheinputmodelwas
corruptedbyadding50%randomnoise.Whentheinputmodelisdownsampled(herewe
linearlydownsampledtheinputby50%),RenderNetcanstillrenderahigh-resolutionimagewith
smoothdetails.Thisisadvantageouscomparedtothetraditionalcomputergraphicsmeshrendering,
whichrequiresacleanandhigh-qualitymeshinordertoachievegoodrenderedresults.
ItisalsostraightforwardtocombineRenderNetwithothermodulesfortaskssuchasmappingand
renderingtexture(Figure
4
).Wecreateatexture-mappingnetworktomapa1Dtexturevector
representation(thesearethePCAcoefcientsforgeneratingalbedotextureusingtheBaselFace
dataset)toa3Drepresentationofthetexturethathasthesamewidth,heightanddepthastheshape
input.Thisoutputisconcatenatedalongthechanneldimensionwiththeinput3Dshapebefore
givenRenderNettorenderthealbedomap.Thisisequivalenttoassigningatexturevaluetothe
correspondingvoxelinthebinaryshapevoxelgrid.Wealsoaddanotheroutputbranchof2D
convolutionstoRenderNettorenderthenormalmap.Thealbedomapandthenormalmapproduced
byRenderNetarethencombinedtocreateshadedrenderingsoffacesasdescribedinSection
3.3
.
SeeSection2.3inthesupplementarydocumentfornetworkarchitecturedetails.
4.2Architecturecomparison
Inthissection,wecompareRenderNetwithtwobaselineencoder-decoderarchitecturestorender
Phong-shadedimages.SimilartoRenderNet,thenetworksreceivethe3Dshape,pose,lightposition
andlightintensityasinput.IncontrasttoRenderNet,the3Dshapegiventothealternativenetworkis
inthecanonicalpose,andthenetworkshavetolearntotransformthe3Dinputtothegivenpose.
ThenetworkfollowsthenetworkarchitecturebyDosovitskiyetal.
[
16
]
,whichconsistsofa
6
Figure3:
Generalization.
Evenwithinputfromunseencategoriesoroflowquality,RenderNetcan
stillproducegoodresultsindifferentstyles(left)andfromdifferentviews(right).
Figure4:Renderingtextureandmanipulatingrenderinginputs.Bestviewedincolor.
seriesoffully-connectedlayersandup-convolutionlayers.Thesecondnetworkissimilarbuthasa
deeperdecoderthantheonebyaddingresidualblocks.Forthe3Dshape,weuseanencoding
networktomaptheinputtoalatentshapevector(refertoSection2.2inthesupplementarydocument
fordetails).WecallthesetwonetworksECandEC-Deep,respectively.Thesenetworksaretrained
directlyonshadedimageswithabinarycross-entropyloss,usingthechaircategoryfromShapeNet.
RenderNet,ontheotherhand,rendersthenormalmap,andcombinesthiswiththelightinginput
tocreatetheshadedimageusingtheshadingequationinSection
3.3
.
AsshowninFigure
5
,thealternativemodel(hereweshowtheECmodel)failstoproduceimportant
detailsoftheobjectsandachieveslowerPSNRscoreonthePhong-shadedchairdataset.More
importantly,thisarchitectureﬁremembersﬂtheglobalstructureoftheobjectsandfailstogeneralize
toobjectsofunseencategoryduetotheuseofthefullyconnectedlayers.Incontrast,ourmodelis
betterforrenderingtasksasitgeneralizeswelltodifferentcategoriesofshapesandscenes.
4.3Shapereconstructionfromimages
HerewedemonstratethatRenderNetcanbeusedforsingle-imagereconstruction.Itachievesthis
goalviaaniterativeoptimizationthatminimizesthefollowingreconstructionloss:
minimize
z
k
I

f
(
z;;˚;
)
k
2
(2)
where
I
istheobservedimageand
f
isourpre-trainedRenderNet.
z
istheshapetoreconstruct,

and

aretheposeandlightingparameters,and
˚
isthetexturevariable.Inessence,thisprocess
maximizesthelikelihoodofobservingtheimage
I
giventheshape
z
.
7
PSNRscore
RenderstylePSNR
RenderNetPhong
25.39
ECPhong24.21
EC-DeepPhong20.88
RenderNetContour19.70
RenderNetToon17.77
RenderNetAO22.37
RenderNetFace27.43
Figure5:
Left:
Architecturecomparisonindifferenttasks:
a)
Novel-viewsynthesis,
b)
Relighting
and
c)
Generalization.
Right:
PSNRscoreofdifferentshaders,includingthetwoalternative
architectures.
However,directlyminimizingthislossoftenleadstonoisy,unstableresults(showninFigure2
inthesupplementarydocument).Inordertoimprovethereconstruction,weuseashapepriorfor
regularizingtheprocessŒapre-trained3Dauto-encodersimilartotheTL-embeddingnetwork[
39
]
with80000shapes.Insteadofoptimizing
z
,weoptimizeitslatentrepresentation
z
0
:
minimize
z
0
;˚
0

k
I

f
(
g
(
z
0
)
;;h
(
˚
0
)
;
)
k
2
(3)
where
g
isthedecoderofthe3Dauto-encoder.Itregularizesthereconstructedshape
g
(
z
0
)
byusing
thepriorshapeknowledge(weightsinthedecoder)forshapegeneration.Similarly,weusethe
decoder
h
thatwastrainedwithRenderNetforthetexturerenderingtaskinSection
4.1
toregularize
thetexturevariable
˚
0
.ThiscorrespondstoMAPestimation,wherethepriortermistheshape
decoderandthelikelihoodtermisgivenbyRenderNet.Notethatitisstraightforwardtoextendthis
methodtothemulti-viewreconstructiontaskbysummingovermultipleper-imagelosseswithshared
shapeandappearance.
WecompareRenderNetwithDC-IGNbyKulkarnietal.
[
29
]
inFigure
6
.DC-IGNlearnsto
decomposeimagesintoagraphicscode
Z
,whichisadisentangledrepresentationcontainingasetof
latentvariablesforshape,poseandlighting,allowingthemtomanipulatethesepropertiestogenerate
novelviewsorperformimagerelighting.Incontrasttotheirwork,weexplicitlyreconstructthe3D
geometry,pose,lightingandtexture,whichgreatlyimprovestaskssuchasout-of-planerotation,
andallowsustodore-texturing.Wealsogenerateresultswithmuchhigherresolution(512

512)
comparedtoDC-IGN(150

150).Ourresultsshowthathavinganexplicitreconstructionnotonly
createssharperimageswithhigherlevelofdetailsinthetaskofnovel-viewprediction,butalso
givesusmorecontrolintherelightingtasksuchaslightcolor,brightness,orlightposition(herewe
manipulatetheelevationandazimuthofthelightposition),andespecially,there-texturingtask.
Forthefacedataset,wereporttheIntersection-over-Union(IOU)betweenthegroundtruthand
reconstructedvoxelgridof42.99

0.64for95%interval.Wealsoperformthesame
experimentforthechairdatasetŒrefertoSection1inthesupplementarymaterialforimplementation
detailsandadditionalresults.
8
Figure6:
Image-basedreconstruction.
Weshowboththereconstructedimagesandnormalmaps
fromasingleimage.Thecrossindicatesafactornotlearntbythenetwork.Note:forthere-texturing
task,weonlyshowthealbedotovisualizethechangeintexturemoreclearly.Bestviewedincolor.
5Discussionandconclusion
Inthispaper,wepresentedRenderNet,aconvolutionaldifferentiablerenderingnetworkthatcan
betrainedend-to-endwithapixel-spaceregressionloss.Despitethesimplicityinthedesignofthe
networkarchitectureandtheprojectionunit,ourexperimentsdemonstratethatRenderNetsuccessfully
performsrenderingandinverserendering.Moreover,asshowninSection
4.1
,thereisthepotentialto
combinedifferentshadersinonenetworkthatsharesthesame3Dconvolutionsandprojectionunit,
insteadoftrainingdifferentnetworksfordifferentshaders.Thisopensuproomforimprovement
andexploration,suchasextendingRenderNettoworkwithunlabelleddata,usingotherlosses
likeadversariallossesorperceptuallosses,orcombiningRenderNetwithotherarchitectures,such
asU-Netoramulti-scalearchitecturewheretheprojectionunitisusedatdifferentresolutions.
AnotherinterestingpossibilityistocombineRenderNetwithastyle-transferlossforstylizationof
3Drenderings.
Therealworldisthree-dimensional,yetthemajorityofcurrentimagesynthesisCNNs,suchas
GAN[
17
]orDC-IGN[
29
],onlyoperatesin2Dfeaturespaceandmakesalmostnoassumptions
aboutthe3Dworld.Althoughthesemethodsyieldimpressiveresults,webelievethathavinga
moregeometricallygroundedapproachcangreatlyimprovetheperformanceandtheofthe
generatedimages,especiallyfortaskssuchasnovel-viewsynthesis,ormoreediting
taskssuchastextureediting.Forexample,insteadofhavingaGANgenerateimagesfromanoise
vectorvia2Dconvolutions,aGANusingRenderNetcouldgeneratea3Dshape,whichisthen
renderedtocreatetheimage.WehopethatRenderNetcanbringmoreattentiontothecomputer
graphicsliterature,especiallygeometry-groundedapproaches,toinspirefuturedevelopmentsin
computervision.
Acknowledgments
WethankChristianRichardtforhelpfuldiscussions.WethankLucasTheisforhelpfuldiscussionsand
feedbackonthemanuscript.ThisworkwassupportedinpartbytheEuropeanUnion'sHorizon2020
researchandinnovationprogrammeundertheMarieSklodowska-CuriegrantagreementNo665992,
theUK'sEPSRCCentreforDoctoralTraininginDigitalEntertainment(CDE),EP/L016540/1,and
CAMERA,theRCUKCentrefortheAnalysisofMotion,EntertainmentResearchandApplications,
EP/M023281/1.WealsoreceivedGPUsupportfromLambdaLabs.
9
References
[1]
DaniloJimenezRezende,S.M.AliEslami,ShakirMohamed,PeterBattaglia,MaxJaderberg,andNicolas
Heess.Unsupervisedlearningof3dstructurefromimages.In
NIPS
,pages4996Œ5004.2016.
[2]
MatthewM.LoperandMichaelJ.Black.OpenDR:Anapproximatedifferentiablerenderer.In
ECCV
,
pages154Œ169.2014.
[3]
HiroharuKato,YoshitakaUshiku,andTatsuyaHarada.Neural3dmeshrenderer.In
IEEECVPR
,2018.
[4]
XinchenYan,JimeiYang,ErsinYumer,YijieGuo,andHonglakLee.Perspectivetransformernets:
Learningsingle-view3dobjectreconstructionwithout3dsupervision.In
NIPS
,pages1696Œ1704.2016.
[5]
RuiZhu,HamedKianiGaloogahi,ChaoyangWang,andSimonLucey.Rethinkingreprojection:Closing
theloopforpose-awareshapereconstructionfromasingleimage.In
IEEECVPR
,pages57Œ65,2017.
[6]
JiajunWu,YifanWang,TianfanXue,XingyuanSun,WilliamTFreeman,andJoshuaBTenenbaum.
MarrNet:3DShapeReconstructionvia2.5DSketches.In
NIPS
,2017.
[7]
ShubhamTulsiani,TinghuiZhou,AlexeiA.Efros,andJitendraMalik.Multi-viewsupervisionfor
single-viewreconstructionviadifferentiablerayconsistency.In
IEEECVPR
,pages209Œ217,2017.
[8]
PaulHendersonandVittorioFerrari.Learningtogenerateandreconstruct3dmesheswithonly2d
supervision.In
BritishMachineVisionConference(BMVC)
,2018.
[9]
KyleGenova,ForresterCole,AaronMaschinot,AaronSarna,DanielVlasic,andWilliamT.Freeman.
Unsupervisedtrainingfor3dmorphablemodelregression.In
TheIEEEConferenceonComputerVision
andPatternRecognition(CVPR)
,June2018.
[10]
AbhijitKundu,YinLi,andJamesM.Rehg.3d-rcnn:Instance-level3dobjectreconstructionviarender-
and-compare.In
CVPR
,2018.
[11]
E.Richardson,M.Sela,R.Or-El,andR.Kimmel.Learningdetailedfacereconstructionfromasingle
image.In
2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR)
,pages5553Œ5562,
July2017.doi:10.1109/CVPR.2017.589.
[12]
JunYoungGwak,ChristopherBChoy,ManmohanChandraker,AnimeshGarg,andSilvioSavarese.Weakly
supervised3dreconstructionwithadversarialconstraint.In
3DVision(3DV),2017FifthInternational
Conferenceon3DVision
,2017.
[13]
MinLin,QiangChen,andShuichengYan.Networkinnetwork.In
ICLR
,2014.
[14]
GaoHuang,ZhuangLiu,LaurensvanderMaaten,andKilianQWeinberger.Denselyconnectedconvolu-
tionalnetworks.In
IEEECVPR
,pages2261Œ2269,2017.
[15]
O.Ronneberger,P.Fischer,andT.Brox.U-net:Convolutionalnetworksforbiomedicalimagesegmentation.
In
MedicalImageComputingandComputer-AssistedIntervention(MICCAI)
,volume9351,pages234Œ241,
2015.
[16]
AlexeyDosovitskiy,JostTobiasSpringenberg,MaximTatarchenko,andThomasBrox.Learningto
generatechairs,tablesandcarswithconvolutionalnetworks.39(4):692Œ705,2017.
[17]
IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,Aaron
Courville,andYoshuaBengio.Generativeadversarialnets.InZ.Ghahramani,M.Welling,C.Cortes,
N.D.Lawrence,andK.Q.Weinberger,editors,
AdvancesinNeuralInformationProcessingSystems27
,
pages2672Œ2680.CurranAssociates,Inc.,2014.
[18]
QifengChenandVladlenKoltun.Photographicimagesynthesiswithcascadednetworks.In
IEEEICCV
,pages1520Œ1529,2017.
[19]
OliverNalbach,ElenaArabadzhiyska,DushyantMehta,Hans-PeterSeidel,andTobiasRitschel.Deep
shading:Convolutionalneuralnetworksforscreen-spaceshading.
ComputerGraphicsForum(Proc.
EGSR)
,36(4):65Œ78,2017.
[20]
JonathanTBarronandJitendraMalik.Shape,illumination,andfromshading.
TPAMI
,2015.
[21]
TatsunoriTaniaiandTakanoriMaehara.Neuralinverserenderingforgeneralphotometricstereo.
In
ICML
,volume80of
JMLRWorkshopandConferenceProceedings
,pages4864Œ4873.JMLR.org,2018.
10
[22]
MaximTatarchenko,AlexeyDosovitskiy,andThomasBrox.Octreegeneratingnetworks:Efcient
convolutionalarchitecturesforhigh-resolution3doutputs.In
IEEEICCV
,pages2107Œ2115,2017.
[23]
Peng-ShuaiWang,YangLiu,Yu-XiaoGuo,Chun-YuSun,andXinTong.O-cnn:Octree-basedconvolu-
tionalneuralnetworksfor3dshapeanalysis.
ACMTOG(Siggraph)
,36(4):72:1Œ72:11,2017.
[24]
BuiTuongPhong.Illuminationforcomputergeneratedpictures.
Commun.ACM
,18(6):311Œ317,1975.
[25]
DougDeCarlo,AdamFinkelstein,SzymonRusinkiewicz,andAnthonySantella.Suggestivecontoursfor
conveyingshape.
ACMTOG(Siggraph)
,22(3):848Œ855,July2003.
[26]
HolgerWinnemöller,SvenC.Olsen,andBruceGooch.Real-timevideoabstraction.
ACMTOG(Siggraph)
,
25(3):1221Œ1226,2006.
[27]
GavinMiller.Efntalgorithmsforlocalandglobalaccessibilityshading.In
Proc.ACMSIGGRAPH
,
pages319Œ326,1994.
[28]
PhillipIsola,Jun-YanZhu,TinghuiZhou,andAlexeiA.Efros.Image-to-imagetranslationwithconditional
adversarialnetworks.In
IEEECVPR
,pages5967Œ5976,2017.
[29]
TejasDKulkarni,WilliamF.Whitney,PushmeetKohli,andJoshTenenbaum.Deepconvolutionalinverse
graphicsnetwork.In
NIPS
,pages2539Œ2547,2015.
[30]
EunbyungPark,JimeiYang,ErsinYumer,DuyguCeylan,andAlexanderC.Berg.Transformation-
groundedimagegenerationnetworkfornovel3dviewsynthesis.In
IEEECVPR
,pages702Œ711,2017.
[31]
K.Rematas,C.H.Nguyen,T.Ritschel,M.Fritz,andT.Tuytelaars.Novelviewsofobjectsfromasingle
image.
IEEETrans.PatternAnal.Mach.Intell.
,39(8):1576Œ1590,2017.
[32]
JimeiYang,ScottEReed,Ming-HsuanYang,andHonglakLee.Weakly-superviseddisentanglingwith
recurrenttransformationsfor3dviewsynthesis.In
NIPS
,pages1099Œ1107.2015.
[33]
HaoSu,FanWang,LiYi,andLeonidasJ.Guibas.3d-assistedimagefeaturesynthesisfornovelviewsof
anobject.
CoRR
,abs/1412.0003,2014.
[34]
BertholdK.P.Horn.Determininglightnessfromanimage.
ComputerGraphicsandImageProcessing
,3
(4):277Œ299,1974.
[35]
AngelX.Chang,ThomasA.Funkhouser,LeonidasJ.Guibas,PatHanrahan,Qi-XingHuang,ZimoLi,
SilvioSavarese,ManolisSavva,ShuranSong,HaoSu,JianxiongXiao,LiYi,andFisherYu.Shapenet:
Aninformation-rich3dmodelrepository.
CoRR
,abs/1512.03012,2015.
[36]
F.S.NooruddinandG.Turk.andrepairofpolygonalmodelsusingvolumetrictechniques.
IEEETrans.onVis.andComp.Graphics
,9(2):191Œ205,2003.
[37]
PascalPaysan,ReinhardKnothe,BrianAmberg,SamiRomdhani,andThomasVetter.A3dfacemodel
forposeandilluminationinvariantfacerecognition.In
Proceedingsofthe2009SixthIEEEInternational
ConferenceonAdvancedVideoandSignalBasedSurveillance
,pages296Œ301,2009.
[38]
DiederikP.KingmaandJimmBa.Adam:Amethodforstochasticoptimization.In
International
ConferenceonLearningRepresentations(ICLR)
,2015.
[39]
RohitGirdhar,DavidF.Fouhey,MikelRodriguez,andAbhinavGupta.Learningapredictableand
generativevectorrepresentationforobjects.In
ECCV
,pages484Œ499,2016.
[40]
MathieuAubry,DanielMaturana,AlexeiEfros,BryanRussell,andJosefSivic.Seeing3dchairs:exemplar
part-based2d-3dalignmentusingalargedatasetofcadmodels.In
CVPR
,2014.
[41]
ShubhamTulsiani,AlexeiA.Efros,andJitendraMalik.Multi-viewconsistencyassupervisorysignalfor
learningshapeandposeprediction.In
ComputerVisionandPatternRecognition(CVPR)
,2018.
[42]
K.He,X.Zhang,S.Ren,andJ.Sun.DelvingdeepintoSurpassinghuman-levelperformance
onimagenetIn
2015IEEEInternationalConferenceonComputerVision(ICCV)
,pages
1026Œ1034,Dec2015.
11
Appendices
Inthisdocument,weprovidemoredetailsfortheimage-basedreconstructiontask,andadditionalresultsfor
thechairdataset(Section
A
).Wealsoprovidemoredetailsfordifferentnetworkarchitectures(Section
B
).In
Section
C
,wecommentonthecurrentlimitationsofRenderNet.
AImage-basedreconstruction
A.1Detailsontheoptimizationprocess
Weoptimizeforthelossfunction:
minimize
z
0
;˚
0

jj
I

f
(
g
(
z
0
)
;;h
(
˚
0
)
;
)
jj
2
(4)
where
I
istheobservedimage,
z
0
isalatentvectorrepresentationofthe3Dshape,
g
isthedecoderofan
autoencoderusedtolearnpriorinformationaboutshapes(SeeSection
A.3
),

istheposeparameter,

isthe
lightingparameter,
˚
0
isthetexturevectorand
h
isthetexturedecoder.
z
0
isa200-dimensionalvectorinthis
experiment.
Fortheposeparameter

,sinceweonlyobservefacesfromthefrontalhemisphere,wesubdividetheposespace
of[0Œ180]degreesforazimuth,[0Œ180]degreesforelevationintoagridandusethegridpointsforinitialization.
Thegridislaterfurthersubdividedaroundcurrentbestposeparameters.Toavoidlocalminima,weinitialize
multiplesof
(
z
0
i
;
i
;˚
0
i
;
i
)
(
i
2f
1
;
2
;:::;
5
g
inourexperiment)andusegradientdescentforoptimizingallof
thevariables.Were-initialisetheparameterswiththecurrentbestonesafterevery200steps,andcontinuewith
theoptimizationuntilconvergence,whichtakesaround1800steps.
A.2Chairreconstructionfromasingleimage
Weoptimizeforthelossfunction:
minimize
z

jj
I

f
(
g
(
z
0
)
;
)
jj
2
+

(
z


)
T


1
(
z


)
(5)
where
I
istheobservedimage,
z
0
isalatentvectorrepresentationofthe3Dshape,
g
isthedecoderofan
autoencoderusedtolearnpriorinformationaboutshapes,

istheposeparameter,

and

arethemeanand
covarianceof
z
0
estimatedfromthetrainingsetrespectively,and

and

aretheweightsofthelossterms(we
use

=5
,

=1
).
z
0
isa250-dimensionalvectorinthisexperiment.WealsocomparewithDC-IGN[
29
],
however,wecouldnotdownloadthesamedataset[
40
]thatwasusedforDC-IGNduetobrokendownloadlinks.
Therefore,weusethechaircategoryfromShapeNet,whichisverysimilarandgreatlyoverlapswiththedataset
usedinDC-IGN,asasubstituteinthisexperiment.Weusethesechairmodelstocreategreyscaleshadedimages
usedasinputsforthereconstructiontask.
Weadoptthesameoptimisationstrategyaswiththefacereconstruction(gridsubdivisionandgradientdescent
fortheposeandshapevector).Theoptimisationconvergesafter2000steps.TheresultsareshowninFigure
7
.Reconstructingchairsisamuchmorechallengingtaskthanreconstructingfaces,duetothelargersearch
spacefortheposeparameter([0Œ360]forazimuth,[0Œ180]forelevation),aswellasthelargervarianceinthe
geometryofdifferentchairs,especiallythosecontainingverythinparts,thatmightnotbefullycapturedbythe
shapeprior.However,forataskaschallengingassimultaneousshapeandposeestimation,theresultsshow
greatpotentialofourmethod,insteadofusingafeed-forwardnetworksimilartotheworkofTulsianietal.
[
41
]
.
Furtherworkisneededtoimprovethespeedandperformanceofthismethod.
A.33Dshapeautoencoderforlearningshapeprior
Wetrainanantoencodertolearnapriorof3Dshapes.Theencoderisaseriesof3Dconvolutionswithchannels
{64,128,256,512},kernelsizes{5,5,2,2},andstrides{2,2,2,2}respectively.Thefully-connectedlayer
inthemiddlemapstheoutputofthelastconvolutionlayertoa200-dimensionalvector.Thisisfollowedby
asigmoidactivationfunctionandanotherfully-connectedlayerthatmapsthe200-dimensionalvectortoa
(4

4

512)
-dimensionalvector.Thisvectoristhenreshapedtoatensorofsize4

4

512beforebeingfedtoa
seriesof3Dup-convolutionswithchannels{256,128,64,32,1},kernelsizes{4,4,4,4,4},andstrides{2,2,2,
2,1}.HereweuseELUactivationfunctionsforalllayers,apartfromthelastconvolutionlayerintheencoder
anddecoder,whichusessigmoidfunctions.
12
Figure7:Reconstructingchairsfromasingleimage,comparedtoDC-IGN[
29
].Thecrosses
indicatefactorsnotlearntbythenetwork.Wewereabletorecoverboththeposeandshapeofthe
chairs,whichcanbeusedtoachievesharperresultsinthetaskofnovel-viewsynthesis,aswellas
enablingimagerelighting.
A.4Reconstructionwithoutprior
Toshowtheimportanceofusingtheshapeprior
g
(
z
0
)
,herewecomparethereconstructionresultsbetween
thosewiththeshapepriorandthosewithout,i.e.,wedirectlyoptimizefortheshape
z
withoutusingtheprior
g
(
z
0
)
.Figure
8
showsthatthereconstructionwithoutusingtheshapepriorfailstogenerategoodresults.
BNetworkArchitecture
AllofourlayersuseparametricRelu(PReLU)[
42
],apartfromthelastlayer,whichusesasigmoidfunction.
Wealsousedropoutswiththeprobabilityof0.5duringtrainingaftereveryconvolution,exceptthoseusedinthe
residualblocks.
Each3Dresidualblockconsistsofa3

3

33Dconvolution,aPReLUactivationfunction,andanother3

3

3
3Dconvolution.Theinputtotheblockisthenaddedtotheoutputofthesecondconvolution(shortcutconnection).
Each2Dresidualblockissimilartothe3Done,butwereplace3Dconvolutionswith2Dconvolutions.
B.1RenderNet
The3Dinputencoderconsistsofanencodermadeupof3Dconvolutionswithchannels{8,16,16},kernel
sizes{5,3,3},andstrides{2,2,1}respectively.Weaddten3Dresidualblocks,beforefeedingtheresultof
thelastblocktotheprojectionunit.TheunitresizesthetensorfromW

H

32

16toW

H

(32

16)before
feedingittoa1

1convolutionwiththesamenumberofchannels.Thisisfollowedbyten2Dresidualblocks,
a4

4convolutionwith
(32

8)
channels,andanotherve2Dresidualblocks.Toproducetherendered
image,weuseaseriesof2Dconvolutionswithchannels{32

4,32

2,32,16,3(or1forgreyscaleimage)},
kernelsizes{4,4,4,4,4}andstrides{1,2,2,2,1},respectively.Togenerateothermodalitiesoftheoutput
(forexample,inSection4.1wherewerenderboththealbedomapandnormalmap),wesimplycreateanother
branchof2Dconvolutionslayersstartingatthestridedup-convolutionlayer,andtrainitjointlywiththerest
ofthenetwork.Thisallowsdifferentmodalitiestosharehigh-levelinformation,suchasobjectvisibility,and
13
Figure8:Comparisonbetweenthereconstructionresultswith/withoutprior.
onlydifferinthepixelappearance(shading).Thisshowsthepotentialtocombinetrainingdifferentshading
stylesintotrainingonemodelthatshareshigh-levelinformationandseparateslow-levelconvolutionlayersfor
differentshadingstyles.
B.2Alternativearchitecture
HerewedescribethearchitectureofthetwoalternativemodelsECandEC-deepthatareusedtocompareagainst
RenderNet(seeSection4.2inthemainpaper).
The3Dinputencoderconsistsofanencodermadeupof3Dconvolutionswithchannels{64,128,256,512},
kernelsizes{4,4,4,4},andstrides{2,2,2,2}respectively.AlloftheseconvolutionsuseparametricReLU.This
isfollowedbyafully-connectedlayertomapthetensorstoa200-dimensionalvectorandasigmoidactivation
function.
ForEC,wedirectlyconcatenatethelightingandposeparameterstotheshapelatentvector.ForEC-deep,
wefeedeachofthemthroughafully-connectedlayertomapeachtoa512-dimensionalvector.Thesetwo
vectorsarethenconcatenatedtotheshapelatentvector.Theconcatenatedvectoristhenfedthrough2
fully-connectedlayerstomaptoa1024-dimensionalvector,andanotherfully-connectedlayertomaptoa
(8

8

512)
-dimensionalvector.Theoutputofthislayerisreshapedintoatensorofsize8

8

512beforebeing
fedtothedecoder.
ForEC,thedecoderconsistsof2Dconvolutionswith4

4kernelswithchannels{512,512,256,256,128,128,
64,64,32,32,16,1}andstrides{2,1,2,1,2,1,2,1,2,1,2,1}.ForEC-deep,wereplaceeachnon-strided
convolutioninECwithtwo2Dresidualblocks.
B.3Texturedecoder
Thetexturedecoderconsistsofafully-connectedlayertomapthe199-dimensionalvectorinputtoavectorof
size
(32

32

32

4)
,whichisthenreshapedintoatensorofsize32

32

32

4.Thisisfollowedbyaseriesof
3Dconvolutionswithchannels{4,8,4},kernelsizes{4,4,4},andstrides{1,2,1}respectively.Theoutputisa
tensorofsize64

64

64

4.
CLimitations
RenderNetwastrainedusingmeansquarederrorloss(orbinarycross-entropylossforgreyscaleimages),which
tendstocreateblurryresults.Theeffectismoreobviousincertainshaders,suchastheAmbientOcclusion.This
canpotentiallybesolvedbyaddinganadversarialloss,butweconsiderthistobefuturework.
14
Figure9:Failurecases.
Anotherpotentiallimitationofourmethodistheinputvoxelgridresolution.Wemitigatethislimitationby
trainingRenderNetonsmaller,croppedvoxelgridsandrunninginferenceonlargervoxelgrids.Thisismade
possiblebythefullyconvolutionaldesignofourarchitecture.Notethattheoutputsizeisnotlimited.Inthe
future,wecouldleveragedatastructuressuchasoctreesordifferentdataformatssuchasunstructuredpoint
cloudstofurtherimprovethescalabilityofourmodel.
AsshowninFigure
9
,RenderNethasatendencytoover-smoothensharpdiagonalshapes.RenderNetalsofails
torenderextremelythinfeatures,whichcaneasilybehandledwiththemeshrenderer.However,thiscanbe
consideredtobethelimitationofthevoxelizingtool,astheinputvoxelgridfailstocaptureverythinfeatures,
andnotofRenderNetitself.
References
[1]
DaniloJimenezRezende,S.M.AliEslami,ShakirMohamed,PeterBattaglia,MaxJaderberg,andNicolas
Heess.Unsupervisedlearningof3dstructurefromimages.In
NIPS
,pages4996Œ5004.2016.
[2]
MatthewM.LoperandMichaelJ.Black.OpenDR:Anapproximatedifferentiablerenderer.In
ECCV
,
pages154Œ169.2014.
[3]
HiroharuKato,YoshitakaUshiku,andTatsuyaHarada.Neural3dmeshrenderer.In
IEEECVPR
,2018.
[4]
XinchenYan,JimeiYang,ErsinYumer,YijieGuo,andHonglakLee.Perspectivetransformernets:
Learningsingle-view3dobjectreconstructionwithout3dsupervision.In
NIPS
,pages1696Œ1704.2016.
[5]
RuiZhu,HamedKianiGaloogahi,ChaoyangWang,andSimonLucey.Rethinkingreprojection:Closing
theloopforpose-awareshapereconstructionfromasingleimage.In
IEEECVPR
,pages57Œ65,2017.
[6]
JiajunWu,YifanWang,TianfanXue,XingyuanSun,WilliamTFreeman,andJoshuaBTenenbaum.
MarrNet:3DShapeReconstructionvia2.5DSketches.In
NIPS
,2017.
[7]
ShubhamTulsiani,TinghuiZhou,AlexeiA.Efros,andJitendraMalik.Multi-viewsupervisionfor
single-viewreconstructionviadifferentiablerayconsistency.In
IEEECVPR
,pages209Œ217,2017.
[8]
PaulHendersonandVittorioFerrari.Learningtogenerateandreconstruct3dmesheswithonly2d
supervision.In
BritishMachineVisionConference(BMVC)
,2018.
[9]
KyleGenova,ForresterCole,AaronMaschinot,AaronSarna,DanielVlasic,andWilliamT.Freeman.
Unsupervisedtrainingfor3dmorphablemodelregression.In
TheIEEEConferenceonComputerVision
andPatternRecognition(CVPR)
,June2018.
[10]
AbhijitKundu,YinLi,andJamesM.Rehg.3d-rcnn:Instance-level3dobjectreconstructionviarender-
and-compare.In
CVPR
,2018.
[11]
E.Richardson,M.Sela,R.Or-El,andR.Kimmel.Learningdetailedfacereconstructionfromasingle
image.In
2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR)
,pages5553Œ5562,
July2017.doi:10.1109/CVPR.2017.589.
15
[12]
JunYoungGwak,ChristopherBChoy,ManmohanChandraker,AnimeshGarg,andSilvioSavarese.Weakly
supervised3dreconstructionwithadversarialconstraint.In
3DVision(3DV),2017FifthInternational
Conferenceon3DVision
,2017.
[13]
MinLin,QiangChen,andShuichengYan.Networkinnetwork.In
ICLR
,2014.
[14]
GaoHuang,ZhuangLiu,LaurensvanderMaaten,andKilianQWeinberger.Denselyconnectedconvolu-
tionalnetworks.In
IEEECVPR
,pages2261Œ2269,2017.
[15]
O.Ronneberger,P.Fischer,andT.Brox.U-net:Convolutionalnetworksforbiomedicalimagesegmentation.
In
MedicalImageComputingandComputer-AssistedIntervention(MICCAI)
,volume9351,pages234Œ241,
2015.
[16]
AlexeyDosovitskiy,JostTobiasSpringenberg,MaximTatarchenko,andThomasBrox.Learningto
generatechairs,tablesandcarswithconvolutionalnetworks.39(4):692Œ705,2017.
[17]
IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,Aaron
Courville,andYoshuaBengio.Generativeadversarialnets.InZ.Ghahramani,M.Welling,C.Cortes,
N.D.Lawrence,andK.Q.Weinberger,editors,
AdvancesinNeuralInformationProcessingSystems27
,
pages2672Œ2680.CurranAssociates,Inc.,2014.
[18]
QifengChenandVladlenKoltun.Photographicimagesynthesiswithcascadednetworks.In
IEEEICCV
,pages1520Œ1529,2017.
[19]
OliverNalbach,ElenaArabadzhiyska,DushyantMehta,Hans-PeterSeidel,andTobiasRitschel.Deep
shading:Convolutionalneuralnetworksforscreen-spaceshading.
ComputerGraphicsForum(Proc.
EGSR)
,36(4):65Œ78,2017.
[20]
JonathanTBarronandJitendraMalik.Shape,illumination,andfromshading.
TPAMI
,2015.
[21]
TatsunoriTaniaiandTakanoriMaehara.Neuralinverserenderingforgeneralphotometricstereo.
In
ICML
,volume80of
JMLRWorkshopandConferenceProceedings
,pages4864Œ4873.JMLR.org,2018.
[22]
MaximTatarchenko,AlexeyDosovitskiy,andThomasBrox.Octreegeneratingnetworks:Efcient
convolutionalarchitecturesforhigh-resolution3doutputs.In
IEEEICCV
,pages2107Œ2115,2017.
[23]
Peng-ShuaiWang,YangLiu,Yu-XiaoGuo,Chun-YuSun,andXinTong.O-cnn:Octree-basedconvolu-
tionalneuralnetworksfor3dshapeanalysis.
ACMTOG(Siggraph)
,36(4):72:1Œ72:11,2017.
[24]
BuiTuongPhong.Illuminationforcomputergeneratedpictures.
Commun.ACM
,18(6):311Œ317,1975.
[25]
DougDeCarlo,AdamFinkelstein,SzymonRusinkiewicz,andAnthonySantella.Suggestivecontoursfor
conveyingshape.
ACMTOG(Siggraph)
,22(3):848Œ855,July2003.
[26]
HolgerWinnemöller,SvenC.Olsen,andBruceGooch.Real-timevideoabstraction.
ACMTOG(Siggraph)
,
25(3):1221Œ1226,2006.
[27]
GavinMiller.Efntalgorithmsforlocalandglobalaccessibilityshading.In
Proc.ACMSIGGRAPH
,
pages319Œ326,1994.
[28]
PhillipIsola,Jun-YanZhu,TinghuiZhou,andAlexeiA.Efros.Image-to-imagetranslationwithconditional
adversarialnetworks.In
IEEECVPR
,pages5967Œ5976,2017.
[29]
TejasDKulkarni,WilliamF.Whitney,PushmeetKohli,andJoshTenenbaum.Deepconvolutionalinverse
graphicsnetwork.In
NIPS
,pages2539Œ2547,2015.
[30]
EunbyungPark,JimeiYang,ErsinYumer,DuyguCeylan,andAlexanderC.Berg.Transformation-
groundedimagegenerationnetworkfornovel3dviewsynthesis.In
IEEECVPR
,pages702Œ711,2017.
[31]
K.Rematas,C.H.Nguyen,T.Ritschel,M.Fritz,andT.Tuytelaars.Novelviewsofobjectsfromasingle
image.
IEEETrans.PatternAnal.Mach.Intell.
,39(8):1576Œ1590,2017.
[32]
JimeiYang,ScottEReed,Ming-HsuanYang,andHonglakLee.Weakly-superviseddisentanglingwith
recurrenttransformationsfor3dviewsynthesis.In
NIPS
,pages1099Œ1107.2015.
[33]
HaoSu,FanWang,LiYi,andLeonidasJ.Guibas.3d-assistedimagefeaturesynthesisfornovelviewsof
anobject.
CoRR
,abs/1412.0003,2014.
[34]
BertholdK.P.Horn.Determininglightnessfromanimage.
ComputerGraphicsandImageProcessing
,3
(4):277Œ299,1974.
16
[35]
AngelX.Chang,ThomasA.Funkhouser,LeonidasJ.Guibas,PatHanrahan,Qi-XingHuang,ZimoLi,
SilvioSavarese,ManolisSavva,ShuranSong,HaoSu,JianxiongXiao,LiYi,andFisherYu.Shapenet:
Aninformation-rich3dmodelrepository.
CoRR
,abs/1512.03012,2015.
[36]
F.S.NooruddinandG.Turk.andrepairofpolygonalmodelsusingvolumetrictechniques.
IEEETrans.onVis.andComp.Graphics
,9(2):191Œ205,2003.
[37]
PascalPaysan,ReinhardKnothe,BrianAmberg,SamiRomdhani,andThomasVetter.A3dfacemodel
forposeandilluminationinvariantfacerecognition.In
Proceedingsofthe2009SixthIEEEInternational
ConferenceonAdvancedVideoandSignalBasedSurveillance
,pages296Œ301,2009.
[38]
DiederikP.KingmaandJimmBa.Adam:Amethodforstochasticoptimization.In
International
ConferenceonLearningRepresentations(ICLR)
,2015.
[39]
RohitGirdhar,DavidF.Fouhey,MikelRodriguez,andAbhinavGupta.Learningapredictableand
generativevectorrepresentationforobjects.In
ECCV
,pages484Œ499,2016.
[40]
MathieuAubry,DanielMaturana,AlexeiEfros,BryanRussell,andJosefSivic.Seeing3dchairs:exemplar
part-based2d-3dalignmentusingalargedatasetofcadmodels.In
CVPR
,2014.
[41]
ShubhamTulsiani,AlexeiA.Efros,andJitendraMalik.Multi-viewconsistencyassupervisorysignalfor
learningshapeandposeprediction.In
ComputerVisionandPatternRecognition(CVPR)
,2018.
[42]
K.He,X.Zhang,S.Ren,andJ.Sun.DelvingdeepintoSurpassinghuman-levelperformance
onimagenetIn
2015IEEEInternationalConferenceonComputerVision(ICCV)
,pages
1026Œ1034,Dec2015.
17
"
12,SubGram: Extending Skip-gram Word Representation with Substrings,http://arxiv.org/pdf/1806.06571v1.pdf,https://github.com/tomkocmi/SubGram,"arXiv:1806.06571v1  [cs.CL]  18 Jun 2018SubGram:ExtendingSkip-gramWord
RepresentationwithSubstrings
TomKocmiandOndrejBojar
CharlesUniversityinPrague,
FacultyofMathematicsandPhysics,
InstituteofFormalandAppliedLinguistics
f
kocmi,bojar
g
@ufal.mff.cuni.cz
Abstract.
Skip-gram(word2vec)isarecentmethodforcreatingvector
representationsofwords(\distributedwordrepresentati
ons"")usinga
neuralnetwork.Therepresentationgainedpopularityinva
riousareasof
naturallanguageprocessing,becauseitseemstocapturesy
ntacticand
semanticinformationaboutwordswithoutanyexplicitsupe
rvisionin
thisrespect.

WeproposeSubGram,arenementoftheSkip-grammodeltocon
sider
alsothewordstructureduringthetrainingprocess,achiev
inglargegains
ontheSkip-gramoriginaltestset.

Keywords:
Distributedwordrepresentations,unsupervisedlearning
of
morphologicalrelations
1Introduction

Vectorrepresentationsofwordslearnedusingneuralnetworks
(NN)haveproven
helpfulinmanyalgorithmsforimageannotation[1]or[2],languagemod
eling
[3],[4]and[5]orothernaturallanguageprocessing(NLP)tasks[6]
or[7].
Traditionally,everyinputwordofanNNisstoredinthe\one-hot""re
pre-
sentation,wherethevectorhasonlyoneelementsettooneandth
erestofthe
vectorarezeros.Thesizeofthevectorequalstothesizeofthev
ocabulary.The
NNistrainedtoperformsomeprediction,e.g.topredictsurroundin
gwords
givenawordofinterest.Insteadofusingthispredictioncapacityin
sometask,
thepracticeistoextracttheoutputofNN'shiddenlayerofeachwo
rd(called
distributedrepresentation
)anddirectlyusethisdeterministicmapping
vec
(
)of
wordformstothevectorsofrealnumbersasthewordrepresen
tation.
Theinputone-hotrepresentationofwordshastwoweaknesses:
thebloatof
thesizeofthevectorwithmorewordsinvocabularyandtheinabilityt
oprovide
anyexplicitsemanticorsyntacticinformationtotheNN.
Thelearneddistributedrepresentationofwordsreliesonmuchsho
rtervectors
(e.g.vocabulariescontainingmillionswordsarerepresentedinvecto
rsofafew
hundredelements)andsemanticorsyntacticinformationisoftenf
oundtobe
implicitlypresent(\embedded"")inthevectorspace.Forexample,t
heEuclidean
distancebetweentwowordsinthevectorspacemayberelatedtos
emanticor
syntacticsimilaritybetweenthem.
2SubGram:ExtendingSkip-gramWordRepresentationwithSu
bstrings
1.1Skip-gramModel

Theauthorsof[8]createdamodelcalledSkip-gram,inwhichlinearve
ctor
operationsallowtondrelatedwordswithsurprisinglygoodresults.
Forinstance
vec
(
king
)

vec
(
man
)+
vec
(
woman
)givesavaluecloseto
vec
(
queen
).
Inthispaper,weextendSkip-grammodelwiththeinternalwordst
ructure
andshowhowitimprovestheperformanceonembeddingmorpho-sy
ntacticin-
formation.
TheSkip-grammodeldenedin[8]istrainedtopredictcontextwor
dsof
theinputword.Givenacorpus
T
ofwords
w
andtheircontextwords
c
2
C
(
w
)
(i.e.individualwords
c
appearingclosetheoriginalword
w
),itconsidersthe
conditionalprobabilities
p
(
c
jw
).Thetrainingndstheparameters

of
p
(
c
jw
;
)
tomaximizethecorpusprobability:
argmax

Y
w2
T
Y
c
2
C
(
w)
p
(
c
jw
;
)(1)
TheSkip-grammodelisaclassicNN,whereactivationfunctionsarer
emoved
andhierarchicalsoft-max[9]isusedinsteadofsoft-maxnormaliz
ation.Theinput
representationisone-hotsotheactivationfunctionisnotneeded
onhiddenlayer,
thereisnothingtobesummedup.Thisway,themodelislearnedmuch
faster
thancomparablenon-linearNNsandlendsitselftolinearvectoroper
ations
possiblyusefulforndingsemanticallyorsyntacticallyrelatedword
s.
2RelatedWork

In[10]wasproposedtoappendpart-of-speech(POS)tagstoea
chwordandtrain
Skip-grammodelonthenewvocabulary.Thisavoidedconating,e.g
.nounsand
verbs,leadingtoabetterperformance,atthecostof(1)there
lianceonPOS
tagsandtheiraccurateestimationand(2)theincreasedsparsity
ofthedatadue
tothelargervocabulary.
Theauthorsin[11]usedcharacter-levelinputtotrainlanguagemo
delsusing
acomplexsetupofNNsofseveraltypes.Theirmodelwasabletoas
signmeaning
toout-of-vocabularywordsbasedontheclosestneighbor.Oned
isadvantageof
themodelisitsneedtorunthecomputationonaGPUforalongtime.
Theauthorsof[12]proposedanextensionofSkip-grammodelwhic
huses
charactersimilarityofwordstoimproveperformanceonsyntactic
andsemantic
tasks.Theyareusingasetofsimilarwordsasadditionalfeaturesf
ortheNN.
Varioussimilaritymeasuresaretested:Levenshtein,longestcomm
onsubstring,
morphemeandsyllablesimilarity.
Theauthorsof[13]addedtheinformationaboutword'sroot,ax
es,syl-
lables,synonyms,antonymsandPOStagstocontinuousbag-of-w
ordsmodel
(CBOW)proposedby[8]andshowedhowthesetypesofknowledgele
adtobet-
terwordembeddings.TheCBOWmodelisasimplermodelwithusuallywo
rse
performancethanSkip-gram.
SubGram:ExtendingSkip-gramWordRepresentationwithSub
strings3
3SubGram

Weproposeasubstring-orientedextensionofSkip-grammodelwh
ichinduces
vectorembeddingsfromcharacter-levelstructureofindividual
words.Thisap-
proachgivestheNNmoreinformationabouttheexaminedwordwithn
odraw-
backsindatasparsityorrelianceonexplicitlinguisticannotation.
Weappendthecharacters^and$tothewordtoindicateitsbeginnin
g
andend.Inordertogeneratethevectorofsubstrings,wetake
allcharacter
bigrams,trigramsetc.uptothelengthoftheword.Thisway,even
theword
itselfisrepresentedasoneofthesubstrings.FortheNN,eachinp
utwordisthen
representedasabinaryvectorindicatingwhichsubstringsappear
intheword.
TheoriginalSkip-grammodel[8]usesone-hotrepresentationofa
wordin
vocabularyastheinputvector.Thisrepresentationmakestrainin
gfastbecause
nosummationornormalizationisneeded.Theweights
w
i
oftheinputword
i
canbedirectlyusedastheoutputofhiddenlayer
h
(andasthedistributedword
representation):
h
j
=
w
i
j
Inourapproach,weprovidethenetworkwithabinaryvectorrepr
esenting
allsubstringsoftheword.Tocomputetheinputofhiddenlayerwed
ecidedto
usemeanvalueasitiscomputationallysimplerthansigmoid:
h
j
=
P
j
X
j
i
=1
x
i

w
ij
jS
j(2)
where
jS
jisthenumberofsubstringsoftheword
x
.4EvaluationandDataSets

WetrainourNNonwordsandtheircontextsextractedfromtheEn
glishwikipedia
dumpfromMay2015.Wehavecleanedthedatabyreplacingallnumbe
rswith0
andremovingspecialcharactersexceptthoseusuallypresentint
heEnglishtext
likedots,brackets,apostrophesetc.Forthenaltrainingdata
wehaverandomly
selectedonly2.5Msegments(mostlysentences).Itconsistof96M
runningwords
withthevocabularysizeof1.09Mdistinctwordforms.
Weconsideronlythe141Kmostfrequentwordformstosimplifythet
raining.
Theremainingwordformsfalloutofvocabulary(OOV),sotheorigin
alSkip-
gramcannotprovidethemwithanyvectorrepresentation.OurSu
bGramrelies
onknownsubstringsandalwaysprovidesatleastsomeapproximatio
n.
Wetestourmodelontheoriginaltestset[8].Thetestsetconsist
sof19544
\questions"",ofwhich8869arecalled\semantic""and10675arecalle
d\syntactic""
andfurtherdividedinto14types,seeTable1.Eachquestionconta
instwopairs
ofwords(
x
1
;x
2
;y
1
;y
2
)andcapturesrelationslike\Whatisto`woman'(
y
1
)
as`king'(
x
2
)isto`man'(
x
1
)?"",togetherwiththeexpectedanswer`queen'
(
y
2
).Themodelisevaluatedbyndingthewordwhoserepresentation
isthe
nearest(cosinesimilarity)tothevector
vec
(
king
)

vec
(
man
)+
vec
(
woman
).If
thenearestneighboris
vec
(
queen
),weconsiderthequestionansweredcorrectly.
4SubGram:ExtendingSkip-gramWordRepresentationwithSu
bstrings
QuestionTypeSamplePair
capital-countriesAthens{Greece

capital-worldAbuja{Nigeria

currencyAlgeria{dinar

city-in-stateHouston{Texas

familyboy{girl
adjective-to-adverbcalm{calmly

oppositeaware{unaware

comparativebad{worse

superlativebad{worst

present-participlecode{coding

nationality-adjectiveAlbania{Albanian

past-tensedancing{danced

pluralbanana{bananas

plural-verbsdecrease{decreases
Table1.
Mikolov'stestsetquestiontypes,theupperpartare\seman
tic""questions,
thelowerpartare\syntactic"".
Inthiswork,weuseMikolov'stestsetwhichisusedinmanypapers.Af
ter
acloserexaminationwecametotheconclusion,thatitdoesnottest
whatthe
broadterms\syntactic""and\semanticrelations""suggest.\Sem
antics""iscov-
eredbyquestionsofonly3types:predictacitybasedonacountry
orstate,
currencynamefromthecountryandthefemininevariantofnouns
denotingfam-
ilyrelations.Theauthorsof[14]showed,thatmanyothersemantic
relationships
couldbetested,e.g.walk-run,dog-puppy,bark-dog,cook-eat
andothers.
\Syntactic""questionscoverawiderrangeofrelationsattheboun
daryof
morphologyandsyntax.Theproblemisthatallquestionsofagivent
ypeare
constructedfromjustafewdozensofwordpairs,comparingpair
switheach
other.Overall,thereare313distinctpairsthroughoutthewholes
yntactictest
setof10675questions,whichmeansonlyaround35derentpairsp
erquestion
set.Moreover,ofthe313pairs,286pairsareregularlyformed(e
.g.byadding
thesux`ly'tochangeanadjectiveintothecorrespondingadverb
).Thoughit
hastobementionedthatoriginalmodelcouldnotusethiskindofinfo
rmation.
Wendsuchasmalltestsetunreliabletoanswerthequestionwheth
erthe
embeddingcapturessemanticandsyntacticpropertiesofwords.

4.1Rule-BasedBaselineApproach

Althoughtheoriginaltestsethasbeenusedtocompareresultsins
everalpapers,
no-onetriedtoprocessitwithsomebaselineapproach.Therefore
,wecreateda
verysimplesetofrulesforcomparisononthesyntacticpartofthe
testset.The
rulescoveronlythemostfrequentgrammaticalphenomenona.
{
adjective-to-adverb:Add
ly
attheendoftheadjective.
{
opposite:Add
un
atthebeginningofpositiveform.
SubGram:ExtendingSkip-gramWordRepresentationwithSub
strings5
{
comparative:Iftheadjectiveendswith
y
,replaceitwith
ier
.Ifitendswith
e
,add
r
.Otherwiseadd
er
attheend.
{
superlative:Iftheadjectiveendswith
y
,replaceitwith
iest
.Ifitendswith
e
,add
st
.Otherwiseadd
est
attheend.
{
present-participle:Iftheverbendswith
e
,replaceitwith
ing
,otherwiseadd
ing
attheend.
{
nationality-adjective:Add
n
attheend,e.g.
Russia
!
Russian
.{
past-tense:Remove
ing
andadd
ed
attheendoftheverb.
{
plural:Add
s
attheendoftheword.
{
plural-verbs:Ifthewordendswithavowel,add
es
attheend,elseadd
s
.4.2OurTestSet

Wehavedecidedtocreatemoregeneraltestsetwhichwouldconsid
ermorethan
35pairsperquestionset.Sinceweareinterestedinmorphosyntac
ticrelations,we
extendedonlythequestionsofthe\syntactic""typewithexceptio
nofnationality
adjectiveswhichisalreadycoveredcompletelyinoriginaltestset.
Weconstructedthepairsmoreorlessmanually,takinginspirationint
he
CzechsideoftheCzEngcorpus[15],whereexplicitmorphologicalann
otation
allowstoidentifyvariouspairsofCzechwords(derentgradesofa
djectives,
wordsandtheirnegations,etc.).Theword-alignedEnglishwordsof
tenshared
thesameproperties.Anothersourcesofpairswereacquiredfro
mvariousweb-
pagesusuallywrittenforlearnersofEnglish.Forexampleforverbt
ense,we
reliedonafreelyavailablelistofEnglishverbsandtheirmorphologicalv
ari-
ations.Wehaveincluded100{1000derentpairsforeachquestion
set.The
questionswereconstructedfromthepairssimilarlyasbyMikolov:ge
nerating
allpossiblepairsofpairs.Thisleadstomillionsofquestions,sowerand
omly
selected1000instancesperquestionset,tokeepthetestsetint
hesameorder
ofmagnitude.Additionally,wedecidedtoextendsetofquestionson
opposites
tocovernotonlyoppositesofadjectivesbutalsoofnounsandver
bs.
InordertotestourextensionofSkip-gramonout-of-vocabular
ywords,we
createdanadditionalsubsetofourtestsetwithquestionswhere
atleastoneof
x
1
;x
2
and
y
1
isnotamongtheknownwordforms.Notethatthelastword
y
2
mustbeinvocabularyinordertocheckiftheoutputvectoriscorre
ct.
5ExperimentsandResults

WeusedaPythonimplementationofword2vec
1
asthebasisforourSubGram,
whichwehavemadefreelyavailable
2
.1
http://radimrehurek.com/gensim

Gensimimplementsthemodeltwice,inPythonandanoptimize
dversioninC.For
ourprototype,weoptedtomodifythePythonversion,whichu
nfortunatelyresulted
inacodeabout100timesslowerandforcedustotrainthemode
lonlyonthe
96MwordcorpusasopposedtoMikolov's100000Mword2vectra
iningdatausedin
trainingofthereleasedmodel.
2
https://github.com/tomkocmi/SubGram
6SubGram:ExtendingSkip-gramWordRepresentationwithSu
bstrings
Welimitthevocabulary,requiringeachwordformtoappearatleast1
0times
inthecorpusandeachsubstringtoappearatleast500timesinthec
orpus.This
way,wegetthe141Kuniquewordsmentionedaboveand170Kunique
substrings
(+141Kwords,aswedownsamplewordsseparately).
Ourwordvectorshavethesizeof100.Thesizeofthecontextwind
owis5.
Theaccuracyiscomputedasthenumberofcorrectlyansweredqu
estions
dividedbythetotalnumberofquestionsintheset.BecausetheSk
ip-gram
cannotanswerquestionscontainingOOVwords,wealsoprovideres
ultswith
suchquestionsexcludedfromthetestset(scoresinbrackets).
Rulebased
ReleasedSkip-gram
OurSkip-gram
SubGram
capital-countries0%
18.6%(24.7%)
71.9%(71.9%)
0%(0%)
capital-world0%
2.2%(15.0%)
53.6%(54.6%)
0%(0%)
currency0%
7%(12.2%)
3%(4.7%)
0.1%(0.2%)
city-in-state0%
9.2%(14%)
40.5%(40.5%)
0.1%(0.1%)
family0%
84.6%(84.6%)
82.6%(82.6%)
5.9%(5.9%)
Overallsemantic
0%
10.2%(24.8%)
47.7%(50%)
0%(0%)
adjective-to-adverb90.6%
28.5%(28.5%)
16.3%(16.3%)
73.7%(73.7%)
opposite65.5%
42.7%(42.7%)
9.4%(10.1%)
43.1%(46.3%)
comparative89.2%
90.8%(90.8%)
72.1%(72.1%)
46.5%(46.5%)
superlative88.2%
87.3%(87.3%)
24.4%(25.9%)
45.9%(48.8%)
present-participle87.9%
78.1%(78.1%)
44.2%(44.2%)
43.5%(43.5%)
nationality-adjective31.7%
13.3%(21.9%)
60.4%(60.4%)
21.8%(21.8%)
past-tense42.5%
66%(66%)
35.6%(35.6%)
15.8%(15.8%)
plural86.5%
89.9%(89.9%)
46.8%(46.8%)
44.7%(44.7%)
plural-verbs93.3%
67.9%(67.9%)
51.5%(51.5%)
74.3%(74.3%)
Overallsyntactic
71.9%
62.5%(66.5%)
42.5%(43%)
42.3%(42.7%)
Table2.
Resultsonoriginaltestsetquestions.Thevaluesinbracke
tsarebasedon
questionswithoutanyOOVs.
Table2andTable3reporttheresults.Therstcolumnshowstheru
le-
basedapproach.Thecolumn\ReleasedSkip-gram""showsresultso
fthemodel
releasedbyMikolov
3
andwastrainedona100billionwordcorpusfromGoogle
Newsandgenerates300dimensionalvectorrepresentation.The
thirdcolumn
showsSkip-grammodeltrainedonourtrainingdata,thesamedata
asusedfor
thetrainingoftheSubGram.Lastcolumnshowstheresultsobtaine
dfromour
SubGrammodel.
ComparingSkip-gramandSubGramontheoriginaltestset(Table2)
,wesee
thatourSubGramoutperformsSkip-graminseveralmorpho-syn
tacticquestion
setsbutoverallperformssimilarly(42.5%vs.42.3%).Ontheotherh
and,itdoes
notcapturethetestedsemanticrelationsatall,gettingazerosco
reonaverage.
Whencomparingmodelsonourtestset(Table3),weseethatgivent
he
sametrainingset,SubGramsigncantlyoutperformsSkip-grammo
del(22.4%
3
https://code.google.com/archive/p/word2vec/
SubGram:ExtendingSkip-gramWordRepresentationwithSub
strings7
Rulebased
ReleasedSkip-gram
OurSkip-gram
SubGram
Type
Ourtestset
OOV
adjective-to-adverb68.4%
18.8%(20.9%)
1.9%(3.7%)
32.3%(62.7%)
2.3%
opposite3.7%
6.3%(6.4%)
5.3%(5.6%)
0.6%(0.6%)
0.7%
comparative90.2%
67.1%(68.9%)
12%(31.5%)
14.4%(37.8%)
0%
superlative92.5%
57%(59.9%)
4.4%(16.1%)
12.2%(44.7%)
0.5%
present-participle88.7%
50.2%(53%)
12.8%(16.2%)
37.3%(47.3%)
4.8%
past-tense75%
53.5%(56.5%)
17.1%(22.8%)
24.2%(32.3%)
0.5%
plural26.8%
39.1%(42.1%)
8.9%(13.8%)
13.6%(21.1%)
1.7%
plural-verbs85.8%
56.1%(59%)
15.4%(20.3%)
44.3%(58.5%)
2%
Overallsyntactic
66.4%
43.5%(45.9%)
9.7%(15.4%)
22.4%(35.4%)
1.6%
Table3.
Resultsonourtestsetquestions.
vs.9.7%).TheperformanceofSkip-gramtrainedonthemuchlarge
rdatasetis
higher(43.5%)anditwouldbeinterestingtoseetheSubGrammodel,if
wecould
getaccesstosuchtrainingdata.Notehowever,thattheRule-ba
sedbaselineis
signcantlybetteronbothtestsets.
ThelastcolumnsuggeststhattheperformanceofourmodelonOO
Vwords
isnotveryhigh,butitisstillanimprovementoveratzerooftheSkip
-gram
model.TheperformanceonOOVsisexpectedtobelower,sincethem
odelhas
noknowledgeofexceptionsandcanonlybenetfromregularitiesins
ubstrings.
6FutureWork

Weareworkingonabettertestsetforwordembeddingswhichwould
include
manymorerelationsoveralargervocabularyespeciallysemanticsre
lations.We
wanttoextendthetestsetwithCzechandperhapsotherlanguag
es,toseewhat
wordembeddingscanbringtolanguagesmorphologicallyricherthanE
nglish.
Asshownintheresults,therulebasedapproachoutperformNNap
proach
onthistypeoftask,thereforewewouldliketocreateahybridsyst
emwhich
coulduserulesandpart-of-speechtags.Wewillalsoincludemorpho
logicaltags
inthemodelasproposedin[10]butwithoutmakingthedatasparse.
Finally,weplantoreimplementSubGramtoscaleuptolargertrainingda
ta.
7Conclusion

WedescribedSubGram,anextensionoftheSkip-grammodelthatc
onsidersalso
substringsofinputwords.Thelearnedembeddingsthenbetterca
pturealmost
allmorpho-syntacticrelationstestedontestsetwhichweextend
edfromoriginal
describedin[8].Thistestsetisreleasedforthepublicuse
4
.Anusefulfeatureofourmodelistheabilitytogeneratevectorem
beddings
evenforunseenwords.ThiscouldbeexploitedbyNNsalsoinderent
tasks.
4
https://ufal.m.cuni.cz/tom-kocmi/syntactic-questio
ns
8SubGram:ExtendingSkip-gramWordRepresentationwithSu
bstrings
8Acknowledgment

ThisworkhasreceivedfundingfromtheEuropeanUnion'sHorizon20
20research
andinnovationprogrammeundergrantagreementno.645452(QT
21),thegrant
GAUK8502/2016,andSVVprojectnumber260333.
Thisworkhasbeenusinglanguageresourcesdeveloped,storedan
ddis-
tributedbytheLINDAT/CLARINprojectoftheMinistryofEducat
ion,Youth
andSportsoftheCzechRepublic(projectLM2015071).

References
1.Lazaridou,A.,Pham,N.T.,Baroni,M.:Combininglanguag
eandvisionwitha
multimodalskip-grammodel.arXivpreprintarXiv:1501.02
598(2015)
2.Weston,J.,Bengio,S.,Usunier,N.:Wsabie:Scalingupto
largevocabularyimage
annotation.IJCAI,vol.11(2011)
3.Schwenk,H.,Gauvain,J.L.:Neuralnetworklanguagemode
lsforconversational
speechrecognition.INTERSPEECH(2004)
4.Schwenk,H.,Dchelotte,D.,Gauvain,J.L.:Continuoussp
acelanguagemodels
forstatisticalmachinetranslation.ProceedingsoftheCO
LING/ACLonMain
conferencepostersessions(2006)
5.Mnih,A.,Hinton,G.:Threenewgraphicalmodelsforstati
sticallanguagemod-
elling.Proceedingsofthe24thinternationalconferenceo
nMachinelearning(2007)
6.Soricut,R.,Och,F.:Unsupervisedmorphologyinduction
usingwordembeddings.
Proc.NAACL(2015)
7.Wang,Z.,Zhang,J.,Feng,J.,Chen,Z.:Knowledgegraphan
dtextjointlyem-
bedding.Proceedingsofthe2014ConferenceonEmpiricalMe
thodsinNatural
LanguageProcessing(EMNLP).AssociationforComputation
alLinguistics(2014)
8.Mikolov,T.,Chen,K.,Corrado,G.,Dean.,J.:Ecientest
imationofwordrepre-
sentationsinvectorspace.arXivpreprintarXiv:1301.378
1(2013)
9.Morin,F.,Bengio,Y.:Hierarchicalprobabilisticneura
lnetworklanguagemodel.
ProceedingsoftheinternationalworkshoponAIandstatist
ics(2005)
10.Lin,Q.,Cao,Y.,Nie,Z.,Rui,Y.:Learningwordrepresen
tationconsideringprox-
imityandambiguity.Twenty-EighthAAAIConferenceonArti
cialIntelligence
(2014)
11.Yoon,K.,Jernite,Y.,Sontag,D.,Rush,A.M.:Character
-awareneurallanguage
models.arXivpreprintarXiv:1508.06615(2015)
12.Cui,Q.,Gao,B.,Bian,J.,Qiu,S.,Liu,T.Y.:Aframework
forlearningknowledge-
poweredwordembedding.AFrameworkforLearningKnowledge
-PoweredWord
Embedding(2014)
13.Bian,J.,Gao,B.,Liu,T.Y.:Knowledge-powereddeeplea
rningforwordembed-
ding.MachineLearningandKnowledgeDiscoveryinDatabase
s(2014)
14.Vylomova,E.,Rimmel,L.,Cohn,T.,Baldwin,T.:Takeand
took,gaggleand
goose,bookandread:Evaluatingtheutilityofvectordier
encesforlexicalrelation
learning.arXivpreprintarXiv:1509.01692(2015)
15.Bojar,O.,Dusek,O.,Kocmi,T.,Libovicky,J.,Novak
,M.,Popel,M.,Sudarikov,
R.,Varis,D.:Czeng1.6:Enlargedczech-englishparallel
corpuswithprocessing
toolsdockered.In:SpringerInternationalPublishing(in
press).(2016)
"
13,ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing,http://arxiv.org/pdf/1706.07929v2.pdf,https://github.com/jianzhangcs/ISTA-Net-PyTorch,"ISTA-Net:InterpretableOptimization-InspiredDeepNetworkforImage
CompressiveSensing
JianZhang,BernardGhanem
KingAbdullahUniversityofScienceandTechnology(KAUST),SaudiArabia
jian.zhang@kaust.edu.sa,bernard.ghanem@kaust.edu.sa
Abstract
Withtheaimofdevelopingafastyetaccuratealgorithm
forcompressivesensing(CS)reconstructionofnaturalim-
ages,wecombineinthispaperthemeritsoftwoexisting
categoriesofCSmethods:thestructureinsightsoftra-
ditionaloptimization-basedmethodsandthespeedofre-
centnetwork-basedones.,weproposeanovel
structureddeepnetwork,dubbedISTA-Net,whichisin-
spiredbytheIterativeShrinkage-ThresholdingAlgorithm
(ISTA)foroptimizingageneral
`
1
normCSreconstruction
model.TocastISTAintodeepnetworkform,wedevelop
aneffectivestrategytosolvetheproximalmappingasso-
ciatedwiththesparsity-inducingregularizerusingnonlin-
eartransforms.AlltheparametersinISTA-Net(
e.g
.non-
lineartransforms,shrinkagethresholds,stepsizes,etc.)
arelearnedend-to-end,ratherthanbeinghand-crafted.
Moreover,consideringthattheresidualsofnaturalimages
aremorecompressible,anenhancedversionofISTA-Net
intheresidualdomain,dubbedISTA-Net
+
,isderivedto
furtherimproveCSreconstruction.ExtensiveCSexperi-
mentsdemonstratethattheproposedISTA-Netsoutperform
existingstate-of-the-artoptimization-basedandnetwork-
basedCSmethodsbylargemargins,whilemaintaining
fastcomputationalspeed.Oursourcecodesareavailable:
http://jianzhang.tech/projects/ISTA-Net
.
1.Introduction
Frommuchfeweracquiredmeasurementsthandeter-
minedbyNyquistsamplingtheory,CompressiveSens-
ing(CS)theorydemonstratesthatasignalcanberecon-
structedwithhighprobabilitywhenitexhibitssparsityin
sometransformdomain[2,1].Thisnovelacquisitionstrat-
egyismuchmorehardware-friendlyanditenablesim-
ageorvideocapturingwithasub-Nyquistsamplingrate
[29,47].Inaddition,byexploitingtheredundancyinher-
enttoasignal,CSconductssamplingandcompressionat
thesametime,whichgreatlyalleviatestheneedforhigh
transmissionbandwidthandlargestoragespace,enabling
Figure1.CSreconstructionresultsproducedbyourproposed
ISTA-Netmethodandarecentnetwork-basedCSreconstruction
method(ReconNet[11]),whentheCSratiois25%.ISTA-Net
clearlyproducesahigherreconstruction.
low-coston-sensordatacompression.CShasbeenapplied
inmanypracticalapplications,includingbutnotlimitedto
single-pixelimaging[1,26],acceleratingmagneticreso-
nanceimaging(MRI)[3],wirelesstele-monitoring[28]and
cognitiveradiocommunication[27].
Mathematically,thepurposeofCSreconstructionisto
infertheoriginalsignal
x
2
R
N
fromitsrandomizedCS
measurements
y
=

2
R
M
.Here,

2
R
M

N
isa
linearrandomprojection(matrix).Because
M
˝
N
,this
inverseproblemistypicallyill-posed,wherebytheCSra-
tioisas
M
N
.Inthispaper,wemainlyfocusonCS
reconstructionofnaturalimages.However,itisworthnot-
ingthatourproposedframeworkcanbeeasilyextendedto
videosandothertypesofdata.
Inthepastdecade,agreatdealofimageCSreconstruc-
tionmethodshavebeendeveloped[4,8,43,11].Most
traditionalmethodsexploitsomestructuredsparsityasan
imagepriorandthensolveasparsity-regularizedoptimiza-
tionprobleminaniterativefashion[5,7,19,49,9].Based
onsomewell-studiedimageformationmodelsandintrin-
sicimageproperties,thesemethodsenjoytheadvantagesof
strongconvergenceandtheoreticalanalysisinmostcases.
However,theyusuallysufferfromhighcomputationalcom-
plexity,andtheyarealsofacedwiththechallengesof
choosingoptimaltransformsandtuningparametersintheir
solvers.Fueledbythepowerfullearningabilityofdeepnet-
works,severaldeepnetwork-basedimageCSreconstruc-
arXiv:1706.07929v2  [cs.CV]  18 Jun 2018Figure2.
IllustrationofourproposedISTA-Netframework.,ISTA-Netiscomposedof
N
p
phases,andeachphasestrictlycorrespondsto
oneiterationinISTA.Theforwardtransform
F
(
k
)
isdesignedasacombinationoftwolinearconvolutionaloperatorsseparatedbyalinearunit
(ReLU),andthebackwardtransform
e
F
(
k
)
isdesignedtoexhibitastructuresymmetrictothatof
F
(
k
)
.Notethat
F
(
k
)
and
e
F
(
k
)
satisfythesymmetry
constraint
e
F
(
k
)
F
(
k
)
=
I
,where
I
istheidentityoperator.
N
f
denotesthenumberoffeaturemaps.
tionalgorithmshavebeenrecentlyproposedtodirectly
learntheinversemappingfromtheCSmeasurementdo-
maintotheoriginalsignaldomain[10,43].Compared
tooptimization-basedalgorithms,thesenon-iterativealgo-
rithmsdramaticallyreducetimecomplexity,whileachiev-
ingimpressivereconstructionperformance.However,ex-
istingnetwork-basedCSalgorithmsaretrainedasa
black
box
,withlimitedinsightsfromtheCSdomain.
Inthispaper,wedesignanoveldeeparchitecture,
dubbedISTA-Net,bymappingthetraditionalISTA[12]for
optimizingageneral
`
1
normCSreconstructionmodelinto
adeepnetwork.Inparticular,ISTA-Netiscomposedofa
ednumberofphases,eachofwhichstrictlycorresponds
toanISTA-likeiteration,asillustratedinFigure2.Rather
thantraditionallineartransforms,nonlinearlearnableand
sparsifyingtransformsareadoptedinISTA-Net,andanef-
andeffectivestrategytosolvetheproximalmapping
ofthenonlineartransformisdeveloped.Alltheparameters
involvedinISTA-Net(
e.g
.nonlineartransforms,shrinkage
thresholds,stepsizes,etc.)arelearnedend-to-endusing
back-propagation.Moreover,borrowingmoreinsightsfrom
thecompressionrealm,anenhancedversion,dubbedISTA-
Net
+
,isderivedfromISTA-Netbysparsifyingnaturalim-
agesexplicitlyintheresidualdomain.Interestingly,the
skipconnectionsintroducedbyISTA-Net
+
furtherfacili-
tatethenetworktraining.Infact,theproposedISTA-Nets
canbeviewedasanattempttobridgethegapbetweenthe
twoaforementionedcategoriesofCSmethods.
Contributions.
Themaincontributionsofthispaperare
summarizedasfollows:
1)
WedevelopanovelISTA-Net,
whichadoptsthestructureofISTAupdatestepstodesigna
learnabledeepnetworkmanifestation,whereallparameters
arediscriminatelylearnedinsteadofbeinghand-craftedor
ed.Bylearningsparsifyingtransformsintheresidualdo-
main,anenhancedversionISTA-Net
+
isderivedtofurther
improveCSperformance.Assuch,ISTA-Netsenjoythe
advantagesoffastandaccuratereconstructionwithwell-
interpretability.
2)
Theproximalmappingproblem
associatedtoanonlinearsparsifyingtransformissolvedin
ageneralandefway,whichactuallyenablesmap-
pingotheroptimizationalgorithmsintodeepnetworkform.
3)
ExtensiveexperimentsonnaturalimageandMRICSre-
constructionclearlyshowthatISTA-Netout-
performsthestate-of-the-art,whilemaintainingattractive
computationalcomplexity.
2.RelatedWork
WegenerallygroupexistingCSreconstructionmeth-
odsofimagesintotwocategories:
traditionaloptimization-
based
CSmethodsandrecent
network-based
CSmethods.
Inwhatfollows,wegiveabriefreviewofbothandfocuson
themethodsmostrelevanttoourown.
Optimization-basedCSreconstruction:
Giventhelin-
earmeasurements
y
,traditionalimageCSmethodsusually
reconstructtheoriginalimage
x
bysolvingthefollowing
(generallyconvex)optimizationproblem:
min
x
1
2
k


y
k
2
2
+

k

k
1
;
(1)
where

denotesthetransformcoefof
x
withre-
specttosometransform

andthesparsityofthevector

isencouragedbythe
`
1
normwith

beingthe(gener-
allyregularizationparameter.
Sincenaturalimagesaretypicallynon-stationary,the
classiceddomains(
e.g
.DCT,wavelet[4],andgradi-
entdomain[7])usuallyresultinpoorreconstructionper-
formance.Manyworksincorporateadditionalpriorknowl-
edgeabouttransformcoef(
e.g
.statisticaldependen-
cies[5,52],structure[16],etc.)intotheCSreconstruc-
tionframework.Furthermore,someelaboratepriorsex-
ploitingthenon-localself-similaritypropertiesofnatural
imageshavebeenproposedtoimproveCSreconstruction
[6,19,8,51].Metzler
etal
.proposetointegratethewell-
BM3Ddenoiserintotheapproximatemessagepass-
ing(AMP)frameworkforCSreconstruction[9].Quitere-
cently,somefastandeffectiveconvolutionalneuralnetwork
(CNN)denoisersaretrainedandintegratedintothehalf
quadraticsplitting(HQS)[31]andthealternatingdirection
methodofmultipliers(ADMM)[32]tosolveimageinverse
problems.However,allthesetraditionalimageCSmethods
requirehundredsofiterationstosolveEq.(1)bymeansof
variousiterativesolvers,whichinevitablygivesrisetohigh
computationalcostthusrestrictingtheapplicationofCS.In
addition,theselectedimageprior(
e.g
.optimaltransform)
ortheoptimizationparameters(
e.g
.stepsizeandregular-
izationparameter)areusuallyhand-craftedandquitechal-
lengingto
Network-basedCSreconstruction:
Inspiredbythepow-
erfullearningcapabilityofdeepnetworks[53]andits
successincomputervisiontasks[17,18],severaldeep
network-basedimageCSreconstructionalgorithmshavere-
centlybeenproposed[10,43,11,33].Mousavi
etal
.
proposetoapplyastackeddenoisingauto-encoder(SDA)
tolearntherepresentationfromtrainingdataandtorecon-
structtestdatafromtheirCSmeasurements[10].Adler
etal
.andIliadis
etal
.separatelyproposetoutilizefully-
connectedneuralnetworksforimageandvideoCSre-
construction[35,43].Kulkarni
etal
.furtherdevelopa
CNN-basedCSalgorithm,dubbedReconNet,whichlearns
toregressanimageblock(output)fromitsCSmeasure-
ment(input)[11].MousaviandBaraniukrecentlypropose
anall-convolutionalnetworkforimageCSreconstruction
[33].Amainfeatureofnetwork-basedimageCSmeth-
odsisthattheyarenon-iterative,whichdramaticallyre-
ducestimecomplexityascomparedwiththeiroptimization-
basedcounterparts.However,thisisdonewitheitherfully-
connectedorrepetitiveconvolutionallayers.Webelieve
thattheirlackofstructuraldiversity,whichoriginatesfrom
theabsenceofCSdomaininsightsinherentto
optimization-basedmethods,isthebottleneckforfurther
performanceimprovement.
Thetremendoussuccessofdeeplearningformanyim-
ageprocessingapplicationshasalsoledresearcherstocon-
siderrelatingiterativeoptimizationmethodstoneuralnet-
works[44,45,46,48].Forinstance,inthecontextofsparse
coding,GreforandLeCunproposeafastalgorithmtocalcu-
lategoodapproximationsofoptimalsparsecodesbyintro-
ducingtheLearnedISTA(LISTA),inwhichtwomatrices
inclassicalISTAarelearnedinsteadofusingpre-computed
ones[21].Mark
etal
.extendapproximatemessagepass-
ing(AMP)algorithmstoso-calledLearnedAMPnetworks
forsolvingsparselinearinverseproblems[25].Relying
onLISTA,somesparse-codingbasednetworksforimage
super-resolutionanddeblockingareproposed[40,42,41].
Forimagedenoisinganddeconvolution,SchmidtandRoth
proposetolearnthelinearandshrinkagefunctions
undertheframeworkofhalf-quadraticoptimization[23].
Chen
etal
.proposeatrainablereactiondiffusionmodelby
learningseveralparameterizedlinearand
functionsforimagedenoisinganddeblocking[22].Inthe
contextofCSforsparsesignals,KamilovandMansourpro-
posetolearntheoptimalthresholdingfunctionsforISTA
basedonaB-splinedecomposition[20].
Recently,Yang
etal
.proposeaso-calledADMM-Net
architecturebyreformulatingADMMforCSmagneticres-
onanceimaging(CS-MRI)usingdeepnetworks[24].Al-
thoughbothADMM-NetandourproposedISTA-Nethave
similarinspirations,theyarequitedifferent.Infact,there
aretwomaindifferencesbetweenbothmethods.First,
ADMM-NetisdesignedanddevelopedforCS-
MRIbasedonADMM,whileourISTA-Netismuchmore
general,sinceitworkswellforbothgeneralCSandCS-
MRIbasedonISTA.Second,ADMM-Netonlyutilizessev-
erallinearwhileISTA-Netgoesbeyondthattoadopt
nonlineartransformstomoreeffectivelysparsifynatural
imagesanddevelopsanefstrategyforsolvingtheir
proximalmappingproblems.Thedetailedcomparisonwith
ADMM-NetforCS-MRIcanbefoundinSection5.3.
Inanutshell,theproposedISTA-Netcanbeessentially
viewedasaextensionofLISTA[21],from
thesparsecodingproblemtogeneralCSreconstruction.
Comparedwithtraditionaloptimization-basedCSmethods,
ISTA-Netisabletolearnitsoptimalparameters,
i.e
.thresh-
olds,stepsizesaswellasnonlineartransforms,without
hand-craftedsettings.Inaddition,ISTA-Nethasthesame
computationalcomplexityasseveraliterationsoftraditional
ISTA,whichismorethan100timesfasterthanexisting
methodsofthiscategory.Comparedwithnetwork-based
CSmethods,ISTA-Netborrowsinsightsfromtraditional
optimizationmethodstoallowforinterpretabilityinitsnet-
workdesignanditutilizesthestructuraldiversityoriginat-
ingfromtheCSdomain.Extensiveexperimentsdemon-
stratethatISTA-Netoutperformstheexisting
optimization-basedandnetwork-basedCSmethods,even
whencomparedagainstmethodsthataredesignedforaspe-
domain(
e.g
.CS-MRI).
3.ProposedISTA-NetforCompressiveSensing
Inthissection,wereviewtraditionalISTA
optimizationforimageCSreconstruction,andthenelabo-
rateonthedesignofourproposedISTA-Net.
3.1.TraditionalISTAforCS
Theiterativeshrinkage-thresholdingalgorithm(ISTA)
isapopularorderproximalmethod,whichiswell
suitedforsolvingmanylarge-scalelinearinverseproblems.
,ISTAsolvestheCSreconstructionproblemin
Eq.(1)byiteratingbetweenthefollowingupdatesteps:
r
(
k
)
=
x
(
k

1)

ˆ

>
(

(
k

1)

y
)
;
(2)
x
(
k
)
=argmin
x
1
2
k
x

r
(
k
)
k
2
2
+

k

k
1
:
(3)
Here,
k
denotestheISTAiterationindex,and
ˆ
isthestep
size.Eq.(2)istrivial,whileEq.(3)isactuallyaspecial
caseoftheso-calledproximalmapping,
i.e
.
prox

(
r
(
k
)
)
,
when
˚
(
x
)=
k

k
1
.Formally,theproximalmappingof
regularizer
˚
denotedby
prox

(
r
)
isas
prox

(
r
)=argmin
x
1
2
jj
x

r
jj
2
2
+

(
x
)
:
(4)
Solving
prox

(
r
)
inanefandeffectivewayis
criticalforISTA[50],aswellasforotheroptimization
methods,suchasADMM[13]andAMP[9].Forexam-
ple,when
˚
(
x
)=
k
Wx
k
1
(
W
iswavelettransformma-
trix),wehave
prox

(
r
)=
W
>
soft
(
Wr
;
)
duetothe
orthogonalityof
W
.However,itremainsnon-trivialtoob-
tain
x
(
k
)
inEq.(3)foramorecomplexnon-orthogonal(or
evennonlinear)transform

.Inaddition,ISTAusuallyre-
quiresmanyiterationstoobtainasatisfactoryresult,suffer-
ingfromextensivecomputation.Theoptimaltransform

andalltheparameterssuchas
ˆ
and

are(do
notchangewith
k
),andverychallengingtotuneapriori.
3.2.ISTFramework
BytakingfulladvantageofthemeritsofISTA-basedand
network-basedCSmethods,thebasicideaofISTA-Netis
tomapthepreviousISTAupdatestepstoadeepnetwork
architecturethatconsistsofaednumberofphases,each
ofwhichcorrespondstooneiterationintraditionalISTA.
Inordertoimprovereconstructionperformanceandin-
creasenetworkcapacityandinsteadofthehand-crafted
transform

inEq.(1),ISTA-Netadoptsageneralnon-
lineartransformfunctiontosparsifynaturalimages,de-
notedby
F
(

)
,whoseparametersarelearnable.Inpartic-
ularandinspiredbythepowerfulrepresentationpowerof
CNN[14]anditsuniversalapproximationproperty[36],
weproposetodesign
F
(

)
asacombinationoftwolinear
convolutionaloperators(withoutbiasterms)separatedbya
linearunit(ReLU).AsillustratedinFigure2,the
convolutionaloperatorin
F
(

)
correspondsto
N
f

ters(eachofsize
3

3
inourexperiments)andthesecond
convolutionaloperatorcorrespondstoanothersetof
N
f

ters(eachofsize
3

3

N
f
inourexperiments).Inour
implementation,weset
N
f
=32
bydefault.Obviously,
F
(

)
canalsobeequivalentlyformulatedinmatrixformas
F
(
x
)=
B
ReLU
(
Ax
)
,where
A
and
B
correspondtothe
abovetwoconvolutionaloperators,respectively.Withits
learnableandnonlinearcharacteristics,
F
(

)
isexpectedto
beabletoachievearicherrepresentationfornaturalimages.
Replacing

inEq.(1)with
F
(

)
,weobtainthefollow-
ingsparsity-inducingregularizationproblemwithanonlin-
eartransform:
min
x
1
2
k


y
k
2
2
+

kF
(
x
)
k
1
:
(5)
BysolvingEq.(5)usingISTA,Eq.(2)isunchanged
whileEq.(3)becomes
x
(
k
)
=argmin
x
1
2
k
x

r
(
k
)
k
2
2
+

kF
(
x
)
k
1
:
(6)
Inthefollowing,wearguethattheabovetwosteps
Eq.(2)andEq.(6)inthe
k
-
th
ISTAiterationbothadmit
efsolutions,andwecastthemintotwoseparatemod-
ulesinthe
k
-
th
phaseofISTA-Net,namelythe
r
(
k
)
module
andthe
x
(
k
)
module
,asillustratedinFigure2.

r
(
k
)
Module:
ItcorrespondstoEq.(2)andisusedto
generatetheimmediatereconstructionresult
r
(
k
)
.Notethat

>
(

(
k

1)

y
)
isessentiallythegradientofthedata-
term
1
2
k


y
k
2
2
,computedat
x
(
k

1)
.Topreserve
theISTAstructurewhileincreasingnetworkxibility,we
allowthestepsize
ˆ
tovaryacrossiterations(whileitis
edintraditionalISTA),sotheoutputofthismodulewith
input
x
(
k

1)
isas:
r
(
k
)
=
x
(
k

1)

ˆ
(
k
)

>
(

(
k

1)

y
)
:
(7)

x
(
k
)
Module:
Itaimstocompute
x
(
k
)
accordingto
Eq.(6)withinput
r
(
k
)
.NotethatEq.(6)isactuallythe
proximalmapping
prox

F
(
r
(
k
)
)
associatedwiththenon-
lineartransform
F
(

)
.Inthispaper,weproposetosolve
prox

F
(
r
(
k
)
)
efintwosteps,whichisalsooneof
ourmaincontributions.
First,notethat
r
(
k
)
istheimmediatereconstructionre-
sultof
x
(
k
)
atthe
k
-
th
iteration.Inthecontextofimage
inverseproblems,onegeneralandreasonableassumptionis
thateachelementof
(
x
(
k
)

r
(
k
)
)
followsanindependent
normaldistributionwithcommonzeromeanandvariance
˙
2
[19].Here,wealsomakethisassumption,andthenwe
furtherprovethefollowingtheorem:
Theorem1
Let
X
1
;:::;X
n
beindependentnormalrandom
variableswithcommonzeromeanandvariance
˙
2
.If
~
X
=[
X
1
;:::;X
n
]
>
andgivenanymatrices
A
2
R
m

n
and
B
2
R
s

m
,anewrandomvariable
~
Y
=
B
ReLU
(
A
~
X
)=
B
max(
0
;
A
~
X
)
.Then,
E
[
k
~
Y

E
[
~
Y
]
k
2
2
]
and
E
[
k
~
X

E
[
~
X
]
k
2
2
]
arelinearlyrelated,
i.e
.
E
[
k
~
Y

E
[
~
Y
]
k
2
2
]=

E
[
k
~
X

E
[
~
X
]
k
2
2
]
,where

isonlyafunction
of
A
and
B
.(Pleaserefertothe
supplementarymaterial
fortheproofandmoredetails.)
Theorem
1canbeeasilyextendedtoanormaldistribu-
tion.Supposethat
r
(
k
)
and
F
(
r
(
k
)
)
arethemeanvaluesof
x
and
F
(
x
)
respectively,thenwecanmakethefollowing
Figure3.Illustrationofthe
k
-
th
phaseoftheproposedISTA-Net
+
.
D
(
k
)
;
G
(
k
)
;
H
(
k
)
;
e
H
(
k
)
arelearnablelinearconvolutionaloperators.
approximationbasedon
Theorem
1:
kF
(
x
)
F
(
r
(
k
)
)
k
2
2
ˇ

k
x

r
(
k
)
k
2
2
;
(8)
where

isascalarthatisonlyrelatedtotheparametersof
F
(

)
.ByincorporatingthislinearrelationshipintoEq.(6),
weobtainthefollowingoptimization:
x
(
k
)
=argmin
x
1
2
kF
(
x
)
F
(
r
(
k
)
)
k
2
2
+

kF
(
x
)
k
1
;
(9)
where

and

aremergedintooneparameter

,
i.e
.

=

.
Therefore,wegetaclosed-formversionof
F
(
x
(
k
)
)
:
F
(
x
(
k
)
)=
soft
(
F
(
r
(
k
)
)
;
)
:
(10)
Second,motivatedbytheinvertiblecharacteristicsofthe
wavelettransformthatleadstoaclosed-formsolutionfor
Eq.(3),weintroducetheleftinverseof
F
(

)
,denotedby
e
F
(

)
suchthat
e
FF
=
I
,where
I
istheidentityoperator.
,
e
F
(

)
isdesignedtoexhibitastructuresymmet-
rictothatof
F
(

)
,soitisalsomodeledastwolinearconvo-
lutionaloperatorsseparatedbyaReLUoperator,asshown
inFigure2.Because
F
(

)
and
e
F
(

)
arebothlearnable,we
willenforcethe
symmetryconstraint
e
FF
=
I
byincor-
poratingitintothelossfunctionduringnetworktraining.
Therefore,
x
(
k
)
canbeefcomputedinclosed-form
as:
x
(
k
)
=
e
F
(
soft
(
F
(
r
(
k
)
)
;
))
:
(11)
Itisworthemphasizingthat

,asashrinkagethresh-
old,isalearnableparameterinthismodule.Similarly,to
increasenetworkcapacity,wedonotconstrainthat
F
(

)
,
e
F
(

)
,and

bethesameateachphase.Thatis,eachphase
ofISTA-Nethasitsown
fF
(
k
)
(

)
;
e
F
(
k
)
(

)
;
(
k
)
g
,asillus-
tratedinFigure2.Therefore,withallthelearnableparam-
eters,theoutput
x
(
k
)
inthismoduleshouldbeupdatedas:
x
(
k
)
=
e
F
(
k
)
(
soft
(
F
(
k
)
(
r
(
k
)
)
;
(
k
)
))
:
(12)
Figure2clearlyillustrateshowEq.(6)withtheclosed-
formsolutioninEq.(12)ismappedintoadeepnetworkin
the
k
-
th
phaseofISTA-Net.
ParametersinISTA-Net:
Eachmoduleineachphaseof
ISTA-Netstrictlycorrespondstotheupdatesstepsinan
ISTAiteration.ThelearnableparametersetinISTA-Net,
denotedby

,includesthestepsize
ˆ
(
k
)
inthe
r
(
k
)
mod-
ule,theparametersoftheforwardandbackwardtransforms
F
(
k
)
(

)
and
e
F
(
k
)
(

)
,andtheshrinkagethreshold

(
k
)
inthe
x
(
k
)
module.Assuch,

=
f
ˆ
(
k
)
;
(
k
)
;
F
(
k
)
;
e
F
(
k
)
g
N
p
k
=1
,
where
N
p
isthetotalnumberofISTA-Netphases.Allthese
parameterswillbelearnedasneuralnetworkparameters.
Initialization:
LiketraditionalISTA,ISTA-Netalsore-
quiresaninitializationdenotedby
x
(0)
inFigure2.Instead
ofrandomvalues,weproposetodirectlyusealinearmap-
pingtocomputetheinitialization.,giventhe
trainingdatapairsthatincludetheimageblocksandtheir
correspondingCSmeasurements,
i.e
.
f
(
y
i
;
x
i
)
g
N
b
i
=1
with
x
i
2
R
N
;
y
i
2
R
M
,thelinearmappingmatrix,denotedby
Q
init
,canbecomputedbysolvingaleastsquaresproblem:
Q
init
=argmin
Q
k
QY

X
k
2
F
=
XY
>
(
YY
>
)

1
.Here,
X
=[
x
1
;:::;
x
N
b
]
,and
Y
=[
y
1
;:::;
y
N
b
]
.Hence,given
anyinputCSmeasurement
y
,itscorrespondingISTA-Net
initialization
x
(0)
iscomputedas:
x
(0)
=
Q
init
y
:
3.3.LossFunctionDesign
Giventhetrainingdatapairs
f
(
y
i
;
x
i
)
g
N
b
i
=1
,ISTA-Net
takestheCSmeasurement
y
i
asinputandgenerates
thereconstructionresult,denotedby
x
(
N
p
)
i
,asoutput.We
seektoreducethediscrepancybetween
x
i
and
x
(
N
p
)
i
while
satisfyingthesymmetryconstraint
e
F
(
k
)
F
(
k
)
=
I8
k
=
1
;:::;N
p
.Therefore,wedesigntheend-to-endlossfunc-
tionforISTA-Netasfollows:
L
total
(

)=
L
discrepancy
+

L
constraint
;
(13)
with:
(
L
discrepancy
=
1
N
b
N
P
N
b
i
=1
k
x
(
N
p
)
i

x
i
k
2
2
L
constraint
=
1
N
b
N
P
N
b
i
=1
P
N
p
k
=1
k
e
F
(
k
)
(
F
(
k
)
(
x
i
))

x
i
k
2
2
;
where
N
p
,
N
b
,
N
,and

arethetotalnumberofISTA-Net
phases,thetotalnumberoftrainingblocks,thesizeofeach
block
x
i
,andtheregularizationparameter,respectively.In
ourexperiments,

issetto0.01.
4.EnhancedVersion:ISTA-Net
+
Motivatedbythefactthattheresidualsofnaturalimages
andvideosaremorecompressible[37,38],anenhanced
version,dubbedISTA-Net
+
,isderivedfromISTA-Netto
furtherimproveCSperformance.StartingfromEq.(6),
weassumethat
x
(
k
)
=
r
(
k
)
+
w
(
k
)
+
e
(
k
)
,where
e
(
k
)
standsforsomenoiseand
w
(
k
)
representssomemissing
high-frequencycomponentin
r
(
k
)
,whichcanbeextracted
byalinearoperator
R
(

)
from
x
(
k
)
,
i.e
.
w
(
k
)
=
R
(
x
(
k
)
)
.
Furthermore,
R
(

)
isas
R
=
GD
,where
D
corre-
spondsto
N
f
(eachofsize
3

3
inourexperiments)
Figure4.PSNRcomparisonbetweenISTA-NetandISTA-Net
+
withvariousnumbersofphasesandepochsduringtraining.
and
G
correspondsto
1
(withsize
3

3

N
f
).Bymod-
eling
F
=
HD
,where
H
consistsoftwolinearconvo-
lutionaloperatorsandoneReLU,asillustratedinFigure3,
wecanreplace
F
inEq.(9)with
HD
toobtain:
min
x
1
2
jjH
(
D
(
x
))
H
(
D
(
r
(
k
)
))
jj
2
2
+

jjH
(
D
(
x
))
jj
1
:
(14)
ByexploitingtheapproximationusedinEq.(9)andfol-
lowingthesamestrategyasinISTA-Net,wetheleft
inverseof
H
as
e
H
,whichhasastructuresymmetrictothat
of
H
andthesymmetryconstraint
e
HH
=
I
.
Thus,theclosedformoftheISTA-Net
+
updatefor
x
(
k
)
is:
x
(
k
)
=
r
(
k
)
+
G
(
e
H
(
soft
(
H
(
D
(
r
(
k
)
))
;
)))
:
(15)
SimilartoISTA-Net,eachphaseofISTA-Net
+
also
hasitsownlearnableparameters,andthe
k
-
th
phase
ofISTA-Net
+
isillustratedinFigure3.Hence,the
learnableparameterset

+
ofISTA-Net
+
is

+
=
f
ˆ
(
k
)
;
(
k
)
;
D
(
k
)
;
G
(
k
)
;
H
(
k
)
;
e
H
(
k
)
g
N
p
k
=1
.Thelossfunction
ofISTA-Net
+
isanalogouslydesignedbyincorporatingthe
constraints
e
H
(
k
)
H
(
k
)
=
I
intoEq.(13).
5.ExperimentalResults
Forfaircomparison,weusethesamesetof91im-
agesasin[11]fortraining.Followingcommonpractices
inpreviousCSwork,wegeneratethetrainingdatapairs
f
(
y
i
;
x
i
)
g
N
b
i
=1
byextractingtheluminancecomponent
of88,912randomlycroppedimageblocks(eachofsize
33

33),
i.e
.
N
b
=88,912and
N
=1,089.Then,foragivenCS
ratio,thecorrespondingmeasurementmatrix

2
R
M

N
isconstructedbygeneratingarandomGaussianmatrixand
thenorthogonalizingitsrows,
i.e
.

>
=
I
,where
I
is
theidentitymatrix.Applying
y
i
=

i
yieldstheset
ofCSmeasurements,where
x
i
isthevectorizedversion
ofanimageblock.WeuseTensorFlow[39]toimple-
mentandtraintheISTA-NetsseparatelyforarangeofCS
ratios
f
1%
;
4%
;
10%
;
25%
;
30%
;
40%
;
50%
g
.Totrainthe
networks,weuseAdamoptimization[15]withalearning
rateof0.0001(200epochs),andabatchsizeof64.Allthe
experimentsareperformedonaworkstationwithIntelCore
i7-6820CPUandGTX1060GPU.TrainingISTA-Netswith
phasenumber
N
p
=
9
roughlytakes10hours.Fortesting,we
utilizetwowidelyusedbenchmarkdatasets:Set11[11]and
BSD68[34],whichhave11and68grayimages,respec-
tively.Thereconstructionresultsarereportedastheaverage
PeakSignal-to-NoiseRatio(PSNR)overthetestimages.
5.1.ISTvs.IST
+
TodemonstratethesuperiorityofISTA-Net
+
overISTA-
Net,wecomparethemintwoaspects:performanceand
convergence.Figure4(top)showstheaveragePSNR
curvesforthetestingset(Set11)withrespecttodifferent
phasenumbers,whentheCSratiois25%.Weobserve
thatbothPSNRcurvesincreaseasphasenumber
N
p
in-
creases;however,thecurvesarealmostwhen
N
p

9
.
Thus,consideringthetradeoffbetweennetworkcomplexity
andreconstructionperformance,wesetthedefaultphase
number
N
p
=
9
forbothISTA-NetandISTA-Net
+
intherest
oftheexperiments.Clearly,ISTA-Net
+
achievesabout1
dBgainoverISTA-Netwhen
N
p
=
9
.Furthermore,Figure4
(bottom)plotstheaveragePSNRcurvesforSet11withre-
specttodifferentnumbersofepochsduringtraining,when
theCSratiois25%and
N
p
=
9
.BothISTA-Netsgethigher
PSNRwhentrainedforalargernumberofepochs,but
ISTA-Net
+
achievesfastertrainingconvergenceandbetter
reconstructionperformanceonthetestset(Set11).Dueto
limitedspace,pleasereferto
supplementarymaterial
for
thethatarelearnedbyISTA-Nets.
WeattributethesuperiorityofISTA-Net
+
overISTA-
Nettotwofactors.First,ISTA-Net
+
explicitly
theimagesintheresidualdomain,leadingtoasparserrep-
resentationascomparedtoISTA-Net.Second,theskipcon-
nectionsintroducedbyISTA-Net
+
coincidewiththecentral
ideaofthepopularResNet[30]architecture,whichfacili-
tatesthetrainingofdeepernetworks.
5.2.ComparisonwithMethods
WecompareourproposedISTA-NetandISTA-Net
+
withverecentandstate-of-the-artimageCSmethods,
namelyTVAL3[7],D-AMP[9],IRCNN[31],SDA[10],
andReconNet[11]
1
.Thethreeareoptimization-based
methods,whilethelasttwoarenetwork-basedmethods.
TheaveragePSNRreconstructionperformanceonSet11
withrespecttosevenCSratiosaresummarizedinTable1.
Forfaircomparisonandfollowingtheevaluationstrategy
of[11],allthecompetingmethodsreconstructeachimage
blockfromitsCSmeasurementindependently.FromTa-
ble1,weobservethatSDAandReconNetworkbetterat
1
Weusethesourcecodemadepubliclyavailablebytheauthorsof
TVAL3[7],D-AMP[9],IRCNN[31],andReconNet[11]andimplement
SDA[10]ourselves,sinceitssourcecodeisnotavailable.
Figure5.ComparisonofsevenCSreconstructionmethods(includingourISTA-NetandISTA-Net
+
),whenappliedtothe

image
inSet11(CSratiois25%).
Table1.AveragePSNR(dB)performancecomparisonsonSet11withdifferentCSratios.Thebestperformanceislabeledinboldandthe
secondbestisunderlined.Notethatthelasttwocolumnsisarun-timeanalysisofallthecompetingmethods,showingtheaveragetimeto
reconstructa
256

256
imageandthecorrespondingframes-per-second(FPS).
Algorithm
CSRatio
Time
CPU/GPU
FPS
CPU/GPU
50%
40%
30%
25%
10%
4%
1%
TVAL3[7]
33.55
31.46
29.23
27.92
22.99
18.75
16.43
3.135s/ŠŠŒ
0.32/ŠŒ
D-AMP[9]
35.92
33.56
30.39
28.46
22.64
18.40
5.21
51.21s/ŠŠŒ
0.02/ŠŒ
IRCNN[31]
36.23
34.06
31.18
30.07
24.02
17.56
7.70
ŠŠŒ/68.42s
Š-/0.015
SDA[10]
28.95
27.79
26.63
25.34
22.65
20.12
17.29
ŠŠ/0.0032s
Š-/312.5
ReconNet[11]
31.50
30.58
28.74
25.60
24.28
20.63
17.27
ŠŠŒ/0.016s
ŠŠ/62.5
ISTA-Net
37.43
35.36
32.91
31.53
25.80
21.23
17.30
0.923s/0.039s
1.08/25.6
ISTA-Net
+
38.07
36.06
33.82
32.57
26.64
21.31
17.34
1.375s/0.047s
0.73/21.3
extremelylowCSratiosof1%and4%,whiletraditional
optimization-basedmethodsperformbetterathigherCSra-
tios.However,ISTA-NetandISTA-Net
+
outperformallthe
existingmethodsbyalargemarginacrossalltheCSratios.
Thisclearlydemonstratesthattheycombinethemeritsof
bothcategoriesofCSmethods.Asexpected,ISTA-Net
+
performsbetterthanISTA-Net.Thelasttwocolumnsin
Table1isarun-timeanalysisofallthecompetingmethods.
TheseresultsindicatethattheproposedISTA-Netsproduce
consistentlybetterreconstructionresults,whileremaining
computationallyattractive.InFigure5,weshowtherecon-
structionsofallsevenmethodsofthe

imagewhen
theCSratiois25%.TheproposedISTA-Net
+
isableto
reconstructmoredetailsandsharperedges.
TofurthervalidatethegeneralizabilityofourISTA-Nets,
wealsocomparethemtonetwork-basedmethodsSDAand
ReconNetonthelargerBSD68dataset.AsshowninTable
2,ISTA-Net
+
achievesthebestperformance,whileISTA-
NetregisterssecondbestamongallveCSratios.ISTA-
NetsoutperformSDAandReconNet,especiallyathigher
CSratios.Inaddition,itisworthemphasizingthatapre-
trainedISTA-NetorISTA-Net
+
usingoneCSmeasurement
matrix

canbedirectlyusedforanynewmeasurement
matrixwiththesameCSratioas

,avoidingtrainingnew
networkfromscratch.Theonlydifferenceisthatweneedto
recalculatetheinitializationmatrix
Q
init
forthenewmea-
Table2.AveragePSNR(dB)performancecomparisonofvarious
network-basedalgorithmsontheBSD68dataset.
Algorithm
CSRatio
50%
40%
30%
10%
4%
SDA[10]
28.35
27.41
26.38
23.12
21.32
ReconNet[11]
29.86
29.08
27.53
24.15
21.66
ISTA-Net
33.60
31.85
29.93
25.02
22.12
ISTA-Net
+
34.01
32.21
30.34
25.33
22.17
surementmatrix,whichusuallytakeslessthan1second.
Pleasereferto
supplementarymaterial
formoreresults.
5.3.Comparisonwithfor
TodemonstratethegeneralityofISTA-Net
+
,wedirectly
extendISTA-Net
+
totheproblemofCSMRIre-
construction,whichaimsatreconstructingMRimagesfrom
asmallnumberofunder-sampleddatain
k
-space.Inthis
applicationandfollowingcommonpractices,wesetthe
samplingmatrix

inEq.(1)to

=
PF
,where
P
isan
under-samplingmatrixand
F
isthediscreteFouriertrans-
form.Inthiscase,wecompareagainstADMM-Net[24]
2
,
whichisanetwork-basedmethodinspiredbyADMMand
designedfortheCS-MRIdomain.Itisworth-
whiletonotethatADMM-Netcannotbetriviallyextended
tootherCSdomains,sinceitimposesastructure
tothesamplingmatrix

.Utilizingthesametrainingand
2
https://github.com/yangyan92/Deep-ADMM-Net
Figure6.PSNR(dB)comparisonbetweentwoversionsofISTA-
Net
+
:withandwithoutReLU(when
N
f
=
8
and
N
f
=
16
).
Table3.AveragePSNR(dB)comparisonbetweenADMM-Net
[24]andourproposedISTA-NetsforCS-MRI.
Algorithm
CSRatio
Time
20%
30%
40%
50%
GPU
ADMM-Net
37.17
39.84
41.56
43.00
0.9535s
ISTA-Net
38.30
40.52
42.12
43.60
0.1246s
ISTA-Net
+
38.73
40.89
42.52
44.09
0.1437s
Table4.AveragePSNR(dB)performancewithdifferentshared
typesofISTA-Net
+
.
SharedType
NumberofParameters
PSNR
Shared
ˆ
(
k
)
;
(
k
)
;
T
(
k
)
(37440+1+1)=
37,442
31.53
Shared
ˆ
(
k
)
;
T
(
k
)
(37440+1)+1*9=37,450
32.28
Shared

(
k
)
;
T
(
k
)
(37440+1)+1*9=37,450
32.08
Shared
T
(
k
)
37440+(1+1)*9=37,458
32.36
Unshared(default)
(37440+1+1)*9=336,978
32.57
testingbrainmedicalimagesasADMM-Net,theCS-MRI
resultsofISTA-Netswith
N
p
=11phasesaresummarized
inTable3forCSratiosof20%,30%,40%and50%.It
isclearthatISTA-NetsoutperformADMM-Netnotonlyin
termsofreconstructionbutalsointermsofruntime.
5.4.AblationStudiesandDiscussions
Thissectionmainlyfocusesonthenonlinearityandx-
ibilityoftheproposedISTA-Nets.Inwhatfollows,wean-
alyzeISTA-Net
+
with
N
p
=
9
phases.

Linearvs.NonlinearTransforms:
Thenonlinearityof
ISTA-Net
+
isintroducedbytheReLUoperatorin
H
(
k
)
and
e
H
(
k
)
,asshowninFigure3.Toevaluatetheimpactofthe
nonlinearity,wetrainISTA-Net
+
modelswithReLU(non-
lineartransforms)andwithoutReLU(lineartransforms).
Figure6plotstheaveragePSNRcurvesforeachISTA-
Net
+
variantonSet11throughouttraining.Notethatpa-
rameter
N
f
,thenumberoffeaturemapsin
H
(
k
)
and
e
H
(
k
)
,
issetto
8
or
16
inthisexperiment.Itisclearthatthe
nonlinearityintroducedbytheReLUiscriticalforhigh
delityCSreconstructionperformance.Inaddition,when
N
f
>
30
,experimentsindicatethatISTA-Net
+
without
ReLUislessstableintrainingthanISTA-Net
+
withReLU,whichstillperformswell.Weconcludethatthe
nonlinearityplaysanimportantroleinfacilitatingsatisfac-
tionofthesymmetryconstraint,improvingnetworkstabil-
ity,andlearningasuitabletransformpossibleforCS.

Sharedvs.Unshared:
Asdescribedpreviously,each
phaseofISTA-Net
+
(
N
f
=
32
)hasthreetypesofparame-
terswiththeirdimensionalitylistedinparentheses:step
size
ˆ
(
k
)
(
1
),threshold

(
k
)
(
1
),andtransform
T
(
k
)
=
fD
(
k
)
;
G
(
k
)
;
H
(
k
)
;
e
H
(
k
)
g
(
32

3

3+32

3

3

32

2+32

3

3

32

2+1

3

3

32=37440
).The
xibilityofISTA-Net
+
indicatesthatthesametypeofpa-
rametersindifferentphasesdonotneedtobethesame.
Todemonstratetheimpactofthisxibility,wetrainsev-
eralvariantsofISTA-Net
+
,wherewevarytheparame-
tersthataresharedamongthephases.Asummaryofthe
averagePSNRresultsonSet11ata25%CSratioisre-
portedinTable4.Obviously,thedefault
unshared
ISTA-
Net
+
(mostxiblewithlargestnumberofparameters)
achievesthebestperformance,whilethevariantofISTA-
Net
+
thatsharesallparameters
(
ˆ
(
k
)
;
(
k
)
;
T
(
k
)
)
inallits
phases(leastxiblewithsmallestnumberofparameters)
obtainstheworstperformance.Whenonly
(
ˆ
(
k
)
;
T
(
k
)
)
or
(

(
k
)
;
T
(
k
)
)
areshared,theseISTA-Net
+
variantsregister
0.75dBand0.55dBgainsoverhevariantwithallshared
parameters.Interestingly,theISTA-Net
+
variantwithonly
sharedtransforms
T
(
k
)
obtainsverycompetitivePSNRre-
sultscomparedtotheunsharedvariant.Thisindicatesthat
furthercompressioninISTA-Net
+
parametersispossible,
withlimitedaffectonreconstructionperformance.
6.ConclusionandFutureWork
InspiredbytheIterativeShrinkage-ThresholdingAlgo-
rithm(ISTA),weproposeanovelstructureddeepnetwork
forimagecompressivesensing(CS)reconstruction,dubbed
ISTA-Net,aswellas,itsenhancedversionISTA-Net
+
.The
proposedISTA-Netshaveinterpretability,and
makefulluseofthemeritsofbothoptimization-basedand
network-basedCSmethods.AlltheparametersinISTA-
Netsarediscriminatelylearnedend-to-end.Extensiveex-
perimentsshowthatISTA-Netsgreatlyimproveuponthe
resultsofstate-of-the-artCSmethods,whilemaintaininga
fastruntime.Sincethedevelopedstrategytosolvetheprox-
imalmappingproblemassociatedtoanonlinearsparsifying
transformisquitegeneralandefonedirectionofin-
terestistodesigndeepnetworksbasedonotheroptimiza-
tioninspirations,suchasFISTA[12].Theotherdirection
ofourfutureworkistoextendISTA-Netsforotherimage
inverseproblems,suchasdeconvolutionandinpainting.
Acknowledgments.
ThisworkwassupportedbytheKing
AbdullahUniversityofScienceandTechnology(KAUST)
OfofSponsoredResearch.Theauthorwouldlike
tosincerelythankAdelBibiforhishelpfuldiscussion.
References
[1]
Duarte,M.F.,Davenport,M.A.,Takbar,D.,Laska,J.
N.,Sun,T.,Kelly,K.F.,andBaraniuk,R.G.(2008).
Single-pixelimagingviacompressivesampling.
IEEE
SignalProcessingMagazine
,25(2),83-91.
[2]
Candes,E.JandTao,T.(2006).Near-optimalsignal
recoveryfromrandomprojections:Universalencoding
strategies?.
IEEETransactionsonInformationTheory
,
52(12),5406-5425.
[3]
Lustig,M.,Donoho,D.,Pauly,J.M.(2007).Sparse
MRI:Theapplicationofcompressedsensingforrapid
MRimaging.
MagneticResonanceinMedicine
,58(6),
1182-1195.
[4]
Mun,S.,Fowler,J.E.(2009).Blockcompressedsens-
ingofimagesusingdirectionaltransforms.
IEEEICIP
.
[5]
Kim,Y.,Nadar,M.S.,Bilgin,A.(2010).Compressed
sensingusingaGaussianscalemixturesmodelin
waveletdomain.
IEEEICIP
.
[6]
Zhang,J.,Zhao,D.,Zhao,C.,Xiong,R.,Ma,S.,
Gao,W.(2012).Imagecompressivesensingrecovery
viacollaborativesparsity.
IEEEJournalonEmerging
andSelectedTopicsinCircuitsandSystems
,2(3),380-
391.
[7]
Li,C.,Yin,W.,Jiang,H.,Zhang,Y.(2013).Anef
cientaugmentedLagrangianmethodwithapplications
tototalvariationminimization.
ComputationalOpti-
mizationandApplications
,56(3),507-530.
[8]
Dong,W.,Shi,G.,Li,X.,Ma,Y.,Huang,F.(2014).
Compressivesensingvianonlocallow-rankregulariza-
tion.
IEEETransactionsonImageProcessing
,23(8),
3618-3632.
[9]
Metzler,C.A.,Maleki,A.,Baraniuk,R.G.(2016).
Fromdenoisingtocompressedsensing.
IEEETransac-
tionsonInformationTheory
,62(9),5117-5144.
[10]
Mousavi,A.,Patel,A.B.,Baraniuk,R.G.(2015,
September).Adeeplearningapproachtostructuredsig-
nalrecovery.
AnnualAllertonConferenceonCommuni-
cation,Control,andComputing
,pp.1336-1343.
[11]
Kulkarni,K.,Lohit,S.,Turaga,P.,Kerviche,R.,
Ashok,A.(2016).ReconNet:Non-iterativereconstruc-
tionofimagesfromcompressivelysensedmeasure-
ments.
CVPR
,pp.449-458.
[12]
Beck,A.,Teboulle,M.(2009).Afastiterative
shrinkage-thresholdingalgorithmforlinearinverse
problems.
SIAMjournalonImagingSciences
,2(1),
183-202.
[13]
Afonso,M.V.,Bioucas-Dias,J.M.,Figueiredo,M.
A.(2011).AnaugmentedLagrangianapproachtothe
constrainedoptimizationformulationofimagingin-
verseproblems.
IEEETransactionsonImageProcess-
ing
,20(3),681-695.
[14]
Dong,C.,Loy,C.C.,He,K.,Tang,X.(2014).Learn-
ingadeepconvolutionalnetworkforimagesuper-
resolution.
ECCV
.
[15]
Kingma,D.P.,Ba,J.(2014).Adam:Amethod
forstochasticoptimization.
arXivpreprint
,
arXiv:1412.6980.
[16]
He,L.,Carin,L.(2009).Exploitingstructurein
wavelet-basedBayesiancompressivesensing.IEEE
TransactionsonSignalProcessing,57(9),3488-3497.
[17]
Krizhevsky,A.,Sutskever,I.,Hinton,G.E.(2012).
Imagenetwithdeepconvolutionalneural
networks.
NIPS
,pp.1097-1105.
[18]
Long,J.,Shelhamer,E.,Darrell,T.(2015).Fullycon-
volutionalnetworksforsemanticsegmentation.
CVPR
,
pp.3431-3440.
[19]
Zhang,J.,Zhao,D.,Gao,W.(2014).Group-
basedsparserepresentationforimagerestoration.IEEE
TransactionsonImageProcessing,23(8),3336-3351.
[20]
Kamilov,U.S.,Mansour,H.(2016).Learningopti-
malnonlinearitiesforiterativethresholdingalgorithms.
IEEESignalProcessingLetters
,23(5),747-751.
[21]
Gregor,K.,LeCun,Y.(2010).Learningfastapproxi-
mationsofsparsecoding.
ICML
,pp.399-406.
[22]
Chen,Y.,Yu,W.,Pock,T.(2015).Onlearningopti-
mizedreactiondiffusionprocessesforeffectiveimage
restoration.
CVPR
,pp.5261-5269.
[23]
Schmidt,U.,Roth,S.(2014).Shrinkageforef-
fectiveimagerestoration.
CVPR
,pp.2774-2781.
[24]
Yang,Y.,Sun,J.,Li,H.,Xu,Z.(2016).DeepADMM-
NetforcompressivesensingMRI.
NIPS
.
[25]
Borgerding,M.,Schniter,P.,Rangan,S.(2017).
AMP-InspiredDeepNetworksforSparseLinearIn-
verseProblems.
IEEETransactionsonSignalProcess-
ing
,65(16),4293-4308.
[26]
Rousset,F.,Ducros,N.,Farina,A.,Valentini,G.,
DAndrea,C.,Peyrin,F.(2017).Adaptivebasisscan
bywaveletpredictionforsingle-pixelimaging.
IEEE
TransactionsonComputationalImaging
,3(1),36-46.
[27]
Sharma,S.K.,Lagunas,E.,Chatzinotas,S.,Ottersten,
B.(2016).Applicationofcompressivesensingincog-
nitiveradiocommunications:Asurvey.
IEEECommu-
nicationSurveys&Tutorials
,18(3),1838-1860.
[28]
Zhang,Z.,Jung,T.P.,Makeig,S.,Rao,B.D.(2013).
Compressedsensingforenergy-efwirelesstele-
monitoringofnoninvasivefetalECGviablocksparse
Bayesianlearning.
IEEETransactionsonBiomedical
Engineering
,60(2),300-309.
[29]
Sankaranarayanan,A.C.,Studer,C.,Baraniuk,R.
G.(2012).CS-MUVI:Videocompressivesensingfor
spatial-multiplexingcameras.
ICCP
.
[30]
He,K.,Zhang,X.,Ren,S.,Sun,J.(2016).Deepresid-
uallearningforimagerecognition.
CVPR
.
[31]
Zhang,K.,Zuo,W.,Gu,S.,Zhang,L.(2017).Learn-
ingdeepCNNdenoiserpriorforimagerestoration.
CVPR
.
[32]
RickChang,J.H.,Li,C.L.,Poczos,B.,VijayaKu-
mar,B.V.K.,Sankaranarayanan,A.C.(2017).One
NetworktoSolveThemAllŒSolvingLinearInverse
ProblemsUsingDeepProjectionModels.
ICCV
.
[33]
Mousavi,A.,Baraniuk,R.G.(2017).Learningtoin-
vert:Signalrecoveryviadeepconvolutionalnetworks.
ICASSP.
[34]
Martin,D.,Fowlkes,C.,Tal,D.,Malik,J.(2001).A
databaseofhumansegmentednaturalimagesandits
applicationtoevaluatingsegmentationalgorithmsand
measuringecologicalstatistics.
ICCV
.
[35]
Adler,A.,Boublil,D.,Elad,M.,Zibulevsky,
M.(2016).Adeeplearningapproachtoblock-
basedcompressedsensingofimages.
arXivpreprint
,
arXiv:1606.01519.
[36]
Hornik,K.,Stinchcombe,M.,White,H.(1989).Mul-
tilayerfeedforwardnetworksareuniversalapproxima-
tors.
NeuralNetworks
,2(5),359-366.
[37]
Wallace,G.K.(1992).TheJPEGstillpicturecom-
pressionstandard.
IEEETransactionsonConsumer
Electronics
,38(1),xviii-xxxiv.
[38]
Sullivan,G.J.,Ohm,J.,Han,W.J.,Wiegand,T.
(2012).Overviewofthehighefyvideocoding
(HEVC)standard.
IEEETransactionsonCircuitsand
SystemsforVdeoTechnology
,22(12),1649-1668.
[39]
Abadi,M.,Barham,P.,Chen,J.,Chen,Z.,Davis,A.,
Dean,J.,...Kudlur,M.(2016).TensorFlow:ASystem
forLarge-ScaleMachineLearning.
OSDI
.
[40]
Wang,Z.,Liu,D.,Yang,J.,Han,W.,Huang,T.
(2015).Deepnetworksforimagesuper-resolutionwith
sparseprior.
ICCV
.
[41]
Liu,D.,Wang,Z.,Wen,B.,Yang,J.,Han,W.,Huang,
T.S.(2016).Robustsingleimagesuper-resolutionvia
deepnetworkswithsparseprior.
IEEETransactionson
ImageProcessing
,25(7),3194-3207.
[42]
Wang,Z.,Liu,D.,Chang,S.,Ling,Q.,Yang,Y.,
Huang,T.S.(2016).D3:Deepdual-domainbasedfast
restorationofJPEG-compressedimages.
CVPR
.
[43]
Iliadis,M.,Spinoulas,L.,Katsaggelos,A.K.(2018).
Deepfully-connectednetworksforvideocompressive
sensing.
DigitalSignalProcessing
,72,9-18.
[44]
Jin,K.H.,McCann,M.T.,Froustey,E.,Unser,M.
(2017).Deepconvolutionalneuralnetworkforinverse
problemsinimaging.
IEEETransactionsonImagePro-
cessing
,26(9),4509-4522.
[45]
Wang,S.,Fidler,S.,Urtasun,R.(2016).Proximal
deepstructuredmodels.
NIPS
.
[46]
Xin,B.,Wang,Y.,Gao,W.,Wipf,D.,Wang,B.
(2016).Maximalsparsitywithdeepnetworks?.
NIPS
.
[47]
Liutkus,A.,Martina,D.,Popoff,S.,Chardon,G.,
Katz,O.,Lerosey,G.,...Carron,I.(2014).Imaging
withnature:Compressiveimagingusingamultiply
scatteringmedium.
Reports
,4,5552.
[48]
Riegler,G.,Rther,M.,Bischof,H.(2016).ATGV-net:
Accuratedepthsuper-resolution.
ECCV
.
[49]
Zhang,J.,Zhao,C.,Zhao,D.,Gao,W.(2014).Image
compressivesensingrecoveryusingadaptivelylearned
sparsifyingbasisviaL0minimization.
SignalProcess-
ing
,103,114-126.
[50]
Zhang,J.,Zhao,D.,Jiang,F.,Gao,W.(2013).Struc-
turalgroupsparserepresentationforimagecompressive
sensingrecovery.
IEEEDataCompressionConference
(DCC)
.
[51]
Zhao,C.,Zhang,J.,Ma,S.,Gao,W.(2016).Non-
convexLpnuclearnormbasedADMMframeworkfor
compressedsensing.
IEEEDataCompressionConfer-
ence(DCC)
.
[52]
Zhao,C.,Ma,S.,Zhang,J.,Xiong,R.,Gao,W.
(2017).Videocompressivesensingreconstructionvia
reweightedresidualsparsity.
IEEETransactionsonCir-
cuitsandSystemsforVideoTechnology
,27(6),1182-
1195.
[53]
Xie,J.,Xu,L.,Chen,E.(2012).Imagedenoisingand
inpaintingwithdeepneuralnetworks.
NIPS
.
"
14,Incremental Sparse Bayesian Ordinal Regression,http://arxiv.org/pdf/1806.06553v1.pdf,https://github.com/chang-li/SBOR,"arXiv:1806.06553v1  [cs.LG]  18 Jun 2018IncrementalSparseBayesianOrdinalRegression
ChangLi
a,

,MaartendeRijke
a
a
UniversityofAmsterdam,SciencePark904,1098XHAmsterda
m,TheNetherlands
Abstract

OrdinalRegression(OR)aimstomodeltheorderinginformat
ionbetweendifferentdatacat-
egories,whichisacrucialtopicinmulti-labellearning.A
nimportantclassofapproachesto
ORmodelstheproblemasalinearcombinationofbasisfuncti
onsthatmapfeaturestoahigh-
dimensionalnon-linearspace.However,mostofthebasisfu
nction-basedalgorithmsaretime
consuming.WeproposeanincrementalsparseBayesianappro
achtoORtasksandintroducean
algorithmtosequentiallylearntherelevantbasisfunctio
nsintheordinalscenario.Ourmethod,
calledIncrementalSparseBayesianOrdinalRegression(IS
BOR),automaticallyoptimizesthe
hyper-parametersviathe
type-IImaximumlikelihood
method.Byexploitingfastmarginallikeli-
hoodoptimization,ISBORcanavoidbigmatrixinverses,whi
chisthemainbottleneckinapply-
ingbasisfunction-basedalgorithmstoORtasksonlarge-sc
aledatasets.WeshowthatISBOR
canmakeaccuratepredictionswithparsimoniousbasisfunc
tionswhileofferingautomaticesti-
matesofthepredictionuncertainty.Extensiveexperiment
sonsyntheticandrealworddatasets
demonstratetheefciencyandeffectivenessofISBORcompa
redtootherbasisfunction-based
ORapproaches.

Keywords:
Ordinalregression,sparseBayesianlearning,basisfunct
ion-basedmethod
1.Introduction
Thetaskofmodelingordinaldatahasattractedattentionin
variousareas,includingcomputer
vision[1,2],informationretrieval[3],recommendersyst
ems[4]andmachinelearning[5,6,
7,8,9].Becauseoftheexplicitorimplicitrelationshipbe
tweenlabels,simpleregressionor
multi-classicationalgorithmsmayfailtondoptimaldec
isionboundaries,whichmotivatesthe
developmentofdedicatedmethods.
Generally,ORalgorithmscanbeclassiedintothreecatego
ries:naiveapproaches,ordinal
binarydecompositions,andthresholdmodels[5].Fornaive
approaches,ORtasksaresimplied
intotraditionalmulti-classicationorregressiontasks
,omittingorderinginformation,andsolved
bysimplemachinelearningalgorithms,e.g.,SupportVecto
rMachine(SVM)Regression[10].
Forordinalbinarydecomposition,theordinallabelsarede
composedintoseveralbinarypairs,
whicharethenmodeledbyasingleormultipleclassiers.Fo
rthethresholdmodels,theOR
problemisaddressedbytrainingathresholdmodel,whichmo
delsthehiddenscorefunctionand
animplicitsetofthresholdsthatderivetheordinalparadi
gm.Amongthesethreecategories,the

Correspondingauthor.

Emailaddresses:
c.li@uva.nl
(ChangLi),
derijke@uva.nl
(MaartendeRijke)
PreprintsubmittedtoNeuralNetworks
June19,2018
third,thresholdmodels,isthemostpopularwaytomodelthe
ORproblems[5].Thus,inthis
paper,wefocusonthresholdmodels.
Sincedatamaylieinalow-dimensionalspacewheredataaren
otdistinguishablebyalinear
combinationofthefeatures,basisfunctionsarewidelyuse
dinallthreetypesofORalgorithm.
Thebasisfunctioncanmapfeaturestohighlynon-linearspa
ceswherethedatacanbedistin-
guishablebyalinearcombinationofbasisfunctions[11].W
ecallthiskindofalgorithms
basis
function-basedalgorithms
.Mostofthecurrentbasisfunction-basedORalgorithmsdon
otscale
well,astheyarebatchmethodsandrequireaccesstothefull
trainingdataset.
Toaddressthisscalabilityproblem,wepropose
IncrementalSparseBayesianOrdinalRe-
gression
(ISBOR),whichutilizesanincrementalBayesianapproacht
olearning.Weimposea
zero-meanGaussianprioroverfunctionparametersandutil
izetheordinallikelihood[12],which
isregardedasaprobitfunctionofORtomodeltheordinalrel
ationshipbetweencategories.Then
weapplytheLaplacemethod[13]toderiveaMaximumaPosteri
ori(MAP)estimateoftheun-
knownparametersoverthedataset.InordertoderiveafullB
ayesiansolution,wederiveatype-II
maximumlikelihoodoptimization[14],inwhichISBORautom
aticallyoptimizesthethresholds
thatdeterminethedecisionboundariesoforderingcategor
iesashyper-parameters.Finally,to
acceleratetraining,wefollowtheideaoffastmarginallik
elihoodlearning[15]andderivean
incrementaltrainingstrategyforISBOR.
Withthispaper,wemakeanimportantsteptowardsefciento
rdinalregressionbasedon
basisfunctions.Inparticular,themaincontributionsare
asfollows:

Weproposeabasisfunction-basedsequentialsparseBayesi
antreatmentforordinalregres-
sion,ISBOR,whichscaleswellwiththenumberoftrainingsa
mples.

WeprovideanexperimentalevaluationofISBOR'sperforman
ceagainstexistingbasis
function-basedORalgorithmsintermsofefcacy,efcienc
yandsparseness.
Theremainderofthepaperisorganizedasfollows.Section2
revisitstherelatedwork.Section3
presentsISBOR.Section4detailsthehyper-parameteropti
mizationofISBOR.Wereportonthe
experimentalresultsinSection5.Thepaperisconcludedin
Section6.
2.RelatedWork
Inthispaper,wefocusonso-calledbasisfunction-basedap
proachestoordinalregression,
whichbringnon-linearpatternstothelineardecisionfunc
tionsandarewellstudiedinma-
chinelearning.Threetypesofbasisfunction-basedapproa
chesarewidelyusedfortheORtask:
SVMs[11],GaussianProcesses(GP)[16]andSparseBayesian
Learning(SBL)[14].SVM
approachesconvertthelearningprocesstoaconvexoptimiz
ationproblemforwhichthereare
efcientalgorithms,e.g.,SMO[17],tondglobalminima.H
owever,SVMisnotequippedwith
aprobabilisticinterpretation,asaresultofwhichitisha
rdtouseexpertorpriorknowledgeand
maketheprobabilisticpredictionswithSVM.GP[16]andSBL
areBayesianmethods,which
takeexpertknowledgeaspriorinformationandinterpretth
epredictionwiththeposterioridistri-
bution.InordertoconductBayesianinferenceandmodelsel
ection,mostofthemrequireoneto
computetheinverseofthebasisfunctionmatrix,whichlead
sto
O
(
N
3
)computationalcomplex-
ity,where
N
isthenumberoftrainingsamples.
Inthefollowing,wedescribesomeofthesealgorithmstopro
videcontextforourwork.The
SVM-basedSupportVectorOrdinalRegression(SVOR)approa
ch[18]isanaccurateORalgo-
rithm[5].SVORisoptimizedusingasequentialminimalopti
mizationstrategy,whichbringsthe
2
upperbounddownto
O
(
N
2
log
N
).SolvingSVORinthedualproblemboilsdowntooptimizing
withL2-regularization,whichleadstoaslightlysparseso
lution.
IncrementalSupportVectorMachineforOrdinalRegression
(ISVOR)[19]addressesthe
problemofbasisfunction-basedbatchalgorithmsforOR.It
decomposestheORprobleminto
ordinalbinaryclassicationandsimultaneouslybuildsde
cisionboundarieswithlinearcompu-
tationalcomplexity.However,ISVORsuffersfromtheprobl
emofstabilityanditdoublesthe
problemsizebecauseofitsbinarydecompositionapproach.
Themaindifferencebetweenthe
proposedISBORandSVM-basedmethodsisthatISBORcanusepr
iorknowledgeandmake
probabilisticpredictions.
GaussianProcessOrdinalRegression(GPOR)[12]istherst
GPalgorithmthathasbeen
proposedfortheORtask.GPORemploysaGPprioronthelatent
functions,andusesanordinal
likelihood,whichisageneralizationofthe
probit
function,toestimatethedistributionofordinal
dataconditionalonthemodel.Toconductmodeladaptation,
GPORappliestwoBayesianinfer-
encetechniques:Laplaceapproximation[13]andexpectati
onpropagationapproximation[20],
respectively.SinceapproximateBayesianinferencemetho
dsrequiresonetocomputetheinverse
ofan
N

N
matrix,thecomputationalcomplexityofGPORis
O
(
N
3
).Themaindifferences
betweenGPORandISBORaretwofold:
1.ISBORisasparsemethod,asaresultofwhichthepredictio
nisonlybasedontherelevant
samples.Incontrast,GPORmakespredictionsbasedonthewh
oletrainingdata.
2.ISBORisanincrementallearningalgorithm,whileGPORis
abatchalgorithm:during
training,GPORneedstocomputethematrixinverseofsize
N

N
,whileISBORonly
computesthematrixinverseofsize
M

M
,where
M
˝
N
isthenumberofrelevant
samples.
BasedonGPOR,variousORalgorithmshavebeenproposed[21,
22,23,24].However,they
areallbatchalgorithms.Incontrast,theproposedmethod,
ISBORisanincrementallearning
algorithmandgetsridofcomputingtheinverseof
N

N
matrix.
BasedonSBL,SparseBayesianOrdinalRegression(SBOR)[25
]buildsaprobabilisticso-
lutiontotheORproblem.Here,ﬁsparseﬂthatmeansSBORutil
izesasparsenessassumption
thatenablesittomakepredictionsbasedonafewrelevantsa
mpleswitha
O
(
M
3
)computa-
tionalbound,where
M
isthenumberofrelevantsamples.However,SBORisstillaba
tchal-
gorithmandrequiresonetohandlematrixinversiononthefu
lldatasetduringinitialiterations.
Otherbasisfunction-basedbatchORalgorithmsincludeKer
nelDiscriminateforOrdinalRe-
gression(KDOR)[26].
Insummary,ISBORdiffersfromtheabovealgorithmsinthefo
llowingways.Insteadofop-
eratinginbatch,ISBORutilizesanincrementalwaytoseque
ntiallychooserelevantsamples.Be-
causeofthesparsityassumption,duringsequentialtraini
ngISBORonlyselectsasmallportion
ofthetrainingdatawithlinearcomputationalcomplexityi
neachiteration.Moreover,insteadof
designingordinalpartitionslikeISVOR,ISBORdirectlyle
arnstheimplicitthresholdsandscore
function,whichisamorenaturalwaytorevealordinalrelat
ions.
3.IncrementalSparseBayesianOrdinalRegression
Westartthissectionbydeningthenotationusedinthepape
r.Thetrainingsetis
D
=
f
x
n
;
y
n
g
N
n
=
1
,where
x
n
2
R
d
isthefeaturevector,
y
n
2f
1
;
2
;:::;
r
g
isthecorrespondingcategory;
r
isthenumberofcategories.Weusenormal-faceletterstode
notescalarandboldfacelettersto
denotevectorsandmatrices.
3
WepresentISBORinfoursteps:modelspecication,likelih
ooddenition,priorassumption
andmaximumaposterior.

3.1.Modelspecication
AsathresholdORmodel[5],ISBORchoosesalinearcombinati
onofbasisfunctionsasthe
scorefunction,
f
(
x
n
;
w
),whichmapsasamplefromthe
d
-dimensionalfeaturespacetoareal
number:
f
(
x
n
)
=
N
X
i
=
1
˚
i
(
x
n
)
w
i
=
˚
(
x
n
)
w
;
(1)
where
w
2R
N
denotestheparametervector
1
and
˚
(
x
n
)
=
[
˚
1
(
x
n
)
;:::;˚
N
(
x
n
)]isthebasis
function,e.g.,theGaussianRadialBasisFunction(RBF):
˚
(
x
n
;
x
i
)
=
exp(


k
x
n

x
i
k
2

2
)
:
(2)
Aftermapping,ISBORexploitsasetofthresholds,[
b
0
;:::;
b
r
],todetermineintervalsofdif-
ferentcategories.Inordertorepresenttheorderinginfor
mation,thesethresholdsarechosenas
asetofascendingnumbers,e.g.,
b
i
+
1
>
b
i
,andworkwithasetofpositiveauxiliarynumbers,
[

2
;:::;

r

1
],with
b
n
denedas
b
n
=
b
1
+
P
n

i
=
2

i
.Duringprediction,asample
x
n
isclassied
toatarget
y
n
ifandonlyif
b
y
n

1
<
f
(
x
n
)

b
y
n
.Weset
b
0
=
1
and
b
r
=
1
.
3.2.Ordinallikelihood
Tomodelordinaldata,wetaketheordinallikelihoodpropos
edinGPOR[12].Thelikelihood
isthejointdistributionofthesamplesconditionalonthem
odelparameters,andwiththeI.I.D.
assumption;itiscomputedas:
p
(
Y
j
X
;
w
)
=
N
Y
n
=
1
p
(
y
n
j
X
;
w
)
;
where
Y
=
f
y
n
g
N
n
=
1
and
X
=
f
x
n
g
N
n
=
1
.Followingthestandardprobabilisticassumption[12],we
assumethattheoutputsofascorefunctionarecontaminated
withrandomGaussiannoise:‹
y
n
=
f
(
x
n
)
+

,where

˘N
(0
;˙
2
).
˙
isthestandarddeviationofthenoisedistribution,whichi
s
learnedbythemodelselection(Section4.2).Inthisway,th
escorefunctionislinkedtothe
probabilisticoutput
p
(‹
y
n
j
w
;
x
n
;
)
=
N
(‹
y
n
j
f
(
x
n
)
;˙
2
).Andthelikelihoodoverasampleis
computedasfollows:
p
ideal
(
y
n
j
x
n
;
w
;
)
=
8

>

>

<

>

>

:
1if
b
y
n

1
<
‹
y
n

b
y
n
;
0otherwise
:
(3)
Since
b
i
+
1
=
b
i
+

i
+
1
and

i
+
1
>
0,[
b
0
;:::;
b
r
]dividethereallineinto
r
ordinalintervals.Thus,
withtheseintervals,theideallikelihoodmapstherealval
ueoutput
f
(
x
)toordinalcategories.
However,becauseoftheuniformdistribution,Eq.(3)isnot
differentiable,andhencewecannot
1
Here,
w
n
controlstherelevanceofthe
n
-thbasisfunction
˚
n
(
w
):if
w
n
=
0,the
n
-thbasisfunctionisirrelevantfor
thedecision,whichisequivalenttothrowthe
n
-thsampleawayandretaintherelevantbasisfunctions.
4
implementBayesianinference.Totacklethisissue,weinte
grateoutthenoisetermandobtaina
differentiablelikelihoodasfollows:
p
(
y
n
j
x
n
;
w
;˙
)
=
Z

p
ideal
(
y
n
j
x
n
;
w
;
)
N
(

j
0
;˙
2
)
d

=
 
(
z
n
;
1
)

 
(
z
n
;
2
)
;
(4)
where
z
n
;
1
=
b
y
n

f
(
x
n
)
˙
and
z
n
;
2
=
b
y
n

1

f
(
x
n
)
˙
;
and
 
(
z
)istheGaussiancumulativedistributionfunction.Basedo
nEq.(4),maximumlikelihood
estimationisequivalenttomaximizingtheareaunderthest
andardGaussiandistributionbetween
z
n
;
1
and
z
n
;
2
,whichisdifferentiable.
3.3.Prioriassumption
Forlargescaledatasets,ifwedirectlylearnparametersby
maximumlikelihoodestimation,
wemayeasilyencountersevereover-tting.Toavoidthis,w
eaddanadditionalconstrainton
parameters:theregularizationterm.InBayesianlearning
,weachievethisbyintroducingazero-
meanGaussianpriorfor
w
:
p
(
w
n
j

n
)
=
N
(
w
n
;0
;

1
n
).Assumingthateachparameterismutu-
allyindependent,theprioroverparametersiscomputedas:
p
(
w
j

)
=
N
Y
n
=
1
N
(
w
n
j
0
;

1
n
)
;
(5)
where

=
[

1
;:::;
N
]and

n
,theinverseofvariance,servesastheregularizationterm
.Ifthe
valueof

n
islarge,theposteriorof
w
n
willbemainlyconstrainedbythepriorand
w
n
willbe
boundtoasmallneighborhoodof0.
2
Tocompletethedenitionofthesparseprior,wedenea
setofatGammahyper-priorsover

,whichtogetherwithGaussianpriorsresultinStudent's-t
priorandworkas
L
1
regularization[14].
3.4.Maximumaposterior
Havingdenedthepriorandlikelihood,ISBORproceedsbyco
mputingtheposteriorover
alltrainingdata,basedonBayes'rule:
p
(
w
j
D
)
=
p
(
Y
j
X
;
w
;˙
)
p
(
w
j

)
p
(
D
j

)
;
(6)
where
D
isthetrainingdataset,
p
(
w
j

)denedinEq.(5)istheprior,
p
(
Y
j
X
;
w
;˙
)dened
inEq.(4)isthelikelihood,thedenominator
p
(
D
j

)
=
R
p
(
Y
j
X
;
w
;˙
)
p
(
w
j

)
d
w
isthe
marginallikelihood,whichweuseformodelselectionandhy
per-parameteroptimizationinthe
nextsection.Tosimplifyournotation,wecollectallthehy
per-parameters,includingnoiselevel
˙
,thresholdsand

,into

.
Wepreferthe
w

withthehighestposteriorprobability,andformulatetheM
APpointesti-
mateas
w

=
max
w
p
(
w
j
D
).However,wecannotintegrate
w
outinthemarginallikelihood
2
Practically,when
w
n
issmallerthanavalue,e.g.,10

3
,wewillconsiderittobe0,whichboilsdowntothrowing
awaythecorrespondingsample.
5
w-8-6-4-202468
ln p(w|D)-120-100-80-60-40-200Ordinal posterior y=1y=2y=3w-8-6-4-202468
@ p(w|D)/ @ w-20-15-10-505101520First derivative of ordinal posterior w-8-6-4-202468
@2 p(w|D)/ @ w2-2-1.8-1.6-1.4-1.2-1Second derivative of ordinal posterior Figure1:Theordinalposterioranditsrstandsecondderiv
atives.
analytically.InourMAPestimationweusethefactthat
p
(
w
j
D
)
/
p
(
Y
j
X
;
w
;˙
)
p
(
w
j

)and
workwiththelogarithmoftheposterior:
ln
p
(
w
j
D
)
=
ln
p
(
Y
j
X
;
w
;˙
)
+
ln
p
(
w
j

)
+
const
ˇ
N
X
n
=
1
ln(
 
(
z
n
;
1
)

 
(
z
n
;
2
))

1
2
w
T
Aw
;
(7)
where
A
isadiagonalmatrixwithdiagonalelements[

1
;:::;
N
],
const
isatermindependent
of
w
.Therstpartofthelastline,fromthelikelihood,worksas
thelossterm;thesecondpart,
fromtheprior,actsastheregularizationterm.
Next,theNewton-Raphsonmethod[27]isappliedtocomputet
heMAPestimate.First,we
computetherstandsecondorderderivativesoftherstter
m(log-likelihoodpart),
L
=
ln
p
(
Y
j
X
;
w
):
@
L
@
w
=

N
X
n
=
1
1
˙
N
(
z
n
;
1
j
0
;
1)
N
(
z
n
;
2
j
0
;
1)
 
(
z
n
;
1
)

 
(
z
n
;
2
)
˚
n
=

T

(8)
@
2
L
@
w
@
w
T
=


T
H

;
(9)
where

n
=
1
˙
N
(
z
n
;
1
j
0
;
1)
N
(
z
n
;
2
j
0
;
1)
 
(
z
n
;
1
)

 
(
z
n
;
2
)
H
nn
=
1
˙
2
2

6

6

6

6

6

4
 
N
(
z
n
;
1
j
0
;
1)

N
(
z
n
;
2
j
0
;
1)
 
(
z
n
;
1
)

 
(
z
n
;
2
)
!
2
z
n
;
1
N
(
z
n
;
1
j
0
;
1)

z
n
;
2
N
(
z
n
;
2
j
0
;
1)
 
(
z
n
;
1
)

 
(
z
n
;
2
)
3

7

7

7

7

7

5
:
Then,combiningEq.(7),Eq.(8)andEq.(9),weobtaintheder
ivativeofthelog-posterioras
@
2
log
p
(
w
j
D
)
@
w
@
w
T
=


T
H


A
:
Notethat

T
H

isaquadraticformand
A
isadiagonalmatrixwithpositivediagonalelements,
so

@
2
log
p
(
w
j
D
)
@
w
@
w
T
6
isapositivedenitematrix,whichimpliesthatMAPestimat
ionisaconcaveprogrammingprob-
lem,withaglobalmaximum.
HavingfoundtheMAPpoint
w

,weusetheLaplacemethodtoapproximatetheposterior
distributionbyaGaussiandistribution
N
(
w
j
w

;

),where
w

and

arethemeanandvariance
andcomputedasfollows:

=
(
A
+

T
H

)

1
(10)
w

=

T
H
‹

t
;
(11)
where
‹

t
=
H

1

+

w

.
UsingalocalGaussianattheMAPpointtorepresenttheposte
riordistributionoverweights
isoftenconsideredasaweaknessoftheBayesiantreatment,
especiallyforcomplexmodels.
However,aspointedoutbyTipping[14],alog-concaveposte
riorimpliesamuchbetteraccuracy
andnoheaviersparsitythanL1-regularization.Aswediscu
ssedabove,theposteriorofISBOR
hasthefeatureoflog-concavity.Wereporttheplotsofthel
og-posterioraswellasitsrstand
secondorderderivativesinFigure1andseethat
@
L
@
w
ismonotonicallydecreasingw.r.t.
w
,while
@
2
L
@
2
w
isalwayssmallerthan0.SotheMAPhereisessentiallyalog-
concaveoptimizationproblem,
whichimpliesthattheLaplaceapproximationinISBORenjoy
sthesamefeaturesofaccuracy
andsparsityasintheRelevanceVectorMachine(RVM)[14].

4.Hyper-parameterOptimization
ISBORusesvarioushyper-parameters,including

inthepriorestimation(Eq.(5)),thenoise
variance
˙
inEq.(4),andthethresholds[
b
1
;:::;
b
r
].Inthissectionwedetailhowtolearnthese
hyper-parameters.

4.1.Marginallikelihood
AsafullyBayesianframework,hyper-parametersareoptimi
zedbymaximizingtheposterior
modeofhyper-parameters
p
(

j
D
)
/
p
(
D
j

)
p
(

),where

containsallhyper-parameters.As
weassumeanon-informativeGammahyper-prior,theoptimiz
ationisequivalenttomaximizing
themarginallikelihood
p
(
D
j

),whichiscomputedas
p
(
D
j

)
=
R
p
(
D
j
w
;˙
)
p
(
w
j

)
d
w
.
Asthereisnoclosedformforthisequation,again,weapplyL
aplaceapproximationandgetthe
followingapproximations:
p
(
D
j

)
=
p
(
Y
j
w

)
p
(
w

j

)(2
ˇ
)
n
=
2

1
=
2
ln
p
(
D
j

)
=
L
1
2
w

T
Aw

+
1
2
ln
j
A
j
+
1
2
ln
j

j
:
(12)
Intherestofthissection,wedealwiththelog-marginallik
elihood,andmaximizeEq.(12)with
respecttoeachhyper-parameter.

4.2.Thresholdandnoisehyper-parameters
Forthethresholdhyper-parameters,weonlyneedtodetermi
ne
r

1values:
b
1
and[

2
,...,

r

1
].Sincewecannotcomputetheseanalytically,weexploitgr
adientdescent(ascent,actually)
7
toiterativelychoosetheseparameters.Thederivativesof
thelog-marginallikelihood,Eq.(12),
withrespectto
b
1
and

i
,arecomputedasfollows:
@
ln
p
(
D
j

)
@
b
=



;
(13)
@
ln
p
(
D
j

)
@

i
=
8

>

>

>

>

>

<

>

>

>

>

>

:




n
if
y
n
>
i
1
˙
N
(
z
1
;0
;
1)

(
z
1
)


(
z
2
)
if
y
n
=
i
0otherwise
:
(14)
Basedonthesetwoequations,weusegradientdescenttosear
chforproperthresholds.
Forthenoiseterm
˙
,settingthederivative
ln
p
(
D
j

)
˙
=
0
;
weobtainanupdateruleforthenoiseterm:
˙
2
=
k
‹

t


w
k
2
N

P
n
(1


n

nn
)
;
(15)
where
‹

t
=
H

1

+

w

.
4.3.Fastmarginallearning
Wecomputethecontributionofthesparsityhyper-paramete
r

tothemarginallikelihoodas
follows:
ln
p
(
D
j

)
=
L
1
2
ln
j
C
j
1
2
‹
t
C

1
‹
t
;
(16)
wherewecompute
C
asfollows:
C
=
H

1
+

A

1

T
=
H

1
+
X
n
,
j

n
˚
n
˚
T

n
+


1
j
˚
j
˚
T
j
:
(17)
Sincecomputing
C
requiresmatrixinversion,itisimpracticaltomaximizeit
forlargescale
trainingsets.Fortunately,TippingandFaul[15]proposed
asequentialwaytomaximizethe
marginallikelihood.Wetakethisstrategyandoptimize

asfollows:

First,weusetheestablishedmatrixdeterminantandinvers
eidentities[28]tocomputethe
determinationandinverseof
C
asfollows:
j
C
j
=
j
C
=
j
jj
I
+


1
j
˚
j
˚
T
j
j
C

1
=
C

1
=
j

C

1
=
j
˚
j
˚
T
j
C

1
=
j

j
+
˚
T
j
C

1
=
j
˚
j
;
(18)
where
I
istheidentitymatrix,and
C
=
j
denotes
C
withoutthecontributionofthe
j
-th
sample.
8

Second,wedenetwoauxiliaryvariables:
s
j
=
˚
T
j
C

1
=
j
˚
j
;
q
j
=
˚
T
j
C

1
=
j
‹
t
:
(19)
CombiningEqn.(16),(18)and(19),weisolatethecontribut
ionofsample
j
tothemarginal
likelihoodasfollows:
ln
p
(
D
j

j
)
=
1
2
[ln

j

ln
j

j
+
s
j
j
+
q
2
j
s
j
+

j
]
:
(20)
Forsimplicity,wedene
g
(

j
)
=
ln
p
(
D
j

j
).

However,westillneedtocomputetheinverseof
C
=
j
inEq.(19).Tospeedupthecompu-
tation,wedenethefollowauxiliaryvariables:
Q
j
=
˚
T
j
C

1
‹
t
=
˚
T
j
H
‹
t

˚
T
j
H

T
H
‹
t
S
j
=
˚
T
j
C

1
˚
j
=
˚
T
j
H
˚
j

˚
T
j
H

T
H
˚
j
;
where

2
R
M

M
isthecovarianceoftheposteriordistribution(Eq(10)).
3
Then,wecan
compute
s
j
=

j
S
j

j

S
j
and
q
j
=

j
Q
j

j

S
j
:

Finally,setting
@
g
(

j
)
@
j
=
0
;
wegettheclosedformsolutionfor

j
:

j
=
s
2
j
q
2
j

s
j
:
(21)
Since

j

0,thedenominatorofEq.(21),denotedas
f
j
=
q
2
j

s
j
>
0,whichworksasan
importantcriterionfordeterminingtherelevantsamples.
4.4.ISBOR
Wesummarizethepseudo-codeofISBORinAlgorithm1.Weprov
idebriefcommentson
threeingredients.First,weinitializeISBOR(line4)byra
ndomlypickingasamplefromeach
categoryastheinitialrelevantsamples.Basedonthese
r
samples,weinitialize
Q
,
S
and
f
.On
Line6,wecomputethedeltamarginallikelihoodforthesamp
lesnotyetconsidered.Astothe
calltoEstimate()(line13),weupdate
w
basedonEq.(11);update

basedonEq.(21);update
ml
basedonEq.(12)andusegradientsearchtoupdatethreshold
b
basedonEq.(13)andEq.(14).
3
Becauseofthesparseassumption,
M
˝
N
,andthuscomputingtheinverseof

ismuchfasterthanthatof
C
.
9
Algorithm1
IncrementalSparseBayesianOrdinalRegression(ISBOR)
1:
Input:D
=
f
x
;
y
g
;
,maxItsandminDelta.
2:
Output:w
,
b
and
˙
.
3:

=basis(
x
;
);
4:
w
;
˚
;

;˙;
b
;
Q
;
S
;
f
=
Initialize(

;
y
);
5:
for
i
=
1
;
2
;:::;
maxIts
do
6:
deltaML=[
g
(

1
)
;:::;
g
(

n
)];
7:
˚
n
 
max(deltaML);
8:
if
˚
n
2
˚
and
f
n
<
0
then
9:
f
w
;

;
˚
g f
w
;

;
˚
gf
w
n
;
n
;˚
n
g
;
10:
elseif
f
n
>
0
then
11:
f
w
;

;
˚
g f
w
;

;
˚
g[f
w
n
;
n
;˚
n
g
;
12:
endif
13:
w
;

;
b
;
ml
=
Estimate(
w
;

;
b
;

;
˚
;˙
);
14:
compute
˙
basedonEq.(15);
15:
compute
Q
;
S
basedonEq.(21);
16:
compute
q
;
s
;
f
;
17:
if
abs(
ml

ml
old
)
<
minDelta
then
18:
break;
19:
endif
20:
ml
old
=
ml
;
21:
endfor
4.5.Computationalanalysis
Themaximizationruleformarginallikelihoodisbasedonth
eMAPestimatewhich,in
Eq.(10),requirestheinversionofamatrixwith
O
(
M
3
)computationalcomplexityand
O
(
M
2
)
memory.However,asweconstructivelymaximizethemargina
llikelihood,
M
˝
N
,rst,we
chooseonesamplefromeachcategorytoinitializethealgor
ithm;second,webenetfromthe
sparselearning,asthescaleof
M
remainssmall(aroundafewdozenbasedonourexperiments).
Inthiscase,matrixinversionisnotthemaincomputational
bottle-neckforeachiteration.
AlthoughweapplyanincrementalstrategytotrainISBOR,we
havetocomputethebasis
functionmatrixintheinitializationstep,whichhas
O
(
N
2
)computationalcomplexityand
O
(
N
2
)
memory.Combiningthesetwoparts,thetotalcomputational
complexityofISBORis
O
(
N
2
+
M
3
)
andthememorycomplexity
O
(
N
2
).However,weshouldmentionthatthebasisfunctionmatrix
canbecomputedinthepre-trainingsession,sothecomputat
ionalcomplexityisessentially
O
(
N
+
M
3
).Forcomparison,wereportthecomputationalandspacecom
plexityofSBORandother
state-of-the-artmethodsinTable1.WeseethatISBORhasth
ebestcomputationalcomplexity,
andthus,ISBORismoreefcientthanothers,atleasttheore
tically.
Ascomputingtheposteriorcovariancerequirestheinverse
oftheHessianmatrix,(
A
+

T
H

)

1
,itisinevitabletoencounterthesingularvalues.Theoret
icallyspeaking,
H
and
A
arethediagonalmatriceswithpositiveelements,

T
H

isthequadraticform.However,there
stillexistsingularproblems,especiallywhensome

areextremelylarge.Inordertoavoidill-
conditioning,wemanuallyprunetrainingsampleswithlarg
e

values.
10
Table1:Computationalandspacecomplexityofordinalregr
essionalgorithms.
N
and
M
representthenumberoftraining
samplesandthenumberofrelevantand/orsupportsamplesre
spectively.
KDOR/GPOR/ISVORISBOR
SVOR/SBOR
Computationalcomplexity
O
(
N
3
)
O
(2
N
+
8
M
3
)
O
(
N
+
M
3
)
Spacecomplexity
O
(
N
2
)
O
(4
N
2
)
O
(
N
2
)
4.6.Sparsityanalysis
ThesimpleGaussianpriorworkingasanL2-regularizationi
ntheposteriormodelleadstoa
non-sparseMAPestimate.However,withtheGammahyper-pri
or,therealpriorover
w
follows
aStudent'stdistributionwhichisconsideredasasparsepr
iorwithasharppeakat0[14,Section
5.1].Duringinference,wedonotintegrateout

,whichimpliesthat

isthedirectfactorto
sparsity,whichinturnmeansthatforirrelevantvectorsth
ecorresponding

shouldbelarge.
However,thelearned

inthesequentialmodelarerelativelysmall:weonlyaddpot
entially
relevantsampleswhose

areessentiallysmalltothemodel.Thereisnoreasontolear
n

of
samplesexcludedfromthemodel,whichhavelargevalues.

5.ExperimentalEvaluation
Ourexperimentalevaluationaimsataddressingthefollowi
ngthreeresearchquestions.
RQ1
Efcacy:Isthegeneralizationperformanceoftheproposed
algorithm,ISBOR,comparable
tootherbaselines?
RQ2
Efciency:DoesfastmarginalanalysisreduceISBOR'scomp
utationalcomplexitycom-
paredtobaselines?
RQ3
Sparseness:CanISBORachievethecompetitivepredictions
onlybasedonasmallsubset
ofthetrainingset?
5.1.Experimentaldesign
Theresearchquestionslistedaboveleadustotwoexperimen
taldesigns.Therstinvolves
asyntheticdatasettogiveusanunderstandingoftheefcac
y,effectivenessandsparsity.The
secondisonbenchmarkdatasets,i.e.,7widelyusedordinal
datasetstoextensivelyevaluatethe
performanceofISBOR.

5.1.1.Datasets

Syntheticdataset.
Tocreateasyntheticdatasetwefollowthedata-generating
strategyin[29].
First,21
;
000two-dimensionalpointsaresampledwithinthesquarear
ea[0
;
10]

[0
;
10]under
auniformdistribution.Second,eachpointisassignedasco
rebythefunction
f
(
x
)
=
10(
x
1

0
:
5)(
x
2

0
:
5)
+

,where

˘N
(0
;
0
:
5
2
)actsasaGaussianrandomnoise.Finally,wechoosesix
thresholds
f1
,

60,

9,15,60,
+
1g
,andeachpointisattachedwithacategorybycomputing:
y
=
argmin
r
2f
1
;
2
;
3
;
4
;
5
g
b
r

1

10(
x
1

0
:
5)(
x
2

0
:
5)
+


b
r
:
Inthismanner,wegenerateave-categorydatasetandthenu
mbersofdatapointsassignedto
eachcategoryare4431,4535,3949,3780and4305,respectiv
ely.Wechoose10differentsizes
11
oftrainingsets:1000,2000,...,10000andusetherestofth
edataastestsets.Foreachsize
trainingsets,werandomlygenerate30differentpartition
s.Then,theexperimentsareconducted
onall30partitions.

Benchmarkdatasets.
WealsocompareISBORwithvealgorithmsonsevenbenchmark
datasets.
4
ThedetailsofthebenchmarkdatasetsaresummarizedinTabl
e2.Eachbenchmarkdatasetisran-
Table2:Benchmarks:Detailedinformation.
Dataset#Training#Test#Features#Categories
BS46815743

SWD750250104

Marketing6,7442,249749

Bank8,0005085

Computer8,092100125

CalHouse20,49015085

Census22,584200165
domlysplitinto20partitions.

5.1.2.Metrics
WeuseMeanAbsoluteError(MAE)tomeasuretheefcacy:
MAE
=
1
N
N
X
n
=
1
j
y
n

‹
y
n
j
;
where‹
y
n
isthepredictedcategory.Asforefciency,wechooserunni
ngtime(inseconds)asthe
measurement.

5.1.3.Methodsusedforcomparison
WechooseKDOR,GPOR,SVOR,SBORandISBORdiscussedinthere
latedworksectionas
baselines.WeusetheORCApackage[5](inMATLAB)
5
forKDOR.TheauthorsofSVORand
GPORprovideapubliclyavailableimplementationinC.
6
WeuseaMATLABimplementation
ofISVORsharedbytheauthors.SBORandISBORareimplemente
dinMATLAB.
5.1.4.Settingsandparameters
WechoosetheGaussianRBFinEq.(2)asthebasisfunctionfor
eachalgorithm.Weinitialize
ISBORbysetting

=
10

3
,
˙
=
1.
7
Weselectthekernelwidthvia5-foldcross-validation
onthetrainingsetwithinthevaluesof

2f
10

2
;
10

1
;:::;
10
g
.GPORautomaticallylearns
thehyper-parameters,whichdoesnotrequireanypre-selec
tionprocess.Forothermethods,we
4
http://www.uco.es/grupos/ayrna/ucobigfiles/datasets
-orreview.zip
5
https://github.com/ayrna/orca
6
http://www.gatsby.ucl.ac.uk/
Ÿ
chuwei/#software
7
ThisisaheuristicsetupinspiredbyChuandGhahramani[12]
,althoughthebetterwaytochoosethestartingpoints
isbytryingdifferentvaluesandselectingthebestcombina
tion.
12
followthemodelselectionprocessin[5]anduseanested5-f
oldcross-validationonthetraining
settosearchforthebesthyper-parameters.Specically,w
echoose

2f
10

3
;
10

2
;:::;
10
3
g
for
everyalgorithm.Theadditionalregularizationparameter
ofSVORandISVORarechosenwithin
thevaluesof
c
2f
10

1
;:::;
10
3
g
.ForKDOR,wechoosetheregularizationparameterwithin
therangeof
c
2f
0
:
1
;
1
;
10
g
,sincetheregularizationparameterofKDORpresentsadiff
erent
interpretationfromtheoneinSVM.Additionally,KDORrequ
iresanothersingularity-avoiding
parameter,whichischosenintherangeof
u
2f
10

6
;
10

5
;:::;
10

1
g
.
Cross-validationisconductedusingMAE.Thatis,oncetheh
yper-parameterswiththelowest
MAEareobtained,weapplythemtothewholetrainingsetandt
henvalidatethemonthetest
sets.
TheexperimentsarerunonaserverwithIntel(R)Xeon(R)CPU
E5-2683v32.00GHz(16
Cores)and32Gigabyte.

5.2.Experimentalresults

5.2.1.Efcacy
Webeginbyaddressing
RQ1
concerningefcacy.Werstconsidertheresultsonthesyn-
theticdataset.Figure2(a)showstheperformanceintermso
fMAEonthesyntheticdataset.From
thegure,weseethatotherthanISVOR,allthealgorithmswo
rkwellontheSyntheticdatasets,
intermsofefcacy.Specically,ISBORandSVORarethetwob
estperformingalgorithms.
Whenthedatasizesarelargerthan5000,SVORoutperformsIS
BOR,butthegapsaresmall.
Data size1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
MAE00.020.040.060.080.10.120.140.16ISBORISVORKDORSBORSVOR(a)MAE.
Data size1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
Running time in seconds100101102103ISBORISVORKDORSBORSVOR(b)Runningtime.
Figure2:MAEandrunningtimeofORalgorithmsonthesynthet
icdataset.
Next,weturntothebenchmarkdatasets.TheMAEscoresarepr
esentedinTable3(tophalf).
Theresultsareaveragedover20partitions.
Todeterminethesignicanceofobserveddifferences,weus
etheWilcoxontest[30,31]and
comparetheefcacyofeachpairofalgorithms.Sincewecomp
are6algorithms,thereare30
comparisonsforeachdatasetintotal.Wechoosethesignic
ancelevel

=
0
:
1andtakethe
numberofcomparisonsintoaccount,andobtainthecorrecte
dsignicancelevelas

=
0
:
1
=
30
ˇ
0
:
0033.Foreachalgorithm,werecordthenumberofstatistica
llysignicantwins,losses(or
failuresinnishingthetrainingontime)anddraws.TheWil
coxontestresultsarereportedin
Table4.
BasedonthetophalfofTable3andTable4,wendthatSVORist
hebestperforming
ordinalregressionalgorithmintermsofMAE.Specically,
SVORwins24timesoutof35pair-
13
Table3:Benchmarkresults:MAEandrunningtime.Standardd
eviations(ofMAE)indicatedinbrackets.Failureto
completeallrunsin24hoursisindicatedwith`Œ';bestresu
ltsaremarkedin
boldface
,secondbestin
italics
.
MAE
BSSWDMarketBankComputerCalHouseCensus
KDOR0.17(0.03)0.58(0.03)1.60(0.03)0.21(0.07)0.39(0.
03)
0.46(0.05)
0.63(0.06)
GPOR0.03(0.02)
0.41(0.03)
ŒŒŒŒŒ
SVOR
0.00(0.00)0.41(0.03)0.83(0.01)
0.20(0.06)
0.40(0.03)0.63(0.05)Œ
SBOR0.04(0.06)0.52(0.06)1.45(0.03)0.34(0.19)0.44(0.
12)Œ
0.60(0.30)
ISVOR0.36(0.53)0.56(0.04)
1.20(0.08)
0.80(0.10)
0.36(0.04)
0.87(0.07)0.65(0.06)
ISBOR
0.02(0.01)
0.43(0.02)1.74(0.05)
0.19(0.05)
0.37(0.05)0.50(0.04)0.63(0.05)
Runningtime
BSSWDMarketBankComputerCalHouseCensus
KDOR
0.080.27
159.9297.2796.861,369.141,696.18
GPOR359.94205.60ŒŒŒŒŒ

SVOR
0.08
0.8344.49932.592,682.194,350.30Œ
SBOR0.962.9693.73986.12204.85Œ3,713.62

ISVOR1.53
0.7765.22
73.8973.61
907.95774.22
ISBOR0.641.35
62.76
91.0294.84
810.22710.84
Table4:WilcoxontestsfortheMAEresultsobtainedusingth
ebenchmarkdatasetsandreportedinTable3.
Method#wins#draws#losses
GPOR4625

SVOR2474

SBOR111212

KDOR11816

ISVOR111113

ISBOR17108
wisecomparisons.ISBOR,thesecondbestperformingalgori
thm,wins17comparisons.Because
ofthetimelimitation,GPORfailstocompletetheexperimen
tson5datasetsandperformsworse.
Therestalgorithmsperformssimilarwitheachothersandwi
n11times.
Tosumup,theseresultsanswer
RQ1
asfollows:althoughSVORhasthebestgeneralization
performance,ISBORoutperformsotherbaselinesandiscomp
arabletoSVOR.
5.2.2.Efciency
Weturnto
RQ2
.Wereporttherunningtimeofcompetingalgorithmsonthesy
ntheticdataset
withdifferentdatascalesinFigure2(b).Generally,theim
plementationsinCrunmuchfasterthan
thoseinpureMATLAB.Tosuppressthiseffect,wecomparethe
runningtimesonalogarithmic
scale.WeomitplottingtheresultsofGPOR,becauseafterru
nning24hoursGPORfailedto
completeanyrunonanypartition.
ConsideringFigure2(b),whenitcomestoefciency,ISBORi
sfasterthanallalgorithms
exceptforSVOR,whichisimplementedinC.ComparingtoSBOR
,whichcanberegardedas
theofineversionofISBOR,thegapsbetweenISBORandSBORa
regettinglargerwiththesize
ofdataincreasing.On10000-sizedata,ISBORisabout10tim
esfasterthanSBOR.Theseresults
demonstratethatincrementallearningtogetherwiththesp
arsenessassumptioncanacceleratethe
trainingspeedofISBOR.Insummary,Figure2showsthatISBO
Rcanbeanefcientordinal
14
regressionalgorithmwhilepreservingacomparablepredic
tionaccuracytoSVOR.
FromthebottompartofTable3,wenoticethatonthesmalldat
asets,ISBORdoesnot
showanyadvantagesinrunningtime.However,onthelargeda
tasets,ISBORoutperformsthe
baselines.Specically,wecanseeatrendthatthelargersc
aleofthedatasetis,thebiggerthe
gapsbetweenISBORandthebatchalgorithmsare.Thistrendp
rovidesananswerto
RQ2
:the
incrementalsettingmakesISBORafasterORalgorithm.

5.2.3.Sparseness
Finally,weaddress
RQ3
.SinceGPORandKDORmakepredictionsbasedonalltraining
samples,inTable5,weonlyreportthenumberofsupportorre
levantsamplesofSVOR,ISVOR,
SBORandISBORsoastoanswerthesparsenessquestion(
RQ3
).
Table5:RelevantandsupportsamplesusedontheBenchmarkd
atasets.Bestresultsmarkedin
boldface
,secondbestin
italics
.
DatasetSVORISVORSBORISBOR
BS60.2283.3
9.0
17
SWD718.9454.1
104.8
58.5
Marketing3,756.010,185.1
51.3
51.1
Bank5,685.18,128.5
16.6
44.8
Computer3,373.17,739.5
3,056.1
30.9
CalHouse12,788.3
23,919.8
Œ
84.8
CensusŒ28,348.5
1,001.0
73.0
AnalyzingTable5,wenoticethatthesparseBayesbasedSBOR
andISBORemploymuch
smallernumbersoftrainingsamplestomakepredictionstha
ntheSVM-basedSVORandISVOR.
8
Amongthesevenbenchmarkdatasets,ISBORwins5timesandSB
ORwins2times,whichsup-
portsourclaimthatISBORisaparsimoniousordinalregress
ionalgorithmandcanmakeeffective
predictionsbasedonasmallsubsetofthetrainingset.This
ndinganswers
RQ3
onsparseness.
6.Conclusion
Wehavepresentedanovelincrementalordinalregressional
gorithmwithinanefcientsparse
Bayesianlearningframework.Insteadofprocessingthewho
letrainingsetinonego,thepro-
posedalgorithmcanincrementallylearnfromrepresentati
onsoftrainingsamplesandhaslinear
computationalcomplexityinthetrainingdatasize.Ouremp
iricalresultsshowthatIncremental
SparseBayesianOrdinalRegression(ISBOR)iscomparableo
rsuperiortostate-of-the-artOR
algorithmsbasedonbasisfunctionsintermsofefcacy,ef
ciencyandsparseness.
Wehopethatthisworkpavesthewayforresearchintolarge-s
caleordinalregression.We
believethatthedesignofISBORcanbeimprovedinmultipled
irections.FromaBayesianview-
point,amoreelegantwaytooptimizethehyper-parametersw
ouldbetomaximize
p
(

j
D
)rather
than
p
(
D
j

)withadditionalhyper-assumptions.Thisisachievablevi
aotherapproximationin-
ferencemethodslikevariationalBayesandexpectationpro
pagation[32,Chapter10].Froman
8
NoticehowISVORusesmoresamplesthanthegroundtruthprov
idesduetobinarydecomposition,asexplainedin
Section2.
15
applicationview,wecanequipISBORwithothersparseBayes
ianarchitecturesandadaptitto
otherproblemslikesemi-supervisedlearning[33,8,23]an
dfeatureselection[34,35].Froma
rankingviewpoint,higherpositionsaremoreimportant.So
far,ISBORignorespair-wiseprefer-
encesandconsiderseachpositionequallyimportant,which
amountstoapoint-wiseapproach.
Anotherpromisingfuturedirection,therefore,istotakep
air-wisepositioninformationintoac-
countandapplyISBORtorankingproblems.

Codeanddata
Tofacilitatereproducibilityoftheresultsinthispaper,
wearesharingthecodeandthedata
usedtoruntheexperimentsinthispaperat
https://github.com/chang-li/SBOR
.
Acknowledgments
Wethankouranonymousreviewersfortheirvaluablefeedbac
kandsuggestions.
ThisresearchwaspartiallysupportedbyAholdDelhaize,Am
sterdamDataScience,the
BloombergResearchGrantprogram,theChinaScholarshipCo
uncil,theCriteoFacultyRe-
searchAwardprogram,Elsevier,theEuropeanCommunity'sS
eventhFrameworkProgramme
(FP7/2007-2013)undergrantagreementnr312827(VOX-Pol)
,theGoogleFacultyResearch
Awardsprogram,theMicrosoftResearchPh.D.program,theN
etherlandsInstituteforSound
andVision,theNetherlandsOrganisationforScienticRes
earch(NWO)underprojectnrsCI-
14-25,652.002.001,612.001.551,652.001.003,andYandex
.Allcontentrepresentstheopinion
oftheauthors,whichisnotnecessarilysharedorendorsedb
ytheirrespectiveemployersand/or
sponsors.

References

References
[1]Z.Niu,M.Zhou,L.Wang,X.Gao,G.Hua,OrdinalRegressio
nwithMultipleOutputCNNforAgeEstimation,
in:2016IEEEConferenceonComputerVisionandPatternReco
gnition(CVPR),4920Œ4928,2016.
[2]Y.Xiao,B.Liu,Z.Hao,Multiple-InstanceOrdinalRegre
ssion,IEEETransactionsonNeuralNetworksandLearn-
ingSystemsPP(99)(2017)1Œ16.
[3]T.-Y.Liu,LearningtoRankforInformationRetrieval,F
oundationsandTrendsinInformationRetrieval3(3)(2009)
225Œ331.
[4]J.Hu,P.Li,CollaborativeFilteringviaAdditiveOrdin
alRegression,in:ProceedingsoftheEleventhACMInter-
nationalConferenceonWebSearchandDataMining,WSDM'18,
ACM,NewYork,NY,USA,243Œ251,2018.
[5]P.A.Guti´errez,M.Perez-Ortiz,J.Sanchez-Monedero,
F.Fern´andez-Navarro,C.Hervas-Martinez,OrdinalRe-
gressionMethods:SurveyandExperimentalStudy,IEEETran
sactionsonKnowledgeandDataEngineering28(1)
(2016)127Œ146.
[6]P.A.Guti´errez,P.Tino,C.Herv´as-Mart´nez,Ordin
alregressionneuralnetworksbasedonconcentrichypersph
eres,
NeuralNetworks59(2014)51Œ60.
[7]B.Gu,J.-D.Wang,Y.-C.Yu,G.-S.Zheng,Y.-F.Huang,T.X
u,AccurateOn-line

-SupportVectorLearning,Neural
Networks27(2012)51Œ59.
[8]M.P´erez-Ortiz,P.A.Guti´errez,M.Carbonero-Ruz,C.
Herv´as-Mart´nez,Semi-supervisedLearningforOrdinal
KernelDiscriminantAnalysis,NeuralNetworks84(2016)57
Œ66.
[9]F.Tang,P.Tino,OrdinalRegressionBasedonLearningV
ectorQuantization,NeuralNetworks93(2017)76Œ88.
[10]A.J.Smola,B.Sch¨olkopf,ATutorialonSupportVector
Regression,StatisticsandComputing14(3)(2004)199Œ
222.
[11]V.N.Vapnik,AnOverviewofStatisticalLearningTheor
y,IEEETransactionsonNeuralNetworks10(5)(1999)
988Œ999.
[12]W.Chu,Z.Ghahramani,GaussianProcessesforOrdinalR
egression,JournalofMachineLearningResearch6(Jul)
(2005)1019Œ1041.
16
[13]D.J.MacKay,TheEvidenceFrameworkAppliedtoClassi
cationNetworks,NeuralComputation4(5)(1992)
720Œ736.
[14]M.E.Tipping,SparseBayesianLearningandtheRelevan
ceVectorMachine,JournalofMachineLearningRe-
search1(Jun)(2001)211Œ244.
[15]M.E.Tipping,A.C.Faul,FastMarginalLikelihoodMaxi
misationforSparseBayesianModels,in:AISTATS,
2003.
[16]C.E.Rasmussen,GaussianProcessesinMachineLearnin
g,in:Advancedlecturesonmachinelearning,Springer,
63Œ71,2004.
[17]S.S.Keerthi,S.K.Shevade,C.Bhattacharyya,K.R.K.M
urthy,ImprovementstoPlatt'sSMOAlgorithmforSVM
ClassierDesign,NeuralComputation13(3)(2001)637Œ649
.
[18]W.Chu,S.S.Keerthi,SupportVectorOrdinalRegressio
n,NeuralComputation19(3)(2007)792Œ815.
[19]B.Gu,V.S.Sheng,K.Y.Tay,W.Romano,S.Li,Incrementa
lSupportVectorLearningforOrdinalRegression,
IEEETransactionsonNeuralNetworksandLearningSystems2
6(7)(2015)1403Œ1416.
[20]T.P.Minka,AFamilyofAlgorithmsforApproximateBaye
sianInference,Ph.D.thesis,Cambridge,MA,USA,
2001.
[21]P.Srijith,S.Shevade,S.Sundararajan,AProbabilist
icLeastSquaresApproachtoOrdinalRegression,in:Aus-
tralasianJointConferenceonArticialIntelligence,Spr
inger,683Œ694,2012.
[22]P.Srijith,S.Shevade,S.Sundararajan,Validation-b
asedSparseGaussianProcessesforOrdinalRegression,in:
InternationalConferenceonNeuralInformationProcessin
g,Springer,409Œ416,2012.
[23]P.Srijith,S.Shevade,S.Sundararajan,Semi-supervi
sedGaussianProcessOrdinalRegression,in:JointEuropea
n
ConferenceonMachineLearningandKnowledgeDiscoveryinD
atabases,Springer,144Œ159,2013.
[24]W.Chu,Z.Ghahramani,PreferenceLearningwithGaussi
anProcesses,in:Proceedingsofthe22ndInternational
ConferenceonMachinelearning,ACM,137Œ144,2005.
[25]X.Chang,Q.Zheng,P.Lin,OrdinalRegressionwithSpar
seBayesian,in:InternationalConferenceonIntelligent
Computing,Springer,591Œ599,2009.
[26]B.-Y.Sun,J.Li,D.D.Wu,X.-M.Zhang,W.-B.Li,Kerneld
iscriminantlearningforordinalregression,IEEE
TransactionsonKnowledgeandDataEngineering22(6)(2010
)906Œ910.
[27]T.J.Ypma,HistoricalDevelopmentoftheNewtonŒRaphs
onmethod,SIAMReview37(4)(1995)531Œ551.
[28]K.B.Petersen,M.S.Pedersen,etal.,TheMatrixCookbo
ok,TechnicalUniversityofDenmark7(15)(2008)510.
[29]J.F.P.daCosta,H.Alonso,J.S.Cardoso,TheUnimodalM
odelfortheClassicationofOrdinalData,Neural
Networks21(1)(2008)78Œ91.
[30]F.Wilcoxon,IndividualComparisonsbyRankingMethod
s,BiometricsBulletin1(6)(1945)80Œ83.
[31]J.Demsar,StatisticalComparisonsofClassiersove
rMultipleDataSets,JournalofMachineLearningResearch
7(Jan)(2006)1Œ30.
[32]C.M.Bishop,PatternRecognitionandMachineLearning
,Springer,2006.
[33]Y.Xiao,B.Liu,Z.Hao,AMaximumMarginApproachforSem
isupervisedOrdinalRegressionClustering,IEEE
TransactionsonNeuralNetworksandLearningSystems27(5)
(2016)1003Œ1019.
[34]B.Jiang,C.Li,H.Chen,X.Yao,M.deRijke,Probabilist
icFeatureSelectionandClassicationVectorMachine,
arXivpreprintarXiv:1609.05486.
[35]C.Li,H.Chen,SparseBayesianApproachforFeatureSel
ection,in:2014IEEESymposiumonComputational
IntelligenceinBigData(CIBD),1Œ7,2014.
17
"
15,A Simple Reservoir Model of Working Memory with Real Values,http://arxiv.org/pdf/1806.06545v1.pdf,https://github.com/anthony-strock/ijcnn2018,"ASimpleReservoirModelofWorkingMemory
withRealValues
AnthonyStrock
2
;
1
;
3
,NicolasP.Rougier
1
;
2
;
3
andXavierHinaut
1
;
2
;
3
;

1-INRIABordeauxSud-Ouest,Talence,France
2-LaBRI,UniversitédeBordeaux,CNRSUMR5800,Talence,France
3-IMN,UniversitédeBordeaux,CNRSUMR5293,Bordeaux,France

Correspondingauthor
Abstract
ŠTheprefrontalcortexisknowntobeinvolvedinmany
high-levelcognitivefunctions,inparticular,workingmemory.
Here,westudytowhatextentagroupofrandomlyconnected
units(namelyanEchoStateNetwork,ESN)canstoreandmain-
tain(asoutput)anarbitraryrealvaluefromastreamedinput,
i.e.canactasasustainedworkingmemoryunit.Furthermore,we
exploretowhatextentsuchanarchitecturecantakeadvantage
ofthestoredvalueinordertoproducenon-linearcomputations.
Comparisonbetweendifferentarchitectures(withandwithout
feedback,withandwithoutaworkingmemoryunit)showsthat
anexplicitmemoryimprovestheperformances.
Keywords
Š
WorkingMemory,GatedMemory,ReservoirComputing,
PrefrontalCortex,EchoStateNetworks,ESN
I.I
NTRODUCTION
Prefrontalcortex(PFC),noteworthyforitshighlyrecurrent
connections[9],isinvolvedinmanyhighlevelcapabilities,
suchasdecisionmaking[1],workingmemory[9],goal-
directedbehavior[23],temporalorganisationandreasoning
[7].Romoetal.[26]haveshownthatPFCneuronsofnon-
humanprimatecanmaintaininformationaboutastimulus
forseveralseconds.Theirratewascorrelatedwiththe
codingofadimension(frequency)ofthestimulus
maintainedinmemory.WhenMachensetal.[20]laterre-
analyzedthedataofthisexperiment,theyshowedthatthe
stimuluswasactuallyencodedoverasubpopulationusing
adistributedrepresentation.Similarly,whenRigottietal.
[25]analyzedsingleneuronactivityrecordedinthelateral
PFCofmonkeysperformingcomplexcognitivetasks,they
foundseveralneuronsdisplayingtask-relatedactivity.Once
theydiscardedalltheneuronsthatweredisplayingatask-
relatedactivity,theywerestillabletodecodetaskinfor-
mationwithalineardecoderandproposedthatthePFC
hostshigh-dimensionallinearandnon-linearmixed-selectivity
activity.Here,wecandrawaninterestingparallelbetweenthis
linear-decoderanalysisandthereservoircomputerparadigm
asoriginallyproposedby[3,5,14,19].Incomputational
neuroscience,reservoirsareoftenusedasmodelsofgeneric
neuralcircuits[13,19,28].Inparticular,severalauthorsused
themtomodelcorticalareassuchasPFC[5,6,10,21,11].
Becharaetal.[1]showedthatworkingmemoryanddecision
makingdependonseparateanatomicalPFCsubstrates.Corres-
pondingly,Dambreetal.[4]demonstratedtheexistenceofa
universaltrade-offbetweenthenon-linearityofthecomputa-
tionandtheshort-termmemoryintheinformationprocess-
ingcapacityofanydynamicalsystems,includingechostate
networks[14].Inotherwords,thehyperparametersusedto
generateanoptimalreservoirforsolvingagivenmemorytask
wouldnotbeoptimalforanon-linearcomputationtask.This
conclusionisquitepuzzlingwhenoneconsidersmodelssuch
aslongshort-termmemory(LSTM)networks[8,12]thathave
beenshowntosolvecomplextaskswithlong-termtemporal
dependencies
1
.However,thesemodelstakeadvantageofan
explicitgatingmechanisminsideeachunitinordertostore
valuesforlongperiodsoftime
2
.TheLSTMnetworkuses
explicitlyengineeredunitsthatenablestostoreavaluefor
longtimespansbytheuseofgatingmechanism.However,
thereisnoreasontothinkthatthebrainhassuchengineered
mechanisms,especiallybecausethelearningalgorithm,back
propagationthroughtime(BPTT)[30],isunfoldingtime:this
wouldbelikeifthebraincouldvirtuallyduplicateitstate
values(forhundredsormoretimepoints)inordertolearna
particulartime-dependencybetweentwoeventsforinstance.
Withthisstudy(andfutureones)wewanttoexplorehowthis
gatingmechanismcouldbeperformedwithoutsuchengineered
mechanisms,andexplorehowitcouldbeperformedwith
populationsofneuronswithlessaprioriconstraints(e.g.
randomRNNs).
Inthemeantime,Jaeger[15]exploredthecapabilitiesofan
ESNtomaintaintemporalinformationduringlongtimespans
andtoexploititforsolvingtasks.Heconsideredthe
suiteofsynthetictasksoriginallyproposedbyHochreiterand
Schmidhuber[12]inordertotesttheirLSTMmodel.These
synthetictaskswerealsoreusedbyMartensandSutskever
[22]usinganHessian-freeoptimizationofRecurrentneural
networks(RNN).Jaeger[15]showedthatacorrectlyde-
signedESNcouldhandlesuchtaskswiththesamesuccess
criteriaoriginallyproposedbyHochreiterandSchmidhuber
[12].However,thestudywaslimitedtotransientshort-term
memory,andnotonﬁattractor-likeﬂworkingmemorymech-
anisms.Conversely,otherstudieshavefocusedonreservoirs
withdedicatedoutputsactingasworkingmemory(WM)units
[13,24],whichexploitdifferentsub-regionsofthereservoir
high-dimensionalspaceasﬁattractorsﬂ[29].Theseworking
memoryunitsweretrainedtostorebinaryvaluesthatwere
input-dependent;theseWMunitshadfeedbackconnections
projectingtothereservoir.ThankstotheseWMunits,it
enabledthereservoirtoaccessandusesuchinformation,
freeingthesystemtorelyonlyonthereservoirshort-term
dynamicstomaintainWMinformation.PascanuandJaeger
1
Long-termtemporaldependenciesimplynon-linearcomputationsifunits
havenon-linearactivationfunction.
2
Thisdoesnotactuallyrefutethetrade-offproposalby[4].
arXiv:1806.06545v1  [q-bio.NC]  18 Jun 2018[24]useduptosixbinaryWMunitstostoreinformationin
ordertosolveanestedbracketinglevelstask.WithaPrincipal
ComponentAnalysisPascanuandJaeger[24]showedthatsuch
binaryWMunitsdrivethereservoirinlowerdimensional
ﬁattractorsﬂ
3
.Additionally,Hoerzeretal.[13]showedthat
analogWMunits(butencodingabinaryinformation)actually
drivethereservoirintoalowerdimensionalspace(i.e.99%
ofthevariabilityofthereservoiractivitiesareexplainedby
fewerprincipalcomponentswhenthereisfeedbackfromWM
unitstothereservoir).Inthisstudy,wewanttoextrapolate
thisideaofﬁattractordrivenbyWMunitsﬂtoWMunitsthat
havecontinuousvaluesandnotconstrainedtoapopulationof
binaryvalues.
Thepaperisorganizedasfollows:SectionIIintroducesthe
differenttasksandthereservoirdesignwhileSectionIII
introducedresultsonthedifferenttasks.Adiscussionofthe
studyisgiveninSectionIV.
II.M
ETHODSANDEXPERIMENTS
Inordertobetterunderstandthepropertiesofworkingmemory
mechanisms,weexplore
4
theideaofareservoirmodelcon-
nectedtoworkingmemory(WM)units.More,we
areinterestedinthecapacityofareservoirtostoreanarbitrary
information(e.g.arealvalueinsteadofbinaryvalueslike
in[13,24])fromastreamedinputbyusingatrainedgating-
likemechanism.AspicturedinTableII,wewillexploresix
versionsofasimplearchitecturecomposedofanEchoState
Network(ESN)andoptionallyaWMunitand/oranoutput
containingtheresultofanon-linearcomputation.Ina
task,weconsideramodelwithtwoinputs:randomvaluesin
[-1,1]changingovertimeandtriggerevents,signalingwhento
storeinformation.Theoutputshouldbeclampedtothevalue
givenwiththelasttriggerevent:thisformsacontinuousWM
unitthatwillchangeitsvalueeachtimeanewtriggerisgiven.
Inasecondtask,westudytheusefulnessofsuchWMunitin
ordertosolveanonlineartask:multiplicationoftheinputby
thememorizedvalue.
A.Model:EchoStateNetwork
InthisworkweuseanEchoStateNetwork(ESN)[14]
withleakyneurons,andfeedbackfromoutputtothereservoir
describedbythefollowingupdateequations:
x
[
n
]=(1


)
x
[
n

1]+

~
x
[
n
]
(1)
~
x
[
n
]=tanh(
Wx
[
n

1]+
W
in
[1;
u
[
n
]]+
W
fb
y
[
n

1])
(2)
y
[
n
]=
f
(
W
out
[1;
x
[
n
]])
(3)
where
u
[
n
]
,
x
[
n
]
and
y
[
n
]
arerespectivelythestreamedinput
vector,thevectorcontainingthereservoiractivations,andthe
outputvectorattime
n
.
W
,
W
in
,
W
fb
and
W
out
arerespectively
therecurrent,theinput,thefeedbackandtheoutputweight
matrices.
[
:
;
:
]
standsfortheconcatenationoftwovectors,
tanh
(hyperbolictangentfunction)and
f
(linearorpiece-wiselinear,
3
TheWMunitsdrivethereservoirtoparticularsub-regionsofthehigh-
dimensionalspace.TheseWMunitsactasﬁattractorsﬂoftheentiresystem,
ie.ifnoparticularinputisseen,thesystemstaysinthesamesub-region.
4
Thecodeandthedetailsonthehyperparametersareavailableat
https://github.com/anthony-strock/ijcnn2018
theidentityfunctionclampedto

1
in]

,-1]andto
1
in
[
1
;
+
1
[)
5
areappliedelement-wiseand

istheleakingrate.
Thematrix
W
israndomlyuniformlysampledbetween

0
:
5
and
0
:
5
andthenitisrescaledinordertosetitsmaximal
absoluteeigenvalue,a.k.aspectralradius,tothechosenone.
Thematrices
W
in
and
W
fb
arebothsampleduniformlybetween
avalue
s
anditsopposite

s
.Thesetwovaluesarerespectively
calledtheinputandthefeedbackscaling.Inalltheresults,the
reservoircontains100neurons.
AsillustratedintheofTableII,onlytheoutputweights
willbelearned;input,recurrentandfeedbackweightare
generatedrandomlyandkepted.Theoutputweightsare
learnedusingridgeregressionwithteacherforcing[18],like
statedinEquation4.Whentherearetwooutputweightsto
traintheyarelearnedbothatthesametimeconsideringthe
outputvectortobe2-dimensional.
W
out
=
YX
T
(
XX
T
+
I
)

1
(4)
where
X
istheconcatenationofthereservoiractivitiesatall
timestepswithabiasvectorat1,eachrowcorrespondingtoa
timestep.Similarly,
Y
istheconcatenationofdesiredoutputs
and

istheregularizationparameter.
B.Task1:Storingatriggeredrealvalue
Forthistask,themodelusedistheﬁMemoryonlyﬂ
architecture(seeTableII).Anexampleoftheinputsand
outputsisshowninFigure2.
Inthistask,themodelreceivestwodifferentkindsofinputs:an
inputthatindicatesthevaluetobemaintainedwhenatrigger
occurs,andaninputthatindicatesthetriggers.Beforethe
triggerevent,thevaluetobestorediszero.Ascanbeseenin
Figure1threescenarioswerestudied.
Fig.1:Threescenariosonwhichthetrainedmodelsaretested.
A.Singleupdatescenario.B.Periodicupdatescenario.C.
Continuousupdatescenario.Eachcolumnrepresentsatime
step.Firsttwolines:inputcomposedofthevaluesandthe
trigger.Bottomline:desiredoutput.
C.Task2:Productoftriggeredvalueandcurrentvalue
Herewestudyavariantoftheprevioustaskwheretheoutputis
acombinationofthestoredvalueandthecurrentinputsignal.
Theinputsarekeptthesame.Ateachtimestep,thereadout
unithastooutputtheproductoftwovalues:thevalueof
thecurrenttimestepandthevaluesynchronizedwiththelast
trigger(i.e.thestored/memorizedvalue).Asbeforewhenno
5
f
ischosenpiece-wiselinearonlyforTableIandFigure3
triggersoccurredyet,thelastvaluetobememorizedshould
bezero,andsodoestheproduct.
Themotivationtostudythistaskisasfollows.Ifthepreviously
triggedvalueisavailabletothereservoir,themultiplication
taskistrivial(suchacaseiscalled
Oracle
).However,if
thispreviouslytriggeredvalueisnoteasilyavailable,thetask
becomesincreasinglycomplicatedastimeruns,especiallyfor
areservoirofonly100units.Inordertoexploreandcompare
thecapabilitiesindifferentarchitectures(sixintotal,see
TableII),wemakeacombinationofthreedifferentscenarios:

Noexplicitmemory:Themodelistrainedonlyto
performthistaskwithoutanyfurtherhelp.

Trainedexplicitmemory:Themodelistrainedatthe
sametimetosolvetheproducttaskandthegating
task.

Oracleexplicitmemory:Themodelhastoperform
theproducttaskandthetriggeredvalueisgivenasa
supplementaryinputbyanoracle.Onlythecapability
tocomputetheproductistestedinsuchcase.
D.Trainingdata
Thetrainingdataconsistsof100inputsequencesof100time
stepsthatwererandomlysampledinthefollowingway:

Value
isobtainedbysamplingauniformlydistributed
valuebetween-1and1;

Trigger
isobtainedbysamplingavaluebeingeither
0or1.Whenthetriggeris1,wecallitatriggerevent.
Intotal5triggereventsoccurandtheyareuniformly
sampledbetweentimestep30andtimestep49.
Forallexperiments,weusedthesametrainingdatathatwas
sampledoncewhensearchingforgoodhyperparameters.For
theprincipalcomponentanalysis,thetrainingdataconsisted
of100inputsequencesof10,000timestepssimilartothe
previousone.Thedifferenceisthatthereisaconstrainton
thedelaybetweentwotriggers.InFigure4thereareexactly
200timestepsbetweentriggerswhereasinFigure6thereare
between100and200timestepsbetweentriggers,thedelay
betweentriggerswassampleduniformly.
E.Testdata
Ifnotstateddifferently,thetestsequencesweregeneratedwith
thesameguidelinesasthetrainingdata.Theyconsistof100
otherinputsequencesof100timesteps.
For
task1
,wetestedthetrainedmodelonfourdifferenttest
sets.Wetestedourmodelondatawithsimilarstatistics
thanthetrainingset.Wenameditthe""training-likescenario"".
For
task2
,forallvariants,wetestedthetrainedmodelonthe
same""training-likescenario"".Moreover,inordertounderstand
howtheworkingmemoryofthemodelworkedin
task1
,we
testeditonthreeotherscenariosforwhichthenetworkwasnot
trained,inordertotesttheabilitiesofthesystemtogeneralize
overdifferentstatisticalinputs.Thesescenarioscanbeseen
inFigure1.Theyareparticularlychallenginginthesense
thatthemodelhasnotbeentrainedwithsolonginputs.Each
scenariowasbuiltusing100inputsequencesof10,000time
stepsgeneratedinthefollowingway:

Value
isobtained,asbefore,bysamplingauniformly
randomlydistributedvaluebetween-1and1;

Trigger
isobtained,unlikebefore,bychoosingby
handwhenthetriggeroccurs.Forthesingleupdate
scenario(A)thetriggeroccursonlyattimestep100.
Fortheperiodicupdatescenario(B)thetrigger
occursattimestep100andthenatriggeroccursevery
1000timestep.Finally,forthecontinuousupdate
scenario(C)thetriggeroccursattimestep100
andtriggerswilloccurateverytimestepafterthat.
Inordertogenerateanunderstandableviewofhowthetask
isbehaving,wecreatedtwoextrascenariosfortheFigures2,
5.Forboth,wehavesmoothedtheinputsbyconvolvingthe
randomsignalbyanexponentialwindowofwidth10and
decay5.For
task1
theinputsareinputsequencesof100
timestepswhereasfor
task2
,inordertounderstandhowthe
memorywasusedintheTrainedexplicitmemoryarchitecture,
theinputsareinputsequencesof200timesteps.Allthe
otherresultsaregivenforrandominputsvalueswithoutany
smoothing.
F.Evaluationprocedure
Wetooktwodifferentevaluationprocedures.Forbothtasks,
whileusingthetraining-likescenariowesearchedforbest
hyper-parametersinordertosolvethetask;weusedthe
hyperopt
Pythontoolbox[2].
First,forallarchitectures,inordertobeablenotonlyto
betterhyper-parametersbuttoextractinformationonthe
ofhyper-parametersonthetaskwedecidedtousea
randomsearchfor1000hyper-parameters,the200ones
beingchosenpurelyrandomlyandthefollowingoneusing
aBayesianapproachcalledTree-structuredParzenEstimator
(TPE)
6
.InordertoevaluatetheperformanceoftheESN,in
thiscase,wegenerated10instancesofthemodelusingthe
sampledhyper-parameters.WetrainedalltheseESNsonthe
sametrainingdata(seesubsectionII-D).
Secondly,inthethreeotherscenarioson
task1
,wepickedthe
besthyper-parametersfoundforthistask
4
.Thistimeinorder
toevaluatetheperformanceoftheESNoneachscenariowe
generated100instancesofthemodel.Oncetrained,wetested
allinstancesonthe3lastscenarios(seesubsectionII-E).The
followingisperformedthesamewaythaninthetraining-like
scenario.
Beforefeedinganewinputsequencetoareservoirweini-
tializedallitsactivationstozero.Theerrorweconsiderata
giventimestepistheabsolutedifferencebetweentheproduced
outputandthedesiredoutput.Theglobalperformancecriterion
weconsideristheRootMeanSquareError(RMSE).
III.R
ESULTS
A.Task1:Storingatriggeredrealvalue
First,westudiedtheglobalperformanceofthemodelrelatively
tothetask.TableIreportsagoodgeneralizationonthe
6
Wesampledfromaverylargerangeofvalues:wesampledlog-uniformly
thespectralradius(between1e-10and1e10),theleakingrate(1e-10,1.0),the
input(resp.feedback)scaling(1e-10,1e10)andtheregularizationparameter
(1e-15,1e-1).
ScenarioRMSE(Mean

Std)
Training-like2.28e-4

1.90e-4
Single(A)4.55e-2

4.19e-2
Periodic(B)5.04e-3

4.17e-3
Continuous(C)9.25e-5

3.66e-5
TABLEI:Globalperformancesforthefourscenariosfor
task1:Mean,standarddeviationoftheRootMeanSquare
Error(RMSE).
Fig.2:Behaviorofthemodeltrainedtoperform
task1
(gating
task)inavisuallyunderstandablecase.Themodelistrained
withtheuniformrandominputsasvaluesandtestedona
smoothedversionofauniformrandomsignal.
Value
:values
receivedbythemodel,inthisexampletheyareobtainedby
withanexponentialwindowofwidth10anddecay5
auniformrandomsamplebetween-1and1.
Trigger
:triggers
receivedbythemodelobtainedinthesamewaythanin
thetrainingscenario.
Memory
:thedesiredoutputisinblue
whichstartsafterthewarm-up,andtheproducedoutputbythe
modelindottedred.
Error
:theabsolutedifferencebetween
thedesiredandproducedoutputinlogscale.Inputsshownare
differentfromwhatthenetworkhasbeentrainedon:theyare
pseudo-randominsteadofuniformlyrandominordertogive
amorecomprehensiblevisualization.
training-likescenariowithameanRMSEof1.63e-4(

1.14e-
4)andthegeneralizationremainsgoodforalltheother
scenarios.Inallcases,theRMSEisequalorsmallerthan
5e-2.Thesingleupdatescenario(A)(RMSEabout5e-2)is
harderforourmodelthantheperiodicupdatescenario(B)
(RMSEaround5e-3)which,inturn,isharderforourmodel
thanthecontinuousupdatescenario(C)(RMSEaround1e-4).
Thenweanalyzedhowwasevolvingtheerrorovertime.In
Figure2weshowanexampleoftheevolutionoftheoutput.
Wecannotetwopropertiesonthistheerrorremains
Fig.3:Statisticsontheevolutionoftheerrorovertime
forthreetestingscenarios(task1).Median(blackline),5th
percentile(bottomedgeofthegraysurface)and95thpercentile
(topedge)oftheabsoluteerrorvaryingovertime.A.Single
updatescenario.B.Periodicupdatescenario.C.Continuous
updatescenario.Theredverticallinesrepresentthetriggers,
theconstantupdatebeingshownbyahorizontalreddotline.
stablearound1e-4duringthewholerunanditevengoes
downto1e-4duringwarmupwaybeforeithasbeentrained
tobe.Thelattermightsuggestthatasmallerwarmupcould
beused.Averylightdriftseemsalsotoappear:wecreated
thescenariosA,B,andCinordertostudymoreextensively
thisdrift.Inthefollowing,wefocusonthevariationofthe
errorovertimeshowninFigure3.Inallcases,after10,000
timesteps,theerrorisfarbelow1e-1.Moreover,onecan
seethatafteratriggerevent,theerrorgetsaround1e-4,and
in1,000stepstheerrorhasnoteventhetimetoreach5e-2.
Furthermore,inscenarioB,themeanerror10timestepsafter
atriggeris1.51e-04(

7.87e-04),andfor100timesteps,itis
8.88e-04(

1.14e-03).
Tosummarize,wecannoticeaconstantincreasingdriftof
theoutputwhenthereisnotrigger(A).However,thisdrift
canbeboundedthankstoaperiodictriggerthatfeedanew
valueintothereservoir(B),thefastertheupdatethelowerthe
bound(C).Fortasksforwhichthegatedvaluehastobekept
foraverylongamountoftimewithhighprecision(andif
onedonotwanttoincreasethereservoirsizeof100units),a
simplecountingmechanismcouldprovidethisperiodictrigger.
Thiscouldprobablybeimplementedwiththesamereservoir.
Inordertounderstandhowthememorizedvalueisencoded
inthereservoir,weperformedaprincipalcomponentanalysis
(PCA)onthereservoirstates.SimilarlytoPascanuandJaeger
[24]andHoerzeretal.[13],thisPCAanalysisshowsthatthe
dynamicevolvesinalowdimensionalspacewheretwocompo-
nentsexplainmorethan99%ofthevariance.Infact,eventhe
Fig.4:Task1,
Memoryonly
architecture.
Left
Evolutionofthevaluememorizedintheworkingmemoryunitalongthetwo
principalcomponentsthatexplainmorethan99%ofthevarianceoftheinternalactivities.FirstPCiscorrelatedwithtime:we
canseethesequenceofthememorizedvalues.
Right
Evolutionofthevaluememorizedintheworkingmemoryunitalongthe
principalcomponentsthatarethemostcorrelatedwiththememorizedvalue.
componentaloneexplainsmorethan89%ofthevariance.
However,thetwocomponentsdonotseemdirectlyrelated
tothestoredvalueintheWMunit.Figure4(Left)shows
theevolutionofthereservoirforainputsequence.In
thiswecanseethatthecomponentiscorrelated
withtheevolutionoftimeasitkeepsincreasingwithtime,the
correlationwithtimeisactuallynear1.Thesecondcomponent
doesn'tcontaindirectlythememorizedvaluebutit'sdiscrete
derivative
7
does:thecorrelationbetweenitsdiscretederivative
andthememorizedvalueisnear1.Combiningthetwo,we
canseethatonthetwocomponentsthedynamicevolves
piece-wiselinearlyandthememorizedvalueisactuallythe
slopeofthelines.Inordertocomponentsthatcould
explainbetterhowisstoredthememorizedvalue,wealso
watchedatthecomponentsthatwerethemostcorrelatedwith
thememorizedvalue.Notsosurprisinglythatwasnotatallthe
oneofbeforeasthememorizedvalueisnotlinearlyencoded
inanyofthem.Thesecomponentsexplainnotmuchaboutthe
variancebut,aswecanseeinFigure4(Right),thesamekind
ofphenomenaappears.Whenagivenvalueisstored,these
componentsevolveonaline,butnowthat'snottheslopethat
containsthememorizedvaluealltheslopearesimilar,that's
theoffsetoftheline.Moreover,wecannotethatinthese
componentsthevaluesstoredinmemoryarebetterdistributed
inthespace.Whenanewvalueisstoredthesecomponents
jumpsinanotherplace.Onecannoticealinearcolorgradient
alongtheaxisperpendiculartothelines.So,similarlythan
in[20]forneuralrecording,thecomponentsthatexplainthe
bestthevariancemightnotbethebestonestoexplainhow
thememorizedvalueisencoded.
B.Usingaworkingmemoryunittosolveacomplextask
Giventheresultsfromtask1,weknowthatwehaveaunitable
tostorearealvalue.Inthefollowing,westudiedhowsuch
abilitycanhelptoperformanothertask:outputtheproduct
betweenthecurrentinputandapreviousinputindicatedbya
trigger.
First,wetriedtoanswer3questionsandtheresultsweused
toprovideanswersaresummarizedinTableII:

Doesthepresenceofanexplicitmemoryhelp?
The
meanRMSEobtainedwithoutexplicitmemoryisat
least3orderofmagnitudeabovetheonesusinga
workingmemoryunit.Moreover,themeanRMSEin
thelattercaseisat7.26e-4

1.88e-4,sothetaskis
actuallyperformedquitewell.Thus,havingaworking
memoryunitgreatlyhelps.

Howdoesthetrainingbetweenmemoryandproduct
interfere?
Toanswerthisquestion,wereplacedthe
WMunitwithanOracle.Thedifferencebetweenthe
architecturewiththeWMunitcomparedwiththe
Oracleislowerthanoneorderofmagnitude:7.26e-
4

1.88e-4against1.99e-4

3.15e-5fortheOracle.
Therefore,thefactoftrainingtheworkingmemoryat
thesametimeastheproductoutputdoesnotseemto
interferemuchwiththeperformanceobtainedonthe
product.

IsthefeedbackhelpingwhenthereisnoWMunit?
Inthetwocasesstudied,thefactofhavingornota
feedbackdidnotchangequalitativelytheresults.
Afterward,westudiedthebehaviorofthemodel.Here,we
focusonthe""Trainedexplicitmemory""architecture:with
7
Wecalldiscretederivativeofavaluethatevolvesinadiscretetime,the
evolutionofthedifferencebetweenthevalueatatimeandattheprevious
time.
TaskArchitectureRMSE
Memory
only
1.55e-4

7.42e-5
No
explicit
memory
3.03e-1

4.53e-4
No
explicit
memory
(NoFB)
3.05e-1

3.67e-4
Trained
explicit
memory
7.26e-4

1.88e-4
Oracle
explicit
memory
1.99e-4

3.15e-5
Oracle
explicit
memory
(NoFB)
7.10e-5

2.65e-5
TABLEII:Summaryoftheperformanceforthebesthyper-
parametersfoundbyBayesianoptimization.Thenumberof
neuronswasedto100.
Fig.5:Illustrationofthebehaviorofthemodeltrainedto
performtheproducttaskinamoreunderstandablecase.
Value
:
valuesreceivedbythemodel.
Trigger
:triggersreceivedbythe
model.
Memory
:twocurvessuperposed,thedesired(working)
memoryoutputinbluewhichstartsafterthewarm-up,andin
dottedredthememoryoutputobtained.
Product
:twocurves
superposed,inbluethedesiredproductoutput(i.e.theproduct
ofthedesiredmemoryandthevaluereceived),andindotted
redtheproductoutputobtained.
Productbymemory
:two
curvessuperposed,inbluethedesiredproductoutputifithad
tobeconsistentwiththememorystored(i.e.theproductof
theactualmemoryandthevaluereceived)andinreddotted
theproductoutputobtained.
Error
:theabsolutedifference
betweenthedesiredandproducedoutput(inlogscalefor
Memory).Eachplotcorrespondstotheerroroftheplotjust
above.Thedesiredoutputandtheerrorareshownafterthe
warm-upthathasbeenusedduringtraining.Theverticalred
dotlineisheretorepresentthetimestepafterwhichthemodel
hasnotbeentrainedtoperformthetask.(Inputsshownare
differentfromwhatthenetworkhasbeentrainedon.)
Fig.6:Task2,
Trainedexplicitmemory
architecture
Left
Evolutionofthevaluememorizedintheworkingmemoryunitalong
theandthethirdprincipalcomponents(PC).ThePCiscorrelatedwithtime:wecanseethesequenceofthememorized
values.
Right
EvolutionofthevaluememorizedintheworkingmemoryunitalongthePCsoftheinternalactivitiesthatarethe
mostcorrelatedwiththememorizedvalue.Inordertobetterseethetemporalevolution,thedotsatatimestepisconnectedto
theoneattheprevioustimestep,exceptinthecasewherethemodelreceivesatrigger(inordertoavoidmeaninglessartifacts).
feedbackfromboththeoutputandtheworkingmemoryunit.
InFigure5,weshowhowthemodelbehaveswith
randominputvalues.Onthe100timesteps,the
modelcankeepaworkingmemoryunitstableasithasbeen
trainedtoandperformtheproductwithaverysmallerror.
Bycomparingtheerrorinthecurvesbelowthe""Product""and
""Productbymemory""curves,wecanseethatthevalueused
toperformtheproductistheactualstoredvalueandnotthe
theoreticalvalue(thatshouldhavebeenstored).Themodel
seemstousetheworkingmemoryunitaswewouldexpectit
todo.Thisissimilartowhat[13]haveshown,butwithreal
valuesinsteadofbinaryvalues(orswitches).
Finally,westudiedtheevolutionoftheinternalactivitiesof
ourmodel:asbefore,toreducethedimensionalityaprincipal
componentanalysis(PCA)hasbeenperformedonthereservoir
states.Againasin[13],thetwoprincipalcomponents
containalotofinformation:heretheycanexplain99.8%of
thevarianceoftheactivations.However,inthiscase,the
componentalreadyexplainsmorethan96%ofthevariance
(comparedto89%for
task1
),andthusexplainsevenmore
variabilitythanearlier.Asbefore,thecomponentislinked
totime,butherethecorrelationisnon-linear.However,the
linkbetweenthememorizedvalueandthesecondcomponent
islessclearthanbefore.AsshowninFigure6(Left),with
thethirdcomponentwecanseesomethingsimilar(evenif
noisier)tothesecondcomponentofthepreviousPCAanalysis
(Figure4,Left).Ontheandthirdcomponents,theactivity
nearlyevolvesaslinesandtheslopeofthislinedependson
thevaluestored:whenthememorizedvalueisnegative(red,
orange)itdecreases,whenitispositiveitincreases(green,
blue).Inthiscase,wealsowatchedthetwocomponentsthat
werethemostcorrelatedwiththememorizedvalue.Asbefore,
theactivityseemstoevolvenoisilyonparallellinesdepending
onthevaluestored,theblue(resp.red)lines,corresponding
topositive(resp.negative)memorizedvalues.
IV.D
ISCUSSION
Wehaveshownhowasmallgroupofrandomlyconnected
unitsisabletomaintainanarbitraryvalueatanarbitrary
timefromastreamedinput.Itistobenotedthatthemodel
hasnotbeentrainedatmemorizing(byheart)everypossible
valuesincethereisvirtuallyannumberofvalues
between-1and+1.Whatthemodelhasactuallylearnedis
togateaninputvalueintoaplaceholder,a.k.a.aworking
memoryunit.Thispropertyofthemodelcanbeconsidered
asagatedmemory:avalueentersthememoryatthemoment
ofthe(input)triggerandiskeptconstantinfaceofincoming
distractors(thecontinuousstreamedinput).Suchrobustnessis
actuallycharacteristicofagatedworkingmemory:information
enterswhilethegateisopenedandiskeptconstantonce
thegateisclosed.Inthatregard,itistobenotedthat
previousworkshaveaddressedthequestionbutindifferent
ways.LimandGoldman[17]haveshownthatabalanced
corticalmicrocircuitrycangiveaccountofthemaintenance
ofinformation.Themodelusesacorrectivenegativefeedback
thatmakesitrobustagainstcommonperturbationbutrequires
abalancedamountofexcitationandinhibition.Similarly,
Sternetal.[27]explainshowrandomneuralnetworkscan
exhibitabi-stablebehaviorusingstronglocalconnectivityand
randominter-unitconnections.Inanotherkindofrecentwork
Jaeger[16]use
conceptors
toprojectanddrivethedynamics
inasub-spaceoflowerdimension.
Wehavealsodemonstratedhowsuchexplicitworkingmemory
iscriticalinsolvingamultiplicationtask.Probablythemost
interestingpointinthistaskisthefollowing:thesamemodel
deprivedoftheexplicitworkingmemoryfailsatsolvingthe
taskandexhibitsverybadperformances(errorof3.03e-1
insteadof7.26e-4).Thisdemonstratesthecriticalityofthe
presenceoftheworkingmemoryunit.Interestingly,these
resultsarecoherentwiththeDambreetal.[4]trade-off
hypothesis:theﬁNoexplicitmemoryﬂmodelperformsworse
thanthemodelswithWMunitsbecauseitsdynamicsarenot
abletoperformthenon-linearcomputationsandthelong-term
memorizationatthesametime.
Inthisstudy,wedidnottrytotheoptimalnumberof
reservoirunitsneededforeachtask.Conversely,wevoluntary
limitedthesizeofthereservoirto100neuronsinordertosee
ifsuchrathersmallreservoirsweresufcompetitive.
Moreover,eventhoughwehaveoptimizedthehyperparameters
ofthemodelinordertothebestperformances,therandom
natureofthenetworksuggeststhatsuchworkingmemory
propertyisanintrinsicpropertyofany(recurrent)groupof
neuronsundersomeconditions(size,spectralradius,leak
rate)
8
.Furthermore,eventhoughitwasnotthemaingoalhere,
thisstudyisastepinexploringtheplausibilityofhaving
anetworkofsuchsmallgatedreservoirsbeinginterconnected
together,similarlyasLSTMcellsinanLSTMnetwork.
R
EFERENCES
[1]
ABechara,HDamasio,DTranel,andSWAnderson.
Dissociationofworkingmemoryfromdecisionmaking
withinthehumanprefrontalcortex.
Journalofneuro-
science
,18(1):428Œ437,1998.
[2]
JBergstra,DYamins,andDDCox.Hyperopt:Apython
libraryforoptimizingthehyperparametersofmachine
learningalgorithms.In
Proceedingsofthe12thPython
inScienceConference
,pages13Œ20.Citeseer,2013.
[3]
DVBuonomanoandMMMerzenich.Temporal
informationtransformedintoaspatialcodebyaneural
networkwithrealisticproperties.
Science
,267(5200):
1028Œ1030,1995.
[4]
JDambre,DVerstraeten,BSchrauwen,andSMassar.
Informationprocessingcapacityofdynamicalsystems.
reports
,2:514,2012.
[5]
PFDominey.Complexsensory-motorsequencelearning
basedonrecurrentstaterepresentationandreinforcement
learning.
Biologicalcybernetics
,73(3):265Œ274,1995.
[6]
PEnel,EProcyk,RQuilodran,andPFDominey.
Reservoircomputingpropertiesofneuraldynamicsin
prefrontalcortex.
PLoScomputationalbiology
,12(6):
e1004967,2016.
[7]
JMFuster.TheprefrontalcortexŠanupdate:timeis
oftheessence.
Neuron
,30(2):319Œ333,2001.
[8]
FAGers,JSchmidhuber,andFCummins.Learning
toforget:ContinualpredictionwithLSTM.
Neural
computation
,12(10):2451Œ2471,2000.
[9]
PSGoldman-Rakic.Circuitryofprimateprefrontal
cortexandregulationofbehaviorbyrepresentational
memory.
ComprehensivePhysiology
,1987.
[10]
XHinautandPFDominey.Real-timeparallelprocessing
ofgrammaticalstructureinthefronto-striatalsystem:
Arecurrentnetworksimulationstudyusingreservoir
computing.
PloSone
,8(2):e52946,2013.
[11]
XHinaut,FLance,CDroin,MPetit,GPointeau,andPF
Dominey.Corticostriatalresponseselectioninsentence
production:Insightsfromneuralnetworksimulationwith
reservoircomputing.
Brainandlanguage
,150:54Œ68,
2015.
8
Wechoosesomeparticularhyperparametersamongmanygoodenough
ones:eachparameterseemsrobustinaquitesubstantialrangeofvalues.
[12]
SHochreiterandJSchmidhuber.Longshort-termmem-
ory.
Neuralcomputation
,9(8):1735Œ1780,1997.
[13]
GMHoerzer,RLegenstein,andWMaass.Emergence
ofcomplexcomputationalstructuresfromchaoticneural
networksthroughreward-modulatedhebbianlearning.
Cerebralcortex
,24(3):677Œ690,2012.
[14]
HJaeger.Theﬁechostateﬂapproachtoanalysingand
trainingrecurrentneuralnetworks.
Bonn,Germany:
GMDTechnicalReport
,148(34),2001.
[15]
HJaeger.Longshort-termmemoryinechostatenet-
works:Detailsofasimulationstudy.Technicalreport,
JacobsUniversityBremen,2012.
[16]
HJaeger.Controllingrecurrentneuralnetworksby
conceptors.
arXivpreprintarXiv:1403.3369
,2014.
[17]
SLimandMSGoldman.Balancedcorticalmicrocir-
cuitryformaintaininginformationinworkingmemory.
NatureNeuroscience
,16(9):1306Œ1314,aug2013.
[18]
MLukoıevi

cius.Apracticalguidetoapplyingechostate
networks.In
Neuralnetworks:Tricksofthetrade
,pages
659Œ686.Springer,2012.
[19]
WMaass,TNatschläger,andHMarkram.Real-time
computingwithoutstablestates:Anewframeworkfor
neuralcomputationbasedonperturbations.
Neuralcom-
putation
,14(11):2531Œ2560,2002.
[20]
CKMachens,RRomo,andCDBrody.Functional,
butnotanatomical,separationofﬁwhatﬂandﬁwhenﬂin
prefrontalcortex.
JournalofNeuroscience
,30(1):350Œ
360,2010.
[21]
FMannellaandGBaldassarre.Selectionofcortical
dynamicsformotorbehaviourbythebasalganglia.
Biologicalcybernetics
,109(6):575Œ595,2015.
[22]
JMartensandISutskever.Learningrecurrentneural
networkswithhessian-freeoptimization.In
ICML-11
Proceedings
,pages1033Œ1040.Citeseer,2011.
[23]
EKMillerandJDCohen.Anintegrativetheoryofpre-
frontalcortexfunction.
Annualreviewofneuroscience
,
24(1):167Œ202,2001.
[24]
RPascanuandHJaeger.Aneurodynamicalmodelfor
workingmemory.
Neuralnetworks
,24(2):199Œ207,2011.
[25]
MRigotti,OBarak,MRWarden,X-JWang,ND
Daw,EKMiller,andSFusi.Theimportanceofmixed
selectivityincomplexcognitivetasks.
Nature
,497(7451):
585,2013.
[26]
RRomo,CDBrody,AHernández,andLLemus.
Neuronalcorrelatesofparametricworkingmemoryin
theprefrontalcortex.
Nature
,399(6735):470,1999.
[27]
MStern,HSompolinsky,andLFAbbott.Dynamics
ofrandomneuralnetworkswithbistableunits.
Physical
ReviewE
,90(6),dec2014.
[28]
DSussillo.Neuralcircuitsascomputationaldynamical
systems.
Currentopinioninneurobiology
,25:156Œ163,
2014.
[29]
DSussilloandOBarak.Openingtheblackbox:
low-dimensionaldynamicsinhigh-dimensionalrecurrent
neuralnetworks.25(3):626Œ649,2013.
[30]
PJWerbos.Backpropagationthroughtime:whatitdoes
andhowtodoit.
ProceedingsoftheIEEE
,78(10):1550Œ
1560,1990.
"
16,"HitNet: a neural network with capsules embedded in a Hit-or-Miss layer, extended with hybrid data augmentation and ghost capsules",http://arxiv.org/pdf/1806.06519v1.pdf,https://github.com/bakirillov/capsules,"HitNet:aneuralnetworkwithcapsulesembeddedina
Hit-or-Misslayer,extendedwithhybriddataaugmentation
andghostcapsules
AdrienDeliege,AnthonyCioppa,MarcVanDroogenbroeck
June19,2018
Abstract
Neuralnetworksdesignedforthetaskofhavebecomeacommodityinrecent
years.Manyworkstargetthedevelopmentofbetternetworks,whichresultsina
cationoftheirarchitectureswithmorelayers,multiplesub-networks,oreventhecombination
ofmultipleclaInthispaper,weshowhowtoredesignasimplenetworktoreachex-
cellentperformances,whicharebetterthantheresultsreproducedwithCapsNetonseveral
datasets,byreplacingalayerwithaHit-or-Misslayer.Thislayercontainsactivatedvectors,
calledcapsules,thatwetraintohitormissacentralcapsulebytailoringaspcentripetal
lossfunction.Wealsoshowhowournetwork,namedHitNet,iscapableofsynthesizinga
representativesampleoftheimagesofagivenclassbyincludingareconstructionnetwork.
Thispossibilityallowstodevelopadataaugmentationstepcombininginformationfromthe
dataspaceandthefeaturespace,resultinginahybriddataaugmentationprocess.Inaddi-
tion,weintroducethepossibilityforHitNet,toadoptanalternativetothetruetargetwhen
neededbyusingthenewconceptofghostcapsules,whichisusedheretodetectpotentially
mislabeledimagesinthetrainingdata.
1Introduction
Convolutionalneuralnetworks(CNNs)havebecomeanomnipresenttoolforimage
andhavebeenrevolutionizingtheofcomputervisionforthelastfewyears.Withtheemer-
genceofcomplextaskssuchasImageNet[4],thenetworkshavegrownbiggerand
Figure1:Graphicalrepresentationofthestructureofournewnetwork,namedHitNet.Our
contributionsarehighlightedinred,andcompriseanewHit-or-Misslayer,acentripetalloss,
prototypesthatcanbebuiltwiththedecoder,andghostcapsulesthatcanbeembeddedinthe
HoMlayer.Sourcecodeisavailableathttp://www.telecom.ulg.ac.be/hitnet.
1
arXiv:1806.06519v1  [cs.CV]  18 Jun 2018deeperwhileregularlyfeaturingnewlayersandotherextensions.However,CNNsarenotin-
trinsicallyviewpoint-invariant,meaningthatthespatialrelationsbetweentfeaturesare
generallynotpreservedwhenusingCNNs.Therefore,somemodelsweredesignedinthespiritof
increasingtheirrepresentationalpowerbyencapsulatinginformationinactivatedvectorscalled
capsules,anotionintroducedbyHintonin[6].
Recentadvancesoncapsulesarepresentedin[22],inwhichSabour
etal
.mainlyfocusonMNIST
digitsclascation[11].Forthatpurpose,theydevelopCapsNet,aCNNthatshowsmajorchanges
comparedtoconventionalCNNs.Asdescribedin[22],\acapsuleisagroupofneuronswhose
activityvectorrepresentstheinstantiationparametersofasptypeofentitysuchasanobject
oranobjectpart.""Hence,theconceptofcapsulesomehowaddsa(geometrical)dimensiontothe
\capsuled""layers,whichismeanttocontainricherinformationaboutthefeaturescapturedby
thenetworkthaninconventionalfeaturemaps.Thetransferofinformationfromthecapsulesof
alayertothecapsulesofthenextlayerislearnedthroughadynamicroutingmechanism[7,22].
Thelengthofthecapsulesofthelastlayer,calledDigitCaps,isusedtoproduceaprediction
vectorwhoseentriesareinthe[0
;
1]rangethankstoanorientation-preservingsquashingactivation
appliedbeforehandtoeachcapsule,andwhichencodesthelikelihoodoftheexistenceofeachdigit
ontheinputimage.Thispredictionvectorisevaluatedthrougha\marginloss""thatdisplays
similaritieswiththesquaredHingeloss.Inanencoder-decoderspirit,thecapsulesofDigitCaps
canbefedtoadecodersub-networkthataimsatreconstructingtheinitialimage,whichconfers
thecapsulesanaturalinterpretationofthefeaturesthattheyencoded.State-of-the-artresultsare
reportedbySabour
etal
.in[22]onMNISTdataset.Otherexperimentscarriedouton[26],
multiMNIST[22],SVHN[18],smallNORB[12]andCIFAR10[9](withanensembleof7networks)
showpromisingresultsaswell.Unfortunately,currentimplementationsofCapsNetwithdynamic
routingareconsiderablyslowerthanconventionnalCNNs,whichisamajordrawbackofthis
process.
Sincethepublicationof[22],severalworkshavebeenconductedtoimproveCapsNet'sspeedand
structure([3,7,21,28])andtoapplyittomorecomplexdata([1,14,19])andvarioustasks
([16]forlocalization,[10]forsegmentation,[29]forhypernymydetection,[2]forreinforcement
learning).However,itappearsthatalltheattempts(
e.g
.[5,15,17,24])toreproducetheresults
providedin[22]failedtoreachtheperformancesreportedbySabour
etal
.
Thepartofthisworkisdevotedtotheconstructionofaneuralnetwork,named
HitNet
,
thatusesthecapsuleapproachonlyinonelayer,called
Hit-or-Misslayer
(HoM,thecounterpart
ofDigitCaps)andthatprovidesfastandrepeatedlybetterperformancesthanthosereportedin
[5,15,17,24]withCapsNet.Wealsoprovideitsassociatedloss,thatwecall
centripetalloss
(counterpartofthemarginloss).
ThestrongrepresentationalbehaviorexpectedfromCapsNetallowstoperformthejointmulti-task
ofandreconstruction.Itisthuspossibletotakeadvantageofcapsulestocapturethe
naturalvariationsofimportantclass-spfeaturesofthetrainingdata,asillustratedin[22].By
browsingthroughthespaceoffeaturesandusingthedecoderappropriately,itisthuspossibleto
performdatagenerationanddataaugmentation.Augmentingthedataisrecognizedasapowerful
waytopreventovandincreasetheabilityofthenetworktogeneralizetounseendataat
testtime,whichleadstobetterresults[20].Thisprocessisoftenappliedentirely
eitherinthedataspaceorinthefeaturespace[30].Asasecondpart,wepresentawayofusing
thecapsulesofHoMtoderiveahybriddataaugmentationalgorithmthatreliesonbothrealdata
andsyntheticfeature-baseddatabyintroducingthenotionof
prototype
,aclassrepresentative
learnedindirectlybythedecoder.
Notonlydoweusecapsulesfordata-drivenapplicationssuchasdatagenerationanddataaug-
mentation,butwecanalsousethemtonewnotionsthatservenovelpurposes.Theone
thatwewanttohighlightasthirdpartisthepossibilitytoallowHitNettoadoptanalternative
choicetothetrueclasswhenneeded,throughthenotionof
ghostcapsule
thatweembedinHoM
.
Moresp,inthiswork,weshowhowtouseghostcapsulestoanalyzethetrainingsetand
2
todetectpotentiallymislabeledtrainingimages,whichisofteneludedinpracticedespitebeingof
paramountimportance.
Inshort,ourcontributionsarethreefold:
1.
WedevelopHitNet,aneuralnetworkthatusescapsulesinanewwaythroughaHit-or-Miss
layerandacentripetalloss,andwedemonstratethesuperiorityofHitNetovertheresults
reproducedbyotherauthorswithCapsNet.
2.
Wederiveahybriddataspaceandfeaturespacedataaugmentationprocessviathecapsules
ofHoMandprototypes.
3.
WeprovideawayforHitNettoidentifyanotherplausibleclassforthetrainingimagesif
necessarywiththenewnotionofghostcapsules.Weexemplifythisnotionbydetecting
potentiallymislabeledtrainingimages,ortrainingimagesthatmayneedtwolabels.
ThesecontributionsaredescribedinthatorderinSection2,thentestedinSection3.
2HitNet:rethinkingDigitCapsandbeyond
HitNetessentiallyintroducesanewlayer,theHit-or-Misslayer,thatisuniversalenoughtobe
usedinmanytnetworks.HitNetaspresentedhereafteristhusaninstanceofashallow
networkthathoststhisHoMlayerandillustratesitspotential.
2.1Introducinghits,misses,thecentripetalapproach,andtheHit-or-
Misslayer
InthecaseofCapsNet,largeactivatedvaluesareexpectedfromthecapsuleofDigitCapscor-
respondingtothetrueclassofagivenimage,similarlytousualnetworks.Fromageometrical
perspectiveinthefeaturespace,thisresultsinacapsulethatcanbeseenasapointthatthe
networkistrainedtopushfarfromthecenteroftheunithypersphere,inwhichitendsupthanks
tothesquashingactivationfunction.Wequalifysuchanapproachas\centrifugal"".Inthatcase,
apossibleissueisthatonehasnocontrolonthepart(s)ofthespherethatwillbetargetedby
CapsNet,andasecondoneisthatthecapsulesoftwoimagesofthesameclassmightbelocated
farfromeachother([23,32]),whicharetwodebatablebehaviors.
Tosolvetheseissues,wehypothesizethatalltheimagesofagivenclasssharesomeclass-sp
featuresandthatthisassumptionshouldalsomanifestthroughtheirrespectivecapsules.Hence,
givenaninputimage,weimposethatHitNettargetsthecenterofthefeaturespacetowhichthe
capsuleofthetrueclassbelongs,sothatitcorrespondstowhatwecalla
hit
.Thecapsulesrelated
totheotherclasseshavethustobesentfarfromthecenteroftheirrespectivefeaturespaces,
whichcorrespondstowhatwecalla
miss
.OurpointofviewisthustheoppositeofSabour
et
al
.'s;instead,wehavea
centripetalapproach
withrespecttothetrueclass.
Thesquashingactivationfunctioninducesadependencybetweenthefeaturesofacapsuleof
DigitCaps,inthesensethattheirvaluesareconditionedbytheoveralllengthofthecapsule.If
onefeatureofacapsulehasalargevalue,thenthesquashingpreventstheotherfeaturesofthat
capsuletotakelargevaluesaswell;alternatively,ifthenetworkwishestoactivatemanyfeatures
inacapsule,thennoneofthemwillbeabletohavealargevalue.Noneofthesetwocases
withtheperspectiveofprovidingstrongactivationsforseveralrepresentativefeaturesasdesired
inSabour
etal
.Besides,theorientationofthecapsules,preservedwiththesquashingactivation,
3
isnotusedexplicitlyforthepreservingtheorientationmightthusbeasup
constraint.
Therefore,wereplacethissquashingactivationbyaBatchNormalization(BN,[8])followedby
aconventionalsigmoidactivationfunctionappliedelement-wise.Weobtainalayercomposed
ofcapsulesaswellthatwecallthe
Hit-or-Miss
(HoM)layer,whichisHitNet'scounterpartof
DigitCaps.Consequently,allthefeaturesobtainedinHoM'scapsulescanspanthe[0
;
1]range
andtheycanreachanyvalueinthisintervalindependentlyoftheotherfeatures.Thefeature
spacesinwhichthecapsulesofHoMliearethusunithypercubes.
thecentripetalloss
Giventheuseoftheelement-wisesigmoidactivation,thecentersofthereshapedtargetspacesare,
foreachofthem,the
centralcapsules
C
:(0
:
5
;:::;
0
:
5).The
k
-thcomponentoftheprediction
vector
y
pred
ofHitNet,denoted
y
pred
;k
,isgivenbytheEuclideandistancebetweenthe
k
-thcapsule
ofHoMand
C
:
y
pred
;k
=
jj
HoM
k

C
jj
:
(1)
Togiveatractableformtothenotionsofhits,misses,centripetalapproachdescribedaboveand
justifyHoM'sname,wedesignacustomcentripetallossfunctionwiththefollowingrequirements:
1.
ThelossgeneratedbyeachcapsuleofHoMhastobeindependentoftheothercapsules.We
thusgetridofanyprobabilisticnotionduringthetraining.
2.
Thecapsuleofthetrueclassdoesnotgenerateanylosswhenbelongingtoacloseisotropic
neighborhoodof
C
,whichthe
hitzone
.Outsidethatneighborhood,itgeneratesa
lossincreasingwithitsdistanceto
C
.Thecapsulesrelatedtotheremainingclassesgenerate
alossdecreasingwiththeirdistanceto
C
insideawideneighborhoodof
C
anddonot
generateanylossoutsidethatneighborhood,whichisthe
misszone
.Theseloss-freezones
areimposedtostoppenalizingcapsulesthatarealreadytlyclose(ifassociatedwith
thetrueclass)orfar(ifassociatedwiththeotherclasses)from
C
intheirrespectivefeature
space.
3.
Thegradientofthelosswithrespectto
y
pred
;k
cannotgotozerowhenthecorresponding
capsuleapproachestheloss-freezonesinrequirement2.Toguaranteethisbehavior,
weimposeaconstantgradientaroundthesezones.Thisisimposedtohelpthenetwork
makehitsandmisses.
4.
Forthesakeofconsistencywithrequirement3,weimposepiecewiseconstantgradientswith
respectto
y
pred
;k
,whichthusnaturalbinsaround
C
,astheringsofarcherytargets,
inwhichthegradientisconstant.
Alltheseelementscontributetoalosswhichisapiecewiselinearfunctionofthepredictions
andwhichis
centripetalwithrespecttothecapsuleofthetrueclass
.Wethuscallitour
centripetal
loss
.Itsderivativewithrespectto
y
pred
;k
isastaircase-likefunction,whichgoesupwhen
k
isthe
indexofthetrueclass(seeFigure2a)andgoesdownotherwise(seeFigure2b).Agenericanalytic
formulaofafunctionofavariable
x
,whosederivativeisanincreasingstaircase-likefunctionwhere
thestepshavelength
l
andheight
h
andvanishon[0
;m
]ismathematicallygivenby:
L
l;h;m
(
x
)=
H
f
x

m
g
(
f
+1)
h
(
x

m

0
:
5
fl
)
;
(2)
where
H
f
:
g
denotestheHeavisidestepfunctionand
f
=

x

m
l

(
b
:
c
istheorfunction).Hence
thelossgeneratedbythecapsuleofthetrueclassisgivenby
L
l;h;m
(
y
pred
;k
),where
k
istheindex
ofthetrueclass.Thelossgeneratedbythecapsulesoftheotherclassescanbedirectlyobtained
4
(a)
(b)
(c)
(d)
Figure2:RepresentationofthecentripetallossgiveninEquation2asfunctionofthedistanceof
thecapsuleofthetrueclass(a)andoftheotherclasses(b)from
C
.Then,visualizationofthe
centripetallossinthe2-dimensionalcase(
n
=2).Thelossassociatedwiththecapsuleofthetrue
classisgivenbyplot(c).Theloss-freehitzoneistheareawithintheblackcircle,withradius
m
.
Thelossgeneratedbytheothercapsulesisgivenbyplot(d).Theloss-freemisszoneisthearea
outsidetheblackcircle,withradius
m
0
.
fromEquation2as
L
l
0
;h
0
;
p
n=
2

m
0
(
p
n=
2

y
pred
;k
0
)(foranyindex
k
0
oftheotherclasses)ifthe
stepshavelength
l
0
,height
h
0
,vanishafter
m
0
andifthecapsuleshave
n
components.Theuse
of
p
n=
2originatesfromthefactthatthemaximaldistancebetweenacapsuleofHoMand
C
is
givenby
p
n=
2andthustheentriesof
y
pred
willalwaysbeintheinterval[0
;
p
n=
2].Consequently,
thecentripetallossofagiventrainingimageisgivenby
L
=
K
X
k
=1
y
true
;k
L
l;h;m
(
y
pred
;k
)+

(1

y
true
;k
)
L
l
0
;h
0
;
p
n=
2

m
0
(
p
n=
2

y
pred
;k
)
(3)
where
K
isthenumberofclasses,
y
true
;k
denotesthe
k
-thcomponentofthevector
y
true
,and

isadown-weightingfactorsetas0
:
5asin[22].Thelossassociatedwiththecapsuleofthetrue
classandthelossassociatedwiththeothercapsulesarerepresentedinFigure2inthecasewhere
n
=2.
ArchitectureofHitNet
Basically,HitNetincorporatesaHoMlayerbuiltuponfeaturemapsandusedinpairwiththe
centripetalloss.Inourexperiments,wehaveadoptedashallowstructuretoobtainthesefeature
mapstohighlighttheboftheHoMlayer.HitNet'scompletearchitectureisdisplayedin
Figure1.Itiscomposedofthefollowingelements:
5
1.
Two9

9(withstrides(1,1)then(2,2))convolutionallayerswith256channelsandReLU
activations,toobtainfeaturemaps.
2.
Afullyconnectedlayertoa
K

n
matrix,followedbyaBNandanelement-wisesigmoid
activation,whichproducesHoMcomposedof
K
capsulesofsize
n
.
3.
TheEuclideandistancewiththecentralcapsule
C
:(0
:
5
;:::;
0
:
5)iscomputedforeach
capsuleofHoM,whichgivesthepredictionvectorofthemodel
y
pred
.
4.
AllthecapsulesofHoMaremasked(setto0)excepttheonerelatedtothetrueclass(to
thepredictedclassattesttime),thentheyareconcatenatedandsenttoadecoder,which
producesanoutputimage
X
rec
,thataimsatreconstructingtheinitialimage.Thedecoder
consistsintwofullyconnectedlayersofsize512and1024withReLUactivations,andone
fullyconnectedlayertoamatrixwiththesamedimensionsastheinputimage,witha
sigmoidactivation(thisisthesamedecoderasin[22]).
If
X
istheinitialimageand
y
true
itsone-hotencodedlabel,then
y
true
and
y
pred
producealoss
L
1
throughthecentripetallossgivenbyEquation3while
X
and
X
rec
generatealoss
L
2
through
themeansquarederror.Thecompositelossassociatedwith
X
isgivenby
L
=
L
1
+
L
2
,
where

issetto0
:
392([5,22]).Forthetask,thelabelpredictedbyHitNetisthe
indexofthelowestentryof
y
pred
.Thehyperparametersinvolvedin
L
1
arechosenas
l
=
l
0
=0
:
1,
h
=
h
0
=0
:
2,
m
=0
:
1,
m
0
=0
:
9,
n
=16and

=0
:
5asin[22].
2.2BeyondHitNet:Prototypes,datagenerationandhybriddataaug-
mentation
Prototypes
ThesimultaneoususeofHoMandadecodernewpossibilitiesintermsofimageprocessing.
Itisessentialtounderlinethatinourcentripetalapproach,weensurethatalltheimagesofagiven
classwillhaveallthecomponentsoftheircapsuleofthatclasscloseto0
:
5.Inotherwords,we
regroupthesecapsulesinaconvexspacearound
C
.Thiscentralcapsule
C
standsforaxedpoint
ofreference,hencetfromacentroid,fromwhichwemeasurethedistanceofthecapsulesof
HoM;fromthenetwork'spointofview,
C
standsforacapsuleofreferencefromwhichwemeasure
deformations.Inconsequence,wecanuse
C
insteadofthecapsuleofaclassofHoM,zerooutthe
othercapsulesandfeedtheresultinthedecoder:thereconstructedimagewillcorrespondtothe
imagethatthenetworkconsidersasacanonicalimageofreferenceforthatclass,whichwecall
its
prototype.
Datageneration
Afterconstructingtheprototypes,wecanslightlydeformthemtoinducevariationsinthere-
constructionwithoutbeingdependentonanytrainingimage,justbyfeedingthedecoderwith
azeroedoutHoMplusonecapsuleinaneighborhoodof
C
.Thisallowstoidentifywhatthe
featuresofHoMrepresent.Forthesamepurpose,Sabour
etal
.needtorelyonatrainingimage
becausethecentrifugalapproachdoesnotdirectlyallowstobuildprototypes.Inourcase,itis
evenpossibletocomputeanapproximaterangeinwhichthecomponentscanbetweaked.Ifa
tamountoftrainingdataisavailable,wecanexpecttheindividualfeaturesofthecapsules
ofthetrueclassestobeapproximatelyGaussiandistributedwithmean0
:
5andstandarddevia-
tion
m=
p
n
1
,thusthe[0
:
5

2
m=
p
n;
0
:
5+2
m=
p
n
]intervalshouldprovideasatisfyingoverview
ofthephysicalinterpretationembodiedinagivenfeatureofHoM.Theapproximateknowledgeof
1
ComesfromEquation1,withthehypothesisthatallthevaluesofsuchacapsulefrom0
:
5fromroughly
thesameamount.
6
thedistributionsalsoenablesustoperformdatageneration,bysamplingforinstanceaGaussian
vectorofsize
n
,withmean0
:
5andstandarddeviation
m=
p
n
,insertingitasacapsuleinHoM,
zeroingouttheothercapsulesandfeedingtheresultinthedecoder.
Hybriddataaugmentation
ThecapsulesofHoMonlycapturetheimportantfeaturesthatallowthenetworktoidentifythe
classoftheimagesandtoperformanapproximatereconstructionviathedecoder.Thisimplies
thattheimagesproducedbythedecoderarenotdetailedenoughtolookrealistic.Thedetails
arelostintheprocess;generatingthembackishard.Itiseasiertousealreadyexistingdetails,
i.e
.thoseofimagesofthetrainingset.Wecanthussetupahybridfeature-basedanddata-based
dataaugmentationprocess:

Takeatrainingimage
X
andfeedittoatrainedHitNetnetwork.

ExtractitsHoMandmodifythecapsulecorrespondingtotheclassof
X
.

Reconstructtheimageobtainedfromtheinitialcapsule,
X
rec
,andfromthemoone,
X
mod
.

Thedetailsof
X
arecontainedin
X

X
rec
.Thusthenew(detailed)imageis
X
mod
+
X

X
rec
.Clipthevaluestoensurethattheresultingimagehasvaluesintheappropriaterange
(
e.g
.[0,1]).
2.3BeyondHitNet:Ghostcapsules
OneofthemainassetsofHitNetistheyintheuseoftheHoMlayer.Itcanbeeasily
exploitedtoperformttasksotherthanInordertoshowanadditional
possibilityofHitNet,wedevelopthenotionof
ghostcapsules,
thatcanbeintegratedwithinthe
network.Theuseofghostcapsulesallowsthenetwork,foreachtrainingsample,tozero-out
thelossassociatedwithacapsulerelatedtoaclassthatthenetworkconsidersasareasonable
predictionandthatistfromthetrueclass.Severalsituationscanbfromthisprocess.
Forexample,itcanbeusedtoassessthequalityofthetrainingsetthroughthedetectionof
potentiallymislabeledimagesinthatset.Detectingthemisanimportantaspectsincemislabeled
trainingdatapollutethewholetrainingbyforcingthenetworktolearnunnaturalfeatures,or
evenmistakes;thisconstrainsthenetworktomemorizeoutliersindedicatedneurons.
of\ghostcapsule""
Inordertogivethenetworkthecapacitytoallowanalternativetothelabelsprovided,weintroduce
thenotionof\ghostcapsule"",whichisassociatedwitheveryimageofthetrainingset.Thekey
ideaisthefollowing:duringthetrainingphase,foreachimage,insteadofforcingthenetworkto
produceonecapsulethatmakesahitand
K

1capsulesthatmakemisses,wedemandonehit
forthecapsuleofthetrueclassand
K

2missesforasmanycapsulesoftheotherclasses;the
remainingcapsuleistheso-called
\ghostcapsule""
,denotedbyGChereafter.By\capsuleofthe
trueclass""ofanimage,wemeanthecapsuleassociatedwiththeclasscorrespondingtothelabel
providedbytheannotatorwiththeimage.TheGCisthecapsuleoftheHoMlayerwhichisthe
closestto
C
amongthe
K

1capsulesnotcorrespondingtothetrueclass.Thelossassociated
withtheGCiszeroedout,hencetheGCisnotforcedtomakeahitnoramiss,anditisnot
involvedintheupdateoftheweightsfromonebatchtothenext;itisessentiallyinvisibletothe
lossintheback-propagation,henceitsname.
Fromanimplementationpointofview,trainingHitNetwithaGCperimageissimilartotraining
itwithoutGC;onlythecentripetallossneedstobeadjusted.Givenatrainingimage,itsone-hot
7
encodedlabel
y
true
andtheoutputvectorofthenetwork
y
pred
,thecentripetallossinitially
byEquation3formallybecomes
L
ghost
=
K
X
k
=1
y
true
;k
L
l;h;m
(
y
pred
;k
)+

(1

~
y
true
;k
)
L
l
0
;h
0
;
p
n=
2

m
0
(
p
n=
2

y
pred
;k
)
;
(4)
where
~
y
true
;k
=1if
k
isthetrueclassindexortheGCclassindex,and
~
y
true
;k
=0otherwise.
TwoimportantcharacteristicsarethusassociatedwithaGC:itsclass,whichisalwaysoneofthe
K

1classesnotcorrespondingtothetrueclassofthesample,anditsdistancewith
C
.The
GCclassofanimageisobtainedinadeterministicway,itisnotaDropout([25])noralearned
Dropoutvariant(as
e.g
.[13]).Besides,thisclassislikelytochangefromoneepochtothenextin
theearlystagesofthetraining,untilthenetworkdecideswhatchoiceisthebest.Ideally,aGC
willmakeahitwhenitsclassisaplausiblealternativetothetrueclassoriftheimagedeservesan
extralabel,andwillmakeamissotherwise.TheevolutionofaGCduringthetrainingisdictated
bytheoverallevolutionofthenetwork.
Subsequently,inthesituationsdescribedabove,attheendofthetraining,mislabeledorconfusing
imagesshouldactuallydisplaytwohits:onehitforthecapsulecorrespondingtothetrueclass
sincethenetworkwasforcedtomakethathit,andonehitforthecapsulecorrespondingtothe
otherplausibleclasssincethenetworkwasnottoldtopushthiscapsuletowardsthemisszone
andsincetheimagedisplaysthefeaturesneededtoidentifythisalternateclass.Lookingatthe
imageswithaGCinthehitzoneattheendofthetrainingallowstodetecttheimagesforwhich
thenetworksuspectsanerrorinthelabel,orwhichimagespossiblydeservetwolabels.
3Experimentsandresults
Inthissection,wepresentsomeexperimentsandtheresultsobtainedwithHitNet.Thestructure
ofthesectionmirrorsthatofSection2,
i.e
.itisdividedinthreeparts,whichdirectlycorrespond
tothethreepartsofSection2:(1)generalperformancesofHitNet,(2)useofthedecoder,and
(3)useofghostcapsules.
3.1resultsofHitNet
Hereafter,wereporttheresultsobtainedwithHitNet.First,wecomparetheperformancesof
HitNetonMNISTtaskwithbaselinemodelstoshowthatHitNetproducesrepeat-
edlyclosetostate-of-the-artperformances.Then,wecomparetheperformancesofHitNetwith
reproducedexperimentswithCapsNetonseveraldatasets.
Descriptionofthenetworksusedforcomparison
WecomparetheperformancesofHitNettothreeothernetworksfortheMNISTdigits
task.Forthesakeofafaircomparison,astructuresimilartoHitNetisusedasmuchaspossible
forthesenetworks.First,theyaremadeoftwo9

9convolutionallayerswith256channels(with
strides(1,1)then(2,2))andReLUactivationsasforHitNet.Then,thenetworkisN1:

N1(baselinemodel,conventionalCNN)hasafullyconnectedlayertoavectorofdimension
10,thenBNandSoftmaxactivation,andisevaluatedwiththeusualcategoricalcross-entropy
loss.Nodecoderisused.
Thetwoothernetworks,notedN2andN2b,haveafullyconnectedlayertoa10

16matrix,
followedbyaBNlayerasN1andHitNet,then
8

N2(CapsNet-likemodel)hasasquashingactivation.TheEuclideandistancewith
O
:
(0
;:::;
0)iscomputedforeachcapsule,whichgivestheoutputvectorofthemodel
y
pred
.
Themarginloss(centrifugal)of[22]isused;

N2bhasasigmoidactivation.TheEuclideandistancewith
C
:(0
:
5
;:::;
0
:
5)iscomputedfor
eachcapsule,whichgivestheoutputvectorofthemodel
y
pred
.Themarginloss(centrifugal)
of[22]isused.
Letusrecallthat,comparedtoN2andN2b,HitNethasasigmoidactivation,whichproducesthe
capsulesofHoM.The
L
2
distancewith
C
:(0
:
5
;:::;
0
:
5)iscomputedforeachcapsule,whichgives
theoutputvectorofthemodel
y
pred
,andthecentripetallossgivenbyEquation3isused.Network
N2bistestedtoshowthebofthecentripetalapproachofHitNetoverthecentrifugalone,
regardlessofthesquashingorsigmoidactivations.Also,duringthetrainingphase,thedecoder
usedinHitNetisalsousedwithN2andN2b.
resultsonMNIST
Eachnetworkistrained20timesduring250epochswiththeAdamoptimizerwithaconstant
learningrateof0
:
001,withbatchesof128images.Theimagesofabatcharerandomlyshiftedof
upto2pixelsineachdirection(left,right,top,bottom)withzeropaddingasin[22].Themetrics
presentedhereforagivennetworkaretheaveragemetricsoverthe20runsofthatnetwork
calculatedontheMNISTtestset.Thelearningrateiskeptconstanttoremoveitspossible
ontheresults,whichmayfromonenetworkstructuretoanother.Besides,this
leadustoevaluatethe\natural""convergenceofthenetworkssincetheconvergenceisnotforced
byadecreasinglearningratemechanism.Toourknowledge,thispracticeisnotcommonbut
shouldbeusedtoproperlyanalyzethenaturalconvergenceofanetwork.
TheresultsthroughouttheepochsareplottedinFigure3.Theerrorratesofthefourmodelsare
reportedinTable1and,asitcanalsobeseeninFigure3,theyclearlyindicatethatthecentripetal
approachofHitNetisbettersuitedthanacentrifugalapproach,regardlessoftheactivationused.
ThisobservationisifthesquashingfunctionisusedinHitNet,inwhichcaseatest
errorrateofabout0
:
40%isobtained.
InTable1,thecolumn\Standarddeviation(Std)""representsthevariabilitythatisobtainedin
theerrorrateamongthe20runsofagivennetwork.Thecolumn\Irregularity""relatestothe
average(overthe20runs)ofthestandarddeviationofthelast100errorratesrecordedforeach
run.Alowirregularityrepresentsamore\natural""convergenceoftherunsofagivennetwork
sinceitmeasuresthevariabilityoftheerrorratewithinarun.Thisindicatormakessenseinthe
contextofaconstantlearningrateandnoovasobservedwithHitNet.
ThesetwometricsbothindicatesuperiorperformanceswhenusingHitNet,inthesensethatthere
isanintrinsicallybetternaturalconvergenceassociatedwiththecentripetalapproach(lowerirreg-
ularity)andmoreconsistentresultsbetweentherunsofasamemodel(lowerstandarddeviation).
LetusnotethatalltherunsofHitNetconvergedandnoovisobserved.Thequestionof
theconvergenceisnotstudiedin[22]andthenetworkisstoppedbeforeitisobserveddiverging
in[17].
Then,werunthesameexperimentsbutwithadecreasinglearningrate,toseehowtheresultsare
impactedwhentheconvergenceisforced.Thelearningrateismultipliedbyafactor0
:
95atthe
endofeachepoch.Asaresult,thenetworksstabilizemoreeasilyaroundalocalminimumofthe
lossfunction,improvingtheiroverallperformances.ItcanbenotedthatHitNetislessimpacted,
whichindicatesthatHitNetconvergestosimilarstateswithorwithoutdecreasinglearningrate.
Theconclusionsarethesameaspreviously:HitNetperformsbetter.Letusnotethatinthis
case,theirregularityisnotusefulanymore.Wereplaceitbythebesterrorrateobtainedfora
convergedrunofeachtypeofnetwork.
9
Figure3:EvolutionoftheaveragetesterrorrateonMNISToverthe20runsofeachnetworkas
afunctionoftheepochs,withaconstantlearningrate.ThesuperiorityofHitNetcanbeseen.
Theconvergenceisalsoerinthatcase,inlightoftheweakeroscillationsfromoneepochtothe
next(lowerirregularity).
Constantlearningrate
Decreasinglearningrate
Network
Err.rate%
Std(

10

2
)
Irreg.(

10

2
)
Err.rate%
Std(

10

2
)
Best%
Baseline
0
:
52
0
:
060
0
:
060
0
:
42
0
:
027
0
:
38
CapsNet-like
0
:
79
0
:
089
0
:
053
0
:
70
0
:
076
0
:
53
N2b
0
:
76
0
:
072
0
:
048
0
:
72
0
:
074
0
:
62
HitNet
0
:
38
0
:
033
0
:
025
0
:
36
0
:
025
0
:
32
Table1:PerformancemetricsonMNISTofthefournetworksdescribedinthetext,showingthe
superiorityofHitNet.
ComparisonswithreportedresultsofCapsNetonseveraldatasets
AsfarasMNISTisconcerned,thebesttesterrorratereportedin[22]is0
:
25%,whichisobtained
withdynamicroutingandisanaverageof3runsonly.However,toourknowledge,thisresult
hasyettobeasthebesttentativereproductionsreacherrorrateswhichcomparewith
ourresults,asshowninTable2.Letusnotethat,in[27],authorsreporta0
:
21%testerror
rate,whichisthebestperformancepublishedsofar.Nevertheless,thisscoreisreachedwitha
votingcommitteeofenetworksthatweretrainedwithrandomcrops,rotationandscalingas
dataaugmentationprocesses.Theyachieve0
:
63%withoutthecommitteeandthesetechniques;
ifonlyrandomcropsareallowed(asdoneherewiththeshiftsofupto2pixels),theyachieve
0
:
39%.Itisimportanttounderlinethatsuchimplementationsreportexcessivelylongtraining
times,mainlyduetothedynamicroutingpart.Forexample,theimplementation[5]appearsto
beabout13timesslowerthanHitNet,forcomparableperformances.Therefore,HitNetproduces
resultsconsistentwithstate-of-theartmethodsonMNISTwhilebeingsimple,lightandfast.
TheresultsobtainedwithHitNetandthoseobtainedwithCapsNetintsourcesonFashion-
MNIST,CIFAR10,SVHN,arealsocomparedinTable2.Forfaircomparisons,the
architectureofHitNetdescribedinSection2.1isleftuntouched.Theresultsreportedareobtained
withaconstantlearningrateandareaverageerrorrateson20runsaspreviously.Somecomments
abouttheseexperimentsaregivenbelow:
1.
Fashion-MNIST:HitNetoutperformsreproductionsofCapsNetexceptfor[5],butthisresult
isobtainedwithhorizontalasadditionaldataaugmentationprocess.
2.
CIFAR10:HitNetoutperformsthereproductionsofCapsNet.Theresultprovidedin[22]
isobtainedwithanensembleof7models.However,theindividualperformancesofHitNet
10
CapsNetfrom
MNIST
Fashion-MNIST
CIFAR10
SVHN

Sabour
etal
.[22]
0.25
-
10.60
4.30
21.00
Nair
etal
.[17]
0.50
10.20
32.47
8.94
-
Guo[5]
0.34
6.38
27.21
-
-
Liao[15]
0.36
9.40
-
-
-
Shin[24]
0.75
10.98
30.18
-
24.11
HitNet
0.38/0.32
7.70
26.70
5.50
16.97
Table2:Comparisonbetweentheerrorrates(in%)reportedonvariousexperimentswithCapsNet
andHitNet,inwhichcasetheaverageresultsover20runsarereported.
andofthereproductionsdonotsuggestthatensemblingthemwouldleadtothatresult,as
alsosuggestedin[31],whichreachesbetween28%and32%testerrorrates.
3.
SVHN:HitNetoutperformsCapsNetfrom[17],whichistheonlysourceusingCapsNetwith
thisdataset.
4.
HitNetoutperformstheresultsprovidedin[24]andeveninSabour
etal
.bya
comfortablemargin.Weperformedthesameexperimentastheonedescribedin[22].Each
imageoftheMNISTtrainsetisplacedrandomly(onceandforall)onablackbackground
of40

40pixels,whichconstitutesthetrainingsetoftheexperiment.Theimagesofthe
batchesarenotrandomlyshiftedofupto2pixelsineachdirectionanymore.Aftertraining,
themodelsaretestedontestset,whichconsistsintransformationsofMNIST
testset.Letusnotethatatesterrorrateofonly2
:
7%isobtainediftheimagesofthebatches
arerandomlyshiftedofupto2pixelsineachdirectionasforthepreviousexperiments.
3.2Usingthedecoderforvisualizationandhybriddataaugmentation
Inthissection,wepresentsomeresultsrelatedtoSection2.2abouttheusesofthedecoderto
buildprototypes,toperformdatagenerationanddataaugmentation.
Constructingprototypes,interpretingthefeaturesanddatageneration
AsmentionedinSection2.2,thecentripetalapproachgivesaparticularroletothecentralcapsule
C
:(0
:
5
;:::;
0
:
5),inthesensethatitcanbeusedtogenerateprototypesofthetclasses.
TheprototypesobtainedfromaninstanceofHitNettrainedonMNISTaredisplayedinFigure4.
Itisthenparticularlyeasytovisualizewhateachcomponentrepresentsbytweakingthecom-
ponentsof
C
around0
:
5;thereisnoneedtodistortrealimagesasin[22].Also,inourcase,
asmentionedinSection2.2,therange[0
:
5

2
m=
p
n;
0
:
5+2
m=
p
n
]issuitabletotweakthepa-
rameters,whiletheserangesarenotpredictablewithCapsNetandmayvaryfromonefeature
toanother,asitcanbeinferredfrom[23,32]wherethecapsulesofaclassdonotnecessarily
fallclosetoeachother.Therangeinquestion[0
:
5

2
m=
p
n;
0
:
5+2
m=
p
n
]isactually
bylookingatthedistributionsoftheindividualfeaturesembodiedinthecapsules.OnMNIST,
HitNetcapturessomefeaturesthatarepositionalcharacteristics,otherscanberelatedtothe
widthofthefontortosomelocalpeculiaritiesofthedigits,asin[22].Samplingrandomcapsules
closeto
C
foragivenclassthusgeneratesnewimageswhosecharacteristicsarecombinationsof
thecharacteristicsofthetrainingimages.Itthusmakessensetoencompassallthecapsulesof
trainingimagesofthatclassinaconvexspace,asdonewithHitNet,toensuretheconsistencyof
theimagesproduced,whileCapsNetdoesnotguaranteethisbehavior([23,32]).
Letusnotethat,asunderlinedin[17],thereconstructionsobtainedforFashion-MNISTlacks
detailsandthoseofCIFAR10andSVHNaresomehowblurredbackgrounds;thisisalsothecase
11
Figure4:Prototypesobtainedattheendofthetrainingbyfeedingthedecoderwithcapsulesof
zerosexceptone,whichisreplacedbythecentralcapsule
C
:(0
:
5
;:::;
0
:
5).Theseprototypescan
beseenasthereferenceimagesfromwhichHitNetevaluatesthesimilaritywiththeinputimage
throughHoM.
fortheprototypes.Webelievethatatleastthreefactorscouldprovideanexplanation:thedecoder
istooshallow,thesizeofthecapsulesistooshort,andthefactthatthedecoderhastoreconstruct
thewholeimage,includingthebackground,whichiscounterproductive.
Hybriddataaugmentation
Thequalityofthedatageneratedasdescribedabovedependsonmultiplefactors,suchasthe
numberoffeaturesextractedandthequalityofthedecoder.Givenarestrictedamountofin-
formation,thatis,capsulesofsize16,thedecodercanonlyreconstructapproximationsofinitial
images,whichmaynotlookrealisticenoughinsomecontexts.Inordertoincorporatethede-
tailslostinthecomputationofHoM,thehybridfeature-basedanddata-baseddataaugmentation
techniquedescribedinSection2.2canbeapplied.Theimportanceofaddingthedetailsand
thustheboverthesoledatagenerationprocesscanbevisualizedinFigure5withthe
FashionMNISTdataset.
Theperformancescanbemarginallyincreasedwiththisdataaugmentationprocess
asitappearedthatnetworkstrainedfromscratchonsuchdata(continuouslygenerated
performslightlybetterthanwhentrainedwiththeoriginaldata.OnMNIST,theaverageerror
rateon20modelsdecreasedto0
:
33%withaconstantlearningrateandto0
:
30%withadecreasing
learningrate.Inourexperiments,3ofthesemodelsconvergedto0
:
26%,oneconvergedto0
:
24%.
Somerunsreached0
:
20%testerrorrateatsomeepochs.Withabitofluck,ablindselectionof
onetrainednetworkcouldthusleadtoanewstateoftheart,eventhoughitisknownthatMNIST
digitsresultswillprobablynotreachbetterperformancesduetoinconsistenciesin
thetestset.TheaveragetesterrorrateonFashion-MNISTdecreasesby0
:
2%,onCIFAR10by
1
:
5%andonSVHNby0
:
2%.Theseresultscouldpresumablybeimprovedwithmoreelaborate
featuremapsextractorsanddecoders,giventheincreasedcomplexityofthesedatasets.Thedata
augmentationisnotperformedonsincethepurposeofthisdatasetistomeasurethe
robustnessofthenetworktotransformations.
3.3Analyzingatrainingsetanddetectingpossiblymislabeledimages
withghostcapsules
Inthissection,weillustratetheuseofghostcapsulesdescribedinSection2.3toanalyzethe
trainingsetofMNISTanddetectpotentiallyerroneouslabels.WetrainHitNet20timeswith
12
(a)
(b)
(c)
(d)
Figure5:ExamplesofhybriddataaugmentationusingtheFashion-MNISTdataset(theresolution
isincreasedformorevisualcomfort).In(a)and(b),thetopleftimageisanoriginalimage
X
fromthedatasetandtheimageonitsrightisitscorrespondingreconstruction
X
rec
fromHoM
(theseimagesarethesamein(a)and(b)).In(a),the23remainingimagesaremoedversions
of
X
rec
obtainedbytweakingthecomponentsofitscapsuleinHoM.Theseimagescorrespond
to23examplesof
X
mod
.Wecanseethattheydonotcontainanydetailbutratherserveasa
skeletononwhichthedetails
X

X
rec
shouldbeadded.Addingthesedetailstoeachofthem
givesthe23remainingimagesof(b),thusdisplaying
X
mod
+
X

X
rec
,clippedtothe[0
;
1]range,
whicharetheimagesgeneratedbythehybriddataaugmentationexplainedinthetext.Thesame
processisshownwithatoriginalimage
X
in(c)and(d).Wecanseethataddingthe
detailsallowstogenerateconvincingdetailedimages.
MNIST
Fashion-MNIST
CIFAR10
SVHN
HitNet
0.38
7.70
26.70
5.50
HitNetwithDA
0.33
7.50
25.20
5.30
HitNetwithDAanddlr(best)
0.24
-
-
-
Table3:Testerrorratesobtainedwiththehybriddataaugmentation(DA)process,showing
slightlybetterperformances.OnMNIST,weobtainedstate-of-the-artperformancewithade-
creasinglearningrate.
13
GC,whichgivesus20models,andweexaminethe20GC
2
associatedwitheachimagebythese
models.
Agreementbetweenthemodels
Westudytheagreementbetweenthemodelsaboutthe20GCclassesselectedforeachimage.
Forthatpurpose,weexaminethedistributionofthenumberofdierentclasses
N
c
giventothe
20GCofeachimage.Itappearsthatthe20modelsallagree(
N
c
=1)onthesameGCclassfor
28%ofthetrainingimages,whichrepresents16
:
859images.
Toouranalysis,wenowfocusonthe16
:
859imagesthathavealltheirGCinthesameclass
(
N
c
=1)andthatpotentiallymakesomehits.Theseimageshaveapair(trueclass,GCclass)
andtheirdistributionsuggeststhatsomepairsaremorelikelytooccur,suchas(3
;
5)for2333
images,(4
;
9)for2580images,(7
;
2)for1360images,(9
;
4)for2380images,whichgivesaglimpse
oftheclassesthatmightbelikelytobemixedupbythemodels.Confusionsmayoccurbecause
oferrorsinthetruelabels,butsincethesenumbersareobtainedonthebasisofanagreementof
20modelstrainedfromthesamenetworkstructure,thismayalsoindicatethelimitationsofthe
networkstructureitselfinitsabilitytoidentifythetclasses.However,adeeperanalysis
isneededtodetermineifthesenumbersindicatearealconfusionornot,thatis,whichofthem
makehitsandwhichdonot.
Sofar,weknowthatforthese16
:
859images,the20modelsagreeononeGCclass(
N
c
=1).We
canexamineifthe20modelsagreeontheirdistancefrom
C
fortheirclass.Forthatpurpose,for
eachimage,wecomputethemeanandthestandarddeviationofits20GCdistancesfrom
C
.An
interestingobservationisthatwhenthemeandistancegetsclosertothehitzonethreshold(which
is
m
=0
:
1),thenthestandarddeviationdecreases,whichindicatesthatallthemodelstendto
agreeonthefactthatahitisneeded.Thisisparticularlyinterestinginthecaseofmislabeled
images(examplesareprovidedhereafter,inFigure6).Indeed,ifanimageofa\4""islabeledas
\5""(confusion),notonlyarethe20modelslikelytoprovide20GCwiththesameclass(\4""here),
buttheywillalsoallmakeahitinthatclass\4""andthusidentifyitasahighlydoubtfulimage.
Idenofdoubtfulimages
Wecannownarrowdowntheanalysistotheimagesthatarethemostlikelytobemislabeled,that
is,thosewithameandistancesmallerthan
m
;thereareonly71suchimagesleft.Theassociated
pairs(trueclass,GCclass)aregiveninTable4.Fromtheexpectedconfusionsmentionedabove
(3
;
5),(4
;
9),(7
;
2),(9
;
4),wecanseethat(7
;
2)isactuallynotsomuchrepresentedinthepresent
case,whilethepairs(1
;
7)and(7
;
1)subsistedinalargerproportion,andthatthepairs(4
;
9)and
(9
;
4)accountforalmosthalfoftheimagesofinterest.
Thelasttthatwemakeisalookatthenumberofhitsamongthe20GCofthese71
images.Weknowthattheirmeandistancefrom
C
issmallerthan
m
,butneitherthismean
distancenorthestandarddeviationclearlyindicatehowmanyofthe20modelsactuallymake
ahitforthese71images.Itappearsthatalltheseimageshaveatleast55%(11
=
20)oftheir
GCinthehitzoneandthatmorethan75%(55
=
71)oftheimageshaveahitforatleast75%
(15
=
20)ofthemodels,whichindicatesthatwhenthemeandistanceissmallerthan
m
,itisthe
resultofastrongagreementbetweenthemodels.Finally,the71images,sortedbynumberofhits,
arerepresentedinFigure6.Thetruelabel,theGCclass,andthenumberofhitsareindicated
foreachimage.Someoftheseimagesareclearlymislabeledandsomeareterriblyconfusingby
lookingalmostthesamebuthavingtlabels,whichexplainstheGCclassselectedandthe
numberofhitsobtained.Whilepursuingadierentpurpose,theDropMaxtechniqueusedin[13]
2
Forlighternotations,wenoteGCboththesingularandthepluralformofghostcapsule(s),whichwillnotbe
misleadinginthefollowinggiventhecontext.
14
Trueclass
Total
0123456789
GCclass
0
0000000000
0
1
0000
2
00
6
00
8
2
0000000
1
00
1
3
00000
2
0000
2
4
000000
1
00
20
21
5
000
8
00
1
000
9
6
00000
2
0000
2
7
0
7
2
0
1
0000
2
12
8
000
1
000000
1
9
000
1
13
00
1
00
15
Total
0721016428022
71
Table4:Distributionofpairs(trueclass,GCclass)forMNISTtrainingimageshavingaunique
GCclassandtheirmeandistancefrom
C
smallerthanthehitzonethreshold.
allowedtheauthorstoidentify\hardcases""ofthetrainingsetwhichareamongthe71images
representedinFigure6.
Followingthesameprocessasabove,weuseghostcapsuleswithSVHN.Again,weareableto
detectimagesthatareclearlymislabeled,asseeninFigure6.Inaddition,inthiscase,theuse
ofghostcapsulesallowsustodetectimagesthatdeservemultiplelabelssincemultipledigitsare
presentintheimages,asseeninFigure6.
Ghostcapsulescouldbeusedinothercontexts.Forexample,wecouldimaginethatahuman
annotatorhesitatesbetweentwoclassesinthelabellingprocessbecausetheimageisconfusing.
Theannotatorchoosesoneclassandtheghostcapsuledealswiththeconfusionbypotentially
choosingtheotherclasswithoutpenalizingthenetwork.Inanotherscenario,severalannotators
maygivetwotlabelsforthesameimage.Wecouldhandlethispotentialctwith
ghostcapsules,byattributingtheimageoneofthelabelssuggestedbytheannotators(
e.g
.
themostsuggested)thentrainingthenetworkandcheckingwhethertheghostcapsuleof
thatimageisassociatedwiththeotherlabelsuggestedand,ifso,whetheritmakesahitornot.
4Conclusion
WeintroduceHitNet,adeeplearningnetworkcharacterizedbytheuseofaHit-or-Misslayer
composedofcapsules,whicharecomparedtocentralcapsulesthroughanewcentripetalloss.The
ideaisthatthecapsulecorrespondingtothetrueclasshastomakeahitinitstargetspace,and
theothercapsuleshavetomakemisses.Thenoveltiesresideinthereinterpretationandintheuse
oftheHoMlayer,whichprovidesnewinsightsonhowtousecapsulesinneuralnetworks.Besides,
wepresenttwoadditionalpossibilitiesofusingHitNet.Intheone,weexplainhowtobuild
prototypes,whichareclassrepresentatives,howtodeformthemtoperformdatageneration,and
howtosetupahybriddataaugmentationprocess.Thisisdonebycombininginformationfrom
thedataspaceandfromthefeaturespace.Inthesecondone,wedesignghostcapsulesthatare
usedtoallowthenetworktoalleviatethelossofcapsulesrelatedtoplausiblealternativeclasses.
Inourexperiments,wedemonstratethatHitNetiscapableofreachingstate-of-the-artperfor-
mancesonMNISTdigitstaskwithashallowarchitectureandthatHitNetoutper-
formstheresultsreproducedwithCapsNetonseveraldatasets,whilebeingatleast10timesfaster.
TheconvergenceofHitNetdoesnotneedtobeforcedbyadecreasinglearningratemechanismto
reachgoodperformances.HitNetdoesnotseemtofromovandprovidesasmall
variabilityintheresultsobtainedfromseveralruns.Wealsoshowhowprototypescanbebuilt
15
(a)
(b)
Figure6:(a)The71images(resolutionisincreasedformorevisualcomfort)ofMNISTtraining
setwhose20GChaveameandistancefrom
C
smallerthanthehitzonethreshold,sortedby
anincreasingnumberofhits.Thethreenumbersinbracketsindicaterespectivelythetruelabel,
theGCclass,thenumberofhitsintheGCclass.(b)AselectionofimagesofSVHNobtained
followingthesameprocesswithghostcapsules.Intherow,theimagesareclearlymislabeled.
Inthesecondrow,theimagesdeserveseverallabels.
16
asclassrepresentativesandweillustratethehybriddataaugmentationprocesstogeneratenew
realisticdata.Thisprocesscanalsobeusedtomarginallyincreaseperformances.
Finally,weexamplifyhowtheghostcapsuleshelpidentifyingsuspiciouslabelsinthetrainingset,
whichallowstopinpointimagesthatshouldbeconsideredcarefullyinthetrainingprocess.
Futurework
Asfarastheperformancesareconcerned,oneofthemainadvantagesoftheHoM
layeristhatiscanbeincorporatedinanyothernetwork.Thisimpliesthatthesub-partofHitNet
usedtocomputethefeaturemapsthatarefullyconnectedtoHoMcanbereplacedbymore
elaboratenetworkstoincreasetheperformancesonmorecomplextasks.
Inasimilarway,theprototypesandallthereconstructionsmadebythedecodercouldbeimproved
byusingamoreadvanceddecodersub-networkandcapsuleswithmorecomponents.Inreal-life
casessuchasCIFAR10andSVHN,itcouldalsobeusefultomakeadistinctionbetweentheobject
ofinterestandthebackground.Forexample,featuresdesignedtoreconstuctthebackgroundonly
couldbeused.Ifsegmentationmasksareavailable,onecouldalsousethecapsulestoreconstruct
theobjectofinterestinthesegmentedimage,orsimplythesegmentationmask.Onecouldalso
imaginetoattachtweightstothefeaturescapturedbythecapsules,sothatthosenot
usefulfortheareusedinthereconstructiononly.ThetyofHoMallowsto
implementsuchideaseasily.
Regardingghostcapsules,theycouldbeusedtoperformabuilt-intop-kbyusing
k
ghostcapsulesinthetrainingprocess.Inabinarytask,theycouldbeusedonly
afteragivennumberofepochs,whichwouldmakemoresensethanusingthematthebeginningof
thetraining.Theghostcapsulecouldalsogeneratesomelossbutwithagivenprobabilitytotake
intoaccountthefactthatitdoesnotcorrespondtothetrueclassandthatisshouldbepenalized
insomeway.
ComparingtheresultsonotherbenchmarkdatasetswouldhelppromotingHitNetinanearfuture
aswell.
Acknowledgements
WearegratefultoM.BrahamforintroducingustotheworkofSabouretal.andforallthe
fruitfuldiscussions.
ThisresearchissupportedbytheDeepSportprojectoftheWalloonregion,Belgium.A.Cioppa
hasagrantfundedbytheFRIA,Belgium.WealsothankNVIDIAforthesupport.
References
[1]
P.Afshar,A.Mohammadi,andK.Plataniotis.Braintumortypeviacapsule
networks.
CoRR
,abs/1802.10200,February2018.
[2]
P.-A.Andersen.Deepreinforcementlearningusingcapsulesinadvancedgameenvironments.
CoRR
,abs/1801.09597,January2018.
[3]
M.Bahadori.Spectralcapsulenetworks.In
InternationalConferenceonLearningRepresen-
tations(ICLR)
,April-May2018.Withdrawnpaper.
[4]
J.Deng,W.Dong,R.Socher,L.Li,K.Li,andL.Fei-Fe.ImageNet:Alarge-scalehierar-
chicalimagedatabase.In
IEEEInternationalConferenceonComputerVisionandPattern
Recognition(CVPR)
,pages248{255,Miami,Florida,USA,June2009.
17
[5]
X.Guo.CapsNet-Keras.https://github.com/XifengGuo/CapsNet-Keras,2017.
[6]
G.Hinton,A.Krizhevsky,andS.Wang.Transformingauto-encoders.In
InternationalCon-
ferenceonANeuralNetworks(ICANN)
,volume6791of
LectureNotesinComputer
Science
,pages44{51.Springer,2011.
[7]
G.Hinton,S.Sabour,andN.Frosst.MatrixcapsuleswithEMrouting.In
International
ConferenceonLearningRepresentations(ICLR)
,Vancouver,BC,Canada,April-May2018.
[8]
S.andC.Szegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducing
internalcovariateshift.In
InternationalConferenceonMachineLearning(ICML)
,pages
448{456,Lille,France,July2015.
[9]
A.Krizhevsky.Learningmultiplelayersoffeaturesfromtinyimages,2009.Technicalreport,
UniversityofToronto.
[10]
R.LalondeandU.Bagci.Capsulesforobjectssegmentation.
CoRR
,abs/1804.04241,May
2018.
[11]
Y.Lecun,L.Bottou,Y.Bengio,andP.Gradient-basedlearningappliedtodocument
recognition.
ProceedingsofIEEE
,86(11):2278{2324,November1998.
[12]
Y.LeCun,FuJieHuang,andL.Bottou.Learningmethodsforgenericobjectrecognition
withinvariancetoposeandlighting.In
IEEEInternationalConferenceonComputerVision
andPatternRecognition(CVPR)
,volume2,pages97{104,Washington,DC,USA,June-July
2004.
[13]
H.Lee,J.Lee,S.Kim,E.Yang,andS.Hwang.DropMax:Adaptivevariationalsoftmax.
CoRR
,abs/1712.07834,December2017.
[14]
Y.Li,M.Qian,P.Liu,Q.Cai,X.Li,J.Guo,H.Yan,F.Yu,K.Yuan,J.Yu,L.Qin,H.Liu,
W.Wu,P.Xiao,andZ.Zhou.TherecognitionofriceimagesbyUAVbasedoncapsule
network.
ClusterComputing
,pages1{10,March2018.
[15]
H.Liao.w.https://github.com/naturomics/CapsNet-Tw,2018.
[16]
W.Liu,E.Barsoum,andJ.Owens.Objectlocalizationandmotiontransferlearningwith
capsules.
CoRR
,abs/1805.07706,May2018.
[17]
P.Nair,R.Doshi,andS.Keselj.Pushingthelimitsofcapsulenetworks.Technicalnote,
2018.
[18]
Y.Netzer,T.Wang,A.Coates,A.Bissacco,B.Wu,andA.Ng.Readingdigitsinnatural
imageswithunsupervisedfeaturelearning.In
AdvancesinNeuralInformationProcessing
Systems(NIPS)
,volume2011,Granada,Spain,December2011.
[19]
J.O'Neill.Siamesecapsulenetworks.
CoRR
,abs/1805.07242,May2018.
[20]
L.PerezandJ.Wang.Theenessofdataaugmentationinimageclasusing
deeplearning.
CoRR
,abs/1712.04621,December2017.
[21]
D.Rawlinson,A.Ahmed,andG.Kowadlo.Sparseunsupervisedcapsulesgeneralizebetter.
CoRR
,abs/1804.06094,April2018.
[22]
S.Sabour,N.Frosst,andG.Hinton.Dynamicroutingbetweencapsules.
CoRR
,
abs/1710.09829,October2017.
[23]
A.Shahroudnejad,A.Mohammadi,andK.Plataniotis.Improvedexplainabilityofcapsule
networks:Relevancepathbyagreement.
CoRR
,abs/1802.10204,February2018.
18
[24]
T.Shin.CapsNet-TensorFlow.https://github.com/shinseung428/CapsNetTw,2018.
[25]
N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhutdinov.Dropout:
Asimplewaytopreventneuralnetworksfromov
JournalofMachineLearning
Research
,15(1):1929{1958,January2014.
[26]
T.Tieleman.https://www.cs.toronto.edu/2013.
[27]
L.Wan,M.Zeiler,S.Zhang,Y.LeCun,andR.Fergus.Regularizationofneuralnetworks
usingdropconnect.In
InternationalConferenceonMachineLearning(ICML)
,volume28,
pages1058{1066,Atlanta,Georgia,USA,June2013.
[28]
D.WangandQ.Liu.Anoptimizationviewondynamicroutingbetweencapsules.In
Interna-
tionalConferenceonLearningRepresentations(ICLR)
,Vancouver,BC,Canada,April-May
2018.
[29]
Q.Wang,T.Ruan,Y.Zhou,C.Xu,D.Gao,andP.He.Anattention-basedBi-GRU-CapsNet
modelforhypernymydetectionbetweencompoundentities.
CoRR
,abs/1805.04827,May
2018.
[30]
S.Wong,A.Gatt,V.Stamatescu,andM.McDonnell.Understandingdataaugmentationfor
Whentowarp?In
DigitalImageComputing:TechniquesandApplications
,
pages1{6,GoldCoast,Queensland,Australia,November-December2016.
[31]
E.Xi,S.Bing,andY.Jin.Capsulenetworkperformanceoncomplexdata.
CoRR
,
abs/1712.03480,December2017.
[32]
L.Zhang,M.Edraki,andG.-J.Qi.CapProNet:Deepfeaturelearningviaorthogonalpro-
jectionsontocapsulesubspaces.
CoRR
,abs/1805.07621,May2018.
19
"
17,Predicting Citation Counts with a Neural Network,http://arxiv.org/pdf/1806.04641v2.pdf,https://github.com/tmistele/predicting-citation-counts-net,"PredictingCitationCountswithaNeuralNetwork
TobiasMistele

,TomPrice,SabineHossenfelder


FrankfurtInstituteforAdvancedStudies
Ruth-Moufang-Str.1,D-60438FrankfurtamMain,Germany
Abstract
Weheredescribeandpresentresultsofasimpleneuralnetworkthatpredicts
individualresearchers'futurecitationcountsbasedonavarietyofdatafromthe
researchers'past.Forpublicationsavailableontheopenaccess-serverarXiv.org
weahigherpredictabilitythanpreviousstudies.
1Introduction
Measuringandpredictingexcellenceisasdauntingascontroversial.Butre-
gardlessofhowonefeelsaboutquantifyingquality,measuresforsuccessare
beingused,andtheywillcontinuetobeused.Thebestthatwe,asscientists,candoisto
atleastcomeupwithgoodmeasures.
Theattempttoquantifysuccessbythenumberofpublicationsdatesback
totheearly19thcentury[1],buttheideareallytookoffwiththeadventoftheinternet
whendataforpublicationsandcitationsbecameeasilyaccessible.Sincethen,alargeva-
rietyofdifferentmeasureshasbeensuggestedwhichrelyoncitationcounts,publication
statistics,impactfactors,mediacoverage,andmore.Forarecentreview,thereaderis
referredto[2].
Wemaintainthatthebestwaytoassessaresearcher'spromiseistostudytheirwork
indepth.Butbowingtotheneedtohaveatoolforadministrativeandorganizational
purposesthatisfastandeasytouseandallowsatleastsuperevaluation,wehere
reportonthetrainingofaneuralnetworktoimproveonpresentlyexistingmeasures.
Duetothelargevarietyofexistingmeasures,wewillnotcomparethemethodpre-
sentedheretoallofthem.Wewillfocusinparticularonthreepreviousstudiesthatput
ourworkintoperspective.Thisis(1)theoriginalpaperaboutthepredictivityofthe
Hirsch-index[3](hereafter
h
-index),(2)a2012study[4]whichusedalinearregression
modeltopredictthe
h
-indexforresearchinthelifesciences,and(3)a2017report[5]on
variousmachinelearningmodelsusedtopredictthe
h
-indexforalargecohortofauthors
inthecomputersciences.
1
arXiv:1806.04641v2  [cs.DL]  18 Jun 2018PredictingCitationCountswithaNeuralNetwork
Ourpaperisorganizedasfollows.Inthenextsectionweclarifyexactlywhatweaim
toachieve.Insection3wedocumentwhichinputdatawehaveusedandhowwehave
encodedit.Insection4weexplainhowwesetuptheneuralnetwork,andin5wewill
presentourresults.Wewithadiscussionandconclusioninsection6.
2Aim
Beforewebuildaneuralnetwork,weneedtomakeprecisewhatwemeanbyﬁpre-
dictiveﬂandhowwewillmeasurethisﬁpredictiveness.ﬂ
Ourneuralnetworkwillbefedpublicationdata(section3)foratraininggroupof
researchersduringaphaseofpublishingactivity,hereafterreferredtoasthe`ini-
tialperiod'.Theaimisthentousetheneuralnetwork(section4)topredictindividual
authors'performanceinasecondphaseofpublishingactivity,hereafterreferredtoas
the`forecastingperiod'.Thisinputdatafortheinitialperioddoesnotincludecitations
frompapersthatwereonlypublishedduringtheforecastingperiod.Thepredictionpe-
riodstartsat1/1/2008whichischosensuchthatwecanevaluatetheneuralnetwork's
performanceforpredictionsupto10yearsintothefutureoftheinitialperiod.
Oncethenetworkhaslearnedforecastingfromthetraininggroup,wemakeaforecast
fortheremainingresearchersŒtheﬁvalidationgroupﬂŒandevaluatehowgoodour
forecastwas.Thisistosay,inthispresentstudywedonotmakeactualfutureforecasts
becausewewanttoassesshowwelloutnetworkperforms,butourmethodisdesigned
sothatitcouldbeusedtomakerealforecasts.
Wewillusetwodifferentapproachestoevaluatetheforecastingperformanceinthe
validationgroup.
Theapproach(seesection5.1)followstheprocedureusedinRef.[3],which
evaluatedthepredictivenessofthe
h
-indexforvariousquantitiesintermsofthecorrela-
tioncoef
r
.Inthisapproach,wedonotincludecitationsfrompapersintheinitial
periodinthenumberofcitationstobeforecastedintheforecastingperiod.Inmaking
thiscleanseparation,wegetabettergrasponpredicting
future
achievementas
opposedto
cumulative
achievement.
Forthisapproach,wefollowthenotationof[3]anddenotethenumberofcita-
tionsreceivedbetween
t
1
and
t
2
,i.e.duringtheforecastingperiod,as
N
c
(
t
1
;
t
2
)
.Sinceit
isknownthatthe
h
-indexroughlyscaleswiththesquare-rootofcitations,wewillmore
preciselyuse
N
c
(
t
1
;
t
2
)
1
=
2
tomakeiteasiertocompareourresultswiththoseofRef.[3].
Thesecondapproach(seesection5.2)followsadifferentprocedurewhichisbetter
suitedforcomparisonwiththeresultsofRefs.[4,5].Forthis,wefeedtheneuralnetwork
thesameinputdataasintheapproachbutthenpredictthecumulative
h
-indexafter
n
yearsuntiltheendoftheforecastingperiod.Furthermore,inthissecondapproachweuse
thecoefofdetermination,
R
2
,insteadofthecorrelationcoef
r
,toquantify
thegoodnessofourpredictionbecausethesameprocedurewasfollowedinRefs.[4,5].
Unlessotherwisestated,ourgeneralprocedureistoemploy20roundsofMonte
2
PredictingCitationCountswithaNeuralNetwork
Carlocross-validationŒi.e.redotherandomsplitintotrainingandvalidationdata20
timesandretraintheneuralnetworkŒandreportthemeanaswellasthestandarddevia-
tion.Onereasonforemployingcross-validationisthatdifferentsplitsofourdatasetinto
trainingandvalidationdataleadstoslightlydifferentresultsaswillbefurtherdiscussed
below.Anotherreasonisthatourdatasetisnotparticularlylarge.Cross-validationthen
allowstoavoidsplittingourdatasetintoatraining,avalidation,andtestgroupwhilestill
avoidingovtoaparticularsplitintotrainingandvalidationdata.
3Data
WehaveobtainedthepublicationsforeachauthorfromthearXivthroughthepublicly
availableOpenArchivesInitiativeAPI[6]andcorrespondingcitationdatafromPaper-
scape[7].Forthepurposesofthispresentwork,weconsideronlythepublicationsinthe
`physics'set,whichgivesusatotalof934,650papers.JournalImpactFactors(JIFs)are
takenfromRef.[8].Wegrouptogethersimilarauthornamesandtreatthemasasingle
authorbythesameprocedureaslaidoutin[9].
Fromthecompletedataset,weselectasampleofauthorsandtrimitinvarious
ways.First,werequirethattheypublishedtheirarXivpaperbetween1/1/1996
and1/1/2003.Wehavechosenthatperiodtospanthetimebetween5and12yearsprior
tothecutoffwhichmatchestheprocedureofRef.[5].
Wethenremoveauthorswhohavepublishedfewerthan5ormorethan500papers
toavoidstatisticaloutlierswhichwouldundulydecreasethepredictivityofourmethod.
Finally,weexcludecollaborations,sincetheirpublicationactivitydiffersgreatly
fromthatofindividuals.Forthis,weremoveallauthornamesthatcontaintheword
`collaboration'andallpaperswithmorethan30authors.
Afterthis,weareleftwithasampleof39,412authorIDs.Fromthese,werandomly
choseasubsetof28,000asthe`traininggroup'.Therestisourvalidationdatabyhelp
ofwhichweevaluatehowwelltheneuralnetworkperformsaftertrainingiscompleted.
Notethatthisrandomsplitintotrainingandvalidationdataisdoneindependentlyfor
eachroundofcross-validation.
4TheNeuralNetwork
TheneuralnetworkitselfisbuiltusingKeras[10]withtheTensorFlowbackend[11].
Weusedafeedforwardneuralnetwork,whichmeansthattheneuralnetworkconsists
oflayersofneuronswheretheinputoftheneuronsinonelayeristheoutputofthe
neuronsinthepreviouslayerandthelayersarenotarrangedasacycle.Theoutputof
thelayeraretheinputdatadescribedinAppendixA,andtheoutputofthelastlayer
istakenastheoutputofthewholeneuralnetwork.Inourcase,thelastlayerconsistsof
tenneuronssuchthattheneuralnetwork'soutputislistoftenrealnumbers.
3
PredictingCitationCountswithaNeuralNetwork
Forthisnetwork,theoutputoftheneuronsinonelayerfollowsfromtheoutputof
theneuronsinthepreviouslayerby
x
0
=
s
(
W

x
+
b
)
:
(1)
Here,
x
0
isavectorwhichcontainstheoutputsofthe
N
0
neuronsinonelayer,
x
isavector
whichcontainstheoutputsofthe
N
neuronsinthepreviouslayer,
W
isareal
N
0

N
matrixwhoseelementsarecalledtheweightsand
b
isarealvectorwith
N
0
elements
whicharecalledthebiases.Further,
s
isafunctionwhichisappliedtoeachelement
ofthevector
W

x
+
b
andiscalledtheactivationfunction.Theweightsandbiases
aredifferentforeachlayer,sothatforeachaddedlayeronegetsanotherweightmatrix
andanotherbiasvector.Thesearethefreeparametersoftheneuralnetworkwhichare
determinedbythetrainingprocedure.
Figure1:Flowdiagramoftheneuralnetwork.Networkelementssurroundedbysingle
thinlinesaresingleneurons;thosewhithdoublethinlinesarecollectionsofneurons.
Connectionsshownwithboldlineshaveaedweightof1.
Duringthetrainingoftheneuralnetwork,theoutputoftheneuralnetworkiscal-
culatedwiththetrainingdatadescribedaboveasinputandtheweightsandbiasesare
adjustedinordertogettheoutputoftheneuralnetworkforeachauthorascloseaspos-
sibletotheactual
N
c
(
t
1
;
t
2
)
1
=
2
.Moreprecisely,theweightsandbiasesareadjustedin
ordertominimizetheso-calledlossfunctionwhichwetaketobethemeansquarederror
acrossallauthorsinthetrainingset.Notethatforaso-calledfully-connectedlayerall
elementsoftheweightsmatrixandthebiasvectorareindependentlyadjusted,whilefor
4
PredictingCitationCountswithaNeuralNetwork
othertypesoflayers,e.g.so-calledconvolutionallayers,somestructureisimposed.As
willbeexplainedbelow,weusebothtypesoflayers.
Sinceourdatasetisnotparticularlylarge,wetriedtoavoidovbyreducing
thenumberofparametersandstructuredthenetworkasfollows(seeFigure1):

Thelayerisaconvolutionallayerandcontainsk=70neuronsforeachpaper.
Althoughconvolutionallayersarenormallyusedtoexploittranslationalsymmetry
intheinputdata,weuseaconvolutionallayerwithaconvolutionwindowofa
singlepaperinordertoensureinvarianceunderpermutationsofthepapers.The
effectofthisisthateachpaperhasacorrespondingsetof70neuronswhichsee
onlythedatafromthatpaper,andeachpaper'sneuron-sethasmatchingweights
andbiases.Theinputtothislayercontainseveryinputexceptthebroadnessvalue,
i.e.,itcontainsallperpaperinputbutnottheperauthorinput.

Inanextstep,these70neuronsperpaperarereducedto70neuronsintotalbyav-
eragingeachofthe70neuronsoverthedifferentpapers.Afterthis,noinformation
aboutindividualpapersisleft,andonlyaveragevaluesareretainedintheremain-
ing70neurons.Thislayerdoesnotaddfreeparameterstotheneuralnetwork.

Afterthat,afully-connectedlayerwith70neuronsisadded.Inadditiontothe
outputofthepreviouslayer,thislayerobtainsinputwhichisctotheauthor
nottotheindividualpapers.

Thelayerisafully-connectedlayerwithtenneuronswithaReLuactivation
function.Theneuronrepresentstheprediction1yearafterthecutoffandthe
otherneuronsrepresentthedifferencesbetweenthepredictionafter
n
and
n

1
years.Forinstance,iftheneuralnetwork'soutputis
[
5
;
0
;
1
;:::
]
,thecorresponding
predictionis5for1
=
1
=
2009,5for1
=
1
=
2010,6for1
=
1
=
2011,etc.Thisensures
thattheneuralnetwork'spredictionisamonotonicallyincreasingtimeseries.
AdetailedlistofinputdataforeachlevelcanbefoundinAppendixA.Theneural
networkarchitecturedescribedabovecanbeimplementedinKeraswithjustafewlines
ofcodewhicharereproducedinAppendixB.
Theneurons,exceptfortheneuronsintheoutputlayer,aretakentohavetanhactiva-
tionfunctions.TrainingisdoneusinganAdamoptimizerandameansquarederrorloss
function.Noregularizationisemployedandtheresultisobtainedafter150epochs
oftrainingwithabatchsizeof50.
Wewouldliketoendthissectionwithacommentonthewaytheneuralnetwork
describedabovemakespredictionsfordifferentyears
n
afterthecutoff.Asdescribed
above,eachyear
n
correspondstooneofthe10neuronsintheverylastlayerofthe
neuralnetwork.Analternativewouldbetohave10neuralnetworkswithonlyasingle
neuronintheoutputlayer.Eachofthenetworkswouldthenbetrainedtomakeapre-
dictionforoneparticular
n
.Onemightarguethatthisalternativeleavesmorefreedom
5
PredictingCitationCountswithaNeuralNetwork
fortheneuralnetworkstolearntherequirementsinmakingapredictionforone

n
insteadofforall
n
atthesametime.However,itseemsthisisnotthecase
inpractice,sincewehavetriedbothapproacheswiththeonlydifferenceintheneural
networkarchitecturebeingthenumberofneuronsintheoutputlayerandtheresulting
performancewasverysimilar.
5Results
Forbothapproaches,wewillcomparetheneuralnetwork'sperformancetothatofanaive
h
-indexpredictorwhichisgivenonlythe
h
-indexofanauthorforwhichapredictionisto
bemade.Bythisnaive
h
-indexpredictorwemeanthefollowing:Foragivenquantityto
predict,i.e.thefuturecumulative
h
-indexor
N
c
(
t
1
;
t
2
)
1
=
2
,takeallauthorsinthetraining
setwithagiven
h
-index
h
0
atthetimeofthecutoff.Then,calculatethearithmeticmean
ofthequantitytobepredictedandtakethismeanvalueasapredictionforauthorsinthe
validationsetgiventheir
h
0
.
Notethattheremaybeauthorsinthevalidationsetwith,typicallyhigh,valuesof
h
0
thatarenotpresentinthetrainingset.Forthoseauthors,thepredictionisdeterminedas
follows.First,alinearpolynomialistothenaive
h
-indexpredictorforthevaluesof
h
0
thatcanbecalculatedinthewaydescribedinthepreviousparagraph.Then,thevalue
ofthepolynomialistakenasthepredictionfortheothervaluesof
h
0
.
5.1Comparisonwiththe
h
-index
Fortheneuralnetworktrainedtopredict
N
c
(
t
1
;
t
2
)
1
=
2
,weareonlyinterestedinthepre-
dictionfor
n
=
10yearsafterthecutoff.Therefore,weignorethepredictionofboththe
neuralnetworkandthenaive
h
-indexpredictorforthe9yearsafterthecutoff.
Thecorrelationcoef
r
betweeentheneuralnetwork'spredictionand
N
c
(
t
1
;
t
2
)
1
=
2
is
r
=
0
:
725

0
:
007(seeFig.2).Theerrorhereandinthefollowingisthestandarddevi-
ationacrossthe20roundsofcross-validation.Incomparison,thenaive
h
-indexpredictor
describedaboveyields
r
=
0
:
551

0
:
008.Weclearlyseethattheneuralnetworkper-
formsbetterthanthenaive
h
-indexpredictor.
Next,wetestedwhetheranyonetypeofinputdataisespeciallyimportanttothe
performanceoftheneuralnetworkbytrainingthenetworkwithsingletypesofinput
dataseparatelyremoved.This,wefound,barelychangestheresults.Thebiggestim-
pactcomesfromremovingthepaperdatesandthenumberofcitations,whichmakethe
correlationcoefdropto
r
=
0
:
684

0
:
007and
r
=
0
:
718

0
:
005,respectively,
whichisstillveryclosetotheoriginal
r
=
0
:
725

0
:
007.Wemayspeculatethatthe
networkgathersitsinformationfromcombinationsofinputwhicharethemselvespartly
redundant,sothatremovinganysingleonehaslittleeffect.
Finally,wecheckedwhethertheneuralnetstillperformsbetterthanthe
h
-index
whengivenonlycitationcountsasinputdata.Thisresultedinacorrelationcoefof
6
PredictingCitationCountswithaNeuralNetwork
Figure2:Theneuralnetwork'spredictionfor
N
c
(
t
1
;
t
2
)
1
=
2
comparedtotheactualvalue
forallauthorsinthevalidationdataset.Shownistheresultfromtheroundofcross-
validation.
r
=
0
:
579

0
:
007.Weseethattheneuralnetperformsbetterthanthe
h
-indexevenwith
onlythenumberofcitationsasinputdata.
FormoredetailsonthecomparisonbetweenourresultsandthoseofRef.[3],please
seeAppendixC.
5.2Comparisonwithearliermachinelearningpredictions
Forthesecondapproach,wecomparethepredictedwiththeactualcumulative
h
-index
for
n
=
1
;
2
:::
10yearsafterthecutoffdate.Wequantifythegoodnessofthisprediction
withthecoefofdetermination,
R
2
,bothfortheneuralnetworkandforthenaive
h
-indexpredictordiscussedatthebeginningofthissection.Notethatwecalculate
R
2
separatelyforeach
n
byrestrictingthepredictionsaswellastheactualvaluestothat
particular
n
.TheresultisshowninFig.3.Theerrorbarsshow

1standarddeviations
calculatedfromthe20roundsofcross-validation.
Weseethattheneuralnetandthenaive
h
-indexpredictoraresimilarlypredictive
for
n
=
1,buttheneuralnetwork'spredictionbecomesbetterincomparisonforlarger
n
.
ThisagreeswiththeRefs.[4,5].
Asmentionedinsection2,thereareoftheneuralnetwork'sperformance
withtheroundsofcross-validation.TheseareillustratedinFig.4.Inpartic-
ular,Fig.4showstheneuralnetwork's
R
2
(Fig.4,left)and
r
(Fig.4,right)after
n
=
10
7
PredictingCitationCountswithaNeuralNetwork
Figure3:
R
2
ofthepredictionofthecumulative
h
-indexasafunctionofyearsaftercutoff.
yearsforthedifferentroundsofcross-validation.Weseethattherearenon-negligible
inthenetwork'sperformance.Apossibleexplanationcouldbethatthe
tuationsareduetotheneuralnetworkovonparticularkindsofsplitsofthewhole
datasetintotrainingandvalidationdata.However,wethinkitismorelikelythatthe
tuationsareduetointrinsicpropertiesofthedataset.Thisisbecausethenaive
h
-index
predictorhasonlyafewtensofparameterssothatovshouldnotbeanissue
butFig.4showsthattherearecomparableinthisnaive
h
-indexpredictor's
performancenonetheless.
InFig.5,left,weshowtheresultfortheneuralnetworkoperatingonthetraining
andvalidationdatasetsoftheroundofcross-validationwhentrainingfor5and10
additionalepochs.Weseethatthereareoftheneuralnetwork'sperformance
withthetrainingepochwhichtypicallyaffect
R
2
atorbelowtheonepercentlevel.In
8
PredictingCitationCountswithaNeuralNetwork
Figure4:Left:Neuralnetwork'sandnaive
h
-indexpredictor's
R
2
after
n
=
10yearsfor
eachroundofcross-validation.Right:Neuralnetwork'sandnaive
h
-indexpredictor's
r
after
n
=
10yearsforeachroundofcross-validation.
Fig.5,right,weshowtheresultofaveragingacrossallroundofcross-validation,where
theaveragingisdoneseparatelyfortheneuralnetworksobtainedaftertrainingfor150,
155,and160epochs.Weseethatthewiththetrainingepochhavecancelled
andthedifferencebetweentheaveragesaftertrainingfor150,155,and160epochsis
negligible.
Quantitatively,ourresultsgivehighervaluesof
R
2
thanbothRef.[4](0.48after10
years)andRef.[5](0.72after10years).However,sincenotonlyourneuralnetworkbut
alsooursimple
h
-indexpredictorgivehigher
R
2
-valuesthanRefs.[4]and[5],thisdiffer-
enceisprobablypartlyduetothedifferentdatasets.Wehavethereforealsoappliedthe
predictorproposedin[4]toourdata-set,withtheresultsshowninFigure3(seeAppendix
Dfordetails).Thepredictabilityofourdata-setisindeedhigherthanthatstudiedin[4],
butthepredictionofournetworkstilloutperformsthepreviousstudy.Unfortunately,a
similardirectcomparisontotheresultsfrom[5]whichusedyetanotherdata-setisnot
possible.Still,ourvalueof
R
2
=
0
:
856

0
:
004aftertenyearsisremarkablypredictive,
especiallygiventhatthemethodswehaveemployedherearelikelytoimprovefurtherin
thesoonfuture.
Sincethevalueof
R
2
byitselfisnotsoilluminating,weshowinFigure6some
examplesforwhichwedisplaytheactual
h
-indexversusthenetwork-predictionwiththe
trainingandvalidationdatasetsfromtheroundofcross-validation.
1
Heretoowehaveinvestigatedhowimportantvariousinputdataaretotheneural
1
Thereaderbewarnedthattheseexampleswerenotrandomlychosen.Wehand-selectedasetofnotice-
ablydifferent
h
-indexoutcomesforpurelyillustrativepurposes.
9
PredictingCitationCountswithaNeuralNetwork
Figure5:Left:Networktrainedbyourdefaultvalueof150epochs,comparedwith
trainingfor155and160epochsfortheroundofcross-validation.Right:Average
across20roundsofcross-validationofthenetworktrainedbyourdefaultvalueof150
epochs,comparedwithtrainingfor155and160epochs.
network'sperformancebyremovingoneatatime.Theresultsfor
n
=
1
;
5
;
and10are
showninFig.7,whereweplottheratioofthecoefofdeterminationwithallinput
(
R
2
)andwithcertaininputsremoved(
R
2
rem
).Thelowertheratio,themoreimportantthe
removedinputwas.Theerrorbarsshow

1standarddeviationscalculatedfromthe20
roundsofcross-validation.
Weseethatfor
n
=
1,thenumberofcitationsistheonlyimportantinput,while
for
n
>
1otherinputsgainimportancewiththenumberofcitationsstillbeingthemost
importantone.Thatthecitationdataaremostimportantfor
n
=
1agreeswiththeresults
ofRef.[4]andRef.[5].WealsoseefromFig.7that
R
2
rem
=
R
2
isalwayslargerthan0
:
9
whichagainindicatesthattheinputdataarepartlyredundant.However,thechangeswe
noticeduetosomeinputremovalsaresosmallthatinthetrainingresultsare
nolongernegligible.
6Discussion
Wehavedemonstratedherethatneuralnetsarepowerfultoolstomakepredictionsfor
thecitationsaresearcher'sworkaccumulates.Thesepredictionsarelikelytoimprove
inthefuture.Oneofthemajorlimitationsofthispresentstudy,forexample,isthatour
sampledoesnotincludepaperswhicharenotonthearXiv,andthataboutoneofve
publishedpaperscouldnotbeassociatedwithaJournalImpactFactor(seeAppendixA).
Butthemorebibliometricdatabecomesavailablethemoreinputthenetworkcanbefed,
10
PredictingCitationCountswithaNeuralNetwork
Figure6:Exampletrajectoriesforactualdevelopmentofthe
h
-indexovertime
(solid/squares)andtrajectoriespredictedbythenetwork(dashed,circles)forthetraining
andvalidationdataoftheroundofcross-validation.
andthuspredictivityisboundtobecomebetterforsomemoretime.
Themethodsweusedherearestraight-forwardtoimplementanddonotrequiremuch
computingresources.Itisthusforeseeablethatinthenearfuturetheuseofneuralnets
topredictresearcher'spromisewillbecomemorewidelyspread.
Wethereforewanttourgethecommunitytonotignorethistrendinthehopeitwill
goaway.Itwouldacademicresearchifscientiststhemselvesproposedavariety
ofpredictors,andofferedavarietyofdata,tomoreaccuratelypresentthevarietyofways
todohigh-qualityresearch.
Inthisworkwefocusedonthe
h
-indexinordertocompareourresultswithprevious
results,andalsobecausethisvalueiseasytoextractfromexistingdata.Butalotof
valuableinformationaboutresearchersandtheirworkispresentlydiforimpossible
toobtainandanalyze.Forexample,howoftenresearchersarenamedinacknowledge-
ments,theirseminarandconferenceactivity,thefrequencybywhichtheyactaspeer
reviewersand/oreditors,orhowspecializedtheirresearchtopicis.Alltheseareimpor-
tantfactualsaboutthemanyindividualapproachestoresearch.Providingandanalyzing
suchdatawouldenableustodevelopmeasuresforsuccesstailoredtopurposes
andtherebyavoidthatresearchersfocuseffortsonoptimizingcitationcounts.
Acknowledgements
TMthanksSebastianWeichwaldforhelpfuldiscussions.Thisworkwassupportedby
theFoundationalQuestionsInstitute(FQXi).
11
PredictingCitationCountswithaNeuralNetwork
Figure7:Ratioof
R
2
oftheneuralnettothesameindicatorfortheneuralnetwithvarious
inputdataremoved,
R
2
rem
,asafunctionofyearspastcutoff.
AppendixA
Thebuildingblocksofmostoftheinputdatafortheneuralnetworkarelistswherethe
positioninthelistcorrespondstoapaperandthevalueatacertainpositioncorresponds
tosomeinputdataassociatedwiththecorrespondingpaper.E.g.thereisalistcontaining
thenumberofcitationsforeachpaperofthegivenauthorwhichinPythonsyntaxwould
looklike
[
130
;
57
;:::
]
,meaningthemost-citedpaperofthisauthorhas130citations,the
second-mostcitedpaperhas57citationsetc.Acorrespondinglistforthenumberof
coauthorswouldlooklike
[
0
;
1
;:::
]
,meaningthepaperwith130citationswasasingle-
authoredpaper,thepaperwith57citationshastwoauthorsetc.
Sincedifferentauthorshavedifferentnumbersofpapers,theselistswillhavediffer-
entlengthsfordifferentauthors.However,ourneuralnetworkrequiresaed-sizeinput.
Therefore,wetakethelistsforallauthorstohavethesamelengthasthoseoftheauthor
withthelargestnumberofpapers.Thepositionsintheliststhatdonotcorrespondtoa
paper,arewithzeros.
Thebiggestpartoftheinputtotheneuralnetworkisthenthelistofallthelists
describedaboveandconsistsof
1.
Alistwhichcontains1ateachpositioninthelistwhichcorrespondstoapaper
and0ateachpositionwhichcorrespondstozero-padding.
2.
Alistwhichcontainsthenumberofcitationsofeachpaper.
3.
Alistwhichcontainsthepublicationdateofeachpaperrelativetothecutoffdate.
12
PredictingCitationCountswithaNeuralNetwork
4.
Alistwhichcontainseachpaper'spagerank[12],aninteractivemeasureofrele-
vancethatworkssimilartoGoogle'spagerankalgorithmjustthat,insteadbeing
basedonhyperlinks,apaper'spagerankisbasedonthecitationgraph.Thepager-
ankiscalculatedfromthecitationgraphatthecutoffdate,1/1/2008.
5.
Alistwhichcontainseachpaper'slength.
6.
Alistwhichcontains0foreachpaperwithanemptyjournalreferenceand1for
eachpaperwithanon-emptyjournalreference.
7.
AlistwhichcontainstheJIFofthejournaleachpaperispublishedin(further
detailsbelow).IfapaperisnotpublishedornoJIFisknownforajournal,wetake
thecorrespondinginputtobe0.TheJIFsaretakenatthecutoffdate.
8.
Alistwhichcontainsthenumberofcoauthorsofeachpaper.
9.
Threelistswhichcontainthecoauthors'minimum,maximum,andaveragepager-
ank.Here,thepagerankiscalculatedfromthecoauthorgraphatthecutoffdate.
10.
ForeacharXivcategoryalistwhichcontainszerosexceptatpositionwhichcorre-
spondtopaperswhichareintherespectivecategory.Inordertoreducetheamount
ofdata,categoriesoftheforma.barealltreatedascategorya,e.g.astro-ph.CO
andastro-ph.HEaretreatedasthesamecategory.
11.
Foreachpaper,a50-dimensionalvectorrepresentingapaper'slatenttopicdis-
tribution,obtainedfromthekeywordanalysisdoneby[9]whenoperatingonthe
arXivdatauptothecutoff.
Theinputtotheneuralnetworkisa`broadness'valuecalculatedfromakeyword
analysis[9]withthesamedataasthepapertopicsdescribedabove.Thisbroadness
howwidelyspreadthetopicswhichanauthorpublishesonareoverallarXiv
categories.Sincethisbroadnessvalueisonevalueperauthorandnotonevalueperpaper
itishandledseparatelyfromtheotherinputstotheneuralnetwork.
Allinputdataexceptthecategoriesandthepapervectorsisnormalizedtounitvari-
anceandzeromean.More,foreachinput(e.g.numberofcitations,publica-
tiondate,broadness,...)atransformationisdeterminedfromthetrainingdatasuchthat
thistransformationbringsthegiveninputtozeromeanandunitvarianceforthetraining
dataacrossallauthors.Thistransformationisthenappliedbothtothetrainingandthe
validationinputdata.
ToassigntheJIFs,weassociatepapersinourdatabasewithajournalbyheuristically
matchingthejournalreferencegiveninthearXivmetadatatothejournalabbreviation
fromRef.[8].Concretely,wereduceboththejournalreferenceinthearXivmetadata
andthejournalabbreviationfromRef.[8]tolower-casealphanumericcharactersandcut
atthenumericcharacter.Next,weremovethesufes'vol'and'volume'ifpresent.
13
PredictingCitationCountswithaNeuralNetwork
Ifthetwovaluesobtainedthiswayareidentical,weconsiderthegivenarXivpapertobe
publishedinthecorrespondingjournal.
Toreducethenumberofpaperswherethisproceduredoesnotwork,wehavefurther
usedamanuallyassembledtranslationtable.Thistablecontainsbetween
reducedarXivjournalreferencestojournalsfromRef.[8]forwhichthemethodoutlined
inthepreviousparagraphdoesnotwork.Thetableallowsustomatchthe69most
frequentreducedjournalreferencesfromthearXivthatcouldnotbemappedbythe
previousmethod.Bythisprocedure,wehaveassignedaJournalImpactFactorto378,134
ofthe477,176paperswithanon-emptyarXivjournalreference.
AppendixB
from
keras.models
import
Model
from
keras.layers
import
Input,Dense,Conv1D,concatenate,
n
GlobalAveragePooling1D
perpaper
inputs=Input(shape=perpaper
shape,
name='perpaper
inputs')
perauthor
inputs=Input(shape=perauthor
shape,
name='perauthor
inputs')
tmp=Conv1D(
filters=70,
kernel
size=1,
strides=1,
activation=activation,
input
shape=perpaper
shape)(perpaper
inputs)
tmp=GlobalAveragePooling1D()(tmp)
tmp=concatenate([tmp,perauthor
inputs])
tmp=Dense(units=70,activation=activation)(tmp)
outputs=Dense(units=10,activation='relu')(tmp)
model=Model(inputs=[perpaper
inputs,perauthor
inputs],
outputs=outputs)
14
PredictingCitationCountswithaNeuralNetwork
AppendixC
Thecorrelationcoefobtainedfromthenaive
h
-indexpredictorisroughlythesame
asthecorrelationcoefobtainedbyplotting
N
c
(
t
1
;
t
2
)
1
=
2
overthe
h
-indexatthe
timeofthecutoffforboththetrainingandvalidationdataandcalculatingthecorrelation
coeffromthat,seeFig.8.Notethatthissecondwayofcalculatingacorrelation
coefcorrespondstowhatwasdoneinRef.[3].
ThecorrelationcoeffromFig.8areconsistenlysmallerthanthoseofthesam-
plePRB80fromRef.[3]buthigherthanthoseofthesampleAPS95fromRef.[3].See
Ref.[3]foradiscussionofthedifferencesbetweenthesamplesPRB80andAPS95
regardingtheirdifferingcorrelationcoefOursamplediffersfrombothPRB80
andAPS95inboththedatasourceandthecutsapplied.Therefore,itisnotsurpris-
ingthattherearedifferencesintheresultingcorrelationcoefandtheresultsare
notdirectlycomparable.Oneimportantdifferenceisthatweemploythesamecutoff,
1/1/2008,forallauthorswhileRef.[3]appliesadifferentcutoffforeachauthorat12
yearsaftereachauthor'spaper.
AppendixD
Weusedwhattheauthorsof[4]refertoasthemodel,ﬂthatŒastheyhave
shownŒperformsalmostaswellastheirfullmodelontheirdata-set.Wechangedthe
selectedjournalsfromNature,Science,NatureNeuroscience,PNASandNeurontoSci-
ence,Nature,PNAS,andPRL.Aslaidoutinthesupplementarymaterialof[4],weused
theR-package`glmnet'with
a
=
0
:
2.NotethatwedidnotemployourownMonteCarlo
cross-validationhere.InsteadŒaswasdonein[4]Œwereliedonthecross-validation
includedinthe`glmnet`package.
References
[1]
A.Csiszar.Thecataloguethatmademetrics,andchangedscience.
Nature
,
551(7679):163Œ165,2017.
[2]
L.Waltman.Areviewoftheliteratureoncitationimpactindicators.
arXiv:1507.02099[cs.DL],2015.
[3]
J.E.Hirsch.Doesthehindexhavepredictivepower?
ProceedingsoftheNational
AcademyofSciences
,104(49):19193Œ19198,2007.
[4]
D.E.Acuna,S.Allesina,andKordingK.P.Predictingsuccess.
Nature
,
489(7415):201202,2012.
15
PredictingCitationCountswithaNeuralNetwork
[5]
L.WeihsandO.Etzioni.Learningtopredictcitation-basedimpactmeasures.In
2017ACM/IEEEJointConferenceonDigitalLibraries(JCDL)
,pages1Œ10,June
2017.
[6]
arxivoaiapi.
https://arxiv.org/help/oa/index
.
[7]
DamienP.GeorgeandRobertKnegjens.Paperscape.
http://paperscape.
org
.
[8]
ClarivateAnalytics.2001-2009journalcitationreports,2017.
[9]
T.PriceandS.Hossenfelder.Measuringbroadness.arXiv:1805.04647
[physics.soc-ph],2018.
[10]
Franc¸oisCholletetal.Keras.
https://github.com/keras-team/keras
,
2015.
[11]
MartinAbadi,AshishAgarwal,PaulBarham,EugeneBrevdo,ZhifengChen,
CraigCitro,GregS.Corrado,AndyDavis,JeffreyDean,MatthieuDevin,San-
jayGhemawat,IanGoodfellow,AndrewHarp,GeoffreyIrving,MichaelIsard,
YangqingJia,RafalJozefowicz,LukaszKaiser,ManjunathKudlur,JoshLeven-
berg,DanMan
´
e,RajatMonga,SherryMoore,DerekMurray,ChrisOlah,Mike
Schuster,JonathonShlens,BenoitSteiner,IlyaSutskever,KunalTalwar,Paul
Tucker,VincentVanhoucke,VijayVasudevan,FernandaVi
´
egas,OriolVinyals,
PeteWarden,MartinWattenberg,MartinWicke,YuanYu,andXiaoqiangZheng.
Tw:Large-scalemachinelearningonheterogeneoussystems.
https:
//www.tensorflow.org/
,2015.
[12]
SamuelM.H.Pagerank+sparsematrices+python(ipython
notebook).
http://blog.samuelmh.com/2015/02/
pagerank-sparse-matrices-python-ipython.html
.
16
PredictingCitationCountswithaNeuralNetwork
Figure8:Correlationofvariousquantitiescalculatedfromthecompletedatasetinclud-
ingbothtrainingandvalidationdatawiththerespectivecorrelationcoef
r
.The
notationisthatofRef.[3].E.g.,
h
(
t
1
)
isthe
h
-indexascalculatedfromanauthor's
10yearsofpublishing,
h
(
t
2
)
isthecumulative
h
-indexafteranauthor's20yearsof
publishing,and
h
(
t
1
;
t
2
)
isthe
h
-indexcalculatedfromthesecond10yearsofanauthor's
publishingexcludingpaperswrittenandcitationsreceivedoutsidethisperiodoftime.
Fordifferentdatasets,theseplotscanbefoundinRef.[3].
17
"
18,"An Ensemble of Transfer, Semi-supervised and Supervised Learning Methods for Pathological Heart Sound Classification",http://arxiv.org/pdf/1806.06506v2.pdf,https://github.com/osnandhu/DS5500_PCG_Classification,"AnEnsembleofTransfer,Semi-supervisedandSupervisedLearningMethods
forPathologicalHeartSound
AhmedImtiazHumayun
1
,Md.TauhiduzzamanKhan
1
,ShabnamGhaffarzadegan
2
,ZheFeng
2
andTHasan
1
1
mHealthLab,Dept.ofBiomedicalEngineering,BangladeshUniversityofEngineeringandTechnology(BUET),Bangladesh.
2
HumanMachineInteractionGroup-2,RobertBoschResearchandTechnologyCenter(RTC),Sunnyvale,CA.
taufiq@bme.buet.ac.bd,shabnam.ghaffarzadegan@us.bosch.com
Abstract
Inthiswork,weproposeanensembleoftodistin-
guishbetweenvariousdegreesofabnormalitiesoftheheart
usingPhonocardiogram(PCG)signalsacquiredusingdigital
stethoscopesinaclinicalsetting,fortheINTERSPEECH2018
ComputationalParalinguistics(ComParE)HeartBeatsSub-
Challenge.Ourprimaryframeworkconstitutesa
convolutionalneuralnetworkwith1D-CNNtime-convolution
(tConv)layers,whichusesfeaturestransferredfromamodel
trainedonthe2016PhysionetHeartSoundDatabase.Wealso
employaRepresentationLearning(RL)approachtogenerate
featuresinanunsupervisedmannerusingDeepRecurrentAu-
toencodersanduseSupportVectorMachine(SVM)andLin-
earDiscriminantAnalysis(LDA)Finally,weutilize
anSVMonahigh-dimensionalsegment-levelfeature
extractedusingvariousfunctionalsonshort-termacousticfea-
tures,i.e.,Low-LevelDescriptors(LLD).Anensembleofthe
threedifferentapproachesprovidesarelativeimprovementof
11.13%comparedtoourbestsinglesub-systemintermsofthe
UnweightedAverageRecall(UAR)performancemetriconthe
evaluationdataset.
IndexTerms
:Representationlearning,HeartSound
tion,Time-convolutionalLayers.
1.Introduction
Cardiacauscultationisthemostpracticednon-invasiveand
cost-effectiveprocedurefortheearlydiagnosisofvariousheart
diseases.Effectivecardiacauscultationrequirestrainedphysi-
cians,aresourcewhichislimitedespeciallyinlow-income
countriesoftheworld[1].Thislackofskilleddoctorsopensup
opportunitiesforthedevelopmentofmachinelearningbased
assistivetechnologiesforpoint-of-carediagnosisofheartdis-
eases.Withtheadventofsmartphonesandtheirincreasedcom-
putationalcapabilities,machinelearningbasedautomatedheart
soundsystemsimplementedwithasmart-phone
attachabledigitalstethoscopeinthepoint-of-carelocationscan
beofimpactforearlydiagnosisofcardiacdiseases.
AutomatedofthePCG,i.e.,theheartsound,
havebeenextensivelystudiedandresearchedinthepastfew
decades.Previousresearchonautomaticofheart
soundscanbebroadlyintotwoareas:(i)PCGseg-
mentation,i.e.,detectionoftheandsecondheartsounds
(S1andS2),and(ii)detectionofrecordingsaspathologic
orphysiologic.Forthelatterapplication,researchersinthe
pasthaveutilizedNeuralNetworks(ANN)[2],Sup-
portVectorMachines(SVM)[3]andHiddenMarkovModels
(HMM)[4].In,the2016Physionet/CinCChallengewasorga-
nizedandanarchiveof
4430
PCGrecordingswerereleasedfor
binaryofnormalandabnormalheartsounds.This
particularchallengeencouragednewmethodsbeingutilizedfor
thistask.Notablefeaturesusedforthisdatasetincluded,time,
frequencyandstatisticalfeatures[5],Mel-frequencyCepstral
Coef(MFCC)[6],andContinuousWaveletTransform
(CWT).Mostofthesystemsadoptedthesegmentationalgo-
rithmdevelopedbySpringeretal.[7].Amongthetopscoring
systems,Maknickasetal.[8]extractedMel-frequencySpec-
tralCoef(MFSC)fromunsegmentedsignalsanduseda
2DCNN.Plesingeretal.[9]proposedanovelsegmentation
method,ahistogrambasedfeatureselectionmethodandpa-
rameterizedsigmoidfunctionsperfeature,todiscriminatebe-
tweenclasses.Variousmachinelearningalgorithmsincluding
SVM[10],k-NearestNeighbor(k-NN)[6],MultilayerPercep-
tron(MLP)[11,12],RandomForest[5],1D[13]and2DCNNs
[8],andRecurrentNeuralNetwork(RNN)[14]wereemployed
inthechallenge.Agoodnumberofsubmissionsusedanen-
sembleofwithavotingalgorithm[5,11,12,13].The
bestperformingsystemwaspresentedbyPotesetal.[13]that
combineda1D-CNNmodelwithanAdaboost-Abstainclassi-
usingathresholdbasedvotingalgorithm.
Inaudiosignalprocessing,ter-banksarecommonlyem-
ployedasastandardpre-processingstepduringfeatureextrac-
tion.Thiswasdonein[13]beforethe1D-CNNmodel.We
proposeaCNNbasedFiniteImpulseResponse(FIR)-
bankfront-end,thatautomaticallylearnsfrequencycharacter-
isticsoftheFIRutilizingtime-convolution(tConv)
layers.TheINTERSPEECHComParEHeartSoundShenzhen
(HSS)Datasetisarelativelysmallercorpus,withthreeclass
labelsaccordingtothedegreeofthedisease;whilethePhys-
ionetHeartSoundsDatasethasbinaryannotations.Wetrain
ourmodelonthePhysionetChallengeDatasetandtransferthe
learnedweightsforthethreeclasstask.Wealso
availunsupervised/semi-supervisedlearningtolatentrep-
resentationsofPCG.
2.DataPreparation
2.1.Datasets
2.1.1.TheINTERSPEECH2018ComParEHSSDataset
TheINTERSPEECH2018ComParEChallenge[15]released
theHeartSoundsShenzhenPCGsignalcorpuscontaining
845
recordingsfrom
170
differentsubjects.Therecordingswere
collectedfrompatientswithcoronaryheartdisease,arrhythmia,
valvularheartdisease,congenitalheartdisease,etc.ThePCG
recordingsaresampledat
4
KHzandannotatedwiththreeclass
labels:(i)
Normal
,(ii)
Mild
,and(iii)
Moderate/Severe
(heart
disease).
2.1.2.PhysioNet/CinCChallengeDataset
The2016PhysioNet/CinCChallengedataset[16]containsPCG
recordingsfromsevendifferentresearchgroups.Thetrain-
arXiv:1806.06506v2  [cs.CV]  7 Oct 2018Figure1:
DatasetpreparationfortransferLearning,super-
visedLearningandrepresentationlearningusingPhysionet
andComParEcorpus.
ingdatacontains
3153
heartsoundrecordingscollectedfrom
764
patientswithatotalnumberof
84
;
425
annotatedcar-
diaccyclesrangingfrom
35
to
159
bpm.CardiacAnomalies
rangefromcoronaryheartdisease,arrhythmia,valvularsteno-
sis/regurgitation,etc.Thedatasethas
2488
and
665
PCGsig-
nalsannotatedas
Normal
and
Abnormal
,respectively.The
AristotleUniversityofThessalonikiheartsoundsdatabase(AU-
THHSDB)[17],asubsetofthePhysionetcorpus(training-c),
containsadditionalmetadatabasedontheseverityoftheheart
diseases.Therecordingsaresampledat2000Hz.
2.2.DataImbalanceProblem
TheINTERSPEECHComParEHSSDatasetsuffersfromsig-
classimbalanceinitstrainingset,whichcouldintro-
duceperformancereductionforbothclassicalmachinelearn-
inganddeeplearningbasedThetrainingsetis
dividedinaratioof16.7/55.0/28.3percentbetweenthe
Nor-
mal
/
Mild
/
Severe
classes,withmorethanhalfofthetrainingdata
comprisingofPCGsignalsannotatedas
Mild
ﬂ.Theresultofthe
imbalancewasevidentinourrecallmetricswhicharediscussed
laterinSec.7.
2.3.FusedTrainingSets
Tocopewiththeclassimbalanceandincreasethevolumeof
thetrainingdata,wecreated
3
newfusedtrainingcorporaout
oftheINTERSPEECHComParEHSSDatasetandthePhy-
sionet/CinCChallengeDatasettrainingpartitions.TheAU-
THHSDB(training-c)partitionofthedatasetwasrelabeledus-
ingthemetadataprovidedtohave
7
Normal
,
8
Mild
and
16
Severe
annotatedrecordings.Thedatasetdistributionsare
depictedinFig.1.ThefuseddatasetspreparedforTransfer
Learning(TL),SupervisedLearning(SL)andRepresentation
Learning(RL)willbereferredtoasTL-Data,SL-DataandRL-
Datarespectively.
3.ProposedTransferLearningFramework
3.1.1D-CNNModelforAbnormalHeartSoundDetection
ThePhysionet/CinCChallengePCGdatabaseisalargercor-
puswithNormalandAbnormallabelsdesignedforabinary
task.Weproposea1D-CNNNeuralNetworkim-
provingthetopscoringmodel[13]ofthePhysionet/CinC2016
challenge.First,thesignalisre-sampledto1000Hz(afteran
anti-aliasinganddecomposedintofourfrequencybands
(
25

45
,
45

80
,
80

200
,
200

500
Hz).Next,spikesin
therecordingsareremoved[18]andPCGsegmentationisper-
formedtoextractcardiaccycles[7].Takingintoaccountthe
longestcardiaccycleinthecorpus,eachcardiaccycleiszero
paddedtobe
2
:
5
sinlength.Fourdifferentfrequencybands
ofextractedfromeachcardiaccyclearefedintofourdifferent
inputbranchesofthe1D-CNNarchitecture.Eachbranchhas
twoconvolutionallayersofkernelsize
5
,followedbyaRec-
LinearUnit(ReLU)activationandamax-poolingof
2
.
Theconvolutionallayerhas
8
whilethesecondhas
4
.TheoutputsofthefourbranchesarefedtoanMLPnet-
workafterbeingandconcatenated.TheMLPnetwork
hasahiddenlayerof
20
neuronswithReLUactivationandtwo
outputneuronswithsoftmaxactivation.Theresultingmodel
providespredictionsoneveryheartsoundsegment(cardiaccy-
cle),whichareaveragedovertheentirerecordingandrounded
forinference.
3.2.Filter-bankLearningusingTime-Convolutional
(tConv)Layers
Foracausaldiscrete-timeFIRoforder
N
withcoef-

b
0
;b
1
;:::b
N
,theoutputsignalsamples
y
[
n
]
isobtained
byaweightedsumofthemostrecentsamplesoftheinputsignal
x
[
n
]
.Thiscanbeexpressedas:
y
[
n
]=
b
0
x
[
n
]+
b
1
x
[
n

1]+
:::::
+
b
N
x
[
n

N
]
=
N
X
i
=0
b
i
x
[
n

i
]
:
(1)
A1D-CNNperformscross-correlationbetweenitsinputand
itskernelusingaspatiallycontiguousreceptiveofkernel
neurons.Theoutputofaconvolutionallayer,withakernelof
oddlength
N
+1
,canbeexpressedas:
y
[
n
]=
b
0
x
[
n
+
N
2
]+
b
1
x
[
n
+
N
2

1]+
::::
+
b
N
2
x
[
n
]+
::::
+
b
N

1
x
[
n

N
2
+1]+
b
N
x
[
n

N
2
]
=
N
X
i
=0
b
i
x
[
n
+
N
2

i
]
(2)
where
b
0
;b
1
;:::b
N
arethekernelweights.Consideringacausal
systemtheoutputoftheconvolutionallayerbecomes:
y
[
n

N
2
]=
˙
 

+
N
X
i
=0
b
i
x
[
n

i
]
!
(3)
where
˙
(

)
istheactivationfunctionand

isthebiasterm.
Therefore,a1Dconvolutionallayerwithlinearactivationand
zerobias,actsasanFIRwithanaddeddelayof
N=
2
[19].Wedenotesuchlayersastime-convolutional(tConv)lay-
ers[20].Naturally,thekernelsoftheselayers(similarto-
bankcoefcanbeupdatedwithStochasticGradientDe-
scent(SGD).Theselayersthereforereplacethestaticthat
decomposethepre-processedsignalintofourbands(Sec.3.1).
WeuseaspecialvariantofthetConvlayerthatlearnscoef
cientswithalinearphase(LP)response.
3.3.TransferLearningfromPhysionetModel
OurproposedtConvNeuralNetworkistrainedonthePhysionet
CinCChallengeDatasetwithfour-foldinhousecrossvalidation
[21].Themodelachievesameancross-validationaccuracyof
87
:
10%
andRecallof
90
:
91%
.Theweightsup-tothe
layeraretransferred[22]toanewconvolutionalneuralnetwork
architecturewithafullyconnectedlayerwithtwohiddenlay-
ersof239and20neuronsand3outputneuronsfor
Normal
,
Figure2:
ProposedarchitectureincorporatingtConvlayersforTransferLearning.
Mild
and
Severe
classes(Fig.2).Themodelweightsare
tunedonTL-Data.TL-Datacomprisesofallofthesamples
fromtheINTERSPEECHComParEDatasetandthe
Normal
signalsfromthePhysionetinhousevalidationfold,fromwhich
thetrainedweightsaretransferred.Wechosetheweightsofa
modeltrainedonFold1forbetterpercardiaccyclevalidation
accuracy.Thecross-entropylossisoptimizedwithastochastic
gradientdescentoptimizerwithalearningrateof
4
:
5

10

05
.
Dropoutof
0
:
5
isappliedtoallofthelayersexceptfortheout-
putlayer.Themodelhyperparameterswere
not
optimizedwhile
withTL-Data.Thecostfunctionwasweightedto
accountfortheclassimbalance.
4.RepresentationLearning(RL)with
RecurrentAutoencoders
Figure3:
ReconstructedMel-spectrogramofrecordingthresh-
oldedtoreducebackgroundnoisea)below-30dBb)below-45
dB
Representationlearningisparticularlyofinterestwhena
largeamountofunlabeleddataisavailablecomparedtoa
smallerlabeleddataset.Consideringthetwocorporaathand,
weapproachtheproblemfromasemi-supervisedrepresenta-
tionlearningperspectivetotrainrecurrentsequencetosequence
autoencoders[23]onunlabeledRL-Data(Sec.2.3)andthen
uselowerdimensionalrepresentationsofSL-Datatotrainclas-
Sequence-to-sequencelearningisabouttranslatingse-
quencesfromonedomaintoanother.UnsupervisedSequence-
to-sequencerepresentationlearningwaspopularizedintheuse
ofmachinetranslation[24].Ithasalsobeenemployedforaudio
withsuccess[25].Itoffersthechanceofresolv-
ingtheovproblemexperiencedwhentraininganendto
enddeeplearningmodel.
First,mel-spectrogramof
126
bandsareextractedwitha
windowsizeof
320
mswith
50
%overlap.Therawaudio
areclippedto30secondsinlength.Toreducebackground
noise,thespectrogramisthresholdedbelow

30
,

45
,

60
and

75
dB.Thisresultsinfourdifferentspectrograms.Themodel
istrainedonallfouroftheseseparately,whichresultsinfour
differentfeaturesets.BoththeencoderanddecoderRecurrent
NeuralNetworkhad2hiddenlayerswith256GatedRecurrent
Unitseach.ThehiddenstatesofalltheGRUsareconcate-
natedintoa1024dimensionalfeaturevector.Fig.3portrays
thereconstructedoutputsformel-spectrogramsclippedbelow

30
dBand

45
dB.Fourdifferentfeaturevectorsforthefour
differentspectrogramsarealsoconcatenatedtoformfusedfea-
tures.FeaturerepresentationsofSL-Datawereusedtotrain
Themodelisdeployedandtrainedusingthe
AU
D
EEP
toolkit[26].
5.SupervisedLearningwithSegment-level
Features
5.1.ComParEAcousticFeatureSet
Inthissub-system,weutilizetheacousticfeaturesetdescribed
in[27].Thisfeaturesetcontains
6373
staticfeaturesresulting
fromthecomputationofvariousfunctionalsoverLLDparam-
eters[15].TheLLDparametersandfunctionalsutilizedare
describedin[27].ThefeaturesareextractedusingtheopenS-
MILEtoolkit[28].
5.2.
Wehaveimplementedseveralmachinelearningalgorithmsfor
heartsoundfromtheComParEAcousticfeature
set.Theevaluatedinclude:SupportVectorMachine
(SVM),LinearDiscriminantAnalysis(LDA),andMulti-Layer
Perceptron(MLP).SVMwithcomplexityC
=10

4
andtoleranceL
=0
:
3
outperformedtheother
6.ExperimentalEvaluationandResults
TheevaluationmetricfortheINTERSPEECHComParEChal-
lengeisUnweightedAverageRecall(UAR)sincethedatasets
areclassunbalanced.Wealsomonitorclasswiserecalland
accuracyforevaluationofmodelperformance.Performance
metricsonboththedevelopmentandtestsetarelistedonTa-
ble1withthetrainingdatasetsmentioned.TheComp-SVM
model,evaluatedontheComParEtestset,acquired45.9%UAR
and51.5%overallaccuracy.Ourtransferlearningbasedmodel
Table1:
Performanceevaluationofproposedmethodscom-
paredtotheofbaselinesystems.
BaselineSystems
ModelName
Dataset
Features

UAR(%)dev
Acc.(%)dev
UAR(%)test
OPENSMILE[15]
INTERSPEECH
ComParEHSS
ComParE
Featureset
SVM
50.3
52.2
46.4
AUDEEP[15]
INTERSPEECH
ComParEHSS
Fused
Autoencoder
Features
SVM
38.6
-
47.9
END2YOU[15]
INTERSPEECH
ComParEHSS
CNN
LSTM
41.2
-
37.7
Fusionofbest2systems[15]
-
-
56.2
ProposedSystems
ModelName
Dataset
Features

UAR(%)dev
Acc.(%)dev
UAR(%)test
ComP-SVM
INTERSPEECH
ComParEHSS
ComParE
Featureset
SVM
52.1
53.9
45.9
RL-SVM
RL-Data
SL-Data

60
dB
Autoencoder
Features
SVM
42.9
48.9
-
RL-LDA
RL-Data
SL-Data

60
&

75
dB
Autoencoder
Features
LDA
51.4
54.4
34.4
LP-tConv
TL-Data
tConv
CNN
MLP
44.6
56.1
39.5
SystemEnsembles
EnsembleSystemName
UAR(%)dev
Acc.(%)dev
UAR(%)test
FusionofComp-SVM,RL-SVMandLP-tConvmodels
57.92
63.9
39.3
HierarchicalwithFusion
57.93
64.2
42.1
Figure4:
Meanvaluesofthe
4096
featureslearnedfromthe4
mel-spectrogramsbytheRNN-Autoencoders.
withavariantofourproposedtConvlayeracquiredimproved
performancecomparedtotheendtoenddeeplearningbase-
line(END2YOU).Trainingonalargercorpushasprovidedan
improvedperformanceonthedevelopmentsetusingrepresen-
tationlearningwithreducedperformanceonthe
testset.Comp-SVM,RL-SVMandLP-tConvmodelsareen-
sembledusingamajorityvotingalgorithm.ItyieldsUARof
57.92%onthedevelopmentset,andUARof39.2%onthe
testset.Toimprovethe
Normal
hitrateahierarchicaldeci-
sionsystemisimplementedwhereanLP-tConvnetworktrained
onPhysionet/CincDatabaseisusedforbinary
tionbetween
Normal
and
Abnormal
recordings.Followingthat,
anensembleofComp-SVM,RL-SVMandLP-tConvisused
toclassifybetween
Mild
and
Severe
classes.Thehierarchical
modelhasacquiredadevsetUARof57.93%andtestsetUAR
of42.1%.
7.Discussion
OurproposedendtoendLP-tConvmodelsupersededthe
testsetmetricforthestandalonebaselineendtoendmodel
(END2YOU).Otherproposedsystemsfailedtobeatthebase-
linesystemstestsetUARwhileitoutperformedthedevelop-
mentsetUAR.Thiscouldindicateovonthedevelop-
mentset.Ontheotherhand,forthebaselinesystemsatendency
Figure5:
Recallscoresobtainedonthevalidationdataafter
eachtrainingepoch.Asteadyincreaseinthemildrecallis
visiblewhilerecallfortheotherclassesaresteadilydecreasing.
ofovonthetestsetwasvisible.Thisisbecausethein-
dividualapproach/hyperparametersperformingbestonthetest
sethasbeenchosenasbaselines[15].Ageneralizedfeature-
systemshouldyieldsimilarUARonbothdevelopment
andtestdatasetifthedevelopmentandtestdatadistributions
areconsistent.ThiswasnoticeableonlyfortheopenSMILE
featureswithanSVM.
Moreinterestinginsightswererevealedduringthetraining
oftherecurrentautoencoders.Thelowerdimensionalrepresen-
tationslearnedweredifferentforthePhysionetCinCChallenge
databaseandtheINTERSPEECHComParEHSSdatabase.The
RLmodelwastrainedonbothRL-dataandtheINTERSPEECH
HSSdatabase.Fig.4showsthemeanoftheconcatenated
(fused)representationslearnedfromthe4mel-spectrograms.
Adistinctdifferencecanbevisualizedfromfeaturedimension
1700.Thelast2048dimensionsarerepresentationslearned
fromthe-60dBandthe-75dBmel-spectrograms,thesearethe
dimensionswherethefeaturemeansdeviatethemost.Quite
interestingly,the-60dBand-75dBspectrogramfeaturesyield
betterresultscomparedtotheothers.Aftertrainingthemodel
withpreprocessedsignals(resampledto1000Hzandband-pass
between20-400Hz),therepresentationdifferencesin
themeanreducedforcertaindimensions.Thiscouldmean
thatthecorrespondingdimensionsrepresentinformationfrom
thehigherendofthefrequencyspectrum.Anotherobservation
experiencedduringexperimentationwastheNormalRecallvs
Mild/SevereRecalltrade-off.WhiletraininganendtoendLP-
tConvmodel,wehaveseenadivergentbehaviorbetweenthe
normalandmild/severerecallmetrics(Fig.5)whichpersisted
evenwhenthepercentageof
Normal
recordingsweremorethan
Mild
recordings.
8.Conclusions
Inthiswork,wehavepresentedanensembleof
forautomaticallydetectingabnormalheartsoundsofdifferent
severitylevelsfortheINTERSPEECH2018ComParEHeart
BeatsSub-Challenge.Theprimaryframeworkwasbasedon
transferlearningofparametersfroma1D-CNNmodelpre-
trainedonthePhysionetHSdataset.Wehavealso
deployedunsupervisedfeaturerepresentationlearningfrom
mel-spectrogramsusingadeepautoencoderbasedarchitec-
ture.Finally,wehavealsoimplementedasegment-levelfeature
basedsystemusingtheComParEfeaturesetandanSVMclas-
.Thehierarchicalensembleofthesystemsprovided
withaUARof57.9%onthedevelopmentdatasetand42.1%on
thetestdataset.
9.Acknowledgement
TheTitanXPascalusedforthisresearchwasdonatedbythe
NVIDIACorporation.
10.References
[1]U.Alam,O.Asghar,S.Q.Khan,S.Hayat,andR.A.Malik,ﬁCar-
diacauscultation:anessentialclinicalskillindecline,ﬂ
Br.J.Car-
diology
,vol.17,no.1,p.8,2010.
[2]H.U

guz,ﬁAbiomedicalsystembasedonneuralnetwork
andprincipalcomponentanalysisfordiagnosisoftheheartvalve
diseases,ﬂ
J.Med.Syst.
,vol.36,no.1,pp.61Œ72,2012.
[3]A.Gharehbaghi,P.Ask,M.Lind
´
en,andA.Babic,ﬁAnovelmodel
forscreeningaorticstenosisusingphonocardiogram,ﬂin
Proc.
NBCBME
.Springer,2015,pp.48Œ51.
[4]R.Sarac¸O

gLu,ﬁHiddenmarkovmodel-basedof
heartvalvediseasewithpcafordimensionreduction,ﬂ
J.Eng.
Appl.Artif.Intell.
,vol.25,no.7,pp.1523Œ1528,2012.
[5]M.N.HomsiandP.Warrick,ﬁEnsemblemethodswithoutliersfor
phonocardiogramﬂ
Physiol.Meas.
,vol.38,no.8,p.
1631,2017.
[6]I.J.D.Bobillo,ﬁAtensorapproachtoheartsoundﬂ
in
Proc.IEEECinC
,2016,pp.629Œ632.
[7]D.B.Springer,L.Tarassenko,andG.D.Clifford,ﬁLogistic
regression-HSMM-basedheartsoundsegmentation,ﬂ
IEEETrans.
onBiomed.Eng.
,vol.63,no.4,pp.822Œ832,2016.
[8]V.MaknickasandA.Maknickas,ﬁRecognitionofnormalŒ
abnormalphonocardiographicsignalsusingdeepconvolutional
neuralnetworksandmel-frequencyspectralcoefﬂ
Phys-
iol.Meas.
,vol.38,no.8,p.1671,2017.
[9]F.Plesinger,I.Viscor,J.Halamek,J.Jurco,andP.Jurak,ﬁHeart
soundsanalysisusingprobabilityassessment,ﬂ
Physiol.Meas.
,
vol.38,no.8,p.1685,2017.
[10]B.M.Whitaker,P.B.Suresha,C.Liu,G.D.Clifford,andD.V.
Anderson,ﬁCombiningsparsecodingandtime-domainfeatures
forheartsoundﬂ
Physiol.Meas.
,vol.38,no.8,p.
1701,2017.
[11]E.KayandA.Agarwal,ﬁDropconnectedneuralnetworkstrained
ontime-frequencyandinter-beatfeaturesforclassifyingheart
sounds,ﬂ
Physiol.Meas.
,vol.38,no.8,p.1645,2017.
[12]M.Zabihi,A.B.Rad,S.Kiranyaz,M.Gabbouj,andA.K.Kat-
saggelos,ﬁHeartsoundanomalyandqualitydetectionusingen-
sembleofneuralnetworkswithoutsegmentation,ﬂin
Proc.IEEE
CinC
,2016,pp.613Œ616.
[13]C.Potes,S.Parvaneh,A.Rahman,andB.Conroy,ﬁEnsembleof
feature-basedanddeeplearning-basedfordetectionof
abnormalheartsounds,ﬂin
Proc.IEEECinC
,2016,pp.621Œ624.
[14]T.-c.I.YangandH.Hsieh,ofacousticphysio-
logicalsignalsbasedondeeplearningneuralnetworkswithaug-
mentedfeatures,ﬂin
Proc.IEEECinC
,2016,pp.569Œ572.
[15]B.Schuller,S.Steidl,A.Batliner,E.Bergelson,J.Krajewski,
C.Janott,A.Amatuni,M.Casillas,A.Seidl,M.Soderstrom
etal.
,
ﬁTheinterspeech2018computationalparalinguisticschallenge:
Atypical&self-assessedaffect,crying&heartbeats,ﬂin
Proc.
ISCAInterspeech
,2018.
[16]C.Liu,D.Springer,Q.Li,B.Moody,R.A.Juan,F.J.Chorro,
F.Castells,J.M.Roig,I.Silva,A.E.Johnson
etal.
,ﬁAnopenac-
cessdatabasefortheevaluationofheartsoundalgorithms,ﬂ
Phys-
iol.Meas.
,vol.37,no.12,p.2181,2016.
[17]C.D.PapadaniilandL.J.Hadjileontiadis,ﬁEfheartsound
segmentationandextractionusingensembleempiricalmodede-
compositionandkurtosisfeatures,ﬂ
IEEEJ.Biomed.Health.In-
form.
,vol.18,no.4,pp.1138Œ1152,2014.
[18]S.Schmidt,C.Holst-Hansen,C.Graff,E.Toft,andJ.J.Stru-
ijk,ﬁSegmentationofheartsoundrecordingsbyaduration-
dependenthiddenmarkovmodel,ﬂ
Physiologicalmeasurement
,
vol.31,no.4,p.513,2010.
[19]R.MateiandG.Liviu,ﬁAclassofcircularly-symmetricCNNspa-
tiallinearﬂvol.19,pp.299Œ316,012006.
[20]T.N.Sainath,R.J.Weiss,A.Senior,K.W.Wilson,and
O.Vinyals,ﬁLearningthespeechfront-endwithrawwaveform
CLDNNs,ﬂin
Proc.ISCAInterspeech
,2015.
[21]A.I.Humayun,S.Ghaffarzadegan,Z.Feng,andT.Hasan,
ﬁLearningfront-end-bankparametersusingconvolutional
neuralnetworksforabnormalheartsounddetection,ﬂin
Proc.
IEEEEMBC
.IEEE,2018.
[22]J.Yosinski,J.Clune,Y.Bengio,andH.Lipson,ﬁHowtransfer-
ablearefeaturesindeepneuralnetworks?ﬂin
Adv.Neural.Inf.
Process.Syst.
,2014,pp.3320Œ3328.
[23]I.Sutskever,O.Vinyals,andQ.V.Le,ﬁSequencetosequence
learningwithneuralnetworks,ﬂin
Adv.Neural.Inf.Process.Syst.
,
2014,pp.3104Œ3112.
[24]D.Bahdanau,K.Cho,andY.Bengio,ﬁNeuralmachinetrans-
lationbyjointlylearningtoalignandtranslate,ﬂ
arXivpreprint
arXiv:1409.0473
,2014.
[25]S.Amiriparian,M.Freitag,N.Cummins,andB.Schuller,ﬁSe-
quencetosequenceautoencodersforunsupervisedrepresentation
learningfromaudio,ﬂin
Proc.oftheDCASE2017Workshop
,
2017.
[26]M.Freitag,S.Amiriparian,S.Pugachevskiy,N.Cummins,and
B.Schuller,ﬁaudeep:Unsupervisedlearningofrepresentations
fromaudiowithdeeprecurrentneuralnetworks,ﬂ
arXivpreprint
arXiv:1712.04382
,2017.
[27]F.Weninger,F.Eyben,B.W.Schuller,M.Mortillaro,andK.R.
Scherer,ﬁOntheacousticsofemotioninaudio:whatspeech,mu-
sic,andsoundhaveincommon,ﬂ
Frontiersinpsychology
,vol.4,
p.292,2013.
[28]F.Eyben,M.W
¨
ollmer,andB.Schuller,ﬁOpensmile:themunich
versatileandfastopen-sourceaudiofeatureextractor,ﬂin
Proc.
ACMMM
.ACM,2010,pp.1459Œ1462.
"
19,Conditional Affordance Learning for Driving in Urban Environments,http://arxiv.org/pdf/1806.06498v3.pdf,https://github.com/xl-sr/CAL,"ConditionalAffordanceLearning
forDrivinginUrbanEnvironments
AxelSauer
1,2
NikolaySavinov
1
AndreasGeiger
1,3
1
ComputerVisionandGeometryGroup,ETHZ
¨
urich
2
ChairofRoboticsScienceandSystemIntelligence,TechnicalUniversityofMunich
3
AutonomousVisionGroup,MPIforIntelligentSystemsandUniversityofT
¨
ubingen
Abstract:
Mostexistingapproachestoautonomousdrivingfallintooneoftwo
categories:modularpipelines,thatbuildanextensivemodeloftheenvironment,
andimitationlearningapproaches,thatmapimagesdirectlytocontroloutputs.
Arecentlyproposedthirdparadigm,directperception,aimstocombinethead-
vantagesofbothbyusinganeuralnetworktolearnappropriatelow-dimensional
intermediaterepresentations.However,existingdirectperceptionapproachesare
restrictedtosimplehighwaysituations,lackingtheabilitytonavigateintersec-
tions,stopattraflightsorrespectspeedlimits.Inthiswork,weproposea
directperceptionapproachwhichmapsvideoinputtointermediaterepresenta-
tionssuitableforautonomousnavigationincomplexurbanenvironmentsgiven
high-leveldirectionalinputs.Comparedtostate-of-the-artreinforcementandcon-
ditionalimitationlearningapproaches,weachieveanimprovementofupto68%
ingoal-directednavigationonthechallengingCARLAsimulationbenchmark.In
addition,ourapproachisthetohandletraflightsandspeedsignsbyusing
image-levellabelsonly,aswellassmoothcar-following,resultingina
reductionoftrafaccidentsinsimulation.
Keywords:
AutonomousDriving,SensorimotorControl,AffordanceLearning
1Introduction
Anautonomousvehicleisacognitivesystemandhencefollowstheconceptofarationalagent:
inordertooperatesafelyitmustaccuratelyobserveitsenvironment,makerobustdecisionsand
performactionsbasedonthesedecisions[
1
].Autonomousnavigationcanbedescribedasamapping
functionfromsensoryinputtocontroloutput.Toimplementthisfunction,threemajorapproaches
havebeenproposed:modularpipelines,imitationlearning,anddirectperception.
Mostautonomousvehiclesarebasedon
modularpipelines
(MP)[
4
,
5
].AnMPsplitstheau-
tonomousdrivingproblemintosmaller,easiersubproblems:perception,pathplanning,andcontrol.
Theapproachoftenreliesonvarioussensorsinordertoproduceaconsistentrepresentationofthe
surroundingenvironment.Adrivingdecisionisthenmadebasedonthisrepresentation.WhileMPs
arerelativelyinterpretableduetotheirmodularity,theyrelyoncomplexintermediaterepresenta-
tionswhicharemanuallychosen(e.g.,opticalw),oftenhardtoestimatewithsufaccuracy
andmightnotnecessarilybetheoptimalchoiceforsolvingthesensorimotorcontroltask.Further-
more,MPsrequirelargeamountsofannotateddatawhichcanbecostlytoobtain,e.g.,pixel-wise
semanticsegmentationfortrainingneuralnetworksormapsforlocalization.
Thesecondapproach,
imitationlearning
(IL)[
18
],mapsdirectlyfromrawinputtocontroloutput
andislearnedfromdatainanend-to-endfashion,skippingtheintermediatestepofbuildingan
explicitenvironmentmodel.IncontrasttoMPs,datafortrainingIL-basednavigationsystemscan
becollectedwithrelativeease,i.e.,bydrivingaroundandrecordingthevideofootagetogetherwith
thedriventrajectory.However,ILapproachesarefacedwiththeproblemoflearningaverycomplex
mapping(fromrawinputtocontrol)inasinglestep.Thus,amodelwithhighcapacityaswellasa
verylargeamountoftrainingdataisrequiredforhandlingthelargevarietyofrealworldsituations
whichmaybeencounteredattesttime.Besides,ILlackstransparencyasitishardtocomprehend
theinternaldecision-makingprocessinaneuralnetwork.Thisraisessecurityconcerns:adriving
systemthatdoesnotacttransparentlymightnotbetrustedorusedbyhumans.
2ndConferenceonRobotLearning(CoRL2018),Z
¨
urich,Switzerland.
arXiv:1806.06498v3  [cs.RO]  3 Nov 2018Figure1:
ConditionalAffordanceLearning(CAL)forAutonomousUrbanDriving.
Theinput
videoandthehigh-leveldirectionalinputarefedintoaneuralnetworkwhichpredictsasetof
affordances.Theseaffordancesareusedbyacontrollertocalculatethecontrolcommands.
Thethirdapproach,
directperception
(DP)[
2
],aimstocombinetheadvantagesofMPandIL.
Insteadofpredictingadetailedrepresentationoftheenvironment,thegoalinDPistopredictalow-
dimensionalintermediaterepresentationoftheenvironmentwhichisthenusedinaconventional
controlalgorithmtomaneuverthevehicle.Thus,DPneitherrequiresthenetworktolearnthe
complexsensorimotorcontrolproblemend-to-end,nordoesitassumetheavailabilityofdatasets
withpixel-levelorbox-levellabelsthataremoretime-consumingtoobtainthanimage-
levellabels.Chenetal.[
2
]demonstratedgoodresultswhenapplyingthisapproachtohighway
drivingusinganopen-sourcesimulator.However,highwaydrivingisarathereasytaskcompared
todrivinginruralorurbanareas.Whenconsideringnavigationinurbanareas,severaldif
areadded:theagentmustobeytrafrules(speedlimits,redlights,etc.),takeintoaccountpossible
obstaclesontheroad(e.g.,pedestrianscrossingthestreet),andhandlejunctionswithmorethanone
possibledirection.
Atthecoreofthedirectperceptionapproachliesthechoiceofintermediaterepresentationtobe
predicted.Ideally,thisrepresentationshouldbeoflowdimensionalitywhilecomprisingallthenec-
essaryinformationformakingadrivingdecision.Onechoiceofsuchrepresentationare
affordances
,
attributesoftheenvironmentwhichlimitthespaceofallowedactions.Asanexample,considerthe
distancetothevehicleaheadwhichlimitstheabilityoftheego-vehicletospeedup.Affordancescan
alsobeconditionedonhigh-leveldrivingdecisionsasprovided,e.g.,byaconventionalnavigation
system.Consideravehicleatanintersection:thedistancetothecenterlinechangeswiththedesired
turnwewouldlikethevehicletotake(andthusthelanewewouldlikeittofollow).
Thegoalofthisworkistogeneralizethedirectperceptionapproachtotheurbansetting.Wedevelop
novelintermediaterepresentationsintheformofaffordancessuitableforurbannavigationaswell
asconditionalmodelswherethedecisionisnotbasedontheimageinputalone,butalsoonhigh-
levelnavigationcommandsasprovidedbyaconsumergradenavigationsystem(e.g.,ﬁturnleftat
thenextintersectionﬂ).Moreover,oursystemisdesignedtodrivemoresafelyandcomfortably,
alleviatingthecommonproblemofjerkydrivingbehaviorinexistingapproachesforautonomous
driving[
3
].WecointhedevelopedapproachﬁConditionalAffordanceLearningﬂ(CAL)asthe
predictedaffordancesandthemodeloutputareconditionedonhigh-levelnavigationcommands.
Figure
1
showsanoverviewofourapproach.Themaincontributionsofourworkaresummarized
asfollows:

Wedevelopnovelintermediaterepresentationswhicharelow-dimensional,yetprovidethenec-
essaryinformationtoallowfordrivinginurbanenvironments.

Wedeployaconditionalnavigationmodelwhichallowsfortakinghigh-leveldirectionalcom-
mandspriortointersections.Wefurtherimplementacontrolalgorithmwhichallowsforasmooth
ridewhileobeyingtrafrules.

Weprovideanin-depthevaluationofdifferentnetworkarchitecturesandparameterizations.Our
networkisabletopredictallintermediaterepresentationsinasingleforwardpass.

Wedemonstratetheofvideo-basedrepresentationlearningfornavigation.Priorworks
eitheroperateframe-by-frameordonotevaluateinanonlinesetting.
Thecodeandourtrainedmodelscanbefoundat
https://github.com/xl-sr/CAL
.
2
2RelatedWork
Modularpipelinesarethemostpopularapproachtoautonomousdrivingandareemployedbymost
researchprojects[
4
,
5
].Tobuildtheenvironmentalmodel,theperceptionstackneedstodetect
allaspectsofthetrafscenethatarelikelytoberelevantforthedrivingdecision.Thesedetection
tasksareusuallytrainedandsolvedseparately[
6
],mostrecentlybyexploitingdeepneuralnetworks,
e.g.,forobjectdetection[
7
,
8
],imagesegmentation[
9
,
10
],ormotionestimation[
13
,
14
].This
informationcanbeaggregatedintoamodeloftheenvironment[
15
,
16
]andaplanningmodule
generatesanobstacle-freetrajectorywhichisexecutedbythecontrolmodule[
17
].
Theimplementationoftheimitationlearning(IL)approachintherealworld,ALVINN[
18
],
datesbacktothe80s.Morerecently,Mulleretal.[
19
]employasimilarsystemtonavigateanoff-
roadtruckinavarietyofterrains,weather,andlightingconditionswhileavoidingobstacles.Using
morecomputationalpowerandamodernCNNarchitecture,contemporaryapproachessuchasthe
onebyBojarskietal.[
20
]demonstrateimpressiveperformanceonreal-worldtaskslikehighway
followinganddrivingincourses.Codevillaetal.[
21
]proposeaconditionalimitationlearning
formulationthatisabletotraverseintersectionsbasedonhigh-levelnavigationalcommands.
Theideaofdirectperceptionistopredictseveralaffordanceindicatorsdescribingthedrivingscene.
TheconceptofaffordanceswasoriginallyproposedbyGibson[
22
]inthedofpsychologyand
hasbeenappliedtotheautonomousdrivingtaskbyChenetal.[
2
],demonstratingstrongperfor-
manceinaracingsimulation.Al-Qizwinietal.[
23
]improvetheoriginalapproachofChenetal.[
2
]
byanalyzingdifferentCNNsforthemappingfromimagetoindicators.TheyGoogLeNet[
24
]
andVGG16[
25
]toperformbestonthistask.
Duetothedifoftrainingandtestinginrealenvironments,drivingsimulatorsareoften
exploitedinrelatedwork.Twopopulardrivingsimulatorsforresearcharetheopensourcecar
racinggameTORCS[
2
,
23
,
26
]andthecommercialgameGrandTheftAutoV(GTAV)[
27
,
28
].
However,TORCSisneitherphotorealisticnorcomplexenough,lackingimportantsceneelements
suchasintersections,pedestriansandoncomingtrafGTAVcanbeconsideredphotorealistic,
butisclosedsourceandhencelimitedinitsabilitytocustomizeandcontroltheenvironment.Inthis
work,weusetherecentlyreleasedopen-sourcesimulatorCARLA[
29
]whichprovidesatrade-off
betweenrealismandxibility,thusaddressingsomeoftheproblemsofprevioussimulators.
3ConditionalAffordanceLearning
Ourautonomousdrivingsetupconsistsofanagentwhichinteractswiththeenvironmentasillus-
tratedinFigure
2
.Ahigh-levelcommandforthemaneuvertobeperformed(e.g.,ﬁgostraightﬂ,
ﬁturnleftﬂ,ﬁturnrightﬂ)isprovidedbyaplanner,mimickingoneofthemostcommondrivingsce-
narios:ahumandriver(theagent)followingthecommandsofanavigationsystem(theplanner).
CARLAusesanA*topologicalplannertogeneratethesecommands.Basedonthehigh-levelcom-
mandandtheobservationfromtheenvironment,theagentcalculatesthevalueofthrottle,brake
andsteeringwheel.Thosevaluesaresentbacktotheenvironmentmodelwhichprovidesthenext
observationandhigh-levelcommand,andtheprocesscontinuesuntilthegoalisreached.
Inthiswork,wefocusonvisual-basednavigation:weonlyconsiderobservationsfromasingle
front-facingcamera.Camerashavetheadvantagethattheyareubiquitousandcheap.Inaddition,
theyprovideinformationthatlaserscannersorradarsensorscannotprovide,e.g.,thecolorofa
traflightorthetypeofaspeedsign.Thustheyaretheonlysensorthatallowstosolvethetask
withoutanyadditionalsensors.Notethatamonocularsetupiscomparabletohowhumanssolve
thedrivingtask.Thestereobaselineofthehumanvisualsystemisinsuftoperceiveaccurate
depthatdistancesof50metersandbeyondbasedonthiscuealone.
Inthefollowingwewilldiscusstheintermediaterepresentationsrequiredforautonomousnavigation
inurbanenvironmentsandthesubmodulesoftheCALagentforperceptionandcontrol.
3.1Affordances
Agoodintermediaterepresentationshouldbelow-dimensional,yetexpressiveenoughtomeetthe
necessaryrequirementsforautonomousnavigation:(i)theagentshouldbeabletodrivefromAtoB
asfastaspossiblewhileobeyingtrafrules.(ii)infractionsagainsttrafrulesshouldbeavoided
3
Figure2:
OverviewofourSystem.
TheCALagent(top)receivesthecurrentcameraimageand
adirectionalcommand(ﬁstraightﬂ,ﬁleftﬂ,ﬁrightﬂ)fromCARLA[
29
].Thefeatureextractorconverts
theimageintoafeaturemap.Theagentstoresthelast
N
featuremapsinmemory,where
N
is
thelengthoftheinputsequencerequiredfortheperceptionstack.Thissequenceoffeaturemaps,
togetherwiththedirectionalcommandsfromtheplanner,areexploitedbythetaskblockstopredict
affordances.Differenttasksutilizedifferenttemporalreceptiveandtemporaldilationfactors.
ThecontrolcommandscalculatedbythecontrolleraresentbacktoCARLAwhichupdatesthe
environmentandprovidesthenextobservationanddirectionalcommand.
atalltimes.Inthiswork,weconsider
6
typesofinfractions:drivingonthewronglane,drivingon
thesidewalk,runningaredlight,collidingwithothervehicles,hittingpedestriansandhittingstatic
objects.Moreover,thespeedlimitmustbeobeyedatalltimes.(iii)theagentshouldbeableto
provideapleasantdrivingexperience.Thecarshouldstayinthecenteroftheroadandtaketurns
smoothly.Inaddition,itshouldbeabletofollowaleadingcaratasafedistance.
BasedontheserequirementsweproposetheaffordancesetshowninFigure
3
(left).Theagent
shouldstopfor
redlights
andchangeitsspeedaccordingtothecurrent
speedsign
.Forthis
purposewean
observationarea
A
1
inthelocal
(
x;y
)
coordinatesoftheagent,asshownin
Figure
3
(right).Ifatraflightoraspeedsigniswithin
A
1
,thecorrespondingaffordanceindicator
switchesto
True
ortothespeedlimitvalue.Theagentkeepsaninternalmemoryofthespeed
limit.Onceaspeedsignispassed,thecurrentspeedlimitisupdatedandtheagentadjustsitsspeed
accordingly.IfanobstacleisinfrontofthecarŠlocatedwithinan
observationarea
A
2
Šthe
agentshouldperforma
hazardstop
Šcometoacompletestopasquicklyaspossible.InCARLA,
possibleroadobstaclesarecarsandpedestrians.Fordrivinginthecenteroftheroad,theposition
andtheorientationofthecarontheroadmustbeknown.Thepositionisbythe
distanceto
centerline
d
,thelateraldistanceofthecenterofthevehicle'sfrontaxletotheclosestpointonthe
centerline.The
relativeangle
 
describestheorientationofthecarwithrespecttotheroadtangent.
Toachievesteadyvehiclefollowing,the
distancetothevehicle
`
aheadmustbeknown.Ifacar
islocatedinan
observationarea
A
3
,wemeasure
`
astheshortestdistancebetweenthebounding
boxesofbothcars,otherwiseset
`
=50
.
3.2Perception
Weformulateperceptionasamulti-tasklearning(MTL)problem:usingasingleneuralnetworkwe
predictallaffordancesinasingleforwardpass.ThisyieldsacomputationallyefsolutionŒ
foraforwardpass,thesystem(Network+Controller)takes50msonaverageonaNVIDIAGTX
1080TiGPU.Thisiswellwithinreal-timerequirementsforautonomousdrivingsystems,where
latencyshouldreachlevelsbelow100ms[
30
].Furthermore,MTLhasbeenshowntolearninternal
representationswhichareabletoimprovethenetwork'sgeneralizationabilities[
31
].Xuetal.[
32
]
demonstratethiseffectinthecontextofautonomousdriving.Inournetwork,allconvolutional
layersaresharedbetweentasks.Theoutputofthelastconvolutionallayerisusedasinputforthe
brancheswhichwewillcallﬁtaskblocksﬂinthefollowing.
4
TypeConditionalAffordancesAcronymRangeofvalues
discreteNoHazardstop-
2f
True;False
g
RedTrafLight-
2f
True;False
g
SpeedSign[km/h]-
2f
None;
30
;
60
;
90
g
continuousNoDistancetovehicle[m]
`
2
[0
;
50]
continuousYesRelativeangle[rad]
 
2
[

ˇ;ˇ
]
Distancetocenterline[m]
d
2
[

2
;
2]
Figure3:
Affordances.
Left:Wecategorizeaffordancesaccordingtotheirtype(dis-
crete/continuous)andwhethertheyareconditional(dependentondirectionalinput)oruncondi-
tional.Right:Illustrationoftheaffordances(red)andobservationareasusedbyourmodel.
Neuralnetworkarchitecture.
Toextractfeaturesfromrawinputimages,weusetheconvolutional
layersofaVGG16network[
25
]pretrainedonImageNet[
33
].Thesimplestructureofthenetwork
makeitanappealingchoiceforafeatureextractorwithreal-timerequirements.Thefeatureextractor
isappliedtoeveryimageintheinputsequencetoextractasequenceoffeaturemaps.Thetaskblocks
areshallownetworksandconsistofalayer,followedbyabatchnormalizationlayer
[
34
]andadropoutlayer.Forthelayersweexperimentwithdenselayers,LSTMs[
35
],
GRUs[
36
],andtemporalconvolutionallayers.Eachtaskblockhasadifferenttemporalreceptive
andtemporaldilationfactors(e.g.,usingonlyeveryotherimage).Theoutputnodesofthe
conditionaltaskblocksaredividedintothreeequallysizedgroups.Thedirectionalcommand
c
2
f
straight;left;right
g
isusedtoswitchbetweenthesegroupsofnodes.Onlytheactivegroup
isusedtopredictthetargetlabel.Duringtrainingthegradientsareonlybackpropagatedthrough
thisgroup.Weobservedthattrainingspecializedsubmodulesforeachdirectionalcommandleads
tobetterperformancecomparedtousingthedirectionalcommandasanadditionalinputtothetask
networks.ThisobservationagreeswithCodevillaetal.[
21
].However,notethatincontrasttotheir
ConditionalImitationLearningapproach,weusespecializedtasknetworkstopredictconditional
affordancesratherthanconditionalsteeringcommands.Conditioninginthenetworkhasseveral
advantagesoverconditioninginthecontroller,seethesupplementarymaterialforadiscussion.
Training.
Asourapproachdecouplesaffordancepredictionandcontrol,weareabletotrainourper-
ceptionstackindependentlyfromthechosencontrolalgorithm.Totraintheaffordanceprediction
network,werecordedadatasetwiththeCARLAsimulator[
29
].CARLAprovidestwotowns:Town
1fortrainingandTown2fortesting.ForcollectingtrainingdatainTown1,wenavigatethrough
thecityusingourcontroller(describedinsection
1.2
)basedonthegroundtruthaffordances.During
datacollection,thedirectionalinputatintersectionsischosenatrandom.Allothertrafpartici-
pantsandalltraflightsarecontrolledbythein-gameAI.Thefulldatasetcontainsapproximately
240thousandimagesandthecorrespondinggroundtruthlabelsforeachofthesixaffordances.Five
percentofthedatasetisusedforvalidation.Thenetworkistrainedonmini-batchesofsize32.
ForoptimizationweuseAdam[
37
]withabaselearningrateof5e-5.Forweusethe
class-weightedcategoricalcross-entropy
H
wheretheweightsarechoseninverselyproportionalto
theclasslabeloccurrenceinthetrainingset.Forregression,weusethemeanaverageerror
MAE
.
Allsixaffordancelossfunctionsareaddedtoyieldthelossfunctionforthewholenetwork:
L
=
3
X
j
=1
H
j
+
3
X
k
=1
MAE
k
:
(1)
Dataaugmentationiscrucialtoachievegeneralization.Following[
29
],weusedthefollowingaug-
mentations:changeincolor,contrast,andbrightnessaswellasGaussianblurandsalt-and-pepper
noise.Commontechniqueslikeverticalandhorizontalarenotused,becauseinthedriving
contextverticalwouldcorrespondtoswitchingfromright-toleft-handtrafandhorizontal
Štodrivingupsidedown.Inaddition,weemploycameraposerandomization.Itiswell
knownthatforimitationlearningutilizingasinglexedcameraposeleadstounstablebehavior
[
20
,
38
]duetocontrolerrorsandunexpectedobservationsreinforcingeachother.Thus,weusea
three-camerasetupthatrecordsthesameepisodeat
3
differentbaselines(

50
cm,
0
cm,
+50
cm).
Inaddition,eachcameraisrandomlyrotatedarounditsverticalaxisbyupto

15

.Thissetupis
onlyusedforcollectingthedataset.Duringtesting,weusetheinputofasinglecenteredcamera.
5
During10%ofthetrainingepisodes,weadditionallysetthedistancetotheprecedingvehicle
`
to
zerotoprovokerear-endcollisionswithothercarswhichotherwisewouldnotoccurinourtraining
set.Thiswaythenetworklearnstocorrectlypredictthesesituationashazardstops.
Hyperparametersearch.
Thetaskblockhyperparametersentailthe
type
oflayer(Dense,LSTM,
GRU,TemporalConvolution),thenumberof
nodes
withintheselayers,thedropoutratio
p
,the
sequencelength
seq
,andthedilationvalue
dil
.Thearchitectureofournetworkisoptimizedina
two-stepprocedure.First,werandomlychoose
type
andsamplevaluesfor
nodes;p;seq;
and
dil
fromvalueranges(seesupplementarymaterialfortherespectiveranges).Weinitial-
izethenetworkwiththesamesetofparametersforeachtaskblockandtrainituntiltheerroron
thevalidationsetconverges.Second,theoverallbestperformingtaskblocksarecombinedintoa
newmodel.Thecombinedmodelisthentrainedjointlyuntilthevalidationerrorconverges.The
hyperparametersofthebestperformingtaskblocksareshowninthesupplementarymaterial.
3.3Controller
Thecontrolalgorithmissplitintolongitudinalcontrol(throttle,brake)andlateralcontrol(steering)
sinceitispossibletodecouplethesetwotasks[
39
].
Longitudinalcontrol.
Thelongitudinalcontrollerissubdividedintoseveralstates,seethesup-
plementarymaterialforanillustration.Thestatesare(inascendingimportance):
cruising
,
fol-
lowing
,
over
limit
,
red
light
,and
hazard
stop
.Allstatesaremutuallyexclusive.For
cruising
the
targetspeed
v

isthecurrentspeedlimit.Giventhecurrentspeed
v
(
t
)
weminimizetheerrorterm
e
=
v

(
t
)

v
(
t
)
usingaPIDcontroller[
43
]forthethrottlevalue.Thebrakeissettozero.The
valueof
v

alsodependsonthecurrentroadtype.Iftheagentreceivesadirectionalinputtoturn
leftorright,wereduce
v

by
10
km/htoenablesmoothandsafenavigationthroughturns.Thestate
switchesto
following
ifanothercarislessthan
35
mahead.Wechosethisstaticthreshold,because
wefoundthatadynamicthresholdcombinedwiththeuncertaintyofthenetworkpredictionsleads
toundesiredstateswitchingbetweenfollowingandcruising.Wewilldenotethedistancetothefront
caras
`
andthecurrentspeedlimitas
v
max
.Weusetheoptimalcar-followingmodelofChenetal.
[
2
]andminimizetheerrorterm
e
(
t
)=
v

(
t
)

v
(
t
)=
v
max

1

exp


c
v
max
`
(
t
)


d


v
(
t
)
(2)
withasecondPIDcontroller.Ifthecarisdrivingfasterthanthecurrentspeedlimit,thestate
switchesto
over
limit
,foradetectedredlightitswitchesto
red
light
,foradetectedobstacleit
switchesto
hazard
stop
.For
over
limit
,
red
light
,and
hazard
stop
,thethrottleandbrakevalue
dependonthevehiclespeed,thepredictionprobabilityandtheurgencytostop.Thedetailsaswell
asadescriptionofthePIDtuningprocedureareprovidedinthesupplementarymaterial.
Lateralcontrol.
ForlateralcontrolweusetheStanleyController(SC)[
4
].TheSCusestwoerror
metrics:thedistancetocenterline
d
(
t
)
andtherelativeangle
 
(
t
)
.Thecontrollawtocalculatethe
steeringangle

SC
(
t
)
atthecurrentvehiclespeed
v
(
t
)
isgivenby

SC
(
t
)=
 
(
t
)+arctan

kd
(
t
)
v
(
t
)

(3)
where
k
isagainparameter.Tocalculatethesteeringoutput

,weaddadampingterm

d
which
reducesswayingonstraightroads.

d
(
t
)
iscalculatedbyusingthecurrentoutputofthecontrollaw

SC
(
t
)
,thesteeringangleattheprevioustimestep

(
t

1)
,andapositivedampingconstant
D
:

(
t
)=

SC
(
t
)+

d
(
t
)
where

d
(
t
)=

D

(

SC
(
t
)


(
t

1))
(4)
4Results
Weusethedrivingbenchmarkandevaluationprotocolof[
29
]asthebasisofourevaluation.The
benchmarkiscomposedoffourdifferentdrivingtaskswithincreasingdifdrivingstraight,
drivingthroughasingleturn,navigatingthroughthetowntakingseveralturns,andnavigating
throughthetownwithadditionaldynamicobstacles(cars,pedestrians).Theagentisinitialized
atapre-pointinthetownandhastoreachagoalunderatimelimit.Thetime
limitequalsthetimeneededtoreachthegoalwhendrivingalongtheoptimalpathat
10
km/h.
6
Table1:
QuantitativeEvaluationonGoal-DirectedNavigationTasks.
Weshowthepercentage
ofsuccessfullycompletedepisodespertaskandcompareCALtoaModularPipeline(MP)[
29
],
ConditionalImitationLearning(CIL)[
21
]andReinforcementLearning(RL)[
29
].
TrainingconditionsNewweatherNewtown
Newtownand
newweather
TaskMPCILRLCALMPCILRLCALMPCILRLCALMPCILRLCAL
Straight989589
100100
9886
100
92
97
7493508068
94
Oneturn828934
97
959016
96
615912
82
504820
72
Navigation808614
9294
8429024403
70
47446
68
Nav.dynamic77
83
7
8389
8228224382
64
44424
64
Thesimulationengineprovidestwodifferenttowns(Town1,Town2).OnlyTown1hasbeenseen
duringtraining.Theevaluationdistinguishesbetweenweatherconditionsthatwereseenduring
trainingandthosethatwerenot.Foreachcombinationoftask,town,andweathercondition,
25
dif-
ferentepisodesareevaluated.WecompareourConditionalAffordanceLearning(CAL)approachto
ConditionalImitationLearning(CIL)[
21
]aswellastheModularPipeline(MP)andReinforcement
Learning(RL)approachpresentedin[
29
].Asstatedintheoriginalpaper,theRLbaselinecannot
beclaimedstate-of-the-artasitstillhasroomforimprovementregardingitsimplementation.Note
thatthebaselinesignoreallredlightsandtargetacruisingspeedof
20
km/h.Incontrast,ouragent
stopsforredlightsandrecognizesspeedsigns.Toprovideafaircomparisontopriorart,welimit
ouralgorithmtoacruisingspeedof
20
km/hforthequantitativeevaluationagainstthebaselines.
Weusethesamemodelforalltasksandweatherconditionswithoutparameter
4.1Goal-DirectedNavigation
Table
1
showsourquantitativecomparisonwithrespecttothestate-of-the-artonCARLAonthe
taskofgoal-directednavigation.Ouragentoutperformsthebaselinesinmostofthetasksandunder
mostconditions.Asexpected,performancedecreasesforallmethodswithincreasingtaskdif.
CALparticularlyexcelsingeneralizingtothenewtown,ascenariowheremostbaselinesdidnot
performwell.Forthenewtownundertrainingweatherconditions,theperformanceoftheCAL
agentisupto
68
%betterthanthebestbaseline.
CALoutperformsMPonalmostallofthetasksexceptforthetasksinthetrainingtownundernew
weatherconditionswhereMPperformsslightlybetter.Inalmostalltasksandconditions,CAL
showsbetterperformancecomparedtoCIL.Predictionoftherelativeangle
 
especially
fromthetemporalstructureoftheCALnetworkasshowninthesupplementarymaterial.Also,
theadditionalparametersofthecontroller(e.g.,damping
D
)improvedrivingperformance.CALis
outperformedbyCILonlyononetask:drivingstraightunderthetrainingweatherconditionsinthe
testtown.OuranalysisrevealedthatthishappensduetoasystematicfailureoftheCALperception
stackinoneweatherconditionwhereseveraltreescastshadowsontothestreet,affecting
thepredictionofthedistance
d
tocenterlinesuchthattheagentoccasionallydriftsoffthestreet.
4.2InfractionAnalysis
Table
2
showstheaveragedistancesdrivenbetweentwoinfractionsduringthehardesttaskofthe
benchmark:navigationwithdynamicobstacles.Undertestconditionsourapproachisabletoout-
performthebaselinesinalmosteverymeasure.Thisindicatesthestronggeneralizationabilityof
ourapproachincomparisontothebaselines.CALperformsparticularlywellinavoidingcollisions
withothervehiclesandpedestrians.Inthenewenvironment,ouragentperformsmorethan
10
times
betterthanthebestbaseline(RL).Duringtesting,pedestriansarecrossingthestreetmorefrequently
thaninthetrainingscenario.Thefactthatouragentisstillabletoperformwelldemonstratesthat
thehazardstopnetworkwasabletopredictthisaffordancerobustlydespiteitssparsity.
Occasionally,theCALagentdrivesonthesidewalk.Thisinfractionoccursprimarilyduringright
turns.SincerightsturnsinCARLAaregenerallysharperthanleftturns,therelativemotionof
pixelsbetweenconsecutiveframesislargerduringarightturn.Thismayposeabiggerchallengeto
ourperceptionnetworkwhichisoperatingonsequentialinput.Drivingonthesidewalksometimes
alsoresultsincrasheswithstaticobjects,e.g.,traflights.Notethattheinfractioncountforstatic
collisionsisgenerallyhigherthanthatfordrivingonthesidewalkastheagentsometimesslides
alongarailinglocatednexttotheroad,continuouslyincreasingtheinfractioncount.
7
Table2:
InfractionAnalysis
.Weshowtheaveragedistance(inkm)drivenbetweentwoinfractions.
'
>
'indicatesthatnoinfractionoccurredoverthewholecourseoftesting.Notethatthetotaldriven
distancedependsontheamountandlengthoftheepisodesthatanagent
TrainingconditionsNewweatherNewtown
Newtownand
newweather
TaskMPCILRLCALMPCILRLCALMPCILRLCALMPILRLCAL
Oppositelane10.2
33.4
0.186.716.1
57.3
0.09
>
60
0.451.120.23
2.21
0.400.780.21
2.24
Sidewalk
18.3
12.90.756.124.2
>
57
0.726.00.460.760.43
0.88
0.430.810.48
1.34
Collision:Static
10.0
5.380.422.5
16.1
4.050.246.0
0.44
0.400.230.36
0.45
0.280.250.31
Collision:Car
16.4
3.260.5812.120.21.860.85
>
60
0.510.590.41
2.04
0.470.440.37
1.68
Collision:Pedestrian18.96.3517.8
30.3
20.411.220.6
>
60
1.401.882.55
26.49
1.461.412.00
6.72
Figure4:
AttentionAnalysis.
Left:Thenetworkfocusesattheperipheryforrecognizingtraf
lights.Middle:Thenetworkisanticipatingahazardstopbyobservingtheshadowofapedestrian.
Right:Thenetworkfocusesitsattentiononthepedestriantocorrectlypredictahazardstop.
4.3AttentionAnalysis
Inordertogainmoreinsightintothedecisionmakingprocessofourperceptionstackweexplore
gradient-weightedclassactivationmaps(Grad-CAMs)[
41
]whichexposetheimplicitattentionof
theCNNwithrespecttotheinputimage.Figure
4
showstheresultsfortheaffordances
hazard
stop
and
red
light
.Weencodethepredictionprobabilityforaparticularclasswithdifferentcolors
(greenmeanslowprobability,redmeanshighprobability).Theattentiongiventoaparticularimage
regionisencodedintheintensitychannel.Thestrongertheattention,themoreintensethecolor.
Regionswithverylowattentionareshowntransparentforclarity.Forimitationlearningthisatten-
tionanalysiswouldbemuchlessinterpretablesincepredictingasinglecontroloutput,e.g.throttle,
entailsthedetectionofspeedlimits,redlights,carsandpedestrians-allinonesignal.
Figure
4
(left)showstheagentdrivingthroughtown1withtheattentionmapfor
red
light
overlaid.
Notethatthenetworkfocusesitsattentionontheroadoutlinesandstructuresontheroadsidewhere
traflightsareexpectedtobelocated.InFigure
4
(middle)theshadowofapedestrianisvisible
atthebottomleftoftheimage.Interestingly,thenetworkanticipatesanimpending
hazard
stop
andfocusesitsattentiontothisregiondespitethefactthatthepedestrianitselfisnotvisibleinthe
image.Finally,inFigure
4
(right)weillustrateahazardstopsituation.Thenetworkfocusesallits
attentiononthepedestrian'slowerbody.Apossibleexplanationisthatthenetworkisableestimate
thepedestrian'swalkingdirectionbyobservingitslegsandfeet.Notethatnopixel-wisesupervision
hasbeenprovidedtothenetworkduringtraining.Instead,thisbehaviorhasbeenlearnedfrombinary
image-levellabels(
hazard
stop
=
True=False
)alone.
5Conclusion
WehaveproposedConditionalAffordanceLearningwhichcombinestheadvantagesofbothmodu-
larpipelinesandend-to-endapproachesfornavigatingcomplexurbanenvironmentsusinghigh-level
directionalcommandsasinput.Wetestedourapproachextensivelyinsimulationanddemonstrated
performancegainswithrespecttothestate-of-the-artonCARLA.Inthefuture,weplan
toextendourworkinseveraldirections.Trainingonmorediverseenvironmentswilllikelyimprove
theagent'sperformanceinalldomains.Anextendedstudyofnetworkarchitecturescanimprove
thequalityofthepredictionsandthereforeimprovethedrivingcapabilitiesoftheagent.Wealso
plantoextendthesensorsetup,e.g.,usingastereocameratoimprovedistanceprediction.More
sophisticatedcontrolalgorithmslikemodelpredictivecontrolmayfurtherimprovetheperformance,
inparticularwhentrainingtheparametersofthecontrollerjointlywiththeperceptionstack.Finally,
inspiredbyrecentwork[
42
],weplantotransferourresultsfromsimulationtotherealworld.
8
SupplementaryMaterial:
ConditionalAffordanceLearning
forDrivinginUrbanEnvironments
AxelSauer
1,2
NikolaySavinov
1
AndreasGeiger
1,3
1
ComputerVisionandGeometryGroup,ETHZ
¨
urich
2
ChairofRoboticsScienceandSystemIntelligence,TechnicalUniversityofMunich
3
AutonomousVisionGroup,MPIforIntelligentSystemsandUniversityofT
¨
ubingen
Abstract:
This
supplementarydocument
providesfurtherimplementationde-
tailsofourCALagentinSection1,adetaileddescriptionofourgroundtruth
acquisitionprocessinSection2andadditionalexperimentsinSection3.
The
supplementaryvideo
showsseveralnavigationexamplesandvisualizesthe
attentionofouragentfordifferentaffordanceindicatorsovertime.Thevideois
availableat
https://www.youtube.com/watch?v=UtUbpigMgr0
.
1ImplementationDetails
Inthissection,welistthevaluerangesandresultsofourhyperparametersearch.Wealsoprovide
additionaldetailsaboutourlongitudinalcontrolalgorithmandthePIDtuningprocedure.
1.1HyperparameterSearch
Table
3
showseachhyperparameteranditsrangeofvaluesfortherandomsearchdescribedinthe
mainpaper.Weinitializethenetworkwithrandomlysampledparametersfromtherespectiveranges.
Table
4
showstheparametersofthebestperformingtaskblocksafteroptimization.
Table3:
HyperparameterSearch.
Valuerangesforeachhyperparameter.
Hyperparameter
LayertypeNumberofnodesDropoutamountSequencelengthDilationvalue
Acronym
typenodespseqdil
Rangeofvalues
Dense,GRU,LSTM,
temporalconvolution
[10
;
200][0
:
25
;
0
:
75][1
;
20][1
;
3]
Table4:
Besttaskblockparameters.
Weoptimizedthelayer
type
,thenumberof
nodes
,the
dropoutratio
p
,thesequencelength
seq
,andthedilationvalue
dil
.
Task
typenodespseqdil
RedlightGRU1850.27142
HazardstopTemp.convolution1600.6861
SpeedsignDense1600.5511
VehicledistanceGRU1600.38111
RelativeangleTemp.convolution1000.44101
CenterdistanceTemp.convolution1000.44101
9
Figure5:
StatesofLongitudinalController.
Thestatesareorderedindescendingimportancefrom
top-to-bottomasindicatedbythecolorintensity.Allstatearemutuallyexclusive.
1.2Controller
LongitudinalControl.
ThestatesofthelongitudinalcontrollerareillustratedinFigure
5
.
Thethrottleandbrakevaluesforthestates
over
limit
,
red
light
,and
hazard
stop
areasfollows:

over
limit
:Thisstateisactivated,iftheagentisdrivingmorethan15km/hfasterthan
thespeedlimit
v

.Thissituationtypicallyoccurswhenenteringalow-speedzonefroma
high-speedzone.Todeceleratequickly,wesetthethrottlevaluetozeroandcalculatethe
brakevaluedependingonthecurrentspeed
v
(
t
)
:
brake
=0
:
3

v
(
t
)
v

(
t
)
(5)
Asanexample,drivingwithfullspeed(90km/h)intoa30km/hzoneyields
brake
=0
:
9
,
henceapplyingthebrakesalmostfully.

red
light
:Ifthepredictionprobability
P
fortheclass
red
light
,i.e.,theactualsoftmax
output,ishigherthanathreshold
P
rl
,thecontrollerswitchestothestate
red
light
.The
throttleissettozeroandthebrakeisappliedwith:
brake
=0
:
2

v
(
t
)
30
(6)
Weempiricallyfoundthatathresholdof
P
rl
=0
:
9
reducesfalsepositiveswhilestill
beingabletoreliablestopinfrontofredlights.Notethatweuseasmallermultiplier
(
0
:
2
)comparedtothe
over
limit
stateasredlightstypicallyoccurin30km/hzones.We
considerthecurrentspeed
v
(
t
)
tograduallyslowdownthecarinfrontofredlights.

hazard
stop
:Thisstateisactivatedwhenanobstacleinfrontoftheagentisdetected,
i.e.,when
P
(
hazard
stop
)
>P
hs
whereweempiricallydeterminedthethreshold
P
hs
as
P
hs
=0
:
7
.Notethethresholdislowerthanthatforthe
red
light
state,sincepreventing
crasheswithroadhazardsismorecriticalforsuccessfulgoal-directednavigation.Whena
hazard
stop
hasbeendetected,thethrottleissettozeroandthebrakeissettoone.
PIDControllerTuning.
ThePIDcontrollersusedinthe
cruising
and
following
statefollowthe
standardPIDcontrolscheme[
43
].Theoverallcontrolfunctionisgivenasfollows
u
(
t
)=
K
p
e
(
t
)+
K
i
Z
t
0
e
(
t
0
)
dt
0
+
K
d
de
(
t
)
dt
;
(7)
where
K
p
,
K
i
and
K
d
aretheproportional,integralandderivativecoefTobeabletotune
thecoefofthetwoPIDcontrollers,weimplementedavisualizationtoolofspeed,distance
tothecenterline,andotherimportantsignals.Withthisdirectvisualfeedback,itispossibletouse
standardPIDtuningmethods.Inthiswork,weleveragethemethodbyZiegler-Nichols[
46
].First,
10
Table5:
ObservationAreas.
Theobservationareasarerectangularboxes.Thistableliststhe
(
x;y
)
coordinatesofthevertices
v
ofeachobservationarea(providedin
c
local
inmeters.)
AreaDetectionof
v
1
v
2
v
3
v
4
A
1
red
light
speed
sign
(7.4,-0.8)(7.4,-5.8)(14.0,-0.8)(14.0,-5.8)
A
2
hazard
stop
(0.0,2.0)(0.0,-2.0)(8.2,2.0)(8.2,-2.0)
A
3
distance
to
vehicle
(0.0,1.6)(0.0,-1.6)(50.0,1.6)(50.0,-1.6)
allcoefaresettozero.Then,theproportionalgain
K
p
isincreaseduntiltheoutputofthe
loopstartstooscillate.Giventhisﬁultimategainﬂ
K
u
andtheoscillationperiod
T
u
,wesetthe
coefto
K
p
=0
:
6
K
u
;
(8)
K
i
=
T
u
=
2
;
(9)
K
d
=
T
u
=
8
:
(10)
Usingthesevaluesasastartingpoint,weempiricallythecoefforoptimalperfor-
mance,withthegoalofenablingfastbutsmoothreactionstodisturbances.
1.3ConditioningintheNetworkvs.ConditioningintheController
Conditioningintheneuralnetworkisadvantageouscomparedtodirectlyconditioningthecon-
trollers.Conditioningthecontrollerswouldrequirethepredictionofallaffordancesincludingthe
oneswhicharenotrelevantforthecurrentnavigationtaskateverypointintime.
Consideranintersectionwherethegoalistogostraight.Eventhoughtheonlyrelevantsignalin
termsofdistancetothecenterlineisthedistancetothecenterlineofthestraightlane,thedistance
tothecenterlinesoftheotherlanes(leftturning,rightturning)mustalsobepredictedinordertolet
thecontrollerselecttherelevantsignalbasedonthedirectionalcommand.However,predictingthe
irrelevantaffordancesisverydifastherequiredimagefeaturesmaynotbepresent(whengoing
straight,theturninglanesleavetheofview).Furthermore,thetaskisalsohighlyambiguous
(oncetheintersectionhasbeentraversedviathestraightlane,thedistancestothecenterlineofthe
leftandrightturninglanesarenotwellThus,requiringthenetworktopredictallaffor-
dancesineverysituationwouldintroducenoiseduringtrainingandhencelowertheperformanceof
thesystem.Besides,itwouldalsoincreaseruntimeasmoresignalsmustbepredictedatanypoint
intime.
2GroundTruthAcquisition
TheAPIofCARLAsuppliesmeasurementsabouttheagent(speed,acceleration,location,orien-
tation)andaboutotherobjectsinthescene(cars,pedestrians,traflights,andspeedlimitsigns).
Thesemeasurementsincludethecurrentstatusofthetraflight(green,orange,red)andthetype
ofthespeedsign(30,60,90).Locationandorientationareinaworldcoordinatesystem
c
global
=(
x
g
;y
g
;z
g
)
|
.Asthesemeasurementsdonotdirectlyexpresstheaffordanceswewantto
learn,weimplementedaproceduretoconvertthemintothedesiredgroundtruth.
2.1ObservationArea
Wealocalcoordinatesystem
c
local
=(
x
l
;y
l
;z
l
)
|
atthecenterofthefrontaxleofthecar
withthex-axiscorrespondingtothecar'slateralaxisandthez-axiscorrespondingtotheupvector.
Theagent'sorientation
 
andtheagent'sposition
(
x
ego
;y
ego
)
issuppliedin
c
global
.Usingthis
information,weconvertthepositionofallotherobjectsto
c
local
.Next,wethe
observation
areas
asrectanglesinthex-yplaneof
c
local
,seeFigure3ofthemainpaperforanillustration.If
anobjectfallsintoanobservationareaitisconsideredﬁdetectedﬂ.
11
Table6:
ComparisonofTemporalandNon-temporalTaskBlocks
.Thelastcolumnshowsthe
relativechangeinperformance.Thehigherthe
IoU
andthelowerthe
MAE
thebetter.
Bestperformingtaskblocks
TaskMetricnon-temporaltemporalrelativeChange
Hazardstop
IoU
84.96%87.41%+2.88%
Speedsign
IoU
91.95%92.71%+0.83%
Redlight
IoU
92.41%93.95%+1.67%
RelativeangleMAE0.007970.00207-74.03%
CenterdistanceMAE0.096420.08465-12.21%
VehicledistanceMAE0.044970.03289-26.86%
Thelength,width,position,andorientationoftheobservationareasarechosenbasedonthere-
spectiveaffordances.Thus,theobservationareafor
red
light
andfor
speed
sign
islocatedonthe
rightsideoftheagentastheirrespectivesignalsarelocatedontherightsidewalk.Theobservation
areafor
hazard
stop
isdirectlyinfrontofthecarandveryshort,inordertoonlyinitiateahazard
stopifanaccidentcouldnotbeavoidedotherwise.Theobservationareafor
vehicle
distance
is
infrontofthecarandhasalengthof50m.Ifanothervehicleislocatedwithinthisarea,thedis-
tanceoftheclosestvehicletotheagentismeasured.Ifthereisnocarinfront,thedefaultvaluefor
vehicle
distance
(50m)isused.Table
5
liststhecoordinatesforeachobservationarea.
2.2DirectionalInput
CARLAprovidesahigh-leveltopologicalplannerbasedontheA*algorithm.Ittakestheagents
positionandthecoordinatesofthedestinationandcalculatesalistofcommands.Thisﬁplanﬂadvises
theagenttoturnleft,rightortokeepstraightatintersections.
3AdditionalExperiments
Inthissection,weprovideadditionalexperiments.First,wecomparetemporaltonon-temporal
taskblockstoassesstheoftheadditionaltemporalinformationprovidedbyvideodata.
Second,weprovideaqualitativeevaluationofouragent'sdrivingbehavior.
3.1ComparisonofTemporalandNon-temporalTaskBlocks
Table
6
showsthebestperformingtaskblocksforeachtask.Byusingatemporaltaskblock,all
theandregressionresultsimprove.Thisdemonstratesthateachtaskfromthe
additionaltemporalinformation.
Thebiggestrelativeimprovementcanbeseenforthe
relative
angle
taskblock.Theerrorofthe
temporaltaskblockisalmostfourtimeslowerthanthenon-temporaltaskblock.Thissuggests
thatthistaskmorefromthetemporalcontextthanothertasks.Thesmallestimprovementis
achievedforthe
speed
sign
task.Tokeepcomputationtimelowduringtrainingandinference,we
thereforeusethenon-temporaltaskblockforthistaskinourmodel.
Inaddition,weempiricallyobservedthatthereisnodominatingtemporallayerintermsofperfor-
mance.LSTMs,GRUsandtemporalconvolutionlayersperformverysimilar.
3.2DrivingBehaviour
Theexperimentsinthemaintextexaminedwhetherthegoalwasreachedandiftherewereanyrule
violations.Thissectionfocusesonqualitativedrivingexperience,i.e.,howthedrivingwouldbe
perceivedbyapassenger.Theevaluationisdoneforthetaskﬁnavigationwithoutdynamicobjectsﬂ
toevaluatethegeneraldrivingbehaviorwithoutdistortingtheresultsbythechallengesofstopping
forcarsorpedestrians.
12
Table7:
DrivingBehaviour.
Qualitativeevaluationofthegeneraldrivingperformance,thelowera
metricthebetter.
MetricsUnitCILRLCAL
Distancetocenterline
[
m
]
0.3900.755
0.334
Longitudinaljerk
[
m=s
3
]
0.4491.368
0.333
Lateraljerkdrivingstraight
[
m=s
3
]
0.0840.336
0.052
Lateraljerkdrivingturns
[
m=s
3
]
0.2420.548
0.065
Weusethefollowingmetricsforevaluation:

Centerlinedistance:
Stayinginthemiddleoftheroadisthemaintaskofeverylane
keepingsystem.Weevaluatetheabilityofthealgorithmtominimizethedeviationfrom
thecenterline.Thereportedresultisthemedianoverallepisodes.Themedianismore
descriptiveforthequalitativedrivingexperiencethanthemeanvaluesincefailedepisodes
duringwhichanagentdriftsofftheroadproducelargeoutliers.

Jerk
istherateofchangeofacceleration.Thejerkcanbefeltduringdrivingintheform
ofajoltorsuddenshock.Itiscommonlyusedtoquantifyridecomfort[
45
].Asmoother
rideresultsinlowerfuelconsumptionincreasedpassengercomfortandmoresafety.Away
toquantifyjerkistocomparetherootmeansquare(RMS)jerk[
44
].Theanalysisfurther
distinguishesbetweenlongitudinalandlateraljerkwithlateraljerkseparatelyevaluatedfor
straightroadsandinturns.
Table
7
reportsourresults.Incontrasttothepreviousevaluations,theresultsarenotreported
dependingontheweathersconditionsortheenvironments.Theresultsareverysimilarunderall
conditionsforallagents.TheCALagentisabletoachievethebestperformanceonallfourmetrics.
DistancetoCenterline
Allagentsperformonasimilarlevelandareabletokeepthedistancetothecenterlinelow.The
exceptionistheRLapproach.Whendrivingonastraightroad,theRLagentregularlystartsswaying
fromsidetosideovertheentirelane,resultinginthehighvalue.
LongitudinalJerk
TheCALagentperformsbest,followedbyCILandRL.ThecontrolparametersoftheCALagent
arefreelyadjustablewhichallowstoaccelerateanddeceleratesmoothlyaswellasdrivingata
constantpace.TheRLagentisonlyabletosetthethrottlevaluetoeither0or1.Thisresultsina
suddenjerkeverytimetheagentutilizesthethrottle.
LateralJerkwhileDrivingStraight
Onstraightroads,boththeCALandtheCILagentperformsimilarly.Whendrivingstraight,the
RLagentoftenoutputsasteeringvalueof0.Thisleadstotheagentdriftingofftheroad.When
correctingagainstthedrift,theRLagentsteersabruptly,resultinginlargejerkvalues.
LateralJerkwhileTurning
TheCALagentperformsexceptionallywell.Thereareseveralreasonsforthis.First,theagentis
slowingdownwhenapproachingaturn.Second,ouragentturnssmoothlywithoutabruptchangesin
steering.Third,thejerkpeaksaregenerallylowerthanfortheotherapproaches.Despitethisgood
performance,thetransitionfromturnstostraightroadsleavesroomforimprovement.Changesin
thedirectionalswitchresultinsuddenjumpsinthepredictionoftherelativeangleinsomecases,
resultinginslightshort-timedjerk.TheCILagentisnotasgoodastheCALagent,butitisgen-
erallyabletodrivethroughturnssmoothly.RL,incontrast,conductsstrongandabruptsteering
movements,resultinginahigherjerkvalue.
13
Acknowledgments
WewouldliketothankVanessaDiedrichsformanyhelpfuldiscussionsaboutautonomousdriving
anddatavisualizationandforhergeneralsupport.
References
[1]
S.J.RussellandP.Norvig.
intelligence:amodernapproach
.2016.
[2]
C.Chen,A.Seff,A.Kornhauser,andJ.Xiao.Deepdriving:Learningaffordancefordirect
perceptioninautonomousdriving.In
ICCV
,2015.
[3]
R.Metz.Andtheawardformostnauseatingself-drivingcargoesto...
MITTechnologyReview
,
Jan2018.
[4]
S.Thrun,M.Montemerlo,H.Dahlkamp,D.Stavens,A.Aron,J.Diebel,P.Fong,J.Gale,
M.Halpenny,andG.Hoffmann.Stanley:TherobotthatwontheDARPAgrandchallenge.
JournalofFieldRobotics
,23(9):661Œ692,2006.
[5]
J.Ziegler,P.Bender,M.Schreiber,H.Lategahn,T.Strauss,C.Stiller,T.Dang,U.Franke,
N.Appenrodt,andC.G.Keller.MakingBerthadriveŠAnautonomousjourneyonahistoric
route.
IEEEIntelligentTransportationSystemsMagazine
,6(2):8Œ20,2014.
[6]
A.Geiger,P.Lenz,C.Stiller,andR.Urtasun.Visionmeetsrobotics:TheKITTIdataset.
The
InternationalJournalofRoboticsResearch
,32(11):1231Œ1237,2013.
[7]
J.Redmon,S.Divvala,R.Girshick,andA.Farhadi.Youonlylookonce:real-time
objectdetection.In
CVPR
,2016.
[8]
S.Ren,K.He,R.Girshick,andJ.Sun.FasterR-CNN:Towardsreal-timeobjectdetectionwith
regionproposalnetworks.In
NIPS
,2015.
[9]
K.He,G.Gkioxari,P.Doll
´
ar,andR.Girshick.MaskR-CNN.In
ICCV
,2017.
[10]
Y.Li,H.Qi,J.Dai,X.Ji,andY.Wei.Fullyconvolutionalinstance-awaresemanticsegmenta-
tion.In
CVPR
,2017.
[11]
A.Kendall,H.Martirosyan,S.Dasgupta,andP.Henry.End-to-endlearningofgeometryand
contextfordeepstereoregression.In
ICCV
,2017.
[12]
J.

ZbontarandY.LeCun.Stereomatchingbytrainingaconvolutionalneuralnetworktocom-
pareimagepatches.
JournalofMachineLearningResearch(JMLR)
,17(65):1Œ32,2016.
[13]
D.Sun,X.Yang,M.-Y.Liu,andJ.Kautz.PWC-Net:CNNsforOpticalFlowUsingPyramid,
Warping,andCostVolume.
arXiv.org
,2017.
[14]
F.G
¨
uneyandA.Geiger.Deepdiscretew.In
ACCV
,2016.
[15]
H.Zhang,A.Geiger,andR.Urtasun.Understandinghigh-levelsemanticsbymodelingtraf
patterns.In
ICCV
,2013.
[16]
A.Geiger,M.Lauer,C.Wojek,C.Stiller,andR.Urtasun.3DTrafSceneUnderstanding
fromMovablePlatforms.
IEEETransactionsonPatternAnalysisandMachineIntelligence
,36
(5):1012Œ1025,2014.
[17]
W.Schwarting,J.Alonso-Mora,andD.Rus.Planninganddecision-makingforautonomous
vehicles.
AnnualReviewofControl,Robotics,andAutonomousSystems
,1(1):187Œ210,2018.
[18]
D.A.Pomerleau.Alvinn:Anautonomouslandvehicleinaneuralnetwork.In
NIPS
,1989.
[19]
U.Muller,J.Ben,E.Cosatto,B.Flepp,andY.L.Cun.Off-roadobstacleavoidancethrough
end-to-endlearning.In
NIPS
,2006.
14
[20]
M.Bojarski,D.DelTesta,D.Dworakowski,B.Firner,B.Flepp,P.Goyal,L.D.Jackel,
M.Monfort,U.Muller,andJ.Zhang.Endtoendlearningforself-drivingcars.
arXiv.org
,
2016.
[21]
F.Codevilla,M.M
¨
uller,A.Dosovitskiy,A.Lopez,andV.Koltun.End-to-enddrivingvia
conditionalimitationlearning.In
ICRA
,2018.
[22]
J.J.Gibson.
Thesensesconsideredasperceptualsystems.
1966.
[23]
M.Al-Qizwini,I.Barjasteh,H.Al-Qassab,andH.Radha.Deeplearningalgorithmforau-
tonomousdrivingusingGoogLeNet.In
IntelligentVehiclesSymposium
,2017.
[24]
C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan,V.Vanhoucke,and
A.Rabinovich.Goingdeeperwithconvolutions.In
CVPR
,2015.
[25]
K.SimonyanandA.Zisserman.Verydeepconvolutionalnetworksforlarge-scaleimagerecog-
nition.
arXiv.org
,2014.
[26]
J.Huang,I.Tanev,andK.Shimohara.Evolvingageneralelectronicstabilityprogramforcar
simulatedinTORCS.In
CIG
,2015.
[27]
S.R.Richter,V.Vineet,S.Roth,andV.Koltun.Playingfordata:Groundtruthfromcomputer
games.In
ECCV
,2016.
[28]
S.Ebrahimi,A.Rohrbach,andT.Darrell.Gradient-freepolicyarchitecturesearchandadap-
tation.In
CoRL
,2017.
[29]
A.Dosovitskiy,G.Ros,F.Codevilla,A.Lopez,andV.Koltun.CARLA:Anopenurban
drivingsimulator.In
CoRL
,2017.
[30]
S.Lin,Y.Zhang,C.Hsu,M.Skach,M.Haque,L.Tang,J.Mars.TheArchitecturalImplica-
tionsofAutonomousDriving:ConstraintsandAcceleration.In
ASPLOS
,2018.
[31]
R.Caruana.Multitasklearning:Aknowledge-basedsourceofinductivebias.In
ICML
,1993.
[32]
H.Xu,Y.Gao,F.Yu,andT.Darrell.End-to-endlearningofdrivingmodelsfromlarge-scale
videodatasets.In
CVPR
,2017.
[33]
J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei.ImageNet:ALarge-Scale
HierarchicalImageDatabase.In
CVPR
,2009.
[34]
S.IoffeandC.Szegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducing
internalcovariateshift.
arXiv.org
,2015.
[35]
S.HochreiterandJ.Schmidhuber.Longshort-termmemory.
Neuralcomputation
,9(8):1735Œ
1780,1997.
[36]
K.Cho,B.vanMerrienboer,C¸.G
¨
ulc¸ehre,F.Bougares,H.Schwenk,andY.Bengio.Learn-
ingphraserepresentationsusingRNNencoder-decoderforstatisticalmachinetranslation.
arXiv.org
,2014.
[37]
D.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization.
arXiv.org
,2014.
[38]
M.M
¨
uller,V.Casser,J.Lahoud,N.Smith,andB.Ghanem.Sim4cv:Aphoto-realisticsimu-
latorforcomputervisionapplications.
InternationalJournalofComputerVision
,2018.
[39]
J.Kosecka,R.Blasi,C.J.Taylor,andJ.Malik.Acomparativestudyofvision-basedlateral
controlstrategiesforautonomoushighwaydriving.In
ICRA
,1998.
[43]
K.H.Ang,G.Chong,andY.Li.Pidcontrolsystemanalysis,design,andtechnology.
IEEE
transactionsoncontrolsystemstechnology
,13(4):559Œ576,2005.
[41]
R.R.Selvaraju,M.Cogswell,A.Das,R.Vedantam,D.Parikh,andD.Batra.Grad-cam:
Visualexplanationsfromdeepnetworksviagradient-basedlocalization.
arXiv.org
,2016.
15
[42]
M.M
¨
uller,A.Dosovitskiy,B.Ghanem,andV.Koltun.Drivingpolicytransferviamodularity
andabstraction.
arXiv.org
,2018.
[43]
KiamHeongAng,GregoryChong,andYunLi.Pidcontrolsystemanalysis,design,and
technology.
IEEEtransactionsoncontrolsystemstechnology
,13(4):559Œ576,2005.
[44]
DHrovatandMHubbard.Optimumvehiclesuspensionsminimizingrmsrattlespace,
sprung-massaccelerationandjerk.
JournalofDynamicSystems,Measurement,andControl
,
103(3):228Œ236,1981.
[45]
QuananHuangandHuiyiWang.Fundamentalstudyofjerk:evaluationofshiftqualityand
ridecomfort.Technicalreport,SAETechnicalPaper,2004.
[46]
JohnGZieglerandNathanielBNichols.Optimumsettingsforautomaticcontrollers.
trans.
ASME
,64(11),1942.
16
"
20,Women also Snowboard: Overcoming Bias in Captioning Models,http://arxiv.org/pdf/1803.09797v4.pdf,https://github.com/dtak/local-independence-public,"WomenalsoSnowboard:
OvercomingBiasinCaptioningModels
KayleeBurns*
1
,LisaAnneHendricks*
1
,KateSaenko
2
,
TrevorDarrell
1
,AnnaRohrbach
1
1
UCBerkeley
2
BostonUniversity
Abstract.
Mostmachinelearningmethodsareknowntocaptureandexploitbi-
asesofthetrainingdata.Whilesomebiasesareforlearning,othersare
harmful.,imagecaptioningmodelstendtoexaggeratebiasespresent
intrainingdata(e.g.,ifawordispresentin60%oftrainingsentences,itmight
bepredictedin70%ofsentencesattesttime).Thiscanleadtoincorrectcaptions
indomainswhereunbiasedcaptionsaredesired,orrequired,duetoover-reliance
onthelearnedpriorandimagecontext.Inthisworkweinvestigategenerationof
gendercaptionwords(e.g.man,woman)basedontheperson'sappear-
anceortheimagecontext.Weintroduceanew
Equalizer
modelthatencourages
equalgenderprobabilitywhengenderevidenceisoccludedinasceneand
dentpredictionswhengenderevidenceispresent.Theresultingmodelisforced
tolookatapersonratherthanusecontextualcuestomakeagenderpre-
diction.Thelossesthatcompriseourmodel,the
AppearanceConfusionLoss
and
the
Loss
,aregeneral,andcanbeaddedtoanydescriptionmodelin
ordertomitigateimpactsofunwantedbiasinadescriptiondataset.Ourproposed
modelhaslowererrorthanpriorworkwhendescribingimageswithpeopleand
mentioningtheirgenderandmorecloselymatchesthegroundtruthratioofsen-
tencesincludingwomentosentencesincludingmen.Finally,weshowthatour
modelmoreoftenlooksatpeoplewhenpredictingtheirgender.
1
Keywords:
Imagedescription,Captionbias,Rightfortherightreasons
1Introduction
Exploitingcontextualcuescanfrequentlyleadtobetterperformanceoncomputervi-
siontasks[1,2,3].Forexample,inthevisualdescriptiontask,predictingaﬁmouseﬂ
mightbeeasiergiventhatacomputerisalsointheimage.However,insomecases
makingdecisionsbasedoncontextcanleadtoincorrect,andperhapsevenoffensive,
predictions.Inthiswork,weconsideronesuchscenario:generatingcaptionsabout
menandwomen.Wepositthatwhendescriptionmodelspredictgenderedwordssuch
asﬁmanﬂorﬁwomanﬂ,theyshouldconsidervisualevidenceassociatedwiththede-
scribedperson,andnotcontextualcueslikelocation(e.g.,ﬁkitchenﬂ)orotherobjects
inascene(e.g.,ﬁsnowboardﬂ).Notonlyisitimportantfordescriptionsystemstoavoid
egregiouserrors(e.g.,alwayspredictingthewordﬁmanﬂinsnowboardingscenes),but
1
https://people.eecs.berkeley.edu/
Ÿ
lisa_anne/snowboard.html
*Authorscontributedequally,listedalphabetically.
arXiv:1803.09797v4  [cs.CV]  13 Mar 20192Burns*,Hendricks*,Saenko,Darrell,Rohrbach
Fig.1:Exampleswhereourproposedmodel(Equalizer)correctsbiasinimagecap-
tions.Theoverlaidheatmapindicateswhichimageregionsaremostimportantforpre-
dictingthegenderword.Ontheleft,thebaselinepredictsgenderincorrectly,presum-
ablybecauseitlooksatthelaptop(nottheperson).Ontheright,thebaselinepredicts
thegendercorrectlybutitdoesnotlookatthepersonwhenpredictinggenderandis
thusnotacceptable.Incontrast,ourmodelpredictsthecorrectgenderwordandcor-
rectlyconsidersthepersonwhenpredictinggender.
itisalsoimportantforpredictionstoberightfortherightreason.Forexample,Figure1
(left)showsacasewherepriorworkpredictstheincorrectgender,whileourmodelac-
curatelypredictsthegenderbyconsideringthecorrectgenderevidence.Figure1(right)
showsanexamplewherebothmodelspredictthecorrectgender,butpriorworkdoes
notlookatthepersonwhendescribingtheimage(itisrightforthewrongreasons).
Biasinimagecaptioningisparticularlychallengingtoovercomebecauseofthe
multimodalnatureofthetask;predictedwordsarenotonlybyanimage,
butalsobiasedbythelearnedlanguagemodel.Though[4]studiedbiasforstructured
predictiontasks(e.g.,semanticrolelabeling),theydidnotconsiderthetaskofimage
captioning.Furthermore,thesolutionproposedin[4]requiresaccesstotheentiretest
setinordertorebalancegenderpredictionstothedistributioninthetrainingset.
Consequently,[4]reliesontheassumptionthatthedistributionofgendersisthesame
attrainingandtesttime.Wemakenosuchassumptions;weconsideramorerealistic
scenarioinwhichcaptionsaregeneratedforimagesindependentofothertestimages.
Inordertoencouragedescriptionmodelstogeneratelessbiasedcaptions,wein-
troducethe
Equalizer
Model.Ourmodelincludestwocomplementarylossterms:the
AppearanceConfusionLoss(ACL)
andthe
Loss(Conf)
.TheAppearance
ConfusionLossisbasedontheintuitionthat,givenanimageinwhichevidenceof
genderisabsent,descriptionmodelsshouldbeunabletoaccuratelypredictagendered
word.However,itisnotenoughtoconfusethemodelwhengenderevidenceisabsent;
wemustalsoencouragethemodeltoconsidergenderevidencewhenitispresent.Our
Losshelpstoincreasethemodel'swhengenderisintheimage.
ThesecomplementarylossesallowtheEqualizermodeltobecautiousintheabsence
ofgenderinformationanddiscriminativeinitspresence.
WomenalsoSnowboard:OvercomingBiasinCaptioningModels3
OurproposedEqualizermodelleadstolessbiasedcaptions:notonlydoesitlead
tolowererrorwhenpredictinggenderedwords,butitalsoperformswellwhenthe
distributionofgendersinthetestsetisnotalignedwiththetrainingset.Additionally,
weobservethatEqualizergeneratesgenderneutralwords(likeﬁpersonﬂ)whenitisnot
ofthegender.Furthermore,wedemonstratethatEqualizerfocusesonhumans
whenpredictinggenderwords,asopposedtofocusingonotherimagecontext.
2RelatedWork
UnwantedDatasetBias.
Unwanteddatasetbiases(e.g.,gender,ethnicbiases)have
beenstudiedacrossawidevarietyofAIdomains[5,6,7,8,9,10].Onecommonthemeis
thenotionof
bias
,inwhichbiasisnotonlylearned,but[4,7,6].
Forexample,intheimagecaptioningscenario,if70%ofimageswithumbrellasinclude
awomanand30%includeaman,attesttimethemodelmightamplifythisbiasto85%
and15%.Eliminatingbiasisnotassimpleasbalancingacrossattributes
foracategory.[6]studybiasinclandthateventhoughwhite
andblackpeopleappearinﬁbasketballﬂimageswithsimilarfrequency,modelslearn
toclassifyimagesasﬁbasketballﬂbasedonthepresenceofablackperson.Oneexpla-
nationisthatthoughthedataisbalancedinregardtotheclassﬁbasketballﬂ,thereare
manymorewhitepeopleinthedataset.Consequently,toperfectlybalanceadataset,
onewouldhavetobalanceacrossallpossibleco-occurrenceswhichisinfeasible.
Naturallanguagedataissubjectto
reportingbias
[7,11,12,13]inwhichpeopleover-
reportlesscommonco-occurrences,suchasﬁmalenurseﬂ[7]orﬁgreenbananaﬂ[12].
[13]alsodiscusshowvisualdescriptionsculturalbiases(e.g.,assumingawoman
withachildisamother,eventhoughthiscannotbeinanimage).Weobserve
thatannotatorsspecifygenderevenwhengendercannotbeinanimage(e.g.,
asnowboardermightbelabeledasﬁmanﬂevenifgenderevidenceisoccluded).
Ourworkismostsimilarto[4]whoconsiderbiasinsemanticrolelabelingand
multilabel(asopposedtoimagecaptioning).Toavoidbiason,
[4]rebalancethetesttimepredictionstomoreaccuratelythetrainingtimeword
ratios.Thissolutionisunsatisfactorybecause(i)itrequiresaccesstotheentiretestset
and(ii)itassumesthatthedistributionofobjectsattesttimeisthesameasattraining
time.Weconsideramorerealisticscenarioinourexperiments,andshowthattheratio
ofwomantomaninourpredictedsentencescloselyresemblestheratioingroundtruth
sentences,evenwhenthetestdistributionisdifferentfromthetrainingdistribution.
Fairness.
BuildingAIsystemswhichtreat
protectedattributes
(e.g.,age,gender,sexual
orientation)inafairmannerisincreasinglyimportant[14,15,16,17].Inthemachine
learningliterature,ﬁfairnessﬂgenerallyrequiresthatsystemsdonotuseinformation
suchasgenderorageinawaythatdisadvantagesonegroupoveranother.Weconsider
isdifferentscenarioaswearetryingto
predict
protectedattributes.
Distributionmatching
hasbeenusedtobuildfairsystems[17]byencouragingthe
distributionofdecisionstobesimilaracrossdifferentprotectedclasses,aswellasfor
otherapplicationssuchasdomainadaption[18,19]andtransductionlearning[20].Our
AppearanceConfusionLossissimilarasitencouragesthedistributionofpredictions
tobesimilarformanandwomanclasseswhengenderinformationisnotavailable.
4Burns*,Hendricks*,Saenko,Darrell,Rohrbach
RightfortheRightReasons.
Assuringmodelsareﬁrightfortherightreasons,ﬂorcon-
sidersimilarevidenceashumanswhenmakingdecisions,helpsresearchersunderstand
howmodelswillperforminrealworldapplications(e.g.,whenpredictingoutcomesfor
pneumoniapatientsin[21])ordiscoverunderlyingdatasetbias[22].Wehypothesize
thatmodelswhichlookatappropriategenderevidencewillperformbetterinnewsce-
narios,whenthegenderdistributionattestandtrainingtimearedifferent.
Recently,[23]developalossfunctionwhichcomparesexplanationsforadecisionto
groundtruthexplanations.However,[23]generatingexplanationsforvisualdecisions
isadifandactiveareaofresearch[24,25,26,27,28,29].Insteadofrelyingon
ourmodeltoaccuratelyexplainitselfduringtraining,weverifythatourformulation
encouragesmodelstoberightfortherightreasonattesttime.
VisualDescription.
Mostvisualdescriptionwork(e.g.,[30,31,32,33,34])focuseson
improvingoverallsentencequality,withoutregardtocapturedbiases.Thoughwepay
specialattentiontogenderinthiswork,allcaptioningmodelstrainedonvisualdescrip-
tiondata(MSCOCO[35],Flickr30k[36],MSR-VTT[37]tonameafew)implicitly
learntoclassifygender.Howevercurrentcaptioningmodelsdonotdiscussgenderthe
wayhumansdo,but
amplify
genderbias;ourintentistogeneratedescriptionswhich
moreaccuratelyhumandescriptionswhendiscussingthisimportantcategory.
Gender
Gendermodelsfrequentlyfocusonfacialfea-
tures[38,39,40].Incontrast,wearemainlyconcernedaboutwhethercontextualclues
incomplexscenesbiastheproductionofgenderedwordsduringsentencegeneration.
Genderationhasalsobeenstudiedinnaturallanguageprocessing([41,42],
[43]).
EthicalConsiderations.
Frequently,genderisseenasabinarytask:data
pointsarelabeledaseitherﬁmanﬂorﬁwomanﬂ.However,AIpractitioners,bothinin-
dustrial
2
andacademic
3
settings,areincreasinglyconcernedthatgender
systemsshouldbeinclusive.Ourcaptioningmodelpredictsthreegendercategories:
male,female,andgenderneutral(e.g.,person)basedonvisualappearance.Whende-
signinggendersystems,itisimportanttounderstandwherelabelsare
sourcedfrom[44].Wedeterminegenderlabelsusingapreviouslycollectedpublicly
releaseddatasetinwhichannotatorsdescribeimages[35].Importantly,peopleinthe
imagesarenotaskedtoidentifytheirgender.Thus,weemphasizethatwearenotclas-
sifyingbiologicalsexorgenderidentity,butratheroutwardgenderappearance.
3Equalizer:OvercomingBiasinDescriptionModels
Equalizerisbasedonthefollowingintuitions:ifevidencetosupportagender
decisionisnotpresentinanimage,themodelshouldbe
confused
aboutwhichgenderto
predict(enforcedbyanAppearanceConfusionLossterm),andifevidencetosupporta
genderdecisionisinanimage,themodelshouldbe

initsprediction(enforced
byaLossterm).Totrainourmodelwerequirenotonlypairsofimages,
I
,
andsentences,
S
,butalsoannotationmasks
M
whichindicatewhichevidenceinan
2
https://clarifai.com/blog/socially-responsible-pixels-a-look-inside-clarifais-new-demographics-recognition-model
3
https://www.media.mit.edu/projects/gender-shades/faq
WomenalsoSnowboard:OvercomingBiasinCaptioningModels5
Fig.2:Equalizerincludestwonovellossterms:theLossonimageswith
menorwomen(top)andtheAppearanceConfusionLossonimageswheremenand
womenareoccluded(bottom).Togethertheselossesencourageourmodeltomake
correctpredictionswhenevidenceofgenderispresent,andbecautiousinitsabsence.
WealsoincludetheCaptionCorrectnessLoss(crossentropyloss)forbothimagetypes.
imageisappropriatefordetermininggender.Thoughweuse[30]asourbasenetwork,
Equalizerisgeneralandcanbeintegratedintoanydeepdescriptionframeworks.
3.1Background:DescriptionFramework
Togenerateadescription,highlevelimagefeaturesareextractedfromtheIncep-
tionV3[45]model.TheimagefeaturesarethenusedtoinitializeanLSTMhiddenstate.
Tobeginsentencegeneration,astartofsentencetokenisinputintotheLSTM.Foreach
subsequenttimestepduringtraining,thegroundtruthword
w
t
isinputintotheLSTM.
Attesttime,thepreviouslypredictedword
w
t

1
isinputintotheLSTMateachtime
step.Generationconcludeswhenanendofsequencetokenisgenerated.Like[30],we
includethestandardcrossentropyloss(
L
CE
)duringtraining:
L
CE
=

1
N
N
X
n
=0
T
X
t
=0
log
(
p
(
w
t
j
w
0:
t

1
;I
))
;
(1)
where
N
isthebatchsize,
T
isthenumberofwordsinthesentence,
w
t
isaground
truthwordattime
t
,and
I
isanimage.
3.2AppearanceConfusionLoss
OurAppearanceConfusionLossencouragestheunderlyingdescriptionmodeltobe
confused
whenmakinggenderdecisionsiftheinputimagedoesnotcontainappropri-
ateevidenceforthedecision.TooptimizetheAppearanceConfusionLoss,werequire
groundtruthrationalesindicatingwhichevidenceisappropriateforaparticulargender
6Burns*,Hendricks*,Saenko,Darrell,Rohrbach
decision.Weexpecttheresultingrationalestobemasks,
M
,whichare
1
forpixels
whichshouldnotcontributetoagenderdecisionand
0
forpixelswhichareappropri-
atetoconsiderwhendetermininggender.TheHadamardproductofthemaskandthe
originalimage,
I

M
,yieldsanewimage,
I
0
,withgenderinformationthattheimple-
menterdeemsappropriateforremoved.Intuitively,foranimagedevoidof
genderinformation,theprobabilityofpredictingmanorwomanshouldbeequal.The
AppearanceConfusionLossenforcesafairpriorbyassertingthatthisisthecase.
ToourAppearanceConfusionLoss,wea
confusion
function(
C
)
whichoperatesoverthepredicteddistributionofwords
p
(~
w
t
)
,asetofwomangender
words(
G
w
),andasetofmangenderwords(
G
m
):
C
(~
w
t
;I
0
)=
j
X
g
w
2G
w
p
(~
w
t
=
g
w
j
w
0:
t

1
;I
0
)

X
g
m
2G
m
p
(~
w
t
=
g
m
j
w
0:
t

1
;I
0
)
j
:
(2)
Inpractice,the
G
w
consistsonlyofthewordﬁwomanﬂand,likewise,the
G
m
con-
sistsonlyofthewordﬁmanﬂ.Thesearebyfarthemostcommonlyusedgenderwords
inthedatasetsweconsiderandwethatusingtheseﬁsetsﬂresultsinsimilarperfor-
manceasusingmorecompletesets.
WecannowourAppearanceConfusionLoss(
L
AC
)as:
L
AC
=
1
N
N
X
n
=0
T
X
t
=0
1
(
w
t
2G
w
[G
m
)
C
(~
w
t
;I
0
)
;
(3)
where
1
isanindicatorvariablethatdenoteswhetherornot
w
t
isagenderedword.
Fortheremainingnon-genderedwordsthatcorrespondtoimages
I
0
,weapplythe
standardcrossentropylosstoencouragethemodeltodiscussobjectswhicharestill
visiblein
I
0
.Inadditiontoencouragingsentencestobeimagerelevantevenwhenthe
genderinformationhasbeenremoved,thisalsoencouragesthemodeltolearnrepre-
sentationsofwordslikeﬁdogﬂandﬁfrisbeeﬂthatarenotreliantongenderinformation.
3.3Loss
Inadditiontobeingunsurewhengenderevidenceisoccluded,wealsoencourageour
modeltobewhengenderevidenceispresent.Thus,weintroducethe
dentLossterm,whichencouragesthemodeltopredictgenderwordscorrectly.
OurLossencouragestheprobabilitiesforpredictedgenderwordstobe
highonimages
I
inwhichgenderinformationispresent.Givenfunctions
F
W
and
F
M
whichmeasurehowthemodelpredictswomanandmanwordsrespectively,
wecanwritetheLossas:
L
Con
=
1
N
N
X
n
=0
T
X
t
=0
(
1
(
w
t
2G
w
)
F
W
(~
w
t
;I
)+
1
(
w
t
2G
m
)
F
M
(~
w
t
;I
))
:
(4)
Tomeasuretheofpredictedgenderwords,weconsiderthequotient
betweenpredictedprobabilitiesformanandgenderwords(
F
M
isofthesameform):
F
W
(~
w
t
;I
)=
P
g
m
2G
m
p
(~
w
t
=
g
m
j
w
0:
t

1
;I
)
(
P
g
w
2G
w
p
(~
w
t
=
g
w
j
w
0:
t

1
;I
))+

(5)
WomenalsoSnowboard:OvercomingBiasinCaptioningModels7
where

isasmallepsilonvalueaddedfornumericalstability.
Whenthemodelisofagenderprediction(e.g.,forthewordﬁwomanﬂ),
theprobabilityofthewordﬁwomanﬂshouldbeconsiderablyhigherthantheprobability
ofthewordﬁmanﬂ,whichwillresultinasmallvaluefor
F
W
andthusasmallloss.One
nicepropertyofconsideringthequotientbetweenpredictedprobabilitiesisthatwe
encouragethemodeltodistinguishbetweengenderedwordswithoutforcingthemodel
topredictagenderedword.Forexample,ifthemodelpredictsaprobabilityof
0
:
2
for
ﬁmanﬂ,
0
:
5
forﬁwomanﬂ,and
0
:
3
forﬁpersonﬂonaﬁwomanﬂimage,our
losswillbelow.However,themodelisstillabletopredictgenderneutralwords,like
ﬁpersonﬂwithrelativelyhighprobability.Thisisdistinctfromotherpossiblelosses,
likeplacingalargerweightongenderwordsinthecrossentropyloss,whichforcesthe
modeltopredictﬁmanﬂ/ﬁwomanﬂwordsandpenalizesthegenderneutralwords.
3.4TheEqualizerModel
Ourmodelisalinearcombinationofallaforementionedlosses:
L
=

L
CE
+

L
AC
+

L
Con
;
(6)
where

,

,and

arehyperparameterschosenonavalidationset(
;
=1
,

=10
in
ourexperiments).
OurEqualizermethodisgeneralandourbasecaptioningframeworkcanbesubsti-
tutedwithanyotherdeepcaptioningframework.Bycombiningalloftheseterms,the
Equalizermodelcannotonlygenerateimagerelevantsentences,butalsomakecon-
genderpredictionsundersufevidence.WethatboththeAppearance
ConfusionLossandtheLossareimportantincreatingayetcau-
tiousmodel.Interestingly,theEqualizermodelachievestheloweston
rateonlywhenthesetwolossesarecombined,highlightingthecomplementarynature
ofthesetwolossterms.
4Experiments
4.1Datasets
MSCOCO-Bias.
Toevaluateourmethod,weconsiderthedatasetusedby[4]foreval-
uatingbiasinstructuredpredictionproblems.Thisdatasetconsistsofim-
agesfromMSCOCO[35]whicharelabeledasﬁmanﬂorﬁwomanﬂ.Thoughﬁpersonﬂis
anMSCOCOclass,ﬁmanﬂandﬁwomanﬂarenot,so[4]employgroundtruthcaptions
todetermineifimagescontainamanorawoman.Imagesarelabeledasﬁmanﬂifat
leastonedescriptionincludesthewordﬁmanﬂandnodescriptionsincludetheword
ﬁwomanﬂ.Likewise,imagesarelabeledasﬁwomanﬂifatleastonedescriptionincludes
thewordﬁwomanﬂandnodescriptionsincludethewordﬁmanﬂ.Imagesarediscarded
ifbothﬁmanﬂandﬁwomanﬂarementioned.WerefertothisdatasetasMSCOCO-Bias.
MSCOCO-Balanced.
Wealsoevaluateonasetwherewepurposelychangethegender
ratio.Webelievethisisrepresentativeofrealworldscenariosinwhichdifferentdistri-
butionsofmenandwomenmightbepresentattesttime.TheMSCOCO-Biassethasa
8Burns*,Hendricks*,Saenko,Darrell,Rohrbach
roughly1:3womantomanratiowhereasthisset,calledMSCOCO-Balanced,hasa1:1
womantomanratio.Werandomlyselect500imagesfromMSCOCO-Biassetwhich
includethewordﬁwomanﬂand500whichincludeﬁmanﬂ.
PersonMasks.
TotrainEqualizer,weneedgroundtruthhumanrationalesforwhya
personshouldbepredictedasamanorawoman.Weusethepersonsegmentationmasks
fromtheMSCOCOdataset.Oncethemaskedimageiscreated,wethesegmentation
maskwiththeaveragepixelvalueintheimage.Weusethemasksbothattrainingtime
tocomputeAppearanceConfusionLossandduringevaluationtoensurethatmodels
arepredictinggenderwordsbylookingattheperson.WhileforMSCOCOtheperson
annotationsarereadilyavailable,forotherdatasetse.g.apersondetectorcouldbeused.
4.2Metrics
Toevaluateourmethods,werelyonthefollowingmetrics.
Error.
Duetothesensitivenatureofpredictionforprotectedclasses(genderwords
inourscenario),weemphasizetheimportanceofalowerror.Theerrorrateisthe
numberofman/womanwhilegenderneutraltermsarenotconsid-
erederrors.Weexpectthatthebestmodelwouldratherpredictgenderneutralwordsin
caseswheregenderisnotobvious.
GenderRatio.
Second,weconsidertheratioofsentenceswhichbelongtoaﬁwomanﬂ
settosentenceswhichbelongtoaﬁmanﬂset.Weconsiderasentencetofallina
ﬁwomanﬂsetifitpredictsanywordfromaprecompiledlistoffemalegenderedwords,
andrespectivelyfallinaﬁmanﬂsetifitpredictsanywordfromaprecompiledlistof
malegenderedwords.
RightfortheRightReasons.
Finally,tomeasureifamodelisﬁrightfortheright
reasonsﬂweconsiderthepointinggame[46]evaluation.Wecreatevisualexpla-
nationsforﬁwomanﬂ/ﬁmanﬂusingtheGrad-CAMapproach[25]aswellassaliency
mapscreatedbyoccludingimageregionsinaslidingwindowfashion.Tomeasureif
ourmodelsarerightfortherightreason,weverifywhetherthepointwiththehighest
activationintheexplanationheatmapfallsinthepersonsegmentationmask.
4.3TrainingDetails
AllmodelsareinitializedfromtheShowandTellmodel[30]pre-trainedonallof
MSCOCOfor1millioniterations(withoutthroughthevisualrepresenta-
tion).Modelsaretrainedforadditional500,000iterationsontheMSCOCO-Biasset,
throughthevisualrepresentation(Inceptionv3[45])for500,000iterations.
4.4BaselinesandAblations
Baseline-FT.
ThesimplestbaselineistheShowandTellmodelthroughthe
LSTMandconvolutionalnetworksusingthestandardcross-entropylossonourtarget
dataset,theMSCOCO-Biasdataset.
Balanced.
WetrainaBalancedbaselineinwhichwere-balancethedatadistribution
attrainingtimetoaccountforthelargernumberofmeninstancesinthetrainingdata.
WomenalsoSnowboard:OvercomingBiasinCaptioningModels9
MSCOCO-Bias
MSCOCO-Balanced
Model
ErrorRatio

ErrorRatio

Baseline-FT
12.830.15
19.300.51
Balanced
12.850.14
18.300.47
UpWeight
13.560.08
16.300.35
Equalizerw/oACL
7.570.04
10.100.26
Equalizerw/oConf
9.620.09
13.900.40
Equalizer
7.02-0.03
8.100.13
Table1:Evaluationofpredictedgenderwordsbasedonerrorrateandratioofgenerated
sentenceswhichincludetheﬁwomanﬂwordstosentenceswhichincludetheﬁmanﬂ
words.Equalizerachievesthelowesterrorrateandpredictssentenceswithagender
ratiomostsimilartothecorrespondinggroundtruthcaptions(Ratio

),evenwhenthe
testsethasadifferentdistributionofgenderwordsthanthetrainingset,asisthecase
fortheMSCOCO-Balanceddataset.
Eventhoughwecannotknowthecorrectdistributionofourdataattesttime,wecan
enforceourbeliefthatpredictingawomanormanshouldbeequallylikely.Attraining
time,were-sampletheimagesofwomensothatthenumberoftrainingexamplesof
womenisthesameasthenumberoftrainingexamplesofmen.
UpWeight.
Wealsoexperimentwithupweightingthelossvalueforgenderwordsinthe
standardcrossentropylosstoincreasethepenaltyforaForeachtime
stepwherethegroundtruthcaptionsaysthewordﬁmanﬂorﬁwomanﬂ,wemultiplythat
terminthelossbyaconstantvalue(10inreportedexperiments).Intuitively,upweight-
ingshouldencouragethemodelstoaccuratelypredictgenderwords.However,unlike
ourLoss,upweightingdrivesthemodeltomakeeitherﬁmanﬂorﬁwomanﬂ
predictionswithouttheopportunitytoplaceahighprobabilityongenderneutralwords.
Ablations.
ToisolatetheimpactofthetwolosstermsinEqualizer,wereportresults
withonlytheAppearanceConfusionLoss(Equalizerw/oConf)andonlythe
denceLoss(Equalizerw/oACL).WethenreportresultsofourfullEqualizermodel.
4.5Results
Error.
Table1reportstheerrorrateswhendescribingmenandwomenontheMSCOCO-
BiasandMSCOCO-Balancedtestsets.Comparingtobaselines,Equalizershowscon-
sistentimprovements.Importantly,ourfullmodelconsistentlyimprovesuponEqualizer
w/oACLandEqualizerw/oConf.WhencomparingEqualizertobaselines,weseea
largerperformancegainontheMSCOCO-Balanceddataset.Asdiscussedlater,thisis
inpartbecauseourmodeldoesaparticularlygoodjobofdecreasingerroronthemi-
norityclass(woman).Unlikebaselinemodels,ourmodelhasasimilarerrorrateon
eachset.Thisindicatesthattheerrorrateofourmodelisnotassensitivetoshiftsinthe
genderdistributionattesttime.
Interestingly,theresultsoftheBaseline-FTmodelandBalancedmodelarenotsub-
stantiallydifferent.Onepossibilityisthattheco-occurrencesacrosswordsarenotbal-
anced(e.g.,ifthereisgenderimbalanceforimageswithﬁumbrellaﬂjustbal-
10Burns*,Hendricks*,Saenko,Darrell,Rohrbach
Women
Men
OutcomeDivergence
Model
CorrectIncorrectOther
CorrectIncorrectOther
betweenGenders
Baseline-FT
46.2834.1119.61
75.054.2320.72
0.121
Balanced
47.6733.8018.54
75.894.3819.72
0.116
UpWeight
60.59
29.829.58
87.84
6.985.17
0.078
Equalizerw/oACL
56.1816.0227.81
67.58
4.15
28.26
0.031
Equalizerw/oConf
46.0324.8429.13
61.113.4735.42
0.075
Equalizer(Ours)
57.38
12.99
29.63
59.024.6136.37
0.018
Table2:AccuracyperclassforMSCOCO-Biasdataset.ThoughUpWeightachievesthe
highestrecallforbothmenandwomenimages,italsohasahigherror,especiallyfor
women.Onecriterionofaﬁfairﬂsystemisthatithassimilaroutcomesacrossclasses.
WemeasureoutcomesimilaritybycomputingtheJensen-Shannondivergencebetween
Correct/Incorrect/Othersentencesformenandwomenimages(lowerisbetter)andob-
servethatEqualizerperformsbestonthismetric.
ancingthedatasetbasedongenderwordcountsisnotsuftobalancethedataset).
Weemphasizethatbalancingacrossallco-occurringwordsisdifinlarge-scale
settingswithlargevocabularies.
GenderRatio
Wealsoconsidertheratioofcaptionswhichincludeonlyfemalewords
tocaptionswhichincludeonlymalewords.InTable1wereportthe
difference
between
thegroundtruthratioandtheratioproducedbyeachcaptioningmodel.Impressively,
Equalizerachievestheclosestratiotogroundtruthonbothdatasets.Again,theACLand
lossesarecomplementaryandEqualizerhasthebestoverallperformance.
PerformanceforEachGender.
Imageswithfemalescompriseamuchsmallerportion
ofMSCOCOthanimageswithmales.Thereforetheoverallperformanceacrossclasses
(i.e.man,woman)canbemisleadingbecauseitdownplaystheerrorsintheminority
class.Additionally,unlike[4]whoconsiderascenarioinwhichthemodel
isforcedtopredictagender,ourdescriptionmodelscanalsodiscussgenderneutral
termssuchasﬁpersonﬂorﬁplayerﬂ.InTable2foreachgender,wereportthepercentage
ofsentencesinwhichgenderispredictedcorrectlyorincorrectlyandwhennogender
wordisgeneratedontheMSCOCO-Biasset.
Acrossallmodels,theerrorforMenisquitelow.However,ourmodelcantly
improvestheerrorfortheminorityclass,Women.Interestingly,weobservethatEqual-
izerhasasimilarrecall(Correct),error(Incorrect),andOtherrateacrossbothgen-
ders.Acaptionmodelcouldbeconsideredmoreﬁfairﬂif,foreachgender,thepossible
outcomes(correctgendermentioned,incorrectgendermentioned,genderneutral)are
similar.Thisresemblesthenotionofequalizedoddsinfairnessliterature[14],which
requiresasystemtohavesimilarfalsepositiveandfalsenegativeratesacrossgroups.
Toformalizethisnotionoffairnessinourcaptioningsystems,wereporttheoutcome
typedivergencebetweengendersbymeasuringtheJensen-Shannon[47]divergencebe-
tweenCorrect/Incorrect/OtheroutcomesforMenandWomen.Lowerdivergenceindi-
catesthatWomenandMenclassesresultinasimilardistributionofoutcomes,andthus
themodelcanbeconsideredmoreﬁfairﬂ.Equalizerhasthelowestdivergence(
0
:
018
).
WomenalsoSnowboard:OvercomingBiasinCaptioningModels11
Fig.3:Accuracyacrossman,woman,andgenderneutraltermsfordifferentmodelsas
afunctionofannotatorWhenonlyoneannotatordescribesanimagewith
agenderedword,Equalizerhasalowaccuracyasitmorelikelypredictsgenderneu-
tralwordsbutwhenmoreannotationsmentiongenderedwords,Equalizerhashigher
accuracythanothermodels.
Annotator
Asdescribedabove,genderlabelsareminedfromcaptions
providedintheMSCOCOdataset.Eachimagecorrespondstovecaptions,butnotall
captionsforasingleimageincludeagenderedword.Countingthenumberofsentences
whichincludeagenderedwordprovidesaroughestimateofhowapparentgenderisin
animageandhowimportantitistomentionwhendescribingthescene.
Tounderstandhowwellourmodelcapturesthewayannotatorsdescribepeople,
insteadoflabelingimagesaseitherﬁmanﬂorﬁwomanﬂ,welabelimagesasﬁmanﬂ,
ﬁwomanﬂ,orﬁgenderneutralﬂbasedonhowmanyannotatorsmentionedgenderin
theirdescription.Forathresholdvalue
T
,weconsideranimagetobelong
totheﬁmanﬂorﬁwomanﬂclassif
T
ormoreannotatorsmentionthegenderintheir
description,andﬁgenderneutralﬂotherwise.Wecanthenmeasureaccuracyoverthese
threeclasses.Whereasanaivesolutionwhichrestrictsvocabularytoincludenogender
wordswouldhavelowerrorasinTable1,itwouldnotcapturethewayhumans
usegenderwordswhendescribingimages.Indeed,theMSCOCOtrainingsetincludes
over200,000instancesofwordswhichdescribepeople.Overhalfofallwordsused
todescribepeoplearegendered.Byconsideringaccuracyacrossthreeclasses,wecan
bettermeasurehowwellmodelscapturethewayhumansdescribegender.
Figure3plotstheaccuracyofeachmodelwithrespecttothethreshold
T
.Atlowthresholdvalues,Equalizerperformsworseasittendstomorefrequently
outputgenderneutralterms,andtheUpWeightmodel,whichalmostalwayspredicts
genderedwords,performsbest.However,asthethresholdvalueincreases,Equalizer
performsbetterthanothermodels,includingatathresholdvalueof
3
whichcorresponds
toclassifyingimagesbasedoffthemajorityvote.ThisindicatesthatEqualizernaturally
captureswhenhumansdescribeimageswithgenderedorgenderneutralwords.
ObjectGenderCo-Occurrence.
Weanalyzehowgenderpredictionpre-
dictionofotherwordsontheMSCOCO-Biastestset.,weconsiderthe
80
MSCOCOcategories,excludingthecategoryﬁpersonﬂ.Weadoptthebias
12Burns*,Hendricks*,Saenko,Darrell,Rohrbach
Accuracy
Woman
Man
All
Random
22.6
19.5
21.0
Baseline-FT
39.8
34.3
37.0
Balanced
37.6
34.1
35.8
UpWeight
43.3
36.4
39.9
Equalizerw/oACL
48.1
39.6
43.8
Equalizerw/oConf
43.9
36.8
40.4
Equalizer(Ours)
49.9
45.2
47.5
Accuracy
Woman
Man
All
Random
25.1
17.5
21.3
Baseline-FT
45.3
40.4
42.8
Balanced
48.5
42.2
45.3
UpWeight
54.1
45.5
49.8
Equalizerw/oACL
54.7
47.5
51.1
Equalizerw/oConf
48.9
46.7
47.8
Equalizer(Ours)
56.3
51.1
53.7
(a)Visualexplanationisa
Grad-CAM
map.(b)Visualexplanationisa
saliency
map.
Table3:
Pointinggame
evaluationthatmeasureswhetherthevisualexplanationsfor
ﬁmanﬂ/ﬁwomanﬂwordsfallinthepersonsegmentationground-truth.Evaluationis
doneforground-truthcaptionsontheMSCOCO-Balanced.
tionmetricproposedin[4],andcomputethefollowingratios:
count
(
man
&
object
)
count
(
person
&
object
)
and
count
(
woman
&
object
)
count
(
person
&
object
)
,where
man
referstoallmalewords,
woman
referstoallfemale
words,and
person
referstoallmale,female,orgenderneutralwords.Ideally,these
ratiosshouldbesimilarforgeneratedcaptionsandgroundtruthcaptions.However,
e.g.for
man
and
motorcycle
,thegroundtruthratiois0.40andfortheBaseline-FT
andEqualizer,theratiois0.81and0.65,respectively.ThoughEqualizerover-predicts
thispair,theratioisclosertothegroundtruththanwhencomparingBaseline-FTto
thegroundtruth.Likewise,for
woman
and
umbrella
,thegroundtruthratiois0.40,
Baseline-FTratiois0.64,andEqualizerratiois0.56.Asamoreholisticmetric,weaver-
agethe
difference
ofratiosbetweengroundtruthandgeneratedcaptionsacrossobjects
(lowerisbetter).Formalewords,EqualizerissubstantiallybetterthantheBaseline-FT
(0.147vs.0.193)andsimilarforfemalewords(0.096vs.0.99).
CaptionQuality.
Qualitatively,thesentencesfromallofourmodelsarelinguistically
(indeed,comparingsentencesinFigure4wenotethatusuallyonlytheword
referringtothepersonchanges).However,wedonoticeasmalldropinperformanceon
standarddescriptionmetrics(25.2to24.3onMETEOR[48]whencomparingBaseline-
FTtoourfullEqualizer)onMSCOCO-Bias.Onepossibilityisthatourmodelisoverly
cautiousandispenalizedforproducinggenderneutraltermsforsentencesthathumans
describewithgenderedterms.
RightfortheRightReasons.
Wehypothesizethatmanyerrorsoccur
duetothemodellookingatthewrongvisualevidence,e.g.conditioninggenderpredic-
tiononcontextratherthanontheperson'sappearance.Wequantitativelythis
hypothesisandshowthatourproposedmodelimprovesthisbehaviorbylookingatthe
appropriateevidence,i.e.isbeingﬁrightfortherightreasonsﬂ.Toevaluatethiswerely
ontwovisualexplanationtechniques:Grad-CAM[25]andsaliencymapsgeneratedby
occludingimageregionsinaslidingwindowfashion.
Unlike[25]whoapplyGrad-CAMtoanentirecaption,wevisualizetheevidence
forgeneratingwords,i.e.ﬁmanﬂandﬁwomanﬂ.y,weapplyGrad-
WomenalsoSnowboard:OvercomingBiasinCaptioningModels13
CAMtothelastconvolutionallayerofourimageprocessingnetwork,InceptionV3
[45],weobtain8x8weightmatrices.Toobtainsaliencymaps,weresizeaninputimage
to
299

299
anduniformlydivideitinto
32

32
pixelregions,obtaininga
10

10
grid(thebottom/rightmostcellsbeingsmaller).Next,foreverycellinthegrid,wezero
outtherespectivepixelsandfeedtheobtainedﬁpartiallyblockedoutﬂimagethrough
thecaptioningnetwork(similartoaswasdoneintheocclusionsensitivityexperiments
in[29]).Then,fortheground-truthcaption,wecomputetheﬁinformationlossﬂ,i.e.
thedecreaseinpredictingthewordsﬁmanﬂandﬁwomanﬂas

log
(
p
(
w
t
=
g
m
))
and

log
(
p
(
w
t
=
g
w
))
,respectively.Thisissimilartothetop-downsaliencyapproachof
[24],whozero-outalltheintermediatefeaturedescriptorsbutone.
Toevaluatewhetherthevisualexplanationforthepredictedwordisfocusedona
person,werelyonpersonmasks,obtainedfromMSCOCOground-truthpersonseg-
mentations.Weusethe
pointinggame
evaluation[46].Weupscalevisualexplanations
totheoriginalimagesize.Weaﬁhitﬂtobewhenthepointwiththehighestweight
iscontainedinthepersonmask.Theaccuracyiscomputedas
#
hits
#
hits
+#
misses
.
ResultsontheMSCOCO-BalancedsetarepresentedinTable3(a)and(b),forthe
Grad-CAMandsaliencymaps,respectively.Forafaircomparisonweprovideallmod-
elswithground-truthcaptions.Forcompletenesswealsoreporttherandombaseline,
wherethepointwiththehighestweightisselectedrandomly.WeseethatEqualizer
obtainsthebestaccuracy,improvingovertheBaseline-FTandallmodel
variants.Asimilarevaluationontheactualgeneratedcaptionsshowsthesametrends.
Lookingatobjects.
Usingourpointingtechnique,wecanalsoanalyzewhichMSCOCO
objectsmodelsareﬁlookingﬂatwhenthey
donot
pointatthepersonwhilepredicting
ﬁmanﬂ/ﬁwomanﬂ.,wecountahitifthehighestactivationisonanobjectin
question.Wecomputethefollowingratioforeachgender:numberofimageswherean
objectispointedattothetruenumberofimageswiththatobject.Wethatthereare
differencesacrossgenders,e.g.ﬁumbrellaﬂ,ﬁbenchﬂ,ﬁsuitcaseﬂaremoreoftenpointed
atwhendiscussingwomen,whilee.g.ﬁtruckﬂ,ﬁcouchﬂ,ﬁpizzaﬂwhendiscussingmen.
Ourmodelreducestheoverallﬁdeltaﬂbetweengendersforgroundtruthsentencesfrom
anaverage0.12to0.08,comparedtotheBaseline-FT.E.g.forﬁdiningtableﬂEqualizer
decreasesthedeltafrom0.07to0.03.
QualitativeResults.
Figure4comparesGrad-CAMvisualizationsforpredictedgen-
derwordsfromourmodeltotheBaseline-FT,UpWeight,andEqualizerw/oACL.We
consistentlyseethatourmodellooksatthepersonwhendescribinggenderedwords.In
Figure4(top),allothermodelslookatthedogratherthanthepersonandpredictthe
genderﬁmanﬂ(groundtruthlabelisﬁwomanﬂ).Inthisparticularexample,thegenderis
somewhatambiguous,andourmodelconservativelypredictsﬁpersonﬂratherthanmis-
classifythegender.InFigure4(middle),theBaseline-FTandUpWeightexampleboth
incorrectlypredictthewordﬁwomanﬂanddonotlookattheperson(womenoccurmore
frequentlywithumbrellas).Incontrast,boththeEqualizerw/oACLandtheEqualizer
lookatthepersonandpredictthecorrectgender.Finally,inFigure4(bottom),allmod-
elspredictthecorrectgender(man),butourmodelistheonlymodelwhichlooksatthe
personandisthusﬁrightfortherightreasons.ﬂ
Discussion.
WepresenttheEqualizermodelwhichincludesanAppearanceConfusion
Losstoencouragepredictionstobeconfusedwhenpredictinggenderifevidenceis
14Burns*,Hendricks*,Saenko,Darrell,Rohrbach
Fig.4:Qualitativecomparisonofmultiplebaselinesandourmodel.Inthetopexample,
beingconservative(ﬁpersonﬂ)isbetterthanbeingwrong(ﬁmanﬂ)asthegenderisnot
obvious.Inthebottomexamplethebaselinesarelookingatthewrongvisualevidence.
obscuredandtheLosswhichencouragespredictionstobewhen
genderevidenceispresent.OurAppearanceConfusionLoss,requireshumanratio-
nalesaboutwhatisvisualevidenceisappropriatetoconsiderwhenpredictinggender.
Westresstheimportanceofhumanjudgmentwhendesigningmodelswhichinclude
protectedclasses.Forexample,ourmodelcanuseinformationaboutclothingtype
(e.g.,dresses)topredictagenderwhichmaynotbeappropriateforallapplications.
Thoughweconcentrateongenderinthiswork,webelievethegeneralityofourframe-
workcouldbeappliedwhendescribingotherprotectedattributes,e.g.,race/ethnicity
andbelieveourresultssuggestEqualizercanbeavaluabletoolforovercomingbiasin
captioningmodels.
Acknowledgements.
ThisworkwaspartiallysupportedbyUSDoD,theDARPAXAI
program,andtheBerkeleyIntelligenceResearch(BAIR)Lab.
WomenalsoSnowboard:OvercomingBiasinCaptioningModels15
Supplemental
AContent
Thissupplementarymaterialprovidesadditionalquantitativeandqualitativeresultsto
ourmainpaper.Thedocumentisstructuredasfollows.
SectionBprovidesdetailedbreakdownforper-wordperformanceofthecompared
approaches,discussesmodels'behavioronthemaskedimages,andprovidesresultsfor
trainingwithasetofgenderedwords.
SectionCshowsmorequalitativeexamplesforthebaselinesandourmodel.
BAdditionalanalysis
Performancebreakdownforbiasedwords.
Weadditionallyanalyzeobjectswhichco-
occurwithonegendermorethantheother.Foracarefulanalysis,wechoosevewords
thatarebiasedtoco-occurwithwomen(umbrella,kitchen,cellphone,table,andfood)
andvewordswhichfrequentlyco-occurwithmen(skateboard,baseball,tie,motorcy-
cle,andsnowboard).Tochoosebiasedwords,wecomputebiasasisdonein[4]Section
3forthemostcommonlyoccurringnouns(
>
250times)intheMSCOCO-Biastrain-
ingset.Wecomputetheerrorrateandthe
difference
betweenthegroundtruthratioof
womentomenandtheratioproducedbyeachcaptioningmodel,forimagescontaining
theaboveobjects(Table4).Weobservesimilartrendstoourobservationsinthemain
paper.EqualizerandEqualizerw/oACLhavethelowesterrors,withEqualizerw/o
ACLperformingslightlybetter,suggestingthetermisimportantforlow
errorrate.Consideringdistancetothegroundtruthgenderratio,theEqualizermodel
consistentlyoutperformsothermodels.Oneparticularlyinterestingcasestudyisthe
wordﬁkitchenﬂinwhichthegroundtruthwomantomangenderratiois0.946(recall
thatthedatasetcontainsaroughly1:3womantomangenderratio,soagenderratio
closeto1.0foraobjectsuggeststhatahigherproportionofﬁwomanﬂimages
includeaﬁkitchenﬂthanﬁmanﬂimages).TheEqualizermodelpredictsagenderratio
of1.0(delta0.054)whereasthenextbestmodel(Equalizerw/oACL)predictsagen-
derratioof0.806(delta0.14).TheBaseline-FTmodelpredictsaratioof0.586(delta
0.361).
MaskedImages.
Wealsoconsiderthegenderratiowhenpredictingsentencesformasked
images(imagesinthetestsetaremaskedinthesamewayaswasdonetotraintheAp-
pearanceConfusionLossterm).Ideally,theratioofpredictedgenderwordsshould
becloseto
1
:
0
onthemaskedimagesasgenderinformationisobscured.Themanto
womangenderratiosfortheEqualizerw/oACL,Equalizerw/oConf,andEqualizerare
3.45,2.87,and1.98respectively(allothermodelshavelargerratiosthanEqualizerw/o
ACL).ThissuggeststhatagainbothourACLlossandConflossareimportantforpre-
dictingafairgenderratiowhengenderinformationisnotpresentintheimage.Again,
weachievethebestperformancewithourfullEqualizermodel.
16Burns*,Hendricks*,Saenko,Darrell,Rohrbach
Umbrella
Kitchen
CellPhone
Table
Food
Skateboard
Baseball
Tie
Motorcycle
Snowboard
Error
Baseline-FT
0.303
0.277
0.172
0.200
0.154
0.028
0.072
0.017
0.083
0.073
Equalizerw/oACL
0.210
0.157
0.145
0.100
0.085
0.020
0.085
0.021
0.054
0.031
Equalizerw/oConf
0.250
0.269
0.158
0.189
0.166
0.028
0.038
0.017
0.107
0.100
Equalizer
0.176
0.181
0.119
0.119
0.128
0.031
0.113
0.011
0.064
0.081

Ratio(Women:Men)
Baseline-FT
1.074
0.358
0.278
0.351
0.213
0.011
0.004
0.013
0.103
0.094
Equalizerw/oACL
1.274
0.137
0.028
0.151
0.092
0.009
0.018
0.002
0.086
0.051
Equalizerw/oConf
1.931
0.291
0.212
0.275
0.127
0.008
0.023
0.015
0.084
0.081
Equalizer
2.335
0.057
0.009
0.131
0.031
0.001
0.046
0.008
0.077
0.033
Table4:Breakdownoferrorrateanddifferencetoground-truthwoman:manratioover
imageswithbiasedwords.WeseethatthefullEqualizergenerallyoutperforms
theBaseline-FT.Onerror,Equalizerw/oACLperformsbest,followedbyEqualizer.
Equalizerperformsbestwhenconsideringpredictedgenderratio.
MSCOCO-Bias
MSCOCO-Balanced
Model
ErrorRatio

ErrorRatio

Baseline-FT
12.830.15
19.300.51
Equalizer
7.02-.03
8.10
0.13
Equalizer(MultipleGenderWords)
6.81.00
8.9
0.19
Table5:Evaluationofpredictedgenderwordsbasedonerrorrateandratioofgenerated
sentenceswhichincludetheﬁwomanﬂwordstosentenceswhichincludetheﬁmanﬂ
words.TrainingEqualizerwithsetsofmultiplegenderedwordsimproveserrorand
ratioontheMSCOCO-Biasset,butnotso.
TrainingwithaSetofGenderedWords.
Toachievetheresultsinthemainpaper,
weonlyapplytheAppearanceConfusionLossandlosstosets
f
man
g
and
f
woman
g
.AsshowninTable5,applyingtheselossestolargersetsofgenderedwords,
includingmanandwoman,doesnotaffectperformanceonMSCOCO.
Thesetsofgenderedwordsusedinthisexperimentare
f
girl,sister,mom,wife,woman,
bride,female,lady,women
g
and
f
boy,brother,dad,husband,man,groom,male,guy,
men
g
forwomenandmen,respectively.
CQualitativeExamples
InthefollowingwerelyonGrad-CAMmapsforvisualization.
InFigure5weprovidemultipleexamplesofimageswhereourmodelEqualizer
predictsﬁpersonﬂratherthanﬁwomanﬂorﬁmanﬂ.Inmanycasesthisoccurswhenthe
genderevidenceischallenging(e.g.examplewhereonlytheperson'shandsand
armsarevisibleandsecondexamplewheretheperson'sfaceisoccludedbythegiraffe)
WomenalsoSnowboard:OvercomingBiasinCaptioningModels17
ortheperson'sposeisunusual(thirdexample).However,wealsoobservecaseslikethe
oneatthebottom,whereEqualizerpredictsﬁpersonﬂdespitelookingattheclear/correct
genderevidence.WeattributethistotheLossterm,whichallowsforneutral
wordsgenerationwhenthemodelisuncertainaboutgender.
Figure6presentsmorequalitativeexamplesforthebaselinesandourmodel.At
thetopweshowsuccesscaseswhereourmodelpredictstherightgenderfortheright
reasons.Atthebottomweshowfailurecaseswithincorrectlypredictedgenderandthe
wronggenderevidence.
18Burns*,Hendricks*,Saenko,Darrell,Rohrbach
Fig.5:Qualitativecomparisonofbaselinesandourmodelwhenourmodelpredicts
ﬁpersonﬂratherthanﬁwomanﬂorﬁmanﬂ.
WomenalsoSnowboard:OvercomingBiasinCaptioningModels19
Fig.6:Qualitativecomparisonofbaselinesandourmodel.Atthetopweshowsuccess
caseswhereourmodelpredictstherightgenderfortherightreasons.Atthebottomwe
showfailurecaseswithincorrectlypredictedgenderandthewronggenderevidence.
20Burns*,Hendricks*,Saenko,Darrell,Rohrbach
References
1.
Torralba,A.,Sinha,P.:Statisticalcontextprimingforobjectdetection.In:Proceedingsof
theIEEEInternationalConferenceonComputerVision(ICCV).Volume1.,IEEE(2001)
763Œ770
2.
Torralba,A.:Contextualmodulationoftargetsaliency.In:AdvancesinNeuralInformation
ProcessingSystems(NIPS).(2002)1303Œ1310
3.
Gkioxari,G.,Girshick,R.,Malik,J.:Contextualactionrecognitionwithr*cnn.In:Proceed-
ingsoftheIEEEInternationalConferenceonComputerVision(ICCV).(2015)1080Œ1088
4.
Zhao,J.,Wang,T.,Yatskar,M.,Ordonez,V.,Chang,K.W.:Menalsolikeshopping:Reduc-
inggenderbiasusingcorpus-levelconstraints.In:ProceedingsoftheConfer-
enceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).(2017)
5.
Ryu,H.J.,Adam,H.,Mitchell,M.:Inclusivefacenet:Improvingfaceattributedetectionwith
raceandgenderdiversity.In:WorkshoponFairness,Accountability,andTransparencyin
MachineLearning(FAT/ML).(2018)
6.
Stock,P.,Cisse,M.:Convnetsandimagenetbeyondaccuracy:Explanations,biasdetection,
adversarialexamplesandmodelcriticism.arXivpreprintarXiv:1711.11443(2017)
7.
Bolukbasi,T.,Chang,K.W.,Zou,J.Y.,Saligrama,V.,Kalai,A.T.:Manistocomputerpro-
grammeraswomanistohomemaker?debiasingwordembeddings.In:AdvancesinNeural
InformationProcessingSystems(NIPS).(2016)4349Œ4357
8.
Buolamwini,J.A.:Gendershades:intersectionalphenotypicanddemographicevaluation
offacedatasetsandgenderPhDthesis,MassachusettsInstituteofTechnology
(2017)
9.
Barocas,S.,Selbst,A.D.:Bigdata'sdisparateimpact.CaliforniaLawReview
104
(2016)
671
10.
ofthePresident,U.S.E.O.,Podesta,J.:Bigdata:Seizingopportunities,preservingvalues.
WhiteHouse,ExecutiveOfofthePresident(2014)
11.
Gordon,J.,VanDurme,B.:Reportingbiasandknowledgeacquisition.In:Proceedingsof
the2013workshoponAutomatedKnowledgeBaseConstruction,ACM(2013)25Œ30
12.
Misra,I.,Zitnick,C.L.,Mitchell,M.,Girshick,R.:Seeingthroughthehumanreportingbias:
Visualfromnoisyhuman-centriclabels.In:ProceedingsoftheIEEEConference
onComputerVisionandPatternRecognition(CVPR),IEEE(2016)2930Œ2939
13.
vanMiltenburg,E.:Stereotypingandbiasinthedataset.In:WorkshoponMulti-
modalCorpora:Computervisionandlanguageprocessing.(2016)
14.
Hardt,M.,Price,E.,Srebro,N.,etal.:Equalityofopportunityinsupervisedlearning.In:
AdvancesinNeuralInformationProcessingSystems(NIPS).(2016)3315Œ3323
15.
Dwork,C.,Hardt,M.,Pitassi,T.,Reingold,O.,Zemel,R.:Fairnessthroughawareness.
In:Proceedingsofthe3rdInnovationsinTheoreticalComputerScienceConference,ACM
(2012)214Œ226
16.
Zhang,B.H.,Lemoine,B.,Mitchell,M.:Mitigatingunwantedbiaseswithadversariallearn-
ing.In:AAAI/ACMConferenceonIntelligence,Ethics,andSociety(AIES).
(2018)
17.
Quadrianto,N.,Sharmanska,V.:Recyclingprivilegedlearninganddistributionmatchingfor
fairness.In:AdvancesinNeuralInformationProcessingSystems(NIPS).(2017)677Œ688
18.
Tzeng,E.,Hoffman,J.,Darrell,T.,Saenko,K.:Simultaneousdeeptransferacrossdo-
mainsandtasks.In:ProceedingsoftheIEEEInternationalConferenceonComputerVision
(ICCV),IEEE(2015)4068Œ4076
19.
Zhang,X.,Yu,F.X.,Chang,S.F.,Wang,S.:Deeptransfernetwork:Unsuperviseddomain
adaptation.arXivpreprintarXiv:1503.00591(2015)
WomenalsoSnowboard:OvercomingBiasinCaptioningModels21
20.
Quadrianto,N.,Petterson,J.,Smola,A.J.:Distributionmatchingfortransduction.In:Ad-
vancesinNeuralInformationProcessingSystems(NIPS).(2009)1500Œ1508
21.
Caruana,R.,Lou,Y.,Gehrke,J.,Koch,P.,Sturm,M.,Elhadad,N.:Intelligiblemodelsfor
healthcare:Predictingpneumoniariskandhospital30-dayreadmission.In:Proceedings
ofthe21thACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandData
Mining,ACM(2015)1721Œ1730
22.
Tan,S.,Caruana,R.,Hooker,G.,Lou,Y.:Detectingbiasinblack-boxmodelsusingtrans-
parentmodeldistillation.In:AAAI/ACMConferenceonIntelligence,Ethics,and
Society.(2018)
23.
Ross,A.S.,Hughes,M.C.,Doshi-Velez,F.:Rightfortherightreasons:Trainingdifferen-
tiablemodelsbyconstrainingtheirexplanations.In:ProceedingsoftheInternationalJoint
ConferenceonIntelligence(IJCAI).(2017)
24.
Ramanishka,V.,Das,A.,Zhang,J.,Saenko,K.:Top-downvisualsaliencyguidedbycap-
tions.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
(CVPR).Volume1.(2017)7
25.
Selvaraju,R.R.,Cogswell,M.,Das,A.,Vedantam,R.,Parikh,D.,Batra,D.:Grad-cam:
Visualexplanationsfromdeepnetworksviagradient-basedlocalization.In:Proceedingsof
theIEEEInternationalConferenceonComputerVision(ICCV).(2017)
26.
Fong,R.C.,Vedaldi,A.:Interpretableexplanationsofblackboxesbymeaningfulpertur-
bation.In:ProceedingsoftheIEEEInternationalConferenceonComputerVision(ICCV).
(2017)
27.
Ribeiro,M.T.,Singh,S.,Guestrin,C.:Whyshoulditrustyou?:Explainingthepredictions
ofany.In:Proceedingsofthe22ndACMSIGKDDInternationalConferenceon
KnowledgeDiscoveryandDataMining,ACM(2016)1135Œ1144
28.
Zintgraf,L.M.,Cohen,T.S.,Adel,T.,Welling,M.:Visualizingdeepneuralnetworkde-
cisions:Predictiondifferenceanalysis.In:ProceedingsoftheInternationalConferenceon
LearningRepresentations(ICLR).(2017)
29.
Zeiler,M.D.,Fergus,R.:Visualizingandunderstandingconvolutionalnetworks.In:Proceed-
ingsoftheEuropeanConferenceonComputerVision(ECCV),Springer(2014)818Œ833
30.
Vinyals,O.,Toshev,A.,Bengio,S.,Erhan,D.:Showandtell:Aneuralimagecaptiongen-
erator.In:ComputerVisionandPatternRecognition(CVPR),2015IEEEConferenceon,
IEEE(2015)3156Œ3164
31.
Donahue,J.,AnneHendricks,L.,Guadarrama,S.,Rohrbach,M.,Venugopalan,S.,Saenko,
K.,Darrell,T.:Long-termrecurrentconvolutionalnetworksforvisualrecognitionandde-
scription.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecog-
nition(CVPR).(2015)2625Œ2634
32.
Karpathy,A.,Fei-Fei,L.:Deepvisual-semanticalignmentsforgeneratingimagedescrip-
tions.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
(CVPR).(2015)3128Œ3137
33.
Xu,K.,Ba,J.,Kiros,R.,Cho,K.,Courville,A.,Salakhudinov,R.,Zemel,R.,Bengio,Y.:
Show,attendandtell:Neuralimagecaptiongenerationwithvisualattention.In:Proceedings
oftheInternationalConferenceonMachineLearning(ICML)
34.
Anderson,P.,He,X.,Buehler,C.,Teney,D.,Johnson,M.,Gould,S.,Zhang,L.:Bottom-up
andtop-downattentionforimagecaptioningandvqa.In:ProceedingsoftheIEEEConfer-
enceonComputerVisionandPatternRecognition(CVPR).(2018)
35.
Lin,T.Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ramanan,D.,Doll
´
ar,P.,Zitnick,C.L.:
Microsoftcoco:Commonobjectsincontext.In:ProceedingsoftheEuropeanConference
onComputerVision(ECCV),Springer(2014)740Œ755
36.
Young,P.,Lai,A.,Hodosh,M.,Hockenmaier,J.:Fromimagedescriptionstovisualdeno-
tations:Newsimilaritymetricsforsemanticinferenceovereventdescriptions.Transactions
oftheAssociationforComputationalLinguistics(TACL)
2
(2014)67Œ78
22Burns*,Hendricks*,Saenko,Darrell,Rohrbach
37.
Xu,J.,Mei,T.,Yao,T.,Rui,Y.:Msr-vtt:Alargevideodescriptiondatasetforbridgingvideo
andlanguage.In:ProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition(CVPR),IEEE(2016)5288Œ5296
38.
Levi,G.,Hassner,T.:Ageandgenderusingconvolutionalneuralnetworks.
In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognitionWork-
shops(CVPRWorkshops).(2015)34Œ42
39.
Zhang,K.,Tan,L.,Li,Z.,Qiao,Y.:Genderandsmileusingdeepconvolutional
neuralnetworks.In:ProceedingsoftheIEEEConferenceonComputerVisionandPattern
RecognitionWorkshops(CVPRWorkshops).(2016)34Œ38
40.
Eidinger,E.,Enbar,R.,Hassner,T.:Ageandgenderestimationoffaces.IEEE
TransactionsonInformationForensicsandSecurity
9
(12)(2014)2170Œ2179
41.
Argamon,S.,Koppel,M.,Pennebaker,J.W.,Schler,J.:Miningtheblogosphere:Age,gender
andthevarietiesofself-expression.FirstMonday
12
(9)(2007)
42.
Yan,X.,Yan,L.:Genderofweblogauthors.In:AAAISpringSymposium:
ComputationalApproachestoAnalyzingWeblogs,PaloAlto,CA(2006)228Œ230
43.
Burger,J.D.,Henderson,J.,Kim,G.,Zarrella,G.:Discriminatinggenderontwitter.In:
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing
(EMNLP),AssociationforComputationalLinguistics(2011)1301Œ1309
44.
Larson,B.N.:Genderasavariableinnatural-languageprocessing:Ethicalconsiderations.
(2017)
45.
Szegedy,C.,Vanhoucke,V.,Ioffe,S.,Shlens,J.,Wojna,Z.:Rethinkingtheinceptionarchi-
tectureforcomputervision.In:ProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognition.(2016)2818Œ2826
46.
Zhang,J.,Lin,Z.,Brandt,J.,Shen,X.,Sclaroff,S.:Top-downneuralattentionbyexcita-
tionbackprop.In:ProceedingsoftheEuropeanConferenceonComputerVision(ECCV),
Springer(2016)543Œ559
47.
Lin,J.:Divergencemeasuresbasedontheshannonentropy.IEEETransactionsonInforma-
tiontheory
37
(1)(1991)145Œ151
48.
Lavie,M.D.A.:Meteoruniversal:Languagetranslationevaluationforanytarget
language.In:ProceedingsoftheAnnualMeetingoftheAssociationforComputational
Linguistics(ACL).(2014)376
"
21,Fast Convex Pruning of Deep Neural Networks,http://arxiv.org/pdf/1806.06457v2.pdf,https://github.com/DNNToolBox/Net-Trim-v1,"FastConvexPruningofDeepNeuralNetworks
AlirezaAghasi

AfshinAbdi
y
JustinRomberg
y
Abstract
Wedevelopafast,tractabletechniquecalledNet-Trimforsimplifyingatrainedneuralnetwork.The
methodisaconvexpost-processingmodule,whichprunes(sparsi˝es)atrainednetworklayerbylayer,while
preservingtheinternalresponses.WepresentacomprehensiveanalysisofNet-Trimfromboththealgorithmic
andsamplecomplexitystandpoints,centeredonafast,scalableconvexoptimizationprogram.Ouranalysis
includesconsistencyresultsbetweentheinitialandretrainedmodelsbeforeandafterNet-Trimapplication
andguaranteesonthenumberoftrainingsamplesneededtodiscoveranetworkthatcanbeexpressedusinga
certainnumberofnonzeroterms.Speci˝cally,ifthereisasetofweightsthatusesatmost
s
termsthatcan
re-createthelayeroutputsfromthelayerinputs,wecan˝ndtheseweightsfrom
O
(
s
log
N=s
)
samples,where
N
istheinputsize.ThesetheoreticalresultsaresimilartothoseforsparseregressionusingtheLasso,andour
analysisusessomeofthesamerecently-developedtools(namelyrecentresultsontheconcentrationofmeasure
andconvexanalysis).Finally,weproposeanalgorithmicframeworkbasedonthealternatingdirectionmethod
ofmultipliers(ADMM),whichallowsafastandsimpleimplementationofNet-Trimfornetworkpruningand
compression.
Keywords:
PruningNeuralNetworks,DeepNeuralNetworks,CompressedSensing,BowlingScheme,
RademacherComplexity
1Introduction
Deepneuralnetworksarebecomingaprominenttooltolearndatastructuresofarbitrarycomplexity.Thissuccess
ismainlythankstotheir˛exible,yetcompactnonlinearformulation,andthedevelopmentofcomputational
andarchitecturaltechniquestoimprovetheirtraining(c.f.[
27
,
11
]foracomprehensivereview).Increasingthe
numberoflayers,andthenumberofneuronswithineachlayerisgenerallythemoststandardwayofaddingmore
˛exibilitytoaneuralnetwork.Whileaddingsuch˛exibilityiscapableofimprovingthe˝ttingofthemodeltothe
trainingdata(i.e.,reducingthemodelbias),itmakesthemodelspronetoover-parameterizationandover˝tting
(i.e.,increasingthemodelvariance),whichinturncandegradethepredictivecapabilityofthenetwork.
Tosimplifyorstabilizeneuralnetworks,variousregularizingtechniquesandpruningstrategieshavebeen
considered.Inspiredbytheclassicregularizersforlinearmodels,suchasRidge[
16
]andLasso[
29
],thetrainingof
neuralnetworksisalsoequippedwith
`
2
or
`
1
penalties[
26
,
10
]tocontroltheirvarianceandcomplexity.Adding
randomnesstothetrainingprocessisalsoshowntohaveregularizinge˙ects,relevanttowhichwemayreferto
Dropout[
28
]andDropConnect[
33
],whichrandomlyremoveactiveconnectionsinthetrainingphaseandare
likelytoproduceprunednetworks.Batchnormalization[
18
],associatedwithstochasticgradientdescent-type
˝ttingtechniques,canalsobeconsideredasatoolofsimilarnature,whereinthetrainingprocesstheupdatesof
thehiddenunitsareweightedbythestandarddeviationoftherandomexamplesincludedinthemini-batch.
Inthispaper,weadvocateadi˙erentapproach.Wetrainthenetworkusingstandardtechniques.Wethen
extracttheinternaloutputs(theintermediatefeatures)ateachlayerand˝ndasparsesetofweightsthat
reproducesthesefeaturesacrossallthetrainingdata.Thephilosophyhereisthatthemostimportantproductof
trainingthenetworkisthefeaturesthatitextracts,nottheweightsthatitsettlesontoproducethosefeatures.

(CorrespondingAuthor)RobinsonCollegeofBusiness,GeorgiaStateUniversity,Atlanta,GA.Email:
aaghasi@gsu.edu
y
SchoolofElectricalandComputerEngineering,GeorgiaTech,Atlanta,GA.Emails:
{abdi,jrom}@ece.gatech.edu
.
1
arXiv:1806.06457v2  [cs.LG]  26 Feb 2019Forlargenetworks,therewillbemanysetsofweightsthatproduceexactlythesameinternalfeatures;ofthose
weights,wechoosethesimplest.
Ourmethodfor˝ndingsparsesetsofweights,presentedindetailinSection3,isrelatedtowell-known
techniquesforsparseregression,e.g.theLasso[
29
]instatisticsandcompressedsensing[
6
]insignalprocessing.
Themaindi˙erenceisthenon-linearityinthemappingofinternalfeaturesfromonelayertoanother.Ifthis
non-linearityispiecewiselinearandconvex(asistherecti˝edlinearunit,
ReLU
(
x
)=
max
(
x
;
0
)
,thatweusein
allofouranalysisbelow),thenthereisanaturalwaytorecasttheconditionthattheoutputsandinputsofa
layermatchasasetoflinearinequalityconstraints.Thereisasimilarwaytorecastanapproximatematching
asinclusioninaconvexset.Usingthe
`
1
normasaproxyforsparsity,theentireprogrambecomesconvex.
ThisopensthedoorforathoroughanalysisofhowwellandunderwhatconditionswecanexpectNet-Trimto
performwell,andallowsustoleveragedecadesofresearchinconvexoptimizationto˝ndascalablealgorithm
withpredictableconvergencebehavior.
ThetheoryinSection4presentsanupperboundonthenumberoftrainingsamplesneededtodiscovera
weightmatrixthatissparse.Givenasetoflayerinputvectors
x
1
;:::;
x
P
andoutputvectors
y
1
;:::;
y
P
,we
solvetheprogram
minimize
W
k
W
k
1
subjectto
ReLU
(
W
>
x
p
)=
y
p
;
(
p
=1
;:::;P
)
;
(1)
where
k
W
k
1
=
P
n;m
j
w
n;m
j
isthesumoftheabsolutevaluesoftheentriesinamatrix
W
2
R
N

M
.Asthe
`
1
normisconvexandthe
ReLU
(

)
functionispiecewiselinear,meaningthatconstraintsintheprogramabove
canbebrokenintoaseriesoflinearequalityandinequalityconstraints,theprogramaboveisconvex.Weshow
thatifthe
x
p
areindependentsamplesofasubgaussianrandomvectorthatisnon-degenerate(meaningthatthe
correlationmatrixisfull-rank)andthereexistsa
W
?
withmaximally
s
-sparsecolumnsthatdoesindeedsatisfy
y
p
=
ReLU
(
W
>
?
x
p
)
forall
p
,thenthesolutionto
(1)
isexactly
W
?
whenthenumberoftrainingsamples
P
is
(almost)proportionaltothesparsity
s
:werequire
P
&
s
log(
N=s
)
:
Wealsoshowthatifthe
x
p
aresubgaussian,thensoarethe
y
p
.Asaresults,thetheorycanbeapplied
layer-by-layer,yieldingasamplingresultfornetworksofarbitrarydepth.(Whenweapplythealgorithmin
practice,theequalityconstraintsin
(1)
arerelaxed;thisisdiscussedindetailinSection3.1.)Alongwiththese
theoreticalguarantees,Net-Trimo˙ersstate-of-the-artperformanceonrealisticnetworks.InSection6,wepresent
somenumericalexperimentsthatshowthatcompressionfactorsbetween10xand50x(removing90%to98%of
theconnections)arepossiblewithverylittlelossintestaccuracy.
Contributionsandrelationstopreviouswork
ThispaperprovidesafulldescriptionoftheNet-Trim
methodfrombothatheoreticalandalgorithmicperspective.InSection3,wepresentourconvexformulation
forsparsifyingtheweightsinthelinearlayersofanetwork;wedescribehowtheprocedurecanbeapplied
layer-by-layerinadeepnetworkeitherinparallelorserially(cascadingtheresults),andpresentconsistency
boundsforbothapproaches.Section4presentsourmaintheoreticalresult,statedpreciselyinTheorem4.This
resultderivesanupperboundonthenumberofdatasamplesweneedtoreliablydiscoveralayerthathasat
most
s
connectionsinitslinearlayerweshowthatifthedatasamplesarerandom,thentheseweightscanbe
learnedfrom
O
(
s
log
N=s
)
samples.Mathematically,thisresultiscomparabletothesamplecomplexitybounds
fortheLassoinperformingsparseregressiononalinearmodel(alsoknownasthecompressedsensingproblem).
Ouranalysisisbasedonthebowlingscheme[
30
,
24
];themaintechnicalchallengesareadaptingthistechniqueto
thepiecewiselinearconstraintsintheprogram
(1)
,andthefactthattheinputvectors
f
x
p
g
intoeachlayerare
non-centeredinawaythatcannotbeaccountedforeasily.
2
Thereareseveralotherexamplesoftechniquesforsimplifyingnetworksbyre-trainingintherecentliterature.
Thesetechniquesaretypicallypresentedasmodelcompressiontools(e.g.,[
15
,
8
,
14
])forremovingtheinherent
modelredundancies.Inwhatisperhapsthemostcloselyrelatedworktowhatwepresenthere,[
15
]proposes
apruningschemethatsimplytruncatessmallweightsofanalreadytrainednetwork,andthenre-adjuststhe
remainingactiveweightsusinganotherroundoftraining.Incontrast,ouroptimizationschemeensuresthatthe
layerinputsandoutputsstayconsistentasthenetworkispruned.
TheNet-Trimframeworkwas˝rstpresentedin[
2
].Thispaperprovidesafarmorerigorousandcomplete
analysis(samplecomplexitybound)oftheNet-Trimalgorithmfornetworkswithmultiplelayers(theprevious
workonlyconsideredasinglelayerofthenetwork).Inaddition,wepresentascalable(yetrelativelysimple)
implementationofNet-Trimusingthealternationdirectionmethodofmultipliers(ADMM).Thisisaniterative
methodwitheachiterationrequiringasmallnumberofmatrix-vectormultiplies.Thecode,alongwithallthe
examplespresentedinthepaper,isavailableonline
1
.
Notation
Weuselowercaseanduppercaseboldfaceforvectorsandmatrices,respectively.Speci˝cally,the
notation
I
isreservedfortheidentitymatrix.Foramatrix
A
,
A

1
;
:
denotesthesubmatrixformedbyrestricting
therowsof
A
totheindexset

1
.Similarly,
A
:
;

2
restrictsthecolumnsof
A
to

2
,and
A

1
;

2
isformedby
extractingbothrowsandcolumns.Givenavector
x
(ormatrix
X
),
supp
x
(or
supp
X
)isthesetofindiceswith
non-zeroentries,andsupp
c
x
(orsupp
c
X
)isthecomplementset.
For
X
=[
x
m;n
]
2
R
M

N
,thematrixtraceisdenotedby
tr
(
X
)
.Furthermore,weuse
k
X
k
1
,
P
M
m
=1
P
N
n
=1
j
x
m;n
j
asanotationforthesumofabsoluteentries
2
,and
k
X
k
F
astheFrobeniusnorm.Theneuralnetworkactivation
usedthroughoutthepaperistherecti˝edlinearunit(ReLU),whichisappliedcomponent-wisetovectorsand
matrices,
(
ReLU
(
X
))
m;n
=max(
x
m;n
;
0)
:
Wewillsometimesusethenotation
X
+
asshorthandfor
ReLU
(
X
)
.Foranindexset

f
1
;

;M
gf
1
;

;N
g
,
W

representsamatrixofidenticalsizeas
W
=[
w
m;n
]
withentries
(
W

)
m;n
=
ˆ
w
m;n
(
m;n
)
2

0(
m;n
)
=
2

:
Finally,weuse
S
N
todenotestheunitspherein
R
N
+1
;andthenotation
f
&
^
f
(or
f
.
^
f
)whenthereexists
anabsoluteconstant
C
suchthat
f

C
^
f
(or
f

C
^
f
).
Outline.
Theremainderofthepaperisstructuredasfollows.InSection2,webrie˛yoverviewtheneural
networkarchitectureconsidered.Section3presentsthepruningideaandtheconsistencyresultsbetweenthe
initialandretrainednetworks.Thestatisticalarchitectureofthenetworkandthegeneralsamplecomplexity
resultsarepresentedinSection4.ToimplementtheNet-Trimunderlyingconvexprogram,inSection5wepresent
anADMMschemeapplicabletotheoriginalNet-Trimformulation.Finally,Section6presentssomeexperiments,
alongwithconcludingremarks.Allthetechnicalproofsofthetheoremsandresultspresentedinthispaperare
movedtoSection7.
1
Thelinktothecodeandrelatedmaterial:
https://dnntoolbox.github.io/Net-Trim/
2
Thenotation
k
X
k
1
shouldnotbeconfusedwiththematrixinduced
`
1
norm
3
2FeedforwardNetworkModel
Inthissection,webrie˛yoverviewthetopologyofthefeedforwardnetworkmodelconsidered.Thetrainingof
thenetworkisperformedvia
P
samples
x
p
,
p
=1
;

;P
,where
x
p
2
R
N
isthenetworkinput.Tocompactly
representthetrainingsamples,weformamatrix
X
2
R
N

P
,structuredas
X
=
[
x
1
;

;
x
P
]
.Considering
L
layersinthenetwork,theoutputofthenetworkatthe˝nallayerisdenotedby
X
(
L
)
2
R
N
L

P
,whereeach
columnin
X
(
L
)
isaresponsetothecorrespondingtrainingcolumnin
X
.
InaReLUnetwork,theoutputofthe
`
-thlayeris
X
(
`
)
2
R
N
`

P
,generatedbyapplyingthea˚netransfor-
mation
W
>
`
(

)+
b
(
`
)
toeachcolumnofthepreviouslayer
X
(
`

1)
,followedbyaReLUactivation:
X
(
`
)
=
ReLU

W
>
`
X
(
`

1)
+
b
(
`
)
1
>

;`
=1
;

;L:
(2)
Here
W
`
2
R
N
`

1

N
`
,
X
(0)
=
X
and
N
0
=
N
.Byaddinganadditionalrowto
W
`
and
X
(
`

1)
,onecanabsorb
theintercepttermandcompactlyrewrite(2)as
X
(
`
)
=
ReLU

W
>
`
X
(
`

1)

;`
=1
;

;L:
(3)
Oftenthelastlayerofaneuralnetworkskipsanactivationbymerelygoingthroughthea˚netransformation.As
amatteroffact,theresultspresentedinthispaperalsoapplytosucharchitecture(seeanalysisexamplesin[
2
]).
Aneuralnetworkthatfollowsthemodelin
(3)
canbefullyidenti˝edby
X
and
^
W
`
,
`
=1
;

;L
.Throughout
thepaper,suchnetworkwillbedenotedby
N
et
(
f
W
`
g
L
`
=1
;
X
)
.
3TheNet-TrimPruningAlgorithm
Net-Trimisapostprocessingschemewhichprunesaneuralnetworkafterthetrainingphase.Similartomany
otherregularizationtechniques,Net-Trimiscapableofsimplifyingtrainedmodelsattheexpenseofacontrollable
increaseinthebias.
Afterthetrainingphaseandlearning
W
`
,Net-Trimretrainsthenetworksothatforthesametrainingdata
thelayeroutcomesstaymoreorlessclosetotheinitialmodel,whiletheredesignednetworkissparser,i.e.,
`
=1
;

;L
:
nnz

^
W
`

˝
nnz
(
W
`
)
;
while
^
X
(
`
)
ˇ
X
(
`
)
:
Here,nnz
(
:
)
denotesthenumberofnonzeroentries,and
^
W
`
and
^
X
(
`
)
arerespectivelytheredesignedlayer
matricesandthecorrespondinglayeroutcomes.
Asidefromthepost-processingnatureandsomedi˙erencesintheconvexformulations,Net-Trimshares
manysimilaritieswiththeLasso(leastabsoluteshrinkageandselectionoperator[
29
]),astheybothusean
`
1
proxytopromotemodelsparsity.IntheremainderofthissectionweoverviewtheNet-Trimformulationandthe
correspondingpruningschemes.
3.1PruningaSingleLayer
Consider
X
in
2
R
N

P
and
X
out
2
R
M

P
tobealayerinputandoutputmatricesafterthetraining,whichbased
onthemodelin(2)(or(3))areconnectedvia
X
out
=
ReLU

W
>
X
in

:
4
Toexploreasparsercoe˚cientmatrix,wemayconsidertheminimization
minimize
U
k
U
k
1
subjectto



ReLU

U
>
X
in


X
out



F


(4)
whichmaypotentiallygenerateasparser
W
-matrixrelating
X
in
and
X
out
,attheexpenseofa(controllable)
discrepancybetweenthelayeroutcomesbeforeandaftertheretraining.
Despitetheconvexobjective,theconstraintsetin
(4)
isnon-convex.Usingthefactthattheentriesof
X
out
areeitherzeroorstrictlypositivequantities,[2]proposethefollowingconvexproxyto(4):
^
W
=argmin
U
k
U
k
1
subjectto
8
<
:




U
>
X
in

X
out





F



U
>
X
in


c

0
;
(5)
where
=
supp
X
out
=
n
(
m;p
):

X
out

m;p
>
0
o
:
Themainideabehindthisconvexsurrogateisimposingsimilaractivationpatternsbeforeandaftertheretraining
viathesecondinequalityin(5),i.e.,

W
>
X
in

+

c
=

^
W
>
X
in

+

c
=
0
;
andallowingthe

-discrepancyonlyontheset

.Foramorecompactpresentationoftheconvexconstraintset,
forgivenmatrices
X
;
Y
and
V
weusethenotation
U
2C

(
X
;
Y
;
V
)
()
8
<
:




U
>
X

Y





F



U
>
X


c

V

c
;
for
=
supp
Y
:
(6)
Usingthisnotation,theconvexprogramin(5)maybecastas
^
W
=argmin
U
k
U
k
1
subjectto
U
2C


X
in
;
X
out
;
0

:
(7)
3.2PruningtheNetwork
Havingaccesstothetoolstoretrainanylayerwithinthenetwork,exclusivelybasedontheinputandtheoutput,
wemayconsider
parallel
or
cascade
frameworkstoretraintheentirenetwork.
TheparallelNet-Trimisastraightforwardapplicationoftheconvexprogram
(7)
toeachlayerinthenetwork.
Basically,eachlayerisprocessedindependentlybasedontheinitialmodelinputandoutput,withouttakinginto
accounttheretrainingresultfromthepreviouslayers.Speci˝cally,denoting
X
(
`

1)
and
X
(
`
)
astheinputand
outputofthe
`
-thlayeroftheinitialtrainednetwork,weproposetoretrainthecoe˚cientmatrix
W
`
viathe
convexprogram
^
W
`
=argmin
U
k
U
k
1
subjectto
U
2C

`

X
(
`

1)
;
X
(
`
)
;
0

;`
=1
;

;L:
(8)
Animmediatequestionwouldbeifeachlayerofanetworkisretrainedvia
(8)
andonereplaces
N
et
(
f
W
`
g
L
`
=1
;
X
)
withtheretrainednetwork
N
et
(
f
^
W
`
g
L
`
=1
;
X
)
,howdothediscrepancies

`
propagateacrossthenetwork,andhow
farapartwouldbethe˝nalresponsesofthetwonetworksto
X
?Thefollowingresultaddressesthisquestion.
5
Theorem1(ParallelNet-Trim)
Consideranormalizednetwork
N
et
(
f
W
`
g
L
`
=1
;
X
)
,suchthat
k
W
`
k
1
=1
for
`
=1
;

;L
.Solve
(8)
foreachlayerandformtheretrainednetwork
N
et
(
f
^
W
`
g
L
`
=1
;
X
)
.Denotingby
^
X
(
`
)
=
ReLU
(
^
W
`
>
^
X
(
`

1)
)
theoutcomesoftheretrainednetwork,where
^
X
(0)
=
X
(0)
=
X
,thelayeroutcomes
oftheoriginalandretrainednetworksobey



^
X
(
`
)

X
(
`
)



F

`
X
j
=1

j
;`
=1
;

;L:
(9)
Itisnoteworthythatthenormalizationassumption
k
W
`
k
1
=1
inTheorem1ismadewithnolossingenerality,
andisonlyawayofpresentingtheresultinastandardform.Thisissimplybecause
ReLU
(
j

j
x
)=
j

j
ReLU
(
x
)
,
andascalingofanyoftheweightmatrices
W
`
wouldscale
X
(
L
)
(or
X
(
`
0
)
where
`
0

`
)bythesameamount.
Speci˝cally,theoutcomesofthenetworkbeforeandaftertheprocessobey



^
X
(
L
)

X
(
L
)



F

L
X
j
=1

j
;
whichmakesparallelNet-Trimastableprocess,producingacontrollableoveralldiscrepancy.
Amoreadaptivewayofretraininganetwork,whichwewouldrefertoasthecascadeNet-Trim,incorporates
theoutcomeofthepreviouslyprunedlayerstoretrainatargetlayer.Basically,inacascadeNet-Trim,retraining
W
`
takesplacebyexploringapathbetweentheinput/outputpairs
(
^
X
(
`

1)
;
X
`
)
insteadof
(
X
(
`

1)
;
X
`
)
.Due
tosomefeasibilityconcerns,thatwillbedetailedinthesequel,acascadeformulationdoesnotsimplyhappenby
replacing
X
(
`

1)
with
^
X
(
`

1)
in(8),andtheformulationrequiressomemodi˝cations.
Toderivethecascadeformulation,considerstartingtheprocessbyretrainingthe˝rstlayervia
^
W
1
=argmin
U
k
U
k
1
subjectto
U
2C

1

X
;
X
(1)
;
0

:
(10)
Setting
^
X
(1)
=
ReLU
(
^
W
1
>
X
)
,toadaptivelyprunethesecondlayer,onewouldideallyconsidertheprogram
minimize
U
k
U
k
1
subjectto
U
2C

2

^
X
(1)
;
X
(2)
;
0

:
(11)
Itisnothardtoseethatthesimplegeneralizationin
(11)
isnotguaranteedtobefeasible,thatis,thereexistsa
matrix
W
suchthatfor
=
supp
X
(2)
:
8
<
:




W
>
^
X
(1)

X
(2)





F


2

W
>
^
X
(1)


c

0
:
(12)
Ifinsteadof
^
X
(1)
theconstraintset
(12)
wasparameterizedby
X
(1)
,anaturalfeasiblepointwouldhavebeen
W
=
W
2
.Nowthat
^
X
(1)
isaperturbedversionof
X
(1)
,theconstraintsetneedstobeproperlyslackedto
maintainthefeasibilityof
W
2
.Inthiscontext,onemayeasilyverifythat
W
2
isfeasiblefortheslackedprogram
minimize
U
k
U
k
1
subjectto
U
2C

2

^
X
(1)
;
X
(2)
;
W
>
2
^
X
(1)

;
(13)
aslongasforsome


1
,

2
=





W
>
2
^
X
(1)

X
(2)





F
:
6
The

-coe˚cientisafreeparameter,whichwerefertoasthe
in˛ationrate
.When

=1
,thematrix
W
2
isonly
tightlyfeasiblefor
(13)
andthefeasiblesetcanattheveryleastbecomeasingleton.However,increasingthe
in˛ationratewouldexpandthesetofpermissiblematricesandmakes
(13)
capableofproducingsparsersolutions.
Theprocessappliedtothesecondlayermaybegeneralizedtothesubsequentlayersandformacascade
paradigmtoprunethenetworklayerbylayer.ThepseudocodeinAlgorithm1summarizestheNet-Trimcascade
scheme,whereweset

1
=

forthe˝rstlayer,andconsiderthein˛ationrates

`
,
`
=2
;

;L
,forthesubsequent
layers.
Algorithm1:
CascadeNet-Trim
^
W
1
 
argmin
U
k
U
k
1
subjectto
U
2C


X
;
X
(1)
;
0

^
X
(1)
 
ReLU

^
W
1
>
X

for
`
=2
;

;L
do

 
supp
X
(
`
)
;

`
 

`




W
>
`
^
X
(
`

1)

X
(
`
)





F
^
W
`
 
argmin
U
k
U
k
1
subjectto
U
2C

`

^
X
(
`

1)
;
X
(
`
)
;
W
>
`
^
X
(
`

1)

^
X
(
`
)
 
ReLU

^
W
`
>
^
X
(
`

1)

endfor
Similartotheparallelscheme,wecanshowaboundeddiscrepancybetweentheoutcomesoftheinitialnetwork
N
et
(
f
W
`
g
L
`
=1
;
X
)
andtheretrainednetwork
N
et
(
f
^
W
`
g
L
`
=1
;
X
)
,asfollows.
Theorem2(CascadeNet-Trim)
Consideranormalizednetwork
N
et
(
f
W
`
g
L
`
=1
;
X
)
,suchthat
k
W
`
k
1
=1
for
`
=1
;

;L
.IfthenetworkisretrainedaccordingtoAlgorithm1,thelayeroutcomesoftheoriginaland
retrainednetworkswillobey



^
X
(
`
)

X
(
`
)



F


`
Y
j
=2

j
:
(14)
Speci˝cally,whenanidenticalin˛ationrateisusedacrossallthelayers,onewouldhave
k
^
X
(
L
)

X
(
L
)
k
F


(
L

1)

,
whichisacontrollablysmallquantity,giventhat

canbeselectedarbitrarilycloseto1.Forinstancewhen

=1
:
01
and
L
=10
,thetotalnetworkdiscrepancywouldbestilllessthan
1
:
1

.Aswillbedemonstratedinthe
experimentssection,forthesameleveloftotalnetworkdiscrepancy,thecascadeNet-Trimiscapableofproducing
sparsernetworks.However,suchreductionisachievedattheexpenseofthelossindistributability,whichmakes
theparallelschemecomputationallymoreattractiveforbigdataproblems.
4SampleComplexityBoundsUsingSubgaussianRandomFlow
Intheprevioussectionwediscussedandanalyzedtheconvexretrainingschemeanditsconsistencywiththe
referencemodel.Inthissectionweanalyzethesamplecomplexityoftheproposedretrainingframework.Basically,
thegoalofthissectionisaddressingthefollowingquestion:
ifthereexistsasparsetransformationmatrixrelating
theinputandoutputofalayer,howmanyrandomsamplesaresu˚cienttorecoveritviatheproposedretraining
scheme?
Aswillbedetailedinthesequel,wewillshowthatretrainingeachneuronwithinthenetworkispossiblewith
fewersamplesthantheneurondegreesoffreedom.Morespeci˝cally,foratrainedneuronwith
N
inputports,if
7
generatinganidenticalresponseispossiblewith
s
˝
N
nonzeroweights,Net-Trimisabletorecoversuchmodel
withonly
O
(
s
log
(
N=s
))
randomsamples.Thisresultisvalidfortheneuronsofanylayerwithinthenetwork,as
longassomestandardstatisticalpropertiescanbeestablishedfortheinputsamples.
Unlikethepreviouswork[
2
],whichestablishesasimilarresultforonlytheneuronswithinthe˝rstlayer,
here,duetosomefavorabletailpropertiesofsubgaussianrandomvectors,weareabletogeneralizetheresultto
theentirenetwork.Basically,wewillshowthatwhenthenetworkinputsamplesareindependentlydrawnfrom
astandardnormal(oranyothersubgaussian)distribution,theinputsamplesatallsubsequentlayersremain
independentandsubgaussian(whatwerefertoasasubgaussian˛ow).Bycarefullyusingsometechnicaltools
fromthestructuredsignalrecoveryliterature[
30
,
24
],weareabletopresentthemainsamplecomplexityresultin
ageneralform.
Topresenttheresults,we˝rststartwithabriefoverviewofsubgaussianrandomvariables.Foramore
comprehensiveoverview,thereaderisreferredto[32]andof[31].
De˝nition1(subgaussianrandomvariable)
Arandomvariable
'
issubgaussian
3
ifthereexistsaconstant

,suchthatforall
t

0
,
P
fj
'
j
>t
g
exp

1

t
2

2

:
(15)
Equivalently,
'
issubgaussianifthereexistsaconstant
^

suchthat
E
exp

'
2
^

2


e:
(16)
Thesubgaussiannormof
'
,alsoreferredtoastheOrlicznorm,isdenotedby
k
'
k
 
2
,andde˝nedas
k
'
k
 
2
,
sup
p

1
p

1
2
(
E
j
'
j
p
)
1
p
:
WhilecalculatingtheexactOrlicznormcanbechallenging,ifeitheroneoftheproperties
(15)
or
(16)
hold,
k
'
k
 
2
isthesmallestpossiblenumber(

or
^

)ineitheroneoftheseinequalities,uptoanabsoluteconstant.
De˝nition2(subgaussianrandomvector)
Arandomvector
'
2
R
N
issubgaussianifforall

2
R
N
(or
equivalentlyall

2
S
N

1
),theone-dimensionalmarginals

>
'
aresubgaussian.
ThenotionofOrlicznormalsogeneralizestothevectorcaseas
k
'
k
 
2
,
sup

2
S
N

1
k

>
'
k
 
2
=sup

2
S
N

1
sup
p

1
p

1
2

E



>
'


p

1
p
:
(17)
Wearenowreadytostatethe˝rstresult,whichwarrantsasubgaussianrandom˛owacrossthenetwork,aslong
asthenetworkinputsamplesareindependentlydrawnfromastandardGaussian(orsubgaussian)distribution.
Theorem3
Consideranetworkwith˝xedparameters
W
`
,
b
(
`
)
,wheretheinputandoutputtoeachlayerare
relatedvia
x
(
`
)
=
ReLU

W
>
`
x
(
`

1)
+
b
(
`
)

;`
=1
;

;L:
(18)
Ifthenetworkisfedwithi.i.dsamplevectors
x
(0)
1
;

;
x
(0)
P
˘N
(
0
;
I
)
,theresponsesamplesateachlayeroutput
remaini.i.dsubgaussian.
3
Ingeneral,theright-handexpressionin(15)canbereplacedwith
c
exp


t
2

2

usingtwoabsoluteconstants
c
and

8
x
P
x
2
x
1

x
(1)
P
x
(1)
2
x
(1)
1

x
(
L
)
P
x
(
L
)
2
x
(
L
)
1

!
!
!
!
!
!
!
!
!

!
!
!

W
>
1
x
+
b
(1)

+

W
>
2
x
(1)
+
b
(2)

+

W
>
L
x
(
L

1)
+
b
(
L

1)

+
or
W
>
L
x
(
L

1)
+
b
(
L

1)

|
{z
}
i:i:dSG

|
{z
}
i:i:dSG

|
{z
}
i:i:dSG
Figure1:When
x
1
;
x
2
;

;
x
P
,theinputsamplestotheproposedneuralnetwork,areindependentlydrawnfrom
asubgaussiandistribution,theinput/outputvectorsofeverysubsequentlayerremaini.i.dandsubgaussian
Asshownintheproof,theresultofTheorem3stillholdswhenthenetworkinputsamplesareindependently
drawnfromasubgaussiandistributioninsteadofastandardnormal,and/orwhenthelastlayerskipsaReLU
activation.Speci˝cally,whenthenetworkisfedwith
x
(0)
1
;
x
(0)
2
;

;
x
(0)
P
,independentlydrawnfromasubgaussian
distribution,theresultingresponses
x
(
`
)
1
;
x
(
`
)
2
;

;
x
(
`
)
P
atanylayer
`
remainindependentandsubgaussian.The
diagraminFigure1demonstratessuchstatisticalstructureamongthelayerinputsacrossthenetwork.
Havingindependentsubgaussiansamplesatanylayerinputportallowsustorelatethenumberofsamplesto
therecoveryofareducedmodel.Exchangingthelayerindexwiththegeneralinput/outputnotation,when
X
in
2
R
N

P
and
X
out
2
R
M

P
arerespectivelytheinputandoutputtoalayer,relatedvia
X
out
=
ReLU
(
W
>
X
in
)
,
obtainingtheprunedlayermatrix
^
W
2
R
N

M
isperformedvia
^
W
=argmin
W
k
W
k
1
subjectto
W
2C


X
in
;
X
out
;
0

:
(19)
When

=0
,theprogramin
(19)
decouplesinto
M
individualconvexprogramseachretrainingacolumnin
W
.
Basically,insteadofsolving
(19)
for
^
W
,if
x
out
>
2
R
P
isarowin
X
out
,thecorrespondingcolumnin
^
W
canbe
calculatedvia
minimize
w
k
w
k
1
subjectto
w
2C
0

X
in
;
x
out
>
;
0

;
(20)
reducing
(19)
toretrainingeachofthe
M
outputneurons,individually.Ourfocusonthecaseof

=0
(whichalso
makesthecascadeandparallelschemesequivalent)isworkinginanunderdeterminedregime,wheretherequired
samplesareshowntobemuchlessthanthelayer(neuron)degreesoffreedom.Inthiscase,therelationship
between
X
in
and
X
out
canbeestablishedviain˝nitelymany
W
matricesandoneseeksauniquesparsesolution
via(19).
Beforestatingthemaintechnicalresult,wewouldliketointroducesomenotionsusedinthepresentation.
Whenaneuronisinitiallytrainedviaavector
w
0
2
R
N
andfedwithi.i.dinstancesof
x
,theactivationpatternof
theneuronisfullycontrolledbythesignof
w
>
0
x
.Inthiscase,oneexpectstogainthemainretraininginformation
fromthecaseswhenReLUisinthelinearmode(i.e.,
w
>
0
x
>
0
).Inthisregard,correspondingtotherandom
input
x
,wede˝netherandom
virtualinput
as
˛
=
x
1
w
>
0
x
>
0
=
(
xw
>
0
x
>
0
0
w
>
0
x

0
:
Thevirtualrandomvectorplaysakeyroleinourpresentation.Ourpresentationalsodependsonthesmallest
9
eigenvalueofthevirtualcovariancematrix,whichfollowsthestandardde˝nition:

min
(cov(
˛
))=inf

2
S
N

1

>
cov(
˛
)

;
where
cov(
˛
)=
E

˛˛
>


E
(
˛
)
E
(
˛
)
>
:
Theorem4
Forthemodel
(3)
,consideratrainedneuronobeying
x
out
=
ReLU
(
X
in
>
w
0
)
,where
X
in
=
[
x
1
;

;
x
P
]
2
R
N

P
and
x
;
x
1
;

;
x
P
areindependentsamplesofasubgaussiandistribution.Assume,an
s
-sparsevector
w

2
R
N
iscapableofgeneratinganidenticalresponseto
X
in
as
x
out
.Fix


1
=
2
and
t

0
,
thenif
P
&
C
;˛

s
log

N
s

+
s
+1+
t

;
(21)
retrainingtheneuronvia
(20)
recovers
w

withprobabilityexceeding
1

e

ct
.Theabsoluteconstant
c
isuniversal
andtheconstant
C
;˛
dependsonthestatisticsofthevirtualinput
˛
=
x
1
w
>
0
x
>
0
via
C
;˛
=(1+

)
2
 
k
˛

E
˛
k
2
 
2

min
(cov(
˛
))
!
3+
1

:
(22)
WewouldliketohighlightsometechnicaldetailsrelatedtoTheorem4.Toestablishtheresultweusethe
bowlingscheme
proposedby[
30
],whichdiscussestherecoveryofastructured(e.g.,sparse)signalfromindependent
linearmeasurements.Below,wemakeaconnectionbetweenourproblemwithnonlinearconstraintstotheproblem
withlinearconstraintsdescribedthere.Whileweusedthecompactmodel
(3)
foramoreconcisepresentation,the
modelin
(2)
isstillcoveredbyTheorem4,treatingtheinterceptasaconstantfeatureappendedtotheneuron
input.
ItisimportanttonotethatduetotheapplicationoftheReLUateachlayer,therandomsamplesentering
thenextlayerarenon-centeredandthisrequiresacarefulanalysisoftheproblem.Infact,themajorityofthe
measurementsystemsinthestructuredrecoveryliteratureworkwithcenteredrandommeasurements,assomeof
thepowerfulanalysistools,suchastherestrictedisometryproperty[
4
,
6
],thecerti˝cateofduality[
5
,
12
],andthe
Mendelson'ssmallballmethodwhichstandsasthebackboneforthebowlingscheme[
19
,
24
,
25
]relycritically
ontherandomvectorsbeingcentered.InthepresentationofTheorem4,theconstantisrelatedtothestatistics
ofthecenteredvirtualinput,regardlessofthemeanshiftthatthepreviousactivationunitshavecausedtothe
input
4
.
Finally,Theorem4canbeusedasageneralandpowerfultooltoestimatetheretrainingsamplecomplexity
foranylayerwithinthenetwork.Toestablishthe
O
(
s
log
(
N=s
))
rateforagivenlayer,weonlyneedtoshow
thatforthecorrespondinginput
x
andinitiallytrainedweights
w
0
,thevirtualinput
˛
=
x
1
w
>
0
x
>
0
satis˝esthe
followingtwoconditions:

min
(cov(
˛
))
&
1
;
and
k
˛

E
˛
k
 
2
.
1
:
(23)
Asaninsightfulexample,wegothroughtheexerciseofestablishingtheboundsin
(23)
foralayerfedwithi.i.d
Gaussiansamples;thisisnotanunreasonablescenarioforthe˝rstlayerofaneuralnetwork.Aswillbedetailed
inSection4.1below,usingstandardtoolstoverifytheconditionsin
(23)
,convenientlyprovesthe
O
(
s
log
(
N=s
))
rateforsuchlayer.
Foranetworkfedwithi.i.dGaussiansamples,goingthroughasimilarexerciseforthesubsequentlayers(say
layer
`>
1
,withindependentcopiesoftherandominput
x
(
`
)
),requirestracingthestatisticsof
˛
(
`
)
=
x
(
`
)
1
w
>
0
x
(
`
)
>
0
downtotheGaussianinput
x
(0)
.Insuchcase,warrantingtheconditionsin
(23)
wouldrequirestatingrealistic
conditionsontheinitiallytrained
W
j
for
j
=1
;

;`
.Suchgeneralizationcouldbeapplicationspeci˝cand
beyondthecurrentloadofthepaper,whichisleftasapotentialfuturework.
4
ThisisimportantbecausetheOrlicznormofanoncenteredrandomvectorcaneasilybecomedimension-dependent.Forinstance,
ifthecomponentsof
x
2
R
N
arei.i.dstandardGaussians,onecaneasilyverifythat


x
+


 
2
=
O
(
p
N
)
,while


x
+

E
x
+


 
2
=
O
(1)
.
10
4.1FeedingaNeuronwithi.i.dGaussianSamples
Inthissectionwegothroughtheexerciseofestablishingtheconditionsin
(23)
foraneuronfedwithindependent
copiesof
x
˘N
(
0
;
I
)
,
x
2
R
N
.Below,wegothrougheachboundin
(23)
,separately.Inallthecalculations,
w
0
6
=
0
isa˝xedvectorthatcorrespondstotheinitiallytrainedmodel.In[
2
],theauthorsgothroughachainof
techniquestoprovean
O
(
s
log
N
)
samplecomplexitybycarefullyconstructingadualcerti˝catefortheconvex
program.HerewewillseethatthankstoTheorem4,suchprocessismarkedlyreducedtoestablishingthe
conditionsin(23),whichisconvenientlyful˝lledusingstandardtools.
4.1.1Step1:BoundingtheCovarianceMatrix
Toevaluatethevirtualinputcovariancematrixwehave

min

cov

x
1
w
>
0
x
>
0

=

min

E
xx
>
1
w
>
0
x
>
0


E
x
1
w
>
0
x
>
0

E
x
1
w
>
0
x
>
0

>



min

E
xx
>
1
w
>
0
x
>
0

+

min



E
x
1
w
>
0
x
>
0

E
x
1
w
>
0
x
>
0

>

=

min

E
xx
>
1
w
>
0
x
>
0





E
x
1
w
>
0
x
>
0



2
;
(24)
wherethesecondlinefollowsfromWeyl'sinequality.Toconvenientlycalculatetherequiredmoments,wecan
makeuseofthefollowinglemma,whichreducesthecalculationstothebivariatecase.
Lemma1
Consider
x
=(
x
1
;

;x
N
)
>
˘N
(
0
;
I
)
andlet
g
(
:
):
R
!
R
beareal-valuedfunction.Then,forany
˝xedvectors

;

2
S
N

1
:
E
x
g


>
x

1

>
x
>
0
=
E
x
1
;x
2
g



>


x
1
+
q
1

(

>

)
2
x
2

1
x
1
>
0
:
(25)
Withnolossofgeneralitywecanassume
w
0
2
S
N

1
,andapply
(25)
tothe˝rstright-handsidetermin
(24)
to
get

min

E
xx
>
1
w
>
0
x
>
0

=inf

2
S
N

1
E


>
x

2
1
w
>
0
x
>
0
=inf

2
S
N

1
E
x
1
;x
2



>
w
0

x
1
+
q
1

(

>
w
0
)
2
x
2

2
1
x
1
>
0
=inf

2
S
N

1
1
2


>
w
0

2
+
1
2

1



>
w
0

2

=
1
2
11
Forthesecondtermin(24)wehave



E
x
1
w
>
0
x
>
0



=sup

2
S
N

1
E


>
x

1
w
>
0
x
>
0
=sup

2
S
N

1
E
x
1
;x
2



>
w
0

x
1
+
q
1

(

>
w
0
)
2
x
2

1
x
1
>
0
=sup

2
S
N

1
1
p
2
ˇ


>
w
0

=
1
p
2
ˇ
;
asaresultofwhichonehas

min

cov

x
1
w
>
0
x
>
0


1
=
2

1
=
(2
ˇ
)
.
4.1.2Step2:BoundingtheOrliczNorm
ToboundtheOrlicznormofthecenteredvirtualinputbyaconstant,weonlyneedtointroduceaconstant

suchthatforall

2
S
N

1
themarginals

>
(
x
1
w
>
0
x
>
0

E
x
1
w
>
0
x
>
0
)
obey(15).Tothisend,onehas
8

2
S
N

1
:




>

x
1
w
>
0
x
>
0

E
x
1
w
>
0
x
>
0









>
x
1
w
>
0
x
>
0



+



E
x
1
w
>
0
x
>
0







>
x


+
1
p
2
ˇ
:
Asaresult,for
x
˘N
(
0
;
I
)
andany˝xed

2
S
N

1
:
8
t

0:
P
n




>

x
1
w
>
0
x
>
0

E
x
1
w
>
0
x
>
0




>t
o

P
ˆ



>
x


+
1
p
2
ˇ
>t
˙
=
P
ˆ



>
x


>
max

t

1
p
2
ˇ
;
0


exp
 

1
2
max

t

1
p
2
ˇ
;
0

2
!
;
whereinthelastinequalityweusedthefactthat

>
x
˘N
(0
;
1)
andforastandardnormalvariable
z
,
P
fj
z
j
t
g
exp
(

t
2
=
2)
forall
t

0
.FinallywecanusethebasicinequalitystatedinLemma2oftheproofs
sectiontoget
8
t

0:
P
n




>

x
1
w
>
0
x
>
0

E
x
1
w
>
0
x
>
0




>t
o

exp

1

t
2
2+
1
2
ˇ

;
whichimpliesthat



x
1
w
>
0
x
>
0

E
x
1
w
>
0
x
>
0



 
2
.
1
.
5Net-TrimImplementation
InthissectionwediscussdetailsofanADMMimplementationfortheNet-Trimconvexprogram.Theapproach
thatwesuggesthereisbasedonthe
globalvariableconsensus
(seeof[
3
]).Thistechniqueisusefulin
addressingconvexoptimizationswithadditivelyseparableobjectives.
12
For
W
2
R
N

M
,
X
in
2
R
N

P
,and

f
1
;

;M
gf
1
;

;P
g
theNet-Trimcentralprogram
minimize
W
k
W
k
1
subjectto
8
<
:




W
>
X
in

X
out





F



W
>
X
in


c

V

c
;
(26)
canbecastastheequivalentform
minimize
W
(1)
2
R
M

P
W
(2)
;
W
(3)
2
R
N

M
f
1

W
(1)

+
f
2

W
(2)

subjectto
(
W
(1)
=
W
(3)
>
X
in
W
(2)
=
W
(3)
;
(27)
where
f
1
(
W
)=
I
k
W


X
out

k
F


(
W
)+
I
W

c

V

c
(
W
)
;
and
f
2
(
W
)=
k
W
k
1
:
Here
I
C
(

)
representstheindicatorfunctionoftheset
C
,
I
C
(
W
)=
ˆ
0
W
2
C
+
1
W
=
2
C
:
Fortheconvexprogram
(27)
,theADMMupdateforeachvariableatthe
k
-thiterationfollowsthestandardforms
W
(1)
k
+1
=argmin
W
f
1
(
W
)+
ˆ
2




W
+
U
(1)
k

W
(3)
k
>
X
in




2
F
;
(28)
W
(2)
k
+1
=argmin
W
f
2
(
W
)+
ˆ
2



W
+
U
(2)
k

W
(3)
k



2
F
;
(29)
W
(3)
k
+1
=

X
in
X
in
>
+
I


1

X
in

W
(1)
k
+1
+
U
(1)
k

>
+
W
(2)
k
+1
+
U
(2)
k

;
(30)
andthedualupdatesareperformedvia
U
(1)
k
+1
=
U
(1)
k
+
W
(1)
k
+1

W
(3)
k
+1
>
X
in
;
U
(2)
k
+1
=
U
(2)
k
+
W
(2)
k
+1

W
(3)
k
+1
:
Theupdatestatedin
(30)
isderivedby˝ndingtheminimizeroftheaugmentedLagrangianwithrespectto
W
(3)
,
whichamountstotheminimization
minimize
W
ˆ
2



W
(1)
k
+1
+
U
(1)
k

W
>
X
in



2
F
+
ˆ
2



W
(2)
k
+1
+
U
(2)
k

W



2
F
:
Whiletheupdatesfor
W
(1)
and
W
(2)
,asin
(28)
and
(29)
,arestatedinthegeneralform,theycanbefurther
simpli˝edandpresentedinclosedform.Tothisend,a˝rstobservationisthat
(28)
canbedecoupledinto
independentminimizationsintermsof
W

and
W

c
,i.e.,
W
(1)
k
+1
=argmin
W

:
k
W


X
out

k
F


ˆ
2




W

+

U
(1)
k

W
(3)
k
>
X
in






2
F
+argmin
W

c
:
W

c

V

c
ˆ
2




W

c
+

U
(1)
k

W
(3)
k
>
X
in


c




2
F
:
(31)
13
The˝rstminimizationontheright-handsideof
(31)
isbasicallytheproblemof˝ndingtheclosestpointofan

-radiusEuclideanballtoagivenpoint.Forthenon-trivialcasethatthegivenpointisoutsidetheball,the
solutionistheintersectionoftheballsurfacewiththelineconnectingthepointtothecenteroftheball.More
speci˝cally,for˝xed
Y
and
Z
,
argmin
W

:
k
W


Z

k
F


ˆ
2
k
W


Y

k
2
F
=
(
Y

if
k
Y


Z

k
F


Z

+

Y


Z

k
Y


Z

k
F
else
:
Thesecondtermin(31)isaninstanceofaprojectionontoanorthantandcanbedeliveredinclosedformas
argmin
W

c
:
W

c

V

c
ˆ
2
k
W

c

Y

c
k
2
F
=
Y

c

(
Y

c

V

c
)
+
:
Finally,thesolutionto
(29)
isthestandardsoftthresholdingoperator(e.g.,seeof[
3
]),whichreducesthe
updateto

W
(2)
k
+1

n;m
=
S
1
=ˆ


W
(3)
k

U
(2)
k

n;m

;
where
S
c
(
w
)=
8
<
:
w

cw>c
0
j
w
j
c
w
+
cw<

c
:
Aftercombiningthestepsabove,weproposeAlgorithm2asacomputationalschemetoaddresstheNet-Trim
centralprogram.Theonlycomputationalloadoftheproposedschemeisthelinearsolve
(30)
,forwhichthe
coe˚cientmatrix
X
in
X
in
>
+
I
onlyneedstobecalculatedonce.Asobservable,theprocessingtimeforeach
ADMMstepisrelativelylow,andonlyinvolvesfewmatrixmultiplications.
Algorithm2:
ImplementationoftheNet-TrimCentralProgram
input:
X
in
2
R
N

P
,
X
out
2
R
M

P
,

,
V

,

,
ˆ
initialize
:
U
(1)
;
U
(2)
and
W
(3)
%allinitializationscanbewith
0
C
 
X
in
X
in
>
+
I
while
notconverged
do
Y
 
W
(3)
>
X
in

U
(1)
if


Y


X
out



F


then
W
(1)

 
Y

else
W
(1)

 
X
out

+



Y


X
out




1
F

Y


X
out


endif
W
(1)

c
 
Y

c

(
Y

c

V

c
)
+
W
(2)
 
S
1
=ˆ
(
W
(3)

U
(2)
)
%
S
1
=ˆ
appliestoeachelementofthematrix
W
(3)
 
C

1
(
X
in
(
W
(1)
+
U
(1)
)
>
+
W
(2)
+
U
(2)
)
U
(1)
 
U
(1)
+
W
(1)

W
(3)
>
X
in
U
(2)
 
U
(2)
+
W
(2)

W
(3)
endwhile
return
W
(3)
5.1Net-TrimforConvolutionalLayers
Sincetheconvolutionoperatorislinear,similarstepsastheonesabovecanbetakentoimplementaversion
ofNet-Trimforconvolutionallayersandinputsintheformoftensors.Themaindi˙erenceisaddressingthe
least-squaresupdatein
(30)
,whichcanbeperformedbyincorporatingtheadjointoperator.Thedetailsof
implementingNet-TrimforconvolutionallayersarepresentedinSection8.1oftheSupplementaryMaterials.
14
6ExperimentsandRemarks
Whilethemainpurposeofthispaperisintroducingatheoreticalframeworkforaclassofpruningtechniquesin
deeplearning,webrie˛ypresentsomeexperimentswhichhighlighttheperformanceofNet-Triminreal-world
problems.Duetospacelimitation,mostdetailsofthesimulationsalongwithadditionalexperimentsarepresented
inSection8.2oftheSupplementaryMaterials.AlsoNet-Trimimplementationismadepubliclyavailableonline
5
.
Our˝rstsetofexperimentscorrespondstoacomparisonbetweenthecascadeandparallelframeworks.For
thisexperimentweuseafullyconnected(FC)neuralnetworkofsize
784

300

1000

100

10
(composedof
fourlayers),trainedtoclassifytheMNISTdataset.ThroughoutthesectionwerefertothisnetworkastheFC
model.Whilethetheorysupportsretrainingthenetworkwithnewsamples,inpracticeNet-Trimcanbeapplied
tothedatasetusedtotraintheoriginalnetwork.Inthisexperimentwealsoassessthepossibilityofapplying
Net-Trimtoonlyaportionofthetrainingdata(i.e.,workingwithasubsetofcolumnsin
X
).Clearly,working
withsmaller
X
matricesiscomputationallymoredesirable.
Figure2summarizestheparallelandcascadepruningresults.Aquickcomparisonbetweentherangeof
relativediscrepanciesinpanels(a)and(c)(calculatedas
k
^
X
(
L
)

X
(
L
)
k
F
=
k
X
(
L
)
k
F
)revealsthatformoreor
lesssimilarsparsityrates,cascadeNet-Trimproducesasmalleroveralldiscrepancycomparedtotheparallel
scheme(notetheaxisranges).Thismaybeconsideredasthereturnforgoingthroughanon-distributablescheme.
However,acomparisonofthetestaccuraciesinpanels(b)and(d),andespeciallyforlargervaluesofthesparsity
ratio,showsalesssigni˝cantdi˙erencebetweenthetestaccuraciesofthetwoschemes;speci˝callythatusingthe
parallelschemeanditsdistributablenatureismoredesirableforbigdata.
OurnextsetofexperimentscorrespondstotheapplicationofNet-TrimtotheLeNetconvolutionalnetwork
[
22
]tohighlightitsperformanceagainstwell-establishedmethodsofDropoutand
`
1
regularization.Inthese
experimentsthemeantestaccuracyandinitialmodelsparsityarereportedforthecasesofDropout,
`
1
penalty,
andacombinationofboth.Foreachrun,thetuningparameters(

:thecoe˚cientof
`
1
-penalty,
p
:theDropout
probability,orboth)arevariedinarangeofvaluesandthemeanquantitiesarereported.Itisnoteworthythat
Net-Trimcanalwaysbefollowedbyanoptional˝ne-tuningstep(FT),whichperformsfewtrainingiterationson
theweightsthatNet-Trimhasleftnonzero.TheplotsinFigure3showhowtheapplicationofNet-Trimcan
furthercontributetothesparsityandaccuracyofthenetwork.Forinstancepanel(c)indicatesthatwithouta
lossintheaccuracy,applyingNet-Trimtoanetwork,wherealmost88%oftheweightsareprunedviaDropout
and
`
1
regularization,canelevatethesparsitytoalmost98%.
Anotherwell-knownschemeinmodelpruningisthealgorithmbyHan,Pool,TranandDally(HPTD:[
15
]).
TheHPTDalgorithmisaheuristictoolusedfornetworkcompression,whichtruncatesthesmallweightsacrossa
trainednetworkandperformsanotherroundoftrainingontheactiveweights(sameasthe˝ne-tuningscheme
explainedabove).Figure4presentsacomprehensivecomparisonbetweentheNet-TrimandHPTDontheFC,
LeNet,andaCIFAR-10model.TheinitialCIFAR-10modelusesanaugmentedtrainingsetofsize6.4Msamples,
toretrainwhichNet-Trimuses50Ksamples.OneofthemaindrawbackswiththeHPTDisthetruncationbased
onthemagnitudeoftheweights,whichinmanycasesmaydiscardconnectionstotheimportantfeaturesand
variablesinthenetwork.ThatismainlythereasonthatNet-Trimconsistentlyoutperformsthismethod.In
fact,Net-Trimcanalsopresentvitalinformationaboutthedatastructureandimportantfeaturesthatarenot
immediatelyavailableusingothertechniques.
InFigure5wehavedepictedtheretrained
^
W
1
matrixoftheFCmodelafterapplyingNet-TrimandHPTD.In
panel(b)wecanseemanycolumnsthatarefullyzero.AfterplottingthehistogramoftheMNISTsamples(asin
panel(d)),onewouldimmediatelyobservethatthezerocolumnsin
^
W
1
correspondtotheboundarypixelswith
theleastlevelofinformation.AsHPTDonlyreliesonthetruncationbasedontheweightmagnitudes,despite
5
Toaccessthealgorithmimplementation,visit:
https://dnntoolbox.github.io/Net-Trim/
15
Layer1
Layer2
Layer3
(b)
(a)
Layer4
(d)
(c)
Figure2:RetrainingtheFCnetworkthatisinitiallytrainedwiththefullMNISTtrainingsetandretrainedwith
10K,20K,30Kand55Ksamples(eachcolumncorrespondstoalayer);(a)networkrelativetotaldiscrepancy
(RTD)vsthelayerpercentageofzeros(LPZ)afteraparallelscheme;(b)testaccuracyvsLPZafteraparallel
scheme;(c)RTDvsLPZafteracascadescheme;(d)testaccuracyvsLPZafteracascadescheme;
thesimilarnumberofzerosinpanels(b)and(c),thelatterdoesnothighlightsuchdatastructure.Toobtain
asimilarpatternasinpanel(b),theauthorsin[
15
]suggestaniterativepruningpathwitha˝ne-tuningafter
truncatingaportionofthenetworkweights.However,thisisnotacomputationallye˚cientpathasitrequires
retrainingthenetworkmultipletimes,whichcantakealotoftimeforlargedatasetsandisnotguaranteedto
identifytherightstructures.
16
(a)
(b)
(c)
Figure3:MeantestaccuracyvsmeanmodelsparsityaftertheapplicationofNet-TrimtotheLeNetnetwork
initiallyregularizedvia
`
1
penalty,Dropout,orboth(theregularizationparameterandDropoutprobabilityare
pickedfromarangeofvaluesandthemeanaccuracyandsparsityarereported);(a)amodeltrainedwithDropout
only:
0
:
3

p

0
:
8
;(b)amodeltrainedwith
`
1
penaltyonly:
10

5



5

10

3
;(c)amodeltrainedwith
Dropoutand
`
1
:
10

5



2

10

4
,
0
:
5

p

0
:
75
;
TestAccuracy(%)
PercentageofZeros
TestAccuracy(%)
PercentageofZeros
TestAccuracy(%)
PercentageofZeros
Figure4:ComparisonofNet-TrimandHPTDtestaccuracyvspercentageofzeros,without˝ne-tuning,andwith
˝ne-tuningusing10and30epochs(a)FCmodel,(b)LeNetmodel;(c)CIFAR-10model;
6.1ConcludingRemarks
Net-Trimcanbegeneralizedtoalargeclassofproblems,wherethearchitectureofeachlayerinatrainednetwork
isrestructuredviaaprogramofthetype
minimize
U
2
R
N

M
R
(
U
)
subjectto
˙

U
>
X
in

ˇ
X
out
:
(32)
17
(a)
(b)
(c)
(d)
(e)
Figure5:Instantidenti˝cationofimportantfeaturesusingNet-Trim;(a)samplesfromMNISTdataset;(b)
visualizationof
^
W
1
>
intheFCretrainedmodelusingNet-Trim;(c)similarvisualizationof
^
W
1
>
intheFC
retrainedmodel,usingHPTDwithasingle˝ne-tuningstep;(d)histogramofthepixelvaluesintheMNISTdata
set;(e)thegreenmaskcorrespondingtothezerocolumnsinpanel(b);
Theobjective
R
(

)
aimstopromoteadesiredstructure,andtheconstraintenforcesaconsistencybetweenthe
initialandretrainedmodels.Whileinthispaperwemerelyemphasizedon
R
(
U
)=
k
U
k
1
,avarietyofother
structuresmaybeexploredbyadaptivelyselectingtheobjective.Forinstance,otherthantheRidgeandthe
elasticnetpenaltiesasregularizingtools,choosing
R
(
U
)=
k
U
k
2
;
1
=
P
N
n
=1
k
U
n;
:
k
canpromoteselectionofa
subsetoftherowsin
X
in
,andactasafeatureselectionornode-droppingtoolforeachlayer.Totalvariation,or
rankpenalizingobjectivesmayalsodirectlyapplytonetworkcompressionproblems.
Whileinthispaperwespeci˝callyfocusedon
˙
=
ReLU
(

)
toexploittheconvexformulation,inprincipalother
formsofactivationmaybeexplored.Evenifaconvex(re)formulationissuboptimalornotpossible,powerful
toolsfromnon-convexanalysiswouldstillallowustohaveanunderstandingofwhenandhowwellprograms
oftype
(32)
work.Clearly,thetechniquesusedforsuchtypeofanalysismightbeinitialization-sensitive,and
di˙erentthanthoseusedinthispaper.
Net-Trimcanspeci˝callybecomeausefultoolwhenthenumberoftrainingsamplesislimited.Whileover˝tting
islikelytohappeninthissituation,Net-Trimallowsreducingthecomplexityofthemodels,yetmaintaining
theconsistencywiththeoriginalmodel.Fromadi˙erentperspective,Net-Trimmaysimplifytheprocessof
determiningthenetworksize.Forlargenetworksthataretrainedwithinsu˚cientsamples,employingNet-Trim
canreducethesizeofthemodelstoanordermatchingthedata.
7Proofs
Beforewestartadetailedproofoftheresults,wewouldliketostatetwoinequalitiesthatwillbefrequentlyused
throughoutthissection:
8
X
2
R
d
1

d
2
:


X
+


F
k
X
k
F
;
(33)
8
X
;
Y
2
R
d
1

d
2
:


X
+

Y
+


F
k
X

Y
k
F
:
(34)
18
The˝rstinequalityisstraightforwardtoverify.Toverify(34)wenotethatforall
x;y
2
R
:
x
+
=(
x

y
+
y
)
+

(
x

y
)
+
+
y
+
j
x

y
j
+
y
+
;
whichisinterchangeablein
x
and
y
,andyields
j
x
+

y
+
jj
x

y
j
.
7.1ProofofTheorem1
Thecentralconvexprogram(7)requiresthatfor
=
supp
X
out
:




^
W
>
X
in

X
out





F


and

^
W
>
X
in


c

0
:
(35)
Asthe˝rststep,noticethatfor
~
X
out
=(
^
W
>
X
in
)
+
onehas



~
X
out

X
out



2
F
=




~
X
out

X
out





2
F
+




~
X
out

X
out


c



2
F
=





^
W
>
X
in

+


X
out





2
F
=





^
W
>
X
in

+



X
out

+





2
F





^
W
>
X
in

X
out





2
F


2
;
(36)
wherethe˝rstinequalityisthanksto
(34)
.Nowconsider
^
X
in
beanymatrixsuchthat
k
^
X
in

X
in
k
F


in
,
thenfor
^
X
out
=(
^
W
>
^
X
in
)
+
onehas



^
X
out

X
out



F




^
X
out

~
X
out



F
+



~
X
out

X
out



F






^
W
>
^
X
in

+


^
W
>
X
in

+




F
+





^
W
>

^
X
in

X
in




F
+





^
W



F



^
X
in

X
in



F
+



in
+

(37)
Topresentthelastinequalityweusedthefactthat



^
W



F




^
W



1
k
W
k
1
=1
:
Wemaynowcompletetheproofviaasimpleinduction.Fortheparallelschemesketchedin
(8)
,inequality
(36)
impliesthat
k
^
X
(1)

X
(1)
k
F


1
.Also,
(36)
requiresthat
k
^
X
(
`
)

X
(
`
)
k
F


`
,andassumingthat
k
^
X
(
`

1)

X
(
`

1)
k
F

P
`

1
j
=1

j
,(37)yields



^
X
(
`
)

X
(
`
)



F

`
X
j
=1

j
:
19
7.2ProofofTheorem2
ForthecascadeschemeoutlinedinAlgorithm1,replacingthe
`
indexingwiththe
in=out
notation,thelayer
retrainingtakesplacebyaddressingtheconvexprogram
^
W
=argmin
U
k
U
k
1
subjectto
U
2C


^
X
in
;
X
out
;
W
>
^
X
in

;
(38)
where
^
X
in
istheretrainedmodelinput,
X
out
=(
W
>
X
in
)
+
istheinitiallytrainedmodeloutput,andfor
=
supp
X
out
,

=





W
>
^
X
in

X
out





F
:
Thecentralconvexprogram(38),hencerequiresthat




^
W
>
^
X
in

X
out





F






W
>
^
X
in

X
out





F
;
(39)

^
W
>
^
X
in


c


W
>
^
X
in


c
:
(40)
Fortheoutputoftheinitialandretrainedmodels,onehas



^
X
out

X
out



2
F
=





^
W
>
^
X
in

+


X
out





2
F
+





^
W
>
^
X
in

+

c




2
F
:
(41)
Forthe˝rsttermin(41)thanksto(34)and(39),onehas





^
W
>
^
X
in

+


X
out





2
F
=





^
W
>
^
X
in

+



X
out

+





2
F





^
W
>
^
X
in

X
out





2
F


2




W
>
^
X
in

X
out





2
F
:
(42)
Thesecondtermin(41)canalsobeboundedby





^
W
>
^
X
in

+

c




2
F






W
>
^
X
in

+

c




2
F
=





W
>
^
X
in

+

c


W
>
X
in

+

c




2
F





W
>
^
X
in

W
>
X
in


c



2
F
:
(43)
Using
X
out

=(
W
>
X
in
)

,andapplyingtheresultsof(42)and(43)to(41)yields



^
X
out

X
out



2
F


2




W
>
^
X
in

W
>
X
in





2
F
+




W
>
^
X
in

W
>
X
in


c



2
F


2



W
>

^
X
in

X
in




2
F


2



^
X
in

X
in



2
F
:
(44)
InacascadeNet-Trim,the˝rstlayergoesthroughthestandardretraining
(10)
with

1
=

,forwhichTheorem1
warrants
k
^
X
(1)

X
(1)
k
F


.Ontheotherhand,for
`

2
,
(44)
warrants
k
^
X
(
`
)

X
(
`
)
k
F


`
k
^
X
(
`

1)

X
(
`

1)
k
F
,
whichtogetherwiththediscrepancyofthe˝rstlayeryieldtheadvertisedresultin(14).
20
7.3ProofofTheorem3
Itsu˚cestoshowthefollowingstatements:

If
x
2
R
N
isasubgaussianvector,thenforgiven
W
2
R
N

M
and
b
2
R
M
,therandomvector
y
=
W
>
x
+
b
issubgaussian.

If
x
2
R
N
isasubgaussianvector,
y
=
x
+
isalsosubgaussian.
Westartbyprovingthe˝rststatement.Thesubgaussianityof
x
impliesthatthereexistsaconstant

suchthat
foranygiven

2
S
N

1
:
8
t

0:
P




>
x


>t


c
exp


t
2

2

:
(45)
Nowconsidering

2
S
N

1
wehave



>
y






(
W
)
>
x



+



>
b


=
k
W
k






W
k
W
k

>
x





+



>
b


k
W
k






W
k
W
k

>
x





+
k
b
k
;
whichimmediatelyimpliesthat
8

2
S
N
`

1
:
n
x
:




>

W
>
x
+
b




>t
o

(
x
:
k
W
k






W
k
W
k

>
x





+
k
b
k
>t
)
:
Bythemeasurecomparisonweget
P
n




>

W
>
x
+
b




>t
o

P
(
k
W
k






W
k
W
k

>
x





+
k
b
k
>t
)
=
P
(






W
k
W
k

>
x





>
max

t
k
b
k
k
W
k
;
0

)

c
exp
 

max

t
k
b
k

k
W
k
;
0

2
!
:
UsingLemma2below,for

0
2


2
k
W
k
2
+
k
b
k
2
and
c
0
=
ce
,thefollowingshouldhold:
8
t

0:
P
n




>

W
>
x
+
b




>t
o

c
0
exp


t
2

0
2

;
whichcompletesthe˝rstpartoftheproof.
Lemma2
Fix
a>
0
and
b

0
.Then,for
c
2

a
2
+
b
2
,
8
t

0:exp
 

max

t

b
a
;
0

2
!

exp

1

t
2
c
2

:
(46)
21
Proof:
For
t

b
,theproposedconditionsrequire
1

t
2
=c
2
>
0
,forwhich
(46)
automaticallyholds.Inthecaseof
t>b
,
toestablish(46)itsu˚cestoshowthat

t

b
a

2

t
2
c
2

1
;
orinasimpli˝edform

1

a
2
c
2

t
2

2
bt
+
a
2
+
b
2

0
:
(47)
Thediscriminantofthequadraticexpressionin
(47)
is
4
a
2

(
a
2
+
b
2
)
=c
2

1

,whichisneverpositiveandthe
expressionalwaystakesanidenticalsignto
1

a
2
=c
2
.

Wenextshowthesubgaussianityof
x
+
forasubgaussianrandomvector
x
2
R
N
.Forthispurposewe
introduceaconstant

0
suchthat
E
exp
((

>
x
+
)
2

0
2
)

e
forall

2
S
N

1
.Tothisend,we˝rstboundthe
magnitudeofthemarginalsas
(

>
x
+
)
2
k

k
2
k
x
+
k
2
k
x
k
2
:
(48)
Wenowmakeuseofthefollowinglemmaborrowedfrom[17](seeTheorem2.1andRemark2.3therein).
Lemma3
Let
A
2
R
N

N
beamatrixand

=
A
>
A
.Suppose
x
2
R
N
isarandomvectorsuchthatforsome

2
R
N
and


0
8

2
R
N
:
E
exp


>
(
x


)


exp

k

k
2

2
=
2

;
(49)
thenfor

0
2

2

2
k

k
,
E
exp
 
k
Ax
k
2

0
2
!

exp
 

2
tr
(

)

0

2
+

4
tr


2


0

4
+
k

k
2

0

2
1

2

2
k

k

0

2
!
:
Condition
(49)
istechnicallyacerti˝cateofthesubgaussianityof
x
[
17
,
32
].Setting
A
inLemma3totheidentity
matrixandmakinguseof(48)verifythatfor

2
S
N

1
and

0
2

2

2
,
E
exp

(

>
x
+
)
2

0
2


E
exp
 
k
x
k
2

0
2
!

exp
 
N
2

0

2
+
N
4

0

4
+
k

k
2

0

2
1

2

2

0

2
!
:
(50)
Byselecting

0
su˚cientlylarge,speci˝cally

0
&
max
(
p
N
k

k
)
,onecanupperboundtheright-handside
expressionof(50)by
e
.
7.4ProofofTheorem4
Withreferenceto(20),for
X
in
=[
x
1
;

;
x
P
]
2
R
N

P
and
=

p
:
x
out
p
>
0

=

p
:
x
>
p
w
0
>
0

;
weneedtoderivetheconditionsthat
w

istheuniquesolutionto
min
w
k
w
k
1
subjectto
(
X
in
:
;

>
w
=
x
out

X
in
:
;

c
>
w

0
:
(51)
22
Forgeneral
X
;
X
0
2
R
N

P
and

f
1
;

;P
g
,considertheoperator
T
X
0

X
,
X
diag
(
1

)+
X
0
;
where
1

2
R
P
istheindicatoroftheset

.Simply,
T
0

X
replacescolumnsof
X
indexedby

c
withzerovectors.
Exploitingthenotionofminimumconicsingularvalue,we˝rststateauniqueoptimalityresultfor
(51)
,which
generallyholdsregardlessofthespeci˝cstructureof
X
in
.
Lemma4
Fix

2
R
N
and
˙
2
R
f
0
g
.Consider
w

2
R
N
tobea(sparse)feasiblevectorfor
(51)
,andde˝ne
thedescentcone
D
=
[
˝>
0

y
z

2
R
N
+1
:
k
w

+
˝
y
k
1
k
w

k
1
˙
:
For

=
T


1
>

X
in
,if
inf





>
˙
1

v


:
v
2D\
S
N

>
0
;
(52)
then
w

istheuniquesolutionto
(51)
.
Proof:
Showingthefollowingthreestatementswouldcompletetheproof:
(S.1)
If
w

isfeasiblefor(51),thenthepair
(
w

;˙

1

>
w

)
isfeasiblefortheconvexprogram:
minimize
(
w
;u
)
k
w
k
1
subjectto


>
˙
1


w
u

=

x
out

0

:
(53)
(S.2)
Foranypair
(
w

;u

)
thatisfeasiblefor
(53)
,ifcondition
(52)
holds,then
(
w

;u

)
istheuniquesolutionto
(53).
(S.3)
If
(
w

;˙

1

>
w

)
istheuniquesolutionto(53),then
w

istheuniquesolutionto(51).
Basedonthede˝nition

=
T


1
>

X
in
,verifying(S.1)istrivial.Claim(S.2)isadirectapplicationofthe
minimumconicsingularvalueresult(e.g.,seeProp.2.2of[
7
],orProp.2.6of[
30
]).Toprove(S.3),supposeunder
theproposedassumption,
(51)
hasadi˙erentsolution
^
w
,where
k
^
w
k
1
k
w

k
1
.Then(S.1)requires
(
^
w
;˙

1

>
^
w
)
tobefeasiblefor
(53)
.Howevertheobjectiveforthisfeasiblepointislessthan
k
w

k
1
,whichisincontradiction
with
(
w

;˙

1

>
w

)
beingtheuniquesolutionto(53).

UsingLemma4andthebowlingschemesketchedin[
30
],wecontinuewithlower-boundingtheminimumconic
singularvalueawayfromzero,andrelatingtheconditionstothenumberofsamples,
P
.
Tothisend,wemaylookintothestructureofthematrix

inLemma4asbeingpopulatedwithindependent
copiesof
x
1
w
>
0
x
>
0


asthecolumns,andexploittheindependencerequiredforthebowlingscheme.Toassure
centeredcolumns,wechoose

=
E
x
1
w
>
0
x
>
0
,makingcolumnsof

=[
'
1
;

;
'
P
]
independentcopiesofthe
centeredsubgaussian
6
randomvector
'
,
x
1
w
>
0
x
>
0

E
x
1
w
>
0
x
>
0
:
Forreasonsthatbecomeapparentlaterintheproof,ourarbitrarychoiceof
˙
inLemma4isnarrowedto
˙
0
,
p
2
k
'
k
 
2
:
Inarandomsetting,tolower-boundtheminimumconicsingularvalue,weadaptthefollowingresultfrom[
30
],
Prop.5.1(orseeTheorem5.4of[24]fortheoriginalstatement).
6
Since
j

>
x
1
w
>
0
x
>
0
jj

>
x
j
andfor
t

0
,
P
fj

>
x
1
w
>
0
x
>
0
j
>t
g
P
fj

>
x
j
>t
g
,
(15)
con˝rmsthat
x
beingsubgaussian
implies
x
1
w
>
0
x
>
0
tobesubgaussian.
23
Theorem5
Fixaset
E
ˆ
R
d
.Let
˚
bearandomvectoron
R
d
,andlet
˚
1
;

;
˚
P
beindependentcopiesof
˚
.
For
˘

0
,supposethemarginaltailrelationbelowholds:
inf
v
2
E
P
n



˚
>
v




˘
o

C
˘
>
0
:
Let
""
1
;

;""
P
beindependentRademacherrandomvariables,independentfromeverythingelse,andde˝nethe
meanempiricalwidthoftheset
E
:
W
P
(
E
;
˚
)
,
E
sup
v
2
E
h
h
;
v
i
;
where
h
=
1
p
P
P
X
p
=1
""
p
˚
p
:
(54)
Then,forany
˘>
0
and
t>
0
,withprobabilityatleast
1

exp(

t
2
=
2)
:
inf
v
2
E
 
P
X
p
=1

˚
>
p
v

2
!
1
2

˘
2
C
˘
p
P

2
W
P
(
E
;
˚
)

˘
2
t:
(55)
Foramorecompact(andinline)notation,weusethefollowingnotationfortheconcatenationofavector
w
andascalar
u
,
w
_
u
,

w
u

:
Also,foragivenobjectiveandpoint
v
0
2
R
d
,wedenotethedescentconeby
D
v
(
f
(
v
);
v
0
)=
[
˝>
0

y
2
R
d
:
f
(
v
0
+
˝
y
)

f
(
v
0
)

:
Toshowthatcondition
(52)
holdsfortheprescribed
s
-sparsevector
w

,wewillshowthatforsu˚cientlylarge
P
,
theright-handsideexpressionin
(55)
canbeboundedawayfromzero.ToapplyTheorem5toourproblemin
(52),therandomvector
˚
andtheset
E
toconsiderare
˚
=
'
_
˙
0
;
and
E
=
D
w
_
u
(
k
w
k
1
;
w

_
u

)
\
S
N
;
where
D
w
_
u
(
k
w
k
1
;
w

_
u

)=
[
˝>
0

y
_
z
2
R
N
+1
:
k
w

+
˝
y
k
1
k
w

k
1

=
D
w
(
k
w
k
1
;
w

)

R
;
and
u

=
˙

1
0

E
x
1
w
>
0
x
>
0

>
w

.Notethatintheformulationabove,
D
w
_
u
(
:
;
:
)
ˆ
R
N
+1
,while
D
w
(
:
;
:
)
ˆ
R
N
.
Theremainderoftheprooffocusesonboundingthecontributingtermsontheright-handsideexpressionof
(55)
.
7.4.1BoundingtheMeanEmpiricalWidth
Inthissectionoftheproof,weaimtoupper-bound
W
P

D
w
_
u
(
k
w
k
1
;
w

_
u

)
\
S
N
;
'
_
˙
0

;
24
wherefollowingtheformulationin(54)wehave
h
=
1
p
P
P
X
p
=1
""
p
'
_
p
˙
0
=
 
0
˙
0
p
P
P
P
p
=1
""
p
!
|
{z
}
h
u
+
 
1
p
P
P
P
p
=1
""
p
'
p
0
!
|
{z
}
h
_
w
0
:
Usingthecompactnotations
K
w

;u

=
D
w
_
u
(
k
w
k
1
;
w

_
u

)
;
and
K
w

=
D
w
(
k
w
k
1
;
w

)
;
oneshas
W
P

K
w

;u

\
S
N
;
'
_
˙
0

=
E
sup
v
2K
w

;u

\
S
N
h
h
;
v
i

E
sup
v
2K
w

;u

\
S
N
h
h
u
;
v
i
+
E
sup
v
2K
w

;u

\
S
N
h
h
_
w
0
;
v
i
:
(56)
Forthe˝rsttermin(56)notethat
E
sup
w
_
u
2K
w

;u

\
S
n
h
h
u
;
w
_
u
i
=
E
sup
w
_
u
2K
w

;u

\
S
N
 
˙
0
p
P
P
X
p
=1
""
p
!
u
=
E





˙
0
p
P
P
X
p
=1
""
p






˙
0
p
P
0
@
E
 
P
X
p
=1
""
p
!
2
1
A
1
2
=
˙
0
:
(57)
Toboundthesecondtermin(56),weproceedby˝rstshowingthatforany˝xed
h
w
2
R
N
,
sup
w
_
u
2K
w

;u

\
S
N
h
h
_
w
0
;
w
_
u
i
=sup
w
2K
w

\
S
N

1
h
h
w
;
w
i
:
(58)
Forthispurposeonlythefollowingtwocasesneedtobeconsidered:

case1:
h
h
w
;
w
i
0
;
8
w
2K
w

:
Inthiscasethesupremumvalueforbothsidesof
(58)
iszero,whichmaybeattainedbypicking
w
=
0
and
u
=1
.

case2:
9
w
2K
w

,suchthat
h
h
w
;
w
i
>
0
:
Toshowtheequalityinthiscase,weonlyneedtoshowthatif
^
v
=
^
w
_
^
u
isapointatwhichthe(positive)
supremumisattained,i.e.,
sup
v
2K
w

;u

\
S
N
h
h
_
w
0
;
v
i
=
h
h
_
w
0
;
^
v
i
=
h
h
w
;
^
w
i
;
thenwemusthave
^
u
=0
.If
^
u
6
=0
,thenthecondition
^
v
2
S
N
requiresthat
k
^
w
k
<
1
.Inthiscasethealternative
feasiblepoint
~
v
=
k
^
w
k

1
^
w
_
0
producesagreaterinnerproduct:
h
h
_
w
0
;
~
v
i
=
1
k
^
w
k
h
h
w
;
^
w
i
>
h
h
w
;
^
w
i
;
25
whichcannotbepossible.Therefore
^
u
=0
,andforbothsidesof
(58)
thesupremumvalueis
h
h
w
;
^
w
i
.Combining
cases1and2establishestheclaimin(58).
Now,employing(58)and(57)in(56)certi˝esthatfor
h
w
=
1
p
P
P
X
p
=1
""
p
'
p
;
andsomeabsoluteconstant
C
,onehas
W
P

K
w

;u

\
S
N
;
'
_
˙
0


˙
0
+
E
sup
w
2K
w

\
S
N

1
h
h
w
;
w
i

C
k
'
k
 
2
 
s
s
log

N
s

+
s
+1
!
:
(59)
Thelastlinein
(59)
isthankstothefollowinginequality(seeof[
30
]),whichrelatesthemeanempiricalwidth
ofacenteredsubgaussianrandomvector
'
totheGaussianwidth:
E
sup
w
2K
w

\
S
N

1
h
h
w
;
w
i
.
k
'
k
 
2
E
sup
w
2K
w

\
S
N

1
g
˘N
(
0
;
I
)
h
g
;
w
i
.
k
'
k
 
2
s
s
log

N
s

+
s:
7.4.2RelatingtheMarginalTailBoundandtheVirtualCovariance
Asthenextstepinlower-boundingtheright-handsideexpressionin(55),notingthat
inf
v
2K
w

;u

\
S
N
P



v
>
'
_
˙
0



˘


inf
v
2
S
N
P



v
>
'
_
˙
0



˘

;
(60)
inthissectionwefocusonlowerboundingtheright-handsideexpressionin
(60)
intermsof
k
'
k
 
2
andthe
minimumeigenvalueofthevirtualcovariancematrix.Tothisend,usingthenotation
~

min
,

min
(cov(
˛
))=

min

E
''
>

;
onehas
E
'
_
˙
0
'
_
˙
0
>
=

E
''
>
0
0
>
˙
2
0


min

~

min
;˙
2
0

I
:
(61)
Ontheotherhand,fromthesubgaussianpropertiesof
'
wehave
k
'
k
 
2

sup
w
2
S
N

1
2

1
2

E


w
>
'


2

1
2

inf
w
2
S
N

1
2

1
2

E


w
>
'


2

1
2
=
s
~

min
2
;
whichsimplyimpliesthat
˙
2
0

~

min
andcombiningwith(61)yields
inf
v
2
S
N
E


v
>
'
_
˙
0


2

min

~

min
;˙
2
0

=
~

min
:
(62)
26
Consideringapositiverandomvariable
˜
anda˝xed
˘

0
,wecanderiveavariantofthePaley-Zygmund
inequalitybywriting
˜
2
=
˜
2
1
f
˜
2
<˘
2
g
+
˜
2
1
f
˜
2

˘
2
g
,whichusingtheHölder'sinequalitynaturallyyields
E
˜
2

˘
2
+(
P
f
˜

˘
g
)

1+


E
˜
2(1+

)

1
1+

;>
0
:
Subsequently,selecting
˘
2
[0
;
p
~

min
]
warrantsthat
8
v
2
S
N
:
P



v
>
'
_
˙
0



˘


0
B
@
~

min

˘
2

E
j
v
>
'
_
˙
0
j
2(1+

)

1

+1
1
C
A
1+
1

:
Wecanalsousethesubgaussianpropertiesof
'
toboundthedenominatorasfollows
8
w
_
u
2
S
N
;

1:

E


w
_
u
>
'
_
˙
0




1

=

E


w
>
'
+
˙
0
u




1



E


w
>
'




1

+
˙
0
j
u
j
=
k
w
k

E




w
>
k
w
k
'






1

+
˙
0
j
u
j

p

k
'
k
 
2
k
w
k
+
p
2
k
'
k
 
2
j
u
j

p

+2
k
'
k
 
2
;
wherethe˝rstinequalityisadirectapplicationoftheMinkowskiinequality,thesecondinequalityusesthe
subgaussiande˝nition
(17)
andthelastboundisthankstotheCauchy-Schwarzinequality.Asaresultfor
˘
2
[0
;
p
~

min
]
inf
v
2
S
N
P



v
>
'
_
˙
0



˘


 
~

min

˘
2
2(2+

)
k
'
k
2
 
2
!
1+
1

:
(63)
7.4.3CombiningtheBounds
Wecannowcombinethebounds
(59)
and
(63)
,anduseTheorem5tostatethatwithprobabilityatleast
1

exp(

t
2
=
2)
:
inf
v
2K
w

;u

\
S
N
 
P
X
p
=1

'
_
p
˙
>
0
v

2
!
1
2

˘
2
p
P
 
~

min

˘
2
2(2+

)
k
'
k
2
 
2
!
1+
1


2
C
k
'
k
 
2
 
s
s
log

N
s

+
s
+1
!

˘
2
t:
Selecting
˘
=
p
~

min
=
3
wouldboundtheexpressionaboveawayfromzero,aslongas
P

36

9

1+

2

k
'
k
2
 
2

2+
2

~

min

2
~

min

2+
2

 
2
C
k
'
k
 
2
 
s
s
log

N
s

+
s
+1
!
+
p
~

min
6
t
!
2
:
(64)
27
Notingthat
~

min

2
k
'
k
2
 
2
andusingthebasicinequality
(
a
+
b
)
2

2
a
2
+2
b
2
twiceyields
 
2
C
k
'
k
 
2
 
s
s
log

N
s

+
s
+1
!
+
p
~

min
6
t
!
2
.
k
'
k
2
 
2

s
log

N
s

+
s
+1+
t
2
72
C
2

:
Also,since
(1+

2
)
2

.
1
for


1
,onehas
36

9

1+

2

k
'
k
2
 
2

2+
2

~

min

2
~

min

2+
2

.

1+

2

2
k
'
k
4+
4

 
2
~

3+
2

min
:
Therefore,thedesiredconditionin(52)holds,aslongas
P
&

1+

2

2
k
'
k
6+
4

 
2
~

3+
2

min

s
log

N
s

+
s
+1+
t
2
72
C
2

:
Finally,setting

0
=
=
2
and
t
0
=
t
2
=
(72
C
2
)
yieldstheadvertisedclaimin(21).
7.5ProofofLemma1
Wefollowasimilarlineofargumentasof[23].Toevaluate
I
=
E
x
g


>
x

1

>
x
>
0
=
1
(2
ˇ
)
N
2
Z

>
x
>
0
g


>
x

exp
 

k
x
k
2
2
!
d
x
;
weassumethat

and

arenotaligned(forthealignedcaseasimilarprocedureappliestomerely

).We
considertheunitarymatrix
E
=[
e
1
;

;
e
N
]
,where
e
1
=

;
e
2
=




>



r
1



>


2
;
and
e
3
;

e
N
areanycompletionoftheortho-basis.Setting
x
=
Ez
yields

>
x
=
z
1
and

>
x
=


>


z
1
+
r
1



>


2
z
2
:
Takingintoaccounttheinjectivityofthelinearmap
E
,wecanreformulatetheintegralinthe
z
-domainas(see
Theorem263Dof[9]fortheformalstatement)
I
=
1
2
ˇ
Z
z
1
>
0
g
 


>


z
1
+
r
1



>


2
z
2
!
exp


z
2
1
+
z
2
2
2

d
z
1
d
z
2
:
Acknowledgement:
A.AghasiwouldliketothankRomanVershyninandRichardKuengfortheinsightful
suggestionsandcommunications.
28
8SupplementaryMaterials
8.1Net-TrimforConvolutionalLayers
When
X
in
2
T
in
,
W
2
T
w
and
X
out
2
T
out
aretensors,and

indicatesasubsetofthetensorelements,our
centralprogramtakesthefollowingform:
minimize
W
k
W
k
1
subjectto
(
k
(
A
X
in
(
W
)

X
out
)

k
F


(
A
X
in
(
W
))

c

V

c
;
(65)
where
kk
1
and
kk
F
naturallyapplytothevectorizedtensors.Asbefore,foragiventensor
Z
,
Z

isatensorof
similarsize,whichtakesidenticalvaluesas
Z
on

andzerovalueson

c
.
Theoperator
A
X
in
:
T
w
!
T
out
isalinearoperatorthatisparameterizedby
X
in
.Forinstanceinconvolutional
layersitisatensorconvolutionoperatorwithoneoftheoperandsbeing
X
in
.Throughoutthetextweassume
that
A

X
in
:
T
out
!
T
w
istheadjointoperator.Theadjointoperatorneedstosatisfythefollowingproperty:
8
W
2
T
w
;
8
Z
2
T
out
:
hA
X
in
(
W
)
;
Z
i
T
out
=
h
W
;
A

X
in
(
Z
)
i
T
w
:
GoingthroughanidenticallineofargumentasSection5yieldssimilarADMMsteps,onlydi˙erentintheway
that
X
in
interactswith
W
(3)
.Morespeci˝cally,
W
(1)
k
+1
=argmin
W
f
1
(
W
)+
ˆ
2



W
+
U
(1)
k
A
X
in

W
(3)
k




2
F
;
(66)
W
(2)
k
+1
=argmin
W
f
2
(
W
)+
ˆ
2



W
+
U
(2)
k

W
(3)
k



2
F
;
(67)
W
(3)
k
+1
=argmin
W
ˆ
2



W
(1)
k
+1
+
U
(1)
k
A
X
in
(
W
)



2
F
+
ˆ
2



W
(2)
k
+1
+
U
(2)
k

W



2
F
;
(68)
andthedualupdatesareperformedvia
U
(1)
k
+1
=
U
(1)
k
+
W
(1)
k
+1
A
X
in

W
(3)
k

;
U
(2)
k
+1
=
U
(2)
k
+
W
(2)
k
+1

W
(3)
k
+1
:
Basedonthestepsabove,thefollowingalgorithmisastraightforwardmodi˝cationoftheoriginalNet-Trim
29
implementationpresentedinoperatorform.
Algorithm3:
ImplementationoftheNet-TrimforConvolutionalLayers
input:
X
in
2
T
in
,
X
out
2
T
out
,

,
V

,

,
ˆ
initialize
:
U
(1)
2
T
out
;
U
(2)
2
T
w
and
W
(3)
2
T
w
%allinitializationscanbewith
0
while
notconverged
do
Y
 A
X
in

W
(3)


U
(1)
if


Y


X
out



F


then
W
(1)

 
Y

else
W
(1)

 
X
out

+



Y


X
out




1
F

Y


X
out


endif
W
(1)

c
 
Y

c

(
Y

c

V

c
)
+
W
(2)
 
S
1
=ˆ
(
W
(3)

U
(2)
)
%
S
1
=ˆ
appliestoeachelementofthematrix
W
(3)
 
argmin
W
1
2



A
X
in
(
W
)


W
(1)
+
U
(1)




2
F
+
1
2



W


W
(2)
+
U
(2)




2
F
U
(1)
 
U
(1)
+
W
(1)
A
X
in

W
(3)

U
(2)
 
U
(2)
+
W
(2)

W
(3)
endwhile
return
W
(3)
Theonlyundiscussedpartinthisalgorithmistheupdatefor
W
(3)
,whichweaddressusinganoperatorformof
theconjugategradientalgorithm.
8.1.1LeastSquaresUpdateUsinganOperatorConjugateGradient
Inthissectionweaddresstheminimization
minimize
W
2
T
w
1
2
kA
X
in
(
W
)

B
k
2
F
+
1
2
k
W

C
k
2
F
;
(69)
whichiscentraltothe
W
(3)
updatein
(68)
.Here
B
2
T
out
and
C
2
T
w
aretensors,and
A
X
in
:
T
w
!
T
out
is.
Theminimizerto(69)canbefoundbytakingaderivativeandsettingittozero,i.e.,
A

X
in
(
A
X
in
(
W
)

B
)+
W

C
=
0
;
(70)
or
A

X
in
(
A
X
in
(
W
))+
W
=
A

X
in
(
B
)+
C
:
(71)
Solving
(71)
for
W
ise˚cientlypossibleviathemethodofconjugategradient.Thefollowingalgorithmoutlines
theprocessofsolving
(71)
,whichisavariantoftheoriginalCGalgorithm(e.g.,see[
1
])reformulatedinoperator
30
form.
Algorithm4:
LeastSquaresUpdateintheNet-TrimUsingConjugateGradient
initialize
:
W
0
=
0
;
R
0
=
A

X
in
(
B
)+
C
;
P
0
=
R
0
for
k
=1
;:::;K
max
do
T
k

1
=
A
X
in
(
P
k

1
)

k
=
k
R
k

1
k
2
F
k
T
k

1
k
2
F
+
k
P
k

1
k
2
F
W
k
=
W
k

1
+

k
P
k

1
R
k
=
R
k

1


k

A

X
in
(
T
k

1
)+
P
k

1


k
=
k
R
k
k
2
F
k
R
k

1
k
2
F
P
k
=
R
k
+

k
P
k

1
endfor
Output:
W
K
max
8.2Experiments
Inthissection,wepresentmoredetailsoftheexperimentalsetupandprovideadditionalsimulationswhichwere
excludedfromtheoriginalmanuscriptduetospacelimitation.
Inour˝rstsetofexperiments,wepresentedacomparisonbetweenthecascadeandparallelframeworks.As
stated,forthispurposeweusetheFCnetworkofsize
784

300

1000

100

10
(composedoffourlayers:
W
1
2
R
784

300
;
W
2
2
R
300

1000
,etc),trainedtoclassifytheMNISTdataset.Panels(a)and(b)inFigure2of
thepapersummarizetheoutcomeofapplyingtheNet-TrimparallelschemetothetrainedFCmodel.Byvarying
thevalueof

,onemayexploredi˙erentlevelsoflayersparsityanddiscrepancy.Panel(a)reportstherelative
valueoftheoveralldiscrepancyasafunctionoftherelativesparsityateachlayer(i.e.,percentageofzerosin
^
W
`
).Eachplotisobtainedbyvarying

forarangeofvaluesandretrainingtheFCmodelwith10K,20K,30K
andtheentire55Ktrainingsamples.Asexpected,allowingmorediscrepancyimprovesthelevelofsparsity.Since
practicallytheoveralldiscrepancyisnotagoodindicationofthechangesinthemodelaccuracy,inpanel(b)we
replaceitwiththetestaccuracyoftheretrainedmodels.Aninterestingobservationisthatretrainingthemodels
withfewersamplesdoesnotsigni˝cantlydegradethetestaccuraciesandeveninsomecases(e.g.,30Kversus
55K)itcausesaslightimprovementintheaccuracyoftheretrainedmodels.Panels(c)and(d)reportasimilar
setofexperimentsforthecascadeNet-Trim,whereincreasingthein˛ationrateawayfromoneallowsproducing
sparsernetworks.
Employingtheparallelscheme(thankstoitsdistributablenature),andtheuseofasubsetofthetraining
dataintheNet-Trimretrainingprocessarebothcomputationallyattractivepaths,andtheexperimentsinFigure
2indicatethatatleastforareasonablesparsityrange,theycouldbebothexploredwithoutmuchdegradationof
themodelaccuracies.Intheremainderoftheexperimentsinthissection,wewillconsistentlyusetheparallel
schemeforourretrainingpurposes,andwillnomorereferencetotheNet-Trimparallelorcascadenature.
Inthenextsetofexperiments,weinvestigatetheadditionalpruningthatNet-Trimbringstothearchitecture
ofneuralnetworksbeyondDropoutand
`
1
regularization.Forthispurposeweconsidertheapplicationofan
`
1
regularization,DropoutandacombinationofbothtothetrainingofourstandardFCmodel.Wealsoapplya
similarsetoftoolstotheLeNetconvolutionalnetwork[
22
],whichiscomposedoftwoconvolutionallayers(32
˝ltersofsize
5

5
atthe˝rstlayer,and64˝ltersofsimilarsizeatthesecondlayer,bothfollowedby
2

2
max
poolingunits),andtwofullyconnectedlayers(
3136

512

10
).Whilethelinearityoftheconvolutionoperator
immediatelyallowstheapplicationofNet-Trim,inourexperimentsweomitretrainingtheconvolutionallayersas
31
thenumberofparametersinsuchlayersismuchlessthanthefullyconnectedlayers.
Forbothnetworkarchitectureswevary

(the
`
1
penalty),and
p
(theDropoutprobabilityofkeeping)ina
rangeofvaluesthattendtoproducereasonablyhightestaccuracies.ThestatisticsreportedinTable1correspond
totheFCandLeNetmodels,whichresultedinthehighesttestaccuracies.Forbotharchitecturesthebestresults
Table1:ApplicationofNet-Trimfordi˙erentvaluesof

tothestandard(FC)andconvolutional(LeNet)networks
trainedviacarefulchoiceofthe
`
1
regularizationandDropoutprobability(

=10

5
,
p
=0
:
75
fortheFCmodel,
and

=10

5
,
p
=0
:
5
fortheLeNetarchitecture);improvedquantitiescomparedtotheinitialmodelsare
highlightedinbold
FC
LeNet
Network
TestAcc.
TestAcc.
Network
TestAcc.
TestAcc.
Zeros(
%
)
(NoFT)
WithFT
Zeros(
%
)
(NoFT)
WithFT
InitialModel
43.69
98.65

33.65
99.57

Net-Trim

=0
:
01
71.93
98.65
98.76
83.80
99.59
99.60

=0
:
02
76.13
98.65
98.72
88.76
99.60
99.57

=0
:
04
80.02
98.56
98.66
92.75
99.54
99.53

=0
:
06
81.98
98.54
98.59
94.46
99.49
99.47

=0
:
08
83.34
98.36
98.48
95.40
99.35
99.44

=0
:
1
84.30
98.08
98.38
96.01
98.26
99.35

=0
:
2
86.99
96.76
97.88
97.37
98.83
99.22

=0
:
3
88.61
94.69
97.31
97.89
98.61
99.07
happenedwhentheDropoutand
`
1
regularizationwereappliedsimultaneously.The˝rstrowreportstheinitial
modelstatisticsandthesubsequentrowscorrespondtotheapplicationoftheNet-Trimusingdi˙erentvalues
of

.Inthisexperimentthethirdcolumnofeacharchitecturesectioncorrespondstoanadditional˝ne-tuning
stepafterNet-Trimprunesthenetwork.This(optional)stepusestheNet-Trimsolutionasaninitializationfora
secondarytraining,whichonlyappliestothenon-zeroweightsidenti˝edbytheNet-Trim.Such˝ne-tuningoften
resultsinanimprovementinthegeneralizationerrorwithoutchangingthesparsityofthenetwork.
AquickassessmentofTable1revealsthatapplyingNet-Trimcansigni˝cantlyimprovethesparsity(andeven
atthesametimetheaccuracy)ofthemodels.Forinstance,intheFCmodelwecanimprovethetestaccuracyto
98.76%,andatthesametimeincreasethepercentageofnetworkzerosfrom43.69%to71.93%.Asimilartrend
holdsfortheLeNetmodel.Ifweallowsomedegradationinthetestaccuracy,thepercentageofzeroscanbe
signi˝cantlyincreasedto88.61%intheFCmodel,and97.89%intheLeNetarchitecture.
Table1onlyreportstheNet-Trimperformanceonthemostaccuratemodels.InFigure3ofthepaperwe
presentedamorecomprehensivesetofexperimentsontheLeNetNetwork.Figure6showsasimilarsetof
experimentsontheFCnetwork.Intheseexperimentsthemeantestaccuracyandinitialmodelsparsityare
reportedforthecasesofDropout,
`
1
regularization,andacombinationofboth.Foreachsetupthetuning
parameters(

,
p
,orboth)arevariedinarangeofvaluesandunlikeTable1,themeanquantitiesarereported.
Forinstance,panel(a)indicatesthatapplyingtheDropouttotheFCmodelwith
0
:
3

p

0
:
8
yieldsanaverage
networkzeropercentageof
0
:
07%
,andapproximately97.5%testaccuracy.However,applyingNet-Trimalong
withtheFTstep,canelevatetheaverageaccuracytoaround98%,andatthesametimeincreasethenetwork
percentageofzerostoalmost45%.Theplotalsorevealsthatwithnolossinthemodelaccuracies,wecanimprove
thesparsityofthemodelstoupto56%(correspondingtothepointwheretheredandthedashedlinesintersect).
Anassessmentofallpanels(speci˝callythecrossingoftheredcurvesandthedashedlines)revealsthatinall
threescenarios(Dropout,
`
1
regularizationandacombinationofboth),andforbotharchitectures(FCandLeNet),
anadditionalapplicationofNet-Trimcanimprovethemodelsbothintermsofaccuracyandthenumberof
underlyingparameters.Evenincasesthattheaccuracyisdegradedtosomeextent,butthemodelissigni˝cantly
pruned,theprunednetworkmaybeconsideredamorereliablemodel.InFigure
(7)
wehavedemonstratedthe
32
(a)
(b)
(c)
Figure6:MeantestaccuracyvsmeanmodelsparsityaftertheapplicationofNet-TrimforFCnetworkinitially
regularizedvia
`
1
penalty,Dropout,orboth(theregularizationparameterandDropoutprobabilityarepicked
fromarangeofvaluesandmeanquantitiesarereported);(a)amodeltrainedwithDropoutonly:
0
:
3

p

0
:
8
;
(b)amodeltrainedwith
`
1
penaltyonly:
10

5



5

10

3
;(c)amodeltrainedwithDropoutand
`
1
:
10

5



2

10

4
,
0
:
5

p

0
:
75
;
FCandLeNetmodelsinitiallytrainedwithDropoutandretrainedusingNet-Trim.Despiteanaccuracylossof
1.3%fortheFCmodel,and1.7%fortheLeNetmodel,thepercentageofzeroshavebeenincreasedto63.32%and
96.8%,respectively.Asaresultofthisreduction,whenthemodelsaretestedwithdi˙erentnoisyversionsofthe
originaltestset,thereducedmodelsexhibitaloweraccuracydegradation(i.e.,morerobustness)tothenoise
increase.
(a)
(b)
Figure7:Noiserobustnessofinitialandretrainednetworks;(a)FC;(b)LeNet
ThankstothesimpleimplementationofNet-Trim,intheaforementionedexperiments,theretrainingofthe
layermatriceswasonlyinorderoffewminutesonastandarddesktopcomputer,whileinthemajorityofthe
33
cases,theinitialtrainingofthenetworkstookmuchlonger.Wewouldliketonotethatwedidnotmakeany
e˙ortstooptimizetheNet-Trimcodeandfullyexploittheparallelfeatures(e.g.,matrixproducts,processingof
layersinparallel,etc).Thedistributablenatureofourimplementationsupportsyetmuchfastersoftwarethan
theonecurrentlypresent.
InthepaperwealsocomparedNet-TrimwiththeHPTD[
15
].TheHPTDalgorithmdoesnotcomewithany
performanceguarantees,however,thebasicimplementationideahasmadeitawidespreadtoolinthenetwork
compressioncommunity.UsingtoolssuchasquantizationandHu˙mancoding,moreadvancedframeworkssuch
astheDeepCompression[
13
]havebeendevelopedlater.However,theirfocusismainlycompressingthenetwork
parametersonthememory,andHPTDpruningschemeisyetthemostrelevantsingle-moduleframeworkthat
couldbecomparedwithNet-Trim.
WithreferencetoFigure4ofthepaper,Figure8presentsmoredetailsofthecomparisonbetweentheNet-Trim
andHPTDontheFCandLeNetmodels.FortheNet-Trimweusedi˙erentvaluesof

toprunethetrained
TestAccuracy(%)
PercentageofZeros
(a)
TestAccuracy(%)
Fine-TuningEpoch

PercentageofZeros
TestAccuracy(%)
PercentageofZeros
(b)
TestAccuracy(%)
Fine-TuningEpoch

1
PercentageofZeros
Figure8:ComparisonofNet-TrimandHPTDindi˙erentsettingsfor(a)FCmodel,(b)LeNetmodel;the
leftpanelscompareNet-TrimandHPTDtestaccuracyvspercentageofzeros,without˝ne-tuning,andwith
˝ne-tuningusing10and30epochs;middlepanelsshowthenumberof˝ne-tuningepochsandtheacquired
accuracyusingNet-TrimandHPTD;therightpanelsindicatethepercentageofzerosasafunction

forNet-Trim
34
networks.TocomparethemethodwiththeHPTD,aftereachapplicationoftheNet-Trimandcountingthe
numberofzeros,thesamenumberofelementsaretruncatedfromtheinitialnetworktobeusedfortheHPTD
implementation.HPTDisfollowedbya˝ne-tuningstepafterthetruncation,whichisalsoanoptionaltaskfor
Net-Trim.Nevertheless,bothalgorithmsarecomparedwithout˝ne-tuning,orwith˝ne-tuningusing10or30
epochs.Theleftplotsinpanels(a)and(b)showthatinallscenariosNet-TrimoutperformsHPTDingenerating
moreaccuratemodelswhenthelevelsofsparsityarematched.Themiddleplotsalsoshowtheimprovements
intheaccuracyasafunctionofthenumberofepochsrequiredinthe˝ne-tuningprocessforthetwoschemes.
Inbothscenarios,Net-Trimrequiresonlyfewepochstoachievethetopaccuracy,whileachievingsuchlevelof
accuracyfortheHPTDiseithernotfeasibleortakesmany˝ne-tuningepochs.
Figure9demonstratesanothersetofcomparativeexperimentsbetweenNet-TrimandHPTD,performedona
muchlargeraugmenteddataset.ThereferencetrainingsetistheCIFAR10color-imagedatabase,whichcontains
50Ksamplesofsize
32

32
fromtenclasses[20,21].
Inordertoobtainhighertestaccuracies,thetrainingimagesaremultiplicatedbytaking
24

24
windowsto
randomlycropthem,andeachcroppedimageishorizontally˛ippedwithprobability
0
:
5
.Thisprocessaugments
thetrainingsetto6,400,000samples.Theneuralnetworkemployedtoaddresstheinitialclassi˝cationproblemis
convolutional,wherethe˝rstlayerofthetrainednetworkuses64˝ltersofsize
5

5

3
,followedbyamaxpooling
unit(size:
3

3
,stride:
2

2
).Thesecondlayerisalsoconvolutionalwith64˝ltersofsize
5

5

64
andasimilar
maxpoolingunit.Theremainderofthenetworkcontainsthreefullyconnectedlayers(
3136

384

192

10
).
ForthisrelativelylargedatasetwealsogothroughtheexerciseofretrainingtheNet-Trimwithonlypart
ofthetrainingsamples,speci˝cally25K,50Kand75Ksamplesoftheentire6.4Mtrainingset.Asimilarset
ofcomparisonsbetweentheNet-TrimandtheHPTDasinFigure8isperformed,notingthatthe˝ne-tuning
stepforbothschemesiscarriedoutusingallthetrainingsamples.Similartothepreviousexperiment,Net-Trim
consistentlyoutperformsHPTDinallsimilarsetups.Asidefromsuchsuperiority,wehighlightthepossibilityof
retrainingNet-Trimusingonlypartofthetrainingsamples.Forinstance,acomparisonofpanels(a)and(b)
showsthatalmostidenticalresultscanbeachievedintermsofaccuracyversussparsity,whenNet-Trimissolved
with25Ksamplesinsteadof50Ksamples.Forinstance,forbothpanels,aNet-Trimapplicationfollowedbya
single˝ne-tuningstepcanincreasethepercentageofthezerosinthenetworktomorethan80%,withalmostno
lossinthemodelaccuracy.Basically,asalsodiscussedpreviouslywithreferencetoFigure2,forlargedatasets
formulatingtheNet-Trimwithonlyaportionofthedatacanbeconsideredasageneralcomputationshortcut.
Anotherinterestingobservation,whichismoreapparentontheleftplotofpanel(c),isthat˝ne-tuningdoes
notalwaysimprovetheaccuracyofthemodelsaftertheapplicationofNet-Trim,andespeciallyinlowpruning
regimesmaycausedegradingtheaccuracyduetophenomenasuchasover˝tting.Forexample,inpanel(c),upto
apruningpercentageofalmost65%,a˝ne-tuningstepaftertheNet-Trimslightlydegradestheaccuracy.Whilea
˝ne-tuningstepislikelytohelpinthemajorityofcases,ouraccesstobothNet-Trim'splainoutcome,andthe
˝ne-tunedversionprovidesthe˛exibilityofpickingthemostcompressedandaccuratemodelamongthetwo.
References
[1]
Conjugategradientalgorithm
.
https://math.aalto.fi/opetus/inv/CGalgorithm.pdf
.Accessed:2018-07-
18.
[2]
A.Aghasi,A.Abdi,N.Nguyen,andJ.Romberg
,
Net-trim:Convexpruningofdeepneuralnetworks
withperformanceguarantee
,inAdvancesinNeuralInformationProcessingSystems31,CurranAssociates,
Inc.,2017,pp.
35
TestAccuracy(%)
PercentageofZeros
(a)
TestAccuracy(%)
Fine-TuningEpoch

PercentageofZeros
TestAccuracy(%)
PercentageofZeros
(b)
TestAccuracy(%)
Fine-TuningEpoch

PercentageofZeros
TestAccuracy(%)
PercentageofZeros
(c)
TestAccuracy(%)
Fine-TuningEpoch

PercentageofZeros
Figure9:SimilarcomparisonplotsasinFigure8forCIFAR-10:(a)retrainingofNet-TrimandHPTDperformed
using25Ksamples;(b)retrainingperformedusing50Ksamples;(c)retrainingperformedusing75Ksamples
36
[3]
S.Boyd,N.Parikh,E.Chu,B.Peleato,andJ.Eckstein
,
Distributedoptimizationandstatistical
learningviathealternatingdirectionmethodofmultipliers
,FoundationsandTrends
R

inMachineLearning,
3(2011),pp.
[4]
E.Candes
,
Therestrictedisometrypropertyanditsimplicationsforcompressedsensing
,ComptesRendus
Mathematique,346(2008),pp.
[5]
E.CandesandY.Plan
,
Aprobabilisticandriplesstheoryofcompressedsensing
,IEEETransactionson
InformationTheory,57(2011),pp.
[6]
E.Candes,J.Romberg,andT.Tao
,
Stablesignalrecoveryfromincompleteandinaccuratemeasurements
,
Communicationsonpureandappliedmathematics,59(2006),pp.
[7]
V.Chandrasekaran,B.Recht,P.Parrilo,andA.Willsky
,
Theconvexgeometryoflinearinverse
problems
,FoundationsofComputationalmathematics,12(2012),pp.
[8]
W.Chen,J.Wilson,S.Tyree,K.Weinberger,andY.Chen
,
Compressingneuralnetworkswiththe
hashingtrick
,inInternationalConferenceonMachineLearning,2015,pp.
[9]
D.Fremlin
,
Measuretheory
,TorresFremlin,2(2000).
[10]
F.Girosi,M.Jones,andT.Poggio
,
Regularizationtheoryandneuralnetworksarchitectures
,Neural
computation,7(1995),pp.
[11]
I.Goodfellow,Y.Bengio,andA.Courville
,
DeepLearning
,MITPress,2016.
[12]
D.Gross
,
Recoveringlow-rankmatricesfromfewcoe˚cientsinanybasis
,IEEETransactionsonInformation
Theory,57(2011),pp.66.
[13]
S.Han,H.Mao,andW.Dally
,
Deepcompression:Compressingdeepneuralnetworkswithpruning,
trainedquantizationandhu˙mancoding
,InternationalConferenceonLearningRepresentations(ICLR),
(2016).
[14]
S.Han,H.Mao,andW.J.Dally
,
Deepcompression:Compressingdeepneuralnetworkswithpruning,
trainedquantizationandhu˙mancoding
,arXivpreprintarXiv:1510.00149,(2015).
[15]
S.Han,J.Pool,J.Tran,andW.Dally
,
Learningbothweightsandconnectionsfore˚cientneural
network
,inAdvancesinNeuralInformationProcessingSystems,2015,pp.
[16]
A.HoerlandR.Kennard
,
Ridgeregression:Biasedestimationfornonorthogonalproblems
,Technometrics,
12(1970),pp.
[17]
D.Hsu,S.Kakade,andT.Zhang
,
Atailinequalityforquadraticformsofsubgaussianrandomvectors
,
ElectronicCommunicationsinProbability,17(2012).
[18]
S.IoffeandC.Szegedy
,
Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternal
covariateshift
,arXivpreprintarXiv:1502.03167,(2015).
[19]
V.KoltchinskiiandS.Mendelson
,
Boundingthesmallestsingularvalueofarandommatrixwithout
concentration
,InternationalMathematicsResearchNotices,2015(2015),pp.
[20]
A.Krizhevsky
,
Convolutionaldeepbeliefnetworksoncifar-10
,(2010).
37
[21]
A.Krizhevsky,I.Sutskever,andG.Hinton
,
Imagenetclassi˝cationwithdeepconvolutionalneural
networks
,inAdvancesinNeuralInformationProcessingSystems,2012.
[22]
Y.LeCun,L.Bottou,Y.Bengio,andP.Haffner
,
Gradient-basedlearningappliedtodocument
recognition
,ProceedingsoftheIEEE,86(1998),pp.
[23]
C.Louart,Z.Liao,andR.Couillet
,
Arandommatrixapproachtoneuralnetworks
,arXivpreprint
arXiv:1702.05419,(2017).
[24]
S.Mendelson
,
Learningwithoutconcentration
,inConferenceonLearningTheory,2014,pp.
[25]
S.Mendelson
,
Learningwithoutconcentrationforgenerallossfunctions
,ProbabilityTheoryandRelated
Fields,(2017).
[26]
S.NowlanandG.Hinton
,
Simplifyingneuralnetworksbysoftweight-sharing
,Neuralcomputation,4
(1992),pp.
[27]
J.Schmidhuber
,
Deeplearninginneuralnetworks:Anoverview
,NeuralNetworks,61(2015),pp.
[28]
N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhutdinov
,
Dropout:asimple
waytopreventneuralnetworksfromover˝tting
,TheJournalofMachineLearningResearch,15(2014),
pp.
[29]
R.Tibshirani
,
Regressionshrinkageandselectionviathelasso
,JournaloftheRoyalStatisticalSociety.
SeriesB(Methodological),(1996),pp.
[30]
J.Tropp
,
Convexrecoveryofastructuredsignalfromindependentrandomlinearmeasurements
,inSampling
Theory,aRenaissance,Springer,2015,pp.
[31]
A.vanderVaartandJ.Wellner
,
WeakConvergenceandEmpiricalProcesses:WithApplicationsto
Statistics
,SpringerScience&BusinessMedia,1996.
[32]
R.Vershynin
,
Introductiontothenon-asymptoticanalysisofrandommatrices
,CambridgeUniversityPress,
2012,pp.21
https://doi.org/10.1017/CBO9780511794308.006
.
[33]
L.Wan,M.Zeiler,S.Zhang,Y.LeCun,andR.Fergus
,
Regularizationofneuralnetworksusing
dropconnect
,inProceedingsofthe33rdInternationalConferenceonMachineLearning,2016.
38
"
22,Online Prediction of Switching Graph Labelings with Cluster Specialists,https://arxiv.org/pdf/1806.06439v3.pdf,https://github.com/jamesro/cluster-specialists,"OnlinePredictionofSwitchingGraphLabelingswith
ClusterSpecialists
MarkHerbster
DepartmentofComputerScience
UniversityCollegeLondon
London
UnitedKingdom
m.herbster@cs.ucl.ac.uk
JamesRobinson
DepartmentofComputerScience
UniversityCollegeLondon
London
UnitedKingdom
j.robinson@cs.ucl.ac.uk
Abstract
Weaddresstheproblemofpredictingthelabelingofagraphinanonlinesetting
whenthelabelingischangingovertime.Wepresentanalgorithmbasedona
specialist
[
11
]approach;wedevelopthemachineryofclusterspecialistswhich
probabilisticallyexploitstheclusterstructureinthegraph.Ouralgorithmhas
twovariants,oneofwhichsurprisinglyonlyrequires
O
(log
n
)
timeonanytrial
t
onan
n
-vertexgraph,anexponentialspeedupoverexistingmethods.We
proveswitchingmistake-boundguaranteesforbothvariantsofouralgorithm.
Furthermorethesemistakebounds
smoothly
varywiththemagnitudeofthechange
betweensuccessivelabelings.WeperformexperimentsonChicagoDivvyBicycle
Sharingdataandshowthatouralgorithmsoutperformanexisting
algorithm(akernelizedPerceptron)aswellasseveralnaturalbenchmarks.
1Introduction
Westudytheproblemofpredictinggraphlabelingsthatevolveovertime.Considerthefollowing
gameforpredictingthelabelingofagraphintheonlinesetting.
Nature
presentsagraph
G
;
Nature
queriesavertex
i
1
2
V
=
f
1
;
2
;:::;n
g
;the
learner
predictsthelabelofthevertex
^
y
1
2
1
;
1
g
;
Nature
presentsalabel
y
1
;
Nature
queriesavertex
i
2
;the
learner
predicts
^
y
2
;andsoforth.The
learner
'sgoalistominimizethetotalnumberofmistakes
M
=
jf
t
:^
y
t
6
=
y
t
gj
.If
Nature
is
strictlyadversarial,the
learner
willincuramistakeoneverytrial,butif
Nature
isregularor
simple,thereishopethatthe
learner
mayincuronlyafewmistakes.Thus,acentralgoalof
mistake-boundedonlinelearningistodesignalgorithmswhosetotalmistakescanbeboundedrelative
tothecomplexityof
Nature
'slabeling.This(non-switching)graphlabelingproblemhasbeen
studiedextensivelyintheonlinelearningliterature[
16
,
15
,
7
,
33
,
17
].Inthispaperwegeneralize
thesettingtoallowtheunderlyinglabelingtochangearbitrarilyovertime.The
learner
hasno
knowledgeofwhenachangeinlabelingwilloccurandthereforemustbeabletoadaptquicklyto
thesechanges.
Consideranexampleofservicesplacedthroughoutacity,suchaspublicbicyclesharingstations.
AsthepopulationusestheseservicesthestateofeachstationŒsuchasthenumberofavailable
bikesŒnaturallyevolvesthroughouttheday,attimesgraduallyandothersabruptly,andwemight
wanttopredictthestateofanygivenstationatanygiventime.Sincethelocationofagivenstation
aswellasthestateofnearbystationswillberelevanttothislearningproblemitisnaturaltousea
graph-basedapproach.Anothersettingmightbeagraphofmajorroadjunctions(vertices)connected
byroads(edges),inwhichonewantstopredictwhetherornotajunctioniscongestedatanygiven
time.Trafcongestionisnaturallynon-stationaryandalsoexhibitsbothgradualandabruptchanges
tothestructureofthelabelingovertime[24].
Preprint.Underreview.
arXiv:1806.06439v3  [cs.LG]  17 Jun 2019Thestructureofthispaperisasfollows.InSection2wediscussthebackgroundliterature.In
Section3wepresentthe
S
WITCHING
C
LUSTER
S
PECIALISTS
algorithm(SCS),aofthe
methodofspecialists[
11
]withthenovelmachineryof
clusterspecialists
,asetofspecialiststhatina
roughsensecorrespondtoclustersinthegraph.Weconsidertwodistinctsetsofspecialists,
B
n
and
F
n
,where
B
n
ˆF
n
.Withthesmallersetofspecialiststheboundisonlylargerbyfactorof
log
n
.
Ontheotherhand,predictionisexponentiallyfasterpertrial,remarkablyrequiringonly
O
(log
n
)
timetopredict.InSection4weprovideexperimentsonChicagoDivvyBicycleSharingdata.In
Section5weprovidesomeconcludingremarks.Allproofsarecontainedinthetechnicalappendices.
1.1Notation
Wepresentcommonnotation.Let
G
=(
V;E
)
beanundirected,connected,
n
-vertexgraphwith
vertexset
V
=
f
1
;
2
;:::;n
g
andedgeset
E
.Eachvertexofthisgraphmaybelabeledwithoneof
twostates

1
;
1
g
andthusalabelingofagraphmaybedenotedbyavector
u
2
1
;
1
g
n
where
u
i
denotesthelabelofvertex
i
.Theunderlyingassumptionisthatwearepredictingvertexlabelsfrom
asequence
u
1
;:::;
u
T
2
1
;
1
g
n
ofgraphlabelingsover
T
trials.Theset
K
:=
f
t
2f
2
;:::;T
g
:
u
t
6
=
u
t

1
g[f
1
g
containsthetrialofeachofthe
j
K
j
ﬁsegmentsﬂofthepredictionproblem.
Eachsegmentcorrespondstoatimeperiodwhentheunderlyinglabelingisunchanging.The
cut-size
ofalabeling
u
onagraph
G
isas

G
(
u
):=
jf
(
i;j
)
2
E
:
u
i
6
=
u
j
gj
,i.e.,thenumberof
edgesbetweenverticesofdisagreeinglabels.
Welet
r
G
(
i;j
)
denotethe
resistancedistance
(effectiveresistance)betweenvertices
i
and
j
when
thegraph
G
isseenasacircuitwhereeachedgehasunitresistance(e.g.,[
26
]).The
resistance
diameter
ofagraphis
R
G
:=max
i;j
2
V
r
G
(
i;j
)
.The
resistanceweighted
cut-sizeofalabeling
u
is

r
G
(
u
):=
P
(
i;j
)
2
E
:
u
i
6
=
u
j
r
G
(
i;j
)
.Let

n
=
f

2
[0
;
1]
n
:
P
n
i
=1

i
=1
g
bethe
n
-dimensionalprobability
simplex.For

2

n
we
H
(

):=
P
n
i
=1

i
log
2
1

i
tobetheentropyof

.For

;
!
2

n
we
d
(

;
!
)=
P
n
i
=1

i
log
2

i
!
i
tobetherelativeentropybetween

and
!
.Foravector
!
and
asetofindices
I
let
!
(
I
):=
P
i
2I
!
i
.Foranypositiveinteger
N
we
[
N
]:=
f
1
;
2
;:::;N
g
andforanypredicate
[
PRED
]:=1
if
PRED
istrueandequals0otherwise.
2RelatedWork
Theproblemofpredictingthelabelingofagraphinthebatchsettingwasintroducedasafoundational
methodforsemi-supervised(transductive)learning.Inthiswork,thegraphwasbuiltusingboththe
unlabeledandlabeledinstances.Theseminalworkby[
3
]usedametricontheinstancespaceandthen
builtakNNor

-ballgraph.Thepartiallabelingwasthenextendedtothecompletegraphbysolving
awproblemwhereopposingbinarylabelsrepresentedsourcesandsinks.Inpractice
thismethodsufferedfromveryunbalancedcuts.practicalandtheoreticaladvances
weremadebyreplacingthewmodelwithmethodsbasedonminimisingaquadratic
formofthegraphLaplacian.earlyresultsincludebutarenotlimitedto[
38
,
2
,
37
].A
limitationofthegraphLaplacian-basedtechniquesisthatthesebatchmethodsŒdependingontheir
implementationŒtypicallyrequire

n
2
)
to

n
3
)
timetoproduceasinglesetofpredictions.Inthe
onlineswitchingsettingwewillaimforourfastestalgorithmtohave
O
(log
n
)
timecomplexityper
trial.
Predictingthelabelingofagraphintheonlinesettingwasintroducedby[
20
].Theauthorsproved
boundsforaPerceptron-likealgorithmwithakernelbasedonthegraphLaplacian.Sincethis
worktherehasbeenanumberofextensionsandimprovementsinboundsincludingbutnotlimited
to[
16
,
6
,
15
,
18
,
17
,
32
].Commontoallofthesepapersisthatadominanttermintheirmistake
boundsisthe(resistance-weighted)cut-size.
Fromaperspective,themethodsforpredictingthelabelingofagraph(online)splitinto
twoapproaches.Theapproachworksdirectlywiththeoriginalgraphandisusuallybasedona
graphLaplacian[
20
,
15
,
17
];itprovidesboundsthatutilizetheadditionalconnectivityofnon-tree
graphs,whichareparticularlystrongwhenthegraphcontainsuniformly-labeledclustersofsmall
(resistance)diameter.Thedrawbacksofthisapproacharethattheboundsareweakerongraphswith
largediameter,andthatcomputationtimesareslower.
2
Thesecondapproachistoapproximatetheoriginalgraphwithanappropriatelyselectedtreeorﬁlineﬂ
graph[
16
,
7
,
6
,
33
].Thisenablesfastercomputationtimes,andboundsthatarebetterongraphswith
largediameters.Thesealgorithmsmaybeextendedtonon-treegraphsbyselectingaspanning
treeuniformlyatrandom[
7
]andthenapplyingthealgorithmtothesampledtree.Thisrandomized
approachinduces
expected
mistakeboundsthatalsoexploittheclusterstructureinthegraph(see
Section2.2).Ouralgorithmtakesthisapproach.
2.1SwitchingPrediction
Inthispaperratherthanpredictingasinglelabelingofagraphweinsteadwillpredicta(switching)
sequenceoflabelings.
Switching
inthemistake-orregret-boundsettingreferstotheproblemof
predictinganonlinesequencewhentheﬁbestcomparatorﬂischangingovertime.Inthesimplestof
switchingmodelsthesetofcomparatorsis
structureless
andwesimplypayperswitch.Aprominent
earlyresultinthismodelis[
21
]whichintroducedthe
e
updatewhichwillplayaprominent
roleinourmainalgorithm.Otherprominentresultsinthestructurelessmodelincludebutarenot
limitedto[
35
,
4
,
12
,
28
,
27
,
5
].Astrongermodelistoinsteadproveaboundthatholdsforany
arbitrarycontiguoussequenceoftrials.Suchaboundiscalledan
adaptive-regret
bound.Thistype
ofboundautomaticallyimpliesaboundonthestructurelessswitchingmodel.Adaptive-regretwas
introducedin[13]
1
otherprominentresultsinthismodelinclude[1,5,9].
Thestructurelessmodelmaybegeneralizedbyintroducingadivergencemeasureonthesetof
comparators.Thus,whereasinthestructurelessmodelwepayforthenumberofswitches,inthe
structuredmodelweinsteadpayinthesumofdivergencesbetweensuccessivecomparators.This
modelwasintroducedin[22];prominentresultsinclude[25,5].
In[
18
]theauthorsalsoconsiderswitchinggraphlabelprediction.However,theirresultsarenot
directlycomparabletoourssincetheyconsiderthecombinatoriallymorechallengingproblemof
repeatedswitchingwithinasmallsetoflabelingscontainedinalargerset.Thatset-upwasaproblem
originallyframedintheﬁexpertsﬂsettingandposedasanopenproblemby[
10
]andsolvedin[
4
].If
weapplytheboundin[
18
]tothecasewherethereis
not
repeatedswitchingwithinasmallerset,then
theirboundisuniformlyandweakerthantheboundsinthispaperandthealgorithm
isquiteslowrequiring

(
n
3
)
timepertrialinatypicalimplementation.Alsocontainedin[
18
]is
abaselinealgorithmbasedonakernelperceptronwithagraphLaplaciankernel.Theboundof
thatalgorithmhasthedrawbackinthatitscaleswithrespecttotheﬁworstﬂlabelingin
asequenceoflabelings.However,itissimpletoimplementandweuseitasabenchmarkinour
experiments.
2.2RandomSpanningTreesandLinearization
Sinceweoperateinthetransductivesettingwheretheentireunlabeledgraphispresentedtothe
learner
beforehand,thisaffordsthe
learner
theabilitytoperformanytothegraph
asapreprocessingstep.Theboundsofmostexistingalgorithmsforpredictingalabelingonagraph
areusuallyexpressedintermsofthecut-sizeofthegraphunderthatlabeling.Anaturalapproach
thenistouseaspanningtreeoftheoriginalgraphwhichcanonlyreducethecut-sizeofthelabeling.
Theeffectiveresistancebetweenvertices
i
and
j
,denoted
r
G
(
i;j
)
,isequaltotheprobabilitythat
aspanningtreeof
G
drawnuniformlyatrandom(fromthesetofallspanningtreesof
G
)includes
(
i;j
)
2
E
asoneofits
n

1
edges(e.g.,[
30
]).Asobservedby[
6
],byselectingaspanningtree
uniformlyatrandomfromthesetofallpossiblespanningtrees,mistakeboundsexpressedintermsof
thecut-sizethenbecome
expected
mistakeboundsnowintermsoftheeffective-resistance-weighted
cut-sizeofthegraph.Thatis,if
R
isarandomspanningtreeof
G
then
E

R
(
u
)]=
r
G
(
u
)
andthus

r
G
(
u
)


G
(
u
)
.Arandomspanningtreecanbesampledfromagraphefusingarandom
walkorsimilarmethods(seee.g.,[36]).
Toillustratethepowerofthisrandomizationconsidertheexampleofagraphwithtwo
cliqueseachofsize
n
=
2
,whereonecliqueislabeleduniformlywith`+1'andtheother`-1'with
anadditionalarbitrary
n
=
2
ﬁcutﬂedgesbetweenthecliques.Thisdensegraphexhibitstwodisjoint
clustersand

G
(
u
)=
n
=
2
.Ontheotherhand

r
G
(
u
)=
,sincebetweenanytwoverticesin
theopposingcliquesthereare
n
=
2
edgedisjointpathsoflength

3
andthustheeffectiveresistance
1
However,seetheanalysisofWMLin[29]foraprecursoryresult.
3
betweenanypairofverticesis

1
=
n
)
.Sinceboundsusuallyscalelinearlywith(resistance-weighted)
cut-size,thecut-sizeboundwouldbevacuousbuttheresistance-weightedcut-sizeboundwouldbe
small.
Wewillmakeuseofthispreprocessingstepofsamplingauniformrandomspanningtree,aswell
asa
linearization
ofthistreetoproducea(spine)line-graph,
S
.Thelinearizationof
G
to
S
as
apreprocessingstepwasproposedby[
16
]andhassincebeenappliedin,e.g.,[
7
,
31
].In
ordertoconstruct
S
,arandom-spanningtree
R
ispickeduniformlyatrandom.Avertexof
R
is
thenchosenandthegraphisfullytraversedusinga
stsearch
generatinganorderedlist
V
L
=

i
l
1
;:::;i
l
2
m
+1

ofverticesintheordertheywerevisited.Verticesin
V
mayappearmultiple
timesin
V
L
.Asubsequence
V
L
0

V
L
isthenchosensuchthateachvertexin
V
appearsonlyonce.
Thelinegraph
S
isthenformedbyconnectingeachvertexin
V
L
0
toitsimmediateneighborsin
V
L
0
withanedge.Wedenotetheedgesetof
S
by
E
S
andlet

t
:=(
u
t
)
,wherethecut

is
withrespecttothelinearembedding
S
.Surprisingly,asstatedinthelemmabelow,thecutonthis
linearizedgraphisnomorethantwicethecutontheoriginalgraph.
Lemma1
([
16
])
.
Givenalabeling
u
2
1
;
1
g
n
onagraph
G
,forthemapping
G!R!S
,as
above,wehave

S
(
u
)


R
(
u
)


G
(
u
)
.
Bycombiningtheaboveobservationswemayreducetheproblemoflearningonagraphtothat
oflearningonalinegraph.Inparticular,ifwehaveanalgorithmwithamistakeboundofthe
form
M
O

G
(
u
))
thisimplieswethenmaygivean
expected
mistakeboundoftheform
M
O

r
G
(
u
))
bysamplingarandomspanningtreeandthenlinearizingitasabove.Thus,for
simplicityinpresentation,wewillonlystatethedeterministicmistakeboundsintermsofcut-size,
althoughtheexpectedboundsintermsofresistance-weightedcut-sizeswillholdsimultaneously.
3SwitchingSpecialists
Inthissectionwepresentanewmethodbasedontheideaof
specialists
[
11
]fromthe
prediction
withexpertadvice
literature[
29
,
34
,
8
].Althoughtheachievedboundsareslightlyworsethanother
methodsforpredictinga
single
labelingofagraph,thederivedadvantageisthatitispossibleto
obtainﬁcompetitiveﬂboundswithfastalgorithmstopredictasequenceofchanginggraphlabelings.
Ourinductivebiasistopredictwellwhenalabelinghasa
small
(resistance-weighted)cut-size.The
complementaryperspectiveimpliesthatthelabelingconsistsofa
few
uniformlylabeledclusters.
Thissuggeststheideaofmaintainingacollectionofbasisfunctionswhereeachsuchfunctionis
specializedtopredictaconstantfunctiononagivenclusterofvertices.Toaccomplishthistechnically
weadaptthemethodof
specialists
[
11
,
27
].Aspecialistisapredictionfunction
""
fromaninputspace
toanextendedoutputspacewith
abstentions
.Soforustheinputspaceisjust
V
=[
n
]
,thevertices
ofagraph;andtheextendedoutputspaceis

1
;
1
;

g
where

1
;
1
g
correspondstopredicted
labelsofthevertices,but`

'indicatesthatthespecialistabstainsfrompredicting.Thusaspecialist
specializes
itspredictiontopartoftheinputspaceandinourapplicationthespecialistscorrespondto
acollectionofclusterswhichcoverthegraph,eachclusteruniformlypredicting

1
or
1
.
InAlgorithm1wegiveourswitchingspecialistsmethod.Thealgorithmmaintainsaweightvector
!
t
overthespecialistsinwhichthemagnitudesmaybeinterpretedasthecurrentwehavein
eachofthespecialists.Theupdatesandtheiranalysesareacombinationofthreestandardmethods:i)
Halving
lossupdates,ii)specialistsupdatesandiii)(delayed)ed-shareupdates.Thelossupdate
(2)
zerostheweightcomponentsofincorrectlypredictingspecialists,whilethenon-predictingspecialists
arenotupdatedatall.In
(1)
wegiveour
delayed
ed-sharestyleupdate.Astandardedshare
updatemaybewritteninthefollowingform:
!
t;""
=(1


)_
!
t

1
;""
+

jEj
:
(3)
Although
(3)
appearsdifferentto
(1)
,infactthesetwoupdatesareexactlythesame
intermsofpredictionsgeneratedbythealgorithm.Thisisbecause
(1)
cachesupdatesuntilthe
givenspecialistisagainactive.Thepurposeofthiscomputationallyisthatiftheactivespecialists
are,forexample,logarithmicinsizecomparedtothetotalspecialistpool,wemaythenachievean
exponentialspeedupover(3);whichinfactwewillexploit.
4
input:
Specialistsset
E
parameter:

2
[0
;
1]
initialize:
!
1
 
1
jEj
1
,
_
!
0
 
1
jEj
1
,
p
 
0
,
m
 
0
for
t
=1
to
T
do
receive
i
t
2
V
set
A
t
:=
f
""
2E
:
""
(
i
t
)
6
=

g
foreach
""
2A
t
do
//
delayedshareupdate
!
t;""
 
(1


)
m

p
""
_
!
t

1
;""
+
1

(1


)
m

p
""
jEj
(1)
predict
^
y
t
 
sign(
P
""
2A
t
!
t;""
""
(
i
t
))
receive
y
t
2
1
;
1
g
set
Y
t
:=
f
""
2E
:
""
(
i
t
)=
y
t
g
if
^
y
t
6
=
y
t
then
//
lossupdate
_
!
t;""
 
8
>
<
>
:
0
""
2A
t
\

Y
t
_
!
t

1
;""
""
62A
t
!
t;""
!
t
(
A
t
)
!
t
(
Y
t
)
""
2Y
t
(2)
foreach
""
2A
t
do
p
""
 
m
m
 
m
+1
else
_
!
t
 
_
!
t

1
Algorithm1:
S
WITCHING
C
LUSTER
S
PECIALISTS
Inthefollowingtheoremwewillgiveourswitchingspecialistsbound.Thedominantcostofswitching
ontrial
t
to
t
+1
isgivenbythenon-symmetric
J
E
(

t
;

t
+1
):=
jf
""
2E
:

t;""
=0
;
t
+1
;""
6
=0
gj
,
i.e.,wepayonlyforeachnewspecialistintroducedbutwedonotpayforremovingspecialists.
Theorem2.
Foragivenspecialistset
E
,let
M
E
denotethenumberofmistakesmadeinpredicting
theonlinesequence
(
i
1
;y
1
)
;:::;
(
i
T
;y
T
)
byAlgorithm1.Then,
M
E

1
ˇ
1
log
jEj
+
T
X
t
=1
1
ˇ
t
log
1
1


+
j
K

1
X
i
=1
J
E


k
i
;

k
i
+1

log
jEj

;
(4)
foranysequenceof
consistent
and
well-formed
comparators

1
;:::;

T
2

jEj
where
K
:=
f
k
1
=1
<

<k
j
K
j
g
:=
f
t
2
[
T
]:

t
6
=

t

1
g[f
1
g
,and
ˇ
t
:=

t
(
Y
t
)
.
Theboundintheabovetheoremdependscruciallyonthebestsequenceof
consistent
and
well-
formed
comparators

1
;:::;

T
.Theconsistencyrequirementimpliesthatoneverytrialthereisno
activeincorrectspecialistassignedﬁmassﬂ(

t
(
A
t
nY
t
)=0
).Wemayeliminatetheconsistency
requirementbyﬁsofteningﬂthelossupdate
(2)
.Acomparator

2

jEj
is
well-formed
if
8
v
2
V
,
thereexistsa
unique
""
2E
suchthat
""
(
v
)
6
=

and

""
>
0
,andfurthermorethereexistsa
ˇ
2
(0
;
1]
suchthat
8
""
2E
:

""
2f
0
;ˇ
g
,i.e.,eachspecialistinthesupportof

hasthesamemass
ˇ
and
thesespecialistsdisjointlycovertheinputspace(
V
).Atconsiderablecomplicationtotheformofthe
boundthewell-formednessrequirementmaybeeliminated.
Theaboveboundisﬁsmoothﬂinthatitscaleswithagradualchangeinthecomparator.Inthenext
sectionwedescribethenovelspecialistssetsthatwe'vetailoredtograph-labelpredictionsothata
smallchangeincomparatorcorrespondstoasmallchangeinagraphlabeling.
3.1ClusterSpecialists
Inordertoconstructthe
clusterspecialists
overagraph
G
=(
V
=[
n
]
;E
)
,weconstructa
linegraphasdescribedinSection2.2.Aclusterspecialististhenby
""
l;r
y
(

)
whichmaps
V
!
1
;
1
;

g
where
""
l;r
y
(
v
):=
y
if
l

v

r
and
""
l;r
y
(
v
):=

otherwise.Hencecluster
specialist
""
l;r
y
(
v
)
correspondstoafunctionthatpredictsthelabel
y
ifvertex
v
liesbetweenvertices
l
and
r
andabstainsotherwise.Recallthatbysamplingarandomspanningtreetheexpectedcut-size
ofalabelingonthespineisnomorethantwicetheresistance-weightedcut-sizeon
G
.Thus,given
alabeledgraphwithasmallresistance-weightedcut-sizewithdenselyinterconnectedclustersand
modestintra-clusterconnections,thisimpliesacut-bracketedlinearsegmentonthespinewillin
5
expectationroughlycorrespondtooneoftheoriginaldenseclusters.Wewillconsidertwobasissets
ofclusterspecialists.
Basis
F
n
:
Weintroducethe
complete
basisset
F
n
:=
f
""
l;r
y
:
l;r
2
[
n
]
;l

r
;
y
2
1
;
1
gg
.
Wesaythatasetofspecialists
C
u
E
2

1
;
1
;

g
n
frombasis
E
covers
alabeling
u
2
1
;
1
g
n
if
forall
v
2
V
=[
n
]
and
""
2C
u
that
""
(
v
)
2f
u
v
;

g
andif
v
2
V
thenthereexists
""
2C
u
suchthat
""
(
v
)=
u
v
.Thebasis
E
is
complete
ifeverylabeling
u
2
1
;
1
g
n
iscoveredbysome
C
u
E
.The
basis
F
n
iscompleteandinfacthasthefollowingapproximationproperty:forany
u
2
1
;
1
g
n
thereexistsacoveringset
C
u
F
n
suchthat
jC
u
j
=
S
(
u
)+1
.Thisfollowsdirectlyasalinewith
k

1
cutsisdividedinto
k
segments.Wenowillustratetheuseofbasis
F
n
topredictthelabelingof
agraph.Forsimplicityweillustratebyconsideringtheproblemofpredictingasinglegraphlabeling
withoutswitching.Asthereisnoswitchwewillset

:=0
andthusifthegraphislabeledwith
u
2
1
;
1
g
n
withcut-size

S
(
u
)
thenwewillneed

S
(
u
)+1
specialiststopredictthelabeling
andthusthecomparatorsmaybepost-hocoptimallydeterminedsothat

=

1
=

=

T
and
therewillbe

S
(
u
)+1
componentsof

eachwithﬁweightﬂ
1
=

S
(
u
)+1)
,thus
1
=
ˇ
1
=
S
(
u
)+1
,
sincetherewillbeonlyonespecialist(withnon-zeroweight)activepertrial.Sincethecardinality
of
F
n
is
n
2
+
n
,bysubstitutinginto
(4)
wehavethatthenumberofmistakeswillbeboundedby

S
(
u
)+1)log(
n
2
+
n
)
.Noteforasinglegraphlabelingonaspinethisboundisnotmuchworse
thanthebestknownresult[
16
,Theorem4].Intermsofcomputationtimehoweveritis
slowerthanthealgorithmin[
16
]requiring

n
2
)
timetopredictonatypicaltrialsinceonaverage
thereare

n
2
)
specialistsactivepertrial.
Basis
B
1
;n
:
Wenowintroducethebasis
B
n
whichhas

n
)
specialistsandonlyrequires
O
(log
n
)
timepertrialtopredictwithonlyasmallincreaseinbound.Thebasisisas
B
p;q
:=
(
f
""
p;q

1
;""
p;q
1
g
p
=
q;
f
""
p;q

1
;""
p;q
1
g[B
p;
b
p
+
q
2
c
[B
b
p
+
q
2
c
+1
;q
p
6
=
q
andisanalogoustoabinarytree.Wehavethefollowingapproximationpropertyfor
B
n
:=
B
1
;n
,
Proposition3.
Thebasis
B
n
iscomplete.Furthermore,foranylabeling
u
2
1
;
1
g
n
thereexists
acoveringset
C
u
B
n
suchthat
jC
u
j

S
(
u
)+1)
d
log
2
n
2
e
for
n>
2
.
Fromacomputationalperspectivethebinarytreestructureensuresthatthereareonly

n
)
specialistsactivepertrial,leadingtoanexponentialspeed-upinprediction.Asimilarsetofspecialists
wereusedforobtainingadaptive-regretboundsin[
9
,
23
].Inthatworkhowevertheﬁbinarytreeﬂ
structureisoverthetimedimension(trialsequence)whereasinthisworkthebinarytreeisover
thespacedimension(graph)andaed-shareupdateisusedtoobtainadaptivityoverthetime
dimension.
2
Inthecorollarythatfollowswewillexploitthefactthatbymakingthealgorithm
conservative
we
mayreducetheusual
log
T
terminthemistakeboundinducedbyaed-shareupdateto
loglog
T
.
Aconservativealgorithmonlyupdatesthespecialists'weightsontrialsonwhichamistakeismade.
Furthermoretheboundgiveninthefollowingcorollaryis
smooth
asthecostperswitchwillbe
measuredwithaHamming-likedivergence
H
ontheﬁcutﬂedgesbetweensuccessivelabelings,
as
H
(
u
;
u
0
):=
X
(
i;j
)
2
E
S
[[[
u
i
6
=
u
j
]
_
[
u
0
i
6
=
u
0
j
]]
^
[[
u
i
6
=
u
0
i
]
_
[
u
j
6
=
u
0
j
]]]
:
Observethat
H
(
u
;
u
0
)
issmallerthantwicethehammingdistancebetween
u
and
u
0
andisoften
smaller.Toachievetheboundswewillneedthefollowingproposition,whichupper
boundsdivergence
J
by
H
,asubtletyisthattherearemanydistinctsetsofspecialistsconsistent
withagivencomparator.Forexample,considerauniformlabelingon
S
.Onemayﬁcoverﬂthis
labelingwithasinglespecialistoralternatively
n
specialists,onecoveringeachvertex.Forthesake
ofsimplicityinboundswewillalwayschoosethesmallestsetofcoveringspecialists.Thuswe
introducethefollowingnotionsof
consistency
and
minimal-consistency
.
4.
Acomparator

2

jEj
isconsistentwiththelabeling
u
2
1
;
1
g
n
if

iswell-
formedand

""
>
0
impliesthatforall
v
2
V
that
""
(
v
)
2f
u
v
;

g
.
2
Aninterestingopenproblemistotrytogoodboundsandtime-complexitywithsetsofspecialistsover
both
thetimeandspacedimensions.
6
5.
Acomparator

2

jEj
isminimal-consistentwiththelabeling
u
2
1
;
1
g
n
ifit
isconsistentwith
u
andthecardinalityofitssupportset
jf

""
:

""
>
0
gj
istheminimumofall
comparatorsconsistentwith
u
.
Proposition6.
Foralinearizedgraph
S
,forcomparators

;

0
2

jF
n
j
thatareminimal-consistent
with
u
and
u
0
respectively,
J
F
n
(

;

0
)

min(2
H
(
u
;
u
0
)
;

S
(
u
0
)+1)
:
AproofisgiveninAppendixC.InthefollowingcorollarywesummarizetheresultsoftheSCS
algorithmusingthebasissets
F
n
and
B
n
withanoptimally-tunedswitchingparameter

.
Corollary7.
Foraconnected
n
-vertexgraph
G
andwithrandomlysampledspine
S
,thenumber
ofmistakesmadeinpredictingtheonlinesequence
(
i
1
;y
1
)
;:::;
(
i
T
;y
T
)
bytheSCSalgorithmwith
optimally-tuned

isupperboundedwithbasis
F
n
by
O
0
@

1
log
n
+
j
K

1
X
i
=1
H
(
u
k
i
;
u
k
i
+1
)(log
n
+log
j
K
j
+loglog
T
)
1
A
andwithbasis
B
n
by
O
0
@
0
@

1
log
n
+
j
K

1
X
i
=
i
H
(
u
k
i
;
u
k
i
+1
)(log
n
+log
j
K
j
+loglog
T
)
1
A
log
n
1
A
foranysequenceoflabelings
u
1
;:::;
u
T
2
1
;
1
g
n
suchthat
u
t;i
t
=
y
t
forall
t
2
[
T
]
.
Thustheboundsareequivalentuptoafactorof
log
n
althoughthecomputationtimesvarydramat-
ically.SeeAppendixDforatechnicalproofoftheseresults,anddetailsontheselectionofthe
switchingparameter

.Notethatwemayavoidtheissueofneedingtooptimallytune

usingthe
followingmethodproposedby[
14
]andby[
28
].Weuseatime-varyingparameterandontrial
t
we
set

t
=
1
=
t
+1
.Wehavethefollowingguaranteeforthismethod,seeAppendixEforaproof.
Proposition8.
Foraconnected
n
-vertexgraph
G
andwithrandomlysampledspine
S
,theSCS
algorithmwithbases
F
n
and
B
n
inpredictingtheonlinesequence
(
i
1
;y
1
)
;:::;
(
i
T
;y
T
)
nowwith
time-varying

setequalto
1
=
t
+1
ontrial
t
achievesthesameasymptoticmistakeboundsasin
Corollary7withanoptimally-tuned

,undertheassumptionthat

S
(
u
1
)

P
j
K

1
i
=1
J
E
(

k
i
;

k
i
+1
)
.
4Experiments
Inthissectionwepresentresultsofexperimentsonrealdata.TheCityofChicagocurrentlycontains
608
publicbicyclestationsforitsﬁDivvyBikeﬂsharingsystem.Currentandhistoricaldatais
availablefromtheCityofChicago
3
containingavarietyoffeaturesforeachstation,including
latitude,longitude,numberofdocks,numberofoperationaldocks,andnumberofdocksoccupied.
Thelatestdataoneachstationispublishedapproximatelyeverytenminutes.
Weusedasampleof
72
hoursofdata,consistingofthreeconsecutiveweekdaysinApril
2019
.The

24
hoursofdatawereusedforparameterselection,andtheremaining
48
hoursofdatawereused
forevaluatingperformance.Oneachten-minutesnapshotwetookthepercentageofemptydocksof
eachstation.Wecreatedabinarylabelingfromthisdatabysettingathresholdof
50%
.Thuseach
bicyclestationisavertexinourgraphandthelabelofeachvertexindicateswhetherthatstationis
`mostlyfull'or`mostlyempty'.Duetothisthresholdingthelabelsofsome`quieter'stationswere
observednottoswitch,asthepercentageofavailabledocksrarelychanged.Thesestationstendedto
beonthe`outskirts',andthusweexcludedthesestationsfromourexperiments,giving
404
vertices
inourgraph.
Usingthegeodesicdistancebetweeneachstation'slatitudeandlongitudinalpositionaconnected
graphwasbuiltusingtheunionofa
k
-nearestneighborgraph(
k
=3
)andaminimumspanning
tree.Foreachinstanceofouralgorithmthegraphwasthentransformedinthemannerdescribed
3
https://data.cityofchicago.org/Transportation/Divvy-Bicycle-Stations-Historical/eq45-8inv
7
Figure1:
Left:Meancumulativemistakesover
25
iterationsforallalgorithmsandbenchmarksover
48
hours
(
8640
trials)ona
404
-vertexgraph.AcomparisonofthemeanperformanceofSCSwithbases
F
n
and
B
n
(SCS-FandSCS-Brespectively)usinganensembleofsize
1
and
65
isshown.Right:Anexampleoftwobinary
labelingstakenfromthemorningandeveningofthe
24
hoursofdata.An`orange'labelimpliesthatstation
is
<
50%
fullanda`black'labelimpliesthatstationis

50%
full.
inSection2.2,bydrawingaspanningtreeuniformlyatrandomandthenlinearizingusing
search.
Asnaturalbenchmarksforthissettingweconsideredthefollowingfourmethods.
1
:
)
Forallvertices
predictwiththemostfrequentlyoccurringlabeloftheentiregraphfromthetrainingdata(ﬁGlobalﬂ).
2
:
)
Foreachvertexpredictwithitsmostfrequentlyoccurringlabelfromthetrainingdata(ﬁLocalﬂ).
3
:
)
Forallverticesatanygiventimepredictwiththemostfrequentlyoccurringlabeloftheentire
graphatthattimefromthetrainingdata(ﬁTemporal-Globalﬂ)
4
:
)
Foreachvertexatanygiventime
predictwiththatvertex'slabelobservedatthesametimeinthetrainingdata(ﬁTemporal-Localﬂ).We
alsocompareouralgorithmsagainstakernelPerceptronproposedby[
18
]forpredictingswitching
graphlabelings(seeAppendixFfordetails).
Followingtheexperimentsof[
7
]inwhichensemblesofrandomspanningtreesweredrawnand
aggregatedbyanunweightedmajorityvote,wetestedtheeffectonperformanceofusingensem-
blesofinstancesofouralgorithms,aggregatedinthesamefashion.Wetestedensemblesizesin
f
1
;
3
;
5
;
9
;
17
;
33
;
65
g
,usingoddnumberstoavoidties.
Foreveryten-minutesnapshot(labeling)wequeried
30
verticesuniformlyatrandom(withreplace-
ment)inanonlinefashion,givingasequenceof
8640
trialsover
48
hours.Theaverageperformance
over
25
iterationsisshowninFigure1.Thereareseveralsurprisingobservationstobemadefrom
ourresults.Firstly,bothSCSalgorithmsperformedbetterthanallbenchmarksand
competingalgorithms.Additionallybasis
B
n
outperformedbasis
F
n
byquitealargemargin,despite
havingtheweakerboundandbeingexponentiallyfaster.Finallyweobservedaincrease
inperformanceofbothSCSalgorithmsbyincreasingtheensemblesize(seeFigure1),additional
detailsontheseexperimentsandresultsofallensemblesizesaregiveninAppendixG.
Interestinglywhentuning

wefoundbasis
B
n
tobeveryrobust,while
F
n
wasverysensitive.This
observationcombinedwiththelogarithmicper-trialtimecomplexitysuggeststhatSCSwith
B
n
has
promisetobeaverypracticalalgorithm.
5Conclusion
Ourprimaryresultwasanalgorithmforpredictingswitchinggraphlabelingswithaper-trialprediction
timeof
O
(log
n
)
andamistakeboundthat
smoothly
trackschangestothegraphlabelingovertime.In
thelongversionofthispaperweplantoextendtheanalysisoftheprimaryalgorithmtotheexpected
regretsettingbyrelaxingoursimplifyingassumptionofthe
well-formed
comparatorsequencethatis
minimal-consistent
withthelabelingsequence.Fromatechnicalperspectivetheopenproblemthat
wefoundmostintriguingistoeliminatethe
loglog
T
termfromourbounds.Thenaturalapproachto
thiswouldbetoreplacetheconservative
e
updatewitha
variable-share
update[
21
];inour
effortshoweverwefoundmanytechnicalproblemswiththisapproach.Onboththemorepractical
andspeculativeside;weobservethatthespecialistssets
B
n
,and
F
n
werechosentoﬁproveboundsﬂ.
8
Inpracticewecanuseany
hierarchical
graphclusteringalgorithmtoproducea
complete
specialist
setandfurthermoremultiplesuchclusteringsmaybepooled.Suchapooledsetofsubgraphﬁmotifsﬂ
couldbethenbeusedforexampleinamulti-tasksetting(seeforexample,[27]).
References
[1]
D.Adamskiy,W.M.Koolen,A.Chernov,andV.Vovk.Acloserlookatadaptiveregret.In
Proceedingsofthe23rdInternationalConferenceonAlgorithmicLearningTheory
,ALT'12,
pages290Œ304,2012.
[2]
M.BelkinandP.Niyogi.Semi-supervisedlearningonriemannianmanifolds.
Machinelearning
,
56(1-3):209Œ239,2004.
[3]
A.BlumandS.Chawla.Learningfromlabeledandunlabeleddatausinggraphmincuts.In
ProceedingsoftheEighteenthInternationalConferenceonMachineLearning
,ICML'01,pages
19Œ26,2001.
[4]
O.BousquetandM.K.Warmuth.Trackingasmallsetofexpertsbymixingpastposteriors.
JournalofMachineLearningResearch
,3(Nov):363Œ396,2002.
[5]
N.Cesa-Bianchi,P.Gaillard,G.Lugosi,andG.Stoltz.Mirrordescentmeetsedshare(and
feelsnoregret).In
Proceedingsofthe25thInternationalConferenceonNeuralInformation
ProcessingSystems-Volume1
,NIPS'12,pages980Œ988,2012.
[6]
N.Cesa-Bianchi,C.Gentile,andF.Vitale.Fastandoptimalpredictiononalabeledtree.In
Proceedingsofthe22ndAnnualConferenceonLearningTheory
,pages145Œ156.Omnipress,
2009.
[7]
N.Cesa-Bianchi,C.Gentile,F.Vitale,andG.Zappella.Randomspanningtreesandthe
predictionofweightedgraphs.
JournalofMachineLearningResearch
,14(1):1251Œ1284,2013.
[8]
N.Cesa-BianchiandG.Lugosi.
Prediction,Learning,andGames
.CambridgeUniversityPress,
NewYork,NY,USA,2006.
[9]
A.Daniely,A.Gonen,andS.Shalev-Shwartz.Stronglyadaptiveonlinelearning.In
Proceedings
ofthe32ndInternationalConferenceonInternationalConferenceonMachineLearning-
Volume37
,ICML'15,pages1405Œ1411,2015.
[10]
Y.Freund.Privatecommunication,2000.Alsopostedonhttp://www.learning-theory.org.
[11]
Y.Freund,R.E.Schapire,Y.Singer,andM.K.Warmuth.Usingandcombiningpredictors
thatspecialize.In
ProceedingsoftheTwenty-ninthAnnualACMSymposiumonTheoryof
Computing
,STOC'97,pages334Œ343,1997.
[12]
A.György,T.Linder,andG.Lugosi.Trackingthebestofmanyexperts.In
Proceedingsofthe
18thAnnualConferenceonLearningTheory
,COLT'05,pages204Œ216,2005.
[13]
E.HazanandC.Seshadhri.Adaptivealgorithmsforonlinedecisionproblems.
Electronic
ColloquiumonComputationalComplexity(ECCC)
,14(088),2007.
[14]
M.Herbster.TrackingthebestexpertII.Unpublishedmanuscript,1997.
[15]
M.HerbsterandG.Lever.Predictingthelabellingofagraphviaminimum$p$-seminorm
interpolation.In
COLT2009-The22ndConferenceonLearningTheory
,2009.
[16]
M.Herbster,G.Lever,andM.Pontil.Onlinepredictiononlargediametergraphs.In
Proceedings
ofthe21stInternationalConferenceonNeuralInformationProcessingSystems
,NIPS'08,
pages649Œ656,2008.
[17]
M.Herbster,S.Pasteris,andS.Ghosh.Onlinepredictionatthelimitofzerotemperature.In
Proceedingsofthe28thInternationalConferenceonNeuralInformationProcessingSystems-
Volume2
,NIPS'15,pages2935Œ2943,2015.
9
[18]
M.Herbster,S.Pasteris,andM.Pontil.Predictingaswitchingsequenceofgraphlabelings.
JournalofMachineLearningResearch
,16(1):2003Œ2022,2015.
[19]
M.HerbsterandM.Pontil.Predictiononagraphwithaperceptron.In
Proceedingsofthe19th
InternationalConferenceonNeuralInformationProcessingSystems
,NIPS'06,pages577Œ584,
2006.
[20]
M.Herbster,M.Pontil,andL.Wainer.Onlinelearningovergraphs.In
Proceedingsofthe22nd
InternationalConferenceonMachineLearning
,ICML'05,pages305Œ312,2005.
[21]
M.HerbsterandM.Warmuth.Trackingthebestexpert.
MachineLearning
,32(2):151Œ178,
1998.
[22]
M.HerbsterandM.K.Warmuth.Trackingthebestlinearpredictor.
JournalofMachine
LearningResearch
,1:281Œ309,Sept.2001.
[23]
K.Jun,F.Orabona,S.Wright,andR.Willett.Improvedstronglyadaptiveonlinelearningusing
coinbetting.In
Proceedingsofthe20thInternationalConferenceonIntelligenceand
Statistics
,volume54of
ProceedingsofMachineLearningResearch
,pages943Œ951.PMLR,
20Œ22Apr2017.
[24]
B.S.Kerner.Experimentalfeaturesofself-organizationintrafw.
Phys.Rev.Lett.
,
81:3797Œ3800,Oct1998.
[25]
J.Kivinen,A.Smola,andR.Williamson.Onlinelearningwithkernels.
Trans.Sig.Proc.
,
52(8):2165Œ2176,Aug.2004.
[26]
D.J.KleinandM.Randi
´
c.Resistancedistance.
Journalofmathematicalchemistry
,12(1):81Œ95,
1993.
[27]
W.M.Koolen,D.Adamskiy,andM.K.Warmuth.Puttingbayestosleep.In
Proceedingsofthe
25thInternationalConferenceonNeuralInformationProcessingSystems-Volume1
,NIPS'12,
pages135Œ143,2012.
[28]
W.M.KoolenandS.Rooij.Combiningexpertadviceefciently.In
21stAnnualConferenceon
LearningTheory-COLT2008
,pages275Œ286,2008.
[29]
N.LittlestoneandM.K.Warmuth.Theweightedmajorityalgorithm.
Informationand
Computation
,108(2):212Œ261,1994.
[30]
R.LyonsandY.Peres.
ProbabilityonTreesandNetworks
.CambridgeUniversityPress,New
York,NY,USA,1stedition,2017.
[31]
O.H.M.Padilla,J.Sharpnack,J.G.Scott,andR.J.Tibshirani.Thedfsfusedlasso:Linear-time
denoisingovergeneralgraphs.
JournalofMachineLearningResearch
,18(1):1Œ36,2018.
[32]
A.RakhlinandK.Sridharan.Efcientonlinemulticlasspredictionongraphsviasurrogate
losses.In
Proceedingsofthe20thInternationalConferenceonIntelligenceand
Statistics,AISTATS2017
,pages1403Œ1411,2017.
[33]
F.Vitale,N.Cesa-Bianchi,C.Gentile,andG.Zappella.Seethetreethroughthelines:The
shazooalgorithm.In
AdvancesinNeuralInformationProcessingSystems23
,pages1584Œ1592,
2011.
[34]
V.Vovk.Aggregatingstrategies.In
ProceedingsoftheThirdAnnualWorkshoponComputa-
tionalLearningTheory
,COLT'90,pages371Œ386,1990.
[35]
V.Vovk.Derandomizingstochasticpredictionstrategies.
MachineLearning
,35(3):247Œ282,
1999.
[36]
D.B.Wilson.Generatingrandomspanningtreesmorequicklythanthecovertime.In
ProceedingsoftheTwenty-eighthAnnualACMSymposiumonTheoryofComputing
,STOC'96,
pages296Œ303,1996.
10
[37]
D.Zhou,O.Bousquet,T.N.Lal,J.Weston,andB.Schölkopf.Learningwithlocaland
globalconsistency.In
Proceedingsofthe16thInternationalConferenceonNeuralInformation
ProcessingSystems
,NIPS'03,pages321Œ328,2003.
[38]
X.Zhu,Z.Ghahramani,andJ.D.Lafferty.Semi-supervisedlearningusinggaussianand
harmonicfunctions.In
ProceedingsoftheTwentiethInternationalConferenceonInternational
ConferenceonMachineLearning
,ICML'03,pages912Œ919,2003.
11
Appendix
AProofofTheorem2
Proof.
Recallthatthecachedshareupdate
(1)
isequivalenttoperforming
(3)
.Wethussimulatethe
latterupdateinouranalysis.Wearguetheinequality
[^
y
t
6
=
y
t
]

1

t
(
Y
t
)
(
d
(

t
;
!
t
)

d
(

t
;
_
!
t
))
;
(5)
asthisisderivedbyobservingthat
d
(

t
;
!
t
)

d
(

t
;
_
!
t
)=
X
""
2E

t;""
log
_
!
t;""
!
t;""
=
X
""
2Y
t

t;""
log
_
!
t;""
!
t;""


t
(
Y
t
)[^
y
t
6
=
y
t
]
;
wherethesecondlinefollowsthefactthat

t;""
log
_
!
t;""
!
t;""
=0
if
""
62Y
t
aseitherthespecialist
""
predicts`

'and
_
!
t;""
=
!
t;""
oritpredictsincorrectlyandhence

t;""
=0
.Thethirdlinefollowsas
for
""
2Y
t
,
_
!
t;""
!
t;""

2
iftherehasbeenamistakeontrial
t
andotherwisetheratiois

1
.Indeed,
sinceAlgorithm1isconservative,thisratioisexactly
1
whennomistakeismadeontrial
t
,thus
withoutlossofgeneralitywewillassumethealgorithmmakesamistakeoneverytrial.
Forclaritywewillnowusenotationandlet
ˇ
t
:=

t
(
Y
t
)
.Wenowprovethefollowing
inequalitieswhichwewilladdto
(5)
tocreateatelescopingsumofrelativeentropytermsandentropy
terms.
1
ˇ
t
[
d
(

t
;
_
!
t
)

d
(

t
;
!
t
+1
)]

1
ˇ
t
log
1
1


;
(6)
1
ˇ
t
d
(

t
;
!
t
+1
)

1
ˇ
t
+1
d
(

t
+1
;
!
t
+1
)

1
ˇ
t
H
(

t
)+
1
ˇ
t
+1
H
(

t
+1
)

J
E
(

t
;

t
+1
)log
jEj

:
(7)
Firstly(6)isprovedwiththefollowing
d
(

t
;
_
!
t
)

d
(

t
;
!
t
+1
)=
X
""
2E

t;""
log
!
t
+1
;""
_
!
t;""

X
""
2E

t;""
log

(1


)_
!
t;""
_
!
t;""

=log(1


)
;
wheretheinequalityhasused
!
t
+1
;""

(1


)_
!
t;""
from(3).
Toprove(7)wethefollowingsets.

t
:=
f
""
2E
:

t

1
;""
6
=0
;
t;""
=0
g
;

t
:=
f
""
2E
:

t

1
;""
6
=0
;
t;""
6
=0
g
;

t
:=
f
""
2E
:

t

1
;""
=0
;
t;""
6
=0
g
:
12
Wenowexpandthefollowing
1
ˇ
t
d
(

t
;
!
t
+1
)

1
ˇ
t
+1
d
(

t
+1
;
!
t
+1
)
=
1
ˇ
t
d
(

t
;
!
t
+1
)

1
ˇ
t
d
(

t
+1
;
!
t
+1
)+
1
ˇ
t
d
(

t
+1
;
!
t
+1
)

1
ˇ
t
+1
d
(

t
+1
;
!
t
+1
)
=
1
ˇ
t
X
""
2E

t;""
log

t;""
!
t
+1
;""

1
ˇ
t
X
""
2E

t
+1
;""
log

t
+1
;""
!
t
+1
;""
+
1
ˇ
t
X
""
2E

t
+1
;""
log

t
+1
;""
!
t
+1
;""

1
ˇ
t
+1
X
""
2E

t
+1
;""
log

t
+1
;""
!
t
+1
;""
=

1
ˇ
t
H
(

t
)+
1
ˇ
t
H
(

t
+1
)+
X
""
2E


t;""
ˇ
t


t
+1
;""
ˇ
t

log
1
!
t
+1
;""

1
ˇ
t
H
(

t
+1
)+
1
ˇ
t
+1
H
(

t
+1
)+
X
""
2E


t
+1
;""
ˇ
t


t
+1
;""
ˇ
t
+1

log
1
!
t
+1
;""
:
(8)
Recallthatacomparator

2

jEj
is
well-formed
if
8
v
2
V
,thereexistsa
unique
""
2E
suchthat
""
(
v
)
6
=

and

""
>
0
,andfurthermorethereexistsa
ˇ
2
(0
;
1]
suchthat
8
""
2E
:

""
2f
0
;ˇ
g
,i.e.,
eachspecialistinthesupportof

hasthesamemass
ˇ
andthesespecialistsdisjointlycovertheinput
space(
V
).Thus,bycollectingtermsintothethreesets

t
+1
,

t
+1
,and

t
+1
wehave
X
""
2E


t;""
ˇ
t


t
+1
;""
ˇ
t

log
1
!
t
+1
;""
=
X
""
2

t
+1

t;""
ˇ
t
log
1
!
t
+1
;""
+
X
""
2

t
+1


t;""
ˇ
t


t
+1
;""
ˇ
t

log
1
!
t
+1
;""

X
""
2

t
+1

t
+1
;""
ˇ
t
log
1
!
t
+1
;""
=
X
""
2

t
+1

t;""
ˇ
t
log
1
!
t
+1
;""
+
X
""
2

t
+1

1


t
+1
;""
ˇ
t

log
1
!
t
+1
;""

X
""
2

t
+1

t
+1
;""
ˇ
t
log
1
!
t
+1
;""
;
(9)
andsimilarly
X
""
2E


t
+1
;""
ˇ
t


t
+1
;""
ˇ
t
+1

log
1
!
t
+1
;""
=
X
""
2

t
+1


t
+1
;""
ˇ
t

1

log
1
!
t
+1
;""
+
X
""
2

t
+1


t
+1
;""
ˇ
t

1

log
1
!
t
+1
;""
:
(10)
Substituting(9)and(10)into(8)andsimplifyinggives
1
ˇ
t
d
(

t
;
!
t
+1
)

1
ˇ
t
+1
d
(

t
+1
;
!
t
+1
)
=

1
ˇ
t
H
(

t
)+
1
ˇ
t
+1
H
(

t
+1
)+
X
""
2

t
+1

t;""
ˇ
t
log
1
!
t
+1
;""

X
""
2

t
+1
log
1
!
t
+1
;""

1
ˇ
t
H
(

t
)+
1
ˇ
t
+1
H
(

t
+1
)
j

t
+1
j
log
jEj

;
(11)
wheretheinequalityhasusedthefactthat

jEj

!
t
+1
;""

1
from(3).
Summingoveralltrialsthenleavesatelescopingsumofrelativeentropyterms,acostof
1
ˇ
t
log
1
1


oneachtrial,and
j

t
+1
j
log
jEj

foreachswitch.Thus,
T
X
t
=1
[^
y
t
6
=
y
t
]

1
ˇ
1
d
(

1
;
!
1
)+
1
ˇ
1
H
(

1
)+
T
X
t
=1
1
ˇ
t
log
1
1


+
j
K

1
X
i
=1
J
E


k
i
;

k
i
+1

log
jEj

;
(12)
where
J
E
(

k
i
;

k
i
+1
)=
j

k
i
+1
j
,andsince
!
1
=
1
jEj
1
,wecancombinetheremainingentropyand
relativeentropytermstogive
1
ˇ
1
d
(

1
;
!
1
)+
1
ˇ
1
H
(

1
)=
1
ˇ
1
log
jEj
,concludingtheproof.
13
BProofofProposition3
Werecalltheproposition:
Thebasis
B
n
iscomplete.Furthermore,foranylabeling
u
2
1
;
1
g
n
thereexistsacoveringset
C
u
B
n
suchthat
jC
u
j

S
(
u
)+1)
d
log
2
n
2
e
.
Wegiveabriefintuitionoftheproof;anyrequiredtermswillbemorecompletelylater.
Foragivenlabeling
u
2
1
;
1
g
n
ofcut-size

S
(
u
)
,thespine
S
canbecutinto

S
(
u
)+1
clusters
,
whereaclusterisacontiguoussegmentofverticeswiththesamelabel.Wewillupperboundthe
maximumnumberofclusterspecialistsrequiredtocoverasinglecluster,andthereforeobtainan
upperboundfor
jC
u
j
bysummingoverthe

S
(
u
)+1
clusters.
Withoutlossofgeneralityweassume
n
=2
r
forsomeinteger
r
andthusthestructureof
B
n
is
analogoustoaperfectbinarytreeofdepth
d
=log
2
n
.Indeed,foraedlabelparameter
y
wewill
adopttheterminologyofbinarytreessuchthatforinstancewesayspecialist
""
i;j
y
for
i
6
=
j
hasa
so-called
left-child
""
i;
b
i
+
j
2
c
y
and
right-child
""
d
i
+
j
2
e
;j
y
.Similarly,wesaythat
""
i;
b
i
+
j
2
c
y
and
""
d
i
+
j
2
e
;j
y
are
siblings
,and
""
i;j
y
istheir
parent
.Notethatanyspecialistisbothanancestorandadescendantofitself,
andaproperdescendantofaspecialistisadescendantofoneofitschildren.Finallythe
depth
of
specialist
""
i;j
y
istobeequaltothedepthofthecorrespondingnodeinabinarytree,suchthat
""
1
;n
y
isofdepth
0
,
""
1
;
n
2
y
and
""
n
2
+1
;n
y
areofdepth
1
,etc.
Theclaimofthepropositioniseasytoproveas
f
""
i;i

1
;""
i;i
1
:
i
2
[
n
]
gˆB
n
andthusanylabeling
u
2
1
;
1
g
n
canbecovered.Wenowprovethesecondclaimoftheproposition.
Wewilldenoteauniformly-labeledcontiguoussegmentofverticesbythepair
(
l;r
)
,where
l;r
2
[
n
]
arethetwoendverticesofthesegment.Forcompletenesswewillallowthetrivialcasewhen
l
=
r
.Givenalabeling
u
2
1
;
1
g
n
,let
L
u
:=
f
(
l;r
):1

l

r

n
;
u
l
=
:::
=
u
r
;
u
l

1
6
=
u
l
;
u
r
+1
6
=
u
r
g
bethesetofmaximum-sizedcontiguoussegmentsofunifmormly-labeledvertices.
Notethat
u
l

1
or
u
r
+1
maybevacuous.Whenthecontextisclear,wewillalsodescribe
(
l;r
)
asa
cluster
,andasthesetofvertices
f
l;:::;r
g
.
Foragiven
u
2
1
;
1
g
n
andcluster
(
l;r
)
2L
u
,wesay
B
(
l;r
)
B
n
isan
(
l;r
)
-coveringsetwith
respectto
u
ifforall
""
i;j
y
2B
(
l;r
)
wehave
l

i;j

r
,andifforall
k
2
(
l;r
)
thereexistssome
""
i;j
y
2B
(
l;r
)
suchthat
i

k

j
and
y
=
u
k
.Thatis,everyvertexintheclusteris`covered'byat
leastonespecialistandnospecialistscoveranyvertices
k=
2
(
l;r
)
.We
D
(
l;r
)
tobethesetof
allpossible
(
l;r
)
-coveringsetswithrespectto
u
.
Wenow

(
B
(
l;r
)
):=
jB
(
l;r
)
j
tobethe
complexity
of
B
(
l;r
)
2D
(
l;r
)
.
Foragiven
u
2
1
;
1
g
n
andcluster
(
l;r
)
2L
u
,wewishtoproducean
(
l;r
)
-coveringsetof
minimum
complexity,whichwedenote
B

(
l;r
)
:=argmin
B
(
l;r
)
2D
(
l;r
)

(
B
(
l;r
)
)
.Notethatan
(
l;r
)
-covering
setofminimumcomplexitycannotcontainanytwospecialistswhicharesiblings,sincetheycanbe
removedfromthesetandreplacedbytheirparentspecialist.
Lemma9.
Forany
u
2
1
;
1
g
n
,forany
(
l;r
)
2L
u
,the
(
l;r
)
-coveringsetofminimumcomplexity,
B

(
l;r
)
=argmin
B
(
l;r
)
2D
(
l;r
)

(
B
(
l;r
)
)
containsatmosttwospecialistsofeachuniquedepth.
Proof.
Wegiveanintuitivesketchoftheproof.Foragiven
u
2
1
;
1
g
n
andcluster
(
l;r
)
2L
u
assumethatthereareatleastthreespecialistsofequaldepthin
B

(
l;r
)
,thenanyofthesespecialists
thatareinthe`middle'mayberemoved,alongwithanyoftheirsiblingsorproperdescendantsthat
arealsomembersof
B

(
l;r
)
withoutcreatingany`holes'inthecovering,decreasingthecomplexityof
B

(
l;r
)
.
Weuseaproofbycontradiction.Supposeforcontradictionthatforagiven
u
2
1
;
1
g
n
and
(
l;r
)
2L
u
,the
(
l;r
)
-coveringsetofminimumcomplexity,
B

(
l;r
)
,containsthreedistinctspecialists
ofthesamedepth,
""
a;b
y
;""
c;d
y
;""
e;f
y
.Withoutlossofgeneralitylet
a;b<c;d<e;f
.Notethatwehave
14
l

a<f

r
.Weconsiderthefollowingtwopossiblescenarios:whentwoofthethreespecialists
aresiblings,andwhennoneare.
If
""
a;b
y
and
""
c;d
y
aresiblings,thenwehave
""
a;d
y
2B
n
andthus
f
""
a;d
y
g[B

(
l;r
)
nf
""
a;b
y
;""
c;d
y
g
isan
(
l;r
)
-coveringsetofsmallercomplexity,leadingtoacontradiction.Theequivalentargumentholdsif
""
c;d
y
and
""
e;f
y
aresiblings.
Ifnonearesiblings,thenlet
""
c
0
;d
0
y
bethesiblingof
""
c;d
y
andlet
""
C;D
y
betheparentof
""
c;d
y
and
""
c
0
;d
0
y
.
Notethat
a;b<c
0
;d
0
;c;d
and
c
0
;d
0
;c;d<e;f
andhence
l<C<D<r
.Ifanancestorof
""
C;D
y
isin
B

(
l;r
)
,then
B

(
l;r
)
nf
""
c;d
y
g
isan
(
l;r
)
-coveringsetofsmallercomplexity,leadingtoa
contradiction.Alternatively,ifnoancestorof
""
C;D
y
isin
B

(
l;r
)
,then
""
c
0
;d
0
y
orsomeofitsproper
descendantsmustbein
B

(
l;r
)
,otherwisethereexistssomevertex
k
0
2
(
c
0
;d
0
)
suchthatthereexists
nospecialist
""
i;j
y
2B

(
l;r
)
suchthat
i

k
0

j
,andtherefore
B

(
l;r
)
wouldnotbean
(
l;r
)
-covering
set.Let
""
p;q
y
beadescendantof
""
c
0
;d
0
y
whichiscontainedin
B

(
l;r
)
.Then
f
""
C;D
y
g[B

(
l;r
)
nf
""
c;d
y
;""
p;q
y
g
isan
(
l;r
)
-coveringsetofsmallercomplexity,leadingtoacontradiction.
Weconcludethattherecanbenomorethan
2
specialistsofthesamedepthin
B

(
l;r
)
forany
u
2
1
;
1
g
n
andany
(
l;r
)
2L
u
.
Wenowproveanupperboundonthemaximumminimum-complexityofan
(
l;r
)
-coveringsetunder
anylabeling
u
.
Corollary10.
Forall
u
2
1
;
1
g
n
,
max
(
l;r
)
2L
u
min
B
(
l;r
)
2D
(
l;r
)

(
B
(
l;r
)
)

2log
n
2
:
(13)
Proof.
Forany
u
2
1
;
1
g
n
,and
(
l;r
)
2L
u
,since
B

(
l;r
)
cancontainatmost
2
specialistsofthe
samedepth(Lemma9)an
(
l;r
)
-coveringsetofminimum-complexitycanhaveatmosttwospecialists
ofdepths
2
;
3
;:::;d
.Thissetcannotcontaintwospecialistsofdepth
1
astheyaresiblings.Thisupper
boundsthemaximumminimum-complexityofany
(
l;r
)
-coveringsetby
2(
d

2)=2log
n
2
.
Finallyweconcludethatforanylabeling
u
2
1
;
1
g
n
ofcut-size

S
(
u
)
,thereexists
C
u
B
n
suchthat
jC
u
j
2log
2
(
n
2

S
(
u
)+1)
.
CProofofProposition6
Firstrecallthepropositionstatement.
Proposition11.
Foralinearizedgraph
S
,forcomparators

;

0
2

jF
n
j
thatareminimal-
consistentwith
u
and
u
0
respectively,
J
F
n
(

;

0
)

min(2
H
(
u
;
u
0
)
;

S
(
u
0
)+1)
:
Proof.
Weprovebothinequalitiesseparately.Weprove
J
F
n
(

;

0
)


S
(
u
0
)+1
.This
followsdirectlyfromthefactthat
J
E
(

;

0
):=
jf
""
2E
:

""
=0
;
0
""
6
=0
gj
andtherefore
J
F
n
(

;

0
)
jf
""
2F
n
:

0
""
6
=0
gj
=
S
(
u
0
)+1
.
Wenowprove
J
F
n
(

;

0
)

2
H
(
u
;
u
0
)
.Recallthatif
u
6
=
u
0
thenbyoftheminimal-
consistentcomparators

and

0
,theset
f
""
2F
n
:

""
=0
;
0
""
6
=0
g
correspondstothesetof
maximum-sizedcontiguoussegmentsofverticesin
S
sharingthesamelabelinthelabeling
u
0
that
didnotexistinthelabeling
u
.Fromhereonwerefertoamaximum-sizedcontiguoussegmentas
justacontiguoussegment.
Whenswitchingfromlabeling
u
to
u
0
,weconsiderthefollowingthreecases.Firstwhenanon-cut
edge(withrespectto
u
)becomesacutedge(withrespectto
u
0
),secondwhenacutedge(with
respectto
u
)becomesanon-cutedge(withrespectto
u
0
),andlastlywhenacutedgeremainsacut
edge,butthelabelingofthetwocorrespondingverticesare`swapped'.
15
Formallythen,foranedge
(
i;j
)
2
E
S
suchthat
[
u
i
=
u
j
]
^
[
u
0
i
6
=
u
0
j
]
thereexiststwonew
contiguoussegmentsofverticessharingthesamelabelthatdidnotexistinthelabeling
u
,their
boundarybeingtheedge
(
i;j
)
.
Converselyforanedge
(
i;j
)
2
E
S
suchthat
[
u
i
6
=
u
j
]
^
[
u
0
i
=
u
0
j
]
thereexistsonenewcontiguous
segmentofverticessharingthesamelabelthatdidnotexistinthelabeling
u
,thatsegmentwill
containtheedge
(
i;j
)
.
Finallyforanedge
(
i;j
)
2
E
S
suchthat
[[
u
i
6
=
u
j
]
^
[
u
0
i
6
=
u
0
j
]]
^
[[
u
i
6
=
u
0
i
]
_
[
u
j
6
=
u
0
j
]]
thereexists
twonewcontiguoussegmentsofverticessharingthesamelabelthatdidnotexistinthelabeling
u
,
theirboundarybeingtheedge
(
i;j
)
.
Weconcludethatthenumberofnewcontiguoussegmentsofverticessharingthesamelabelthatdid
notexistinthelabeling
u
isupperboundedby
2
X
(
i;j
)
2
E
S
[[
u
i
6
=
u
j
]
_
[
u
0
i
6
=
u
0
j
]]
^
[[
u
i
6
=
u
0
i
]
_
[
u
j
6
=
u
0
j
]]
:
DProofofCorollary7
Firstrecallthecorollarystatement.
Corollary11.
Foraconnected
n
-vertexgraph
G
andwithrandomlysampledspine
S
,thenumber
ofmistakesmadeinpredictingtheonlinesequence
(
i
1
;y
1
)
;:::;
(
i
T
;y
T
)
bytheSCSalgorithmwith
optimally-tuned

isupperboundedwithbasis
F
n
by
O
0
@

1
log
n
+
j
K

1
X
i
=1
H
(
u
k
i
;
u
k
i
+1
)(log
n
+log
j
K
j
+loglog
T
)
1
A
andwithbasis
B
n
by
O
0
@
0
@

1
log
n
+
j
K

1
X
i
=
i
H
(
u
k
i
;
u
k
i
+1
)(log
n
+log
j
K
j
+loglog
T
)
1
A
log
n
1
A
foranysequenceoflabelings
u
1
;:::;
u
T
2
1
;
1
g
n
suchthat
u
t;i
t
=
y
t
forall
t
2
[
T
]
.
Proof.
SinceAlgorithm1hasaconservativeupdate,wemayignoretrialsonwhichnomistakeis
madeandthusfromthepointofviewofthealgorithmamistakeismadeoneverytrial,wewill
thereforeassumethat
T
=
M
.Thiswillleadtoaself-referentialmistakeboundintermsofthe
numberofmistakesmadewhichwewilltheniterativelysubstituteintoitself.
Let
c
:=log
2
e
,wewillusethefactthat
log
2
(
1
1

x
y
+
x
)

c
x
y
for
x;y>
0
.Wewilloptimally
tune

togiveourtunedmistakeboundforageneralbasisset
E
,andthenderivetheboundsforbases
F
n
and
B
n
respectively.Thevalueof

thatminimizes(4)is

=
j
K

1
P
i
=1
J
E


k
i
;

k
i
+1

T
P
t
=1
1
ˇ
t
+
j
K

1
P
i
=1
J
E


k
i
;

k
i
+1

;
(14)
whichwhensubstitutedintothesecondtermof(4)gives
M
E

1
ˇ
1
log
jEj
+
c
j
K

1
X
i
=1
J
E


k
i
;

k
i
+1

+
j
K

1
X
i
=1
J
E


k
i
;

k
i
+1

log
jEj

:
(15)
Wenowupperbound
1

forsubstitutioninthelasttermof(15)forbases
F
n
and
B
n
separately.
16
Basis
F
n
:
For
F
n
observethat
jEj
=
n
2
+
n
,andsinceanylabeling
u
t
2
1
;
1
g
n
ofcut-size

S
(
u
t
)
iscoveredby

S
(
u
t
)+1
specialists,wehavethat
ˇ
t
=1
=

S
(
u
t
)+1)
onalltrials.We
letthenumberofmistakesmadebySCSwithbasis
F
n
bedenotedby
M
F
n
.Thus
(15)
immediately
becomes
M
F
n


1
+1)log
jF
n
j
+
c
j
K

1
X
i
=1
J
F
n


k
i
;

k
i
+1

+
j
K

1
X
i
=1
J
F
n


k
i
;

k
i
+1

log
jF
n
j

:
(16)
Toupperbound
1

wenotethatif

k
i
6
=

k
i
+1
then
J
F
n


k
i
;

k
i
+1


1
,andthatfor
F
n
,
1
ˇ
i
=
k
i
+1

n
,thusfrom(14)wehave
1

=1+
T
P
t
=1
1
ˇ
t
j
K

1
P
i
=1
J
F
n


k
i
;

k
i
+1


1+
nT
j
K
j
1

nT
+
j
K
j
1
j
K
j
1

(
n
+1)
T
j
K
j
1
:
Substituting
1


(
n
+1)
T
j
K

1
into(16)gives
M
F
n


1
+1)log
jF
n
j
+
j
K

1
X
i
=1
J
F
n


k
i
;

k
i
+1


log(
e
jF
n
j
)+log(
n
+1)+log
T
j
K
j
1

(17)
Wenowshowourmethodtoreducethe
log
T
terminourboundto
loglog
T
bysubstitutingthe
self-referentialmistakeboundintoitself.Wesimplify(17)andsubstitute
T
=
M
F
n
,
M
F
n


1
+1)log
jF
n
j
+
j
K

1
X
i
=1
J
F
n


k
i
;

k
i
+1

log

e
jF
n
j
(
n
+1)
j
K
j
1

|
{z
}
=:
Z
+
j
K

1
X
i
=1
J
F
n


k
i
;

k
i
+1

|
{z
}
=:
J
log
M
F
n
Z
+
J
log(
Z
+
J
log
M
F
n
)
Z
+
J
log
Z
+
J
log
J
+
J
loglog
M
F
n
;
using
log(
a
+
b
)

log(
a
)+log(
b
)
for
a;b

2
.Weusethefactthat
J
=
O
(
n
j
K
j
)
togive
J
log
J
=
O
(
J
log(
n
j
K
j
))
andsimilarly
J
log
Z
=
O
(
J
log
1
log
n
+
J
log
n
))
=
O
(
J
log((
n
+
J
)log
n
)))
=
O
(
J
log(
n
+
J
))
=
O
(
J
log(
n
j
K
j
))
;
togive
M
F
n
O
0
@

1
log
n
+
j
K

1
X
i
=1
J
F
n


k
i
;

k
i
+1

(log
n
+log
j
K
j
+loglog
T
)
1
A
:
Basis
B
n
:
For
B
n
weapplythesametechniqueasabove,butobservethefollowing.Without
lossofgeneralityassume
n
=2
r
forsomeinteger
r
,wethenhave
jEj
=4
n

2
.Weletthenumber
ofmistakesmadebySCSwithbasis
B
n
bedenotedby
M
B
n
.Thusforbasis
B
n
(15)becomes
M
B
n

2log
n
2

1
+1)log
jB
n
j
+
c
j
K

1
X
i
=1
J
B
n


k
i
;

k
i
+1

+
j
K

1
X
i
=1
J
B
n


k
i
;

k
i
+1

log
jB
n
j

:
(18)
17
Recallproposition3(that
jC
u
j
2log
2
(
n
2

S
(
u
)+1)
)andsince
ˇ
t
=
1
jC
u
j
,thenforanylabeling
u
t
2
1
;
1
g
n
ofcut-size

S
(
u
t
)
wehave
1

S
(
u
t
)+1)log
n
2

ˇ
t

1

S
(
u
t
)+1
.Wethenapplythe
sameargumentupperbounding
1

,
1

=1+
T
P
t
=1
1
ˇ
t
j
K

1
P
i
=1
J
B
n


k
i
;

k
i
+1


1+
2
n
log

n
2

T
j
K
j
1

2
n
log

n
2

T
+
j
K
j
1
j
K
j
1


2
n
log

n
2

+1

T
j
K
j
1
;
andsubstituting
1


(2
n
log(
n
2
)+1)
T
j
K

1
intothelasttermof(18)gives
M
B
n

2log
2
n
2

1
+1)log
jB
n
j
+
j
K

1
X
i
=1
J
B
n


k
i
;

k
i
+1


c
+log
jB
n
j
+ln2
n
+log
T
j
K
j
1
+loglog
n

:
Applyingthesamerecursivetechniqueasaboveyieldsaboundof
M
B
n
O
0
@

1
(log
n
)
2
+
j
K

1
X
i
=1
J
B
n


k
i
;

k
i
+1

(log
n
+log
j
K
j
+loglog
T
)
1
A
:
Usingthesameargumentgiveninproposition3foranytwolabelings
u
;
u
0
2
1
;
1
g
n
,for
twoconsistentwell-formedcomparators

;

0
2

jB
n
j
respectively,andfortwoconsistentwell-
formedcomparators
^

;
^

0
2

jF
n
j
,wehavethat
J
B
n
(

;

0
)

2log
n
2
J
F
n
(
^

;
^

0
)
.Finallyweuse
J
F
n

2
H
(
u
;
u
0
)
fromProposition6tocompletetheproof.
EProofofProposition8
Proof.
Usingatime-dependent

wecanre-write(4)as
M
E

1
ˇ
1
log
jEj
+
T
X
t
=1
1
ˇ
t
log
1
1


t
+
j
K

1
X
i
=1
J
E


k
i
;

k
i
+1

log
jEj

k
i
+1
;
(19)
andletting

t
:=
1
t
+1
,andletting
c
:=log
2
e
,givesthefollowing,
M
E

1
ˇ
1
log
jEj
+
T
X
t
=1
1
ˇ
t
log
1
1

1
t
+1
+
j
K

1
X
i
=1
J
E


k
i
;

k
i
+1

log(
jEj
(
k
i
+1
+1))
(20)

1
ˇ
1
log
jEj
+
c
T
X
t
=1
1
ˇ
t
1
t
+
j
K

1
X
i
=1
J
E


k
i
;

k
i
+1

log(
jEj
T
)
(21)

1
ˇ
1
log
jEj
+
c

max
t
2
[
T
]
1
ˇ
t

T
X
t
=1
1
t
+
j
K

1
X
i
=1
J
E


k
i
;

k
i
+1

log(
jEj
T
)
(22)

1
ˇ
1
log
jEj
+

max
t
2
[
T
]
1
ˇ
t

log(
eT
)+
j
K

1
X
i
=1
J
E


k
i
;

k
i
+1

log(
jEj
T
)
(23)
wherethestepfrom
(20)
to
(21)
hasused
log
2
(1+
x
)

cx
for
x>
0
,andthestepfrom
(22)
to
(23)
hasused
P
t
2
[
T
]
1
t
<
R
T
1
1
t
dt
+1=ln(
eT
)=
1
c
log
2
(
eT
)
.
18
Wenowusethefollowingupperboundon
max
t
2
[
T
]
1
ˇ
t
,
max
t
2
[
T
]
1
ˇ
t

1
ˇ
1
+
j
K

1
X
i
=1
J
E
(

k
i
;

k
i
+1
)
;
andtheassumptionthat
j
K

1
P
i
=1
J
E
(

k
i
;

k
i
+1
)

1
ˇ
1
,togive
max
t
2
[
T
]
1
ˇ
t

2
j
K

1
X
i
=1
J
E
(

k
i
;

k
i
+1
)
:
(24)
Substituting(24)into(23)thengives
M
E

1
ˇ
1
log
jEj
+2
j
K

1
X
i
=1
J
E


k
i
;

k
i
+1


log(
eT
)+
1
2
log(
jEj
T
)

=
1
ˇ
1
log
jEj
+2
j
K

1
X
i
=1
J
E


k
i
;

k
i
+1


1
2
log(
jEj
)+log(
e
)+
3
2
log(
T
)

Usingaconservativeupdate(seesection3.1),wesimilarlyset

t
:=
1
m
+1
,where
m
isthecurrent
numberofmistakesofthealgorithm.Wenextusethesame`recursivetrick'asthatintheproofof
Corollary7.Theprooffollowsanalogously,leaving
M
F
n
O
0
@

1
log
n
+
j
K

1
X
i
=1
J
F
n


k
i
;

k
i
+1

(log
n
+log
j
K
j
+loglog
T
)
1
A
forthebasisset
F
n
,and
M
B
n
O
0
@

1
(log
n
)
2
+
j
K

1
X
i
=1
J
B
n


k
i
;

k
i
+1

(log
n
+log
j
K
j
+loglog
T
)
1
A
forthebasisset
B
n
.
FTheSwitchingGraphPerceptron
InthissectionforcompletenessweprovidethekernelizedPerceptronalgorithmforswitchinggraph
prediction.Thealgorithmisdescribedandamistakeboundgivenfortheswitching-graphlabeling
problemin[18,Sec.6.2].
Thekeytotheapproachistousethefollowinggraphkernel(introducedby[
19
])
K
:=
L
+
G
+
R
L
11
>
with
R
L
:=max
i
(
e
>
i
L
+
G
e
i
)
,where
L
+
G
denotesthepseudo-inverseofthegraphLaplacian,andfor
i
2
[
n
]
,welet
e
i
denotethe
i
-thunitbasisvector,i.e.,
e
i;i
0
=0
if
i
6
=
i
0
andequals
1
if
i
0
=
i
.The
norminducedbythiskernelisdenoted
k
u
k
K
:=
p
u
>
K

1
u
.
GFurtherDetailsonExperiments
InthissectionwegivefurtherdetailsontheexperimentalmethodsofSection4.Datawascollected
spanning72hoursfrom
4:55
am
on
8
th
April
2019
to
4:55
amon
11
th
April
2019
.Anystations
thatwerenotinserviceduringanyofthe
72
hourswereremoved(inthiscasetherewasonlyone
suchstation).
AsdescribedinSection4,thevariablemeasuredwasthepercentageofoccupieddocksineachstation,
andathresholdof
50%
wassettoinduceabinarylabeling.Anystationswhoseinducedlabelingdid
notchangeoverthe
72
hourswerealsoremovedfromthedataset.Thisleftagraphof
404
stations.
The
24
hoursofdatawereusedforparameterselection.Parametersweretunedusingexhaustive
searchovertherangesinTable1,takingthemeanminimizerover
10
iterations.
19
input:
Graph
G
parameter:
>
0
initialize:
w
1
 
0
K
 
L
+
G
+max
i
2
[
n
]
(
e
>
i
L
+
G
e
i
)
11
>
for
t
=1
to
T
do
receive
i
t
2
V
predict
^
y
t
 
sign
(
w
t;i
t
)
receive
y
t
2
1
;
1
g
if
^
y
t
6
=
y
t
then
_
w
t
 
w
t
+
y
t
Ke
i
t
K
i
t
;i
t
if
k
_
w
t
k
K
>
then
w
t
+1
 
_
w
t
k
_
w
t
k
K

else
w
t
+1
 
_
w
t
Algorithm2:
S
WITCHING
G
RAPH
P
ERCEPTRON
Table1:Parameterrangesusedforoptimizingthethreealgorithmswithtunableparameters.
AlgorithmParameterParameterRangeOptimizedParameter
KernelPerceptron

3
:
5

53
:
89
SCS-F

1

10

12

1

10

6
7
:
4

10

10
SCS-B

1

10

5

5

10

4
3
:
0

10

4
Table2:Meanerror

stdover
25
iterationsona
404
-vertexgraphforallalgorithmsandbenchmarks,
andforallensemblesizesofSCS-FandSCS-B.
EnsembleSize
Algorithm
1359173365
SCS-F
1947

491597

321475

301364

281293

261247

211218

19
SCS-B
1438

321198

271127

251079

241050

231032

221021

18
KernelPerceptron
3326

43
------
Local
3411

55
------
Global
4240

44
------
Temporal(Local)
2733

42
------
Temporal(Global)
3989

44
------
20
"
23,Compressed Sensing with Deep Image Prior and Learned Regularization,https://arxiv.org/pdf/1806.06438v4.pdf,https://github.com/davevanveen/compsensing_dip,"CompressedSensingwithDeepImagePriorand
LearnedRegularization
DaveVanVeen
y
vanveen@utexas.edu
AjilJalal
y
ajiljalal@utexas.edu
MahdiSoltanolkotabi
z
soltanol@usc.edu
EricPrice
x
ecprice@cs.utexas.edu
SriramVishwanath
y
sriram@austin.utexas.edu
AlexandrosG.Dimakis
y
dimakis@austin.utexas.edu
Abstract
Weproposeanovelmethodforcompressedsensingrecoveryusinguntraineddeep
generativemodels.OurmethodisbasedontherecentlyproposedDeepImagePrior
(DIP),whereintheconvolutionalweightsofthenetworkareoptimizedtomatchthe
observedmeasurements.Weshowthatthisapproachcanbeappliedtosolveany
differentiablelinearinverseproblem,outperformingpreviousunlearnedmethods.
Unlikevariouslearnedapproachesbasedongenerativemodels,ourmethoddoes
notrequirepre-trainingoverlargedatasets.Wefurtherintroduceanovellearned
regularizationtechnique,whichincorporatespriorinformationonthenetwork
weights.Thisreducesreconstructionerror,especiallyfornoisymeasurements.
Finallyweprovethat,usingtheDIPoptimizationapproach,moderatelyoverparam-
eterizedsingle-layernetworkscanperfectlyanysignaldespitethenonconvex
natureoftheproblem.Thistheoreticalresultprovidesforearly
stopping.
1Introduction
Weconsiderthewell-studiedcompressedsensingproblemofrecoveringanunknownsignal
x

2
R
n
byobservingasetofnoisymeasurements
y
2
R
m
oftheform
y
=
Ax

+
:
(1)
Here
A
2
R
m

n
isaknownmeasurementmatrix,typicallygeneratedwithrandomindependent
Gaussianentries.Sincethenumberofmeasurements
m
issmallerthanthedimension
n
ofthe
unknownvector
x

,thisisanunder-determinedsystemofnoisylinearequationsandhenceill-posed.
Therearemanysolutions,andsomestructuremustbeassumedon
x

tohaveanyhopeofrecovery.
Pioneeringresearch[
18
,
8
,
10
]establishedthatif
x

isassumedtobesparseinaknownbasis,asmall
numberofmeasurementswillbeprovablysufcienttorecovertheunknownvectorinpolynomial
timeusingmethodssuchasLasso[75].
Sparsityapproacheshaveprovensuccessful,butmorecomplexmodelswithadditionalstructure
havebeenrecentlyproposedsuchasmodel-basedcompressivesensing[
5
]andmanifoldmodels[
33
,

Equalcontribution
y
DepartmentofElectricalandComputerEngineering,UniversityofTexas.Austin,TX
z
DepartmentofElectricalEngineering,UniversityofSouthernCalifornia.LosAngeles,CA
x
DepartmentofComputerScience,UniversityofTexas.Austin,TX
arXiv:1806.06438v4  [stat.ML]  29 Oct 202032
,
21
].Boraetal.[
7
]showedthatdeepgenerativemodelscanbeusedasexcellentpriorsfor
images.Theyalsoshowedthatbackpropagationcanbeusedtosolvethesignalrecoveryproblemby
performinggradientdescentinthegenerativelatentspace.Thismethodenabledimagegenerationwith
fewermeasurementscomparedtoLassoforagivenreconstructionerror.Compressed
sensingusingdeepgenerativemodelswasfurtherimprovedinveryrecentwork[
77
,
25
,
37
,
74
,
23
,
3
].
Additionallyatheoreticalanalysisofthenonconvexgradientdescentalgorithm[
7
]wasproposedby
Handetal.[29]undersomeassumptionsonthegenerativemodel.
Inspiredbytheseimpressiveofdeepgenerativemodels,wechosetoinvestigatethepotential
applicationofsuchmethodsformedicalimaging,acanonicalapplicationofcompressivesensing.A
problem,however,isthatallthesepreviousmethodsrequiretheexistenceof
pre-trained
models.Whilethishasbeenachievedforvarioustypesofimages,e.g.humanfacesofCelebA[45]
viaDCGAN[
66
],itremainsmorechallengingformedicalimages[
85
,
71
,
58
,
72
].
Insteadofaddressingthisproblemingenerativemodels,wefoundaneasierwaytocircumventit.
SurprisingrecentworkbyUlyanovetal.[
79
]proposedDeepImagePrior(DIP),whichuses
untrained
convolutionalneuralnetworks.InDIP-basedschemes,aconvolutionalneuralnetworkgenerator
(e.g.DCGAN)isinitializedwithrandomweights;theseweightsaresubsequentlyoptimizedtomake
thenetworkproduceanoutputasclosetothetargetimageaspossible.Thisprocedureisunlearned,
usingnopriorinformationfromotherimages.Thepriorisenforcedonlybytheedconvolutional
structureofthegeneratornetwork.
GeneratorsusedforDIParetypicallyover-parameterized,i.e.thenumberofnetworkweightsismuch
largercomparedtotheoutputdimension.ForthisreasonDIPhasempiricallybeenfoundtoovto
noiseifrunfortoomanyiterations:Thereconstructionerrorinitiallydecreasesandthenplateaus,
atapproximately
500
iterations,asthenetworktheoriginalimage.Then,atroughly
10
;
000
iterations,theerrordecreasesfurther,asthenetworkstartsthenoise[
79
].Earlystoppingis
aheuristicintendedtoterminatetheoptimizationprocedurewithinthisplateauregion,andavoid
ovetonoise.Inthispaperwetheoreticallyprovethatthisovphenomenonoccurswith
gradientdescentforanysignalandhencejustifytheuseofearlystoppingandotherregularization
methods.
OurContributions:
Ł
InSection3weproposeDIPforcompressedsensing(CS-DIP).Ourbasicmethodisas
follows.InitializeaDCGANgeneratorwithrandomweights;usegradientdescentto
optimizetheseweightssuchthatthenetworkproducesanoutputwhichagreeswiththe
observedmeasurementsasmuchaspossible.Thisunlearnedmethodcanbeimprovedwitha
novel
learnedregularization
technique,whichregularizestheDCGANweightsthroughout
theoptimizationprocess.
Ł
InSection4wetheoreticallyprovethatDIPwillanysignaltozeroerrorwithgradient
descent.Ourresultisestablishedforanetworkwithasinglehiddenlayerandsuf
constantfractionover-parametrization.Whileitisexpectedthatover-parametrizedneural
networkscananysignal,thefactthatgradientdescentcanprovablysolvethisnon-convex
problemisinterestingandprovidestheoreticalforearlystopping,aphenomenon
empiricallybyUlyanovetal.[79].
Ł
InSection5weempiricallyshowthatCS-DIPoutperformspreviousunlearnedmethods
inmanycases.Whilepre-trainedorﬁlearnedﬂmethodsfrequentlyperformbetter[
7
],we
havetheadvantageofnotrequiringagenerativemodeltrainedoverlargedatasets.Assuch,
wecanapplyourmethodtovariousmedicalimagingdatasetsforwhichdataacquisitionis
expensiveandgenerativemodelsarediftotrain.
2Background
2.1CompressedSensing:ClassicalandUnlearnedApproaches
Aclassicalassumptionmadeincompressedsensingisthatthevector
x

is
k
-sparseinsomebasissuch
aswaveletordiscretecosinetransform(DCT).Findingthesparsestsolutiontoanunderdetermined
linearsystemofequationsisNP-hardingeneral;however,ifthematrix
A
conditionssuchas
theRestrictedEigenvalueCondition(REC)orRestrictedIsometryProperty(RIP)[
9
,
6
,
18
,
75
],then
2
x

canberecoveredinpolynomialtimeviaconvexrelaxations[
78
]oriterativemethods.Thereis
extensivecompressedsensingliteratureregardingassumptionson
A
,numerousrecoveryalgorithms,
andvariationsofRIPandREC[6,57,1,4,46].
Compressedsensingmethodshavefoundmanyapplicationsinimaging,forexamplethesingle-pixel
camera(SPC)[
20
].Medicaltomographicapplicationsincludex-rayradiography,microwaveimaging,
magneticresonanceimaging(MRI)[
84
,
13
,
47
].Obtainingmeasurementsformedicalimagingcan
becostly,time-consuming,andinsomecasesdangeroustothepatient[
65
].Assuch,animportant
goalistoreducethenumberofmeasurementswhilemaintaininggoodreconstructionquality.
Asidefromtheclassicaluseofsparsity,recentworkhasusedotherpriorstosolvelinearinverse
problems.Plug-and-playpriors[
81
,
11
]andRegularizationbyDenoising[
67
]haveshownhow
imagedenoiserscanbeusedtosolvegenerallinearinverseproblems.Akeyexampleofthisis
BM3D-AMP,whichappliesaBlock-Matchingand3D(BM3D)denoisertoanApproximate
MessagePassing(D-AMP)algorithm[
55
,
54
].AMPhasalsobeenappliedtolinearmodelsinother
contexts[
73
].AnotherrelatedalgorithmisTVAL3[
86
,
43
]whichleveragesaugmentedLagrangian
multiplierstoachieveimpressiveperformanceoncompressedsensingproblems.Inmanydifferent
settings,wecompareouralgorithmtothesepriormethods:BM3D-AMP,TVAL3,andLasso.
2.2CompressedSensing:LearnedApproaches
Whilesparsityinsomechosenbasisiswell-established,recentworkhasshownbetterempirical
performancewhenneuralnetworksareused[
7
].Thissuccessisattributedtothefactthatneural
networksarecapableoflearningimagepriorsfromverylargedatasets[
24
,
38
].Thereis
recentworkonsolvinglinearinverseproblemsusingvariouslearnedtechniques,e.g.recurrent
generativemodels[
51
]andauto-regressivemodels[
15
].Additionallyapproximatemessagepassing
(AMP)hasbeenextendedtoalearnedsettingbyMetzleretal.[53].
Boraetal.[
7
]istheclosesttoourset-up.Inthisworktheauthorsassumethattheunknownsignalis
intherangeofapre-trainedgenerativemodelsuchasagenerativeadversarialnetwork(GAN)[
24
]
orvariationalautoencoder(VAE)[
38
].Therecoveryoftheunknownsignalisobtainedviagradient
descentinthelatentspacebysearchingforasignalthatthemeasurements.Thiscanbe
directlyappliedforlinearinverseproblemsandmoregenerallytoanydifferentiablemeasurement
process.Recentworkhasbuiltuponthesemethodsusingnewoptimizationtechniques[
12
],uncer-
taintyautoencoders[
26
],andotherapproaches[
16
,
37
,
56
,
62
,
70
,
28
].Thekeypointisthatallthis
priorworkrequirespre-trainedgenerativemodels,incontrasttoCS-DIP.Finally,thereis
ongoingworktounderstandDIPanddeveloprelatedapproaches[30,31,17].
3ProposedAlgorithm
Let
x

2
R
n
bethesignalthatwearetryingtoreconstruct,
A
2
R
m

n
bethemeasurement
matrix,and

2
R
m
beindependentnoise.Giventhemeasurementmatrix
A
andtheobservations
y
=
Ax

+

,wewishtoreconstructan
^
x
thatiscloseto
x

.
Agenerativemodelisadeterministicfunction
G
(

;
w
):
R
k
!
R
n
whichtakesasinput
z
2
R
k
and
isparameterizedbyﬁweightsﬂ
w
2
R
d
,producinganoutput
G
(
z
;
w
)
2
R
n
.Thesemodelshave
shownexcellentperformancegeneratingreal-lifesignalssuchasimages[
24
,
38
]andaudio[
80
].We
investigatedeepconvolutionalgenerativemodels,aspecialcaseinwhichthemodelarchitecture
hasmultiplecascadedlayersofconvolutional[
39
].Inthispaperwerestrictthesignaltobe
imagesandapplyaDCGAN[
66
]model.ThismodelarchitecturecontrastswithUlyanovetal.[
79
],
whoemployaU-net[
68
].Byusingadifferentconvolutionalarchitecture,wefurthersupportthe
workinghypothesisthatnetworkstructure,notrepresentationlearning,isthekeycomponentin
imagereconstruction.OurchoiceofDCGANalsoallowsforaclearercomparisonintheCSsetting,
e.g.learnedmethodsofBoraetal.[
7
]whichemploythisarchitecture.Lastly,we'vefoundDCGAN
tohave
5
-
10

fasterruntimethanU-net.
3.1CompressedSensingwithDeepImagePrior(CS-DIP)
Ourapproachistoasetofweightsfortheconvolutionalnetworksuchthatthemeasurement
matrixappliedtothenetworkoutput,i.e.
AG
(
z
;
w
)
,matchesthemeasurements
y
wearegiven.
3
Henceweinitializean
untrained
network
G
(
z
;
w
)
withsomeed
z
andsolvethefollowing:
w

=argmin
w
k
y

AG
(
z
;
w
)
k
2
:
(2)
Thisis,ofcourse,anon-convexproblembecause
G
(
z
;
w
)
isacomplexfeed-forwardneuralnetwork.
Stillwecanusegradient-basedoptimizersforanygenerativemodelandmeasurementprocessthatis
differentiable.GeneratornetworkssuchasDCGANarebiasedtowardsmooth,naturalimagesdueto
theirconvolutionalstructure;thusthenetworkstructurealoneprovidesagoodpriorforreconstructing
imagesinproblemssuchasinpaintinganddenoising[
79
].Ouristhatthisappliestogeneral
linearmeasurementprocesses.Furthermore,ourmethodalsodirectlyappliestoanydifferentiable
forwardoperator
A
.Werestrictoursolutiontolieinthespanofaconvolutionalneuralnetwork.Ifa
sufnumberofmeasurements
m
isgiven,weobtainanoutputsuchthat
x

ˇ
G
(
z
;
w

)
.
Notethatthismethodusesanuntrainedgenerativemodelandoptimizesoverthenetworkweights
w
.
Incontrastpreviousmethods,suchasthatofBoraetal.[
7
],useatrainedmodelandoptimizeover
thelatent
z
-space,solving
z

=argmin
z
k
y

AG
(
z
;
w
)
k
2
.Weinsteadinitializearandom
z
with
Gaussiani.i.d.entriesandkeepthisedthroughouttheoptimizationprocess.
Inouralgorithmweleveragethewell-establishedtotalvariationregularization[
69
,
82
,
44
],denoted
as
TV
(
G
(
z
;
w
))
.Wealsoproposeanadditionallearnedregularizationtechnique,
LR
(
w
)
;notethat
withoutthistechnique,i.e.when

L
=0
,ourmethodiscompletelyunlearned.Lastlyweuseearly
stopping,aphenomenonthatwillbetheoreticallyinSection4.
Thustheoptimizationproblembecomes
w

=argmin
w
k
y

AG
(
z
;
w
)
k
2
+
R
(
w
;

T
;
L
)
:
(3)
Theregularizationtermcontainshyperparameters

T
and

L
fortotalvariationandlearnedregular-
ization:
R
(
w
;

T
;
L
)=

T
TV
(
G
(
z
;
w
))+

L
LR
(
w
)
.Nextwediscussthis
LR
(
w
)
term.
3.2LearnedRegularization
WithoutlearnedregularizationCS-DIPreliesonlyonlinearmeasurementstakenfromoneunknown
image.Wenowintroduceanovelmethodwhichleveragesasmallamountoftrainingdatatooptimize
regularization.Inthiscasetrainingdatareferstomeasurementsfromadditionalgroundtruthofa
similartype,e.g.measurementsfromotherx-rayimages.
Toleveragethisadditionalinformation,weposeEqn.3asaMaximumaPosteriori(MAP)estimation
problemandproposeanovelpriorontheweightsofthegenerativemodel.Thispriorthenactsasa
regularizationterm,penalizingthemodeltowardanoptimalsetofweights
w

.
Forasetofweights
w
2
R
d
,wemodelthe
likelihood
ofthemeasurements
y
=
Ax;y
2
R
m
;
and
thepriorontheweights
w
asGaussiandistributionsgivenby
p
(
y
j
w
)=
exp


k
y

AG
(
z
;
w
)
k
2
2

L

p
(2
ˇ
L
)
m
;
p
(
w
)=
exp


1
2
(
w


)
T


1
(
w


)

p
(2
ˇ
)
d
j

j
;
where

2
R
d
and

2
R
d

d
.
Inthissettingwewanttoasetofweights
w

thatmaximizesthelogposterioron
w
given
y
,i.e.,
w

=argmax
w
p
(
w
j
y
)

argmin
w
k
y

AG
(
z
;
w
)
k
2
+

L
(
w


)
T


1
(
w


)
:
(4)
Thisgivesusthelearnedregularizationterm
LR
(
w
)=(
w


)
T


1
(
w


)
;
(5)
wherethecoef

L
inEqn.4controlsthestrengthoftheprior.
OurmotivationforassumingaGaussiandistributionontheweightsistobuildupontheprovensuccess
of
`
2
regularization,whichalsomakesthisassumption.Noticethatwhen

=0
and
=
I
d

d
;
4
thisregularizationtermisequivalentto
`
2
-regularization.Thusthismethodcanbethoughtofas
anadaptiveversionofstandardweightdecay.Further,becausethenetworkweightsareinitialized
Gaussiani.i.d.,weassumedtheoptimizedweightswouldalsobeGaussian.Previousworkhas
shownevidencethattheconvolutionalweightsinatrainednetworkdoindeedfollowaGaussian
distribution[49].
3.2.1LearningthePriorParameters
Intheprevioussection,weintroducedthelearnedregularizationterm
LR
(
w
)
inEqn.5.
Howeverwehavenotyetlearnedvaluesforparameters
(


thatincorporatepriorknowledgeof
thenetworkweights.Wenowproposeawaytoestimatetheseparameters.
Assumewehaveasetofmeasurements
S
Y
=
f
y
1
;y
2
;

;y
Q
g
from
Q
differentimages
S
X
=
f
x
1
;x
2
;

;x
Q
g
,eachobtainedwithadifferentmeasurementmatrix
A
.Foreachmeasurement
y
q
;q
2f
1
;
2
;:::;Q
g
;
werunCS-DIPtosolvetheoptimizationprobleminEqn.3andobtainan
optimalsetofweights
W

=
f
w

1
;w

2
;

;w

Q
g
.Notethatwhenoptimizingfortheweights
W

;
weonlyhaveaccesstothemeasurements
S
Y
,notthegroundtruth
S
X
.
Thenumberofweights
d
indeepnetworkstendstobeverylarge.Assuch,learningadistributionover
eachweight,i.e.estimating

2
R
d
and

2
R
d

d
,becomesintractable.Weinsteadusealayer-wise
approach:with
L
networklayers,wehave

2
R
L
and

2
R
L

L
.Thuseachweightwithinlayer
l
2f
1
;
2
;:::;L
g
ismodeledaccordingtothesame
N
(

l
;

ll
)
distribution.Forsimplicityweassume

ij
=0
8
i
6
=
j
,i.e.thatnetworkweightsareindependentacrosslayers.Theprocessofestimating
statistics
(


from
W

isdescribedinAlgorithm1.
Weusethislearned
(


intheregularizationterm
LR
(
w
)
fromEqn.5forreconstructingmea-
surementsofimages.Werefertothistechniqueas
learnedregularization
.Whilethismayseem
analogoustobatchnormalization[
35
],notethatweonlyuse
(


topenalizethe
`
2
-normofthe
weightsanddonotnormalizethelayeroutputsthemselves.
3.2.2DiscussionofLearnedRegularization
TheproposedCS-DIPdoesnotrequiretrainingifnolearnedregularizationisused,i.e.if

L
=0
in
Eqn.3.ThismeansthatCS-DIPcanbeappliedonlywithmeasurementsfromasingleimageandno
priorinformationofsimilarimagesinadataset.
Ournextidea,learnedregularization,utilizesasmallamountofpriorinformation,requiringaccess
tomeasurementsfromasmallnumberofsimilarimages(roughly
10
).Incontrast,otherpre-trained
modelssuchasthatofBoraetal.[
7
]requireaccesstogroundtruthfromamassivenumberofsimilar
images(tensofthousandsforCelebA).Ifsuchalargedatasetisavailable,andifagoodgenerative
modelcanbetrainedonthatdataset,weexpectthatpre-trainedmodelswouldoutperformourmethod.
Ourapproachisinsteadmoresuitableforreconstructingproblemswherelargeamountsofdataor
goodgenerativemodelsarenotreadilyavailable.
4TheoreticalResults
Inthissectionweprovidetheoreticalevidencetohighlighttheimportanceofearlystoppingfor
DIP-basedapproaches.Herewefocusondenoisinganoisysignal
y
2
R
m
byoptimizingover
networkweights.Thisproblemtakestheform:
min
w
L
(
w
):=
k
y

AG
(
z
;
w
)
k
2
:
(6)
Wefocusongeneratorsconsistingofasinglehidden-layerReLUnetworkwith
k
inputs,
d
hidden
units,and
n
outputs.Using
w
=(
W;V
)
thegeneratormodelinthiscaseisgivenby
G
(
z
;
W;V
)=
V

ReLU
(
Wz
)
;
(7)
where
z
2
R
k
istheinput,
W
2
R
d

k
theinput-to-hiddenweights,and
V
2
R
n

d
thehidden-to-
outputweights.Wehencetrainover
W
usinggradientdescentandassume
V
ised
5
.Withthese
formulationsinplace,wearenowreadytostateourtheoreticalresult.
5
Wenotethatourtheoreticalframeworkcanalsoallowfortrainingover
V
.Infact,sincetheproblemis
quadraticover
V
,thisanalysisisinprinciplenotmuchmoredifcult.However,weavoidthisasitmakesour
proofsmorediftofollowwithoutaddinganyfurtherinsight.
5
Theorem4.1.
Considermeasurements
AG
(
z
;
W;V
)
fromtheoutputofageneratoroftheform
W
7!
G
(
z
;
W;V
)=
V

ReLU
(
Wz
)
toasignal
y
2
R
m
with
A
2
R
m

n
,
z
2
R
k
,
W
2
R
d

k
,
V
2
R
n

d
,and
ReLU
(
z
)=max(0
;z
)
.Furthermore,let
A
beamatrixwithorthonormalrows
(i.e.
AA
T
=
I
m
)andassume
V
isarandommatrixwithi.i.d.
N
(0
;
2
)
entrieswith

=
1
p
dm
k
y
k
k
z
k
.
Startingfromaninitialweightmatrix
W
0
selectedatrandomwithi.i.d.
N
(0
;
1)
entries,werun
gradientdescentupdatesoftheform
W
˝
+1
=
W
˝


rL
(
W
˝
)
ontheloss
L
(
W
)=
1
2
k
AV

ReLU
(
Wz
)

y
k
2
;
withstepsize

=


k
y
k
2
8
m
4
m
+
d
where



1
.Assumingthat
d

Cm;
with
C
anumerical
constant,then
k
AV

ReLU
(
W
˝
z
)

y
k
3

1



8(4
m
+
d
)

˝
k
y
k
holdsforall
˝
withprobabilityatleast
1

5
e

m=
2

e

d=
2

e

4
d
2
3
m
1
3
.
Ourtheoreticalresultshowsthataftermanyiterativeupdates,gradientdescentwillsolvethisnon-
convexoptimizationproblemandanysignal
y
,ifthegeneratornetworkissufwide.This
occursassoonasthenumberofhiddenunits
d
exceedsthesignalsize
n
byaconstantfactor.Our
theoremdirectlyappliestomanycompressedsensingmeasurementmatrices,inparticularanymatrix
obtainedbysubsamplingtherowsofanorthonormalmatrix(e.g.sub-samplingaFouriermatrix).
Thisispossiblebecause,foranysuchorthonormalmatrix,
AV
hasthesamedistributionasaGaussian
matrixwithi.i.d.entries.ThisresultdemonstratesthatearlystoppingisnecessaryforDIP-based
methodstobesuccessful;otherwisethenetworkcananysignal,includingonethatisnoisy.
OurproofbuildsontheoreticalideasfromOymaketal.[
61
]whichprovideageneralframework
forestablishingglobalconvergenceguaranteesforoverparameterizednonlinearlearningproblems
basedonvariouspropertiesoftheJacobianmappingalongthegradientdescenttrajectory.Whileour
proofleveragesrelevantpriorwork[
19
,
60
],ourargumentisquitespecializedandintricatewithnew
techniquessuchasGordon'sLemma.Thisallowsustohavemoderatenetworkoverparameterization
thatisonlylinearinthenumberofmeasurements,contrarytootherresultsintheliteraturewhich
requireaamountofoverparameterization.Ultimatelywecombinetoolsfromempirical
processtheory,randommatrixtheory,andmatrixalgebratoshowthat,startingfromarandom
initialization,theJacobianmappingacrossalliterateshasfavorablepropertieswithhighprobability,
hencefacilitatingconvergencetoaglobaloptima.
5Experiments
5.1ExperimentalSetup
Measurements:
Weevaluateouralgorithmusingtwodifferentmeasurementsprocesses,i.e.matrices
A
2
R
m

n
.Firstwesettheentriesof
A
tobeGaussiani.i.d.suchthat
A
i;j
˘N
(0
;
1
m
)
.Recall
m
isthenumberofmeasurements,and
n
isthenumberofpixelsinthegroundtruthimage.This
measurementprocessisstandardpracticeincompressedsensingliterature;henceweuseitoneach
dataset.AdditionallyweuseaFouriermeasurementprocesscommoninMRIapplications[
52
,
50
,
27,41,48].
Datasets:
WeuseouralgorithmtoreconstructbothgrayscaleandRGBimages.Forgrayscalewe
usethe100imagesinthetestsetofMNIST[
40
]andalso60randomimagesfromtheShenzhen
ChestX-RayDataset[
36
],downsamplinga
512

512
cropto
256

256
pixels.ForRGBweuse
retinopathyimagesfromtheSTAREdataset[
34
]with
512

512
cropsdownsizedto
128

128
pixels.
Baselines:
Wecompareouralgorithmtostate-of-the-artunlearnedmethodssuchasBM3D-AMP[
55
,
54
],TVAL3[
42
,
43
,
86
],andLassoinaDCTbasis[
2
].WealsoevaluatedtheperformanceofLasso
inaDaubechieswaveletbasis[
14
,
83
]butfoundthisperformedworsethanLasso-DCTonall
datasets.ThushereonwerefertoLasso-DCTasﬁLassoﬂanddonotincluderesultsofLasso-
Wavelet.Weusedsci-kitlearn[
64
]fortheimplementationofLassoandcodeprovidedbytheoriginal
6
Table1:Evaluatingtheoflearnedregularization(LR)onx-rayimageswithvaryinglevelsof
noiseandnumberofmeasurements.Tablevaluesarepercentdecreaseinerror,e.g.at
˙
2

=0
and
m
=500
,LRreducesMSEby
9
:
9%
.Theterm
˙
2

correspondstovarianceofthenoisevector

in
Eqn.1,i.e.eachentryof

isdrawnindependently
N
(0
;
˙
2

=
m
)
.TheseresultsindicatethatLRtends
toprovidegreaterwithnoisysignalsandwithfewermeasurements.
Measurements,
m
˙
2

5001000200040008000
09.9%2.9%0.2%2.0%0.6%
1011.6%4.6%4.5%2.4%1.0%
10014.9%19.2%5.0%3.9%2.8%
100037.4%30.6%19.8%3.0%6.2%
authorsforBM3D-AMPandTVAL3.Astandardgridsearchwasperformedovereachbaselineto
tunehyperparameters.
Metrics:
Toquantitativelyevaluatetheperformanceofouralgorithm,weuseper-pixelmean-squared
error(MSE)betweenthereconstruction
^
x
andtrueimage
x

,i.e.
k
^
x

x

k
2
n
.Notethatbecausethese
pixelsareovertherange
[

1
;
1]
,it'spossibleforMSEtobegreaterthan
1
.
Implementation:
Toasetofweights
w

thatminimizeEqn.3,weusePyTorch[
63
]witha
DCGANarchitecture.Ournetworkhasdepth7andusesconvolutionallayerswithReLUactivations.
WeusetheRMSPropoptimizer[
76
]withlearningrate
10

3
,momentum
0
:
9
,and
1000
updatesteps
foreverysetofmeasurements.Theseparametersarethesameacrossalldatasets.Weinitializeone
randommeasurementmatrix
A
foreachimage.
Moreimplementationdetailscanbefoundintheappendix,suchashyperparametersearch,network
initializations,andearlystoppingcriterion.CodefortheseexperimentsisavailableinourGitHub
repository:
github.com/davevanveen/compsensing_dip
.
5.2ExperimentalResults
5.2.1Results:LearnedRegularization
Weevaluatetheoflearnedregularizationbycomparingouralgorithmwithandwithout
learnedregularization,i.e.

L
=100
and

L
=0
,respectively,whileallotherparametersacross
thiscomparisonareheldconstant.Thelattersettingof

L
=0
isanunlearnedmethod,asweare
notleveraging(


)fromadataset.Intheformersettingof

L
=100
,welearn(


)
fromaparticularsetoftenx-rayimages;wethenevaluateonadifferentsetofx-rayimages.We
comparethesetwosettingswithvaryingnoiseanddifferentnumberofmeasurements.
OurresultsinTable1showthatlearnedregularizationdoesindeedprovideparticularly
withincreasingnoiseorfewermeasurements.ThuswecaninferthatassumingalearnedGaussian
distributionoverweightsisuseful,especiallywhentheoriginalsignalisnoisyory
compressed.Incontrastwesawnoimprovementwithvanilla
`
2
-regularization,indicatingthatthe
oflearnedregularizationcanbeattributedtopriorinformationofsimilarimages.
5.2.2Results:UnlearnedCS-DIP
Fortheremainderofthissection,weevaluateouralgorithminthenoiselesscasewithoutlearned
regularization,i.e.when

=0
inEqn.1and

L
=0
inEqn.3.HenceCS-DIPiscompletely
unlearned;assuch,wecompareittootherstate-of-the-artunlearnedalgorithmsonvariousdatasets
andwithdifferentmeasurementmatrices.
MNIST:
InFigure1bweplotreconstructionerrorwithvaryingnumberofmeasurements
m
of
n
=
784.Thisdemonstratesthatouralgorithmoutperformsbaselinesinalmostallcases.Figure2bshows
reconstructionsfor75measurements,whileremainingreconstructionsareintheappendix.
Chestx-rays:
InFigure1aweplotreconstructionerrorwithvaryingnumberofmeasurements
m
of
n
=65536.Figure2ashowsreconstructionsfor2000measurements;remainingreconstructions
7
(a)MSE-chestx-ray(65536pixels)
(b)MSE-MNIST(784pixels)
Figure1:Per-pixelreconstructionerror(MSE)vs.numberofmeasurements.Verticalbarsindicate
95%intervals.BM3D-AMPfrequentlyfailstoconvergeforfewerthan
4000
measure-
mentsonx-rayimages,asdenotedbyerrorvaluesfarabovetheverticalaxis.
areintheappendix.OnthisdatasetweoutperformallbaselinesexceptBM3D-AMPforhigher
m
.Howeverforlower
m
,e.g.whentheratio
m
n

3%
,BM3D-AMPoftendoesn'tconverge.This
seemstosupporttheworkofMetzleretal.[
54
]:BM3D-AMPperformswellonhigher
m
,
e.g.
m
n

10%
,butrecoveryatlowersamplingratesisnotdemonstrated.
Comparisontopre-trainedDCGAN:
TheapproachofBoraetal.[
7
]similarlyemploysaDCGAN
butispre-trainedoveralargedataset.Asexpected,thismethodoutperformsoursforlowernumber
ofmeasurements
m
;however,as
m
increases,thepre-trainedmethod'sperformancesaturateswhile
ouralgorithmcontinuestoimproveandconsequentlyoutperformitspre-trainedcounterpart,per
Figure4aintheappendix.Thiscanbeattributedtoourmethodoptimizingovertheweights
w
as
opposedtothelatentspace
z
,allowingforamoreexpressivenetworkcapableofreconstructing
complicatedsignalse.g.medicalimages.ThemethodofBoraetal.iscomparativelylessexpressive;
indeeditisonlycapableofreconstructingsimpleimages,suchasMNISToralignedCelebA[7].
(a)Reconstructions-chestx-ray
(b)Reconstructions-MNIST
Figure2:Reconstructionresultsonx-rayimagesform=2000measurements(ofn=65536pixels)
andMNISTform=75measurements(ofn=784pixels).Fromtoptobottomrow:originalimage,
reconstructionsbyouralgorithm,thenreconstructionsbybaselinesBM3D-AMP,TVAL3,andLasso.
Forx-rayimagesthenumberofmeasurementsobtainedare3%thenumberofpixels(i.e.
m
n
=
:
03
),
forwhichBM3D-AMPoftenfailstoconverge.
8
Additionalexperiments:
Intheappendixwefurtherdemonstrateouralgorithm(1)usingaFourier
measurementprocessfor
A
insteadofaGaussiani.i.d.matrix,(2)onRGBretinopathyimages,and
(3)inthepresenceofadditivenoise.Wealsoperformaruntimeanalysis.
6Conclusion
WedemonstratehowDeepImagePrior(DIP)canbegeneralizedtosolveanydifferentiablelinear
inverseproblem,inmanycasesoutperformingstate-of-the-artunlearnedmethods.Wefurtherpropose
learnedregularizationwhichenforcesalearnedGaussianprioronthenetworkweights.Thisprior
reducesreconstructionerror,particularlyfornoisyorcompressedmeasurements.Lastlyweprove
thattheDIPoptimizationtechniquecananysignalgivenasufwidesingle-layernetwork.
Thisprovidestheoreticalforregularizationmethodssuchasearlystopping.
9
References
[1]
AlekhAgarwal,SahandNegahban,andMartinJWainwright.Fastglobalconvergenceratesofgradient
methodsforhigh-dimensionalstatisticalrecovery.In
AdvancesinNeuralInformationProcessingSystems
,
pages37Œ45,2010.
[2]
NasirAhmed,T_Natarajan,andKamisettyRRao.Discretecosinetransform.
IEEEtransactionson
Computers
,100(1):90Œ93,1974.
[3]
MuhammadAsim,FahadShamshad,andAliAhmed.Solvingbilinearinverseproblemsusingdeep
generativepriors.
CoRR
,abs/1802.04073,2018.
[4]
FrancisBach,RodolpheJenatton,JulienMairal,GuillaumeObozinski,etal.Optimizationwithsparsity-
inducingpenalties.
FoundationsandTrends®inMachineLearning
,4(1):1Œ106,2012.
[5]
RichardGBaraniuk,VolkanCevher,MarcoFDuarte,andChinmayHegde.Model-basedcompressive
sensing.
IEEETransactionsonInformationTheory
,56(4):1982Œ2001,2010.
[6]
PeterJBickel,Ya'acovRitov,AlexandreBTsybakov,etal.Simultaneousanalysisoflassoanddantzig
selector.
TheAnnalsofStatistics
,37(4):1705Œ1732,2009.
[7]
AshishBora,AjilJalal,EricPrice,andAlexandrosGDimakis.Compressedsensingusinggenerative
models.
arXivpreprintarXiv:1703.03208
,2017.
[8]
EmmanuelJCandès,JustinRomberg,andTerenceTao.Robustuncertaintyprinciples:Exactsignal
reconstructionfromhighlyincompletefrequencyinformation.
IEEETransactionsoninformationtheory
,
52(2):489Œ509,2006.
[9]
EmmanuelJCandes,JustinKRomberg,andTerenceTao.Stablesignalrecoveryfromincompleteand
inaccuratemeasurements.
Communicationsonpureandappliedmathematics
,59(8):1207Œ1223,2006.
[10]
EmmanuelJCandesandTerenceTao.Decodingbylinearprogramming.
IEEEtransactionsoninformation
theory
,51(12):4203Œ4215,2005.
[11]
StanleyHChan,XiranWang,andOmarAElgendy.Plug-and-playADMMforimagerestoration:
Fixed-pointconvergenceandapplications.
IEEETransactionsonComputationalImaging
,3(1):84Œ98,
2017.
[12]
Jen-HaoRickChang,Chun-LiangLi,BarnabásPóczos,B.V.K.VijayaKumar,andAswinC.Sankara-
narayanan.Onenetworktosolvethemall-solvinglinearinverseproblemsusingdeepprojectionmodels.
CoRR
,abs/1703.09912,2017.
[13]
Guang-HongChen,JieTang,andShuaiLeng.Priorimageconstrainedcompressedsensing(piccs):
amethodtoaccuratelyreconstructdynamicctimagesfromhighlyundersampledprojectiondatasets.
Medicalphysics
,35(2):660Œ663,2008.
[14]
IngridDaubechies.Orthonormalbasesofcompactlysupportedwavelets.
Communicationsonpureand
appliedmathematics
,41(7):909Œ996,1988.
[15]
AkshatDave,AnilKumarVadathya,RamanaSubramanyam,RahulBaburajan,andKaushikMitra.Solving
inversecomputationalimagingproblemsusingdeeppixel-levelprior.
arXivpreprintarXiv:1802.09850
,
2018.
[16]
ManikDhar,AdityaGrover,andStefanoErmon.Modelingsparsedeviationsforcompressedsensingusing
generativemodels.
arXivpreprintarXiv:1807.01442
,2018.
[17]
SorenDittmer,TobiasKluth,PeterMaass,andDanielOteroBaguer.Regularizationbyarchitecture:A
deeppriorapproachforinverseproblems.
arXivpreprintarXiv:1812.03889
,2018.
[18]
DavidLDonoho.Compressedsensing.
IEEETransactionsoninfotheory
,52(4):1289Œ1306,2006.
[19]
SimonSDu,XiyuZhai,BarnabasPoczos,andAartiSingh.Gradientdescentprovablyoptimizesover-
parameterizedneuralnetworks.
arXivpreprintarXiv:1810.02054
,2018.
[20]
MarcoFDuarte,MarkADavenport,DharmpalTakhar,JasonNLaska,TingSun,KevinFKelly,and
RichardGBaraniuk.Single-pixelimagingviacompressivesampling.
IEEEsignalprocessingmagazine
,
25(2):83Œ91,2008.
[21]
ArminEftekhariandMichaelBWakin.Newanalysisofmanifoldembeddingsandsignalrecoveryfrom
compressivemeasurements.
AppliedandComputationalHarmonicAnalysis
,39(1):67Œ109,2015.
10
[22]
EnderMEksiogluandAKorhanTanc.DenoisingAMPforMRIreconstruction:BM3D-AMP-MRI.
SIAM
JournalonImagingSciences
,11(3):2090Œ2109,2018.
[23]
AlysonKFletcherandSundeepRangan.Inferenceindeepnetworksinhighdimensions.
arXivpreprint
arXiv:1706.06549
,2017.
[24]
IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,Aaron
Courville,andYoshuaBengio.Generativeadversarialnets.In
NeurIPS
,pages2672Œ2680,2014.
[25]
AdityaGroverandStefanoErmon.Amortizedvariationalcompressivesensing.
ICLRWorkshop
,2018.
[26]
AdityaGroverandStefanoErmon.Uncertaintyautoencoders:Learningcompressedrepresentationsvia
variationalinformationmaximization.
arXivpreprintarXiv:1812.10539
,2018.
[27]
KerstinHammernik,TeresaKlatzer,ErichKobler,MichaelPRecht,DanielKSodickson,ThomasPock,
andFlorianKnoll.Learningavariationalnetworkforreconstructionofacceleratedmridata.
Magnetic
resonanceinmedicine
,79(6):3055Œ3071,2018.
[28]
PaulHand,OscarLeong,andVladVoroninski.Phaseretrievalunderagenerativeprior.In
Advancesin
NeuralInformationProcessingSystems
,pages9154Œ9164,2018.
[29]
PaulHandandVladislavVoroninski.Globalguaranteesforenforcingdeepgenerativepriorsbyempirical
risk.
arXivpreprintarXiv:1705.07576
,2017.
[30]
ReinhardHeckelandPaulHand.Deepdecoder:Conciseimagerepresentationsfromuntrainednon-
convolutionalnetworks.
arXivpreprintarXiv:1810.03982
,2018.
[31]
ReinhardHeckel,WenHuang,PaulHand,andVladislavVoroninski.Deepdenoising:Rate-optimal
recoveryofstructuredsignalswithadeepprior.
arXivpreprintarXiv:1805.08855
,2018.
[32]
ChinmayHegdeandRichardGBaraniuk.Signalrecoveryonincoherentmanifolds.
IEEETransactionson
InformationTheory
,58(12):7204Œ7214,2012.
[33]
ChinmayHegde,MichaelWakin,andRichardBaraniuk.Randomprojectionsformanifoldlearning.In
Advancesinneuralinformationprocessingsystems
,pages641Œ648,2008.
[34]
ADHoover,ValentinaKouznetsova,andMichaelGoldbaum.Locatingbloodvesselsinretinalimages
bypiecewisethresholdprobingofamatchedresponse.
IEEETransactionsonMedicalimaging
,
19(3):203Œ210,2000.
[35]
SergeyIoffeandChristianSzegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducing
internalcovariateshift.
arXivpreprintarXiv:1502.03167
,2015.
[36]
StefanJaeger,SemaCandemir,SameerAntani,Yì-XiángJWáng,Pu-XuanLu,andGeorgeThoma.Two
publicchestx-raydatasetsforcomputer-aidedscreeningofpulmonarydiseases.
Quantitativeimagingin
medicineandsurgery
,4(6):475,2014.
[37]
MayaKabkab,PouyaSamangouei,andRamaChellappa.Task-awarecompressedsensingwithgenerative
adversarialnetworks.
arXivpreprintarXiv:1802.01284
,2018.
[38]
DiederikPKingmaandMaxWelling.Auto-encodingvariationalbayes.
preprintarXiv:1312.6114
,2013.
[39]
AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.Imagenetwithdeepconvolutional
neuralnetworks.In
Advancesinneuralinformationprocessingsystems
,pages1097Œ1105,2012.
[40]
YannLeCun,LéonBottou,YoshuaBengio,andPatrickHaffner.Gradient-basedlearningappliedto
documentrecognition.
ProceedingsoftheIEEE
,86(11):2278Œ2324,1998.
[41]
JaakkoLehtinen,JacobMunkberg,JonHasselgren,SamuliLaine,TeroKarras,MiikaAittala,andTimo
Aila.Noise2noise:Learningimagerestorationwithoutcleandata.
preprintarXiv:1803.04189
,2018.
[42]
ChengboLi.Compressivesensingfor3ddataprocessingtasks:applications,modelsandalgorithms.
Technicalreport,RiceUniversity,2011.
[43]
ChengboLi,WotaoYin,andYinZhang.User'sguidefortval3:Tvminimizationbyaugmentedlagrangian
andalternatingdirectionalgorithms.
CAAMreport
,20(46-47):4,2009.
[44]
JiamingLiu,YuSun,XiaojianXu,andUlugbekSKamilov.Imagerestorationusingtotalvariation
regularizeddeepimageprior.
arXivpreprintarXiv:1810.12864
,2018.
11
[45]
ZiweiLiu,PingLuo,XiaogangWang,andXiaoouTang.Deeplearningfaceattributesinthewild.In
ProceedingsofInternationalConferenceonComputerVision(ICCV)
,2015.
[46]
Po-LingLohandMartinJWainwright.High-dimensionalregressionwithnoisyandmissingdata:Provable
guaranteeswithnon-convexity.In
NeurIPS
,pages2726Œ2734,2011.
[47]
MichaelLustig,DavidDonoho,andJohnMPauly.SparseMRI:Theapplicationofcompressedsensing
forrapidMRimaging.
Magneticresonanceinmedicine
,58(6):1182Œ1195,2007.
[48]
MichaelLustig,DavidLDonoho,JuanMSantos,andJohnMPauly.CompressedsensingMRI.
IEEE
signalprocessingmagazine
,25(2):72Œ82,2008.
[49]
FangchangMa,UlasAyaz,andSertacKaraman.Invertibilityofconvolutionalgenerativenetworksfrom
partialmeasurements.In
AdvancesinNeuralInformationProcessingSystems
,pages9628Œ9637,2018.
[50]
MortezaMardani,EnhaoGong,JosephYCheng,ShreyasVasanawala,GregZaharchuk,MarcusAlley,
NeilThakur,SongHan,WilliamDally,JohnMPauly,etal.Deepgenerativeadversarialnetworksfor
compressedsensingautomatesMRI.
arXivpreprintarXiv:1706.00051
,2017.
[51]
MortezaMardani,HatefMonajemi,VardanPapyan,ShreyasVasanawala,DavidDonoho,andJohn
Pauly.Recurrentgenerativeadversarialnetworksforproximallearningandautomatedcompressiveimage
recovery.
arXivpreprintarXiv:1711.10046
,2017.
[52]
MortezaMardani,QingyunSun,ShreyasVasawanala,VardanPapyan,HatefMonajemi,JohnPauly,
andDavidDonoho.Neuralproximalgradientdescentforcompressiveimaging.
arXivpreprint
arXiv:1806.03963
,2018.
[53]
ChrisMetzler,AliMousavi,andRichardBaraniuk.LearnedD-AMP:Principledneuralnetworkbased
compressiveimagerecovery.In
NeurIPS
,pages1772Œ1783,2017.
[54]
ChristopherAMetzler,ArianMaleki,andRichardGBaraniuk.BM3D-AMP:Anewimagerecovery
algorithmbasedonBM3Ddenoising.In
ImageProcessing(ICIP),2015IEEEInternationalConference
on
,pages3116Œ3120.IEEE,2015.
[55]
ChristopherAMetzler,ArianMaleki,andRichardGBaraniuk.Fromdenoisingtocompressedsensing.
IEEETransactionsonInformationTheory
,62(9):5117Œ5144,2016.
[56]
DustinGMixonandSoledadVillar.Sunlayer:Stabledenoisingwithgenerativenetworks.
arXivpreprint
arXiv:1803.09319
,2018.
[57]
SahandNegahban,BinYu,MartinJWainwright,andPradeepKRavikumar.Aframeworkforhigh-
dimensionalanalysisof
m
-estimatorswithdecomposableregularizers.In
AdvancesinNeuralInformation
ProcessingSystems
,pages1348Œ1356,2009.
[58]
DongNie,RogerTrullo,JunLian,CarolinePetitjean,SuRuan,QianWang,andDinggangShen.Medical
imagesynthesiswithcontext-awaregenerativeadversarialnetworks.In
InternationalConferenceon
MedicalImageComputingandComputer-AssistedIntervention
,pages417Œ425.Springer,2017.
[59]
RobertoImbuzeiroOliveira.Thelowertailofrandomquadraticforms,withapplicationstoordinaryleast
squaresandrestrictedeigenvalueproperties.
arXivpreprintarXiv:1312.2903
,2013.
[60]
SametOymakandMahdiSoltanolkotabi.Overparameterizednonlinearlearning:Gradientdescenttakes
theshortestpath?
.
,122018.
[61]
SametOymakandMahdiSoltanolkotabi.Towardsmoderateoverparameterization:globalconvergence
guaranteesfortrainingshallowneuralnetworks.
arXivpreprintarXiv:1902.04674
,2019.
[62]
ParthePandit,MojtabaSahraee,SundeepRangan,andAlysonKFletcher.Asymptoticsofmapinference
indeepnetworks.
arXivpreprintarXiv:1903.01293
,2019.
[63]
AdamPaszke,SamGross,SoumithChintala,GregoryChanan,EdwardYang,ZacharyDeVito,Zeming
Lin,AlbanDesmaison,LucaAntiga,andAdamLerer.Automaticdifferentiationinpytorch.
OpenReview
,
2017.
[64]
F.Pedregosa,G.Varoquaux,A.Gramfort,V.Michel,B.Thirion,O.Grisel,M.Blondel,P.Prettenhofer,
R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,andE.Duchesnay.
Scikit-learn:MachinelearninginPython.
JournalofMachineLearningResearch
,12:2825Œ2830,2011.
[65]
SaadQaisar,RanaMuhammadBilal,WafaIqbal,MuqaddasNaureen,andSungyoungLee.Compressive
sensing:Fromtheorytoapplications.
JournalofCommunicationsandnetworks
,15(5):443Œ456,2013.
12
[66]
AlecRadford,LukeMetz,andSoumithChintala.Unsupervisedrepresentationlearningwithdeep
convolutionalgenerativeadversarialnetworks.
arXivpreprintarXiv:1511.06434
,2015.
[67]
YanivRomano,MichaelElad,andPeymanMilanfar.Thelittleenginethatcould:Regularizationby
denoising(red).
SIAMJournalonImagingSciences
,10(4):1804Œ1844,2017.
[68]
OlafRonneberger,PhilippFischer,andThomasBrox.U-net:Convolutionalnetworksforbiomedical
imagesegmentation.In
InternationalConferenceonMedicalimagecomputingandcomputer-assisted
intervention
,pages234Œ241.Springer,2015.
[69]
LeonidIRudin,StanleyOsher,andEmadFatemi.Nonlineartotalvariationbasednoiseremovalalgorithms.
PhysicaD:nonlinearphenomena
,60(1-4):259Œ268,1992.
[70]
AndreiARusu,DushyantRao,JakubSygnowski,OriolVinyals,RazvanPascanu,SimonOsindero,and
RaiaHadsell.Meta-learningwithlatentembeddingoptimization.
arXivpreprintarXiv:1807.05960
,2018.
[71]
ThomasSchlegl,PhilippSeeböck,SebastianMWaldstein,UrsulaSchmidt-Erfurth,andGeorgLangs.
Unsupervisedanomalydetectionwithgenerativeadversarialnetworkstoguidemarkerdiscovery.In
InternationalConferenceonInformationProcessinginMedicalImaging
,pages146Œ157.Springer,2017.
[72]
JoSchlemper,JoseCaballero,JosephVHajnal,AnthonyNPrice,andDanielRueckert.Adeepcascade
ofconvolutionalneuralnetworksfordynamicmrimagereconstruction.
IEEEtransactionsonMedical
Imaging
,37(2):491Œ503,2017.
[73]
PhilipSchniter,SundeepRangan,andAlysonKFletcher.Vectorapproximatemessagepassingforthe
generalizedlinearmodel.In
ACSSC
,pages1525Œ1529.IEEE,2016.
[74]
VirajShahandChinmayHegde.Solvinglinearinverseproblemsusingganpriors:Analgorithmwith
provableguarantees.
arXivpreprintarXiv:1802.08406
,2018.
[75]
RobertTibshirani.Regressionshrinkageandselectionviathelasso.
JournaloftheRoyalStatisticalSociety.
SeriesB(Methodological)
,pages267Œ288,1996.
[76]
TijmenTielemanandGeoffreyHinton.Lecture6.5-rmsprop:Dividethegradientbyarunningaverageof
itsrecentmagnitude.
COURSERA:Neuralnetworksformachinelearning
,4(2):26Œ31,2012.
[77]
SubarnaTripathi,ZacharyCLipton,andTruongQNguyen.Correctionbyprojection:Denoisingimages
withgenerativeadversarialnetworks.
arXivpreprintarXiv:1803.04477
,2018.
[78]
JoelATropp.Justrelax:Convexprogrammingmethodsforidentifyingsparsesignalsinnoise.
IEEE
transactionsoninformationtheory
,52(3):1030Œ1051,2006.
[79]
DmitryUlyanov,AndreaVedaldi,andVictorLempitsky.Deepimageprior.In
ProceedingsoftheIEEE
ConferenceonComputerVisionandPatternRecognition
,pages9446Œ9454,2018.
[80]
AaronVanDenOord,SanderDieleman,HeigaZen,KarenSimonyan,OriolVinyals,AlexGraves,Nal
Kalchbrenner,AndrewSenior,andKorayKavukcuoglu.Wavenet:Agenerativemodelforrawaudio.
arXivpreprintarXiv:1609.03499
,2016.
[81]
SinganallurVVenkatakrishnan,CharlesABouman,andBrendtWohlberg.Plug-and-playpriorsformodel
basedreconstruction.In
GlobalSIP,2013IEEE
,pages945Œ948.IEEE,2013.
[82]
YilunWang,JunfengYang,WotaoYin,andYinZhang.Anewalternatingminimizationalgorithmfor
totalvariationimagereconstruction.
SIAMJournalonImagingSciences
,1(3):248Œ272,2008.
[83]
FWasilewski.Pywavelets:Discretewavelettransforminpython,2010.
[84]
DavidWWinters,BarryDVanVeen,andSusanCHagness.Asparsityregularizationapproachtothe
electromagneticinversescatteringproblem.
IEEEtransactionsonantennasandpropagation
,58(1):145Œ
154,2010.
[85]
JelmerMWolterink,TimLeiner,MaxAViergever,andIvanaIıgum.Generativeadversarialnetworksfor
noisereductioninlow-dosect.
IEEEtransactionsonmedicalimaging
,36(12):2536Œ2545,2017.
[86]
JianZhang,ShaohuiLiu,RuiqinXiong,SiweiMa,andDebinZhao.Improvedtotalvariationbasedimage
compressivesensingrecoverybynonlocalregularization.In
CircuitsandSystems(ISCAS),2013IEEE
InternationalSymposiumon
,pages2836Œ2839.IEEE,2013.
13
Table2:Runtime(seconds)foreachalgorithmwithvaryingnumberofmeasurements.
Algorithm1000200040008000
CS-DIP15.6
17.120.429.9
BM3D-AMP51.154.067.871.2
TVAL3
13.8
22.131.956.7
LassoDCT27.133.052.296.4
AImplementationDetails
Hyperparametersearch:
Afterastandardgridsearchprocedure,wesettheTVhyperparameter

T
=0
:
01
,whichaidsinprovidingasharperreconstructionoftheimage'shighfrequencycompo-
nents.ForthelearnedregularizationexperimentsinSection5.2.1,asimilargridsearchwasperformed
toset

L
=100
.Thecriteriaforselectingahyperparameterisonethatprovideslowesterrorwiththe
observedmeasurements,i.e.withoutobservinggroundtruth.Wetunehyperparametersonarandom
setoftenimages;thissetisdisjointfromtheimagesusedforevaluation.
Initalizations:
Themeasurementmatrix
A
isinitializedatrandomforeachsample.Similarly
networkinput
z
inEqn.3isinitializedwithrandomGaussiani.i.d.entriesandthenheldedas
weoptimizeovernetworkweights
w
.Wesetthedimensionof
z
tobe
128
,astandardchoicefor
DCGANarchitectures.Forasufcientnumberofpixels
n
,i.e.forchestx-rayandretinopathy
images,differentinitializationsof
z
donotaffectperformance.Howeverforsmaller
n
,i.e.for
MNISTimages,performancecanvarywithdifferentinitializationsof
z
.
Earlystopping:
Westopafter
1000
iterationsinallexperiments.SimilartoFigure2inUlyanov
etal.[
79
],wefoundMSEtodecreaseinitiallyandthenplateauuntilroughly
10
;
000
iterations,at
whichpointthenetworkovtonoise.Henceweterminatetheoptimizationprocedurewithinthis
plateauregiontoavoidovThisearlystoppingtechniqueiscommoninDIPmethods,hence
ourmotivationtojustifyittheoreticallyinSection4.
BAdditionalExperiments
Fouriermeasurementprocess:
Allotherexperimentsusedameasurementmatrix
A
containing
Gaussiani.i.d.entries.Wenowconsiderthecasewherethemeasurementmatrixisasubsampled
Fouriermatrix.Fora2Dimage
x
andasetofindices

,themeasurementswereceivearegiven
by
y
(
i;j
)
=[
F
(
x
)]
(
i;j
)
;
(
i;j
)
2

,where
F
isthe2DFouriertransform.Wechoose

tobeindices
alongradiallines,asshowninFigure13oftheappendix;thischoiceof

iscommoninliterature[
8
]
andMRIapplications[
50
,
48
,
22
].WhileFouriersubsamplingiscommoninMRIapplications,we
useithereonimagesofx-rayssimplytodemonstratethatouralgorithmperformswellwithdifferent
measurementprocesses.
InFigure3b,wecompareouralgorithmtobaselinesonthex-raydatasetfor
f
3
;
5
;
10
;
20
g
radiallines
intheFourierdomain,whichcorrespondsto
f
381
;
634
;
1260
;
2500
g
Fouriercoefcients,respectively.
Quantitativelyweoutperformallbaselines.QualitativereconstructionscanbefoundinFigure14.
Retinopathy:
Weplotreconstructionerrorwithvaryingnumberofmeasurements
m
of
n
=49152
inFigure3a.OnthisRGBdatasetwequantitativelyoutperformallbaselinesexceptBM3D-AMP
onhigher
m
;however,evenatthesehigher
m
,patchesofgreenandpurplepixelscorrupttheimage
reconstructionsasseeninFigure10.Similartox-rayforlower
m
,BM3D-AMPoftenfailstoproduce
anythingsensible.Allretinopathyreconstructionsarelocatedintheappendix.
Robustnesstonoise:
InFigure4bwedemonstratethatouralgorithmisrobusttoadditivenoise,i.e.
when

6
=0
inEqn.1,achievingsimilarbehaviortobaselines.
Runtime:
Wedemonstrateruntimesforallalgorithmsonthex-raydatasetinTable2.Whileour
algorithmisfasterinmostcases,weacknowledgethisisnotafaircomparisonasbaselinesdonot
havetheofrunningGPU.MeanwhileouralgorithmwasrunonaNVIDIAGTX1080-Ti.
Ultimatelythisdemonstratesthatouralgorithmexecutesinareasonableamountoftime,whichcan
beanissuewithDIPmethodsemployingaU-netarchitecture.
14
(a)MSE-retinopathy(RGB)withGaussianmea-
surements
(b)MSE-chestx-raywithFouriermeasurements
Figure3:Per-pixelreconstructionerror(MSE)vs.numberofmeasurements.Verticalbarsindicate
95%condenceintervals.UnfortunatelyanRGBversionofTVAL3doesnotcurrentlyexist,although
relatedTValgorithmssuchasFTVdperformsimilardenoisingtasks[82].
(a)Reconstructionerror(MSE)onMNISTforvary-
ingnumberofmeasurements.Asexpected,the
trainedalgorithmofBoraetal.(CSGM)outper-
formsourmethodforfewermeasurements;how-
ever,CSGMsaturatesafter75measurements,asits
outputisconstrainedtotherangeofthegenerator.
ThissaturationisdiscussedinBoraetal.,Section
6.1.1.
(b)Reconstructionerror(MSE)onx-rayimagesfor
varyingamountsofnoise;numberofmeasurements
m
edat
2000
.Theterm
˙
2
correspondstovari-
anceofthenoisevector

in
y
=
Ax
+

,i.e.each
entryof

isdrawnindependently
N
(0
;
˙
2
m
)
.Other
baselineshaveerrorfarabovetheverticalaxisand
arethusnotvisibleinthisplot.
Figure4
Algorithm1
Estimate
(

,

foradistributionofoptimalnetworkweights
W

Input:
Setofoptimalweights
W

=
f
w

1
;w

2
;

;w

Q
g
obtainedfrom
L
-layerDCGANrunover
Q
images;numberofsamples
S
;numberofiterations
T
.
Output:
Meanvector

2
R
L
;covariancematrix

2
R
L

L
.
1:
for
t
=1
to
T
do
2:
Sample
q
uniformlyfrom
f
1
;:::;Q
g
3:
for
l
=1
to
L
{foreachlayer}
do
4:
Get
v
2
R
S
,avectorof
S
uniformlysampledweightsfromthe
l
th
layerof
w

q
5:
M
t
[
l;
:]
 
v
T
where
M
t
[
l;
:]
isthe
l
th
rowofmatrix
M
t
2
R
L

S
6:

t
[
l
]
 
1
S
P
S
i
=1
v
i
7:
endfor
8:

t
 
1
S
M
t
M
T
t


t

T
t
9:
endfor
10:

 
1
T
P
T
t
=1

t
11:

 
1
T
P
T
t
=1

t
15
CProofofSection4:TheoreticalJforEarlyStopping
InthissectionweproveourtheoreticalresultinTheorem4.1.Webeginwithasummaryofsome
notationsweusethroughoutinSectionC.1.Next,westatesomepreliminarycalculationsinSection
C.2.Then,westateafewkeylemmasinSectionC.3withtheproofsdeferredtoAppendixD.Finally,
wecompletetheproofofTheorem4.1inSectionC.4.Wenotethatwhen
V
hasi.i.d.Gaussianentries
and
A
containsorthonormalrows,
AV
alsohasi.i.d.Gaussianentries.Thereforewithoutlossof
generalitywecarryouttheproofwith
A
=
I
and
m
=
n
.Theresultstatedinthetheoremsimply
followsbyreplacing
V
inourproofwith
AV
.
C.1Notation
Inthissectionwegathersomenotationusedthroughouttheproofs.Weuse
˚
(
z
)=
ReLU
(
z
)=
max(0
;z
)
with
˚
0
(
z
)=
I
f
z

0
g
.Fortwomatrices/vectors
x
and
y
ofthesamesizeweuse
x

y
to
denotetheentrywiseHadamardproductofthesetwomatrices/vectors.Wealsouse
x

y
todenote
theirKroneckerproduct.Fortwomatrices
B
2
R
n

d
1
and
C
2
R
n

d
2
,weusetheKhatrio-Rao
productasthematrix
A
=
B

C
2
R
n

d
1
d
2
withrows
A
i
givenby
A
i
=
B
i

C
i
.Foramatrix
M
2
R
m

n
weusevect
(
M
)
2
R
mn
todenoteavectorobtainedbyaggregatingtherowsofthe
matrix
M
intoavector,i.e.vect
(
M
)=[
M
1
M
2
:::M
m
]
T
.Foramatrix
X
weuse
˙
min
(
X
)
and
k
X
k
denotestheminimumsingularvalueandspectralnormof
X
.Similarly,forasymetric
matrix
M
weuse

min
(
M
)
todenoteitssmallesteigenvalue.
C.2Preliminaries
Inthissectionwecarryoutsomesimplecalculationsyieldingsimpleformulasforthegradientand
Jacobianmappings.Webeginbynotingwecanrewritethegradientdescentiterationsintheform
vect
(
W
˝
+1
)=
vect
(
W
˝
)


vect
(
rL
(
W
˝
))
:
Here,
vect
(
rL
(
W
˝
))=
J
T
(
W
˝
)
r
(
W
˝
)
;
where
J
(
W
)=
@
@
vect
(
W
)
f
(
W
)
and
istheJacobianmappingassociatedtothenetworkand
r
(
W
)=
˚
(
V˚
(
Wz
))

y:
istheorresidualvector.Notethat
@
@
vect
(
W
)
v
T
˚
(
Wz
)=

v
1
˚
0

w
T
1
z

x
T
:::v
k
˚
0

w
T
k
z

x
T

=(
v

˚
0
(
Wx
))
T

x
T
Thus
J
(
W
)=(
V
diag
(
˚
0
(
Wz
)))


1
z
T

;
Thisinturnyields
J
(
W
)
J
T
(
W
)=

V
diag
(
˚
0
(
Wz
))
diag
(
˚
0
(
Wz
))
V
T

:::


k
z
k
2
11
T

=
k
z
k
2
V
diag
(
˚
0
(
Wz
)

˚
0
(
Wz
))
V
T
(8)
C.3LemmasforcontrollingthespectrumoftheJacobianandinitial
InthissectionwestateafewlemmasconcerningthespectralpropertiesoftheJacobianmapping,its
perturbationandinitialofthemodelwiththeproofsdeferredtoAppendixD.
16
LemmaC.1
(MinimumsingularvalueoftheJacobianatinitialization)
.
Let
V
2
R
n

d
and
W
2
R
d

k
berandommatriceswithi.i.d.
N
(0
;
2
)
and
N
(0
;
1)
entriesandtheJacobianmapping
J
(
W
)=(
V
diag
(
˚
0
(
Wz
)))


1
z
T

.Thenaslongas
d

3828
n
,
˙
min
(
J
(
W
))

1
2

p
d
k
z
k
:
holdswithprobabilityatleast
1

2
e

n
.
LemmaC.2
(Perturbationlemma)
.
Let
V
2
R
n

d
beamatrixwithi.i.d.
N
(0
;
2
)
entries,
W
2
R
d

k
,andtheJacobianmapping
J
(
W
)=(
V
diag
(
˚
0
(
Wz
)))


1
z
T

.Alsolet
W
0
bea
matrixwithi.i.d.
N
(0
;
1)
entries.Then,
kJ
(
W
)
J
(
W
0
)
k

k
z
k
 
2
p
n
+
:::
s
6(2
dR
)
2
3
log

d
3(2
dR
)
2
3

!
;
holdsforall
W
2
R
d

k
obeying
k
W

W
0
k
R
withprobabilityatleast
1

e

n=
2

e

(2
dR
)
2
3
6
.
LemmaC.3
(SpectralnormoftheJacobian)
.
Let
V
2
R
n

d
beamatrixwithi.i.d.
N
(0
;
2
)
entries,
W
2
R
d

k
,andtheJacobianmapping
J
(
W
)=(
V
diag
(
˚
0
(
Wz
)))


1
z
T

.Then,
kJ
(
W
)
k


p
d
+2
p
n

k
z
k
;
holdsforall
W
2
R
d

k
withprobabilityatleast
1

e

n=
2
.
LemmaC.4
(Initial
.
Let
V
2
R
n

d
beamatrixwithi.i.d.
N
(0
;
2
)
entrieswith

=
1
p
dn
k
y
k
k
z
k
.
Alsolet
W
2
R
d

k
beamatrixwithi.i.d.
N
(0
;
1)
entries.Then
k
V˚
(
Wz
)

y
k
3
k
y
k
;
holdswithprobabilityatleast
1

e

n=
2

e

d=
2
.
C.4ProofofTheorem4.1
Consideranonlinearleast-squaresoptimizationproblemoftheform
min

2
R
p
L
(

):=
1
2
k
f
(

)

y
k
2
;
with
f
:
R
p
7!
R
n
and
y
2
R
n
.SupposetheJacobianmappingassociatedwith
f
obeysthefollowing
threeassumptions.
Assumption1.
Fixapoint

0
.Wehavethat
˙
min
(
J
(

0
))

2

.
Assumption2.
Let
kk
denoteanormthatisdominatedbytheEuclideannormi.e.
k

kk

k
holdsforall

2
R
p
.Fixapoint

0
andanumber
R>
0
.Forany

satisfying
k



0
k
R
,we
havethat
kJ
(

0
)
J
(

)
k
=
3
.
Assumption3.
Forall

2
R
p
,wehavethat
kJ
(

)
k

.
Undertheseassumptionswecanstatethefollowingtheoremfrom[61].
TheoremC.5
(Non-smoothOverparameterizedOptimization)
.
Given

0
2
R
p
,supposeAssumptions
1,2,and3holdwith
R
=
5
k
y

f
(

0
)
k

:
Then,pickingconstantlearningrate


1

2
,allgradientiterationsobeythefollowings
k
y

f
(

˝
)
k
(1


2
4
)
˝
k
y

f
(

0
)
k
(9)

5
k

˝


0
k
+
k
y

f
(

˝
)
kk
y

f
(

0
)
k
:
(10)
17
Weshallapplythistheoremtothecasewheretheparameteris
W
andthenonlinearmappingisgiven
by
V˚
(
Wz
)
and
˚
=
ReLU
.Allthatisneededtobeabletoapplythistheoremischeckthatthe
assumptionshold.Pertheassumptionsofthetheoremweuse

=
1
p
dn
k
y
k
k
z
k
:
TothisaimnotethatusingLemmaC.1Assumption1holdswith

=
1
4

p
d
k
z
k
=
1
4
p
n
k
y
k
;
withprobabilityatleast
1

2
e

n
.Furthermore,byLemmaC.3Assumption3holdswith

=
k
y
k
p
8
n
p
4
n
+
d

1
2
 
r
d
4
n
+1
!
k
y
k
=


p
d
+2
p
n

k
z
k
:
withprobabilityatleast
1

e

n=
2
.Allthatremainsforapplyingthetheoremaboveistoverify
Assumption2holdswithhighprobability
R
=60
p
n
=15
k
y
k


5

k
V˚
(
Wz
)

y
k
IntheabovewehaveusedLemmaC.4toconcludethat
k
V˚
(
Wz
)

y
k
3
k
y
k
holdswith
probabilityatleast
1

e

n=
2

e

d=
2
.Thus,usingLemmaC.2allthatremainsistoshowthat
1
p
dn
k
y
k
0
@
2
p
n
+
v
u
u
t
6(2
dR
)
2
3
ln
 
d
3(2
dR
)
2
3
!
1
A


3
=
k
y
k
12
p
n
;
holdswith
R
=60
p
n
andwithprobabilityatleast
1

e

n=
2

e

(120)
2
3
6
d
2
3
n
1
3

1

e

n=
2

e

4
d
2
3
n
1
3
.
Thelatterisequivalentto
2
p
n
+
v
u
u
t
6

120
d
p
n

2
3
ln
 
d
3(120
d
p
n
)
2
3
!

p
d
12
;
whichcanberewrittenintheform
2
r
n
d
+
v
u
u
t
6(120)
2
3
3
r
n
d
ln
 
1
3(120)
2
3
3
p
n
d
!

1
12
;
whichholdsaslongas
d

4
:
3

10
15
n
.Thuswith
d

Cn
thenAssumptions1,2,and3holdswith
probabilityatleast
1

5
e

n=
2

e

d=
2

e

4
d
2
3
n
1
3
.Thus,TheoremC.5holdswithhighprobability.
ApplyingTheoremC.5completestheproof.
DProofofLemmasfortheSpectralPropertiesoftheJacobian
D.1ProofofLemmaC.1
Weprovetheresultfor

=1
,thegeneralresultfollowsfromasimplere-scaling.thevectors
a
`
=
V
`
˚
0
(
h
w
`
;z
i
)
2
R
n
;
18
with
V
`
the
`
thcolumnof
V
.Using(8)wehave
J
(
W
)
J
T
(
W
)=
k
z
k
2
V
diag
(
˚
0
(
Wz
)

˚
0
(
Wz
))
V
T
;
=
k
z
k
2
 
d
X
`
=1
a
`
a
T
`
!
;
=
d
k
z
k
2
 
1
d
d
X
`
=1
a
`
a
T
`
!
:
(11)
Toboundtheminimumeigenvaluewestatearesultfrom[59].
TheoremD.1.
Assume
A
1
;:::;A
d
2
R
n

n
arei.i.d.randompositivematriceswhose
coordinateshaveboundedsecondmoments.
:=
E
[
A
1
]
(thisisanentry-wiseexpectation)
and
b

d
=
1
d
d
X
`
=1
A
`
:
Let
h
2
(1
;
+
1
)
besuchthat
q
E

(
u
T
A
1
u
)
2


hu
T

u
forall
u
2
R
n
.Thenforany

2
(0
;
1)
wehave
P
(
8
u
2
R
n
:
u
T
b

k
u

 
1

7
h
r
n
+2ln(2

)
d
!
u
T

u
)

1


Weshallapplythistheoremwith
A
`
:=
a
`
a
T
`
.Todothisweneedtocalculatethevariousparameters
inthetheorem.Webeginwith

andnotethatforReLUwehave
:=
E
[
A
1
]
=
E

a
1
a
T
1

=
E
w
˘N
(0
;I
k
)

(
˚
0
(
h
w;z
i
))
2

E
v
˘N
(0
;I
n
)
[
vv
T
]
=
E
w
˘N
(0
;I
k
)

˚
0
(
w
T
z
)

2

I
n
=
E
w
˘N
(0
;I
k
)

I
f
w
T
z

0
g

I
n
=
1
2
I
n
:
Tocalculate
h
wehave
q
E

(
u
T
A
1
u
)
2


r
E
h

a
T
1
u

4
i

r
E
w
˘N
(0
;I
k
)

I
f
w
T
z

0
g


E
v
˘N
(0
;I
n
)
h
(
v
T
u
)
4
i

r
3
2
k
u
k
4

p
3
p
2
k
u
k
2
=
p
6
u
T

1
2
I
n

u
=
p
6

u
T

u:
Thuswecantake
h
=
p
6
.Therefore,usingTheoremD.1with

=2
e

n
wecanconcludethat

min
 
1
d
d
X
`
=1
a
`
a
T
`
!

1
4
19
holdswithprobabilityatleast
1

2
e

n
aslongas
d

3528

n:
Pluggingthisinto(11)weconcludethatwithprobabilityatleast
1

2
e

n
˙
min
(
J
(
W
))

1
2
p
d
k
z
k
:
D.2ProofofLemmaC.2
Weprovetheresultfor

=1
,thegeneralresultfollowsfromasimplerescaling.Basedon
(8)
we
have
(
J
(
W
)
J
(
W
0
))(
J
(
W
)
J
(
W
0
))
T
:::
=
k
z
k
2
V
diag

(
˚
0
(
Wz
)

˚
0
(
W
0
z
))

:::
(
˚
0
(
Wz
)

˚
0
(
W
0
z
))

V
T
:
Thus
kJ
(
W
)
J
(
W
0
)
kk
z
kk
V
diag
(
˚
0
(
Wz
)

˚
0
(
W
0
z
))
k
(12)
=
k
z
k


V
diag

I
f
Wz

0
g

I
f
W
0
z

0
g



k
z
k


V
S
(
W
)


;
(13)
where
S
(
W
)
ˆf
1
;
2
;:::;d
g
isthesetofindiceswhere
Wz
and
W
0
z
havedifferentsigns
i.e.
S
(
W
):=
f
`
:
sgn
(
e
T
`
Wz
)
6
=
sgn
(
e
T
`
W
0
z
)
g
and
V
S
(
W
)
isasubmatrix
V
obtainedbypick-
ingthecolumnscorrespondingto
S
(
W
)
.
TocontinuefurthernotethatbyGordon'slemmawehave
sup
jS
s
k
V
S
k
p
n
+
p
2
s
log(
d=s
)+
t;
withprobabilityatleast
1

e

t
2
=
2
.Inparticularusing
t
=
p
n
weconcludethat
sup
jS
s
k
V
S
k
2
p
n
+
p
2
s
log(
d=s
)
;
(14)
withprobabilityatleast
1

e

n=
2
.Tocontinuefurtherwestatealemmacontrollingthesizeof
jS
(
W
)
j
basedonthesizeoftheradius
R
.
LemmaD.2
(signchangesinlocalneighborhood)
.
Let
W
0
2
R
d

k
beamatrixwithi.i.d.
N
(0
;
1)
entries.Alsoforamatrix
W
2
R
d

k

S
(
W
):=
f
`
:
sgn
(
e
T
`
Wz
)
6
=
sgn
(
e
T
`
W
0
z
)
g
.Thenfor
any
W
2
R
d

k
obeying
k
W

W
0
k
R
jS
(
W
)
j
2
d
(2
dR
)
2
3
e
holdswithprobabilityatleast
1

e

(2
dR
)
2
3
6
.
Combining(12)togetherwith(14)(using
s
=3(2
dR
)
2
3
)andLemmaD.2weconcludethat
kJ
(
W
)
J
(
W
0
)
k
:::
k
z
k
0
@
2
p
n
+
v
u
u
t
6(2
dR
)
2
3
log
 
d
3(2
dR
)
2
3
!
1
A
holdswithprobabilityatleast
1

e

n=
2

e

(2
dR
)
2
3
6
.
20
D.3ProofofLemmaD.2
Toprovethisresultweutilizetwolemmasfrom[
61
].Intheselemmasweuse
j
v
j
m

todenotethe
m
thsmallestentryof
v
aftersortingitsentriesintermsofabsolutevalue.
LemmaD.3
(LemmaC.2in[61])
.
Givenaninteger
m
,suppose
k
W

W
0
k
p
m
j
W
0
z
j
m

k
z
k
;
then
jS
(
W
)
j
2
m:
LemmaD.4
(LemmaC.3in[
61
])
.
Let
z
2
R
k
.Alsolet
W
0
2
R
d

k
beamatrixwithi.i.d.
N
(0
;
1)
entries.Then,withprobabilityatleast
1

e

m
6
,
j
W
0
z
j
m

k
z
k

m
2
d
:
Combiningthelattertwolemmaswith
m
=
d
(2
dR
)
2
3
e
weconcludethatwhen
k
W

W
0
k
R

m
3
2
2
d

p
m
m
2
d

p
m
j
W
0
z
j
m

k
z
k
;
thenwithprobabilityatleast
1

e

(2
dR
)
2
3
6
wehave
jS
(
W
)
j
2
m

2
d
(2
dR
)
2
3
e
:
D.4ProofofLemmaC.3
Weprovetheresultfor

=1
,thegeneralresultfollowsfromasimplerescaling.Using
(8)
wehave
J
(
W
)
J
T
(
W
)=
k
z
k
2
V
diag
(
˚
0
(
Wz
)

˚
0
(
Wz
))
V
T
Thus
kJ
(
W
)
kk
z
kk
V
diag
(
˚
0
(
Wz
))
k
k
z
kk
V
k
TheproofiscompletebyusingstandardconcentrationresultsforthespectralnormofaGaussian
matrixthatallowustoconcludethat
k
V
k
p
d
+2
p
n;
holdswithprobabilityatleast
1

e

n=
2
.
D.5ProofofLemmaC.4
Bythetriangularinequalitywehave
k
V˚
(
Wz
)

y
kk
V˚
(
Wz
)
k
+
k
y
k
(15)
Tocontinuefurtherletusconsideroneentryof
V˚
(
Wz
)
andnotethatithasthesamedistributionas
V˚
(
Wz
)
˘

k
˚
(
Wz
)
k
g;
21
where
g
2
R
d
israndomGaussianvectorswithdistribution
g
˘N
(0
;I
d
)
.Thus
k
V˚
(
Wz
)
k˘

k
˚
(
Wz
)
kk
g
k
p
2

k
˚
(
Wz
)
k
(16)

p
2

k
Wz
k
;
(17)
withprobabilityatleast
1

e

n=
2
.Furthermore,notethat
Wz
˘k
z
k
e
g;
where
e
g
2
R
d
israndomGaussianvectorswithdistribution
e
g
˘N
(0
;I
d
)
.Combiningthelatterwith
(16)weconcludethat
k
V˚
(
Wz
)
k
2
p

k
z
k
=2
k
y
k
;
holdswithprobabilityatleast
1

e

n=
2

e

d=
2
.Combiningthelatterwith(15)weconcludethat
k
V˚
(
Wz
)

y
k
3
k
y
k
;
holdswithprobabilityatleast
1

e

n=
2

e

d=
2
.
22
(a)25measurements
(b)50measurements
Figure5:ReconstructionresultsonMNISTform=25,50measurementsrespectively(ofn=
784pixels).Fromtoptobottomrow:originalimage,reconstructionsbyouralgorithm,then
reconstructionsbybaselinesBM3D-AMP,TVAL3,andLasso.
(a)100measurements
(b)200measurements
Figure6:ReconstructionresultsonMNISTform=100,200measurementsrespectively(ofn
=784pixels).Fromtoptobottomrow:originalimage,reconstructionsbyouralgorithm,then
reconstructionsbybaselinesBM3D-AMP,TVAL3,andLasso.
23
(a)500measurements
(b)1000measurements
Figure7:Reconstructionresultsonx-rayimagesform=500,1000measurementsrespectively(ofn
=65536pixels).Fromtoptobottomrow:originalimage,reconstructionsbyouralgorithm,then
reconstructionsbybaselinesBM3D-AMP,TVAL3,andLasso.
(a)4000measurements
(b)8000measurements
Figure8:Reconstructionresultsonx-rayimagesform=4000,8000measurementsrespectively(of
n=65536pixels).Fromtoptobottomrow:originalimage,reconstructionsbyouralgorithm,then
reconstructionsbybaselinesBM3D-AMP,TVAL3,andLasso.
24
(a)500measurements
(b)1000measurements
Figure9:Reconstructionresultsonretinopathyimagesform=500,1000measurementsrespectively
(ofn=49152pixels).Fromtoptobottomrow:originalimage,reconstructionsbyouralgorithm,
thenreconstructionsbybaselinesBM3D-AMPandLasso.
Figure10:Reconstructionresultsonretinopathyimagesform=2000measurements(ofn=
49152pixels).Fromtoptobottomrow:originalimage,reconstructionsbyouralgorithm,then
reconstructionsbybaselinesBM3D-AMPandLasso.Inthiscasethenumberofmeasurementsis
muchsmallerthanthenumberofpixels(roughly4%ratio),forwhichBM3D-AMPfailstoconverge,
asdemonstratedbyerroneousgreenandpurplepixels.Werecommendviewingincolor.
25
Figure11:Reconstructionresultsonretinopathyimagesform=4000(ofn=49152pixels).Fromtop
tobottomrow:originalimage,reconstructionsbyouralgorithm,thenreconstructionsbybaselines
BM3D-AMPandLasso.
26
Figure12:Reconstructionresultsonretinopathyimagesform=8000(ofn=49152pixels).Fromtop
tobottomrow:originalimage,reconstructionsbyouralgorithm,thenreconstructionsbybaselines
BM3D-AMPandLasso.
27
Figure13:Aradialsamplingpatternofcoef

intheFourierdomain.Themeasurementsare
obtainedbysamplingFouriercoefalongtheseradiallines.
28
Figure14:Reconstructionresultsonx-rayimagesform=1260Fouriercoef(ofn=65536pix-
els).Fromtoptobottomrow:originalimage,reconstructionsbyouralgorithm,thenreconstructions
bybaselinesBM3D-AMPandTVAL3.
29
"
24,A Novel Hybrid Machine Learning Model for Auto-Classification of Retinal Diseases,http://arxiv.org/pdf/1806.06423v1.pdf,https://github.com/huckiyang/EyeNet,"ANovelHybridMachineLearningModelforof
RetinalDiseases
C.-H.HuckYang*;Jia-HongHuang*;FangyuLiu;Fang-YiChiu;MengyaGao;WeifengLyu;
I-HungLinM.D.;JesperTegner
chao-han.yang@kaust.edu.sa*;r03543018@ntu.edu.tw*;fangyu.liu@uwaterloo.ca;
fachiu@eng.ucsd.edu;daisygao@gatech.edu;wlyu6@gatech.edu;petercard@gmail.com;
jesper.tegner@kaust.edu.sa
LivingSystemsLaboratory,BESE,CEMSE,KingAbdullahUniversityofScienceandTechnology,Thuwal,
KSA*;NationalTaiwanUniversity,TaipeiCity,Taiwan*;UniversityofWaterloo,ON,Canada;Universityof
CaliforniaSanDiego,CA,USA.;GeorgiaInstituteofTechnology,GA,USA.;DepartmentofOphthalmology,
Tri-ServiceGeneralHospital,NationalDefenseMedicalCenter,Taiwan
Abstract
Automaticclinicaldiagnosisofretinaldis-
easeshasemergedasapromisingapproachto
facilitatediscoveryinareaswithlimitedac-
cesstospecialists.Weproposeanovelvisual-
assisteddiagnosishybridmodelbasedonthe
supportvectormachine(SVM)anddeepneu-
ralnetworks(DNNs).Themodelincorpo-
ratescomplementarystrengthsofDNNsand
SVM.Furthermore,wepresentanewclinical
retinalabelcollectionforophthalmologyin-
corporating32retinadiseasesclasses.Using
EyeNet,ourmodelachieves89.73%diagnosis
accuracyandthemodelperformanceiscom-
parabletotheprofessionalophthalmologists.
1.Introduction
ComputationalRetinaldiseasemethods(
Tanetal.
,
2009
;
Lalezaryetal.
,
2006
)hasbeeninvestigated
extensivelythroughtsignalprocessingtech-
niques.Retinaldiseasesisaccessibletomachinedriven
techniquesduetotheirvisualnatureincontrastother
commonhumandiseasesrequiringinvasivetechniques
fordiagnosisortreatments.Typically,thediagnosis
accuracyofretinaldiseasesbasedontheclinicalretinal
imagesishighlydependentonthepracticalexperience
ofphysicianorophthalmologist.However,notevery
doctorhastpracticalexperience.Therefore,
developinganautomaticretinaldiseasesdetectionsys-
temisimportantanditwillbroadlyfacilitatediagnos-
Proceedingsofthe35
th
InternationalConferenceonMa-
chineLearning
,Stockholm,Sweden,2018.JMLR:W&CP
volume28.Copyright2018bytheauthor(s).
Figure1.
Thisrepresentsourproposedhybrid-
model.ArawretinalimageasprovidedasinputofDNNs,
U-Net,andthenwepasstheoutputofU-Nettothedimen-
sionreductionmodule,PCA.Finally,theoutputofPCA
modulesentasinputtotheretinadiseaseSVM,
whichoutputsthenameofpredictedretinadisease.
ticaccuracyofretinaldiseases.Fortheremoterural
area,wheretherearenoophthalmologistslocallyto
screenretinaldisease,theautomaticretinaldiseases
detectionsystemalsocanhelpnon-ophthalmologists
tothepatientoftheretinaldisease,andfurther,
referthemtothemedicalcenterforfurthertreatment.
Thedevelopingofautomaticdiseasesdetection(ADD)
(
etal.
,
2002
)alleviateenormouspressurefrom
socialhealthcaresystems.Retinalsymptomanalysis
(
etal.
,
2010
)isoneoftheimportantADD
applications.Moreover,theincreasingnumberofcases
ofdiabeticretinopathygloballyrequiresextendingef-
fortsindevelopingvisualtoolstoassistintheanalytic
oftheseriesofretinaldisease.Thesedecisionsupport
systemsforretinalADD,as(
Bhattacharya
,
2014
)for
non-proliferativediabeticretinopathyhavebeenim-
provedfromrecentmachinelearningsuccessonthe
highdimensionalimagesprocessingbyfeaturingde-
tailsonthebloodvessel.(
Linetal.
,
2000
)demon-
stratedanautomatedtechniqueforthesegmentation
ofthebloodvesselsbytrackingthecenterofthevessels
onKalmanFilter.However,thesepatternrecognition
basedstillrelyonhand-craftedfeatures
arXiv:1806.06423v1  [cs.CV]  17 Jun 2018ANovelHybridMachineLearningModelforofRetinalDiseases
Figure2.
TheshowstheresultofU-Nettestedon
(a),anunseeneyeballclinicalimage.(b)istheground
truthand(c)isthegeneratedresultofU-Net.Basedon
(b)and(c),wediscoverthatthegeneratedresultishighly
similartothegroundtruth.
andonlyspecifyforevaluatingsingleretinalsymp-
tom.Despiteextensiveusingwaveletsignal
processing,retinalADDremainsaviabletargetfor
improvedmachinelearningtechniquesapplicablefor
point-of-care(POC)medicaldiagnosisandtreatment
intheagingsociety(
Cochocki&Unbehauen
,
1993
).
Tothebestofourknowledge,theamountofclinical
retinalimagesarelesscomparedtoothercellimag-
ingdata,suchasbloodcellandacancercell.Yet,
avanilladeeplearningbaseddiseasesdiagnosissys-
temrequireslargeamountsofdata.Herewe,there-
fore,proposeanovelvisual-assisteddiagnosisalgo-
rithmwhichisbasedonanintegrationofsupportvec-
tormachineanddeepneuralnetworks.Theprimary
goalofthisworkistoautomaticallyclassify32spe-
retinaldiseasesforhumanbeingswiththereliable
clinical-assistedabilityontheintelligentmedicineap-
proaches.Tofosterthelong-termvisualanalyticsre-
search,wealsopresentavisualclinicallabelcollection,
EyeNet,includingseveralcrucialsymptomsasAMN
MacularNeuroretinopathy,andBull'sEyeMaculopa-
thyChloroquine.
Contributions.

Wedesignanovelvisual-assisteddiagnosisalgo-
rithmbasedonthesupportvectormachineand
deepneuralnetworkstofacilitatemedicaldiagno-
sisofretinaldiseases.

Wepresentanewclinicallabelcollection,EyeNet,
forOphthalmologywith32retinadiseasesclasses.

Finally,wetrainamodelbasedontheproposed
EyeNet.Theconsistentdiagnosticaccuracyof
ourmodelwouldbeacrucialaidtotheophthal-
mologist,andelyinapoint-of-caresce-
nario.
2.Methodology
Inthissection,wepresentthewwofourpro-
posedmodel,referringtoFigure
1
.
Figure3.
ThisillustratesthequalitativeresultsofU-
Nettestedonthecolorfulimageson(a)Myelinatedretinal
nerveerlayer(b)age-relatedmaculardegeneration(c)
dischemorrhage
2.1.U-Net
DNNshasgreatlyboostedtheperformanceofimage
duetoitspowerofimagefeaturelearn-
ing(
Simonyan&Zisserman
,
2014
).Activeretinaldis-
easeischaracterizedbyexudatesaroundretinalvessels
resultinginofthevessels(
Khurana
,
2007
).However,ophthalmologyimagesfromclini-
calmicroscopyareoftenoverlayedwithwhitesheath-
ingandminorfeatures.Segmentationofretinalim-
ageshasbeeninvestigatedasacritical(
Rezaeeetal.
,
2017
)visual-aidtechniqueforophthalmologists.U-
Net(
Ronnebergeretal.
,
2015
)isafunctionalDNNs
especiallyforsegmentation.Here,weproposedamod-
versionofU-Netbyreducingthecopyandcrop
processeswithafactoroftwo.Theadjustmentcould
speedupthetrainingprocessandhavebeenvas
anadequatesemanticonsmallsizeimages.We
usecross-entropyforevaluatingthetrainingprocesses
as:
E
=
X
x
2

w
(
x
)
log
(
p
l
(
x
))(1)
where
p
l
istheapproximatedmaximumfunction,and
theweightmapisthencomputedas:
w
(
x
)=
w
c
(
x
)+
w
0

exp
(

(
d
x
1
+
d
x
2
)
2
˙
2
)
2
(2)
d
x
1
designatesthedistancetotheborderofthenear-
estedgesand
d
x
2
designatesthedistancetothebor-
derofthesecondnearestedges.LBscoreisshown
as(
Cochocki&Unbehauen
,
1993
).Weusethedeep
convolutionalneuralnetwork(CNNs)oftwo3

3con-
volutions.Eachstepfollowedbyalinearunit
(ReLU)anda2

2maxpoolingoperationwithstride
2fordownsampling;alayerwithanevenx-andy-size
isselectedforeachoperation.Ourproposedmodel
convergesatthe44thepochwhentheerrorrateofthe
ANovelHybridMachineLearningModelforofRetinalDiseases
modelislowerthan0
:
001.TheaccuracyofourU-Net
modelis95.27%byvalidatedona20%testsetamong
EyeNetshownasFigure
2
.Thismodelisrobustand
feasiblefortretinalsymptomsasillustratedin
Figure
3
.
2.2.PrincipalComponentAnalysis
Principalcomponentanalysis(PCA)isastatistically
matrix-basedmethodbyorthogonaltransformations.
WeusePCAcombinedwithSVMtolower
thecomputingcomplexityandavoidtheresultofover-
onthedecisionboundary.WeoptimizeSVM
withPCAatthe62
nd
principlecomponent.
2.3.SupportVectorMachine
SupportVectorMachineisamachinelearningtech-
niqueforregression,andotherlearning
tasks.Supportvector(SVC)inSVM,
mapdatafromaninputspacetoahigh-dimensional
featurespace,inwhichanoptimalseparatinghyper-
planethatmaximizestheboundarymarginbetween
thetwoclassesisestablished.Thehingelossfunction
isshownas:
1
n
""
n
X
i
=1
max
(0
;
1

y
i
(
~w

~x
i
))
#
+

k
~w
k
2
(3)
Wheretheparameter

determinesthetradebe-
tweenincreasingthemargin-sizeandensuringthatthe
~x
i
lieontherightsideofthemargin.Parametersare
criticalforthetrainingtimeandperformanceonma-
chinelearningalgorithms.Wepickupcostfunction
parametercas128andgammaas0.0078.TheSVM
hascomparablyhighperformancewhenthecostcoef-
thigherthan132.Weuseradialbasisfunction
(RBF)andpolynomialkernelforSVC.
3.onRetinalLabelCollection
RetinaImageBank(RIB)isaninternationalclinical
projectlaunchedbyAmericanSocietyofRetinaSpe-
cialistsin2012,whichallowsophthalmologistsaround
theworldtosharetheexistingclinicalcasesonline
formedicine-educationalproposes.Herewepresent
EyeNetwhichismainlybasedontheRIB.Tothis
end,wemanuallycollectedthe32symptomsfrom
RIB,especiallyontheretina-relateddiseases.er-
entfromthetraditionalretinadataset(
Staaletal.
,
2004
)focusedonthemorphologyanalysis,ourgiven
opensourcedatasetlabeledfromtheRIBprojectis
concentratedonthebetweendiseaseforfea-
sibleaid-diagnosisandmedicalapplications.With
therecentsuccessoncollectinghigh-qualitydatasets,
suchasImageNet(
Krizhevskyetal.
,
2012
),webelieve
thatcollectingandminingRIBforamoredeveloper-
friendlydatapipelineisvaluableforbothOphthalmol-
Figure4.
AplotoflosstrainingAlexNetinitializedwith
ImageNetpretrainedweights(orange)andinitializedwith
randomweights(blue).
ogyandComputerVisioncommunityenablingdevel-
opmentofadvancedanalyticaltechniques.
4.Experiments
Inthissection,wedescribetheimplementationde-
tailsandexperimentsweconductedtovalidateour
proposedmethod.
4.1.Dataset
Forexperiments,theoriginalEyeNetisrandomlydi-
videdintothreeparts:70%fortraining,10%forval-
idationand20%fortesting.Allthetrainingdata
havetogothroughthePCAbeforeSVM.Allclas-
experimentsaretrainedandtestedonthe
samedataset.
4.2.Setup
TheEyeNethasbeenprocessedtoU-Nettogener-
ateasubsetwiththesemanticfeatureofablood
vessel.FortheDNNsandTransferLearningmod-
els,wedirectlyusetheRGBimagesfromreti-
nallabelcollection.EyeNetispublishedonline:
github.com/huckiyang/EyeNet
4.3.DeepConvolutionalNeuralNetworks
CNNshavedemonstratedextraordinaryperformance
invisualrecognitiontasks(
Krizhevskyetal.
,
2012
),
andthestateoftheartisinagreatmanyvision-
relatedbenchmarksandchallenges(
Xieetal.
,
2017
).
Withlittleornopriorknowledgeandhuman
infeaturedesign,ityetprovidesageneraland
tivemethodsolvingvariantvisiontasksinvariantdo-
mains.Thisnewdevelopmentincomputervisionhas
alsoshowngreatpotentialhelping/replacinghuman
judgmentinvisionproblemslikemedicalimaging(
Es-
ANovelHybridMachineLearningModelforofRetinalDiseases
tevaetal.
,
2017
),whichisthetopicwetrytoaddress
inthispaper.Inthissection,weintroduceseveral
baselinesinmulti-classimagerecognitionandcompare
theirresultsontheEyeNet.
4.3.1.Baseline1-AlexNet
AlexNet(
Krizhevskyetal.
,
2012
)vthefeasi-
bilityofapplyingdeepneuralnetworksonlargescale
imagerecognitionproblems,withthehelpofGPU.
Itbroughtupasuccinctnetworkarchitecture,with
5convolutionallayersand3fully-connectedlayers,
adoptingReLU(
Nair&Hinton
,
2010
)astheactiva-
tionfunction.
4.3.2.Baseline2-VGG11
VGG(
Simonyan&Zisserman
,
2014
)usesverysmall
(3x3)repeatedlytoreplacethelarge
(5x5,7x7)intraditionalarchitectures.Bypushing
depthsofthenetwork,itachievedstate-of-the-artre-
sultsonImageNetwithfewerparameters.
4.3.3.Baseline3-SqueezeNet
Realworldmedicalimagingtasksmayrequireasmall
yetemodeltoadapttolimitedresourcesof
hardware.Assomeverydeepneuralnetworkscan
costseveralhundredmegabytestostore,SqueezeNet
(
Iandolaetal.
,
2016
)adoptingmodelcompression
techniqueshasachievedAlexNetlevelaccuracywith
˘
500xsmallermodels.
4.4.TransferLearning
Weexploitatransferlearningframeworkfromnormal-
izedImageNet(
Krizhevskyetal.
,
2012
)totheEyeNet
forsolvingthesmallsamplesissueonthecomputa-
tionalretinalvisualanalytics.Withtanduti-
lizabletrainingmodel,TransferLearningcan
resolvethechallengeofMachineLearninginthelimit
ofaminimalamountoftraininglabelsbymeansof
TransferLearning,whichdrasticallyreducethedata
requirements.ThefewlayersofDNNslearnfea-
turessimilartoGaborandcolorblobsandthese
featuresappearnottobespecitoanyparticulartask
ordatasetandthusapplicabletootherdatasetsand
tasks(
Yosinskietal.
,
2014
).Experimentshaveshown
timprovementafterapplyingpretrainedpa-
rametersonourdeeplearningmodels,referringtoTa-
ble
1
andTable
2
.
4.5.Hybrid-SVMsResults
AllSVMareimplementedinMatlabwithlibsvm
(
Chang&Lin
,
2011
)module.Weseparateboththe
originalretinaldatasetandthesubsettothreeparts
included70%trainingset,20%testset,and10%val-
idationset.Bytrainingtwomultiple-classesSVM
modelsonbothoriginalEyeNetandthesubset,we
Hybrid-Ratio
RBFkernel
Polyno.kernel
0%:100%
0.8203
0.8439
40%:60%
0.8371
0.8381
47%:53%
0.8973
0.8781
61%:39%
0.8903
0.8940
100%:0%
0.8626
0.8733
Table1.
AccuracycomparisonofHybrid-SVMwithRBF
andPolynomialkernel.Weintroduceahybrid-ratioofthe
mixedweightedvotingbetweentwomulti-SVCstrained
fromEyeNetandtheU-Netsubset.
Model
Pretrained
RandomInit.
AlexNet
0.7903
0.4839
VGG11
0.8871
0.7581
SqueezeNet
0.8226
0.5633
Table2.
AccuracycomparisonofthreeDNNsbaselines
implementaweightedvotingmethodtoidentifythe
candidateofretinasymptom.Wehave
entweightratioas
Hybrid

Ratio
,SVMmodelwith
f
RGBImages:SVMmodelwithU-Netsubset
g
,be-
tweenEyeNetandthesubsetwithVesselfeaturesto
makeahigheraccuracyatTable
1
.Wehavev
themodelwithoutovbythevalidationsetvia
normalizationontheaccuracywith
~
2.31%
4.6.DeepNeuralNetworksResults
AllDNNsareimplementedinPyTorch.Weuseidenti-
calhyperparametersforallmodels.Thetraininglasts
400epochs.The200epochstakealearningrate
of1e-4andthesecond200take1e-5.Besides,we
applyrandomdataaugmentationduringtraining.In
everyepoch,thereis70%probabilityforatraining
sampletobetransformedbyoneoftheop-
erationsin
f
rotate,transpose

randomcrop
g
.
ThoughImageNetandourRetinallabelcollectionare
mucht,usingweightspretrainedonImageNet
ratherthanrandomoneshasboostedtestaccuracy
ofanymodelswith5to15percentages,referringto
Table
2
.Besides,pretrainedmodelstendtoconverge
muchfasterthanrandominitializedonesassuggested
inFigure
4
.TheperformanceofDNNsonourretinal
datasetcangreatlybfromaknowledgeofother
domains.
5.ConclusionandFutureWork
Inthiswork,wehavedesignedanovelhybridmodel
forvisual-assisteddiagnosisbasedontheSVMandU-
Net.Theperformanceofthismodelshowsthehigher
accuracy,89.73%,overtheotherpre-trainedDNNs
modelsasanaidforophthalmologists.Also,wepro-
posetheEyeNettobthemedicalinformaticsre-
searchcommunity.Finally,sinceourlabelcollection
ANovelHybridMachineLearningModelforofRetinalDiseases
notonlycontainsimagesbutalsotextinformationof
theimages,VisualQuestionAnswering(
Huangetal.
,
2017b
;
c
;
a
)basedontheretinalimagesisoneofthe
interestingfuturedirections.Ourworkmayalsohelp
theremoteruralarea,wheretherearenoophthalmol-
ogistslocally,toscreenretinaldiseasewithoutthehelp
ofophthalmologistsinthefuture.
Acknowledgement
Thisworkissupportedbycompetitiveresearchfund-
ingfromKingAbdullahUniversityofScienceand
Technology(KAUST).Also,wewouldliketoacknowl-
edgeGoogleCloudPlatformandRetinaImageBank,
aprojectfromtheAmericanSocietyofRetinaSpe-
cialists.
References
MichaelD,Garvin,MonaK,andSonka,
Milan.Retinalimagingandimageanalysis.
Reviews
inbiomedicalengineering
,3:169{208,2010.
Bhattacharya,Sharbani.Watermarkingdigitalimages
usingfuzzymatrixcompositionsand(

,

)-cutof
fuzzyset.
InternationalJournalofAdvancedCom-
puting
,(2051-0845),2014.
Chang,Chih-ChungandLin,Chih-Jen.Libsvm:a
libraryforsupportvectormachines.
TIST
,2(3):27,
2011.
Cochocki,AandUnbehauen,Rolf.
Neuralnetworks
foroptimizationandsignalprocessing
.JohnWiley
&Sons,Inc.,1993.
Esteva,Andre,Kuprel,Brett,Novoa,RobertoA,
Ko,Justin,Swetter,SusanM,Blau,HelenM,and
Thrun,Sebastian.Dermatologist-levelc
ofskincancerwithdeepneuralnetworks.
Nature
,
542(7639):115,2017.
Huang,Jia-Hong,Alfadly,Modar,andGhanem,
Bernard.Robustnessanalysisofvisualqamodels
bybasicquestions.
arXiv:1709.04625
,2017a.
Huang,Jia-Hong,Alfadly,Modar,andGhanem,
Bernard.Vqabq:visualquestionansweringbybasic
questions.
arXiv:1703.06492
,2017b.
Huang,Jia-Hong,Dao,CuongDuc,Alfadly,Modar,
andGhanem,Bernard.Anovelframework
forrobustnessanalysisofvisualqamodels.
arXiv:1711.06232
,2017c.
Iandola,ForrestN,Han,Song,Moskewicz,
MatthewW,Ashraf,Khalid,Dally,WilliamJ,and
Keutzer,Kurt.Squeezenet:Alexnet-levelaccuracy
with50xfewerparametersand<0.5mbmodelsize.
arXiv:1602.07360
,2016.
Khurana,AK.
Comprehensiveophthalmology
.New
AgeInternationalLtd,2007.
Krizhevsky,Alex,Sutskever,Ilya,andHinton,Geof-
freyE.Imagenetwithdeepconvolu-
tionalneuralnetworks.In
AdvancesinNIPS
,pp.
1097{1105,2012.
Lalezary,Maziar,Medeiros,FelipeA,Weinreb,
RobertN,Bowd,Christopher,Sample,PamelaA,
Tavares,IvanM,Tafreshi,Ali,andZangwill,
LindaM.Baselineopticalcoherencetomography
predictsthedevelopmentofglaucomatouschange
inglaucomasuspects.
Americanjournalofophthal-
mology
,142(4):576{582,2006.
Lin,Ching-Yung,Wu,Min,Bloom,A,Cox,
IngemarJ,Miller,MattL,andLui,YuiMan.
Rotation-,scale-,andtranslation-resilientpublic
watermarkingforimages.In
Securityandwater-
markingofmultimediacontentsII
,volume3971,pp.
90{99.InternationalSocietyforOpticsandPhoton-
ics,2000.
Nair,VinodandHinton,E.linear
unitsimproverestrictedboltzmannmachines.In
Proceedingsofthe27thinternationalconferenceon
machinelearning(ICML-10)
,pp.807{814,2010.
Rezaee,Khosro,Haddadnia,Javad,andTashk,
Ashkan.Optimizedclinicalsegmentationofretinal
bloodvesselsbyusingcombinationofadaptive
tering,fuzzyentropyandskeletonization.
Applied
SoftComputing
,52:937{951,2017.
Ronneberger,Olaf,Fischer,Philipp,andBrox,
Thomas.U-net:Convolutionalnetworksfor
biomedicalimagesegmentation.In
International
ConferenceonMICCAI
,pp.234{241.Springer,
2015.
Mohsen,Fathy,Mahmood,andMahmoudi,
MaryamTayefeh.Aandcomparative
studyofedgedetectionalgorithms.In
Informa-
tionTechnology:CodingandComputing,2002.Pro-
ceedings.InternationalConferenceon
,pp.117{120.
IEEE,2002.
Simonyan,KarenandZisserman,Andrew.Verydeep
convolutionalnetworksforlarge-scaleimagerecog-
nition.
arXiv:1409.1556
,2014.
Staal,Joes,MichaelD,Niemeijer,Mein-
dert,Viergever,MaxA,andVanGinneken,Bram.
Ridge-basedvesselsegmentationincolorimagesof
theretina.
TMI
,23(4):501{509,2004.
ANovelHybridMachineLearningModelforofRetinalDiseases
Tan,Ou,Chopra,Vikas,Lu,AkeTzu-Hui,Schuman,
JoelS,Ishikawa,Hiroshi,Wollstein,Gadi,Varma,
Rohit,andHuang,David.Detectionofmaculargan-
glioncelllossinglaucomabyfourier-domainopti-
calcoherencetomography.
Ophthalmology
,116(12):
2305{2314,2009.
Xie,Saining,Girshick,Ross,ar,Piotr,Tu,
Zhuowen,andHe,Kaiming.Aggregatedresidual
transformationsfordeepneuralnetworks.In
CVPR
,
pp.5987{5995.IEEE,2017.
Yosinski,Jason,Clune,Bengio,Yoshua,andLip-
son,Hod.Howtransferablearefeaturesindeep
neuralnetworks?In
NIPS
,pp.3320{3328,2014.
"
25,Learning to Evaluate Image Captioning,http://arxiv.org/pdf/1806.06422v1.pdf,https://github.com/richardaecn/cvpr18-caption-eval,"LearningtoEvaluateImageCaptioning
YinCui
1
;
2
GuandaoYang
1
AndreasVeit
1
;
2
XunHuang
1
;
2
SergeBelongie
1
;
2
1
DepartmentofComputerScience,CornellUniversity
2
CornellTech
Abstract
Evaluationmetricsforimagecaptioningfacetwochal-
lenges.Firstly,commonlyusedmetricssuchasCIDEr,ME-
TEOR,ROUGEandBLEUoftendonotcorrelatewellwith
humanjudgments.Secondly,eachmetrichaswellknown
blindspotstopathologicalcaptionconstructions,andrule-
basedmetricslackprovisionstorepairsuchblindspots
onceForexample,thenewlyproposedSPICE
correlateswellwithhumanjudgments,butfailstocapture
thesyntacticstructureofasentence.Toaddressthesetwo
challenges,weproposeanovellearningbaseddiscrimina-
tiveevaluationmetricthatisdirectlytrainedtodistinguish
betweenhumanandmachine-generatedcaptions.Inaddi-
tion,wefurtherproposeadataaugmentationschemetoex-
plicitlyincorporatepathologicaltransformationsasnega-
tiveexamplesduringtraining.Theproposedmetriciseval-
uatedwiththreekindsofrobustnesstestsanditscorrelation
withhumanjudgments.Extensiveexperimentsshowthat
theproposeddataaugmentationschemenotonlymakesour
metricmorerobusttowardseveralpathologicaltransforma-
tions,butalsoimprovesitscorrelationwithhumanjudg-
ments.Ourmetricoutperformsothermetricsonbothcap-
tionlevelhumancorrelationinFlickr8kandsystemlevel
humancorrelationinCOCO.Theproposedapproachcould
beservedasalearningbasedevaluationmetricthatiscom-
plementarytoexistingrule-basedmetrics.
1.Introduction
Learningtoautomaticallygeneratecaptionstosumma-
rizethecontentofanimageisconsideredasacrucialtaskin
ComputerVision.Theevaluationofimagecaptioningmod-
elsisgenerallyperformedusingmetricssuchasBLEU[
27
],
METEOR[
20
],ROUGE[
23
]orCIDEr[
32
],allofwhich
mainlymeasurethewordoverlapbetweengeneratedand
referencecaptions.TherecentlyproposedSPICE[
3
]mea-
suresthesimilarityofscenegraphsconstructedfromthe
candidateandreferencesentence,andshowsbettercorrela-
tionwithhumanjudgments.
Thesecommonlyusedevaluationmetricsfacetwochal-
lenges.Firstly,manymetricsfailtocorrelatewellwithhu-
Figure1.Anoverviewofourproposedcaptioningevaluationmet-
ric.Fromasetofimagesandcorrespondinghumanwrittenand
machinegeneratedcaptions,wetrainamodeltodiscriminatebe-
tweenhumanandgeneratedcaptions.Themodelcomprisesthree
majorcomponents:aCNNtocomputeimagerepresentations,an
RNNwithLSTMcellstoencodethecaption,andabinaryclassi-
asthecritique.Aftertraining,thelearnedcritiquecanbeused
asametrictoevaluatethequalityofcandidatecaptionswithre-
specttothecontext(
i.e
.,theimageandreferencehumancaptions).
manjudgments.Metricsbasedonmeasuringwordoverlap
betweencandidateandreferencecaptionsitdif
tocapturesemanticmeaningofasentence,thereforeoften
leadtobadcorrelationwithhumanjudgments.Secondly,
eachevaluationmetrichasitswell-knownblindspot,and
rule-basedmetricsareoftenxibletoberesponsiveto
newpathologicalcases.Forexample,SPICEissensitiveto
thesemanticmeaningofacaptionbuttendstoignoreits
syntacticquality.Liu
etal
.[
25
]showsthatSPICEprefers
togivehighscoretolongsentenceswithrepeatingclauses.
It'snoteasytoletSPICEtakesuchpathologicalcasesinto
account.Sinceit'sdiftocompletelyavoidsuchblind
spots,agoodevaluationmetricforimagecaptioningshould
1
arXiv:1806.06422v1  [cs.CV]  17 Jun 2018bexibleenoughtoadapttopathologicalcasesonceiden-
whilecorrelatingwellwithhumanjudgments.
Toaddresstheaforementionedtwochallenges,wepro-
poseametricthatdirectlydiscriminatesbetweenhuman
andmachinegeneratedcaptionswhilebeingabletoxi-
blyadapttopathologicalcasesofourinterests.Sincereal
humanjudgmentisimpracticaltoobtainatscale,ourpro-
posedlearningbasedmetricistrainedtoperformlikeahu-
mancritique,asillustratedinFig.
1
.Weuseastate-of-
the-artCNNarchitecturetocapturehigh-levelimagerep-
resentations,andaRNNwithLSTMcellstoencodecap-
tions.Todesignthelearnedcritique,wefollowinsights
fromtheCOCOCaptioningChallengein2015[
1
,
7
],in
whichalarge-scalehumanjudgmentexperimentwasper-
formed.Inparticular,ourcritiqueisabinarythat
makesaTuringTesttypejudgmentinwhichitdifferentiates
betweenhuman-writtenandmachine-generatedcaptions.
Inordertocapturetargetedpathologicalcases,wepro-
posetoincorporatethesepathologicalsentencesasnegative
trainingexamples.Tosystematicallycreatesuchpathologi-
calsentences,weseveraltransformationstogenerate
unnaturalsentencesthatmightgethighscoresinaneval-
uationmetric.Ourproposeddataaugmentation(Sec.
3.3
)
schemeusesthesetransformationstogeneratelargenumber
ofnegativeexamples,whichguideourmetrictoexplorea
varietyofpossiblesentenceconstructionsthatareraretobe
foundinrealworlddata.Further,weproposeasystematic
approachtomeasuretherobustnessofanevaluationmetric
toagivenpathologicaltransformation(Sec.
3.4
).Extensive
experiments(Sec.
4
)verifytheeffectivenessandrobustness
ofourproposedevaluationmetricanddemonstratebetter
correlationwithhumanjudgmentsonCOCOandFlickr8k,
comparedwithcommonly-usedimagecaptioningmetrics.
Ourkeycontributionscanbesummarizedasfollows:

Weproposeanovellearnedbasedcaptioningevalu-
ationmetricthatdirectlycaptureshumanjudgments
whilebeingxibletotargetedpathologicalcases.

Wedemonstratekeyfactorsforhowtosuccessfully
trainagoodcaptioningevaluationmetric.

Weconductcomprehensivestudiesthatdemonstrates
theeffectivenessoftheproposedmetric,inparticular
itscorrelationtohumanjudgmentandrobustnessto-
wardpathologicaltransformations.
2.RelatedWork
Captioningevaluation
.Despiterecentinterests,image
captioningisnotoriouslydiftoevaluateduetothein-
herentambiguity.Humanevaluationscoresarereliablebut
costlytoobtain.Thus,currentimagecaptioningmodelsare
usuallyevaluatedwithautomaticmetricsinsteadofhuman
judgments.CommonlyusedevaluationmetricsBLEU[
27
],
METEOR[
20
],ROUGE[
23
]andCIDEr[
32
]aremostly
basedonn-gramoverlapandtendtobeinsensitivetose-
manticinformation.Anderson
etal
.recentlyproposedthe
SPICE[
3
]thatisbasedonscenegraphsimilarity.Al-
thoughSPICEobtainshighercorrelationwith
humanjudgments,itencountersdifwithrepetitive
sentences,aspointedoutin[
25
].Itisworthnotingthatall
abovementionedmetricsrelysolelyonsimilaritybetween
candidateandreferencecaptions,withouttakingtheimage
intoconsideration.Ourproposedmetric,ontheotherhand,
takesimagefeatureasinput.Whileallthepreviousmetrics
arerule-based,ourproposedmetriclearnstoscorecandi-
datecaptionsbytrainingtodistinguishpositiveandneg-
ativeexamples.Moreover,ourproposedtrainingscheme
couldxiblytakenewpathologicalcasesintoaccount,yet
traditionalmetricsithardtoadapt.
Adversarialtrainingandevaluation
.GenerativeAd-
versarialNetworks(GANs)[
12
]havebeenrecentlyapplied
togenerateimagecaptions[
8
,
22
,
6
,
31
].AlthoughGANs
couldprovidediscriminatorstotellaparthumanandma-
chinegeneratedcaptions,theydifferfromourworksasour
discriminatorfocusesonevaluationinsteadofgeneration.
Allexistingadversarialevaluationapproachesthe
generatorperformancetobeinverselyproportionaltothe
performanceofthediscriminator,motivated
bytheintuitionthatagoodgeneratorshouldproduceout-
putsthatarehardforthediscriminatortodistinguishfrom
realdata.Thedifferamongtheap-
proaches.Im
etal
.[
16
]proposetotrainapairofGANsand
interchangetheiropponentsduringtesting.Iowe
etal
.[
26
]
attempttotrainasinglediscriminatoronalargecorpusof
dialogueresponsesgeneratedbydifferentdialoguesystems.
Otherapproaches[
17
,
4
,
21
]trainonediscriminatorsep-
aratelyforeachmodel.Differentfromimplicitlygener-
atednegativeexamplesbyageneratorinthesework,we
incorporateexplicitlypathologicaltransformations
togeneratenegativeexamples.Moreover,noneoftheabove
literaturehasvtheeffectivenessoftheirmetricsby
thecorrelationwithhumanjudgments.
3.DiscriminativeEvaluation
Acaptionisconsideredofhighqualityifitisjudgedwell
byhumans.Inparticular,thequalityofageneratedcaption
ismeasuredbyhowsuccessfulitcanfoolacritiqueinto
believingitiswrittenbyhuman.
3.1.EvaluationMetric
Theproposedevaluationmetricfollowsthegeneralsetup
ofaTuringTest.First,wetrainanautomaticcritiquetodis-
tinguishgeneratedcaptionsfromhuman-writtenones.We
thenscorecandidatecaptionsbyhowsuccessfultheyarein
foolingthecritique.
Formally,givenacritiqueparametrizedby

,areference
image
i
,andageneratedcaption
^
c
,thescoreisas
theprobabilityforthecaptionofbeinghuman-written,as
Figure2.ThemodelarchitectureoftheproposedlearnedcritiquewithCompactBilinearPooling.Weuseadeepresidualnetworkand
anLSTMtoencodethereferenceimageandhumancaptionintocontextvector.TheidenticalLSTMisappliedtogettheencodingofa
candidatecaption.Thecontextfeatureandthefeatureextractedfromthecandidatecaptionarecombinedbycompactbilinearpooling.The
issupervisedtoperformaTuringTestbyrecognizingwhetheracandidatecaptionishumanwrittenormachinegenerated.
assignedbythecritique:
score

(^
c;i
)=
P
(^
c
ishumanwritten
j
i;

(1)
Thescoreisconditionedonthereferenceimage,because
thetaskoftheevaluationisnotsimplytodecidewhethera
givensentenceiswrittenbyahumanormachinegenerated,
butalsotoevaluatewhetheritaccuratelycapturestheimage
contentandfocusesontheimportantaspectsoftheimage.
Moregenerally,thereferenceimagerepresentsthecon-
textinwhichthegeneratedcaptionisevaluated.Toprovide
furtherinformationabouttherelevanceandsalienceofthe
imagecontent,areferencecaptioncanadditionallybesup-
pliedtothecontext.Let
C
(
i
)
denotesthecontextofimage
i
,thenreferencecaption
c
couldbeincludedaspartofcon-
text,
i.e
.,
c
2C
(
i
)
.Thescorewithcontextbecomes
score

(^
c;i
)=
P
(^
c
ishumanwritten
jC
(
i
)
;

(2)
3.2.ModelArchitecture
Theproposedmodelcanbegenerallydescribedintwo
parts.Inthepart,thecontextinformationincludingthe
imageandreferencecaptionareencodedasfeaturevectors.
Thesetwofeaturevectorsarethenconcatenatedasasingle
contextvector.Inthesecondpart,thecandidatecaption
isencodedintoavector,inthesamewayasthereference
caption.Wethenfeditintoabinary,togetherwith
thecontextvector.Fig.
2
givesanoverviewofthemodel.
Toencodetheimage
i
asafeaturevector
i
,weuse
aResNet[
13
]pre-trainedonImageNet[
9
]withed
weights.Thereferencecaption
c
aswellasthecandidate
caption
^
c
areencodedasfeaturevectors
c
and
^c
usingan
LSTM-based[
14
]sentenceencoder.Toformtheinputof
theLSTM,eachwordisrepresentedasa
d
-dimensional
wordembeddingvector
x
2
R
d
whichisinitializedfrom
GloVe[
28
].TheLSTMsusedtoencodethetwocaptions
sharethesameweights.Theweightsoftheinitialwordem-
beddingaswellasoftheLSTMareupdatedduringtraining.
Oncetheencodedfeaturevectorsarecomputed,theyare
combinedintoasinglevector.Inourexperiments,weuse
twodifferentwaystocombinethesefeatures;bothmethods
providecomparableresults.Themethodsimplycon-
catenatesthevectorsfollowedbyaMLP:
v
=
ReLU
(
W

concat
([
i
;
c
;
^c
])+
b
)
(3)
whereReLU
(
x
)=max(
x;
0)
.Forthesecondmethod,we
concatenatethecontextinformationasconcat
([
i
;
c
])
andsubsequentlycombineitwiththecandidatecaptionus-
ingCompactBilinearPooling(CBP)[
11
],whichhasbeen
demonstratedin[
10
]tobeveryeffectiveincombininghet-
erogeneousinformationofimageandtext.CBPusesCount
Sketch[
5
,
29
]toapproximatetheouterproductbetween
twovectorsinalowerdimensionalspace.Thisresultsina
featurevector
v
thatcaptures
2
nd
orderfeatureinteractions
compactlyasrepresentedby:
v
=

concat
([
i
;
c
])




^c

(4)
where


)
representsCountSketchand

isthecircu-
larconvolution.Inpractice,circularconvolutionisusually
calculatedinfrequencydomainviaFastFourierTransform
(FFT)anditsinverse(FFT

1
).
Thefeaturecombinationisfollowedbya2-waysoftmax
representingtheclassprobabilitiesofbeinghu-
manwrittenormachinegenerated.Finally,theis
trainedusingthecross-entropylossfunction
H
(

;

)
:
L
=
1
N
N
X
n
=1
H
(
p
n
;q
n
)
(5)
where
N
isthenumberoftrainingexamples,
p
istheoutput
ofthesoftmaxand
q
isaone-hotvectorindicating
Figure3.Examplegroundtruthcaptionsbeforeandaftertransfor-
mations.Robustnessofourlearnedmetricsisevaluatedonhuman
captionsfromsimilarimages(
T
RC
)aswellaswithrandom(
T
RW
)
andpermuted(
T
WP
)words.
thegroundtruthofwhetheracandidatecaptionisindeed
humanwrittenormachinegenerated.
Byassigningalossfunctionthatdirectlycaptureshu-
manjudgment,thelearnedmetriciscapableofmeasuring
theobjectiveoftheimagecaptioningtask.Duringinfer-
ence,theprobabilityfromthesoftmaxofbeing
thehumanwrittenclassisusedtoscorecandidatecaptions.
3.3.Datasamplingandaugmentation
Wewouldliketousedataaugmentationtoincorporate
pathologicalcasesasnegativeexamplesduringtraining.We
severaltransformationsofthetrainingdatatogener-
atealargeamountofpathologicalsentences.Formally,a
transformation
T
takesanimage-captiondatasetandgener-
atesanewone:
T
(
f
(
c;i
)
2Dg
;

)=
f
(
c
0
1
;i
0
1
)
;:::;
(
c
0
n
;i
0
n
)
g
(6)
where
i;i
0
i
areimages,
c;c
0
i
arecaptions,
D
isalistof
caption-imagetuplesrepresentingtheoriginaldataset,and

isahyper-parameterthatcontrolsthestrengthofthetrans-
formation.,wefollowingthreetransfor-
mationstogeneratepathologicalimage-captionspairs:
RandomCaptions(RC)
.Toensureourmetricpaysat-
tentiontotheimagecontent,werandomlysamplehuman
writtencaptionsfromotherimagesinthetrainingset:
T
RC
(
D
;

)=
f
(
c
0
;i
)
j
(
c;i
)
;
(
c
0
;i
0
)
2D
;i
0
2N

(
i
)
g
(7)
where
N

(
i
)
representsthesetofimagesthataretop

per-
centnearestneighborstoimage
i
.
WordPermutation(WP)
.Tomakesurethatourmetric
paysattentiontosentencestructure,werandomlypermute
atleast2wordsinthereferencecaption:
T
WP
(
D
;

)=
f
(
c
0
;i
)
j
(
c;i
)
2D
;c
0
2P

(
c
)
nf
c
gg
(8)
where
P

(
c
)
representsallsentencesgeneratedbypermut-
ing

percentofwordsincaption
c
.
Figure4.Relativewordfrequency(inlogscale)inthecaptions
generatedbyﬁNeuralTalkﬂ[
18
],ﬁShowandTellﬂ[
33
],ﬁShow,At-
tendandTellﬂ[
34
],andhumancaptions.Machinegeneratedcap-
tionshavedrasticallydifferentwordfrequencydistributionsfrom
humanwrittencaptions,ashumancaptionstendtocontainmuch
moreinfrequentwords.Asaresult,adiscriminatorcouldsimply
detecttherarewordsandachievelowloss.
RandomWord(RW)
.Toexplorerarewordswereplace
from2toallwordsofthereferencecaptionwithrandom
wordsfromthevocabulary:
T
RW
(
D
;

)=
f
(
c
0
;i
)
j
(
c;i
)
2D
;c
0
2W

(
c
)
nf
c
gg
(9)
where
W

(
c
)
representsallsentencesgeneratedbyran-
domlyreplacing

percentwordsfromcaption
c
.
Notethatallthe

'saretobeaper-
centage.

%=0
denotestheoriginalcaptionwithouttrans-
formation,while

%=1
providesthestrongestpossible
transformations.Fig.
3
showsexamplecaptionsbeforeand
afterthesetransformations.
Theneedfordataaugmentationcanbefurtherillustrated
byobservingthewordfrequencies.Fig.
4
showstherelative
wordfrequencyinthecaptionsgeneratedbythreepopular
captioningmodelsaswellasthefrequencyinhumancap-
tions.Apparently,adiscriminatorcaneasilytellhumanand
generatedcaptionsapartbysimplylookingatwhatwords
areused.Infact,asimplecritiqueonlytrainedonhuman
writtenandmachine-generatedcaptionstendstobelieve
thatasequenceofrandomwordsiswrittenbyahuman,
simplybecauseitcontainsmanyrarewords.Toaddressthis
problem,ouraugmenteddataalsoincludescaptionsgener-
atedusingMonteCarloSampling,whichcontainsamuch
highervarietyofwords.
3.4.PerformanceEvaluation
Alearnedcritiqueshouldbecapableofcorrectlydistin-
guishinghumanwrittencaptionsfrommachinegenerated
ones.Therefore,theobjectiveofthecritiqueistoassign
scorescloseto0togeneratedcaptionsandscorescloseto
1tohumancaptions.Inlightofthis,wetheperfor-
manceofacritiqueashowcloseitgetstotheidealobjec-
tives,whichiseitherthescoreassignedtoahumancaption
oroneminusthescoreassignedtoageneratedcaption:
s

;
(^
c;i
))=
(
1

score

(^
c;i
)
;
if
^
c
isgenerated
score

(^
c;i
)
;
otherwise
where

representsthecritique,
^
c
isthecandidatecaption,
and
i
istheimage
^
c
summarizes.Theperformanceofa
modelisthenastheaveragedperformanceonall
theimage-captionpairsinatestorvalidationset:
s

;
D
)=
1
jDj
X
(^
c;i
)
2D
s

;
(^
c;i
))
(10)
where
D
isthesetofallimage-captionpairsinaheld-out
validationortestset.
Givenapathologicaltransformation
T
and

,wecould
computetheaveragescoreofametric

onthetransformed
validationset
T
(
D
;
)
,
i.e
.
s

;
T
(
D
;
))
.Wethe
robustnessscorewithrespecttotransformation
T
asthe
Area-Under-Curve(AUC)of
s

;
T
(
D
;
))
byvaryingall
possible

:
R

;
T
)=
Z
s

;
T
(
D
;
))

(11)
Weexpectarobustevaluationmetrictogivelowscores
totheimage-captionpairsgeneratedbythepathological
transformations.Tocomparemetricswithdifferentscales,
wenormalizethescoresgivenbyeachmetricsuchthatthe
groundtruthhumancaptionreceivesascoreof1.Detailed
experimentsarepresentedinSec.
4.2
andSec.
4.3
.
3.5.UsingtheLearnedMetrics
Tousethelearnedmetricsinpractice,oneneedsto
boththemodelarchitectureofthediscriminatorandall
thehyper-parametersofthetrainingprocess.Whenevalu-
atingacaptioningmodel,weneedthegeneratedcaptions
ofthemodelforasetofimages(
i.e
.,validationortest
setofaimagecaptioningdataset).Wethensplitthere-
sultsintotwofolds.Thediscriminativemetricistrained
withimage-captionpairsinfoldastrainingdata,to-
getherwithgroundtruthcaptionswrittenbyhuman.Then
weusethetrainedmetrictoscoretheimage-captionpairs
ontheotherfold.Similarly,wescorealltheimage-caption
pairsinthefoldusingametrictrainedfromthesecond
fold.Oncewegetalltheimage-captionpairsscoredinthe
dataset,theaveragescorewillbeusedastheevaluationof
thecaptioningmodel.Onecouldreducethevarianceofthe
evaluationscorebytrainingthemetricmultipletimesand
usetheaveragedevaluationscoreacrossalltheruns.
4.Experiments
4.1.Experimentsetup
Data.
WeusetheCOCOdataset[
24
]toevaluatethe
performanceofourproposedmetric.Totestthecapability
(Sec.
4.2
)androbustness(Sec.
4.3
)oftheproposedmodels,
weusethedatasplitfrom[
18
],whichre-splitstheoriginal
COCOdatasetintoatrainingsetwith113,287images,a
validationandatestset,eachcontains5,000images.Each
imageisannotatedbyroughly5humanannotators.Weuse
thevalidationsetforparametertuning.Forthesystemlevel
humancorrelationstudy(Sec.
4.5
),weuse12submission
entriesfromthe2015COCOCaptioningChallengeonthe
COCOvalidationset
1
.
Thecaptionlevelhumancorrelationstudy(Sec.
4.4
)uses
humanannotationsinFlickr8kdataset[
15
].Flickr8kcol-
lectstwosetsofhumanannotations,eachonadifferentset
ofimagecaptionpairs.Amongtheseimage-captionpairs,
candidatecaptionsaresampledfromhumancaptionsinthe
dataset.Inthesetofhumanannotation(ExpertAnno-
tation),humanexpertsareaskedtoratetheimage-caption
pairswithscoresrangingfrom1:Theselectedcaptionis
unrelatedtotheimageto4:Theselectedcaptiondescribes
theimagewithoutanyerrors.Thesecondsetofannotation
(CrowdFlowerAnnotation)iscollectedbyaskinghuman
raterstodecidewhetheracaptiondescribesthecorrespond-
ingimageornot.
ImageCaptioningModels.
Weusepubliclyavailable
implementationsofﬁNeuralTalkﬂ(
NT
)[
18
],ﬁShowand
Tellﬂ(
ST
)[
33
],ﬁShow,AttendandTellﬂ(
SAT
)[
34
]asim-
agecaptioningmodelstotrainandevaluateourmetric.
ImplementationDetails.
Ourimagefeaturesareex-
tractedfromaDeepResidualNetworkwith152layers
(ResNet-152)[
13
]pre-trainedonImageNet.Wefollowthe
preprocessingfrom[
33
,
34
,
18
]topreparevocabularyon
COCOdataset.WethestepsizeoftheLSTMtobe
15
,
paddingshortersentenceswithaspecialtokenwhilecutting
longeronesto15words.Allwordsarerepresentedas300-
dimensionalvectorsinitializedfromGloVe[
28
].Weusea
batchsizeof100andsampleanequalnumberofpositive
andnegativeexamplesineachbatch.Linearprojectionis
usedtoreducethedimensionofimagefeaturetomatchthat
ofcaptionfeatures.ForCompactBilinearPooling,weuse
thefeaturedimensionof8192assuggestedin[
11
].Weuse
1LSTMlayerwithahiddendimensionof512inallexper-
imentsunlessotherwisestated.Allthemodelaretrained
usingAdam[
19
]optimizerfor
30
epochswithaninitial
learningrateof
10

3
.Wedecaythelearningratebyafac-
torof
0
:
9
aftereveryepoch.Ourcode(inTw[
2
])is
availableat:
https://github.com/richardaecn/
cvpr18-caption-eval
.
1
Among15participatingteams,3didn'tprovidesubmissionsonvali-
dationset.Thus,weusesubmissionentriesfromtheremaining12teams.
(a)scoresforhumancaptions
(b)scoresforgeneratedcaptionsby
ST
,
SAT
and
NT
Figure5.Top:Averagescoreofhumancaptionsfromthevali-
dationset.Bottom:Averagescoreofgeneratedcaptions.Color
ofthebarindicateswhatcontextinformationisusedforthecri-
tique.Thehorizontalaxisrepresentsthreedifferentstrategiesfor
thecombinationofthefeaturesfromcandidatecaptionwiththe
contextaswellastheused(Concat+Linear:concate-
nationandlinearBilinear+Linear:compactbilinear
poolingandlinearConcat+MLP:concatenationand
MLPwithonehiddenlayerofsize512).
4.2.Capability
Tomeasurethecapabilityofourmetrictodifferentiate
betweenhumanandgeneratedcaptions,wetrainvariantsof
modelsusinggeneratedcaptionsfrom
ST
,
SAT
and
NT
,to-
getherwithhumancaptionsfromthetrainingset.Fig.
5(a)
andFig.
5(b)
showtheaveragescoreonthevalidationset
forhumanandgeneratedcaptionsrespectively.Theresults
showthatallmodelsgivemuchhigherscorestohumancap-
tionsthanmachinegeneratedcaptions,indicatingthatthey
areabletodifferentiatehuman-writtencaptionsfromthe
machine-generatedones.
Withrespecttothechoiceofcontext,weobservethat
includingimagefeaturesintothecontextclearlyimproves
performance.Also,addingareferencecaptiondoesnotlead
toaimprovementoveronlyusingimagefea-
tures.Thisindicatesthattheimageitselfprovidesenough
contextualinformationforthecritiquetosuccessfullydis-
criminatebetweenhumanandmachinegeneratedcaptions.
Thereasonthatnoneofthecommonlyusedmetricsin-
cludesimagesascontextislikelyduetothedif
ofcapturingimage-textsimilarity.Ourmetriccircumvents
thisissuebyimplicitlylearningtheimage-textrelationship
directlyfromthedata.
Itisworthnotingthatachievinghighmodelperformance
intermsofdiscriminationbetweenhumanandgenerated
captionsdoesnotnecessarilyimplythatthelearnedmetric
isgood.Infact,weobservethatacritiquetrainedwith-
outdataaugmentationcanachieveevenhigherdiscrimina-
tionperformance.Suchcritique,however,alsogiveshigh
scorestohumanwrittencaptionsfromotherimages,indi-
catingthattheproblemisessentiallyreduced
toputtingcaptionsintocategoriesofhumanandnon-human
writtenwithoutconsideringthecontextimage.Iftrained
withtheproposeddatasamplingandaugmentationtech-
nique,thecritiquelearnstopayattentiontoimagecontext.
4.3.Robustness
Toevaluatewhethertheproposedmetriccancapture
pathologicalimage-captionpairs,weconductrobustness
studiesasdescribedinSec.
3.4
onthethreepathological
transformationsinSec.
3.3
.Therobustnesscom-
parisonsareillustratedinFig.
6
.Intherowwecom-
paredifferentvariantsoftheproposedmetric.Theresults
illustratethat,althoughachievinghighdiscriminationper-
formance,ametriclearnedwithoutdatasamplingoraug-
mentationalsogiveshighscorestohumancaptionsfrom
otherimages(
T
RC
),withrandomwords(
T
RW
),orword
permutations(
T
WP
).Thisindicatesthatthemodeltends
tofocusonanoverallhumanvs.non-human
withoutconsideringcontextualinformationintheimageor
thesyntacticstructureofthecandidatesentence.
Further,evenwithdataaugmentation,alinearmodel
withconcatenatedcontextandcandidatecaptionfeatures
giveshighscorestohumancaptionsfromotherimages,pos-
siblybecausethereisnosufinteractionbetweenthe
contextandcandidatecaptionfeatures.Non-linearinter-
actionssuchasCompactBilinearPoolingoranon-linear
withhiddenlayerssolvethislimitation.Thenon-
linearmodelinFig.
6
referstoamodelwithconcatenated
contextandcandidatefeaturesfollowedbyanonlinearclas-
.Compactbilinearpooling(notshowninthefor
clarityofvisualization)achievessimilarresults.
InthesecondrowofFig.
6
wecompareourmetricwith
othercommonlyusedimagecaptioningmetrics.Thepro-
posedmetricoutperformsallotherswithrespecttorandom
(
T
RW
)aswellaspermuted(
T
WP
)wordsandisreasonably
robusttohumancaptionsfromsimilarimages(
T
RC
).Fur-
ther,weobservethattherecentlyproposedmetricsCIDEr
andSPICEperformwellforhumancaptionsfromsimilar
images,butfallbehindwithrespecttosentencestructure.
Thiscouldbecausedbytheirincreasedfocusoninforma-
tiveandscenewords.
Figure6.Normalizedevaluationscorefortransformations
T
RC
(humancaptionfromotherimages),
T
WP
(randompermutationofwords),
and
T
RW
(wordsreplacedbyrandomwords)withdifferentamountoftransformation(

%
).When

%=0%
,theoriginaldatasetiskept
unchanged;when

%=100%
,maximumamountoftransformationisappliedtothedataset.Therowshowsresultsofourmetrics
usingeitherlinearornon-linearmodeltrainedwithorwithoutdataaugmentation.Thesecondrowcomparesournon-linearmodeltrained
withdataaugmentationtoothermetrics.ThescoreaftereachmetricshowstherobustnessscoreinSec.
3.4
,
i.e
.,theAreaUnder
Curve(AUC).Thelowerthescorethemorerobustthemetricis.
4.4.CaptionLevelHumanCorrelation
WeuseboththeExpertAnnotationsandtheCrowd
FlowerAnnotationsfromFlickr8kdataset[
15
]tocompute
captionlevelcorrelationwithhumanjudgments.Wefollow
theprocedureinSPICEpaper[
3
]tocomputetheKendall's
˝
rankcorrelationintheExpertAnnotations.The
˝
correla-
tionfortheCrowdFlowerAnnotationiscomputedbetween
scoresgeneratedbytheevaluationmetricandpercentageof
raterswhothinkthatthecaptiondescribestheimagewith
possiblyminormistakes.Duringtraining,allnegativesam-
plesaregeneratedbytransformation
T
RC
,
i.e
.,humancap-
tionfromrandomimage.
TheresultsinTable
1
showthatourmetricsachieve
thebestcaptionlevelcorrelationinbothExpertAnnota-
tionsandCrowdFlowerAnnotations.NotethattheCrowd
FlowerAnnotationsuseabinaryratingsetup,whiletheset-
upfromtheExpertAnnotationsmakesa-grainedrat-
ings.Despitethefactthatourmodelistrainedonasimpler
binaryobjective,itstillcorrelateswellwithhumanjudg-
mentsfromtheExpertAnnotations.Notethatwedonot
useanyhumanannotationsduringthetraining,sinceallof
ournegativeexamplescouldbegeneratedautomatically.
4.5.SystemLevelHumanCorrelation
WecompareourmetricwithothersonthePearson's
ˆ
correlationbetweenallcommonmetricsandhuman
judgmentscollectedinthe2015COCOCaptioningChal-
lenge[
1
].Inparticular,weusetwohumanjudgmentM1:
Percentageofcaptionsthatareevaluatedasbetterorequal
tohumancaptionandM2:Percentageofcaptionsthatpass
theTuringTest.Wedon'tuseM3:correctness,M4:detail-
nessandM5:salience,astheyarenotusedtorankimage
captioningmodels,butareintendedforanablationstudyto
understandwhichaspectsmakecaptionsgood.
Sincewedon'thaveaccesstotheCOCOtestsetannota-
tions,wherethehumanjudgmentsarecollectedon,weper-
formourexperimentsontheCOCOvalidationset.There
are15teamsparticipatedinthe2015COCOcaptioning
challengeandweuse12ofthemthatsubmittedresultson
thevalidationset.Weassumethehumanjudgmentonthe
validationsetissufsimilartothejudgmentonthe
testset.Wedon'tuseanyadditionaltrainingdatabesides
thesubmissiononthevalidationsetandthedataaug-
mentationdescribedinSec.
3.3
.Togetevaluationscoreson
thewholevalidationset,wesplitthesetintwohalvesand,
ExpertAnnotations
CrowdFlower
BLEU-1
0.191*
0.206
BLEU-2
0.212
0.212
BLEU-3
0.209
0.204
BLEU-4
0.206*
0.202
METEOR
0.308*
0.242
ROUGE-L
0.218*
0.217
CIDEr
0.289*
0.264
SPICE
0.456
0.252
Ours
0.466
0.295
Inter-human
0.736
-
ExpertAnnotations
:expertsscoreimage-captionpairs
from1to4;1meanscaptiondoesn'tdescribetheimage.
CrowdFlower
:humanratersmark1ifthecandidate
captiondescribestheimage,andmark0ifnot.
Table1.CaptionlevelKendall's
˝
correlationbetweenFlickr8K
[
15
]'shumanannotationsandevaluationmetrics'scores.Ourre-
portedscoreswith*differfromtheonesreportedinSPICE[
3
].
M1
M2
ˆp
-value
ˆp
-value
BLEU-1
0.124(0.687)
0.135(0.660)
BLEU-2
0.037(0.903)
0.048(0.877)
BLEU-3
0.004(0.990)
0.016(0.959)
BLEU-4
-0.019(0.951)
-0.005(0.987)
METEOR
0.606(0.028)
0.594(0.032)
ROUGE-L
0.090(0.769)
0.096(0.754)
CIDEr
0.438(0.134)
0.440(0.133)
SPICE
0.759(0.003)
0.750(0.003)
Ours(noDA)
0.821
(0.000)
0.807
(0.000)
Ours
0.939
(0.000)
0.949
(0.000)
M1
:Percentageofcaptionsthatareevaluatedasbetter
orequaltohumancaption.
M2
:PercentageofcaptionsthatpasstheTuringTest.
Table2.Pearson's
ˆ
correlationbetweenhumanjudgmentsand
evaluationmetrics.Thehumancorrelationofourproposedmetric
surpassesallothermetricsbylargemargins.Scoresreportedin
SPICE[
3
]werecalculatedontheCOCOtestsetforall15teams,
whereasourswerefrom12teamsontheCOCOvalidationset.
foreachsubmission,trainourcritiqueoneachsplitandget
scores(probabilityofbeinghumanwritten)ontheother.
TheresultsinTable
2
showthatourlearnedmetricsur-
passesallothermetricsincludingtherecentlyproposed
SPICE[
3
]bylargemargins,especiallytrainedwithdata
augmentation.Thisindicatesthataligningtheobjective
withhumanjudgmentsandusingdataaugmentationyield
abetterevaluationmetric.Fig.
7
illustratesourmetriccom-
paredwithhumanjudgment-M1onCOCOvalidationset.
Ourmetricalignswellwithhumanjudgment,especiallyfor
topperformingmethods.
Figure7.Ourmetric
vs
.humanjudgmentonCOCOvalidation
set.Ourmetricisabletomostoftherankingsfromhuman
judgmentcorrectly,especiallyfortopperformingmethods.
5.ConclusionandFutureWork
Inthispaper,wehaveproposedanovellearningbased
evaluationmetricforimagecaptioningthatistrainedtoact
likeahumancritiquetodistinguishbetweenhuman-written
andmachine-generatedcaptionswhilealsobeingxible
toadapttotargetedpathologicalcases.Further,wehave
shownhowtousedatasamplingandaugmentationtosuc-
cessfullytrainametricthatbehavesrobustlyagainstcap-
tionsgeneratedfrompathologicaltransformations.From
extensiveexperimentalevaluations,wehavedemonstrated
thattheproposedmetricisrobustandcorrelatesbetterto
humanjudgmentsthanpreviousmetrics.Inconclusion,the
proposedmetriccouldbeaneffectivecomplementarytothe
existingrule-basedmetrics,especiallywhenthepathologi-
calcasesareeasytogeneratebutdiftocapturewith
traditionalhand-craftedmetrics.
Inthisstudy,wehavenottakendifferentpersonalities
amonghumanannotatorsintoconsideration.Differenthu-
manpersonalitiescouldgiverisetodifferenttypesofhu-
mancaptions.Onedirectionoffutureworkcouldaimto
capturetheheterogeneousnatureofhumanannotatedcap-
tionsandincorporatesuchinformationintocaptioningeval-
uation.Anotherdirectionforfutureworkcouldbetraining
acaptiongeneratortogetherwiththeproposedevaluation
metric(discriminator)inagenerativeadversarialsetting.
Finally,gameabilityisaconcern,notonlyforour
learningbasedmetric,butalsoforotherrule-basedmetrics.
Learningtobemorerobusttoadversarialexamplesisalso
afuturedirectionoflearningbasedevaluationmetrics.
Acknowledgments.
Thisworkwassupportedinpartbya
GoogleFocusedResearchAward,AWSCloudCreditsfor
ResearchandaFacebookequipmentdonation.Wewould
liketothanktheCOCOConsortiumforagreeingtorunour
codeonentriesinthe2015COCOCaptioningChallenge.
References
[1]
Thecoco2015captioningchallenge.
http://mscoco.
org/dataset/#captions-challenge2015
.
2
,
7
,
11
[2]
M.Abadi,P.Barham,J.Chen,Z.Chen,A.Davis,J.Dean,
M.Devin,S.Ghemawat,G.Irving,M.Isard,etal.Tensor-
w:Asystemforlarge-scalemachinelearning.In
OSDI
,
2016.
5
[3]
P.Anderson,B.Fernando,M.Johnson,andS.Gould.Spice:
Semanticpropositionalimagecaptionevaluation.In
ECCV
,
2016.
1
,
2
,
7
,
8
[4]
S.R.Bowman,L.Vilnis,O.Vinyals,A.M.Dai,R.Jozefow-
icz,andS.Bengio.Generatingsentencesfromacontinuous
space.In
CoNLL
,2016.
2
[5]
M.Charikar,K.Chen,andM.Farach-Colton.Findingfre-
quentitemsindatastreams.In
ICALP
,2002.
3
[6]
T.-H.Chen,Y.-H.Liao,C.-Y.Chuang,W.-T.Hsu,J.Fu,and
M.Sun.Show,adaptandtell:Adversarialtrainingofcross-
domainimagecaptioner.In
ICCV
,2017.
2
[7]
X.Chen,H.Fang,T.-Y.Lin,R.Vedantam,S.Gupta,
P.Doll
´
ar,andC.L.Zitnick.Microsoftcococaptions:Data
collectionandevaluationserver.
arXiv
,2015.
2
[8]
B.Dai,S.Fidler,R.Urtasun,andD.Lin.Towardsdiverse
andnaturalimagedescriptionsviaaconditionalgan.In
ICCV
,2017.
2
[9]
J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-
Fei.Imagenet:Alarge-scalehierarchicalimagedatabase.In
CVPR
,2009.
3
[10]
A.Fukui,D.H.Park,D.Yang,A.Rohrbach,T.Darrell,and
M.Rohrbach.Multimodalcompactbilinearpoolingforvi-
sualquestionansweringandvisualgrounding.In
EMNLP
,
2016.
3
[11]
Y.Gao,O.Beijbom,N.Zhang,andT.Darrell.Compact
bilinearpooling.In
CVPR
,2016.
3
,
5
[12]
I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,
D.Warde-Farley,S.Ozair,A.Courville,andY.Bengio.Gen-
erativeadversarialnets.In
NIPS
,2014.
2
[13]
K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearning
forimagerecognition.In
CVPR
,2016.
3
,
5
,
10
[14]
S.HochreiterandJ.Schmidhuber.Longshort-termmemory.
Neuralcomputation
,1997.
3
[15]
M.Hodosh,P.Young,andJ.Hockenmaier.Framingimage
descriptionasarankingtask:Data,modelsandevaluation
metrics.
J.Artif.Int.Res.
,47(1):853Œ899,May2013.
5
,
7
,
8
[16]
D.J.Im,C.D.Kim,H.Jiang,andR.Memisevic.Generating
imageswithrecurrentadversarialnetworks.
arXiv
,2016.
2
[17]
A.KannanandO.Vinyals.Adversarialevaluationofdia-
loguemodels.
arXiv
,2017.
2
[18]
A.KarpathyandL.Fei-Fei.Deepvisual-semanticalign-
mentsforgeneratingimagedescriptions.In
CVPR
,2015.
4
,
5
[19]
D.KingmaandJ.Ba.Adam:Amethodforstochasticopti-
mization.
ICLR
,2015.
5
[20]
M.D.A.Lavie.Meteoruniversal:Languagetrans-
lationevaluationforanytargetlanguage.In
ACL
,2014.
1
,
2
[21]
J.Li,W.Monroe,T.Shi,A.Ritter,andD.Jurafsky.Adver-
sariallearningforneuraldialoguegeneration.In
EMNLP
,
2017.
2
[22]
X.Liang,Z.Hu,H.Zhang,C.Gan,andE.P.Xing.Recurrent
topic-transitionganforvisualparagraphgeneration.2017.
2
[23]
C.-Y.Lin.Rouge:Apackageforautomaticevaluationof
summaries.In
ACLWorkshop
,2004.
1
,
2
[24]
T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra-
manan,P.Doll
´
ar,andC.L.Zitnick.Microsoftcoco:Com-
monobjectsincontext.In
ECCV
,2014.
5
[25]
S.Liu,Z.Zhu,N.Ye,S.Guadarrama,andK.Murphy.Im-
provedimagecaptioningviapolicygradientoptimizationof
spider.In
ICCV
,2017.
1
,
2
[26]
R.Lowe,M.Noseworthy,I.Serban,N.Angelard-Gontier,
Y.Bengio,andJ.Pineau.Towardsanautomaticturingtest:
Learningtoevaluatedialogueresponses.In
ACL
,2017.
2
[27]
K.Papineni,S.Roukos,T.Ward,andW.-J.Zhu.Bleu:a
methodforautomaticevaluationofmachinetranslation.In
ACL
,2002.
1
,
2
[28]
J.Pennington,R.Socher,andC.D.Manning.Glove:Global
vectorsforwordrepresentation.In
EMNLP
,2014.
3
,
5
,
10
[29]
N.PhamandR.Pagh.Fastandscalablepolynomialkernels
viaexplicitfeaturemaps.In
KDD
,2013.
3
[30]
O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,
S.Ma,Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,
etal.Imagenetlargescalevisualrecognitionchallenge.
IJCV
,2015.
10
[31]
R.Shetty,M.Rohrbach,L.A.Hendricks,M.Fritz,and
B.Schiele.Speakingthesamelanguage:Matchingmachine
tohumancaptionsbyadversarialtraining.In
ICCV
,2017.
2
[32]
R.Vedantam,C.LawrenceZitnick,andD.Parikh.Cider:
Consensus-basedimagedescriptionevaluation.In
CVPR
,
2015.
1
,
2
[33]
O.Vinyals,A.Toshev,S.Bengio,andD.Erhan.Showand
tell:Aneuralimagecaptiongenerator.In
CVPR
,2015.
4
,
5
[34]
K.Xu,J.Ba,R.Kiros,K.Cho,A.C.Courville,R.Salakhut-
dinov,R.S.Zemel,andY.Bengio.Show,attendandtell:
Neuralimagecaptiongenerationwithvisualattention.In
ICML
,2015.
4
,
5
Appendices
A.ImplementationDetails
A.1.ImageRepresentations
Toextractimagefeatures,weusea152-layerResidual
Network(ResNet-152)[
13
]pretrainedonImageNet,which
achievedstate-of-the-artperformanceonthelarge-scaleim-
agetask[
30
].Insteadofthestandardfea-
tureextractionprocedureofextractingfeaturesfromare-
sizedandcropped
224

224
image,weextractthefeatures
fromtheoriginalimagewithoutanyresizingandcropping.
Thefeaturemapfromthelastconvolutionlayerisaverage-
pooled,resultingina
2048
-dimensionalfeaturevectoras
ourimagefeaturerepresentation.Theimagefeaturesare
remainedduringtraining.
A.2.CaptionRepresentations
Weconstructavocabularylistbytakingthe10,000most
frequentwordsthatappearatleast5timesinthehuman
annotatedcaptionsfromthetrainingset.Aspecialtokenis
addedtothevocabularytorepresentanywordthatisnot
amongthetop10,000words.Supposethelengthofthe
vocabularylistis
n
.Eachwordinthevocabularycanbe
representedbyaone-hotvector
w
2f
0
;
1
g
n
,wherefor
word
i
,
w
i
=1
andforall
j
6
=
i
,
w
j
=0
.Then,aword
embeddingmatrix
E
2
R
n

d
isusedtoencodeeachword
asa
d
-dimensionalvector
x
=
w
E
2
R
d
astheinputtothe
LSTM.ThewordembeddingisinitializedfromGloVe[
28
].
Weuseawordembeddingdimensionof
d
=300
forallof
ourexperiments.WethestepsizeoftheLSTMtobe15.
Thatis,shortersentencesarepaddedwithaspecialtoken
andlongercaptionsarecutat15words.Duringtraining,
amaskisappliedtoremovethepaddedpartofacaption
whenwecomputetheloss.
A.3.Training
Duringtraining,wesampleequalnumberofpositiveand
negativeexamples.Togeneratepositiveexamples,we
randomlychooseanimagefromthedatabase,andsuchim-
ageshouldcorrespondtoseveralreferencecaptions.We
useonereferencecaptionasthecontext,andadifferentone
asthecandidatecaption.Tocomposeanegativeexample,
wechoosewithequalprobabilityoneofthefollowing
typesofnegativeexamples:1)usingacaptiongenerator;2)
sampleacaptionfromapathologicallytransformeddataset;
or3)generateacaptionusingMonteCarloSampling.If
weareusingapathologicallytransformeddataset,wewill
chooseinequalprobabilityamongthreetransformations:
T
RC
(humancaptionforadifferentimage),
T
WP
(refer-
encecaptionwithwordpermutation),and
T
RW
(reference
captionwithrandomwordreplacement).
A.4.Evaluation
Toevaluatehowgoodacandidatecaptionis,weiterate
throughallthereferencecaptionsfortheimageandcom-
puteascoreusingeachreferencecaptionascontextforthe
candidatecaption.Theaverageofthesescoresisthe
scoreforthecandidatecaption.
Toevaluateacaptiongenerator,wetrainourmodelfor
10epochsusingonlythisgeneratortoproducethetype
ofnegativeexamples.Weusepathologicaltransformation
andMonteCarloSamplingforallmodelevaluation.Fi-
nally,weuseourmodeltoscoreallcandidatecaptionsthis
generatorproducesonaheld-outsetofdata.Theaverage
ofthesescoreisusedastheindicatorforhowgoodthe
captiongeneratoris.
Whilecomputingthecaptionlevelcorrelationwithhu-
man,weuseacandidatemetrictocomputeascore
foreachpairofimageandcandidatecaption
(
i;c
)
,where
i
indicatestheimageand
c
indicatesthecandidatecaption.
Supposea
(
i;c
)
pairhascorrespondinghumanannotations
A
i;c
andourcomputedscores
S
i;c
,wecreateallpairsbe-
tweenhumanannotationsandcomputedscores
[(
h;s
)
j
h
2
A
i;c
;s
2S
i;c
]
.Finally,wecomputetheKendalls
˝
Rank
Correlationforallscorepairswecouldgenerate,
i.e
.,
˝
([(
h;s
)
j
h
2A
i;c
;s
2S
i;c
;
8
(
i;c
)])
(12)
B.TheChoiceofHyper-parameters
Fig.
8
comparescapabilityperformanceofmodelswith
differentLSTMlayersandhiddenfeaturesizes.Thepro-
posedmodelisrobustwithrespecttovariantLSTMparam-
eters.Usingmodelswithhighercapacity,
i.e
.,morelayers,
higherdimensionalhiddenfeatures,havenoobvious
intermsofcapabilityperformance.Consideringthetrade-
offbetweenperformancegainandefy,wetherefore
use1LSTMlayerandmakethehiddenfeatureoftheLSTM
tobe512dimensionalinourpaper.
Fig.
9
showsmodelstrainedwithoutdataaugmentation.
Modelstrainedwithorwithoutdataaugmentationareca-
pableoflearningtogivehigherscorestohumancaptions
thanmachinegeneratedcaptions.Interestingly,acritique
trainedwithoutdataaugmentationcanachieveevenhigher
discriminationperformancethanmodelswithdataaugmen-
tation.However,asshowninSec.4.3andFig.6inthe
paper,modelstrainedwithoutdataaugmentationareactu-
allylearningtoperformamuchsimplertask,focusingonly
ondiscriminatinghumangeneratedcaptionsfromthema-
chinegeneratedoneswithoutconsideringthecontext(
i.e
.,
imageandgroundtruthcaptions).Therefore,modelsthat
merelyperformwellindiscriminationtaskmightbeeasily
gamedwithpathologicaltransformations.Trainingwithap-
propriatedataaugmentationandarchitecture(non-linearity)
isessentialtoforcecritiquestopayattentiontocontexts.
(a)modelswithdifferentLSTMlayers
(b)modelswithdifferentLSTMhiddenfeaturesize
Figure8.Top:modelswithvariantLSTMlayers(512hidden
size).Bottom:modelswithvariantLSTMhiddenfeaturesize(1
layer).Allthemodelsaretrainedwithbothimageandreference
groundtruthcaptionsascontexts,usingconcatenationofcontext
informationandcandidatecaptionfollowedbyalinear
andwithdataaugmentation.
C.CaptionEvaluationExamples
Figure
10
providesexamplescaptionsofbothsuccess
andfailurecases.Exampleswhereourmetricperforms
betterthanSPICEaremarkedwithgreenboundingboxes,
whileexampleswhereourmetricisworsearemarkedwith
redones.Byutilizingtheimageascontext,ourmetricis
abletorecognizesomecaptionsthatarereferringtowrong
objects(left),andgivehighscorestocaptionsthatarese-
manticallyrelevanttotheimage(center).Typicalfailure
casesofourmetricareduetomisleadingvisualinforma-
tion(right).
D.SystemLevelHumanCorrelationonCOCO
Intheoriginalpaper,wedidn'tcomparetometricsM3,
M4andM5becausetheywerenotusedtorankimagecap-
tioningmodels,butwereintendedforanablationstudyto
understandwhichaspectsmakecaptionsgood[
1
].Since
ourmetricwasdesignedtoevaluatetheoverallqualityof
animagecaption,weonlycomparedM1andM2.Forbet-
terunderstandingofourmetricfromdifferentperspectives,
inTable
3
,wecalculatethePearson's
ˆ
correlationbetween
humanjudgementsonall5metrics(M1-M5)usedin2015
COCOCaptioningChallenge[
1
].
(a)scoresforhumancaptions
(b)scoresforgeneratedcaptionsby
ST
,
SAT
and
NT
Figure9.ThisissameasFig.5inthepaperexceptall
modelsaretrainedwithoutdataaugmentation.
Fromtheresults,wecanseethatthehumancorrelation
ofourproposedevaluationmetricssurpassesallothermet-
ricsbylargemarginsonM1,M2,M4andM5.OnM3,our
metricachievescomparablecorrelationscoreswithother
commonly-usedmetrics.Itisworthnoticingthatallother
metricsfailtocapturethehumancorrelationonthedetail-
nessofcaptions(M4),whereasourmetriccorrelatesrea-
sonablywellwithhumansonM4.
E.HowtoUsetheProposedMetricinPractice
Wesuggestthechallengeorganizertoboththe
modelarchitectureofthediscriminatorandallthehyper-
parametersofthetrainingprocess,thensplitthetestset
intotwofolds.Thisincludethenumberoftraining
iterationsforeachsubmission.Foreachsubmission,use
thesamesettingtotrainthediscriminatorononefold,and
thenusethetrainedmetrictoevaluatetheotherfold.Vise
versafortheothersplit.Afterthat,wegetevaluationresults
onthefulltestset.Duringtraining,themachinegenerated
captionscomefromonlythetargetedsubmission,sothat
submissionfromoneparticipantswon'taffectthescoreof
theotherparticipants.
Figure10.ExemplarcandidatecaptionsandtheirevaluationscoresusingourmetricandSPICEontheCOCOvalidationset.
M1
M2
M3
M4
M5
ˆp
-value
ˆp
-value
ˆp
-value
ˆp
-value
ˆp
-value
BLEU-1
0.124(0.687)
0.135(0.660)
0.549(0.052)
-0.517(0.070)
0.241(0.428)
BLEU-2
0.037(0.903)
0.048(0.877)
0.483(0.094)
-0.572(0.041)
0.162(0.598)
BLEU-3
0.004(0.990)
0.016(0.959)
0.471(0.105)
-0.588(0.035)
0.143(0.641)
BLEU-4
-0.019(0.951)
-0.005(0.987)
0.459(0.114)
-0.577(0.039)
0.139(0.650)
METEOR
0.606(0.028)
0.594(0.032)
0.808(0.001)
0.085(0.784)
0.685(0.010)
ROUGE-L
0.090(0.769)
0.096(0.754)
0.529(0.063)
-0.526(0.065)
0.208(0.494)
CIDEr
0.438(0.134)
0.440(0.133)
0.763(0.002)
-0.149(0.628)
0.559(0.047)
SPICE
0.759(0.003)
0.750(0.003)
0.871
(0.000)
0.250(0.411)
0.809(0.001)
Ours(noDA)
0.821
(0.000)
0.807
(0.000)
0.430(0.143)
0.844
(0.000)
0.704(0.007)
Ours
0.939
(0.000)
0.949
(0.000)
0.720(0.006)
0.626
(0.026)
0.867
(0.000)
M1
:Percentageofcaptionsthatareevaluatedasbetterorequaltohumancaption.
M2
:PercentageofcaptionsthatpasstheTuringTest.
M3(Correctness)
:Averagecorrectnessofthecaptionsonascale1-5(incorrect-correct).
M4(Detailness)
:Averageamountofdetailofthecaptionsonascale1-5(lackofdetails-verydetailed).
M5(Salience)
:Percentageofcaptionsthataresimilartohumandescription.
Table3.Pearson's
ˆ
correlationbetweenhumanjudgementsandevaluationmetrics.Weusethe12availableentriestothe2015MS-COCO
captioningchallengethatsubmittedresultsonthevalidationset.ﬁOurs(noDA)ﬂmeansourmetrictrainedwithoutdataaugmentation.
"
26,One-to-one Mapping between Stimulus and Neural State: Memory and Classification,http://arxiv.org/pdf/1805.09001v6.pdf,https://github.com/lansiz/neuron,"One-to-onemappingbetweenstimulusandneuralstate:
Memoryandclassi˝cation
SizhongLan
1
1
ChinaMobileResearchInstitute,Beijing,100053,China
a)
(Dated:25April2019)
Synapticstrengthcanbeseenasprobabilitytopropagateimpulse,andaccordingtosynapticplasticity,functioncould
existfrompropagationactivitytosynapticstrength.Ifthefunctionconstraintssuchascontinuityandmono-
tonicity,theneuralnetworkunderexternalstimuluswillalwaysgotoedpoint,andtherecouldbeone-to-onemap-
pingbetweentheexternalstimulusandthesynapticstrengthatedpoint.Inotherwords,neuralnetwork""memorizes""
externalstimulusinitssynapses.Abiologicalisproposedtoutilizethismapping.
I.INTRODUCTION
Knownexperimentresultsshowthatsynapticconnection
strengthensorweakensovertimeinresponsetoincreasesor
decreasesinimpulsepropagation
1
.Itisalsopostulatedthat
""neuronsthattogetherwiretogether""
2,3
.Thisbiochem-
icalmechanism,calledsynapticplasticity
4,5
,isbelievedto
playacriticalroleinthememoryformation
6Œ9
,althoughit
isstillarguedifsynapseisthesolelocusoflearningand
memory
10,11
.Meanwhile,asynapsepropagatesimpulses
stochastically
12Œ14
,whichmeansthatsynapticstrengthcould
bemeasuredwiththeprobabilityofpropagatinganimpulse
successfully.Withthisprobabilistictreatmentweout
that,intheplasticityprocessasynapse'sstrengthwouldbe
inevitablyattractedtowardsthesameedpointregardlessof
itsinitialstrength,andforaneuralnetworktherecouldex-
istaone-to-onemappingbetweentheexternalstimulusfrom
environmentandthesynapses'strengthatedpoint.This
one-to-onemappingservestheverypurposeofidealmem-
ory:todevelopdifferentstableneuralstatefordifferentstim-
ulusfromtheenvironment,anddevelopthesamestableneural
stateforthesamestimulusnomatterwhatstateisinitialized
with.Itfollowsthatthesynapsesalonecouldsufgive
risetopersistentmemory:theycouldthesolelocusoflearn-
ingandmemory.
Theremainderofpapergoesasfollows.SectionIIiden-
theconstraintsunderwhichsynapticplasticityofone
synapticconnectionleadstoitsedstateandone-to-one
stimulus-statemapping(memory).SectionIIIextendsthe
conceptsofedstateandone-to-onemappingfortheneu-
ralnetworkconsistingofmanysynapticconnections.Section
IVproposesasimpleneuralutilizingthismemoryto
classifyhandwrittendigitimages.
II.SYNAPTICCONNECTIONANDITSFIXEDPOINT
LetusstartwithonesynapticconnectionasshowninFIG1.
Innature,synapsesareknowntobeplastic,low-precisionand
unreliable
15
.Thisstochasticityallowsustoassumesynap-
ticstrength
s
tobetheprobability(reliability)ofpropagat-
a)
Electronicmail:lsz@nzqrc.cn
FIG.1.Asynapticconnectionwithstrength
s
isdirectedfromneuron
1toneuron2.Thestimulusfromenvironmentorupstreamneurons
stimulatesneuron1toactionpotentialwithprobability
x
.The
synapticconnectionpropagatesnerveimpulse(actionpotential)to
neuron2.Asaresult,neuron2isstimulatedtowithprobability
y
.Thatis,neuron1and2simultaneouslytogether"")with
probability
y
.
inganerveimpulsethrough,insteadofbeingweight(usu-
allyunboundedrealnumber)asinNeuralNetwork
16
(ANN).Easilywehave
y
=
xs
where
x
;
s
;
y
2
[
0
;
1
]
.Nowwe
treatsynapticplasticity,i.e.therelationbetweensynaptic
strength
s
andsimultaneousprobability
y
,asafunction
s

=
l
(
y
)
:
(1)
Here
s

2
[
0
;
1
]
representsthetargetvaluethataconnection's
strengthwillbestrengthenedorweakenedtoiftheconnec-
tionisunderconstantsimultaneousprobability
y
(while
s
in
y
=
xs
representscurrentstrength).By
y
=
xs
andEq.(1),
wehave
s

=
l
(
xs
)
statingthat,underconstantstimulusprob-
ability
x
,theconnectioninitializedwithstrength
s
willevolve
towards
s

.
Function
l
ofEq.(1)trulylinkstogether""and
""wiringtogether"".Forcomparison,Hebbianlearningrule
2
treatssynapticplasticity,inthecontextofANN,asafunction
D
w
=
h
xy
tolearnconnections'weightfromthetrainingpat-
terns;thefunctiontranslatestogether""into""neuron's
inputandoutputbothbeingpositiveornegative"".Different
fromANN,ourmodelactuallymakesnoassumptionofneu-
ronbeingcomputationalunit,andaimstoshowthatwith
l
stimuluscouldsufandpreciselycontroltheenduring
edstateofsynapticconnection.Thefollowingreasoning
willhingeonthis""targetstrengthfunction""
l
,andwewill
putconstrainsonthisunchartedfunctiontoseehowtheyaf-
fectthedynamicsofconnectionstrengthandmostimportantly
howstimulusisone-to-onemappedtothestrengthat
state.
arXiv:1805.09001v6  [q-bio.NC]  24 Apr 20192
FIG.2.Twoexamplesof
l
(
xs
)
aredepictedasredboldlines,and
theiredpointsasbluedots.
(a)
Givenanyinitial
s
1
<
l
(
xs
1
)
,there
mustexistaedpoint
s
+
2
(
s
1
;
1
]
;strength
s
tendstoincreasefrom
s
1
aslongastargetstrength
l
(
xs
)
>
s
.Givenanyinitial
s
2
>
l
(
xs
2
)
,
theremustexistaedpoint
s
+
2
[
0
;
s
2
)
;strength
s
tendstode-
creasefrom
s
2
aslongastargetstrength
l
(
xs
)
<
s
.Controlledby
thesetwotendencies,
s
willreachandstayatedpoint
s
+
such
that
s
+
=
l
(
xs
+
)
.
(b)
Therearethreeedpoints
s
+
1
,
s
+
2
,
s
+
3
.Start-
ingfromanyinitial
s
1
2
(
s
+
1
;
s
+
3
)
,strengthdecreasesto
s
+
1
.Starting
fromanyinitial
s
2
2
(
s
+
3
;
s
+
2
)
,strengthincreasesto
s
+
2
.Thenstrength
tendstoleaveunstableedpoint
s
+
3
forstable
s
+
1
or
s
+
2
.Notethat
ifcountableedpointsexistfor
l
(
xs
)
,oneofthemmustbestable.
Algorithm1
connectionstrength'stendencytoedpoints
Input:
stimulusprobability
x
,initialsynapticstrength
s
0
,target
strengthfunction
l
,strengthstep
D
s
,andinterations
I
.
Output:
trajectoryofstrength
s
.
1:
initializerecorder(10
4
-entriesarray):
recorder
(
0.
2:
initializerecorderpointer:
p
 
0.
3:
initializecurrentstrength:
s
 
s
0
.
4:
for
i
=
0
to
I
do
5:
presetcurrentpointedentryofrecorder:
recorder
[
p
]
 
0.
6:
pickrandom
r
1and
r
2fromuniformdistribution
Unif
(
0
;
1
)
.
7:
if
x
>
r
1
and
s
>
r
2
then
8:
neuron1and2together:
recorder
[
p
]
 
1.
9:
endif
10:
if
recorderhasbeentraversedonce(
i

10
4
)
then
11:
set
y
withtheproportionof1-entriesinrecorder.
12:
settargetstrength:
s

 
l
(
y
)
.
13:
if
s

>
s
then
14:
step-increasecurrentstrength:
s
 
min
(
s
+
D
s
;
1
)
.
15:
endif
16:
if
s

<
s
then
17:
step-decreasecurrentstrength:
s
 
max
(
0
;
s

D
s
)
.
18:
endif
19:
endif
20:
forwardrecorderpointer:
p
 
(
p
+
1
)
mod10
4
.
21:
endfor
Hereisourconstraint:

iscontinuouson
y
.This
constraintisneurobiologicallyregardingsynaptic
plasticity,sincesufsmallchangeinimpulseprobabil-
itywouldmostprobablyresultinarbitrarilysmallchangein
synapticstrength.Inthatcase,givenany
x
,
l
(
xs
)
isacontinu-
ousfunctionon
s
fromunitinterval
[
0
;
1
]
tounitinterval
[
0
;
1
]
,
andaccordingtoBrouwer'sed-pointtheorem
17
theremust
existaedpoint
s
+
2
[
0
;
1
]
suchthat
s
+
=
l
(
xs
+
)
:connection
strengthat
s
+
willevolveto
s
+
andhencenolonger
FIG.3.ThesimulationresultsofAlgorithm1forfourtypical
l
func-
tions.Foreach
l
,eleventrailsparameteriedwithincrementalinitial
strength
s
0
arerunfor10
5
iterations;alltrailssharethesamestim-
ulusprobability
x
=
0
:
8.Ineach
l
'stheleftchartdepicts
l
(
y
)
asblackline,itshorizontallyscaled
l
(
xs
)=
l
(
0
:
8
s
)
inredline
andedpointsasbluedots;therightchartshowsthestrengthtra-
jectoriesstartingfromincremental
s
0
.
(a)
l
(
y
)=
0
:
9
y
+
0
:
05.There
existsonesingleedpointfor
l
(
0
:
8
s
)
.Allstrengthtrajectories
convergetothisedpoint.
(b)
l
(
y
)=
0
:
5
sin
(
4
p
y
)+
0
:
5.Thereare
threeedpointsfor
l
(
0
:
8
s
)
,twoofwhicharestableonesforthe
trajectoriestoconvergetowithoscillation.
(c)
l
(
y
)=

y
+
1.All
trajectoriesconvergetoonesingleedpoint.
(d)
l
(
y
)
isdiscontin-
uousat
y
=
0
:
5.Thereisnoedpointsincethereisno
s
2
[
0
;
1
]
such
that
l
(
0
:
8
s
)=
s
,andconsequentlythetrajectoriesdon'tconverge.In-
stead,theywithinedinterval"",whichaswewillseeisa
useablecompromiseofedpoint.
strengthenedorweakened.HerethecrucialBrouwer'stheo-
remisaed-pointtheoremintopology,whichstatesthat,for
anycontinuousfunction
f
(
t
)
mappingacompactconvexset
(e.g.interval
[
0
;
1
]
inourcase;couldbemulti-dimensional)to
itself,thereisalwaysapoint
t
+
suchthat
f
(
t
+
)=
t
+
.More-
over,asillustratedinFIG2,givenanyinitialvaluethestrength
isalwaysattractedtowardsedpoint.Therefore,agentle
constraintofcontinuityon
l
functioncouldpreferablydrive
synapticconnectiontotheedstate.
Toverifyconnectionstrength'stendencytowardsed
points,wedesignAlgorithm1tosimulateourconnection
model.Inthissimulation
18
,recentsimultaneousare
recordedandtherateissupposedtoapproximatethesimulta-
neousprobability
y
;theconnectionupdatesitsstrength
3
byasmallstep
D
s
=
10

4
eachiterationtothedirectionoftar-
getstrength.AsshowninFIG3,werunthesimulationfor
fourtypicaltargetstrengthfunctions,andthestrengthtrajec-
toriesresultedshowthattheconstraintofcontinuityensures
thetendencytowardsedpointsgivenanyinitialstrength.
Ourgoalistoestablishaone-to-onemappingbetweenthe
stimulusandtheconnectionstrengthatedpoint.Specif-
ically,wecould
(1)
givenanystimulus
x
2
[
0
;
1
]
,identifythe
edpoint
s
+
ofconnectionstrengthwithoutambiguity;
(2)
givenanystrength
s
+
2
[
0
;
1
]
atedpoint,identifystimulus
x
withoutambiguity.Amongthefourtargetstrengthfunc-
tionsinFIG3,
l
(
y
)=
0
:
9
y
+
0
:
05and
l
(
y
)=

y
+
1canlead
toone-to-onestimulus-strengthmapping.Givenanystimulus
x
,asynapticconnectionequippedwithoneofthesefunctions
willhaveonesingleedpointofstrengthregardlessofits
initialstrength,suchthattherelationbetweenstimulusand
edpointstrengthcanbetreatedasafunction
s
+
=
q
(
x
)
.In
FIG4,simulationshowsthat
q
couldbestrictlymonotonic
andhenceone-to-onemappingfrom
x
to
s
+
,suchthat
q
(
x
)
hasone-to-oneinversefunction
q

1
(
s
+
)
.Bycontrast,FIG5
showsthat
l
(
y
)=
0
:
5
sin
(
4
p
y
)+
0
:
5cannotensuretheunique-
nessofedpointandthusthereisnosuchone-to-one
q
(
x
)
;
FIG6showsthatthereisno
q
eitherforthediscontinuous
l
functioninFIG3(d).
FIG.4.ThesimulationresultsofAlgorithm1torevealtherelationof
stimulusprobability
x
andedpointstrength
s
+
.Foreach
l
,sim-
ulationisparameterizedwithincremental
x
(ratherthan
x
=
0
:
8asin
FiG3)andrandomized
s
0
.Tentrailsarerunforeachincremental
x
,
andthetenconverged
s
valuesareaveragedtobethe
s
+
valuecorre-
spondingtoitsinput
x
.Theredlinerepresentstheaveraged
s
+
values
fromsimulation,whilethebluelinerepresentsthetrue
s
+
=
q
(
x
)
.
(a)
Simulationisparameterizedwith
l
(
y
)=
0
:
9
y
+
0
:
05andtheresults
match
q
(
x
)=
0
:
05
=
(
1

0
:
9
x
)
whichismonotonicallyincreasing.
(b)
Simulationisparameterizedwith
l
(
y
)=

y
+
1andtheresultsmatch
q
(
x
)=
1
=
(
1
+
x
)
whichismonotonicallydecreasing.
Infact,wecanpinpointmoreconstraintson
l
asthecon-
ditionsforfunction
q
tobeone-to-onemapping.Inaddi-
tiontoconstraintofcontinuity,let

(
y
)
bestrictlymono-
tonicon
[0
;
1]
andhenceone-to-one;let

(0)
6
=
0
toruleout
edpoint
s
+
=
0.Inthatcase,
l
hasinversefunction
l

1
(
s
)
whichisstrictlymonotonicbetween
l
(
0
)
and
l
(
1
)
,andgiven
anyedpointstrength
s
+
betweenwecanidentifystimu-
lus
x
=
l

1
(
s
+
)
=
s
+
.Thatis,function
q

1
(
s
+
)=
l

1
(
s
+
)
=
s
+
exists.Let


1
(
s
)
=
s
bestrictlymonotonicbetween

(0)
and

(1)
.Thengivenanystimulus
x
2
[
0
;
1
]
thereisonesin-
gleedpoint
s
+
suchthat
x
=
l

1
(
s
+
)
=
s
+
.Thatis,function
s
+
=
q
(
x
)
exists.Bothof
l
(
y
)=
0
:
9
y
+
0
:
05and
l
(
y
)=

y
+
1
FIG.5.ThesimulationresultsofAlgorithm1for
l
(
y
)=
0
:
5
sin
(
4
p
y
)+
0
:
5inFIG3(b)toidentifytherelationbe-
tween
x
and
s
+
.
(a)
Given
x
=
0
:
3thereisonesingleedpoint
regardlessofinitialstrength
s
0
.
(b)
Aswith
x
=
0
:
8inFIG3(b),
given
x
=
0
:
9therearetwostableedpoints.Higher
s
0
convergesto
higheredpoint;lower
s
0
convergestolowerone;Noconvergence
tothemiddleunstableedpoint.
(c)
Trailswithincremental
x
arerunandtheaveraged
s
+
valuesaredepictedasinFIG4.From
x
=
0
:
65andupwards,therearetwopossiblestableedpointsto
convergetodependingonwhatvalueinitialstrengthisrandomized
to,whichmeansthatthereexistsno
q
functionfrom
x
to
s
+
.
FIG.6.Thesimulationresultstothe
q
functionwithrespectto
thediscontinuous
l
inFIG3(d).When
x
&
0
:
6,strengthcanevolveto
anypointwithinaedinterval""eachtimesimulationis
Theabsenceofedpointdoesn'tallowtheexistenceof
q
.
obeyallthoseconstraintsandtheirone-to-one
q
functions
canbevbythesimulationresultsinFIG4,whereas
l
(
y
)=
0
:
5
sin
(
4
p
y
)+
0
:
5isnotevenstrictlymonotonic.How-
ever,neither
l
(
y
)=
0
:
9
y
+
0
:
05nor
l
(
y
)=

y
+
1isidealfor
ourpurpose.Guidedbytheseconstraints,wechoose
l
func-
tioncarefullysuchthatitsderived
q
(
x
)
functionismonoton-
icallyincreasingandrangeofwhichspansnearlytheentire
[
0
;
1
]
interval,asshowninFIG7.Ofallthe
l
constraints,
continuityandstrongmonotonicityarereasonablerequire-
mentsofconsistencyontheneurobiologicalprocessofsynap-
ticplasticity,whereas
l
(
0
)
6
=
0andstrongmonotonicityof
l

1
(
s
)
=
s
areratherandpeculiarclaims.Admittedly,
those
l
constraintsneedtobesupportedbyexperimentalevi-
dences.
Nowwehavetheone-to-one(continuousandstrictlymono-
tonic)functions
l
,
l

1
,
q
and
q

1
,andinthosefunctionsini-
tialstrength
s
0
isirrelevant.Given
s
+
wecanidentify
x
and
y
withoutambiguity,andviceversa.Ourinterpretationofthese
mappingsis,thesynapticconnectionatedpointprecisely
""memorizes""theinformationofwhat(stimulus)itsensesand
4
FIG.7.
(a)
Ourchoicesof
l
functionsare
l
L
(
y
)=
0
:
99
p
y
+
0
:
01in
blueand
l
T
(
y
)=
2
1
+
e

4
:
4
(
y
+
0
:
01
)

1ingreen.Here
l
T
isasegmentof
shiftedandscaledSigmoidfunction.Theybothobeythe
l
con-
straintsasdiscussedpreviously.
(b)
Simulationresultsshowthat,
l
L
leadstolinear-like
q
L
inbluesuchthat
q
L
(
x
)
ˇ
x
,and
l
T
leadsto
threshold-like
q
T
ingreen.
howitresponses(withimpulsepropagation).
III.NEURALNETWORKANDITSFIXEDPOINT
FIG.8.Aneuralnetwork(ofoneormultipleagents)consistsof
n

2
neuronsand
c

1directedsynapticconnections.Anexampleof
n
=
8
and
c
=
7isdepicted.Eachneuronreceivesstimulusfromtheenvi-
ronmentwithprobabilityandpropagatesoutnerveimpulsesthrough-
outthesynapticconnections,e.g.,triggeredbystimulusneuron1
propagatesimpulsesstochasticallydownalongthedirectedpaths
1
 
7
 
8
 
2and1
 
7
 
8
 
5
 
3.Cyclicpath(e.g.3
 
8
 
5
 
3)
isallowedandyetloop(e.g.3
 
3)isn't.Eachneuroncouldhave
eitheroutboundorinboundconnections,orneither,orboth.
NowletusturntotheneuralnetworkshowninFIG8.A
neuralnetworkcouldbetreatedasan""aggregateconnection""
asitturnsout.Weshallseethat,theandreasoning
forneuralnetworkalignwellwithneuralconnectioninlast
section.
Aswithsynapticconnection,wecandescribeaneuralnet-
workby
(1)
theexternalstimulusasan
n
-dimensional
vector
X
2
[
0
;
1
]
n
inwhicheach
x
i
istheprobabilityofneuron
i
receivingstimulus;
(2)
thestrengthofallconnectionsasa
c
-
dimensionalvector
S
2
[
0
;
1
]
c
inwhicheach
s
ij
isthestrength
ofconnectionfromneuron
i
toneuron
j
(denotedas
i
 
j
);
(3)
thesimultaneousprobabilitiesoverallconnectionsasa
c
-dimensionalvector
Y
2
[
0
;
1
]
c
inwhicheach
y
ij
isthesimul-
taneousprobabilityover
i
 
j
.Infact,onesingleneural
connectionisaspecialcaseofneuralnetworkwith
c
=
1and
n
=
2.
Stimulusandstrengthuniquelydetermineimpulsesprop-
agationwithinneuralnetwork,sothereexistsamapping
Y
:
(
X
;
S
)
!
Y
.Presumably,themapping
Y
iscontinuouson
S
.ByEq.(1),thereexistsamapping
L
:
Y
!
S

suchthat
s

ij
=
l
ij
(
y
ij
)
foreach
y
ij
in
Y
anditscounterpart
s

ij
in
S

.
Here
S

2
[
0
;
1
]
c
is
c
-dimensionalvectorofconnections'tar-
getstrength,andmapping
L
couldbevisualizedasavec-
toroftargetstrengthfunctionssuchthatentry
L
ij
is
l
ij
.
Thenwithmapping
Y
and
L
wehaveacompositemapping
L

Y
:
(
X
;
S
)
!
S

.Ifeach
l
ij
functioniscontinuousonits
y
ij
,mapping
L

Y
mustbecontinuouson
S
andaccordingto
Brouwer'sed-pointtheoremgiven
X
theremustexistone
edpoint
S
+
2
[
0
;
1
]
c
suchthat
L

Y
(
X
;
S
+
)=
S
+
.Andunder
constantstimulus
X
,neuralnetworkwillgotoedpoint
S
+
aseachconnection
i
 
j
goestoitsedpoint
s
+
ij
.Oursimu-
lationvthattendencyasshowninFIG9.Inthissimula-
tion,impulsestraversetheneuralnetworkstochasticallysuch
thateachneuronisatmostonceperiteration;synaptic
connectionsupdatetheirstrengthasinAlgorithm1.
FIG.9.Simulationresultsofneuralnetwork'stendencyforthefour
typical
l
functionsasinFIG3.Theneuralnetworkhas
n
=
8and
c
=
19.Thefollowingobservationsholdtrueforanyexternalstimu-
lusandconnectionsation:
(a)
Ifallconnectionsareequipped
with
l
(
y
)=
0
:
9
y
+
0
:
05,thewholeneuralnetworkhasonesingleed
pointandthetrajectoriesofmeanofallconnections'strengthcon-
vergetoonepoint.
(b)
l
(
y
)=
0
:
5
sin
(
4
p
y
)+
0
:
5.Becauseeachcon-
nectionhastwostableedpoints,thereare2
19
stableedpoints
forthewholeneuralnetworkand20possibleconvergencepointsof
strengthmean.
(c)
l
(
y
)=

y
+
1.Thereisonesingleedpointfor
theneuralnetwork.Thetrajectoriesconvergetoonepoint.
(d)
Dis-
continuous
l
.Theneuralnetworkhasnoedpointaseachsynaptic
connectionhasnoedpoint.Thetrajectoriesdon'tconverge.
Generallythenumberofstableedpointsforaneuralnet-
workis
Õ
c
f
ij
whereeach
f
ij
isthenumberofstableed
pointsof
i
 
j
.AsinFIG9(b),
Õ
c
f
ij
canbeenormouswhen
each
f
ij

2.Aswithsynapticconnection,ourgoalistoestab-
lishone-to-onemappingbetweenstimulus
X
andedpoint
S
+
forneuralnetworkandmeanwhilekeepinitialstrength
S
0
outofpicture.
l
'scontinuityalonecannotensuretheunique-
nessofedpoint,suchthat
S
0
candeterminewhiched
pointtogofor.Nowwithallthe
l
constraints,wehave:
(1)
L
isaone-to-onemappingandthushasinversemapping
L

1
:
S

!
Y
;
(2)
thereexistsamapping
Q
:
X
!
S
+
,becauseun-
derstimulus
X
theneuralnetworkwillgotothesameunique
5
edpoint
S
+
nomatterwhatinitialstrength
S
0
tobeginwith;
(3)
if
Q
isaone-to-onemapping,
Q
hasinversemapping
Q

1
:
S
+
!
X
.Withmapping
L
,
L

1
,
Q
and
Q

1
beingone-
to-one,given
S
+
wecanidentify
X
and
Y
withoutambiguity,
andviceversa.Therefore,thesameinterpretationwithrespect
tosynapticconnectioncouldapplyhere:theneuralnetwork
atedpointprecisely""memorizes""theinformationaboutthe
stimulusonmanyneuronsandtheimpulsepropagationacross
manyconnections.
Nevertheless,evenallof
l
constraintsarenotsufto
secureone-to-one
Q
:
X
!
S
+
foraneuralnetwork,asopposed
totheneuralconnection.Hereisacase.For
Q
tobeone-
to-one,allneuronsmusthaveoutboundconnection.Other-
wise,e.g.,foraneuralnetworkwiththreeneurons(say0,
1and2)andtwoconnections(say0
 
1and1
 
2),stimu-
lus
X
1
=(
1
;
1
;
0
)
and
X
2
=(
1
;
1
;
1
)
willresultinthesameed
pointbecausestimulusonneuron3,nomatterwhatitis,af-
fectsnoconnection.Orequivalently,for
Q
tobeone-to-one,
theof
X
shouldconsideronlytheneuronswithout-
boundconnectionssuchthat
X
'sdimension
dim
(
X
)

n
.In
theperspectiveofinformationtheory
19
,many-to-one
Q
in-
troducesequivocationtotheneuralnetworkatedpoint,
asifinformationlossoccurredduetonoisychannel.If
dim
(
X
)
>
dim
(
S
)=
c
,mapping
Q
conducts""dimensionreduc-
tion""onstimulus
X
,andinformationlossisboundtooccur.
Hereisatrivialcaseregardingstimulusdependence.Con-
sideraneuralnetworkwith0
 
2,1
 
2and2
 
3,andstim-
ulus
X
=(
x
0
;
x
1
)
.Whentheneuralnetworkisatedpoint,
x
2
=
x
0
s
+
02
+
x
1
s
+
12

s
+
02
s
+
12
x
0
Pr
(
1
j
0
)
wherePr
(
1
j
0
)
istheproba-
bilityofneuron1beingstimulatedconditionalonneuron0
beingstimulated.Pr
(
1
j
0
)
6
=
x
1
ifstimulusonneuron1and
2arenotindependent.Pr
(
1
j
0
)
affects
s
+
23
andhence
S
+
,or
inotherwordstheneuralnetworkatedpointgainsthe
hiddeninformationofPr
(
1
j
0
)
.However,ifPr
(
1
j
0
)
varies,
givenmere
X
therewillbeuncertaintyabout
S
+
suchthat
mapping
Q
doesn'texistunlessstimulus
X
is""augmented""
to
X
=(
x
0
;
x
1
;
Pr
(
1
j
0
))
.
IV.ANAPPLICATIONFORCLASSIFICATION
Ideally,aneuralnetworkwithmemoryofstimulus
X
Š
formally,mapping
Q
castsmemoryofstimulus
X
ased
point
S
+
Šshouldresponsetostimulus
X
more""intensely""
thantheneuralnetworkwithdifferentmemoryresponsesto
X
.Memorywouldmanifestitselfasimpulsespropagation
throughoutensembleofneurons
2,20Œ22
.Thus,itisnaturalto
differentiateresponsebycountingtheneuronsorsynap-
ticconnectionspropagatedbyimpulses.Giventhereasoning
thatsynapsecouldbethesolelocusofmemory
10,11
,weadopt
thecountofsynapticconnectionspropagatedasamacro-
scopicmeasureofhowintenselymemoryresponsestostim-
ulusorstimulus""recalls""memory.Andaccordinglywepro-
poseaconsistingof
g
neuralnetworks,whichclas-
stimulusintooneof
g
classesbythedecisioncriteria
ofwhichneuralnetworkgetsthemostsynapticconnections
propagated.Reminiscentofsupervisedlearning
23
,eachneu-
ralnetworkofourclasistrainedtoitsedpointby
itsparticulartrainingstimulus,andthenatestingstimulusis
testedonall
g
neuralnetworksindependentlytoseewhich
getsthemostconnectionspropagated.Forsimplicityweas-
sumetestingitselfdoesn'tjeopardizetheedpointsofneural
networks.Andmostimportantlyweassumethatforeachneu-
ralnetworkgivenanystimulusthereisonesingleedpoint
suchthatmapping
Q
:
X
!
S
+
exists.
Consideraneuralnetworkinthetobetrained
by

X
toedpoint
S
+
andthentestedby
X
.Inother
words,neuralnetworkmemorizing

X
as
S
+
istestedby
X
.Becauseimpulsespropagateacrosstheneuralnetwork
stochastically,thecountofsynapticconnectionspropagated
inonetestshouldberandomvariable.Letitbe
Z

XX
.
ThenfortheneuralnetworkinFIG8
Z

XX
=
å
c
z
ij
where
eachr.v.
z
ij
˘
Bernoulli
(
x
i
s
+
ij
)
,i.e.,synapticconnection
i
 
j
ispropagatedwithprobability
x
i
s
+
ij
inthetestsuchthat
Pr
(
z
ij
=
1
)=
x
i
s
+
ij
.Easily
z
ij
'sexpectedvalueis
E
[
z
ij
]=
x
i
s
+
ij
,
anditsvarianceisVar
(
z
ij
)=
x
i
s
+
ij
(
1

x
i
s
+
ij
)
.Bycentrallimit
theorem,
Z
'sdistributioncouldtendtowardsGaussian-like
(bellcurve)as
c
increases,evenifall
z
ij
arenotindependent
andidenticallydistributed.Wehave
E
[
Z

XX
]=
c
å
i
 
j
E
[
z
ij
]=
c
å
i
 
j
x
i
s
+
ij
:
(2)
Andwhen
c
islarge,
Var
(
Z

XX
)
ˇ
c
å
i
 
j
Var
(
z
ij
)=
c
å
i
 
j
x
i
s
+
ij
(
1

x
i
s
+
ij
)
:
(3)
Forany
i
 
j
,inthetrainingstagebecause
S
+
=
Q
(

X
)
we
have
s
+
ij
=
q
ij
(

X
)
,andinthetestingstage
x
i
isuniquelydeter-
minedby
S
+
and
X
suchthat
x
i
isafunctionof

X
and
X
.
FIG.10.Thedepictedneuralnetworkisbasicallythegeneralone
inFIG8exceptthat,tomimicreal-lifenervoussystem,anarrayof
sensorneuronsarespecializedforreceivingstimulusfromnoother
neuronsbuttheenvironment.Thereare64sensorneuronstoaccom-
modate8

8-pixelimage,andtherestareaclusterof50neurons.
Eachsensorneuronhas6outboundconnectionstowardscluster,and
eachclusterneuronhas5outboundconnectionstowardswithinclus-
ter.Connectionsarerandomlyputbetweenneuronsbeforetraining.
Weexperimentwiththistoclassifyhandwritten
digitimages
24
.Tenidenticalneuralnetworks(hence
g
=
10)
ofFIG10,eachdesignatedforadigitfrom0to9,aretrained
totheiredpointsbytheirtrainingimagesinFIG11as
stimulus,andthentestingimages,alsoasstimulus,areclas-
intothedigitwhosedesignatedneuralnetworkgetsthe
6
FIG.11.Adigitimagehas8

8
=
64pixels,andpixelgrayscaleis
normalizedtothevaluebetween0and1(bydividing16)asstimulus
probability.Theupperrowshowssamplesofdigitimages,andthe
lowerrowshowsthebetterwritten""averageimages"",eachofwhich
isactuallypixel-wiseaverageofasetofimagesofadigit.Eachneu-
ralnetworkistrainedineachiterationbythesame""averageimage"",
orequivalentlyineachiterationbyimagerandomlydrawnfromthe
setofimages.
biggest
Z
value.Werunmanyteststoevaluate
accuracy,andcollect
Z
valuestoapproximater.v.
Z
'sdis-
tribution.Withallsynapticconnectionsequippedwith
l
L
in
FIG7,thehasaccuracy
˘
44%,and
˘
51%with
l
T
.
Notethat,equippedwith
l
L
or
l
T
,theneuralnetworkofFIG
10willhaveone-to-one
Q
L
or
Q
T
accordingtolastsection.
FIG12andFIG13showthat,inpositivetesting(e.g.digit-6
imageistestedinneuralnetworktrainedbydigit-6images),
Z
'sexpectedvalue(samplemean)couldbeconsiderablybig-
gerthanthatinnegativetesting(e.g.digit-6imageistested
inneuralnetworktrainedbydigit-1images),soastodiscrim-
inatedigit-6imagesfromtheothers.Giventhesametesting
imagetargetcanbedifferenttestbytestsince
theten
Z
outcomesarerandomized.Toimprove
tionaccuracy,weshalldistancethedistributionofpositive
testing
Z
asfaraspossiblefromthoseofnegativetesting
Z
.
WepresentanothertwospecialneuralnetworksinFIG14to
demonstratehowourutilizesmemorytoclassifyim-
agesandhowtoimproveitsaccuracyintheneurobiological
way.
FIG.12.Thehistogram(inprobabilitydensityform)of
Z
.Tocollect
Z
values,adigit-6imageistestedmanytimesoneachoftheten
trainedneuralnetworks.Allconnectionsareequippedwith
l
T
.
Z
66
ofpositivetestingisinred,andtheothernine
Z
k
6
ofnegativetesting,
where
k
=
0
;
1
;
2
;
3
;
4
;
5
;
7
;
8
;
9,areingray.
Z
'ssamplemeanforeach
digitisdepictedasverticaldottedline.
WhentheadoptstenneuralnetworksofFIG14(a)
andequipsallconnectionswith
l
L
inFIG7,ac-
curacyis
˘
31%and
Z
'sdistributionfortestingdigit-6im-
agesisshowninFIG15(a).Wealreadyknowthat
l
L
makes
q
L
(
x
)
ˇ
x
.Thenforonetestwehave
E
[
Z

XX
]=
64
å
x
i
q
i
(

x
i
)=
64
å
x
i
q
L
(

x
i
)
ˇ
64
å
x
i

x
i
=

X
|
X
:
(4)
FIG.13.Thehistogramsof
Z
foralltendigits.Foreachdigit,ran-
domlydrawntestingimage,insteadofthesameone,isusedineach
test.Fromdigit0to9,accuracyisapproximately70%,
41%,56%,42%,53%,33%,77%,51%,57%and32%.Generally,
better
Z
-distributionseparationofpositiveandnagativetestingre-
sultsinhigheraccuracy.
FIG.14.Thesetwoneuralnetworksinheritthesensor-clusterstruc-
tureofFIG10.
(a)
Eachsensorneuronconnectstoonesingleclus-
terneuronsuchthateachpixelstimulus
x
i
onlyaffectsonesingle
connection.Then
s
+
i
=
q
i
(

x
i
)
.ByEq.(2)andEq.(3),wehave
E
[
Z

XX
]=
å
64
x
i
q
i
(

x
i
)
andVar
(
Z

XX
)
ˇ
å
64
x
i
q
i
(

x
i
)[
1

x
i
q
i
(

x
i
)]
.
(b)
Eachsensorneuronconnectstoitsowndedicatedclusterofmany
neuronsandsynapticconnections,andtheclustersareofdifferent
sizes.Inthatcase,inatesteach
x
i
causes
w
i
(insteadofjustone)
synapticconnectionstobepropagatedwithprobability
x
i
s
+
i
ornone
withprobability1

x
i
s
+
i
.Wheneach
w
i
isanonrandomvariable,
wehave
E
[
Z

XX
]=
å
64
x
i
q
i
(

x
i
)
w
i
andVar
(
Z

XX
)
ˇ
å
64
x
i
q
i
(

x
i
)[
1

x
i
q
i
(

x
i
)]
w
2
i
.
Here

X
|
X
isthedotproductoftrainingvector

X
2
[
0
;
1
]
64
andtestingvector
X
2
[
0
;
1
]
64
.Generally,thedotproductof
twovectors,ascalarvalue,isessentiallyameasureofsimi-
laritybetweenthevectors.Thebigger
E
[
Z

XX
]
is,themorein-
tenselyneuralnetworkwithmemoryoftraining

X
responses
totesting
X
,andthemoresimilar

X
and
X
aretoeachother.
Therefore,Eq.(4)simplylinksotherwiseunrelatedneuralre-
sponseintensityandstimulussimilarity.Bycomparingten
7
E
[
Z

XX
]
values,wecantellwhich

X
isthemostsimilarto
X
andhencewhichdigitistarget.Only,
Z

XX
value
fromtestactuallydeviatesaroundthetrue
E
[
Z

XX
]
randomly,
whichmakesitauseableandyetunreliablecri-
teria.
Whentheequipsallconnectionswiththreshold-
like
l
T
inFIG7,accuracyraisesto
˘
44%.By
comparingFIG15(b)withFIG15(a),thedistancebetween
Z
66
'sdistributionandtheothernine
Z
k
6
;
k
6
=
6
'sdistributionis
biggerwiththreshold-like
l
T
thanwithlinear-like
l
L
.This
accuracyimprovementcanbeexplainedconvenientlywitha
truethresholdfunction(orstepfunction)
q
step
(
x
)=
(
0
;
0

x
<
x
step
1
;
x
step

x

1
:
Ofthesumtermsin
å
64
x
i

x
i
ofEq.(4),
q
step
basicallydi-
minishessmall
x
i
2
[
0
;
x
step
)
to0andenhancesbig
x
i
2
[
x
step
;
1
]
to1,suchthatmostprobably
E
[
Z
66
]
wouldincreasebyhav-
ing
x
i
=
1inthesumtermswithbig
x
i
whiletheothernine
E
[
Z
k
6
;
k
6
=
6
]
woulddecreasebyhaving
x
i
=
0inthesumterms
withbig
x
i
,soastopreferablyincrease
E
[
Z
66
]

E
[
Z
k
6
;
k
6
=
6
]
.
AndlikewiseVar
(
Z
)
wouldmostprobablydecrease.Asare-
sult,
q
step
increasesthedistancebetweenthedistributionof
Z
66
and
Z
k
6
;
k
6
=
6
andthusbetterseparatesthem.
FIG.15.The
Z
histogramoftestingdigit-6imagesforthreedif-
ferentsettings.Foreachsetting,valuesof
Z
66
and
Z
k
6
;
k
6
=
6
aretransformedtoz-scores(i.e.thenumberofstandard
deviationsfromthemeanadatapointis)withrespecttothedistri-
butionofall
Z
k
6
;
k
6
=
6
'svaluescombined.Thedistancebetweenthe
distributionof
Z
66
and
Z
k
6
;
k
6
=
6
isapproximatelyevaluatedby
E
[
Z
66
]
andstandarddeviation
s
(
Z
66
)
.
(a)
withneuralnetworks
ofFIG14(a)and
l
L
.
E
[
Z
66
]
ˇ
0
:
93and
s
(
Z
66
)
ˇ
0
:
98.
(b)
Classi-
withneuralnetworksofFIG14(a)and
l
T
.
E
[
Z
66
]
ˇ
1
:
33and
s
(
Z
66
)
ˇ
0
:
9.
(c)
withneuralnetworksofFIG14(b),
l
L
and
w
i
(

x
i
)=
100
x
i
3
.
E
[
Z
66
]
ˇ
1
:
39and
s
(
Z
66
)
ˇ
0
:
87.
FIG14(b)providesanothertypeofneuralnetworktoim-
proveaccuracywithoutadoptingthreshold-like
l
functionforallsynapticconnections.Letthelinear-like
l
L
beequippedbackandtake
w
i
=
100
x
i
3
simplyforexample.
Withthissettingourhasaccuracy
˘
47%.Herewe
TABLEI.accuracyondifferentsettings.In
eachsetting,neuralnetworkcanbetheoneillustratedinFIG10,
FIG14(a)or14(b),andtargetstrengthfunctioncanbeoneofthose
inFIG3andFIG7.Theaccuracylistedistheaverageofmanyout-
comestakenfromthesametrained,andthuscould
slightlyfromonetrainingtoanother.
l
or
q
functionsFIG10FIG14(a)FIG14(b)
l
L
(
y
)=
0
:
99
p
y
+
0
:
0144%31%47%
l
T
(
y
)=
2
1
+
e

4
:
4
(
y
+
0
:
01
)

151%44%51%
q
step
-48%
a
60%
b
l
(
y
)=
0
:
9
y
+
0
:
0514%16%19%
l
(
y
)=
0
:
5
sin
(
4
p
y
)+
0
:
55%
c
6%2%
l
(
y
)=

y
+
14%5%1%
d
Discontinuous
l
23%20%28%
a
x
step
issetto0
:
6.
b
x
step
issetto0
:
2.
c
Accuracyunder10%isactuallyworsethanwildguessing.
d
Ifcriteriaischangedto""whichneuralnetworkgetsthe
fewestsynapticconnectionspropagated"",theaccuracywillbe
˘
40%.
have
E
[
Z

XX
]
ˇ
å
64
x
i
(
100
x
i
4
)
where100
x
i
4
,like
q
step
,actually
transforms
x
i
2
[
0
;
4
p
0
:
1
)
(here
4
p
0
:
1
ˇ
0
:
56)towithin
[
0
;
10
)
andtransforms
x
i
2
[
4
p
0
:
1
;
1
]
toacross
[
10
;
100
]
Šagain,the
strongtrainingpixel-stimulusaregreatlyweightedwhilethe
weakonesarerelativelysuppressed.AsshowninFIG15(c)
thedistancebetweenthedistributionof
Z
66
and
Z
k
6
;
k
6
=
6
isin-
creasedcomparedtoFIG15(a).Hereourneurobiologicalin-
terpretationregarding
w
i
=
100
x
i
3
is,thetrainingstimulusaf-
fectsnotonlysynapticstrength,butalsothegrowthofneuron
clusterinthereplicationofneuroncellsandintheformation
ofnewsynapticconnections.Againthisclaimneedstobe
supportedbyevidences.
TABLEIsummarizestheperformanceofourwith
differenttypesofneuralnetworksandtargetstrengthfunc-
tions.Thefourtypical
l
functionsinFIG3arealsoevalu-
atedtodemonstratehowthesesomewhat""pathological""target
strengthfunctionsaffect
ByEq.(4),theofhandwrittendigitim-
agescouldbetoataskofrestrictedlinear

23
:giventenclasseseachwithitsdiscriminative
function
d
i
(
X
)=

X
|
i
X
where

X
;
X
2
[
0
;
1
]
64
,image
X
isclassi-
totheclasswiththelargest
d
i
value.Ourneuralclassi-
simplytakesoverthecomputationofvectors'dotproduct

X
|
i
X
andaddsrandomnesstothetenresults.Toparameterize
theten
d
i
withtheir

X
i
,the""supervisors""couldtraintheneural
networksinwiththeimagestheydeembestŠ""av-
erageimages""inourcaseordigitslearningcardsinteachers'
case.Ourneuralisratherunreliableandprimitive
comparedtoANNwhichisalsocapableoflinear
tion.Ononehand,giventhesameimageANNalwaysoutputs
thesamepredictionresult.Ontheotherhand,ANNisnot
onlyaclbutalsomoreimportantlya""learner"",which
learnsfromallkindsofhandwrittendigitstotheopti-
mal

X
i
fortheten
d
i
;ANNwithoptimal

X
i
ismoretolerant
withpoorhandwriting,andthushaslessand
betterpredictionaccuracy.Only,ANN'slearningoptimal

X
i
,
anoptimizationprocessofmanyiterations,requiresmassive
8
computationalpowertocarryout,whichisunlikelytobepro-
videdbythereal-lifenervoussystemŠthereisnoevidence
thatanindividualneuroncanevenconductbasicarithmetic
operations.Despiteofitsweakness,ourneuralhas
meritinitsbiologicalnature:itreducesthecomputationof
vectors'dotproducttosimplecountingofsynapticconnec-
tionspropagated;itstrainingandtestingcouldbepurelyneu-
robiologicaldevelopmentandactivitieswherenoarithmetic
operationisinvolved;itsclcriteria,i.e.""deciding""
or""feeling""whichneural(sub)networkhasthemostconnec-
tionspropagated,couldbeanintrinsiccapabilityofintelligent
agents.Thismightprojectnewinsightsontheneural
reality,hopefully.
V.CONCLUSION
Thispaperproposesamathematicaltheorytoexplainhow
memoryformsandworks.Itallbeginswithsynapticplas-
ticity.Weoutthat,synapticplasticityismorethanim-
pulsesaffectingsynapses;itactuallyplaysasaforcethatcan
driveneuralnetworkeventuallytoalong-lastingstate.We
alsooutthat,undercertainconditionstherewouldbea
one-to-onemappingbetweentheneuralstateandtheexternal
stimulusthatneuralnetworkisexposedto.Withthemap-
ping,givenstimulusweknowexactlywhatneuralstatewill
be;givenneuralstateweknowpreciselywhatstimulushas
been.Themappingisessentiallyalinkbetweenpasteventand
neuralpresent;betweentheshort-livedandtheenduring.In
thatsense,themappingitselfismemory,orthemappingcasts
memoryinneuralnetwork.Next,westudyhowmemoryaf-
fectsneuralnetwork'sresponsetostimulus.Weoutthat,
theneuralnetworkwithmemoryofstimuluscanresponseto
similarstimulusmoreintenselythantothestimulusofless
similarity,ifresponseintensityisevaluatedbythenumberof
synapticconnectionspropagatedbyimpulses.Thatistosay,
aneuralnetworkwithmemoryisabletoclassifystimulus.To
verifythisability,weexperimentwiththeconsist-
ingoftenneuralnetworks,andtheyturnouttohaveconsider-
ableaccuracyinclassifyingthehandwrittendigitimages.The
provesthatneuronscouldcollectivelyprovidefully
biologicalcomputationfor
Ourreasoningtakesrootinthemathematicaltreatmentof
synapticplasticityastargetstrengthfunction
l
fromimpulse
frequencytosynapticstrength.Weputhypotheticalcon-
straintsonthis
l
functiontoensurethattheidealone-to-one
mappingexists.Althoughtheseconstraintsarenecessaryto
keepourtheorymathematicallysound,theyraiseconcerns.
Firstly,theycouldbeoverlyrestrictive.Takecontinuitycon-
straintforexample.EventhediscontinuousfunctionofFIG
3(d),whosenonexistent
q
functionwouldmapcertainstimu-
lustoanypointwithinaedinterval""insteadofa
edpointasshowninFIG6,canbeauseable
l
inourclas-
accordingtoTABLEI.Inthiscase,edpointperse
doesn'thavetoexist,andmeretendencytoseekoutforit
couldservethepurpose.Secondly,asdiscussedinSectionII
those
l
constraintshaveyettobesupportedbyneurobiolog-
icalevidences.Aboveall,theevidencethatrevealstrue
l
is
vitaltoclarifytheuncertainty.
VI.ACKNOWLEDGEMENT
Wethanktheanonymousreviewersfortheircommentsthat
improvedthemanuscript.
1
J.Hughes.Post-tetanicpotentiation.
PhysiolRev.
,38(1):91Œ113,1958.
2
D.Hebb.
Theorganizationofbehavior:aneuropsychologicaltheory
.John
Wiley&Sons,1949.
3
S.LowelandW.Singer.Selectionofintrinsichorizontalconnectionsinthe
visualcortexbycorrelatedneuronalactivity.
Science
,255(5041):209Œ212,
1992.
4
G.BerlucchiandH.Buchtel.Neuronalplasticity:historicalrootsandevo-
lutionofmeaning.
ExperimentalBrainResearch
,192(3):307Œ19,2009.
5
A.CitriandR.Malenka.Synapticplasticity:multipleforms,functions,
andmechanisms.
Neuropsychopharmacology
,33:18Œ41,2008.
6
S.Martin,P.Grimwood,andR.Morris.Synapticplasticityandmemory:
anevaluationofthehypothesis.
AnnuRevNeurosci.
,23(649-711),2000.
7
E.Takeuchi,A.Duszkiewicz,andR.Morris.Thesynapticplasticityand
memoryhypothesis:encoding,storageandpersistence.
PhilosTransRSoc
LondBBiolSci.
,369(1633),2014.
8
S.Nabavi,R.Fox,C.Proulx,J.Lin,andR.TsienR.Malinow.Engineering
amemorywithltdandltp.
Nature
,511(348Œ352),2014.
9
Y.Yang,D.Liu,W.Huang,J.Deng,Y.Sun,Y.Zuo,andM.Poo.Selective
synapticremodelingofamygdalocorticalconnectionsassociatedwithfear
memory.
Nat.Neurosci.
,19(1348Œ1355),2016.
10
P.Trettenbrein.Thedemiseofthesynapseasthelocusofmemory:a
loomingparadigmshift?
Front.Syst.Neurosci.
,10,2016.
11
J.LangilleandR.Brown.Thesynaptictheoryofmemory:ahistorical
surveyandreconciliationofrecentopposition.
Front.Syst.Neurosci.
,12,
2018.
12
C.LaingandG.Lord.
Stochasticmethodsinneuroscience
.OxfordUniver-
sityPress,2010.
13
G.Deco,E.Rolls,andR.Romo.Stochasticdynamicsasaprincipleof
brainfunction.
Progressinneurobiology
,88(1):1Œ16,2009.
14
T.BrancoandT.Staras.Theprobabilityofneurotransmitterrelease:
variabilityandfeedbackcontrolatsinglesynapses.
NatRevNeurosci.
,
10(5):373Œ83,2009.
15
C.Baldassi,F.Gerace,H.Kappen,C.Lucibello,L.Saglietti,E.Tartaglione,
andR.Zecchina.Roleofsynapticstochasticityintraininglow-precision
neuralnetworks.
Phys.Rev.Lett.
,120(26),2018.
16
J.Schmidhuber.Deeplearninginneuralnetworks:anoverview.
Neural
Networks
,61:85Œ117,2015.
17
I.Istratescu.
Fixedpointtheory:anintroduction
.Springer,1981.
18
Sourcecodecanbefoundat
https://github.com/lansiz/neuron
.
19
C.Shannon.Amathematicaltheoryofcommunication.
TheBellSystem
TechnicalJournal
,27(3):379Œ423,1948.
20
G.Buzsaki.Neuralsyntax:cellassemblies,synapsemblesandreaders.
Neuron
,63(362Œ385),2010.
21
C.Butler,Y.Wilson,J.Gunnersen,andM.Murphy.Trackingthefear
memoryengram:discretepopulationsofneuronswithinamygdala,hy-
pothalamusandlateralseptumareactivatedbyauditoryfear
conditioning.
Learn.Mem.
,22(370Œ384),2015.
22
C.Butler,Y.Wilson,J.Oyrer,T.Karle,S.Petrou,J.Gunnersen,M.Mur-
phy,andC.Reid.Neuronsactivatedbyfearlearninginlateral
amygdaladisplayincreasedsynapticstrength.
eNeuro
,2018.
23
T.Hastie,R.Tibshirani,andJ.Friedman.
Theelementsofstatisticallearn-
ing
.Springer,2009.
24
Thedatasetof1797handwrittendigitimagescanbeobtainedwithPython
code
""fromsklearnimportdatasets;datasets.load_digits()""
.
"
27,MedGAN: Medical Image Translation using GANs,http://arxiv.org/pdf/1806.06397v2.pdf,https://github.com/ExplainableML/UncerGuidedI2I,"1
MedGAN:MedicalImageTranslationusingGANs
KarimArmanious
1,2
,ChenmingJiang
1
,MarcFischer
1,2
,ThomasKustner
1,2,3
,KonstantinNikolaou
2
,
SergiosGatidis
2
,andBinYang
1
1
UniversityofStuttgart,InstituteofSignalProcessingandSystemTheory,Stuttgart,Germany
2
UniversityofTubingen,DepartmentofRadiology,Tubingen,Germany
3
King'sCollegeLondon,BiomedicalEngineeringDepartment,London,England
Abstract
|Image-to-imagetranslationisconsidered
anewfrontierintheofmedicalimageanaly-
sis,withnumerouspotentialapplications.However,a
largeportionofrecentapproachesindividualized
solutionsbasedonspecializedtask-sparchitec-
turesorrequiretthroughnon-end-to-end
training.Inthispaper,weproposeanewframework,
namedMedGAN,formedicalimage-to-imagetransla-
tionwhichoperatesontheimagelevelinanend-to-
endmanner.MedGANbuildsuponrecentadvances
intheofgenerativeadversarialnetworks(GANs)
bymergingtheadversarialframeworkwithanew
combinationofnon-adversariallosses.Weutilizea
discriminatornetworkasatrainablefeatureextractor
whichpenalizesthediscrepancybetweenthetranslated
medicalimagesandthedesiredmodalities.Moreover,
style-transferlossesareutilizedtomatchthetex-
turesandofthedesiredtargetimages
tothetranslatedimages.Additionally,wepresenta
newgeneratorarchitecture,titledCasNet,whichen-
hancesthesharpnessofthetranslatedmedicaloutputs
throughprogressivetviaencoder-decoder
pairs.Withoutanyapplication-spmo
weapplyMedGANonthreettasks:PET-
CTtranslation,correctionofMRmotionartefacts
andPETimagedenoising.Perceptualanalysisbyra-
diologistsandquantitativeevaluationsillustratethat
theMedGANoutperformsotherexistingtranslation
approaches.
IndexTerms
|Generativeadversarialnetworks,
Deepneuralnetworks,Imagetranslation,PET
attenuationcorrection,MRmotioncorrection.
1Introduction
I
Ntheeldofmedicalimaging,awiderangeofmethods
isusedtoobtainspatiallyresolvedinformationabout
organsandtissuesin-vivo.Thisincludesplainradiog-
raphy,computedtomography(CT),magneticresonance
imaging(MRI)andpositronemissiontomography(PET).
Theunderlyingphysicalprinciplesaremanifold,produc-
ingimagingdataofntdimensionalityandofvarying
contrasts.Thisvarietyvariousdiagnosticoptions
butalsoposesachallengewhenitcomestotranslationof
imageinformationbetweentmodalitiesort
acquisitionswithinonemodality.
Correspondingauthor:
KarimArmanious
E-mailaddress:
karim.armanious@iss.uni-stuttgart.de
Postaladdress:
waldring47,70569,Stuttgart,Germany
Often,asituationoccurswheretwoimagingmodalities
orimagecontrastsprovidesupplementaryinformationso
thattwoormoreacquisitionsarenecessaryforacomplete
diagnosticprocedure.Oneexampleishybridimaging,e.g.
PET/CTwhereCTisusedforthetechnicalpurposeof
attenuationcorrection(AC)ofPETdata(
Colsheretal.
,
2008
).Similarly,CTisusedfordosimetryinradiation
oncologyandhastobeacquiredinadditiontoadiagnostic
planningMR(
Marketal.
,
2001
).
Additionally,optimizationofimagequalityisanimpor-
tantsteppriortotheextractionofdiagnosticinformation.
Especiallywhenusingautomatedimageanalysistools,
highimagequalityisrequiredfortheaccuracyandrelia-
bilityoftheresults.Inspsituations,thegeneration
ofadditionalimageinformationmaybefeasiblewithout
additionalexaminationsusinginformationfromalready
acquireddata.Therefore,aframeworkwhichiscapable
oftranslatingbetweenmedicalimagemodalitieswould
shortenthediagnosticprocedurebymakingadditional
scansunnecessary.Thisenhanceddiagnostic
couldprovetobebenotonlyformedicalprofes-
sionalsbutitalsowouldbemoreconvenientandt
forpatientsalike.
Nevertheless,thetaskoftranslatingfromaninputim-
agemodalitytoanoutputmodalityischallengingdueto
thepossibilityofintroducingunrealisticinformation.This
wouldevidentlyrenderthesyntheticimageunreliablefor
useindiagnosticpurposes.However,insptechnical
situations,itisnotthedetailedimagecontentinthesyn-
theticimagethatisrequiredbutratheraglobalcontrast
property.Inthesesituations,thetranslatedimagesare
usedtoenhancethequalityoffurtherpost-processing
tasksratherthandiagnosis.AnexampleisPETtoCT
translation,wherethesyntheticCTimagesarenotused
directlyfordiagnosisbutratherforPETAC.
1.1Relatedwork
Inthelastdecade,severalcomputationalmethodshave
beenintroducedforthetranslationofmedicalimagesus-
ingmachinelearningapproaches.Forexample,structured
randomforestwasusedinconjunctionwithanauto-
contextmodeltoiterativelytranslateMRpatchesintocor-
respondingCTforthepurposeofPETAC(
Huynhetal.
,
2016
).ForagivenMRimage,thesyntheticCTpatches
arXiv:1806.06397v2  [cs.CV]  4 Apr 20192
arecombinedtogivetheACprediction.Goingina
similardirection,pseudo-CTimageswerepredictedfrom
inputMRpatchesusingak-nearestneighbour(KNN)
regressionalgorithm.Theofthepredictionwas
improvedbylocaldescriptorslearnedthrougha
superviseddescriptorlearning(SDL)algorithm(
Zhong
etal.
,
2016
)andmorerecentlythroughthecombination
offeaturematchingwithlearnednon-linearlocaldescrip-
tors(
Yangetal.
,
2018d
).Inanotherapplicationdomain,
thecorrectionofrigidandnon-rigidmotionartefactsin
medicalimagescouldbeviewedasadomaintranslation
problemfrommotion-corruptedimagesintomotion-free
images.Kustneretal(
Kustneretal.
,
2017
)presented
amethodforcardiacandrespiratorymotioncorrection
forPETimagesviasimultaneouslyacquiredMRmotion
modelandacorrespondingcompressedsensingreconstruc-
tionscheme.Afurtherinterestingapplicationisresolution
enhancementofMRimages.Asuper-resolutionmethod,
whichtranslatesalow-resolutionMRimageintoahigher-
resolutionversion,wasdevelopedbasedonasparserepre-
sentationframeworkwhichincorporatesmulti-scaleedge
analysisandadimensionalityreductionschemeformore
treconstruction(
Ruedaetal.
,
2012
).
Recently,thecomputervisioncommunityhasgained
momentumintheareaofmedicalimageanalysis(
Litjens
etal.
,
2017
).Thisisduetorecentadvancesinarange
ofapplicationssuchaslesiondetectionand
(
Shinetal.
,
2016
;
Douetal.
,
2017
),semanticsegmentation
(
Havaeietal.
,
2015
;
Kamnitsasetal.
,
2016
),registration
(
Miaoetal.
,
2016
)andimageenhancement(
Chenetal.
,
2017
;
Bahramietal.
,
2016
;
Oktayetal.
,
2016
)withthe
developmentofdeeplearningalgorithms,especiallythe
convolutionalneuralnetwork(CNN)(
LeCunetal.
,
2015
).
Thishasledtothedevelopmentofseveralapproachesfor
thegenerationandtranslationofimagedata.Themost
prominentofthoseareGANs.
In2014,I.Goodfellow(
Goodfellowetal.
,
2014
)in-
troducedGenerativeAdversarialNetworks(GANs).They
aregenerativemodelswiththeobjectiveoflearningthe
underlyingdistributionoftrainingdatainordertogener-
atenewrealisticdatasampleswhichareindistinguishable
fromtheinputdataset.PriortotheintroductionofGANs,
state-of-the-artgenerationmodels,suchasVariational
Autoencoders(VAE)(
KingmaandWelling
,
2013
;
Rezende
etal.
,
2014
),tackledthistaskbyperformingexplicit
densityestimation.GANsconstituteanalternativetothis
byahigh-levelgoalsuchas""generateoutputdata
sampleswhichareindistinguishablefrominputdata""and
minimizingthelossfunctionthroughasecondadversarial
networkinsteadofexplicitlyit.
ThemainunderlyingprincipleofGANsisthatof
rivalryandcompetitionbetweentwoco-existingnetworks.
Thenetwork,thegenerator,takesrandomnoiseas
inputandoutputssyntheticdatasamples.Thesecond
network,thediscriminator,actsasabinaryrwhich
attemptstodistinguishbetweenrealtrainingdatasamples
andfakesyntheticsamplesfromthegenerator.Inthe
trainingprocedure,thetwonetworksaretrainedsimulta-
neouslywithopposinggoals.Thegeneratorisinstructed
tomaximizetheprobabilityoffoolingthediscriminator
intothinkingthesyntheticdatasamplesarerealistic.On
theotherhand,thediscriminatoristrainedtominimize
thecrossentropylossbetweenrealandgeneratedsamples,
thusmaximizetheprobabilityofcorrectlyclassifyingreal
andsyntheticimages.ConvergenceisachievedbyGANs
fromagametheorypointofviewbyreachingNash
equilibrium(
Zhaoetal.
,
2016
).Thus,thedistributionof
thegeneratornetworkwillconvergetothatofthetraining
dataandthediscriminatorwillbemaximallyconfusedin
distinguishingbetweenrealandfakedatasamples.
Researchershaveadaptedadversarialnetworksfordif-
ferenttasks.Intuitively,GANsarethestate-of-the-art
modelforimagesynthesiswithrecentmodelsachieving
unprecedentedlevelsofimagerealism(
Karrasetal.
,
2017
).C.Ledig(
Ledigetal.
,
2016
)achievedstate-of-the-
artresultsintheofimagesuper-resolutionviathe
combinationoftheadversariallosstogetherwithcontent
lossintheSuper-ResolutionGAN(SR-GAN)framework.
OtherapplicationsutilizingtheGANsincludes
tion(
Salimansetal.
,
2016
),imagedenoising(
Zhangetal.
,
2017
)andtexttoimagesynthesis(
Zhangetal.
,
2016a
)
amongmanyothers.ThemostrelevantutilizationofGAN
intheofmedicalimageanalysisisimage-to-image
translation.
In2016,P.Isola(
Isolaetal.
,
2016
)introducedthe
pix2pixGANframeworkasgeneralsolutiontosupervised
image-to-imagetranslationproblems.Inthiscase,thegen-
eratorreceivesasinputanimagefromtheinputdomain
(e.gagrayscalephoto)andistaskedtotranslateitto
thetargetdomain(e.gacolouredphoto)byminimizinga
pixel-reconstructionerror(L1loss)aswellastheadversar-
ialloss.Ontheotherhand,thediscriminatoristaskedto
tiatebetweenthefakeoutputofthegeneratorand
thedesiredgroundtruthoutputimage.Severalmo
tionsofthisframeworkhavebeendevelopedtoenhance
thequalityoftheoutputimages.Forexample,PAN(
Wang
etal.
,
2018
)replacedthepixellosswithafeaturematching
lossfromthediscriminatortoreducetheblurrinessofthe
outputimages(
Larsenetal.
,
2016
).Forthepurposeof
one-to-manytranslation,Fila-sGAN(
Zhaoetal.
,
2017
)
utilizedapre-trainednetworkforthecalculationofstyle
losses(
Johnsonetal.
,
2016
)totransferthetextureofinput
styleimagesontothetranslatedimage.Moreover,several
unsupervisedvariantswereintroducedthatdonotrequire
adatasetofpairedinput/targetimagesfortraining,such
asCycle-GANs(
Zhuetal.
,
2017
)andDisco-GANs(
Kim
etal.
,
2017
).
Recently,GANshavebeengainingmoreattentionin
themedicalldespeciallyforimage-to-imagetransla-
tiontasks.Forinstance,apix2pixarchitecturewithan
addedgradient-basedlossfunctionwasutilizedforthe
translationfromMRtoCTimages(
Nieetal.
,
2018
).
Thisarchitecturefromlimitedmodellingcapac-
ityduetopatch-wisetraining.Thisrenderedend-to-end
traininginfeasible.Instead,itisnecessarytotrainseveral
GANframeworksoneafteranotherviaanauto-context
3
modeltoretheresults.Asimilarbutunsupervised
approachwasproposedin(
Wolterinketal.
,
2017a
)via
Cycle-GANs.Pix2pixGANswerealsoutilizedforthe
taskofdenoisinglowdoseCTimagesbytranslatingit
intoahighdosecounterpart(
Wolterinketal.
,
2017b
).
AlsoforthetaskofCTdenoising,(
Yangetal.
,
2018c
)
utilizedapre-trainednetworkforthecalculationoffeature
matchinglossestogetherwiththeWassersteindistance
loss.Synonymouswiththeabovementionedwork,(
Yang
etal.
,
2018a
)utilizedalargelysimilararchitectureforthe
taskofcompressedsensing(CS)MRIreconstruction.
Mostrelevanttoourwork,(
Quanetal.
,
2018
)presented
ageneratorarchitecturesptailoredforthetaskof
CSMRIreconstruction.Thearchitectureconsistsoftwo
residualnetworksconcatenatedinanend-to-endmanner.
Althoughtheresultsofsuchanarchitecturesurpassed
thatofconventionalpix2pix,itfromthelimita-
tionofbeingspctoCSMRIreconstructionandnot
extendabletoothertranslationtasksinwhichthetarget
domainrstlyfromtheinputdomain(e.g.
MRtoCTtranslation).Othermedicaltranslationtasks
havealsobeenrecentlyexploredsuchasCTtoPET(
Ben-
Cohenetal.
,
2018
)and2TMRto1TMRtranslation
(
Yangetal.
,
2018b
;
Daretal.
,
2018
).Theabove-presented
approachesisanoverviewoftheutilizationofGANsfor
medicalimage-to-imagetranslationtasks.However,new
applicationsdomainsarecontinuallybeingexploredby
radiologistsandengineersalike.Amorein-depthsurvey
ofutilizationofadversarialnetworksformedicalimaging
canbefoundin(
XinYi
,
2018
).
1.2Contributions
Ananalysisoftheabovementionedmedicaladversarial
frameworksidenacommonphenomenon.Alarge
portionoftheexistingapproachesareapplication-sp
orfromalimitedmodellingcapacity.Thus,these
modelscannotbeeasilyre-appliedtoothermedical
imagingtasks.
Inthiswork,weproposeanewGANframeworkfor
medicalimage-to-imagetranslation,titledMedGAN.
InspiredbypreviousworkssuchasResNets(
Heetal.
,
2016
),pix2pix,PANandFila-sGAN,ourworkcombines
thefragmentedbofprevioustranslationapproaches
withanewhigh-capacitygeneratorarchitecture.The
resultantframeworkisapplicabletotmedical
taskswithoutapplication-spmoRather
thandiagnosis,themainpurposeofMedGANis
toenhancefurthertechnicalpost-processingtasks
thatrequiregloballyconsistentimageproperties.The
proposedMedGANframeworkoutperformsotherexisting
translationapproachesinqualitativeandquantitative
comparisons.
Ourcontributionsaresummarizedasfollows:

MedGANasanewframeworkformedicaltranslation
tasks.MedGANcapturesthehighandlowfrequency
componentsofthedesiredtargetmodalitybycombin-
ingtheadversarialframeworkwithanewcombina-
tionofnon-adversariallosses.Sp,amo
perceptuallossisutilizedtogetherwithstyle-transfer
losses.

Anewgeneratorarchitecture,namedCasNet.In-
spiredbyResNets,thisarchitecturechainstogether
severalfullyconvolutionalencoder-decodernetworks
withskipconnectionsintoasinglegeneratornetwork.
Astheinputmedicalimagepropagatesthroughthe
encoder-decoderpairs,thetranslatedimageswillpro-
gressivelybetoensureahighresolutionand
crispoutput.CasNetisanend-to-endarchitecturenot
sptoanyparticularapplication.Concurrentto
thiswork,asimilararchitecture,refereedtoasstacked
U-Nets,wasdevelopedfornaturalimagesegmenta-
tion(
Shahetal.
,
2018
).

ApplicationofMedGANonthreechallengingtasksin
medicalimagingwithnoapplication-spcmo
cationstothehyperparameters.Thesearetranslation
fromPETimagesintosyntheticCTimages,PETim-
agedenoisingandtheretrospectivecorrection
ofrigidMRmotionartefacts.

QuantitativeandqualitativecomparisonofMedGAN
withotheradversarialtranslationframeworks.Afur-
theranalysisofindividuallossfunctionswasper-
formedtoillustratethatMedGANismorethanthe
sumofitscomponents.

Thesubjectiveperformanceandyofthetrans-
latedmedicalimageswasinvestigatedfromamedical
perspective.Thiswasdonebyconductingapercep-
tualstudyinwhich5experiencedradiologistswhere
taskedtoratetheresults.
2Materialsandmethods
AnoverviewoftheproposedMedGANframeworkfor
medicalimage-to-imagetranslationtasksispresentedin
Fig.1
.Inthissection,thetlosscomponentsand
networkarchitectureofMedGANwillbepresentedstart-
ingwithsomepreliminaryinformation.
2.1Preliminaries
2.1.1Generativeadversarialnetworks:
GANsconsistof
twomaincomponents,ageneratorandadiscriminator.
Thegenerator
G
receivesasinputsamples
z
froma
priornoisedistribution
p
noise
(e.g.anormaldistribution)
andistaskedtomapittothedataspace^
x
=
G
(
z
)
inducingamodeldistribution
p
model
.Ontheotherhand,
thediscriminatorisabinarywhoseobjectiveis
toclassifydatasamples
x
˘
p
data
asreal,
D
(
x
)=1,and
generatedsamples^
x
˘
p
model
asfake,
D
(^
x
)=0.
Bothnetworksarepittedinacompetitionagainsteach
other.Thegeneratorattemptstoproducesampleswhich
areindistinguishablefromtherealsamples,
p
model
ˇ
p
data
,
thusfoolingthediscriminator.Inthemeantime,thedis-
criminator'sobjectiveistoavoidbeingfooledthrough
learningmeaningfulfeatureswhichbetterdistinguishbe-
tweenrealandgeneratedsamples.Thisconceptofad-
versarybetweenopposingnetworksiswellrepresentedby
4
Fig.1:OverviewoftheMedGANframeworkcomprisingofanovelCasNetgeneratorarchitecture,adiscriminatorand
apre-trainedfeatureextractor.Thegenerator
G
istaskedwithtranslatinginputimagesfromthesourcedomain
y
(e.g
PET)tothetargetdomain^
x
(e.g.CT)throughprogressivetviaencoder-decoderblocks.Theadversarial
discriminator
D
istrainedtodistinguishbetweenrealandtransformedimagesandco-serveasatrainablefeature
extractortocalculatethemoperceptualloss.Thepre-trainedfeatureextractorisusedtoextractdeeprich
features
V
i
(^
x
)tocalculatestyletransferlossesinorderfortheoutputtomatchthetarget'sstyle,texturesandcontent.
theprinciplesofgametheoryviathefollowingmin-max
optimizationtask:
min
G
max
D
L
GAN
(1)
where
L
GAN
istheadversariallossgivenby:
L
GAN
=
E
x
˘
p
data
[log
D
(
x
)]+
E
z
˘
p
noise
[log(1

D
(
G
(
z
)))]
(2)
Thecostfunctionofeachnetworkisdependenton
theopposingnetworkparameters,thereforeconvergence
isachievedbyreachingNashequilibrium(i.e.saddle
point)ratherthanalocalminimum.Thetheoretically
motivatedapproachoftrainingthediscriminatorto
optimalityforageneratornetworktypicallyresults
inavanishinggradientproblem.Alternatively,itwas
foundthatalternatingbetweenupdatingtheopposing
networksoneatatimewhiletheotherhelpsto
avoidthisproblem(
Goodfellowetal.
,
2014
).
2.1.2Image-to-imagetranslation:
Theunderlyingprin-
cipleofadaptingadversarialnetworksfromimagegen-
erationtotranslationaltasksisreplacingthegenerator
networkbyitsconditionalvariant(cGAN)(
Isolaetal.
,
2016
).Inthiscase,thegeneratoraimstomapasource
domainimage
y
˘
p
source
intoitscorrespondingground
truthtargetimage
x
˘
p
target
viathemappingfunction
G
(
y;z
)=^
x
˘
p
model
.Thiscangenerallybeviewedasa
regressiontaskbetweentwodomainsthatsharethesame
underlyingstructuresbutinsurfaceappearance.
Anexamplewouldbethetranslationofgrayscaleim-
agerytocorrespondingcolourimagery.However,instead
ofusingmanuallyconstructedlossfunctionstomeasure
thesimilaritybetweenthetranslatedandtargetimages,
cGANutilizesabinarythediscriminator,asan
alternative.
Inthiscase,theadversariallossisrewrittenas:
L
cGAN
=
E
x;y
[log
D
(
x;y
)]+
E
z;y
[log(1

D
(
G
(
y;z
)
;y
))]
(3)
suchthatthediscriminatoraimstoclassifytheconcate-
nationofthesourceimage
y
anditscorrespondingground
truthimage
x
asreal,
D
(
x;y
)=1,whileclassifying
y
and
thetransformedimage^
x
asfake,
D
(^
x;y
)=0.
Nevertheless,image-to-imagetranslationframeworks
thatrelysolelyontheadversariallossfunctiondonot
produceconsistentresults.Morespcally,theoutput
imagesmaynotshareasimilarglobalstructureasthe
desiredgroundtruthimage.Tocounteractthisissue,a
pixelreconstructionloss,suchastheL1loss,isusually
incorporated(
Isolaetal.
,
2016
;
Zhaoetal.
,
2017
).This
isachievedbycalculatingthemeanabsoluteerror(MAE)
betweenthetargetandsyntheticimages:
L
L1
=
E
x;y;z
[
k
x

G
(
y;z
)
k
1
](4)
suchthatthetrainingobjectiveisgivenby:
min
G
max
D
L
cGAN
+

L
L1
(5)
with
>
0asaweightinghyperparameter.
2.2Perceptualloss
Despitetheadvantagesofpixel-reconstructionlosses,
theyalsocommonlyleadtoblurryresults(
Pathaketal.
,
2016
;
Zhangetal.
,
2016b
).Asaresult,thetranslation
5
frameworkswhichutilizesuchlossfunctionsoftenresult
inoutputswithwellmaintainedglobalstructuresatthe
costofdistortionsandlossofdetails.Suchpixellossesfail
tocapturetheperceptualqualityofhumanjudgement.
Thisiseasilyexaminedwheninspectingtwoidentical
imagesshiftedbyafewpixelsfromeachother.Unlike
thehumanbrainwhichwillimmediatelycapturethesim-
ilaritiesbetweentheimages,apixel-wisecomparisonwill
judgetheimagesasvastlyt(
Johnsonetal.
,
2016
).
Thisphenomenoniscriticalinthedomainofmedical
imageswheresmallstructurescouldtlyalterthe
diagnosticinformationofanimage.
Therefore,tocapturethediscrepancybetweenthehigh
frequencycomponentswithinanimageaperceptualloss
isadditionallyutilized.Thislossisbasedonusingthe
discriminatornetworkatrainablefeatureextractorto
extractintermediatefeaturerepresentations.TheMAE
betweenthefeaturemapsofthetargetimages
x
andthe
translatedimages^
x
isthencalculatesas:
P
i
(
G
(
y;z
)
;x
)=
1
h
i
w
i
d
i
k
D
i
(
G
(
y;z
)
;y
)

D
i
(
x;y
)
k
1
(6)
where
D
i
denotesthefeaturerepresentationsextracted
fromthe
i
th
hiddenlayerofthediscriminatornetwork,
and
h
i
,
w
i
and
d
i
representstheheight,widthanddepth
ofthefeaturespace,respectively.
Theperceptuallossisthenbeformulatedas:
L
perceptual
=
L
X
i
=0

pi
P
i
(
G
(
y;z
)
;x
)(7)
with
L
thenumberofhiddenlayersofthediscriminator
and

pi
>
0isatuninghyperparameterwhichrepresents
theofthe
i
th
layer.Analogousto

fortheL1
loss,

pi
isoptimizedpriortothetrainingofthenetwork
foreachlayer
i
viaahyperparameteroptimizationprocess.
Thiswillbefurtherdiscussedintheendofthissection.
ItisimportanttonotethatunlikeotherGANframe-
workswhichutilizefeaturematchinglosses(e.g.PAN
(
Wangetal.
,
2018
)),theproposedperceptuallossdoes
noteliminatethepixelreconstructioncomponent.This
isduetotheobservationthatpenalizingthediscrepancy
inthepixel-spacehasapositiveimpactonthequality
oftheresultsandshouldnotbeignoredforthesakeof
strengtheningtheoutputdetails.
Additionally,inordertoextractmoremeaningfulfea-
turesforthecalculationoftheperceptualloss,itisneces-
sarytostabilizetheadversarialtrainingofthediscrimina-
tor.Forthispurpose,spectralnormalizationregularization
wasutilized(
Miyatoetal.
,
2018
).Thisisachievedby
normalizingtheweightmatrix

D;i
ofeachlayer
i
inthe
discriminator:

D;i
=

D;i

(

D;i
)(8)
where

(

D;i
)representsthespectralnormofthematrix

D;i
.Asaresult,theLipschitzconstantofthediscrimina-
torfunction
D
(
x;y
)willbeconstrainedto1.Practically,
insteadofapplyingsingularvaluedecompositionforthe
calculationofthespectralnorm,anapproximationviathe
poweriterationmethod
^

(
W
i
)wasusedinsteadinorder
toreducetherequiredcomputationcomplexity(
Miyato
etal.
,
2018
).
2.3Styletransferlosses
Imagetranslationofmedicalimagesisachallengingtask
sincebothglobalyandhighfrequencysharpness,
andthusclarityofdetails,arerequiredforfurthermedical
post-processingtasks.Forexample,inPETtoCTtrans-
lation,thesynthesizedCTimagemustexhibitdetailed
bonestructureforaccuratePETattenuationcorrection.
Furthermore,inthecorrectionofMRmotionartefacts,the
resultingimagemustcontainaccuratesoft-tissuestruc-
turesasthiswilltheresultsofsubsequentpost-
processingtaskssuchassegmentationandorganvolume
calculation.
Toachievetherequiredlevelofdetails,MedGANin-
corporatesnon-adversariallossesfromrecentimagestyle
transfertechniques(
Gatysetal.
,
2016
;
Johnsonetal.
,
2016
).Theselossestransferthestyleofaninputimage
ontotheoutputimage,matchingtheirtexturesanddetails
intheprocess.Similartotheperceptualloss,featuresfrom
thehiddenlayersofadeepCNNareusedforlosscalcu-
lations.However,insteadofutilizingthediscriminator,a
featureextractor,pre-trainedforanimageclass
task,isused.Comparedtothediscriminator,thepre-
trainednetworkhastheadvantageofbeingadeeper
neuralnetworkconsistingofmultipleconvolutionalblocks.
Thisallowstheextractionofrichfeaturesfromalarger
receptivetoalsoenhancetheglobalstructuresof
thetranslatedimagesinadditiontothedetails.Style
transferlossescanbedividedintotwomaincomponents:
stylelossandcontentloss.
2.3.1Styleloss:
Thestylelossisusedtopenalize
thediscrepancyinthestylerepresentationsbetweenthe
translatedimagesandtheircorrespondingtargetimages.
Thestyledistributioncanbecapturedbycalculatingthe
correlationsbetweenfeaturerepresentationsinthespatial
extent.
V
j;i
(
x
)denotethefeaturemapsextractedfrom
the
j
th
convolutionalblockand
i
th
layerofthefeature
extractornetworkforinputimage
x
.Thefeaturemaps
havethenthesize
h
j

w
j

d
j
with
h
j
,
w
j
,
d
j
beingthe
height,widthandspatialdepth,respectively.Inthiswork,
onlythelayerofeachconvolutionalblockisused,thus
thesub-index
i
isassumedtobe1andwillbeomittedin
thefollowingnotations.Thefeaturecorrelationsarerep-
resentedbytheGrammatrix
Gr
j
(
x
)ofeachconvolutional
block.Thismatrixisoftheshape
d
j

d
j
anditselements
arecalculatedbytheinnerproductbetweenfeaturemaps
overtheheightandwidthdimensions:
Gr
j
(
x
)
m;n
=
1
h
j
w
j
d
j
h
j
X
h
=1
w
j
X
w
=1
V
j
(
x
)
h;w;m
V
j
(
x
)
h;w;n
(9)
ThestylelossisthencalculatedastheFrobenius
squarednormofthebetweenthefeaturecorre-
6
Fig.2:TheproposedCasNetgeneratorarchitecture.CasNetconcatenatesseveralencoder-decoderpairs(U-blocks)to
progressivelythedesiredoutputimage.
lationsofthetranslatedoutputs^
x
andthegroundtruth
inputs
x
:
L
style
=
B
X
j
=1

sj
1
4
d
2
j
k
Gr
j
(
G
(
y;z
))

Gr
j
(
x
)
k
2
F
(10)
where

sj
>
0isalsoatuninghyperparameters
representingtheweightofthecontributionofthe
j
th
convolutionalblockand
B
isthetotalnumberof
convolutionalblocks.
2.3.2Contentloss:
Thecontentlossdirectlypenalizes
thebetweenfeaturerepresentationsextracted
fromthefeatureextractornetwork.Contrarytothestyle
loss,thecontentlossdoesnotcapturediscrepanciesin
styleortexture.However,itservesanauxiliarypurpose
analogoustothatofthepixel-reconstructionlossbyen-
hancinglowfrequencycomponentsandensuringglobal
consistencyofthetransformedimages.Thecontentloss
isgivenby:
L
content
=
B
X
j
=1

cj
1
h
j
w
j
d
j
k
V
j
(
G
(
y;z
))

V
j
(
x
)
k
2
F
(11)
where

cj
>
0isahyperparameterrepresentingthe
oftherstlayerofthe
j
th
convolutionalblock.
2.4MedGANarchitecture
2.4.1U-blocks:
Thetaskofimage-to-imagetranslation
canbedescribedasmappingahighdimensionalinput
tensorintoanoutputtensorwithtsurfaceap-
pearancebutofthesameunderlyingstructure.From
anotheraspect,themainarchitecturalconsiderationof
theMedGANframeworkisrobustnesstotin-
putmodalitieswithnoapplication-spmos.
Therefore,thefundamentalbuildingblockofMedGAN
waschosentobeanencoder-decoderarchitecture,which
werefertoasaU-block.
AU-blockisafullyconvolutionalencoder-decodernet-
workfollowingthearchitectureintroducedin(
Isolaetal.
,
2016
).ItisinspiredbyU-nets(
Ronnebergeretal.
,
2015
)
whichhavebeenadaptedaccordingtothearchitectural
guidelinesin(
Radfordetal.
,
2016
)tostabilizetheadver-
sarialtrainingprocess.Theencodingpathmapstheimage
fromtheinputdomain,in256

256resolution,intoahigh
levelrepresentationusingastackof8convolutionallayers
eachfollowedbybatchnormalizationandLeaky-ReLU
activationfunctions.Thenumberofconvolutionalis
64,128,256,512,512,512,512and512respectivelywith
kernelsize4

4andstride2.Forthedesiredpurposeof
medicalimagetranslation,stochasticityisnotdesiredand
theencodingpathonlyreceivesthesourcedomainimage
y
asinput.Thesubscript
z
,in
Eq.(3)
,denotinginputnoise
sampleswillhencebeomittedfromfuturenotations.The
decodingpathmirrorstheencodingarchitecturealbeit
utilizingfractionallystrideddeconvolutions.Thisenlarges
theresolutionbyafactoroftwoaftereachlayer,which
invertsthedownsamplingbytheencodingpathandmaps
fromthehighlevelrepresentationintotheoutputimage
domain.Theupsamplingpathconsistsof512,1024,1024,
1024,1024,512,256and128respectively,ineachof
thelayerswhichutilizeReLUactivationfunctionsexcept
forthelastdeconvolutionallayerwhichusesaTanh
activationinstead.
Additionally,aU-blockcontainsskip-connectionswhich
concatenatespatialchannelsbetweenmirroredlayersin
theencoderanddecoderpaths,e.g.betweenthe2
nd
en-
codinglayerandthe7
th
decodinglayer.Theseconnections
arefundamentalforimagetransformationtaskssincethey
passcriticallowlevelinformationbetweentheinputand
outputimages.Thisinformationwillotherwisebelost
throughthebottlenecklayerleadingtoseveredegradation
inoutputquality.
2.4.2CasNet:
Translationofmedicalimagesposesa
challengecomparedtoregularimagetransformationtasks.
Thisisduetotheamountofrelevantmedicalinformation
containedindetailedstructuresintheimageswhichcan
belostordistortedduringthetranslationprocess.Inorder
tocircumventthisissue,currentapproachesutilizeeither
specializedarchitecturesforagivenmedicaltransforma-
tiontask(
Quanetal.
,
2018
)orrequirethetrainingof
severalframeworksoneaftertheother(
Nieetal.
,
2018
).
Inordertoconstructanon-application-spsolution
7
theCasNetarchitectureisproposed,illustratedin
Fig.2
.
InspiredbyResNets(
Heetal.
,
2016
),whichcascades
theso-calledresidualblocks,CasNetsincreasesthegen-
erativecapabilitiesofMedGANbyconcatenatingseveral
U-blocksinanend-to-endmanner.Thisisdonesuch
thattheoutputoftheU-blockispassedasthe
inputofthesecondblocktillthe
N
th
block.Asaresult,
thetranslationtaskiscarriedoutusingthecollective
capacityoftheU-blocksinanend-to-endmanner.Thus,
thetranslatedoutputsareprogressivelyasthey
passthroughtheencoder-decoderpairs.Backpropagation
ofthelossgradientsthroughsuchnetworkdepthmay
resultinavanishinggradientproblem.However,duetothe
utilizationofskipconnectionswithinindividualU-blocks
thisproblemismitigated.
AlthoughCasNetsandResNetssharethesamebasic
principleofconcatenatingamorebasicbuildingblock,
fundamentalncesexistbetweenthetwonetworks.
Theisconcerningthedepth.Residualblocksconsist
ofonly2-4convolutionallayerswhereasU-blockshave
adeeperarchitectureof16convolutionallayers,which
increasesthegenerativecapacityofCasNets.Moreover,
CasNetsutilizeintermediateskipconnectionstopasslow
levelinformationandpreventvanishinggradientsinstead
ofusingidentitymappingstoconnecttheinputimageto
theoutputoftheresidualblock.
2.4.3Discriminatorarchitecture:
Forthediscriminator,
amoPatchGANarchitectureisutilized,proposedin
(
Isolaetal.
,
2016
).Insteadofclassifyingthetargetand
outputimagesasbeingrealornot,PatchGANisdesigned
tohaveareducedreceptivesuchthatitdividesthe
inputimagesconvolutionallyintosmallerimagepatches
beforeclassifyingthemandaveragingouttheresult.
Consequently,thediscriminator'sattentionisrestricted
tosmallimagepatcheswhichencouragehighfrequency
correctnessandenablesdetailedoutputsbythegenerator.
Generally,70

70patchesistheconventionalpatchsize
toutilizeinordertoavoidthetypicaltilingartefacts
withsmallerpatchsizes.However,weempiricallyfound
thattheutilizationofsmallerpatchesincombination
withthepreviouslyintroducednon-adversariallosses,e.g.
perceptualandstyletransferlosses,promotessharper
resultswhileeliminatingconventionaltilingartefacts.As
aresult,a16

16patchsizeisutilizedbyincorporating
twoconvolutionallayerswith64and128spatial
followedbybatchnormalizationandLeaky-ReLUactiva-
tionfunctions.Lastly,tooutputtherequired
probabilitymap,aconvolutionlayerofoutputdimension
1andasigmoidactivationfunctionisused.
2.5MedGANframeworkandtraining
Insummary,theMedGANframeworkconsistsofa
CasNetgeneratornetworkpenalizedfromtheperceptual
andpixelperspectivesviaanadversarialdiscriminator
network.Additionally,MedGANutilizesstyletransfer
lossestoensurethattranslatedoutputmatchesthedesired
Algorithm1
TrainingpipelineforMedGAN
Require:
Pairedtrainingdataset
f
(
x
l
;y
l
)
g
T
l
=1
Require:
Numberoftrainingepochs
N
epoch
=200,
numberoftrainingiterationsforgenerator
N
G
=3,

1
=20and

2
=

3
=0
:
0001
Require:
LoadpretrainedVGG-19networkparameters
Initialize:
Weightparametersofgeneratoranddiscrimi-
nator

G
,

D
1:
for
n
=1
;:::;N
epoch
do
2:
for
l
=1
;:::;T
do
3:
for
t
=1
;:::;N
G
do
4:
L
cGAN
 
log(
D
(
G
(
y
l
)
;y
l
))
5:
L
perceptual
 
P
i

pi
P
i
(
G
(
y
l
)
;x
l
)
6:
L
style
 
P
j

sj
4
d
2
j
k
Gr
j
(
G
(
y
l
))

Gr
j
(
x
l
)
k
2
F
7:
L
content
 
P
j

cj
h
j
w
j
d
j
k
V
j
(
G
(
y
l
))

V
j
(
x
l
)
k
2
F
8:

G
+
 

G
[
L
cGAN
+

1
L
perceptual
+

2
L
style
+

3
L
content
]
9:
endfor
10:
L
cGAN
 
log(
D
(
x
l
;y
l
))+log(1

D
(
G
(
y
l
)
;y
l
))
11:

D
+
 r

D
[
L
cGAN
]
12:
Spectralnormalization:

D;i
=

D;i

(

D;i
)
13:
endfor
14:
endfor
targetimageinstyle,textureandcontent.Theframework
istrainedviaamin-maxoptimizationtaskusingthe
followingcumulativelossfunction:
L
MedGAN
=
L
cGAN
+

1
L
perceptual
+

2
L
style
+

3
L
content
(12)
where

1
,

2
and

3
arehyperparametersthatbalance
outthecontributionofthetlosscomponents.
Asaresultofextensivehyperparameteroptimization,

1
=20and

2
=

3
=0
:
0001wasutilized.Additionally,

pi
waschosentoallowbothlayersofthediscriminator
tohaveequalenceontheloss.Similarly,

cj
was
settoallowallbutthedeepestconvolutionalblocksto
thecontentloss.However,thestyleloss

sj
was
chosentoincludeonlytheoftheandlast
convolutionalblocksofthepre-trainedVGG-19network.
Regardingthefeatureextractor,adeepVGG-19network
pre-trainedonImageNettask(
Simonyanand
Zisserman
,
2014
)wasused.Itconsistsof5convolutional
blocks,eachof2-4layers,andthreefullyconnectedlayers.
Althoughitispre-trainedonnon-medicalimages,the
featuresextractedbytheVGG-19networkwasfoundtobe
bialinrepresentingthetextureandstyleinformation
aswillbeshowninthefollowingresultssection.For
training,wemakeuseoftheADAMoptimizer(
Kingma
andBa
,
2014
)withmomentumvalueof0
:
5andalearning
rateof0
:
0002.Instancenormalizationwasappliedwith
abatchsizeof1,whichwasshowntobebialfor
imagetranslationtasks(
Ulyanovetal.
,
2016
).Forthe
optimizationofMedGAN,thepatchdiscriminatorwas
trainedonceforeverythreeiterationsoftrainingthe
CasNetgenerator.Thisleadstoamorestabletraining
andproduceshigherqualityresults.Theentiretraining
8
Input
Target
(a)PET-CTtranslation
Input
Target
(b)MRmotioncorrection
Input
Target
(c)PETdenoising
Fig.3:AnexampleofthethreedatasetsusedforthequalitativeandquantitativeevaluationoftheMedGANframework.
processisillustratedinAlgorithm1.
TheMedGANframeworkwastrainedonasingleNvidia
Titan-XGpuwithaCasNetgeneratorarchitecturecon-
sistingof
N
=6U-blocks.Thetrainingtimeislargely
dependentonthesizeofthedatasetusedbutwasfound
tobeanaverageof36hours.Theinferencetime,however,
wasfoundtobe115millisecondsforeachtestimage.The
implementationoftheMedGANframeworkwillbemade
publiclyavailableuponthepublishingofthiswork
1
.
3Experimentalevaluations
3.1Datasets
ToshowcaseMedGANasanon-application-sp
framework,MedGANwasdirectlyappliedonthreechal-
lengingtasksinmedicalimagery.Notask-specmod-
tothehyperparametersorarchitectureswas
applied.Theutilizeddatasetsareillustratedin
Fig.3
.
Forthetapplication,PETimagesaretranslated
intocorrespondingsyntheticCTimages.Thisisanon-
trivialtasksincethetargetmodalitycontainsmorede-
tailedinformation,e.g.bonestructuresandsofttissues,
comparedtotheinputsourcemodality.Forthatpurpose,
ananonymizeddatasetof46patientsofthebrainregion
acquiredonajointPET/CTscanner(SOMATOMmCT,
SiemensHealthineers,Germany)wasused.TheCTdata
hasanoriginalresolutionof0
:
85

0
:
85

5mm
3
anda
matrixsizeof512

512,whilePETdatahaveavoxel
sizeof2

2

3mm
3
andamatrixsizeof400

400.The
resolutionofbothmodalitieswasresampledtoavoxelsize
of1

1

1mm
3
,alignedusingtheheaderinformation
andthencentrecroppedtoextracttherelevanthead
region.Duetohardwarelimitations,only2-dimensional
axialslicesofresolution256

256pixelswereusedduring
thetrainingprocess,withadatasetof1935pairedtraining
imagesfrom38patients,and414imagesfrom8separate
patientsforvalidation.
Thesecondapplicationisconcernedwiththeretrospec-
tivecorrectionofmotionartefactsinMRimages.Motion-
corruptedMRimageswastranslatedintocorresponding
motion-freeimages.Thisisachallengingtask,notonly
becauseoftheseverityofrigidmotionartefactsinthe
acquireddatasetsbutalsobecauseoftheyachiev-
ingpixel-wisealignmentbetweenmotionfreeandmotion
corruptedMRscanstakensequentiallyintime.This
highlightstherobustnessofMedGANagainstalignment
1
https://github.com/KarimArmanious
errorsintherequiredtrainingdatasets.Ananonymized
datasetof11volunteersfromthebrainregionwasacquired
usingaclinicalMRscanner(BiographmMR3Tesla,
SiemensHealthineers,Germany).AT1-weightedspinecho
(SE)sequencewasacquiredonceunderrestingconditions
andanotherunderrigidheadmotionforallvolunteers
(
Kustneretal.
,
2018
).SimilartothePET-CTdataset,
theMRdatawasscaledtoaspacingof1

1

1mm
3
and2Daxialslicesof256

256resolutionwasextracted
fromthebrainregion.Imagedatawerepairedinthata
motion-freeandamotion-corruptedimagewereacquired
andalignedusingtheheaderinformation.Thetraining
datasetsconsistedof1445MRimagesfrom7patients,
whileevaluationwascarriedoutonaseparatedatasetof
556imagesfrom4patients.
Fortheapplicationofthisstudy,theMedGAN
frameworkwasutilizedfordirectdenoisingofPETimag-
ingdata.Forthisstudyanonymizeddatasetswereusedfor
thehead,torsoandabdomenregionsfrom33patientsus-
ingaPETscanner(BiographmCT,SiemensHealthineers,
Germany).Thescanshavearesolutionof2
:
8

2
:
8

2mm
3
andavolumeof256

256

479.NoisyPETscans,
producedbyreconstructingPETimagesfromonly25%of
theoriginalacquisitiontime,andoriginalPETscanswere
pairedtogetherinadatasetof11420training2Daxial
slicesand4411validationimages.
3.2Experimentalsetup
3.2.1Analysisoflossfunctions:
Inadditiontothe
conditionaladversarialloss,MedGANincorporatesanew
combinationofnon-adversariallossesaspartofitsframe-
work.Namely,theperceptual,styleandcontentlosses.
Thiscombinationofdtlossfunctionsisessentialto
capturethelowfrequencies,ensuringglobalconsistency,as
wellasthehighfrequencydetailsofthedesiredtargetim-
ages.Thesetofexperimentsisconcernedwithstudy-
ingtheimpactofindividuallosscomponentsandshowing
thatMedGANismorethanthesumofitsparts.To
thisend,separatemodels,eachutilizinganindividualloss
component,weretrainedandcomparedwithMedGANfor
thetaskofPET-CTtranslation.Forafaircomparison,all
trainedmodelsutilizedidenticalarchitecturesconsistingof
asingleU-blockgeneratoranda16

16patchdiscrimina-
tornetwork.However,forMedGANtwoseparatevariants
wereinvestigated.Sp,aMedGANincorporating
aCasNetarchitectureof6U-blocksandaMedGAN
whosegeneratorconsistsofonly1U-block,referredtoas
9
MedGAN-1G.Thisistoillustratethattheperformance
ofMedGANisnotsolelyduetotheincreasedcapacity
providedbytheCasNetarchitecturebutalsotheutilized
non-adversariallosses.
3.2.2Comparisonwithstate-of-the-arttechniques:
Inthesecondsetofexperiments,theperformanceof
MedGANwasinvestigatedonthreechallengingtaskswith
notask-spmotothehyperparameters.
Forthispurpose,severaltranslationapproacheswhere
re-implemented,trainedonthethreeacquireddatasets
andcomparedqualitativelyandquantitativelywiththe
MedGANframework.Toensureafaithfulrepresentation
ofthemethodsusedinthecomparativestudy,apublicly
vimplementationofpix2pixwasusedasbasisfor
there-implementationofthetapproaches(
Isola
andHesse
,
2016
).
First,thecGANlosswascombinedwithanL1pixel
reconstructionlossintothepix2pixframework(
Isolaetal.
,
2016
).Thismethodwasusedpreviouslyforvariousmedi-
calapplicationssuchasMRtoCTtranslation(
Nieetal.
,
2018
),CTdenoising(
Wolterinketal.
,
2017b
)and2T
to1TMRtranslation(
Yangetal.
,
2018b
).Moreover,
aperceptualadversarialnetwork(PAN)(
Wangetal.
,
2018
)wasalsoimplementedbyincorporatingaperceptual
losscomponentsimilartothatproposedbyMedGAN.
However,theperceptuallossutilizedwithintheMedGAN
frameworkadditionallyincludesapixellosscomponent
bycalculatingtheMAEoftherawinputsaswellas
thatofthehiddenfeaturesextractedbythediscriminator.
Thiscomponentwasfoundtobebcialinmaintaining
theensureglobalconsistencyofthetranslatedimages.
Additionally,PANpenalizesthediscriminatortopreserve
theperceptualdiscrepancybetweenthehiddenfeatures
inordertostabilizeadversarialtraining.However,inour
experiments,itwasfoundoutthatsuchapenaltyterm
oftenleadstoblurrydetailsintheresultantmedical
images.TheFila-sGANwasdevelopedwithat
objectivecomparedtoMedGAN.Itattemptstotransfer
thetexturesofaninputstyleimageontoaGANtranslated
imageinordertogeneratemultiplevariationsofthesame
underlyingstructure(
Zhaoetal.
,
2017
).However,itis
similartoMedGANinthatitutilizesapre-trainedVGG
networktocalculatestyleandcontentlossesinaddition
toatotalvariationlossandaL1pixelreconstructionloss.
Therefore,were-implementFila-sGANwiththeobjective
ofenhancedimagetranslationbyreplacingthestyleinput
imageswiththeoriginalsourcedomainimages.The
translationapproachusedinthiscomparativestudyis
theID-CGAN,designedforthedenoisingofweather
artefactsinnaturalimages(
Zhangetal.
,
2017
).ID-CGAN
incorporatesacombinationoftheadversarialloss,L2pixel
reconstructionlossandthecontentlossextractedfrom
apre-trainedVGG-19network.Forfaircomparison,all
methodsweretrainedusingthesamesettings,hyperpa-
rametersandarchitecture(asingleU-blockgeneratorand
patchdiscriminator)astheMedGANframeworkwhich
onlyinutilizingaCasNetgeneratorarchitecture
of6U-blocks.
TABLEI
Quantitativecomparisonoflosscomponents
LossSSIMPSNR(dB)MSEVIFUQILPIPS
cGAN0.896023.65313.20.38580.93000.2592
Peceptual0.907124.20287.00.41830.95140.2628
Style-content0.904624.12282.80.41050.94350.2439
MedGAN-1G0.912124.51271.80.4348
0.95690.2142
MedGAN
0.916024.62264.60.4464
0.95580.23015
3.2.3Perceptualstudyandvalidation:
Tojudgethe
yofthetranslatedimages,aseriesofexperiments
wereconductedinwhich5experiencedradiologistswere
presentedaseriesoftrialseachcontainingtheground
truthtargetimageandtheMedGANoutput.Themain
purposeofthisstudyistoinvestigatehowrealisticthe
translatedimagesbyMedGANcomparedtogroundtruth
medicalimagery.However,asabaselineofcomparisonthe
samestudywasrepeatedforthepix2pixframework.In
eachofthetrails,theimagesappearedinarandomized
orderandparticipantswereaskedtoclassifywhichwas
thegroundtruthimageaswellasratethequalityofeach
imageusinga4-pointscore,with4beingthemostrealistic.
Eachparticipanttestedonetranslationapplicationata
timeandwaspresented60triadsofimagesfromthat
respectivedataset.Allimageswerepresentedin256

256
resolution.
3.3Evaluationmetrics
TheperformanceoftheMedGANframeworkwaseval-
uatedontheabove-mentioneddatasetsbothqualitatively
andquantitatively.Withrespecttoquantitativeexperi-
ments,thereisnoconsensusinthesciencommunity
regardingthebestevaluationmetricstoassestheper-
formanceofgenerativemodels(
Borji
,
2018
).Therefore,
severalimagequalitymetricswereutilizedtojudgethe
qualityofthetranslatedmedicalimagessuchasStructural
SimilarityIndex(SSIM)(
Wangetal.
,
2004
),PeakSignal
toNoiseRatio(PSNR),MeanSquaredError(MSE),Vi-
sualInformationFidelity(VIF)(
SheikhandBovik
,
2006
)
andUniversalQualityIndex(UQI)(
WangandBovik
,
2002
).Nevertheless,recentstudiespointedoutthatthese
metricscouldnotbecounteduponsolelyasreference
forhumanjudgementofimagequality.Hence,therecent
metrictitledLearnedPerceptualImagePatchSimilarity
(LPIPS)wasutilized,whichwasreportedtooutperform
previousmetricsasaperceptualmeasureofquality(
Zhang
etal.
,
2018
).Forthequalitativecomparisons,wepresent
theinput,transformedandground-truthtargetimages.
4Results
4.1Analysisoflossfunctions
Theresultsofutilizingindividuallossfunctionsincom-
parisontoMedGANarepresentedin
Fig.4
and
TableI
10
Input
cGAN
Perceptual
Style-content
MedGAN-1G
MedGAN
Target
Fig.4:ComparisonoftheenessoftlossfunctionsusedwithintheMedGANframework.Ontheleftmost
column,inputPETimagesaregivenwhichcorrespondstothegroundtruthCTimagesgivenintherightmostcolumn
intwoslices.IntermediatecolumnsshowsyntheticallytranslatedCTimagesasaresultoftrainingusingt
individuallosscomponents.
TABLEII
QuantitativecomparisonbetweenMedGANandothertranslationframeworks
Method
(a)PET-CTtranslation(b)MRmotioncorrection(c)PETdenoising
SSIMPSNR(dB)MSEVIFUQILPIPS
SSIMPSNR(dB)MSEVIFUQILPIPS
SSIMPSNR(dB)MSEVIFUQILPIPS
pix2pix
0.901723.93299.20.40240.95190.2537
0.813823.79335.20.34640.52200.2885
0.970734.8937.200.60680.94400.0379
PAN
0.902724.10292.20.40840.91900.2582
0.811623.91311.40.35480.53990.2797
0.971334.9738.760.60680.94310.0348
ID-CGAN
0.903924.13288.60.40590.93890.2423
0.8214
24.26
289.80.36850.58550.2747
0.969934.2839.450.60230.94350.0346
Fila-sGAN
0.903924.08289.60.41460.90540.2320
0.811423.91318.70.34310.49570.2570
0.972635.0535.80
0.62790.9472
0.0328
MedGAN
0.916024.62264.60.44640.95580.2302
0.8363
24.18
289.90.37350.60370.2391
0.972935.2333.54
0.61680.9443
0.0292
respectively.Fromaqualitativepointofview,itwasfound
outthatthetraditionaladversarialloss
L
cGAN
leadsto
theworstresults(
Fig.4
).Thisisalsointhe
quantitativescores(
TableI
)wherecGANachievesthe
worstnumericalscoresacrossthechosenmetrics.Onthe
otherhand,theperceptuallossimprovestheresultsby
enhancingthedetailsoftheresultantbonestructures.
Italsoestheglobalconsistencyduetothepixel-
wisecomponentoftheloss.However,whencomparedto
thegroundtruthtargetimages,itisobservedthatthe
translatedCTimageshaveareducedlevelofdetails.
Combiningthegenerativeframeworkwithapre-trained
featureextractor(VGG-19)forthecalculationofstyle
andcontentlossesfurtherimprovesthequalitativeresults.
Thisisrbythetransformedimageshavingsharper
detailsandmoreedstructuresduetomatching
thetarget'stexturalandglobalcontent.TheMedGAN-
1Gframeworkresultsinanincreasedsharpnessofthe
translatedimagesaswellasanotableimprovementof
thequantitativemetricscomparedtotheindividualloss
components.Yet,incorporatingtheCasNetgeneratorar-
chitecturefurtherenhancesthetranslatedoutputiamges
withmorebonestructuresanddetails.Asshown
inTableI,thisisbyatreductionin
theMSEaswellasincreasesintheSSIM,PSNRand
VIFcomparedtoMedGAN-1Gwithareactivelysmall
intheUQIandLPIPSscores.
4.2Comparisonwithstate-of-the-arttechniques
Forthesecondsetofexperiments,theperformance
ofMedGANwascomparedagainstseveralstate-of-the-
arttranslationframeworksincludingpix2pix,PAN,ID-
CGANandFila-sGAN.Theresultsaregivenin
Fig.5
and
TableII
forthequalitativeandquantitativecom-
parisons,respectively.Pix2pixproducestheworstresults
withPANonlyslightlyoutperformingit.ForMRmo-
tioncorrection,pix2pixandPANsucceededinproducing
globallyconsistentMRimages,albeitwithblurrydetails.
However,forPETtoCTtranslationtheoutputimages
lackedsharpnessandhomogeneity,includingrealisticbone
structures.Thiswasalsoquantitativelywith
thesemethodsachievingtheworstscoresin
TableII
.ID-
CGANoutperformedthepreviousmethodsinPETto
CTtranslationwiththeresultantimageshavingamore
consistentglobalstructure.However,ID-CGANdidnot
performasstronglyontheotherdatasets.Forexample,
ID-CGANresultedinttiltingartefactsaswellas
blurredoutputdetailsinMRmotioncorrection.Similarly,
Fila-sGANproducedinconsistentresultsonthet
datasets.WhileitproducedpositiveresultsinPETto
CTtranslation,FilaresultedinblurreddenoisedPET
imagesandunrealistictexturesinthemotioncorrected
MRimages.TheMedGANframeworkoutperformedthe
otherapproachesonthethreettranslationtasks.
Itproducessharperandmorehomogeneousoutputsfrom
11
Input
pix2pix
PAN
ID-CGAN
Fila-sGAN
MedGAN
Target
(a)PET-CTtranslation
Input
pix2pix
PAN
ID-CGAN
Fila-sGAN
MedGAN
Target
(b)MRmotioncorrection
Input
pix2pix
PAN
ID-CGAN
Fila-sGAN
MedGAN
Target
(c)PETdenoising
Fig.5:ComparisonbetweenMedGANandtimagetranslationapproachesfortheproposedmedicalimage
translationtasks.Tworespectiveimageslicesareshownforeachtask.
thevisualperspective.TheperformanceofMedGANwas
alsoquantitativelyin
TableII
.Itresultedinthe
bestscoreforthettasksacrossthelargemajority
ofthechosenmetrics.
4.3Perceptualstudyandvalidation
Theresultsoftheperceptualstudyconductedbyradi-
ologistsonthethreeutilizeddatasetsarepresentedin
Ta-
bleIII
.Thecolumnofthistablestatesthepercentage
ofimagesbyradiologistsasrealoutofthetriad
ofpresentedimages.InthePETtoCTtranslation,25.3%
ofthesyntheticallygeneratedCTimagesbytheMedGAN
frameworkmanagedtoconvinceradiologistsintothinking
theyaregroundtruthimagesfromarealCTscanner.In
MRmotioncorrectionandPETdenoising,thepercentage
ofMedGANimagesasrealwas6.7%and14.3
%respectively.Additionally,radiologistsratedtheoutput
oftheMedGANframeworkhighlywithameanscoreof
12
TABLEIII
Resultsofperceptualstudy
Method
(a)PET-CTtranslation
meanSDreal%
pix2pix
1.700.5310.00
MedGAN
3.220.59025.3
Groundtruth
3.810.39474.7
Method
(b)MRmotioncorrection
meanSDreal%
pix2pix
1.980.5140.00
MedGAN
2.810.5736.70
Groundtruth
3.870.33793.3
Method
(c)PETdenoising
meanSDreal%
pix2pix
1.730.5110.00
MedGAN
3.020.54214.3
Groundtruth
3.700.46185.7
3.22incomparisonto1.70achievedbypix2pixand3.81
bythegroundtruthimages.TheperformanceofMedGAN
wasalsointheremainingtwoapplications,where
MedGANachievedameanscoreof2.81incomparisonto
1.98,andascoreof3.02incomparisonto1.73bypix2pix
inMRmotioncorrectionandPETdenoising,respectively.
5Discussion
Inthiswork,MedGANwaspresentedasanend-to-end
frameworkformedicalimagetranslationtasks.MedGAN
incorporatesanewcombinationofnon-adversariallosses,
namelytheperceptualandstyle-contentlosses,ontop
ofanadversarialframeworktocapturethehighandlow
frequencycomponentsofthetargetimages.Theproposed
frameworkutilizestheCasNetarchitecture,agenerator
networkwhichprogressivelythetranslatedimage
viaencoder-decoderpairsinanend-to-endmanner.This
leadstohomogeneousandrealisticglobalstructuresas
wellastexturesanddetails.
AnanalysisperformedonthetaskofPETtoCT
translation,presentedin
Fig.4
and
TableI
,illustrated
thatMedGANsurpassestheperformanceofitsindividual
losscomponents.ThecGANframeworkresultsinthe
worstperformancebothqualitativelyandquantitatively.
Spally,theresultingCTimagesofthismethodhave
alargelynon-homogeneousglobalstructurecomparedto
thedesiredgroundtruthimages.Agoodexamplewould
beexaminingthebonestructuresofthenoseregionin
theresultantimages.Comparatively,theutilizationofthe
perceptuallossandthestyle-contentlossesresultedinan
overallimprovedperformance.However,itwasobserved
thatthestyle-contentlosseshaveamoretimpact
uponthequalityoftheresultantimages.Nevertheless,
thisimpactwasnotrinthequantitativeresults
in
TableI
wheretheperceptuallossexcelsincomparison.
Thismaybeattributedtothereportedfactthatthe
quantitativescoresmaynotalwaystheperceptual
qualityofhumanjudgement(
Zhangetal.
,
2018
).The
proposedMedGANframeworkcombinestheabovemen-
tionedbtsofindividuallosscomponents,asitjointly
ensuresglobalhomogeneityoftheresultantimagesand
enhancesthelevelofoutputdetails.Thisimprovement
isnotonlytheresultoftheincreasedcapacityprovided
bytheCasNetarchitecture.MedGAN-1G,withasingle
U-blockgenerator,alsosurpassesqualitativelyandquan-
titativelytheresultsofmodelsutilizingindividualloss
componentsandidenticalarchitectures.Furtheranalysis
oftheperformanceoftheCasNetarchitectureispresented
inAppendixA.
MedGANwasdirectlyappliedwithnoapplication-
spcmoonthreetranslationtasksinmedical
imaging:PETtoCTtranslation,correctionofmotion
artefactsinMRimagesandPETdenoising.Foreachof
thesetasks,theperformanceofMedGANwascompared
againstotherimagetranslationapproaches.Inthetaskof
PETtoCTtranslation,MedGANproducedrealisticand
homogeneousbonestructuresintheresultantCTimages
thatcloselymatchedthegroundtruthCTimagesand
surpassesvisuallythoseproducedbyID-CGANandFila-
sGAN.InthetaskofMRmotioncorrection,theresultant
syntheticMRimagesareartefact-freewithrealistictex-
turesandctures.Finally,inthetaskofPETimage
denoising,MedGANproducedsharpdenoisedimagesas
opposedtotheblurredresultsbytheothermethods.Qual-
itativecomparisonsarehighlysubjectiveandcannotbe
reliedsolelyupon.Nevertheless,quantitativeassessments,
givenin
TableII
,alsotheaboveconclusionswith
MedGANoutperformingtheotherapproachescrossthe
majorityoftheutilizedmetricsforthettranslation
tasks.AdditionalresultsarepresentedinAppendixB.
Also,aperceptualstudyconductedby5radiolo-
gistsillustratedthedelityofthetranslatedimagesby
MedGAN.ThequalityoftheoutputimagesbyMedGAN
wasratedbetween2
:
8

3
:
2outofascaleof4.For
reference,thegroundtruthimageswereratedbetween
3
:
7

3
:
8andimagesfromthepix2pixframeworkwere
ratedbetween1
:
7

2
:
0.Furthermore,asubsetof6

25%
oftheimagesinthestudyconvincedtheradiologistsinto
thinkingtheyaremorerealisticthegroundtruthimages.
Thisworkisnotfreefromlimitations,withfur-
therimprovementsessentialforpracticalapplicabilityof
MedGANinmedicalpost-processingtasks.Translationof
2Dslicesissubstantiallymorecomputationallyt
than3D.Thus,operatingin2Dwasadvantageousfor
thepurposeofthisworkasitenabledientexperi-
mentationsontlossfunctions,architecturesand
regularizationtechniques.However,volumetricinforma-
tionin3Ddataisessentialforthemajorityofmedical
tasks.Therefore,inthefuture,MedGANwillbeap-
propriatelyadaptedtooperateon3Dmedicalvolumes.
Moreover,medicalacquisitionstypicallyresultinmulti-
channelvolumes.Inthiswork,onlysingle-channelinputs
wereconsideredforcomputationalreasons.However,this
isdetrimentalfortaskssuchasMRmotioncorrection
wherethephaseinformationisimportantfortheaccurate
13
correctionofmotionartefacts.Inthefuture,weaimto
overcomethisdisadvantagebyexpandingtheMedGAN
frameworktoaccommodatemulti-channelinputs.Finally,
themainpurposeoftheMedGANframeworkisenhancing
technicalpost-processingtasksthatrequiregloballycon-
sistentimageproperties.Atthisstage,itisnotsuitable
fordiagnosticapplications.Futureresearchwillbe
directedtowardsinvestigatingthepossibilityofreaching
diagnosticquality.
6Conclusion
MedGANisanewend-to-endframeworkformedi-
calimagetranslationtasks.Itcombinestheconditional
adversarialframeworkwithanewcombinationofnon-
adversariallossesandaCasNETgeneratorarchitecture
toenhancetheglobalconsistencyandhighfrequency
detailsofresults.MedGANwasappliedwithnotask-
spmoonthreechallengingtasksinmedical
imaging:PET-CTtranslation,MRmotioncorrectionand
PETdenoising.Theproposedframeworkoutperformed
othersimilartranslationapproachesquantitativelyand
qualitativelyacrossthetproposedtasks.Finally,
thesubjectiveperformanceandyofMedGAN'sre-
sultswerepositivelyattestedby5experiencedradiologists.
Futuretswillbedirectedfortheextensionof
MedGANtoaccommodate3Dmulti-channelvolumes.
Additionally,theperformanceofMedGANintechnical
post-processingtaskswillbeinvestigated.Forinstance,
theutilizationofsyntheticallytranslatedCTimagesfor
theattenuationcorrectionofPETvolumes.Also,weplan
toexploretheapplicabilityofutilizingretrospectively
correctedMRimagesinalargecohortforsegmentation
andorganvolumecalculation.
References
Bahrami,K.,Shi,F.,Rekik,I.,Shen,D.,2016.Convolutionalneural
networkforreconstructionof7T-likeimagesfrom3TMRIusing
appearanceandanatomicalfeatures.In
InternationalConference
OnMedicalImageComputingandComputerAssistedInterven-
tion
,pages39{47.
Ben-Cohen,A.,Klang,E.,Raskin,S.P.,S.,Ben-Haim,S.,
Konen,E.,Amitai,M.M.,Greenspan,H.,2018.Cross-modality
synthesisfromCTtoPETusingFCNandGANnetworksfor
improvedautomatedlesiondetection.
arXivpreprint
.URL
http://arxiv.org/abs/1802.07846
.
Borji,A.,2018.ProsandconsofGANevaluationmeasures.
arXiv
preprint
.URL
http://arxiv.org/abs/1802.03446
.
Chen,H.,Zhang,Y.,K.Kalra,M.,Lin,F.,Chen,Y.,Liao,P.,Zhou,
J.,Wang,G.,2017.Low-DoseCTwitharesidualencoder-decoder
convolutionalneuralnetwork.In
IEEETransactionsonMedical
Imaging
,volume36,pages2524{2535.
Colsher,J.G.,Hsieh,J.,Thibault,J.B.,Lonn,A.,Pan,T.,Lokitz,
S.J.,Turkington,T.G.,2008.UltralowdoseCtforattenuation
correctioninPET/CT.In
IEEENuclearScienceSymposium
ConferenceRecord
,pages5506{5511.
Dar,S.U.H.,Yurt,M.,Karacan,L.,Erdem,A.,Erdem,E.,C˘ukur,
T.,2018.Imagesynthesisinmulti-contrastMRIwithconditional
generativeadversarialnetworks.
arXivpreprint
.URL
http://
arxiv.org/abs/1802.01221
.
Dou,Q.,Chen,H.,Yu,L.,Qin,J.,Heng,P.A.,2017.Multi-level
contextual3-DCNNsforfalsepositivereductioninpulmonary
noduledetection.volume64,pages1558{1567.
Gatys,L.A.,Ecker,A.S.,Bethge,M.,2016.Imagestyletransfer
usingconvolutionalneuralnetworks.In
IEEEConferenceon
ComputerVisionandPatternRecognition
,pages2414{2423.
Goodfellow,I.J.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-
Farley,D.,Ozair,S.,Courville,A.C.,Bengio,Y.,2014.Generative
adversarialnets.In
ConferenceonNeuralInformationProcessing
Systems
,pages2672{2680.
Havaei,M.,Davy,A.,Warde-Farley,D.,Biard,A.,Courville,A.,
Bengio,Y.,Pal,C.,Jodoin,P.-M.,Larochelle,H.,2015.Brain
tumorsegmentationwithdeepneuralnetworks.In
MedicalImage
Analysis
,volume35,pages18{31.
He,K.,Zhang,X.,Ren,S.,Sun,J.,2016.Deepresiduallearningfor
imagerecognition.In
IEEEConferenceonComputerVisionand
PatternRecognition(CVPR)
,pages770{778.
Huynh,T.,Gao,Y.,Kang,J.,Wang,L.,Zhang,P.,Lian,J.,Shen,
D.,2016.EstimatingCTimagefromMRIdatausingstructured
randomforestandauto-contextmodel.In
IEEETransactionson
MedicalImaging
,volume35,pages174{183.
Isola,P.,Hesse,C.pix2piximplementation.
https://github.com/
yer/pix2pix-w
,2016.
Isola,P.,Zhu,J.,Zhou,T.,Efros,A.A.,2016.Image-to-image
translationwithconditionaladversarialnetworks.In
Conference
onComputerVisionandPatternRecognition
,pages5967{5976.
URL
http://arxiv.org/abs/1611.07004
.
Johnson,J.,Alahi,A.,Li,F.,2016.Perceptuallossesforreal-time
styletransferandsuper-resolution.In
EuropeanConferenceon
ComputerVision
,pages694{711.
Kamnitsas,K.,Ledig,C.,Newcombe,V.,P.Simpson,J.,Kane,
A.,Menon,D.,Rueckert,D.,Glocker,B.,2016.tmulti-
scale3DCNNwithfullyconnectedCRFforaccuratebrainlesion
segmentation.In
MedicalImageAnalysis
,volume36.
Karras,T.,Aila,T.,Laine,S.,Lehtinen,J.,2017.Progressive
growingofGANsforimprovedquality,stability,andvariation.
In
InternationalConferenceonLearningRepresentations
.URL
http://arxiv.org/abs/1710.10196
.
Kim,T.,Cha,M.,Kim,H.,Lee,J.K.,Kim,J.,2017.Learning
todiscovercross-domainrelationswithgenerativeadversarial
networks.In
InternationalConferenceonMachineLearning
.URL
http://arxiv.org/abs/1703.05192
.
Kingma,D.P.,Ba,J.,2014.Adam:Amethodforstochastic
optimization.In
InternationalConferenceonLearningRepresen-
tations
.URL
http://arxiv.org/abs/1412.6980
.
Kingma,D.P.,Welling,M.,2013.Auto-Encodingvariationalbayes.
In
arXivpreprint
.URL
https://arxiv.org/abs/1312.6114
.
Kustner,T.,Schwartz,M.,Martirosian,P.,Gatidis,S.,Seith,F.,
Gilliam,C.,Blu,T.,Fayad,H.,Visvikis,D.,Schick,F.,Yang,
B.,Schmidt,H.,Schwenzer,N.,2017.MR-basedrespiratoryand
cardiacmotioncorrectionforPETimaging.In
Medicalimage
analysis
,volume42,pages129{144.
Kustner,T.,Liebgott,A.,Mauch,L.,Martirosian,P.,Bamberg,F.,
Nikolaou,K.,Yang,B.,Schick,F.,Gatidis,S.,2018.In
Automated
reference-freedetectionofmotionartifactsinmagneticresonance
images
,volume31,pages243{256.URL
https://doi.org/10.1007/
s10334-017-0650-z
.
Larsen,A.,S˝nderby,S.,Larochelle,H.,Winther,O.,2016.Autoen-
codingbeyondpixelsusingalearnedsimilaritymetric.volume48
of
ProceedingsofMachineLearningResearch
,pages1558{1566.
LeCun,Y.,Bengio,Y.,Hinton,G.,2015.
DeepLearning
,Nature,vol-
ume521,pages436{44.URL
h
10.1038/nature14539
.
Ledig,C.,Theis,L.,Huszar,F.,Caballero,J.,Aitken,A.P.,Tejani,
A.,Totz,J.,Wang,Z.,Shi,W.,2016.Photo-realisticsingle
imagesuper-resolutionusingagenerativeadversarialnetwork.In
ConferenceonComputerVisionandPatternRecognition
.URL
http://arxiv.org/abs/1609.04802
.
Litjens,G.J.S.,Kooi,T.,Bejnordi,B.E.,Setio,A.A.A.,Ciompi,
F.,Ghafoorian,M.,vanderLaak,J.,vanGinneken,B.,anchez,
C.I.,2017.Asurveyondeeplearninginmedicalimageanalysis.
In
Medicalimageanalysis
,volume42,pages60{88.
Mark,O.,H.,S.J.,Anil,S.,A.,J.D.,2001.Highresolutiongel
dosimetrybyopticalCTandMRscanning.In
MedicalPhysics
,
volume28,pages1436{1445.
Miao,S.,Wang,Z.J.,Liao,R.,2016.ACNNregressionapproachfor
real-time2D/3Dregistration.In
IEEETransactionsonMedical
Imaging
,volume35,pages1352{1363.
Miyato,T.,Kataoka,T.,Koyama,M.,Yoshida,Y.,2018.Spectral
normalizationforgenerativeadversarialnetworks.In
Interna-
tionalConferenceonLearningRepresentations
.URL
http://
arxiv.org/abs/1802.05957
.
Nie,D.,Trullo,R.,Lian,J.,Wang,L.,Petitjean,C.,Ruan,S.,
Wang,Q.,Shen,D.,2018.Medicalimagesynthesiswithdeep
14
convolutionaladversarialnetworks.In
IEEETransactionson
BiomedicalEngineering
.
Oktay,O.,Bai,W.,Lee,M.C.H.,Guerrero,R.,Kamnitsas,K.,
Caballero,J.,deMarvao,A.,Cook,S.A.,O'Regan,D.P.,Rueck-
ert,D.,2016.Multi-inputcardiacimagesuper-resolutionusing
convolutionalneuralnetworks.In
InternationalConferenceOn
MedicalImageComputingandComputerAssistedIntervention
.
Pathak,D.,ahenbuhl,P.,Donahue,J.,Darrell,T.,Efros,A.A.,
2016.Contextencoders:Featurelearningbyinpainting.In
ConferenceonComputerVisionandPatternRecognition
.URL
http://arxiv.org/abs/1604.07379
.
Quan,T.M.,Nguyen-Duc,T.,Jeong,W.K.,2018.Compressed
sensingMRIreconstructionusingagenerativeadversarialnetwork
withacyclicloss.In
IEEETransactionsonMedicalImaging
,
volume37,pages1488{1497.
Radford,A.,Metz,L.,Chintala,S.,2016.Unsupervisedrepresenta-
tionlearningwithdeepconvolutionalgenerativeadversarialnet-
works.In
InternationalConferenceonLearningRepresentations
.
Rezende,D.J.,Mohamed,S.,Wierstra,D.,2014.Stochasticback-
propagationandapproximateinferenceindeepgenerativemodels.
In
Proceedingsofthe31stInternationalConferenceonMachine
Learning
,volume32,pages1278{1286.URL
http://proceedings.
mlr.press/v32/rezende14.html
.
Ronneberger,O.,Fischer,P.,Brox,T.,2015.U-net:Convolutional
networksforbiomedicalimagesegmentation.In
MedicalImage
ComputingandComputer-AssistedIntervention
,pages234{241.
Rueda,A.,Malpica,N.,Romero,E.,2012.Single-imagesuper-
resolutionofbrainMRimagesusingovercompletedictionaries.
In
MedicalImageAnalysis
,volume17,pages113{132.
Salimans,T.,Goodfellow,I.J.,Zaremba,W.,Cheung,V.,Radford,
A.,Chen,X.,2016.ImprovedtechniquesfortrainingGANs.In
ConferenceonNeuralInformationProcessingSystems
.
Shah,S.,Ghosh,P.,Davis,L.S.,Goldstein,T.StackedU-Nets:a
no-frillsapproachtonaturalimagesegmentation.
https://arxiv.
org/abs/1804.10343
,2018.arXivpreprint.
Sheikh,H.R.,Bovik,A.C.,2006.Imageinformationandvisual
quality.In
IEEETransactionsonImageProcessing
,volume15,
pages430{444.
Shin,H.C.,Roth,H.R.,Gao,M.,Lu,L.,Xu,Z.,Nogues,I.,
Yao,J.,Mollura,D.,Summers,R.M.,2016.Deepconvolutional
neuralnetworksforcomputer-aideddetection:CNNarchitectures,
datasetcharacteristicsandtransferlearning.In
IEEETransac-
tionsonMedicalImaging
,volume35,pages1285{1298.
Simonyan,K.,Zisserman,A.,2014.Verydeepconvolutionalnetworks
forlarge-scaleimagerecognition.
arXivpreprint
.URL
http://
arxiv.org/abs/1409.1556
.
Ulyanov,D.,Vedaldi,A.,Lempitsky,V.S.,2016.Instancenormal-
ization:Themissingingredientforfaststylization.
arXivpreprint
.
URL
http://arxiv.org/abs/1607.08022
.
Wang,C.,Xu,C.,Wang,C.,Tao,D.,2018.Perceptualadversarial
networksforimage-to-imagetransformation.In
IEEETransac-
tionsonImageProcessing
,volume27,pages4066{4079.
Wang,Z.,Bovik,A.C.,March2002.Auniversalimagequalityindex.
In
IEEESignalProcessingLetters
,volume9,pages81{84.
Wang,Z.,Bovik,A.C.,Sheikh,H.R.,Simoncelli,E.P.,2004.Image
qualityassessment:fromerrorvisibilitytostructuralsimilarity.In
IEEETransactionsonImageProcessing
,volume13,pages600{
612.
Wolterink,J.M.,Dinkla,A.M.,Savenije,M.H.F.,Seevinck,P.R.,
vandenBerg,C.A.T.,Isgum,I.,2017a.DeepMRtoCT
synthesisusingunpaireddata.In
InternationalConferenceon
MedicalImageComputingandComputerAssistedIntervention
.
URL
http://arxiv.org/abs/1708.01155
.
Wolterink,J.M.,Leiner,T.,A.Viergever,M.,Isgum,I.,2017b.
Generativeadversarialnetworksfornoisereductioninlow-dose
CT.In
IEEETransactionsonMedicalImaging
.
XinYi,P.B.,EktaWalia,2018.Generativeadversarialnetworkin
medicalimaging:Areview.
arXivpreprint
.URL
http://arxiv.
org/abs/1809.07294
.
Yang,G.,Yu,S.,Dong,H.,Slabaugh,G.,Dragotti,P.L.,Ye,X.,
Liu,F.,Arridge,S.,Keegan,J.,Guo,Y.,Firmin,D.,2018a.
DAGAN:deepde-aliasinggenerativeadversarialnetworksforfast
compressedsensingMRIreconstruction.In
IEEETransactions
onMedicalImaging
,volume37,pages1310{1321.
Yang,Q.,Li,N.,Zhao,Z.,Fan,X.,Chang,E.I.-C.,Xu,Y.,2018b.
MRIimage-to-imagetranslationforcross-modalityimageregistra-
tionandsegmentation.
arXivpreprint
.URL
https://arxiv.org/
abs/1801.06940
.
Yang,Q.,Yan,P.,Zhang,Y.,Yu,H.,Shi,Y.,Mou,X.,Kalra,
M.K.,Zhang,Y.,Sun,L.,Wang,G.,2018c.Low-doseCTimage
denoisingusingagenerativeadversarialnetworkwithwasserstein
distanceandperceptualloss.In
IEEETransactionsonMedical
Imaging
,volume37,pages1348{1357.
Yang,W.,Zhong,L.,Chen,Y.,Lin,L.,Lu,Z.,Liu,S.,Wu,Y.,Feng,
Q.,Chen,W.,2018d.PredictingCTimagefromMRIdatathrough
featurematchingwithlearnednonlinearlocaldescriptors.In
IEEE
TransactionsonMedicalImaging
,volume37,pages977{987.
Zhang,H.,Xu,T.,Li,H.,Zhang,S.,Huang,X.,Wang,X.,Metaxas,
D.N.,2016a.StackGAN:Texttophoto-realisticimagesynthesis
withstackedgenerativeadversarialnetworks.In
International
ConferenceonComputerVision
.URL
http://arxiv.org/abs/1612.
03242
.
Zhang,H.,Sindagi,V.,Patel,V.M.,2017.Imagede-rainingusinga
conditionalgenerativeadversarialnetwork.
arXivpreprint
.URL
http://arxiv.org/abs/1701.05957
.
Zhang,R.,Isola,P.,Efros,A.,2016b.Colorfulimagecolorization.
In
EuropeanConferenceonComputerVision
,pages649{666.
Zhang,R.,Isola,P.,Efros,A.A.,Shechtman,E.,Wang,O.,2018.
Theunreasonableenessofdeepfeaturesasaperceptual
metric.In
ConferenceonComputerVisionandPatternRecogni-
tion
.URL
http://arxiv.org/abs/1801.03924
.
Zhao,H.,Li,H.,Cheng,L.,2017.Synthesizingtarystructured
imageswithgans.
arXivpreprint
.URL
https://arxiv.org/abs/
1706.02185
.
Zhao,J.J.,Mathieu,M.,LeCun,Y.,2016.Energy-basedgenerative
adversarialnetwork.
arXivpreprint
.URL
https://arxiv.org/abs/
1609.03126
.
Zhong,L.,Lin,L.,Lu,Z.,Wu,Y.,Lu,Z.,Huang,M.,Yang,
W.,Feng,Q.,2016.PredictCTimagefromMRIdatausing
KNN-regressionwithlearnedlocaldescriptors.In
IEEE13th
InternationalSymposiumonBiomedicalImaging
,pages743{746.
Zhu,J.,Park,T.,Isola,P.,Efros,A.A.,2017.Unpairedimage-to-
imagetranslationusingcycle-consistentadversarialnetworks.In
InternationalConferenceonComputerVision
.URL
http://arxiv.
org/abs/1703.10593
.
15
AppendixA
AnalysisofGeneratorArchitecture
(a)PSNRandSSIMscores
(b)MSEandLPIPSscores
Fig.6:Quantitativeperformanceofapix2pixnetworkusingaCasNetwithavaryingnumberofU-blocks.
Input
pix2pix-1G
pix2pix-3G
pix2pix-5G
pix2pix-6G
MedGAN
Target
Fig.7:Comparisonofpix2pixwithaCasNetof1,3,5and6U-blocks,respectively,versustheMedGANframework.
ToinvestigatetheperformanceoftheproposedCasNet
architecture,weimplementedpix2pixmodelsutilizing
CasNetwithtblockdepth,from1to7U-blocks.
Quantitativeperformanceispresentedin
Fig.6
.Itcan
beseenthatastheCasNetutilizesgreatercapacity
throughalargerconcatenationofU-blocks,quantitative
performanceincreasestlyupuntilthe6th
U-block.Beyondthispoint,performanceeithersaturates,
e.g.PSNRandMSE,orstartstodegrade,inthe
caseofSSIMandLPIPSscores.Furtherinvestigations
arerequiredtodeterminetheoptimumdepthofCasNet.
Fig.7
illustratestheofCasNetonthetranslated
imagesbypix2pixoftCasNetdepthincomparison
toMedGAN(6U-blocks).Itcanbeseenthatasthe
numberofU-blocksincreasesvisualqualityoftranslated
imagesistlyimproved.
16
AppendixB
AdditionalResults
Input
pix2pix
PAN
ID-CGAN
Fila-sGAN
MedGAN
Target
(a)PET-CTtranslation
Input
pix2pix
PAN
ID-CGAN
Fila-sGAN
MedGAN
Target
(b)MRmotioncorrection
Input
pix2pix
PAN
ID-CGAN
Fila-sGAN
MedGAN
Target
(c)PETdenoising
Fig.8:AdditionalresultsforMedGANandothertranslationapproachesontheproposedmedicaltranslationtasks.
"
28,Multimodal Grounding for Language Processing,https://arxiv.org/pdf/1806.06371v2.pdf,https://github.com/UKPLab/coling18-multimodalSurvey,"MultimodalGroundingforLanguageProcessing
LisaBeinborn

3
TeresaBotschen

4
IrynaGurevych
4
3
LanguageTechnologyLab,UniversityofDuisburg-Essen
4
UbiquitousKnowledgeProcessingLab(UKP)andResearchTrainingGroupAIPHES
DepartmentofComputerScience,TechnischeUniversit
¨
atDarmstadt
www.ukp.tu-darmstadt.de
Abstract
Thissurveydiscusseshowrecentdevelopmentsinmultimodalprocessingfacilitateconceptual
groundingoflanguage.Wecategorizetheinformationowinmultimodalprocessingwithre-
specttocognitivemodelsofhumaninformationprocessingandanalyzedifferentmethodsfor
combiningmultimodalrepresentations.Basedonthismethodologicalinventory,wediscussthe
ofmultimodalgroundingforavarietyoflanguageprocessingtasksandthechallenges
thatarise.Weparticularlyfocusonmultimodalgroundingofverbswhichplayacrucialrolefor
thecompositionalpoweroflanguage.
TitleandAbstractinGerman
MultimodalekonzeptuelleVerankerungf
¨
urdieautomatischeSprachverarbeitung
Dieser
¨
Uberblicker
¨
ortert,wieaktuelleEntwicklungeninderautomatischenVerarbeitungmul-
timodalerInhaltediekonzeptuelleVerankerungsprachlicherInhalteerleichternk
¨
onnen.Die
automatischenMethodenzurVerarbeitungmultimodalerInhaltewerdenzun
¨
achsthinsichtlich
derzugrundeliegendenkognitivenModellemenschlicherInformationsverarbeitungkategorisiert.
DarausergebensichverschiedeneMethodenumRepr
¨
asentationenunterschiedlicherModalit
¨
aten
miteinanderzukombinieren.AusgehendvondiesenmethodischenGrundlagenwirddisku-
tiert,wieverschiedeneForschungsproblemeinderautomatischenSprachverarbeitungvonmulti-
modalerVerankerungk
¨
onnenundwelcheHerausforderungensichdabeiergeben.Ein
besondererSchwerpunktwirddabeiaufdiemultimodalekonzeptuelleVerankerungvonVerben
gelegt,dadieseeinewichtigekompositorischeFunktionerf
¨
ullen.
1Introduction
Naturallanguagesarecontinuallydevelopingconstructsthatincludenumerousvariationsandirregulari-
ties.Modelingthesubtletiesoflanguageinaformal,processablewayhasdrivencomputationallinguis-
ticsfordecades.Inrecentyears,distributionalapproacheshavebecomethemostwidelyacceptedsolu-
tiontomodeltheassociativecharacterofwordmeaning(Harris,1954;Collobertetal.,2011;Mikolovet
al.,2013;Penningtonetal.,2014).Theseapproacheslearnwordrepresentationsinahigh-dimensional
vectorspacebasedoncontextpatternsinlargetextcollections.Machinelearningresearchersaimat
reducingexternalknowledgetoanabsoluteminimumandsimplyinterpretlanguageasacontinuous
streamofcharacters.Fromanengineeringperspective,thesedata-drivenapproachesarehighlyattrac-
tivebecausetheyreducetheneedofdomainexperts.
Fromacognitiveperspective,processinglanguageinisolationwithoutinformationonsituationalcon-
textseemstobeanoverlysetup.Humanacquisitionofsemanticrepresentationsdoesnot
occurbasedonpurelanguageinput.Thetermconceptualgroundingreferstotheideathatlanguageis

TheworkbyLisaBeinbornhasbeencarriedoutduringherafwithUbiquitousKnowledgeProcessingLab(UKP)
andResearchTrainingGroupAIPHES,TechnischeUniversit
¨
atDarmstadt.

Theandthesecondauthorscontributedequallytothiswork.
ThisworkislicensedunderaCreativeCommonsAttribution4.0InternationalLicense.Licensedetails:
http://
creativecommons.org/licenses/by/4.0/
arXiv:1806.06371v2  [cs.CL]  3 Jul 2019(a)Cross-modaltransfer.Information
frommodality
A
isalignedtocompa-
rableinformationin
B
.
(b)Cross-modalinterpretation.Rel-
evantinformationinmodality
A
is
summarizedandinterpretedinmodal-
ity
B
.
(c)Jointmultimodalprocessing.Left:
Modality
A
and
B
bothcontributeto
ajointprediction.Right:Interac-
tiveexchangeofinformationbetween
modalities.
Figure1:Informationwinmultimodaltasks.Blueandyellowshapesrefertomodality
A
and
B
.
groundedinperceptualexperienceandsensorimotorinteractionswiththeenvironment(Barsalou,2008).
Initsstrictestinterpretation,thisembodiedperspectiveimpliesthatlanguageproductionandlanguage
comprehensioninvolveperceptualandmotorsimulationsofthedescribedsituation(Goldman,2006).
Animpressivenumberofrecentneuroimagingstudiesindicatethatprocessingawordactivatesareas
inthebrainthatcorrespondtotheassociatedsensorymodalityofitssemanticcategory:action-related
wordslike
kick
triggeractivityinthemotorcortexandobject-relatedwordslike
cup
activatevisualareas
(Pulverm
¨
ulleretal.,2005;GaragnaniandPulverm
¨
uller,2016).Whileitremainsacontroversialques-
tiontowhatextentconceptualrepresentationsareactuallysharedacrossmodalities(Louwerse,2011;
LeshinskayaandCaramazza,2016),ithasbeenwidelyacceptedthatconceptualandsensorimotorrep-
resentationsaretightlycoupledandinteractwitheachother.Cognitivelyplausiblelanguageprocessing
shouldthusinterpretlanguageasonemodalitywithinamultimodalenvironment.
Thissurveydiscusseshowrecentdevelopmentsinmultimodalprocessingfacilitateconceptualground-
ingoflanguage.Itintentstoprovideabridgebetweentheofmultimodalmachinelearning(Bal-
tru

saitisetal.,2017)andthecognitivetheoriesforgroundingdistributionalsemantics(Baroni,2016).As
thisisawideinterdisciplinarytopicwhichmanywefocusonmultimodalground-
ingforcomputationallinguistics.Forabetterunderstandingoftheinteractionbetweenmodalities,we
categorizemultimodaltasksaccordingtotheinformationwbetweenthemodalities.Inasecondstep,
weanalyzedifferentmethodsforcombiningmultimodalinformation.Basedonthismethodologicalin-
ventory,wediscusstheofmultimodalgroundingforavarietyoflanguageprocessingtasks.In
multimodalprocessing,groundingisusuallylimitedtoconcreteconceptsleadingtoareductionofrefer-
entialambiguity.Weprovideadetailedanalysisofthechallengesthatarisewhenmultimodalgrounding
isextendedtoopen-domainlanguagesettings.Weparticularlyfocusonmultimodalgroundingofverbs
whichisessentialfortheinterpretationofsequencesandtheofrelationsbetweenconcepts.
2Multimodalprocessingmodels
Thetermﬁmultimodalﬂhasbeenusedinabroadrangeofdifferentinterpretationseveninthecomputa-
tionallinguisticsliteraturealone.Inthecommoninterpretation,modalitiesrefertosensoryinputsuch
asaudio,vision,touch,smell,andtaste.Otherstretchoverdifferentcommunicativechannels
suchaslanguageandgesture,orsimplydifferentﬁmodesﬂofthesamemodality(e.g.,dayandnight
pictures).Inthissection,weanalyzethewofmultimodalinformationindifferentmultimodaltasks
exbythreemodalities:naturallanguageencodedastexts,visualsignalsencodedasimages
orvideos,andaudiosignalsencodedassoundForanoverviewofthechallengesandmachine
learningmethodsassociatedwitheachtask,theinterestedreaderisreferredtoBaltru

saitisetal.(2017).
Weproposeaofmultimodaltaskswithrespecttotheinformationwbetweenmodalities
intocross-modaltransfer,cross-modalinterpretation,andjointmultimodalprocessing.Fromahistori-
calperspective,progressinmultimodalprocessingcanbealignedwithcognitivetheoriesofmultimodal
organizationinthehumanbrain.
2.1Cross-modaltransfer
Inthe1980sand90s,cognitiveprocessingtheorieswereheavilybythetheoryofthemodu-
larityofmind(Fodor,1985).Itassumesthatprocessingoccursinencapsulatedmodules
thatdonotinteractwitheachother.Earlierapproachestomultimodalengineeringhavetakenasimilar
modularperspective.Theymodeltheinformationwineachmodalityseparatelyandtheout-
comeisthentransferredoralignedtoanothermodality.Wegrouptasksinwhichonemodalityservesas
theinterfacetoqueryorrepresentthecontentfromanothermodalityunderthecategoryofcross-modal
transfer,seeFigure1a.
Aclassicalexampleforcross-modaltransferaresearchandretrievaltasks.Thehumanuserprovides
anaturallanguagedescriptiontoqueryanartifact(i.e.,animage,video,oraudiofromadatabase
(Atreyetal.,2010).Thecross-modalalignmentbetweenthequeryandtheartifactrequiresqueryexpan-
sionanddisambiguationforreferentialindexing.Inspeech-relatedtransfertasks,textualcontentneeds
tobemappedtoaudiosamples.Speechsynthesistransformstextintogeneratedphonemes
foruserswhocannotread(Zenetal.,2009).Thereversetaskoftranscribingaudioandvideocontent
isaddressedbyapproachesforspeechrecognition(JuangandRabiner,2005)andsubtitlegeneration
(Daelemansetal.,2004).Forlipreadingtasks,mutevideoinputofpeoplespeakingistransformedinto
textrepresentingtheirutterances(Ngiametal.,2011).
Inthesecross-modaltransfertasks,synchronousprocessingoftheinputinonemodalityisnotdirectly
byinformationfromtheoutputmodality.Themainchallengeliesinappropriate
translationsoralignmentsfromonemodalitytotheother.Informationfromtheoutputmodalityis
mainlyusedforrerankingofinputhypotheses.Thisviewcorrespondstomentalmodelsofalanguage
hubinthebrainthatdoesnotdirectlyincorporateperceptualinformation(Chomsky,1986).
2.2Cross-modalinterpretation
Inordertoexplainhowhumanscanselectrelevantinformationfromperceptualinput,theconceptofat-
tentionhasbecomeverypopular.BridewellandBello(2016)arguethatattentionservesasﬂabottleneck
forinformationwinacognitivesystemﬂthatredirectsmentalresources.Inmultimodalprocessing,
theconceptofattentionasamediatorbetweenmodalitiesisrelevantforcross-modalinterpretation.For
thesetasks,thegoalistoobtainacompressedandstructuredintermediaterepresentationoftheinputto
generateausefulinterpretationinthetargetmodality.Attentionmechanisms(Bahdanauetal.,2014)are
usedfortheofrelevantinformation,seeFigure1b.
Atextualinterpretationofavisuallypresentedsceneisgeneratedinimagecaptioning(Xuetal.,
2015)andsketchrecognition(Lietal.,2015).Thegoalistoidentifyrelevantelements,groupindividual
elementstosemanticconcepts,identifyrelationsbetweenconcepts,andexpresstheserelationsinnatural
language.Theoutputsequenceisgeneratedwhilepayingattentiontodifferentsalientareasintheimage.
Toourknowledge,abidirectionalinformationwthatincludescuesfromthelanguagegeneration
moduleintheimagerecognitionprocesshasnotyetbeenimplemented.However,semanticinformation
couldhelptobetterdirecttheattentionforimagerecognition,e.g.,thegenerationofaverblike
eat
could
constrainthevisualrecognitiontoedibleobjectsasroles.Yatskaretal.(2016)proposethetaskof
situationrecognitiontoapproximatethisproblem.
Complementaryapproachesattempttogeneratevisualrepresentationstosummarizedocumentsand
presentthemostrelevantinformationinanintuitiveway(KucherandKerren,2015).Themostpop-
ularapproachareso-calledwordcloudswhicharefrequency-basedvisualizationsfortopicmodeling
(Batemanetal.,2008).Morerecentapproachesincludesemanticrelationsbetweenwordsforamore
conceptual-driveninterpretation(Xuetal.,2016).Conceptmapshighlightstructuralrelationsbetween
conceptsinagraph-basedvisualization(Zubrinicetal.,2012).Onekeychallengeforcross-modalin-
terpretationtasksliesintheevaluationoftheoutputbecauseinterpretationsarebysubjective
anddivergentsolutionscanbeequallyvalid.Accumulationsovervarioushumanratingsarecurrently
consideredtobebetterqualityapproximationsthananyautomaticmetrics(Vedantametal.,2015).
2.3Jointmultimodalprocessing
Duetoawaveofexperimentalthatsupportthecognitivetheoryofembodiedprocessing,thesep-
aratingaspectsbetweendifferentmodalitieshavebecomeblurred(Pulverm
¨
ulleretal.,2005).Asimilar
developmentcanbeobservedinmultimodalmachinelearning.Taskswhichexplicitlyrequirethecombi-
nationofknowledgefromdifferentmodalitiesgaverisetojointmultimodalprocessing(Figure1c).For
(a)Multimodalfusion.Concatenate
knownrepresentationsfrommodality
A
and
B
andapplydimensionalityre-
duction.
(b)Mapping.Learnamappingfunc-
tion
f
frommodality
A
to
B
thatcan
beappliedonunknownconcepts
c
n
.
(c)Jointlearning.Optimizetwoob-
jectivessimultaneously:qualityof
unimodalrepresentationsandcross-
modalalignment.
Figure2:Methodsforlearningmultimodalrepresentations.Blueandyellowshapesindicatetherepre-
sentationspaceofmodalityAandB.
emotionrecognition(Morencyetal.,2011)orpersuasivenessprediction(Santosetal.,2016),theactual
contentofanutteranceandparaverbalcues(e.g.,pitch,facialexpression)needtobejointlyevaluated.
Anironictoneofvoicemightreversetheconceptualinterpretationofthelanguagecontent.
Recentworkfromthevisioncommunitygoesonestepfurtherandtacklestasksthatimperativelyre-
quireaninteractivewofinformation.Invisualquestionanswering,ahumanusercanaskquestions
aboutanimagethatthesystemshouldanswer(Malinowskietal.,2015).Thisrequiresseveralsteps:
understandingthequestion,determiningthesalientelementsintheimage,interpretingtheimagewith
respecttothequestion,andgeneratingacoherentnaturallanguageanswerthatmatchesthequestion.
Forthistask,exchangeofinformationbetweenthemodalitiesiscrucial.Inanoverview,Wuetal.
(2017)compare29approachestovisualquestionanswering.23oftheseapproachesuseajointrepre-
sentationoftextualandvisualinformation.Theremaining6approachesorganizetheexchangeeither
throughacoordinatednetworkarchitectureorthroughsharedexternalknowledgebases.Novelinterac-
tiveapproachesmakeitpossibletodirectlymodulatetheinformationwinonemodalitybyinputfrom
anothermodality(Vriesetal.,2017)orbyhumanfeedback(LingandFidler,2017).
Themainchallengeforjointprocessingliesinefcombininginformationfromthemodalities,
sothatredundantinformationisintegratedwithoutlosingcomplementarycues.Inhumanlanguageun-
derstanding,thisprocessseemstobeperformedinaneffortlessandhighlyaccuratemanner(Crocker
etal.,2010).However,theunderlyingmechanismsofmultimodalrepresentationsremainpoorlyunder-
stood.InSection3,wediscussdifferentmethodsforobtainingjointrepresentationscomputationally.
3Multimodalrepresentationlearning
Multimodalrepresentationscombineinformationfromseparatemodalities.Wediscussmethodsfor
representingknownconcepts,projectinginformationtorepresentunknownconcepts,andforcombining
conceptrepresentationsintocompositionalrepresentationsforsequences.
3.1Conceptrepresentations
Eveninunimodaltasks,researchersexperimentwithmanydifferentvariationsofrepresentingconcepts
andtheirrelations.Earlierworkonmultimodalrepresentationsusedhuman-elicitedvisualfeatures(Sil-
bererandLapata,2012;RollerandSchulteImWalde,2013).Conveniently,integratingknowledgefrom
differentmodalitieshasbeenfacilitatedduetothenowcommonlow-levelrepresentationsoftheinput
(alsoknownasembeddings).Imagesarerepresentedasgroupsofpixels,videosasseriesofimage
frames,audiodataaswindowsofwaveformsamples,andlanguageassequencesofdistributionalword
representations.Thesevaluesarethenfedintoaneuralnetworkthatlearnstocompressandnormalize
therepresentationsuchthatitbettergeneralizesacrossinputsamples(KielaandBottou,2014).Acon-
ceptrepresentationisusuallyobtainedbyaveragingovermanydifferentsamplesfortheconcept(e.g.,
theconcept
bird
isrepresentedbyaveragingovertherepresentationfor
n
imagesshowingabird).The
representationsareexpressedashigh-dimensionalmatriceswhichcanbeprojectedintoacommonspace.
Thisapproachfacilitatesajointinformationwbetweendifferentmodalitiesandhascontributedtothe
growingsuccessofmultimodalprocessing.
Fusion
Themostintuitiveapproachismultimodalfusion(Figure2a).Assumingthataunimodalvec-
torrepresentation
v
fortheconcept
c
andthemodalities
m
1
and
m
2
exists,themultimodalrepresentation
v
mm
consistsoftheconcatenation
a
ofthetwovectorsweightedbyatunableparameter

:
v
mm
(
c
)=


v
m
1
(
c
)
a
(1


)

v
m
2
(
c
)
.
Theconcatenationoccursdirectlyontheconceptlevelandisthuscalledfeature-levelfusionorearly
fusion(LeongandMihalcea,2011;Brunietal.,2011).Inthecaseofpureconcatenation,theunimodal
representationsresideinseparateconceptualspaces.Theconcatenatedrepresentationfor
cat
couldgive
ustheinformationthat
cat
isvisuallysimilarto
panther
andtextuallysimilarto
dog
,butitisnotpossible
todeterminecross-modalsimilarity.Inordertosmooththeconcatenatedrepresentationswhilemaintain-
ingmultimodalcorrelations,dimensionalityreductiontechniquessuchassingularvaluedecomposition
(Brunietal.,2014)orcanonicalcorrelationanalysis(SilbererandLapata,2012)havebeenapplied.
3.2Projection
Inpractice,conceptsthathavearepresentationinonemodalityarenotnecessarilycoveredbyrepresen-
tationsinanothermodality.Theprojectionofunseenconceptsisknownaszeroshotlearning.Itcan
eitherbeperformedonamappedorajointrepresentationalspace.
Mapping
Toovercomethelackofrepresentationsforonemodality,severalresearchersproposedto
maprepresentationsfromonemodalitytotheother(Figure2b).Theideaistolearnamappingfunction
f
from
m
1
to
m
2
thatmaximizesthesimilaritybetweenaknownrepresentationof
c
in
m
2
andits
projectionfromtherepresentationin
m
1
:
c
m
2
˘
f
(
c
m
1
)
.
Thechoiceofthesimilarityandthelossmeasuresforlearningthemappingfunctionvary.Amax-margin
optimizationwhichmaximizesthesimilaritybetweentruepairsofconceptrepresentations
(
c
m
1
;c
m
2
)
andminimizesthesimilarityforpairswithrandomtargetrepresentations
(
c
m
1
;random
m
2
)
hasbeen
showntobeagoodchoiceforimagelabeling(Fromeetal.,2013).Inthistask,themappingapproach
isappliedintheimage-to-textdirectiontoclassifyunknownobjectsinimagesbasedontheirsemantic
similaritytoknownobjects(Socheretal.,2013).Lazaridouetal.(2014)andCollelletal.(2017)
proceedinthereversetext-to-imagedirectiontogroundwordsinthevisualworld.Similarpropagation
approacheshadalreadybeenexaminedbyJohnsandJones(2012)andHillandKorhonen(2014),but
theyusedhuman-elicitedperceptualfeaturesfromtheMcRaedataset(McRaeetal.,2005)insteadof
automaticallyderivedimagerepresentations.
Jointlearning
Themappingapproachesassumeadirectedtransformationfromonemodalitytothe
other.Jointestimationapproachesaimtolearnsharedrepresentationsinstead(Figure2b).Anapproach
inspiredbytopicmodelinginterpretsaligneddataasamultimodaldocumentandusesLatentDirich-
letAllocationtoderivemultimodaltopics(Andrewsetal.,2009;FengandLapata,2010;Silbererand
Lapata,2012;RollerandSchulteImWalde,2013).Unfortunately,thisapproachcannotbeeasilyused
forzeroshotlearning.Lazaridouetal.(2015)enrichtheskip-grammodelbyMikolovetal.(2013)
withvisualfeatures.Theirmodeloptimizestwoconstraints:therepresentationofconcepts
c
withre-
specttotheirtextualcontexts(unsupervisedskip-gramobjectivein
m
1
)andthesimilaritybetweenword
representationsandtheirvisualcounterparts(supervisedmax-marginobjectivefor
(
c
m
1
;c
m
2
)
).Intheir
approach,thevisualrepresentationsremained,butthetextualrepresentationsarelearnedfromscratch.
SilbererandLapata(2014)goonestepfurtherandusestackedmultimodalautoencoderstosimultane-
ouslylearngoodrepresentationsforeachmodality(unsupervisedreconstructionobjectivefor
m
1
and
m
2
)andtheiroptimalmultimodalcombination(supervisedobjectivefor
(
c
m
1
;c
m
2
)
).Both
approachesimplicitlyalsolearnamappingbetweenthetwomodalitiesandcanbeadjustedtoinducea
directionalprojectionforzeroshotlearning.Jointlearningofmultimodalrepresentationsisverypopular
inthevisioncommunity(Karpathyetal.,2014;SrivastavaandSalakhutdinov,2012;Ngiametal.,2011).
3.3Compositionalrepresentations
Fortasksthatrequirerepresentinglongersequences,ana
¨
eapproachissequence-levelfusion.Inthis
setting,theunimodalsequencerepresentationisobtainedbyperforminganarithmeticoperation(e.g.,
average,max)overtheconceptrepresentationsforeachwordinthesequence.Multimodalfusionisthen
performedonthisaveragedrepresentation(Glava

setal.,2017;Brunietal.,2014).Shutovaetal.(2016)
workwithshortphrasesconsistingoftwowordsanddirectlylearnphraserepresentations.Missing
conceptrepresentationsinonemodalitycanbeobtainedbymappingfunctions(Botschenetal.,2018).
Forimagecaptioningapproaches,representationsforapairofanimageandthecorrespondingcap-
tionarelearnedjointly(Kirosetal.,2014;Socheretal.,2014).Pre-trainedunimodalrepresentations
arefedintoaneuralnetworkwhichistrainedwiththemax-marginobjectivetodistinguishbetweentrue
andfalsecaptionsforanimage.Themultimodalsequencerepresentationcanbeobtainedfromthelast
hiddenlayerofthenetwork.Theintroductionofattentionvariablescanfunctionasamediatorbetween
thevisualandthetextualmodality(seeSection2.2).Foramoredetailedoverviewofmultimodalse-
quencerepresentationsinthevisioncommunity,theinterestedreaderisreferredtoWuetal.(2017).The
approachesforcompositionalrepresentationshavefocusedonenrichingnounandadjectivemeaning
multimodally.Themultimodalinterpretationofverbsasanintegralpartofcompositionalsequenceshas
notyetbeenthoroughlyexamined.
4Multimodalgroundingforlanguageprocessing
Theprogressinjointmultimodalprocessingandtheincreasingavailabilityofmultimodaldatasetsand
representationsopenupnewpossibilitiesforgroundedapproachestolanguageprocessing.Wereview
recentworksforgroundingconcepts,groundingphrases,andgroundinginteraction.Thechallengesthat
arisefromtheseeffortsarediscussedinSection5.
4.1Groundingconcepts
Multimodalconceptrepresentationsaremotivatedbytheideathatsemanticrelationsbetweenwords
aregroundedinperception.Beingabletoassesssemanticrelationsbetweenconceptsisanimportant
prerequisiteformodelinggeneralizationcapabilitiesinlanguageprocessing.Thecombinationofthe
textualandthevisualmodalityhasreceivedmostattentionforconceptualgrounding,butperceptual
informationfromtheauditoryandtheolfactorychannelhavealsobeenusedfordedicatedtasks(Kiela
etal.,2015;KielaandClark,2017).Inordertoprovideamoreconcretediscussion,wefocusonthe
combinationoftextualandvisualcuesfortheremainderofthesurvey.
Thequalityofconceptrepresentationsiscommonlyevaluatedbytheirabilitytomodelsemanticprop-
erties.Differentapproachestolearningconceptualmodelsarecomparedbytheirperformanceonsimi-
laritydatasets,e.g.,
WordSim353
(Finkelsteinetal.,2002),
SimLex-999
(Hilletal.,2015),
MEN
(Bruni
etal.,2012),
SemSim
,and
VisSim
(SilbererandLapata,2014)).Thesedatasetscontainpairsofwords
thathavebeenannotatedwithsimilarityscoresforthetwoconcepts.Severalevaluationsofsemantic
modelshaveshownthatmultimodalconceptrepresentationsoutperformunimodalones(FengandLap-
ata,2010;SilbererandLapata,2012;Brunietal.,2014;Kielaetal.,2014).Kielaetal.(2016)perform
acomparisonofdifferentimagesourcesandarchitecturesandtheirabilitytomodelsemanticsimilar-
ity.Despitetheadvantagesofmultimodalmodelsincapturingsemanticrelations,itremainsanopen
questionwhethertheycontributetoacognitivelymoreplausibleapproximationofhumanconceptual
grounding.Bulatetal.(2017b)andAndersonetal.(2017)conductexperimentstolabelbrainactivity
scansbyhumansubjectswiththecorrespondingconceptsthatelicitedthebrainactivity.Theycompare
differentdistributionalsemanticmodelsandobtainmixedresults.Bulatetal.(2017b)thatvisual
informationisformodelingconcretewords,whereasAndersonetal.(2017)concludethattex-
tualmodelssufintegratevisualproperties.Furtherinterdisciplinaryresearchinvolvingcomputer
science,neuroscience,andpsycholinguisticsisrequiredtoobtainadeeperunderstandingofcognitively
plausiblelanguageprocessing(EmbickandPoeppel,2015).
4.2Groundingphrases
Mostexperimentsforconceptualgroundingindicatethatprovidingamultimodalrepresentationforab-
stractconceptsismorechallengingduetothelackofperceptualpatternsassociatedwith
abstractwords(Hilletal.,2014).Forgroundingphrases,themeaningforconcreteandabstractconcepts
needtobecombined(seeSection3.3).Brunietal.(2012)examinethecompositionalmeaningofcolor
adjectivesandthatmultimodalrepresentationsaresuperiorinmodelingcolor.However,theyfailto
distinguishbetweenliteralandnon-literalusageofcoloradjectives(e.g.,
greencup
vs
greenfuture
).
Vividimageryandsynaestheticassociationsplayanimportantroleintheinterpretationofve
language.Intheirtheoryofmetaphor,LakoffandJohnson(1980)arguethatabstractconcepts
canbegroundedmetaphoricallyinembodiedandsituatedknowledge.Theyassumethatmetaphorscan
beunderstoodasamappingfromaconcretesourcedomaintoamoreabstracttargetdomain.Forex-
ample,
time
isoftenviewedasa
stream
that

inadirection.Turneyetal.(2011)operationalizethis
theoreticalaccountbyidentifyingmetaphoricphrasesbasedonthediscrepancyinconcretenessofsource
andtargetterm.Shutovaetal.(2016)andBulatetal.(2017a)buildontheirapproachandusemultimodal
modelsforidentifyingmetaphoricwordusageinadjective-nouncombinations.Theyshowthatwords
usedinametaphoricalcombination(
drywit
)exhibitlesssimilaritythanwordsinnon-metaphorical
phrases(
dryskin
).Westronglybelievethatprogressinmultimodalcompositionalgroundingwillpave
thewayforamoreholisticunderstandingofvelanguageprocessing.Asaprerequisite,multi-
modalgroundingneedstobeexaminedbeyondtherepresentationofconcreteobjects(seeSection5.2).
Representingverbs,compositionalphrases,andevenfullsentencesbymeansofmultimodalinformation
hasnotyetbeensufexamined.
4.3Groundinginteraction
Theoriginsofgroundingtheorieswereinitiatedtoaccountforsituationallanguageuseandinterac-
tion.Wedistinguishtwomainscenariosforinteractivelanguageuse:languagelearningandsituational
groundingofactiondescriptions.
Groundedlanguagelearning
Languagelearningisdeeplyrootedinsocialinteractionandinitially
emergeswithrespecttoaconcretereferentialcontext(Tomasello,2010).Childrenacquirelanguage
ininteractionwiththeirparentsandforeignlanguagelearningproceedsmuchfasterinanenvironment
thatforcesthelearnertointeractintheforeignlanguage(Nation,1990).Usage-basedapproachesto
languagelearningthataccountforthefrequencyandthequalityofthelanguagestimulushavealong
tradition(DaleandChall,1948).BrysbaertandNew(2009)haveshownthatfrequencyinformation
groundedinauditoryandvisualcommunicativecuescanbettermodelhumanprocessingeffectsthan
frequencyinformationextractedfrompurelytextualcorpora.Lazaridouetal.(2016)showthatamul-
timodaldistributionalapproachbetterapproximateswordlearningfrominteractivechild-directedinput
thanunimodalapproaches.Thesamemodelcanalsoconvincinglysimulatewordmeaninginductionby
adults(Lazaridouetal.,2017a).Psycholinguisticresearchindicatesthatconceptualmappingmodulated
byvisualpropertiesisnotonlyrelevantforlanguageacquisition,butisalsousedasameanstoes-
tablishcross-linguallinksinforeignlanguagelearning(Beinbornetal.,2014).BergsmaandVanDurme
(2011)andVuli
´
cetal.(2016)takeadvantageofthisobservationandusemultimodalrepresentationsto
inducemultilingualrepresentations.
Groundingsequencesinactions
Situationalgroundingofactiondescriptionsrequirestherepresenta-
tionofsequencesandtheircompositionalinterpretation.Regnerietal.(2013)buildacorpusthatgrounds
descriptionsofactionsinvideosshowingtheseactions.Fortheinterpretationofsequences,evaluating
verbsandtheirargumentsplaysafundamentalrole.Yatskaretal.(2016)developedthe
imSitu
dataset
whichconsistsofimagesdepictingverbsandannotationswhichlinktheverbargumentstovisualref-
erents.Thisdatasetcanbeusedforthemultimodaltaskofsituationrecognition(MallyaandLazebnik,
2017;ZellersandChoi,2017),anditservesasamultimodalresourceforverbprocessing.Ground-
ingverbsisparticularlychallengingbecauseofthevarietyoftheirpossiblevisualinstantiations.For
example,animageofanadultdrinkingbeerhasverylittleincommonwithazebradrinkingwater.
Multimodalinterpretationofsequencesishighlyrelevantforroboticsresearch(Chaplotetal.,2017).
MordatchandAbbeel(2017)examinetheemergenceofcompositionalityingroundedmulti-agentcom-
munication.Thelanguagelearnedbyagentsisnotnecessarilyinterpretablebyhumans.Lazari-
douetal.(2017b)showthatagentswhichdeveloptheirownlanguageforrepresentingconceptsthatare
groundedinimagesinfersimilartaxonomicrelationsashumans.Theirworksuggeststhatthelearned
conceptscanevenbemappedbackintonaturallanguage.Agent-agentcommunicationhasalreadybeen
examinedinthetalkingheadexperiments,inwhichtwoagentslearntodiscriminatebetweenobjectsand
developtheirownlanguageofreferringexpressions(SteelsandVogt,1997;Steels,2002).Hermannet
al.(2017)andHeinrichandWermter(2018)explicitlyfocusonhuman-robotinteractionandtraintheir
agenttoassociatenaturallanguagedescriptionsofactionswithperceptualinputfromitssensors.
Forexperimentsongroundedlanguageunderstanding,thesituationalenvironmentisusually
ciallyrestrictedtoaverysmalldomain.Thissettingfacilitatestheanalysisofcompositional
expressionsandtheirreferentialinterpretationascomplexobjectdescriptionsoractionsequences.In
open-domainlanguageunderstanding,semanticdisambiguationisevenmorechallenging.Approaches
usingmultimodalinformationforthedisambiguationofconcepts(Xieetal.,2017),namedentities
(Moonetal.,2018),andsentences(Botschenetal.,2018;Shutovaetal.,2017)showpromisingten-
dencies,buttheunderlyingcompositionalprinciplesarenotyetunderstood.
5Challengesforgroundedlanguageprocessing
Multimodalgroundingoflanguagehasbeenalongstandinggoaloflanguageresearchers.Thediscussion
hasgainednewmomentumduetotherecentdevelopmentsinlearningdistributedmultimodalrepresen-
tations.Mostevaluationsindicatethatmultimodalrepresentationsareforavarietyoftasks,
butexplanatoryanalysesofthiseffectarestillinadevelopingphase.Inthissection,wediscussopen
challengesthatarisefromexistingwork.Forfuturework,weproposetoexaminemultimodalgrounding
beyondconcretenounsandadjectives.Inordertodothis,largermultimodaldatasetsencompassinga
widerrangeofwordclassesneedtobebuild.Thesedatasetswouldenableustoanalyzecompositional
representationsinmoredetailandtodevelopmoreelaboratemodelsofselectivemultimodalgrounding.
5.1Combiningcomplementaryinformation
Differentmodalitiescontributequalitativelydifferentconceptualinformation.Brunietal.(2014)argue
thathighlyrelevantvisualpropertiesareoftennotrepresentedbylinguisticmodelsbecausetheyaretoo
obvioustobeexplicitlymentionedintext(e.g.,birdshavewings,violinsarebrown).Textualmodels,on
theotherhand,provideabetterintuitionoftaxonomicandfunctionalrelationsbetweenconceptswhich
cannoteasilybederivedfromimages(CollellandMoens,2016).Ideally,multimodalrepresentations
shouldintegratethecomplementaryperspectivesforamorecoherentgroundedinterpretationoflan-
guage.Fromamoreskepticalperspective,Louwerse(2011)statesthatperceptualinformationisalready
sufencodedintextualcues.Inthiscase,thesuperiorperformanceofmultimodalrepresentations
thathasbeenestablishedbyseveralresearcherswouldmainlybeduetoamorerobustrepresentationof
highlyredundantinformation.TheresultsbySilbererandLapata(2014)andHilletal.(2014)support
theintuitiveassumptionthattextualrepresentationsbettermodeltextualsimilarityandvisualrepresen-
tationsbettermodelvisualsimilarity.Asthemultimodalmodelsimproveonbothsimilaritytasks,the
integrationofcomplementaryinformationseemstobesuccessful.Interestingly,bothevaluationsshow
thatsimplyconcatenatingthetwomodalitiesalreadyyieldsaquitecompetitivemodel.Thereported
havebeenevaluatedonmodelsworkingwithhuman-annotatedperceptualfeatures.Thesefea-
turesinherentlyrepresenttaxonomicknowledgethatcannotbedirectlyinferredfromvisualinput.It
remainsanopenquestiontowhichextentautomaticallyderivedimagerepresentationscancontribute
complementaryinformationwhencombinedwithtextualrepresentations.Mostmultimodalresearchto
datefocusesontherepresentationofindividualconcepts(nouns)andtheirproperties(adjectives).The
ofmultimodalrepresentationsforlanguagetasksgoingbeyondconceptsimilarityneedstobe
examinedinmoredetailfromboth,engineeringandtheoreticalperspectives.
Multimodalgroundingofverbs
Verbsplayafundamentalroleforexpressingrelationsbetweencon-
ceptsandtheirsituationalfunctionality(Hartshorneetal.,2014).Thedynamicnatureofverbsposesa
challengeformultimodalgrounding.Toourbestknowledge,onlyHilletal.(2014)andCollelletal.
(2017)considerverbsintheirevaluation.Theyreportthatresultsforverbsareworse,but
Figure3:IllustrationforthequalityofverbrepresentationsindicatedasSpearmancorrelationbetween
thecosinesimilarityofverbsandtheircorrespondingsimilarityratinginthe
SimVerb
dataset.
donotelaborateonthisWepresentstepstowardsaninvestigationofverbgrounding.
1
Fig-
ure3illustratethequalityofverbrepresentationsinthemostcommonpubliclyavailableapproachesfor
multimodalrepresentations.Inlinewithpreviouswork,thequalityoftherepresentationsisevaluatedas
theSpearmancorrelationbetweenthecosinesimilarityoftwoverbsandtheircorrespondingsimilarity
ratinginthe
SimVerb
dataset(Gerzetal.,2016).Wecomparethequalityof3498verbpairs
2
intextual
Gloverepresentations(Penningtonetal.,2014)andtwovisualdatasets:theGoogledatasetthatper-
formedbestinKielaetal.(2016)andhasthehighestcoveragefortheverbpairs(493pairs,14%)
3
and
the
imSitu
datasetwhichhasbeenintentionallydesignedforverb(354pairs,10%).The
resultsshowthatmodelswhichincludevisualinformationoutperformpurelytextualrepresentationsfor
knownconcepts.However,thegeneralqualityoftheverbrepresentationsismuchlowerthanthequality
reportedfornouns.Asaconsequence,themappingtounseenverbpairsyieldsunsatisfactoryresultsfor
thefull
SimVerb
dataset.Ourencouragingresultsforthe
imSitu
datasetindicatethatitisrecommended
todirectlyobtainvisualrepresentationsforverbsinsteadofprojectingthemeaning.Buildinglarger
multimodaldatasetswithafocusonverbsseemstobeapromisingstrandofresearchforfuturework.
5.2Imageabilityofabstractwords
Conceptualgroundingoflanguagecanbeintuitivelyperformedforconcretewordsthathavedirectref-
erentsinsensoryexperience.Brunietal.(2014)andHillandKorhonen(2014)showthatmultimodal
representationsareforevaluatingconcretewords,buthavelittletonoimpactontheevaluation
ofabstractwords.Projectingunseenconceptsintotherepresentationspacebasedontheirrelationsto
seenconceptsinanothermodalityprovidesanelegantmethodforzeroshotlearning,butitisquestion-
ablewhethermultimodalrelationsbetweenconcreteconceptsaresuftoinferrelationsbetween
abstractconcepts.Lazaridouetal.(2015)analyzeprojectedabstractwordsbyextractingthenearestvi-
sualneighborfromtheirmultimodalrepresentation.Theneighborswerepairedwithrandomimagesand
humanratersjudgedhowwelleachimagerepresentstheword.Thehypothesisthatconcreteobjectsare
morelikelytobecapturedadequatelybymultimodalrepresentationswasHowever,theyalso
provideillustratingexampleswhichrepresentabstractwordslike
together
or
theory
surprisinglywell.
Embodimentofverbs
Fromamultimodalperspective,verbscanbecategorizedaccordingtotheir
degreeofembodiment.Thismeasureindicatestowhichextentverbmeaningsinvolvebodilyexperience
(Sidhuetal.,2014).Weobtainembodimentratingsfor1163pairs.
4
Theclass
highembodiment
contains
pairslike
fall-dive
inwhichtheembodimentofbothverbscanbefoundinthehighestquartile(135
pairs),
lowembodiment
containspairswithembodimentratingsinthelowestquartile(81pairs)like
know-decide
.
5
Coherentwithpreviousworkonconcreteandabstractnouns(Hilletal.,2014),itcanbe
1
Thepre-trainedembeddingsandthescripttoreproduceourresultsareavailableforresearchpurposes:
https://github.com/UKPLab/coling18-multimodalSurvey.
2
Twopairshadtobeexcludedbecause
misspend
wasnotcoveredinthetextualrepresentations.
3
Thecoveragein
WN9-IMG
(Xieetal.,2017)andthedatasetusedbyCollelletal.(2017)islower.
4
https://psychology.ucalgary.ca/languageprocessing/node/22
.Weonlyincludeapair,ifanem-
bodimentratingisavailableforbothverbs.
5
Itshouldbenotedthatnotallinstancesofthetwoclassesarecoveredbythevisualrepresentations.Thesmallnumberof
instancesmighthaveanimpactonthecorrelationvalues.
seenthatvisualrepresentationsbettercapturethesimilarityofverbswithahighlevelofembodiment.
Themappedrepresentationsmaintainthissenseofembodiment,whereastheconcatenatedandfused
representationsbettercapturethesimilarityforverbsreferringtomoreconceptualactions.This
indicatesthatmultimodalinformationisnotequallyforallwords.
5.3Selectivemultimodalgrounding
Theexpressivepoweroflanguageisessentiallyduetoitscombinatorialcapabilities.Understanding
howtocombineconceptrepresentationstorepresentmulti-wordexpressionsorevenfullsentenceshas
beenaquestionofongoingresearchincomputationallinguisticsfordecades.Theinclusionofadditional
modalitiesfurthercomplicatesthisdebate.Glava

setal.(2017)andBotschenetal.(2018)obtainmulti-
modalsentencerepresentationsbyaveragingoverthemultimodalrepresentationsforeachword.They
reportimprovedresultsforthetasksofsentencesimilarityandframeOurcomparison
aboveindicatesthatthissuperiorperformanceismainlyduetoabetterrepresentationofconcepts.This
raisestheassumptionthatmultimodalgroundingshouldonlybeperformedonselectedwords.Glava

s
etal.(2017)proposetoconditiontheinclusionofvisualinformationontheprototypicalityofaconcept
asmeasuredbytheimagedispersionscore(Kielaetal.,2014).Thismeasurecalculatestheaverage
pairwisecosinedistanceinasetofimagestomodeltheassumptionthatanimagecollectionforan
abstractconceptlike
happiness
ismorediversethanforaconcreteconceptlike
ladder
.Lazaridouet
al.(2015)andHesseletal.(2018)proposealternativeconcretenessmeasuresbasedonthesameidea.
Unfortunately,thesemeasuresarehighlydependentontheimageretrievalalgorithmwhichmightbeop-
timizedtowardsobtainingadiverserangeofimages.Nevertheless,weassumethatselectivemultimodal
groundingconstitutesamoreplausibleapproachtosentenceprocessing.Somefunctionalwords(e.g.,
locativeexpressions)mightfrommultimodalinformation,butitcurrentlyremainsunclearhow
wordswithsyntacticfunctions(e.g.,coordinatingexpressions)shouldberepresentedvisually.
6Conclusion
Weanalyzedhowmultimodalprocessinghasdevelopedfromtransferbetweenencapsulatedmodalities
tointeractiveprocessingoverjointmultimodalrepresentations.Thesedevelopmentscontributetonew
avenuesofresearchforgroundedlanguageprocessing.Westronglybelievethattheintegrationofmul-
timodalinformationwillimproveourunderstandingofconceptualsemanticmodels,velanguage
processing,languagelearning,andsituatedinteraction.Imagedatasetsareoftenoptimizedtowardspro-
vidingavarietyofvisualinstantiations.Developingalgorithmsfordeterminingmoreprototypicalvisual
representationscouldcontributetobettergroundingofverbsandmightalsoserveasacriterionfor
selectivemultimodalgrounding.
Acknowledgements
ThisworkhasbeensupportedbytheDFG-fundedresearchtraininggroupﬁAdaptivePreparationof
InformationformHeterogeneousSourcesﬂ(AIPHES,GRK1994/1)atTechnischeUniversit
¨
atDarmstadt.
WethankFarazSaeedanforhisassistancewiththecomputationofthevisualembeddingsfortheimSitu
images.Wethanktheanonymousreviewersfortheirinsightfulcomments.
References
AndrewJAnderson,DouweKiela,StephenClark,andMassimoPoesio.2017.Visuallygroundedandtextual
semanticmodelsdifferentiallydecodebrainactivityassociatedwithconcreteandabstractnouns.
Transactions
oftheAssociationforComputationalLinguistics(TACL)
,5(1):17Œ30.
MarkAndrews,GabriellaVigliocco,andDavidVinson.2009.Integratingexperientialanddistributionaldatato
learnsemanticrepresentations.
Psychologicalreview
,116(3):463Œ498.
PradeepKAtrey,MAnwarHossain,AbdulmotalebElSaddik,andMohanSKankanhalli.2010.Multimodal
fusionformultimediaanalysis:asurvey.
Multimediasystems
,16(6):345Œ379.
DzmitryBahdanau,KyungHyunCho,andYoshuaBengio.2014.Neuralmachinetranslationbyjointlylearning
toalignandtranslate.
arXivpreprintarXiv:1409.0473
.
TadasBaltru

saitis,ChaitanyaAhuja,andLouis-PhilippeMorency.2017.Multimodalmachinelearning:Asurvey
andtaxonomy.
arXivpreprintarXiv:1705.09406
.
MarcoBaroni.2016.Groundingdistributionalsemanticsinthevisualworld.
LanguageandLinguisticsCompass
,
10(1):3Œ13.
LawrenceWBarsalou.2008.Groundedcognition.
AnnualReviewofPsychology
,59:617Œ645.
ScottBateman,CarlGutwin,andMiguelNacenta.2008.Seeingthingsintheclouds:theeffectofvisualfeatures
ontagcloudselections.In
ProceedingsofthenineteenthACMconferenceonHypertextandhypermedia
,pages
193Œ202.ACM.
LisaBeinborn,TorstenZesch,andIrynaGurevych.2014.Readabilityforforeignlanguagelearning:Theimpor-
tanceofcognates.
InternationalJournalofAppliedLinguistics
,165(2):136Œ162.
ShaneBergsmaandBenjaminVanDurme.2011.Learningbilinguallexiconsusingthevisualsimilarityoflabeled
webimages.In
ProceedingsoftheInternationalJointConferenceonIntelligence(IJCAI)
,volume22,
pages1764Œ1769.
TeresaBotschen,IrynaGurevych,Jan-ChristophKlie,HatemMoussellySergieh,andStefanRoth.2018.Mul-
timodalframewithmultilingualevaluation.In
Proceedingsofthe16thAnnualConferenceof
theNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies
(NAACL)
,pages1481Œ1491.AssociationforComputationalLinguistics.
WillBridewellandPaulFBello.2016.Atheoryofattentionforcognitivesystems.
AdvancesinCognitive
Systems
,4:1Œ16.
EliaBruni,GiangBinhTran,andMarcoBaroni.2011.Distributionalsemanticsfromtextandimages.In
Proceed-
ingsoftheGEMSworkshopongeometricalmodelsofnaturallanguagesemantics
,pages22Œ32.Association
forComputationalLinguistics.
EliaBruni,GemmaBoleda,MarcoBaroni,andNam-KhanhTran.2012.Distributionalsemanticsintechnicolor.
In
Proceedingsofthe50thAnnualConferenceoftheAssociationforComputationalLinguistics(ACL)
,pages
136Œ145.AssociationforComputationalLinguistics.
EliaBruni,Nam-KhanhTram,andMarcoBaroni.2014.Multimodaldistributionalsemantics.
Journalof
IntelligenceResearch
,49:1Œ47.
MarcBrysbaertandBorisNew.2009.MovingbeyondKu

ceraandFrancis:Acriticalevaluationofcurrentword
frequencynormsandtheintroductionofanewandimprovedwordfrequencymeasureforAmericanEnglish.
Behaviorresearchmethods
,41(4):977Œ990.
LuanaBulat,StephenClark,andEkaterinaShutova.2017a.Modellingmetaphorwithattribute-basedsemantics.
In
Proceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics
(EACL)
,pages523Œ528.AssociationforComputationalLinguistics.
LuanaBulat,StephenClark,andEkaterinaShutova.2017b.Speaking,seeing,understanding:Correlatingseman-
ticmodelswithconceptualrepresentationinthebrain.In
ProceedingsoftheConferenceonEmpiricalMethods
inNaturalLanguageProcessing(EMNLP)
,pages1081Œ1091.
DevendraSinghChaplot,KanthashreeMysoreSathyendra,RamaKumarPasumarthi,DheerajRajagopal,andRus-
lanSalakhutdinov.2017.Gated-attentionarchitecturesfortask-orientedlanguagegrounding.
arXivpreprint
arXiv:1706.07230
.
NoamChomsky.1986.
Knowledgeoflanguage:Itsnature,origin,anduse
.GreenwoodPublishingGroup.
GuillemCollellandMarie-FrancineMoens.2016.Isanimageworthmorethanathousandwords?Onthe
semanticdifferencesbetweenvisualandlinguisticrepresentations.In
Proceedingsofthe26thInter-
nationalConferenceonComputationalLinguistics(COLING)
,pages2807Œ2817.
GuillemCollell,TedZhang,andMarie-FrancineMoens.2017.Imaginedvisualrepresentationsasmultimodal
embeddings.In
ProceedingsoftheThirty-FirstConferenceonIntelligence(AAAI)
,pages4378Œ4384.
RonanCollobert,JasonWeston,L
´
eonBottou,MichaelKarlen,KorayKavukcuoglu,andPavelKuksa.2011.
Naturallanguageprocessing(almost)fromscratch.
JournalofMachineLearningResearch
,12:2493Œ2537.
MatthewWCrocker,PiaKnoeferle,andMarshallRMayberry.2010.Situatedsentenceprocessing:Thecoordi-
natedinterplayaccountandaneurobehavioralmodel.
Brainandlanguage
,112(3):189Œ201.
WalterDaelemans,AnjaH
¨
othker,andErikFTjongKimSang.2004.Automaticsentencefor
subtitlingindutchandenglish.In
Proceedingsofthe4thInternationalConferenceonLanguageResourcesand
Evaluation(LREC)
,pages1045Œ1048.
EdgarDaleandJeanneSChall.1948.Aformulaforpredictingreadability:Instructions.
EducationalResearch
Bulletin
,27(2):37Œ54.
DavidEmbickandDavidPoeppel.2015.Towardsacomputational(ist)neurobiologyoflanguage:correlational,
integratedandexplanatoryneurolinguistics.
Language,CognitionandNeuroscience
,30(4):357Œ366.
YansongFengandMirellaLapata.2010.Visualinformationinsemanticrepresentation.In
Proceedingsof
the11thAnnualConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies(NAACL)
,pages91Œ99,Stroudsburg,USA.AssociationforComputationalLin-
guistics.
LevFinkelstein,EvgeniyGabrilovich,YossiMatias,EhudRivlin,ZachSolan,GadiWolfman,andEytanRuppin.
2002.Placingsearchincontext:Theconceptrevisited.In
Proceedingsofthe10thinternationalconferenceon
WorldWideWeb
,pages406Œ414.ACM.
JerryAFodor.1985.Precisofthemodularityofmind.
Behavioralandbrainsciences
,8(1):1Œ5.
AndreaFrome,GregSCorrado,JonShlens,SamyBengio,JeffDean,TomasMikolov,etal.2013.Devise:A
deepvisual-semanticembeddingmodel.In
AdvancesinNeuralInformationProcessingSystems(NIPS)
,pages
2121Œ2129.
MaxGaragnaniandFriedemannPulverm
¨
uller.2016.Conceptualgroundingoflanguageinactionandperception:
aneurocomputationalmodeloftheemergenceofcategoryandsemantichubs.
EuropeanJournalof
Neuroscience
,43(6):721Œ737.
DanielaGerz,IvanVuli
´
c,FelixHill,RoiReichart,andAnnaKorhonen.2016.SimVerb-3500:ALarge-Scale
EvaluationSetofVerbSimilarity.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP)
,pages2173Œ2182.
GoranGlava

s,IvanVuli
´
c,andSimonePaoloPonzetto.2017.Ifsentencescouldsee:Investigatingvisualinfor-
mationforsemantictextualsimilarity.In
Proceedingsofthe12thInternationalConferenceonComputational
Semantics(IWCS)
,Montpeiller,France.
AlvinIGoldman.2006.
Simulatingminds:Thephilosophy,psychology,andneuroscienceofmindreading
.Oxford
UniversityPress.
ZelligSHarris.1954.Distributionalstructure.
Word
,10(2-3):146Œ162.
JoshuaKHartshorne,ClaireBonial,andMarthaPalmer.2014.Theverbcornerproject:Findingsfromphase1
ofcrowd-sourcingasemanticdecompositionofverbs.In
Proceedingsofthe52ndAnnualConferenceofthe
AssociationforComputationalLinguistics(ACL)
,pages397Œ402.AssociationforComputationalLinguistics.
StefanHeinrichandStefanWermter.2018.Interactivenaturallanguageacquisitioninamulti-modalrecurrent
neuralarchitecture.
ConnectionScience
,30(1):99Œ133.
KarlMoritzHermann,FelixHill,SimonGreen,FuminWang,RyanFaulkner,HubertSoyer,DavidSzepesvari,
WojtekCzarnecki,MaxJaderberg,DenisTeplyashin,etal.2017.Groundedlanguagelearninginasimulated
3dworld.
arXivpreprintarXiv:1706.06551
.
JackHessel,DavidMimno,andLillianLee.2018.Quantifyingthevisualconcretenessofwordsandtopicsin
multimodaldatasets.
arXivpreprintarXiv:1804.06786
.
FelixHillandAnnaKorhonen.2014.Learningabstractconceptembeddingsfrommulti-modaldata:Sinceyou
probablycan'tseewhatimean.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP)
,pages255Œ265.
FelixHill,RoiReichart,andAnnaKorhonen.2014.Multi-modalmodelsforconcreteandabstractconcept
meaning.
TransactionsoftheAssociationforComputationalLinguistics(TACL)
,2:285Œ296.
FelixHill,RoiReichart,andAnnaKorhonen.2015.Simlex-999:Evaluatingsemanticmodelswith(genuine)
similarityestimation.
ComputationalLinguistics
,41(4):665Œ695.
BrendanTJohnsandMichaelNJones.2012.Perceptualinferencethroughgloballexicalsimilarity.
Topicsin
CognitiveScience
,4(1):103Œ120.
Biing-HwangJuangandLawrenceRRabiner.2005.AutomaticspeechrecognitionŒabriefhistoryofthetechnol-
ogydevelopment.
GeorgiaInstituteofTechnology.AtlantaRutgersUniversityandtheUniversityofCalifornia.
SantaBarbara
,1:67.
AndrejKarpathy,ArmandJoulin,andLiFFei-Fei.2014.Deepfragmentembeddingsforbidirectionalimage
sentencemapping.In
AdvancesinNeuralInformationProcessingSystems(NIPS)
,pages1889Œ1897.
DouweKielaandL
´
eonBottou.2014.LearningImageEmbeddingsusingConvolutionalNeuralNetworksforIm-
provedMulti-ModalSemantics.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP)
.
DouweKielaandStephenClark.2017.Learningneuralaudioembeddingsforgroundingsemanticsinauditory
perception.
JournalofIntelligenceResearch
,60:1003Œ1030.
DouweKiela,FelixHill,AnnaKorhonen,andStephenClark.2014.Improvingmulti-modalrepresentations
usingimagedispersion:Whylessissometimesmore.In
Proceedingsofthe52ndAnnualConferenceofthe
AssociationforComputationalLinguistics(ACL)
,volume2,pages835Œ841.AssociationforComputational
Linguistics.
DouweKiela,LuanaBulat,andStephenClark.2015.Groundingsemanticsinolfactoryperception.In
Proceed-
ingsofthe53ndAnnualConferenceoftheAssociationforComputationalLinguistics(ACL)
,pages231Œ236,
Beijing,China,July.AssociationforComputationalLinguistics.
DouweKiela,AnitaVer

o,andStephenChristopherClark.2016.ComparingDataSourcesandArchitecturesfor
DeepVisualRepresentationLearninginSemantics.In
ProceedingsoftheConferenceonEmpiricalMethodsin
NaturalLanguageProcessing(EMNLP)
,pages447Œ456.
RyanKiros,RuslanSalakhutdinov,andRichardSZemel.2014.Unifyingvisual-semanticembeddingswith
multimodalneurallanguagemodels.
arXivpreprintarXiv:1411.2539
.
KostiantynKucherandAndreasKerren.2015.Textvisualizationtechniques:Taxonomy,visualsurvey,and
communityinsights.In
PVisualizationSymposium(Pis)
,pages117Œ121.IEEE.
GeorgeLakoffandMarkJohnson.1980.
MetaphorsWeLiveBy
.UniversityofChicagopress.
AngelikiLazaridou,EliaBruni,andMarcoBaroni.2014.Isthisawampimuk?cross-modalmappingbetween
distributionalsemanticsandthevisualworld.In
Proceedingsofthe52ndAnnualConferenceoftheAssociation
forComputationalLinguistics(ACL)
,volume1,pages1403Œ1414.AssociationforComputationalLinguistics.
AngelikiLazaridou,NghiaThePham,andMarcoBaroni.2015.Combininglanguageandvisionwithamulti-
modalskip-grammodel.In
Proceedingsofthe14thAnnualConferenceoftheNorthAmericanChapterofthe
AssociationforComputationalLinguistics:HumanLanguageTechnologies(NAACL)
,pages153Œ163.Associ-
ationforComputationalLinguistics.
AngelikiLazaridou,GrzegorzRaquelFern
´
andez,andMarcoBaroni.2016.Multimodalsemanticlearn-
ingfromchild-directedinput.In
Proceedingsofthe15thAnnualConferenceoftheNorthAmericanChapter
oftheAssociationforComputationalLinguistics:HumanLanguageTechnologies(NAACL)
,pages387Œ392.
AssociationforComputationalLinguistics.
AngelikiLazaridou,MarcoMarelli,andMarcoBaroni.2017a.Multimodalwordmeaninginductionfromminimal
exposuretonaturaltext.
Cognitivescience
,41(S4):677Œ705.
AngelikiLazaridou,AlexanderPeysakhovich,andMarcoBaroni.2017b.Multi-agentcooperationandtheemer-
genceof(natural)language.
arXivpreprintarXiv:1612.07182
.
CheeWeeLeongandRadaMihalcea.2011.Goingbeyondtext:Ahybridimage-textapproachformeasuring
wordrelatedness.In
Proceedingsofthe5thInternationalJointConferenceonNaturalLanguageProcessing
(IJCNLP)
,pages1403Œ1407.
AnnaLeshinskayaandAlfonsoCaramazza.2016.Foracognitiveneuroscienceofconcepts:Movingbeyondthe
groundingissue.
Psychonomicbulletin&review
,23(4):991Œ1001.
YiLi,TimothyM.Hospedales,Yi-ZheSong,andShaogangGong.2015.Free-handsketchrecognitionbymulti-
kernelfeaturelearning.
ComputerVisionandImageUnderstanding
,137:1Œ11.
HuanLingandSanjaFidler.2017.Teachingmachinestodescribeimagesvianaturallanguagefeedback.In
AdvancesinNeuralInformationProcessingSystems(NIPS)
.
MaxM.Louwerse.2011.Symbolinterdependencyinsymbolicandembodiedcognition.
TopicsinCognitive
Science
,3(2):273Œ302.
MateuszMalinowski,MarcusRohrbach,andMarioFritz.2015.Askyourneurons:Aneural-basedapproachto
answeringquestionsaboutimages.In
ProceedingsoftheIEEEInternationalConferenceonComputerVision
(ICCV)
,December.
ArunMallyaandSvetlanaLazebnik.2017.Recurrentmodelsforsituationrecognition.In
Proceedingsofthe
IEEEConferenceonComputerVisionandPatternRecognition(CVPR)
,pages455Œ463.
KenMcRae,GeorgeSCree,MarkSSeidenberg,andChrisMcNorgan.2005.Semanticfeatureproductionnorms
foralargesetoflivingandnonlivingthings.
Behaviorresearchmethods
,37(4):547Œ559.
TomasMikolov,KaiChen,GregCorrado,andJeffreyDean.2013.Efestimationofwordrepresentationsin
vectorspace.
InternationalConferenceonLearningRepresentationsWorkshop(ICLR)
.
SeungwhanMoon,LeonardoNeves,andVitorCarvalho.2018.Multimodalnamedentityrecognitionforshort
socialmediaposts.In
Proceedingsofthe16thAnnualConferenceoftheNorthAmericanChapteroftheAsso-
ciationforComputationalLinguistics:HumanLanguageTechnologies(NAACL)
,pages852Œ860.Association
forComputationalLinguistics.
IgorMordatchandPieterAbbeel.2017.Emergenceofgroundedcompositionallanguageinmulti-agentpopula-
tions.
arXivpreprintarXiv:1703.04908
.
Louis-PhilippeMorency,RadaMihalcea,andPayalDoshi.2011.Towardsmultimodalsentimentanalysis:Har-
vestingopinionsfromtheweb.In
Proceedingsofthe13thinternationalconferenceonmultimodalinterfaces
,
pages169Œ176.ACM.
IanStephenPaulNation.1990.
TeachingandLearningVocabulary
.TeachingMethods.Heinle&Heinle.
JiquanNgiam,AdityaKhosla,MingyuKim,JuhanNam,HonglakLee,andAndrewYNg.2011.Multimodaldeep
learning.In
Proceedingsofthe28thInternationalConferenceonMachineLearning(ICML)
,pages689Œ696.
JeffreyPennington,RichardSocher,andChristopherManning.2014.Glove:Globalvectorsforwordrepresen-
tation.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
,
pages1532Œ1543.
FriedemannPulverm
¨
uller,OlafHauk,VadimV.Nikulin,andRistoJ.Ilmoniemi.2005.Functionallinksbetween
motorandlanguagesystems.
EuropeanJournalofNeuroscience
,21(3):793Œ797.
MichaelaRegneri,MarcusRohrbach,DominikusWetzel,StefanThater,BerntSchiele,andManfredPinkal.2013.
Groundingactiondescriptionsinvideos.
TransactionsoftheAssociationofComputationalLinguistics(TACL)
,
1:25Œ36.
StephenRollerandSabineSchulteImWalde.2013.Amultimodalldamodelintegratingtextual,cognitiveand
visualmodalities.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing
(EMNLP)
,pages1146Œ1157.
PedroBispoSantos,LisaBeinborn,andIrynaGurevych.2016.Adomain-agnosticapproachforopinionpredic-
tiononspeech.InMalvinaNissim,VivianaPatti,andBarbaraPlank,editors,
ProceedingsoftheWorkshopon
ComputationalModelingofPeoplesOpinions,Personality,andEmotionsinSocialMedia(PEOPLES)
,pages
163Œ172.
EkaterinaShutova,DouweKiela,andJeanMaillard.2016.Blackholesandwhiterabbits:Metaphor
withvisualfeatures.In
Proceedingsofthe15thAnnualConferenceoftheNorthAmericanChapteroftheAsso-
ciationforComputationalLinguistics:HumanLanguageTechnologies(NAACL)
,pages160Œ170.Association
forComputationalLinguistics.
EkaterinaShutova,AndreasWundsam,andHelenYannakoudakis.2017.Semanticframesandvisualscenes:
Learningsemanticroleinventoriesfromimageandvideodescriptions.In
Proceedingsofthe6thJointConfer-
enceonLexicalandComputationalSemantics(*SEM)
,pages149Œ154.
DavidMSidhu,RachelKwan,PennyMPexman,andPaulDSiakaluk.2014.Effectsofrelativeembodimentin
lexicalandsemanticprocessingofverbs.
Actapsychologica
,149:32Œ39.
CarinaSilbererandMirellaLapata.2012.Groundedmodelsofsemanticrepresentation.In
Proceedingsofthe
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
,pages1423Œ1433.Association
forComputationalLinguistics.
CarinaSilbererandMirellaLapata.2014.Learninggroundedmeaningrepresentationswithautoencoders.In
Proceedingsofthe52ndAnnualConferenceoftheAssociationforComputationalLinguistics(ACL)
,volume1,
pages721Œ732.AssociationforComputationalLinguistics.
RichardSocher,MilindGanjoo,ChristopherDManning,andAndrewNg.2013.Zero-shotlearningthrough
cross-modaltransfer.In
AdvancesinNeuralInformationProcessingSystems(NIPS)
,pages935Œ943.
RichardSocher,AndrejKarpathy,QuocVLe,ChristopherDManning,andAndrewYNg.2014.Grounded
compositionalsemanticsforanddescribingimageswithsentences.
TransactionsoftheAssociationfor
ComputationalLinguistics(TACL)
,2(1):207Œ218.
NitishSrivastavaandRuslanSalakhutdinov.2012.Learningrepresentationsformultimodaldatawithdeepbelief
nets.In
Internationalconferenceonmachinelearningworkshop
,volume79.
LucSteelsandPaulVogt.1997.Groundingadaptivelanguagegamesinroboticagents.In
Proceedingsofthe4th
europeanconferenceonlife
,volume97.
LucSteels.2002.Groundingsymbolsthroughevolutionarylanguagegames.In
Simulatingtheevolutionof
language
,pages211Œ226.Springer.
MichaelTomasello.2010.
Originsofhumancommunication
.MITpress.
PeterDTurney,YairNeuman,DanAssaf,andYohaiCohen.2011.Literalandmetaphoricalsense
throughconcreteandabstractcontext.In
ProceedingsoftheConferenceonEmpiricalMethodsinNatural
LanguageProcessing(EMNLP)
,pages680Œ690.AssociationforComputationalLinguistics.
RamakrishnaVedantam,CLawrenceZitnick,andDeviParikh.2015.Cider:Consensus-basedimagedescription
evaluation.In
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR)
,pages
4566Œ4575.
HarmDeVries,FlorianStrub,J
´
er
´
emieMary,HugoLarochelle,OlivierPietquin,andAaronC.Courville.2017.
Modulatingearlyvisualprocessingbylanguage.In
AdvancesinNeuralInformationProcessingSystems(NIPS)
,
pages6597Œ6607.
IvanVuli
´
c,DouweKiela,StephenClark,andMarie-FrancineMoens.2016.Multi-modalrepresentationsfor
improvedbilinguallexiconlearning.In
Proceedingsofthe54thAnnualConferenceoftheAssociationfor
ComputationalLinguistics(ACL)
,pages188Œ194.AssociationforComputationalLinguistics.
QiWu,DamienTeney,PengWang,ChunhuaShen,AnthonyDick,andAntonvandenHengel.2017.Visual
questionanswering:Asurveyofmethodsanddatasets.
ComputerVisionandImageUnderstanding
,163:21Œ
40.
RuobingXie,ZhiyuanLiu,HuanboLuan,andMaosongSun.2017.Image-embodiedknowledgerepresentation
learning.In
Proceedingsofthe26thInternationalJointConferenceonIntelligence(IJCAI)
,pages
3140Œ3146.
KelvinXu,JimmyBa,RyanKiros,KyunghyunCho,AaronCourville,RuslanSalakhudinov,RichZemel,and
YoshuaBengio.2015.Show,attendandtell:Neuralimagecaptiongenerationwithvisualattention.In
InternationalConferenceonMachineLearning
,pages2048Œ2057.
JinXu,YuboTao,andHaiLin.2016.Semanticwordcloudgenerationbasedonwordembeddings.In
P
VisualizationSymposium(Pis)
,pages239Œ243.IEEE.
MarkYatskar,LukeZettlemoyer,andAliFarhadi.2016.Situationrecognition:Visualsemanticrolelabelingfor
imageunderstanding.In
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
(CVPR)
,pages5534Œ5542.
RowanZellersandYejinChoi.2017.Zero-shotactivityrecognitionwithverbattributeinduction.In
Proceedings
oftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
,pages946Œ958.
HeigaZen,KeiichiTokuda,andAlanWBlack.2009.Statisticalparametricspeechsynthesis.
SpeechCommuni-
cation
,51(11):1039Œ1064.
KrunoslavZubrinic,DamirKalpic,andMarioMilicevic.2012.Theautomaticcreationofconceptmapsfrom
documentswrittenusingmorphologicallyrichlanguages.
Expertsystemswithapplications
,39(16):12709Œ
12718.
"
29,Initialization of ReLUs for Dynamical Isometry,https://arxiv.org/pdf/1806.06362v3.pdf,https://github.com/alinadubatovka/information_propagation,"InitializationofReLUsforDynamicalIsometry
RebekkaBurkholz
DepartmentofBiostatistics
HarvardT.H.ChanSchoolofPublicHealth
655HuntingtonAvenue,Boston,MA02115
rburkholz@hsph.harvard.edu
AlinaDubatovka
DepartmentofComputerScience
ETHZurich
Universitätstrasse6,8092Zurich
alina.dubatovka@inf.ethz.ch
Abstract
Deeplearningreliesongoodinitializationschemesandhyperparameterchoices
priortotraininganeuralnetwork.Randomweightinitializationsinducerandom
networkensembles,whichgiverisetothetrainability,trainingspeed,andsome-
timesalsogeneralizationabilityofaninstance.Inaddition,suchensemblesprovide
theoreticalinsightsintothespaceofcandidatemodelsofwhichoneisselected
duringtraining.Theresultsobtainedsofarrelyonmeanapproximations
thatassumelayerwidthandthatstudyaveragesquaredsignals.Wederive
thejointsignaloutputdistributionexactly,withoutmeanassumptions,for
fully-connectednetworkswithGaussianweightsandbiases,andanalyzedeviations
fromthemeanresults.Forlinearunits,wefurtherdiscusslimitations
ofthestandardinitializationscheme,suchasitslackofdynamicalisometry,and
proposeasimplealternativethatovercomesthesebyinitialparametersharing.
1Introduction
Deeplearningreliescriticallyongoodparameterinitializationpriortotraining.Twoapproachesare
commonlyemployed:randomnetworkinitialization[
4
,
7
,
14
]andtransferlearning[
26
](including
unsupervisedpre-training),whereanetworkthatwastrainedforadifferenttaskorapartofit
isretrainedandextendedbyadditionalnetworklayers.Whilethelattercanspeeduptraining
considerablyandalsoimprovethegeneralizationabilityofthenewmodel,itsbiastowardsthe
originaltaskcanalsohindersuccessfultrainingifthelearnedfeaturesbarelyrelatetothenewtask.
Randominitializationofparameters,meanwhile,requirescarefultuningofthedistributionsfrom
whichneuralnetworkweightsandbiasesaredrawn.Whileheterogeneityofnetworkparametersis
neededtoproducemeaningfuloutput,atoobigvariancecanalsodilutetheoriginalsignal.Toavoid
explodingorvanishinggradients,thedistributionscanbeadjustedtopreservesignalvariancefrom
layertolayer.Thisenablesthetrainingofverydeepnetworksbysimplestochasticgradientdescent
(SGD)withouttheneedofcomputationallyintensivecorrectionsasbatchnormalization[
8
]orvariants
thereof[
12
].Thisapproachisjustbythesimilarupdaterulesofgradientback-propagationand
signalforwardpropagation[
20
].Inadditiontotrainability,goodparameterinitializationsalsoseem
tosupportthegeneralizationabilityofthetrained,overparametrizednetwork.Accordingto[3],the
parametervaluesremainclosetotheinitializedones,whichhasaregularizationeffect.
Anearlyexampleofapproximatesignalvariancepreservationisproposedin[
4
]forfullyconnected
feedforwardneuralnetworks,animportantbuildingblockofmostcommonneuralarchitectures.
Inspiredbythosederivations,Heetal.[
7
]foundthatforlinearunits(ReLUs)andGaussian
weightinitialization
w
˘N
(
˙
2
)
theoptimalchoiceiszeromean

=0
,variance
˙
2
=2
=N
and
zerobias
b
=0
,where
N
referstothenumberofneuronsinalayer.Theseareby
meantheory,whichassumeswidenetworklayerstoemploythecentrallimittheorem
andfocusonnormaldistributions.Similarresultshavebeenobtainedfor
tanh
[
16
,
18
,
20
],residual
networkswithdifferentactivationfunctions[
24
],andconvolutionalneuralnetworks[
23
].Thesame
33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019),Vancouver,Canada.
arXiv:1806.06362v3  [stat.ML]  24 Oct 2019derivationsalsoleadtotheinsightthatwidefully-connectedneuralnetworksapproximately
learnthekernelofaGaussianprocess[
11
].Accordingtotheseworks,notonlythesignalvariance
butalsocorrelationsbetweensignalscorrespondingtodifferentinputsneedtobepreservedtoensure
goodtrainabilityofinitializedneuralnetworks.Thisway,theaverageeigenvalueofthesignalinput-
outputJacobianinmeanneuralnetworksissteeredtowards
1
.Furthermore,ahighconcentration
ofthefullspectraldensityoftheJacobiancloseto
1
seemstosupporthighertrainingspeeds[
14
,
15
].
Thispropertyiscalleddynamicalisometryandisbetterrealizedbyorthogonalweightinitializations
[
19
].Sofar,theseinsightsrelyonthemeanassumptionoflayerwidth.[
6
,
5
]have
derivedsizecorrectionsfortheaveragesquaredsignalnormandansweredthequestionwhen
themeanassumptionholds.
Inthisarticle,wedeterminetheexactsignaloutputdistributionwithoutrequiringmeanap-
proximations.Forfully-connectednetworkensembleswithGaussianweightsandbiasesforgeneral
nonlinearactivationfunctions,wethattheoutputdistributiononlydependsonthescalarproducts
betweendifferentinputs.Wethereforefocusontheirpropagationthroughanetworkensemble.In
particular,westudyalineartransitionoperatorthatadvancesthesignaldistributionlayer-wise.We
conjecturethatthespectralpropertiesofthisoperatorcanbemoreinformativeoftrainabilitythan
theaveragespectraldensityoftheinput-outputJacobian.Additionally,thedistributionofthecosine
similarityindicateshowwellaninitializednetworkcandistinguishdifferentinputs.
Wefurtherdiscusswhennetworklayersofwidtharewellrepresentedbymeananalysisand
whentheyarenot.Furthermore,wehighlightimportantdifferencesintheanalysis.Byspecializing
ourderivationstoReLUs,wevariantsoftheHeinitialization[
7
]thatthesamecriteriabut
alsosufferfromthesamelackofdynamicalisometry[
14
].Inconsequence,suchinitializedneural
networkscannotbetrainedeffectivelywithoutbatchnormalizationforhighdepth.Toovercome
thisproblem,weproposeasimpleinitializationschemeforReLUlayersthatguaranteesperfect
dynamicalisometry.AsubsetoftheweightscanstillbedrawnfromGaussiandistributionsor
chosenasorthogonalwhiletheremainingonesaredesignedtoensurefullsignalpropagation.Both
consistentlyoutperformtheHeinitializationinourexperimentsonMNISTandCIFAR-10.
2SignalpropagationthroughGaussianneuralnetworkensembles
2.1Backgroundandnotation
Westudyfully-connectedneuralnetworkensembleswithzeromeanGaussianweightsandbiases.
Wethusmakethefollowingassumption:
Anensemble
f
G
g
L;N
l
;˚;˙
w
;˙
b
offully-connectedfeedforwardneuralnetworksconsistsofnetworks
withdepths
L
,widths
N
l
,
l
=0
;:::;L
,independentlynormallydistributedweightsandbiaseswith
w
(
l
)
ij
˘N

0
;˙
2
w;l

,
b
(
l
)
i
˘N

0
;˙
2
b;l

,andnon-decreasingactivationfunction
˚
:
R
!
R
.Starting
fromtheinputvector
x
(0)
,signal
x
(
l
)
propagatesthroughthenetwork,asusual,as:
x
(
l
)
=
˚

h
(
l
)

;
h
(
l
)
=
W
(
l
)
x
(
l

1)
+
b
(
l
)
;
x
(
l
)
i
=
˚

h
(
l
)
i

;h
(
l
)
i
=
N
l

1
X
j
=1
w
(
l
)
ij
x
(
l

1)
j
+
b
(
l
)
i
;
for
l
=1
;:::;L
,where
h
(
l
)
isthepre-activationatlayer
l
,
W
(
l
)
istheweightmatrix,and
b
(
l
)
isthe
biasvector.Ifnotindicatedotherwise,
1
-dimensionalfunctionsappliedtovectorsareappliedtoeach
componentseparately.Toeasenotation,wefollowtheconventiontosuppressthesuperscript
(
l
)
and
write,forinstance,
x
i
insteadof
x
(
l
)
i
,
x
i
insteadof
x
(
l

1)
i
,and
x
i
insteadof
x
(
l
+1)
i
,whenthelayer
referenceisclearfromthecontext.
Ideally,theinitializednetworkisclosetothetrainedonewithhighprobabilityandcanbereached
fastinasmallnumberoftrainingsteps.Hence,ourgoalistounderstandtheensembleabove
andthetrainabilityofaninitializednetworkwithoutrequiringmeanapproximationsof
N
l
.Inparticular,wederivetheprobabilitydistributionoftheoutput
x
(
L
)
.Withinthisframework,
oursecondgoalistolearnhowtoimproveontheHeinitialization,i.e.,thechoice
˙
w;l
=
p
2
=N
l
and
b
(
l
)
i
=0
.EventhoughitpreservesthevarianceforReLUs,i.e.,
˚
(
x
)=max
f
0
;x
g
,asactivation
2
functions[
7
],neitherthisparameterchoicenororthogonalweightsleadtodynamicalisometry[
14
].
Thus,theaveragespectrumoftheinput-outputJacobianisnotconcentratedaround
1
forhigher
depthsandwidth.Inconsequence,ReLUsarearguedtobeaninferiorchoicecomparedto
sigmoids[
14
].Thus,ourthirdgoalistoprovideaninitializationschemeforReLUsthatovercomes
theresultingproblemsandprovidesdynamicalisometry.
Westartwithourresultsaboutthesignalpropagationforgeneralactivationfunctions.Theproofs
foralltheoremsaregiveninthesupplementarymaterial.Asweshow,thesignaloutputdistribution
dependsontheinputdistributiononlyviascalarproductsoftheinputs.Higherordertermsdo
notpropagatethroughanetworkensembleatinitialization.Inconsequence,wecanfocusonthe
distributionofsuchscalarproductslaterontoderivemeaningfulcriteriaforthetrainabilityof
initializeddeepneuralnetworks.
2.2Generalactivationfunctions
Let'sassumethatthesignal
x
ofthepreviouslayerisgiven.Then,eachpre-activationcomponent
h
i
ofthecurrentlayerisnormallydistributedas
h
i
=
P
N
l
j
=1
w
ij
x
j
+
b
i
˘N

0
;˙
2
w
P
j
x
2
j
+
˙
2
b

,
sincetheweightsandbiasareindependentlynormallydistributedwithzeromean.Thenon-linear
monotonicallyincreasingtransformation
x
i
=
˚
(
h
i
)
isdistributedas
x
i
˘


˚

1
(

)
˙

,where
˚

1
denotesthegeneralizedinverseof
˚
,i.e.
˚

1
(
x
):=inf
f
y
2
R
j
˚
(
y
)

x
g
,

thecumulative
distributionfunction(cdf)ofastandardnormalrandomvariable,and
˙
2
=
˙
2
w
j
x
j
2
+
˙
2
b
.Thus,we
onlyneedtoknowthedistributionof
j
x
j
2
asinputtocomputethedistributionof
x
i
.Thesignal
propagationisthusreducedtoa1-dimensionalproblem.Notethattheassumptionofequal
˙
2
w
forall
incomingedgesintoaneuronarecrucialforthisresult.Otherwise,
h
i
˘N

0
;
P
j
˙
2
w;j
x
2
j
+
˙
2
b;i

wouldrequiretheknowledgeofthedistributionof
P
j
˙
2
w;j
x
2
j
,whichdependsontheparameters
˙
2
w;j
.Basedon
˙
2
w;j
=
˙
2
w
however,wecancomputetheprobabilitydistributionofoutputs.
Proposition1.
Lettheprobabilitydensity
p
0
(
z
)
ofthesquaredinputnorm
j
x
(0)
j
2
=
P
N
0
i
=1

x
(0)
i

2
beknown.Then,thedistribution
p
l
(
z
)
ofthesquaredsignalnorm
j
x
(
l
)
j
2
dependsonlyonthe
distributionofthepreviouslayer
p
l

1
(
z
)
astransformedbyalinearoperator
T
l
:
L
1
(
R
+
)
!
L
1
(
R
+
)
sothat
p
l
=
T
l
(
p
l

1
)
.
T
l
isas
T
l
(
p
)[
z
]=
Z
1
0
k
l
(
y;z
)
p
(
y
)
dy;
(1)
where
k
(
y;z
)
isthedistributionofthesquaredsignal
z
atlayer
l
giventhesquaredsignalat
thepreviouslayer
y
sothat
k
l
(
y;z
)=
p

N
l

1
˚
(
h
y
)
2
(
z
)
,where

standsforconvolutionand
p
˚
(
h
y
)
2
(
z
)
denotesthedistributionofthesquaredtransformedpre-activation
h
y
,whichisnormallydistributed
as
h
y
˘N

0
;˙
2
w
y
2
+
˙
2
b

.Thisdistributionservestocomputethecumulativedistributionfunction
(cdf)ofeachsignalcomponent
x
(
l
)
i
as
F
x
(
l
)
i
(
x
)=
Z
1
0
dzp
l

1
(
z

 
˚

1
(
x
)
p
˙
2
w
z
+
˙
2
b
!
;
(2)
where
˚

1
denotesthegeneralizedinverseof
˚
and

thecdfofastandardnormalrandomvariable.
Accordingly,thecomponentsarejointlydistributedas
F
x
(
l
)
1
;:::;x
(
l
)
N
l
(
x
)=
Z
1
0
dzp
l

1
(
z

N
l
i
=1


˚

1
(
x
i
)
˙
z

;
(3)
whereweusetheabbreviation
˙
z
=
p
˙
2
w
z
+
˙
2
b
.
Ascommon,the
N
-foldconvolutionofafunction
f
2
L
1
(
R
+
)
isasrepeatedconvolution
with
f
,i.e.,byinduction,
f

N
(
z
)=
f

f

(
N

1)
(
z
)=
R
z
0
f
(
x
)
f

(
N

1)
(
z

x
)
dx
.InProp.1,we
notetheradialsymmetryoftheoutputdistribution.Itonlydependsonthesquarednormoftheinput.
Forasingleinput
x
(0)
,
p
0
(
z
)
isgivenbytheindicatorfunction
p
0
(
z
)=
1
j
x
(0)
j
2
(
z
)
.Interestingly,
3
(a)Squaredsignalnormdistributionatdifferent
depthsfor
N
l
=200
.Theinitialdistribution(
L
=
0
)isbyMNIST.
(b)Eigenvalues

correspondingtoeigenfunctions
y
m
of
T
l
.
N
l
=10
(blackcircles),
N
l
=20
(blue
triangles),
N
l
=100
(red
+
).
Figure1:Layer-wisetransitionofthesquaredsignalnormdistributionforReLUswithHeinitializa-
tionparameters
˙
w
=
p
2
=N
l
,
˙
b
=0
.
meananalysisalsofocusesontheaverageorthesquaredsignal,whichislikewiseupdated
layer-wise.Prop.1explainsandthefocusofmeantheoryonthesquaredsignalnorm.
Moreinformationisnottransmittedfromlayertolayertodeterminethestate(distribution)ofasingle
neuron.Thedifferencetomeandtheoryhereisthatweregardthefulldistribution
p
l

1
ofthe
previouslayerinsteadofitsaverageonlyonlargelayers.Thelinearoperator
T
l
governsthis
distribution.
p
x
(
L
)
=
Q
L
l
=1
T
l
p
x
(0)
,wheretheproductisbyfunctioncomposition.Hence,the
linearoperator
Q
L
l
=1
T
l
canalsobeinterpretedastheJacobiancorrespondingtothe(linear)function
thatmapsthesquaredinputnormdistributiontothesquaredoutputnormdistribution.
T
l
isdifferent
fromthesignalinputoutputJacobianstudiedinmeanrandommatrixtheory,yet,itsspectral
propertiescanalsoinformusaboutthetrainabilityofthenetworkensemble.Conveniently,weonly
havetostudyonespectrumandnotadistributionofeigenvaluesthatarepotentiallycoupledasin
randommatrixtheory.Foranynonlinearactivationfunction,
T
l
canbeapproximatednumericallyon
anequidistantgrid.Theconvolutioninthekerneloncanbecomputedefcientlywiththe
helpofFastFourierTransformations.Theeigenvaluesofthematrixapproximating
T
l
the
approximatesignalpropagationalongtheeigendirections.
However,weonlyreceivethefullpicturewhenweextendourstudytolookatthejointoutput
distribution,i.e.,theoutputscorrespondingtodifferentinputs.
Proposition2.
Thesamecomponentofpre-activationsofsignals
h
1
;:::;h
D
correspondingto
differentinputs
x
(0)
1
;:::;
x
(0)
D
,arejointlynormallydistributedwithzeromeanandcovariancematrix
V
by
v
ij
=
Cov
(
h
i
;h
j
)=
˙
2
w
h
x
i
;
x
j
i
+
˙
2
b
(4)
for
i;j
=1
;:::;D
conditionalonthesignals
x
i
ofthepreviouslayercorrespondingto
x
(0)
i
,where
D
denotesthenumberofdatapoints.
Afternon-linearactivation,thesignalsarenotjointlynormallydistributedanymore.Buttheir
distributionisafunctionofthesquarednormsandscalarproductsbetweensignalsoftheprevious
layeronly.Thus,itissufcienttopropagatethejointdistributionofthreevariablesthatcan
attaindifferentvalues,i.e.,
j
x
1
j
2
,
j
x
2
j
2
,
h
x
1
;
x
2
i
,throughthelayerstodeterminethejointoutput
distributionoftwosignals
x
1
and
x
1
correspondingtodifferentinputs.Nootherinformationaboutthe
jointdistributionofinputs,e.g.,highermoments,cantheensembleoutputdistributionand
thusourchoiceofweightandbiasparameters.Inconsequence,thefocusonquantitiescorresponding
totheaboveinmeantheoryisforGaussianparameterinitializationanddoesnotrequire
anyapproximation.Yet,themeanassumptionthatpre-activationsignalsareexactlynormally
distributedandnotonlyconditionalontheprevioussignalisapproximate.Accordingly,theoutput
distributionforneuralnetworksdoesnotfollowaGaussianprocesswithaveragecovariance
matrix
V
asinmeantheory[
11
].
V
followsaprobabilitydistributionthatisdeterminedby
4
thepreviouslayers.FortheinitializationschemeforReLUsthatweproposelater,wecanstate
thedistributionof
V
explicitly.Firsthowever,weanalyzeReLUsinthestandardframeworkand
specializetheabovetheorems.
2.3LinearUnits(ReLUs)
Theminimuminitializationcriteriontoavoidvanishingorexplodinggradientsistopreservethe
expectedsquaredsignalnorm.Fornetworks,thisisgivenasfollows.
Corollary3.
ForReLUs,theexpectationvalueofthesquaredsignalconditionalonthesquared
signalofthepreviouslayerisgivenby:
E

j
x
(
l
)
j
2


j
x
(
l

1)
j
2
=
y

=(
˙
2
w;l
y
+
˙
2
b;l
)
N
l
2
:
(5)
Consequently,theexpectationofthesquaredsignalnormdependsontheinitialinputas:
E

j
x
(
L
)
j
2


j
x
(0)
j
2

=
j
x
(0)
j
2

L
l
=1
N
l
˙
2
w;l
2
+
˙
2
b;L
N
L
2
+
L

1
X
l
=1
˙
2
b;l
N
l
2

L
n
=
l
+1
N
n
˙
2
w;n
2
(6)
SimilarrelationsfortheexpectedsignalcomponentsandtheirvariancefollowfromEq.(6)andare
coveredinthesupplementarymaterial.[
6
]hasderivedasimplerversionofEq.(6)forequal
˙
w;l
and
N
l
acrosslayers.
Astraightforwardwaytopreservetheaveragesquaredsignalorthesquaredoutputsignalnorm
distributionisexactlytheHeinitialization
˙
b;l
=0
and
˙
w;l
=
p
2
=N
l
[
7
],whichisalso
bymeananalysis.Yet,wehavemanymorechoicesevenwhen
˙
2
b;l
=0
.Weonlyneedto
onecondition,i.e.,
0
:
5
L

L
l
=1
N
l
˙
2
w;l
ˇ
1
.Incasethatwenormalizetheinputsothat
j
x
(0)
j
2
=1
,
˙
2
b;l
6
=0
isalsoavalidoptionandwehave
2
L

1
degreesoffreedom.
Thereremainsthequestionwhetherthereexistfurthercriteriatobethatimprovethe
trainabilityoftheinitialnetworkensemble.Thewholeoutputdistributioncouldprovidethoseandits
derivationisgiveninthesupplementarymaterial.AccordingtoProp.2,itisguidedbythelayer-wise
jointdistributionofthevariables

j
x
1
j
2
;
j
x
2
j
2
;
h
x
1
;
x
2
i

given

j
x
1
j
2
;
j
x
2
j
2
;
h
x
1
;
x
2
i

.Asthisis
computationallyintensivetoobtain,wefocusonmarginals,i.e.,thedistributionsof
j
x
j
2
and
h
x
1
;
x
2
i
.
Thesearesufcienttohighlightseveraldrawbacksoftheinitializationapproachandprovideuswith
insightstoproposeanalternativethatovercomestheseshortcomings.
First,wefocuson
j
x
(
l
)
j
2
andderiveaclosedformsolutionfortheintegralkernel
k
l
(
y;z
)
of
T
l
inProp.1andanalysesomeofitsspectralpropertiesforReLUs.Thisallowsustoreasonabout
theshapeofthestationarydistributionof
T
l
,i.e.,thelimitoutputdistributionfornetworkswith
increasingdepth.
Proposition4.
ForReLUs,thelinearoperator
T
l
inProp.1isby
k
l
(
y;z
)=0
:
5
N
l
 

0
(
z
)+
N
l
X
k
=1

N
l
k

1
˙
2
y
p
˜
2
k

z
˙
2
y

!
(7)
with
˙
y
=
p
˙
2
w
y
+
˙
2
b
,where

0
(
z
)
denotesthe

-distributionpeakedat
0
and
p
˜
2
k
thedensityof
the
˜
2
distributionwith
k
degreesoffreedom.For
˙
b
=0
,thefunctions
f
m
(
y
)=
y
m
1
]0
;
1
]
(
y
)
are
eigenfunctionsof
T
l
forany
m
2
R
(eventhoughtheyarenotelementsof
L
1
(
R
+
)
andthusnot
normalizableasprobabilitymeasures)withcorrespondingeigenvalue

l;m
2
R
:
T
l
f
m
=

l;m
f
m
with

l;m
=0
:
5
N
l

m

1
1
˙
2
m
+2
w
N
l
X
k
=1

N
l
k


k=
2

m

1)

k=
2)
(8)
Notethat,for
˙
b
=0
,theeigenfunctions
y
m
cannotbenormalizedon
R
+
,astheantiderivative
divergesatzerofor
m

1
.However,ifwediscretize
T
l
innumericalexperimentstheycan
benormalizedandtherealeigenvectorsrepresentingprobabilitydistributionsattainshapes
ˇ
y
m
.
5
Fig.1aprovidesanexampleoftheoutputdistributionfor
9
layerseachconsistingof
N
l
=200
neuronswithHeinitializationparameters.Theaveragesquaredsignalisindeedpreservedbut
becomesmorerighttailedfordeeperlayers.Fig.1bshowsthecorrespondingeigenvaluesof
T
l
asinProp.4.Insummary,weobserveawindow
m
crit
<m

1
witheigenvalues

l;m
<
1
.
,fortheHevalues
˙
b;l
=0
and
˙
w;l
=
p
2
=N
l
,numericalexperimentsreveala
relation
m
crit
ˇ
3
:
2559793

1
:
6207083
N
l
.Signalpartswithinthiswindowaredampeddownin
deeperlayers,whiletheremainingpartsexplode.Only
y
m
crit
ispreservedthroughthelayersand
dependsonthechoiceof
˙
w;l
.Interestingly,for
m
=

1
,

l;m
isindependentof
˙
w;l
andgivenby

l;

1
=1

0
:
5
N
l
.Thus,itapproaches

l;

1
=1
forincreasing
N
l
.FortheHeinitialization,
y
m
crit
convergestothe

0
(
y
)
forincreasing
N
l
.Incontrasttomeananalysis,notthewholespaceof
eigenfunctionscorrespondstoeigenvalue
1
fortheHeinitialization.Inparticular,eigenvaluesbigger
thanoneexistthatcanbeproblematicforexplodinggradients.Toreducetheirnumber,broaderlayers
promisebetterprotectionaswellassmallervaluesof
˙
w;l
.Ultimately,wecareabouttheproductof
layer-wiseeigenvalues,i.e.,theeigenvaluesof

l
T
l
.Again,settingtheseto
1
imposesaconstraint
onlyontheproduct

l
˙
2
w;l
likeinCor.3.Hence,wegainnoadditionalconstraintonourinitial
parametersandhavenomeanstopreventeigenvalueslargerthan
1
.
Thebiggestchallengefortrainability,however,istheabilitytodifferentiatesimilarsignals.We
thereforestudytheevolutionofthecosinesimilarity
h
x
(
l
)
1
;
x
(
l
)
2
i
oftwoinputs
x
(0)
1
and
x
(0)
2
orthe
unnormalizedscalarproductthroughlayers
l
.
Theorem5.
ForReLUs,let
x
1
=
˚
(
h
1
)
;x
2
=
˚
(
h
2
)
bethesamesignalcomponent,i.e.,neuron,
whereeachcorrespondstoadifferentinput
x
(0)
1
or
x
(0)
2
.Letthecorrelation
ˆ
=
v
12
=
p
v
11
v
22
ofthe
pre-activations
h
1
;h
2
begiven,where
V
denotesthe2-dimensionalcovariancematrixasin
Prop.2.Then,thecorrelationafternon-linearactivationis
Cor
(
x
1
;x
2
)=
p
1

ˆ
2

1+2
ˇˆg
(
ˆ
)
ˇ

1
:
(9)
g
(
ˆ
)
isas
g
(
ˆ
)=
1
p
2
ˇ
R
1
0


ˆ
p
1

ˆ
2
u

exp


1
2
u
2

du
for
j
ˆ
j6
=1
and
g
(

1)=0
,
g
(1)=0
:
5
.Theaverageofthesumofallcomponents
E
(
h
x
1
;
x
2
i
)
conditionalonthepreviouslayer
is:
E
(
h
x
1
;
x
2
ij
ˆ
)=
N
l
p
v
11
v
22
 
g
(
ˆ
)
ˆ
+
p
1

ˆ
2
2
ˇ
!
ˇ
N
l
p
v
11
v
22
1
4
(
ˆ
+1)
:
(10)
Furthermore,conditionalonthesignalsofthepreviouslayer,
h
x
1
;
x
2
i
isdistributedas
f

N
l
prod
(
t
)
,
where
f
prod
(
y
)=(1

g
(
ˆ
))

0
(
y
)+
1
2
ˇ
p
det(
V
)
exp

v
12
y
2det(
V
)

K
0

p
v
11
v
22
det(
V
)
y

and
K
0
(
w
)=
R
1
0
cos(
w
sinh(
t
))
dt
denotestheBesselfunctionofsecondkind.
Notethat[
2
]studiesasimilarintegralbutinthemeanlimit.Thecorrelationofthesignal
componentsonlydependson
ˆ
(andisalwayssmallerthan
ˆ
).Analogoustothec-mapinmean
approaches[
18
],theactualquantityofinterestwouldbethedistributionofthecorrelation
ˆ
,i.e.,
ˆ
=
˙
2
w
h
x
1
;
x
2
i
+
˙
2
b
q
(
˙
2
w
j
x
1
j
2
+
˙
2
b
)(
˙
2
w
j
x
2
j
2
+
˙
2
b
)
.Interestingly,for
˙
b
=0
,
ˆ
=
h
x
1
;
x
2
i
j
x
1
jj
x
2
j
coincideswiththecosine
similarityofthetwosignals.Thepreservationofthisquantityonaveragehasbeenshowntobethe
mostindicativecriterionfortrainabilityofReLUresidualnetworks[24].Wethereforetakeacloser
lookatitsdistribution.Tosavecomputationaltimeandspace,wesample
N
l
componentsiidfroma2
dimensionalnormaldistributionasintroducedinProp.2andtransformthecomponentsbyReLUsto
obtaintwovectors
x
1
and
x
2
andcalculatetheircosinesimilarity.Repeatingthisprocedure
10
6
times
resultsinFig.2.First,wenotethatcorrelationscanonlybepositiveafterthelayer,sinceall
signalcomponentsarepositive(orzero)aftertransformationbyReLUs.Negativecosinesimilarities
cannotbepropagatedthroughGaussianReLUensembles.Datatransformationtoobtainpositive
inputscanmitigatethisissue.Yet,Eq.(10)highlightsanunavoidableproblemfordeepmodels,i.e.,
theaveragecosinesimilarityincreasesfromlayertolayeruntilitreaches
1
athighdepths.Then,all
signalsbecomeparallelandthusindistinguishable.
Whilethiseffectcannotbemitigatedcompletelywithinourinitializationscheme,aslightlysmaller
choiceof
˙
w
thantheHeinitializationreducestheaveragecosinedistanceandasmallernumber
6
ofneuronsinonelayerincreasesthevarianceofthecosinedistance,asshowninFig.2.Ahigher
varianceincreasestheprobabilitythatsmallervaluesofthecosinedistancecanbepropagated.
WehypothesizethatthiseffectcontributestothebettertrainabilityofReLUneuralnetworkswith
DropOutorDropConnect[
22
],sincebothreducetheeffectivenumberofneurons
N
l
.Yet,asmaller
˙
w
andDropOut(orDropConnect)introduceariskofvanishinggradientsindeepneuralnetworks
[
17
].Anadjustmentof
˙
w
bytheDropOutratetoavoidthiseffect[
17
]wouldalsodestroypossible
effectsonthecosinesimilarity.
Figure2:Probabilitydistributionofthecosinesimilarity
conditionalonthepreviouslayerwith
j
x
1
j
=
j
x
2
j
=1
and
h
x
1
;
x
2
i
equals
0
(circles),0.25(squares),0.5(diamonds),
and0.75(triangles)for
N
l
=100
(dashedlinesand
symbols)and
N
l
=200
(linesandsymbols)neurons.
Forsmaller
˙
2
=1
=N
,[
1
]observesa
phenomenonrelatedtothecosinesim-
ilarity,i.e.shatteredgradients.How-
ever,inthissetting,theeffectofvan-
ishinggradientsandincreasingcorre-
lationsareindistinguishable.Infact,
theauthorsobservedecreasingcorre-
lations,whilewehighlighttheprob-
lemofincreasingonesfortheHeini-
tialization.Interestingly,intheHe
case(
˙
2
=2
=N
),[
1
]thatalso
batchnormalizationcannotprovide
bettertrainability.Forﬁtypicalﬂinputs
thatareshowntobecommoninnet-
workswithbatchnormalization(but
notinnetworkswithout,whichwe
studyhere),thecovariancebetween
outputscorrespondingtodifferentin-
putsdecaysexponentially.
Wethereforediscussanalternativesolutionthat[
1
]proposesalsoforconvolutionalandresidual
neuralnetworksandhasbeenintroducedby[21].
3Alternativeinitializationoffully-connectedReLUdeepneuralnetworks
TheissuesoftrainingdeepneuralnetworkswithReLUsarecausedbythefactthatnegativesignal
canneverpropagatethroughthenetworkandaneuron'sstateiszeroinhalfofthecases.Hence,we
discussaninitialization,wherethefullsignalistransmitted.Wesetthebiasvector
b
(
l
)
i
=0
andthe
weightmatrices
W
(
l
)
2
R
N
l

1

N
l
areinitiallydeterminedbyasubmatrix
W
(
l
)
0
2
R
N
l

1
2

N
l
2
as
W
(
l
)
=
""
W
(
l
)
0

W
(
l
)
0

W
(
l
)
0
W
(
l
)
0
#
:
Regardlessofthechoiceof
W
(
l
)
0
,wereceiveasignalvector
x
(
l
)
,wherehalfoftheentriescorrespond
tothepositivepartofthepre-activationsandthesecondhalftothenegativepart,i.e.,if
i

N
l
=
2
and
h
(
l
)
i
=
P
j
w
(
l
)
ij
x
(
l

1)
j
>
0
,wehave
x
(
l
)
i
=
h
(
l
)
i
and
x
(
l
)
i
+
N
l
=
2
=0
ortheotherwayaroundfor
h
(
l
)
i
<
0
.Thisway,effectively
h
(
l
)
i
=
P
N
l
=
2
j
=1
w
(
l
)
0
;ij
h
(
l

1)
j
ispropagatedsothatwehaveinitially
linearnetworks
h
(
L
)
=
Q
L
l
=0
W
(
l
)
0
h
0
.Notethatwestillhavetotrainthefull
N
l

1
N
l
parameters
of
W
(
l
)
andcanlearnnon-linearfunctions.[
21
]observedthatconvolutionalneuralnetworkseven
trainedthelayerstoresemblelinearneuralnetworks,whichinspiredthischoiceofinitialization.
Inthissetting,wehaveseveralgoodchoicesof
W
(
l
)
0
.First,weassumeiidentries
w
(
l
)
0
;ij
˘
N

0
;˙
2
w;l

asbefore.Wecallthisvariant
Gaussiansubmatrix
(GSM)initialization.Inthis
case,ourassumptionsfromtheprevioussectionsaremetandwecanrepeattheanalysisfornetworks
ofwidth
N
l
=
2
and
˚
(
h
)=
h
,i.e.settheactivationfunctiontotheidentity.Thesameparameter
choiceasinCor.3,e.g.,
˙
2
w;l
=2
=N
l
,preservesthevarianceandnowalsothecosinedistance
betweensignalscorrespondingtodifferentinputs.Theanalysisisratherstraight-forwardandthe
outputdistributionisbythedistributionof
Q
L
l
=0
W
(
l
)
0
x
(
0
)
.
Q
L
l
=0
W
(
l
)
0
followsaproduct
Wishartdistributionwithknownspectrum[13,14].
7
Accordingto[
14
]however,dynamicalisometryleadstobettertrainingresultsandspeed.This
demandsaninput-outputJacobianclosetotheidentityoraspectrumofthesignalpropagation
operator
T
l
thatishighlyconcentratedaround
1
.Previously,thiswasbelievedtobebetterachievable
by
tanh
orsigmoidratherthanbyReLUaschoiceofactivationfunctions[
14
].Yet,intheparameter
sharingsettingabove,perfectdynamicalisometryfor
h
canbeachievedbyorthogonal
W
(
l
)
0
,i.e.,itis
drawnuniformlyatrandomfromallmatrices
W

W
T
W
=
˙
2
w;l
I
with
˙
2
w;l
=1
.Thisis
oursecondinitializationproposalinadditiontoGSM.
Alternatively,[
25
]recommendstoshiftthesignal
h
(
l
)
i
byanon-zerobias
b
(
l
)
i
toenablenegative
signaltopassthroughaReLUactivationinsteadoftheproposedparametersharingsolution.Wealso
consideredasimilarapproachbutdecidedfortheproposedoneasitispoint-symmetric,guaranties
thereforeperfectdynamicalisometry,iscomputationallessintensive,asitdoesnotneedtocompute
adatadependentbias
b
(
l
)
i
asinbatchnormalization,andismoreconvenientfortheoreticalanalysis,
whichcanrelyonarichliteratureonlineardeepneuralnetworks.
4Experimentsfordifferentinitializationschemes
Figure3:testaccuracyonMNISTfordifferentwidths
N
,depths
L
,andweight
initializationwithparameters
˙
b
=0
,
˙
w
=
p
2
=N
forHeandGSMinitialization,and
˙
w
=1
for
orthogonal
W
0
after
10
4
SGDsteps.Wereporttheaverageof
100
realizationsandthecorresponding
0.95interval.Therightplotisasectionoftheleft.Notethatthelegendsapplytoboth
plots.
Wetrainfully-connectedReLUfeedforwardnetworksofdifferentdepthconsistingof
L
=1
;:::;
10
hiddenlayerswiththesamenumberofneurons
N
l
=
N
=100
;
300
;
500
andanadditionalsoftmax
layeronMNIST[
10
]andCIFAR-10[
9
]tocomparethreedifferentinitialization
schemes:thestandardHeinitializationandourtwoproposalsinSec.3,i.e.,GSMandorthogonal
weights.Wefocusonminimizingthecross-entropybyStochasticGradientDescent(SGD)without
batchnormalizationoranydataaugmentationtechniques.Hence,ourgoalisnottooutperformthe
stateoftheartbuttocomparetheinitializationschemesundersimilarrealisticconditions.
Sincedeepnetworksnormallyrequireasmallerlearningratethantheoneswithasmallnumberof
hiddenlayers,asinRef.[
14
],weadaptthelearningrateto
(0
:
0001+0
:
003

exp(

step=
10
4
))
=L
forMNISTand
(0
:
00001+0
:
0005

exp(

step=
10
4
))
=L
forCIFAR-10for
10
4
SGDstepswitha
batchsizeof100inallcases.Toreducethenumberofparametersandspeedupcomputations,we
clippedoriginalCIFAR-10imagesto
28

28
size.Foreachwetrain100instances
onMNISTand30instancesonCIFAR-10andreporttheaverageaccuracywitha0.95
intervalinFig.3andFig.4respectively.EachexperimentonMNISTwasrunon1NvidiaGTX1080
TiGPU,whileeachexperimentonCIFAR-10wasperformedon4NvidiaGTX1080TiGPUs.
NotethattheaccuracyonCIFAR-10islowerthanforconvolutionalarchitectures,aswerestrict
ourselvestodeepfully-connectednetworkstofocusontheirtrainability.[
1
]showsthatasimilar
8
Figure4:testaccuracyonCIFAR-10fordifferentwidths
N
,depths
L
,andweight
initializationwithparameters
˙
b
=0
,
˙
w
=
p
2
=N
forHeandGSMinitialization,and
˙
w
=1
for
orthogonal
W
0
after
10
4
SGDsteps.Wereporttheaverageof
30
realizationsandthecorresponding
0.95interval.Therightplotisasectionoftheleft.
orthogonalinitializationimprovestrainingresultsalsoforconvolutionalandresidualneuralnetworks.
Assuggestedbyourtheoreticalanalysis,bothproposedinitializationschemesconsistentlyoutperform
theHeinitializationandshowstabletrainingresults,inparticular,fordeepernetworkarchitectures,
wheretheHeinitializednetworksdecreaseinaccuracy.GSMandorthogonal
W
0
bothperformbetter
forhigherwidth
N
,whileorthogonal
W
0
seemstobethemostreliablechoice.
5Discussion
Wehaveintroducedaframeworkfortheanalysisofdeepfully-connectedfeedforwardneural
networksatinitializationwithzeromeannormallydistributedweightsandbiases.Itisexact,does
notrelyonmeanapproximations,providesdistributioninformationofoutputandjointoutput
signals,andappliestonetworkswitharbitrarylayerwidths.Ithasledtotheinsightthatonlythe
scalarproductsbetweeninputsdeterminetheshapeoftheoutputdistribution,butitisnot
byhigherinteractionterms.
Hence,forReLUs,wehaveanalysedthepropagationofthesequantitiesthroughthedeepneural
networkensemble.WhilemeananalysisprovidesonlytheHeinitializationforgoodtraining
results,wehaveextendedthenumberofpossibleparameterchoicesthatavoidvanishingorexploding
gradients.However,noparameterchoicecanavoidthetendencythatsignalsbecomemorealigned
withincreasingdepth.DeepReLUGaussianneuralnetworkensemblescannotdistinguishdifferent
inputcorrelationsandarethereforenotwelltrainablewithoutbatchnormalization.Evenbatch
normalizationdoesnotguarantythetransmissionofcorrelationsbetweendifferentinputs.
Assolutiontothisproblem,wehavediscussedanalternativebutsimpleinitializationschemethat
reliesoninitialparametersharing.Onevariantguaranteesperfectdynamicalisometry.Experiments
onMNISTandCIFAR-10demonstratethatdeeperfully-connectedReLUnetworkscanbecome
bettertrainableintheproposedwaythanbythestandardapproach.
Acknowledgments
WewouldliketothankJoachimM.BuhmannandAlkisGotovosfortheirvaluablefeedbackonthe
manuscriptandthereviewersfortheirinsightfulcomments.Thisworkwaspartiallyfundedbythe
SwissHeartFailureNetwork,PHRT122/2018DRI14(J.M.Buhmann,PI).RBwassupportedbya
grantfromtheUSNationalCancerInstitute(1R35CA220523).
9
References
[1]
DavidBalduzzi,MarcusFrean,LennoxLeary,J.P.Lewis,KurtWan-DuoMa,andBrian
McWilliams.Theshatteredgradientsproblem:Ifresnetsaretheanswer,thenwhatisthequestion?
InDoinaPrecupandYeeWhyeTeh,editors,
Proceedingsofthe34thInternationalConferenceon
MachineLearning
,volume70of
ProceedingsofMachineLearningResearch
,pages342Œ350,
InternationalConventionCentre,Sydney,Australia,06Œ11Aug2017.PMLR.
[2]
YoungminChoandLawrenceK.Saul.Kernelmethodsfordeeplearning.InY.Bengio,D.Schu-
urmans,J.D.Lafferty,C.K.I.Williams,andA.Culotta,editors,
AdvancesinNeuralInformation
ProcessingSystems22
,pages342Œ350.CurranAssociates,Inc.,2009.
[3]
SimonS.Du,XiyuZhai,BarnabasPoczos,andAartiSingh.Gradientdescentprovablyoptimizes
over-parameterizedneuralnetworks.In
InternationalConferenceonLearningRepresentations
,
2019.
[4]
XavierGlorotandYoshuaBengio.Understandingthedifcultyoftrainingdeepfeedforwardneural
networks.In
ProceedingsoftheThirteenthInternationalConferenceonIntelligenceand
Statistics,AISTATS2010,ChiaLagunaResort,Sardinia,Italy,May13-15,2010
,pages249Œ256,
2010.
[5]
BorisHanin.Whichneuralnetarchitecturesgiverisetoexplodingandvanishinggradients?In
S.Bengio,H.Wallach,H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Garnett,editors,
AdvancesinNeuralInformationProcessingSystems31
,pages582Œ591.CurranAssociates,Inc.,
2018.
[6]
BorisHaninandDavidRolnick.Howtostarttraining:Theeffectofinitializationandarchitecture.
InS.Bengio,H.Wallach,H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Garnett,editors,
AdvancesinNeuralInformationProcessingSystems31
,pages571Œ581.CurranAssociates,Inc.,
2018.
[7]
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.DelvingdeepintoSur-
passinghuman-levelperformanceonimagenetIn
Proceedingsofthe2015IEEE
InternationalConferenceonComputerVision(ICCV)
,ICCV'15,pages1026Œ1034,Washington,
DC,USA,2015.IEEEComputerSociety.
[8]
SergeyIoffeandChristianSzegedy.Batchnormalization:Acceleratingdeepnetworktrainingby
reducinginternalcovariateshift.InFrancisBachandDavidBlei,editors,
Proceedingsofthe32nd
InternationalConferenceonMachineLearning
,volume37of
ProceedingsofMachineLearning
Research
,pages448Œ456,Lille,France,07Œ09Jul2015.PMLR.
[9]
AlexKrizhevsky,VinodNair,andGeoffreyHinton.Cifar-10(canadianinstituteforadvanced
research).Technicalreport,2009.
[10]
YannLecun,LéonBottou,YoshuaBengio,andPatrickHaffner.Gradient-basedlearningapplied
todocumentrecognition.In
ProceedingsoftheIEEE
,pages2278Œ2324,1998.
[11]
JaehoonLee,JaschaSohl-dickstein,JeffreyPennington,RomanNovak,SamSchoenholz,and
YasamanBahri.Deepneuralnetworksasgaussianprocesses.In
InternationalConferenceon
LearningRepresentations
,2018.
[12]
DmytroMishkinandJiriMatas.Allyouneedisagoodinit.In
4thInternationalConferenceon
LearningRepresentations,ICLR2016,SanJuan,PuertoRico,May2-4,2016,ConferenceTrack
Proceedings
,2016.
[13]
ThorstenNeuschel.Plancherel-rotachformulaeforaveragecharacteristicpolynomialsofproducts
ofproductsofginibrerandommatricesandthefuss-catalandistribution.
RandomMatrices:
TheoryandApplications
,3(1),2014.
[14]
JeffreyPennington,SamuelS.Schoenholz,andSuryaGanguli.Resurrectingthesigmoidindeep
learningthroughdynamicalisometry:theoryandpractice.In
AdvancesinNeuralInformation
ProcessingSystems30:AnnualConferenceonNeuralInformationProcessingSystems2017,4-9
December2017,LongBeach,CA,USA
,pages4788Œ4798,2017.
[15]
JeffreyPennington,SamuelS.Schoenholz,andSuryaGanguli.Theemergenceofspectral
universalityindeepnetworks.In
InternationalConferenceonIntelligenceandStatistics,
AISTATS2018,9-11April2018,PlayaBlanca,Lanzarote,CanaryIslands,Spain
,pages1924Œ
1932,2018.
[16]
BenPoole,SubhaneilLahiri,MaithraRaghu,JaschaSohl-Dickstein,andSuryaGanguli.Expo-
nentialexpressivityindeepneuralnetworksthroughtransientchaos.InD.D.Lee,M.Sugiyama,
U.V.Luxburg,I.Guyon,andR.Garnett,editors,
AdvancesinNeuralInformationProcessing
Systems29
,pages3360Œ3368.CurranAssociates,Inc.,2016.
10
[17]
ArnuPretorius,ElanvanBiljon,SteveKroon,andHermanKamper.Criticalinitialisationfordeep
signalpropagationinnoisyneuralnetworks.InS.Bengio,H.Wallach,H.Larochelle,
K.Grauman,N.Cesa-Bianchi,andR.Garnett,editors,
AdvancesinNeuralInformationProcessing
Systems31
,pages5717Œ5726.CurranAssociates,Inc.,2018.
[18]
MaithraRaghu,BenPoole,JonM.Kleinberg,SuryaGanguli,andJaschaSohl-Dickstein.Onthe
expressivepowerofdeepneuralnetworks.In
Proceedingsofthe34thInternationalConference
onMachineLearning,ICML2017,Sydney,NSW,Australia,6-11August2017
,pages2847Œ2854,
2017.
[19]
AndrewM.Saxe,JamesL.McClelland,andSuryaGanguli.Exactsolutionstothenonlinear
dynamicsoflearningindeeplinearneuralnetworks.In
2ndInternationalConferenceonLearning
Representations,ICLR2014,Banff,AB,Canada,April14-16,2014,ConferenceTrackProceedings
,
2014.
[20]
SamuelS.Schoenholz,JustinGilmer,SuryaGanguli,andJaschaSohl-Dickstein.Deepinformation
propagation.In
5thInternationalConferenceonLearningRepresentations,ICLR2017,Toulon,
France,April24-26,2017,ConferenceTrackProceedings
,2017.
[21]WenlingShang,KihyukSohn,DiogoAlmeida,andHonglakLee.Understandingandimproving
convolutionalneuralnetworksviaconcatenatedlinearunits.In
Proceedingsofthe
33rdInternationalConferenceonInternationalConferenceonMachineLearning-Volume48
,
ICML'16,pages2217Œ2225.JMLR.org,2016.
[22]
LiWan,MatthewZeiler,SixinZhang,YannLeCun,andRobFergus.Regularizationofneural
networksusingdropconnect.InSanjoyDasguptaandDavidMcAllester,editors,
Proceedingsof
the30thInternationalConferenceonMachineLearning
,volume28of
ProceedingsofMachine
LearningResearch
,pages1058Œ1066,Atlanta,Georgia,USA,17Œ19Jun2013.PMLR.
[23]
LechaoXiao,YasamanBahri,JaschaSohl-Dickstein,SamuelSchoenholz,andJeffreyPennington.
DynamicalisometryandameantheoryofCNNs:Howtotrain10,000-layervanillaconvo-
lutionalneuralnetworks.InJenniferDyandAndreasKrause,editors,
Proceedingsofthe35th
InternationalConferenceonMachineLearning
,volume80of
ProceedingsofMachineLearning
Research
,pages5393Œ5402,Stockholmsmässan,StockholmSweden,10Œ15Jul2018.PMLR.
[24]
GeYangandSamuelSchoenholz.Meanresidualnetworks:Ontheedgeofchaos.InI.Guyon,
U.V.Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,
AdvancesinNeuralInformationProcessingSystems30
,pages7103Œ7114.CurranAssociates,
Inc.,2017.
[25]
GregYang,JeffreyPennington,VinayRao,JaschaSohl-Dickstein,andSamuelS.Schoenholz.A
meantheoryofbatchnormalization.In
InternationalConferenceonLearningRepresentations
,
2019.
[26]
JasonYosinski,JeffClune,YoshuaBengio,andHodLipson.Howtransferablearefeaturesin
deepneuralnetworks?InZ.Ghahramani,M.Welling,C.Cortes,N.D.Lawrence,andK.Q.
Weinberger,editors,
AdvancesinNeuralInformationProcessingSystems27
,pages3320Œ3328.
CurranAssociates,Inc.,2014.
11
"
30,Incorporating Chinese Characters of Words for Lexical Sememe Prediction,http://arxiv.org/pdf/1806.06349v1.pdf,https://github.com/thunlp/Character-enhanced-Sememe-Prediction,"IncorporatingChineseCharactersofWords
forLexicalSememePrediction
HuimingJin
1

y
,HaoZhu
2
y
,ZhiyuanLiu
2,3
z
,RuobingXie
4
,
MaosongSun
2,3
,FenLin
4
,LeyuLin
4
1
ShenyuanHonorsCollege,BeihangUniversity,Beijing,China
2
BeijingNationalResearchCenterforInformationScienceandTechnology,
StateKeyLaboratoryofIntelligentTechnologyandSystems,
DepartmentofComputerScienceandTechnology,TsinghuaUniversity,Beijing,China
3
JiangsuCollaborativeInnovationCenterforLanguageAbility,
JiangsuNormalUniversity,Xuzhou221009China
4
SearchProductCenter,WeChatSearchApplicationDepartment,Tencent,China
Abstract
Sememesareminimumsemanticunitsof
conceptsinhumanlanguages,suchthat
eachwordsenseiscomposedofoneor
multiplesememes.Wordsareusually
manuallyannotatedwiththeirsememes
bylinguists,andformlinguisticcommon-
senseknowledgebaseswidelyusedinvar-
iousNLPtasks.Recently,thelexicalse-
memepredictiontaskhasbeenintroduced.
Itconsistsofautomaticallyrecommend-
ingsememesforwords,whichisexpected
toimproveannotationefyandcon-
sistency.However,existingmethodsof
lexicalsememepredictiontypicallyrely
ontheexternalcontextofwordstorep-
resentthemeaning,whichusuallyfails
todealwithlow-frequencyandout-of-
vocabularywords.Toaddressthisissue
forChinese,weproposeanovelframe-
worktotakeadvantageofbothinternal
characterinformationandexternalcontext
informationofwords.Weexperimenton
HowNet,aChinesesememeknowledge
base,anddemonstratethatourframework
outperformsstate-of-the-artbaselinesbya
largemargin,andmaintainsarobustper-
formanceevenforlow-frequencywords.
i
1Introduction
Asememeisanindivisiblesemanticunitforhu-
manlanguagesbylinguists(

,
1926
).Thesemanticmeaningsofconcepts(e.g.,
words)canbecomposedbyanumberofse-
memes.However,thesememesetofawordis

WorkdonewhiledoinginternshipatTsinghuaUniversity.
y
Equalcontribution.HuimingJinproposedtheoverallidea,designedtheexperiment,conductedbothexperiments,and
wrotethepaper;HaoZhumadesuggestionsonensembling,proposedthesecondexperiment,andspentalotoftimeon
proofreadingthepaperandmakingrevisions.Allauthorshelpedshapetheresearch,analysisandmanuscript.
z
Correspondingauthor:Z.Liu(liuzy@tsinghua.edu.cn)
iCodeisavailableat
https://github.com/thunlp/Character-enhanced-Sememe-Prediction
Figure1:Sememesofthewordﬁ
Á
 
ﬂ(iron-
smith)inHowNet,where
occupation
,
human
and
industrial
canbeinferredbybothexternal(con-
texts)andinternal(characters)information,while
metal
iswell-capturedonlybytheinternalinfor-
mationwithinthecharacterﬁ
Á
ﬂ(iron).
notexplicit,whichiswhylinguistsbuildknowl-
edgebases(KBs)toannotatewordswithsememes
manually.
HowNetisaclassicalwidely-usedsememeKB
(
DongandDong
,
2006
).InHowNet,linguists
manuallyapproximately
2
;
000
sememes,
andannotatemorethan
100
;
000
commonwords
inChineseandEnglishwiththeirrelevantse-
memesinhierarchicalstructures.HowNetiswell
developedandhasawiderangeofapplicationsin
manyNLPtasks,suchaswordsensedisambigua-
tion(
Duanetal.
,
2007
),sentimentanalysis(
Fu
etal.
,
2013
;
Huangetal.
,
2014
)andcross-lingual
wordsimilarity(
Xiaetal.
,
2011
).
Sincenewwordsandphrasesareemergingev-
erydayandthesemanticmeaningsofexisting
conceptskeepchanging,itistime-consumingand
work-intensiveforhumanexpertstoannotatenew
arXiv:1806.06349v1  [cs.CL]  17 Jun 2018conceptsandmaintainconsistencyforlarge-scale
sememeKBs.Toaddressthisissue,
Xieetal.
(
2017
)proposeanautomaticsememeprediction
frameworktoassistlinguistannotation.They
assumedthatwordswhichhavesimilarseman-
ticmeaningsarelikelytosharesimilarsememes.
Thus,theyproposetorepresentwordmeanings
asembeddings(
Penningtonetal.
,
2014
;
Mikolov
etal.
,
2013
)learnedfromalarge-scaletextcor-
pus,andtheyadoptcollaborative(
Sar-
waretal.
,
2001
)andmatrixfactorization(
Koren
etal.
,
2009
)forsememeprediction,whicharecon-
cludedasSememePredictionwithWordEmbed-
dings(SPWE)andSememePredictionwithSe-
memeEmbeddings(SPSE)respectively.How-
ever,thosemethodsignoretheinternalinforma-
tionwithinwords(e.g.,thecharactersinChinese
words),whichisalsoforwordunder-
standing,especiallyforwordswhichareoflow-
frequencyordonotappearinthecorpusatall.
Inthispaper,wetakeChineseasanexampleand
exploremethodsoftakingfulladvantageofboth
externalandinternalinformationofwordsforse-
memeprediction.
InChinese,wordsarecomposedofoneormul-
tiplecharacters,andmostcharactershavecorre-
spondingsemanticmeanings.Asshownby
Yin
(
1984
),morethan
90%
ofChinesecharactersin
modernChinesecorporaaremorphemes.Chinese
wordscanbedividedintosingle-morphemewords
andcompoundwords,wherecompoundwordsac-
countforadominantproportion.Themeanings
ofcompoundwordsarecloselyrelatedtotheir
internalcharactersasshowninFig.
1
.Takinga
compoundwordﬁ
Á
 
ﬂ(ironsmith)forinstance,
itconsistsoftwoChinesecharacters:ﬁ
Á
ﬂ(iron)
andﬁ
 
ﬂ(craftsman),andthesemanticmeaning
ofﬁ
Á
 
ﬂcanbeinferredfromthecombination
ofitstwocharacters(
iron
+
craftsman
!
iron-
smith
).Evenforsomesingle-morphemewords,
theirsemanticmeaningsmayalsobededuced
fromtheircharacters.Forexample,bothcharac-
tersofthesingle-morphemewordﬁ
Ÿ−
ﬂ(hover)
representthemeaningofﬁhoverﬂorﬁlingerﬂ.
Therefore,itisintuitivetotaketheinternalchar-
acterinformationintoconsiderationforsememe
prediction.
Inthispaper,weproposeanovelframeworkfor
Character-enhancedSememePrediction(CSP),
whichleveragesbothinternalcharacterinforma-
tionandexternalcontextforsememeprediction.
CSPpredictsthesememecandidatesforatar-
getwordfromitswordembeddingandthecorre-
spondingcharacterembeddings.,we
followSPWEandSPSEasintroducedby
Xieetal.
(
2017
)tomodelexternalinformationandpro-
poseSememePredictionwithWord-to-Character
Filtering(SPWCF)andSememePredictionwith
CharacterandSememeEmbeddings(SPCSE)to
modelinternalcharacterinformation.Inourex-
periments,weevaluateourmodelsonthetaskof
sememepredictionusingHowNet.Theresults
showthatCSPachievesstate-of-the-artperfor-
manceandstaysrobustforlow-frequencywords.
Tosummarize,thekeycontributionsofthis
workareasfollows:(1)Tothebestofourknowl-
edge,thisworkisthetoconsidertheinter-
nalinformationofcharactersforsememepredic-
tion.(2)Weproposeasememepredictionframe-
workconsideringbothexternalandinternalinfor-
mation,andshowtheeffectivenessandrobustness
ofourmodelsonareal-worlddataset.
2RelatedWork
KnowledgeBases.
KnowledgeBases(KBs),
aimingtoorganizehumanknowledgeinstructural
forms,areplayinganincreasinglyimportantrole
asinfrastructuralfacilitiesofintelligence
andnaturallanguageprocessing.KBsrelyon
manualefforts(
Bollackeretal.
,
2008
),automatic
extraction(
Aueretal.
,
2007
),manualevaluation
(
Suchaneketal.
,
2007
),automaticcompletionand
alignment(
Bordesetal.
,
2013
;
Toutanovaetal.
,
2015
;
Zhuetal.
,
2017
)tobuild,verifyandenrich
theircontents.WordNet(
Miller
,
1995
)andBa-
belNet(
NavigliandPonzetto
,
2012
)aretherepre-
sentativeoflinguistKBs,wherewordsofsimilar
meaningsaregroupedtoformthesaurus(
Nastase
andSzpakowicz
,
2001
).Apartfromotherlinguis-
ticKBs,sememeKBssuchasHowNet(
Dongand
Dong
,
2006
)canplayaroleinunder-
standingthesemanticmeaningsofconceptsinhu-
manlanguagesandarefavorableforvariousNLP
tasks:informationstructureannotation(
Ganand
Wong
,
2000
),wordsensedisambiguation(
Gan
etal.
,
2002
),wordrepresentationlearning(
Niu
etal.
,
2017
;
Faruquietal.
,
2015
),andsentiment
analysis(
Fuetal.
,
2013
)interalia.Hence,lexi-
calsememepredictionisanimportanttasktocon-
structsememeKBs.
AutomaticSememePrediction.
Automaticse-
memepredictionisproposedby
Xieetal.
(
2017
).
Forthistask,theyproposeSPWEandSPSE,
whichareinspiredbycollaborative(
Sar-
waretal.
,
2001
)andmatrixfactorization(
Koren
etal.
,
2009
)respectively.SPWErecommendsthe
sememesofthosewordsthatareclosetotheunla-
belledwordintheembeddingspace.SPSElearns
sememeembeddingsbymatrixfactorization(
Ko-
renetal.
,
2009
)withinthesameembeddingspace
ofwords,anditthenrecommendsthemostrele-
vantsememestotheunlabelledwordintheem-
beddingspace.Inthesemethods,wordembed-
dingsarelearnedbasedonexternalcontextinfor-
mation(
Penningtonetal.
,
2014
;
Mikolovetal.
,
2013
)onlarge-scaletextcorpus.Thesemeth-
odsdonotexploitinternalinformationofwords,
andfailtohandlelow-frequencywordsandout-
of-vocabularywords.Inthispaper,wepropose
toincorporateinternalinformationforlexicalse-
memeprediction.
SubwordandCharacterLevelNLP.
Subword
andcharacterlevelNLPmodelstheinternalin-
formationofwords,whichisespeciallyuseful
toaddresstheout-of-vocabulary(OOV)problem.
Morphologyisatypicalresearchareaofsub-
wordlevelNLP.SubwordlevelNLPhasalsobeen
widelyconsideredinmanyNLPapplications,such
askeywordspotting(
Narasimhanetal.
,
2014
),
parsing(
SeekerandC¸etino

glu
,
2015
),machine
translation(
Dyeretal.
,
2010
),speechrecogni-
tion(
Creutzetal.
,
2007
),andparadigmcomple-
tion(
Sutskeveretal.
,
2014
;
Bahdanauetal.
,
2015
;
Cotterelletal.
,
2016a
;
Kannetal.
,
2017
;
Jinand
Kann
,
2017
).Incorporatingsubwordinformation
forwordembeddings(
Bojanowskietal.
,
2017
;
Cotterelletal.
,
2016b
;
Chenetal.
,
2015
;
Wieting
etal.
,
2016
;
Yinetal.
,
2016
)facilitatesmodeling
rarewordsandcanimprovetheperformanceof
severalNLPtaskstowhichtheembeddingsare
applied.Besides,peoplealsoconsidercharacter
embeddingswhichhavebeenutilizedinChinese
wordsegmentation(
Sunetal.
,
2014
).
Thesuccessofpreviousworkvthefeasi-
bilityofutilizinginternalcharacterinformationof
words.Wedesignourframeworkforlexicalse-
memepredictioninspiredbythesemethods.
3BackgroundandNotation
Inthissection,weintroducetheorganization
of
sememes
,
senses
and
words
inHowNet.Then
weofferaformaloflexicalsememepre-
dictionanddevelopournotation.
3.1Sememes,SensesandWordsinHowNet
HowNetprovidessememeannotationsforChi-
nesewords,whereeachwordisrepresentedasa
hierarchicaltree-likesememestructure.
cally,awordinHowNetmayhavevarious
senses
,
whichrespectivelyrepresentthesemanticmean-
ingsofthewordintherealworld.Each
sense
is
asahierarchicalstructureof
sememes
.For
instance,asshownintherightpartofFig.
1
,the
wordﬁ
Á
 
ﬂ(ironsmith)hasonesense,namely
ironsmith
.Thesense
ironsmith
isbythe
sememeﬁ
º
ﬂ(human)whichisbyse-
memeﬁ
L
M
ﬂ(occupation),ﬁ
Ñ
^
ﬂ(metal)and
ﬁ
å
ﬂ(industrial).InHowNet,linguistsuseabout
2
;
000
sememestodescribemorethan
100
;
000
wordsandphrasesinChinesewithvariouscom-
binationsandhierarchicalstructures.
3.2FormalizationoftheTask
Inthispaper,wefocusontherelationshipsbe-
tweenthe
words
andthe
sememes
.Followingthe
settingsof
Xieetal.
(
2017
),wesimplyignorethe
sensesandthehierarchicalstructureofsememes,
andweregardthesememesofallsensesofaword
togetherasthesememesetoftheword.
Wenowintroducethenotationusedinthispa-
per.Let
G
=(
W;S;T
)
denotesthesememe
KB,where
W
=
f
w
1
;w
2
;:::;w
j
W
j
g
isthesetof
words,
S
isthesetofsememes,and
T

W

S
isthesetofrelationpairsbetweenwordsandse-
memes.WedenotetheChinesecharactersetas
C
,witheachword
w
i
2
C
+
.Eachword
w
has
itssememeset
S
w
=
f
s
j
(
w;s
)
2
T
g
.Takethe
wordﬁ
Á
 
ﬂ(ironsmith)forexample,thesememe
set
S
Á
 
(ironsmith)
consistsofﬁ
º
ﬂ(human),ﬁ
L
M
ﬂ(occupation),ﬁ
Ñ
^
ﬂ(metal)andﬁ
å
ﬂ(indus-
trial).
Givenaword
w
2
C
+
,thetaskoflexicalse-
memepredictionaimstopredictthecorrespond-
ing
P
(
s
j
w
)
ofsememesin
S
torecommendthem
to
w
.
4Methodology
Inthissection,wepresentourframeworkforlex-
icalsememeprediction(SP).Foreachunlabelled
word,ourframeworkaimstorecommendthemost
appropriatesememesbasedontheinternalandex-
ternalinformation.Becauseofintroducingchar-
acterinformation,ourframeworkcanworkfor
bothhigh-frequencyandlow-frequencywords.
Ourframeworkistheensembleoftwoparts:
sememepredictionwithinternalinformation(i.e.,
internal
models),andsememepredictionwithex-
ternalinformation(i.e.,
external
models).Explic-
itly,weadoptSPWE,SPSE,andtheirensemble
(
Xieetal.
,
2017
)as
external
models,andwetake
SPWCF,SPCSE,andtheirensembleas
internal
models.
Inthefollowingsections,weintroduce
SPWEandSPSE.Then,weshowthedetails
ofSPWCFandSPCSE.Finally,wepresentthe
methodofmodelensembling.
4.1SPwithExternalInformation
SPWEandSPSEareintroducedby
Xieetal.
(
2017
)asthestateoftheartforsememepredic-
tion.Thesemethodsrepresentwordmeanings
withembeddingslearnedfromexternalinforma-
tion,andapplytheideasofcollaborative
andmatrixfactorizationinrecommendationsys-
temsforsememepredication.
SPwithWordEmbeddings(SPWE)
isbased
ontheassumptionthatsimilarwordsshouldhave
similarsememes.InSPWE,thesimilarityof
wordsaremeasuredbycosinesimilarity.The
scorefunction
P
(
s
j
j
w
)
ofsememe
s
j
givena
word
w
isas:
P
(
s
j
j
w
)
˘
X
w
i
2
W
cos(
w
;
w
i
)

M
ij

c
r
i
;
(1)
where
w
and
w
i
arepre-trainedwordembeddings
ofwords
w
and
w
i
.
M
ij
2f
0
;
1
g
indicates
theannotationofsememe
s
j
onword
w
i
,where
M
ij
=1
indicatestheword
s
j
2
S
w
i
andother-
wiseisnot.
r
i
isthedescendcosinewordsimi-
larityrankbetween
w
and
w
i
,and
c
2
(0
;
1)
isa
hyper-parameter.
SPwithSememeEmbeddings(SPSE)
aims
tomapsememesintothesamelow-dimensional
spaceofthewordembeddingstopredictthese-
manticcorrelationsofthesememesandthewords.
Thismethodlearnstwoembeddings
s
and
s
for
eachsememebysolvingmatrixfactorizationwith
thelossfunctionas:
L
=
X
w
i
2
W;s
j
2
S

w
i

(
s
j
+
s
j
)+
b
i
+
b
0
j

M
ij

2
+

X
s
j
;s
k
2
S
(
s
j

s
k

C
jk
)
2
;
(2)
where
M
isthesamematrixusedinSPWE.
C
indicatesthecorrelationsbetweensememes,in
which
C
jk
isasthepoint-wisemutualin-
formationPMI
(
s
j
;s
k
)
.Thesememeembeddings
arelearnedbyfactorizingtheword-sememema-
trix
M
andthesememe-sememematrix
C
syn-
chronouslywithedwordembeddings.
b
i
and
b
0
j
denotethebiasof
w
i
and
s
j
,and

isahyper-
parameter.Finally,thescoreofsememe
s
j
given
aword
w
isas:
P
(
s
j
j
w
)
˘
w

(
s
j
+
s
j
)
:
(3)
4.2SPwithInternalInformation
Wedesigntwomethodsforsememeprediction
withonlyinternalcharacterinformationwithout
consideringcontextsaswellaspre-trainedword
embeddings.
4.2.1SPwithWord-to-CharacterFiltering
(SPWCF)
Inspiredbycollaborative(
Sarwaretal.
,
2001
),weproposetorecommendsememesfor
anunlabelledwordaccordingtoitssimilarwords
basedoninternalinformation.Insteadofusing
pre-trainedwordembeddings,weconsiderwords
as
similar
iftheycontainthesamecharactersatthe
samepositions.
InChinese,themeaningofacharactermay
varyaccordingtoitspositionwithinaword(
Chen
etal.
,
2015
).Weconsiderthreepositionswithina
word:
Begin
,
Middle
,and
End
.Forexample,
asshowninFig.
2
,thecharacteratthe
Begin
po-
sitionofthewordﬁ
k
f
Ù
ﬂ(railwaystation)is
ﬁ
k
ﬂwhileﬁ
f
ﬂ(vehicle)andﬁ
Ù
ﬂ(station)
areatthe
Middle
and
End
positionrespectively.
Thecharacterﬁ
Ù
ﬂusuallymeans
station
whenit
isatthe
End
position,whileitusuallymeans
stand
atthe
Begin
positionlikeinﬁ
ÙË
ﬂ(stand),ﬁ
Ù
Š
è
u
ﬂ(standingguard)andﬁ
Ù
w
e
ﬂ(stand
up).
Figure2:Anexampleofthepositionofcharacters
inaword.
Formally,foraword
w
=
c
1
c
2
:::c
j
w
j
,wede-

ˇ
B
(
w
)=
f
c
1
g
,
ˇ
M
(
w
)=
f
c
2
;:::;c
j
w

1
j
g
,
ˇ
E
(
w
)=
f
c
j
w
j
g
,and
P
p
(
s
j
j
c
)
˘
P
w
i
2
W
^
c
2
ˇ
p
(
w
i
)
M
ij
P
w
i
2
W
^
c
2
ˇ
p
(
w
i
)
j
S
w
i
j
;
(4)
thatrepresentsthescoreofasememe
s
j
givena
character
c
andaposition
p
,where
ˇ
p
maybe
ˇ
B
,
ˇ
M
,or
ˇ
E
.
M
isthesamematrixusedinEq.(
1
).
Finally,wethescorefunction
P
(
s
j
j
w
)
of
sememe
s
j
givenaword
w
as:
P
(
s
j
j
w
)
˘
X
p
2f
B;M;E
g
X
c
2
ˇ
p
(
w
)
P
p
(
s
j
j
c
)
:
(5)
SPWCFisasimpleandefmethod.It
performswellbecausecompositionalsemantics
arepervasiveinChinesecompoundwords,which
makesitstraightforwardandeffectivetosim-
ilarwordsaccordingtocommoncharacters.
4.2.2SPwithCharacterandSememe
Embeddings(SPCSE)
ThemethodSememePredictionwithWord-to-
CharacterFiltering(SPWCF)caneffectivelyrec-
ommendthesememesthathavestrongcorrela-
tionswithcharacters.However,justlikeSPWE,
itignorestherelationsbetweensememes.Hence,
inspiredbySPSE,weproposeSememePredic-
tionwithCharacterandSememeEmbeddings
(SPCSE)totaketherelationsbetweensememes
intoaccount.InSPCSE,weinsteadlearnthese-
memeembeddingsbasedoninternalcharacterin-
formation,thencomputethesemanticdistancebe-
tweensememesandwordsforprediction.
InspiredbyGloVe(
Penningtonetal.
,
2014
)and
SPSE,weadoptmatrixfactorizationinSPCSE,
bydecomposingtheword-sememematrixand
thesememe-sememematrixsimultaneously.In-
steadofusingpre-trainedwordembeddingsin
SPSE,weusepre-trainedcharacterembeddings
inSPCSE.Sincetheambiguityofcharactersis
strongerthanthatofwords,multipleembeddings
arelearnedforeachcharacter(
Chenetal.
,
2015
).
Weselectthemostrepresentativecharacterand
itsembeddingtorepresentthewordmeaning.
Becauselow-frequencycharactersaremuchrare
thanthoselow-frequencywords,andevenlow-
frequencywordsareusuallycomposedofcom-
moncharacters,itisfeasibletousepre-trained
characterembeddingstorepresentrarewords.
Duringfactorizingtheword-sememematrix,the
characterembeddingsareed.
Weset
N
e
asthenumberofembeddingsfor
eachcharacter,andeachcharacter
c
has
N
e
em-
beddings
c
1
;:::;
c
N
e
.Givenaword
w
andase-
meme
s
,weselecttheembeddingofacharac-
terof
w
closesttothesememeembeddingbyco-
sinedistanceastherepresentationoftheword
w
,
Figure3:Anexampleofadoptingmultiple-
prototypecharacterembeddings.Thenumbersare
thecosinedistances.Thesememeﬁ
Ñ
^
ﬂ(metal)
istheclosesttooneembeddingofﬁ
Á
ﬂ(iron).
asshowninFig.
3
.,givenaword
w
=
c
1
:::c
j
w
j
andasememe
s
j
,we
^
k;
^
r
=argmin
k;r

1

cos(
c
r
k
;
(
s
0
j
+
s
0
j
))

;
(6)
where
^
k
and
^
r
indicatetheindicesofthecharacter
anditsembeddingclosesttothesememe
s
j
inthe
semanticspace.Withthesameword-sememema-
trix
M
andsememe-sememecorrelationmatrix
C
inEq.(
2
),welearnthesememeembeddingswith
thelossfunction:
L
=
X
w
i
2
W;s
j
2
S

c
^
r
^
k


s
0
j
+
s
0
j

+
b
c
^
k
+
b
00
j

M
ij

2
+

0
X
s
j
;s
q
2
S

s
0
j

s
0
q

C
jq

2
;
(7)
where
s
0
j
and
s
0
j
arethesememeembeddingsfor
sememe
s
j
,and
c
^
r
^
k
istheembeddingofthechar-
acterthatistheclosesttosememe
s
j
within
w
i
.
Notethat,asthecharactersandthewordsarenot
embeddedintothesamesemanticspace,welearn
newsememeembeddingsinsteadofusingthose
learnedinSPSE,henceweusedifferentnotations
forthesakeofdistinction.
b
c
k
and
b
00
j
denotethe
biasesof
c
k
and
s
j
,and

0
isthehyper-parameter
adjustingthetwoparts.Finally,thescorefunction
ofword
w
=
c
1
:::c
j
w
j
isas:
P
(
s
j
j
w
)
˘
c
^
r
^
k


s
0
j
+
s
0
j

:
(8)
4.3ModelEnsembling
SPWCF/SPCSEandSPWE/SPSEtakediffer-
entsourcesofinformationasinput,whichmeans
thattheyhavedifferentcharacteristics:SPWCF/
SPCSEonlyhaveaccesstointernalinformation,
whileSPWE/SPSEcanonlymakeuseofexternal
information.Ontheotherhand,justlikethedif-
ferencebetweenSPWEandSPSE,SPWCForigi-
natesfromcollaborativewhereasSPCSE
usesmatrixfactorization.Allofthosemethods
haveincommonthattheytendtorecommendthe
sememesof
similar
words,buttheydivergein
theirinterpretationof
similar
.
Figure4:Theillustrationofmodelensembling.
Hence,toobtainbetterpredictionperformance,
itisnecessarytocombinethesemodels.Wede-
notetheensembleofSPWCFandSPCSEasthe
internal
model,andwedenotetheensembleof
SPWEandSPSEasthe
external
model.The
ensembleofthe
internal
andthe
external
mod-
elsisournovelframeworkCSP.Inpractice,for
wordswithreliablewordembeddings,i.e.,high-
frequencywords,wecanusetheintegrationofthe
internal
andthe
external
models;forwordswith
extremelylowfrequencies(e.g.,havingnoreliable
wordembeddings),wecanjustusethe
internal
modelandignorethe
external
model,becausethe
externalinformationisnoiseinthiscase.Fig.
4
showsmodelensemblingindifferentscenarios.
Forthesakeofcomparison,weusetheintegration
ofSPWCF,SPCSE,SPWE,andSPSEasCSPin
ourallexperiments.Inthispaper,twomodelsare
integratedbysimpleweightedaddition.
5Experiments
Inthissection,weevaluateourmodelsonthetask
ofsememeprediction.Additionally,weanalyze
theperformanceofdifferentmethodsforvarious
wordfrequencies.Wealsoexecuteanelaborate
casestudytodemonstratethemechanismofour
methodsandtheadvantagesofusinginternalin-
formation.
5.1Dataset
Weusethehuman-annotatedsememeKBHowNet
forsememeprediction.InHowNet,
103
;
843
wordsareannotatedwith
212
;
539
senses,and
eachsenseisasahierarchicalstructure
ofsememes.Thereareabout
2
;
000
sememesin
HowNet.However,thefrequenciesofsomese-
memesinHowNetareverylow,suchthatwecon-
siderthemunimportantandremovethem.Our
naldatasetcontains
1
;
400
sememes.Forlearning
thewordandcharacterembeddings,weusethe
Sogou-Tcorpus
ii
(
Liuetal.
,
2012
),whichcontains
2
:
7
billionwords.
5.2ExperimentalSettings
Inourexperiments,weevaluateSPWCF,SPCSE,
andSPWCF+SPCSEwhichonlyuseinternal
information,andtheensembleframeworkCSP
whichusesbothinternalandexternalinforma-
tionforsememeprediction.Weusethestate-
of-the-artmodelsfrom
Xieetal.
(
2017
)asour
baselines.Additionally,weusetheSPWEmodel
withwordembeddingslearnedbyfastText(
Bo-
janowskietal.
,
2017
)thatconsidersbothinternal
andexternalinformationasabaseline.
Fortheconvenienceofcomparison,weselect
60
;
000
high-frequencywordsinSogou-Tcorpus
fromHowNet.Wedividethe
60
;
000
wordsinto
train,dev,andtestsetsofsize
48
;
000
,
6
;
000
,
and
6
;
000
,respectively,andwekeepthemed
throughoutallexperimentsexceptforSection
5.4
.
InSection
5.4
,weutilizethesametrainanddev
sets,butuseotherwordsfromHowNetasthetest
settoanalyzetheperformanceofourmethodsfor
differentwordfrequencyscenarios.Weselectthe
hyper-parametersonthedevsetforallmodelsin-
cludingthebaselinesandreporttheevaluationre-
sultsonthetestset.
Wesetthedimensionsoftheword,sememe,
andcharacterembeddingstobe
200
.Theword
embeddingsarelearnedbyGloVe(
Pennington
etal.
,
2014
).Forthebaselines,inSPWE,the
hyper-parameter
c
issetto
0
:
8
,andthemodelcon-
sidersnomorethan
K
=100
nearestwords.We
settheprobabilityofdecomposingzeroelements
intheword-sememematrixinSPSEtobe
0
:
5%
.

inEq.(
2
)is
0
:
5
.Themodelistrainedfor
20
epochs,andtheinitiallearningrateis
0
:
01
,which
decreasesthroughiterations.ForfastText,weuse
skip-gramwithhierarchicalsoftmaxtolearnword
embeddings,andwesettheminimumlengthof
charactern-gramstobe1andthemaximumlength
iiSogou-TcorpusisprovidedbySogouInc.,aChinese
commercialsearchenginecompany.
https://www.
sogou.com/labs/resource/t.php
ofcharactern-gramstobe2.Formodelensem-
bling,weuse

SPWE

SPSE
=2
:
1
astheadditionweight.
ForSPCSE,weuseCluster-basedCharacter
Embeddings(
Chenetal.
,
2015
)tolearnpre-
trainedcharacterembeddings,andweset
N
e
to
be
3
.Weset

0
inEq.(
7
)tobe
0
:
1
,andthemodel
istrainedfor
20
epochs.Theinitiallearningrateis
0
:
01
anddecreasesduringtrainingaswell.Since
generallyeachcharactercanrelatetoabout
15
-
20
sememes,wesettheprobabilityofdecompos-
ingzeroelementsintheword-sememematrixin
SPCSEtobe
2
:
5%
.TheensembleweightofSP-
WCFandSPCSE

SPWCF

SPCSE
=4
:
0
.Forbetterper-
formanceoftheensemblemodelCSP,weset

=0
:
1
and

SPWE

SPSE
=0
:
3125
,though
0
:
5
and
2
:
1
arethebestforSPSEandSPWE+SPSE.Finally,
wechoose

internal

external
=1
:
0
tointegratethe
internal
and
external
models.
5.3SememePrediction
5.3.1EvaluationProtocol
Thetaskofsememepredictionaimstorecom-
mendappropriatesememesforunlabelledwords.
Wecastthisasamulti-labeltask,
andadoptmeanaverageprecision(MAP)asthe
evaluationmetric.Foreachunlabelledwordinthe
testset,werankallsememecandidateswiththe
scoresgivenbyourmodelsaswellasbaselines,
andwereporttheMAPresults.Theresultsare
reportedonthetestset,andthehyper-parameters
aretunedonthedevset.
5.3.2ExperimentResults
TheevaluationresultsareshowninTable
1
.We
canobservethat:
MethodMAP
SPSE0.411
SPWE0.565
SPWE+SPSE0.577
SPWCF0.467
SPCSE0.331
SPWCF+SPCSE
0.483
SPWE+fastText0.531
CSP
0.654
Table1:Evaluationresultsonsememeprediction.
TheresultofSPWCF+SPCSEisboldforcom-
paringwithothermethods(SPWCFandSPCSE)
whichuseonlyinternalinformation.
(1)Considerableimprovementsareobtainedvia
modelensembling,andtheCSPmodelachieves
state-of-the-artperformance.CSPcombinesthe
internalcharacterinformationwiththeexternal
contextinformation,whichandcon-
sistentlyimprovesperformanceonsememepre-
diction.Ourresultstheeffectivenessofa
combinationofinternalandexternalinformation
forsememeprediction;sincedifferentmodelsfo-
cusondifferentfeaturesoftheinputs,theensem-
blemodelcanabsorbtheadvantagesofbothmeth-
ods.
(2)TheperformanceofSPWCF+SPCSEis
betterthanthatofSPSE,whichmeansusingonly
internalinformationcouldalreadygivegoodre-
sultsforsememepredictionaswell.Moreover,
in
internal
models,SPWCFperformsmuchbetter
thanSPCSE,whichalsoimpliesthestrongpower
ofcollaborative
(3)TheperformanceofSPWCF+SPCSEis
worsethanSPWE+SPSE.Thisindicatesthatit
isstilldiftooutthesemanticmean-
ingsofawordwithoutcontextualinformation,due
totheambiguityandmeaningvaguenessofin-
ternalcharacters.Moreover,somewordsarenot
compoundwords(e.g.,single-morphemewordsor
transliteratedwords),whosemeaningscanhardly
beinferreddirectlybytheircharacters.InChi-
nese,internalcharacterinformationisjustpartial
knowledge.WepresenttheresultsofSPWCFand
SPCSEmerelytoshowthecapabilitytousethein-
ternalinformationinisolation.Inourcasestudy,
wewilldemonstratethat
internal
modelsarepow-
erfulforlow-frequencywords,andcanbeusedto
predictsensesthatdonotappearinthecorpus.
5.4AnalysisonDifferentWordFrequencies
Toverifytheeffectivenessofourmodelsondiffer-
entwordfrequencies,weincorporatetheremain-
ingwordsinHowNet
iii
intothetestset.Sincethe
remainingwordsarelow-frequency,wemainlyfo-
cusonwordswithlong-taildistribution.Wecount
thenumberofoccurrencesinthecorpusforeach
wordinthetestsetandgroupthemintoeightcat-
egoriesbytheirfrequency.Theevaluationresults
areshowninTable
2
,fromwhichwecanobserve
that:
iiiIndetail,wedonotusethenumeralwords,punctua-
tions,single-characterwords,thewordsdonotappearin
Sogou-Tcorpus(becausetheyneedtoappearatleastfor
onetimetogetthewordembeddings),andforeignabbre-
viations.
wordfrequency
6
5051Œ100101Œ1,0001,001Œ5,0005,001Œ10,00010,001Œ30,000
>
30,000
occurrences
8537486832362036663753686
SPWE
0.3120.4370.4810.5580.5490.5560.509
SPSE
0.1870.2730.3390.4090.4070.4240.386
SPWE+SPSE
0.2840.4140.4780.5560.5480.5540.511
SPWCF
0.4560.4140.4000.4430.4620.4630.479
SPCSE
0.3090.2910.2860.3120.3390.3530.342
SPWCF+SPCSE
0.4670.4370.4180.4560.4770.4770.494
SPWE+fastText
0.4950.4720.4620.5200.5080.4990.490
CSP
0.5270.5550.5550.6260.6320.6410.624
Table2:MAPscoresonsememepredictionwithdifferentwordfrequencies.
wordsmodelsTop5sememes

h
 
(clockmaker)
internal
ººº
(human)
,
LLL
MMM
(occupation)
,
è
ö
(part),
ööö
ôôô
(time)
,
JJJ
ÉÉÉ
(tell)
external
ººº
(human)
,

(ProperName),
0
¹
(place),
'
2
(Europe),
?
(politics)
ensemble
ººº
(human)
,
LLL
MMM
(occupation)
,
JJJ
ÉÉÉ
(tell)
,
ööö
ôôô
(time)
,
(((
www
(tool)
e
¯
a
(Oscar)
internal

(ProperName)
,
0
¹
(place),

(city),
º
(human),
ý
ý
(capital)
external
VVV
±±±
(reward)
,
zzz
(entertainment)
,

(ProperName)
,
(
w
(tool),
‰‰‰
ÅÅÅ
(fact)
ensemble

(ProperName)
,
VVV
±±±
(reward)
,
zzz
(entertainment)
,
W

(famous),
0
¹
(place)
Table3:Examplesofsememeprediction.Foreachword,wepresentthetop
5
sememespredictedbythe
internal
model,
external
modelandtheensemblemodel(CSP).Boldsememesarecorrect.
(1)TheperformancesofSPSE,SPWE,and
SPWE+SPSEdecreasedramaticallywith
low-frequencywordscomparedtothosewith
high-frequencywords.Onthecontrary,the
performancesofSPWCF,SPCSE,andSP-
WCF+SPCSE,thoughweakerthanthatonhigh-
frequencywords,isnotstronglyinthe
long-tailscenario.TheperformanceofCSPalso
dropssinceCSPalsousesexternalinformation,
whichisnotsufwithlow-frequencywords.
Theseresultsshowthatthewordfrequenciesand
thequalityofwordembeddingscanthe
performanceofsememepredictionmethods,es-
peciallyfor
external
modelswhichmainlycon-
centrateontheworditself.However,the
internal
modelsaremorerobustwhenencounteringlong-
taildistributions.Althoughwordsdonotneed
toappeartoomanytimesforlearninggoodword
embeddings,itisstillhardfor
externalmodels
torecommendsememesforlow-frequencywords.
Whilesince
internal
modelsdonotuseexternal
wordembeddings,theycanstillworkinsuchsce-
nario.Asfortheperformanceonhigh-frequency
words,sincethesewordsareusedwidely,the
ambiguityofhigh-frequencywordsisthusmuch
stronger,whilethe
internal
modelsarestillstable
forhigh-frequencywords.
(2)Theresultsalsoindicatethatevenlow-
frequencywordsinChinesearemostlycomposed
ofcommoncharacters,andthusitispossible
toutilizeinternalcharacterinformationforse-
memepredictiononwordswithlong-taildistribu-
tion(evenonthosenewwordsthatneverappear
inthecorpus).Moreover,thestabilityoftheMAP
scoresgivenbyourmethodsonvariouswordfre-
quenciesalsothereliabilityanduniversal-
ityofourmodelsinreal-worldsememeannota-
tionsinHowNet.Wewillgivedetailedanalysisin
ourcasestudy.
5.5CaseStudy
Theresultsofourmainexperimentsalreadyshow
theeffectivenessofourmodels.Inthiscasestudy,
wefurtherinvestigatetheoutputsofourmodels
tothatcharacter-levelknowledgeistruly
incorporatedintosememeprediction.
InTable
3
,wedemonstratethetop
5
sememes
forﬁ

h
 
ﬂ(clockmaker)andﬁ
e
¯
a
ﬂ(Os-
car,i.e.,theAcademyAwards).ﬁ

h
 
ﬂ(clock-
maker)isatypicalcompoundword,whileﬁ
e
¯
a
ﬂ(Oscar)isatransliteratedword.Foreach
word,thetop
5
resultsgeneratedbytheinternal
model(SPWCF+SPCSE),the
external
model
(SPWE+SPSE)andtheensemblemodel(CSP)
arelisted.
Thewordﬁ

h
 
ﬂ(clockmaker)iscomposed
ofthreecharacters:ﬁ

ﬂ(bell,clock),ﬁ
h
ﬂ(clock,
watch)andﬁ
 
ﬂ(craftsman).Humanscanintu-
itivelyconcludethat
clock
+
craftsman
!
clock-
maker
.However,the
external
modeldoesnotper-
formwellforthisexample.Ifweinvestigatethe
wordembeddingofﬁ

h
 
ﬂ(clockmaker),we
canknowwhythismethodrecommendstheseun-
reasonablesememes.Theclosest
5
wordsinthe
trainsettoﬁ

h
 
ﬂ(clockmaker)bycosinesim-
ilarityoftheirembeddingsare:ﬁ
^
ë
ﬂ(Switzer-
land),ﬁ
b

ﬂ(Jean-JacquesRousseau),ﬁ
‰
 
ﬂ
(cobbler),ﬁ
Ñ

¶
ﬂ(inventor)andﬁ
e
0
)
º
ﬂ
(Austrian).Notethatnoneofthesewordsaredi-
rectlyrelevantto
bells
,
clocks
or
watches
.Hence,
thesememesﬁ
ö
ô
ﬂ(time),ﬁ
J
É
ﬂ(tell),andﬁ
(
w
ﬂ(tool)cannotbeinferredbythosewords,even
thoughthecorrelationsbetweensememesarein-
troducedbySPSE.Infact,thosewordsarerelated
to
clocks
inanindirectway:Switzerlandisfa-
mousforwatchindustry;Rousseauwasborninto
afamilythathadatraditionofwatchmaking;cob-
blerandinventoraretwokindsofoccupationsas
well.Withtheabovereasons,thosewordsusu-
allyco-occurwithﬁ

h
 
ﬂ(clockmaker),orusu-
allyappearinsimilarcontextsasﬁ

h
 
ﬂ(clock-
maker).Itindicatesthatrelatedwordembeddings
asusedinan
external
modeldonotalwaysrecom-
mendrelatedsememes.
Thewordﬁ
e
¯
a
ﬂ(Oscar)iscreatedbythe
pronunciationof
Oscar
.Therefore,themeaning
ofeachcharacterinﬁ
e
¯
a
ﬂ(Oscar)isunrelated
tothemeaningoftheword.Moreover,thechar-
actersﬁ
e
ﬂ,ﬁ
¯
ﬂ,andﬁ
a
ﬂarecommonamong
transliteratedwords,thusthe
internal
methodrec-
ommendsﬁ

ﬂ(ProperName)andﬁ
0
¹
ﬂ(place),
etc.,sincemanytransliteratedwordsareproper
nounsorplacenames.
6ConclusionandFutureWork
Inthispaper,weintroducedcharacter-levelinter-
nalinformationforlexicalsememepredictionin
Chinese,inordertoalleviatetheproblemscaused
bytheexclusiveuseofexternalinformation.We
proposedaCharacter-enhancedSememePredic-
tion(CSP)frameworkwhichintegratesbothinter-
nalandexternalinformationforlexicalsememe
predictionandproposedtwomethodsforutiliz-
inginternalinformation.WeevaluatedourCSP
frameworkontheclassicalmanuallyannotatedse-
memeKBHowNet.Inourexperiments,ourmeth-
odsachievedpromisingresultsandoutperformed
thestateoftheartonsememeprediction,espe-
ciallyforlow-frequencywords.
Wewillexplorethefollowingresearchdirec-
tionsinthefuture:(1)ConceptsinHowNetarean-
notatedwithhierarchicalstructuresofsensesand
sememes,butthosearenotconsideredinthispa-
per.Inthefuture,wewilltakestructuredanno-
tationsintoaccount.(2)Itwouldbemeaningful
totakemoreinformationintoaccountforblend-
ingexternalandinternalinformationanddesign
moresophisticatedmethods.(3)BesidesChinese,
manyotherlanguageshaverichsubword-levelin-
formation.Inthefuture,wewillexploremeth-
odsofexploitinginternalinformationinotherlan-
guages.(4)Webelievethatsememesareuniversal
forallhumanlanguages.Wewillexploreageneral
frameworktorecommendandutilizesememesfor
otherNLPtasks.
Acknowledgments
ThisresearchispartoftheNExT++project,
supportedbytheNationalResearchFounda-
tion,PrimeMinister'sOfSingaporeunderits
IRC@SingaporeFundingInitiative.Thisworkis
alsosupportedbytheNationalNaturalScience
FoundationofChina(NSFCNo.61661146007
and61572273)andtheresearchfundofTsinghua
University-TencentJointLaboratoryforInternet
InnovationTechnology.HaoZhuissupportedby
TsinghuaUniversityInitiativeResearch
Program.WewouldliketothankKatharinaKann,
ShenJin,andtheanonymousreviewersfortheir
helpfulcomments.
References
S
¨
orenAuer,ChristianBizer,GeorgiKobilarov,Jens
Lehmann,RichardCyganiak,andZacharyIves.
2007.Dbpedia:Anucleusforawebofopendata.
In
ProceedingsofISWC
,pages722Œ735.
DzmitryBahdanau,KyunghyunCho,andYoshuaBen-
gio.2015.Neuralmachinetranslationbyjointly
learningtoalignandtranslate.In
Proceedingsof
ICLR
.
Leonard1926.Asetofpostulatesforthe
scienceoflanguage.
Language
,2(3):153Œ164.
PiotrBojanowski,EdouardGrave,ArmandJoulin,and
TomasMikolov.2017.Enrichingwordvectorswith
subwordinformation.
TACL
,5:135Œ146.
KurtBollacker,ColinEvans,PraveenParitosh,Tim
Sturge,andJamieTaylor.2008.Freebase:Acollab-
orativelycreatedgraphdatabaseforstructuringhu-
manknowledge.In
ProceedingsofSIGMOD
,pages
1247Œ1250.
AntoineBordes,NicolasUsunier,AlbertoGarcia-
Duran,JasonWeston,andOksanaYakhnenko.
2013.Translatingembeddingsformodelingmulti-
relationaldata.In
ProceedingsofNIPS
,pages
2787Œ2795.
XinxiongChen,LeiXu,ZhiyuanLiu,MaosongSun,
andHuan-BoLuan.2015.Jointlearningofcharac-
terandwordembeddings.In
ProceedingsofIJCAI
,
pages1236Œ1242.
RyanCotterell,ChristoKirov,JohnSylak-Glassman,
DavidYarowsky,JasonEisner,andMansHulden.
2016a.TheSIGMORPHON2016shared
taskŠmorphologicalIn
Proceed-
ingsofSIGMORPHON
,pages10Œ22.
RyanCotterell,HinrichSch
¨
utze,andJasonEisner.
2016b.Morphologicalsmoothingandextrapolation
ofwordembeddings.In
ProceedingsofACL
,pages
1651Œ1660.
MathiasCreutz,TeemuHirsim
¨
aki,MikkoKurimo,
AnttiPuurula,JannePylkk
¨
onen,VesaSiivola,Matti
Varjokallio,EbruArisoy,MuratSarac¸lar,andAn-
dreasStolcke.2007.Analysisofmorph-based
speechrecognitionandthemodelingofout-of-
vocabularywordsacrosslanguages.In
Processings
ofHLT-NAACL
,pages380Œ387.
ZhendongDongandQiangDong.2006.
HowNetand
thecomputationofmeaning
.World
XiangyuDuan,JunZhao,andBoXu.2007.Word
sensedisambiguationthroughsememelabeling.In
ProceedingsofIJCAI
,pages1594Œ1599.
ChrisDyer,JonathanWeese,HendraSetiawan,Adam
Lopez,FerhanTure,VladimirEidelman,JuriGan-
itkevitch,PhilBlunsom,andPhilipResnik.2010.
cdec:Adecoder,alignment,andlearningframework
forandcontext-freetranslationmodels.
In
ProceedingsoftheACL2010SystemDemonstra-
tions
,pages7Œ12.
ManaalFaruqui,JesseDodge,SujayKumarJauhar,
ChrisDyer,EduardHovy,andNoahA.Smith.2015.
wordvectorstosemanticlexicons.In
ProceedingsofHLT-NAACL
,pages1606Œ1615.
XianghuaFu,LiuGuo,GuoYanyan,andWang
Zhiqiang.2013.Multi-aspectsentimentanalysisfor
Chineseonlinesocialreviewsbasedontopicmod-
elingandHowNetlexicon.
Knowledge-BasedSys-
tems
,37:186Œ195.
Kok-WeeGan,Chi-YungWang,andBrianMak.2002.
Knowledge-basedsensepruningusingtheHowNet:
analternativetowordsensedisambiguation.In
Pro-
ceedingsofISCSLP
.
KokWeeGanandPingWaiWong.2000.Annotat-
inginformationstructuresinChinesetextsusing
HowNet.In
ProceedingsofTheSecondChinese
LanguageProcessingWorkshop
,pages85Œ92.
MinlieHuang,BoruiYe,YichenWang,Haiqiang
Chen,JunjunCheng,andXiaoyanZhu.2014.New
worddetectionforsentimentanalysis.In
Proceed-
ingsofACL
,pages531Œ541.
HuimingJinandKatharinaKann.2017.Exploring
cross-lingualtransferofmorphologicalknowledge
insequence-to-sequencemodels.In
Proceedingsof
SCLeM
,pages70Œ75.
KatharinaKann,RyanCotterell,andHinrichSch
¨
utze.
2017.One-shotneuralcross-lingualtransferfor
paradigmcompletion.In
ProceedingsofACL
,pages
1993Œ2003.
YehudaKoren,RobertBell,andChrisVolinsky.2009.
Matrixfactorizationtechniquesforrecommender
systems.
Computer
,42(8).
YiqunLiu,FeiChen,WeizeKong,HuijiaYu,Min
Zhang,ShaopingMa,andLiyunRu.2012.Iden-
tifyingwebspamwiththewisdomofthecrowds.
ACMTransactionsontheWeb
,6(1):2:1Œ2:30.
TomasMikolov,IlyaSutskever,KaiChen,GregSCor-
rado,andJeffDean.2013.Distributedrepresenta-
tionsofwordsandphrasesandtheircompositional-
ity.In
ProceedingsofNIPS
,pages3111Œ3119.
GeorgeAMiller.1995.WordNet:Alexical
databaseforEnglish.
CommunicationsoftheACM
,
38(11):39Œ41.
KarthikNarasimhan,DamianosKarakos,Richard
Schwartz,StavrosTsakalidis,andReginaBarzilay.
2014.Morphologicalsegmentationforkeyword
spotting.In
ProceedingsofEMNLP
,pages880Œ
885.
ViviNastaseandStanSzpakowicz.2001.Wordsense
disambiguationinRoget'sthesaurususingWordNet.
In
ProceedingsoftheWorkshoponWordNetand
OtherLexicalResources:Applications,Extensions
andCustomizations
.
RobertoNavigliandSimonePaoloPonzetto.2012.
BabelNet:Theautomaticconstruction,evaluation
andapplicationofawide-coveragemultilingualse-
manticnetwork.
Intelligence
,193:217Œ
250.
YilinNiu,RuobingXie,ZhiyuanLiu,andMaosong
Sun.2017.Improvedwordrepresentationlearning
withsememes.In
ProceedingsofACL
,pages2049Œ
2058.
JeffreyPennington,RichardSocher,andChristopher
Manning.2014.Glove:Globalvectorsforword
representation.In
ProceedingsofEMNLP
,pages
1532Œ1543.
BadrulSarwar,GeorgeKarypis,JosephKonstan,and
JohnRiedl.2001.Item-basedcollaborative-
ingrecommendationalgorithms.In
Proceedingsof
WWW
,pages285Œ295.
WolfgangSeekerand
¨
OzlemC¸etino

glu.2015.A
graph-basedlatticedependencyparserforjoint
morphologicalsegmentationandsyntacticanalysis.
TACL
,3:359Œ373.
FabianM.Suchanek,GjergjiKasneci,andGerhard
Weikum.2007.Yago:Acoreofsemanticknowl-
edge.In
ProceedingsofWWW
,pages697Œ706.
YamingSun,LeiLin,NanYang,ZhenzhouJi,and
XiaolongWang.2014.Radical-enhancedChinese
characterembedding.In
ProceedingsofICONIP
,
pages279Œ286.
IlyaSutskever,OriolVinyals,andQuocV.Le.2014.
Sequencetosequencelearningwithneuralnet-
works.In
ProceedingsofNIPS
,pages3104Œ3112.
KristinaToutanova,DanqiChen,PatrickPantel,Hoi-
fungPoon,PallaviChoudhury,andMichaelGamon.
2015.Representingtextforjointembeddingoftext
andknowledgebases.In
ProceedingsofEMNLP
,
pages1499Œ1509.
JohnWieting,MohitBansal,KevinGimpel,andKaren
Livescu.2016.Charagram:Embeddingwordsand
sentencesviacharactern-grams.In
Proceedingsof
EMNLP
,pages1504Œ1515.
YunqingXia,TaotaoZhao,JianminYao,andPeng
Jin.2011.MeasuringChinese-Englishcross-lingual
wordsimilaritywithHowNetandparallelcorpus.In
ProceedingsofCICLing
,pages221Œ233.Springer.
RuobingXie,XingchiYuan,ZhiyuanLiu,and
MaosongSun.2017.Lexicalsememepredictionvia
wordembeddingsandmatrixfactorization.In
Pro-
ceedingsofIJCAI
,pages4200Œ4206.
BinyongYin.1984.QuantitativeresearchonChi-
nesemorphemes.
StudiesoftheChineseLanguage
,
5:338Œ347.
RongchaoYin,QuanWang,PengLi,RuiLi,andBin
Wang.2016.Multi-granularityChinesewordem-
bedding.In
ProceedingsofEMNLP
,pages981Œ
986.
HaoZhu,RuobingXie,ZhiyuanLiu,andMaosong
Sun.2017.Iterativeentityalignmentviajoint
knowledgeembeddings.In
ProceedingsofIJCAI
,
pages4258Œ4264.
"
31,Investigating Generative Adversarial Networks based Speech Dereverberation for Robust Speech Recognition,http://arxiv.org/pdf/1803.10132v3.pdf,https://github.com/wangkenpu/rsrgan,"InvestigatingGenerativeAdversarialNetworksbasedSpeechDereverberation
forRobustSpeechRecognition
KeWang
1
;
2
,JunboZhang
2
,SiningSun
1
,YujunWang
2
,FeiXiang
2
,LeiXie
1

1
ShaanxiProvincialKeyLaboratoryofSpeechandImageInformationProcessing,
SchoolofComputerScience,NorthwesternPolytechnicalUniversity,Xi'an,China
2
Xiaomi,Beijing,China
f
kewang,snsun,lxie
g
@nwpu-aslp.org,
f
zhangjunbo,wangyujun,xiangfei
g
@xiaomi.com
Abstract
Weinvestigatetheuseofgenerativeadversarialnetworks
(GANs)inspeechdereverberationforrobustspeechrecogni-
tion.GANshavebeenrecentlystudiedforspeechenhancement
toremoveadditivenoises,buttherestilllacksofaworktoex-
aminetheirabilityinspeechdereverberationandtheadvantages
ofusingGANshavenotbeenfullyestablished.Inthispaper,
weprovidedeepinvestigationsintheuseofGAN-baseddere-
verberationfront-endinASR.First,westudytheeffectiveness
ofdifferentdereverberationnetworks(thegeneratorinGAN)
andthatLSTMleadstoaimprovementascom-
paredwithfeed-forwardDNNandCNNinourdataset.Sec-
ond,furtheraddingresidualconnectionsinthedeepLSTMs
canboosttheperformanceaswell.Finally,wethat,for
thesuccessofGAN,itisimportanttoupdatethegeneratorand
thediscriminatorusingthesamemini-batchdataduringtrain-
ing.Moreover,usingreverberantspectrogramasacondition
todiscriminator,assuggestedinpreviousstudies,maydegrade
theperformance.Insummary,ourGAN-baseddereverberation
front-endachieves14%
˘
19%relativeCERreductionascom-
paredtothebaselineDNNdereverberationnetworkwhentested
onastrongmulti-conditiontrainingacousticmodel.
IndexTerms
:Speechdereverberation,robustspeechrecogni-
tion,generativeadversarialnets,residualnetworks
1.Introduction
Theperformanceofautomaticspeechrecognition(ASR)has
beenboostedtremendouslyinthelastseveralyearsandstate-of-
the-artsystemscanevenreachtheperformanceofprofessional
humantranscribersinsomeconditions[1,2].However,room
reverberationoftenseriouslydegradestheASRperformance,
especiallyinfarspeechrecognitionwherethetalkeris
awayfromthemicrophone[3,4].Therefore,moreattention
hasbeenpaidrecentlyintheresearchcommunitytoaddress
thisissue.
Intheory,reverberantspeechcanberegardedasaroomim-
pulseresponse(RIR)convolvingwiththecleanspeechinthe
timedomain[5].Astraightforwardapproachiscalled
speech
dereverberation
,i.e.,removethereverberationfromthecon-
taminatedspeech.Inthistrack,microphonearrayandmulti-
channelsignalprocessingareveryhelpful[6,7],butsingle-
channelspeechreverberationisstilldesirableinmanyrealap-
plicationsinwhichusingmultiplemicrophonesmaybeim-
practical.Single-microphonespeechdereverberationhasbeen
intensivelystudiedinthesignalprocessingcommunityanda
varietyofapproacheshavebeenproposed[5,8,9,10,11].
Anotherapproachtodealwithreverberation(andnoise)in
speechrecognitionismulti-conditiontraining(MCT),inwhich
speechcontaminatedwithreverberation,eithersimulatedor
real-recorded,isaddedintheacousticmodeltrainingset,let-
tingthemodellearnthereverberationeffectsautomatically.Al-
thoughtheaboveapproachesarereasonablyeffective,itisstill
farawayfromclaimingsuccessintheagainstreverbera-
tioninspeechrecognition.
Recently,duetotheirstrongregressionlearningabilities,
deepneutralnetworks(DNNs)havebeenusedinspeechen-
*Correspondingauthor
hancement[12]andlaterinspeechdereverberation[3,4,13].
Thedeepstructurecanbenaturallyregardedasadereverbera-
tionthatcanlearntheessentialrelationshipbetweenthe
reverberantspeechanditscounterpart,i.e.,thecleanspeech,
throughasetofmulti-conditiondata.Variousdeepstructures,
e.g.,feed-forward[14],recurrent[15]andconvolutional[16],
havebeenexploredintheeld.Eitherdirectspectralmap-
ping[4,14]ormasking[17]canbeconsideredinthedere-
verberationnetwork.Inthetypicalspectralmappingap-
proach[12],themulti-conditiondatasetusedinthenetwork
trainingusuallyconsistsofpairsofreverberantandcleanspeech
representedbylog-powerspectra(LPS).Notethatinspeech
recognition,theoutputofthedereverberationnetworkcanbe
featureslikeFBanksorMFCCs,whichdonotneedtobein-
vertedbacktowaveforms.
AlltheaboveDNN-basedspeechdereverberationap-
proachesaimtominimizethemeansquareerror(MSE)be-
tweentheoutputsofnetworkandthegroundtruth.Hence,
thereisanunderlyinghypothesisthattheenhancedspeechhas
theminimalvalueintheMSElosswiththereferencedclean
speech.However,theMSEobjectivefunctionhasverystrong
implicitassumptions,e.g.,independenceoftemporalorspatial,
equalimportanceofallsignalsamples,andinaccuratetode-
scribethedegreeofsignal[18].Toremedythisde-
y,generativeadversarialnetworks(GANs)[19],which
consistofageneratornetwork(
G
)andadiscriminatornetwork
(
D
),learnedthroughamin-maxadversarialgame,mightbea
goodchoice.lly,Pascual
etal.
haverecentlydemon-
stratedthepromisingperformanceofGANinspeechenhance-
ment[20]inthepresenceofadditivenoise.IntheSEGANap-
proach[20],thegenerator
G
triestolearnthedistributionofthe
cleandataandgenerateenhancedsamplesfromnoisyspeech
tofoolthediscriminator
D
;while
D
aimstodiscriminatebe-
tweenthecleanandenhancedsamples(generatedfrom
G
),
whichcapturestheessentialdifferencebetweenthem.While
SEGANworksonthewaveformlevel,whichtargetstoimprove
theperceptualspeechquality,Donahue
etal.
[21]haveexplored
GAN-basedspeechenhancementforrobustspeechrecognition.
,in[21],GANworksonthelog-Mel-bank
spectrainsteadofwaveforms.Theresultshaveshownthat
GANenhancementimprovestheperformanceofaclean-trained
ASRsystemonnoisyspeechbutitfallsshortoftheperfor-
manceachievedbyconventionalMCT.ByappendingtheGAN-
enhancedfeaturestothenoisyinputsandretraining,a7%WER
improvementrelativetotheMTRsystemwasachieved.
WhilethemajorgoaloftheaboveGANapproachesisto
removeadditivenoises,inthispaper,weinvestigatetheuse
ofGANsinthemapping-basedspeechdereverberationforro-
bustspeechrecognition.Althoughthesameframeworkcanbe
borrowedfromthesepreviousstudies,weprovideaseriesof
deepinvestigationsintheuseofdereverberationfront-endin
ASR.First,westudytheeffectivenessofdifferentdereverbera-
tionnetworks(usedlaterastheGANgenerator)andthat
LSTMdereverberatingnetworkcanachievesuperiorspeech
recognitionperformanceascomparedwithfeed-forwardDNN
andCNN.Second,furtheraddingresidualconnectionsinthe
deepLSTMscancontinuouslyboosttheperformance.Finally,
wethatitisimportanttoupdatethegenerator
G
andthe
discriminator
D
usingthesamemini-batchdataduringtrain-
arXiv:1803.10132v3  [cs.SD]  25 Oct 2018(a)
feed-forwardDNN
(b)
RCED
(c)
LSTMwithlayer-wiseresidue
Figure1:
Architecturesofdifferentdereverberationnetworks.
Figure2:
GANbasedspeechdereverberationframework.
ingforthesuccessofGAN.Moreover,wediscoverthat,us-
ingreverberantspectrogramasaconditionto
D
,assuggested
in[21,22],maydegradetheperformanceof
G
.Insummary,
usingthedereverberationGANcanachieve14%
˘
19%rela-
tivecharactererrorrate(CER)reductionascomparedwiththe
DNNdereverberationbaselinewhentestedonastrongmulti-
conditiontrainingacousticmodel.
2.Mappingbasedspeechdereverberation
Speechdereverberationcanbeachievedbyatypicalmapping
approach[12],inwhicharegressionDNN(showninFig.1a)
istrainedbypairsofreverberantandcleanLPSandalinearac-
tivationfunctionattheoutputofDNNisadoptedinsteadofa
nonlinearone.Moreover,thetargetLPSfeatureisusuallynor-
malizedgloballyoveralltrainingutterancesintozeromeanand
unitvariance(CMVN).Inthedereverberationstage,theLPS
featuresofinputspeecharefedintothewell-trainedregression
DNNtogeneratethecorrespondingenhancedLPSfeatures.Fi-
nally,thedereverberatedwaveformisreconstructedfromthe
predictedspectralmagnitudeandthereverberantspeechphase
withanoverlap-addalgorithm.
BesidesLPS,theinputandoutputofthedereverberation
DNNcanbeotherspeechfeatures,e.g.,MFCCandFBank.
Thespeechfeaturesdonotneedtobeinvertedbacktowave-
forms,whenusedforrobustASR.In[23],resultsshowthat
themappingfromLPStoMFCCcanachievelowerworder-
rorratethanthemappingfromMFCCtoMFCCinaspeech
recognitiontaskunderadditivenoiseconditions.Thisalsoindi-
catesthatthetransformationfordifferentfeaturedomainsand
nonlineardereverberationfunctioncanbelearnedbytheneu-
ralnetworksimultaneously.Furthermore,asshowninFig.1b
and1c,CNNandLSTMcanbeusedasenhancersaswell.We
expectthatthesemorepowerfulnetworkstructurescanbring
furtherimprovementsinthespeechdereverberationtask.We
willelaboratethenetworkandevaluatetheper-
formancesofdifferentnetworkslaterinSection4.3.
3.DereverberationGAN
3.1.GAN
Generativeadversarialnetworks(GANs)[19]aregenerative
modelsimplementedbytwoneuralnetworkscompetingwith
eachotherinatwo-playermin-maxgame.,the
generatornetwork
G
triestolearnadistribution
P
g
(
x
)
over
data
x
andapriorinputnoisevariables
p
z
(
z
)
.Theaimisto
matchthetruedatadistribution
P
data
(
x
)
tofoolthediscrimi-
nator
D
.Thediscriminatornetwork
D
servesasabinaryclassi-
whichaimstodeterminetheprobabilitythatagivensample
comesfromtherealdatasetratherthan
G
.Becauseoftheweak
guidance,thevanillagenerativemodelcannotgeneratedesir-
ablesamples.HencetheconditionalGAN(CGAN)[24]was
proposedtosteerthegenerationbyconsideringextrainforma-
tion
x
c
withthefollowingobjectivefunction:
min
G
max
D
V
(
G;D
)=
E
x
˘
p
data
(
x;x
c
)
[
logD
(
x;x
c
)]
+
E
x
c
˘
p
data
(
x
c
)
;z
˘
p
z
(
z
)
[
log
(1

D
(
G
(
z;x
c
)
;x
c
))]
:
(1)
Inordertostabilizetrainingandincreasethequalityofthe
generatedsamplesin
G
,least-squaresGAN(LSGAN)[25]was
furtherproposedandtheobjectivefunctionchangesto
min
D
V
(
D
)=
1
2
E
x
˘
p
data
(
x;x
c
)
[(
D
(
x;x
c
)

1)
2
]
+
1
2
E
x
c
˘
p
data
(
x
c
)
;z
˘
p
z
(
z
)
[
D
(
G
(
z;x
c
)
;x
c
)
2
]
;
(2)
min
G
V
(
G
)=
1
2
E
x
c
˘
p
data
(
x
c
)
;z
˘
p
z
(
z
)
[(
D
(
G
(
z;x
c
)
;x
c
)

1)
2
]
:
(3)
3.2.SpeechdereverberationwithGAN
ItisstraightforwardtouseGANinspeechdereverberationand
Fig.2illustratessuchakindofarchitecture.Itconsistsofa
G
anda
D
,where
G
,servingasthemapperinconventionalmeth-
ods,triestolearnatransformationfromreverberantspeechto
cleanspeechand
D
triestodeterminewhethertheinputsam-
plescomefrom
G
(
x
c
)
orreal-data
x
.Similarto[23],
G
aims
tolearningamappingfromtheLPSfeatureinputtotheMFCC
featureoutputwhichcanbedirectlyusedinASR.Insome
works[21,22],thelatentcode
z
isexcludedfromthegenerator
G
tolearnadirectmappinginsteadofadivtranslation
intheoriginalimage-to-imagetranslationtask[26].Webor-
rowedthisidea,butweremovethereverberantspectrogramas
aconditionto
D
.AswewillreportinSection4.5,theadded
reverberantspectrogramasaconditionto
D
notonlyincreases
theparametersizeof
D
,butalsodegradestheperformanceof
G
.Therefore,welearnageneratordistribution
P
g
(
x
)
overthe
conditionaldata
P
data
(
x
c
)
withthefollowingproposedobjec-
tivefunction:
min
D
V
(
D
)=
1
2
E
x
˘
p
data
(
x
)
[(
D
(
x
)

1)
2
]
+
1
2
E
x
c
˘
p
data
(
x
c
)
[
D
(
G
(
x
c
))
2
]
;
(4)
min
G
V
(
G
)=
1
2
E
x
c
˘
p
data
(
x
c
)
[(
D
(
G
(
x
c
))

1)
2
]
:
(5)
Tofurtherimprovetheabilityoftheadversarialcomponent,
previousCGANapproacheshaveindicatedthatitis
tomixtheGANobjectivefunctionwithsomenumericalloss
functions[24].Wefollowthisapproachinthedereverberation
GANapproachandtheMSElossiscontrolledbyanewhyper-
parameter

.FinallyEq.(5)becomes
min
G
V
(
G
)=
1
2
E
x
c
˘
p
data
(
x
c
)
[(
D
(
G
(
x
c
))

1)
2
]
+
1
2

L
MSE
(
G
(
x
c
)
;x
)
:
(6)
Inpractice,thegenerator
G
canbeafeed-forwardnetwork,
Table1:
CERs(%)ofCleanandMCTacousticmodels.
AM
Test
CleanRealSimu
Clean
7
:
8623
:
8520
:
24
MCT
7
:
8116
:
0213
:
99
aconvolutionalnetworkoraLSTMRNNnetwork,aswede-
scribedinSection2.Notethatthediscriminator
D
isonlyused
inthetraininganddiscardedinthedereverberationstage.Inour
approach,a2-layerLSTMwithoutresidualconnectionissetto
bethearchitectureof
D
.
4.Experimentsandresults
4.1.Datasets
Intheexperiments,weusedaMandarincorpusasoursourceof
cleanspeechdata,whichconsistsof103,000utterances(about
100hrs).TheRIRswerefrom[27],includingreal-recorded
RIRsandsimulatedRIRsforsmall,mediumandlargerooms.
Werandomlyselected97,000utterancesfornetworktraining
and3000utterancesforvalidation,andconvolvedwiththe
RIRs(bothreal-recordedandsimulated)toobtainthereverber-
antutterances.Therest3000utteranceswereusedfortesting
andconvolvedwiththerealRIRandthesimulatedRIRsfor
small,mediumandlargerooms.Finallyweobtainedatesting
setnamed`Real'thatcontains3000reverberantspeechutter-
ancesconvolvedwithrealRIRsandanothertestingsetnamed
`Simu'thatcontains9000reverberantspeechutterancescon-
volvedwithsimulatedRIRs(3000forsmall/medium/large).To
testthegeneralizationabilityofourapproach,weensuredthe
RIRsusedfortrainingandtestingweretotallydifferent.All
waveformsweresampledat16kHz.WeusedKaldi[28]to
generatethereverberantspeechbyconvolvingthecleansig-
nalwiththecorrespondingRIR.Asforfeatureextraction,the
framelengthwassetto25mswithaframeshiftof10ms.
4.2.ASRback-end
Ourspeechdereverberationfront-endwasusedforspeech
recognitionexperiments.WeusedKalditotrainourback-
endASRsystemwiththesimilaracousticmodelarchitecture
andfeaturesin[29].Theoriginaltrainingdatasetconsistsof
1600hrsMandarinspeechdata.Weusedspeed-perturbation
andvolume-perturbationtechniques[30]tododataaugmen-
tation.Hencethecleanmodelweretrainedusing4800hrsof
speechdata(1600

3).Wealsotrainedanacousticmodel
(AM)usingmulti-conditiontraining(MCT)strategy.Thetrain-
ingdatafortheMCTmodelis6400hrs(1600

4),includ-
ingtheabove4800hrsofcleandataand1600hrsofreverber-
antdatageneratedbyconvolvingthecleandatawiththeRIRs
in[27]asthedereverberationfront-end.
Thetimedelayneuralnetwork(TDNN)acousticmodel
(AM)had6layers,andeachlayerhad850linearunits
(ReLUs)withbatchrenormalization(BRN)[31].Theinput
contextsofTDNNAMweresetto[

2,2]-

1,2
g
-

3,3
g
-

7,2
g
-

3,3
g
-
f
0
g
andtheoutputsoftmaxlayerhad5795
units.Thenotation[

2,2]meanswesplicetogetherframes
t

2
through
t
+2
attheinputlayerandthenotation

1,2
g
meanswesplicetogethertheinputatthecurrentframeminus
1andthecurrentframeplus2.TheinputoftheAMwas40-
dimensionalMFCC.Allthespeechdereverberationfront-ends
weretestedonbothCleanandMCRAMs.Atrigramlanguage
model(LM),whichwastrainedonabout2TBscriptswithmore
than100,000wordsinthevocabulary,wasusedfordecodingin
theexperiments.Wealsousedentropy-basedparameterprun-
ing[32]andthethresholdwassettobe
10

8
.
ThebaselineresultsofCleanandMCTmodelareshownin
Table1.WecanseeaincreaseinCERwhenspeech
iscontaminatedwithreverberations.Inextendingthetraining
dataofacousticmodelbyaddingreverberantspeech,theMCT
AMcangreatlyreduceCER.
Table2:
CERs(%)ofdifferentfront-endnetworks.
InputMethod
CleanAMMCTAM
RealSimuRealSimu
MFCC
DNN
17
:
8616
:
6316
:
3114
:
72
RCED
18
:
2816
:
7316
:
7615
:
09
LSTM
15
:
3813
:
3714
:
2112
:
46
LPS
DNN
16
:
6215
:
3315
:
3514
:
03
RCED
15
:
5514
:
1514
:
1513
:
09
LSTM
15
:
0413
:
1613
:
9712
:
20
Table3:
CER(%)comparisonsfordifferentlayersandresidual
connectionarchitectures.
Method
CleanAMMCTAM
RealSimuRealSimu
2-layerLSTM
15
:
4113
:
5014
:
2512
:
55
+Res-I
16
:
1814
:
4114
:
9913
:
06
+Res-L
16
:
1313
:
7414
:
6112
:
65
4-layerLSTM
15
:
0413
:
1613
:
97
12
:
20
+Res-I
15
:
8113
:
4814
:
6012
:
47
+Res-L
14
:
9913
:
1313
:
90
12
:
22
8-layerLSTMdivergence
+Res-I
15
:
5313
:
5514
:
4812
:
49
+Res-L
14
:
6712
:
7513
:
6212
:
04
4.3.Mapping-basedspeechdereverberation
Weinvestigatedthespeechdereverberationperformances
ofdifferentnetworksandinputfeaturesinthemapping-based
approach.Laterwewillselectthebestnetworkasthegen-
eratorintheGAN-basedspeechdereverberation.
cally,wetestedthreedifferentdereverberationnetworks,i.e.,
feed-forwardDNN,redundantconvolutionalencoderdecoder
(RCED)andLSTM.AsshowninFig.1,theDNNhas4hidden
layersandeachofwhichcontains1024ReLUneurons.The
structureoftheRCEDissimilarwith[16]exceptthelastlayer.
WechangedthelastCNNlayertoafullyconnectedoutput
layerasshowninFig.1b,becauseourinputandtargetfeatures
werenotinthesamedimension.Theinputfeaturecontainsa
contextwindowof11frames(
t

5
)fortheDNNandtheRCED.
ThenumberofandwidthofRCEDmodelwereset
to12-16-20-24-32-24-20-16-12and13-11-9-7-7-7-9-11-13,re-
spectively.Thelearningratewassetto0.001withamini-batch
sizeof256.Moreover,BRNwasalsousedforDNNandRCED
training.InsteadofusingvanillaLSTM,weadoptedanLSTM
withrecurrentprojectionlayer(LSTMP)[33],whichmeanswe
donotneedtoaddanextralayertodoresidualaddlikesDNN2
in[34]toavoiddimensionmismatch.TheLSTMhas4LSTMP
layersfollowedbyalinearoutputlayer.EachLSTMPlayerhas
760memorycellsand257projectionunitsandtheinputtothe
LSTMisasingleacousticframe.Thelearningratewassetto
0.0003andthemodelwastrainedwith8full-lengthutterances
parallelprocessing.
Allthemodelsexploredherewereoptimizedwiththe
Adam[35]methodandinitializedwiththeXavier[36]algo-
rithm.Wealsousedexponentialdecaytodecreasethelearning
ratewhichwassimilarwithKaldinnet3
1
andtheterminated
learningratewas5ordersofmagnitudesmallerthantheinitial
learningrate.
InTable2,welistallexperimentalresultsonbothClean
andMCTAMs.Firstly,weobserveconsistentimprovement
onalldereverberationnetworksbyreplacingMFCCwithLPS
featuresasthenetworkinput.HeretheLPSfeatureis257di-
mensionandtheMFCCfeatureis40dimensions.Notethat
theoutputofallthedereverberationnetworksis40-dimension
MFCCwhichisfedintotheASRsystem.Thisconclusionis
consistentwiththatin[23],whereLPSperformsbetterthan
MFCCwhenusedastheinputofadenoisingnetwork.
WhenwecompareTable2withTable1,wecanthatthe
1
egs/wsj/s5/steps/libs/nnet3/train/common.py(get
learning
rate)
Table4:
CERs(%)comparisonsbypreviousmappingbasednetworksandourproposedframework.ﬁDBﬂmeansweusedifferent
mini-batchdatatoupdatetheparametersofGANandﬁCDﬂmeansweaddtheconditionalinformationtoinputof
D
.Relative
improvementsaregiveninparenthesesw.r.t.thecorrespondingDNNmodel.
Method
CleanAMMCTAM
RealSimuRealSimu
SEGAN
32
:
98(

98
:
44)37
:
14(

142
:
27)30
:
18(

96
:
61)32
:
37(

130
:
72)
DNN
16
:
62(0
:
00)15
:
33(0
:
00)15
:
35(0
:
00)14
:
03(0
:
00)
LSTM
15
:
04(9
:
51)13
:
16(14
:
16)13
:
97(8
:
99)12
:
20(13
:
04)
+Res
14
:
99(9
:
81)13
:
13(14
:
35)13
:
90(9
:
45)12
:
22(12
:
90)
+GAN
14
:
07(15
:
34)
12
:
02(21
:
59)13
:
15(14
:
33)11
:
42(18
:
60)
+GAN+Res
14
:
10(15
:
16)
11
:
96(21
:
98)13
:
14(14
:
40)11
:
40(18
:
75)
+GAN+Res(DB)
15
:
72(5
:
42)13
:
95(9
:
00)14
:
60(4
:
89)12
:
83(8
:
55)
+GAN+Res+CD
14
:
27(14
:
14)12
:
19(20
:
48)13
:
38(12
:
83)11
:
43(18
:
53)
mapping-baseddereverberationworksquitewell.Whentested
ontheCleanAM,allthedereverberationnetworksareeffec-
tivewithCERreduction;whentestedontheMCT
AM,thedereverberationnetworkswiththeLPSinputarestill
effectivewithapparentCERreduction.Comparingdifferent
modelstructures,wediscoverthatLSTMachievesthebestper-
formance.Forinstance,theLPS-LSTMdereverberationnet-
workreducestheCERfrom23.85%(real-reverberationadded)
to15.04%fortheCleanAMandreducestheCERfrom16.02%
(real-reverberationadded)to13.97%fortheMCTAM.Webe-
lievethatthesuperiorpeformanceisbecauseoftheLSTM's
abilitytomodellong-termcontextualinformationthatisessen-
tialisthespeechdereverberationtask.WealsoRCED-
CNNisnotgoodwhenMFCCisusedastheinput.Wewilluse
LSTMasournetworkintherestoftheexperiments.
4.4.AddingResNet
Table3showstheresultsofdifferentresidualconnectionar-
chitectures.Thelayer-wiseresidualconnection(Res-L)struc-
turecanbeseeninFig.1c;whiletheinputresidualconnection
(Res-I)structureissimilarwithRes-Landmoredetailscanbe
foundin[37].Asweexpected,it'snotnecessarytoaddresidual
connectionstoshallownetworks.Performancesdegradewhen
residualconnectionsareusedina2-layerLSTM.Res-Lalways
performsbetterthanRes-I.ThisisreasonablebecauseRes-L
triestolearntheresidueofthehigh-levelabstractfeaturewhile
Res-Ijustlearnstheresidueoftheinputfeature.Whenthe
LSTMisasdeepas4layers,Res-Lstartstoworkandthelowest
CERsareachievedwhentheLSTMhas8layers.Astraininga
8-layerLSTMistime-consuming,weperformtheGANexper-
imentswitha4-layerLSTMgeneratorinthefollowing.
4.5.SpeechdereverberationwithGAN
WeinvestigatedtheabilityofGANinmapping-based
speechdereverberation.WealsoreproducedtheSEGANap-
proach[20]withtheopen-sourcecodes
2
asacomparison.As
showninTable4,SEGANdegradestheASRperformance
withinourexpectation,whichisconsistentwiththereported
resultsin[21].WebelievethisisbecauseSEGANaimstoim-
provetheperceptionofnoisyspeechandtime-domainenhance-
mentmaybenotappropriateforreverberantspeechrecognition.
IntheproposedGAN-basedmethods,thearchitectureof
G
isconsistentwiththatinFig.1cwith4hiddenLSTMPlay-
ers.Thearchitectureof
D
issimilarwith
G
butcontainsonly
2LSTMPlayersandthecellnumberandtheprojectiondimen-
sionaresetto256and40,respectively.Thehyper-parameter

inEq.(6)wassetto200andthelearningrateof
G
and
D
weresetto
0
:
00008
and
0
:
0003
,respectively.Ineachiteration,
weupdatedtheparametersof
G
twiceandtheparametersof
D
once.TostabilizingGANtraining,wealsoaddinstanceGaus-
siannoisetotheMFCCinputof
D
3
.InTable4,wedemon-
stratethatusingGAN(inLSTM+GAN)isnotonlyviablebut
alsooutperformstheLSTMs.
2
https://github.com/santi-pdp/segan
3
http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-
training/
Attheearlystageofourexperiments,weupdatedthepa-
rametersof
G
and
D
usingdifferentmini-batchdatalikethe
waystheydoinimagetasks.Inotherwords,theparameters
of
D
wereupdatedusingonemini-batchdataandthenthepa-
rametersof
G
wereupdatedusinganewmini-batchdata.We
foundthatthistrainingstrategyŠLSTM+GAN+Res(DB)in
thesecond-to-lastrowofTable4Šwasquiteunstableinour
experimentsandwealwaysachievedresultsworsethanthenon-
adversarialtraining(e.g.,LSTM+Res)asshowninTable4.In-
stead,whenwetriedtoupdatetheparametersof
G
and
D
us-
ingthesamemini-batchdata,weachievedconsistentlybetter
results(LSTM+GAN+ResinTable4).Webelievethatthis
strategyisessentialinmakingourGANapproachperform-
ingwell.Addingresidualconnectionsworksformostcases.
LSTM+GAN+ResloweredtheMCTAMCERfrom15.35%
downto13.14%with14.4%relativeCERreductionforthe
RealsetandloweredtheMCTAMCERfrom14.03%downto
11.40%with18.75%relativeCERreductionfortheSimuset.
Finally,wealsotheperformanceofLSTM+GAN+Res+CD
isworsethanLSTM+GAN+Res.Thismeansthataddingthe
reverberantspectrogramasaconditiontoDisuselesstothe
dereverberationperformance.
5.Summary
Inthispaper,weprovideadeepinvestigationofGANin
mapping-basedspeechdereverberationforrobustspeechrecog-
nition.Intheselectionofthegeneratornetwork,wethat
LSTMachievessuperiorperformance,whileaddingresidual
connections(ResNets)indeepLSTMscanfurtherboostthe
performance.IntheuseofGAN,wethatitisessential
toupdatethegeneratorandthediscriminatorusingthesame
mini-batchdataduringmodeltraining;andusingreverberant
spectrogramasaconditiontothediscriminatormaydegradethe
performance.Withtheaboveweareabletoachieve
14%
˘
22%relativeCERreductioninASRascomparedwitha
DNNbaseline,whiletheSEGANbaselineevendoesnotwork
ontheASRtask.Inthefuture,weplantofurtherexplorethe
useofGANinmoreadverseconditions(bothreverberantand
noisy)andtrytocombinetheframeworkwithjoint-training
strategytofurtherimprovetheASRperformance.
6.Acknowledgements
TheauthorswouldliketothankShanYangfromNorthwest-
ernPolytechnicalUniversity,Dr.BoLifromGoogleandDr.
BoWufromXidianUniversityfortheirhelpfulcommentsand
suggestionsonthiswork.Theresearchworkissupportedby
theNationalKeyResearchandDevelopmentProgramofChina
(GrantNo.2017YFB1002102)andtheNationalNaturalScience
FoundationofChina(GrantNo.61571363).
7.References
[1]
W.Xiong,L.Wu,F.Alleva,J.Droppo,X.Huang,andA.Stolcke,
ﬁThemicrosoft2017conversationalspeechrecognitionsystem,ﬂ
arXivpreprintarXiv:1708.06073
,2017.
[2]
G.Kurata,B.Ramabhadran,G.Saon,andA.Sethy,ﬁLanguage
modelingwithhighwaylstm,ﬂ
arXivpreprintarXiv:1709.06436
,
2017.
[3]
K.Kinoshita,M.Delcroix,S.Gannot,E.A.Habets,R.Haeb-
Umbach,W.Kellermann,V.Leutnant,R.Maas,T.Nakatani,
B.Raj
etal.
,ﬁAsummaryofthereverbchallenge:state-of-the-
artandremainingchallengesinreverberantspeechprocessingre-
search,ﬂ
EURASIPJournalonAdvancesinSignalProcessing
,vol.
2016,no.1,p.7,2016.
[4]
B.Wu,K.Li,M.Yang,C.-H.Lee
etal.
,ﬁAreverberation-time-
awareapproachtospeechdereverberationbasedondeepneural
networks,ﬂ
IEEE/ACMTransactionsonAudio,SpeechandLan-
guageProcessing(TASLP)
,vol.25,no.1,pp.102Œ111,2017.
[5]
S.T.NeelyandJ.B.Allen,ﬁInvertibilityofaroomimpulsere-
sponse,ﬂ
TheJournaloftheAcousticalSocietyofAmerica
,vol.66,
no.1,pp.165Œ169,1979.
[6]
M.Delcroix,T.Hikichi,andM.Miyoshi,ﬁDereverberationand
denoisingusingmultichannellinearprediction,ﬂ
IEEETransac-
tionsonAudio,Speech,andLanguageProcessing
,vol.15,no.6,
pp.1791Œ1801,2007.
[7]
K.Kumatani,J.McDonough,andB.Raj,ﬁMicrophonearraypro-
cessingfordistantspeechrecognition:Fromclose-talkingmicro-
phonestofarsensors,ﬂ
IEEESignalProcessingMagazine
,
vol.29,no.6,pp.127Œ140,2012.
[8]
M.WuandD.Wang,ﬁAtwo-stagealgorithmforone-microphone
reverberantspeechenhancement,ﬂ
IEEETransactionsonAudio,
Speech,andLanguageProcessing
,vol.14,no.3,pp.774Œ784,
2006.
[9]
K.Kinoshita,M.Delcroix,T.Nakatani,andM.Miyoshi,ﬁSup-
pressionoflatereverberationeffectonspeechsignalusinglong-
termmultiple-steplinearprediction,ﬂ
IEEETransactionsonAu-
dio,Speech,andLanguageProcessing
,vol.17,no.4,pp.534Œ
545,2009.
[10]
S.Mosayyebpour,M.Esmaeili,andT.A.Gulliver,ﬁSingle-
microphoneearlyandlatereverberationsuppressioninnoisy
speech,ﬂ
IEEETransactionsonAudio,Speech,andLanguage
Processing
,vol.21,no.2,pp.322Œ335,2013.
[11]
N.MohammadihaandS.Doclo,ﬁSpeechdereverberationusing
non-negativeconvolutivetransferfunctionandspectro-temporal
modeling,ﬂ
IEEE/ACMTransactionsonAudio,SpeechandLan-
guageProcessing(TASLP)
,vol.24,no.2,pp.276Œ289,2016.
[12]
Y.Xu,J.Du,L.-R.Dai,andC.-H.Lee,ﬁAnexperimentalstudy
onspeechenhancementbasedondeepneuralnetworks,ﬂ
IEEE
Signalprocessingletters
,vol.21,no.1,pp.65Œ68,2014.
[13]
K.Kinoshita,M.Delcroix,H.Kwon,T.Mori,andT.Nakatani,
ﬁNeuralnetwork-basedspectrumestimationforonlineWPEdere-
verberation,ﬂin
Interspeech
,2017,pp.384Œ388.
[14]
K.Han,Y.Wang,D.Wang,W.S.Woods,I.Merks,andT.Zhang,
ﬁLearningspectralmappingforspeechdereverberationandde-
noising,ﬂ
IEEETransactionsonAudio,Speech,andLanguage
Processing
,vol.23,no.6,pp.982Œ992,2015.
[15]
F.Weninger,S.Watanabe,Y.Tachioka,andB.Schuller,ﬁDeep
recurrentde-noisingauto-encoderandblindde-reverberationfor
reverberatedspeechrecognition,ﬂin
Acoustics,SpeechandSignal
Processing(ICASSP),2014IEEEInternationalConferenceon
.
IEEE,2014,pp.4623Œ4627.
[16]
S.R.ParkandJ.Lee,ﬁAfullyconvolutionalneuralnetworkfor
speechenhancement,ﬂ
arXivpreprintarXiv:1609.07132
,2016.
[17]
D.WilliamsonandD.Wang,ﬁTime-frequencymaskinginthe
complexdomainforspeechdereverberationanddenoising,ﬂ
IEEE/ACMTransactionsonAudio,Speech,andLanguagePro-
cessing
,2017.
[18]
Z.WangandA.C.Bovik,ﬁMeansquarederror:Loveitorleave
it?anewlookatsignalmeasures,ﬂ
IEEEsignalprocessing
magazine
,vol.26,no.1,pp.98Œ117,2009.
[19]
I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-
Farley,S.Ozair,A.Courville,andY.Bengio,ﬁGenerativeadver-
sarialnets,ﬂin
Advancesinneuralinformationprocessingsys-
tems
,2014,pp.2672Œ2680.
[20]
S.Pascual,A.Bonafonte,andJ.Serr
˚
a,ﬁSegan:Speech
enhancementgenerativeadversarialnetwork,ﬂ
arXivpreprint
arXiv:1703.09452
,2017.
[21]
C.Donahue,B.Li,andR.Prabhavalkar,ﬁExploringspeechen-
hancementwithgenerativeadversarialnetworksforrobustspeech
recognition,ﬂ
arXivpreprintarXiv:1711.05747
,2017.
[22]
D.MichelsantiandZ.-H.Tan,ﬁConditionalgenerativeadversarial
networksforspeechenhancementandnoise-robustspeakerveri-
ﬂ
arXivpreprintarXiv:1709.01703
,2017.
[23]
K.Han,Y.He,D.Bagchi,E.Fosler-Lussier,andD.Wang,ﬁDeep
neuralnetworkbasedspectralfeaturemappingforrobustspeech
recognition,ﬂin
SixteenthAnnualConferenceoftheInternational
SpeechCommunicationAssociation
,2015.
[24]
M.MirzaandS.Osindero,ﬁConditionalgenerativeadversarial
nets,ﬂ
arXivpreprintarXiv:1411.1784
,2014.
[25]
X.Mao,Q.Li,H.Xie,R.Y.Lau,Z.Wang,andS.P.Smolley,
ﬁLeastsquaresgenerativeadversarialnetworks,ﬂ
arXivpreprint
ArXiv:1611.04076
,2016.
[26]
P.Isola,J.-Y.Zhu,T.Zhou,andA.A.Efros,ﬁImage-to-image
translationwithconditionaladversarialnetworks,ﬂ
arXivpreprint
arXiv:1611.07004
,2016.
[27]
T.Ko,V.Peddinti,D.Povey,M.Seltzer,andS.Khudanpur,
ﬁAstudyondataaugmentationofreverberantspeechforrobust
speechrecognition.ﬂICASSP,2017.
[28]
D.Povey,A.Ghoshal,G.Boulianne,L.Burget,O.Glembek,
N.Goel,M.Hannemann,P.Motlicek,Y.Qian,P.Schwarz
etal.
,
ﬁThekaldispeechrecognitiontoolkit,ﬂin
IEEE2011workshop
onautomaticspeechrecognitionandunderstanding
,no.EPFL-
CONF-192584.IEEESignalProcessingSociety,2011.
[29]
V.Peddinti,D.Povey,andS.Khudanpur,ﬁAtimedelayneural
networkarchitectureforefmodelingoflongtemporalcon-
texts.ﬂin
INTERSPEECH
,2015,pp.3214Œ3218.
[30]
T.Ko,V.Peddinti,D.Povey,andS.Khudanpur,ﬁAudioaug-
mentationforspeechrecognition.ﬂin
INTERSPEECH
,2015,pp.
3586Œ3589.
[31]
S.Ioffe,ﬁBatchrenormalization:Towardsreducingmini-
batchdependenceinbatch-normalizedmodels,ﬂ
arXivpreprint
arXiv:1702.03275
,2017.
[32]
A.Stolcke,ﬁEntropy-basedpruningofbackofflanguagemodels,ﬂ
arXivpreprintcs/0006025
,2000.
[33]
H.Sak,A.Senior,andF.Beaufays,ﬁLongshort-termmemoryre-
currentneuralnetworkarchitecturesforlargescaleacousticmod-
eling,ﬂin
FifteenthAnnualConferenceoftheInternationalSpeech
CommunicationAssociation
,2014.
[34]
M.TuandX.Zhang,ﬁSpeechenhancementbasedondeepneural
networkswithskipconnections,ﬂin
Acoustics,SpeechandSignal
Processing(ICASSP),2017IEEEInternationalConferenceon
.
IEEE,2017,pp.5565Œ5569.
[35]
D.KingmaandJ.Ba,ﬁAdam:Amethodforstochasticoptimiza-
tion,ﬂ
arXivpreprintarXiv:1412.6980
,2014.
[36]
X.GlorotandY.Bengio,ﬁUnderstandingthedifoftrain-
ingdeepfeedforwardneuralnetworks,ﬂin
Proceedingsofthe
ThirteenthInternationalConferenceonIntelligenceand
Statistics
,2010,pp.249Œ256.
[37]
Z.Chen,Y.Huang,J.Li,andY.Gong,ﬁImprovingmasklearn-
ingbasedspeechenhancementsystemwithrestorationlayersand
residualconnection,ﬂin
Proc.Interspeech
,2017.
"
32,Empirical Evaluation of Speaker Adaptation on DNN based Acoustic Model,http://arxiv.org/pdf/1803.10146v3.pdf,https://github.com/wangkenpu/Adaptation-Interspeech18,"EmpiricalEvaluationofSpeakerAdaptationonDNNbasedAcousticModel
KeWang
1
;
2
,JunboZhang
2
,YujunWang
2
,LeiXie
1

1
ShaanxiProvincialKeyLaboratoryofSpeechandImageInformationProcessing,
SchoolofComputerScience,NorthwesternPolytechnicalUniversity,Xi'an,China
2
Xiaomi,Beijing,China
f
kewang,lxie
g
@nwpu-aslp.org,
f
zhangjunbo,wangyujun
g
@xiaomi.com
Abstract
Speakeradaptationaimstoestimateaspeakeracoustic
modelfromaspeakerindependentonetominimizethemis-
matchbetweenthetrainingandtestingconditionsarisenfrom
speakervariabilities.Avarietyofneuralnetworkadaptation
methodshavebeenproposedsincedeeplearningmodelshave
becomethemainstream.Buttherestilllacksanexperimental
comparisonbetweendifferentmethods,especiallywhenDNN-
basedacousticmodelshavebeenadvancedgreatly.Inthispa-
per,weaimtoclosethisgapbyprovidinganempiricalevalu-
ationofthreetypicalspeakeradaptationmethods:LIN,LHUC
andKLD.Adaptationexperiments,withdifferentsizeofadap-
tationdata,areconductedonastrongTDNN-LSTMacoustic
model.Morechallengingly,here,thesourceandtargetweare
concernedwitharestandardMandarinspeakermodelandac-
centedMandarinspeakermodel.Wecomparetheperformances
ofdifferentmethodsandtheircombinations.Speakeradapta-
tionperformanceisalsoexaminedbyspeaker'saccentdegree.
IndexTerms
:Speakeradaptation,deepneuralnetworks,LIN,
KLD,LHUC
1.Introduction
Speechrecognitionaccuracyhasbeenimproved
sincetheuseofdeeplearningmodels(DLMs),ormorespecif-
ically,deepneuralnetworks(DNNs)[1,2].Variousmodels,
suchasconvolutionalneuralnetworks(CNNs)[3,4],time-
delayneuralnetworks(TDNNs)[5],longshort-termmemory
(LSTM)recurrentneuralnetworks(RNNs)[6,7]andtheirvari-
ants[8,9]andcombinations[10],havebeendevelopedtofur-
therimprovetheperformance.However,theaccuracyofan
automaticspeechrecognition(ASR)systeminrealapplications
stilllagsbehindthatincontrolledtestingconditions.Thisraises
theoldandunsolvedproblemcalled
training-testingmismatch
,
i.e.,thetrainingsetcannotmatchthenewacousticconditions
orfailstogeneralizetonewspeakers.Thusavarietyofacous-
ticmodelcompensationandadaptationmethodshavebeenpro-
posed,tobetterdealwithunseenspeakersandmismatched
acousticconditions.
Thisstudyfocuseson
speakeradaptation
,i.e.,
modifyingageneralmodel,commonlyaspeaker-independent
acousticmodel(SIAM),toworkbetterforanew
speaker,thoughthesameadaptationtechniquecanbeapplied
toothermismatchedconditions.Thehistoryofacousticmodel
speakeradaptationcanbetracedbacktotheGMM-HMM
era[11,12,13,14,15,16,17,18],whilethefocushasbeen
shiftedtoneuralnetworkssincetheriseofDLMs.Various
approacheshavebeendevelopedforneuralnetworkacoustic
modeladaptation[19,20,21,22,23,24,25,26,27,28]andthey
canberoughlycategorizedintothreeclasses:speaker-adapted
layerinsertion,subspacemethodanddirectmodeladapting.
*Correspondingauthor
Inthecategoryofspeaker-adaptedlayerinsertion,linear
transformation,whichaugmentstheoriginalnetworkwithcer-
tainspeakerlinearlayer(s),isasimple-but-effective
approach.Commonmethodsincludelinearinputnetwork
(LIN)[19,20],linearhiddennetwork(LHN)[21],andlinear
outputnetwork(LOH)[20],justtonameafew.Amongthem,
LINisthemostpopularone.Learninghiddenunitcontribution
(LHUC)[22]isanothertypeofspeaker-adaptedlayerinsertion
methodthatmakestheSInetworkparameterstobespeaker-
byinsertingspeciallayerstocontroltheamplitudeof
thehiddenlayers.
Anothercategory,subspacemethod,aimstoalowdi-
mensionalspeakersubspacethatisusedforadaptation.The
moststraightforwardapplicationistousesubspace-basedfea-
tures,e.g.,i-vectors[23,24],asasupplementofacousticfea-
turesintheneuralnetworkforacousticmodeltraining,or
speakeradaptivetraining(SAT).Anotherapproach,serving
thesamepurposewithauxiliaryfeatures,iscalledspeaker
codes[25].Asetofnetworkunitsforeachspeaker
isconnectedandoptimizedwiththeoriginalSInetwork.Note
thati-vectorbasedSAThasbecomeastandardinthetrainingof
deepneuralnetworkacousticmodels[5,24,27,29,30,31]as
thissimpletrickcanbringsmall-but-consistentimprovement.
Astraightforwardideaistousenewspeaker'sdatato
adapttheDNNparametersdirectly.Retrathe
SImodelusingthenewdataisthesimplestway,whichis
alsocalledretrainedspeakerindependent(RSI)adaptation[19].
Toavoidoverconservativetraining,suchasKullback-
Leiblerdivergence(KLD)regularization[26]isfurtherintro-
duced.Thisapproachtriestoforcetheposteriordistribution
oftheadaptedmodeltobeclosertothatestimatedfromthe
SImodel,byaddingaKLDregularizationtermtotheoriginal
crossentropycostfunctiontoupdatethenetworkparameters.
Althoughquiteeffective,thisapproachresultsinanindividual
neuralnetworkforeachspeaker.
Tothebestofourknowledge,therestilllacksathor-
oughexperimentalcomparisonbetweendifferentspeakeradap-
tationmethodsintheliterature,especiallywhentheDNN-
basedacousticmodels(AMs)havebeenadvancedgreatlysince
theintroductionoftheseadaptationtechniques.Inthispaper,
weaimtoclosethisgapbyprovidinganempiricalevalua-
tionofthreetypicalspeakeradaptationmethods:LIN,LHUC
andKLD.Adaptationexperimentsareconductedonastrong
TDNN-LSTMacousticmodel(welltrainedi-vectorbasedSAT-
DNNacousticmodelwithcMLLR[13,15])testedwithdiffer-
entsizeofadaptationdata.Morechallengingly,here,thesource
andtargetweareconcernedwitharestandardMandarinspeaker
modelandaccentedMandarinspeakermodel.Wecomparethe
performanceofdifferentmethodsandtheircombinations.The
speakeradaptationperformanceisalsoexaminedbyspeaker's
accentdegree.Inaword,wewouldliketoprovidereadersabig
pictureontheselectionofspeakeradaptationtechniques.
Therestofthispaperisorganizedasfollows.InSection2,
weintroduceLIN,KLD,LHUCandgiveadiscussion
arXiv:1803.10146v3  [cs.SD]  25 Oct 2018Figure1:
Linearinputnetwork.
ontheirabilities.Next,wedescribeaseriesofexperimentsand
reporttheresultsinSection3.Finally,someconclusionsare
drawninSection4.
2.Speakeradaptationalgorithms
2.1.LIN
Linearinputnetwork(LIN)[19,20]isaclassicalinputtrans-
formationapproachforneuralnetworkadaptation.Asshown
inFigure1,LINassumesthatthemismatchbetweentraining
andtestingcanbecapturedinthefeaturespacebyemploy-
ingatrainablelinearinputlayerwhichmapsspeakerdependent
speechtospeakerindependentnetwork(i.e.,acousticmodel).
Theinsertedlayerusuallyhasthesamedimensionastheorig-
inalinputlayerandisinitializedtoanidentityweightmatrix
and0bias.Unlikeotherlayersoftheneuralnetwork,linear
activationfunction
f
(
x
)=
x
isusedforthisadditionallayer.
Duringadaptation,standarderrorback-propagation(BP)is
usedtoupdatetheLIN'sparameterswhilekeepingallothernet-
workparametersed,byminimizingthelossfunction(e.g.,
crossentropy,meansquareerror)oftheoriginalAM.After
adaptation,eachspeakerLINcapturestherelationsbe-
tweenthespeakerandthetrainingspace.Finally,foreachtest-
ingspeaker,thecorrespondingLINisselectedtodofeature
transformationandthetransformedvectorisdirectlyfedtothe
originalunadaptedAMforspeechrecognition.
2.2.KLDRegularization
Asapopularconservativetrainingadaptationtechnique,
Kullback-Leiblerdivergence(KLD)[26]regularizationtriesto
forcetheposteriordistributionoftheadaptedmodeltobecloser
tothatestimatedfromtheSImodel.Bycontrast,the
L
2
regu-
larizationaimstokeeptheparametersofadaptedmodeltobe
closertothoseoftheSImodel.
Foracousticmodeltraining,itistypicaltominimizethe
crossentropy(CE)
F
CE
=

1
N
N
X
t
=1
S
X
y
=1
~
p
(
y
j
x
t
)log
p
(
y
j
x
t
)
;
(1)
where
N
isthenumberoftrainingsamples,
S
isthetotalnum-
berofstates,
~
p
(
y
j
x
t
)
isthetargetprobabilityand
p
(
y
j
x
t
)
is
neuralnetwork'soutputposteriors.Weusuallyuseahardalign-
mentfromanexistingASRsystemasthetraininglabelsandset
~
p
(
y
j
x
t
)=

(
y
=
s
t
)
,where

istheKroneckerdeltafunction
and
s
t
isthelabelof
t
-thsample.ByaddingtheKLDtermto
Eq.(1)wegetthefollowingoptimizationcriterion:
b
F
CE
=(1

ˆ
)
F
CE

ˆ
1
N
N
X
t
=1
S
X
y
=1
p
SI
(
y
j
x
t
)log
p
(
y
j
x
t
)
=

1
N
N
X
t
=1
S
X
y
=1
h
(1

ˆ
)~
p
(
y
j
x
t
)+
ˆp
SI
(
y
j
x
t
)
i
log
p
(
y
j
x
t
)
=

1
N
N
X
t
=1
S
X
y
=1
^
p
(
y
j
x
t
)log
p
(
y
j
x
t
)
;
(2)
Figure2:
Learninghiddenunitcontribution.
where
ˆ
isregularizationweightandwehave
^
p
(
y
j
x
t
)
,
(1

ˆ
)~
p
(
y
j
x
t
)+
ˆp
SI
(
y
j
x
t
)
:
(3)
BycomparingEq.(1)andEq.(2),wecanthatapply-
ingKLDisequivalenttochangingthetargetdistributioninthe
conventionalBPalgorithm.When
ˆ
=0
,wecanregardthis
asRSI,i.e.,retrainingtheSImodeldirectlyusing
thetraditionalCEloss.
2.3.LHUC
AsshowninFigure2,learninghiddenunitcontribution
(LHUC)[22]theSImodelbyasetofspeaker
dependentparameters

foraspeaker,where

=

r
1
;

;
r
L

and
r
l
isthevectorofspeakerdependentparam-
etersfor
l
-thhiddenlayer.Thentheelement-wisefunction
a
(

)
isadoptedtoconstraintherangeof
r
l
andthespeakerdepen-
denthiddenlayeroutputcanbeasthefollowingfunc-
tion:
h
l
=
a
(
r
l
)

˚
l
(
W
l
>
h
l

1
)
;
(4)
where

isanelement-wisemultiplicationand
a
(

)
istypically
asasigmoidwithamplitude2,i.e.,
a
(
r
l
)
,
2
1+exp(

r
l
)
;
(5)
toconstraintherangeof
r
'selementsto
[0
;
2]
.
LHUC,givenadaptationdata,actuallyrescalesthecontri-
butions(amplitudes)ofthehiddenunitsinthemodelwithout
actuallymodifyingtheirfeaturereceptors.Atthetrainingstage,

isoptimizedwiththestandardBPalgorithmwhilekeeping
alltheotherparametersedforaspeaker.Duringthe
testingstage,thecorresponding

ischosentoconstraintheam-
plitudesofhiddenunitsinordertogetmoreaccurateposterior
probabilityforthespeaker.
2.4.DiscussionandCombination
Wecomparethethreespeakeradaptationapproachesinterms
ofadaptedparametersizeandontheAM.
Ł
SizeofAdaptedParameters:
LHUChasminimal
adaptedparameters,followedbyLIN.ForKLDregu-
larization,sinceeachspeakerhasafullyadaptedneural
networkAM,itresultsinthelargestsizeofadaptedpa-
rameters.
Ł
onAM:
IntheKLDregularizationbased
adaptation,wedonotneedtochangetheoriginalAM
networkstructure,whileonlychangingthelossfunc-
tion.Bycontrast,weneedtoadjustthenetworkstruc-
ture,e.g.,insertinglayersintheuseofLINandLHUC.
However,weneedtotakeextraburdentoanappro-
priateregularizationweight
ˆ
intheKLDregularization
basedadaptation,whichissearchedthroughthevalida-
tionset.
Thethreeapproachesperformnetworkadaptationfromdif-
ferentaspectsandthuscanbeintegratedtoexpectsomeextra
Table1:
CERsofeachspeakeronbaselineTDNN-LSTMi-vectorbasedacousticmodel.ﬁSﬂ,ﬁMﬂandﬁHﬂareshortformsfor
ﬁslightﬂ,ﬁmediumﬂandﬁheavyﬂseparately.
SpeakerS01S02S03S04S05S06S07S08S09S10Avg
Accent
SMSMHHHMHM-
CER(%)
3
:
0021
:
639
:
0916
:
4056
:
6240
:
0736
:
6111
:
1631
:
7422
:
2824
:
86
(a)
(b)
(c)
Figure3:
CERs(%)fordifferentamountofadaptationdata.(a)Comparisonofdifferentadaptationmethods.(b)KLDadaptationwith
differentregularizationweights
ˆ
.ThedashedlineisthebaselineCER.(c)KLDadaptationforeachspeakers.
Figure4:
CERs(%)fordifferentmethodcombinations.
LINandLHUC,bothwithoutchangingtheparameters
oftheoriginalSInetwork,canbedirectlyintegrated.Inother
words,LIN'sparametersandthespeakerdependentparameters

areupdatedusingthetargetspeaker'sdatawhilekeepingthe
parametersoftheoriginalnetworkintact.AstheKLDadap-
tationitselfneedstoupdatetheparametersoftheoriginalnet-
work,intheintegrationofLIN/LHUCwithKLD,weonlyuse
Eq.(2)asthelossfunctiontoupdateLIN'sparametersor/and
thespeakerdependentparameters

whilestillkeepingtheorig-
inalSInetworkparametersunchanged.
3.Experiments
3.1.Experimentalsetup
Intheexperiments,weusedaMandarincorpusthatconsists
of3,000speaker(about1000hrs)withstandardaccenttobuild
abaselineTDNN-LSTMAM.BeforeNNmodeltraining,the
alignmentswereachievedfromaGMM-HMMAM,combined
withfMLLR,trainedusingthesamedataset.Ourspeakeradap-
tationdatasetconsistsof10MandarinspeakersfromHubei
ProvinceofChinaandeachspeakercontributes450utterances
(about0.5hr/speaker).Notethatthe10speakershavedifferent
levelsofaccentsandweexpectthatagoodspeakeradaptation
techniqueshouldhandledifferentlevelsofaccents.Weran-
domlyselected50utterancesasthecrossvalidationset,100
utterancesasthetestsetandtheothersasthetrainingset.Inthe
adaptationexperiments,wevariedthenumberoftrainingutter-
ancesfrom5to300toobservetheperformancesofdifferent
datasize.
ForthebaselineSIacousticmodel,40-dimensionalMel
-bankcepstralcoef(MFCCs)splicedwith2left,2
rightframesand100-dimensionali-vector,furthertransformed
to300-dimensionwithlineardiscriminateanalysis(LDA),were
usedasthenetworkinput.Theoutputsoftmaxlayerhas5,795
unitsrepresentingsenones.Moreover,theTDNN-LSTMmodel
has6TDNNlayers(520neurons)and3LSTMPlayers[6]
(520cellswith130recurrentnodesand130non-recurrent
nodes).Networktrainingstartedfromaninitiallearningrate
of0.0003
1
.Atrigramlanguagemodel(LM)wasusedineval-
uatingboththebaselineandtheadaptedmodels.Moreover,all
ofourexperimentswerebasedonKaldi[32].
3.2.ResultsofBaselineModel
Table1showsthecharactererrorrate(CER)foreachspeakers,
testedwiththebaselineAM.Wecanseethatthebaselinemodel
performsdifferentlyforeachspeakerandtheaverageCERis
24.86%.TheCERhasawiderangefrom3%to56.62%.We
manuallycheckedtherecordingsfromdifferentspeakersand
foundthatspeakerS05hadheavyaccentandspeakerS01had
slightMandarinaccent.Thishugedifferencegivesthespeaker
adaptationmethodsabigchallenge.Wewillreporttheadapta-
tionresultsintermsofaccentlevelslaterinSection3.5.
3.3.ComparisonofLIN,RSI,KLDandLHUC
WeinvestigatedtheadaptationabilityofLIN,RSI,KLDand
LHUCusingdifferentamountofadaptationdata.Previous
studiesonLHUC[22]havedemonstratedthatadaptingmore
layersinthenetworkcangetcontinuouslybetteraccuracy.
HenceweinsertedLHUCparametersaftereachhiddenlay-
ers.ForLIN,modelswereadaptedwithasmalllearningrate
of0.00001,while0.001and0.01wereusedasaninitiallearn-
ingrateforKLDandLHUC,respectively.Fromtheresults
showninFigure3a,wecanseethatKLDachievesthebestper-
formanceandismorestablethanRSIondifferentamountof
1
MoredetailsaboutthisarchitecturecanbefoundinKaldi:
egs/wsj/s5/local/nnet3/run
tdnn
lstm.sh
(a)
SlightAccent
(b)
MediumAccent
(c)
HeavyAccent
Figure5:
CERs(%)fordifferentmethodsondifferentdegreesofaccent.
adaptationdataforallspeakers.LIN,assimplelayer-insertion
method,isalsohelpful,butitsperformanceisnotasgoodas
theothertwo.ForRSIandLHUC,theirperformancesarecom-
parableinmostcases,butoverisoccurredforRSIwhen
theadaptationdatasizeexceeds200.
Furthermore,similarwith[26],wegaveadeepinvestiga-
tiononKLD-basedadaptationandresultsareshowninFig-
ure3b.First,unliketheresultsin[26],whereusingsmall
amountofdata(5or10utterances)forKLDadaptationisun-
fortunatelyharmful,westillcanobtainapparentCERreduction
whenthesamesizeofdataareusedforadaptation.Webelieve
thatthisisbecauseourtestingspeakershavenoticeableaccents,
i.e.,thedifferencebetweentheSIdataandthetargetspeaker
dataisThecomparisonofdifferent
ˆ
intherangeof
[0
:
0625
;
0
:
5]
alsoindicatesthatreasonableCERreductioncan
beobtainedevenwithasmall
ˆ
fordifferentsizeofadaptation
data.Thealsoclearlyshowsthatamediumregularization
weight(e.g.,0.25)ispreferredforlargerandsmalleradaptation
setsandasmallerregularizationweight(e.g.,0.0625)isbet-
terusedformediumsizeofadaptationset.Wealsocompared
theperformancesbetweendifferentspeakers.ResultsfromFig-
ure3cshowsthatKLDworksforeverytestingspeakerandthe
speakerwithhighestCERontheSImodel(i.e.,S5,withthe
heaviestaccent)achievesthelargestCERreduction.Butwith
theincreaseofadaptationdata,thegainoneachspeakerbe-
comessmallerandsmaller.
3.4.Combinations
Wefurtherexperimentedonmethodcombinationsandresults
aresummarizedinFigure4.Wecanseethecombinationsof
differentmethodscannotbringsalientimprovementsandthe
bestperformanceisachievedbyKLDonly.Evenbadly,any
combinationwithLINwilldragtheperformancetoLIN.Com-
biningLHUCwithKLDcanobtainslightlybetterresultthan
thevanillaLHUCforverysmall(lessthan10)andlarge(more
than200)adaptationdataset.Butforsmalladaptationdatasize
(20
˘
80),LHUCitselfperformsbetter.
3.5.Differentdegreesofaccent
AsshowninTable1earlier,thebaselineAM'sperformance
variesondifferentspeakers.It'snecessarytocomparedifferent
adaptationmethodsintermsofaccentlevel.Wemanuallycate-
gorizedthe10speakersinto3accentedgroups:slight,medium
andheavyaccordingtotheirperformancesonthebaselineAM
inTable1.Accordingtotheaccentlevel,resultsaresumma-
rizedinFigure5a(slight),Figure5b(medium)andFigure5c
(heavy).FromFigure5a,wecanseethatLHUCperformscon-
sistentlythebestfortheadaptationonslight-accentspeakers,
whileKLDandRSIarenotstable.Webelievethatthisisbe-
causethebaselinemodelistrainedusingdatamostlyfromMan-
darinspeakerswithstandardaccentandthebaselinemodelit-
selfisrobustenough;inthiscase,directupdateonthenetwork
parametersmaybeharmful.ObservingFigure5b,formedium-
accentspeakers,wecanseethatKLDandLHUCcangetcom-
parableperformanceswithmuchlowerCERthanLIN.RSIis
stillnotstableandoverhappenswhenalargeadaptation
datasetisused.Ifmemoryfootprintisamajorconsideration,
wesuggesttouseLHUCasitshasasmallsetofadaptedparam-
etersforeachspeaker;otherwiseLHUCandKLDcanbeboth
consideredformedium-accentspeakers.AsshowninFigure5c,
forheavy-accentspeakers,KLDcangetabsolutelythebestper-
formanceamongthethreemethods,followedbyLHUC,while
LINstillperformstheworst.WebelievethatKLD'ssuperior
performanceisbecausetheposteriordistributionoftheheavy-
accentspeechisfarawayfromthatoftheunaccentedspeech;in
thiscase,directlyupdatingthenetworkparametersordragging
thetwodistributionscloser,isthemosteffectivemeans.This
alsowhyRSIisbetterthanLHUCandwhywecannot
observeoverinthiscondition.
4.Conclusions
Inthiswork,wehavesystematicallycomparedtheperfor-
manceofthreewidely-usedspeakeradaptationmethodsona
challengingdatasetwithaccentedspeakers.Weshowthati-
vectorbasedSAT-DNNAMisalreadystrongenoughtoslight-
accentspeakersbutperformsbadlytomedium-andheavy-
accentspeakers.ByusingLIN,KLD,LHUC,wecanfur-
therimprovethespeechrecognitionperformancenotonlyfor
medium-andheavy-accentspeakers,butalsoforslight-accent
speakers.Moreover,theexperimentalresultsshowthat,ingen-
eral,KLDandLHUCconsistentlyoutperformLINandKLD
demonstratesthebestperformance.Thecombinationofdiffer-
entmethodscannotbringsalientimprovements.Fortheadap-
tationonslight-accentspeakers,LHUCispreferredwithcon-
sistentimprovement,whileKLDandRSIarenotstable.For
medium-accentspeakers,KLDandLHUCcangetcompara-
bleperformanceswithmuchlowerCERthanLIN.Forheavy-
accentspeakers,KLDcangetabsolutelythebestperformance,
followedbyLHUC,whileLINstillperformstheworst.
5.Acknowledgements
TheauthorswouldliketothankJianLi,MengfeiWuand
YongqingWangfortheirsupportsonthiswork.There-
searchworkissupportedbytheNationalKeyResearchand
DevelopmentProgramofChina(GrantNo.2017YFB1002102)
andtheNationalNaturalScienceFoundationofChina(Grant
No.61571363).
6.References
[1]
G.E.Dahl,D.Yu,L.Deng,andA.Acero,ﬁContext-dependent
pre-traineddeepneuralnetworksforlarge-vocabularyspeech
recognition,ﬂ
IEEETransactionsonaudio,speech,andlanguage
processing
,vol.20,no.1,pp.30Œ42,2012.
[2]
G.Hinton,L.Deng,D.Yu,G.E.Dahl,A.R.Mohamed,N.Jaitly,
A.Senior,V.Vanhoucke,P.Nguyen,T.N.Sainath
etal.
,ﬁDeep
neuralnetworksforacousticmodelinginspeechrecognition:The
sharedviewsoffourresearchgroups,ﬂ
IEEESignalProcessing
Magazine
,vol.29,no.6,pp.82Œ97,2012.
[3]
O.Abdel-Hamid,A.R.Mohamed,H.Jiang,andG.Penn,ﬁAp-
plyingconvolutionalneuralnetworksconceptstohybridnn-hmm
modelforspeechrecognition,ﬂin
IEEEInternationalConference
onAcoustics,SpeechandSignalProcessing
,2012,pp.4277Œ
4280.
[4]
O.Abdel-Hamid,L.Deng,andD.Yu,ﬁExploringconvolutional
neuralnetworkstructuresandoptimizationtechniquesforspeech
recognition.ﬂin
Interspeech
,vol.2013,2013,pp.1173Œ5.
[5]
V.Peddinti,D.Povey,andS.Khudanpur,ﬁAtimedelayneural
networkarchitectureforefmodelingoflongtemporalcon-
texts,ﬂin
SixteenthAnnualConferenceoftheInternationalSpeech
CommunicationAssociation
,2015.
[6]
H.Sak,A.Senior,andF.Beaufays,ﬁLongshort-termmemory
basedrecurrentneuralnetworkarchitecturesforlargevocabulary
speechrecognition,ﬂ
arXivpreprintarXiv:1402.1128
,2014.
[7]
H.Sak,A.Senior,K.Rao,andF.Beaufays,ﬁFastandaccurate
recurrentneuralnetworkacousticmodelsforspeechrecognition,ﬂ
arXivpreprintarXiv:1507.06947
,2015.
[8]
Y.Zhang,G.Chen,D.Yu,K.Yaco,S.Khudanpur,andJ.Glass,
ﬁHighwaylongshort-termmemoryrnnsfordistantspeechrecog-
nition,ﬂin
Acoustics,SpeechandSignalProcessing(ICASSP),
2016IEEEInternationalConferenceon
.IEEE,2016,pp.5755Œ
5759.
[9]
S.Zhang,C.Liu,H.Jiang,S.Wei,L.Dai,andY.Hu,ﬁFeed-
forwardsequentialmemorynetworks:Anewstructuretolearn
long-termdependency,ﬂ
arXivpreprintarXiv:1512.08301
,2015.
[10]
T.N.Sainath,O.Vinyals,A.Senior,andH.Sak,ﬁConvolutional,
longshort-termmemory,fullyconnecteddeepneuralnetworks,ﬂ
in
Acoustics,SpeechandSignalProcessing(ICASSP),2015IEEE
InternationalConferenceon
.IEEE,2015,pp.4580Œ4584.
[11]
P.C.Woodland,ﬁSpeakeradaptationforcontinuousdensity
hmms:Areview,ﬂ2001.
[12]
J.L.GauvainandC.H.Lee,ﬁMaximumaposterioriestimation
formultivariategaussianmixtureobservationsofmarkovchains,ﬂ
IEEETransactionsonSpeechandAudioProcessing
,vol.2,no.2,
pp.291Œ298,1994.
[13]
C.J.LegetterandP.C.Woodland,ﬁMaximumlikelihoodlinear
regressionspeakeradaptationofcontinuousdensityhmms,ﬂ
Com-
puterSpeechandLanguage
,1995.
[14]
V.V.Digalakis,D.Rtischev,andL.G.Neumeyer,ﬁSpeakeradap-
tationusingconstrainedestimationofgaussianmixtures,ﬂ
IEEE
TransactionsonSpeechandAudioProcessing
,vol.3,no.5,pp.
357Œ366,1995.
[15]
M.J.F.Gales,ﬁMaximumlikelihoodlineartransformations
forhmm-basedspeechrecognition,ﬂ
ComputerSpeechandLan-
guage
,vol.12,no.2,p.7598,1998.
[16]
ŠŠ,ﬁClusteradaptivetrainingofhiddenmarkovmodels,ﬂ
SpeechandAudioProcessingIEEETransactionson
,vol.8,no.4,
pp.417Œ428,2000.
[17]
R.Kuhn,J.C.Junqua,P.Nguyen,andN.Niedzielski,ﬁRapid
speakeradaptationineigenvoicespace,ﬂ
IEEETransSpeechAu-
dioProc
,vol.8,no.6,pp.695Œ707,2000.
[18]
L.F.UebelandP.C.Woodland,ﬁAninvestigationintovocaltract
lengthnormalisation,ﬂin
EuropeanConferenceonSpeechCom-
municationandTechnology,Eurospeech1999,Budapest,Hun-
gary,September
,1999.
[19]
J.Neto,L.Almeida,M.Hochberg,C.Martins,L.Nunes,S.Re-
nals,andT.Robinson,ﬁSpeaker-adaptationforhybridhmm-ann
continuousspeechrecognitionsystem,ﬂin
FourthEuropeanCon-
ferenceonSpeechCommunicationandTechnology
,1995.
[20]
B.LiandK.C.Sim,ﬁComparisonofdiscriminativeinputandout-
puttransformationsforspeakeradaptationinthehybridnn/hmm
systems,ﬂin
EleventhAnnualConferenceoftheInternational
SpeechCommunicationAssociation
,2010.
[21]
R.Gemello,F.Mana,S.Scanzio,P.Laface,andR.DeMori,
ﬁLinearhiddentransformationsforadaptationofhybridann/hmm
models,ﬂ
SpeechCommunication
,vol.49,no.10-11,pp.827Œ835,
2007.
[22]
P.SwietojanskiandS.Renals,ﬁLearninghiddenunitcontri-
butionsforunsupervisedspeakeradaptationofneuralnetwork
acousticmodels,ﬂin
SpokenLanguageTechnologyWorkshop
(SLT),2014IEEE
.IEEE,2014,pp.171Œ176.
[23]
G.Saon,H.Soltau,D.Nahamoo,andM.Picheny,ﬁSpeaker
adaptationofneuralnetworkacousticmodelsusingi-vectors.ﬂin
ASRU
,2013,pp.55Œ59.
[24]
Y.Miao,H.Zhang,andF.Metze,ﬁSpeakeradaptivetrain-
ingofdeepneuralnetworkacousticmodelsusingi-vectors,ﬂ
IEEE/ACMTransactionsonAudio,SpeechandLanguagePro-
cessing(TASLP)
,vol.23,no.11,pp.1938Œ1949,2015.
[25]
O.Abdel-HamidandH.Jiang,ﬁFastspeakeradaptationofhy-
bridnn/hmmmodelforspeechrecognitionbasedondiscrimina-
tivelearningofspeakercode,ﬂin
Acoustics,SpeechandSignal
Processing(ICASSP),2013IEEEInternationalConferenceon
.
IEEE,2013,pp.7942Œ7946.
[26]
D.Yu,K.Yao,H.Su,G.Li,andF.Seide,ﬁKl-divergencereg-
ularizeddeepneuralnetworkadaptationforimprovedlargevo-
cabularyspeechrecognition,ﬂin
Acoustics,SpeechandSignal
Processing(ICASSP),2013IEEEInternationalConferenceon
.
IEEE,2013,pp.7893Œ7897.
[27]
A.SeniorandI.Lopez-Moreno,ﬁImprovingdnnspeakerinde-
pendencewithi-vectorinputs,ﬂin
Acoustics,SpeechandSignal
Processing(ICASSP),2014IEEEInternationalConferenceon
.
IEEE,2014,pp.225Œ229.
[28]
Z.Huang,S.M.Siniscalchi,andC.-H.Lee,ﬁBayesianunsuper-
visedbatchandonlinespeakeradaptationofactivationfunction
parametersindeepmodelsforautomaticspeechrecognition,ﬂ
IEEE/ACMTransactionsonAudio,Speech,andLanguagePro-
cessing
,vol.25,no.1,pp.64Œ75,2017.
[29]
G.Saon,H.-K.J.Kuo,S.Rennie,andM.Picheny,ﬁTheibm
2015englishconversationaltelephonespeechrecognitionsys-
tem,ﬂ
arXivpreprintarXiv:1505.05899
,2015.
[30]
D.Povey,V.Peddinti,D.Galvez,P.Ghahremani,V.Manohar,
X.Na,Y.Wang,andS.Khudanpur,ﬁPurelysequence-trainedneu-
ralnetworksforasrbasedonlattice-freemmi.ﬂin
Interspeech
,
2016,pp.2751Œ2755.
[31]
W.Xiong,J.Droppo,X.Huang,F.Seide,M.Seltzer,A.Stolcke,
D.Yu,andG.Zweig,ﬁThemicrosoft2016conversationalspeech
recognitionsystem,ﬂin
Acoustics,SpeechandSignalProcessing
(ICASSP),2017IEEEInternationalConferenceon
.IEEE,2017,
pp.5255Œ5259.
[32]
D.Povey,A.Ghoshal,G.Boulianne,L.Burget,O.Glembek,
N.Goel,M.Hannemann,P.Motlicek,Y.Qian,P.Schwarz
etal.
,
ﬁThekaldispeechrecognitiontoolkit,ﬂin
IEEE2011workshop
onautomaticspeechrecognitionandunderstanding
,no.EPFL-
CONF-192584.IEEESignalProcessingSociety,2011.
"
33,GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations,http://arxiv.org/pdf/1806.05662v3.pdf,https://github.com/YJHMITWEB/GLoMo-tensorflow,"GLoMo:UnsupervisedlyLearnedRelationalGraphs
asTransferableRepresentations
ZhilinYang

1
,Jake(Junbo)Zhao

23
,BhuwanDhingra
1
KaimingHe
3
,WilliamW.Cohen
4
y
,RuslanSalakhutdinov
1
,YannLeCun
23

Equalcontribution
1
CarnegieMellonUniversity,
2
NewYorkUniversity,
3
FacebookAIResearch,
4
Google,Inc.
{zhiliny,bdhingra,rsalakhu}@cs.cmu.edu
{jakezhao,yann}@cs.nyu.com,kaiminghe@fb.com,wcohen@google.com
Abstract
Moderndeeptransferlearningapproacheshavemainlyfocusedonlearning
generic
featurevectorsfromonetaskthataretransferabletoothertasks,suchasword
embeddingsinlanguageandpretrainedconvolutionalfeaturesinvision.However,
theseapproachesusuallytransferunaryfeaturesandlargelyignoremorestructured
graphicalrepresentations.Thisworkexploresthepossibilityoflearninggeneric
latentrelationalgraphs
thatcapturedependencies
between
pairsofdataunits(e.g.,
wordsorpixels)fromlarge-scaleunlabeleddataandtransferringthegraphsto
downstreamtasks.Ourproposedtransferlearningframeworkimprovesperfor-
manceonvarioustasksincludingquestionanswering,naturallanguageinference,
sentimentanalysis,andimageWealsoshowthatthelearnedgraphs
aregenericenoughtobetransferredtodifferentembeddingsonwhichthegraphs
havenotbeentrained(includingGloVeembeddings,ELMoembeddings,and
RNNhiddenunits),orembedding-freeunitssuchasimagepixels.
1Introduction
Recentadvancesindeeplearninghavelargelyreliedonbuildingblockssuchasconvolutional
networks(CNNs)[
19
]andrecurrentnetworks(RNNs)[
14
]augmentedwithattentionmechanisms
[
1
].Whilepossessinghighrepresentationalcapacity,thesearchitecturesprimarilyoperateongrid-like
orsequentialstructuresduetotheirbuilt-inﬁinnatepriorsﬂ.Asaresult,CNNsandRNNslargelyrely
onhighexpressivenesstomodelcomplexstructuralphenomena,compensatingthefactthattheydo
notexplicitlyleveragestructural,graphicalrepresentations.
Thisparadigmhasledtoastandardizednormintransferlearningandanex-
pressivefunctiononalargedatasetwithorwithoutsupervision,andthenapplyingthefunctionto
downstreamtaskdataforfeatureextraction.NotableexamplesincludepretrainedImageNetfeatures
[13]andpretrainedwordembeddings[24,29].
Incontrast,avarietyofreal-worlddataexhibitmuchricherrelational
graph
structuresthanthesimple
grid-likeorsequentialstructures.Thisisalsoemphasizedbyaparallelwork[
3
].Forexamplein
thelanguagedomain,linguistsuseparsetreestorepresentsyntacticdependencybetweenwords;
informationretrievalsystemsexploitknowledgegraphstoentityrelations;andcoreference
resolutionisdevisedtoconnectdifferentexpressionsofthesameentity.Assuch,theseex
structuresareuniversallypresentinalmostanynaturallanguagedataregardlessofthetargettasks,
whichsuggeststhepossibilityoftransferacrosstasks.Theseobservationsalsogeneralizetoother
domainssuchasvision,wheremodelingtherelationsbetweenpixelsisprovenuseful[
28
,
50
,
44
].
y
WorkdoneatCMU
Preprint.Workinprogress.
arXiv:1806.05662v3  [cs.LG]  2 Jul 2018Figure1:
Traditionaltransferlearningversusournewtransferlearningframework.Insteadoftransferring
features,wetransferthegraphsoutputbyanetwork.Thegraphsaremultipliedbyfeatures(e.g.
embeddingsorhiddenstates)toproducestructure-awarefeatures.
Oneobstacleremaining,however,isthatmanyoftheuniversalstructuresareessentiallyhuman-
curatedandexpensivetoacquireonalargescale,whileautomatically-inducedstructuresaremostly
limitedtoonetask[17,42,44].
Inthispaper,weattempttoaddresstwochallenges:1)tobreakawayfromthestandardizednormof
feature-baseddeeptransferlearning
2
,and2)tolearnversatilestructuresinthedatawithadata-driven
approach.Inparticular,weareinterestedinlearningtransferable
latentrelationalgraphs
,where
thenodesofalatentgrapharetheinput
units
,e.g.,allthewordsinasentence.Thetaskoflatent
relationalgraphlearningistolearnan
afmatrix
wheretheweights(possiblyzero)capturethe
dependenciesbetweenanypairofinputunits.
Toachievetheabovegoals,weproposeanovelframeworkofunsupervisedlatentgraphlearning
calledGLoMo(GraphsfromLOw-levelunitMOdeling).,wetrainaneuralnetwork
fromlarge-scaleunlabeleddatatooutputlatentgraphs,andtransferthenetworktoextractgraph
structuresondownstreamtaskstoaugmenttheirtraining.Thisapproachallowsustoseparatethe
featuresthatrepresentthesemanticmeaningofeachunitandthegraphsthathowtheunits
mayinteract.Ideally,thegraphscapturetask-independentstructuresunderlyingthedata,andthus
becomeapplicabletodifferentsetsoffeatures.Figure1highlightsthedifferencebetweentraditional
feature-basedtransferlearningandournewframework.
ExperimentalresultsshowthatGLoMoimprovesperformanceonvariouslanguagetasksincluding
questionanswering,naturallanguageinference,andsentimentanalysis.Wealsodemonstrate
thatthelearnedgraphsaregenericenoughtoworkwithvarioussetsoffeaturesonwhichthe
graphshavenotbeentrained,includingGloVeembeddings[
29
],ELMoembeddings[
30
],andtask-
RNNstates.Wealsoidentifykeyfactorsoflearningsuccessfulgenericgraphs:decoupling
graphsandfeatures,hierarchicalgraphrepresentations,sparsity,unit-levelobjectives,andsequence
prediction.Todemonstratethegeneralityofourframework,wefurthershowimprovedresultson
imagebyapplyingGLoMotomodeltherelationaldependenciesbetweenthepixels.
2UnsupervisedRelationalGraphLearning
Weproposeaframeworkforunsupervisedlatentgraphlearning.Givenaone-dimensionalinput
x
=(
x
1
;

;x
T
)
,whereeach
x
t
denotesaninputunitatposition
t
and
T
isthelengthofthe
sequence,thegoaloflatentgraphlearningistolearna
(
T

T
)
afnitymatrix
G
suchthateachentry
G
ij
capturesthedependencybetweentheunit
x
i
andtheunit
x
j
.Theafmatrixisasymmetric,
representingadirectedweightedgraph.Inparticular,inthisworkweconsiderthecasewhereeach
columnoftheafmatrixsumstoone,forcomputationalconvenience.Inthefollowingtext,with
alittleabuseofnotation,weuse
G
todenoteasetofafmatrices.Weusethetermsﬁafnity
matricesﬂandﬁgraphsﬂinterchangeably.
Duringtheunsupervisedlearningphase,ourframeworktrainstwonetworks,a
graphpredictor
network
g
anda
featurepredictor
network
f
.Giventheinput
x
,ourgraphpredictor
g
producesa
2
Throughoutthepaper,weuseﬁfeatureﬂtorefertounaryfeaturerepresentations,anduseﬁgraphﬂtoreferto
structural,graphicalrepresentations.
2
setofgraphs
G
=
g
(
x
)
.Thegraphs
G
arerepresentedasa3-dtensorin
R
L

T

T
,where
L
isthe
numberof
layers
thatproducegraphs.Foreachlayer
l
,thelasttwodimensions
G
l
a
(
T

T
)
afmatrixthatcapturesthedependenciesbetweenanypairofinputunits.Thefeaturepredictor
network
f
thentakesthegraphs
G
andtheoriginalinput
x
toperformapredictivetask.
Duringthetransferphase,givenaninput
x
0
fromadownstreamtask,weusethegraphpredictor
g
to
extractgraphs
G
=
g
(
x
0
)
.Theextractedgraphs
G
arethenfedastheinputtothedownstreamtask
networktoaugmenttraining.,wemultiply
G
withfeaturessuchasinput
embeddingsandhiddenstates(seeFigure1).Thenetwork
f
isdiscardedduringthetransferphase.
Next,wewillintroducethenetworkarchitecturesandobjectivefunctionsforunsupervisedlearning,
followedbythetransferprocedure.AnoverviewofourframeworkisillustratedinFigure2.
2.1UnsupervisedLearning
GraphPredictor
Thegraphpredictor
g
isinstantiatedastwomulti-layerCNNs,a
key
CNN,
anda
query
CNN.Giventheinput
x
,thekeyCNNoutputsasequenceofconvolutionalfeatures
(
k
1
;

;
k
T
)
andthequeryCNNsimilarlyoutputs
(
q
1
;

;
q
T
)
.Atlayer
l
,basedontheseconvolu-
tionalfeatures,wecomputethegraphsas
G
l
ij
=

ReLU
(
k
l
>
i
q
l
j
+
b
)

2
P
i
0

ReLU
(
k
l
>
i
0
q
l
j
+
b
)

2
(1)
where
k
l
i
=
W
l
k
k
i
and
q
l
j
=
W
l
q
q
j
.Thematrices
W
l
k
and
W
l
q
aremodelparametersatlayer
l
,and
thebias
b
isascalarparameter.Thisresemblescomputingtheattentionweights[
1
]fromposition
j
toposition
i
exceptthattheexponentialactivationinthesoftmaxfunctionisreplacedwithasquared
ReLUoperationŠweuseReLUstoenforcesparsityandthesquareoperationstostabilizetraining.
Moreover,weemployconvolutionalnetworkstoletthegraphs
G
beawareofthelocalorderofthe
inputandcontext,uptothesizeofeachunit'sreceptive
FeaturePredictor
Nowweintroducethefeaturepredictor
f
.Ateachlayer
l
,theinputtothefeature
predictor
f
isasequenceoffeatures
F
l

1
=(
f
l

1
1
;

;
f
l

1
t
)
andanafmatrix
G
l
extractedby
thegraphpredictor
g
.Thezero-thlayerfeatures
F
0
areinitializedtobetheembeddingsof
x
.The
afmatrix
G
l
isthencombinedwiththecurrentfeaturestocomputethenext-layerfeaturesat
eachposition
t
,
f
l
t
=
v
(
X
j
G
l
jt
f
l

1
j
;
f
l

1
t
)
(2)
where
v
isacompositionalfunctionsuchasaGRUcell[
8
]oralinearlayerwithresidualconnections.
Inotherwords,thefeatureateachpositioniscomputedasaweightedsumofotherfeatures,where
theweightsaredeterminedbythegraph
G
l
,followedbytransformationfunction
v
.
ObjectiveFunction
Atthetoplayer,weobtainthefeatures
F
L
.Ateachposition
t
,weusethe
feature
f
L
t
toinitializethehiddenstatesofanRNNdecoder,andemploythedecodertopredict
theunitsfollowing
x
t
.,theRNNdecodermaximizestheconditionallogprobability
log
P
(
x
t
+1
;

;x
t
+
D
j
x
t
;
f
l
t
)
usinganauto-regressivefactorizationasinstandardlanguagemodeling
[
47
](alsoseeFigure2).Here
D
isahyper-parametercalledthe
contextlength
.Theoverallobjective
iswrittenasthesumoftheobjectivesatallpositions
t
,
max
X
t
log
P
(
x
t
+1
;

;x
t
+
D
j
x
t
;
f
L
t
)
(3)
Becauseourobjectiveiscontextprediction,wemasktheconvolutionalandthegraph
G
(see
Eq.1)inthenetwork
g
topreventthenetworkfromaccessingthefuture,following[34].
2.1.1Desiderata
Thereareseveralkeydesiderataoftheaboveunsupervisedlearningframework,whichalsohighlight
theessentialdifferencesbetweenourframeworkandpreviousworkonself-attentionandpredictive
unsupervisedlearning:
3
Figure2:
OverviewofourapproachGLoMo.Duringtheunsupervisedlearningphase,thefeaturepredictor
andthegraphpredictorarejointlytrainedtoperformcontextprediction.Duringthetransferphase,thegraph
predictorisfrozenandusedtoextractgraphsforthedownstreamtasks.AnRNNdecoderisappliedtoall
positionsinthefeaturepredictor,butweonlyshowtheoneatpositionﬁAﬂforsimplicity.ﬁSelectoneﬂmeans
thegraphscanbetransferredtoanylayerinthedownstreamtaskmodel.ﬁFFﬂreferstofeed-forwardnetworks.
Thegraphsoutputbythegraphpredictorareusedastheweightsintheﬁweightedsumﬂoperation(seeEq.2).
Decouplinggraphsandfeatures
Unlikeself-attention[
42
]thatfusesthecomputationofgraphs
andfeaturesintoonenetwork,weemployseparatenetworks
g
and
f
forlearninggraphsandfeatures
respectively.Thefeaturesrepresentthesemanticmeaningofeachunitwhilethegraphhow
theunitsmayinteract.Thisincreasesthetransferabilityofthegraphs
G
because(1)thegraph
predictor
g
isfreedfromencodingnon-structuralinformation,and(2)thedecoupled
settingisclosertoourtransfersetting,wherethegraphsandfeaturesarealsoseparated.
Sparsity
InsteadofusingSoftmaxforattention[
1
],weemployasquaredReLUactivationinEq.(1)
toenforcesparseconnectionsinthegraphs.Infact,mostofthelinguisticallymeaningfulstructures
aresparse,suchasparsetreesandcoreferencelinks.Webelievesparsestructuresreducenoiseand
aremoretransferable.
Hierarchicalgraphrepresentations
Welearnmultiplelayersofgraphs,whichallowsustomodel
hierarchicalstructuresinthedata.
Unit-levelobjectives
InEq.(3),weimposeacontextpredictionobjectiveoneachunit
x
t
.An
alternativeistoemployasequence-levelobjectivesuchaspredictingthenextsentence[
18
]or
translatingtheinputintoanotherlanguage[
42
].However,sincetheweightedsumoperationinEq.
(2)ispermutationinvariant,thefeaturesineachlayercanberandomlyshufwithoutaffectingthe
objective,whichweobservedinourpreliminaryexperiments.Asaresult,theinducedgraphbearsno
relationtothestructuresunderlyingtheinput
x
whenasequence-levelobjectiveisemployed.
Sequenceprediction
Asopposedtopredictingjusttheimmediatenextunit[
28
,
30
],wepredictthe
contextoflengthupto
D
.Thisgivesstrongertrainingsignalstotheunsupervisedlearner.
Laterintheexperimentalsection,wewilldemonstratethatallthesefactorscontributetosuccessful
trainingofourframework.
2.2LatentGraphTransfer
Inthissection,wediscusshowtotransferthegraphpredictor
g
todownstreamtasks.
Supposeforadownstreamtaskthemodelisadeepmulti-layernetwork.,eachlayeris
denotedasafunction
h
thattakesinfeatures
H
=(
h
1
;

;
h
T
)
andpossiblyadditionalinputs,and
outputsfeatures
(
h
0
1
;

;
h
0
T
)
.Thefunction
h
canbeinstantiatedasanyneuralnetworkcomponent,
suchasCNNs,RNNs,attention,andfeed-forwardnetworks.Thissettingisgeneralandsubsumes
themajorityofmodernneuralarchitectures.
Givenaninputexample
x
0
fromthedownstreamtask,weapplythegraphpredictortoobtainthe
graphs
G
=
g
(
x
0
)
.Let

l
=
Q
l
i
=1
G
i
2
R
T

T
denotetheproductofallafmatricesfromthe
4
Table1:Mainresultsonnaturallanguagedatasets.Self-attentionmodulesareincludedinallbaseline
models.Allbaselinemethodsarefeature-basedtransferlearningmethods,includingELMoand
GloVe.Ourmethodscombinegraph-basedtransferwithfeature-basedtransfer.Ourgraphsoperateon
varioussetsoffeatures,includingGloVeembeddings,ELMoembeddings,andRNNstates.ﬁmism.ﬂ
referstotheﬁmismatchedﬂsetting.
Transfermethod
SQuADGloVe
SQuADELMo
IMDBGloVe
MNLIGloVe
EMF1
EMF1
Accuracy
matchedmism.
transferfeatureonly(baseline)
69.3378.73
74.7582.95
88.51
77.1477.40
GLoMoonembeddings
70.8479.90
76.0084.13
89.16
78.3278.00
GLoMoonRNNstates
70.9579.95
75.5983.62
-
--
layerstothe
l
-thlayer.Thiscanbeviewedaspropagatingtheconnectionsamongmultiplelayers
ofgraphs,whichallowsustomodelhierarchicalstructures.Wethentakeamixtureofallthegraphs
in
f
G
l
g
L
l
=1
[f

l
g
L
l
=1
,
M
=
L
X
l
=1
m
l
G
G
l
+
L
X
l
=1
m
l


l
;
s.t.
L
X
l
=1
(
m
l
G
+
m
l

)=1
Themixtureweights
m
l
G
and
m
l

canbeinstantiatedasSoftmax-normalizedparameters[
30
]orcan
beconditionedonthefeatures
H
.Totransferthemixedlatentgraph,weagainadopttheweighted
sumoperationasinEq.(2).,weusetheweightedsum
HM
(seeFigures1and2),in
additionto
H
,astheinputtothefunction
h
.Thiscanbeviewedasperformingattentionwithweights
givenbythemixedlatentgraph
M
.Thissetupoflatentgraphtransferisgeneralandeasytobe
pluggedin,asthegraphscanbeappliedtoanylayerinthenetworkarchitecture,witheitherlearned
orpretrainedfeatures
H
,atvariablelength.
2.3ExtensionsandImplementation
Sofarwehaveintroducedageneralframeworkofunsupervisedlatentgraphlearning.Thisframework
canbeextendedandimplementedinvariousways.
Inourimplementation,atposition
t
,inadditiontopredictingtheforwardcontext
(
x
t
+1
;

;x
t
+
D
)
,
wealsouseaseparatenetworktopredictthebackwardcontext
(
x
t

D
;

;x
t

1
)
,similarto[
30
].
Thisallowsthegraphs
G
tocapturebothforwardandbackwarddependencies,asgraphslearned
fromonedirectionaremaskedonfuturecontext.Accordingly,duringtransfer,wemixthegraphs
fromtwodirectionsseparately.
Inthetransferphase,therearedifferentwaysofeffectivelyfusing
H
and
HM
.Inpractice,wefeed
theconcatenationof
H
andagatedoutput,
W
1
[
H
;
HM
]

˙
(
W
2
[
H
;
HM
])
,tothefunction
h
.Here
W
1
and
W
2
areparametermatrices,
˙
denotesthesigmoidfunction,and

denoteselement-wise
multiplication.Wealsoadoptthemulti-headattention[
42
]toproducemultiplegraphsperlayer.We
useamixtureofthegraphsfromdifferentheadsfortransfer.
Itisalsopossibletoextendourframeworkto2-dor3-ddatasuchasimagesandvideos.The
adaptationsneededaretoadopthigh-dimensionalattention[
44
,
28
],andtopredictahigh-dimensional
context(e.g.,predictingagridoffuturepixels).Asanexample,inourexperiments,weusethese
adaptationsonthetaskofimage
3Experiments
3.1NaturalLanguageTasksandSetting
QuestionAnswering
Thestanfordquestionansweringdataset[
31
](SQuAD)wasrecentlyproposed
toadvancemachinereadingcomprehension.Thedatasetconsistsofmorethan100,000+question-
answerpairsfrom500+Wikipediaarticles.Eachquestionisassociatedwithacorrespondingreading
passageinwhichtheanswertothequestioncanbededuced.
NaturalLanguageInference
WechosetousethelatestMulti-GenreNLIcorpus(MNLI)[
46
].
Thisdatasethascollected433ksentencepairsannotatedwithtextualentailmentinformation.Ituses
thesamemodelingprotocolasSNLIdataset[
4
]butcoversa10differentgenresofbothspoken
5
Table2:Ablationstudy.
Method
SQuADGloVe
SQuADELMo
IMDBGloVe
MNLIGloVe
EMF1
EMF1
Accuracy
matchedmism.
GLoMo
70.8479.90
76.0084.13
89.16
78.32
78.00
-decouple
70.4579.56
75.8983.79
-
--
-sparse
70.1379.34
75.6183.89
88.96
78.0777.75
-hierarchical
69.9279.23
75.7083.72
88.71
77.8777.85
-unit-level
69.2378.66
74.8483.37
88.49
77.58
78.05
-sequence
69.9279.29
75.5083.70
88.96
78.1177.76
uniformgraph
69.4878.82
75.1483.28
88.57
77.2677.50
andformalwrittentext.Theevaluationinthisdatasetcanbesetuptobein-domain(Matched)or
cross-domain(Mismatched).WedidnotincludetheSNLIdataintoourtrainingset.
SentimentAnalysis
Weusethemoviereviewdatasetcollectedin[
22
],with25,000trainingand
25,000testingsamplescrawledfromIMDB.
TransferSetting
WepreprocessedtheWikipediadumpandobtainedacorpusofover700million
tokensaftercleaninghtmltagsandremovingshortparagraphs.Wetrainedthenetworks
g
and
f
on
thiscorpusasdiscussedinSection2.1.Weusedrandomlyinitializedembeddingstotrainboth
g
and
f
,whilethegraphsaretestedonotherembeddingsduringtransfer.Wetransferthegraphpredictor
g
toadownstreamtasktoextractgraphs,whicharethenusedforsupervisedtraining,asintroduced
inSection2.2.Weexperimentedwithapplyingthetransferredgraphstovarioussetsoffeatures,
includingGloVeembeddings,ELMoembeddings,andtheRNNlayer'soutput.
3.2Mainresults
OnSQuAD,wefollowtheopen-sourcedimplementation[
9
]exceptthatwedroppedweightaveraging
toruleoutensemblingeffects.Thismodelemploysaself-attentionlayerfollowingthebi-attention
layer,alongwithmultiplelayersofRNNs.OnMNLI,weadopttheopen-sourcedimplementation
[
5
].Additionally,weaddaself-attentionlayerafterthebi-inferencecomponenttofurthermodel
contextdependency.ForIMDB,ourbaselineutilizesafeedforwardnetworkarchitecturecomposed
ofRNNs,linearlayersandself-attention.Notethestate-of-the-art(SOTA)modelsonthesedatasets
are[
49
,
21
,
25
]respectively.However,theseSOTAresultsoftenrelyondataaugmentation[
49
],
semi-supervisedlearning[
25
],additionaltrainingdata(SNLI)[
20
],orspecializedarchitectures[
20
].
Inthiswork,wefocusoncompetitivebaselineswithgeneralarchitecturesthattheSOTAmodelsare
basedontotestthegraphtransferperformanceandexcludeindependentfactors.Thecode
toreproduceallourresultsisavailableat(
removedforreview
).
ThemainresultsarereportedinTable1.Therearethreeimportantmessages.First,wehave
purposelyincorporatedtheself-attentionmoduleinto
all
ofourbaselinesmodelsŠindeedhaving
self-attentioninthearchitecturecouldpotentiallyinduceasupervisedly-trainedgraph,becauseof
whichonemayarguethatthisgraphcouldreplaceitsunsupervisedcounterpart.However,asis
showninTable1,augmentingtrainingwithunsupervisedly-learnedgraphshasfurtherimproved
performance.Second,asweadoptpretrainedembeddingsin
all
themodels,thebaselinesestablish
theperformanceoffeature-basedtransfer.OurresultsinTable1indicatethatwhencombinedwith
feature-basedtransfer,ourgraphtransfermethodsareabletoyieldfurtherimprovement.Third,the
learnedgraphsaregenericenoughtoworkwithvarioussetsoffeatures,includingGloVeembeddings,
ELMoembeddings,andRNNoutput.
3.3AblationStudy
Inadditiontocomparinggraph-basedtransferagainstfeature-basedtransfer,wefurtherconducted
aseriesofablationstudies.Herewemainlytargetatthefollowingcomponentsinourframework:
decouplingfeatureandgraphnetworks,sparsity,hierarchical(i.e.multiplelayersof)graphs,unit-
levelobjectives,andsequenceprediction.Respectively,weexperimentedwithcouplingthetwo
networks,removingtheReLUactivations,usingonlyasinglelayerofgraphs,usingasentence-level
Skip-thoughtobjective[
18
],andreducingthecontextlengthtoone[
30
].AsisshowninTable2,all
thesefactorscontributetobetterperformanceofourmethod,whichourdesideratadiscussed
inSection2.1.1.Additionally,wedidasanitycheckbyreplacingthetrainedgraphswithuniformly
6
(a)
Relatedtocoreferenceresolution.
(b)
Attendingtoobjectsformodelinglong-termdependency.
(c)
Attendingtonegativewordsandpredicates.
(d)
Attendingtonouns,verbs,andadjectivesfortopicmodeling.
Figure3:
VisualizationofthegraphsontheMNLIdataset.ThegraphpredictorhasnotbeentrainedonMNLI.
Thewordsonthey-axisﬁattendﬂtothewordsonthea-axis;i.e.,eachrowsumsto1.
sampledafnitymatrices(similarto[
15
])duringthetransferphase.Thisresultshowsthatthelearned
graphshaveplayedavaluablerolefortransfer.
3.4VisualizationandAnalysis
WevisualizethelatentgraphsontheMNLIdatasetinFigure3.Weremoveirrelevantrowsin
theafmatricestohighlightthekeypatterns.ThegraphinFigure3aresemblescoreference
resolutionasﬁheﬂisattendingtoﬁGaryBauerﬂ.InFigure3b,thewordsattendtotheobjectssuchas
ﬁGreenGrottoﬂ,whichallowsmodelinglong-termdependencywhenaclauseexists.InFigure3c,the
wordsfollowingﬁnotﬂattendtoﬁnotﬂsothattheyareawareofthenegation;similarly,thepredicate
ﬁsplashedﬂisattendedbythefollowingobjectandadverbial.Figure3dpossiblydemonstratesaway
oftopicmodelingbyattendingtoinformativewordsinthesentence.Overall,thoughseemingly
differentfromhuman-curatedstructuressuchasparsetrees,theselatentgraphsdisplaylinguistic
meaningstosomeextent.AlsonotethatthegraphpredictorhasnotbeentrainedonMNLI,which
suggeststhetransferabilityofthelatentgraphs.
7
Figure4:Visualization.Left:asharkimageastheinput.Middle:weightsoftheedgesconnected
withthe
central
pixel,organizedinto24heads(3layerswith8headseach).Right:weightsofthe
edgesconnectedwiththe
bottom-right
pixel.Notetheuseofmasking.
Method/Base-model
ResNet-18
ResNet-34
baseline
90.93

0.33
91.42

0.17
GLoMo
91.55

0.23
91.70

0.09
ablation:uniformgraph
91.07

0.24
-
Table3:CIFAR-10results.Weadopta42,000/8,000train/validationsplitŠonce
thebestmodelisselectedaccordingtothevalidationerror,wedirectlyforwardittothetestset
withoutdoinganyvalidationsetplace-backretraining.Weonlyusedhorizontalfordata
augmentation.Theresultsareaveragedfrom5roundsofexperiments.
3.5VisionTask
Image
Wearealsopromptedtoextendthescopeofourapproachfromnaturallan-
guagetovisiondomain.Drawingfromnaturallanguagegraphpredictor
g
(

)
leadstheunsupervised
trainingphaseinvisiondomaintoaPixelCNN-likesetup[
27
],butwithasequencepredictionwindow
ofsize3x3(essentiallyonlypredictingthebottom-rightquarterunderthemask).Weleveragethe
entireImageNet[
11
]datasetandhavetheimagesresizedto32x32[
27
].Inthetransferphase,we
choseCIFAR-10asourtargettask.Similartothelanguageexperiments,weaugment
H
by
HM
,andobtaintheinputthroughagatinglayer.ThisresultisthenfedintoaResNet[
13
]
toperformregularsupervisedtraining.Twoarchitectures,i.e.ResNet-18andResNet-34,areex-
perimentedhere.AsshowninTable3,GLoMoimprovesperformanceoverthebaselines,which
demonstratesthatGLoMoasageneralframeworkalsogeneralizestoimages.
InthemeantimewedisplaytheattentionweightsweobtainfromthegraphpredictorinFigure4.We
canseethat
g
hasestablishedtheconnectionsfromkey-pointpixelswhileexhibitingsomevariation
acrossdifferentattentionheads.Therehasbeensimilarvisualizationreportedby[
50
]lately,inwhich
avanillatransformermodelisexploitedforgenerativeadversarialtraining.Puttingtheseresults
togetherwewanttoencouragefutureresearchtotakefurtherexplorationintotherelationallong-term
dependencyinimagemodeling.
4RelatedWork
Thereisanoverwhelmingamountofevidenceonthesuccessoftransferringpre-trainedrepresenta-
tionsacrosstasksindeeplearning.Notableexamplesinthelanguagedomainincludetransferring
wordvectors[
29
,
24
,
30
]andsentencerepresentations[
18
,
10
].Similarly,intheimagedomainitis
standardpracticetousefeatureslearnedinasupervisedmannerontheImageNet[
33
,
37
]dataset
forotherdownstreampredictiontasks[
32
].OurapproachiscomplementarytotheseapproachesŒ
insteadoftransferringfeatureswetransfergraphsofdependencypatternsbetweentheinputsŒand
canbecombinedwiththeseexistingtransferlearningmethods.
Specializedneuralnetworkarchitectureshavebeendevelopedfordifferentdomainswhichrespect
high-levelintuitionsaboutthedependenciesamongthedatainthosedomains.Examplesinclude
CNNsforimages[
19
],RNNsfortext[
14
]andGraphNeuralNetworksforgraphs[
35
].Inthe
languagedomain,moreinvolvedstructureshavealsobeenexploitedtoinformneuralnetwork
architectures,suchasphraseanddependencystructures[
41
,
39
],sentimentcompositionality[
38
],
andcoreference[
12
,
16
].[
17
]combinesgraphneuralnetworkswithVAEstodiscoverlatentgraph
structuresinparticleinteractionsystems.TherehasalsobeeninterestlatelyonNeuralArchitecture
8
Search[
51
,
2
,
26
,
20
],whereaclassofneuralnetworksissearchedovertotheoptimalonefora
particulartask.
Recently,theself-attentionmodule[
42
]hasbeenproposedwhich,inprinciple,iscapableoflearning
arbitrarystructuresinthedatasinceitmodelspairwiseinteractionsbetweentheinputs.Originally
usedforMachineTranslation,ithasalsobeensuccessfullyappliedtosentenceunderstanding[
36
],
imagegeneration[
28
],summarization[
20
],andrelationextraction[
43
].Nonlocalneuralnetworks
[
44
]forimagesalsoshareasimilaridea.Ourworkisrelatedtothesemethods,butourgoalisto
learnauniversalstructureusinganunsupervisedobjectiveandthentransferitforusewithvarious
supervisedtasks.Technically,ourapproachalsodiffersfrompreviousworkasdiscussedinSection
2.1.1,includingseparatinggraphsandfeatures.LISA[
40
],exploredarelatedideaofusingexisting
linguisticstructures,suchasdependencytrees,toguidetheattentionlearningprocessofaself
attentionnetwork.
Anotherlineofworkhasexplored
latenttreelearning
forjointlyparsingsentencesbasedona
downstreamsemanticobjective[
48
,
23
,
6
].Inspiredbylinguistictheoriesofconstituentphrase
structure[
7
],theseworksrestricttheirlatentparsestobebinarytrees.Whilethesemodelsshow
improvedperformanceonthedownstreamsemantictasks,Williamsetal[
45
]showedthatthe
intermediateparsesbearlittleresemblancetoanyknownsyntacticorsemantictheoriesfromthe
literature.Thissuggeststhattheoptimalstructureforcomputationallinguisticsmightbedifferent
fromthosethathavebeenproposedinformalsyntactictheories.Inthispaperweexploretheuseof
unsupervisedlearningobjectivesfordiscoveringsuchstructures.
5Conclusions
Wepresentanoveltransferlearningschemebasedonlatentrelationalgraphlearning,whichis
orthogonaltobutcanbecombinedwiththetraditionalfeaturetransferlearningframework.Through
avarietyofexperimentsinlanguageandvision,thisframeworkisdemonstratedtobecapableof
improvingperformanceandlearninggenericgraphsapplicabletovarioustypesoffeatures.Inthe
future,wehopetoextendtheframeworktomorediversesetupssuchasknowledgebasedinference,
videomodeling,andhierarchicalreinforcementlearningwhererichgraph-likestructuresabound.
Acknowledgement
ThisworkwassupportedinpartbytheOfofNavalResearch,DARPAawardD17AP00001,
Apple,theGooglefocusedaward,andtheNvidiaNVAILaward.Theauthorswouldalsoliketo
thankSamBowmanforusefuldiscussions.
References
[1]
DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointly
learningtoalignandtranslate.
arXivpreprintarXiv:1409.0473
,2014.
[2]
BowenBaker,OtkristGupta,NikhilNaik,andRameshRaskar.Designingneuralnetwork
architecturesusingreinforcementlearning.
arXivpreprintarXiv:1611.02167
,2016.
[3]
PeterWBattaglia,JessicaBHamrick,VictorBapst,AlvaroSanchez-Gonzalez,Vinicius
Zambaldi,MateuszMalinowski,AndreaTacchetti,DavidRaposo,AdamSantoro,Ryan
Faulkner,etal.Relationalinductivebiases,deeplearning,andgraphnetworks.
arXivpreprint
arXiv:1806.01261
,2018.
[4]
SamuelRBowman,GaborAngeli,ChristopherPotts,andChristopherDManning.Alarge
annotatedcorpusforlearningnaturallanguageinference.
arXivpreprintarXiv:1508.05326
,
2015.
[5]
QianChen,XiaodanZhu,Zhen-HuaLing,SiWei,HuiJiang,andDianaInkpen.Enhancedlstm
fornaturallanguageinference.In
Proceedingsofthe55thAnnualMeetingoftheAssociation
forComputationalLinguistics(Volume1:LongPapers)
,volume1,pages1657Œ1668,2017.
[6]
JihunChoi,KangMinYoo,andSang-gooLee.Unsupervisedlearningoftastree
structureswithtree-lstms.
AAAI
,2018.
9
[7]
NoamChomsky.
AspectsoftheTheoryofSyntax
,volume11.MITpress,2014.
[8]
JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio.Empiricalevaluation
ofgatedrecurrentneuralnetworksonsequencemodeling.
arXivpreprintarXiv:1412.3555
,
2014.
[9]
ChristopherClarkandMattGardner.Simpleandeffectivemulti-paragraphreadingcomprehen-
sion.
arXivpreprintarXiv:1710.10723
,2017.
[10]
AlexisConneau,DouweKiela,HolgerSchwenk,LoicBarrault,andAntoineBordes.Supervised
learningofuniversalsentencerepresentationsfromnaturallanguageinferencedata.
Proceedings
ofthe2017conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP)
,2017.
[11]
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scale
hierarchicalimagedatabase.In
ComputerVisionandPatternRecognition,2009.CVPR2009.
IEEEConferenceon
,pages248Œ255.IEEE,2009.
[12]
BhuwanDhingra,QiaoJin,ZhilinYang,WilliamWCohen,andRuslanSalakhutdinov.Neural
modelsforreasoningovermultiplementionsusingcoreference.
NAACL
,2018.
[13]
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningforimage
recognition.In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition
,
pages770Œ778,2016.
[14]
SeppHochreiterandJürgenSchmidhuber.Longshort-termmemory.
Neuralcomputation
,
9(8):1735Œ1780,1997.
[15]
JieHu,LiShen,andGangSun.Squeeze-and-excitationnetworks.
arXivpreprint
arXiv:1709.01507
,2017.
[16]
YangfengJi,ChenhaoTan,SebastianMartschat,YejinChoi,andNoahASmith.Dynamic
entityrepresentationsinneurallanguagemodels.
arXivpreprintarXiv:1708.00781
,2017.
[17]
ThomasKipf,EthanFetaya,Kuan-ChiehWang,MaxWelling,andRichardZemel.Neural
relationalinferenceforinteractingsystems.
arXivpreprintarXiv:1802.04687
,2018.
[18]
RyanKiros,YukunZhu,RuslanRSalakhutdinov,RichardZemel,RaquelUrtasun,Antonio
Torralba,andSanjaFidler.Skip-thoughtvectors.In
Advancesinneuralinformationprocessing
systems
,pages3294Œ3302,2015.
[19]
YannLeCun,YoshuaBengio,etal.Convolutionalnetworksforimages,speech,andtimeseries.
Thehandbookofbraintheoryandneuralnetworks
,3361(10):1995,1995.
[20]
PeterJLiu,MohammadSaleh,EtiennePot,BenGoodrich,RyanSepassi,LukaszKaiser,
andNoamShazeer.Generatingwikipediabysummarizinglongsequences.
arXivpreprint
arXiv:1801.10198
,2018.
[21]
XiaodongLiu,KevinDuh,andJianfengGao.Stochasticanswernetworksfornaturallanguage
inference.
arXivpreprintarXiv:1804.07888
,2018.
[22]
AndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,andChristopher
Potts.Learningwordvectorsforsentimentanalysis.In
Proceedingsofthe49thAnnualMeeting
oftheAssociationforComputationalLinguistics:HumanLanguageTechnologies
,pages
142Œ150,Portland,Oregon,USA,June2011.AssociationforComputationalLinguistics.
[23]
JeanMaillard,StephenClark,andDaniYogatama.Jointlylearningsentenceembeddingsand
syntaxwithunsupervisedtree-lstms.
arXivpreprintarXiv:1705.09189
,2017.
[24]
TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean.Distributedrepre-
sentationsofwordsandphrasesandtheircompositionality.In
Advancesinneuralinformation
processingsystems
,pages3111Œ3119,2013.
[25]
TakeruMiyato,AndrewMDai,andIanGoodfellow.Adversarialtrainingmethodsforsemi-
supervisedtext
arXivpreprintarXiv:1605.07725
,2016.
[26]
RenatoNegrinhoandGeoffGordon.Deeparchitect:Automaticallydesigningandtrainingdeep
architectures.
arXivpreprintarXiv:1704.08792
,2017.
[27]
AaronvandenOord,NalKalchbrenner,andKorayKavukcuoglu.Pixelrecurrentneural
networks.
arXivpreprintarXiv:1601.06759
,2016.
[28]
NikiParmar,AshishVaswani,JakobUszkoreit,Kaiser,NoamShazeer,andAlexander
Ku.Imagetransformer.
arXivpreprintarXiv:1802.05751
,2018.
10
[29]
JeffreyPennington,RichardSocher,andChristopherManning.Glove:Globalvectorsfor
wordrepresentation.In
Proceedingsofthe2014conferenceonempiricalmethodsinnatural
languageprocessing(EMNLP)
,pages1532Œ1543,2014.
[30]
MatthewEPeters,MarkNeumann,MohitIyyer,MattGardner,ChristopherClark,Ken-
tonLee,andLukeZettlemoyer.Deepcontextualizedwordrepresentations.
arXivpreprint
arXiv:1802.05365
,2018.
[31]
PranavRajpurkar,JianZhang,KonstantinLopyrev,andPercyLiang.Squad:100,000+questions
formachinecomprehensionoftext.
arXivpreprintarXiv:1606.05250
,2016.
[32]
AliSharifRazavian,HosseinAzizpour,JosephineSullivan,andStefanCarlsson.Cnnfea-
turesoff-the-shelf:anastoundingbaselineforrecognition.In
ComputerVisionandPattern
RecognitionWorkshops(CVPRW),2014IEEEConferenceon
,pages512Œ519.IEEE,2014.
[33]
OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,Zhiheng
Huang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,AlexanderC.Berg,andLiFei-Fei.
ImageNetLargeScaleVisualRecognitionChallenge.
InternationalJournalofComputerVision
(IJCV)
,115(3):211Œ252,2015.
[34]
TimSalimans,AndrejKarpathy,XiChen,andDiederikPKingma.Pixelcnn++:Improvingthe
pixelcnnwithdiscretizedlogisticmixturelikelihoodandother
arXivpreprint
arXiv:1701.05517
,2017.
[35]
FrancoScarselli,MarcoGori,AhChungTsoi,MarkusHagenbuchner,andGabrieleMonfardini.
Thegraphneuralnetworkmodel.
IEEETransactionsonNeuralNetworks
,20(1):61Œ80,2009.
[36]
TaoShen,TianyiZhou,GuodongLong,JingJiang,ShiruiPan,andChengqiZhang.Disan:
Directionalself-attentionnetworkforrnn/cnn-freelanguageunderstanding.
arXivpreprint
arXiv:1709.04696
,2017.
[37]
KarenSimonyanandAndrewZisserman.Verydeepconvolutionalnetworksforlarge-scale
imagerecognition.
arXivpreprintarXiv:1409.1556
,2014.
[38]
RichardSocher,CliffCLin,ChrisManning,andAndrewYNg.Parsingnaturalscenesand
naturallanguagewithrecursiveneuralnetworks.In
Proceedingsofthe28thinternational
conferenceonmachinelearning(ICML-11)
,pages129Œ136,2011.
[39]
RichardSocher,AlexPerelygin,JeanWu,JasonChuang,ChristopherDManning,AndrewNg,
andChristopherPotts.Recursivedeepmodelsforsemanticcompositionalityoverasentiment
treebank.In
Proceedingsofthe2013conferenceonempiricalmethodsinnaturallanguage
processing
,pages1631Œ1642,2013.
[40]
EmmaStrubell,PatrickVerga,DanielAndor,DavidWeiss,andAndrewMcCal-
lum.Linguistically-informedself-attentionforsemanticrolelabeling.
arXivpreprint
arXiv:1804.08199
,2018.
[41]
KaiShengTai,RichardSocher,andChristopherDManning.Improvedsemanticrepresentations
fromtree-structuredlongshort-termmemorynetworks.
arXivpreprintarXiv:1503.00075
,2015.
[42]
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
Kaiser,andIlliaPolosukhin.Attentionisallyouneed.In
AdvancesinNeuralInforma-
tionProcessingSystems
,pages6000Œ6010,2017.
[43]
PatrickVerga,EmmaStrubell,OferShai,andAndrewMcCallum.Attendingtoallmention
pairsforfullabstractbiologicalrelationextraction.
arXivpreprintarXiv:1710.08312
,2017.
[44]
XiaolongWang,RossGirshick,AbhinavGupta,andKaimingHe.Non-localneuralnetworks.
arXivpreprintarXiv:1711.07971
,2017.
[45]
AdinaWilliams,AndrewDrozdov,andSamuelRBowman.Dolatenttreelearningmodels
identifymeaningfulstructureinsentences?
TransactionsoftheAssociationforComputational
Linguistics
,6:253Œ267,2018.
[46]
AdinaWilliams,NikitaNangia,andSamuelRBowman.Abroad-coveragechallengecorpus
forsentenceunderstandingthroughinference.
arXivpreprintarXiv:1704.05426
,2017.
[47]
ZhilinYang,ZihangDai,RuslanSalakhutdinov,andWilliamWCohen.Breakingthesoftmax
bottleneck:ahigh-rankrnnlanguagemodel.
arXivpreprintarXiv:1711.03953
,2017.
[48]
DaniYogatama,PhilBlunsom,ChrisDyer,EdwardGrefenstette,andWangLing.Learningto
composewordsintosentenceswithreinforcementlearning.
ICLR
,2016.
11
[49]
AdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,
andQuocVLe.Qanet:Combininglocalconvolutionwithglobalself-attentionforreading
comprehension.
arXivpreprintarXiv:1804.09541
,2018.
[50]
HanZhang,IanGoodfellow,DimitrisMetaxas,andAugustusOdena.Self-attentiongenerative
adversarialnetworks.
arXivpreprintarXiv:1805.08318
,2018.
[51]
BarretZophandQuocVLe.Neuralarchitecturesearchwithreinforcementlearning.
arXiv
preprintarXiv:1611.01578
,2016.
12
"
34,Deep Neural Nets with Interpolating Function as Output Activation,http://arxiv.org/pdf/1802.00168v3.pdf,https://github.com/BaoWangMath/DNN-DataDependentActivation,"DeepNeuralNetswithInterpolatingFunctionasOutput
Activation
BaoWang
DepartmentofMathematics
UniversityofCalifornia,LosAngeles
wangbaonj@gmail.com
XiyangLuo
DepartmentofMathematics
UniversityofCalifornia,LosAngeles
xylmath@gmail.com
ZhenLi
DepartmentofMathematics
HongKongUniversityofScienceandTechnology
zli12@mails.tsinghua.edu.cn
WeiZhu
DepartmentofMathematics
DukeUniversity
zhu@math.duke.edu
ZuoqiangShi
YauMathematicalSciencecenter
TsinghuaUniversity
zqshi@math.tsinghua.edu.cn
StanleyOsher
DepartmentofMathematics
UniversityofCalifornia,LosAngeles
sjo@math.ucla.edu
June19,2018
Abstract
Wereplacetheoutputlayerofdeepneuralnets,typicallythesoftmaxfunction,bya
novelinterpolatingfunction.Andweproposeend-to-endtrainingandtestingalgorithms
forthisnewarchitecture.Comparedtoclassicalneuralnetswithsoftmaxfunctionas
outputactivation,thesurrogatewithinterpolatingfunctionasoutputactivationcombines
advantagesofbothdeepandmanifoldlearning.Thenewframeworkdemonstratesthe
followingmajoradvantages:First,itisbetterapplicabletothecasewithinsu˚cienttraining
data.Second,itsigni˝cantlyimprovesthegeneralizationaccuracyonawidevarietyof
networks.ThealgorithmisimplementedinPyTorch,andcodewillbemadepublicly
available.
1Introduction
Generalizabilityiscrucialtodeeplearning,andmanye˙ortshavebeenmadetoimprovethe
trainingandgeneralizationaccuracyofdeepneuralnets(DNNs)[
3
,
15
].Advancesinnetwork
architecturessuchasVGGnetworks[
29
],deepresidualnetworks(ResNets)[
12
,
14
]andmore
recentlyDenseNets[
17
]andmanyothers[
6
],togetherwithpowerfulhardwaremakethetraining
ofverydeepnetworkswithgoodgeneralizationcapabilitiespossible.E˙ectiveregularization
techniquessuchasdropoutandmaxout[
16
,
31
,
10
],aswellasdataaugmentationmethods
[20,29,33]havealsoexplicitlyimprovedgeneralizationforDNNs.
Akeycomponentofneuralnetsistheactivationfunction.Improvementsindesigningof
activationfunctionssuchastherecti˝edlinearunit(ReLU)[
8
],haveledtohugeimprovements
inperformanceincomputervisiontasks[
24
,
20
].Morerecently,activationfunctionsadaptively
trainedtothedatasuchastheadaptivepiecewiselinearunit(APLU)[
1
]andparametricrecti˝ed
linearunit(PReLU)[
11
]haveleadtofurtherimprovementsinperformanceofDNNs.For
outputactivation,supportvectormachine(SVM)hasalsobeensuccessfulyappliedinplaceof
softmax[
30
].ThoughtrainingDNNswithsoftmaxorSVMasoutputactivationise˙ectivein
1
arXiv:1802.00168v3  [cs.LG]  17 Jun 2018manytasks,itispossiblethatalternativeactivationsthatconsidermanifoldstructureofdata
byinterpolatingtheoutputbasedonbothtrainingandtestingdatacanboostperformanceof
thenetwork.Inparticular,ResNetscanbereformulatedassolvingcontrolproblemsofaclass
oftransportequationsinthecontinuumlimit[
22
,
5
].Transporttheorysuggeststhatbyusing
aninterpolatingfunctionthatinterpolatesterminalvaluesfrominitialvaluescandramatically
simplifythecontrolproblemcomparedtoanad-hocchoice.Thisfurthersuggeststhata˝xed
anddata-agnosticactivationfortheoutputlayermaybesuboptimal.
Tothisend,basedontheideasfrommanifoldlearning,weproposeanoveloutputlayer
namedweightednonlocalLaplacian(WNLL)layerforDNNs.TheresultedDNNsachievebetter
generalizationandaremorerobustforproblemswithasmallnumberoftrainingexamples.On
CIFAR10/CIFAR100,weachieveonaveragea30%/20%reductionintermsoftesterroron
awidevarietyofnetworks.TheseincludeVGGs,ResNets,andpre-activatedResNets.The
performanceboostisevenmorepronouncedwhenthemodelistrainedonarandomsubset
ofCIFARwithalownumberoftrainingexamples.Wealsopresentane˚cientalgorithmto
traintheWNLLlayerviaanauxiliarynetwork.TheoreticalmotivationfortheWNLLlayeris
alsogivenfromtheviewpointofbothgametheoryandterminalvalueproblemsfortransport
equations.
Thispaperisstructuredasfollows:InSection2,weintroducethemotivationandpractice
ofusingtheWNLLinterpolatingfunctioninDNNs.InSection2.2,weexplainindetailthe
algorithmsfortrainingandtestingDNNswithWNLLasoutputlayer.Section3providesinsight
ofusinganinterpolatingfunctionasoutputlayerfromtheangleofterminalvalueproblemsof
transportequationsandgametheory.Section4demonstratesthee˙ectivenessofourmethodon
avarietyofnumericalexamples.
2NetworkArchitecture
Incoarsegrainedrepresentation,trainingandtestingDNNswithsoftmaxlayerasoutputare
illustratedinFig.1(a)and(b),respectively.In
k
thiterationoftraining,givenamini-batch
trainingdata
(
X
;
Y
)
,weperform:
Forwardpropagation:
Transform
X
intodeepfeaturesbyDNNblock(ensembleofconvlayers,
nonlinearitiesandothers),andthenactivatedbysoftmaxfunctiontoobtainthepredictedlabels
~
Y
:
~
Y
=Softmax(DNN(
X
;

k

1
)
;
W
k

1
)
:
Thencomputeloss(e.g.,crossentropy)between
Y
and
~
Y
:
L
=Loss(
Y
;
~
Y
)
.
Backpropagation:
Updateweights(

k

1
,
W
k

1
)bygradientdescent(learningrate

):
W
k
=
W
k

1


@
L
@
~
Y

@
~
Y
@
W
;

k
=
k

1


@
L
@
~
Y

@
~
Y
@
~
X

@
~
X
@

:
(a)(b)
Figure1:Training(a)andtesting(b)proceduresofDNNswithsoftmaxasoutputactivation
layer.
Oncethemodelisoptimized,fortestingdata
X
,thepredictedlabelsare:
~
X
=Softmax(DNN(
X
;

;
W
)
;
fornotationalsimplicity,westilldenotethetestsetandoptimizedweightsas
X
,

,and
W
,
respectively.Inessencethesoftmaxlayeractsasalinearmodelonthespaceofdeepfeatures
~
X
,whichdoesnottakeintoconsiderationtheunderlyingmanifoldstructureof
~
X
.TheWNLL
interpolatingfunction,whichwillbeintroducedinthefollowingsubsection,isanapproachto
alleviatethisde˝ciency.
2
2.1ManifoldInterpolation-AnHarmonicExtensionApproach
Let
X
=
f
x
1
;
x
2
;

;
x
n
g
beasetofpointsinahighdimensionalmanifold
Mˆ
R
d
and
X
te
=
f
x
te
1
;
x
te
2
;

;
x
te
m
g
beasubsetof
X
.Supposewehavea(possiblyvectorvalued)label
function
g
(
x
)
de˝nedon
X
te
,andwewanttointerpolateafunction
u
thatisde˝nedonthe
entiremanifoldandcanbeusedtolabeltheentiredataset
X
.Interpolationbyusingbasis
functioninhighdimensionalspacesu˙ersfromthecurseofdimensionality.Instead,anharmonic
extensionisanaturalandelegantapproachto˝ndsuchaninterpolatingfunction,whichis
de˝nedbyminimizingthefollowingDirichletenergyfunctional:
E
(
u
)=
1
2
X
x
;
y
2
X
w
(
x
;
y
)(
u
(
x
)

u
(
y
))
2
;
(1)
withtheboundarycondition:
u
(
x
)=
g
(
x
)
;
x
2
X
te
;
where
w
(
x
;
y
)
isaweightfunction,typicallychosentobeGaussian:
w
(
x
;
y
)=
exp
(

jj
x

y
jj
2
˙
2
)
with
˙
ascalingparameter.TheEuler-LagrangeequationforEq.(1)is:
(
P
y
2
X
(
w
(
x
;
y
)+
w
(
y
;
x
))(
u
(
x
)

u
(
y
))=0
x
2
X
=
X
te
u
(
x
)=
g
(
x
)
x
2
X
te
:
(2)
BysolvingthelinearsystemEq.(2),wegettheinterpolatedlabels
u
(
x
)
forunlabeleddata
x
2
X
=
X
te
.Thisinterpolationbecomesinvalidwhenlabeleddataistiny,i.e.,
j
X
te
j˝j
X
=
X
te
j
.
Therearetwosolutionstoresolvethisissue:oneistoreplacethe
2
-LaplacianinEq.(1)bya
p
-Laplacian[
4
];theotheristoincreasetheweightsofthelabeleddataintheEuler-Lagrange
equation[
28
],whichgivesthefollowingweightednonlocalLaplacian(WNLL)interpolating
function:
8
>
>
<
>
>
:
P
y
2
X
(
w
(
x
;
y
)+
w
(
y
;
x
))(
u
(
x
)

u
(
y
))+

j
X
j
j
X
te
j

1

P
y
2
X
te
w
(
y
;
x
)(
u
(
x
)

u
(
y
))=0
x
2
X
=
X
te
u
(
x
)=
g
(
x
)
x
2
X
te
:
(3)
Fornotationalsimplicity,wenamethesolution
u
(
x
)
toEq.3as
WNLL
(
X
;
X
te
;
Y
te
)
.For
classi˝cationtasks,
g
(
x
)
istheone-hotlabelsfortheexample
x
.ToensureaccuracyofWNLL,
thelabeleddatashouldcoverallclassesofdatain
X
.WegiveanecessaryconditioninTheorem
1.
Theorem1.
Supposewehaveadatapoolformedby
N
classesofdatauniformly,withthe
numberofinstancesofeachclassbesu˚cientlylarge.Ifwewantallclassesofdatatobe
sampledatleastonce,onaverageatleast
N

1+
1
2
+
1
3
+

+
1
N

dataisneedtobesampled
fromthedatapool.Inthiscase,thenumberofdatasampled,inexpectationforeachclass,is
1+
1
2
+
1
3
+

+
1
N
.
2.2WNLLActivatedDNNsandAlgorithms
InbothtrainingandtestingoftheWNLLactivatedDNNs,weneedtoreserveasmallportion
ofdata/labelpairsdenotedas
(
X
te
;
Y
te
)
,tointerpolatethelabel
Y
fornewdata.Wename
(
X
te
;
Y
te
)
asthepreservedtemplate.DirectlyreplacingsoftmaxbyWNLL(Fig.2(a))has
di˚cultiesinbackpropagation,namely,thetruegradient
@
L
@

isdi˚culttocomputesinceWNLL
de˝nesaverycompleximplicitfunction.Instead,totrainWNLLactivatedDNNs,weproposea
proxyviaanauxiliaryneuralnets(Fig.2(b)).OntopoftheoriginalDNNs,weaddabu˙erblock
(afullyconnectedlayerfollowedbyaReLU),andfollowedbytwoparallellayers,WNLLandthe
linear(fullyconnected)layers.TheauxiliaryDNNscanbetrainedbyalternatingbetweenthe
followingtwosteps(trainingDNNswithlinearandWNLLactivations,respectively):
TrainDNNswithlinearactivation:
Run
N
1
stepsofthefollowingforwardandback
propagation,wherein
k
thiteration,wehave:
Forwardpropagation:
Thetrainingdata
X
istransformed,respectively,byDNN,Bu˙erand
Linearblockstothepredictedlabels
~
Y
:
~
Y
=
X
;

k

1
)
;
W
k

1
B
)
;
W
k

1
L
)
:
3
Thencomputelossbetweenthegroundtruthlabels
Y
andpredictedones
~
Y
,denotedas
L
Linear
(e.g.,crossentropyloss,andthesameasfollowing
L
WNLL
).
Backpropagation:
Updateweights(

k

1
,
W
k

1
B
,
W
k

1
L
)bygradientdescent:
W
k
L
=
W
k

1
L


@
L
Linear
@
~
Y

@
~
Y
@
W
L
;
W
k
B
=
W
k

1
B


@
L
Linear
@
~
Y

@
~
Y
@
^
X

@
^
X
@
W
B
;

k
=
k

1


@
L
Linear
@
~
Y

@
~
Y
@
^
X

@
^
X
@
~
X

@
~
X
@

:
TrainDNNswithWNLLactivation:
Run
N
2
stepsofthefollowingforwardandback
propagation,wherein
k
thiteration,wehave:
Forwardpropagation:
Thetrainingdata
X
,template
X
te
and
Y
te
aretransformed,respec-
tively,byDNN,Bu˙er,andWNLLblockstogetpredictedlabels
^
Y
:
^
Y
=
X
;

k

1
)
;
W
k

1
B
)
;
^
X
te
;
Y
te
)
:
Thencomputeloss,
L
WNLL
,betweenthegroundtruthlabels
Y
andpredictedones
^
Y
.
Backpropagation:
Updateweights
W
k

1
B
only,
W
k

1
L
and

k

1
willbetunedinthenext
iterationintrainingDNNswithlinearactivation,bygradientdescent.
W
k
B
=
W
k

1
B


@
L
WNLL
@
^
Y

@
^
Y
@
^
X

@
^
X
@
W
B
ˇ
W
k

1
B


@
L
Linear
@
~
Y

@
~
Y
@
^
X

@
^
X
@
W
B
:
(4)
Hereweusethecomputationalgraphoftheleftbranch(linearlayer)toretrievalthe
approximatedgradientsforWNLL.Foragivenlossvalueof
L
WNLL
,weadopttheapproximation
@
L
WNLL
@
^
Y

@
^
Y
@
^
X
ˇ
@
L
Linear
@
~
Y

@
~
Y
@
^
X
wheretherighthandsideisalsoevaluatedatthisvalue.The
mainheuristicbehindthisapproximationisthefollowing:WNLLde˝nesaharmonicfunction
implicitly,andalinearfunctionisthesimplestnontrivialexplicitharmonicfunction.Empirically,
weobservethissimpleapproximationworkswellintrainingthenetwork.Thereasonwhywe
freezethenetworkintheDNNblockismainlyduetostabilityconcerns.
(a)(b)(c)
Figure2:TrainingandtestingprocedureofthedeepneuralnetswithWNLLasthelastactivation
layer.(a):DirectreplacementofthesoftmaxbyWNLL,(b):Analternatingtrainingprocedure.
(c):Testing.
Theabovealternatingschemeisanalgorithmofagreedyfashion.Duringtraining,WNLL
activationplaystworoles:ononehand,thealternatingbetweenlinearandWNLLactivations
bene˝tseachotherwhichenablestheneuralnetstolearnfeaturesthatisappropriateforboth
linearclassi˝cationandWNLLbasedmanifoldinterpolation.Ontheotherhand,inthecase
wherewelacksu˚cienttrainingdata,thetrainingofDNNsusuallygetsstuckatsomebad
localminimawhichcannotgeneralizewellonnewdata.WeuseWNLLinterpolationwhich
providesaperturbationtothetrainedsub-optimalweightsandcanhelptoarriveatalocal
minimawithbettergeneralizability.Attesttime,weremovethelinearclassi˝erfromtheneural
netsandusetheDNNblocktogetherwithWNLLtopredictnewdata(Fig.2(c)).Thereason
forusingWNLLinsteadofalinearlayerisbecauseWNLLissuperiortothelinearclassi˝erand
thissuperiorityispreservedwhenappliedtodeepfeatures(whichwillbeshowninSection.4).
Moreover,WNLLutilizesboththelearnedDNNsandthepreservedtemplateattesttimewhich
seemstobemorestabletoperturbationsontheinputdata.
4
WesummarizethetrainingandtestingproceduresfortheWNLLactivatedDNNsinAlgo-
rithms1and2,respectively.Ineachroundofthealternatingprocedurei.e.,eachouterloopin
Algorithm.1,theentiretrainingset
(
X
;
Y
)
is˝rstusedtotraintheDNNswithlinearactivation.
Werandomlyseparateatemplate,e.g.,halfoftheentiredata,fromthetrainingsetwhichwill
beusedtoperformWNLLinterpolationintrainingWNLLactivatedDNNs.Inpractice,for
bothtrainingandtesting,weuseminibatchesforboththetemplateandtheinterpolatedpoints
whentheentiredatasetistoolarge.The˝nalpredictedlabelsareobtainedbyamajorityvoted
acrossinterpolationresultsfromallthetemplateminibatches.
Remark1.
InAlgorithm.1,theWNLLinterpolationisalsoperformedinmini-batchmanner
(asshownintheinneriteration).Basedonourexperiments,thisdoesnotreducetheinterpolation
accuracysigni˝cantly.
Algorithm1
DNNswithWNLLasOutputActivation:TrainingProcedure.
Input:
Trainingset:(data,label)pairs
(
X
;
Y
)
.
Output:
AnoptimizedDNNswithWNLLasoutputactivation,denotedas
DNN
WNLL
.
for
iter=1
;
...
;N
(where
N
isthenumberofalternatingsteps.)
do
//Traintheleftbranch:DNNswithlinearactivation.
TrainDNN
+
Linearblocks,anddenotethelearnedmodelas
DNN
Linear
.
//Traintherightbranch:DNNswithWNLLactivation.
Split
(
X
;
Y
)
intotrainingdataandtemplate,i.e.,
(
X
;
Y
)
:
=(
X
tr
;
Y
tr
)
S
(
X
te
;
Y
te
)
.
Partitionthetrainingdatainto
M
mini-batches,i.e.,
(
X
tr
;
Y
tr
)=
S
M
i
=1
(
X
tr
i
;
Y
tr
i
)
.
for
i
=1
;
2
;

;M
do
Transform
X
tr
i
S
X
te
by
DNN
Linear
,i.e.,
~
X
tr
S
~
X
te
=DNN
Linear
(
X
tr
i
S
X
te
)
.
ApplyWNLL(Eq.(3))on
f
~
X
tr
S
~
X
te
;
Y
te
g
tointerpolatelabel
~
Y
tr
.
Backpropagatetheerrorbetween
Y
tr
and
~
Y
tr
viaEq.(4)toupdate
W
B
only.
Algorithm2
DNNswithWNLLasOutputActivation:TestingProcedure.
Input:
Testingdata
X
,template
(
X
te
;
Y
te
)
.Optimizedmodel
DNN
WNLL
.
Output:
Predictedlabel
~
Y
for
X
.
ApplytheDNNblockof
DNN
WNLL
to
X
S
X
te
togettherepresentation
~
X
S
~
X
te
.
ApplyWNLL(Eq.(3))on
f
~
X
S
~
X
te
;
Y
te
g
tointerpolatelabel
~
Y
.
3TheoreticalExplanation
IntrainingWNLLactivatedDNNs,thetwooutputactivationfunctionsintheauxiliarynetworks
are,inasense,eachcompetingtominimizeitsownobjectivewhere,inequilibrium,theneural
netscanlearnbetterfeaturesforbothlinearandinterpolation-basedactivations.Thisin˛avor
issimilartogenerativeadversarialnets(GAN)[
9
].Anotherinterpretationofourmodelisthe
following:Asnotedin[
22
],inthecontinuumlimit,ResNetcanbemodeledasthefollowing
controlproblemforatransportequation:
(
@u
(
x
;t
)
@t
+
v
(
x
;t
)
r
u
(
x
;t
)=0
x
2
X
;t

0
u
(
x
;
1)=
f
(
x
)
x
2
X
:
(5)
Here
u
(

;
0)
istheinputofthecontinuumversionofResNet,whichmapsthetrainingdatatothe
correspondinglabel.
f
(

)
istheterminalvaluewhichanalogoustotheoutputactivationfunction
inResNetwhichmapsdeepfeaturestothepredictedlabel.TrainingResNetisequivalentto
tuning
v
(

;t
)
,i.e.,continuousversionoftheweights,s.t.thepredictedlabel
f
(

)
matchesthatof
thetrainingdata.If
f
(

)
isaharmonicextensionof
u
(

;
0)
,thecorrespondingweights
v
(
x
;t
)
wouldbeclosetozero.Thisresultsinasimplermodelandmaygeneralizebetterfromamodel
selectionpointofview.
5
4NumericalResults
Tovalidatetheclassi˝cationaccuracy,e˚ciencyandrobustnessoftheproposedframework,we
testthenewarchitectureandalgorithmonCIFAR10,CIFAR100[
19
],MNIST[
21
]andSVHN
datasets[
25
].Inallexperiments,weapplystandarddataaugmentationthatiswidelyused
fortheCIFARdatasets[
12
,
17
,
32
].ForMNISTandSVHN,weusetherawdatawithoutany
augmentation.WeimplementouralgorithmonthePyTorchplatform[
27
].Allcomputationsare
carriedoutonamachinewithasingleNvidiaTitanXpgraphicscard.
BeforedivingintotheperformanceofDNNswithdi˙erentoutputactivationfunctions,we
˝rstcomparetheperformanceofWNLLwithsoftmaxontherawinputimagesforvarious
datasets.Thetrainingsetsareusedtotrainthesoftmaxmodelsandinterpolatelabelsfortesting
setinsoftmaxandWNLL,respectively.Table1liststheclassi˝cationaccuraciesofWNLLand
softmaxonthreedatasets.ForWNLLinterpolation,inordertospeedupthecomputation,we
onlyuse15nearestneighborstoensuresparsityoftheweightmatrix,andthe8thneighbor's
distanceisusedtonormalizetheweightmatrix.Thenearestneighborsaresearchedviathe
approximatenearestneighbor(ANN)algorithm[
23
].WNLLoutperformssoftmaxsigni˝cantly
inallthreetasks.TheseresultsshowthepotentialofusingWNLLinsteadofsoftmaxasthe
outputactivationfunctioninDNNs.
Table1:AccuraciesofsoftmaxandWNLLinclassifyingsomeclassicaldatasets.
DatasetCIFAR10MNISTSVHN
softmax39.91%92.65%24.66%
WNLL40.73%97.74%56.17%
Forthedeeplearningexperimentsbelow:Wetaketwopassesalternatingsteps,i.e.,
N
=2
in
Algorithm.1.Forthelinearactivationstage(Stage1),wetrainthenetworkfor
n
=400
epochs.
FortheWNLLstage,wetrainfor
n
=5
epochs.Inthe˝rstpass,theinitiallearningrateis0.05
andhalvedafterevery50epochsintraininglinearactivatedDNNs,and0.0005whentrainingthe
WNLLactivation.ThesameNesterovmomentumandweightdecayasusedin[
13
,
18
]areused
forCIFARandSVHNexperiments,respectively.Inthesecondpass,thelearningrateissetto
beone˝fthofthecorrespondingepochsinthe˝rstpass.Thebatchsizesare128and2000when
trainingsoftmax/linearandWNLLactivatedDNNs,respectively.Forfaircomparison,wetrain
thevanillaDNNswithsoftmaxoutputactivationfor810epochswiththesameoptimizersused
inWNLLactivatedones.All˝naltesterrorsreportedfortheWNLLmethodaredoneusing
WNLLactivationsforpredictiononthetestset.Intherestofthissection,weshowthatthe
proposedframeworkresolvestheissueoflackingbigtrainingdataandbooststhegeneralization
accuraciesofDNNsvianumericalresultsonCIFAR10/CIFAR100.Thenumericalresultson
SVHNareprovidedintheappendix.
4.1ResolvingtheChallengeofInsu˚cientTrainingData
Whenwedonothavesu˚cienttrainingdata,thegeneralizationaccuracytypicallydegradesas
thenetworkgoesdeeper,asillustratedinFig.3.TheWNLLactivatedDNNs,withitssuperior
regularizationoftheparametersandperturbationonbadlocalminima,areabletoovercome
thisdegradation.Theleftandrightpanelsplotthecaseswhenthe˝rst1000and10000datain
thetrainingsetofCIFAR10areusedtotrainthevanillaandWNLLDNNs.AsshowninFig.3,
byusingWNLLactivation,thegeneralizationerrorratesdecayconsistentlyasthenetworkgoes
deeper,incontrasttothedegradationforvanillaDNNs.Thegeneralizationaccuracybetween
thevanillaandWNLLDNNscandi˙erupto10percentwithinourtestingregime.
Figure.4plotstheevolutionofgeneralizationaccuracyduringtraining.Wecomputethetest
accuracyperepoch.Panels(a)and(b)plotthetestaccuraciesforResNet50withsoftmaxand
WNLLactivations(1-400and406-805epochscorrespondstolinearactivation),respectively,
withonlythe˝rst1000examplesastrainingdatafromCIFAR10.Charts(c)and(d)are
thecorrespondingplotswith10000traininginstances,usingapre-activatedResNet50.After
around300epochs,theaccuraciesofthevanillaDNNsplateauandcannotimproveanymore.
Incomparison,thetestaccuracyforWNLLjumpsatthebeginningofStage2in˝rstpass;
duringStage1ofthesecondpass,eventhoughinitiallythereisanaccuracyreduction,the
accuracycontinuestoclimbandeventuallysurpassesthatoftheWNLLactivationinStage2
6
(a)(b)
Figure3:ResolvingthedegradationproblemofvanillaDNNsbyWNLLactivation.Panels(a)
and(b)plotthegenerationerrorswhen1000and10000trainingdataareusedtotrainthevanilla
andtheWNLLactivatedDNNs,respectively.Ineachplot,wetestthreedi˙erentnetworks:
PreActResNet18,PreActResNet34,andPreActResNet50.AlltestsaredoneontheCIFAR10
dataset.
of˝rstpass.Thejumpsinaccuracyatepoch400and800areduetoswitchingfromlinear
activationtoWNLLforpredictionsonthetestset.Theinitialdecaywhenalternatingback
tosoftmaxiscausedpartiallybythe˝nallayer
W
L
notbeingtunedwithrespecttothedeep
features
~
X
,andpartiallyduetopredictionsonthetestsetbeingmadebysoftmaxinsteadof
WNLL.Nevertheless,theperturbationviatheWNLLactivationquicklyresultsintheaccuracy
increasingbeyondthelinearstageinthepreviouspass.
(a)(b)(c)(d)
Figure4:Theevolutionofthegenerationaccuraciesoverthetrainingprocedure.Charts(a)and
(b)aretheaccuracyplotsforResNet50with1000trainingdata,where(a)and(b)areplots
fortheepochv.s.accuracyofthevanillaandtheWNLLactivatedDNNs.Panels(c)and(d)
correspondtothecaseof10000trainingdataforPreActResNet50.Alltestsaredoneonthe
CIFAR10dataset.
4.2ImprovingGeneralizationAccuracy
WenextshowthesuperiorityofWNLLactivatedDNNsintermsofgeneralizationaccuracies
whencomparedtotheirsurrogateswithsoftmaxorSVMoutputactivations.BesidesResNets,we
alsotesttheWNLLsurrogateontheVGGnetworks.Intable2,welistthegeneralizationerrors
for15di˙erentDNNsfromVGG,ResNet,Pre-activatedResNetfamiliesontheentire,˝rst10000
and˝rst1000instancesoftheCIFAR10trainingset.WeobservethatWNLLingeneralimproves
moreforResNetsandpre-activatedResNets,withlessbutstillsigni˝cantimprovementsforthe
VGGs.ExceptforVGGs,wecanachieverelatively20
%
to
30%
testingerrorratereduction
acrossallneuralnets.Allresultspresentedhereandintherestofthispaperarethemedian
of5independenttrials.WealsocomparewithSVMasanalternativeoutputactivation,and
observethattheresultsarestillinferiortoWNLL.Notethatthebiggerbatch-sizeistoensure
theinterpolationqualityofWNLL.Areasonableconcernisthattheperformanceincreasecomes
fromthevariancereductionduetoincreasingthebatchsize.However,experimentsdonewitha
batchsizeof2000forvanillanetworksactuallydeterioratesthetestaccuracy.
Tables2and3listtheerrorratesof15di˙erentvanillanetworksandWNLLactivatednetworks
onCIFAR10andCIFAR100datasets.OnCIFAR10,WNLLactivatedDNNsoutperformsthe
vanillaoneswitharound1.5
%
to2.0
%
absolute,or20
%
to30
%
relativeerrorratereduction.
TheimprovementsonCIFAR100aremoresigni˝cant.WeindependentlyranthevanillaDNNs
onbothdatasets,andourresultsareconsistentwiththeoriginalreportsandotherresearchers'
reproductions[
12
,
14
,
17
].WeprovideexperimentalresultsofDNNs'performanceonSVHN
dataintheappendix.Interestingly,theimprovementaremoresigni˝cantonhardertasks,
7
Table2:GeneralizationerrorratesoverthetestsetofvanillaDNNs,SVMandWNLLactivated
onestrainedovertheentire,the˝rst10000,andthe˝rst1000instancesoftrainingsetof
CIFAR10.(Medianof5independenttrials)
NetworkWhole100001000
VanillaWNLLSVMVanillaWNLLVanillaWNLL
VGG119.23%
7.35%
9.28%10.37%
8.88%
26.75%
24.10%
VGG136.66%
5.58%
7.47%9.12%
7.64%
24.85%
22.56%
VGG166.72%
5.69%
7.29%9.01%
7.54%
25.41%
22.23%
VGG196.95%
5.92%
7.99%9.62%
8.09%
25.70%
22.87%
ResNet209.06%(8.75%[13])
7.09%
9.60%12.83%
9.96%
34.90%
29.91%
ResNet327.99%(7.51%[13])
5.95%
8.73%11.18%
8.15%
33.41%
28.78%
ResNet447.31%(7.17%[13])
5.70%
8.67%10.66%
7.96%
34.58%
27.94%
ResNet567.24%(6.97%[13])
5.61%
8.58%9.83%
7.61%
37.83%
28.18%
ResNet1106.41%(6.43%[13])
4.98%
8.06%8.91%
7.13%
42.94%
28.29%
ResNet186.16%
4.65%
6.00%8.26%
6.29%
27.02%
22.48%
ResNet345.93%
4.26%
6.32%8.31%
6.11%
26.47%
20.27%
ResNet506.24%
4.17%
6.63%9.64%
6.49%
29.69%
20.19%
PreActResNet186.21%
4.74%
6.38%8.20%
6.61%
27.36%
21.88%
PreActResNet346.08%
4.40%
5.88%8.52%
6.34%
23.56%
19.02%
PreActResNet506.05%
4.27%
5.91%9.18%
6.05%
25.05%
18.61%
suggestingpotentialforourmethodstosucceedonothertasks/datasets.Forexample,reducing
thesizesofDNNsisanimportantdirectiontomaketheDNNsapplicableforgeneralizepurposes,
e.g.,auto-drive,mobileintelligence,etc.SofarthemostsuccessfulattemptisDNNsweights
quantization[
2
].Ourapproachisanewdirectionforreducingthesizeofthemodel:toachieve
thesamelevelofaccuracy,comparedtothevanillanetworks,ourmodel'ssizecanbemuch
smaller.
Table3:ErrorratesofthevanillaDNNsv.s.theWNLLactivatedDNNsoverthewhole
CIFAR100dataset.(Medianof5independenttrials)
NetworkVanillaDNNsWNLLDNNsNetworkVanillaDNNsWNLLDNNs
VGG1132.68%
28.80%
ResNet11028.86%
23.74%
VGG1329.03%
25.21%
ResNet1827.57%
22.89%
VGG1628.59%
25.72%
ResNet3425.55%
20.78%
VGG1928.55%
25.07%
ResNet5025.09%
20.45%
ResNet2035.79%
31.53%
PreActResNet1828.62%
23.45%
ResNet3232.01%
28.04%
PreActResNet3426.84%
21.97%
ResNet4431.07%
26.32%
PreActResNet5025.95%
21.51%
ResNet5630.03%
25.36%
5ConcludingRemarks
WearemotivatedbyideasfrommanifoldinterpolationandtheconnectionbetweenResNetsand
controlproblemsoftransportequations.Weproposetoreplacetheclassicaloutputactivation
function,i.e.,softmax,byaharmonicextensiontypeofinterpolatingfunction.Thissimple
surgeryenablesthedeepneuralnets(DNNs)tomakesu˚cientuseofthemanifoldinformation
ofdata.Anend-to-endgreedystyle,multi-stagetrainingalgorithmisproposedtotrainthis
noveloutputlayer.Ononehand,ournewframeworkresolvesthedegradationproblemcausedby
insu˚cientdata;ontheotherhand,itbooststhegeneralizationaccuracysigni˝cantlycompared
tothebaseline.Thisimprovementisconsistentacrossnetworksofdi˙erenttypesanddi˙erent
numberoflayers.Theincreaseingeneralizationaccuracycouldalsobeusedtotrainsmaller
modelswiththesameaccuracy,whichhasgreatpotentialforthemobiledeviceapplications.
8
5.1LimitationandFutureWork
Thereareseverallimitationsofourframeworktoimprovewhichwewishtoremove.Currently,
themanifoldinterpolationstepisstillacomputationalbottleneckinbothspeedandmemory.
Duringtheinterpolation,inordertomaketheinterpolationvalid,thebatchsizeneedstobe
quasilinearwithrespecttothenumberofclasses.ThisposememorychallengesfortheImageNet
dataset[
7
].AnotherimportantissueistheapproximationofthegradientoftheWNLLactivation
function.Linearfunctionisoneoptionbutitisfarfromoptimal.Webelieveabetterharmonic
functionapproximationcanfurtherliftthemodel'sperformance.
Duetotherobustnessandgeneralizationcapabilitiesshownbyourexperiments,weconjecture
thatbyusingtheinterpolationfunctionasoutputactivation,neuralnetscanbecomemorestable
toperturbationsandadversarialattacks[
26
].Thereasonforthisstabilityconjectureisbecause
ourframeworkcombinesbothlearneddecisionboundaryandnearestneighborinformationfor
classi˝cation.
9
Acknowledgments
ThismaterialisbasedonresearchsponsoredbytheAirForceResearchLaboratoryandDARPA
underagreementnumberFA8750-18-2-0066.AndbytheU.S.DepartmentofEnergy,O˚ceof
ScienceandbyNationalScienceFoundation,underGrantNumbersdoe-sc0013838andDMS-
1554564,(STROBE).AndbytheNSFDMS-1737770,NSFC11371220and11671005,andthe
Simonsfoundation.TheU.S.Governmentisauthorizedtoreproduceanddistributereprintsfor
Governmentalpurposesnotwithstandinganycopyrightnotationthereon.
References
[1]
F.Agostinelli,M.Ho˙man,P.Sadowski,andP.Baldi.Learningactivationfunctionsto
improvedeepneuralnetworks.
arXiv
preprint
arXiv:1412.6830,2014.
[2]
M.CourbariauxY.BengioandJ.David.Binaryconnet:Trainingdeepneuralnetworks
withbinaryweights.
NIPS,2015.
[3]
Y.Bengio,P.Lamblin,D.Popovici,andH.Larochelle.Greedylayer-wisetrainingofdeep
networks.
NIPS,2007.
[4]
J.Calder.Thegametheoreticp-laplacianandsemi-supervisedlearningwithfewlabels.
arXiv:1711.10144,2018.
[5]
BoChang,LiliMeng,EldadHaber,FrederickTung,andDavidBegert.Multi-levelresidual
networksfromdynamicalsystemsview.
arXiv
preprint
arXiv:1710.10348,2017.
[6]
Y.Chen,J.Li,H.Xiao,X.Jin,S.Yan,andJ.Feng.Dualpathnetworks.
NIPS,2017.
[7]
J.Deng,W.Dong.,R.Socher,J.Li,K.Li,andF.Li.ImageNet:ALarge-ScaleHierarchical
ImageDatabase.In
CVPR09,2009.
[8]
X.Glorot,A.Bordes,andY.Bengio.Deepsparserecti˝erneuralnetworks.In
Proceedings
of
the
Fourteenth
International
Conference
on
Arti˝cial
Intelligence
and
Statistics
,pages
2011.
[9]
I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley,S.Ozair,A.Courville,
andY.Bengio.Generativeadversarialnets.
Advances
in
Neural
Information
Processing
Systems,pages,2014.
[10]
I.Goodfellow,D.Warde-Farley,M.Mirza,A.Courville,andY.Bengio.Maxoutnetworks.
arXiv
preprint
arXiv:1302.4389,2013.
[11]
K.He,X.Zhang,S.Ren,andJ.Sun.Delvingdeepintorecti˝ers:Surpassinghuman-level
performanceonimagenetclassi˝cation.In
Proceedings
of
the
IEEE
international
conference
on
computer
vision,pages4,2015.
[12]
K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearningforimagerecognition.
CVPR
,
2016.
[13]
K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearningforimagerecognition.
CVPR
,
pages2016.
[14]
K.He,X.Zhang,S.Ren,andJ.Sun.Identitymappingsindeepresidualnetworks.
ECCV
,
2016.
[15]
G.Hinton,S.Osindero,andT.Teh.Afastlearningalgorithmfordeepbeliefnets.
Neural
Computation,2006.
[16]
G.Hinton,N.Srivastava,A.Krizhevsky,I.Sutskever,andR.Salakhutdinov.Improv-
ingneuralnetworksbypreventingco-adaptationoffeaturedetectors.
arXiv
preprint
arXiv:1207.0580,2012.
[17]
G.Huang,Z.Liu,K.Weinberger,andL.vanderMaaten.Denselyconnectedconvolutional
networks.
CVPR,2017.
10
[18]
G.Huang,Y.Sun,Z.Liu,D.Sedra,andK.WeinBerger.Deepnetworkswithstochastic
depth.
ECCV,2016.
[19]
A.Krizhevsky.Learningmultiplelayersoffeaturesfromtinyimages.2009.
[20]
A.Krizhevsky,I.Sutskever,andG.Hinton.Imagenetclassi˝cationwithdeepconvolutional
neuralnetworks.In
Advances
in
neural
information
processing
systems
,pages
2012.
[21]
Y.LeCun.Themnistdatabaseofhandwrittendigits.1998.
[22]
Z.LiandZ.Shi.Deepresiduallearningandpdesonmanifold.
arXiv
preprint
arXiv:1708.05115,2017.
[23]
M.MujaandD.Lowe.Scalablenearestneighboralgorithmsforhighdimensionaldata.
Pattern
Analysis
and
Machine
Intelligence
(PAMI),36,2014.
[24]
V.NairandG.Hinton.Recti˝edlinearunitsimproverestrictedboltzmannmachines.In
Proceedings
of
the
27th
international
conference
on
machine
learning
(ICML-10)
,pages
2010.
[25]
Y.Netzer,T.Wang,A.Coates,A.Bissacco,B.Wu,andA.Ng.Readingdigitsin
naturalimageswithunsupervisedfeatureslearning.
NIPS
Workshop
on
Deep
Learning
and
Unsupervised
Feature
Learning,2011.
[26]
N.Papernot,P.McDaniel,S.Jha,M.Fredrikson,Z.Celik,andA.Swami.Thelimitations
ofdeeplearninginadversarialsettings.
arXiv:1511.07528,2015.
[27]
A.Paszkeandetal.Automaticdi˙erentiationinpyTorch.2017.
[28]
Z.Shi,S.Osher,andW.Zhu.WeightednonlocalLaplacianoninterpolationfromsparse
data.
Journal
of
Scienti˝c
Computing,2017.
[29]
K.SimonyanandA.Zisserman.Verydeepconvolutionalnetworksforlarge-scaleimage
recognition.
arXiv:1409.1556,2014.
[30]
Y.Tang.Deeplearningusinglinearsupportvectormachines.
arXiv:1306.0239,2013.
[31]
L.Wan,M.Zeiler,S.Zhang,Y.LeCun,andR.Fergus.Regularizationofneuralnetworks
usingdropconnect.In
International
Conference
on
Machine
Learning
,pages
2013.
[32]
S.ZagoruykoandN.Komodakis.Wideresidualnetworks.
BMVC,2016.
[33]
W.Zhu,Q.Qiu,J.Huang,R.Carderbank,G.Sapiro,andI.Daubechies.LDMNet:Low
dimensionalmanifoldregularizedneuralnetworks.
UCLA
CAM
Report:
17-66,2017.
11
Appendix
ProofofTheorem1.
Let
X
i
;i
=1
;
2
;

;N
,bethenumberofadditionaldataneededtoobtain
the
i
-typeafter
(
i

1)
distincttypeshavebeensampled.Thetotalnumberofinstancesneeded
is:
X
=
X
1
+
X
2
+

+
X
N
=
N
X
i
=1
X
i
:
Forany
i
,
i

1
distincttypesofinstanceshavealreadybeensampled.Itfollowsthatthe
probabilityofanewinstancebeingofadi˙erenttypeis
1

i

1
N
=
N

i
+1
N
.Essentially,toobtain
the
i
-thdistincttype,therandomvariable
X
followsageometricdistributionwith
p
=
N

i
+1
N
and
E
[
X
i
]=
N
N

i
+1
.Thus,wehave
E
[
X
]=
N
X
i
=1
E
[
X
i
]=
N
X
i
=1
N
N

i
+1
:
Asymptotically,
E
[
X
]
ˇ
N
ln
N
forsu˚cientlylarge
N
.
ResultsonSVHNdata
FortheSVHNrecognitiontask,wesimplytesttheperformancewhenthefulltrainingdataare
used.HereweonlytesttheperformanceoftheResNetsandpre-activatedResNets.Thereisa
relative7
%
-10
%
errorratereductionforalltheseDNNs.
Table4:ErrorratesofthevanillaDNNsv.s.theWNLLactivatedDNNsoverthewholeSVHN
dataset.(Medianof5independenttrials)
NetworkVanillaDNNsWNLLDNNs
ResNet203.76%
3.44%
ResNet323.28%
2.96%
ResNet442.84%
2.56%
ResNet562.64%
2.32%
ResNet1102.55%
2.26%
ResNet183.96%
3.65%
ResNet343.81%
3.54%
PreActResNet184.03%
3.70%
PreActResNet343.66%
3.32%
12
"
35,Laplacian Smoothing Gradient Descent,http://arxiv.org/pdf/1806.06317v5.pdf,https://github.com/BaoWangMath/LaplacianSmoothing-GradientDescent,"LaplacianSmoothGradientDescent
StanleyJ.Osher
DepartmentofMathematics
UniversityofCalifornia,LosAngeles
sjo@math.ucla.edu
BaoWang
DepartmentofMathematics
UniversityofCalifornia,LosAngeles
wangbaonj@gmail.com
PenhangYin
DepartmentofMathematics
UniversityofCalifornia,LosAngeles
yph@ucla.edu
XiyangLuo
DepartmentofMathematics
UniversityofCalifornia,LosAngeles
xylmath@gmail.com
FarzinBarekat
DepartmentofMathematics
UniversityofCalifornia,LosAngeles
fbarekat@math.ucla.edu
MinhPham
DepartmentofMathematics
UniversityofCalifornia,LosAngeles
AlexLin
DepartmentofMathematics
UniversityofCalifornia,LosAngeles
April30,2019
Abstract
Weproposeaclassofverysimplemoofgradientdescentandstochasticgra-
dientdescent.Weshowthatwhenappliedtoalargevarietyofmachinelearningproblems,
rangingfromlogisticregressiontodeepneuralnets,theproposedsurrogatescandramati-
callyreducethevariance,allowtotakealargerstepsize,andimprovethegeneralization
accuracy.Themethodsonlyinvolvemultiplyingtheusual(stochastic)gradientbythe
inverseofapositiveematrix(whichcanbecomputedtlybyFFT)with
alowconditionnumbercomingfromaone-dimensionaldiscreteLaplacianoritshighor-
dergeneralizations.Italsopreservesthemeanandincreasesthesmallestcomponentand
decreasesthelargestcomponent.ThetheoryofHamilton-Jacobipartialtialequa-
tionsdemonstratesthattheimplicitversionofthenewalgorithmisalmostthesameas
doinggradientdescentonanewfunctionwhich(i)hasthesameglobalminimaasthe
originalfunctionand(ii)is\moreconvex"".Moreover,weshowthatoptimizationalgo-
rithmswiththesesurrogatesconvergeuniformlyinthediscreteSobolev
H
p
˙
senseand
reducetheoptimalitygapforconvexoptimizationproblems.Thecodeisavailableat:
https://github.com/BaoWangMath/LaplacianSmoothing-GradientDescent
1Introduction
Stochasticgradientdescent(SGD)[37]hasbeentheworkhorseforsolvinglarge-scalemachine
learning(ML)problems.Itgivesrisetoafamilyofalgorithmsthatenablesttrainingof
manyMLmodelsincludingdeepneuralnets(DNNs).SGDutilizestrainingdataverytly
atthebeginningofthetrainingphase,asitconvergesmuchfasterthanGDandL-BFGS
duringthisperiod[8,16].Moreover,thevarianceofSGDcanhelpgradient-basedoptimization
algorithmscircumventlocalminimaandsaddlepointsandreachthosethatgeneralizewell
[38,18].However,thevarianceofSGDalsoslowsdowntheconvergenceafterthefew
trainingepochs.ToaccountfortheofSGD'svarianceandtoensuretheconvergenceof
SGD,adecayingstepsizehastobeappliedwhichisoneofthemajorbottlenecksforthefast
convergenceofSGD[7,41,40].Moreover,intrainingmanyMLmodels,typicallythestage-wise
scheduleoflearningrateisusedinpractice[39,38].Inthisscenario,thevarianceofSGDusually
leadstoalargeoptimalitygap.
1
arXiv:1806.06317v5  [cs.LG]  27 Apr 2019AnaturalquestionarisesfromtheabovebottlenecksofSGDis:
CanweimproveSGD
suchthatthevarianceofthestochasticgradientisreducedwithnegligible
extracomputationalandmemoryoverheadandalargerstepsizeisallowedtotrain
MLmodels?
Weanswertheabovequestionelybyapplyingthediscreteone-dimensionalLapla-
ciansmoothing(LS)operatortosmooththestochasticgradientvector.TheLS
operationcanbeperformedtlybyusingthefastFouriertransform(FFT).Itisshown
thattheLSreducesthevarianceofstochasticgradientandallowstotakealargerstepsize.
AnotherissueofstandardGDandSGDisthatwhentheHessianoftheobjectivefunctionhas
alargeconditionnumber,gradientdescentperformspoorly.Inthiscase,thederivativeincreases
rapidlyinonedirection,whilegrowingslowlyinanother.Asaby-product,numericallywewill
showthatLScanavoidoscillationalongsteepdirectionsandhelpmakeprogressinshallow
directionsely[25].Theimplicitversionofourproposedapproachislinkedtoanunusual
Hamilton-Jacobipartialtialequation(HJ-PDE)whosesolutionmakestheoriginalloss
functionmoreconvexwhileretainingits(andglobal)minima,andessentiallyworksonthis
surrogatefunctionwithamuchbetterlandscape.See[10]forearlier,relatedwork.
1.1Ourcontribution
Inthispaper,weproposeanewmotothestochasticgradient-basedalgorithms,which
atitscoreusestheLSoperatortoreducethevarianceofstochasticgradientvector.
The(stochastic)gradientsmoothingcanbedonebymultiplyingthegradientbytheinverseof
thefollowingcirculantconvolutionmatrix
A
˙
:=
2
6
6
6
6
4
1+2
˙

˙
0
:::
0

˙

˙
1+2
˙

˙:::
00
0

˙
1+2
˙:::
00
::::::::::::::::::

˙
00
:::

˙
1+2
˙
3
7
7
7
7
5
(1)
forsomepositiveconstant
˙

0.Infact,wecanwrite
A
˙
=
I

˙
L
,where
I
istheidentity
matrix,and
L
isthediscreteone-dimensionalLaplacianwhichactsonindices.Ifwethe
(periodic)forwardrencematrixas
D
+
=
2
6
6
6
6
4

110
:::
00
0

11
:::
00
00

1
:::
00
::::::::::::::::::
100
:::
0

1
3
7
7
7
7
5
:
Then,wehave
A
˙
=
I

˙
D

D
+
,where
D

=

D
>
+
isthebackward
WesummarizethebofthissimpleLSoperationbelow:

Itreducesthevarianceofstochasticgradient,andreducestheoptimalitygap
whenconstantstepsizeisused.

Itallowsustotakealargerstepsizethanthestandard(S)GD.

ItisapplicabletotrainalargevarietyofMLmodelsincludingDNNswithbettergener-
alization.

Itconvergesfasterfortheobjectivefunctionsthathavealargeconditionnumbernumer-
ically.

Itavoidslocalsharpminimaempirically.
Moreover,asastraightforwardextension,wegeneralizetheLStohigh-ordersmoothing
operators,e.g.,biharmonicsmoothing.
2
1.2Relatedwork
Thereisanextensivevolumeofresearchoverthepastdecadesfordesigningalgorithmstospeed
uptheconvergence.Theseincludeusingmomentumandotherheavy-ballmethods,reducethe
varianceofthestochasticgradient,andadaptivethelearningrate.Wewilldiscusstherelated
workfromthesethreeperspectives.
ThetypeofideatoacceleratetheconvergenceofGDandSGDistoapplythemo-
mentum.Aroundlocaloptima,thesurfacecurvescanbemuchmoresteeplyinonedimension
thaninanother[43],whence(S)GDoscillatesacrosstheslopesoftheravinewhileonlymak-
inghesitantprogressalongthebottomtowardsthelocaloptimum.Momentumisproposedto
accelerate(S)GDintherelevantdirectionanddampensoscillations[34].Nesterovaccelerated
gradient(NAG)isalsointroducedtoslowdowntheprogressbeforethesurfacecurveslopesup,
anditprovablyconvergefasterinspscenarios[31].Therearelotsofrecentprogressinthe
developmentofmomentum;arelativelycompletesurveycanbefoundat[3].
Duetothebottleneckofthevarianceofthestochasticgradient,anaturalideaistoreducethe
varianceofthestochasticgradient.Thereareseveralprinciplesindevelopingvariancereduction
algorithms,includingDynamicsamplesizemethods;Gradientaggregation,controlvariatetype
oftechniqueiswidelyusedalongthisdirection,somerepresentativeworksareSAGA[11],SCSG
[24],andSVRG[19];Iterativeaveragingmethods.Athoroughsurveycanbefoundat[8].
AnothercategoryofworktriestospeeduptheconvergenceofGDandSGDbyusingan
adaptivestepsize,whichmakesuseofthehistoricalgradienttoadaptthestepsize.RMSProp
[44]andAdagrad[13]adaptsthelearningratetotheparameters,performingsmallerupdates
(i.e.,lowlearningrates)forparametersassociatedwithfrequentlyoccurringfeatures,andmore
substantialupdates(i.e.,highlearningrates)forparametersassociatedwithinfrequentfea-
tures.BothRMSPropandAdagradmakethelearningratetobehistoricalgradientdependent.
Adadelta[48]extendstheideaofRMSPropandAdagrad,insteadofaccumulatingallpast
squaredgradients,itrestrictsthewindowofaccumulatedpastgradientstosomesize
w
.
Adam[21]andAdaMax[21]behavelikeaheavyballwithfriction,andtheycomputethede-
cayingaveragesofpastandpastsquaredgradientstoadaptivethelearningrate.AMSGrad[36]
theissueofAdamthatmayfailtoconvergetoanoptimalsolution.Adamcanbeviewedas
acombinationofRMSpropandmomentum:RMSpropcontributestheexponentiallydecaying
averageofpastsquaredgradients,whilemomentumaccountsfortheexponentiallydecaying
averageofpastgradients.SinceNAGissuperiortovanillamomentum,Dozat[12]proposed
NAdamwhichcombinestheideaAdamandNAG.
1.3Notations
Throughoutthispaper,weuseboldfaceupper-caseletters
A
,
B
todenotematricesandboldface
lower-caseletters
w
,
u
todenotevectors.Forvectors,weuse
kk
todenotethe
`
2
-normfor
vectorsandspectralnormformatrices,respectively.Andweuse

max
(
A
),

min
(
A
),and

i
(
A
)
todenotethelargest,smallest,andthe
i
-thlargesteigenvalues,respectively.Forafunction
f
:
R
n
!
R
,weuse
r
f
and
r
2
f
todenoteitsgradientandHessian,and
f

todenotealocal
minimumof
f
.Forapositivematrix
A
,weethevectorinducednormbythe
matrix
A
as
k
w
k
A
:=
p
h
w
;
Aw
i
.List
f
1
;
2
;

;n
g
isdenotedby[
n
].
1.4Organization
Weorganizethispaperasfollows:Insection2,weintroducetheLS(S)GDalgorithmandthe
FFT-basedfastsolver.Insection3,weshowthatLS(S)GDallowsustotakealargerstep
sizethan(S)GDbasedontheand
`
2
estimateoftheintroduceddiscreteLaplacianoperator.
Insection4,weshowthatLSreducesthevarianceofSGDbothempiricallyandtheoretically.
WeshowthatLSGDcanavoidsomelocalminimaandspeedupconvergencenumericallyin
section5.Insection6,weshowthebofLSindeeplearning,includingtrainingLeNet
[23],ResNet[17],Wassersteingenerativeadversarialnets(WGAN)[27],anddeepreinforcement
learning(DRL)model.TheconvergenceanalysisforLS(S)GDisprovidedinsection7.The
connectiontotheHamilton-Jacobipartialtialequations(HJ-PDEs)andfuturedirection
arediscussedinsection8.Mostofthetechnicalproofsareprovidedinsection9.
3
Algorithm1
LSSGD
Input:
f
i
(
w
)for
i
=1
;
2
;

;n
.
w
0
:initialguessof
w
,
T
:thetotalnumberofiterations,and

k
,
k
=0
;
1
;

;T
:the
scheduledstepsize.
Output:
Theoptimizedweights
w
opt
.
for
k
=0
;
1
;

;T
do
w
k
+1
=
w
k


A

1
˙

r
f
i
k
(
w
k
)

.
return
w
T
2LaplacianSmoothing(Stochastic)GradientDescent
WepresentouralgorithmforSGDinthesetting.TheGDandothersettingsfollow
straightforwardly.Considerthefollowingoptimization
min
w
F
(
w
):=
1
n
n
X
i
=1
f
i
(
w
)
;
(2)
where
f
i
(
w
)
:
=
f
(
w
;
x
i
;y
i
)isthelossofagivenMLmodelonthetrainingdata
f
x
i
;y
i
g
.This
formalismisanabstractoftrainingmanyMLmodelsmentionedabove.Toresolve
theoptimizationproblemEq.(2),startingfromsomeinitialguess
w
0
,the(
k
+1)-thiteration
ofSGDreads
w
k
+1
=
w
k


k
r
f
i
k
(
w
k
)
;
(3)
where

k
isthestepsize,
i
k
isarandomsamplewithreplacementfrom[
n
].
Weproposetoreplacethestochasticgradient
r
f
i
k
(
w
k
)bytheLaplaciansmoothedsurro-
gate,andwecalltheresultingalgorithmLSSGD,whichiswrittenas
w
k
+1
=
w
k


k
A

1
˙
r
f
i
k
(
w
k
)
:
(4)
Intuitively,comparedtothestandardGD,thisschemesmoothsthegradientbyan
ellipticsmoothingoperatorwhilepreservingthemeanoftheentriesofthegradient.Weadopt
fastFouriertransform(FFT)tocompute
A

1
˙
r
f
(
w
k
),whichisavailableinbothPyTorch[33]
andTensorFlow[2].Givenavector
g
,asmoothedvector
d
canbeobtainedbycomputing
d
=
A

1
˙
g
.Thisisequivalentto
g
=
d

˙
v

d
,where
v
=[

2
;
1
;
0
;

;
0
;
1]
>
and

isthe
convolutionoperator.Therefore
d
=


g
)
1

˙


v
)

;
whereweusecomponent-wisedivision(here,andaretheFFTandinverseFFT,respec-
tively).Hence,thegradientsmoothingcanbedoneinquasilineartime.Thisadditionaltime
complexityisalmostthesameasperformingaonestepupdateontheweightsvector
w
.For
manymachinelearningmodels,wemayneedtoconcatenatetheparametersintoavector.This
reshapingmightleadtosomeambiguity,nevertheless,basedonourtests,bothrowandcolumn
majoredreshapingworkfortheLS-GDalgorithm.Moreover,indeeplearningcases,theweights
inrentlayersmighthavetphysicalmeanings.Forthesecases,weperformlayer-wise
gradientsmoothing,instead.WesummarizetheLSSGDforsolvingtheoptimization
Eq.(2)inAlgorithm1.
Remark1.
Inimageprocessingandelsewhere,theSobolevgradient[20]usesamulti-dimensional
Laplacianoperatorthatoperateson
w
,andisentfromtheone-dimensionaldiscreteLapla-
cianoperatoremployedinourLS-GDschemethatoperatesonindices.
ItisworthnotingthatLSisacomplementtotheheavyball,e.g.,Nesterovmomentum,
andadaptivelearningrate,e.g.,Adam,algorithms.Itcanbecombinedwiththeseacceleration
techniquestospeeduptheconvergence.Wewillshowtheperformanceofthesealgorithmsin
theSection6.
4
2.1Generalizedsmoothinggradientdescent
Wecangeneralize
A
˙
tothe
n
-thorderdiscretehypoperatorasfollows
I
+(

1)
n
˙
L
n
:
=
A
n
˙
:
EachrowofthediscreteLaplacianoperator
L
consistsofanappropriatearrangementofweights
incentraliteapproximationtothe2ndorderderivative.Similarly,eachrowof
L
n
isanarrangementoftheweightsinthecentralapproximationtothe2
n
-thorder
derivative.
Remark2.
The
n
-thordersmoothingoperator
I
+(

1)
n
˙
L
n
canonlybeappliedtotheproblem
withdimensionatleast
2
n
+1
.Otherwise,weneedtoadddummyvariablestotheobjectfunction.
Again,weapplyFFTtocomputethesmoothedgradientvector.Foragivengradientvector
g
,thesmoothedsurrogate,(
A
n
˙
)

1
g
:
=
d
,canbeobtainedbysolving
g
=
d
+(

1)
n
˙
v
n

d
,where
v
n
=(
c
n
n
+1
;c
n
n
+2
;

;c
n
2
n
+1
;
0
;

;
0
;c
n
1
;c
n
2
;

;c
n
n

1
;c
n
n
)isavectorofthesamedimensionas
thegradienttobesmoothed.Andthecotvector
c
n
=(
c
n
1
;c
n
2
;

;c
n
2
n
+1
)canbeobtained
recursivelybythefollowingformula
c
1
=(1
;

2
;
1)
;c
n
i
=
8
>
<
>
:
1
i
=1
;
2
n
+1

2
c
n

1
1
+
c
n

1
2
i
=2
;
2
n
c
n

1
i

1

2
c
n

1
i
+
c
n

1
i
+1
otherwise.
Remark3.
Thecomputationalcomplexitiesforentordersmoothingschemesarethesame
whentheFFTisutilizedforcomputingthesurrogategradient.
3TheChoiceofStepSize
Inthissection,wewilldiscussthestepsizeissueofLS(S)GDwithatheoreticalfocusonLSGD
on
L
-Lipschitzfunctions.
1
(
L
-Lipschitz)
.
Wesaythefunction
F
is
L
-Lipschitz,ifforany
w
;
u
2
R
m
,we
have
k
f
(
w
)

f
(
u
)
k
L
k
w

u
k
.
Remark4.
Ifthefunction
F
is
L
-Lipschitzandentiable,thenforany
w
,wehave
kr
f
(
w
)
k
L
.
For
L
-Lipschitzfunction,itisknownthatthelargestsuitablestepsizeforGDis

GD
max
=
1
L
[32].Inthefollowing,wewillestablisha
`
2
estimateofthesquarerootoftheLSoperatorwhen
itisappliedtoanarbitraryvector.Basedontheseestimates,wewillshowthatLSGDcantake
alargerstepsizethanGD.
TodeterminethelargestsuitablestepsizeforLSGD.Wedoachangeofvariableinthe
LSGD2byletting
v
k
=
H

1
=
2
˙
w
k
where
H
˙
=
A

1
˙
,thenLSGDcanbewrittenas
v
k
+1
=
v
k


k
H
1
=
2
˙
r
F
(
H
1
=
2
˙
v
k
)
;
(5)
whichisactuallytheGDforsolvingthefollowingminimizationproblem
min
v
F
(
H
1
=
2
˙
v
):=min
v
G
(
v
)
:
(6)
Therefore,todeterminethelargestsuitablestepsizeforLSGD,itisequivalenttothelargest
appropriatestepsizeforGDformin
v
G
(
v
).Therefore,ittodeterminetheLipschitz
constantforthefunction
G
(
v
),i.e.,to
L
G
:=inf
v
fkr
G
(
v
)
kj
v
2
dom(
G
)
g
:
Notethatfor
8
v
1
;
v
2
,wehave
k
G
(
v
1
)

G
(
v
2
)
k
=
k
F
(
H
1
=
2
˙
v
1
)

F
(
H
1
=
2
˙
v
2
)
k

L
k
H
1
=
2
˙
v
1

H
1
=
2
˙
v
2
k
Tothelargestappropriatestepsize,weneedtofurtherestimate
k
H
1
=
2
˙
v
1

H
1
=
2
˙
v
2
k
.
5
3.1
`
2
estimatesofH
1
=
2
˙
v
Proposition1.
Givenanyvector
v
2
R
m
,let
w
=
A

1
=
2
˙
v
,then
k
v
k
2
=
k
w
k
2
+
˙
k
D
+
w
k
2
:
(7)
Proof.
Observethat
v
=
A
1
=
2
˙
w
.Therefore,
k
v
k
2
=
D
A
1
=
2
˙
w
;
A
1
=
2
˙
w
E
=
h
A
˙
w
;
w
i
=
h
w

˙
D

D
+
w
;
w
i
=
k
w
k
2

˙
h
D

D
+
w
;
w
i
=
k
w
k
2

˙
h
D
+
w
;

D
+
w
i
=
k
w
k
2
+
˙
k
D
+
w
k
2
;
whereweused
D
T

=

D
+
forthesecondlastequality.
Proposition1showsthattheLipschitzconstantof
G
isnotlargerthanthatof
F
,since
k
H
1
=
2
˙
v
1

H
1
=
2
˙
v
2
k
2
=
k
v
1

v
2
k
2

˙
k
D
+
(
H
1
=
2
˙
v
1

H
1
=
2
˙
v
2
)
k
2
k
v
1

v
2
k
2
:
Therefore,LSGDcantakeatleastthesamestepsizeasGD.However,notethat
k
D
+
w
k
2
can
bearbitrarilyclosetozero,soLSGDcannotalwaystakealargerstepsizethanGD.Next,we
establishahighprobabilityestimationfortakingalargerstepsizewhenusingLSGD.
Withoutanypriorknowledgeabout
v
1

v
2
:=
v
,letusassumeitissampleduniformlyfrom
aballin
R
m
centeredattheorigin.Withoutlossofgenerality,weassumetheradiusofthisball
isone.Forthesakeofnotationsimplicity,inthefollowingwedenote
H
1
=
2
˙
:=
M
˙
.Underthe
aboveansatz,wehavethefollowingresult
Theorem1
(
`
2
-estimate)
.
Let
˙>
0
,and

=
1
m
m
X
i
=1
1
1+2
˙

˙z
i

˙
z
i
;
where
z
1
,

,
z
m
arethe
m
rootsofunity.Let
v
beuniformlydistributedintheunitballofthe
m
dimensional
`
2
space.Then
P
(
k
M
˙
v
k

k
v
k
)

2exp
0
@

2
ˇ
2
m
 



ˇ
p
m

p


+1
!
2
1
A
(8)
forany
>
p

1

ˇ
p
m
.
Theproofofthistheoremisprovidedintheappendix.ForhighdimensionalMLproblems,
e.g.,trainingDNNs,
m
canbeaslargeastensofmillionssothattheprobabilitywillbealmost
one.Theclosedformof

isgiveninLemma1.
Lemma1.
If
z
1
;:::;z
m
denotethe
m
rootsofunity,then

=
1
m
m
X
j
=1
1
1+2
˙

˙z
j

˙

z
j
=
1+

m
(1


m
)
p
4
˙
+1
!
1
p
1+4
˙
;
(9)
as
m
!1
,where
1
>
=
2
˙
+1

p
4
˙
+1
2
˙
>
0
:
Theproofoftheabovelemmarequiressometoolsfromcomplexanalysisandharmonic
analysis,whichisprovidedintheappendix.Table1listssometypicalvaluesfort
˙
and
dimensions
m
.
BasedontheestimateinTheorem1,LSGDcantakethelargeststepsize
1
p
L
forhigh-
dimensional
L
-Lipschitzfunctionwithhighprobability.Wewillverifythisresultnumerically
inthefollowingsections.
6
Table1:Thevaluesof

correspondingtosome
˙
and
m
.

convergesquicklytoitslimiting
valueas
m
increases.
˙
12345
m
=10000.4470.3330.2770.2430.218
m
=100000.4470.3330.2770.2430.218
m
=1000000.4470.3330.2770.2430.218
4VarianceReduction
ThevarianceofSGDisoneofthemajorbottlenecksthatslowsdownthetheoreticalguaranteed
convergencerateintrainingMLmodels.Mostoftheexistingvariancereductionalgorithms
requireeitherthefullbatchgradientorthestorageofstochasticgradientforeachdatapoint
whichmakesittobeusedtotrainthehigh-capacityDNNs.LSisanalternative
approachtoreducethevarianceofthestochasticgradientwithnegligibleextracomputational
timeandmemorycost.Inthissection,werigorouslyshowthatLSreducesthevarianceof
thestochasticgradientandreducetheoptimalitygapundertheGaussiannoiseassumption.
Moreover,wenumericallyverifyourtheoreticalresultsonbothaquadraticfunctionanda
simpleoptimizationproblem.
4.1Gaussiannoiseassumption
Stochasticgradient
r
f
i
k
,forany
i
k
2
[
n
],isanunbiasedestimateof
r
F
,manyexistingworks
modelthevariancebetweenthestochasticgradientandfullbatchgradient
r
F
asGaussian
noise
N
(
0
;
whereisthecovariancematrix[28].Therefore,ignoringthevariable
w
for
simplicityofnotation,wecanwritetheequationinvolvinggradientandstochasticgradient
vectorsas
r
f
i
k
=
r
F
+
n
;
(10)
where
n
˘N
(
0
;
ThusforLSstochasticgradient,wehave
A

1
˙
r
f
i
k
=
A

1
˙
(
r
F
+
n
)
:
(11)
ThevariancesofstochasticgradientandLSstochasticgradientarebasicallythevarianceof
n
and
A

1
˙
n
,respectively.Thefollowingtheoremquanthevariancebetween
n
and
A

1
˙
n
.
Theorem2.
Let

denotetheconditionnumberof

.Then,for
m
dimensionalGaussian
randomvector
n
˘N
(
0
;

,wehave
P
m
i
=1
Var[

(
A
n
˙
)

1
n

i
]
P
m
i
=1
Var[(
n
)
i
]

1

1

+
1

m
X
j
=0
1
[1+4
n
˙
sin
2
n
(
ˇj=m
)]
2
:
(12)
TheproofofTheorem2willbeprovidedintheappendix.
Table2liststheratioofvarianceafterandbeforeLSforan
m
-dimensionalstandardnormal
vector,i.e.,
n
˘N
(
0
;
I
).Inpractice,highordersmoothingreducevariancemoretly.
Table2:Theoreticalupperboundof
P
m
i
=1
Var[

(
A
n
˙
)

1
n

i
]
=
P
m
i
=1
Var[(
n
)
i
]when
n
isan
m
-dimensionalstandardnormalvectorwith
m

10000.
˙
12345
n
=10.2680.1850.1490.1290.114
n
=20.2790.2310.2070.1920.181
n
=30.2900.2560.2380.2260.218
Moreover,LSpreservesthemean(Proposition2),decreasesthelargestcomponentand
increasesthesmallestcomponent(Proposition3)foranyvector.
Proposition2.
Foranyvector
g
2
R
m
,
d
=
A

1
˙
g
,let
j
max
=argmax
i
d
i
and
j
min
=
argmin
i
d
i
.Wehave
max
i
d
i
=
d
j
max

g
j
max

max
i
g
i
and
min
i
d
i
=
d
j
min

g
j
min

min
i
g
i
.
7
Proof.
Since
g
=
A
˙
d
,itholdsthat
g
j
max
=
d
j
max
+
˙
(2
d
j
max

d
j
max

1

d
j
max
+1
)
;
whereperiodicityofsubindexareusedifnecessary.Since2
d
j
max

d
j
max

1

d
j
max
+1

0,We
havemax
i
d
i
=
d
j
max

g
j
max

max
i
g
i
.Asimilarargumentcanshowthatmin
i
d
i
=
d
j
min

g
j
min

min
i
g
i
.
Proposition3.
Theoperator
A

1
˙
preservesthesumofcomponents.Forany
g
2
R
m
and
d
=
A

1
˙
g
,wehave
P
j
d
j
=
P
j
g
j
,orequivalently,
1
>
d
=
1
>
g
.
Proof.
Since
g
=
A
˙
d
,
X
i
g
i
=
1
>
g
=
1
>
(
I
+
˙
D
>
+
D
+
)
d
=
1
>
d
=
X
i
d
i
;
whereweused
D
+
1
=
0
.
4.2Reducetheoptimalitygap
AdirectbofvariancereductionisthatitreducestheoptimalitygapinSGDwhenconstant
stepsizeisapplied.Westatethecorrespondingresultinthefollowing.
Proposition4.
Suppose
f
isconvexwiththeglobalminimizer
w

,and
f

=
f
(
w

)
.Consider
thefollowingiterationwithconstantlearningrate
>
0
w
k
+1
=
w
k


(
A
n
˙
)

1
g
k
where
g
k
isthesampledgradientinthe
k
-thiterationat
w
k
satisfying
E
[
g
k
]=
r
f
(
w
k
)
.Denote
G
A
n
˙
:=lim
K
!1
1
K
P
K

1
k
=0
k
g
k
k
2
(
A
n
˙
)

1
and
w
K
:=
P
K

1
k
=0
w
k
=K
theergodicaverageofiterates.
Thentheoptimalitygapis
lim
K
!1
E
[
f
(
w
K
)]

f


G
A
n
˙
2
:
Proof.
Since
f
isconvex,wehave
hr
f
(
w
k
)
;
w
k

w

i
f
(
w
k
)

f

:
(13)
Furthermore,
E
[
k
w
k
+1

w

k
2
A
n
˙
]=
E
[
k
w
k


(
A
n
˙
)

1
g
k

w

k
2
A
n
˙
]
=
E
[
k
w
k

w

k
2
A
n
˙
]

2

E
[
h
g
k
;
w
k

w

i
]+

2
E
[
k
(
A
n
˙
)

1
g
t
k
2
A
n
˙
]

E
[
k
w
k

w

k
2
A
n
˙
]

2

E
[
hr
f
(
w
k
)
;
w
k

w

i
]+

2
k
g
k
k
2
(
A
n
˙
)

1

E
[
k
w
k

w

k
2
A
n
˙
]

2

(
E
[
f
(
w
k
)]

f

)+

2
k
g
k
k
2
(
A
n
˙
)

1
;
wherethelastinequalityisdueto(13).Werearrangethetermsandarriveat
E
[
f
(
w
k
)]

f


1
2

(
E
[
k
w
k

w

k
2
A
n
˙
]

E
[
k
w
k
+1

w

k
2
A
n
˙
])+

k
g
k
k
2
(
A
n
˙
)

1
2
:
Summingover
k
from0to
K

1andaveragingandusingtheconvexityof
f
,wehave
E
[
f
(
w
K
)]

f


P
K

1
k
=0
E
[
f
(
w
k
)]
K

f


1
2
K
E
[
k
w
0

w

k
2
A
n
˙
]+
P
K

1
k
=0
k
g
k
k
2
(
A
n
˙
)

1
2
K
:
Takingthelimitas
K
!1
aboveestablishestheresult.
Remark5.
Since
G
A
n
˙
issmallerthanthecorrespondingvaluewithoutLS.Itshowsthatthe
optimalitygapisreducedwhenLSisusedwithaconstantstepsize.Inpractice,thisisalsotrue
forthestage-wisestepsizesinceitisaconstantineachstageofthetrainingphase.
8
4.2.1Optimizationforquadraticfunction
Inthispart,weempiricallyshowtheadvantagesoftheLS(S)GDanditsgeneralizedschemesfor
theconvexoptimizationproblems.Considersearchingtheminima
x

ofthequadraticfunction
f
(
x
)inEq.(14).
f
(
x
1
;x
2
;

;x
100
)=
50
X
i
=1
x
2
2
i

1
+
50
X
i
=1
x
2
2
i
10
2
:
(14)
TosimulateSGD,weaddGaussiannoisetothegradientvector,i.e.,atanygivenpoint
x
,
wehave
~
r

f
(
x
):=
r
f
(
x
)+

N
(
0
;
I
)
;
wherethescalar

controlsthenoiselevel,
N
(
0
;
I
)istheGaussiannoisevectorwithzeromean
andunitvarianceineachcoordinate.Thecorrespondingnumericalschemescanbeformulated
as
x
k
+1
=
x
k


k
(
A
n
˙
)

1
~
r

f
(
x
k
)
;
(15)
where
˙
isthesmoothingparameterselectedtobe10
:
0toremovetheintensenoise.Wetakedi-
minishingstepsizeswithinitialvalues0
:
1forSGD/smoothedSGD;0
:
9and1
:
8forGD/smoothed
GD,respectively.Withoutnoise,thesmoothingallowsustotakelargerstepsizes,roundingto
thedigit,0
:
9and1
:
8arethelargestsuitablestepsizeforGDandsmoothedversionhere.
Westudybothconstantlearningrateandexponentiallydecayinglearningrate,i.e.,afterevery
1000iterationthelearningrateisdividedby10.Weapplytschemesthatcorresponding
to
n
=0
;
1
;
2inEq.(15)totheproblem(Eq.(14)),withtheinitialpoint
x
0
=(1
;
1
;

;
1).
Figure.1showstheiterationv.s.optimalitygapwhentheconstantlearningrateisused.
Inthenoisefreecase,allthreeschemesconvergelinearly.Whenthereisnoise,oursmoothed
gradienthelpstoreducetheoptimalitygapandconvergesfasterafterafewiterations.
(a)

=0(b)

=0
:
05
(c)

=0
:
1(d)

=0
:
5
Figure1:Iterationsv.s.optimalitygapforGDandsmoothedGDwithorder1andorder2
smoothingfortheprobleminEq.(14).Constantstepsizeisused.
TheexponentiallydecayinglearningratehelpsoursmoothedSGDtoreachapointwitha
smalleroptimalitygap,andthehigherordersmoothingfurtherreducestheoptimalitygap,as
showninFig.2.Thisisduetothenoiseremovalpropertiesofthesmoothingoperators.
9
(a)

=0(b)

=0
:
05
(c)

=0
:
1(d)

=0
:
5
Figure2:Iterationsv.s.optimalitygapforGDandsmoothedGDwithorder1and2smoothing
fortheprobleminEq.(14).Exponentiallydecayingstepsizeisutilizedhere.
4.2.2Findthecenterofmultiplepoints
Considersearchingthecenterofagivensetof5Krandompoints
f
x
i
2
R
50
g
5000
i
=1
.
1
Thisproblem
canbeformulateasthefollowingoptimization
min
x
F
(
x
):=
1
N
N
X
i
=1
f
i
(
x
)=
1
N
N
X
i
=1
k
x
i

x
k
2
:
(16)
WesolvethisoptimizationproblembyrunningeitherSGDorLSSGDfor20Kiterationsstarting
fromthesamerandominitialpointwithbatchsize20.Theinitialstepsizeissettobe1.0and
1.2,respectively,forSGDandLSSGD,anddecays1.1timesafterevery10iterations.Asthe
learningratedecays,thevarianceofthestochasticgradientdecays[46],thuswedecay
˙
10
timesafterevery1Kiterations.Figure3(a)plotsa2Dcrosssectionofthetrajectoriesof
SGDandLSSGD,anditshowsthatthetrajectoryofSGDismorenoisythanthatofLSSGD.
Figure3(b)plotstheiterationv.s.lossforbothSGDandLSSGDaveragedover3independent
runs.LSSGDconvergesfasterthanSGDandhasasmalleroptimalitygapthanLSSGD.This
numericalresultvourtheoreticalresultsontheoptimalitygap(Proposition4).
4.2.3Multi-classLogisticregression
Considerapplyingtheproposedoptimizationsch{emestotrainthemulti-classLogisticregres-
sionmodel.Werun200epochsofSGDandtordersmoothingalgorithmstomaximize
thelikelihoodofmulti-classLogisticregressionwithbatchsize100.Andweapplytheexponen-
tiallydecayinglearningratewithinitialvalue0
:
5anddecay10timesafterevery50epochs.We
trainthemodelwithonly10%randomlyselectedMNISTtrainingdataandtestthetrained
modelontheentiretestingimages.WefurthercomparewithSVRGunderthesamesetting.
Figure.4showsthehistogramsofgeneralizationaccuracyofthemodeltrainedbySGD(a);
SVRG(b);LS-SGD(order1)(c);LS-SGD(oder2)(d).ItisseenthatSVRGsomewhatim-
provesthegeneralizationwithhigheraveragedaccuracy.However,theandthesecond
orderLSSGDtypealgorithmslifttheaveragedgeneralizationaccuracybymorethan1%and
1
WethankprofessorAdamObermanforsuggestingthisproblemtous.
10
Figure3:Left:acrosssectionofthetrajectoriesofSGDandLSSGD.Right:Iterationv.s.Loss
forSGDandLS-SGD.
reducetntofElectricalEngineeringandComputerSciencesUniversityofhevarianceofthe
generalizationaccuracyover100independenttrialsremarkably.
4.3Iterationv.s.loss
Inthispart,weshowtheevolutionofthelossintrainingthemulti-classLogisticregressionmodel
bySGD,SVRG,LSGDwithandsecondordersmoothing,respectively.Asillustratedin
Fig.5.Ateachiteration,among100independentexperiments,SGDhasthelargestvariance,
SGDwithordersmoothedgradienttlyreducesthevarianceoflossamongt
experiments.Thesecondordersmoothingcanfurtherreducethevariance.Thevarianceofloss
ineachiterationamong100experimentsisminimizedwhenSVRGisusedtotrainthemulti-
classLogisticmodel.However,thegeneralizationperformanceofthemodeltrainedbySVRGis
notasgoodastheonestrainedbyLS-SGD,orhigherordersmoothedgradientdescent(Fig.4
(b)).
4.4Variancereductioninstochasticgradient
Weverifytheofvariancereductionnumericallyinthispart.Wesimplifytheproblem
byapplyingthemulti-classLogisticregressiononlytothedigits1and2oftheMNISTtraining
data.Inordertocomputethevarianceofthe(LS)-stochasticgradients,wecomputedescent
pathof(LS)-GDbyapplyingthefullbatch(LS)-GDwithlearningrate0
:
5startingfromthe
samerandominitialization.Werecordthefullbatch(LS)-gradientoneachpointalongthe
descentpath.Thenwecomputethe(LS)-stochasticgradientsoneachpointsalongthepathby
usingtbatchsizesandsmoothingparameters
˙
.Incomputing(LS)-stochasticgradients
werun100independentexperiments.Thenwecomputethevarianceofthe(LS)-stochastic
gradientamongthese100experimentsandregardingthefullbatch(LS)-gradientasthemean
oneachpointalongthefullbatch(LS)-GDdescentpath.Foreachpairofbatchsizeand
˙
,we
reportthemaximumvarianceoverallthecoordinatesofthegradientandallthepointsalong
thedescentpath.WelistthevarianceresultsinTable3(notethecase
˙
=0correspondsto
theSGD).TheseresultsshowthatcomparedtotheSGD,LSGDwith
˙
=3canreducethe
maximumvariance
˘
100
timesforderentbatchsizes.Itisworthnotingthatthehighorder
smoothingreducesmorevariancethanthelowerordersmoothing,thismightduetothefact
thatthenoiseofSGDisnotGaussian.
Table3:ThemaximumvarianceofthestochasticgradientgeneratedbyLS-SGDwithrent
˙
andbatchsize.
˙
=0recoverstheSGD.
BatchSize25102050
˙
=01.50E-15.49E-22.37E-21.01E-24.40E-3
˙
=13.40E-31.30E-35.45E-42.32E-49.02E-5
˙
=22.00E-37.17E-43.46E-41.57E-45.46E-5
˙
=31.40E-34.98E-42.56E-41.17E-43.97E-5
11
(a)SGD(b)SVRG
(c)LS-GD:Order1(d)LS-GD:Order2
Figure4:Histogramoftestingaccuracyover100independentexperimentsofthemulti-class
Logisticregressionmodeltrainedonrandomlyselected10%MNISTdatabytalgorithms.
5NumericalResultsonAvoidLocalMinimaandSpeed
UpConvergence
WeshowthatLS-GDcanbypasssharpminimaandreachtheglobalminima.Weconsider
thefollowingfunction,inwhichwe`drill'narrowholesonasmoothconvexfunction,
f
(
x;y;z
)=

4
e

(
(
x

ˇ
)
2
+(
y

ˇ
)
2
+(
z

ˇ
)
2
)

(17)
4
X
i
cos(
x
)cos(
y
)
e


(
(
x

r
sin(
i
2
)

ˇ
)
2
+(
y

r
cos(
i
2
)

ˇ
)
2
)
;
wherethesummationistakenovertheindexset
f
i
2
N
j
0

i<
4
ˇ
g
,
r
and

arethe
parametersthatdeterminethelocationandnarrownessofthelocalminimaandaresetto1and
1
p
500
,respectively.WedoGDandLS-GDstartingfromarandompointintheneighborhoods
ofthenarrowminima,i.e.,(
x
0
;y
0
;z
0
)
2f
S
i
U

(
r
sin(
i
2
)+
ˇ;r
cos(
i
2
)+
ˇ;ˇ
)
j
0

i<
4
ˇ;i
2
N
g
,
where
U

(
P
)isaneighborhoodofthepoint
P
withradius

.Ourexperiments(Fig.6)show
that,if


0
:
2GDwillconvergetoanarrowlocalminima,whileLS-GDconvergencestothe
widerglobalminima.
Next,letuscompareLSGDwithsomepopularoptimizationmethodsonthebenchmark
2D-Rosenbrockfunctionwhichisanon-convexfunction.Theglobalminimumisinsidealong,
narrow,parabolicshapedvalley.Tothevalleyistrivial.Toconvergetotheglobal
minimum,however,isThefunctionisby
f
(
x;y
)=(
a

x
)
2
+
b
(
y

x
2
)
2
;
(18)
ithasaglobalminimumat(
x;y
)=(
a;a
2
),andweset
a
=1and
b
=100inthefollowing
experiments.
Startingfromtheinitialpointwithcoordinate(

3
;

4),werun2Kiterationsofthefollowing
optimizersincludingGD,GDwithNesterovmomentum[31],Adam[21],RMSProp[44],and
12
(a)SGD(b)SVRG
(c)LS-GD:Order1(d)LS-GD:Order2
Figure5:Iterationsv.s.lossforSGD,SVRG,andLS-SGDwithorder1andorder2gradient
smoothingfortrainingthemulti-classLogisticregressionmodel.
LSGD(
˙
=0
:
5).Thestepsizeusedforallthesemethodsis3
e

3.Figure7plotsthe
iterationv.s.objectivevalue,anditshowsthatGDtogetherwithNesterovmomentumconverges
fasterthanalltheotheralgorithms.ThesecondbestalgorithmisLSGD.Meanwhile,Nesterov
momentumcanbeusedtospeedupLSGD,andwewillshowthisnumericallyintrainingDNNs
insection6.
Figure8depictssomesnapshots(The300th,600th,900th,and1200thiteration,respectively)
ofthetrajectoriesoftoptimizationalgorithms.Theseshowthateventhough
GDwithmomentumconvergefasterbutitrsfromsomeovershoots,andtheydetourto
convergetothelocalminima.Alltheotheralgorithmsgoalongadirectpathtotheminima,
andLSGDconvergesfastest.
Furthermore,wewillshowthatLSGDcanbefurtheracceleratedbyusingNesterovmomen-
tum.AsshowinFig.9,theLSGDtogetherwithNesterovmomentumconvergesmuchfaster
thanGDwithmomentum,especiallyforhighdimensionalRosenbrockfunction.
6ApplicationtoDeepLearning
6.1Trainneuralnetswithsmallbatchsize
Manyadvancedintelligencetasksmakehighdemandsontrainingneuralnetswith
extremelysmallbatchsizes.Themilestonetechniqueforthisisgroupnormalization[47].In
thissection,weshowthatLS-SGDsuccessfullytrainsDNNwithextremelysmallbatchsize.
WeconsiderLeNet-5[23]forMNISTOurnetworkarchitectureisasfollows
LeNet-5:input
28

28
!
conv
20
;
5
;
2
!
conv
50
;
5
;
2
!
fc
512
!
softmax
:
Thenotationconv
c;k;m
denotesa2Dconvolutionallayerwith
c
outputchannels,eachofwhich
isthesumofachannel-wiseconvolutionoperationontheinputusingalearnablekernelofsize
k

k
,itfurtheraddsReLUnonlinearityandmaxpoolingwithstridesize
m
.fc
512
isan
13
(a)(b)
Figure6:DemoofGDandLS-GD.Panel(a)depictsthesliceofthefunction(Eq.(17))with
z
=2
:
34;panel(b)showsthepathsofGD(red)andLS-GD(black).Wetakethestepsizeto
be0.02forbothGDandLS-GD.
˙
=1
:
0isutilizedforLS-GD.
transformationthattransformstheinputtoavectorofdimension512.Finally,thetensors
areactivatedbyamulti-classLogisticfunction.TheMNISTdataispassedtothelayer
input
28

28
,andfurtherprocessedbythishierarchicalstructure.Werun100epochsofbothSGD
andLS-SGDwithinitiallearningrate0
:
01anddivideby5after50epochs,anduseaweight
decayof0
:
0001andmomentumof0
:
9.Figure.10(a)plotsthegeneralizationaccuracyonthe
testsetwiththeLeNet5trainedwithtbatchsizes.Foreachbatchsize,LS-SGDwith
˙
=1
:
0keepsthetestingaccuracymorethan99
:
4%,SGDreducetheaccuracyto97%when
batchsize4isused.Thebecomejustarandomguess,whenthemodelistrained
bySGDwithbatchsize2.Smallbatchsizeleadstolargenoiseinthegradient,whichmaymake
thenoisygradientnotalongthedecentdirection;however,Lapaciansmoothingrescuesthisby
decreasingthenoise.
6.2Improvegeneralizationaccuracy
TheskipconnectionsinResNetsmooththelandscapeofthelossfunctionoftheclassicalCNN
[17,26].ThismeansthatResNethasfewersharpminima.OnCifar10[22],wecomparethe
performanceofLS-SGDandSGDonResNetwiththepre-activatedResNet56asanillustration.
Wetakethesametrainingstrategyasthatusedin[17],exceptthatwerun200epochswith
thelearningratedecayingbyafactorof5afterevery40epochs.ForResNet,insteadof
applyingLS-SGDforallepochs,weonlyuseLS-SGDinthe40epochs,andtheremaining
trainingiscarriedoutbySGD(thiswillsavetheextracomputationalcostduetoLS,and
wenoticedthattheperformanceissimilartothecasewhenLSisusedforthewholetraining
process).Theparameter
˙
issetto1
:
0.Figure10(b)depictsonepathofthetrainingand
generalizationaccuracyoftheneuralnetstrainedbySGDandLS-SGD,respectively.Itisseen
that,eventhoughthetrainingaccuracyobtainedbySGDishigherthanthatbyLS-SGD,the
generalizationishoweverinferiortothatofLS-SGD.Weconjecturethatthisisduetothe
factthatSGDgetstrappedintosomesharpbutdeeperminimum,whichbetterthana
minimumbutgeneralizesworse.Wecarryout25replicasofthisexperiments,thehistograms
ofthecorrespondingaccuracyareshowninFig.11.
6.3TrainingWassersterinGAN
GenerativeAdversarialNetworks(GANs)[15]arenotoriouslydelicateandunstabletotrain[4].
In[27],Wasserstein-GANs(WGANs)areintroducedtocombattheinstabilityinthetraining
GANs.Inadditiontobeingmorerobustintrainingparametersandnetworkarchitecture,
WGANsprovideareliableestimateoftheEarthMover(EM)metricwhichcorrelateswellwith
thequalityofthegeneratedsamples.Nonetheless,WGANstrainingbecomesunstablewitha
largelearningrateorwhenusedwithamomentumbasedoptimizer[27].Inthissection,we
demonstratethatthegradientsmoothingtechniqueinthispaperalleviatestheinstabilityinthe
training,andimprovesthequalityofgeneratedsamples.SinceWGANswithweightclipping
aretypicallytrainedwithRMSProp[44],weproposereplacingthegradient
g
byasmoothed
14
Figure7:Iterationv.s.lossoftoptimizationalgorithmsinoptimizetheRosenbrock
function.
version
g
˙
=
A

1
˙
g
,andalsoupdatetherunningaveragesusing
g
˙
insteadof
g
.Wenamethis
algorithmLS-RMSProp.
Toaccentuatetheinstabilityintraininganddemonstratetheofgradientsmoothing,
wedeliberatelyusealargelearningratefortrainingthegenerator.Wecomparetheregular
RMSPropwiththeLS-RMSProp.Thelearningrateforthecriticiskeptsmallandtrained
approximatelytoconvergencesothatthecriticlossisstillanectiveapproximationtothe
Wassersteindistance.Tocontrolthenumberofunknownsintheexperimentandmakeamean-
ingfulcomparisonusingthecriticloss,weusetheclassicalRMSPropforthecritic,andonly
applyLS-RMSProptothegenerator.
WetraintheWGANsontheMNISTdatasetusingtheDCGAN[35]forboththecriticand
generator.InFigure12(top),weobservethelossforRMSProptrainedwithalargelearningrate
hasmultiplesharpspikes,indicatinginstabilityinthetrainingprocess.Thesamplesgenerated
arealsolowerinquality,containingnoisyspotsasshowninFigure13(a).Incontrast,thecurve
oftraininglossforLS-RMSPropissmootherandexhibitsfewerspikes.Thegeneratedsamples
asshowninFig.13(b)arealsoofbetterqualityandvisiblylessnoisy.Thegeneratedcharacters
showninFig.13(b)aremorerealisticcomparedtotheonesshowninFig.13(a).Theare
lesspronouncedwithasmalllearningrate,butstillresultinamodestimprovementinsample
qualityasshowninFigure13(c)and(d).WealsoapplyLS-RMSPropfortrainingthecritic,but
donotseeaclearimprovementinthequality.Thismaybebecausethecriticisalreadytrained
nearoptimalityduringeachiteration,anddoesnotbmuchfromgradientsmoothing.
6.4Deepreinforcementlearning
Deepreinforcementlearning(DRL)hasbeenappliedtoplayinggamesincludingCartpole[9],
Atari[30],Go[42,29].DNNplaysavitalroleinapproximatingtheQ-functionorpolicy
function.WeapplytheLaplaciansmoothedgradienttotrainthepolicyfunctiontoplaythe
Cartpolegame.Weapplythestandardproceduretotrainthepolicyfunctionbyusingthe
policygradient[9].Andweusethefollowingnetworktoapproximatethepolicyfunction:
input
4
!
fc
20
!
relu
!
fc
2
!
softmax
:
15
Iteration:300(b)Iteration:600
Iteration:900(d)Iteration:1200
Figure8:SomesnapshotsoftrajectoriesoftoptimizationalgorithmsontheRosenbrock
function.
Figure9:Iterationv.s.objectivevalueforGDwithNesterovmomentumandLSGDwith
Nesterovmomentum.
ThenetworkistrainedbyRMSPropandLS-RMSPropwith
˙
=1
:
0,respectively.Thelearning
rateandotherrelatedparametersaresettobethedefaultonesinPyTorch.Thetrainingis
stoppedoncetheaveragedurationof5consecutiveepisodesismorethan490.Ineachtraining
episode,wesetthemaximalstepstobe500.LeftandrightpanelsofFig.14depictatraining
procedurebyusingRMSPropandLS-RMSProp,respectively.WeseethatLaplaciansmoothed
gradienttakesfewerepisodestoreachthestoppingcriterion.Moreover,weruntheabove
experiments5timesindependently,andapplythetrainedmodeltoplayCartpole.Thegame
lastsmorethan1000stepsforallthe5modelstrainedbyLS-RMSProp,whileonly3ofthem
lastsmorethan1000stepswhenthemodelistrainedbyvanillaRMSProp.
7ConvergenceAnalysis
NotethattheLSmatrix
A

1
˙
ispositiveanditslargestandsmallesteigenvaluesare1
and
1
1+4
˙
,respectively.Itisstraightforwardtoshowthatalltheconvergenceresultsfor(S)GD
stillholdforLS(S)GD.Inthissection,wewillshowsomeadditionalconvergenceforLS(S)GD
16
Figure10:(a).TestingaccuracyofLeNet5trainedbySGD/LS-SGDonMNISTwithvarious
batchsizes.(b).Theevolutionofthepre-activatedResNet56'strainingandgeneralization
accuracybySGDandLS-SGD.(Startfromthe20-thepoch.)
SGDLS-SGDwith
˙
=1
:
0
Figure11:Thehistogramofthegeneralizationaccuracyofthepre-activatedResNet56on
Cifar10trainedbySGDandLS-SGDover25independentexperiments.
withafocusonLSGD,thecorrespondingresultsforLSSGDfollowinasimilarway.
Proposition5.
Considerthealgorithm
w
k
+1
=
w
k


k
(
A
n
˙
)

1
r
f
(
w
k
)
.Suppose
f
is
L
-
Lipschitzsmoothand
0
<
~





<
2
L
.Then
lim
t
!1
kr
f
(
w
k
)
k!
0
.Moreover,ifthe
Hessian
r
2
f
of
f
iscontinuouswith
w

beingtheminimizerof
f
,and


kr
2
f
k
<
1
,then
k
w
k

w

k
A
n
˙
!
0
as
k
!1
,andtheconvergenceislinear.
Proof.
BytheLipschitzcontinuityof
r
f
andthedescentlemma[5],wehave
f
(
w
k
+1
)=
f
(
w
k


k
(
A
n
˙
)

1
r
f
(
w
k
))

f
(
w
k
)


k
hr
f
(
w
k
)
;
(
A
n
˙
)

1
r
f
(
w
k
))
i
+

2
k
L
2
k
(
A
n
˙
)

1
r
f
(
w
k
)
k
2

f
(
w
k
)


k
kr
f
(
w
k
)
k
2
(
A
n
˙
)

1
+

2
k
L
2
kr
f
(
w
k
)
k
2
(
A
n
˙
)

1

f
(
w
k
)

~


1


L
2

kr
f
(
w
k
)
k
2
(
A
n
˙
)

1
:
Summingtheaboveinequalityover
k
,wehave
~


1


L
2

1
X
k
=0
kr
f
(
w
k
)
k
2
(
A
n
˙
)

1

f
(
w
0
)

lim
k
!1
f
(
w
k
)
<
1
:
Therefore,
kr
f
(
w
k
)
k
2
(
A
n
˙
)

1
!
0,andthus
kr
f
(
w
k
)
k!
0.
17
RMSProp
LS-RMSProp,
˙
=3
:
0
Figure12:Criticlosswithlearningrate
lrD
=0
:
0001,
lrG
=0
:
005forRMSProp(top)and
LS-RMSProp(bottom),trainedfor20Kiterations.Weapplyameanofwindowsize13
forbettervisualization.ThelossfromLS-RMSPropisvisiblylessnoisy.
RMSPropLS-RMSProp,
˙
=3
:
0
(a)(b)
RMSPropLS-RMSProp,
˙
=3
:
0
(c)(d)
Figure13:SamplesfromWGANstrainedwithRMSProp(a,c)andLS-RMSProp(b,d).The
learningrateissetto
lrD
=0
:
0001,
lrG
=0
:
005forbothRMSPropandLS-RMSPropin(a)
and(b).And
lrD
=0
:
0001,
lrG
=0
:
0001areusedforbothRMSPropandLS-RMSPropin(c)
and(d).Thecriticistrainedfor5iterationsperstepofthegenerator,and200iterationsper
every500stepsofthegenerator.
Forthesecondclaim,wehave
w
k
+1

w

=
w
k

w



k
(
A
n
˙
)

1
(
r
f
(
w
k
)
r
f
(
w

))
=
w
k

w



k
(
A
n
˙
)

1

Z
1
0
r
2
f
(
w

+
˝
(
w
k
+1

w

))

(
w
k

w

)d
˝

=
w
k

w



k
(
A
n
˙
)

1

Z
1
0
r
2
f
(
w

+
˝
(
w
k
+1

w

))d
˝

(
w
k

w

)

=(
A
n
˙
)

1
2

I


k
(
A
n
˙
)

1
2
Z
1
0
r
2
f
(
w

+
˝
(
w
k
+1

w

))d
˝
(
A
n
˙
)

1
2
)

(
A
n
˙
)
1
2
(
w
k

w

)
18
RMSPropLS-RMSProp,
sigma
=1
:
0
Figure14:Durationsofthecartpolegameinthetrainingprocedure.Leftandrightaretraining
procedurebyRMSPropandLS-RMSPropwith
˙
=1
:
0,respectively.
Therefore,
k
w
k
+1

w

k
A
n
˙





I


t
(
A
n
˙
)

1
2
Z
1
0
r
2
f
(
w

+
˝
(
w
k
+1

w

))d
˝
(
A
n
˙
)

1
2




k
w
k

w

k
A
n
˙
:
Soif

k
kr
2
f
k
1
k
(
A
n
˙
)

1
k
=1,theresultfollows.
Remark6.
TheconvergenceresultinProposition5isalsocall
H
n
˙
-convergence.Thisisbecause
h
u
;
A
n
˙
u
i
=
k
u
k
2
+
˙
k
D
n
+
u
k
2
=
k
u
k
2
H
n
˙
.
8DiscussionandConclusion
8.1SomemorepropertiesofLaplaciansmoothing
InTheorem8,weestablishedahighprobabilityestimateoftheLSoperatorinreducingthe
`
2
normofanygivenvector.The
`
1
typeofhighprobabilityestimationcanbeestablishedinthe
sameway.Theseestimateswillbehelpfultodevelopprivacy-preservingoptimizationalgorithms
totrainMLmodelsthatimprovetheutilityofthetrainedmodelswithouttheprivacy
guarantee[45].
Regardingthe
`
1
/
`
2
estimatesoftheLSoperator,wefurtherhavethefollowingresults.
Proposition8.
Givenvectors
g
and
d
=
A

1
˙
g
,forany
p
2
N
,itholdsthat
k
D
p
+
d
k
1

k
D
p
+
g
k
1
:
Theinequalityisstrictunless
D
p
+
g
isaconstantvector.
Proof.
Observethat
A
˙
and
D
+
commute;therefore,forany
p
2
N
,
A
˙
(
D
p
+
d
)=
D
p
+
g
.Thus
wehave
(1+2
˙
)(
D
p
+
d
)
i
=(
D
p
+
g
)
i
+
˙
(
D
p
+
d
)
i
+1
+
˙
(
D
p
+
d
)
i

1
:
So
(1+2
˙
)
j
(
D
p
+
d
)
i
jj
(
D
p
+
g
)
i
j
+
˙
j
(
D
p
+
d
)
i
+1
j
+
˙
j
(
D
p
+
d
)
i

1
j
:
Theinequalityisstrictiftherearesignchangesamongthe(
D
p
+
d
)
i

1
,(
D
p
+
d
)
i
,(
D
p
+
d
)
i
+1
.
Summingover
i
andusingperiodicity,wehave
(1+2
˙
)
m
X
i
=1
j
(
D
p
+
d
)
i
j
m
X
i
=1
j
(
D
p
+
g
)
i
j
+2
˙
m
X
i
=1
j
(
D
p
+
d
)
i
j
;
andtheresultfollows.Theinequalityisstrictunless
D
p
+
g
isaconstantvector.
Proposition6.
Givenanyvector
g
2
R
m
and
d
=(
A
n
˙
)

1
g
,then
k
g
k
2
=
k
d
k
2
+2
˙
k
D
n
+
d
k
2
+
˙
2
k
L
n
d
k
2
;
(19)
thevarianceof
d
ismuchlessthanthatof
g
.
19
Proof.
Observethat
g
=
A
n
˙
d
=
d
+(

1)
n
˙
L
n
d
.Therefore,
k
g
k
2
=
h
d
+(

1)
n
˙
L
n
d
;
d
+(

1)
n
˙
L
n
d
i
=
k
d
k
2
+2(

1)
n
˙
h
d
;
L
n
d
i
+
˙
2
k
L
n
d
k
2
:
(20)
Next,note
D

and
D
+
arecommute;thus
L
n
=(
D

D
+
)

(
D

D
+
)
|
{z
}
n
=
D


D

|
{z
}
n
D
+

D
+
|
{z
}
n
=
D
n

D
n
+
:
(21)
Now,wehave
h
d
;
L
n
d
i
=
h
d
;
D
n

D
n
+
d
i
=
h
(
D
n

)
T
d
;
D
n
+
d
i
=
h
(

1)
n
D
n
+
d
;
D
n
+
d
i
=(

1)
n
k
D
n
+
d
k
2
;
(22)
whereweusedEq.(21)intheequalityand
D

=

D
T
+
inthesecondtolastequality.
SubstitutingEq.(22)intoEq.(20),yieldsEq.(19).
8.2ConnectiontoHamilton-JacobiPDEs
ThemotivationfortheproposedLS-SGDcomesfromtheHamilton-JacobiPDE(HJ-PDE).
ConsiderthefollowingunusualHJ-PDEwiththeempiricalriskfunction,
f
(
w
)
;
asinitialcon-
dition
(
u
t
+
1
2

r
w
u;
A

1
˙
r
w
u

=0
;
(
w
;t
)
2


[0
;
1
)
u
(
w
;
0)=
f
(
w
)
;
w
2

(23)
BytheHopf-Laxformula[14],theuniqueviscositysolutiontoEq.(23)isrepresentedby
u
(
w
;t
)=inf
v
n
f
(
v
)+
1
2
t

v

w
;
A
˙
(
v

w
)

o
:
Thisviscositysolution
u
(
w
;t
)makes
f
(
w
)""moreconvex"",anintuitiveandtheo-
reticalexplanationof""moreconvex""canbefoundin[10],bybringingdownthelocalmaxima
whileretainingandwideninglocalminima.AnillustrationofthisisshowninFig.15.Ifwe
performthesmoothingGDwithproperstepsizeonthefunction
u
(
w
;t
),itiseasiertoreach
theglobaloratleastaminimaoftheoriginalnonconvexfunction
f
(
w
).
Figure15:
f
(
w
)=
k
w
k
2

1+
1
2
sin(2
ˇ
k
w
k
)

ismademoreconvexbysolvingEq.(23).Theplot
showsthecrosssectionofthe5Dproblemwith
˙
=1andt
t
values.
20
Proposition1.
Suppose
f
(
w
)
isentiable,theLS-GDon
u
(
w
;t
)
w
k
+1
=
w
k

t
A

1
˙
r
w
u
(
w
k
;t
)
isequivalenttothesmoothingimplicitGDon
f
(
w
)
w
k
+1
=
w
k

t
A

1
˙
r
f
(
w
k
+1
)
:
(24)
Proof.
We
z
(
w
;
v
;t
):=
f
(
v
)+
1
2
t
h
v

w
;
A
˙
(
v

w
)
i
;
andrewrite
u
(
w
;t
)=inf
v
z
(
w
;
v
;t
)as
z
(
w
;
v
(
w
;t
)
;t
),where
v
(
w
;t
)=argmin
v
z
(
w
;
v
;t
).
ThenbytheEuler-Lagrangeequation,
r
w
u
(
w
;t
)=
r
w
z
(
w
;
v
(
w
;t
)
;t
)=
J
w
v
(
w
;t
)
r
v
z
(
w
;
v
(
w
;t
)
;t
)+
r
w
z
(
w
;
v
(
w
;t
)
;t
)
;
where
J
w
v
(
w
;t
)istheJacobianmatrixof
v
w.r.t.
w
.Noticethat
r
v
z
(
w
;
v
(
w
;t
)
;t
)=
0
,
r
w
u
(
w
;t
)=
r
w
z
(
w
;
v
(
w
;t
)
;t
)=

1
t
A
˙
(
v
(
w
;t
)

w
)
:
Letting
w
=
w
k
and
w
k
+1
=
v
(
w
k
;t
)=argmin
v
z
(
w
k
;
v
;t
)intheaboveequalities,wehave
r
w
u
(
w
k
;t
)=

1
t
A
˙
(
w
k
+1

w
k
)
:
Insummary,thegradientdescent
w
k
+1
=
w
k

t
A

1
˙
r
w
u
(
w
k
;t
)isequivalenttotheproximal
pointiteration
w
k
+1
=argmin
v
f
(
v
)+
1
2
t
h
v

w
k
;
A
˙
(
v

w
k
)
i
,whichyields
w
k
+1
=
w
k

t
A

1
˙
r
f
(
w
k
+1
).
ThestudiedLS-GDalgorithmisanexplicitrelaxationoftheimplicitalgorithminEq.(24).
8.3Conclusion
MotivatedbythetheoryofHamilton-Jacobipartialtialequations,weproposedLapla-
ciansmoothinggradientdescentanditshighordergeneralizations.Thissimplemo
dramaticallyreducesthevarianceandoptimalitygapinstochasticgradientdescent,allowsusto
takealargerstepsize,andhelpstobetterminima.Extensivenumericalexamplesranging
fromtoycasesandshallowanddeepneuralnetstogenerativeadversarialnetworksanddeepre-
inforcementlearning,alldemonstratetheadvantageoftheproposedsmoothedgradient.Several
issuesremain,inparticulardevisinganadaptivemethodforchoosingthesmoothing
parameter
˙
insteadofusingavalue.
9Appendix
9.1ProofofTheorem1
Inthispart,wewillgiveaproofforTheorem1.
Lemma2.
[1]Let
t;u>
0
,
v
bean
m
-dimensionalstandardnormalrandomvector,andlet
F
:
R
m
!
R
beafunctionsuchthat
k
F
(
x
)

F
(
y
)
kk
x

y
k
forall
x
,
y
2
R
m
.Then
P
(
F
(
v
)

E
F
(
v
)+
u
)

exp
 

tu
+
1
2

ˇt
2

2
!
:
(25)
Taking
t
=
4
ˇ
2
inLemma2,weobtain
Lemma3.
Let
u>
0
,
v
bean
m
-dimensionalstandardnormalrandomvector,andlet
F
:
R
m
!
R
beafunctionsuchthat
k
F
(
x
)

F
(
y
)
kk
x

y
k
forall
x
,
y
2
R
m
.Then
P
(
F
(
v
)

E
F
(
v
)+
u
)

exp


2
ˇ
2
u
2

:
(26)
21
Lemma4.
Let
v
bean
m
-dimensionalstandardnormalrandomvector.Let
1

p
1
.Let
0
<u<
E
k
v
k
`
p
.Let
T
2
R
m

m
besuchthat
k
Tx
k
`
p
k
x
k
`
p
forall
x
2
R
m
.Then
P

k
Tv
k
`
p

E
k
Tv
k
`
p
+
u
E
k
v
k
`
p

u
k
v
k
`
p


2exp


2
ˇ
2
u
2

:
Proof.
ByLemma3,
P
(
k
Tv
k
`
p

E
k
Tv
k
`
p
+
u
)

e

2
ˇ
2
u
2
and
P
(

v
k
`
p

E
k
v
k
`
p
+
u
)

e

2
ˇ
2
u
2
:
Thesecondinequalitygives
P
(
k
v
k
`
p

E
k
v
k
`
p

u
)

e

2
ˇ
2
u
2
:
Therefore,
P

k
Tv
k
`
p

E
k
Tv
k
`
p
+
u
E
k
v
k
`
p

u
k
v
k
`
p


P
(
k
Tv
k
`
p

E
k
Tv
k
`
p
+
u
)+
P
(
k
v
k
`
p

E
k
v
k
`
p

u
)

2
e

2
ˇ
2
u
2
:
Lemma5.
Let
1

p

2
.Let
T
2
R
m

m
.Let
v
bean
m
-dimensionalstandardnormal
randomvector.Then
E
k
Tv
k
`
p

m
1
p

1
2
(Trace
T

T
)
1
2
(
E
j
v
1
j
p
)
1
p
;
where
v
1
isthecoordinateof
v
.
Proof.
Wewrite
T
=(
T
i;j
)
1

i;j

n
.Then
E
k
Tv
k
`
p
=
E
0
@
n
X
i
=1






n
X
j
=1
T
i;j
v
j






p
1
A
1
p

0
@
n
X
i
=1
E






n
X
j
=1
T
i;j
v
j






p
1
A
1
p
=
0
B
@
n
X
i
=1
0
@
n
X
j
=1
T
2
i;j
1
A
p
2
E
j
v
1
j
p
1
C
A
1
p

0
B
@
n
1

p
2
0
@
X
1

i;j

n
T
2
i;j
1
A
p
2
E
j
v
1
j
p
1
C
A
1
p
=
n
1
p

1
2
(Trace
T

T
)
1
2
(
E
j
v
1
j
p
)
1
p
;
wherethesecondequalityfollowsfromtheassumptionthat
v
isan
m
-dimensionalstandard
normalrandomvector.
Lemma6.
Let
v
bean
m
-dimensionalstandardnormalrandomvector.Then
E
k
v
k
`
2

p
m

ˇ:
Proof.
ByLemma3,
P
(
k
v
k
`
2

E
k
v
k
`
2
+
u
)

e

2
ˇ
2
u
2
and
P
(

v
k
`
2

E
k
v
k
`
2
+
u
)

e

2
ˇ
2
u
2
:
22
Thus,
P
(
jk
v
k
`
2

E
k
v
k
`
2
j
u
)

2
e

2
ˇ
2
u
2
:
Considertherandomvariable
W
=
k
v
k
`
2
.Wehave
E
j
W

E
W
j
2
=
Z
1
0
P
(
j
W

E
W
j
p
u
)
du

Z
1
0
2
e

2
ˇ
2
u
du
=
ˇ
2
:
Since
E
j
W

E
W
j
2
=
E
W
2

(
E
W
)
2
,wehave
E
W

(
E
W
2
)
1
2

(
E
j
W

E
W
j
2
)
1
2

p
m

ˇ:
Lemma7.
Let
0
<<
1

ˇ
p
m
.Let
˙>
0
.Let

=
1
m
m
X
i
=1
1
1+2
˙

˙z
i

˙
z
i
;
where
z
1
;:::;z
m
arethe
m
rootsofunity.Let
B
bethecircularshiftoperatoron
R
m
.Let
v
be
an
m
-dimensionalstandardnormalrandomvector.Then
P
 
k
((1+2
˙
)
I

˙
B

˙
B

)

1
=
2
v
k
`
2

p

+

1

ˇ
p
m


k
v
k
`
2
!

2
e

2
ˇ
2

2
:
Proof.
Let
T
=((1+2
˙
)
I

˙
B

˙
B

)

1
=
2
.Taking
u
=
p

inLemma4,wehave
P

k
Tv
k
`
2

E
k
Tv
k
`
2
+
p

E
k
v
k
`
2

p

k
v
k
l
2


2
e

2
ˇ
2

2
:
ByLemma5,
E
k
Tv
k
`
2

(Trace
T

T
)
1
2
.wehaveTrace
T

T
=

.Itiseasytoshowthat
Trace
T

T
)=

So
E
k
Tv
k
`
2

p

.AlsobyLemma6,
E
k
v
k
`
2

p
m

ˇ
.Therefore,
P
 
k
((1+2
˙
)
I

˙
B

˙
B

)

1
v
k
`
2

p

+

1

ˇ
p
m


k
v
k
`
2
!

2
e

2
ˇ
2

2
:
ProofofTheorem1.
Theorem1followsfromLemma7bysubstituting
v
k
v
k
`
2
andusinghomo-
geneityanddirectcalculations.
9.2ProofofTheorem2
Inthispart,wewillgiveaproofforTheorem2.
Lemma8
([6])
.
Let
˚
w
denotesweakmajorization.DenoteeigenvaluesofHermitianmatrix
X
,by

1
(
X
)

:::


m
(
X
)
.ForeverytwoHermitianpositivematrices
A
and
B
,we
have
(

1
(
AB
)
;

;
m
(
AB
))
˚
w
(

1
(
A
)

1
(
B
)
;

;
m
(
A
)

m
(
B
))
:
Inparticular,
m
X
j
=1

j
(
AB
)

m
X
j
=1

j
(
A
)

j
(
B
)
:
proofofTheorem2.
Let

1

:::


m
denotetheeigenvaluesofTheeigenvaluesof(
A
n
˙
)

2
aregivenby
f
[1+4
n
˙
sin
2
n
(
ˇj=m
)]

2
g
j
=
m

1
j
=0
,whichwedenoteby1=

1

:::


m

(1+4
n
˙
)

2
.Wehave
m
X
j
=1
Var[
n
j
]=trace=
m
X
j
=1

j
:
(27)
23
Ontheotherhandwealsohave
m
X
j
=1
Var[(
A
n
˙
)

1
n
j
]=trace((
A
n
˙
)

1

A
n
˙
)

1
)=trace((
A
n
˙
)

2


m
X
j
=1

j

j
;
(28)
wherethelastinequalityisbylemma8.Now,
m
X
j
=1

j

m
X
j
=1

j

j
=
m
X
j
=1
(1


j
)

j


m
(
m

m
X
j
=1

j
)
=

1

(
m

m
X
j
=1

j
)

P
m
j
=1

j

(
m

m
X
j
=1

j
)
Rearrangingandsimplifyingaboveimpliesthat
m
X
j
=1

j

j

(
m
X
j
=1

j
)(1

1

+
P
m
j
=1

j

)
:
SubstitutingEq.(27)andEq.(28)intheaboveinequality,yieldsEq.(12).
9.3ProofofLemma1
ToproofLemma1,weintroducethefollowinglemma.
Lemma9.
For
0



2
ˇ
,suppose
F
(

)=
1
1+2
˙
(1

cos(

))
;
hasthediscrete-timeFouriertransformofseries
f
[
k
]
.Then,forinteger
k
,
f
[
k
]=

j
k
j
p
4
˙
+1
where

=
2
˙
+1

p
4
˙
+1
2
˙
Proof.
By
f
[
k
]=
1
2
ˇ
Z
2
ˇ
0
F
(

)
e
ik

=
1
2
ˇ
Z
2
ˇ
0
e
ik
1+2
˙
(1

cos(

))
:
(29)
ComputingEq.(29)usingResidueTheoremisawell-knowntechniqueincomplexanalysis.
First,notethatbecause
F
(

)isrealvalued,
f
[
k
]=
f
[

k
];therefore,ittocompute
Eq.(29)fornonnegative
k
.Set
z
=
e

.Observethatcos(

)=0
:
5(
z
+1
=z
)and
dz
=
iz
.
SubstitutinginEq.(29)andsimplifyingyieldsthat
f
[
k
]=

1
2
ˇi˙
I
z
k
(
z



)(
z


+
)
dz;
(30)
wheretheintegralistakenaroundtheunitcircle,and


=
2
˙
+1

p
4
˙
+1
2
˙
aretherootsof
quadratic

˙z
2
+(2
˙
+1)
z

˙
.Notethat


lieswithintheunitcircle;whereas,

+
lies
outsideoftheunitcircle.Therefore,because
k
isnonnegative,


istheonlysingularityof
theintegrandinEq.(30)withintheunitcircle.AstraightforwardapplicationoftheResidue
Theoremyieldsthat
f
[
k
]=


k

˙
(




+
)
=

k
p
4
˙
+1
:
Thiscompletestheproof.
24
Next,wegiveaproofforLemma1.
ProofofLemma1.
Firstobservethatwecanre-writethelefthandsideofEq.(9)as
1
m
m

1
X
j
=0
1
1+2
˙
(1

cos(
2
ˇj
m
))
:
(31)
ItremainstoshowthattheabovesummationisequaltotherighthandsideofEq.(9).This
followsbylemmas9andstandardsamplingresultsinFourieranalysis(i.e.sampling

atpoints
f
2
ˇj=m
g
m

1
j
=0
).Nevertheless,weprovidethedetailshereforcompleteness:Observethatthat
theinversediscrete-timeFouriertransformof
G
(

)=
m

1
X
j
=0

(


2
ˇj
m
)
:
isgivenby
g
[
k
]=
(
m=
2
ˇ
if
k
divides
m
,
0otherwise.
Furthermore,let
F
(

)=
1
1+2
˙
(1

cos(

))
;
anduse
f
[
k
]todenoteitsinversediscrete-timeFouriertransform.Now,
1
m
m

1
X
j
=0
1
1+2
˙
(1

cos(
2
ˇj
m
))
=
1
m
Z
2
ˇ
0
F
(

)
G
(

)
=
2
ˇ
m
DTFT

1
[
F

G
][0]
=
2
ˇ
m
(DTFT

1
[
F
]

DTFT

1
[
G
])[0]
=
2
ˇ
m
1
X
r
=

f
[

r
]
g
[
r
]
=
2
ˇ
m
1
X
`
=

f
[

`m
]
m
2
ˇ
=
1
X
`
=

f
[

`m
]
:
Theproofiscompletedbysubstitutingtheresultoflemma9intheabovesumandsimplifying.
25
Acknowledgments
ThismaterialisbasedonresearchsponsoredbytheAirForceResearchLaboratoryunder
grantnumbersFA9550-18-0167andMURIFA9550-18-1-0502,theofNavalResearch
undergrantnumberN00014-18-1-2527,theU.S.DepartmentofEnergyundergrantnumber
DOESC0013838,andbytheNationalScienceFoundationundergrantnumberDMS-1554564,
(STROBE).WewouldliketothankJialinLiuandprofessorsPratikChaudhari,AdamOberman
andMingYanforstimulatingdiscussions.
References
[1]
254a,notes1:Concentrationofmeasure.
https://terrytao.wordpress.com/2010/01/
03/254a-notes-1-concentration-of-measure/
.
[2]
M.Abadi,A.Agarwal,andetal.Tw:Large-scalemachinelearningonheteroge-
neousdistributedsystems.
arXivpreprintarXiv:1603.04467
,2016.
[3]
Z.Allen-Zhu.Katyusha:Thedirectaccelerationofstochasticgradientmethods.
JournalofMachineLearningResearch
,18:1{51,2018.
[4]
M.ArjovskyandL.Bottou.Towardsprincipledmethodsfortraininggenerativeadversarial
networks.
arXivpreprintarXiv:1701.04862
,2017.
[5]
D.P.Bertsekas.
Nonlinearprogramming
.AthenascienBelmont,1999.
[6]
R.Bhatia.
MatrixAnalysis
.Springer,1997.
[7]
L.Bottou.Stochasticgradientdescenttricks.
NeuralNetworks,TricksoftheTrade,
Reloaded
,7700,2012.
[8]
L.Bottou,E.F.Curtis,andJ.Nocedal.Optimizationmethodsforlarge-scalemachine
learning.
SIAMReview
,60(2):223{311,2018.
[9]
G.Brockman,V.Cheung,L.Pettersson,J.Schneider,J.Schulman,J.Tang,and
W.Zaremba.Openaigym.
arXivpreprintarXiv:1606.01540
,2016.
[10]
P.Chaudhari,A.Oberman,S.Osher,S.Soatto,andC.Guillame.Deeprelaxation:partial
tialequationsforoptimizingdeepneuralnetworks.
arXivpreprintarXiv:1704.04932
,
2017.
[11]
A.DefazioandF.Bach.Saga:Afastincrementalgradientmethodwithsupportfor
non-stronglyconvexcompositeobjectives.In
AdvancesinNeuralInformationProcessing
Systems
,2014.
[12]
T.Dozat.Incorporatingnesterovmomentumintoadam.In
4thInternationalConference
onLearningRepresentationWorkshop(ICLR2016)
,2016.
[13]
J.Duchi,E.Hazan,andY.Singer.Adaptivesubgradientmethodsforonlinelearningand
stochasticoptimization.
JournalofMachineLearningResearch
,12:2121{2159,2011.
[14]
L.C.Evans.Partialtialequations.2010.
[15]
I.J.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley,S.Ozair,A.C.
Courville,andY.Bengio.Generativeadversarialnets.In
AdvancesinNeuralInformation
ProcessingSystems
,pages2672{2680,2014.
[16]
M.Hardt,B.Recht,andY.Singer.Trainfaster,generalizebetter:Stabilityofstochastic
gradientdescent.In
33rdInternationalCOnferenceonMachineLearning(ICML2016)
,
2016.
[17]
K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearningforimagerecognition.In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition
,pages770{
778,2016.
26
[18]
S.Jastrzebski,Z.Kenton,N.Ballas,A.Fischer,Y.Bengio,andA.Storkey.Dnn'ssharpest
directionsalongthesgdtrajectory.
arXivpreprintarXiv:1807.05031
,2018.
[19]
R.JohosonandT.Zhang.Acceleratingstochasticgradientdescentusingpredictivevariance
reduction.In
AdvancesinNeuralInformationProcessingSystems
,2013.
[20]
M.Jung,G.Chung,G.Sundaramoorthi,L.Vese,andA.Yuille.Sobolevgradientsand
jointvariationalimagesegmentation,denoising,anddeblurring.In
ComputationalImaging
VII
,volume7246,page72460I.InternationalSocietyforOpticsandPhotonics,2009.
[21]
D.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization.
arXivpreprint
arXiv:1412.6980
,2014.
[22]
A.Krizhevsky.Learningmultiplelayersoffeaturesfromtinyimages.2009.
[23]
Y.LeCun,L.Bottou,Y.Bengio,andP.Gradient-basedlearningappliedto
documentrecognition.
ProceedingsoftheIEEE
,81:2278{2324,1998.
[24]
L.Lei,C.Ju,J.Chen,andM.Jordan.Nonconvexoptimizationviascsgmethods.
In
AdvancesinNeuralInformationProcessingSystems
,2017.
[25]
F.Liandetal.Cs231n:Convolutionalneuralnetworksforvisualrecognition.2018.
[26]
H.Li,Z.Xu,G.Taylor,andT.Goldstein.Visualizingthelosslandscapeofneuralnets.
arXivpreprintarXiv:1712.09913
,2017.
[27]
S.ChintalaM.ArjovskyandL.Bottou.Wassersteingan.
arXivpreprintarXiv:1701.07875
,
2017.
[28]
S.Mandt,M.andD.Blei.Stochasticgradientdescentasapproximatebayesian
inference.
JournalofMachineLearningResearch
,18:1{35,2017.
[29]
Mnihandetal.Human-levelcontrolthroughdeepreinforcementlearning.
Nature
,518:529{
533,2015.
[30]
V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,D.Wierstra,andM.Ried-
miller.PlayingAtariwithdeepreinforcementlearning.
arXivpreprintarXiv:1312.5602
,
2013.
[31]
Y.Nesterov.Amethodforsolvingtheconvexprogrammingproblemwithconvergencerate
o
(1
=k
2
).
Dokl.akad.naukSssr
,269:543{547,1983.
[32]
Y.Nesterov.Introductorylecturesonconvexprogrammingvolumei:Basiccourse.
Lecture
Notes
,1998.
[33]
A.Paszke,S.Gross,S.Chintala,G.Chanan,E.Yang,Z.DeVito,Z.Lin,A.Desmaison,
L.Antiga,andA.Lerer.Automatictiationinpytorch.2017.
[34]
N.Qian.Onthemomentumtermingradientdescentlearningalgorithms.
NeuralNetworks
:TheJournaloftheInternationalNeuralNetworkSociety
,12(1):145{151,1999.
[35]
A.Radford,L.Metz,andS.Chintala.Unsupervisedrepresentationlearningwithdeep
convolutionalgenerativeadversarialnetworks.
arXivpreprintarXiv:1511.06434
,2015.
[36]
S.Reddi,S.Kale,andS.Kumar.Ontheconvergenceofadamandbeyond.In
6th
InternationalConferenceonLearningRepresentation(ICLR2018)
,2018.
[37]
H.RobindsandS.Monro.Astochasticapproximationmethod.
AnnalsofMathematical
Statistics
,22:400{407,1951.
[38]
JSchmidhuber.Deeplearninginneuralnetworks:Anoverview.
arXivpreprint
arXiv:1404.7828
,2014.
[39]
A.Senior,G.Heigold,M.Ranzato,andK.Yang.Anempiricalstudyoflearningrates
indeepneuralnetworksforspeechrecognition.In
IEEEInternationalConferenceon
Acoustics,SpeechandSignalProcessing
,2013.
27
[40]
O.ShamirandT.Zhang.Stochasticgradientdescentfornon-smoothoptimization:Conver-
genceresultsandoptimalaveragingschemes.In
30thInternationalConferenceonMachine
Learning(ICML2013)
,2013.
[41]
A.ShapiroandY.Wardi.Convergenceanalysisofgradientdescentstochasticalgorithms.
JournalofOptimizationTheoryandApplications
,91(2):439{454,1996.
[42]
D.Silverandetal.Masteringthegameofgowithdeepneuralnetworksandtreesearch.
Nature
,529:484{489,2016.
[43]
R.Sutton.Twoproblemswithbackpropagationandothersteepest-descentlearningpro-
ceduresfornetworks.In
Proc.8thAnnualConf.CognitiveScienceSociety
,1986.
[44]
T.TielemanandG.Hinton.Lecture6.5-rmsprop:Dividethegradientbyarunningaverage
ofitsrecentmagnitude.
COURSERA:Neuralnetworksformachinelearning
,4(2):26{31,
2012.
[45]
B.Wang,Q.Gu,M.Boedihardjo,F.Barekat,andS.Osher.Privacy-preservingerm
bylaplaciansmoothingstochasticgradientdescent.
UCLAComputationalandApplied
MathematicsReports
,19-24,2019.
[46]
M.WellingandY.Teh.Bayesianlearningviastochasticgradientlangevindynamics.In
28thInternationalConferenceonMachineLearning(ICML2011)
,2011.
[47]
Y.WuandK.He.Groupnormalization.In
EuropeanConferenceonComputerVision
,
2018.
[48]
M.Zeiler.Adadelta:Anadaptivelearningratemethod.
arXivpreprintarXiv:1212.5701
,
2012.
28
"
36,Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models,http://arxiv.org/pdf/1803.05554v3.pdf,https://github.com/miraep8/Minimal_IMAP_MCMC,"MinimalI-MAPMCMC
forScalableStructureDiscoveryinCausalDAGModels
RajAgrawal
123
TamaraBroderick
12
CarolineUhler
23
Abstract
LearningaBayesiannetwork(BN)fromdata
canbeusefulfordecision-makingordiscovering
causalrelationships.However,traditionalmeth-
odsoftenfailinmodernapplications,whichex-
hibitalargernumberofobservedvariablesthan
datapoints.Theresultinguncertaintyaboutthe
underlyingnetworkaswellasthedesiretoincor-
poratepriorinformationrecommendaBayesian
approachtolearningtheBN,butthehighlycom-
binatorialstructureofBNsposesastrikingchal-
lengeforinference.Thecurrentstate-of-the-art
methodssuchasorderMCMCarefasterthan
previousmethodsbutpreventtheuseofmany
naturalstructuralpriorsandstillhaverunning
timeexponentialinthemaximumindegreeofthe
truedirectedacyclicgraph(DAG)oftheBN.We
hereproposeanalternativeposteriorapproxima-
tionbasedontheobservationthat,ifweincor-
porateempiricalconditionalindependencetests,
wecanfocusonahigh-probabilityDAGassoci-
atedwitheachorderofthevertices.Weshow
thatourmethodallowsthedesiredxibilityin
priorremovestimingdependence
onthemaximumindegree,andyieldsprovably
goodposteriorapproximations;inaddition,we
showthatitachievessuperioraccuracy,scalabil-
ity,andsamplermixingonseveraldatasets.
1.Introduction
Bayesiannetworks
(BNs)Šorprobabilisticgraphicalmod-
elsbasedondirectedacyclicgraphs(DAGs)Šformapow-
erfulframeworkforrepresentingcomplexdependencies
amongrandomvariables.LearningBNsamongobserved
variablesfromdatahasprovenusefulindecisiontasksŠ
1
ComputerScienceandIntelligenceLaboratory
2
InstituteforData,SystemsandSociety
3
LaboratoryforInfor-
mationandDecisionSystems,MassachusettsInstituteofTechnol-
ogy.Correspondenceto:RajAgrawal
<
r.agrawal@csail.mit.edu,
r.agrawal.raj@gmail.com
>
.
suchascreditassessmentorautomatedmedicaldiagnosisŠ
aswellasdiscoverytasksŠsuchaslearninggeneregulatory
networks(
Spirtesetal.
,
2000
;
Friedmanetal.
,
2000
;
Pearl
,
2009
;
Robinsetal.
,
2000
;
Khashei&Mirahmadi
,
2015
).
Whenthenumberofdatapointsismuchlargerthanthe
numberofobservedvariables,apointestimateoftheBN
canbefoundusingconstraint-basedorgreedysearchmeth-
ods(
Spirtesetal.
,
2000
;
Chickering
,
2002
;
Tsamardinos
etal.
,
2006
).However,inmanyapplications,thenumberof
observedvariablesis
larger
thanthenumberofdatapoints.
Inthiscase,manyBNsmayagreewiththeobserveddata.
ABayesianapproachoffersanaturalweightingscheme
acrossBNsviatheBayesianposteriordistribution.This
weightingpropagatescoherentlytopointestimatesandun-
certaintyforstructuralfeaturesoftheBN
(suchasthepresenceofcertaindirectededges).More-
over,aBayesianapproachallowstheincorporationofprior
information,whichiscommoninapplicationsofinterest
(
Mukherjee&Speed
,
2008
).
Unfortunately,duetothecombinatorialexplosionofthe
numberofDAGs,exactposteriorcomputationisintractable
forgraphswithmorethanthirtynodes(
Koivisto&Sood
,
2004
;
Tian&He
,
2009
).Thismotivated
Madigan&York
(
1995
)topropose
structureMCMC
,anapproximatemethod
usingMarkovchainMonteCarlo(MCMC).Toovercome
theslowmixingofstructureMCMCanditsvariants(
Grze-
gorczyk&Husmeier
,
2008
),
Friedman&Koller
(
2003
)
introduced
orderMCMC
.Thisalgorithmachieves
cantlyfastermixingbyrunningaMarkovchainnotover
DAGs,butinthereducedspaceofpermutations(i.e.,or-
ders)oftheverticesoftheDAG.However,orderMCMC
requiresaparticular(oftenundesirable)formfortheprior
onDAGs,anditsiterationssufferfromexponentialtime
andmemorycomplexityinthemaximumindegreeofthe
trueDAG.Heuristicesforscalabilityexist(
Friedman&
Koller
,
2003
),buttheirstatisticalcostsareunclear.
Inthispaper,weproposeanewmethodtoleveragetheim-
provedmixingofMCMCmovesinthepermutationspace;
inaddition,ourapproachcomeswiththeoreticalguarantees
onapproximationqualityandallowsmorerealisticDAG
priors.Thekeynewingredientisanobservationby
Verma
&Pearl
(
1992
)thathasbeenusedforcausalinferencein
arXiv:1803.05554v3  [stat.CO]  24 Jun 2018MinimalI-MAPMCMC
thefrequentistsetting(
Mohammadietal.
,
2018
;
Raskutti
&Uhler
,
2013
);namely,ifwehaveaccesstoconditional
independence(CI)tests,wecanassociateeachpermutation
withauniqueDAGknownasthe
minimalI-MAP
(indepen-
dencemap).ThisisthesparsestDAGthatisconsistentwith
agivenpermutationandMarkovtoagivensetofCIrela-
tions.Weprovethatthevastmajorityofposteriormassis
concentratedonthecorrespondingreducedspaceofDAGs,
andwecallourmethod
minimalI-MAPMCMC
.
WestartinSection
2
byreviewingBNsandBayesianlearn-
ingofBNs.Weshowhowtoreducetothespaceofminimal
I-MAPSinSection
3
andtheoreticallyboundtheposterior
approximationerrorinducedbythisreduction.InSection
4
,
weshowbyan
empiricalBayes
argumentthatsufciently
accurateCItestsallowusingwhatamountstoouroriginal
priorandlikelihoodonDAGsbut,crucially,restrictedto
thespaceofminimalI-MAPs.Thus,wedemonstratethat
ourmethodallowsarbitrarypriorstructuralinformation.In
Section
5
,wepresentanMCMCapproachforsamplingac-
cordingtothisminimalI-MAPmodelandprovideintuition
forwhyitexhibitsgoodmixingproperties.Moreover,we
provethat,for
p
thenumberofobservedvariablesand
k
themaximumindegreeofthetrueDAG,ourmethodtakes
O
(
p
2
)
memoryand
O
(
p
4
)
timeperMCMCiteration(vs.
O
(
p
k
+1
)
timeandmemoryfororderMCMC).InSection
6
weempiricallycompareourmodeltoorderMCMCand
partitionMCMC
(
Kuipers&Moffa
,
2017
),thestate-of-the-
artversionofstructureMCMC.Inexperimentsweobserve
O
(
p
3
)
timescalingforourmethod,andwedemonstratebet-
termixingandROCperformanceforourmethodonseveral
datasets.
2.PreliminariesandRelatedWork
2.1.BayesianNetworks
Let
G
=([
p
]
;A
)
bea
directedacyclicgraph
(DAG)consist-
ingofacollectionofvertices
[
p
]
:
=
f
1
;:::;p
g
andacol-
lectionofarrows(i.e.,directededges)
A
,where
(
i;j
)
2
A
representsthearrow
i
!
j
.ADAGinducesa
partialorder
-
onthevertices
[
p
]
through
(
i;j
)
2
A
ifandonlyif
i
-
j
.
Let
S
p
bethesymmetricgroupoforder
p
.A
topological
order
ofaDAGisapermutation
ˇ
2
S
p
suchthatforevery
edge
(
i;j
)
2
A
,
i
-
j
in
ˇ
;thusitisa
totalorder
thatex-
tends(i.e.,isconsistentwith)thepartialorderoftheDAG,
alsoknownasa
linearextension
ofthepartialorder.
A
Bayesiannetwork
isbyaDAG
G
andacor-
respondingsetofedgeweights

2
R
j
A
j
.Eachnodein
G
isassociatedwitharandomvariable
X
i
.Underthe
MarkovAssumption
,whichweassumethroughout,each
variable
X
i
isconditionallyindependentofitsnondescen-
dantsgivenitsparents,i.e.,thejointdistributionfactorsas
Q
p
i
=1
P

X
i
j
Pa
G
(
X
i
)

;
where
Pa
G
(
X
i
)
denotesthepar-
entsofnode
X
i
(
Spirtesetal.
,
2000
,Chapter4).Thisfactor-
izationimpliesasetof
conditionalindependence
(CI)rela-
tionsthatcanbereadofffromtheDAG
G
by
d-separation
.
The
faithfulnessassumption
statesthattheonlyCIrela-
tionsrealizedby
P
arethoseimpliedbyd-separationin
G
(
Spirtesetal.
,
2000
,Chapter4).DAGsthatsharethesame
d-separationstatementsmakeupthe
Markovequivalence
class
ofaDAG(
Lauritzen
,
1996
,Chapter3).TheMarkov
equivalenceclassofaDAGcanbeuniquelyrepresentedby
its
CP-DAG
,whichplacesarrowsonthoseedgesconsis-
tentacrosstheequivalenceclass(
Anderssonetal.
,
1997
).
ThearrowsoftheCP-DAGarecalled
compellededges
and
representdirectcausaleffects(
Anderssonetal.
,
1997
).
2.2.BayesianInferenceforDAGmodels
Inmanyapplications,thegoalistorecoverafunction
f
(
G
)
oftheunderlyingcausalDAG
G
given
n
i.i.d.samplesonthe
nodes,whichwedenoteby
D
=
f
X
mi
:
m
2
[
n
]
;i
2
[
p
]
g
.
Forexample,wemightaskwhetheradirectededge
(
i;j
)
isin
A
,orwemightwishtodiscoverwhichnodesarein
theMarkovblanketofanode
i
.Inapplicationswhere
n
is
largerelativeto
p
,apointestimateof
G
Šandtherebyof
f
(
G
)
Šsuffrombothapracticalandtheoreticalper-
spective(
Chickering
,
2002
).However,inmanyapplications
ofmoderninterest,
n
issmallrelativeto
p
.Inthiscasethere
maybemanyDAGsthatagreewiththeobserveddataandit
isthendesirabletoinferadistributionacrossDAGsinstead
ofoutputtingjustoneDAG.TakingaBayesianapproach
wecana
prior
P
(
G
)
onthespaceofDAGs,which
canencodepriorstructuralknowledgeabouttheunderlying
DAGŠaswellasdesirablepropertiessuchassparsity.The
likelihood
P
(
D
j
G
)
isobtainedbymarginalizingout

:
P
(
D
j
G
)=
Z

P
(
D;
j
G
)

=
Z

P
(
D
j
;G
)
P
(

j
G
)

andcanbetractablycomputedforcertainclassesofdistri-
butions(
Geiger&Heckerman
,
1999
;
Kuipersetal.
,
2014
).
ApplyingBayestheoremyieldsthe
posteriordistribution
P
(
G
j
D
)
/
P
(
D
j
G
)
P
(
G
)
,whichdescribesthestateof
knowledgeabout
G
afterobservingthedata
D
.Fromthe
posterioronecanthencompute
E
P
(
G
j
D
)
f
(
G
)
,theposterior
meanofthefunctionofinterest.Notethatinthecommon
settingwhere
f
takestheformofanindicatorfunction,this
quantityissimplyaposteriorprobability.
Unfortunately,computingthenormalizingconstantofthe
posteriordistributionisintractablealreadyformoderately
sizedgraphs,sinceitrequiresevaluatingasumoverthe
spaceofallDAGson
p
vertices(
Koivisto&Sood
,
2004
).
Tosamplefromtheposteriorwithoutcomputingthenormal-
izingconstant,
Madigan&York
(
1995
)proposed
structure
MCMC
,whichconstructsaMarkovchainonthespaceof
MinimalI-MAPMCMC
DAGswithstationarydistributionequaltotheexactposte-
rior.
T
samples
f
G
t
g
fromsuchaMarkovchaincanthen
beusedtoapproximatetheposteriormeanofthefunction
ofinterest,namely
E
P
(
G
j
D
)
f
(
G
)
ˇ
T

1
P
T
t
=1
f
(
G
t
)
.
Problematically,theposterioroverDAGsisknowntoex-
hibitmanylocalmaxima,sostructureMCMCexhibitspoor
mixingevenonmoderatelysizedproblems(
Friedman&
Koller
,
2003
;
Ellis&Wong
,
2008
).Toovercomethislimi-
tation,
Friedman&Koller
(
2003
)proposed
orderMCMC
,
whichconstructsaMarkovchainonthespaceofpermu-
tations,wherethemovesaretranspositions.Theposterior
overordersissmootherthantheposterioroverDAGs,since
thelikelihoodcorrespondingtoeachorderisasumover
manyDAGs,andincreasedsmoothnessusuallyleadsto
bettermixingbehavior.However,strongmodularityas-
sumptionsareneededtomakecomputingthelikelihood
tractable.Evenundertheseassumptions,thereremainsa
considerablecomputationalcost:namely,let
k
bethemaxi-
mumindegreeoftheunderlyingDAG,thenthelikelihood
canbecomputedin
O
(
p
k
+1
)
timeandmemory(
Friedman
&Koller
,
2003
).Hence,inpractice
k
canbeatmost3or
4forthismethodtoscaletolargenetworks.TheMonte
Carloestimate
1
T
P
T
i
=1
f
(
G
ˇ
t
)
,where
G
ˇ
t
isdrawnfrom
P
(
G
j
ˇ
t
;D
)
and
ˇ
t
issampledfromtheposteriorover
permutations
P
(
ˇ
j
D
)
,isthenusedtoapproximatethepos-
teriormeanofthefunctionofinterest.
Friedman&Koller
(
2003
)obtainapracticalMCMCsamplerwhentheprior
overpermutationsisuniform,butsuchamodelintroduces
biastowardDAGsthatareconsistentwithmore
permutations(
Ellis&Wong
,
2008
).Correctingforthisbias
byre-weightingeachsampledDAGbytheinversenumber
ofitslinearextensionscanbedone,butitis
#
P
ingeneral
(
Ellis&Wong
,
2008
).
ArecentextensionoforderMCMCis
partialorderMCMC
(
Niinimakietal.
,
2016
).Thismethodworksonthere-
ducedspaceofpartialorders,therebyleadingtoimproved
mixingascomparedtoorderMCMC,butwithasimilarrun-
time.
Kuipers&Moffa
(
2017
)furtherintroducedarelated
methodknownas
partitionMCMC
,whichavoidsthebias
oforderMCMCbyworkingonthelargerspaceofnode
partitionsconsistingofpermutationsandcorresponding
par-
tition
elements.AlthoughpartitionMCMCgenerallymixes
moreslowlythanorderMCMC,itwasempiricallyfoundto
mixmorequicklythanstructureMCMC(
Kuipers&Moffa
,
2017
).
3.ReductiontotheSpaceofMinimalI-MAPs
ToovercomethecomputationalbottleneckoforderMCMC
andatthesametimeavoidtheslowmixingofstructure
MCMC,weproposetorestrictourfocustoacarefullycho-
sen,reducedspaceofDAGsthatisinnearone-to-onecor-
respondencewiththespaceofpermutations.Weconstruct
thissubspaceofDAGsfromtheCIrelationsthatholdinthe
data
D
.InAppendix
A
wereviewaCItestingframework
basedonpartialcorrelationsfortheGaussiansetting.
GivenaCItest,let
^
O
(
n
)
i;j
j
S
(
D;
)
be1ifthecorrespondingCI
testatlevel

basedonthe
n
datapointsin
D
wasrejectedŠ
i.e.,
X
i
6??
X
j
j
X
S
Šand0otherwise.Let
^
O
n
(
D;
)
denote
thecollectionofCItestoutcomesacrossalltriples
(
i;j;S
)
.
Given
^
O
n
(
D;
)
weassociatetoeachpermutation
ˇ
2
S
p
its
minimalI-MAP
^
G
ˇ
:aDAGwithvertices
[
p
]
and
arrows
ˇ
(
i
)
!
ˇ
(
j
)
ifandonlyif
^
O
(
n
)
i;j
j
S
(
D;
)=1
with
i<j
and
S
=
f
ˇ
(1)

ˇ
(
j

1)
gnf
ˇ
(
i
)
g
.Inlightof
Occam'srazoritisnaturaltoconsiderthismapping,since
removinganyedgein
^
G
ˇ
inducesaCIrelationthatisnot
in
^
O
n
(
D;
)
(
Spirtesetal.
,
2000
;
Raskutti&Uhler
,
2013
).
Let
^
G
:
=
f
^
G
ˇ
j
ˇ
2
S
p
g
.Thenanyposterior
P
(
ˇ
j
D
)
byalikelihoodandprioron
S
p
inducesadistribution
onthespaceofallDAGs,denotedby
G
,namely
^
P
(
G
j
D
)
:
=
X
ˇ
2
S
p
1
f
G
=
^
G
ˇ
g
P
(
ˇ
j
D
)
:
(1)
Thisdistributionplacesmassonlyon
^
G
andweightseach
minimalI-MAPaccordingtotheposteriorprobabilityof
samplingapermutationthatisconsistentwithit.
Intheremainderofthissectionweintroduceourmainresult
showingthattheposteriormeanofafunctionbasedonthe
posterior
P
(
G
j
D
)
byalikelihoodandprioron
G
iswellapproximatedbytheposteriormeanofthefunction
basedonthedistribution
^
P
(
G
j
D
)
thathassupportonly
on
^
G
.Beforestatingourmainresult,weintroducethe
assumptionsrequiredforthisresult.
Assumptions3.1.
Let
(
G

;

)
thetruebutunknown
Bayesiannetwork.Let
O

betheequivalentof
^
O
n
(
D;
)
basedonthetruebutunknownjointdistributionon
G

.For
each
ˇ
2
S
p
let
G

ˇ
denotetheminimalI-MAPwithrespect
to
O

.Wemakethefollowingassumptions:
(a)
X
i
j
(
G

;

)
ismultivariateGaussian.
(b)
Let
ˆ

i;j
j
S
bethepartialcorrelationderivedfromthe
Bayesiannetwork
(
G

;

)
forthetriple
(
i;j;S
)
and
let
Q

:
=sup
(
i;j;S
)
fj
ˆ

i;j
j
S
jg
.Thenthereexists
q

<
1
suchthat
P
(
Q

<q

)=1
.
(c)
Let
R

:
=inf
(
i;j;S
)
fj
ˆ

i;j
j
S
j
:
ˆ

i;j
j
S
6
=0
g
.Thenthere
exists
r

>
0
suchthat
P
(
R

>r

)=1
:
(d)
^
G
ˇ
isasufcientstatisticfor
P
(
G
j
ˇ;D
)
,i.e.,
P
(
G
j
ˇ;D
)=
P
(
G
j
^
G
ˇ
)
.
(e)
Let
A
ˇ
denotetheeventthat
f
^
G
ˇ
=
G

ˇ
g
.Then
P
(
A
ˇ
j
^
G
ˇ
)=
P
(
A
ˇ
)
.
(f)
Thereexistssome
M<
1
suchthat
max
G
j
f
(
G
)
j
M
.
MinimalI-MAPMCMC
Anin-depthdiscussionoftheseassumptionsisprovided
inAppendix
C
.Assumption
3.1
(c)canberegardedasthe
Bayesiananalogueofthe
strong-faithfulnessassumption
,
whichisknowntoberestrictive(
Uhleretal.
,
2013
)but
isastandardassumptionincausalinferenceforobtaining
theoreticalguarantees(
Kalisch&Buhlmann
,
2007
;
Zhang
&Spirtes
,
2012
).Practitionersoftenchoose
f
tobean
indicatorfunction(e.g.thepresenceofadirectededge),so
Assumption
3.1
(f)istypicallyinpractice.
Wenowstateourmainresultthatmotivatesconstructinga
MarkovchainonthereducedDAGspace
^
G
insteadof
G
.
Theorem3.2.
UnderAssumptions
3.1
(a)-(f)itholdsthat



E
P
(
G
j
D
)
f
(
G
)

E
^
P
(
G
j
D
)
f
(
G
)




2
f
(
n;p
)
;
where
f
(
n;p
)=
C
1
Mp
2
(
n

p
)exp

C
2
(
r

)
2
(
n

p
)
g
.
Theorem
3.2
isproveninAppendix
D.2
.Themainingre-
dientoftheproofisthefollowinglemmathatboundsthe
probabilityoftheevents
A
C
ˇ
forall
ˇ
.
Lemma3.3.
UnderAssumptions
3.1
(a)-(c)thereexistcon-
stants
C
1
;C
2
thatdependonlyon
q

suchthat
P
(
G

ˇ
6
=
^
G
ˇ
)

f
(
n;p
)
;
forall
ˇ
2
S
p
,where
f
(
n;p
)
isasinTheorem
3.2
and
^
G
ˇ
isconstructedusingFisher'sz-transformtodoCItestingat
level

=2

1



p
nr

2

.
FromTheorem
3.2
andEquation(
1
),itfollowsthat
E
P
(
G
j
D
)
[
f
(
G
)]
ˇ
E
^
P
(
G
j
D
)
[
f
(
G
)]
=
X
ˇ
2
S
p
f
(
^
G
ˇ
)
P
(
ˇ
j
D
)
:
(2)
Hence,usingthenearone-to-onemappingbetween
S
p
and
^
G
toassociatetoeachpermutationaparticularDAG,we
canshowthattheposteriormean
E
P
(
G
j
D
)
f
(
G
)
canbewell
approximatedbysamplingfromaposterioroverpermuta-
tions.Thisisofparticularinterestgiventheobservationby
Friedman&Koller
(
2003
)thataposterioroverpermutations
isgenerallysmootherthanaposterioroverDAGsandhence
moreconducivetofastmixinginMCMCmethods.
4.BayesianInferenceonMinimalI-MAPs
OuroriginalBayesiangenerativemodelconsistedofaprior
P
(
G
)
andalikelihood
P
(
D
j
G
)
.Insomesense,
ˇ
maybe
thoughtofanauxiliaryrandomvariablethataidsourreduc-
tiontotheminimalI-MAPspace.Butinventingapriorand
likelihoodfor
ˇ
inordertoarriveattheposterior
P
(
ˇ
j
D
)
inEquation(
2
)maybeconceptuallydifInparticular,
itisnaturaltoimaginewemighthavepriorandmodel-
inginformationfor
G
ratherthan
ˇ
inapplications.And
S
p
doesnotinduceapartitionin
G
(
Ellis&Wong
,
2008
);
seealsoAppendix
F
.Inthissection,wedemonstratethat,
whentheavailableCIinformationissufreliable,a
goodapproximationto
E
P
(
G
j
D
)
[
f
(
G
)]
canbeobtainedas
follows.
E
P
(
G
j
D
)
[
f
(
G
)]
ˇ
X
^
G
2
^
G
f
(
^
G
)
P
(
^
G
j
D
)
;
where(3)
P
(
^
G
j
D
)
/
P
(
D
j
G
=
^
G
)
P
(
G
=
^
G
)
andthetwotermsaretheoriginallikelihood
P
(
D
j
G
)
andprior
P
(
G
)
restrictedtotheminimalI-MAPspace.This
formulaisintuitivelyappealing;iteffectivelysaysthatwe
canobtainagoodapproximationofthedesiredposterior
expectationbysimplyrestrictingouroriginalmodeltothe
minimalI-MAPspace.
Toshowthis,westartfromEquation(
2
)andlet
^
O
n
:
=
^
O
n
(
D;
)
forbrevity.Notethat
P
(
ˇ
j
D
)=
P
(
ˇ
j
D;
^
O
n
)
since
^
O
n
isafunctionof
D
.Then,byBayestheorem,
P
(
ˇ
j
D
)
/
P
(
D
j
ˇ;
^
O
n
)
P
(
ˇ
j
^
O
n
)
:
(4)
Conditioningonastatisticofthedata,namely
^
O
n
here,
beforeapplyingBayestheoremmaybethoughtofasan
empiricalBayes
procedure(
Darnieder
,
2011
).
Weexamineeachofthetwofactorsontherighthandsideof
Equation(
4
)inturn.Recallthat
A
ˇ
:
=
f
^
G
ˇ
=
G

ˇ
g
isthe
eventthatwemakenoCIerrors.First,notethat
P
(
D
j
ˇ;
^
O
n
)=
X
G
2G
P
(
D
j
ˇ;
^
O
n
;G
)
P
(
G
j
ˇ;
O
n
)
=
X
G
2G
P
(
D
j
G
)
P
(
G
j
^
G
ˇ
)
:
(5)
Moreover,notethat
P
(
G
j
^
G
ˇ
)
=
P
(
G
j
^
G
ˇ
;A
ˇ
)
P
(
A
ˇ
j
^
G
ˇ
)+
P
(
G
j
^
G
ˇ
;A
C
ˇ
)
P
(
A
C
ˇ
j
^
G
ˇ
)
=
P
(
G
j
^
G
ˇ
;A
ˇ
)
P
(
A
ˇ
)+
P
(
G
j
^
G
ˇ
;A
C
ˇ
)
P
(
A
C
ˇ
)
ByAssumption
3.1
(e),
P
(
A
ˇ
j
^
G
ˇ
)=
P
(
A
ˇ
)
.ByLemma
3.3
,
P
(
A
ˇ
)
approaches1exponentiallyfastin
n
,andso
P
(
A
C
ˇ
)
approacheszeroexponentiallyfastin
n
.Observing
that
P
(
G
j
^
G
ˇ
;A
ˇ
)=
1
f
G
=
^
G
ˇ
g
andthat
P
(
G
j
^
G
ˇ
;A
C
ˇ
)
isboundedbyone,we
P
(
G
j
^
G
ˇ
)
ˇ
1
f
G
=
^
G
ˇ
g
fora
sufcientlyaccurateCItest.Therefore,substitutingback
intoEquation(
5
),wethat
P
(
D
j
ˇ;
^
O
n
)
ˇ
P
(
D
j
G
=
^
G
ˇ
)
;
thelikelihoodrestrictedtothespaceofminimalI-MAPs.
Asimilarargument,detailedinAppendix
E
,yieldsthatthe
secondterminEquation(
4
)isapproximatelyequaltothe
priorrestrictedtothespaceofminimalI-MAPs:
P
(
ˇ
j
^
O
n
)
ˇ
P
(
G
=
^
G
ˇ
)
:
MinimalI-MAPMCMC
Finally,ifwelet
P
(
^
G
ˇ
j
D
)
representthedistributionover
^
G
ˇ
proportionaltothelikelihood
P
(
D
j
G
=
^
G
ˇ
)
times
theprior
P
(
G
=
^
G
ˇ
)
,wecanreplaceEquation(
2
)with
Equation(
3
)atthebeginningofthissection,aswasourgoal.
InthenextsectionwedevelopaMarkovChainMonteCarlo
samplerwiththedesiredstationarydistribution,
P
(
^
G
ˇ
j
D
)
.
5.MinimalI-MAPMCMC
InthissectionwedevelopaMarkovChainMonteCarlo
sampler,whichwecall
minimalI-MAPMCMC
,togenerate
approximatesamplesfromthetargetdistribution,
P
(
^
G
ˇ
j
D
)
.
WeshowthatunlikestructureMCMCourapproachis
amenabletofastmixing.Furthermore,weshowthatmin-
imalI-MAPMCMCovercomesthecomputationallimita-
tionsoforderMCMC,sinceitscomplexitydoesnotdepend
onthemaximumindegreeoftheunderlyingDAG
G

.
OurminimalI-MAPMCMCalgorithmisdetailedinAlgo-
rithm
1
fortheGaussiansetting.Algorithm
2
,denotedas
updateminimalI-MAP(UMI)
,isusedasastepinAlgo-
rithm
1
anddescribeshowtocomputeaminimalI-MAP
^
G
˝
fromaminimalI-MAP
^
G
ˇ
when
ˇ
and
˝
differbyan
adjacenttranspositionwithoutrecomputingalledges;see
also
Solusetal.
(
2017
).Weprovethefollowingproposition
aboutthecorrectnessofoursamplerinAppendix
D.3
.
Proposition5.1.
IntheGaussiansetting,thetransitionsin
Algorithm
1
anergodic,aperiodicMarkovchainon
^
G
withstationarydistribution
P
(
^
G
ˇ
j
D
)
.
NotethatminimalI-MAPMCMCcaneasilybeextendedto
thenon-GaussiansettingbyreplacingtheCItestsbasedon
partialcorrelationsbyCItestsbasedonmutualinformation.
However,fornon-Gaussiandataourtheoreticalguarantees
donotnecessarilyhold.
InSection
6
weshowempiricallythatminimalI-MAP
MCMCmixesfasterthanotherMCMCsamplers.The
followingexampleprovidesintuitionforthisbehavior.
Example5.1.
SupposethetrueDAG
G

isthestargraph
witharrows
2
!
1
;
3
!
1
;:::p
!
1
.Forthesakeof
simplicity,suppose
O
n
(
D;
)=
O

.Thenforthepermuta-
tion
˝
=(13

p
2)
thecorrespondingminimalI-MAP
^
G
˝
equalsthefullyconnectedgraph.However,asingletranspo-
sitionfrom
˝
yieldsthepermutation
ˇ
=(23

p
1)
,which
isconsistentwiththeDAG
G

.HenceminimalI-MAP
MCMCcanmoveinasinglestepfromthefullyconnected
graphtothecorrectDAG,whilestructureMCMC,which
updatesoneedgeatatime,wouldrequiremanystepsand
couldgetstuckalongtheway.
Whilethisexampleisclearlyidealized,itcapturesthein-
tuitionthattraversingthespaceofminimalI-MAPsvia
transpositionsallowsthesamplertomakelargejumpsin
DAGspace,whichallowsittoescapelocalmaximafaster
andhencemixfasterthanstructureMCMC.Inthefollowing
resultwecharacterizethememoryandtimecomplexityof
minimalI-MAPMCMC,showingthatunlikeorderMCMC
itdoesnotdependonthemaximumindegreeofthetrue
DAG
G

.TheproofisgiveninAppendix
D.5
.
Proposition5.2.
Let

bethethinningrateoftheMarkov
chainand
T
thenumberofiterations.Considerminimal
I-MAPMCMC(Algorithm
1
)withaproposaldistribution
thatputsmassonlyonadjacenttranspositions,i.e.
q
(
ˇ
t
!
ˇ
t
+1
)=
8
>
>
<
>
>
:
s
if
ˇ
t
=
ˇ
t
+1
1

s
p
if
I
(
ˇ
t
;ˇ
t
+1
)=1
0
otherwise,
where
0
<s<
1
and
I
(

;

)=1
ifthepermutations
differbyasingleadjacenttransposition.Thisalgorithm
takes
O
(
p
2
)
memoryandhasaveragetimecomplexity
of
O
(
Tp
4
+
p
5
)
.Notethatatranspositionbetweenthe
standlastelementofapermutationisstillconsideredan
adjacenttranspositioninour
Usingaproposalthatconsidersonlyadjacenttranspositions
leadstoaconsiderablespeedup.Inparticular,ifweconsider
anypossibletransition,updating
^
G
ˇ
t
to
^
G
ˇ
t
+1
requires
O
(
p
2
)
CItestsingeneral.Butthecostisreducedto

p
)
CItestsforadjacenttranspositionsthatdonotswapthe
andlastelements.SinceperformingaCItestbased
onpartialcorrelationstakes
O
(
p
3
)
time(
Viertl
,
2011
),this
yieldsatotalspeedupofafactorof
p
ateachstep.We
shouldnotethatAlgorithm
1
canbespedupbyconsidering
onlyadjacenttranspositionsthatareconnectedbyanedge;
i.e.,inminimalI-MAPspace
^
G
theseadjacenttranspositions
wouldcorrespondtoconsideringonly
coverededge

(
Spirtesetal.
,
2000
;
Solusetal.
,
2017
).
Wenowcommentonwhyourmethoddoesnotfacethecom-
putationalintractabilityoforderMCMC.Workinginthe
spaceofminimalI-MAPSparametrizedbypermutations
issimilarinspirittoorderMCMC,butourapproxima-
tionoftheposterior(thatis,theapproximationwemake
evenbeforeapplyingMCMC)allowsustoavoidthepoor
scalingoforderMCMC.Inparticular,theintractabilityof
orderMCMCarisesduetothefocusonanexactlikelihood;
acquiringthislikelihoodrequiressummingover
O
(
p
k
+1
)
parentsetsinordertosumoverthefullspaceofDAGs.
Inourcase,weinsteadexploitthefactthatthelikelihood
concentratesaroundasingleDAG
^
G
ˇ
oncewecondition
on
^
O
n
(
D;
)
.
6.Experiments
InthissectionweempiricallycompareminimalI-MAP
MCMCtoorderandpartitionMCMC.Wechose
partition
MCMCsinceitdoesnothavethebiasoforderMCMC
MinimalI-MAPMCMC
Algorithm1
MinimalI-MAPMCMC
Input:
Data
D
,numberofiterations
T
,level

,initialpermutation
ˇ
0
,sparsitystrength

,thinning
rate

Output:
^
G
ˇ
1
;

;
^
G
ˇ
d

e
Construct
^
G
ˇ
0
from
^
O
n
(
D;
)
viaFisher'sz-transform
for
i
=1
to
T
do
Sample
ˇ
i
˘
q
(
ˇ
i

1
!
ˇ
i
)
^
G
ˇ
i
=
UMI
(
ˇ
i
;ˇ
i

1
;
^
G
ˇ
i

1
;;D
)
(Algorithm
2
,Ap-
pendix
B
)
p
i

1
=log
P
(
D
j
^
G
ˇ
i

1
)
P
(
^
G
ˇ
i

1
)
p
i
=log
P
(
D
j
^
G
ˇ
i
)
P
(
^
G
ˇ
i
)
s
i
=min

1
;
exp(
p
i

p
i

1
)

z
i
˘
Bernoulli
(
s
i
)
if
z
i
=0
then
ˇ
i
=
ˇ
i

1
and
^
G
ˇ
i
=
^
G
ˇ
i

1
(chaindoesnotmove)
endif
if
T
isdivisibleby
d
1

e
then
Store
^
G
ˇ
i
endif
endfor
andempiricallyhasfastermixingthanstructureMCMC
(
Kuipers&Moffa
,
2017
).Weusethemax-min-hill-
climbing(MMHC)algorithm(
Tsamardinosetal.
,
2006
)
inconjunctionwiththenonparametricDAGbootstrapap-
proach(
Friedmanetal.
,
1999
)asanadditionalbaselinefor
comparison.Foreachdataset,werantheMarkovchainsfor
10
5
iterations,includingaburn-inof
2

10
4
iterations,and
thinnedtheremainingiterationsbyafactorof100.Seeded
runscorrespondtostartingtheMarkovchainatthepermu-
tation/DAGobtainedusingMMHC.Wealsoconsidered
ﬁcheatﬂrunsthatstartatthetruepermutationwiththein-
tuitionthatweexpecthighscoresonthetruegenerating
model.Intermsofsoftware,weusedthecodeprovidedby
Kuipers&Moffa
(
2017
)torunpartitionandorderMCMC.
Weusedthemethodandsoftwareof
Kangasetal.
(
2016
)
forcountinglinearextensionsforbiascorrection,andwe
implementedminimalI-MAPMCMCusingtheR-package
bnlearn
.
6.1.PriorandLikelihood
Asinmanyapplications,apriorthatinducessparsityin
theunderlyingstructureisdesirableforinterpretabilityand
computation.Further,notethatthetrueDAG
G

isequalto
thesparsestminimalI-MAP
G

ˇ
overallpermutations
S
p
basedonCIrelations
O

(
Verma&Pearl
,
1992
;
Solusetal.
,
2017
;
Raskutti&Uhler
,
2013
);thus,onminimalI-MAP
space,asparsitypriorisnatural.Toachievethisend,we
chooseaprioroftheform
P
(
G
)=
P

(
G
)exp



k
G
k

;
Figure1.
Fromlefttoright,thecolumnsrepresentthe
n
=100
,
n
=1000
,andDream4datasets,respectively.Fromtoptobottom,
therowscorrespondtominimalI-MAP(minIMAP),order,and
partitionMCMC.Theblackdottedlinecorrespondstorunsseeded
withthetruepermutation.Thepurpleandbrownlinescorrespond
torunsseededwitharandompermutationandtheredandgreen
curvesrepresentrunsseededwithMMHC.
where
P

(
G
)
canincludeanystructuralinformationknown
abouttheDAG.Exceptwhereexplicitlymentionedinwhat
follows,weusethispriorwith
P

(
G
)
uniformoverDAGs.
Wenotethat,unlikeourmethodorpartitionMCMC,or-
derMCMCusesauniformprioroverpermutations;the
inducedprioroverDAGsasaresultofsuchaprioris
P
order
(
G
)=
j
#
linext
(
G
)
j
P
(
G
)
,where
j
#
linext
(
G
)
j
de-
notesthenumberoflinearextensionsof
G
(
Ellis&Wong
,
2008
).Finally,eachmethodassumesthedatafollowamul-
tivariateGaussiandistributionwithaWishartprioronthe
networkparameters.Thisassumptionallowscomputation
of
P
(
D
j
G
)
viathe
BGe
score(
Geiger&Heckerman
,
1999
;
Kuipersetal.
,
2014
).
6.2.MixingandConvergence
Weconsiderthreedifferentdatasets.Thetwowere
obtainedbysimulatingdatafromanetworkconsistingof
p
=30
nodeswith
n
=100
and
n
=1000
observations
respectively.Thedataweregeneratedaccordingtoalin-
earstructuralequationmodelwithadditiveGaussiannoise,
wheretheedgeweightsontheunderlyingDAG
G

were
sampleduniformlyfrom
[

1
;

:
25]
[
[
:
25
;
1]
asin(
Solus
etal.
,
2017
).Thethirddatasetisfromthe
Dream4
in-silico
networkchallenge(
Schaffteretal.
,
2011
)ongeneregula-
tion.Inparticular,weexaminethe
multifactorial
dataset
consistingoftennodesandtenobservations.
InFigure
1
weanalyzethemixingperformanceofthediffer-
entmethods.Theconvergenceofdifferentrunstothesame
scoreneighborhoodcanbetakenasanindicationofade-
MinimalI-MAPMCMC
Figure2.
Thetoprowandbottomrowcorrespondtothe
n
=100
and
n
=1000
datasetsrespectively.Eachpointrepresentsthe
posteriorprobabilityofadirectedfeatureestimatedbydifferent
seededrunsofMCMC.Weconsiderallpossiblecombinationsof
randomandMMHCseededrunsforcompleteness.Red(x),blue
(o),andbrown(+)correspondtominIMAP,order,andpartition
MCMCrespectively.
quatemixing.Figure
1
suggeststhatforthe
n
=100
andthe
Dream4datasetallthreemethodshavemixedwellwhilefor
thedatasetwith
n
=1000
samplesthereisevidenceofpoor
mixinginallmethodssincetheposteriorlandscapeismore
peakyduetoincreasedsamplesize.However,thescore
plotsaremarkedlyworsefororderandpartitionMCMC.
Sincetheendgoalisoftentoobtainrobustestimatesof
particularfeatureprobabilities,inFigure
2
weanalyzethe
correlationbetweentheapproximatedposteriorprobabil-
itiesofdirectedfeatureswithrespecttodifferentseeded
runs.Figure
2
showsthatthecorrelationishigheracrossall
modelsforthedatasetwith
n
=100
samples,whichisto
beexpected,sincethechainsseemtohavemixedgiventhe
analysisinthescoreplotsinFigure
1
.Conversely,forthe
n
=1000
dataset,Figure
2
showsthatorderandpartition
MCMCyieldvastlydifferentposteriorprobabilitiesacross
differentruns,whileminimalI-MAPMCMCmaintains
highcorrelation,thussuggestingagainsuperiormixing.
6.3.ROCPerformance
AsdescribedinSection
2
,thebiasoforderMCMCcan
beremovedbydividingthefunctionalofinterest
f
(
G
t
)
bythenumberoflinearextensionsof
G
t
,where
G
t
isa
DAGsampledduringtheMonte-CarloStep.Wedenote
thisby
fullbiascorrection
(
FBC
).Althoughthisleadstoan
unbiasedestimatorfororderMCMC,thereisabias-variance
trade-off.IfasampledDAGhasfewlinearextensions,
thisDAGwillbegivenmoreweightintheMonteCarlo
step,therebyincreasingthevariance.Therefore,wealso
considera
partialbiascorrection
(
PBC
),wheretheweights
aretruncatedandre-normalizedtobelongtothe25thand
75thquartileoftheinverselinearextensioncountsofthe
sampledDAGs.Finally,wedenote
nobiascorrection
by
NBC
.
InTable
1
wereporttheareaundertheROCcurves(AU-
Table1.
AUROCresultsbydatasetandmethod.NBC,PBC,and
FBCstandforno,partial,andfullbiascorrection.Thecolumns
representAUROCvaluesforundirectedandcompelledfeatures
respectively.
M
ETHODN
=100
N
=1000D
REAM
4
MIN
IMAP.946
.6951.00.958
.574.556
O
RDER
-NBC
.957
.675.949.395
:
599
.600
O
RDER
-PBC.956.677.949.393.579.444
O
RDER
-FBC.952
.695
.950.395.563.489
P
ARTITION
.857.660.890.674.497
.733
MMHC-B
OOT
.842.693.892.668.552.533
ROC)fordetectingdirectedandundirectedfeaturesforthe
differentmethods.FororderMCMC,weseeamarginal
performanceboostafterbiascorrectiononthesimulated
datasets,butworseperformanceonDream4.Forthe
n
=100
and
n
=1000
datasets,theBayesianmodels
performbetterthantheMMHCbootstrap.WhileTable
1
showsthatMMHCachievesthehighestAUROCperfor-
manceontheDream4dataset,thecorrespondingROCplot
providedinFigure
4
inAppendix
H
showsthatminimal
I-MAPMCMCandorderMCMCcomparefavorablyto
MMHCwhenthetruenegativerate(TNR),whichequals
oneminusthefalsepositiverate(FPR),isgreaterthan0.4.
ThisrangefortheTNRistherelevantregimeforbiological
applications,whereitisoftenmoreimportanttocontrolfor
TypeIerrors(i.e.incorrectlyspecifyingcausalrelationships
betweennodes).
ThesecondcolumnofTable
1
foreachdatasetshowsAU-
ROCperformanceonthecompellededgesandFigure
4
in
Appendix
H
containsthecorrespondingROCplots.Re-
coveringcompellededgesisimportantbecausetheseare
theonlycausaleffectsthatarefromobserva-
tionaldataalone(
Pearl
,
2009
).Table
1
showsthatminimal
I-MAPMCMCachievesthebestperformanceintermsof
recoveringcompellededgesonthe
n
=1000
datasetand
ismarginallybetterthantheothermethodsonthe
n
=100
dataset.
6.4.TimeandMemoryComplexity
SincepartitionMCMChasasimilartimeandmemorycom-
plexityasorderMCMC,wefocusoncomparingminimal
I-MAPMCMCtoorderMCMCintheseregards.Recallthat
p
denotesthenumberofnodesand
k
denotesthemaximum
indegreeoftheunderlyingDAG
G

.Tocontrolfordifferent
implementations,wecomputedtheaverageiterationtimes
relativetotheaverageiterationtimefor
p
=25
nodes.The
averageiterationtimesdonotincludethetimeittakesto
cacheallthescoresinorderMCMCandthetimeittakes
toconstruct
^
G
ˇ
0
forinitiatingtheMarkovchain.Figure
3
showsthatorderMCMCscalessimilarlytoitspredicted
theoreticalcomplexityof
O
(
p
k
+1
)
.ForminimalI-MAP
MinimalI-MAPMCMC
Figure3.
Averageiterationtimesfordifferentsizednetworks.The
timesarerelativetotheaverageiterationtimefor
p
=25
nodes;
c
denotestheslopeofthedottedlinesandestimatesthecomputa-
tionalcomplexity
O
(
p
c
)
.
MCMC,weprovidedaboundof
O
(
p
4
)
inProposition
5.2
.
Figure
3
suggeststhatthecomplexityscalesbyafactorof
p
betterthantheboundweobtained,namely
O
(
p
3
)
.Finally,
wenotethatorderMCMCrunsoutofmemoryquicklywhen
either
k
or
p
grows.Asasexample,foronly
p
=80
nodesand
k
=5
,orderMCMCtakesover40GBofspace
whileminimalI-MAPMCMCtakesaround1MB.
6.5.IncorporatingPriors
UnlikeminimalI-MAPMCMC,bothpartitionandorder
MCMCrequirethattheprioroverDAGsfactorizesas
P
(
G
)=
Q
p
i
=1
ˆ
(
X
i
;
Pa
G
(
X
i
))
whichisas
struc-
turemodularity
by
Friedman&Koller
(
2003
).
P
(
G
)
is
usedinorderMCMCtospecifytheconditionaldistribution
P
(
G
j
ˇ
)=
I
(
G
-
ˇ
)
P
(
G
)
,whichisneededtocalculate
thelikelihood
P
(
D
j
ˇ
)
inorderMCMC;seealsoAppendix
F
.Theassumptionofstructuremodularityisapractical
limitation.Inbiologicalapplications,forexample,prior
informationoftencomesintheformofpathinformation
betweenclassesofvertices,whichisnotstructuremodular
ingeneral.Inthefollowing,weillustratethispointusingthe
biologicalnetworkstudiedby
Mukherjee&Speed
(
2008
)
(reproducedinFigure
5
inAppendix
H
).Inthisapplica-
tion,wehavepriorknowledgeonbothordersandpaths.In
particular,weexpectligandstocomebeforereceptors,and
receptorsbeforecytosolicproteins.Inaddition,weexpectto
seepathsfromligandstoreceptorsandpathsfromreceptors
tocytosolicproteins(
Mukherjee&Speed
,
2008
).Suchpath
informationcannotbeusedbyorderandpartitionMCMC
sincethisinformationisnotstructuremodular.Totestif
pathknowledgeleadstobetterinference,wecomparedthe
ROCplots(Figure
5
,Appendix
H
)andAUROC(Table
2
)
fordirectededgerecoveryforthedifferentmethods.Ta-
ble
2
showsthatthepathpriorleadstoaboostinAUROC
Table2.
AUROCresultsfordirectededgerecoveryintheprotein
networkinFigure
5
.
M
ETHOD
AUROC
MIN
IMAP
W
/
PATHANDORDERPRIOR
.929
MIN
IMAP
W
/
ORDERPRIOR
.917
O
RDERW
/
ORDERPRIOR
.874
P
ARTITIONW
/
ORDERPRIOR
.912
MMHC-B
OOT
.909
performanceofminimalI-MAPMCMCby
1

2%
percent,
therebysuggestingthatstructuremodularitycanbelimiting
forcertainapplications.Theformofthepathand
orderpriorareprovidedinAppendix
G
.
7.ConcludingRemarks
Inthispaper,weintroducedminimalI-MAPMCMC,a
newBayesianapproachtostructurerecoveryincausalDAG
models.Ouralgorithmworksonthedata-drivenspaceof
minimalI-MAPswiththeoreticalguaranteesonposterior
approximationquality.Weshowedthatunlikeorderorpar-
titionMCMCthecomplexityofaniterationinminimal
I-MAPMCMCdoesnotdependonthemaximumindegree
ofthetrueunderlyingDAG.Thistheoreticalresultwascon-
inourempiricalstudy.Inaddition,ourempirical
studyshowedthatminimalI-MAPMCMCachievessimi-
larorfastermixingthanotherstate-of-the-artmethodsfor
Bayesianstructurerecovery.
WhilewehavefocusedontheGaussiansetting,itwouldbe
interestinginfutureworktoextendthetheoreticalanalysis
tootherdistributions,inparticularthediscretesetting.Fi-
nally,itwouldbeinterestingtoexploretheperformanceof
minimalI-MAPMCMCforobtainingMAPestimatesoras
anewDAGscoringcriterion.Inparticular,thescoringcri-
terionofthe
greedySP(GSP)
algorithm(
Solusetal.
,
2017
)
isequivalenttoourDAGscore(i.e.,unnormalizedposterior
probability)when

!1
inthepriorinSection
6.1
andthe
searchspaceisrestrictedto
^
G
.Inthiscase,thelikelihood
termhasnoinpickingtheminimalI-MAPfrom
^
G
.
Wemightthereforeimprovedperformanceintermsof
structurerecoveryovertheGSPalgorithmbyincorporating
thelikelihoodtermbysetting
<
1
.
Acknowledgements
RajAgrawalwassupportedbyanMITAzizAsharPres-
identialFellowship.TamaraBroderickwassupportedin
partbyanAROYoungInvestigatorProgramAward,ONR
grantN00014-17-1-2072,andaGoogleFacultyResearch
Award.CarolineUhlerwassupportedinpartbyNSF(DMS-
1651995),ONR(N00014-17-1-2147),andaSloanFellow-
ship.
MinimalI-MAPMCMC
References
Andersson,S.A.,Madigan,D.,andPerlman,M.D.Achar-
acterizationofMarkovequivalenceclassesforacyclic
digraphs.
TheAnnalsofStatistics
,25:505Œ541,1997.
Chickering,D.M.Optimalstructurewith
greedysearch.
JournalofMachineLearningResearch
,3:
507Œ554,2002.
Darnieder,W.
BayesianMethodsforData-DependentPri-
ors
.PhDthesis,OhioStateUniversity,2011.
Ellis,B.andWong,W.H.LearningcausalBayesiannet-
workstructuresfromexperimentaldata.
Journalofthe
AmericanStatisticalAssociation
,103:778Œ789,2008.
Fisher,R.A.Frequencydistributionofthevaluesofthe
correlationcoefcientinsamplesfroman
largepopulation.
Biometrika
,10:507Œ521,1915.
Friedman,N.andKoller,D.BeingBayesianaboutnetwork
structure.ABayesianapproachtostructurediscoveryin
Bayesiannetworks.
MachineLearning
,50:95Œ125,2003.
Friedman,N.,Goldszmidt,M.,andWyner,A.J.Dataanal-
ysiswithBayesiannetworks:Abootstrapapproach.In
ProceedingsoftheFifteenthConferenceonUncertainty
inIntelligence
,1999.
Friedman,N.,Linial,M.,Nachman,I.,andPe'er,D.Using
Bayesiannetworkstoanalyzeexpressiondata.In
Pro-
ceedingsoftheFourthAnnualInternationalConference
onComputationalMolecularBiology
,2000.
Geiger,D.andHeckerman,D.Parameterpriorsfordi-
rectedacyclicgraphicalmodelsandthecharacterization
ofseveralprobabilitydistributions.In
Proceedingsofthe
FifteenthConferenceonUncertaintyinIntelli-
gence
,1999.
Grzegorczyk,M.andHusmeier,D.Improvingthestructure
MCMCsamplerforBayesiannetworksbyintroducinga
newedgereversalmove.
MachineLearning
,71:265Œ305,
2008.
Kalisch,M.andBuhlmann,P.Estimatinghigh-dimensional
directedacyclicgraphswiththePC-algorithm.
Journal
ofMachineLearningResearch
,8:613Œ636,2007.
Kangas,K.,Hankala,T.,Niinimaki,T.,andKoivisto,M.
Countinglinearextensionsofsparseposets.In
Proceed-
ingsoftheTwenty-FifthInternationalJointConference
onIntelligence
,2016.
Khashei,M.andMirahmadi,A.Asoftintelligentriskevalu-
ationmodelforcreditscoring
International
JournalofFinancialStudies
,3:411Œ422,2015.
Koivisto,M.andSood,K.ExactBayesianstructurediscov-
eryinBayesiannetworks.
JournalofMachineLearning
Research
,5:549Œ573,2004.
Kuipers,J.andMoffa,G.PartitionMCMCforinference
onacyclicdigraphs.
JournaloftheAmericanStatistical
Association
,112:282Œ299,2017.
Kuipers,J.,Moffa,G.,andHeckerman,D.Addendumon
thescoringofGaussiandirectedacyclicgraphicalmodels.
TheAnnalsofStatistics
,42:1689Œ1691,2014.
Lauritzen,S.
GraphicalModels
.OxfordUniversityPress,
1996.
Madigan,D.andYork,J.Bayesiangraphicalmodelsfor
discretedata.
InternationalStatisticalReview
,63:215Œ
232,1995.
Mohammadi,F.,Uhler,C.,Wang,C.,andYu,J.Generalized
permutohedrafromprobabilisticgraphicalmodels.
SIAM
JournalonDiscreteMathematics
,32:64Œ93,2018.
Mukherjee,S.andSpeed,T.P.Networkinferenceusing
informativepriors.
ProceedingsoftheNationalAcademy
ofSciences
,105:14313Œ14318,2008.
Niinimaki,T.,Parviainen,P.,andKoivisto,M.Structuredis-
coveryinBayesiannetworksbysamplingpartialorders.
JournalofMachineLearningResearch
,17:2002Œ2048,
2016.
Pearl,J.
Causality:Models,ReasoningandInference
.Cam-
bridgeUniversityPress,2ndedition,2009.
Raskutti,G.andUhler,C.Learningdirectedacyclicgraphs
basedonsparsestpermutations.2013.
Robins,J.,Hernan,M.A.,andBrumback,B.Marginal
structuralmodelsandcausalinferenceinepidemiology.
Epidemiology
,11:550Œ60,2000.
Schaffter,T.,Marbach,D.,andFloreano,D.
Bioinformatics
,
27:2263Œ2270,2011.
Solus,L.,Wang,Y.,Matejovicova,L.,andUhler,C.Consis-
tencyguaranteesforpermutation-basedcausalinference
algorithms.
arXiv:1702.03530
,2017.
Spirtes,P.,Glymour,C.,andScheines,R.
Causation,Pre-
diction,andSearch
.MITpress,2ndedition,2000.
Tian,J.andHe,R.Computingposteriorprobabilitiesof
structuralfeaturesinBayesiannetworks.In
Proceed-
ingsoftheTwenty-FifthConferenceonUncertaintyin
Intelligence
,2009.
Tsamardinos,I.,Brown,L.E.,andAliferis,C.F.Themax-
minhill-climbingBayesiannetworkstructurelearning
algorithm.
MachineLearning
,65:31Œ78,2006.
MinimalI-MAPMCMC
Uhler,C.,Raskutti,G.,B
¨
uhlmann,P.,andYu,B.Geometry
ofthefaithfulnessassumptionincausalinference.
The
AnnalsofStatistics
,41:436Œ463,2013.
Verma,T.andPearl,J.Analgorithmfordecidingifaset
ofobservedindependencieshasacausalexplanation.In
ProceedingsoftheEighthConferenceonUncertaintyin
Intelligence
,1992.
Viertl,R.
ProbabilityandBayesianStatistics
.SpringerUS,
2011.
Zhang,J.andSpirtes,P.Strongfaithfulnessanduniform
consistencyincausalinference.In
Proceedingsofthe
NineteenthConferenceonUncertaintyinIntel-
ligence
,2012.
MinimalI-MAPMCMC
Algorithm2
UpdateMinimalI-MAP(
UMI
)
Input:
Currentpermutation
ˇ
i
,previouspermutation
ˇ
i

1
,previousminimalI-MAP
G
ˇ
i

1
,level

,data
D
Output:
G
ˇ
i
k
=minindexofadjacenttransposition
if
k
=1
andlastelementswapped)
then
Compute
^
G
ˇ
i
from
^
O
n
(
D;
)
else
G
ˇ
i
=
G
ˇ
i

1
Reverseedgefrom
X
ˇ
i
(
k
+1)
to
X
ˇ
i
(
k
)
in
G
ˇ
i
ifsuch
anedgeexists
for
s
=1
to
k

1
do
for
j
=
k
to
k
+1
do
S
=
f
ˇ
(1)
;

;ˇ
(
j

1)
gnf
ˇ
(
s
)
g
Let
z
=
^
O
(
n
)
i;j
j
S
(
D;
)
Updateedgefrom
X
ˇ
(
s
)
to
X
ˇ
(
j
)
to
z
in
G
ˇ
i
endfor
endfor
endif
A.CITestingforGaussianData
InthecaseofmultivariateGaussiandata,onemayusethe
Fisherz-transform(
Fisher
,
1915
)toperformCItesting.The
Fisherz-transformisgivenby
Z
(
i;j
j
S
)
:
=
1
2
log(1+^
ˆ
i;j
j
S
)
log(1

^
ˆ
i;j
j
S
)
;
where
^
ˆ
i;j
j
S
istheempiricalpartialcorrelationbetween
X
i
and
X
j
given
X
S
.Toconductatwo-sidedhypothesistest
atlevel

,onemaytestif
p
n
j
S
j
3
j
Z
(
i;j
j
S
)
j


1
(1

=
2)
;
where


1
istheinverseCDFof
N
(0
;
1)
.
B.UpdateAlgorithm
Algorithm
2
theupdateprocedureusedinAlgo-
rithm
1
toreducethenumberofCItestsneeded.
C.DiscussionoftheAssumptions
Basedonthediscussionof
Kalisch&Buhlmann
(
2007
),
Assumption
3.1
(b)isnotsuchastrongassumptionand
seemsmoreofaregularityconditionneededtoprovethe
bounds.Assumption
3.1
(d)hasanintuitiveinterpretation;
itsaysthatthebestpredictionof
G
basedonthedataand
orderiscapturedbytheconstructednetwork.Conditioned
ontheorder,theinferenceproblemisnothard;i.e.,we
justneedtorecovertheskeleton.Sincewecanrecoverthe
skeletonviatheempiricalCIrelations,
^
G
ˇ
isindeedthe
bestpredictionofthenetworkgiventhedataandorderin
manycases,whichwouldimplythat
^
G
ˇ
canreasonablybe
assumedtobeasufcientstatistic.Assumption
3.1
(e)isa
quiteweakassumption;itsaysthattheinformationof
^
G
ˇ
doesnothelpinpredictingtheprobabilityofaCIerror.This
makessensebecausewewanttoknowif
^
G
ˇ
doesnotequal
G

ˇ
.But,withoutobserving
G

ˇ
,orconditioningonsome
propertyof
G

ˇ
inadditionto
^
G
ˇ
,itseemsreasonableto
assumethatourpredictionisleftunchangedwhenknowing
^
G
ˇ
.
D.Proofs
D.1.ProofofLemma
3.3
Theproofreliesheavilyontheconcentrationboundsusedto
provethehigh-dimensionalconsistencyofthePCalgorithm
(
Kalisch&Buhlmann
,
2007
).Tostart,noticethat
P
(
G
ˇ
6
=
^
G
ˇ
j
G;
)=
P
(
CIerror(s)constructing
^
G
ˇ
)

p

1
X
i
=1
p
X
j
=
i
+1
P
(
E
i;j
(
G

;

))
;
(6)
where
E
i;j
(
G

;

)
istheeventthataCIerrorismadewhen
testing
X
ˇ
(
i
)
?
6
?
X
ˇ
(
j
)
j
X
S
,for
S
=
f
ˇ
(1)
;

;ˇ
(
j

1)
gnf
ˇ
(
i
)
g
,conditionedontheBayesiannetwork
(
G

;

)
generatingtheobserveddata.Notethatthesetestsareper-
formedatthelevelprovidedinthestatementof
thelemma.
Byassumption,
Q



;G


q

<
1
and
0
<r


R



;G

(withoutlossofgenerality,ignoremeasurezerosets).Pick-
ingsuch
q

and
r

thensatisfytheassumptionsrequiredin
Lemma4of
Kalisch&Buhlmann
(
2007
).Equations(16)
and(17)from
Kalisch&Buhlmann
(
2007
)implythatthere
existconstants
C
1
,
C
2
thatdepend
only
on
q

suchthat
P
(
E
i;j
(
G

;

))

C
1
(
n

p
)exp

C
2
(
r

)
2
(
n

p
)
g
forany
i;j
.Hence,
P
(
G

ˇ
6
=
^
G
ˇ
j
G;
)

f
(
n;p
)
;
(7)
where
f
(
n;p
)=
p
2
C
1
(
n

p
)exp

C
2
(
r

)
2
(
n

p
)
g
.
Now,
P
(
G

ˇ
6
=
^
G
ˇ
)
=
X
G
2G
Z

P
(
G

ˇ
6
=
^
G
ˇ
j
G;
)
P
(

j
G
)
P
(
G
)


X
G
2G
Z

f
(
n;p
)
P
(

j
G
)
P
(
G
)

=
f
(
n;p
)
;
asdesired.
MinimalI-MAPMCMC
D.2.ProofofTheorem
3.2
Bythetowerproperty,
E
P
(
G
j
D
)
f
(
G
)=
E
P
(
ˇ
j
D
)
E
P
(
G
j
D;ˇ
)
f
(
G
)
:
Asbefore,
A
ˇ
astheeventthat
^
G
ˇ
=
G

ˇ
.Wemay
expand
E
P
(
G
j
D;ˇ
)
f
(
G
)
as
E
P
(
G
j
D;ˇ
)
f
(
G
)
=
X
G
2G
f
(
G
)
P
(
G
j
D;ˇ
)
=
X
G
2G
f
(
G
)
P
(
G;A
ˇ
j
^
G
ˇ
)+
X
G
2G
f
(
G
)
P
(
G;A
C
ˇ
j
^
G
ˇ
)
byAssumption
3.1
(d)andthelawoftotalprobability
=
f
(
^
G
ˇ
)+
P
(
A
C
ˇ
j
^
G
ˇ
)

2
4
X
G
2G
f
(
G
)
P
(
G
j
^
G
ˇ
;A
C
ˇ
)

f
(
^
G
ˇ
)
3
5
bythefactthat
P
(
G
j
^
G
ˇ
;A
ˇ
)=
I
(
G
=
^
G
ˇ
)
accordingtotheexactreasoningusedinSection
4
=
f
(
^
G
ˇ
)+
P
(
A
C
ˇ
)

2
4
X
G
2G
f
(
G
)
P
(
G
j
^
G
ˇ
;A
C
ˇ
)

f
(
^
G
ˇ
)
3
5
;
wheretheequalityusesAssumption
3.1
(e).
Weclaimthat
E
P
(
ˇ
j
D
)
f
(
^
G
ˇ
)=
E
^
P
(
G
j
D
)
f
(
G
)
:
(8)
ToproveEquation(
8
),noticethat
E
P
(
ˇ
j
D
)
f
(
^
G
ˇ
)
=
X
ˇ
2
S
p
f
(
^
G
ˇ
)
P
(
ˇ
j
D
)
=
X
G
2G
f
(
G
)
X
ˇ
2
S
p
1
f
G
2
^
Gg
1
f
G
=
^
G
ˇ
g
P
(
ˇ
j
D
)
=
X
G
2G
f
(
G
)
^
P
(
G
j
D
)
=
E
^
P
(
G
j
D
)
[
f
(
G
)]
:
Finally,






X
G
2G
f
(
G
)
P
(
G
j
^
G
ˇ
;A
C
ˇ
)

f
(
^
G
ˇ
)







2
M
and
P
(
A
c
ˇ
)

C
1
(
n

p
)exp

C
2
(
r

)
2
(
n

p
)
g
byLemma
3.3
.Theresultnowfollowsbytakingexpecta-
tionsandusingtheabovebounds.
D.3.ProofofProposition
5.1
Ergodicityfollowsfromthefactthatanypermutationcan
bereachedfromadjacenttranspositions,andaperiodicity
followsfromourconstraintthat
s
2
(0
;
1)
.Sinceadjacent
transpositionstriviallysatisfythedetailedbalanceequations,
theMarkovchainhasstationarydistribution
P
(
^
G
ˇ
j
D
)
.
D.4.ProofofProposition
D.1
PropositionD.1.
If
ˇ
t
and
ˇ
t
+1
differbyanadjacenttrans-
position,Algorithm
2
correctlycalculates
^
G
ˇ
t
+1
from
^
G
ˇ
t
.
Thisupdaterulewasalsousedby
Solusetal.
(
2017
).We
hereprovidetheproofforcompleteness.Theresulttriv-
iallyfollowsif
ˇ
t
+1
isobtainedbyswappingtheand
lastelementof
ˇ
t
sinceallCItestsarerecomputedinthis
case.Hence,wemayassume
ˇ
t
and
ˇ
t
+1
differbyan
adjacenttranspositionnotattheborder.Suppose
ˇ
t
=
(
n
1

n
i
n
i
+1

n
p
)
and
ˇ
t
+1
=(
n
1

n
i
+1
n
i

n
p
)
,
wherethepermutationsdifferatanadjacentpermutation
atposition
i
.Then,theonlyedgesthatcanbedifferent
in
^
G
ˇ
t
and
^
G
ˇ
t
+1
arethoseedgesconnectednodes
n
i
/
n
i
+1
withnodes
n
k
;
1

k<i
.Correctingtheedges
(
n
i
;n
k
)
and
(
n
i
+1
;n
k
)
correspondstorecomputingthecon-
ditionalindependencestatements
X
n
i
?
6
?
X
n
k
j
X
S
i
and
X
n
i
+1
?
6
?
X
n
k
j
X
S
i
+1
,for
X
S
i
=
f
n
1
;

;n
i
+1
gnf
n
k
g
and
X
S
i
+1
=
f
n
1
;

;n
i

1
gnf
n
k
g
andupdatingthecor-
respondingedges.TheforloopinAlgorithm
2
carriesout
theCItestsintheprevioussentence.Finally,we
needtoreversetheedgebetweennodes
X
n
i
and
X
n
i
+1
if
therewasanedgebetweenthemintheoldDAG
^
G
ˇ
t
;this
reversalisaccomplishedattheverystartofAlgorithm
2
.
D.5.ProofofProposition
5.2
Thememorycomplexityfollowstriviallyfromthefactthat
ittakes
O
(
p
2
)
memorytostore
^
G
ˇ
inanadjacencymatrix.
Computingpartialcorrelationstakesatmost
O
(
p
3
)
time
usingthewell-knownpartialcorrelationrecursiveformula
(
Viertl
,
2011
).Instantiating
^
G
ˇ
0
requires
O
(
p
2
)
CItestsand
hencetakesatmost
O
(
p
5
)
timetocompute.Thesubsequent
^
G
ˇ
i
arecomputedusingAlgorithm
2
.ThecorrectnessofAl-
gorithm
2
wasshowninAppendix
D.4
.WeclaimAlgorithm
2
takesaveragecase
O
(
p
4
)
time.
First,weshowthattheandlastelementsof
ˇ
i
are
swappedwithprobabilitylessthan
1
p
whenmovingfrom
ˇ
i
to
ˇ
i
+1
.Noticefromourdeoftheadjacenttranspo-
sitiondistribution
q
thattheprobabilityofeithertheor
lastelementundergoinganadjacenttranspositionis
2(1

s
)
p
.
Conditionedoneithertheorlastelementbeingchosen
tobeswapped,thereisprobability
1
2
thatthe(last)ele-
mentwillbeswappedwiththelastelement.Hence,
theprobabilityoftheandlastelementbeingswapped
MinimalI-MAPMCMC
equals
(1

s
)
p
whichislessthan
1
p
.Whentheandlast
elementareswapped,all
p
2
CItestsneedtoberecomputed.
Alltheremainingadjacenttranspositionsrequireatmost
2
p
additionalCIteststobeperformedintheforloopof
Algorithm
2
.Hence,onaverage,thenumberofadditional
CItestsis
O
(
p
)
whichimpliestheaveragerunningtimeof
Algorithm
2
is
O
(
p
4
)
.
E.Jforrestrictingthepriorspace
Followingnearlythesamereasoningusedtomotivateour
likelihoodapproximationinSection
4
,herewejustify
P
(
ˇ
j
^
O
n
)
ˇ
P
(
G
=
^
G
ˇ
)
:
Noticethat
P
(
ˇ
j
^
O
n
)
=
P
(
ˇ
j
^
O
n
;A
ˇ
)
P
(
A
ˇ
j
^
O
n
)+
P
(
ˇ
j
^
O
n
;A
C
ˇ
)
P
(
A
C
ˇ
j
^
O
n
)
=
P
(
ˇ
j
^
O
n
;A
ˇ
)
P
(
A
ˇ
)+
P
(
ˇ
j
^
O
n
;A
C
ˇ
)
P
(
A
C
ˇ
)
;
wheretheequalityfollowsfromAssumption
3.1
(e).
Weclaimthat
P
(
ˇ
j
^
O
n
;A
ˇ
)=
P
(
G
=
^
G
ˇ
)
:
(9)
Given
O
n
,wecanconstruct
^
G
ˇ
,andconditionedon
A
ˇ
,
^
G
ˇ
=
G

ˇ
.Eachpermutation
ˇ
maythereforebeassoci-
atedwithitstruecorrespondingDAG
G

ˇ
whichequals
^
G
ˇ
.
Hence,theconditionalprobability
P
(
ˇ
j
^
O
n
;A
ˇ
)
equalsthe
priorprobabilityof
^
G
ˇ
,namely
P
(
G
=
^
G
ˇ
)
.
Finally,since
P
(
A
ˇ
)
goestozeroexponentiallyfastby
Lemma
3.3
,
P
(
ˇ
j
^
O
n
)
iswellapproximatedby
P
(
G
=
^
G
ˇ
)
.
F.PrioronTopological
Orderings
Hereweillustratethecomputationaldifcultyofspecifying
aposterior
P
(
ˇ
j
D
)
thatagreeswithouroriginalprior
P

(
G
)
andlikelihood
P
(
G
j
D
)
onthespaceofDAGs.Noticethat
P
(
D
j
ˇ
)=
X
G
P
(
D
j
G
)
P
(
G
j
ˇ
)
:
(10)
Equation(
10
)impliesthatwemustspecifyaconditionaldis-
tribution
P
(
G
j
ˇ
)
tocalculatethelikelihoodtermfor
P
(
ˇ
j
D
)
.
Tounderstandwhatthisconditionaldistributionshouldbe,
noticethattheinducedprioroverDAGsequals
P
(
G
)=
X
ˇ
2
S
p
P
(
G
j
ˇ
)
P
(
ˇ
)
:
(11)
InorderMCMC,theassumedprior
P
(
ˇ
)
isequalto
1
p
!
(
Friedman&Koller
,
2003
).Anaturaldistributiononemay
specifyfor
P
(
G
j
ˇ
)
,andtheoneassumedin(
Friedman&
Koller
,
2003
),is
P
(
G
j
ˇ
)=
I
(
G
-
ˇ
)
P

(
G
)
:
(12)
However,itistrivialtocheckthatEquation(
12
)im-
pliesEquation(
11
)equals
j
#
linext
(
G
)
j
P

(
G
)
,where
j
#
linext
(
G
)
j
denotesthenumberoflinearextensionsof
G
(
Ellis&Wong
,
2008
).Therefore,weinsteadneed
P
(
G
j
ˇ
)=
1
j
#
linext
(
G
)
j
(
G
-
ˇ
)
P

(
G
)
toconstructamodelthatagreeswithourdesiredprior
P

(
G
)
onDAGs.Thediftyofaprioron
P
(
ˇ
jO
n
)
iscalculating
j
#
linext
(
G
)
j
,whichis
#
P
ingeneral.We
shouldnotethatweavoidtheseissuesbyinsteada
prioron
P
(
ˇ
jO
n
)
.
P
(
ˇ
jO
n
)
allowsustoadistribu-
tionthat
approximately
inducesthecorrectDAGprior;see
thediscussioninSection
4
.
G.PathandOrderPriors
Hereweprovidetheformoftheorderandpath
priorsusedintheexperimentinSection
6.5
.Let
L
,
R
,and
C
denotethesetofligands,receptors,andcytosolicproteins,
respectively,inthenetworkinFigure
5
.Fortheorderprior,
P
(
ˇ
)
,weset
P
(
ˇ
)
:
=exp

X
L
f
L
(
l
)+
X
R
f
R
(
r
)

;
where
f
L
(
l
)
indicatesifligandnode
l
camebeforeallnodes
in
R
[
C
and
f
R
(
r
)
indicatesifreceptor
r
camebeforeall
nodesin
C
andafter
L
in
ˇ
.Forourmethod,theorderprior
isincorporatedintoourprioronDAGs.,we
replacetheDAGpriorof
P
(
G
)=exp



k
G
k

usedin
ourotherexperimentswith,
P
(
G
)
:
=exp



k
G
k

exp

X
L
f
L
(
l
)+
X
R
f
R
(
r
)

:
Werefertotheprioraboveas
minIMAPw/pathprior
in
Table
H
.Toincorporatepathinformation,wetakeapriorof
theform,
exp

X
L
h
L
(
l
)+
X
R
h
R
(
r
)

;
where
h
L
(
l
)
indicatesifligandnode
l
hadapathtoatleast
onenodein
R
and
h
R
(
r
)
indicatesifreceptor
r
hadapath
toatleastonenodein
C
.Combinedwiththeorderprior,
theprior
minIMAPw/pathandorder
inTable
H
isgiven
by,
MinimalI-MAPMCMC
Table3.
Averagecorrelationofdirectedfeaturesbetweenruns
seededwiththetruenetworkandrunsseededwithMMHCfrom
twohundredrandomlygeneratedDAGswith
p
=30
nodes.
Higherisbetter.
M
ETHOD
A
VG
.C
ORRELATION
S
TD
.E
RROR
MIN
IMAP
.977
.004
O
RDER
.928.007
P
ARTITION
.784.006
P
(
G
)
:
=exp



k
G
k

exp

X
L
f
L
(
l
)+
X
R
f
R
(
r
)

exp

X
L
h
L
(
l
)+
X
R
h
R
(
r
)

:
H.AdditionalExperimentsandPlots
Tofurtheranalyzethemixingbehaviorofthedifferentmeth-
ods,wecomputethecorrelationbetweendifferentseeded
runsforestimatingmarginaldirectededgeprobabilities.Ta-
ble
3
showstheaveragecorrelationsandstandarderrors
basedontwohundredsyntheticdatasetswith
n
=1000
ob-
servationsand
p
=30
nodes.Note:Eachmethodwasrun
with
1

10
5
iterationsandaburn-inof
2

10
4
iterations.
TheROCplotsforthe
n
=100
,
n
=1000
,andDream4
datasetsareshowninFigure
4
;seeSection
6.3
foradiscus-
sionoftheseplots.Thenetworkin(
Mukherjee&Speed
,
2008
)usedfortheexperimentsinSection
6.5
isgivenin
Figure
5
.
MinimalI-MAPMCMC
Figure4.
ThetopROCcurvesrepresentrecoveryofundirectedfeaturesandthebottomforcompelledfeatures.Fromlefttoright,the
plotscorrespondtotheDream4,n=100,andn=1000datasets.
MinimalI-MAPMCMC
Figure5.
Thenetworkontheleftistakenfrom(
Mukherjee&Speed
,
2008
).TheROCplotontherightcorrespondstotherecoveryof
directededges.PathandorderreferstoapriorthattakesbothpathandorderinformationintoaccountasinSection
6.5
.For
orderandpartitionMCMC,onlyorderinformationcanbeusedinthepriorasdiscussedinSection
6.5
.
"
37,Personalized Saliency and its Prediction,http://arxiv.org/pdf/1710.03011v2.pdf,https://github.com/xuyanyu-shh/Personalized-Saliency,arXiv:1710.03011v2  [cs.CV]  16 Jun 2018
38,Latent Convolutional Models,http://arxiv.org/pdf/1806.06284v2.pdf,https://github.com/srxdev0619/Latent_Convolutional_Models,"PublishedasaconferencepaperatICLR2019
L
ATENT
C
ONVOLUTIONAL
M
ODELS
ShahRukhAtharEvgenyBurnaevVictorLempitsky

SkolkovoInstituteofScienceandTechnology(Skoltech),Russia
A
BSTRACT
Wepresentanewlatentmodelofnaturalimagesthatcanbelearnedonlarge-scale
datasets.Thelearningprocessprovidesalatentembeddingforeveryimagein
thetrainingdataset,aswellasadeepconvolutionalnetworkthatmapsthelatent
spacetotheimagespace.Aftertraining,thenewmodelprovidesastrongand
universalimagepriorforavarietyofimagerestorationtaskssuchaslarge-hole
inpainting,superresolution,andcolorization.Tomodelhigh-resolutionnatural
images,ourapproachuseslatentspacesofveryhighdimensionality(onetotwo
ordersofmagnitudehigherthanpreviouslatentimagemodels).Totacklethishigh
dimensionality,weuselatentspaceswithaspecialmanifoldstructure(convolu-
tionalmanifolds)parameterizedbyaConvNetofacertainarchitecture.Inthe
experiments,wecomparethelearnedlatentmodelswithlatentmodelslearnedby
autoencoders,advancedvariantsofgenerativeadversarialnetworks,andastrong
baselinesystemusingsimplerparameterizationofthelatentspace.Ourmodel
outperformsthecompetingapproachesoverarangeofrestorationtasks.
1I
NTRODUCTION
Learninggoodimagepriorsisoneofthecoreproblemsofcomputervisionandmachinelearning.
Onepromisingapproachtoobtainingsuchpriorsistolearnadeeplatentmodel,wheretheset
ofnaturalimagesisparameterizedbyacertainsimple-structuredsetorprobabilisticdistribution,
whereasthecomplexityofnaturalimagesistackledbyadeepConvNet(oftencalledageneratoror
adecoder)thatmapsfromthelatentspaceintothespaceofimages.Thebestknownexamplesare
generativeadversarialnetworks(GANs)(Goodfellowetal.,2014)andautoencoders(Goodfellow
etal.,2016).
Givenagooddeeplatentmodel,virtuallyanyimagerestorationtaskcanbesolvedbyalatent
representationthatbestcorrespondstotheimageevidence(e.g.theknownpixelsofanoccluded
imageoralow-resolutionimage).Theattractivenessofsuchapproachisintheuniversalityofthe
learnedimageprior.Indeed,applyingthemodeltoanewrestorationtaskcanbeperformedbysimply
changingthelikelihoodobjective.Thesamelatentmodelcanthereforebereusedformultipletasks,
andthelearningprocessneedsnottoknowtheimagedegradationprocessinadvance.Thisisin
contrasttoapproachesthatusuallytraindeepfeed-forwardConvNetsforindividual
tasks,andwhichhavealimitedabilitytogeneralizeacrosstasks(e.g.afeed-forwardnetworktrained
fordenoisingcannotperformlarge-holeinpaintingandviceversa).
Atthemoment,suchimagerestorationapproachbasedonlatentmodelsislimitedtolow-resolution
images.E.g.(Yehetal.,2017)showedhowalatentmodeltrainedwithGANcanbeusedtoperform
inpaintingoftightly-cropped
64

64
faceimages.Below,weshowthatsuchmodelstrainedwith
GANscannotgeneralizetohigherresolution(eventhoughGAN-basedsystemsarenowableto
obtainhigh-qualitysamplesathighresolutions(Karrasetal.,2018)).Wearguethatitisthelimited
dimensionalityofthelatentspaceinGANsandotherexistinglatentmodelsthatprecludesthemfrom
spanningthespaceofhigh-resolutionnaturalimages.
Toscaleuplatentmodelingtohigh-resolutionimages,weconsiderlatentmodelswithtensof
thousandsoflatentdimensions(ascomparedtofewhundredlatentdimensionsinexistingworks).
Weshowthattrainingsuchlatentmodelsispossibleusingdirectoptimization(Bojanowskietal.,
2018)andthatsuchtrainingleadstogoodimagepriorsthatcanbeusedacrossabroadvarietyof
reconstructiontasks.Inpreviousmodels,thelatentspacehasasimplestructuresuchasasphereora
boxinaEuclideanspace,orafullEuclideanspacewithaGaussianprior.Suchchoice,however,is
notviableinourcase,asvectorswithtensofthousandsofdimensionscannotbeeasilyusedasinputs

CurrentlyalsowithSamsungAICenter,Moscow.
1
arXiv:1806.06284v2  [cs.CV]  2 Nov 2018PublishedasaconferencepaperatICLR2019
Figure1:RestorationsusingthesameLatentConvolutionalModel(images2,4,6)fordifferentimage
degradations(images1,3,5).Attrainingtime,ourapproachbuildsalatentmodelofundegraded
images,andattesttimetherestorationprocesssimplyalatentrepresentationthatmaximizes
thelikelihoodofthecorruptedimage.
toagenerator.Therefore,weconsidertwoalternativeparameterizationsofalatentspace.Firstly,
asabaseline,weconsiderlatentspacesparameterizedbyimagestacks(three-dimensionaltensors),
whichallowstohaveﬁfully-convolutionalﬂgeneratorswithreasonablenumberofparameters.
Ourfullsystemusesamoresophisticatedparameterizationofthelatentspace,whichwecalla
convolutionalmanifold
,wheretheelementsofthemanifoldcorrespondtotheparametervector
ofaseparateConvNet.Suchindirectparameterizationofimagesandimagestackshaverecently
beenshowntoimposeacertainprior(Ulyanovetal.,2018),whichisforrestorationof
naturalimages.Inourcase,weshowthatasimilarpriorcanbeusedwithsuccesstoparameterize
high-dimensionallatentspaces.
Tosumup,ourcontributionsareasfollows.Firstly,weconsiderthetrainingofdeeplatentimage
modelswiththelatentdimensionalitythatismuchhigherthanpreviousworks,anddemonstratethat
theresultingmodelsprovideuniversal(w.r.t.restorationtasks)imagepriors.Secondly,wesuggest
andinvestigatetheconvolutionalparameterizationforthelatentspacesofsuchmodels,andshowthe
ofsuchparameterization.
OurexperimentsareperformedonCelebA(Liuetal.,2015)(128x128resolution),SUNBed-
rooms(Yuetal.,2015)(256x256resolution),CelebA-HQ(Karrasetal.,2018)(1024x1024resolu-
tion)datasets,andwedemonstratethatthelatentmodels,oncetrained,canbeappliedtolargehole
inpainting,superresolutionofverysmallimages,andcolorizationtasks,outperformingotherlatent
modelsinourcomparisons.Tothebestofourknowledge,wearethetodemonstratehowﬁdirectﬂ
latentmodelingofnaturalimageswithoutextracomponentscanbeusedtosolveimagerestoration
problemsattheseresolutions(Figure1).
Otherrelatedwork.
Deeplatentmodelsfollowalonglineofworksonlatentimagemodelsthat
goesbackatleasttotheeigenfacesapproach(Sirovich&Kirby).Intermsofrestoration,acompeting
andmorepopularapproacharefeed-forwardnetworkstrainedforrestorationtasks,which
haveseenrapidprogressrecently.Ourapproachdoesnotquitematchthequalityofe.g.(Iizuka
etal.,2017),thatisdesignedandtrainedfortheinpaintingtask,orthequalityofe.g.(Yu
&Porikli,2016)thatisdesignedandtrainedforthefacesuperresolutiontask.Yetthe
modelstrainedwithinourapproach(likeotherlatentmodels)areuniversal,astheycanhandle
degradationsunanticipatedattrainingtime.
Ourworkisalsorelatedtopre-deeplearning(ﬁshallowﬂ)methodsthatlearnpriorson(potentially-
overlapping)imagepatchesusingmaximumlikelihood-typeobjectivessuchas(Roth&Black,2005;
Karklin&Lewicki,2009;Zoran&Weiss,2011).Theuseofmultiplelayersinourmethodallows
tocapturemuchlongercorrelations.Asaresult,ourmethodcanbeusedsuccessfullytohandle
restorationtasksthatrequireexploitingthesecorrelations,suchaslarge-holeinpainting.
2M
ETHOD
Let
f
x
1
;
x
2
;:::;
x
N
g
beasetoftrainingimages,thatareconsideredtobesamplesfromthedistri-
bution
X
ofimagesinthespace
X
ofimagesofacertainsizethatneedtobemodeled.Inlatent
modeling,weintroduceadifferentspace
Z
andacertaindistribution
Z
inthatspacethatareusedto
re-parameterize
X
.Inpreviousworks,
Z
isusuallychosentobeaEuclideanspacewithfewdozento
fewhundreddimensions,whileourchoicefor
Z
isdiscussedfurtherbelow.
2
PublishedasaconferencepaperatICLR2019
Figure2:TheLatentConvolutionalModelincroproratestwosequentialConvNets.Thesmaller
ConvNet
f
(red)istoeachtrainingimageandiseffectivelyusedtoparameterizethelatent
manifold.ThebiggerConvNet
g
(magenta)isusedasagenerator,anditsparametersaretoall
trainingdata.Theinput
s
tothepipelineisedtoarandomnoiseandnotupdatedduringtraining.
Thedeeplatentmodelingofimagesimplieslearningthegeneratornetwork
g

withlearnable
parameters

,whichusuallyhasconvolutionalarchitecture.Thegeneratornetworkmapsfrom
Z
to
X
andinparticularistrainedsothat
g

(
Z
)
ˇ
X
.Achievingthelatterconditionisextremely
hard,andthereareseveralapproachesthatcanbeused.Thus,generativeadversarialnetworks
(GANs)(Goodfellowetal.,2014)trainthegeneratornetworkinparallelwithaseparatediscriminator
networkthatinsomevariantsofGANsservesasanapproximateratioestimatorbetween
X
and
X
+
g

(
Z
)
overpointsin
X
.Alternatively,autoencoders(Goodfellowetal.,2016)andtheirvariational
counter-parts(Kingma&Welling,2014)trainthegeneratorinparallelwiththeencoderoperating
inthereversedirection,resultinginamorecomplexdistribution
Z
.Ofthesetwoapproaches,only
GANsareknowntobecapableofsynthesizinghigh-resolutionimages,althoughsuchabilitycomes
withadditionaltricksandofthelearningformulation(Arjovskyetal.,2017;Karras
etal.,2018).Inthiswork,westartwithasimplerapproachtodeeplatentmodeling(Bojanowski
etal.,2018)knownastheGLOmodel.GLOmodeloptimizestheparametersofthegenerator
networkinparallelwiththeexplicitembeddingsofthetrainingexamples
f
z
1
;
z
2
;:::;
z
N
g
,suchthat
g

(
z
i
)
ˇ
x
i
bytheendoftheoptimization.Ourapproachdiffersfromandexpands(Bojanowski
etal.,2018)inthreeways:(i)weconsideramuchhigherdimensionalityofthelatentspace,(ii)we
useanindirectparameterizationofthelatentspacediscussedfurtherbelow,(iii)wedemonstratethe
applicabilityoftheresultingmodeltoavarietyofimagerestorationtasks.
Scalinguplatentmodeling.
Relativelylow-dimensionallatentmodelsofnaturalimagespresented
inpreviousworksarecapableofproducingvisually-compellingimagesamplesfromthedistribution
(Karrasetal.,2018),butarenotactuallycapableofmatchingorcoveringaratherhigh-dimensional
distribution
X
.E.g.inourexperiments,noneofGANmodelswerecapableofreconstructingmost
samples
x
fromthehold-outset(orevenfromthetrainingset;thisobservationisconsistentwith
(Bojanowskietal.,2018)andalsowith(Zhuetal.,2016)).Beingunabletoreconstructuncorrupted
samplesclearlysuggeststhatthelearnedmodelsarenotsuitabletoperformrestorationofcorrupted
samples.Ontheotherhand,autoencodersandtherelatedGLOlatentmodel(Bojanowskietal.,2018)
wereabletoachievebetterreconstructionsthanGANonthehold-outsets,yethavedistinctlyblurry
reconstructions(evenonthetrainingset),suggestingstrong
Wepositthatexistingdeeplatentmodelsarelimitedbythedimensionalityofthelatentspacethat
theyconsider,andaimtoscaleupthisdimensionality.Simplyscalingupthelatent
dimensionalitytofewtensofdimensionsisnoteasilyfeasible,ase.g.thegeneratornetworkhasto
workwithsuchavectorasaninput,whichwouldmakethefully-connectedlayerexcessively
largewithhundredsofmillionsofparameters
1
.
Toachieveatractablesizeofthegenerator,onecanconsiderlatentelements
z
tohaveathree-
dimensionaltensorstructure,i.e.tobestacksof2Dimagemaps.Suchchoiceofstructureisvery
1
Onecanconsiderthelayerhavingaverythinmatrixwithareasonablenumberofparametersmapping
thelatentvectortoamuchlower-dimensionalspace.Thishoweverwouldeffectivelyamounttousinglower-
dimensionallatentspaceandwoulddefytheideaofscalinguplatentdimensionality.
3
PublishedasaconferencepaperatICLR2019
naturalforconvolutionalarchitectures,andallowstotrainﬁfully-convolutionalﬂgeneratorswiththe
layerbeingastandardconvolutionaloperation.Thedownsideofthischoice,asweshallsee,is
thatitallowslimitedcoordinationbetweendistantpartsoftheimages
x
=
g

(
z
)
producedbythe
generator.Thisdrawbackisavoidedwhenthelatentspaceisparameterizedusinglatentconvolutional
manifoldsasdescribednext.
Latentconvolutionalmanifolds.
Toimposemoreappropriatestructureonthelatentspace,we
considerstructuringthesespacesas
convolutionalmanifolds
asfollows.Let
s
beastackof
mapsofthesize
W
s

H
s

C
s
andlet
f
f
˚
j
˚
2

g
beasetofconvolutionalnetworksallsharingthe
samearchitecture
f
thatmaps
s
todifferentmapsofsize
W
z

H
z

C
z
.Acertainparametervector
˚
2

thusacertainconvolutionalnetwork
f
˚
.Then,let
z
(
˚
)=
f
˚
(
s
)
beanelementinthe
spaceof
(
W
z

H
z

C
z
)
-dimensionalmaps.Variouschoicesof
˚
thenspanamanifoldembedded
intothisspace,andwerefertoitasthe
convolutionalmanifold
.Aconvolutionalmanifold
C
f;
s
is
thusbytheConvNetarchitecture
f
aswellasbythechoiceoftheinput
s
(whichinour
experimentsisalwayschosentobewithuniformrandomnoise).Additionally,wealsorestrict
theelementsofvectors
˚
toliewithinthe
[

B
;
B
]
range.Formally,theconvolutionalmanifoldis
asthefollowingset:
C
f;
s
=
f
z
j
z
=
f
˚
(
s
)
;˚
2

g
;
=[

B
;
B
]
N
˚
;
(1)
where
˚
servesasanaturalparameterizationand
N
˚
isthenumberofnetworkparameters.Below,we
referto
f
as
latentConvNet
,todisambiguateitfromthegenerator
g
,whichalsohasaconvolutional
structure.
Theideaoftheconvolutionalmanifoldisinspiredbytherecentworkondeepimagepriors(Ulyanov
etal.,2018).Whiletheyeffectivelyuseconvolutionalmanifoldstomodelnaturalimagesdirectly,in
ourcase,weusethemtomodelthelatentspaceofthegeneratornetworksresultingina
learnablelatentimagemodel(whereasthemodelin(Ulyanovetal.,2018)cannotbelearnedona
datasetofimages).Thework(Ulyanovetal.,2018)demonstratesthattheregularizationimposedby
thestructureofaveryhigh-dimensionalconvolutionalmanifoldiswhenmodelingnatural
images.Ourintuitionhereisthatsimilarregularizationwouldbeinregularizinglearning
ofhigh-dimensionallatentspaces.Asourexperimentsbelowreveal,thisintuitionholdstrue.
Learningformulation.
Learningthedeeplatentmodel(Figure2)inourframeworkthenamounts
tothefollowingoptimizationtask.Giventhetrainingexamples
f
x
1
;
x
2
;:::;
x
N
g
,thearchitecture
f
oftheconvolutionalmanifold,andthearchitecture
g
ofthegeneratornetwork,weseekthesetofthe
latentConvNetparametervectors
f
˚
1
;˚
2
;:::;˚
N
g
andtheparametersofthegeneratornetwork

thatminimizethefollowingobjective:
L
(
˚
1
;˚
2
;:::;˚
N
;
)=
1
N
N
X
i
=1
k
g

(
f
˚
i
(
s
))

x
i
k
;
(2)
withanadditionalboxconstraints
˚
j
i
2
[

0
:
01;0
:
01]
and
s
beingarandomsetofimagemaps
withuniformnoise.Following(Bojanowskietal.,2018),thenormin(2)istakentobethe
Laplacian-L1:
k
x
1

x
2
k
Lap-L1
=
P
j
2

2
j
j
L
j
(
x
1

x
2
)
j
1
,where
L
j
isthe
j
thleveloftheLaplacian
imagepyramid(Burt&Adelson,1983).WehavealsofoundthataddinganextraMSElosstermto
theLap-L1losstermwiththeweightof1.0speedsupconvergenceofthemodelswithoutaffecting
theresultsbymuch.
Theoptimization(2)isperformedusingstochasticgradientdescent.Asanoutcomeoftheopti-
mization,eachtrainingexample
x
i
getsarepresentation
z
i
=
f
˚
i
ontheconvolutionalmanifold
C
f;
s
.
Importantly,theelementsoftheconvolutionalmanifoldthenasetofimagesintheimage
space(whichistheimageoftheconvolutionalmanifoldunderlearnedgenerator):
I
f;
s

=
f
x
j
x
=
g

(
f
˚
(
s
))
;˚
2

g
:
(3)
4
PublishedasaconferencepaperatICLR2019
Figure3:Results(perceptualmetricsŒlowerisbetterŒanduserpreferences)forthetwodatasets
(CelebAŒleft,BedroomsŒright)andthreetasks(inpainting,super-resolution,colorization).Forthe
colorizationtasktheperceptualmetricisinadequateasthegrayscaleimagehasthelowesterror,but
isshownforcompleteness.
Table1:MSElossontherestoredimageswithrespecttothegroundtruth.ForinpaintingtheMSE
wascalculatedjustovertheinpaintedregionoftheimages.
CelebALSUN-Bedrooms
LCMGLODIPAEWGAN
LCMGLODIPPGAN
Inpainting
0.00340.00380.00910.00650.0344
0.00650.00850.00630.0097
Super-res
0.00610.00630.00520.00830.0446
0.00710.00690.00570.0183
Colorization
0.00710.00690.01360.01940.0373
0.00660.00750.06960.0205
Whilenotallelementsofthemanifold
I
f;
s

willcorrespondtonaturalimagesfromthedistribution
X
,wehavefoundoutthatwithfewthousanddimensions,theresultingmanifoldscancoverthe
supportof
X
ratherwell.I.e.eachsamplefromtheimagedistributioncanbeapproximatedbythe
elementof
I
f;
s

withalowapproximationerror.Thispropertycanbeusedtoperformallkindsof
imagerestorationtasks.
Imagerestorationusinglearnedlatentmodels.
Wenowdescribehowthelearnedlatentmodel
canbeusedtoperformtherestorationoftheunknownimage
x
0
fromthedistribution
X
,givensome
evidence
y
.Dependingonthedegradationprocess,theevidence
y
canbeanimage
x
0
withmasked
values(inpaintingtask),thelow-resolutionversionof
x
0
(superresolutiontask),thegrayscaleversion
of
x
0
(colorizationtask),thenoisyversionof
x
0
(denoisingtask),acertainstatisticsof
x
0
computed
e.g.usingadeepnetwork(featureinversiontask),etc.
Wefurtherassume,thatthedegradationprocessisdescribedbytheobjective
E
(
x
j
y
)
,whichcanbe
settominuslog-likelihood
E
(
x
j
y
)=

log
p
(
y
j
x
)
ofobserving
y
asaresultofthedegradationof
x
.E.g.forthe
inpainting
task,onecanuse
E
(
x
j
y
)=
k
(
x

y
)

m
k
,where
m
isthe0-1mask
ofknownpixelsand

denoteselement-wiseproduct.Forthe
superresolution
task,therestoration
objectiveisnaturallyas
E
(
x
j
y
)=
k#
(
x
)

y
k
,where
#
(

)
isanimagedownsampling
operator(weuseLanczosintheexperiments)and
y
isthelow-resolutionversionoftheimage.For
the
colorization
task,theobjectiveisas
E
(
x
j
y
)=
k
gray
(
x
)

y
k
,where
gray
(

)
denotesa
projectionfromtheRGBtograyscaleimages(weuseasimpleaveragingofthethreecolorchannels
intheexperiments)and
y
isthegrayscaleversionoftheimage.
5
PublishedasaconferencepaperatICLR2019
Distorted
Image
LCM(Ours)
GLO
DIP
WGAN
AE
Original
Image
Figure4:QualitativecomparisononCelebA(seethetextfordiscussion).
Usingthelearnedlatentmodelasaprior,thefollowingestimationcombiningthelearnedpriorand
theprovidedimageevidenceisperformed:
^
˚
=argmin
˚
E
(
g

(
f
˚
(
s
))
j
y
)
;
^
x
=
g

(
f
^
˚
(
s
))
:
(4)
Inotherwords,wesimplyestimatetheelementoftheimagemanifold(3)thathasthehighest
likelihood.Theoptimizationisperformedusingstochasticgradientdescentovertheparameters
˚
on
thelatentconvolutionalmanifold.
Forthebaselinemodels,whichuseadirectparameterizationofthelatentspace,weperformanalogous
estimationusingoptimizationinthelatentspace:
^
z
=argmin
z
E
(
g

(
z
)
j
y
)
;
^
x
=
g

(
z
)
:
(5)
Intheexperiments,wecomparetheperformanceofourfullmodelandseveralbaselinemodelsover
arangeoftherestorationtasksusingformulations(4)and(5).
3E
XPERIMENTS
Datasets.
Theexperimentswereconductedonthreedatasets.The
CelebA
datasetwasobtainedby
takingthe150Kimagesfrom(Liuetal.,2015)(croppedversion)andresizingthemfrom178

218to
128

128
.Notethatunlikemostotherworks,wehaveperformedanisotropicrescalingratherthan
additionalcropping,leadingtotheversionofthedatasetwithlargerbackgroundportionsandhigher
variability(correspondingtoahardermodelingtask).The
Bedrooms
datasetfromtheLSUN(Yu
etal.,2015)isanotherpopulardatasetofimages.Werescaleallimagestothe
256

256
size.Finally,
the
CelebA-HQ
datasetfrom(Karrasetal.,2018)thatconsistsof30K
1024

1024
imagesoffaces.
6
PublishedasaconferencepaperatICLR2019
DistortedImage
LCM(Ours)
GLO
DIP
PGAN
OriginalImage
Figure5:QualitativecomparisononSUNBedroomsforthetasksofinpainting(rows1-2),superreso-
lution(rows3-4),colorization(rows5-6).TheLCMmethodperformsbetterthanmostmethodsfor
thetwotasks.
Tasks.
Wehavecomparedmethodsforthreediversetasks.Fortheinpaintingtask,wehave
degradedtheinputimagesbymaskingthecenterpartoftheimage(
50

50
forCelebA,
100

100
forBedrooms,
400

400
forCelebA-HQ).Forthesuperresolutiontask,wedownsampledtheimages
byafactorofeight.Forthecolorizationtask,wehaveaveragedthecolorchannelsobtainingthegray
versionoftheimage.
3.1E
XPERIMENTSON
C
ELEB
A
AND
B
EDROOMS
Wehaveperformedextensivecomparisonswithotherlatentmodelsonthetwodatasetswithsmaller
imagesizeandlowertrainingtimes(CelebAandBedrooms).Thefollowinglatentmodelswere
compared:

LatentConvolutionalNetworks(LCMŒOurs):
Each
f
˚
i
has4layers(inCelebA)or5
layers(inBedrooms)or7layers(inCelebA-HQ)andtakesasinputrandomuniformnoise.
TheGenerator,
g

hasanhourglassarchitecture.Thelatentdimensionalityofthemodelwas
24kforCelebAand61kforBedrooms.

GLO
:ThebaselinemodeldiscussedintheendofSection2andinspiredby(Bojanowski
etal.,2018),wherethegeneratornetworkhasthesamearchitectureasinLCM,butthe
7
PublishedasaconferencepaperatICLR2019
convolutionalspaceisparameterizedbyasetofmaps.Thelatentdimensionalityisthesame
asinLCM(andthusmuchhigherthanin(Bojanowskietal.,2018)).Wehavealsotrieda
variantreproducedexactlyfrom(Bojanowskietal.,2018)withvectoriallatentspacesthat
feedintoafully-connectedlayers(forthedimensionalitiesrangingfrom2048to8162Œsee
Appendix
??
),butinvariablyobservedGenerally,wetookextracaretothe
optimalparameterizationthatwouldbemostfavourabletothisbaseline.

DIP
:Thedeepimageprior-basedrestoration(Ulyanovetal.,2018).Weusethearchitecture
proposedbytheauthorsinthepaper.DIPcanberegardedasanextremeversionofour
paperwiththegeneratornetworkbeinganidentity.DIP1Mparameterstoeachimage
forinpaintingandcolorizationand2Mparametersforsuper-resolution.

GAN
:ForCelebAwetrainaWGAN-GP(Gulrajanietal.,2017)withtheDCGANtype
generatorandalatentspaceof256.ForBedroomsweusethepretrainedProgressiveGAN
(PGAN)modelswiththelatentspaceofdimensionality
512
publishedbytheauthorsof
(Karrasetal.,2018).Duringrestoration,wedonotimposeprioronthenormof
z
sinceit
worsenstheproblemofGANs(asdemonstratedinAppendixC).

AE
:FortheCelebAwehavealsoincludedastandardautoencoderusingtheLap-L1and
MSEreconstructionmetricsintothecomparison(latentdimensionality1024).Wehave
alsotriedthevariantwithconvolutionalhigher-dimensionallatentspace,buthaveobserved
verystrongovThevariationalvariant(latentdimensionality1024)leadtostronger
thanthenon-variationalvariant.AstheexperimentsonCelebAclearlyshowed
astrongwehavenotincludedAEintothecomparisononthehigher-resolution
Bedroomsdataset.
ForBedroomsdatasetwerestrictedtrainingtothe200Ktrainingsamples,exceptfortheDIP
(whichdoesnotrequiretraining)andGAN(weusedtheprogressiveGANmodeltrainedonall
3Msamples).Allcomparisonswereperformedonhold-outsetsnotusedfortraining.Following
(Bojanowskietal.,2018),weuseplainSGDwithveryhighlearningrateof1.0totrainLCMandof
10.0totraintheGLOmodels.TheexactarchitecturesaregiveninAppendixD.
Metrics.
Wehaveusedquantitativeanduserstudy-basedassessmentoftheresults.Forthe
quantitativemeasure,wehavechosenthemeansquarederror(MSE)measureinpixelspace,aswell
asthemeansquareddistanceoftheVGG16-features(Simonyan&Zisserman,2015)betweenthe
originalandthereconstructedimages.Such
perceptualmetrics
areknowntobecorrelatedwith
thehumanjudgement(Johnsonetal.,2016;Zhangetal.,2018).Wehaveusedthe[
relu1_2,
relu2_2,relu3_3,relu4_3,relu5_3
]layerscontributingtothedistancemetricwith
equalweight.Generally,weobservedthattherelativeperformanceofthemethodswereverysimilar
fortheMSEmeasure,fortheindividualVGGlayers,andfortheaveragedVGGmetricsthatwereport
here.Whencomputingthelossfortheinpaintingtaskweonlyconsideredthepositionscorresponding
tothemaskedpart.
Quantitativemetricshoweverhavelimitedrelevanceforthetaskswithbigmultimodalconditional
distributions,i.e.wheretwoverydifferentanswerscanbeequallyplausible,suchasallthreetasks
thatweconsider(e.g.therecouldbeverydifferentcolorizationsofthesamebedroomimage).
Inthissituation,humanjudgementofqualityisperhapsthebestmeasureofthealgorithmperformance.
Toobtainsuchjudgements,wehaveperformedauserstudy,wherewehavepicked10randomimages
foreachofthetwodatasetsandeachofthethreetasks.Theresultsofallcomparedmethods
alongsidethedegradedinputswereshowntotheparticipants(100forCelebA,38forBedrooms).
Foreachexample,eachsubjectwasaskedtopickthebestrestorationvariant(weaskedtotakeinto
accountbothrealismandtotheinput).Theresultswerepresentedinrandomorder(shufed
independentlyforeachexample).Wethenjustreportthepercentageofuserchoicesforeachmethod
foragiventaskonagivendatasetaveragedoverallsubjectsandalltenimages.
Results.
TheresultsofthecomparisonaresummarizedinFigure3andTable1withrepresentative
examplesshowninFigure4andFigure5.ﬁTraditionalﬂlatentmodels(builtWGAN/PGANandAE)
performedpoorly.Inparticular,GAN-basedmodelsproducedresultsthatwerebothunrealisticand
poorlythelikelihood.NotethatduringwehavenotimposedtheGaussianprioronthelatent
spaceofGANs.Addingsuchpriordidnotresultinconsiderableincreaseofrealismandleadtoeven
poorertotheevidence(seeAppendixC).
8
PublishedasaconferencepaperatICLR2019
TheDIPmodeldidverywellforinpaintingandsuperresolutionofrelativelyunstructuredBedrooms
dataset.IthoweverperformedverypoorlyonCelebAduetoitsinabilitytolearnfacestructurefrom
dataandonthecolorizationtaskduetoitsinabilitytolearnaboutnaturalimagecolors.
ExceptfortheBedrooms-inpainting,thenewmodelswithverylargelatentspaceproducedresults
thatwereclearlyfavouredbytheusers.LCMperformedbetterthanGLOinallsixusercomparisons,
whileintermsoftheperceptualmetricthecomparisontheperformanceofLCMwasalsobetterthan
GLOforinpaintingandsuperresolutiontasks.Forthecolorizationtask,theLCMisunequivocally
betterintermsofuserpreferences,andworseintermsoftheperceptualmetric.Wenotethat,
however,perceptualmetricisinadequateforthecolorizationtaskastheoriginalgrayscaleimage
scoresbetterthantheresultsofallevaluatedmethods.Wethereforeonlyprovidetheresultsin
thismetricforcolorizationforthesakeofcompletenessgoodquantitativemeasureforthe
highly-ambiguouscolorizationtaskisawell-knownunsolvedproblem).
AdditionalresultsonCelebAandBedroomsdatasetaregiveninAppendicesA,F.
Table2:Metricsofoptimizationoverthez-space,theconvolutionalmanifoldandProgressiveGAN
(Karrasetal.,2018)latentspace
OptimizationOverMSE(knownpixels)MSE(inpaintedpixels)PerceptualMetric
ConvolutionalNetParameters
0.003070.001710.02381
Z-Space
0.001410.008540.02736
PGANLatentSpace
0.004770.002240.02546
DistortedImageOptConvOptZPGANOriginalImage
Figure6:Acomparisionofoptimizationovertheconvolutionalmanifold(column""OptConv""),
thez-space(column""OptZ"")andtheProgressiveGAN(Karrasetal.,2018)latentspace(column
""PGAN"")ontheCelebA-HQdataset(Karrasetal.,2018).
9
PublishedasaconferencepaperatICLR2019
3.2E
XPERIMENTSON
C
ELEB
A-HQ
ANDTHEROLEOFTHECONVOLUTIONALMANIFOLD
.
FortheCelebA-HQ,wehavelimitedcomparisonoftheLCMmodeltothepretrainedprogressive
GANmodel(Karrasetal.,2018)publishedbytheauthors(thisisbecausepropertuningofthe
parametersofotherbaselineswouldtaketoomuchtime).Onthisdataset,LCMusesalatentspaceof
135kparameters.
Additionally,weuseCelebA-HQtohighlighttheroleoftheconvolutionalmanifoldstructureinthe
latentspace.Recallthattheuseoftheconvolutionalmanifoldparameterizationiswhatdistinguish
theLCMapproachfromtheGLObaseline.Theadvantageofthenewparameterizationishighlighted
bytheexperimentsdescribedabove.Onemaywonder,iftheconvolutionalmanifoldconstraintis
neededattesttime,orifduringtherestorationprocesstheconstraintcanbeomitted(i.e.if(5)canbe
usedinsteadof(4)withthegeneratornetwork
g
trainedwiththeconstraint).Generally,weobserved
thattheuseoftheconstraintattesttimehadaminoreffectontheCelebAandBedroomsdataset,
butwasverypronouncedontheCelebA-HQdataset(wherethetrainingsetismuchsmallerandthe
resolutionismuchhigher).
InFigure6andTable2,weprovidequalitativeandquantitativecomparisonbetweentheprogressive
GANmodel(Karrasetal.,2018),theLCMmodel,andthesameLCMmodelappliedwithout
theconvolutionalmanifoldconstraintforthetaskofinpainting.ThefullLCMmodelwiththe
convolutionalmanifoldperformedmarkedlybetterthantheothertwoapproaches.Progressive
GANseverelyeventheknownpixels.Thisisevendespitethefactthatthetrainingsetof
(Karrasetal.,2018)includedthevalidationset(sincetheirmodelwastrainedonfullCelebA-HQ
dataset).UnconstrainedLCMovtheknownpixelswhileprovidingimplausibleinpaintingsfor
theunknown.FullLCMmodelobtainedmuchbetterbalancebetweentheknownpixelsand
inpaintingtheunknownpixels.
4C
ONCLUSION
Theresultsinthisworksuggestthathigh-dimensionallatentspacesarenecessarytogetgoodimage
reconstructionsondesiredhold-outsets.Further,itshowsthatparametrizingthesespacesusing
ConvNetsimposesfurtherstructureonthemthatallowustoproducegoodimagerestorationsfrom
awidevarietyofdegradationsandatrelativelyhighresolutions.Moregenerally,thismethodcan
easilybeextendedtocomeupwithmoreinterestingparametrizationsofthelatentspace,e.g.by
interleavingthelayerswithandparameters.
Theproposedapproachhasseverallimitations.First,whentrainedoververylargedatasets,theLCM
modelrequireslongtimetobetrainedtillconvergence.Forinstance,traininganLCMon150k
samplesofCelebAat
128

128
resolutiontakesabout14GPU-days.NotethattheGLOmodelof
thesamelatentdimensionalitytakesabout10GPU-days.Ontheotherhand,theuniversalityofthe
modelsmeansthattheyonlyneedtobetrainedonceforacertainimagetype,andcanbeappliedto
anydegradationsafterthat.ThesecondlimiatationisthatbothLCMandGLOmodelrequirestoring
theirlatentrepresentationsinmemory,whichforlargedatasetsandlargelatentspacesmayposea
problem.Finally,weobservethatevenwiththelargelatentdimensionalitiesthatweusehere,the
modelsarenotabletothetrainingdataperfectlysufferingfromsuch
R
EFERENCES
MartinArjovsky,SoumithChintala,andLéonBottou.Wassersteingenerativeadversarialnetworks.
In
Proc.ICML
,pp.214Œ223,2017.
P.Bojanowski,A.Joulin,D.Lopez-Paz,andA.Szlam.Optimizingthelatentspaceofgenerative
networks.In
Proc.ICML
,2018.
P.BurtandE.Adelson.Thelaplacianpyramidasacompactimagecode.
IEEETransactionson
Communications
,31(4):532Œ540,Apr1983.ISSN0090-6778.doi:10.1109/TCOM.1983.1095851.
IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,
AaronCourville,andYoshuaBengio.Generativeadversarialnets.In
Proc.NIPS
,pp.2672Œ2680.
2014.
10
PublishedasaconferencepaperatICLR2019
IanGoodfellow,YoshuaBengio,andAaronCourville.
DeepLearning
.MITPress,2016.
http:
//www.deeplearningbook.org
.
IshaanGulrajani,FarukAhmed,MartinArjovsky,VincentDumoulin,andAaronCCourville.
ImprovedtrainingofwassersteinGANs.In
Proc.NIPS
,pp.5767Œ5777.2017.
SatoshiIizuka,EdgarSimo-Serra,andHiroshiIshikawa.GloballyandLocallyConsistentImage
Completion.
Proc.SIGGRAPH
,36(4):107:1Œ107:14,2017.
JustinJohnson,AlexandreAlahi,andLiFei-Fei.Perceptuallossesforreal-timestyletransferand
super-resolution.In
Proc.ECCV
,2016.
YanKarklinandMichaelSLewicki.Emergenceofcomplexcellpropertiesbylearningtogeneralize
innaturalscenes.
Nature
,457(7225):83,2009.
T.Karras,T.Aila,S.Laine,andJ.Lehtinen.ProgressivegrowingofGANsforimprovedquality,
stability,andvariation.In
Proc.ICLR
,2018.
D.PKingmaandM.Welling.Auto-encodingvariationalbayes.In
Proc.ICLR
,2014.
ZiweiLiu,PingLuo,XiaogangWang,andXiaoouTang.Deeplearningfaceattributesinthewild.In
Proc.ICCV
,December2015.
StefanRothandMichaelJBlack.Fieldsofexperts:Aframeworkforlearningimagepriors.In
Proc.
CVPR
,volume2,pp.860Œ867,2005.
K.SimonyanandA.Zisserman.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.
In
Proc.ICLR
,2015.
LawrenceSirovichandMichaelKirby.Low-dimensionalprocedureforthecharacterizationofhuman
faces.
JosaA
.
D.Ulyanov,A.Vedaldi,andV.Lempitsky.Deepimageprior.In
Proc.CVPR
,2018.
R.A.Yeh,C.Chen,T.Y.Lim,A.G.Schwing,M.Hasegawa-Johnson,andM.N.Do.Semantic
imageinpaintingwithdeepgenerativemodels.In
Proc.CVPR
,pp.6882Œ6890,2017.
FisherYu,YindaZhang,ShuranSong,AriSeff,andJianxiongXiao.Lsun:Constructionofalarge-
scaleimagedatasetusingdeeplearningwithhumansintheloop.
arXivpreprintarXiv:1506.03365
,
2015.
XinYuandFatihPorikli.Ultra-resolvingfaceimagesbydiscriminativegenerativenetworks.In
Proc.
ECCV
,pp.318Œ333.Springer,2016.
R.Zhang,P.Isola,A.A.Efros,E.Shechtman,andO.Wang.TheUnreasonableEffectivenessof
DeepFeaturesasaPerceptualMetric.
ArXive-prints
,January2018.
Jun-YanZhu,PhilippKrähenbühl,EliShechtman,andAlexeiA.Efros.Generativevisualmanipula-
tiononthenaturalimagemanifold.In
Proc.ECCV
,pp.597Œ613,2016.
DanielZoranandYairWeiss.Fromlearningmodelsofnaturalimagepatchestowholeimage
restoration.In
Proc.ICCV
,pp.479Œ486,2011.
11
"
39,On Strategyproof Conference Peer Review,https://arxiv.org/pdf/1806.06266v3.pdf,https://github.com/xycforgithub/StrategyProof_Conference_Review,"OnStrategyproofConferencePeerReview
YichongXu

yichongx@cs.cmu.edu
MachineLearningDepartment
CarnegieMellonUniversity,Pittsburgh,PA,USA
HanZhao

han.zhao@cs.cmu.edu
MachineLearningDepartment
CarnegieMellonUniversity,Pittsburgh,PA,USA
XiaofeiShi
xiaofeis@andrew.cmu.edu
DepartmentofMathematicalSciences
CarnegieMellonUniversity,Pittsburgh,PA,USA
JeremyZhang
jbz@andrew.cmu.edu
ComputerScienceDepartment
CarnegieMellonUniversity,Pittsburgh,PA,USA
NiharB.Shah
nihars@cs.cmu.edu
MachineLearningDepartmentandComputerScienceDepartment
CarnegieMellonUniversity,Pittsburgh,PA,USA
Abstract
Weconsiderpeerreviewinaconferencesettingwherethereistypicallyanoverlap
betweenthesetofreviewersandthesetofauthors.Thisoverlapcanincentivizestrategic
reviewstoin˛uencethe˝nalrankingofone'sownpapers.Inthiswork,weaddress
thisproblemthroughthelensofsocialchoice,andpresentatheoreticalframeworkfor
strategyproofande˚cientpeerreview.We˝rstpresentandanalyzeanalgorithmfor
reviewer-assignmentandaggregationthatguaranteesstrategyproofnessandanatural
e˚ciencypropertycalledunanimity,whentheauthorshipgraphsatis˝esasimpleproperty.
Ouralgorithmisbasedontheso-calledpartitioningmethod,andcanbethoughtasa
generalizationofthismethodtoconferencepeerreviewsettings.Wethenempiricallyshow
thattherequisitepropertyontheauthorshipgraphisindeedsatis˝edinthesubmissiondata
fromtheICLRconference,andfurtherdemonstrateasimpletricktomakethepartitioning
methodmorepracticallyappealingforconferencepeerreview.Finally,wecomplementour
positiveresultswithnegativetheoreticalresultswhereweprovethatundervariouswaysof
strengtheningtherequirements,itisimpossibleforanyalgorithmtobestrategyproofand
e˚cient.
1.Introduction
Peerreviewservesasane˙ectivesolutionforqualityevaluationinreviewingprocesses,
especiallyinacademicpaperreview(Dör˛eretal.,2017;Shahetal.,2017)andmassive
openonlinecourses(MOOCs)(DíezPeláezetal.,2013;Piechetal.,2013;Shahetal.,2013).
However,despiteitsscalability,competitivepeerreviewfacestheseriouschallengeofbeing
vulnerabletostrategicmanipulations(Alonetal.,2011;Andersonetal.,2007;Kahngetal.,
2017;Kurokawaetal.,2015;ThurnerandHanel,2011).Bygivinglowerscorestocompetitive

.Equalcontribution.
arXiv:1806.06266v3  [cs.GT]  1 Feb 2020Xu,Zhao,Shi,Zhang&Shah
submissions,reviewersmaybeabletoincreasethechancethattheirownsubmissionsget
accepted.Forinstance,arecentexperimentalstudy(Baliettietal.,2016)onpeerreviewof
art,publishedintheProceedingsoftheNationalAcademyofSciences(USA),concludes
ompetitionincentivizesreviewerstobehavestrategically,whichreducesthe
fairnessofevaluationsandtheconsensusamongrefere
AsnotedbyThurnerandHanel(2011),evenasmallnumberofsel˝sh,strategicreviewers
candrasticallyreducethequalityofscienti˝cstandard.Inthecontextofconferencepeer
review,Langford(2008)callsacademiainherentlyadversarial:
explainswhyyourpaperwasrejectedbasedonpoorlogic.Thereviewerwasn't
concernedwithresearchquality,butratherwithrejectingacomp
Langfordstatesthatanumberofpeopleagreewiththisviewpoint.Thustheimportance
ofpeerreviewinacademiaanditsconsiderablein˛uenceoverthecareersofresearchers
signi˝cantlyunderscorestheneedtodesignpeerreviewsystemsthatareinsulatedfrom
strategicmanipulations.
Inthiswork,wepresentahigher-levelframeworktoaddresstheproblemofstrategic
behaviorinconferencepeerreview.Wepresentaninformaldescriptionoftheframeworkhere
andformalizeitlaterinthepaper.Theproblemsettingcomprisesanumberofsubmitted
papersandanumberofreviewers.Wearegivenagraphwhichwetermasthe
Thecon˛ictgraphisabipartitegraphwiththereviewersandpapersasthetwo
partitionsofvertices,andanedgebetweenanyreviewerandpaperifthatreviewerhasa
con˛ictwiththatpaper.Con˛ictsmayariseduetoauthorship(thereviewerisanauthor
ofthepaper)orotherreasonssuchasbeingassociatedtothesameinstitution.Giventhis
con˛ictgraph,therearetwodesignstepsinthepeerreviewprocedure:(i)assigningeach
papertoasubsetofreviewers,and(ii)aggregatingthereviewsprovidedbythereviewers
togivea˝nalevaluationofeachpaper.Underourframework,thegoalistodesignthese
twostepsofthepeer-reviewprocedurethatsatis˝estwopropertiesstrategyproofnessand
e˚ciency.
Ourgoalistodesignpeer-reviewproceduresthatarestrategyproofwithrespecttothe
givencon˛ictgraph.Apeer-reviewprocedureissaidtobestrategyproofifnoreviewercan
changetheoutcomeforanypaper(s)withwhichshe/hehasacon˛ict.Thisde˝nitionis
formalizedlaterinthepaper.Strategyproofnessnotonlyreassurestheauthorsthatthe
reviewprocessisfair,butalsoensuresthattheauthorsreceiveproperfeedbackfortheir
work.Wenotethatastrategyproofpeer-reviewprocedurealoneisinadequatewithrespect
toanypracticalrequirementssimplygivingouta˝xed,arbitraryevaluationmakesthe
peer-reviewprocedurestrategyproof.
Consequently,inadditiontorequiringstrategyproofness,ourframeworkmeasuresthe
peer-reviewprocedurewithanotheryardstickthatofe˚ciency.Informally,thee˚ciency
ofapeer-reviewprocedureisameasurementofhowwellthe˝naloutcomere˛ectsreviewers'
assessmentsofthequalityofthesubmissions,orameasurementoftheaccuracyintermsof
the˝nalacceptancedecisions.Thereareseveralwaystode˝nee˚ciencyfromasocial
choiceperspectiveorastatisticalperspective.Inthispaper,weconsidere˚ciencyinterms
ofthenotionofunanimityinsocialchoicetheory:anagreementamongallreviewersmust
bere˛ectedinthe˝nalaggregation.
2
OnStrategyproofConferencePeerReview
Inadditiontotheconceptualcontributionbasedonthisframework,wemakeseveral
technicalcontributionstowardsthisimportantproblem.We˝rstdesignapeerreview
algorithmwhichtheoreticallyguaranteesstrategyproofnessalongwithanotionofe˚ciency
thatwetermunanimit.Ourresultrequiresonlyamildassumptiononthecon˛ict
graphofthepeer-reviewdesigntask.Weshowthisassumptionindeedholdstrueinpractice
viaanempiricalanalysisofthesubmissionsmadetotheInternationalConferenceonLearning
Representations(ICLR)conference
1
.Ouralgorithmisbasedonthepopularpartitioning
method,andourpositiveresultscanberegardedasgeneralizingittothesettingofconference
peerreview.Wefurtherdemonstrateasimpletricktomakethepartitioningmethodmore
practicallyappealingforconferencepeerreviewandvalidateitontheICLRdata.
Wethencomplementourpositiveresultswithnegativeresultsshowingthatonecannot
expecttomeetrequirementsthataremuchstrongerthanthatprovidedbyouralgorithm.In
particular,weshowthatundermildassumptionsontheauthorships,thereisnoalgorithm
thatcanbebothstrategyproofandPairwiseunanimityisastronger
notionofe˚ciencythangroupunanimity,andisalsoknownasParetoe˚ciencyinthe
literatureofsocialchoice(Brandtetal.,2016).Weshowthatournegativeresultcontinues
toholdevenwhenthenotionofstrategyproofnessismadeextremelyweak.Wethenprovide
aconjectureandinsightfulresultsontheimpossibilitywhentheassignmentsatis˝esasimple
condition.Finally,weconnectbacktothetraditionalsettingsinsocialchoice
theory,andshowanimpossibilitywheneveryreviewerreviewseverypaper.Thesenegative
resultshighlighttheintrinsichardnessindesigningstrategyproofconferencereviewsystems.
2.RelatedWork
Asearlyasinthe1970s,GibbardandSatterthwaitehadalreadybeenawareoftheimportance
ofahealthyvotingrulethatisstrategyproofinthesettingofsocialchoice(Gibbard,1973;
Satterthwaite,1975).Nowadays,thefactthatprominentpeerreviewmechanismssuchas
theoneusedbytheNationalScienceFoundation(Hazelrigg,2013)andtheonefortime
allocationontelescope(Merri˝eldandSaari,2009)aremanipulablehasfurthercalledfor
strategyproofpeerreviewmechanisms.
Ourworkismostcloselyrelatedtoaseriesofworksonstrategyproofpeerselection(Alon
etal.,2011;Azizetal.,2016,2019;DeClippeletal.,2008;FischerandKlimm,2015;
HolzmanandMoulin,2013;Kahngetal.,2017;Kurokawaetal.,2015),whereagentscannot
bene˝tfrommisreportingtheirpreferencesoverotheragents.
2
DeClippeletal.(2008)
considerstrategyproofdecisionmakingunderthesettingwhereadivisibleresourceisshared
amongasetofagents.Later,Alonetal.(2011);HolzmanandMoulin(2013)consider
strategyproofpeerapprovalvotingwhereeachagentnominatesasubsetofagentsandthe
goalistoselectoneagentwithlargeapprovals.Alonetal.(2011)proposearandomized
strategyproofmechanismusingpartitioningthatachievesprovableapproximateguarantee
tothedeterministicbutnon-strategyproofmechanismthatsimplyselectstheagentwith
1.
https://openreview.net/group?id=ICLR.cc/2017/conference
2.
SomepastliteraturereferstothisrequirementasensuringthatagentsareHowever,theterm
alsohasconnotationson(possiblyimplicit)biasesduetoextraneousfactorssuchassome
featuresabouttheagents(Hojatetal.,2003).Inthispaper,wedeliberatelyusethetermof
inordertomakethescopeofourcontributionclearinthatwedonotaddressimplicitbiases.
3
Xu,Zhao,Shi,Zhang&Shah
maximumapprovals.Bousquetetal.(2014)andFischerandKlimm(2015)furtherextended
andanalyzedthismechanismtoprovideanoptimalapproximateratioinexpectation.
Althoughthe˝rstpartitioning-basedmechanismpartitionsallthevotersintotwodisjoint
subsets,thishasbeenrecentlyextendedto
k
-partitionbyKahngetal.(2017).Inallthese
works,eachagentisessentiallyrequiredtoevaluate
alltheotheragents
exceptherself.Thisis
impracticalforconferencepeerreview,whereeachrevieweronlyhaslimitedtimeandenergy
toreviewasmallsubsetofsubmissions.Inlightofsuchconstraints,Kurokawaetal.(2015)
proposeanimpartialmechanism(
CredibleSubset
)andprovideassociatedapproximation
guaranteesforasettinginwhicheachagentisonlyrequiredtoreviewafewotheragents.
CredibleSubset
isarandomizedmechanismthatoutputsasubsetof
k
agents,butithas
non-zeroprobabilityreturnsanemptyset.BasedontheworkofDeClippeletal.(2008),
Azizetal.(2016)proposeamechanismforpeerselection,termedas
DollarPartition
,whichis
strategyproofandsatis˝esanaturalmonotonicityproperty.Empiricallytheauthorsshowed
that
DollarPartition
outperforms
CredibleSubset
consistentlyandintheworstcaseisbetter
thanpartition-basedapproach.However,evenifthetargetoutputsizeis
k
,
DollarPartition
mayreturnasubsetofsizestrictlylargerthan
k
.Thisproblemhasrecentlybeen˝xed
bythe
ExactDollarPartition
mechanism(Azizetal.,2019),whichempiricallyselectsmore
high-qualityagentsmoreoftenandconsistentlythan
CredibleSubset
.Ourpositiveresults,
speci˝callyour
Divide-and-Rank
algorithmpresentedsubsequently,borrowsheavilyfromthis
lineofliterature.Thatsaid,ourworkaddressestheapplicationofconferencepeerreview
whichismoregeneralandchallengingascomparedtothesettingsconsideredinpastworks.
Oursettingofconferencepeerreviewismorechallengingascomparedtothesepast
worksaseachreviewermayauthormultiplepapersandmoreovereachpapermayhave
multipleauthorsasreviewers.Speci˝cally,thecon˛ictgraphunderconferencepeerreviewis
ageneralbipartitegraph,wherecon˛ictsbetweenreviewersandpaperscanarisenotonly
becauseofauthorships,butalsoadvisor-adviseerelationships,institutionalcon˛icts,etc.In
contrast,pastworksfocusonapplicationsofpeer-gradingandgrantproposalreview,and
henceconsideronlyone-to-onecon˛ictgraphs(thatis,whereeveryrevieweriscon˛icted
withexactlyonepaper).
Apartfromthemostimportantdi˙erencementionedabove,thereareacoupleofother
di˙erencesofthisworkascomparedtosomepastworks.Inthispaperwefocusonordinal
preferenceswhereeachreviewerisaskedtogiveatotalrankingoftheassignedpapers,as
opposedtoprovidingnumericratings.Wedosoinspiredbypastliterature(Barnett,2003;
Douceur,2009;Shahetal.,2016,2013;Stewartetal.,2005;TsukidaandGupta,2011)which
highlightsthebene˝tsofordinaldataintermsofavoidingbiasesaswellasallowingfora
moredirectcomparisonbetweenpapers.Secondly,whilemostpreviousmechanismseither
outputasinglepaperorasubsetofpapers,werequireourmechanismtooutputatotal
rankingoverallpapers.Weconsiderthisrequirementsincethisautomatedoutputinpractice
willbeusedbytheprogramchairsasaguidelinetomaketheirdecisions,andthismore
nuanceddatacomprisingtherankingofthepaperscanbemoreusefultowardsthisgoal.
Anumberofpapersstudyvariousotheraspectsofconferencepeerreview,andwemention
themostrelevantoneshere.Severalworks(CharlinandZemel,2013;Gargetal.,2010;
Hartvigsenetal.,1999;Stelmakhetal.,2019b)designalgorithmsforassigningreviewersto
papersundervariousobjectives,andtheseobjectivesandalgorithmsmayinfactbeused
asalternativede˝nitionsoftheobjectiveofstudiedinthepresentpaper.The
4
OnStrategyproofConferencePeerReview
papersGeetal.(2013);Roosetal.(2011);WangandShah(2019)considerreviewsettings
wherereviewersprovidescorestoeachpaper,withtheaimofaddressingtheproblemsof
miscalibrationinthesescores.Stelmakhetal.(2019a);Tomkinsetal.(2017)studybiases
inpeerreview,Noothigattuetal.(2018)addressissuesofsubjectivity,Gaoetal.(2019)
investigaterebuttals,andFiezetal.(2019)improvethee˚ciencyofthebiddingprocess.
ExperimentsandempiricalevaluationsofconferencepeerreviewscanbefoundinConnolly
etal.(2014);Gaoetal.(2019);LawrenceandCortes(2014);Mathieus(2008);Noothigattu
etal.(2018);Shahetal.(2017);Tomkinsetal.(2017).
3.Problemsetting
Inthissection,we˝rstgiveabriefintroductiontothesettingofourproblem,andthen
introducethenotationusedinthepaper.Wethenformallyde˝nevariousconceptsand
propertiestobediscussedinthesubsequentsections.
Modernreviewprocessisgovernedbyfourkeysteps:(i)anumberofpapersaresubmitted
forreview;(ii)eachpaperisassignedtoasetofreviewers;(iii)reviewersprovidetheir
feedbackonthepaperstheyarereviewing;and(iv)thefeedbackfromallreviewersis
aggregatedtomake˝naldecisionsonthepapers.Let
m
bethenumberofreviewersand
n
bethenumberofsubmittedpapers.De˝ne
R
:
t
r
1
;:::;r
m
u
tobethesetof
m
reviewers
and
P
:
t
p
1
;:::;p
n
u
tobethesetof
n
submittedpapers.
Thereviewprocessmustdealwithcon˛ictsofinterest.Tocharacterizecon˛ictsof
interest,weuseabipartitegraph
C
withvertices
p
R
;
P
q
,whereanedgeisconnectedbetween
areviewer
r
andapaper
p
ifthereexistssomecon˛ictofinterestsbetweenreviewer
r
and
paper
p
.Reviewerswhodonothavecon˛ictsofinterestwithanypaperarenodeswithno
edges.Giventhesetofsubmittedpapersandreviewers,thisgraphis˝xedandcannotbe
controlled.Notethatthecon˛ictgraph
C
de˝nedabovecanbeviewedasageneralizationof
theauthorshipgraphinthepreviously-studiedsettings(Alonetal.,2011;Azizetal.,2016;
FischerandKlimm,2015;HolzmanandMoulin,2013;Kahngetal.,2017;Kurokawaetal.,
2015;Merri˝eldandSaari,2009)ofpeergradingandgrantproposalreview,whereeach
reviewer(paper)isconnectedtoatmostonepaper(reviewer).
Thereviewprocessismodeledbyasecondbipartitegraph
G
,termedas
reviewgraph
,
thatalsohasthereviewersandpapers
p
R
;
P
q
asitsvertices.Thisreviewgraphhasanedge
betweenareviewerandapaperifthatreviewerreviewsthatpaper.Foreveryreviewer
r
i
p
i
Pr
m
sq
,
3
welet
P
i
—
P
denotethesetofpapersassignedtothisreviewerforreview,
orinotherwords,theneighborhoodofnode
r
i
inthebipartitegraph
G
.Theprogram
chairsoftheconferencearefreetochoosethisgraph,butsubjecttocertainconstraintsand
preferences.Toensurebalancedworkloadsacrossreviewers,werequirethateveryreviewer
isassignedatmost

papersforsomeintegers
1
¤

¤
n
.Inotherwords,everynodein
R
hasatmost

neighbors(in
P
)ingraph
G
.Additionally,eachpapermustbereviewedby
acertainminimumnumberofreviewers,andwedenotethisminimumnumberas

.Thus
everynodeintheset
P
musthaveatleast

neighbors(in
R
)inthegraph
G
.Forany
(directedorundirected)graph
H
,weletthenotation
E
H
denotethesetof(directedor
undirected,respectively)edgesingraph
H
.
3.Weusethestandardnotation
r

s
torepresenttheset
t
1
;:::;
u
foranypositiveinteger

.
5
Xu,Zhao,Shi,Zhang&Shah
Attheendofthereviewingperiod,eachreviewerprovidesatotalrankingofthepapers
thatshe/hereviewed.Foranysetofpapers
P
1
—
P
,welet

p
P
1
q
denotethesetofall
permutationsofpapersin
P
1
.Furthermore,foranypaper
p
j
P
P
1
andanypermutation
ˇ
p
P
1
qP

p
P
1
q
,welet
ˇ
j
p
P
1
q
denotethepositionofpaper
p
j
inthepermutation
ˇ
p
P
1
q
.Atthe
endofthereviewingperiod,eachreviewer
r
i
p
i
Pr
m
sq
submitsatotalranking
ˇ
p
i
q
p
P
i
qP

p
P
i
q
ofthepapersin
P
i
.Wede˝nea(partial)rankingpro˝le
ˇ
:
p
ˇ
p
1
q
p
P
1
q
;:::;ˇ
p
m
q
p
P
m
qq
as
thecollectionofrankingsfromallthereviewers.Whentheassignment
P
1
;:::;
P
m
ofpapers
toreviewersis˝xed,weusetheshorthand
p
ˇ
p
1
q
;:::;ˇ
p
m
q
q
forpro˝le
ˇ
.Foranysubsetof
papers
P
1
—
P
,welet
ˇ
P
1
denotetherestrictionof
ˇ
toonlytheinducedrankingson
P
1
.
Finally,whentherankingunderconsiderationisclearfromcontext,weusethenotation
p
¡
p
1
tosaythatpaper
p
isrankedhigherthanpaper
p
1
intheranking.
Underthisframework,thegoalistojointlydesign:(a)apaper-reviewerassignment
scheme,thatis,edgesofthegraph
G
,and(b)anassociatedreviewaggregationrule
f
:
±
m
i

1

p
P
i
qÑ

p
P
q
whichmapsfromtherankingpro˝letoanaggregatetotalranking
ofallpapers.
4
Foranyaggregationfunction
f
,welet
f
j
p
ˇ
q
bethepositionofpaper
p
j
when
theinputto
f
isthepro˝le
ˇ
.
Wenotethatalthoughweassumeordinalfeedbackfromthereviewers,ourresultscontinue
toholdifwehavereviewscoresasourinputinsteadofrankings;ourframeworkis˛exible
enoughtotakethescoresintoaccount(cf.Section4.1).
Inwhatfollowswede˝nestrategyproofnessande˚ciencythatanyconferencereview
mechanism
f
shouldsatisfyunderourpaper-reviewsetting.Inspiredbythetheoryofsocial
choice,inthispaperwede˝nethenotionofe˚ciencyviatwovariantsof,and
wealsodiscusstwonaturalnotionsofstrategyproofness.
3.1Strategyproofness
Intuitively,strategyproofnessmeansthatareviewercannotbene˝tfrombeingdishonest.
Inthecontextofconferencereview,strategyproofnessisde˝nedwithrespecttoagiven
con˛ictgraph
C
;werecallthenotation
E
C
asthesetofedgesofgraph
C
.Itmeansthata
reviewercannotchangethepositionofhercon˛ictingpapers,bymanipulatingtheranking
sheprovides.
De˝nition3.1
(Strategyproofness,SP)
.
Areviewprocess
p
G
;f
q
iscalledstrategyproof
withrespecttoacon˛ictgraph
C
ifforeveryreviewer
r
i
P
R
andpaper
p
j
P
P
suchthat
p
r
i
;p
j
qP
E
C
thefollowingconditionholds:foreverypairofpro˝les(underassignment
G
)that
di˙eronlyintherankinggivenbyreviewer
r
i
,thepositionof
p
j
isunchanged.
5
Formally,
@
ˇ

p
ˇ
p
1
q
;:::;ˇ
p
i

1
q
;ˇ
p
i
q
;ˇ
p
i

1
q
;:::;ˇ
p
m
q
q
and
ˇ
1
p
ˇ
p
1
q
;:::;ˇ
p
i

1
q
;ˇ
p
i
q
1
;ˇ
p
i

1
q
;:::;ˇ
p
m
q
q
,it
mustbethat
f
j
p
ˇ
q
f
j
p
ˇ
1
q
.
Astrategyproofpeerreviewprocedurealoneisinadequatewithrespecttoanypractical
requirementssimplygivingouta˝xed,arbitraryevaluationmakesthepeerreviewprocedure
4.
Tobeclear,thefunction
f
istiedtotheassignmentgraph
G
.Thegraph
G
speci˝esthesets
p
P
1
;:::;
P
m
q
,
andthenthefunction
f
takespermutationsofthesesetsofpapersasitsinputs.Weomitthisfromthe
notationforbrevity.
5.
Arelated(andweaker)de˝nitionofstrategyproofisthatthepositionofany
p
j
cannotbe
improved
.
Itiseasytoshowthatanymechanismthatsatis˝estheweakernotioncanalsosatisfyournotionof
strategyproofness.
6
OnStrategyproofConferencePeerReview
strategyproof.Wethereforeconsidere˚ciencyoftheprocedureinthenextsection,toensure
thattheauthorsreceivemeaningfulandhelpfulfeedbackfortheirwork.
3.2E˚ciency(unanimity)
Consequently,inadditiontorequiringstrategyproofness,wemeasurethepeerreviewpro-
cedurewithanotheryardsticke˚ciency.Thepeerreviewprocedureneedstonotonly
reassuretheauthorsthatthereviewprocessisfair,butalsoensurethattheauthorsreceive
properfeedbackfortheirworkinane˚cientway.
Inthiswork,weconsidere˚ciencyofapeer-reviewprocessintermsofthenotionof
unanimity.Unanimityisoneofthemostprevalentandclassicpropertiestomeasurethe
e˚ciencyofavotingsysteminthetheoryofsocialchoice(Fishburn,2015).Atacolloquial
level,unanimitystatesthatwhenthereisacommonagreementamongallreviewers,then
theaggregationoftheiropinionsmustalsorespectthisagreement.Inthispaperwediscuss
twokindsofunanimity,termedgroupunanimity(GU)andpairwiseunanimity(PU).Both
kindsofunanimityimposerequirementsontheaggregationfunctionforanygivenreviewer
assignment.
We˝rstde˝negroupunanimity:
De˝nition3.2
(GroupUnanimity,GU)
.
Wede˝ne
p
G
;f
q
tobegroupunanimous(GU)
ifthefollowingconditionholdsforeverypossiblepro˝le
ˇ
.Ifthereisanon-emptysetof
papers
P
1
•
P
suchthateveryreviewerranksthepapersshereviewedfrom
P
1
higherthan
thoseshereviewedfrom
P
z
P
1
,then
f
p
ˇ
q
musthave
p
x
¡
p
y
foreverypairofpapers
p
x
P
P
1
and
p
y
P
P
z
P
1
suchthatatleastonereviewerhasreviewedboth
p
x
and
p
y
.
Intuitively,groupunanimitysaysthatifpaperscanbepartitionedintotwosetssuch
thateveryreviewerwhohasreviewedpapersfrombothsetsagreesthatthepapersshehas
reviewedfromthe˝rstsetarebetterthanwhatshereviewedfromthesecondset,thenthe
˝naloutputrankingshouldrespectthisagreement.
Oursecondnotionofunanimity,termedpairwiseunanimity,isalocalre˝nementofgroup
unanimity.ThisnotionisidenticaltotheclassicalnotionofunanimitystatedinArrow's
impossibilitytheorem(Arrow,1950)theclassicalunanimityconsiderseveryreviewerto
reviewallpapers(thatis,
P
i

P
;
@
i
Pr
m
s
),whereasournotionisalsode˝nedforsettings
wherereviewersmayreviewonlysubsetsofpapers.
De˝nition3.3
(PairwiseUnanimity,PU)
.
Wede˝ne
p
G
;f
q
tobepairwiseunanimous
(PU)ifthefollowingconditionholdsforeverypossiblepro˝le
ˇ
andeverypairofpapers
p
j
1
;p
j
2
P
P
:Ifatleastonereviewerhasreviewedboth
p
j
1
and
p
j
2
andallthereviewersthat
havereviewed
p
j
1
and
p
j
2
agreeon
p
j
1
¡
p
j
2
,then
f
j
1
p
ˇ
q
¡
f
j
2
p
ˇ
q
.
Animportantpropertyisthatpairwiseunanimityisstrongerthangroupunanimity.
Proposition3.4.
If
p
G
;f
q
ispairwiseunanimous,then
p
G
;f
q
isalsogroupunanimous.
TheproofofthispropositionisprovidedinSection7.1.
7
Xu,Zhao,Shi,Zhang&Shah
4.PositiveTheoreticalResultsandAlgorithm
Inthissectionweconsiderthedesignofreviewerassignmentsandaggregationrulesfor
strategyproofnessandgroupunanimity(e˚ciency).Itisnothardtoseethatstrategyproofness
andgroupunanimitycannotbesimultaneouslyguaranteedforarbitrarycon˛ictgraphs
C
,
forinstance,when
C
isafully-connectedbipartitegraph.Priorworksonthistopicconsidera
speci˝cclassofcon˛ictgraphsthosewithone-to-onerelationsbetweenpapersandreviewers
whichdonotcaptureconferencepeerreviewsettings.Weconsideramoregeneralclass
ofcon˛ictgraphsandpresentanalgorithmbasedonthepartitioning-basedmethod(Alon
etal.,2011),whichweshowcanachievegroupunanimousandstrategyproofness.
Wethenempiricallydemonstrate,usingsubmissiondatafromtheICLRconference,that
thisclassofcon˛ictgraphsisindeedrepresentativeofpeerreviewsettings.Weobservethat
thequalityofthereviewerassignmentunderourmethod(thatguaranteesstrategyproofness)
isonlyslightlylowerascomparedtotheoptimalqualityintheabsenceofstrategyproo˝ng
requirements.Finally,wepresentasimpletricktosigni˝cantlyimprovethepracticalappeal
ofouralgorithm(andmoregenerallythepartitioningmethod)toconferencepeerreview.
4.1The
Divide-and-Rank
Algorithm
Wenowpresentour
Divide-and-Rank
frameworkconsistingofthereviewerassignment
algorithm(Algorithm1)andtherankaggregationalgorithm(Algorithm2).Atahighlevel,
ouralgorithmperformsapartitionofthereviewersandpapersforassignment,andaggregates
thereviewsbycomputingarankingwhichisconsistentwithanygroupagreements.The
Divide-and-Rank
algorithmworksforageneralcon˛ictgraph
C
aslongasthecon˛ictgraph
canbedividedintotworeasonably-sizeddisconnectedcomponents.
Importantly,theframeworkissimpleyet˛exibleinthattheassignmentwithineach
partitionandtheaggregationamongcertaingroupsofpaperscanleverageanyexisting
algorithmforassignmentandaggregationrespectively,whichisusefulasitallowstofurther
optimizevariousothermetricsinadditiontostrategyproofnessandunanimity.
Belowwedescribeourframeworkinmoredetail.We˝rstintroducetheassignment
procedureinAlgorithm1.
The
Divide-and-Rank
assignmentalgorithmbeginsbypartitioningthecon˛ictgraphinto
twodisconnectedcomponentssuchthat(1)theymeettherequirementsspeci˝edby

and

;and(2)thetwodisconnectedcomponentshaveroughlyequalsizeintermsofnumberof
nodes.Thisisachievedusingthesubroutine
Partition
.Inmoredetail,
Partition
˝rstrunsa
breadth-˝rst-search(BFS)algorithmtopartitiontheoriginalcon˛ictgraphinto
K
connected
components,wherethe
k
thconnectedcomponentcontains
r
k
¥
0
reviewersand
p
k
¥
0
papers.Next,thealgorithmperformsadynamicprogrammingtocomputeallthepossible
subsetsums,i.e.,sumofthenumberofreviewersandthenumberofpapersinagivensubset,
achievablebythe
K
connectedcomponents.Here
T
r
k;r;p
s
1
meansthatthereexistsa
partitionofthe˝rst
k
componentssuchthatonesideofthepartitionhas
r
reviewersand
p
papers,and0otherwise.Thelaststepistocheckwhetherthereexistsasubset
C
satisfying
theconstraintgivenby

and

,andifso,runsastandardbacktrackingalgorithmalongthe
tableto˝ndtheactualsubset
C
.Clearlythe
Partition
runsin
O
p
Knm
q
,andsince
K
¤
nm
,
itrunsinpolynomialtimeinthesizeoftheinputcon˛ictgraph
C
.
8
OnStrategyproofConferencePeerReview
Algorithm1
Divide-and-Rank
assignment
Input:
con˛ictgraph
C
,parameters

,assignmentalgorithm
A
Output:
anassignmentofreviewerstopapers
1:
p
R
C
;
P
C
q
,
p
R
s
C
;
P
s
C
qÐ
Partition
p
C
;
q
2:
usealgorithm
A
toassignpapers
P
s
C
toreviewers
R
C
3:
usealgorithm
A
toassignpapers
P
C
toreviewers
R
s
C
4:
return
theunionofassignmentsfromstep2and3
5:
6:
procedure
Partition
(con˛ictgraph
C
,parameters

)
7:
runaBFSon
C
togetconnected
K
components
tp
R
k
;
P
k
qu
K
k

1
8:
let
r
k
|
R
k
|
;p
k
|
P
k
|
,
@
k
Pr
K
s
9:
initializeatable
T

;

;
Pt
0
;
1
u
K

m

1

n

1
q
sothat
T
r
1
;r
1
;p
1
s
T
r
1
;
0
;
0
s
1
,
otherwise0
10:
for
k

2
to
K
do
11:
T
r
k;r;p
s
T
r
k

1
;r;p
s_
T
r
k

1
;r

r
k
;p

p
k
s
;
@
0
¤
r
¤
m;
0
¤
p
¤
n
12:
endfor
13:
for
0
¤
r
¤
m;
0
¤
p
¤
n
,ifthereisno
T
r
K;r;p
s
1
suchthat
max
t
p
m

r
;
n

p
r
u¤


,
return
error
14:
usethestandardbacktrackinginthetable
T

;

;

toreturn
p
R
C
;
P
C
q
and
p
R
s
C
;
P
s
C
q
15:
endprocedure
Inthenextstep,thealgorithmassignspaperstoreviewersinafashionthatguarantees
eachpaperisgoingtobereviewedbyatleast

reviewersandeachreviewerreviewsatmost

papers.Theassignmentofpapersinanyindividualcomponent(toreviewersintheother
component)canbedoneusinganyassignmentalgorithm(takenasaninput
A
)aslongasthe
algorithmcansatisfythe
p

q
-requirements.Possiblechoicesforthealgorithm
A
include
thepopularTorontopapermatchingsystem(CharlinandZemel,2013)andothers(Garg
etal.,2010;Hartvigsenetal.,1999;Stelmakhetal.,2019b).Wecanalsousethetypical
reviewerbiddingsystem,whileconstrainingthereviewersin
R
C
toreview
P
s
C
and
R
s
C
to
review
P
C
.
WethenintroducetotheaggregationprocedureinAlgorithm2.Atahighlevel,the
papersineachcomponentareaggregatedseparatelyusingthesubroutine
Contract-and-
Sort
.Thisaggregationin
Contract-and-Sort
isperformedbyatopologicalorderingofall
stronglyconnectedcomponents(SCCs)accordingtothereviews,andthenrankingthe
paperswithineachsetusinganyarbitraryaggregationalgorithm(takenasaninput
B
).
6
Possiblechoicesforthealgorithm
B
includethemodi˝edBordacount(Emerson,2013),
Plackett-Luceaggregation(Hajeketal.,2014),orothers(Caragiannisetal.,2017)Moving
backtoAlgorithm2,thetworankingsreturnedby
Contract-and-Sort
respectivelyforthe
twocomponentsaresimplyinterlacedtoobtainatotalrankingoverallthepapers:theslots
for
C
arereservedinset
I
,and
r
n
sz
I
containtheslotsfortheremainingpapers.Inour
extendedversionofthepaperwealsoshowthattheinterleavingonlycausesasmallchange
w.r.tanunderlyingoptimalranking.
Thefollowingtheoremnowshowsthat
Divide-and-Rank
satis˝esgroupunanimityandis
alsostrategyproof,detailedproofisinSection7.2.
6.Inthecasewheretherearemultipletopologicalorderings,anyoneofthemsu˚ces.
9
Xu,Zhao,Shi,Zhang&Shah
Algorithm2
Divide-and-Rank
aggregation
Input:
pro˝le
ˇ
p
ˇ
p
1
q
p
P
1
q
;:::;ˇ
p
m
q
p
P
m
qq
,groups
p
R
C
;
P
C
q
;
p
R
s
C
;
P
s
C
q
with
|
P
C
|¥|
P
s
C
|
,
aggregationalgorithm
B
Output:
totalrankingofallpapers
1:
compute
ˇ
C
astherestrictionofpro˝le
ˇ
toonlypapersin
P
C
,and
ˇ

C
astherestriction
ofpro˝le
ˇ
toonlypapersin
P
s
C
2:
ˇ
C
Ð
Contract-and-Sort
p
ˇ
C
;
B
q
3:
ˇ
s
C
Ð
Contract-and-Sort
p
ˇ

C
;
B
q
4:
de˝ne
I


n
|
P
C
|
]
;
Y
2
n
|
P
C
|
]
;:::;n

5:
return
totalrankingobtainedby˝llingpapersin
P
C
intopositionsin
I
inordergiven
by
ˇ
C
,andpapersin
P
s
C
intopositionsin
r
n
sz
I
inordergivenby
ˇ
s
C
6:
7:
procedure
Contract-and-Sort
(pro˝le
r
ˇ
,aggregationalgorithm
B
)
8:
buildadirectedgraph
G
r
ˇ
withthepapersin
r
ˇ
asitsverticesandnoedges
9:
for
each
i
Pr
m
1
s
do
10:
denoting
ˇ
p
i
q
p
p
i
1
¡
:::
¡
p
i
t
i
),addadirectededgefrom
p
i
j
to
p
i
j

1
in
G
r
ˇ
,
@
j
Pr
t
i

1
s
11:
endfor
12:
foreveryorderedpair
p
p
j
1
;p
j
2
qP
E
G
r
ˇ
,replacemultipleedgesfrom
p
j
1
to
p
j
2
witha
singleedge
13:
computeatopologicalorderingofthestronglyconnectedcomponents(SCCs)in
G
r
ˇ
14:
foreverySCCin
G
r
ˇ
,computeapermutationofthepapersinthecomponentusing
algorithm
B
15:
return
thepermutationofallpapersthatisconsistentwiththetopologicalordering
oftheSCCsandthepermutationswithintheSCCs
16:
endprocedure
Theorem4.1.
Supposetheverticesof
C
canbepartitionedintotwogroups
p
R
C
;
P
C
q
and
p
R
s
C
;
P
s
C
q
suchthattherearenoedgesin
C
acrossthegroupsandthat
max
 
|
P
C
|
|
R
s
C
|
;
|
P
s
C
|
|
R
C
|
(
¤


.
Then
Divide-and-Rank
isgroupunanimousandstrategyproof.
Remark.
Our
Divide-and-Rank
frameworkaptlyhandlesthevariousnuancesofreal-world
conferencespeerreview,whichrenderotheralgorithmsinapplicable.Thisincludesthe
aspectsthateachreviewercanwritemultiplepapersandeachpapercanhavemultiple
authors,andfurthermorethateachreviewermayreviewonlyasubsetofpapers.Evenunder
thischallengingsetting,ouralgorithmguaranteesthatnoreviewercanin˛uencetheranking
ofherownpaperviastrategicbehavior,anditise˚cientfromasocialchoiceperspective.
Further,wedelvealittledeeperintotheinterleavingstep(Step5)oftheaggregation
algorithm.At˝rstglance,thisinterleavingperformedindependentofthereviewers'reports
maybeacauseofconcern.Indeed,assumingthereissomegroundtruthrankingofall
papersandevenundertheassumptionthattheoutputsofthe
Contract-and-Sort
procedure
areconsistentwiththisranking,theworstcasescenarioiswheretheinterleavingcauses
paperstobeplacedatapositionsthatare

p
n
q
awayfromtheirrespectivepositionsinthe
trueranking.Weshowthat,however,suchaworstcasescenarioisunlikelytoarise,when
10
OnStrategyproofConferencePeerReview
thegroundtruthrankingisindependentofthecon˛ictgraph.Wesummarizeour˝ndingsin
thefollowingproposition,withtheproofprovidedlaterinSection7.3.
Proposition4.2.
Suppose
C
satis˝estheconditionsgiveninTheorem4.1andthereexists
aconstant
c
¥
2
suchthat
max
t
|
P
|
|
P
C
|
;
|
P
|
|
P
s
C
|
u¤
c
.Assumetheground-truthranking
ˇ

is
chosenuniformlyatrandomfromallpermutationsin

p
P
q
independentof
C
,andthatthe
twopartialoutputsof
Contract-and-Sort
inAlgorithm2respect
ˇ

.Lettheoutputranking
of
Divide-and-Rank
be
p
ˇ
.Thenforevery
n
¥
4
c
{
log
2
,forany

Pp
0
;
1
q
,withprobabilityat
least
1


,wehave:
max
1
¤
i
¤
n
|
ˇ

i

p
ˇ
i
|¤
2
a
nc

log
p
2
n
{

q
:
Proposition4.2showsthatthemaximumdeviationbetweentheaggregatedrankingandthe
groundtruthrankingis
O
p
a
n
log
p
n
{

qq
withhighprobability.Hencewhen
n
islargeenough,
suchdeviationisnegligiblewhenprogramchairsofconferencesneedtomakeaccept/reject
decisions,wherethenumberofacceptedpapersusuallyscaleslinearlywith
n
.
Extensiontoreviewscores.
Ourframeworkextendstoascore-basedsetting,wherein
eachreviewer
r
i
providestheiropinionasascore
o
ij
foreverypaper
p
j
P
P
i
.Theassignment
algorithmremainsthesameinthissetting;foraggregation,wecanusethesameprocedure
withtherankinginducedbythereviewscores.Theonlydi˙erenceisthatinstep10of
Contract-and-Sort
,weaddanedgebetweeneverypairofpapers
p
j
1
Ñ
p
j
2
suchthat
o
ij
1
¡
o
ij
2
.
Thismakessurethatthegraphre˛ectstheopinionoftherevieweranddoesnotimpose
constraintsonpapersthatareequallyrated.Inthescore-basedsetting,theaggregation
algorithm
B
isallowedtoleveragethereviewscoresforamoregranularizedranking(e.g.,
meanscores).
5.Empiricalevaluations
Inthissection,weperformcertainempiricalevaluationsregardingthefeasibilityandperfor-
manceofour
Divide-and-Rank
algorithmbasedondatafromtheICLRconference
7
.Recall
thatthe
Divide-and-Rank
algorithmrestrictstheassignmentofreviewerstopapersaccording
toapartitionofreviewersandpapersintotwodisconnectedgroups.Bymeansofthese
empiricalevaluations,weinvestigatethefollowingquestions:
Q1.
Issuchapartitionfeasible?
Q2.
Howcanoneimpartmore˛exibilitytothepartition(whichcanallowforbetter
assignments)?
Q3.
Howdoesthequalityoftheassignmentcomparewithstandardsettingswithout
strategyproofness?
Q4.
Onemayenvisagethatreviewersthataremorerelatedtothetopicofapaperwouldbe
morelikelytobeconnected(inthecon˛ictgraph)tothatpaper.Under
Divide-and-Rank
,
7.
Thecodeanddataisavailableat
https://github.com/xycforgithub/StrategyProof_Conference_
Review
.
11
Xu,Zhao,Shi,Zhang&Shah
suchareviewerwillbebarredfrombeingassignedtosucharelatedpaper.Howmuch
doessucharestrictionoftheassignmentbetweenconnectedreviewers-papershurtthe
assignmentqualityascomparedtoassignmentunderauniformrandompartitionof
reviewersandpapers?
Themostprominenttypeofcon˛ictsisauthorships,andthroughoutthissectionwerestrict
attentiontotheauthorshipcon˛ictgraph.
5.1AnalysisoftheCon˛ictGraphonICLR2017submissions(Q1andQ2)
WeaddressquestionsQ1andQ2usingdatafromtheICLR2017conference.Inanutshell:
A1.
Yes,partitioningisfeasible.
A2.
Weshowthatremovingonlyasmallnumberofreviewerscanresultinadramatic
reductioninthesizeofthelargestcomponentinthecon˛ictgraphtherebyproviding
great˛exibilitytowardspartitioningthepapersandreviewers.Forinstance,removing
only
3
:
5%
ofallauthorsfromthereviewerpoolreducesthesizeofthelargestcomponent
(intermsofnumberofpapers)by86%.
WeanalyzeallpaperssubmittedtotheICLR2017conferencewiththegivenauthorship
relationshipasthecon˛ictgraph.ICLR2017received489submissionsby1417authors;
webelievethisdatasetisagoodrepresentativeofamedium-sizedmodernconference.In
theanalysisofthisdataset,weinstantiatethecon˛ictgraphastheauthorshipgraph.Itis
importanttonotethatweconsideronlythesetofauthorsastheentirereviewerpool(since
wedonothaveaccesstotheactualrevieweridentities).Addingreviewersfromoutsidethe
setofauthorswouldonlyimprovetheresultssincetheseadditionalreviewerswillhaveno
edgesintheauthorshipcon˛ictgraph.
Table1:StatisticsofICLR2017submissions.
DescriptionNumber
Numberofsubmittedpapers489
Numberofdistinctauthors1417
Mean#paperswrittenperauthor1.27
Maximum#paperswrittenbyanauthor14
Numberofconnectedcomponents253
#authors;#papersinlargestconnectedcomponent371;133
#authors;#papersinsecondlargestconnectedcomponent65;20
We˝rstinvestigatetheexistenceof(moderatelysized)componentsinthecon˛ictgraph.
Ouranalysisshowsthattheauthorshipcon˛ictgraphisdisconnected,andmoreover,has
morethan250components.Thelargestconnectedcomponent(CC)contains133(thatis,
about
27%
)ofallpapers,andthesecondlargestCCismuchsmaller.Wetabulatetheresults
fromouranalysisinTable1.ThesestatisticsindeedverifyourassumptioninTheorem4.1
thatthecon˛ictgraphisdisconnectedandcanbedividedintotwodisconnectedpartsof
similarsize.
12
OnStrategyproofConferencePeerReview
Thepartitioningmethodhaspreviouslybeenconsideredfortheproblemofpeergrad-
ing(Kahngetal.,2017).Thepeergradingsettingishomogeneousinthateachreviewer
(student)goesthroughthesamecourseandhenceanypaper(homework)canbeassigned
toanyreviewer.Inpeerreview,however,di˙erentreviewerstypicallyhavedi˙erentareas
ofexpertiseandhencetheirabilitiestoreviewanypapervariesbythesubjectareaofthe
paper.Inordertoaccommodatethisdiversityinareaofexpertiseinpeerreview,onemust
haveagreater˛exibilityintermsofassigningpaperstoreviewers.InouranalysisinTable1
wesawthatthelargestconnectedcomponentcomprises372authorsand133papers.Itis
reasonabletoexpectthatalargenumberofreviewerswithexpertiserequiredtoreviewthese
133papersmayfallinthesameconnectedcomponent,meaningthatanaïveapplicationof
Divide-and-Rank
tothisdatawouldassignthese133paperstoreviewerswhomayhavea
lowerexpertiseforthesepapers.Thisisindeedaconcern,andinwhatfollows,wediscussa
simpleyete˙ectivewaytoamelioratethisproblem.
Asimpleyet(aswedemonstratebelow)e˙ectiveideaistoremovesomeauthorsfrom
thereviewerpool.EmpiricallyusingtheICLR2017data,weshowthatbyremovingonlya
smallnumberofauthorsfromthereviewerpool,wecanmakethecon˛ictgraphconsiderably
moresparse,therebyallowingforasigni˝cantlymore˛exibleapplicationofouralgorithm
Divide-and-Rank
(ormoregenerally,anypartition-basedalgorithm).Weusethesimple
heuristicofremovingtheauthorswiththemaximumdegreeinthe(authorship)con˛ict
graph.Wethenstudytheresultingcon˛ictgraph(containingallsubmittedpapersbutonly
theremainingreviewers)intermsofthenumbersandsizesoftheconnectedcomponents.
WepresenttheresultsinTable2.Weseethatonremovingonlyasmallfractionofauthors
50authorswhichisonlyabout
3
:
5%
ofallauthorsthenumberofpapersinthelargest
connectedcomponentreducesby86%tojust18.Likewise,thenumberofauthorsinthe
largestconnectedcomponentreducesto55from371originally.
Table2:Statisticsofthecon˛ictgraphonremovingasmallnumber(
€
7%
)ofauthorsfromthe
reviewerpoolcomprisingthe1417authors.
#Authorsremovedfromreviewerpool
0510152050100
NumberofComponents253268278292302334389
NumberofAuthorsinLargestCC3713133042282055528
NumberofPapersinLargestCC1331141108274188
5.2AnalysisofthePartitionAlgorithmonICLR2018submissions(Q3and
Q4)
Intheprevioussection,weempiricallyveri˝edthatwecanpartitionthereviewersand
papersintotwodisconnectedgroups.Anaturalquestionthatarisesishowsuchapartition
a˙ectstotheoverallreviewer-papermatchingprocess,i.e.,willthepartitioncauseagreat
lossinqualityoftheassignmentalgorithmusedinpractice?Weempiricallyinvestigate
thisquestionbyperformingexperimentsusingdatafromtheICLR2018conference(where
wealsouseICLR2017asareferencepointlater).Inanutshell,usingthepopular
similarityasameasureofthequalityoftheassignment(detailedbelow),weseethat:
13
Xu,Zhao,Shi,Zhang&Shah
A3.
Incomparisontowhenthereisnostrategyproo˝ng,thequalityoftheassignment
reducesby
11%
whenitismadestrategyproofusingthe
Partition
algorithm.
A4.
Theutilityunder
Partition
isonlymarginallylowerthanwhenthepartitionisdone
uniformlyatrandom.Thus
Partition
isroughlyequivalenttoshrinkingtheconference
size(randomly)toahalf.
Wenowdescribetheexperimentinmoredetail.Wefollowtheassignmentframework
usedpopularlyinpractice,whichcomprisesoftwophases.The˝rstphasecomputesa
similarityscoreforevery(reviewer,paper)pair.Ahighersimilarityscoreisinterpretedasa
higherenvisagedqualityofreview.AsinthecaseofICLR2017above,wesetthecollection
ofallauthorsasthereviewerpoolsincetheactualidentitiesofthecollectionofreviewers
arenotavailable.Wethencomputeasimilaritybetweeneveryreviewer-paperpairbased
onthetextofthepaperandthecontentsofthereviewer'spublishedpapers.Wereferthe
readertoAppendixAfordetailsofthisconstruction.
Herearesomebasicstatisticsaboutthecomputedsimilaritymatrix.InFigure1awe
plotthehistogramofthecomputedsimilarityscoresbetween911papersand2435reviewers.
Themeanofthesimilarityscoresacrossallreviewer-paperpairsisapproximately
0
:
03
.This
skeweddistributionofsimilarityscoresisconsistentwithourintuition:foreachpaper,there
isonlyahandfulofreviewerswhohavethealignedbackgroundandexpertise.InFigure1b,
weshowthehistogramofthetopsimilarityscorecomputedforeachpaper(excluding
reviewersthatarealsoauthorsofthecorrespondingpaper).Weseethatthemean(across
allpapers)ofthesetopscoresisapproximately0.14,whichissigni˝cantlyhigherthanthat
of0.03amongallsimilarityscores.
(a)Scoresbetweenreviewersandpapers.
(b)Topscoreofeachpaper.
Figure1:ThelefthistogramisthesimilarityscoresbetweenreviewersandpapersoftheICLR2018
dataset.TherighthistogramplotsthetopsimilarityscoreofeachpaperintheICLR2018dataset.
Inbothplots,theverticalblacklineshowsthemeanofthedistribution.
Thesecondphaseoftheassignmentprocedureusesthesimilarityscorestoassignreviewers
topapers.ThemostwidelyusedassignmentmethodusedinpracticeistheTorontoPaper
MatchingSystemorTPMS(CharlinandZemel,2013).Thisassignmentmethodmaximizes
themeansimilarityscoreacrossallassignedreviewer-paperpairs.
14
OnStrategyproofConferencePeerReview
Inwhatfollows,weevaluatethreedi˙erentmethodsofassigningreviewerstopapersin
termsoftheresultingmeansimilarityscoreacrossallassignedreviewer-paperpairs:

UsingTPMSassignmentwithoutanypartitioning.

The
Divide-and-Rank
(i.e.,partitioningreviewersandpapersintotwosetsthatare
disconnectedintheauthorshipcon˛ictgraph)withTPMSastheassignmentalgorithm
A
.

Partitioningthereviewersandpapersintotwoequalgroupsuniformlyatrandom,and
usingTPMSwiththerestrictionofassigningeachreviewerstopapersfromtheother
group.
Weusethevalues


6
;

3
whicharetypicalofconferencestoday.Weprovidespeci˝c
implementationdetailsinAppendixA.
BasicstatisticsandexperimentresultsareshowninTable3.ICLRhasgrowndramatically
from2017to2018,withthenumberofpapersrisingfrom489to911,andthecorresponding
numbersofauthorsandcomponentsalsoalmostdouble.Thelargestcomponentnowhas757
authorsand274components,twicethesizeof2017.Ontheotherhand,thesecondlargest
componentissmallerthanthatof2017,whichwespeculateisbecausethemachinelearning
communityhasgrownintomorere˝nedsub˝elds,creatingmoresmallerclusters.Nevertheless,
wearestillabletodividetheauthorsandpapersintotwoclustersofapproximatelyequal
sizeusing
Partition
.
Table3:ResultofexperimentsonICLR2018data.
DescriptionNumber
Numberofsubmittedpapers911
Numberofdistinctauthors2428
Numberofconnectedcomponents465
#authors,#papersinlargestconnectedcomponent757,274
#authors,#papersinsecondlargestconnectedcomponent30,11
Meansimilarityscorewithoutpartitioning0.0880
Meansimilarityscorewithrandompartition(20runs)
0
:
0779

0
:
0001
Meansimilarityscorewith
Partition
0.0782
ThemeanofsimilarityscoresusingTPMSscoreassignment(seeAppendixA.2)without
partitioningis0.0880andthatwithpartitionis0.0782,whichrepresentsa
11
:
4%
decrease
frompartitioning.Ontheotherhand,ifwerandomlypartitionthereviewersandpapersto
twosetsofequalsizes,themean(andstandarddeviation)ofthesimilarityis
0
:
0779
(and
0
:
0001
respectively)from20runs.Theresultfrom
Partition
issimilartotheresultobtained
fromrandompartition.Sointermsofauthor-papersimilarities,
Partition
doesnoadditional
harmthanshrinkingtheconferencesize(randomly)toahalf.ForICLR2018,thisjust
correspondstothesizeofICLR2017.
15
Xu,Zhao,Shi,Zhang&Shah
6.NegativeTheoreticalResults
Thepositiveresultsintheprevioussectionfocusongroupunanimity,whichisweakerthan
theconventionalnotionofunanimity(theconventionalnotionisalsoknownaspairwise
unanimity).Moreover,thealgorithmhadadisconnectedreviewgraphwhereasthereview
graphsofconferencestodayaretypicallyconnected(Shahetal.,2017).Itisthusnaturalto
wonderabouttheextenttowhichtheseresultscanbestrengthened:Canapeer-reviewsystem
withaconnectedreviewergraphsatisfytheseproperties?Canastrategyproofpeer-review
systembepairwiseunanimous?Inthissectionwepresentsomenegativeresultstowardthese
questions,therebyhighlightingthecriticalimpedimentstowards(much)strongerresults.
Beforestatingourresults,weintroduceanothernotionofstrategyproofness,whichis
signi˝cantlyweakerthanthenotionofstrategyproofness(De˝nition3.1),andishencetermed
asweakstrategyproofness.Ascomparedtostrategyproofnesswhichisde˝nedwithrespect
toagivencon˛ictgraph,weakstrategyproofnessonlyrequirestheexistenceofacon˛ict
graph(withnon-zeroreviewer-degrees)forwhichthereviewprocessisstrategyproof.
De˝nition6.1
(WeakStrategyproofness,WSP)
.
Areviewprocess
p
G
;f
q
iscalled
weakly
strategyproof
,ifforeveryreviewer
r
i
,thereexistssomepaper
p
j
P
P
suchthatforevery
pairofdistinctpro˝les(underassignment
G
)
ˇ
p
ˇ
p
1
q
;:::;ˇ
p
i

1
q
;ˇ
p
i
q
;ˇ
p
i

1
q
;:::;ˇ
p
m
q
q
and
ˇ
1
p
ˇ
p
1
q
;:::;ˇ
p
i

1
q
;ˇ
p
i
q
1
;ˇ
p
i

1
q
;:::;ˇ
p
m
q
q
,itisguaranteedthat
f
j
p
ˇ
q
f
j
p
ˇ
1
q
.
Inotherwords,weakstrategyproofnessrequiresthatforeachreviewerthereisatleast
onepaper(notnecessarilysharescon˛ictsthisreviewer)whoserankingcannotbein˛uenced
bythereviewer.Asthenamesuggests,strategyproofnessisstrictlystrongerthanweak
strategyproofness,wheneachreviewerhasatleastonepaperofcon˛ict.
Wede˝nethenotionofweakstrategyproofnessmainlyfortheoreticalpurposestoestablish
negativeresults,sinceWSPistooweaktobepractical.However,eventhisextremelyweak
requirementisimpossibletosatisfyinsituationsofpracticalinterest.
Table4:Summaryofournegativeresults(˝rstthreerowsofthetable),andacomparisontoour
positveresult(fourthrow).
UnanimityStrategyproofRequirementon
G
Possible?Reference
PairwiseNoneMild(seeCorollary6.3)NoTheorem6.2
GroupWeakMild(Connected
G
)Conjecture:NoProposition6.4
PairwiseWeakComplete
G
NoTheorem6.5
GroupYesNoneYesTheorem4.1
WesummarizeourresultsinTable4.Recallthatweshowthepropertyofgroupunanimity
andstrategyprooffor
Divide-and-Rank
;asthe˝rstdirectionofpossibleextension,weshowin
Theorem6.2thattheslightlystrongernotionofpairwiseunanimityisimpossibletosatisfy
undermildassumptions,even
without
strategyproofconstraints.TheninSection6.2we
exploretheseconddirectionofextension,byrequiringaconnected
G
;wegiveconjecturesand
insightsthatgroupunanimityandweakstrategyproofnessisimpossibleunderthissetting.
AtlastinTheorem6.5wereverttothetraditionalsettingofsocialchoice,whereevery
reviewergivesatotalrankingofthesetofallpapers
P
;weshowthatinthissettingitis
impossibleforanyreviewprocesstobepairwiseunanimousandweaklystrategyproof.
16
OnStrategyproofConferencePeerReview
6.1ImpossibilityofPairwiseUnanimity
Weshowinthissectionthatpairwiseunanimityistoostrongtosatisfyundermildassumptions.
Theseassumptionsaremildinthesensethataviolationoftheassumptionsleadstoseverely
limitedandsomewhatimpracticalchoicesof
G
.
Inordertopreciselystateourresult,we˝rstintroducethenotionofa
review-relation
graph
H
.Givenapaper-reviewassignment
t
P
i
u
m
i

1
,thereview-relationgraph
H
isan
undirectedgraphwith
r
n
s
asitsverticesandwhereanytwopapers
p
j
1
and
p
j
2
areconnected
i˙thereexistsatleastonereviewerwhoreviewsboththepapers.Withthispreliminaryin
place,wearenowreadytostatethemainresultofthissection:
Theorem6.2.
If
H
hasacycleoflength3ormoreandthereisnosinglereviewerreviews
allthepapersinthecycle,thenthereisnoreviewprocess
p
G
;f
q
thatispairwiseunanimous.
TheproofofTheorem6.2issimilartoaCondorcetcycleproof,andthedetailsarein
Section7.4.Inthecorollarybelowwegivesomedirectimplicationsoftheconditionin
Theorem6.2when
|
P
1
||
P
m
|

,thatis,wheneveryreviewerranksasamenumber
ofpapers.
Corollary6.3.
Suppose
|
P
1
||
P
m
|

¥
2
.If
p
G
;f
q
ispairwiseunanimous,the
followingconditionshold:
(i)
H
doesnotcontainanycyclesoflength


1
ormore.
(ii)
Thesetofpapersreviewedbyanypairofreviewers
r
i
1
and
r
i
2
mustsatisfythecondition
|
P
i
1
X
P
i
2
|Pt
0
;
1
;
u
.Inwords,ifapairofreviewersreviewmorethanonecommon
papers,theymustreviewexactlythesameset.
(iii)
Thenumberofdistinctsetsin
P
i
;:::;
P
m
isatmost
n

1


1
.
Remarks.
Inmodernconferences(Shahetal.,2017),eachreviewerusuallyreviewsaround
3to6papers.Ifwemakethereviewprocesspairwiseunanimous,byCorollary6.3(iii)the
numberofdistinctreviewsetsismuchsmallerthanthenumberofreviewers;thisseverely
limitsthedesignofreviewsets,sincemanyreviewerswouldbenecessitatedtoreviewidentical
setsofpapers.Corollary6.3(ii)isarelated,strongrequirement,sincethespecializationof
reviewersmightnotallowforsuchlimitingoftheintersectionofreviewsets.Forinstance,
therearealargenumberofpairsofreviewerswhoreviewmorethanonecommonpaperbut
nonewithexactlythesamesetofpapers(Shahetal.,2017).
Insummary,Theorem6.2andCorollary6.3showthatitisdi˚culttosatisfypairwise
unanimity,evenwithoutconsideringstrategyproofness.Thisjusti˝esourchoiceofgroup
unanimityinthepositiveresults.
6.2GroupUnanimityandStrategyproofforaConnectedReviewGraph
Havingshownthatpairwiseunanimityistoostrongarequirementtosatisfy,wenowconsider
anotherdirectionforextensionconditionsonthereviewgraph
G
.Anaturalquestionfollows:
Underwhatconditiononthereviewgraph
G
arebothgroupunanimityandstrategyproofness
possible?Althoughwewillleavethequestionof˝ndingtheexactconditionopen,we
conjecturethatifwerequire
G
tobeconnected,thengroupunanimityandstrategyproofness
17
Xu,Zhao,Shi,Zhang&Shah
cannotbesimultaneouslysatis˝ed.Toshowourinsights,weanalyzeanextremelysimpli˝ed
reviewsetting.
Proposition6.4.
Considerany
n
¥
4
andsuppose
P

P
1
Y
P
2
Y
P
3
Y
P
4
,where
P
1
;P
2
;P
3
;P
4
aredisjointnonemptysetsofpapers.Considerareviewgraph
G
with
m

3
reviewers,where
reviewer
r
1
reviews
t
P
1
;P
2
u
,
r
2
reviews
t
P
2
;P
3
u
,and
r
3
reviews
t
P
3
;P
4
u
.Thenthereisno
aggregationfunction
f
thatisbothweaklystrategyproofandgroupunanimous.
Proposition6.4thusshowsthatforthesimplereviewgraphconsideredinthestatement,
groupunanimityandweakstrategyproofnesscannotholdatthesametime.Proofdetails
canbefoundinSection7.6.
Weconjecturethatsuchanegativeresultmayholdformoregeneralconnectedreview
graphs,andsuchanegativeresultmaybeprovedbyidentifyingacomponentofthegeneral
reviewgraphthatmeetstheconditionofProposition6.4.Thisshowsthatourdesignprocess
ofthereviewgraphinSection4isquiteessentialforensuringthoseimportantproperties.
6.3PairwiseUnanimityandStrategyproofunderTotalRanking
Throughoutthepapersofar,motivatedbytheapplicationofconferencepeerreview,we
consideredasettingwhereeveryreviewerreviewsa(small)subsetofthepapers.Incontrast,
abulkoftheclassicalliteratureinsocialchoicetheoryconsidersasettingwhereeachreviewer
ranks
all
candidatesorpapers(Arrow,1950;Satterthwaite,1975).Giventhislonglineof
literature,intellectualcuriositydrivesustostudythecaseofallreviewersreviewingall
papersforourconferencepeer-reviewsetting.
Wenowconsiderournotionofpairwiseunanimousandweaklystrategyproofinthis
sectionunderthistotal-rankingsetting,where
P
1

P
m

P
.Inthiscase,thereview
graph
G
isalwaysacompletebipartitegraph,anditonlyremainstodesigntheaggregation
function
f
.Althoughtotalrankingsmightnotbepracticalforlarge-scaleconferences,itis
stillhelpfulforsmaller-sizedconferencesandworkshops.
Underthistotalrankingsetting,weproveanegativeresultshowingthatpairwise
unanimityandstrategyproofnesscannotbesatis˝edtogether,andfurthermore,eventhe
notionofweakstrategyproofness(togetherwithPU)isimpossibletoachieve.
Theorem6.5.
Suppose
n
¥
2
.If
P
1

P
m

P
,thenthereisnoaggregationfunction
f
thatisbothnoaggregationfunction
f
thatisbothandpairwiseunanimous.
NotethattheconditionsrequiredforTheorem6.2arenotmetinthetotalrankingcase.
ToproveTheorem6.5(detailsinSection7.7.1),weuseCantor'sdiagonalizationargumentto
generateacontradictionbyassumingthereexists
f
thatisbothPUandWSP.
Itisinterestingtonotethatpairwiseunanimitycanbeeasilysatis˝edinthissettingof
totalrankings,byusingasimpleaggregationschemesuchastheBordacount.However,
Theorem6.5showsthatsurprisingly,evenundertheextremelymildnotionofstrategyproof-
nessgivenbyWSP,itisimpossibletoachievepairwiseunanimityandstrategyproofness
simultaneously.
7.Proofs
Inthissection,weprovidethedetailedproofsofalltheresultsfromprevioussections.
18
OnStrategyproofConferencePeerReview
7.1ProofofProposition3.4
Suppose
p
G
;f
q
isPU,and
P
1
•
P
satis˝esthateveryreviewerranksthepapersshereviewed
from
P
1
higherthanthoseshereviewedfrom
P
z
P
1
.Nowforevery
p
x
P
P
1
and
p
y
P
P
z
P
1
andreviewer
r
i
suchthat
r
i
reviewsboth
p
x
and
p
y
,
r
i
mustrank
p
x
¡
p
y
sinceotherwise
theassumptionof
P
1
isviolated.Since
f
isPU,weknowthat
f
p
ˇ
q
mustrespect
p
x
¡
p
y
as
well.Thisargumentholdsforevery
p
x
P
P
1
and
p
y
P
P
z
P
1
thathavebeenreviewedbyat
leastonereviewer,andhence
p
G
;f
q
isalsoGU.
7.2ProofofTheorem4.1
Weassumethattheconditiononthepartitioningofthecon˛ictgraph,asstatedinthe
statementofthistheorem,ismet.Webeginwithalemmawhichshowsthatforany
aggregationalgorithm
B
,
Contract-and-Sort
isgroupunanimous.
Lemma7.1.
Foranyassignmentandaggregationalgorithms
A
and
B
,theaggregation
procedure
Contract-and-Sort
isgroupunanimous.
WeprovethislemmainSection7.2.1.Undertheassumptionson

,

andsizesof
R
C
;
R
s
C
;
P
C
;
P
s
C
,itiseasytoverifythatthereisapaperallocationsatis˝es
|
P
i
|¤

@
i
Pr
m
s
andeachpapergetsatleast

reviews.Thestrategyproofnessof
Divide-and-Rank
followsfrom
thestandardideasinthepastliteratureonpartitioning-basedmethods(Alonetal.,2011):
Algorithm1guaranteesthatreviewersin
R
C
donotreviewpapersin
P
s
C
,andreviewers
in
R
s
C
donotreviewpapersin
P
C
.Hencethefactthat
Divide-and-Rank
isstrategyproof
triviallyfollowsfromtheassignmentprocedurewhereeachreviewerdoesnotreviewthe
papersthatareincon˛ictwithher,asspeci˝edbythecon˛ictgraph
C
.Giventhatallthe
otherreviewsare˝xed,therankingofthepapersincon˛ictwithherwillonlybedetermined
bytheothergroupofreviewersandso˝xednomatterhowshechangesherownranking.On
theotherhand,fromLemma7.1,since
Contract-and-Sort
isgroupunanimous,weknowthat
ˇ
C
and
ˇ
s
C
respectgroupunanimityw.r.t.
ˇ
C
and
ˇ

C
,respectively.Since
ˇ
p
ˇ
C
;
ˇ

C
q
,it
followsthat
ˇ
C
and
ˇ
s
C
alsorespectgroupunanimityw.r.t.
ˇ
.Finally,notethatthereisno
reviewerwhohasreviewedbothpapersfrom
P
C
and
P
s
C
,theinterlacingsteppreservesthe
groupunanimity,whichcompletesourproof.
7.2.1ProofofLemma7.1
Let
f
p
r
ˇ
q
:

Contract-and-Sort
p
r
ˇ
;
B
q
,where
r
ˇ
isapreferencepro˝le.De˝ne
ˇ

f
p
r
ˇ
q
.Let
k
denotethenumberofSCCsin
G
r
ˇ
.Constructadirectedgraph
r
G
r
ˇ
suchthateachofits
verticesrepresentsaSCCin
G
r
ˇ
,andthereisanedgefromonevertextoanotherin
r
G
r
ˇ
i˙
thereexistsanedgegoingfromoneSCCtotheotherintheoriginalgraph
G
r
ˇ
.Let
~
v
1
;:::;
~
v
k
beatopologicalorderingoftheverticesin
r
G
r
ˇ
.Since
~
v
1
;:::;
~
v
k
isatopologicalordering,
thenedgescanonlygofrom
~
v
j
1
to
~
v
j
2
where
j
1
€
j
2
.Nowconsideranycut
p
P
X
;
P
Y
q
in
G
r
ˇ
thatsatis˝estherequirementofgroupunanimity,i.e.,alledgesinthecutdirectfrom
P
X
to
P
Y
.Thenthereisnopairofpapers
p
x
P
P
X
and
p
y
P
P
Y
suchthat
p
x
and
p
y
areinthe
sameconnectedcomponent,otherwisetherewillbebothpathsfrom
p
x
to
p
y
and
p
y
to
p
x
,
contradictingthat
p
P
X
;
P
Y
q
formsacutwherealltheedgesgoinonedirection.Thisshows
that
P
X
and
P
Y
alsoformapartitionofallthevertices
~
v
1
;:::;
~
v
k
.Nowconsideranyedge
p
p
x
;p
y
q
from
P
X
to
P
Y
.Suppose
p
x
isincomponent
~
v
j
x
and
p
y
incomponent
~
v
j
y
.Wehave
19
Xu,Zhao,Shi,Zhang&Shah
j
x
˘
j
y
,since
P
X
and
P
Y
formsapartitionofallSCCs;alsoitcannothappenthat
j
x
¡
j
y
,
otherwise
~
v
1
;:::;
~
v
k
isnotatopologicalorderingreturnedby
f
.Soitmustbe
j
x
€
j
y
,and
theedge
p
p
x
;p
y
q
isrespectedinthe˝nalordering.
7.3ProofofProposition4.2
Wewould˝rstneedalemmaforthelocationofpapers:
Lemma7.2.
Let
I
1

!Y
n
|
P
C
|
]
;
Y
2
n
|
P
C
|
]
;:::;n
)
;I
2

!
g

n
|
P
s
C
|

;g

2
n
|
P
s
C
|

;:::;n

1
)
,where
g
p
x
q
r
x
s

1
isthelargestintegerthatis
strictly
smallerthan
x
.Then
I
1
X
I
2
H
and
I
1
Y
I
2
r
n
s
.
WeprovethislemmainSection7.3.1.
Consideranypaper
p
i
,andsupposeitspositionin
ˇ

is
`
.De˝ne
n
1
|
P
C
|
and
n
2
|
P

C
|
.Withoutlossofgeneralityassume
n
1
¥
n
2
(theothercaseissymmetric)andlet
n
¥
4
c
{
log2
.Wediscussthefollowingtwocasesdependingonwhether
p
i
P
P
C
or
p
i
P
P

C
.
CaseI
:If
p
i
P
P
C
.Let
k
bethenumberofpapersin
P
C
rankedstrictlyhigher(better)
than
`
accordingto
ˇ

.Sincethepermutation
ˇ

isuniformlyrandom,conditionedon
thisvalueof
`
,theotherpapers'positionsinthetruerankingareuniformlyatrandomin
positions
r
n
szt
`
u
.Nowforanypaper
p
j
;j
˘
i
,let
X
j
beanindicatorrandomvariableset
as1ifpositionof
p
j
ishigherthan
`
in
ˇ

,and0otherwise.So
k

°
p
j
P
P
C
zt
p
i
u
X
j
,and
Pr
p
X
j

1
q
`

1
n

1
when
j
˘
i
.ThenusingHoe˙ding'sinequalitywithoutreplacement,we
have
Pr





k
n
1

1

`

1
n

1




¥
""

¤
2exp

2
p
n
1

1
q
""
2
q¤
2exp

n
1
""
2
q
forany
""
¡
0
.Thelastinequalityisdueto
n
1
¥
2
,whichholdsbecause
n
{
n
1
¤
c
witha
constant
c
.Nowsetting
""

b
log
p
2
{

q
n
1
wehavethebound
Pr





k
n
1

1

`

1
n

1




¤
d
log
p
2
{

q
n
1

¥
1

:
NownotethatbyAlgorithm2,thepositionofpaper
p
i
intheranking
p
ˇ
is
p
ˇ
i

Y
p
k

1
q
n
n
1
]
.
Usethisrelationshiptosubstitute
k
intheaboveinequality,andnoticethatbyassumption
max
t
n
{
n
1
;n
{
n
2
u¤
c
,wehave
p
k

1
q
n
n
1
¤

p
n
1

1
q

""

`

1
n

1


1

n
n
1

n
1

1
n
1

n
n

1
p
`

1
q
n
1

1
n
1

n""

n
n
1
¤
n
n
1

n""

`

1
:
20
OnStrategyproofConferencePeerReview
Ontheotherhand,
p
k

1
q
n
n
1
¥

p
n
1

1
q

`

1
n

1

""


1

n
n
1

n
1

1
n
1

n
n

1
p
`

1
q
n
1

1
n
1

n""

n
n
1
¥
n
n
1

n""

n
1

1
n
1

n
n

1
p
`

1
q
:
So
p
k

1
q
n
n
1

`
¥
n
n
1

n""

n
1

1
n
1

n
n

1
p
`

1
q
`
(1)
¥
n
n
1

n""

n
1

1
n
1

n
n

1
p
n

1
q
n
(2)
¥
n"":
Here(2)isbecause
n
1

1
n
1

n
n

1
€
1
,andthusRHSof(1)isminimized(asafunctionof
`
)
when
`

n
.Combiningthetwoinequalitiesabovewehave
|
p
ˇ
i

`
|¤
n""

n
n
1

n
d
log
p
2
{

q
n
1

n
n
1
¤
2
a
nc

log
p
2
{

q
;
wherethelastinequalityisbytheassumptionthat
n
islargeenoughsothat
2
c
¤
a
nc

log
p
2
{

q
.
CaseII
:If
p
i
P
P

C
.Again,let
k
bethenumberofpapersin
P

C
rankedstrictly
higher(better)than
`
accordingto
ˇ

.AstheanalysisinCaseI,similarly,wehave
k

°
p
j
P
P

C
zt
p
i
u
X
j
,and
Pr
p
X
j

1
q
`

1
n

1
.WiththesameanalysisusingHoe˙ding's
inequalitywithoutreplacement,withprobabilityatleast
1


wehave




k
n
2

1

`

1
n

1




¤
d
log
p
2
{

q
n
2
:
NowusingLemma7.2,thepositionofpaper
p
i
in
p
ˇ
inthiscaseis
p
ˇ
i

g

p
k

1
q
n
n
2

.
UsingexactlythesameanalysisasCaseIwehave

n""
¤p
k

1
q
n
n
2

`
¤
n
n
2

n""

1
;
andthus
|
p
ˇ
i

`
|¤
n""

n
n
2

n
d
log
p
2
{

q
n
2

n
n
2
¤
2
a
nc

log
p
2
{

q
:
CombinebothCaseIandCaseII,andnoticethat
ˇ

i
isuniformlydistributedin
r
n
s
.
Usingaunionboundover
i

1
;
2
;:::;n
,withprobability
1


wehave:
max
1
¤
i
¤
n
|
p
ˇ
i

ˇ

i
|¤
2
a
nc

log
p
2
n
{

q
:
21
Xu,Zhao,Shi,Zhang&Shah
7.3.1ProofofLemma7.2
Weshowthatforeveryslot
q
thatthereisno
p
suchthat
Y
p

n
n
1
]

q
,thereexistsoneslot
p
1
for
P
s
C
suchthat
g

p
1

n
n
2


q
,i.e.,allslotsthatareleftemptyby
P
C
aretakenbyslots
of
P
s
C
.Sincethatthetwokindsofslotshaveatotalnumberof
n
,weshowthatthereareno
overlapbetweenthetwokindsofslots,thusprovingthelemma.
Let
t

n
{
n
1
.Supposeifthereisno
p
suchthat
Y
p

n
n
1
]

q
,thentheremustexistsome
^
p
suchthat
q

1
¤
^
pt
€
q

t:
(3)
Thisisbecausetheremustbeamultipleof
t
intherange
r
q;q

t
q
,butourassumption
makesthatthereisnosuchmultiplyin
r
q;q

1
q
.
Nowlet
u

n
{
n
2
.By
n
1

n
2

n
wehave
1
{
u

1
{
t

1
;substituting
t

u
{p
u

1
q
in
(3)wehave
q
€p
q

^
p

1
q
u
¤
q

1
:
Thusthereexists
p
1

g
pp
q

^
p

1
q
u
qP
P
s
C
.Thusweprovethelemma.
7.4ProofofTheorem6.2
TheproofofTheorem6.2isadirectformulationofourintuitioninSection6.1.Withoutloss
ofgeneralitylet
p
p
1
;:::;p
l
q
bethecyclenotreviewedbyasinglereviewer,for
l
¥
3
.Hence
thereexistsapartialpro˝le
ˇ
suchthatforallthereviewerswhohavereviewedboth
p
j
and
p
j

1
,
p
j
¡
p
j

1
;
@
j
Pr
l
s
(de˝ne
p
l

1

p
1
).Ontheotherhand,sinceforeachreviewer,
atleastonepair
p
p
j
;p
j

1
q
isnotreviewedbyher,theconstructedpartialpro˝leisvalid.
Nowassume
f
isPU,thenwemusthave
p
1
¡

¡
p
l
and
p
l
¡
p
1
,whichcontradictsthe
transitivityoftheranking.
7.5ProofofCorollary6.3
Weproveeachoftheconditionsinorder.
Proofofpart(i):
Ifthereisacycleofsize


1
,thennoreviewercanreviewallthepapers
initsinceitexceedsthesizeofreviewsets.Sothereisnosuchcycle.
Proofofpart(ii):
Thestatementtriviallyholdsfor


2
.For

¥
3
,Supposethereare
tworeviewers
r
i
1
and
r
i
2
suchthat
2
¤|
P
i
1
X
P
i
2
|¤


1
.Since
P
i
1
˘
P
i
2
,thereexist
papers
p
j
1
and
p
j
2
suchthat
p
j
1
P
P
i
1
z
P
i
2
and
p
j
2
P
P
i
2
z
P
i
1
.Also
|
P
i
1
X
P
i
2
|¥
2
,andlet
p
j
3
;p
j
4
P
P
i
1
X
P
i
2
.Byde˝nitionitiseasytoverifythat
p
p
j
1
;p
j
3
;p
j
2
;p
j
4
q
formsacyclethat
satis˝estheconditioninTheorem6.2,andhence
p
G
;f
q
isnotpairwiseunanimous.
Proofofpart(iii):
De˝neaer-relation
G
p
asfollows:Givenapaper-review
assignment
t
P
i
u
m
i

1
,thepaper-relationgraph
G
p
isanundirectedgraph,whosenodesarethe
distinct
setsin
t
P
i
u
m
i

1
;weconnecttworeviewsetsi˙theyhaveonepaperincommon.Note
thatby(ii),eachpairofdistinctsetshasatmostonepaperincommon.
22
OnStrategyproofConferencePeerReview
We˝rstshowthat
p
G
;f
q
ispairwiseunanimous,then
G
p
mustnecessarilybeaforest.If
thereisacyclein
G
p
,thenthereisacorrespondingcycleinthereviewrelationgraph
H
.To
seethis,notlosinggeneralitysupposetheshortestcyclein
G
p
is
p
P
1
;:::;
P
l
q
.Also,suppose
P
1
X
P
2
t
p
1
u
;
P
2
X
P
3
t
p
2
u
;:::;
P
l
X
P
1
t
p
l
u
notlosinggenerality.Then
p
p
1
;:::;p
l
q
formsacyclein
G
p
byitsde˝nition.Sinceeachreviewerreviewsexactlyonesetin
G
p
,there
isnoreviewerreviewingallpapersinthiscycleofpapersin
G
p
.Thustheconditionin
Theorem6.2issatis˝ed,and
p
G
;f
q
isnotpairwiseunanimous.
Wenowusethisresulttocompleteourproof.Considertheunionofallsetsofpapers
thatformverticesof
G
p
.Weknowthatthisunioncontainsexactly
n
paperssinceeachpaper
isreviewedatleastonce.Nowlet
k
p
denotethenumberofdistinctreviewsets(thatis,
numberofverticesof
G
p
),andlet
P
i
i
;:::;
P
i
k
p
denotetheverticesof
G
p
.Theunionofthree
ormoresetsin
t
P
i
k
u
k
p
k

1
isempty,sinceotherwisetherewillbeacyclein
G
p
.Usingthisfact,
weapplytheinclusion-exclusionprincipletoobtain
n

k
p
¸
k

1
|
P
i
k
|
¸
1
¤
k
1
€
k
2
¤
k
p
|
P
i
k
1
X
P
i
k
2
|
k
p

|
E
G
p
|
:
Nowusetheinequality
|
E
G
p
|¤
k
p

1
whicharisessince
G
p
isaforest,toobtaintheclaimed
bound
k
p
¤
n

1


1
.
7.6ProofofProposition6.4
Fixsomerankingofpaperswithineachindividualset
P
1
,
P
2
,
P
3
and
P
4
(e.g.,accordingto
thenaturalorderoftheirindices).Intheremainderoftheproof,anyrankingofallpapers
alwaysconsidersthese˝xedrankingswithintheseindividualsets.Withthisinplace,in
whatfollows,werefertoanyrankingintermsoftherankingsofthefoursetsofpapers.
Supposethereisonesuch
f
thatsatis˝esgroupunanimityandweakstrategyproofness
for
G
,andconsiderthefollowing4pro˝les:
(1)
r
1
:
P
1
¡
P
2
;r
2
:
P
2
¡
P
3
;r
3
:
P
3
¡
P
4
(2)
r
1
:
P
2
¡
P
1
;r
2
:
P
3
¡
P
2
;r
3
:
P
4
¡
P
3
(3)
r
1
:
P
2
¡
P
1
;r
2
:
P
2
¡
P
3
;r
3
:
P
3
¡
P
4
(4)
r
1
:
P
2
¡
P
1
;r
2
:
P
3
¡
P
2
;r
3
:
P
3
¡
P
4
BythepropertyofGU,pro˝le(1)leadstooutput
P
1
¡
P
2
¡
P
3
¡
P
4
,whereas(2)leadsto
output
P
4
¡
P
3
¡
P
2
¡
P
1
.Nowcompare(1)and(3):Theoutputof(3)musthave
P
2
at
thetopandsatisfy
P
3
¡
P
4
,bythepropertyofGU.Sotheoutputofpro˝le(3)mustbeone
ofi)
P
2
¡
P
1
¡
P
3
¡
P
4
,ii)
P
2
¡
P
3
¡
P
1
¡
P
4
,oriii)
P
2
¡
P
3
¡
P
4
¡
P
1
.Nownotethat
onlyreviewer
r
1
changesrankingacrosspro˝les(1)and(3),andhencebyWSPtheposition
ofatleastonepaperintheoutputofpro˝le(3)mustbethesameasinthatofpro˝le(1).
Thismakesiii)infeasible,sotheoutputof(3)mustbeeitheri)orii).Similarly,theoutput
of(4)iseither
P
3
¡
P
4
¡
P
2
¡
P
1
or
P
3
¡
P
2
¡
P
4
¡
P
1
.Nowcomparing(3)and(4):only
r
2
changesranking,butnoneofthefourpaperscanbeatthesamepositionnomatterhow
wechoosetheoutputsof(3)and(4).Thisyieldsacontradiction.
7.7ProofofTheorem6.5
Webeginwithade˝nitionofane
G
f
inducedbyanygivenaggregationrule
f
.
23
Xu,Zhao,Shi,Zhang&Shah
De˝nition7.3
(In˛uencegraph)
.
Foranyreviewaggregationrule
f
,thein˛uencegraph
G
f
inducedby
f
isabipartitegraphwithtwogroupsofvertices
R
and
P
,andedges
asfollows.Avertex
r
i
isconnectedtovertex
p
j
i˙thereexistsacertainpro˝le
ˇ
suchthat
r
i
isabletochangetheoutputrankingof
p
j
bychangingherownprefer-
ence.Formally,thereexistsanedgebetweenanypair
p
r
i
;p
j
qP
E
G
f
i˙thereexistpro˝les
ˇ
t
ˇ
p
1
q
;:::;ˇ
p
i

1
q
;ˇ
p
i
q
;ˇ
p
i

1
q
;:::;ˇ
p
m
q
u
and
ˇ
1
t
ˇ
p
1
q
;:::;ˇ
p
i

1
q
;
~
ˇ
p
i
q
;ˇ
p
i

1
q
;:::;ˇ
p
m
q
u
and
j
Pr
n
s
suchthat
f
p
ˇ
qp
j
q˘
f
p
ˇ
1
qp
j
q
.
Fromthisde˝nition,itisthusnothardtoseethat
f
isWSPifandonlyifthedegreeof
everyreviewernodein
G
f
isstrictlysmallerthan
n
.
Weprovetheclaimviaacontradictionargument.Assumethat
f
isbothPUand
WSP.Let
G
f
bethecorrespondingin˛uencegraph.Firstlyweshowthat
deg
p
p
q¡
0
for
everypaper
p
,wherethedegreeisforthein˛uencegraph
G
f
.Supposeotherwisethat
deg
p
p
j
q
0
forsomepaper
p
j
.Thismeans
no
reviewercana˙ecttherankingof
p
j
;in
otherwords,thepositionofpaper
p
j
is˝xedregardlessofthepro˝le.Thiscontradicts
withourassumptionofpairwiseunanimity;toseethis,pickanotherpaper
p
j
1
where
j
1
˘
j
(thisispossiblesince
n
¥
2
).Notlosinggeneralitysuppose
j
€
j
1
.Considerapro˝le
ˇ
whereeveryreviewerranks
p
j
¡
p
j
1
¡
P

j;j
1
q
,andanotherpro˝le
ˇ
1
whereeveryoneranks
p
j
`
¡
p
j
¡
P

j;j
1
q
;here
P

j;j
1
q
meanstheordinalrankingofpapersotherthan
p
j
;p
j
1
,i.e.,
p
1
¡

¡
p
j

1
¡
p
j

1
¡

¡
p
j
1

1
¡
p
j
1

1
¡

¡
p
n
.BythepropertyofPU,when
everyoneranksthesamethe˝nalresultmustbethesameaseveryone;howeverthismeans
thepositionof
p
j
isdi˙erentinthetwopro˝les,andthusthepositionof
p
j
isnot˝xed.This
makescontradictionandweprovethat
deg
p
p
q¡
0
foreverypaper
p
.
Nowforanyreviewer
r
i
P
R
,let
e
p
r
i
qP
P
bethepaperwiththelowestindexin
P
such
that
p
r
i
;e
p
r
i
qqR
E
G
f
.Since
f
isWSP,
e
p
r
i
q
mustexistforall
i
Pr
m
s
.De˝nethesetofsuch
papersas
P
1
:
t
e
1
;:::;e
m
1
ut
e
p
r
i
q
:
r
i
P
R
u
.Notethatwemusthave
m
1
¤
m
andinfact
m
1
canbestrictlysmallerthan
m
becauseofthepossibleoverlapbetween
e
p
r
i
q
;
@
i
Pr
m
s
.
Fromthede˝nitionof
m
1
andpropertyofWSP,itisclearthat
m
1
¥
1
.If
m
1

1
,wehave
p
r
i
;e
1
qR
E
G
f
foreveryreviewer
r
i
;thiscontradictswiththefactthat
deg
p
p
q¥
0
forevery
paper
p
.So
m
1
¡
1
:
Inthisproof,weslightlyoverloadthenotationof
f
e
k
p
ˇ
q
tomeanthepositionofpaper
e
k
in
f
p
ˇ
q
.Basedontheinversemappingfrom
P
1
to
R
,wepartitionallthereviewers
R
into
m
1
groups
t
R
1
1
;:::;
R
1
m
1
u
suchthatallreviewersinanyset
R
1
k
contributedpaper
e
k
whende˝ningset
P
1
.Inparticular,wehavethatnoreviewerin
R
1
j
isconnectedtopaper
e
j
inthein˛uencegraph
G
f
.
Inthedescriptionthatfollows,werestrictattentiontothepapersin
P
1
,andassumethat
inanyrankingallremaining
p
n

n
1
q
papersarepositionedattheendofthepreferencelist
ofanyreviewer.Nowconsiderthefollowingtwopreferencesover
P
1
:
ˇ

e
1
¡
e
2
¡

¡
e
m
1
;ˇ
1

e
2
¡
e
3
¡

¡
e
m
1
¡
e
1
:
Using
ˇ
and
ˇ
1
,de˝nethefollowing
m
1

1
di˙erentpro˝les:
ˇ
0
p
ˇ;:::;ˇ
q
;
ˇ
k
p
ˇ
1
;:::;ˇ
1
loooomoooon
k
;ˇ;:::;ˇ
looomooon
m
1

k
q@
k
Pr
m
1
s
:
24
OnStrategyproofConferencePeerReview
wherein
ˇ
k
the˝rst
k
preferencesare
ˇ
1
andthelast(
m
1

k
)preferencesare
ˇ
.Inwhat
follows,wewilluseadiagonalizationargumenttogenerateacontradictionusingthecondition
that
f
isWSP.We˝rstpresentalemma,whichweproveinSection7.7.1.
Lemma7.4.
If
f
ispairwiseunanimousandweaklystrategyproof,
f
e
k
p
ˇ
k
q
k
forevery
k
Pr
m
1
s
,thatis,underpro˝le
ˇ
k
,the
k
-thpositionintheoutputrankingmustbetakenby
paper
e
k
.
ApplyingLemma7.4with
k

m
1
,weobtainthat
f
e
m
1
p
ˇ
m
1
q
m
1
.However,onthe
otherside,since
ˇ
m
1
p
ˇ
1
;:::;ˇ
1
q
and
ˇ
1

e
2
¡
e
3
¡

¡
e
m
1
¡
e
1
,againbythePU
propertywehave
f
e
m
1
p
ˇ
m
1
q
1
.Thisleadstoacontradiction,hence
f
cannotbebothWSP
andPU.
7.7.1ProofofLemma7.4
Weprovebyinductionon
k
.
Basecase
.Since
f
isPU,theoutputranking
f
p
ˇ
0
q
mustbe:
f
p
ˇ
0
q
e
1
¡
e
2
¡

¡
e
m
1
Consider
k

1
.Notethat
ˇ
and
ˇ
1
di˙eronlyatthepositionof
e
1
,andin
ˇ
1
,only
P
1
1
changestheirpreferenceandalltheotherpreferencesarekept˝xed.ThenbytheWSPof
f
,theoutputrankingof
e
1
willnotbechangedbecause
P
1
1
arenotconnectedto
e
1
inthe
in˛uencegraph,sowemusthave:
f
p
ˇ
1
q
e
1
¡
e
2
¡

¡
e
m
1
;
Inductionstep
.Supposetheclaimofthislemmaholdsfor
t
1
;:::;k
u
.Considerthecaseof
k

1
.
Observethat
f
p
ˇ
k
qp
e
k
q
k
,andinboth
ˇ
and
ˇ
1
wehave:
e
k
¡
e
k

1
¡

¡
e
m
1
:
Thensince
f
isPU,weknowthatthelast
m
1

k

1
positionsintheoutputrankingof
f
p
ˇ
k
q
mustbegivenby
e
k
¡
e
k

1
¡

¡
e
m
1
,i.e.,
f
e
k

1
p
ˇ
k
q
k

1
.Thepro˝les
ˇ
k
and
ˇ
k

1
di˙eronlyinthepreferencegivenby
P
1
k

1
,andnoreviewerinset
P
1
k

1
canin˛uence
thepositionofpaper
e
k

1
.Itfollowsthat
f
e
k

1
p
ˇ
k

1
q
k

1
,whichcompletesourproof.
8.Discussion
Inthispaperweaddresstheproblemofdesigningstrategyproofande˚cientpeer-review
mechanism.Thesettingofpeerreviewischallengingduetothevariousidiosyncrasiesof
thepeer-reviewprocess:reviewersreviewonlyasubsetofpapers,eachpaperhasmultiple
authorswhomaybereviewers,andeachreviewermayauthormultiplesubmissions.We
provideaframeworkandassociatedalgorithmstoimpartstrategyproofnesstoconference
peerreview.Ourframework,besidesguaranteeingstrategyproofness,isimportantlyvery
˛exibleinallowingtheprogramchairstousethedecision-makingcriteriaoftheirchoice.
Wecomplementthesepositiveresultswithnegativeresultsshowingthatitisimpossiblefor
25
Xu,Zhao,Shi,Zhang&Shah
anyalgorithmtoremainstrategyproofandsatisfythestrongernotionofpairwiseunanimity.
Futureworkincludesconsideringe˚ciencyfromastatisticalperspectiveandcharacterizing
theprecisesetofcon˛ict-of-interestgraphsthatpermit(ornot)strategyproofness.
Theframeworkestablishedhereleadstoanumberofusefulopenproblems:

Canrecruitmentofasmallnumberofreviewerswithnocon˛icts(e.g.,incaseof
authorshipcon˛icts,reviewerswhohavenotsubmittedanypapers)leadtosigni˝cant
improvementsine˚ciency?Canbetterwaystoeliminatesomeauthorsfromthe
reviewerpoolincreaseapplicabilityofpartition-basedalgorithms?

Theresultsinthispaperconsideredthesocialchoicepropertyofunanimityasameasure
ofe˚ciency.Whilethiscanberegardedasa˝rst-ordernotionofe˚ciency,itisof
interesttoconsidercomplexnotionsofe˚ciency.Oneusefulnotionofe˚ciencyisthe
statisticalutilityofestimation(Stelmakhetal.,2019b)ofthe(partialorfull)rankingof
papersunderastatisticalmodelforreviewerreports.Analternativenotionofe˚ciency
combinesanassignmentqualitybasedonthesimilaritiesofassignedreviewersand
paperswithgroupunanimity(e.g.,maximizingthesimilarityscoresintheassignment
whilepreservinggroupunanimity).
Acknowledgments
ThisworkwassupportedinpartsbyNSFgrantsCRII:CIF:1755656andCIF:1763734.
References
Alon,N.,Fischer,F.,Procaccia,A.,andTennenholtz,M.(2011).Sumofus:Strategyproof
selectionfromtheselectors.In
Proceedingsofthe13thConferenceonTheoreticalAspects
ofRationalityandKnowledge
,pagesACM.
Anderson,M.S.,Ronning,E.A.,DeVries,R.,andMartinson,B.C.(2007).Theperverse
e˙ectsofcompetitiononscientists'workandrelationships.
Scienceandengineeringethics
,
.
Arrow,K.J.(1950).Adi˚cultyintheconceptofsocialwelfare.
Journalofpoliticaleconomy
,

Aziz,H.,Lev,O.,Mattei,N.,Rosenschein,J.S.,andWalsh,T.(2016).Strategyproofpeer
selection:Mechanisms,analyses,andexperiments.In
AAAI
,pages
Aziz,H.,Lev,O.,Mattei,N.,Rosenschein,J.S.,andWalsh,T.(2019).Strategyproofpeer
selectionusingrandomization,partitioning,andapportionment.
Arti˝cialIntelligence
.
Balietti,S.,Goldstone,R.L.,andHelbing,D.(2016).Peerreviewandcompetitioninthe
artexhibitiongame.
ProceedingsoftheNationalAcademyofSciences
,
Barnett,W.(2003).Themoderntheoryofconsumerbehavior:Ordinalorcardinal?
The
QuarterlyJournalofAustrianEconomics
,
26
OnStrategyproofConferencePeerReview
Bird,S.,Loper,E.,andKlein,E.(2009).Naturallanguageprocessingwithpythono'reilly
mediainc.
Bousquet,N.,Norin,S.,andVetta,A.(2014).Anear-optimalmechanismforimpartial
selection.In
InternationalConferenceonWebandInternetEconomics
,pages
Springer.
Brandt,F.,Conitzer,V.,Endriss,U.,Procaccia,A.D.,andLang,J.(2016).
Handbookof
computationalsocialchoice
.CambridgeUniversityPress.
Caragiannis,I.,Chatzigeorgiou,X.,Krimpas,G.A.,andVoudouris,A.A.(2017).Optimizing
positionalscoringrulesforrankaggregation.In
AAAI
,pages
Charlin,L.andZemel,R.S.(2013).TheTorontoPaperMatchingSystem:Anautomated
paper-reviewerassignmentsystem.
Connolly,R.,Miller,J.,andFriedman,R.(2014).Alongitudinalexaminationofsigite
conferencesubmissiondata,2007-2012.In
Proceedingsofthe15thAnnualConferenceon
Informationtechnologyeducation
,pagesACM.
DeClippel,G.,Moulin,H.,andTideman,N.(2008).Impartialdivisionofadollar.
Journal
ofEconomicTheory
,1
DíezPeláez,J.,LuacesRodríguez,Ó.,AlonsoBetanzos,A.,Troncoso,A.,andBaha-
mondeRionda,A.(2013).Peerassessmentinmoocsusingpreferencelearningviamatrix
factorization.In
NIPSWorkshoponDataDrivenEducation
.
Dör˛er,F.,Xiao,Y.,andvanderSchaar,M.(2017).Incentivedesigninpeerreview:
Ratingandrepeatedendogenousmatching.
IEEETransactionsonNetworkScienceand
Engineering
.
Douceur,J.R.(2009).Paperratingvs.paperranking.
ACMSIGOPSOperatingSystems
Review
,
Emerson,P.(2013).TheoriginalBordacountandpartialvoting.
SocialChoiceandWelfare
,
pages
Fiez,T.,Shah,N.,andRatli˙,L.(2019).ASUPER*algorithmtooptimizepaperbiddingin
peerreview.In
ICMLworkshoponReal-worldSequentialDecisionMaking:Reinforcement
LearningAndBeyond
.
Fischer,F.andKlimm,M.(2015).Optimalimpartialselection.
SIAMJournalonComputing
,
85.
Fishburn,P.C.(2015).
Thetheoryofsocialchoice
.PrincetonUniversityPress.
Fulkerson,D.andGross,O.(1965).Incidencematricesandintervalgraphs.
Paci˝cjournal
ofmathematics
,
Gao,Y.,Eger,S.,Kuznetsov,I.,Gurevych,I.,andMiyao,Y.(2019).Doesmyrebuttal
matter?insightsfromamajornlpconference.
arXivpreprintarXiv:1903.11367
.
27
Xu,Zhao,Shi,Zhang&Shah
Garg,N.,Kavitha,T.,Kumar,A.,Mehlhorn,K.,andMestre,J.(2010).Assigningpapersto
referees.
Algorithmica
,
Ge,H.,Welling,M.,andGhahramani,Z.(2013).ABayesianmodelforcalibratingconference
reviewscores.
Gibbard,A.(1973).Manipulationofvotingschemes:ageneralresult.
Econometrica:journal
oftheEconometricSociety
,pages
Hajek,B.,Oh,S.,andXu,J.(2014).Minimax-optimalinferencefrompartialrankings.In
AdvancesinNeuralInformationProcessingSystems
,pages
Hartvigsen,D.,Wei,J.C.,andCzuchlewski,R.(1999).Theconferencepaper-reviewer
assignmentproblem.
DecisionSciences
,30
Hazelrigg,G.(2013).Dearcolleagueletter:Informationtoprincipalinvestigators(PIs)plan-
ningtosubmitproposalstotheSensorsandSensingSystems(SSS)programOctober1,2013,
deadline.
Deadline(NSFWebsite,http://www.nsf.gov/pubs/2013/nsf13096/nsf13096.jsp)
.
Hojat,M.,Gonnella,J.S.,andCaelleigh,A.S.(2003).Impartialjudgmentbytheate-
keepofscience:fallibilityandaccountabilityinthepeerreviewprocess.
Advancesin
HealthSciencesEducation
,8(
Holzman,R.andMoulin,H.(2013).Impartialnominationsforaprize.
Econometrica
,

Kahng,A.B.,Kotturi,Y.,Kulkarni,C.,Kurokawa,D.,andProcaccia,A.D.(2017).Ranking
wilypeoplewhorankeachother.
TechnicalReport
.
Kurokawa,D.,Lev,O.,Morgenstern,J.,andProcaccia,A.D.(2015).Impartialpeerreview.
In
IJCAI
,pages
Langford,J.(2008).Adversarialacademia.
http://hunch.net/?p=499
.
Lawrence,N.andCortes,C.(2014).TheNIPSExperiment.
http://inverseprobability.
com/2014/12/16/the-nips-experiment
.[Online;accessed3-June-2017].
Makhorin,A.(2001).Gnulinearprogrammingkit.
MoscowAviationInstitute,Moscow,
Russia
,38.
Mathieus,C.(2008).SODAPCmeetings.
http://cs.brown.edu/~claire/SODAnotes.pdf
(lastretrievedMay21,2018).
Merri˝eld,M.R.andSaari,D.G.(2009).Telescopetimewithouttears:adistributed
approachtopeerreview.
Astronomy&Geophysics
,5
Noothigattu,R.,Shah,N.,andProcaccia,A.(2018).Choosinghowtochoosepapers.
arXiv
preprintarxiv:1808.09057
.
Piech,C.,Huang,J.,Chen,Z.,Do,C.,Ng,A.,andKoller,D.(2013).Tunedmodelsofpeer
assessmentinmoocs.
arXivpreprintarXiv:1307.2579
.
28
OnStrategyproofConferencePeerReview
Roos,M.,Rothe,J.,andScheuermann,B.(2011).Howtocalibratethescoresofbiased
reviewersbyquadraticprogramming.In
AAAI
.
Salton,G.,Wong,A.,andYang,C.-S.(1975).Avectorspacemodelforautomaticindexing.
CommunicationsoftheACM
,1
Satterthwaite,M.A.(1975).Strategy-proofnessandarrow'sconditions:Existenceand
correspondencetheoremsforvotingproceduresandsocialwelfarefunctions.
Journalof
economictheory
,
Schütze,H.,Manning,C.D.,andRaghavan,P.(2008).Introductiontoinformationretrieval.
In
Proceedingsoftheinternationalcommunicationofassociationforcomputingmachinery
conference
,volume4.
Shah,N.B.,Balakrishnan,S.,Bradley,J.,Parekh,A.,Ramchandran,K.,andWainwright,
M.J.(2016).Estimationfrompairwisecomparisons:Sharpminimaxboundswithtopology
dependence.
TheJournalofMachineLearningResearch
,1
Shah,N.B.,Bradley,J.K.,Parekh,A.,Wainwright,M.,andRamchandran,K.(2013).A
caseforordinalpeer-evaluationinmoocs.In
NIPSWorkshoponDataDrivenEducation
.
Shah,N.B.,Tabibian,B.,Muandet,K.,Guyon,I.,andvonLuxburg,U.(2017).Designand
AnalysisoftheNIPS2016ReviewProcess.
arXivpreprintarXiv:1708.09794
.
Stelmakh,I.,Shah,N.,andSingh,A.(2019a).Ontestingforbiasesinpeerreview.In
NeurIPS
.
Stelmakh,I.,Shah,N.B.,andSingh,A.(2019b).PeerReview4All:Fairandaccuratereviewer
assignmentinpeerreview.In
AlgorithmicLearningTheory
.
Stewart,N.,Brown,G.D.,andChater,N.(2005).Absoluteidenti˝cationbyrelative
judgment.
Psychologicalreview
,112(4):881.
Thurner,S.andHanel,R.(2011).Peer-reviewinaworldwithrationalscientists:Toward
selectionoftheaverage.
TheEuropeanPhysicalJournalB
,8
Tomkins,A.,Zhang,M.,andHeavlin,W.D.(2017).Reviewerbiasinsingle-versusdouble-
blindpeerreview.
ProceedingsoftheNationalAcademyofSciences
,114(48
Tsukida,K.andGupta,M.R.(2011).Howtoanalyzepairedcomparisondata.Technical
report,DTICDocument.
Wang,J.andShah,N.B.(2019).Your2ismy1,your3ismy9:Handlingarbitrary
miscalibrationsinratings.In
AAMAS
.
AppendixA.MoreDetailsontheICLR2018Experiment
Inthissectionwedescribeinmoredetailonthesimilarscoringmodelandtheoptimization
formulationusedinourICLR2018experimentinSection5.2.
29
Xu,Zhao,Shi,Zhang&Shah
A.1SimilarityScoringModel
We˝rstusetextrepresentationstomodelthereviewers.Inparticular,foreachreviewerin
thepool,wescrapetheir(atmost10)mostrecentpapersfromarXiv
8
asthecorresponding
textrepresentation.Topreprocessthetextofpapersandreviewers,weremovethestop
words,tokenizethetext,andthenusethe
PorterStemmer
(Birdetal.,2009)toobtainthe
wordstemofeachword.Afterthepreprocessingstep,thedictionarycontains
d

290158
uniquewords.Basedonthisdictionary,weusethevectorspacemodel(Saltonetal.,1975)to
representeachreviewer/paperasavectorin
R
d
.TheICLR2018datacontains911submitted
papersand2435reviewers,hencethereare
N

3346
documentsintotal.
Tocomputethesimilarityscoresbetweenreviewersandpapers,foreachdocument
D
(reviewerorpaper),wecomputethecorrespondingtermfrequency-inversedocument
frequency(tf-idf)(Schützeetal.,2008)scoreasthevectorrepresentation.Speci˝cally,fora
term
w
inthedocument,weuse
N
w
todenotethenumberoftimes
w
appearsinthecorpus
thatcontains
N
documents.Thentheinversedocumentfrequencyoftheterm
w
isgivenby:
idf
p
w
q
:

log
N
N
w
:
Topreventabiastowardslongerdocuments,e.g.,rawfrequencyof
w
dividedbytheraw
frequencyofthemostoccurringterminthedocument
D
,weusethefollowingaugmented
frequencyasthetermfrequencyof
w
indocument
D
:
tf
p
w;D
q
:

1
2

1
2
f
w;D
max
t
f
w
1
;D
:
w
1
P
D
u
;
whereweuse
f
w;D
todenotethenumberoftimesterm
w
appearingin
D
.Let
v
D
P
R
d
be
thevectorrepresentationofdocument
D
.Thenthevalueofthecoordinatecorrespondingto
term
w
isgivenby:
v
D
p
w
q
tf
p
w;D
q
idf
p
w
q

1
2

1
2
f
w;D
max
t
f
w
1
;D
:
w
1
P
D
u


log
N
N
w
:
Wethenconstructthesimilaritymatrix
S
P
R
m

n
betweenreviewersandpaperswhoseeach
entry
s
ij
Pr
0
;
1
s
correspondstothesimilarityscorebetweenreviewer
r
i
andpaper
p
j
.
s
ij
is
givenbythecosinesimilarityofthecorrespondingtf-idfvectors:
s
ij

v
T
r
i
v
p
j
}
v
r
i
}
2
}
v
p
j
}
2
Pr
0
;
1
s
:
A.2TheReviewer-PaperAssignmentAlgorithm
Matchingistheprocessofassigningpaperstoreviewers.Giventhesimilarityscorematrix
S
P
R
m

n
,wesolvethefollowingoptimizationproblem,asusedinthecurrentTPMS
system,tocomputetheassignment.Theoptimizationproblemformulatedin
(4)
isaninteger
8.
Toensurethatthedownloadedpapersbelongtothecorrespondingauthorinthegeneralareaofarti˝cial
intelligence,weonlyscrapepapersunderthefollowingcategories:cs.LG,cs.AI,stat.ML,cs.CV,cs.NE,
cs.CL,cs.GTandcs.RO.
30
OnStrategyproofConferencePeerReview
program,wheretheobjectivefunctioncorrespondstomaximizingthesumofsimilarityscores
inthematching.Hereforanyreviewer-paperpair
p
i;j
q
,wehave
a
ij

1
i˙paper
p
j
is
assignedtoreviewer
r
i
inthematching:
maximize
a
ij
¸
i
Pr
m
s
¸
j
Pr
n
s
s
ij
a
ij
subjectto
a
ij
Pt
0
;
1
u
;
@
i
Pr
m
s
;j
Pr
n
s
¸
j
a
ij
¤

@
i
Pr
m
s
¸
i
a
ij
¥

@
j
Pr
n
s
(4)
Theconstraint
°
j
a
ij
¤

meansthatwerestrictthemaximumnumberofpapersassigned
toareviewertobe

.Furthermore,wealsousetheconstraint
°
i
a
ij
¥

toenforcethat
eachpapershouldbereviewedbyatleast

reviewers.IntheICLR2018datathenumberof
optimizationvariables
a
ij
ismorethan2million,whichisintractabletosolveusingexisting
integerprogramsolvers.Soinstead,wecanrelaxtheaboveintegerprogramtothefollowing
linearprogram(LP):
maximize
a
ij
¸
i
Pr
m
s
¸
j
Pr
n
s
s
ij
a
ij
subjectto
0
¤
a
ij
¤
1
;
@
i
Pr
m
s
;j
Pr
n
s
¸
j
a
ij
¤

@
i
Pr
m
s
¸
i
a
ij
¥

@
j
Pr
n
s
(5)
IntheaboveLPwerelaxtheintegralconstraintover
a
ij
in
(4)
to
0
¤
a
ij
¤
1
,
@
i
Pr
m
s
;j
Pr
n
s
.
Duetotherelaxation,itisclearthattheoptimalvalueof
(5)
isatleastthatof
(4)
.Onthe
otherhand,observethatifwereformulatetheconstraints
°
j
a
ij
¤

and
°
i
a
ij
¥

into
thematrixform,thenthecorrespondingconstraintmatrixwillbethenode-edgeincidence
matrixofacompletebipartitegraphconsistingofasetofreviewersandasetofpapers.It
followsfromaknownsu˚cientcondition(FulkersonandGross,1965)thattheincidence
matrixofabipartitegraphis
totallyunimodular
,whichimpliesthatthesolutionoftheLP
in
(5)
isguaranteedtobeintegral.Henceinordertoobtaintheoptimalsolutionof
(4)
,we
canuseexistingpolynomialtimesolverstosolvetherelaxedLP,andsincetheconstraint
matrixin
(4)
istotallyunimodular,thisgivesusapolynomialtimealgorithmtocompute
theoptimalsolutionof
(4)
.InourimplementationweusetheGNULinearProgramming
Kit(GLPK)(Makhorin,2001)thatimplementsthesimplexalgorithmtosolvetheLPin
(5)
.
31
"
40,Retrofitting Distributional Embeddings to Knowledge Graphs with Functional Relations,http://arxiv.org/pdf/1708.00112v3.pdf,https://github.com/roaminsight/roamresearch,"RetrDistributionalEmbeddingstoKnowledgeGraphs
withFunctionalRelations
BenjaminJ.Lengerich
CarnegieMellonUniversity
5000ForbesAvenue
Pittsburgh,PA15213
blengeri@cs.cmu.edu
AndrewL.Maas
and
ChristopherPotts
StanfordUniversity,RoamAnalytics
195East4thAvenue
SanMateo,CA94401
f
amaas,cgpotts
g
@roaminsight.com
Abstract
Knowledgegraphsareaversatileframeworktoencoderichlystructureddatarelationships,but
itcanbechallengingtocombinethesegraphswithunstructureddata.Methodsfor
pre-trainedentityrepresentationstothestructureofaknowledgegraphtypicallyassumethat
entitiesareembeddedinaconnectedspaceandthatrelationsimplysimilarity.However,useful
knowledgegraphsoftencontaindiverseentitiesandrelations(withpotentiallydisjointunderly-
ingcorpora)whichdonotaccordwiththeseassumptions.Toovercometheselimitations,we
present
FunctionalRetr
,aframeworkthatgeneralizescurrentmethodsbyex-
plicitlymodelingpairwiserelations.Ourframeworkcandirectlyincorporateavarietyofpairwise
penaltyfunctionspreviouslydevelopedforknowledgegraphcompletion.Further,itallowsusers
toencode,learn,andextractinformationaboutrelationsemantics.Wepresentbothlinearand
neuralinstantiationsoftheframework.Functionaloutperformsexisting
methodsoncomplexknowledgegraphsandlosesnoaccuracyonsimplergraphs(in
whichrelationsdoimplysimilarity).Finally,wedemonstratetheutilityoftheframeworkby
predictingnewdrugŒdiseasetreatmentpairsinalarge,complexhealthknowledgegraph.
1Introduction
Distributionalrepresentationsofconceptsareofteneasytoobtainfromunstructureddatasets,butthey
tendtoprovideonlyablurrypictureoftherelationshipsthatexistbetweenconcepts.Incontrast,knowl-
edgegraphsdirectlyencodethisrelationalinformation,butitcanbediftosummarizethegraph
structureinasinglerepresentationforeachentity.
Tocombinetheadvantagesofdistributionalandrelationaldata,Faruquietal.(2015)proposeto
retr
embeddingslearnedfromdistributionaldatatothestructureofaknowledgegraph.Theirmethod
learnsentityrepresentationsbasedsolelyondistributionaldataandthenappliesastepto
updatetherepresentationsbasedonthestructureofaknowledgegraph.Thismodularapproachconve-
nientlyseparatesthedistributionaldataandentityrepresentationlearningfromtheknowledgegraphand
model,allowingonetoxiblycombine,reuse,andadaptexistingrepresentationstonew
tasks.
However,acoreassumptionofFaruquietal.(2015)'smodelisthatconnectedentities
shouldhavesimilarembeddings.Thisassumptionoftenfailstoholdinlarge,complexknowledge
graphs,foravarietyofreasons.First,subgraphsofaknowledgegraphoftencontaindistinctclasses
ofentitiesthataremostnaturallyembeddedindisconnectedvectorspaces.Intheextremecase,the
representationsfortheseentitiesmightderivefromverydifferentunderlyingdatasets.Forexample,in
ahealthknowledgegraph,thesubgraphscontainingdiseasesanddrugsshouldbeallowedtoformdis-
jointvectorspaces,andwemightwanttoderivetheinitialrepresentationsfromradicallydifferentdata
sets.Second,manyknowledgegraphscontaindiverserelationshipswhosesemanticsaredifferentfrom
ŒperhapseveninwithŒsimilarity.Forinstance,intheknowledgegraphinFigure1,themodel
ThisworkislicensedunderaCreativeCommonsAttribution4.0InternationalLicense.Licensedetails:
http:
//creativecommons.org/licenses/by/4.0/
Athelas
Kingsfoil
Black
Breath
Nazg
‹
ul
Aragorn
Is
Treats
Treats
Uses
Causes
Figure1:Toyknowledgegraphwithdiverserelationtypesthatconnecttreatments(green),diseases
(blue),andpersons(red)byknown(solid)andunknown(dashed)relations.Traditionalmethods,which
assumethatallrelationsimplysimilarity,wouldAragornandNazg
‹
ultowardsimilarembeddings.
ofFaruquietal.(2015)wouldmodelembeddings
q
Aragorn
ˇ
q
Athelas
ˇ
q
BlackBreath
ˇ
q
Nazg
^
ul
,
whichisproblematicasAragornisnotsemanticallysimilartoaNazg
‹
ul(theyareenemies).
Toaddresstheselimitations,wepresent
FunctionalRetr
,aframeworkthatexplicitly
modelspairwiserelationsasfunctions.Theframeworksupportsawiderangeofdifferentinstantiations,
fromsimplelinearrelationalfunctionstocomplexmultilayerneuralones.Here,weevaluatebothlin-
earandneuralinstantiationsofFunctionalonavarietyofdiverseknowledgegraphs.For
benchmarkingagainstexistingapproaches,weuseFrameNetandWordNet.Wethenmoveintothemed-
icaldomain,whereknowledgegraphsplayanimportantroleinknowledgeaccumulationanddiscovery.
TheseexperimentsshowthatevensimpleinstantiationsofFunctionaloutper-
formbaselinesonknowledgegraphswithsemanticallycomplexrelationsandnoaccuracyon
graphswhereFaruquietal.(2015)'sassumptionsaboutsimilaritydohold.Finally,weusethemodelto
identifypromisingnewdiseasetargetsforexistingdrugs.
CodewhichimplementsFunctionalisavailableat
https://github.com/
roaminsight/roamresearch
.
2Notation
Aknowledgegraph
G
iscomposedofasetofvertices
V
,asetofrelationtypes
R
,andasetofedges
E
whereeachedge
e
2E
isatuple
(
i;j;r
)
inwhichtherelationship
r
2R
holdsbetweenvertices
i
2V
and
j
2V
.Ourgoalistolearnasetofrepresentations
Q
=
f
q
i
:
i
2Vg
whichcontainthe
informationencodedinboththedistributionaldataandtheknowledgegraphstructure,andcanbeused
fordownstreamanalysis.Throughoutthispaper,weuse
a
torefertoascalar,
a
torefertoavector,and
A
torefertoamatrixortensor.
3RelatedWork
Hereweareinterestedin
post-hoc
methods,whichadjustentityembeddingstothestructure
ofapreviouslyunseenknowledgegraph.
3.1RetrModels
TheprimaryintroductionofwasFaruquietal.(2015),inwhichtheauthorsshowedthevalue
ofsemanticembeddingsaccordingtominimizationoftheweightedleastsquaresproblem

G
(
Q
)=
X
i
2V

i
k
q
i

^
q
i
k
2
+
X
(
i;j;r
)
2E

ij
k
q
i

q
j
k
2
(1)
where
^
Q
=
f
^
q
i
:
i
2Vg
istheembeddinglearnedfromthedistributionaldataand

i
,

ij
settherelative
weightingofeachtypeofdata.When

i
=1
and

ij
=
1
degree
(
i
)
,thismodelassignsequalweighttothe
distributionaldataandthestructureoftheknowledgegraph.
Morerecently,Hamiltonetal.(2017)presentedGraphSAGE,atwo-stepmethodwhichlearnsbothan
aggregationfunction
f
:
R
d

n
!
R
k
,tocondensetherepresentationsofneighborsintoasinglepoint,
andanupdatefunction
g
:
R
k
+
d
!
R
d
,tocombinetheaggregationwithacentralvertex.Here,
d
isthe
embeddingdimensionality,
k
istheaggregationdimensionality,and
n>
0
isthenumberofneighborsfor
eachvertex.Notethat
k>d
ispermitted,allowingforaggregationbyconcatenation.Whilethismethod
isextremelyeffectiveforlearningrepresentationsonsimpleknowledgegraphs,itisnotformulatedfor
knowledgegraphswithmultipletypesofrelations.Furthermore,whentherepresentationofarelation
isknown
apriori
,itcanbeusefultoexplicitlysetthepenaltyfunction(e.g.,Mrk

sicetal.(2016)use
hand-craftedfunctionstoeffectivelymodelantonymyandsynonymy).Byaggregatingneighborsinto
apointestimatebeforecalculatingrelationshiplikelihoods,GraphSAGEmakesitdiftoencode,
learn,orextracttherepresentationofapairwiserelation.
Inasimilarvein,Faruquietal.(2016)developedagraph-basedsemi-supervisedlearningmethodto
expandmorpho-syntacticlexiconsfromseedsets.Thoughthetaskisdifferentfromthetask
weconsiderhere,theperformanceandscalabilityoftheirmethoddemonstratetheutilityofdirectly
encodingpairwiserelationsasmessage-passingfunctions.
3.2RelationalPenaltyFunctions
OurnewFunctionalframeworkmodelseachrelationviaapenaltyfunction
f
r
:
R
d
i
+
d
j
!
R

0
actingonapairofentities
(
i;j
)
withembeddingdimensionalities
d
i
and
d
j
,respectively.By
explicitlymodelingrelationsbetweenpairsofentities,Functionalsupportstheuseofawide
arrayofscoringfunctionsthathavepreviouslybeendevelopedforknowledgegraphcompletion.Here,
wepresentabriefreviewofsuchscoringfunctions;foranextensivereview,see(Nickeletal.,2016).
TransE(Bordesetal.,2013)usesadditiverelationsinwhichthepenaltyfunction
f
r
(
q
i
;
q
j
)=
k
q
i
+
a
r

q
j
k
2
2
islowiff
(
i;j;r
)
2E
.ThesimpleUnstructuredModel(Bordesetal.,2012)was
proposedasana
¨
eversionofTransEthatassignsall
a
r
=
0
,leadingtothepenaltyfunction
f
r
(
q
i
;
q
j
)=
k
q
i

q
j
k
2
2
.Thisistheunderlyingpenaltyfunctionof(Faruquietal.,2015).Itcannot
considermultipletypesofrelations.Inaddition,whileitmodels1-to-1relationswell,itstrugglesto
modelmultivaluedrelations.
TransH(Wangetal.,2014)wasproposedtoaddressthislimitationbyusingmultiplerepresentations
forasingleentityviarelationhyperplanes.Forarelation
r
,TransHmodelstherelationasavector
a
r
onahyperplanebynormalvector
w
r
.Foratriple
(
i;j;r
)
2E
,theentityembeddings
q
i
and
q
j
areprojectedtothehyperplaneof
w
r
.Byconstraining
k
w
r
k
2
=1
,wehavethepenaltyfunction
f
r
(
q
i
;
q
j
)=
k
g
r
(
q
i
)+
a
r

g
r
(
q
j
)
k
2
2
where
g
r
(
x
)=
x

w
T
r
xw
r
.
TransR(Linetal.,2015)embedsrelationsinaseparatespacefromentitiesbyamatrix
M
r
2
R
d

k
thatprojectsfromentityspacetorelationspaceandasharedrelationvector
a
2
R
k
that
translatesinrelationspaceby
f
r
(
q
i
;
q
j
)=
k
q
i
M
r
+
a

q
j
M
r
k
2
2
.Weusethismodelastheinspiration
forourlinearpenaltyfunction.
TheNeuralTensorNetwork(NTN;Socheretal.(2013))ascorefunction
f
r
(
q
i
;
q
j
)=
u
T
r
g
(
q
T
i
M
r
q
j
+
M
r;
1
q
i
+
M
r;
2
q
j
+
b
r
)
where
u
r
isalinearlayer,
g
:
R
k
!
R
k
isthetanhoperationappliedelement-wise,
M
r
2
R
d

d

k
isa3-waytensor,and
M
r;
1
;
M
r;
2
2
R
k

d
areweightmatrices.AllofthesemodelscanbedirectlyincorporatedinourFunctional
framework.
4FunctionalRetr
WeproposetheframeworkofFunctional(FR)toincorporateaset
F
of
penaltyfunctions
f
r
:
R
d
i
+
d
j
!
R

0
whichpenalizesembeddingsofentities
i;j
withdimensionalities
d
i
;d
j
,respectively.Thisgivesthecompleteminimization:

G
(
Q
;
F
)=
X
i
2Q

i
jj
q
i

^
q
i
jj
2
+
X
(
i;j;r
)
2E

i;j;r
f
r
(
q
i
;
q
j
)

X
(
i;j;r
)
2E


i;j;r
f
r
(
q
i
;
q
j
)+
X
r
2R
ˆ

(
f
r
)
(2)
where
^
q
i
isobservedfromdistributionaldata,

i
and

i;j;r
settherelativestrengthsofthedistributional
dataandtheknowledgegraphstructure,and
ˆ
regularizes
f
r
withstrengthsetby

.
E

isthe
negative
space
oftheknowledgegraph,asetofedgesthatarenotannotatedintheknowledgegraph.FRuses
E

to
penalizerelationsthatareimpliedbytherepresentationsbutnotannotatedinthegraph.Topopulate
E

,
wesampleasinglenegativeedge
(
i;j
0
;r
)
withthesameoutgoingvertexforeachtrueedge
(
i;j;r
)
2E
.
Theusercancalibratetrustinthecompletenessoftheknowledgegraphviathe

hyperparameter.
Incontrasttopriorwork,FRexplicitlyencodesdirectedrelations.Thisallowsthemodel
tographswhichcontaindiverserelationtypesandentitiesembeddedindisconnectedvectorspaces.
Here,wecomparetheperformanceoftwoinstantiationsofFRŒonewithalllinearrelationsandone
withallneuralrelationsŒandshowthateventhesesimplemodelsprovideperformance
improvements.Inpractice,werecommendthatusersselectfunctionsinaccordance
withthesemanticsoftheirgraph'srelations.
4.1LinearRelations
Weimplementalinearrelationalpenaltyfunction
f
r
(
q
i
;
q
j
)=
k
A
r
q
j
+
b
r

q
i
k
2
with
`
2
regulariza-
tionforminimizationof:

G
(
Q
;
F
)=
n
X
i
=1

i
k
q
i

^
q
i
k
2
+
X
(
i;j;r
)
2E

i;j;r
k
A
r
q
j
+
b
r

q
i
k
2

X
(
i;j;r
)
2E


i;j;r
k
A
r
q
j
+
b
r

q
i
k
2
+

X
r
2R
jj
A
r
jj
2
(3)
IdentityRelations
Faruquietal.(2015)'smodelisaspecialcaseofthisformulationinwhich
A
r
=
I
;
b
r
=
0
8
r;
i;j;r
=
(
1
degree
(
i
)
(
i;j;r
)
2E
0(
i;j;r
)
2E

Throughouttheremainderofthispaper,werefertothisbaselinemodelastheﬁFR-Identityﬂ
method.
Initialization
Weinitializeembeddingsasthoselearnedfromdistributionaldataandrelationstoimplysimilarity:
A
r
=
I
;
b
r
=
0
;
i
=
(
0
^
q
i
=
0

^
q
i
6
=
0
;
i;j;r
=
(

+
d
r
(
i
)
(
i;j;r
)
2E


d
r
(
i
)
(
i;j;r
)
2E

where
d
r
(
i
)
istheout-degreeofvertex
i
forrelationtype
r
,

isahyperparametertotradeoffdistribu-
tionaldataagainststructuraldata,and

setsthetrustincompletenessoftheknowledgegraphstructure.
Inourexperiments,weuse

+
=1
;

=0
forstraightforwardcomparisonwiththemethodofFaruqui
etal.(2015)andoptimize

bycross-validation.Givenpriorknowledgeaboutthesemanticmeaningof
relations,wecouldinitializerelationstorespectthesemeanings(e.g.,antonymycouldberepresentedby
A
r
=

I
).
LearningProcedure
Weoptimizethismodelbyblockoptimization.Conveniently,wehaveclosed-formsolutionswherethe
partialderivativesofEq.3equal
0
:
b
r
=
P
(
i;j
)
(

1)
I
f
(
i;j;r
)
=
2Eg

i;j;r
(
A
r
q
j

q
i
)
P
(
i;j
)
(

1)
I
f
(
i;j;r
)
=
2Eg

i;j;r
(4)
~
A
r
=
UV

1
(5)
U
=
X
(
i;j
):(
i;j;r
)
2E

i;j;r
(
q
i

b
r
)
q
T
j

X
(
i;j
):(
i;j;r
)
2E


i;j;r
(
q
i

b
r
)
q
T
j
(6)
V
=
X
(
i;j
):(
i;j;r
)
2E

i;j;r
q
j
q
T
j

X
(
i;j
):(
i;j;r
)
2E


i;j;r
q
j
q
T
j
+

I
(7)
Constraining
A
r
tobeorthogonalby
A
r
=
~
A
r
(
~
A
T
r
~
A
r
)

1
=
2
,wehave
q
i
=
a
i
b
i
where
a
i
=

i
^
q
i
+
X
(
j;r
):(
i;j;r
)
2E

i;j;r
(
A
r
q
j
+
b
r
)+
X
(
j;r
):(
j;i;r
)
2E

j;i;r
A
T
r
(
q
j

b
r
)

X
(
j;r
):(
i;j;r
)
2E


i;j;r
(
A
r
q
j
+
b
r
)

X
(
j;r
):(
j;i;r
)
2E


j;i;r
A
T
r
(
q
j

b
r
)
(8)
b
i
=

i
+
X
(
j;r
):(
i;j;r
)
2E

i;j;r
+
X
(
j;r
):(
j;i;r
)
2E

j;i;r

X
(
j;r
):(
i;j;r
)
2E


i;j;r

X
(
j;r
):(
j;i;r
)
2E


j;i;r
(9)
4.2NeuralRelations
WealsoinstantiateFRwithaneuralpenaltyfunction
f
r
(
q
i
;
q
j
)=
˙
(
q
T
i
A
r
q
j
)
where
˙
istheelement-
wisetanhoperation,
A
r
2
R
d
i

d
j
,againwith
`
2
regularization.Weinitializeweightsinasimilarmanner
asforthelinearrelationsandupdateviastochasticgradientdescent.Inourexperiments,weuse

+
=


=1
,andsamplethesamenumberofnon-neighborsastrueneighbors.
5Experiments
WetestFRonfourknowledgegraphs.Thetwoarestandardlexicalknowledgegraphs(FrameNet,
WordNet)inwhichFRimprovesqualityoncomplexgraphsandlosesnoac-
curacyonsimplegraphs.Thetwographsarelargehealthcareontologies(SNOMED-CT,Roam
HealthKnowledgeGraph),whichdemonstratethescalabilityoftheframeworkandtheutilityofthenew
embeddings.
Foreachgraph,wesuccessivelyevaluatelinkpredictionaccuracyaftertolinksofother
relationtypes.,foreachrelationtype
r
2R
,weto
G
n
r
=(
V
;
E
n
r
)
where
E
n
r
=
f
(
i;j;r
0
):(
i;j;r
0
)
2E
,
r
0
6
=
r
g
isthesetofedgeswithallrelationsoftype
r
removed.After
wetrainaRandomForesttopredictthepresenceofrelation
r
betweenentities
i
and
j
(with70%
ofverticesselectedastrainingexamplesandtheremainderreservedfortesting).Tohavebalancedclass
labels,wesampleanequivalentnumberofnon-edges,
E

r
=
f
(
i;j;r
):(
i;j;r
)
=
2Eg
with
jE

r
j
=
jEj
and
jf
j
:(
i;j;r
)
2E

r
gj
=
jf
j
:(
i;j;r
)
2Egj8
i
.Thus,therandombaselinerateisset
to
50%
.Otherbaselinesaretheembeddingsbuiltfromdistributionaldataandthemethodof
Faruquietal.(2015),denotedasﬁNoneﬂandﬁFR-Identityﬂ,respectively.
5.1FrameNet
FrameNet(Bakeretal.,1998;Fillmoreetal.,2003)isalinguisticknowledgegraphcontaininginfor-
mationaboutlexicalandpredicateargumentsemanticsoftheEnglishlanguage.FrameNetcontainstwo
distinctentityclasses:
frames
and
lexicalunits
,wherea
frame
isameaninganda
lexicalunit
isasingle
meaningforaword.TocreateagraphfromFrameNet,weconnectlexicalunit
i
toframe
j
if
i
occurs
in
j
.WedenotethisrelationasﬁFrameﬂ,anditsinverseﬁLexicalunitﬂ.Finally,weconnectframesby
thestructureofFrameNet(Table6).DistributionalembeddingsarefromtheGoogleNewspre-trained
Word2Vecmodel(Mikolovetal.,2013a);thecountsofeachentitytypethatwerealsofoundinthe
distributionalcorpusareshowninTable6.
Results
AsseeninTable1,therepresentationslearnedbyFR-LinearandFR-Neuralaremoreuseful
forlinkpredictionthanthoseofthebaselinemethods.
Retr
Model
`Inheritance'`Using'`ReframingMapping'`Subframe'`PerspectiveOn'
(2132/992)(1552/668)(544/312)(356/168)(336/148)
None
87
:
58

1
:
0488
:
59

1
:
9385
:
60

1
:
8091
:
24

0
:
8689
:
59

3
:
25
FR-Identity
90
:
79

0
:
6987
:
87

1
:
4887
:
02

0
:
6394
:
50

1
:
7094
:
24

1
:
02
FR-Linear
92
:
92

0
:
16
92
:
04

1
:
45
89
:
37

2
:
45
94
:
65

1
:
05
94
:
73

1
:
12
FR-Neural
92
:
46

0
:
67
92
:
54

1
:
4589
:
57

0
:
7095
:
65

2
:
21
94
:
04

0
:
58
Retr
Model
`Precedes'`SeeAlso'`CausativeOf'`InchoativeOf'
(220/136)(268/76)(204/36)(60/16)
None
87
:
30

4
:
33
85
:
11

3
:
2086
:
11

6
:
0082
:
50

14
:
29
FR-Identity
85
:
26

4
:
4683
:
81

2
:
1484
:
49

8
:
7278
:
33

20
:
14
FR-Linear
87
:
00

2
:
1891
:
93

1
:
06
92
:
09

6
:
34
82
:
50

14
:
29
FR-Neural
89
:
16

5
:
6093
:
25

1
:
7994
:
33

4
:
6885
:
00

7
:
07
Table1:RetoFrameNet.Reportedvaluesaremeanandstandarddeviationofthelinkprediction
accuraciesoverthreeexperiments.Thenumberofedgesusedfor(training/testing)isshownbeloweach
edgetype.
5.2WordNet
WordNet(Miller,1995;Fellbaum,2005)isalexicaldatabaseconsistingofwords(lemmas)whichare
groupedintounorderedsetsofsynonyms(synsets).ToexaminetheperformanceofFRonknowledge
graphswhichpredominatelysatisfytheassumptionsofFaruquietal.(2015),weextractasimpleknowl-
edgegraphoflemmasandtheconnectionsbetweentheselemmasthatareannotatedinWordNet.These
connectionsaredominatedbyhypernymyandhyponymy(Table7),whichcorrelatewithsimilarity,so
weexpectthebaselinemethodtoperformwell.
Results
AsseeninTable2,theincreasedxibilityoftheFRframeworkdoesnotdegradeembeddingquality
evenwhenthisextraxibilityisnotintuitivelynecessary.Here,weevaluatestandardlexicalmetrics
forwordembeddings:wordsimilarityandsyntaticrelations.Forwordsimiliaritytasks,theevaluation
metricistheSpearmancorrelationbetweenpredictedandannotatedsimilarities;forsyntaticrelation,
theevaluationmetricisthemeancosinesimilaritybetweenthelearnedrepresentationofthecorrect
answerandthepredictionbythevectoroffsetmethod(Mikolovetal.,2013b).Incontrasttoourother
experiments,heretheonlystochasticbehaviorisduetostochasticgradientdescenttraining,notsampling
ofevaluationsamples.EventhoughtheWordNetknowledgegraphlargelytheassumptionsof
thena
¨
emodel,thexibleFRframeworkachievessustainedimprovementsforbothword
similaritydatasets(WordSim-353;Finkelsteinetal.(2001),Mturk-771
1
,andMTurk-287andsyntatic
relations(GoogleAnalogyTestSet
2
).
1
http://www2.mta.ac.il/
Ÿ
gideon/mturk771.html
2
http://download.tensorflow.org/data/questions-words.txt
Retr
Model
WordSimilaritySyntacticRelation
WordSim-353MTurk-771MTurk-287GoogleAnalogy
None
0
:
5120
:
5380
:
6710
:
772
FR-Identity
0
:
5120
:
5320
:
6640
:
774
FR-Linear
0
:
5420
:
5620
:
6790
:
793
FR-Neural
0
:
516

0
:
001
0
:
543

0
:
001
0
:
676

0
:
001
0
:
784

0
:
000
Table2:toWordNet.ReportedvaluesareSpearmancorrelationsforthewordsimilarity
tasksandmeancosinesimilarityforthesyntaticrelationtask.Thesearedeterministicevaluations,sothe
onlysourceofstochasticityistheoptimizationoftheFR-Neuralmodel.
5.3SNOMED-CT
SNOMED-CTisanontologyofclinicalhealthcaretermsandconceptsincludingdiseases,treatments,
anatomicalterms,andmanyothertypesofentities.FromthepubliclyavailableSNOMED-CTknowl-
edgegraph,
3
weextracted327,001entitiesand3,809,639edgesof169differenttypes(Table8).To
createdistributionalembeddings,welinkeachSNOMED-CTconcepttoasetofWikipediaarticles
byindexingtheassociatedsearchtermsinWikiData.
4
Weaggregateeacharticlesetbythemethodof
Aroraetal.(2016),whichperformsTF-IDFweightedaggregationofpre-trainedtermembeddingsto
createsophisticateddistributionalembeddingsofSNOMED-CTconcepts.Thiscreatesasingle300-
dimensionalvectorforeachentity.
Results
AstheSNOMED-CTontologyisdominatedbysynonymy-likerelations,weexpectthesimple
methodstoperformwell.Nevertheless,weseeminimallossinlinkpredictionperformance
byusingthemorexibleFRframework(Table3).Ourimplementationsupportstheuseofdifferent
functionclassestorepresentdifferentrelationtypes;inpractice,werecommendthatusersselectfunction
classesinaccordancewithrelationsemantics.
Retr
Model
`HasFindingSite'`HasPathologicalProcess'`Dueto'`Causeof'
(113748/49070)(19318/8124)(5042/2042)(1166/376)
None
95
:
26

0
:
0198
:
79

0
:
0791
:
47

0
:
8879
:
61

1
:
27
FR-Identity
95
:
25

0
:
11
99
:
09

0
:
11
94
:
69

0
:
6186
:
67

1
:
27
FR-Linear
95
:
35

0
:
0199
:
35

0
:
01
93
:
50

0
:
46
80
:
82

0
:
49
FR-Neural
95
:
22

0
:
0098
:
97

0
:
2291
:
70

0
:
1580
:
29

0
:
80
Table3:toSNOMED-CT.Reportedvaluesaremeanandstandarddeviationofthelink
predictionaccuraciesoverthreeexperiments.Thenumberofedgesusedfor(training/testing)isshown
beloweachedgetype.
5.4RoamHealthKnowledgeGraph
Finally,weinvestigatetheutilityofFRintheRoamHealthKnowledgeGraph(RHKG).TheRHKGis
arichpictureoftheworldofhealthcare,withconnectionsintonumerousdatasources:diversemedical
ontologies,providerandnetworks,productapprovalsandrecalls,populationhealthstatistics,
academicpublications,data,clinicaltrialsummariesandstatistics,andmanyothers.AsofJune
2,2017theRHKGcontains209,053,294vertices,1,021,163,726edges,and6,231,287,999attributes.
Here,webuildaninstanceoftheRHKGusingonlypublicdatasourcesinvolvingdrugsanddiseases.
3
https://www.nlm.nih.gov/healthit/snomedct/index.html
4
https://dumps.wikimedia.org/wikidatawiki/entities/
ThestructureofthisknowledgegraphissummarizedinTable9.Intotal,weselect48,649diseaseŒ
diseaserelations,227,051drugŒdrugrelations,and13,667drugŒdiseaserelationsusedforA
disjointsetof11,306drugŒdiseaserelationsisreservedforevaluation.
IntheRHKG,asinmanyindustrialknowledgegraphs,differentdistributionalcorporaareavailablefor
eachtypeofentity.First,wemine2.9Mclinicaltextsforco-occurrencecountsinphysiciannotes.After
countingco-occurrences,weperformapointwisemutualinformationtransformand
`
2
rownormaliza-
tiontogenerateembeddingsforeachentity.Fordrugembeddings,wesupplementtheseembeddings
withphysicianprescriptionhabits.Weextractprescriptioncountsforeachof808,020providersinthe
2013CentersforMedicare&Medicaid(CMS)dataset
5
and837,629providersinthe2014CMSdataset.
Byaggregatingprescriptionscountsacrossproviderspecialty,weproduce201-dimensionaldistribu-
tionalembeddingsforeachdrug.Finally,wethesedistributionalembeddingstothestructureof
theknowledgegraph(excluding`Treats'edgesreservedforevaluation).
Results
AsshowninTable5,theFRframeworkimprovespredictionof`Treats'relations.Wehy-
pothesizethatthisisduetotheseparablenatureofthegraph;asseeninFigure2,theFR
frameworkcanlearnDiseaseandDrugsubgraphsthatarenearlyseparable.Incontrast,Identity
generatesasingleconnectedspaceanddistortstheembeddings.
Figure2:t-SNEProjectionsoftheembeddingsofthedrugs(blue)anddiseases(orange)
intheRoamHealthKnowledgeGraph,withselectedannotationsthe`Treats'relation.The
distributionalspacestronglyseparatesthetwokindsofentitybecausetheirrepresentationswerelearned
indifferentways.Identityblursthisbasicsemanticdistinctioninordertomakediseasesand
drugsin`Treats'relationsmoresimilar.AsTable5shows,theFRmodelsachievethissame
buttheyneednotdistortthebasicdrug/diseasedistinctiontodoit.
Wealsoinvestigatethepredictionsinducedbytherepresentations.Aninterestinguseof
healthcareknowledgegraphsistopredictdrug
retargets
,thatis,diseasesforwhichthereisnoannotated
treatmentrelationshipwiththedrugbutsucharelationshipmayexistmedically.AsshowninTable4,
5
https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/
Medicare-Provider-Charge-Data/Part-D-Prescriber.html
Retr
Model
DrugDiseaseTargetModel
Score
Plausible
None
NaproxenAnkylosingSpondylitis
0
:
98
Y
Latanoprostinjuryofankle,footandtoes
0
:
96
N
PulmicortPsoriasis,
0
:
96
Y
FurosemideAneurysmofsite
0
:
92
Y
DesonideChlamydiallymphogranuloma(venereum)
0
:
92
N
FR-Identity
Latanoprostinjuryofankle,footandtoes
0
:
98
N
ElixophyllinPneumoniaindiseaseselsewhere
0
:
94
Y
FurosemideAneurysmofsite
0
:
92
Y
OxistatMycosisfungoides
0
:
90
Y
TCongenitalPneumonia
0
:
90
N
FR-Linear
Kenalogcontactdermatitis
0
:
96
Y
KenalogPemphigus
0
:
96
Y
MethyprednisoloneAcetateNephroticSyndrome
0
:
96
Y
FurosemideAneurysmofsite
0
:
94
Y
DexamethasonePemphigus
0
:
90
Y
FR-Neural
OnglyzaType2diabetesmellitus
0
:
98
Y
PradaxaEssential(primary)hypertension
0
:
96
Y
OxytocinPauciarticularjuvenilerheumatoidarthritis
0
:
94
Y
TerbutalinesulfateHIV2asthecauseofdiseaseselsewhere
0
:
94
N
LipitorCerebralinfarction
0
:
92
Y
Table4:HighestdrugtargetsthatwerenotannotatedintheRoamHealthKnowledgeGraph.
Retr
Model
`Treats'
(9152/2490)
None
72
:
02

0
:
50
FR-Identity
72
:
93

0
:
82
FR-Linear
84
:
22

0
:
82
FR-Neural
73
:
52

0
:
89
Table5:Drug-DiseaseLinkPredictionAccuracies.
thetopretargetspredictedbythelinearmodelareallmedicallyplausible.Inparticular,the
modelpredictsthatKenalogwouldtreatcontactdermatitis,aneffectalsofoundinaclinical
trial(UsatineandRiojas,2010).ThesecondmostpredictionofdrugretargetswasthatKenalog
cantreatpemphigus,whichisindicatedonKenalog'sdruglabel,
6
butwasnotpreviouslyincludedin
theknowledgegraph.Thethirdpredictionwasthatmethyprednisoloneacetatewouldtreatnephrotic
syndrome,whichisreasonableasthedrugisnowlabelledtotreatidiopathicnephroticsyndrome.
7
Interestingly,severalmodelspredictthatfurosemidetreatsﬁaneurysmofsiteﬂ,arelationship
notindicatedonthedruglabel
8
,thoughfurosemidehasbeenobservedtoreduceintracranialpressure
(SamsonandBeyerJr,1982),akeyfactorinbrainaneurysms.Finally,boththedistributionaldataand
theembeddingsproducedbythebaselineidentitymodelmakethenonsensicalpredictionthat
Latanoprost,amedicationusedtotreatintraocularpressure,wouldalsotreatankleandfoot
injuries.
Theaccuracyofthepredictionsfromthemorecomplexmodelsunderscorestheutilityofthenew
frameworkfordistributionalembeddingstoknowledgegraphswithrelationsthatdonotimply
similarity.
6
https://www.accessdata.fda.gov/drugsatfda_docs/label/2014/014901s042lbledt.pdf
7
https://dailymed.nlm.nih.gov/dailymed/fda/fdaDrugXsl.cfm?setid=
978b8416-2e88-4816-8a37-bb20b9af4b1d
8
https://dailymed.nlm.nih.gov/dailymed/drugInfo.cfm?setid=eadfe464-720b-4dcd-a0d8-45dba706bd33
6ConclusionsandFutureWork
Wehavepresented
FunctionalRetr
,anewframeworkfor
post-hoc
ofentityembed-
dingstothestructureofaknowledgegraph.Byexplicitlymodelingpairwiserelations,thisframework
allowsuserstoencode,learn,andextractinformationaboutrelationsemanticswhilesimultaneouslyup-
datingentityrepresentations.Thisframeworkextendsthepopularconceptoftoknowledge
graphswithdiverseentityandrelationtypes.Functionalisespeciallyforgraphs
inwhichdistinctdistributionalcorporaareavailablefordifferententityclasses,butitlosesnoaccuracy
whenappliedtosimplerknowledgegraphs.Finally,weareinterestedinthepossibilityofimprovements
totheoptimizationprocedureoutlinedinthispaper,includingdynamicupdatesofthe

and

parameters
toincreasetrustinthegraphstructurewhiletherelationfunctionsarelearned.
Acknowledgements
WewouldliketothankAdamFoster,BenPeloquin,JJPlecs,andWillHamiltonforinsightfulcomments,
andanonymousreviewersforconstructivecriticism.
References
[Aroraetal.2016]
SanjeevArora,YingyuLiang,andTengyuMa.2016.Asimplebuttough-to-beatbaselinefor
sentenceembedtdings.
InternationalConferenceonLearningRepresentations
.
[Bakeretal.1998]
CollinFBaker,CharlesJFillmore,andJohnBLowe.1998.Theberkeleyframenetproject.In
Proceedingsofthe36thAnnualMeetingoftheAssociationforComputationalLinguisticsand17thInternational
ConferenceonComputationalLinguistics-Volume1
,pages86Œ90.AssociationforComputationalLinguistics.
[Bordesetal.2012]
AntoineBordes,XavierGlorot,JasonWeston,andYoshuaBengio.2012.Jointlearningof
wordsandmeaningrepresentationsforopen-textsemanticparsing.In
JMLRW&CP:ProceedingsoftheFif-
teenthInternationalConferenceonIntelligenceandStatistics(AISTATS2012)
.
[Bordesetal.2013]
AntoineBordes,NicolasUsunier,AlbertoGarcia-Duran,JasonWeston,andOksana
Yakhnenko.2013.Translatingembeddingsformodelingmulti-relationaldata.In
Advancesinneuralin-
formationprocessingsystems
,pages2787Œ2795.
[Faruquietal.2015]
ManaalFaruqui,JesseDodge,SujayKumarJauhar,ChrisDyer,EduardHovy,andNoahA.
Smith.2015.wordvectorstosemanticlexicons.In
Proceedingsofthe2015Conferenceofthe
NorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies
,
pages1606Œ1615.AssociationforComputationalLinguistics.
[Faruquietal.2016]
ManaalFaruqui,RyanMcDonald,andRaduSoricut.2016.Morpho-syntacticlexicongenera-
tionusinggraph-basedsemi-supervisedlearning.
TransactionsoftheAssociationofComputationalLinguistics
,
4(1):1Œ16.
[Fellbaum2005]
ChristianeFellbaum.2005.Wordnetandwordnets.InKeithBrownetal.,editor,
Encyclopediaof
LanguageandLinguistics
,page665670.Oxford:Elsevier,secondedition.
[Fillmoreetal.2003]
CharlesJFillmore,ChristopherRJohnson,andMiriamRLPetruck.2003.Backgroundto
framenet.
Internationaljournaloflexicography
,16(3):235Œ250.
[Finkelsteinetal.2001]
LevFinkelstein,EvgeniyGabrilovich,YossiMatias,EhudRivlin,ZachSolan,GadiWolf-
man,andEytanRuppin.2001.Placingsearchincontext:Theconceptrevisited.In
Proceedingsofthe10th
internationalconferenceonWorldWideWeb
,pages406Œ414.ACM.
[Hamiltonetal.2017]
WilliamLHamilton,RexYing,andJureLeskovec.2017.Inductiverepresentationlearning
onlargegraphs.
arXivpreprintarXiv:1706.02216
.
[Linetal.2015]
YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,andXuanZhu.2015.Learningentityand
relationembeddingsforknowledgegraphcompletion.In
AAAI
,pages2181Œ2187.
[Mikolovetal.2013a]
TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean.2013a.Dis-
tributedrepresentationsofwordsandphrasesandtheircompositionality.In
AdvancesinNeuralInformation
ProcessingSystems
,pages3111Œ3119.
[Mikolovetal.2013b]
TomasMikolov,Wen-tauYih,andGeoffreyZweig.2013b.Linguisticregularitiesincon-
tinuousspacewordrepresentations.In
Proceedingsofthe2013ConferenceoftheNorthAmericanChapterof
theAssociationforComputationalLinguistics:HumanLanguageTechnologies
,volume13,pages746Œ751.
[Miller1995]
GeorgeAMiller.1995.Wordnet:alexicaldatabaseforenglish.
CommunicationsoftheACM
,
38(11):39Œ41.
[Mrk

sicetal.2016]
NikolaMrk

sic,DiarmuidOS
´
eaghdha,BlaiseThomson,MilicaGa

sic,LinaRojas-Barahona,
Pei-HaoSu,DavidVandyke,Tsung-HsienWen,andSteveYoung.2016.Counterwordvectorsto
linguisticconstraints.In
Proceedingsofthe2016ConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies
,pages142Œ148.
[Nickeletal.2016]
MaximilianNickel,KevinMurphy,VolkerTresp,andEvgeniyGabrilovich.2016.Areviewof
relationalmachinelearningforknowledgegraphs.
ProceedingsoftheIEEE
,104(1):11Œ33.
[SamsonandBeyerJr1982]
DukeSamsonandChesterWBeyerJr.1982.Furosemideintheintraoperativereduc-
tionofintracranialpressureinthepatientwithsubarachnoidhemorrhage.
Neurosurgery
,10(2):167Œ169.
[Socheretal.2013]
RichardSocher,DanqiChen,ChristopherDManning,andAndrewNg.2013.Reasoningwith
neuraltensornetworksforknowledgebasecompletion.In
Advancesinneuralinformationprocessingsystems
,
pages926Œ934.
[UsatineandRiojas2010]
RichardPUsatineandMarcelaRiojas.2010.Diagnosisandmanagementofcontact
dermatitis.
Americanfamilyphysician
,82(3):249Œ255.
[Wangetal.2014]
ZhenWang,JianwenZhang,JianlinFeng,andZhengChen.2014.Knowledgegraphembedding
bytranslatingonhyperplanes.In
Twenty-EighthAAAIConferenceonIntelligence
.
AStructureofKnowledgeGraphs
A.1FrameNet
ThestructureoftheFrameNet(Bakeretal.,1998;Fillmoreetal.,2003)knowledgegraphisshownin
Table6.
EntityTypeCountW2VCount
Token1357212167
Frame1221464
EdgeTypeConnectsCount
FrameToken
!
Frame13572
Lexical
UnitFrame
!
Token13572
InheritanceFrame
!
Frame1562
UsingFrame
!
Frame1110
ReFraming
MappingFrame
!
Frame428
Persepctive
onFrame
!
Frame242
PrecedesFrame
!
Frame178
See
alsoFrame
!
Frame172
Causative
ofFrame
!
Frame120
Inchoative
ofFrame
!
Frame38
MetaphorFrame
!
Frame8
Table6:StructureoftheFrameNetknowledgegraph.
A.2WordNet
ThestructureoftheWordNetknowledgegraph(Miller,1995)isshowninTable7.
EntityTypeCountW2VCount
Lemma206978115635
EdgeTypeCount
Hypernym136,235
Hyponym136,235
DerivationallyRelatedForm60,250
Antonym5,922
Pertainym5,573
UsageDomain69
Table7:StructureoftheWordNetknowledgegraph.
A.3SNOMED-CT
ThestructureoftheknowledgegraphextractedfromtheSNOMED-CTontologyisshowninTable8.
A.4RoamHealthKnowledgeGraph
ThestructureoftheextractedsubgraphoftheRHKGissummarizedinTable9.Adisjointsetof11,306
drugŒdiseaserelationsisreservedforevaluation.
EdgeTypeCount
EdgeTypeCount
associated
clinical
493258
child242130
has

site205263
has
method200507
has
associated
morphology169778
has
procedure
site79171
has
causative
agent69284
interprets67900
has
active
ingredient58976
part
of47776
has
direct
procedure
site45693
mapped
to37287
same
as30670
has
pathological
process23641
has
dose
form23259
has
intent22845
causative
agent
of19833

site
of19525
has
direct
morphology18380
has
direct
substance16913
has
component15597
has
indirect
procedure
site15596
occurs
in14003
possibly
equivalent
to13459
has

method12754
active
ingredient
of12423
has

manifestation11788
has
direct
device11223
is
interpreted
by10908
has
interpretation10077
procedure
site
of9559
occurs
after7825
has
temporal
context7786
associated
morphology
of7524
has
subject
relationship
context7465
has
part6851
uses
device6407
associated
with6399
has
measured
component6353
uses6221
has
associated
6205
has
focus6122
uses
substance5474
component
of5256
temporally
follows5029
due
to4884
has

context4883
direct
procedure
site
of4252
has
specimen3767
replaces3726
has
laterality3641
associated

of3432
has
associated
procedure3397
has
clinical
course3309
has
course3241
has
procedure
context2945
has
approach2808
measured
component
of2741
has
access2660
has
specimen
source
topography2457
has

informer2229
has
onset2168
has
priority1854
mth
xml
form
of1794
mth
plain
text
form
of1794
mth
has
xml
form1794
mth
has
plain
text
form1794
direct
substance
of1783
focus
of1680
indirect
procedure
site
of1662
has
revision
status1599
uses
access
device1587
has
access
instrument1518
direct
device
of1434
has
indirect
morphology1426
associated
procedure
of1320
has
specimen
procedure1309
has
communication
with
wound1155
cause
of1121
has
extent1082
has
specimen
substance1030
method
of921
has
procedure
device770
uses
energy753
has
procedure
morphology752
has
surgical
approach697
dose
form
of676
direct
morphology
of673
referred
to
by667
has
associated
etiologic
656
used
by608
priority
of586
specimen
source
topography
of584
occurs
before574
specimen
procedure
of555
has
severity525
device
used
by525
substance
used
by507

manifestation
of436
temporally
followed
by406
has
specimen
source
identity327
has
property282
has
instrumentation274
has
subject
of
information272
has
specimen
source
morphology251
access
instrument
of226
has
scale
type206
specimen
substance
of171
has
episodicity168
has
route
of
administration143
has
recipient
category143
associated
etiologic

of143
specimen
of134
approach
of125
subject
relationship
context
of115
has
indirect
device114
interpretation
of109
procedure
device
of107
course
of106
indirect
morphology
of10
Table8:StructureoftheSNOMED-CTknowledgegraph.
EntityTypeCount
Drug223,019
Disease95,559
EdgeTypeConnectsCount
IngredientOfDrug
!
Drug49,218
HasIngredientDrug
!
Drug49,208
IsADrug
!
Drug28,297
HasDescendentDisease
!
Disease22,344
TreatsDrug
!
Disease19,374
HasActiveIngredientDrug
!
Drug18,422
HasChildDisease
!
Disease18,066
ActiveIngredientOfDrug
!
Drug17,175
HasTradeNameDrug
!
Drug11,783
TradeNameOfDrug
!
Drug11,783
InverseIsADrug
!
Drug10,369
HasSymptomDisease
!
Disease7,892
PartOfDrug
!
Drug6,882
HasPartDrug
!
Drug6,624
SameAsDrug
!
Drug5,882
PreciseIngredientOfDrug
!
Drug3,562
HasPreciseIngredientDrug
!
Drug3,562
PossiblyEquivalentToDrug
!
Drug1,233
CausativeAgentofDrug
!
Drug1,070
HasFormDrug
!
Drug602
FormofDrug
!
Drug602
ComponentofDrug
!
Drug436
IncludesDisease
!
Disease347
HasDoseFormDrug
!
Drug138
Table9:StructureofthesubgraphoftheRoamHealthKnowledgeGraph.
"
41,Binary Classification in Unstructured Space With Hypergraph Case-Based Reasoning,http://arxiv.org/pdf/1806.06232v3.pdf,https://github.com/aquemy/HCBR,
42,Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling,http://arxiv.org/pdf/1806.06228v1.pdf,https://github.com/SenticNet/hfusion,"MultimodalSentimentAnalysisusing
HierarchicalFusionwithContextModeling
N.Majumder
a
,D.Hazarika
b
,
A.Gelbukh
a
,E.Cambria
c
,S.Poria
c
a
Centrodeonenon,InstitutoPolitecnicoNacional,Mexico
b
SchoolofComputing,NationalUniversityofSingapore,Singapore
c
SchoolofComputerScienceandEngineering,NanyangTechnologicalUniversity,Singapore
Abstract
Multimodalsentimentanalysisisaveryactivelygrowingofresearch.Apro-
misingareaofopportunityinthisistoimprovethemultimodalfusion
mechanism.Wepresentanovelfeaturefusionstrategythatproceedsinahier-
archicalfashion,fusingthemodalitiestwointwoandonlythenfusingall
threemodalities.Onmultimodalsentimentanalysisofindividualutterances,
ourstrategyoutperformsconventionalconcatenationoffeaturesby1%,which
amountsto5%reductioninerrorrate.Onutterance-levelmultimodalsentiment
analysisofmulti-utterancevideoclips,forwhichcurrentstate-of-the-arttech-
niquesincorporatecontextualinformationfromotherutterancesofthesame
clip,ourhierarchicalfusiongivesupto2.4%(almost10%errorratereduc-
tion)overcurrentlyusedconcatenation.Theimplementationofourmethodis
publiclyavailableintheformofopen-sourcecode.
1.Introduction
Onnumeroussocialmediaplatforms,suchasYouTube,Facebook,orInsta-
gram,peoplesharetheiropinionsonallkindsoftopicsintheformofposts,
images,andvideoclips.Withtheproliferationofsmartphonesandtablets,
whichhasgreatlyboostedcontentsharing,peopleincreasinglysharetheiropin-
ionsonnewlyreleasedproductsoronothertopicsinformofvideoreviewsor
comments.Thisisanexcellentopportunityforlargecompaniestocapitalize
PreprintsubmittedtoElsevierJune19,2018
arXiv:1806.06228v1  [cs.CL]  16 Jun 2018on,byextractingusersentiment,suggestions,andcomplaintsontheirproducts
fromthesevideoreviews.Thisinformationalsoopensnewhorizonstoimprov-
ingourqualityoflifebymakinginformeddecisionsonthechoiceofproducts
webuy,servicesweuse,placeswevisit,ormovieswewatchbasingonthe
experienceandopinionsofotherusers.
Videosconveyinformationthroughthreechannels:audio,video,andtext(in
theformofspeech).Miningopinionsfromthisplethoraofmultimodaldatacalls
forasolidmultimodalsentimentanalysistechnology.Oneofthemajorproblems
facedinmultimodalsentimentanalysisisthefusionoffeaturespertainingto
tmodalities.Forthis,themajorityoftherecentworksinmultimodal
sentimentanalysishavesimplyconcatenatedthefeaturevectorsoft
modalities.However,thisdoesnottakeintoaccountthattmodalities
maycarryconinformation.Wehypothesizethatthefusionmethodwe
presentinthispaperdealswiththisissuebetter,andpresentexperimental
evidenceshowingimprovementoversimpleconcatenationoffeaturevectors.
Also,followingthestateoftheart[1],weemployrecurrentneuralnetwork
(RNN)topropagatecontextualinformationbetweenutterancesinavideoclip,
whichtlyimprovestheresultsandoutperformsthestate
oftheartbyatmarginof1{2%forallthemodalitycombinations.
Inourmethod,weobtainunimodalfeaturesforeachutterancefor
allthreemodalities.Then,usingRNNweextractcontext-awareutterance
features.Thus,wetransformthecontext-awareutterancevectorstothevectors
ofthesamedimensionality.Weassumethatthesetransformedvectorscontain
abstractfeaturesrepresentingtheattributesrelevanttosentiment
Next,wecompareandcombineeachbimodalcombinationoftheseabstract
featuresusingfully-connectedlayers.Thisyieldsfusedbimodalfeaturevectors.
Similarlytotheunimodalcase,weuseRNNtogeneratecontext-awarefeatures.
Finally,wecombinethesebimodalvectorsintoatrimodalvectorusing,again,
fully-connectedlayersanduseaRNNtopasscontextualinformationbetween
them.Weempiricallyshowthatthefeaturevectorsobtainedinthismannerare
moreusefulforthesentimenttask.
2
Theimplementationofourmethodispubliclyavailableintheformofopen-
sourcecode.
1
Thispaperisstructuredasfollows:Section2discussesimportant
previousworkinmultimodalfeaturefusion;Section3describesourmethod
indetails;Section4reportstheresultsofourexperimentsanddiscusstheir
implications;,Section5concludesthepaperanddiscussesfuturework.
2.RelatedWork
Inrecentyears,sentimentanalysishasbecomeincreasinglypopularforpro-
cessingsocialmediadataononlinecommunities,blogs,wikis,microblogging
platforms,andotheronlinecollaborativemedia[2].Sentimentanalysisisa
branchofecomputingresearch[3]thataimstoclassifytext{butsome-
timesalsoaudioandvideo[4]{intoeitherpositiveornegative{butsometimes
alsoneutral[5].MostoftheliteratureisonEnglishlanguagebutrecentlyanin-
creasingnumberofworksaretacklingthemultilingualityissue[6,7],especially
inboomingonlinelanguagessuchasChinese[8].Sentimentanalysistechniques
canbebroadlycategorizedintosymbolicandsub-symbolicapproaches:thefor-
merincludetheuseoflexicons[9],ontologies[10],andsemanticnetworks[11]to
encodethepolarityassociatedwithwordsandmultiwordexpressions;thelatter
consistofsupervised[12],semi-supervised[13]andunsupervised[14]machine
learningtechniquesthatperformsentimentbasedonwordco-
occurrencefrequencies.Amongthese,themostpopularrecentlyarealgorithms
basedondeepneuralnetworks[15]andgenerativeadversarialnetworks[16].
Whilemostworksapproachitasasimplecategorizationproblem,senti-
mentanalysisisactuallyasuitcaseresearchproblem[17]thatrequirestackling
manyNLPtasks,includingwordpolaritydisambiguation[18],subjectivityde-
tection[19],personalityrecognition[20],microtextnormalization[21],concept
extraction[22],timetagging[23],andaspectextraction[24].
1
http://github.com/senticnet
3
Sentimentanalysishasraisedgrowinginterestbothwithinthesciencom-
munity,leadingtomanyexcitingopenchallenges,aswellasinthebusiness
world,duetotheremarkablebtobehadfrom[25]andpo-
litical[26]forecasting,e-health[27]ande-tourism[28],userpr[29]and
communitydetection[30],manufacturingandsupplychainapplications[31],
humancommunicationcomprehension[32]anddialoguesystems[33],etc.
Intheofemotionrecognition,earlyworksbyDeSilvaetal.[34]and
Chenetal.[35]showedthatfusionofaudioandvisualsystems,creatinga
bimodalsignal,yieldedahigheraccuracythananyunimodalsystem.Such
fusionhasbeenanalyzedatbothfeaturelevel[36]anddecisionlevel[37].
Althoughthereismuchworkdoneonaudio-visualfusionforemotionrecog-
nition,exploringcontributionoftextalongwithaudioandvisualmodalities
inmultimodalemotiondetectionhasbeenlittleexplored.Wollmeretal.[38]
andRozgicetal.[39]fusedinformationfromaudio,visualandtextualmodalities
toextractemotionandsentiment.Metallinouetal.[40]andEybenetal.[41]
fusedaudioandtextualmodalitiesforemotionrecognition.Bothapproaches
reliedonafeature-levelfusion.WuandLiang[42]fusedaudioandtextualclues
atdecisionlevel.Poriaetal.[43]usesconvolutionalneuralnetwork(CNN)to
extractfeaturesfromthemodalitiesandthenemploysmultiple-kernellearning
(MKL)forsentimentanalysis.Thecurrentstateoftheart,setforthbyPoria
etal.[1],extractscontextualinformationfromthesurroundingutterancesusing
longshort-termmemory(LSTM).Poriaetal.[3]fusestmodalitieswith
deeplearning-basedtools.Zadehetal.[44]usestensorfusion.Poriaetal.[45]
furtherextendsupontheensembleofCNNandMKL.
Unlikeexistingapproaches,whichusesimpleconcatenationbasedearlyfu-
sion[43,46]andnon-trainabletensorsbasedfusion[44],thisworkproposesa
hierarchicalfusioncapableoflearningthebimodalandtrimodalcorrelationsfor
datafusionusingdeepneuralnetworks.Themethodisend-to-endand,inorder
toaccomplishthefusion,itcanbepluggedintoanydeepneuralnetworkbased
multimodalsentimentanalysisframework.
4
3.OurMethod
Inthissection,wediscussournovelmethodologybehindsolvingthesenti-
mentproblem.Firstwediscusstheoverviewofourmethodand
thenwediscussthewholemethodindetails,stepbystep.
3.1.Overview
3.1.1.UnimodalFeatureExtraction
Weextractutterance-levelfeaturesforthreemodalities.Thisstepisdis-
cussedinSection3.2.
3.1.2.MultimodalFusion
Problemsofearlyfusion.
Themajorityoftheworkonmultimodaldatausecon-
catenation,orearlyfusion(Fig.1),astheirfusionstrategy.Theproblemwith
thissimplisticapproachisthatitcannotoutandorredundant
informationobtainedfromtmodalities.Toaddressthismajorissue,
wedeviseanhierarchicalapproachwhichproceedsfromunimodaltobimodal
vectorsandthenbimodaltotrimodalvectors.
Figure1:Utterance-levelearlyfusion,orsimpleconcatenation
Bimodalfusion.
Wefusetheutterancefeaturevectorsforeachbimodalcombi-
nation,i.e.,T+V,T+A,andA+V.ThisstepisdepictedinFig.2anddiscussed
indetailsinSection3.4.WeusethepenultimatelayerforFig.2asbimodal
features.
Trimodalfusion.
Wefusethethreebimodalfeaturestoobtaintrimodalfeature
asdepictedinFig.3.ThisstepisdiscussedindetailsinSection3.4.
5
Figure2:Utterance-levelbimodalfusion
Additionofcontext.
Wealsoimprovethequalityoffeaturevectors(bothuni-
modalandmultimodal)byincorporatinginformationfromsurroundingutter-
ancesusingRNN.Wemodelthecontextusinggatedrecurrentunit(GRU)as
depictedinFig.4.ThedetailsofcontextmodelingisdiscussedinSection3.3
andthefollowingsubsections.
ation.
Weclassifythefeaturevectorsusingasoftmaxlayer.
3.2.UnimodalFeatureExtraction
Inthissection,wediscussthemethodoffeatureextractionforthreet
modalities:audio,video,andtext.
3.2.1.TextualFeatureExtraction
Thetextualdataisobtainedfromthetranscriptsofthevideos.Weapplya
deepConvolutionalNeuralNetworks(CNN)[47]oneachutterancetoextract
textualfeatures.Eachutteranceinthetextisrepresentedasanarrayofpre-
trained300-dimensional
word2vec
vectors[48].Further,theutterancesare
truncatedorpaddedwithnullvectorstohaveexactly50words.
Next,theseutterancesasarrayofvectorsarepassedthroughtwoderent
convolutionallayers;layerhavingtwoofsize3and4respectively
6
Figure3:Utterance-leveltrimodalhierarchicalfusion.
2
with50featuremapseachandthesecondlayerhasaofsize2with100
featuremaps.Eachconvolutionallayerisfollowedbyamax-poolinglayerwith
window2

2.
Theoutputofthesecondmax-poolinglayerisfedtoafully-connectedlayer
with500neuronswithalinearunit(ReLU)[49]activation,followed
bysoftmaxoutput.Theoutputofthepenultimatefully-connectedlayerisused
asthetextualfeature.Thetranslationofconvolutionovermakesthe
CNNlearnabstractfeaturesandwitheachsubsequentlayerthecontextofthe
featuresexpandsfurther.
3.2.2.AudioFeatureExtraction
Theaudiofeatureextractionprocessisperformedat30Hzframeratewith
100msslidingwindow.WeuseopenSMILE[50],whichiscapableofautomatic
pitchandvoiceintensityextraction,foraudiofeatureextraction.Priortofea-
tureextractionaudiosignalsareprocessedwithvoiceintensitythresholdingand
voicenormalization.Sp,weuseZ-standardizationforvoicenormaliza-
7
Figure4:Context-awarehierarchicalfusion
tion.Inordertooutaudiosegmentswithoutvoice,wethresholdvoice
intensity.OpenSMILEisusedtoperformboththesesteps.UsingopenSMILE
weextractseveralLowLevelDescriptors(LLD)(e.g.,pitch,voiceintensity)and
variousstatisticalfunctionalsofthem(e.g.,amplitudemean,arithmeticmean,
rootquadraticmean,standarddeviation,skewness,kurtosis,quartiles,
inter-quartileranges,andlinearregressionslope).\IS13-ComParE""
tionofopenSMILEisusedtoforourpurposes.Finally,weextractedtotal
6392featuresfromeachinputaudiosegment.
3.2.3.VisualFeatureExtraction
Toextractvisualfeatures,wefocusnotonlyonfeatureextractionfromeach
videoframebutalsotrytomodeltemporalfeaturesacrossframes.Toachieve
this,weuse3D-CNNonthevideo.3D-CNNshavebeensuccessfulinthepast,
speciallyintheeldofobjecton3Ddata[51].Itsstate-of-the-art
performanceonsuchtasksmotivatesitsuseinthispaper.
Letthevideobecalled
vid
2
R
3

f

h

w
,where3representsthethree
RGBchannelsofanimageand
f;h;
and
w
denotethecardinality,height,
andwidthoftheframes,respectively.A3Dconvolutionalnamed
f
lt
2
R
f
m

3

f
d

f
h

f
w
,isappliedtothisvideo,where,similartoa2D-CNN,thel-
8
tertranslatesacrossthevideoandgeneratestheconvolutionoutput
conv
out
2
R
f
m

3

(
f

f
d
+1)

(
h

f
h
+1)

(
w

f
w
+1)
.Here,
f
m
;f
d
;f
h
;
and
f
w
denotenumber
offeaturemaps,depthofheightofandwidthofrespectively.
Finally,weapplymax-poolingoperationtothe
conv
out
,whichselectsthemost
relevantfeatures.Thisoperationisappliedonlytothelastthreedimensions
of
conv
out
.Thisisfollowedbyadenselayerandsoftmaxcomputation.The
activationsofthislayerisusedastheoverallvideofeaturesforeachutterance
video.
Inourexperiments,wereceivethebestresultswithdimensions
f
m
=32
and
f
d
;f
h
;f
w
=5.Also,forthemax-pooling,wesetthewindowsizeas3

3

3
andthesucceedingdenselayerwith300neurons.
3.3.ContextModeling
Utterancesinthevideosaresemanticallydependentoneachother.Inother
words,completemeaningofanutterancemaybedeterminedbytakingpre-
cedingutterancesintoconsideration.Wecallthisthecontextofanutterance.
FollowingPoriaetal.[1],weuseRNN,spGRU
3
tomodelsemantic
dependencyamongtheutterancesinavideo.
Letthefollowingitemsrepresentunimodalfeatures:
f
A
2
R
N

d
A
(acousticfeatures)
;
f
V
2
R
N

d
V
(visualfeatures)
;
f
T
2
R
N

d
T
(textualfeatures)
;
where
N
=maximumnumberofutterancesinavideo.Wepadtheshorter
videoswithdummyutterancesrepresentedbynullvectorsofcorresponding
length.Foreachmodality,wefeedtheunimodalutterancefeatures
f
m
(where
m
2f
A;V;T
g
)(discussedinSection3.2)ofavideoto
GRU
m
withoutputsize
2
Figureadaptedfrom[52]withpermission.
3
LSTMdoesnotperformwell
9
D
m
,whichisas
z
m
=
˙
(
f
mt
U
mz
+
s
m
(
t

1)
W
mz
)
;
r
m
=
˙
(
f
mt
U
mr
+
s
m
(
t

1)
W
mr
)
;
h
mt
=tanh(
f
mt
U
mh
+(
s
m
(
t

1)

r
m
)
W
mh
)
;
F
mt
=tanh(
h
mt
U
mx
+
u
mx
)
;
s
mt
=(1

z
m
)

F
mt
+
z
m

s
m
(
t

1)
;
where
U
mz
2
R
d
m

D
m
,
W
mz
2
R
D
m

D
m
,
U
mr
2
R
d
m

D
m
,
W
mr
2
R
D
m

D
m
,
U
mh
2
R
d
m

D
m
,
W
mh
2
R
D
m

D
m
,
U
mx
2
R
d
m

D
m
,
u
mx
2
R
D
m
,
z
m
2
R
D
m
,
r
m
2
R
D
m
,
h
mt
2
R
D
m
,
F
mt
2
R
D
m
,and
s
mt
2
R
D
m
.Thisyieldshidden
outputs
F
mt
ascontext-awareunimodalfeaturesforeachmodality.Hence,
we
F
m
=
GRU
m
(
f
m
),where
F
m
2
R
N

D
m
.Thus,thecontext-aware
multimodalfeaturescanbeas
F
A
=
GRU
A
(
f
A
)
;
F
V
=
GRU
V
(
f
V
)
;
F
T
=
GRU
T
(
f
T
)
:
3.4.MultimodalFusion
Inthissection,weusecontext-awareunimodalfeatures
F
A
;F
V
;
and
F
T
to
afeaturespace.
Theunimodalfeaturesmayhaverentdimensions,i.e.,
D
A
6
=
D
V
6
=
D
T
.
Thus,wemapthemtothesamedimension,say
D
(weobtainedbestresults
with
D
=400),usingfully-connectedlayerasfollows:
g
A
=tanh(
F
A
W
A
+
b
A
)
;
g
V
=tanh(
F
V
W
V
+
b
V
)
;
g
T
=tanh(
F
T
W
T
+
b
T
)
;
where
W
A
2
R
D
A

D
,
b
A
2
R
D
,
W
V
2
R
D
V

D
,
b
V
2
R
D
,
W
T
2
R
D
T

D
,and
10
b
T
2
R
D
.Wecanrepresentthemappingforeachdimensionas
g
x
=
2
6
6
6
6
6
6
4
c
x
11
c
x
21
c
x
31

c
x
D
1
c
x
12
c
x
22
c
x
32

c
x
D
2
.
.
.
.
.
.
.
.
.

.
.
.
c
x
1
N
c
x
2
N
c
x
3
N

c
x
DN
3
7
7
7
7
7
7
5
;
where
x
2f
V;A;T
g
and
c
x
lt
arescalarsforall
l
=1
;
2
;:::;D
and
t
=1
;
2
;:::;N
.
Also,in
g
x
therowsrepresenttheutterancesandthecolumnsthefeatureval-
ues.Wecanseethesevalues
c
x
lt
asmoreabstractfeaturevaluesderivedfrom
fundamentalfeaturevalues(whicharethecomponentsof
f
A
,
f
V
,and
f
T
).For
example,anabstractfeaturecanbetheangrinessofaspeakerinavideo.We
caninferthedegreeofangrinessfromvisualfeatures(
f
V
;facialmusclemove-
ments),acousticfeatures(
f
A
,suchaspitchandraisedvoice),ortextualfeatures
(
f
T
,suchasthelanguageandchoiceofwords).Therefore,thedegreeofangri-
nesscanberepresentedby
c
x
lt
,where
x
is
A
,
V
,or
T
,
l
issomeinteger
between1and
D
,and
t
issomeintegerbetween1and
N
.
Now,theevaluationofabstractfeaturevaluesfromallthemodalitiesmay
nothavethesamemeritormayevencontradicteachother.Hence,weneedthe
networktomakecomparisonamongthefeaturevaluesderivedfromt
modalitiestomakeamoreevaluationofthedegreeofanger.Tothis
end,wetakeeachbimodalcombination(whichareaudio{video,audio{text,
andvideo{text)atatimeandcompareandcombineeachoftheirrespective
abstractfeaturevalues(i.e.
c
V
lt
with
c
T
lt
,
c
V
lt
with
c
A
lt
,and
c
A
lt
with
c
T
lt
)using
fully-connectedlayersasfollows:
i
VA
lt
=tanh(
w
VA
l
:
[
c
V
lt
;c
A
lt
]
|
+
b
VA
l
)
;
(1)
i
AT
lt
=tanh(
w
AT
l
:
[
c
A
lt
;c
T
lt
]
|
+
b
AT
l
)
;
(2)
i
VT
lt
=tanh(
w
VT
l
:
[
c
V
lt
;c
T
lt
]
|
+
b
VT
l
)
;
(3)
where
w
VA
l
2
R
2
,
b
VA
l
isscalar,
w
AT
l
2
R
2
,
b
AT
l
isscalar,
w
VT
l
2
R
2
,and
b
VT
l
isscalar,forall
l
=1
;
2
;:::;D
and
t
=1
;
2
;:::;N
.Wehypothesizethatitwill
enablethenetworktocomparethedecisionsfromeachmodalityagainstthe
11
othersandhelpachieveabetterfusionofmodalities.
Bimodalfusion
.
Eqs.(1)to(3)areusedforbimodalfusion.Thebimodal
fusedfeaturesforvideo{audio,audio{text,video{textareas
f
VA
=(
f
VA
1
;f
VA
2
;:::;f
VA
(
N
)
)
;
where
f
VAt
=(
i
VA
1
t
;i
VA
2
t
;:::;i
VA
Dt
)
;
f
AT
=(
f
AT
1
;f
AT
2
;:::;f
AT
(
N
)
)
;
where
f
ATt
=(
i
AT
1
t
;i
AT
2
t
;:::;i
AT
Dt
)
;
f
VT
=(
f
VT
1
;f
VT
2
;:::;f
VT
(
N
)
)
;
where
f
VTt
=(
i
VT
1
t
;i
VT
2
t
;:::;i
VT
Dt
)
:
Wefurtheremploy
GRU
m
(Section3.3)(
m
2f
VA;VT;TA
g
),toincorporate
contextualinformationamongtheutterancesinavideowith
F
VA
=(
F
VA
1
;F
VA
2
;:::;F
VA
(
N
)
)=
GRU
VA
(
f
VA
)
;
F
VT
=(
F
VT
1
;F
VT
2
;:::;F
VT
(
N
)
)=
GRU
VT
(
f
VT
)
;
F
TA
=(
F
TA
1
;F
TA
2
;:::;F
TA
(
N
)
)=
GRU
TA
(
f
TA
)
;
where
F
VAt
=(
I
VA
1
t
;I
VA
2
t
;:::;I
VA
D
2
t
)
;
F
VTt
=(
I
AT
1
t
;I
AT
2
t
;:::;I
AT
D
2
t
)
;
F
TAt
=(
I
VT
1
t
;I
VT
2
t
;:::;I
VT
D
2
t
)
;
F
VA
,
F
VT
,and
F
TA
arecontext-awarebimodalfeaturesrepresentedasvectors
and
I
m
nt
isscalarfor
n
=1
;
2
;:::;D
2
,
D
2
=500,
t
=1
;
2
;:::;N
,and
m
=
VA,VT,TA.
Trimodalfusion.
Wecombineallthreemodalitiesusingfully-connectedlayers
asfollows:
z
lt
=tanh(
w
AVT
l
:
[
I
VA
lt
;I
AT
lt
;I
VT
lt
]
|
+
b
AVT
l
)
;
where
w
AVT
l
2
R
3
and
b
AVT
l
isascalarforall
l
=1
;
2
;:::;D
2
and
t
=
1
;
2
;:::;N
.So,wethefusedfeaturesas
f
AVT
=(
f
AVT
1
;f
AVT
2
;:::;f
AVT
(
N
)
)
;
12
where
f
AVTt
=(
z
1
t
;z
2
t
;:::;z
D
2
t
),
z
nt
isscalarfor
n
=1
;
2
;:::;D
2
and
t
=
1
;
2
;:::;N
.
Similarlytobimodalfusion(Section3.4),aftertrimodalfusionwepassthe
fusedfeaturesthrough
GRU
AVT
toincorporatecontextualinformationinthem,
whichyields
F
AVT
=(
F
AVT
1
;F
AVT
2
;:::;F
AVT
(
N
)
)=
GRU
AVT
(
f
AVT
)
;
where
F
AVTt
=(
Z
1
t
;Z
2
t
;:::;Z
D
3
t
),
Z
nt
isscalarfor
n
=1
;
2
;:::;D
3
,
D
3
=550,
t
=1
;
2
;:::;N
,and
F
AVT
isthecontext-awaretrimodalfeaturevector.
3.5.ation
Inordertoperformwefeedthefusedfeatures
F
mt
(where
m
=
AV;VT;TA;
or
AVT
and
t
=1
;
2
;:::;N
)toasoftmaxlayerwith
C
=2
outputs.Thecanbedescribedasfollows:
P
=softmax(
W
softmax
F
mt
+
b
softmax
)
;
^
y
=argmax
j
(
P
[
j
])
;
where
W
softmax
2
R
C

D
,
b
softmax
2
R
C
,
P2
R
C
,
j
=classvalue(0or1),and
^
y
=estimatedclassvalue.
3.6.Training
Weemploycategoricalcross-entropyaslossfunction(
J
)fortraining,
J
=

1
N
N
X
i
=1
C

1
X
j
=0
y
ij
log
P
i
[
j
]
;
where
N
=numberofsamples,
i
=indexofasample,
j
=classvalue,and
y
ij
=
8
>
<
>
:
1
;
ifexpectedclassvalueofsample
i
is
j
0
;
otherwise.
Adam[53]isusedasoptimizerduetoitsabilitytoadaptlearningratefor
eachparameterindividually.Wetrainthenetworkfor200epochswithearly
13
stopping,whereweoptimizetheparameterset

=
[
m
2
M
0
@
[
j
2f
z;r;h
g
f
U
mj
;W
mj
g[f
U
mx
;u
mx
g
1
A
[
[
m
2
M
2
D
2
[
i
=1
f
w
m
i
g[
D
3
[
i
=1
f
w
AVT
i
g[
[
m
2
M
1
f
W
m
;b
m
g
[f
W
softmax
;b
softmax
g
;
where
M
=
f
A;V;T;VA;VT;TA;AVT
g
,
M
1
=
f
A;V;T
g
,and
M
2
=
f
VA;VT;
TA
g
.Algorithm1summarizesourmethod.
4
4.Experiments
4.1.DatasetDetails
Mostresearchworksinmultimodalsentimentanalysisareperformedon
datasetswheretrainandtestsplitsmaysharecertainspeakers.Since,each
individualhasanuniquewayofexpressingemotionsandsentiments,ding
genericandperson-independentfeaturesforsentimentanalysisiscrucial.Ta-
ble1showsthetrainandtestsplitforthedatasetsused.
Dataset
Train
Test
pos.
neg.
happy
anger
sad
neu.
pos.
neg.
happy
anger
sad
neu.
MOSI
709
738
-
-
-
-
467
285
-
-
-
-
IEMOCAP
-
-
1194
933
839
1324
-
-
433
157
238
380
pos.=positive,neg.=negative,neu.=neutral
Table1:Classdistributionofdatasetsinbothtrainandtestsplits.
4.1.1.CMU-MOSI
CMU-MOSIdataset[54]isrichinsentimentalexpressions,where89people
reviewvarioustopicsinEnglish.Thevideosaresegmentedintoutteranceswhere
eachutteranceisannotatedwithscoresbetween

3(stronglynegative)and+3
4
Implementationofthisalgorithmisavailableat
http://github.com/senticnet
14
Algorithm1
Context-AwareHierarchicalFusionAlgorithm
1:
procedure
TrainAndTestModel
(
U
,
V
)
.
U
=trainset,
V
=testset
2:
Unimodalfeatureextraction:
3:
for
i:[1,
N
]
do
.
extractbaselinefeatures
4:
f
i
A
 
AudioFeatures
(
u
i
)
5:
f
i
V
 
VideoFeatures
(
u
i
)
6:
f
i
T
 
TextFeatures
(
u
i
)
7:
for
m
2f
A;V;T
g
do
8:
F
m
=
GRU
m
(
f
m
)
9:
Fusion
:
10:
g
A
 
MapToSpace
(
F
A
)
.
dimensionalityequalization
11:
g
V
 
MapToSpace
(
F
V
)
12:
g
T
 
MapToSpace
(
F
T
)
13:
f
VA
 
BimodalFusion
(
g
V
;g
A
)
.
bimodalfusion
14:
f
AT
 
BimodalFusion
(
g
A
;g
T
)
15:
f
VT
 
BimodalFusion
(
g
V
;g
T
)
16:
for
m
2f
VA;AT;VT
g
do
17:
F
m
=
GRU
m
(
f
m
)
18:
f
AVT
 
TrimodalFusion
(
F
VA
;F
AT
;F
VT
)
.
trimodalfusion
19:
F
AVT
=
GRU
AVT
(
f
AVT
)
20:
for
i:[1,
N
]
do
.
softmax
21:
^
y
i
=argmax
j
(
softmax
(
F
i
AVT
)[
j
])
22:
TestModel
(
V
)
23:
procedure
MapToSpace
(
x
z
)
.
formodality
z
24:
g
z
 
tanh(
W
z
x
z
+
b
z
)
25:
return
g
z
26:
procedure
BimodalFusion
(
g
z
1
,
g
z
2
)
.
formodality
z
1
and
z
2
,where
z
1
6
=
z
2
27:
for
i:[1,
D
]
do
28:
f
i
z
1
z
2
 
tanh(
w
z
1
z
2
i
:
[
g
i
z
1
;g
i
z
2
]
|
+
b
z
1
z
2
i
)
29:
f
z
1
z
2
 
(
f
1
z
1
z
2
;f
2
z
1
z
2
;:::;f
D
z
1
z
2
)
30:
return
f
z
1
z
2
31:
procedure
TrimodalFusion
(
f
z
1
,
f
z
2
,
f
z
3
)
.
formodalitycombination
z
1
,
z
2
,and
z
3
,
where
z
1
6
=
z
2
6
=
z
3
32:
for
i:[1,
D
]
do
33:
f
i
z
1
z
2
z
3
 
tanh(
w
i
:
[
f
i
z
1
;f
i
z
2
;f
i
z
3
]
|
+
b
i
)
34:
f
z
1
z
2
z
3
 
(
f
1
z
1
z
2
z
3
;f
2
z
1
z
2
z
3
;:::;f
D
z
1
z
2
z
3
)
35:
return
f
z
1
z
2
z
3
36:
procedure
TestModel
(
V
)
37:
Similarlytotrainingphase,
V
ispassedthroughthelearntmodelstogetthefeatures
andoutputs.Section3.6mentionsthetrainableparameters(

).
(stronglypositive)byeannotators.Wetooktheaverageoftheseveanno-
tationsasthesentimentpolarityandconsideredonlytwoclasses(positiveand
negative).Giveneveryindividual'suniquewayofexpressingsentiments,real
15
worldapplicationsshouldbeabletomodelgenericpersonindependentfeatures
andberobusttopersonvariance.Tothisend,weperformperson-independent
experimentstoemulateunseenconditions.Ourtrain/testsplitsofthedataset
arecompletelydisjointwithrespecttospeakers.Thetrain/validationsetcon-
sistsofthe62individualsinthedataset.Thetestsetcontainsopinionated
videosbyrestofthe31speakers.Inparticular,1447and752utterancesare
usedfortrainingandtestrespectively.
4.1.2.IEMOCAP
IEMOCAP[55]containstwowayconversationsamongtenspeakers,seg-
mentedintoutterances.Theutterancesaretaggedwiththelabelsanger,hap-
piness,sadness,neutral,excitement,frustration,fear,surprise,andother.We
considerthefouronestocomparewiththestateoftheart[1]andother
works.Itcontains1083angry,1630happy,1083sad,and1683neutralvideos.
Onlythevideosbytheeightspeakersareconsideredfortraining.
4.2.Baselines
Wecompareourmethodwiththefollowingstrongbaselines.
Earlyfusion.
Weextractunimodalfeatures(Section3.2)andsimplyconcate-
natethemtoproducemultimodalfeatures.Followedbysupportvectormachine
(SVM)beingappliedonthisfeaturevectorforthesentiment
Methodfrom[43].
Wehaveimplementedandcomparedourmethodwiththe
approachproposedbyPoriaetal.[43].Intheirapproach,theyextractedvisual
featuresusingCLM-Z,audiofeaturesusingopenSMILE,andtextualfeatures
usingCNN.MKLwasthenappliedtothefeaturesobtainedfromconcatenation
oftheunimodalfeatures.However,theydidnotconductspeakerindependent
experiments.
Inordertoperformafaircomparisonwith[43],weemployourfusionmethod
onthefeaturesextractedbyPoriaetal.[43].
16
Methodfrom[1].
Wehavecomparedourmethodwith[46],whichtakesadvan-
tageofcontextualinformationobtainedfromthesurroundingutterances.This
contextmodelingisachievedusingLSTM.WererantheexperimentsofPoria
etal.[46]withoutusingSVMforclasscationsinceusingSVMwithneuralnet-
worksisusuallydiscouraged.Thisprovidesafaircomparisonwithourmodel
whichdoesnotuseSVM.
Methodfrom[44].
In[44],theyproposedatrimodalfusionmethodbasedon
thetensors.Wehavealsocomparedourmethodwiththeir.Inparticular,their
datasetwastthanussowehaveadaptedtheirpublicly
availablecode
5
andemployedthatonourdataset.
4.3.ExperimentalSetting
Weconsideredtwovariantsofexperimentalsetupwhileevaluatingourmodel.
HFusion.
Inthissetup,weevaluatedhierarchicalfusionwithoutcontext-aware
featureswithCMU-MOSIdataset.WeremovedalltheGRUsfromthemodel
describedinSections3.3and3.4forwardedutterancespfeaturesdirectly
tothenextlayer.ThissetupisdepictedinFig.3.
CHFusion.
ThissetupisexactlyasthemodeldescribedinSection3.
4.4.ResultsandDiscussion
Wediscusstheresultsforthetexperimentalsettingsdiscussedin
Section4.3.
4.4.1.HierarchicalFusion(HFusion)
TheresultsofourexperimentsarepresentedinTable2.Weevaluatedthis
setupwithCMU-MOSIdataset(Section4.1.1)andtwofeaturesets:thefeature
setusedin[43]andthesetofunimodalfeaturesdiscussedinSection3.2.
5
https://github.com/A2Zadeh/TensorFusionNetwork
17
Table2:ComparisonintermsofaccuracyofHierarchicalFusion(HFusion)withotherfusion
methodsforCMU-MOSIdataset;boldfontbestaccuracyforthecorresponding
featuresetandmodalityormodalities,whereTstandsfortext,Vforvideo,andAforaudio.
SOTA
1
=Poriaetal.[43],
SOTA
2
=Zadehetal.[44]
Modality
Combination
[43]featuresetOurfeatureset
SOTA
1
SOTA
2
HFusionEarlyfusion
SOTA
2
HFusion
TN/A75.0%
VN/A55.3%
AN/A56.9%
T+V73.2%73.8%
74.4%
77.1%77.4%
77.8%
T+A73.2%73.5%
74.2%
77.1%76.3%
77.3%
A+V55.7%56.2%
57.5%
56.5%56.1%
56.8%
A+V+T73.5%71.2%
74.6%
77.0%77.3%
77.9%
Ourmodeloutperformed[43],whichemployedMKL,forallbimodaland
trimodalscenariosbyamarginof1{1.8%.Thisleadsustopresenttwoobser-
vations.Firstly,thefeaturesusedin[43]areinferiortothefeaturesextracted
inourapproach.Second,ourhierarchicalfusionmethodisbetterthantheir
fusionmethod.
Itisalreadyestablishedintheliterature[43,56]thatmultimodalanalysis
outperformsunimodalanalysis.Wealsoobservethesametrendinourexper-
imentswheretrimodalandbimodaloutperformunimodal
Thetextualmodalityperformedbestamongotherswithahigherunimodalclas-
accuracyof75%.Althoughothermodalitiescontributetoimprovethe
performanceofmultimodalthatcontributionislittleincompareto
thetextualmodality.
Ontheotherhand,wecomparedourmodelwithearlyfusion(Section4.2)for
aforementionedfeaturesets(Section3.2).Ourfusionmechanismconsistently
outperformsearlyfusionforallcombinationofmodalities.Thissupportsour
hypothesisthatourhierarchicalfusionmethodcapturestheinter-relationamong
themodalitiesandproducebetterperformancevectorthanearlyfusion.Textis
thestrongestindividualmodality,andweobservethatthetextmodalitypaired
withremainingtwomodalitiesresultsinconsistentperformanceimprovement.
Overall,theresultsgiveastrongindicationthatthecomparisonamongthe
abstractfeaturevaluesdampenstheoflessimportantmodalities,which
18
Table3:ComparisonofContext-AwareHierarchicalFusion(CHFusion)intermsofaccuracy
(CHFusion
acc
)andf-score(forIEMOCAP:CHFusion
fsc
)withthestateoftheartforCMU-
MOSIandIEMOCAPdataset;boldfontbestaccuracyforthecorrespondingdataset
andmodalityormodalities,whereTstandstext,Vforvideo,Aforaudio.
SOTA
1
=Poria
etal.[43],
SOTA
2
=Zadehetal.[44].CHFusion
acc
andCHFusion
fsc
aretheaccuracyand
f-scoreofCHFusionrespectively.
Modality
CMU-MOSIIEMOCAP
SOTA
1
SOTA
2
CHFusion
acc
SOTA
1
SOTA
2
CHFusion
acc
CHFusion
fsc
T76.5%73.6%-
V54.9%53.3%-
A55.3%57.1%-
T+V77.8%77.1%
79.3%
74.1%73.7%
75.9%
75.6%
T+A77.3%77.0%
79.1%
73.7%71.1%
76.1%
76.0%
A+V57.9%56.5%
58.8%
68.4%67.4%
69.5%
69.6%
A+V+T78.7%77.2%
80.0%
74.1%73.6%
76.5%
76.8%
wasourhypothesis.Forexample,wecannoticethatforearlyfusionT+V
andT+Abothyieldthesameperformance.However,withourmethodtext
withvideoperformsbetterthantextwithaudio,whichismorealignedwith
ourexpectations,sincefacialmusclemovementsusuallycarrymoreemotional
nuancesthanvoice.
Inparticular,weobservethatourmodeloutperformedallthestrongbase-
linesmentionedabove.Themethodby[43]isonlyabletofuseusingconcate-
nation.Ourproposedmethodoutperformedtheirapproachbyasignit
margin;thankstothepowerofhierarchicalfusionwhichprovesthecapabil-
ityofourmethodinmodelingbimodalandtrimodalcorrelations.However
ontheotherhand,themethodby[44]iscapableoffusingthemodalitiesus-
ingatensor.Interestinglyourmethodalsooutperformedthemandwethink
thereasonisthecapabilityofbimodalfusionandusethatfortrimodalfusion.
Tensorfusionnetworkisincapabletolearntheweightsofthebimodalandtri-
modalcorrelationsinthefusion.TensorFusionismathematicallyformedbyan
outerproduct,ithasnolearn-ableparameters.Whereinourmethodlearnsthe
weightsautomaticallyusinganeuralnetwork(Equation1,2and3).
19
4.4.2.Context-AwareHierarchicalFusion(CHFusion)
TheresultsofthisexperimentareshowninTable3.Thissettingfullyutilizes
themodeldescribedinSection3.Weappliedthisexperimentalsettingfortwo
datasets,namelyCMU-MOSI(Section4.1.1)andIEMOCAP(Section4.1.2).
WeusedthefeaturesetdiscussedinSection3.2,whichwasalsousedbyPoria
etal.[1].Asexpectedourmethodoutperformedthesimpleearlyfusionbased
fusionby[43],tensorfusionby[44].ThemethodbyPoriaetal.[1]useda
schemetolearncontextualfeaturesfromthesurroundingfeatures.However,
asamethodoffusiontheyadaptedsimpleconcatenationbasedfusionmethod
by[43].AsdiscussedinSection3.3,weemployedtheircontextualfeatureex-
tractionframeworkandintegratedourproposedfusionmethodtothat.This
hashelpedustooutperformPoriaetal.[1]bytmarginthankstothe
hierarchicalfusion(HFusion).
CMU-MOSI.
Weachieve1{2%performanceimprovementoverthestateofthe
art[1]forallthemodalitycombinationshavingtextualcomponent.ForA+V
modalitycombinationweachievebetterbutsimilarperformancetothestate
oftheart.Wesuspectthatitisduetobothaudioandvideomodalitybeing
tlylessinformativethantextualmodality.Itisevidentfromtheuni-
modalperformancewhereweobservethattextualmodalityonitsownperforms
around21%betterthanbothaudioandvideomodality.Also,audioandvideo
modalityperformsclosetomajoritybaseline.Ontheotherhand,itisimpor-
tanttonoticethatwithallmodalitiescombinedweachieveabout3.5%higher
accuracythantextalone.
Forexample,considerthefollowingutterance:
sooverallnewmooneven
withthebiggerbetterbudgetshuhitwasstilltoolong
.Thespeakerdiscussesher
opiniononthemovieTwilightNewMoon.Textuallytheutteranceisabundant
withpositivewordshoweveraudioandvideocomprisesofafrownwhichis
observedbythehierarchicalfusionbasedmodel.
IEMOCAP.
AstheIEMOCAPdatasetcontainsfourdistinctemotioncate-
gories,inthelastlayerofthenetworkweusedasoftmaxwhoseout-
20
Table4:Class-wiseaccuracyandf-scoreforIEMOCAPdatasetfortrimodalscenario.
Metrics
Classes
HappySadNeutralAnger
Accuracy74.375.678.479.6
F-Score81.477.071.277.6
putdimensionissetto4.InordertoperformonIEMOCAP
datasetwefeedthefusedfeatures
F
mt
(where
m
=
AV;VT;TA;
or
AVT
and
t
=1
;
2
;:::;N
)toasoftmaxlayerwith
C
=4outputs.Thecanbe
describedasfollows:
P
=softmax(
W
softmax
F
mt
+
b
softmax
)
;
^
y
=argmax
j
(
P
[
j
])
;
where
W
softmax
2
R
4

D
,
b
softmax
2
R
4
,
P2
R
4
,
j
=classvalue(0or1or2or
3),and^
y
=estimatedclassvalue.
Hereaswell,weachieveperformanceimprovementconsistentwithCMU-
MOSI.Thismethodperforms1{2.4%betterthanthestateoftheartforallthe
modalitycombinations.Also,trimodalaccuracyis3%higherthanthesamefor
textualmodality.Since,IEMOCAPdatasetimbalanced,wealsopresentthe
f-scoreforeachmodalitycombinationforabetterevaluation.Onekeyobser-
vationforIEMOCAPdatasetisthatitsA+Vmodalitycombinationperforms
tlybetterthanthesameofCMU-MOSIdataset.Wethinkthatthisis
duetotheaudioandvideomodalityofIEMOCAPbeingricherthanthesame
ofCMU-MOSI.Theperformancewithanotherstrongbaseline[44]is
evenmorerangingfrom2.1%to3%onCMU-MOSIdatasetand2.2%to5%on
IEMOCAPdataset.Thisagainthesuperiorityofthehierarchicalfu-
sionincompareto[44].Wethinkthisismainlybecauseoflearningtheweights
ofbimodalandtrimodalcorrelation(representingthedegreeofcorrelations)
calculationsatthetimeoffusionwhileTensorFusionNetwork(TFN)justre-
liesonthenon-trainableouterproductoftensorstomodelsuchcorrelationsfor
fusion.Additionally,wepresentclass-wiseaccuracyandf-scoreforIEMOCAP
21
fortrimodal(A+V+T)scenarioinTable4.
4.4.3.HFusionvs.CHFusion
WecompareHFusionandCHFusionmodelsoverCMU-MOSIdataset.We
observethatCHFusionperforms1{2%betterthanHFusionmodelforallthe
modalitycombinations.Thisperformanceboostisachievedbytheinclusion
ofutterance-levelcontextualinformationinHFusionmodelbyaddingGRUsin
tlevelsoffusionhierarchy.
5.Conclusion
Multimodalfusionstrategyisanimportantissueinmultimodalsentiment
analysis.However,littleworkhasbeendonesofarinthisdirection.Inthispa-
per,wehavepresentedanovelandcomprehensivefusionstrategy.Ourmethod
outperformsthewidelyusedearlyfusiononbothdatasetstypicallyusedtotest
multimodalsentimentanalysismethods.Moreover,withtheadditionofcontext
modelingwithGRU,ourmethodoutperformsthestateoftheartinmultimodal
sentimentanalysisandemotiondetectionbytmargin.
Inourfuturework,weplantoimprovethequalityofunimodalfeatures,
especiallytextualfeatures,whichwillfurtherimprovetheaccuracyof
cation.Wewillalsoexperimentwithmoresophisticatednetworkarchitectures.
Acknowledgement
TheworkwaspartiallysupportedbytheInstitutoPolitecnicoNacionalvia
grantSIP20172008toA.Gelbukh.
[1]
S.Poria,E.Cambria,D.Hazarika,N.Mazumder,A.Zadeh,L.-P.Morency,
Context-dependentsentimentanalysisinuser-generatedvideos,in:ACL,
873{883,2017.
[2]
E.Cambria,eComputingandSentimentAnalysis,IEEEIntelligent
Systems31(2)(2016)102{107.
22
[3]
S.Poria,E.Cambria,R.Bajpai,A.Hussain,Areviewofecomput-
ing:Fromunimodalanalysistomultimodalfusion,InformationFusion37
(2017)98{125.
[4]
D.Hazarika,S.Poria,A.Zadeh,E.Cambria,L.-P.Morency,R.Zimmer-
mann,Conversationalmemorynetworkforemotionrecognitionindyadic
dialoguevideos,in:NAACL,2122{2132,2018.
[5]
I.Chaturvedi,E.Cambria,R.Welsch,F.Herrera,Distinguishingbetween
factsandopinionsforsentimentanalysis:Surveyandchallenges,Informa-
tionFusion44(2018)65{77.
[6]
S.L.Lo,E.Cambria,R.Chiong,D.Cornforth,Multilingualsentiment
analysis:Fromformaltoinformalandscarceresourcelanguages,
IntelligenceReview48(4)(2017)499{527.
[7]
K.Dashtipour,S.Poria,A.Hussain,E.Cambria,A.Y.Hawalah,A.Gel-
bukh,Q.Zhou,Multilingualsentimentanalysis:stateoftheartandin-
dependentcomparisonoftechniques,Cognitivecomputation8(4)(2016)
757{771.
[8]
H.Peng,Y.Ma,Y.Li,E.Cambria,LearningMulti-grainedAspectTarget
SequenceforChineseSentimentAnalysis,in:Knowledge-BasedSystems,
vol.148,167{176,2018.
[9]
A.Bandhakavi,N.Wiratunga,S.Massie,P.Deepak,Lexicongenerationfor
emotionanalysisoftext,IEEEIntelligentSystems32(1)(2017)102{108.
[10]
M.Dragoni,S.Poria,E.Cambria,OntoSenticNet:ACommonsenseOn-
tologyforSentimentAnalysis,IEEEIntelligentSystems33(3).
[11]
E.Cambria,S.Poria,D.Hazarika,K.Kwok,SenticNet5:Discoveringcon-
ceptualprimitivesforsentimentanalysisbymeansofcontextembeddings,
in:AAAI,1795{1802,2018.
23
[12]
L.Oneto,F.Bisio,E.Cambria,D.Anguita,Statisticallearningtheory
andELMforbigsocialdataanalysis,IEEEComputationalIntelligence
Magazine11(3)(2016)45{55.
[13]
A.Hussain,E.Cambria,Semi-supervisedlearningforbigsocialdataanal-
ysis,Neurocomputing275(2018)1662{1673.
[14]
Y.Li,Q.Pan,T.Yang,S.Wang,J.Tang,E.Cambria,Learningword
representationsforsentimentanalysis,CognitiveComputation9(6)(2017)
843{851.
[15]
T.Young,D.Hazarika,S.Poria,E.Cambria,Recenttrendsindeeplearn-
ingbasednaturallanguageprocessing,IEEEComputationalIntelligence
Magazine13(3).
[16]
Y.Li,Q.Pan,S.Wang,T.Yang,E.Cambria,AGenerativeModelfor
categorytextgeneration,InformationSciences450(2018)301{315.
[17]
E.Cambria,S.Poria,A.Gelbukh,M.Thelwall,SentimentAnalysisisa
BigSuitcase,IEEEIntelligentSystems32(6)(2017)74{80.
[18]
Y.Xia,E.Cambria,A.Hussain,H.Zhao,WordPolarityDisambiguation
UsingBayesianModelandOpinion-LevelFeatures,CognitiveComputation
7(3)(2015)369{380.
[19]
I.Chaturvedi,E.Ragusa,P.Gastaldo,R.Zunino,E.Cambria,Bayesian
networkbasedextremelearningmachineforsubjectivitydetection,Journal
ofTheFranklinInstitute355(4)(2018)1780{1797.
[20]
N.Majumder,S.Poria,A.Gelbukh,E.Cambria,Deeplearning-based
documentmodelingforpersonalitydetectionfromtext,IEEEIntelligent
Systems32(2)(2017)74{79.
[21]
R.Satapathy,C.Guerreiro,I.Chaturvedi,E.Cambria,Phonetic-Based
MicrotextNormalizationforTwitterSentimentAnalysis,in:ICDM,407{
413,2017.
24
[22]
D.Rajagopal,E.Cambria,D.Olsher,K.Kwok,Agraph-basedapproach
tocommonsenseconceptextractionandsemanticsimilaritydetection,in:
WWW,565{570,2013.
[23]
X.Zhong,A.Sun,E.Cambria,Timeexpressionanalysisandrecognition
usingsyntactictokentypesandgeneralheuristicrules,in:ACL,420{429,
2017.
[24]
Y.Ma,H.Peng,E.Cambria,Targetedaspect-basedsentimentanalysisvia
embeddingcommonsenseknowledgeintoanattentiveLSTM,in:AAAI,
5876{5883,2018.
[25]
F.Xing,E.Cambria,R.Welsch,NaturalLanguageBasedFinancialFore-
casting:ASurvey,IntelligenceReview50(1)(2018)49{73.
[26]
M.Ebrahimi,A.Hossein,A.Sheth,Challengesofsentimentanalysisfor
dynamicevents,IEEEIntelligentSystems32(5)(2017)70{75.
[27]
E.Cambria,A.Hussain,T.Durrani,C.Havasi,C.Eckl,J.Munro,Sentic
ComputingforPatientCenteredApplication,in:IEEEICSP,1279{1282,
2010.
[28]
A.Valdivia,V.Luzon,F.Herrera,SentimentanalysisinTripAdvisor,IEEE
IntelligentSystems32(4)(2017)72{77.
[29]
R.Mihalcea,A.Garimella,WhatMenSay,WhatWomenHear:Finding
Gender-SpMeaningShades,IEEEIntelligentSystems31(4)(2016)
62{67.
[30]
S.Cavallari,V.Zheng,H.Cai,K.Chang,E.Cambria,Learningcommunity
embeddingwithcommunitydetectionandnodeembeddingongraphs,in:
CIKM,377{386,2017.
[31]
C.Xu,E.Cambria,P.S.Tan,AdaptiveTwo-StageFeatureSelectionfor
Sentimentin:IEEESMC,1238{1243,2017.
25
[32]
A.Zadeh,P.P.Liang,S.Poria,P.Vij,E.Cambria,L.-P.Morency,Multi-
attentionrecurrentnetworkforhumancommunicationcomprehension,in:
AAAI,5642{5649,2018.
[33]
T.Young,E.Cambria,I.Chaturvedi,H.Zhou,S.Biswas,M.Huang,
AugmentingEnd-to-EndDialogueSystemswithCommonsenseKnowledge,
in:AAAI,4970{4977,2018.
[34]
L.C.DeSilva,T.Miyasato,R.Nakatsu,Facialemotionrecognitionusing
multi-modalinformation,in:ProceedingsofICICS,vol.1,IEEE,397{401,
1997.
[35]
L.S.Chen,T.S.Huang,T.Miyasato,R.Nakatsu,Multimodalhuman
emotion/expressionrecognition,in:ProceedingsoftheThirdIEEEInter-
nationalConferenceonAutomaticFaceandGestureRecognition,IEEE,
366{371,1998.
[36]
L.Kessous,G.Castellano,G.Caridakis,Multimodalemotionrecognitionin
speech-basedinteractionusingfacialexpression,bodygestureandacoustic
analysis,JournalonMultimodalUserInterfaces3(1-2)(2010)33{48.
[37]
B.Schuller,Recognizingfromlinguisticinformationin3Dcontinuous
space,IEEETransactionsoneComputing2(4)(2011)192{205.
[38]
M.Wollmer,F.Weninger,T.Knaup,B.Schuller,C.Sun,K.Sagae,L.-P.
Morency,Youtubemoviereviews:Sentimentanalysisinanaudio-visual
context,IEEEIntelligentSystems28(3)(2013)46{53.
[39]
V.Rozgic,S.Ananthakrishnan,S.Saleem,R.Kumar,R.Prasad,Ensemble
ofSVMtreesformultimodalemotionrecognition,in:Signal&Information
ProcessingAssociationAnnualSummitandConference(APSIPAASC),
2012Asia-PIEEE,1{4,2012.
[40]
A.Metallinou,S.Lee,S.Narayanan,Audio-visualemotionrecognitionus-
inggaussianmixturemodelsforfaceandvoice,in:TenthIEEEInterna-
tionalSymposiumonISM2008,IEEE,250{257,2008.
26
[41]
F.Eyben,M.ollmer,A.Graves,B.Schuller,E.Douglas-Cowie,R.Cowie,
On-lineemotionrecognitionina3-Dactivation-valence-timecontinuum
usingacousticandlinguisticcues,JournalonMultimodalUserInterfaces
3(1-2)(2010)7{19.
[42]
C.-H.Wu,W.-B.Liang,Emotionrecognitionofespeechbasedon
multipleusingacoustic-prosodicinformationandsemanticlabels,
IEEETransactionsoneComputing2(1)(2011)10{21.
[43]
S.Poria,I.Chaturvedi,E.Cambria,A.Hussain,ConvolutionalMKL
basedmultimodalemotionrecognitionandsentimentanalysis,in:ICDM,
Barcelona,439{448,2016.
[44]
A.Zadeh,M.Chen,S.Poria,E.Cambria,L.-P.Morency,TensorFusion
NetworkforMultimodalSentimentAnalysis,in:EMNLP,1114{1125,2017.
[45]
S.Poria,H.Peng,A.Hussain,N.Howard,E.Cambria,Ensembleappli-
cationofconvolutionalneuralnetworksandmultiplekernellearningfor
multimodalsentimentanalysis,Neurocomputing261(2017)217{230.
[46]
S.Poria,E.Cambria,A.Gelbukh,Deepconvolutionalneuralnetworktex-
tualfeaturesandmultiplekernellearningforutterance-levelmultimodal
sentimentanalysis,in:EMNLP,2539{2544,2015.
[47]
A.Karpathy,G.Toderici,S.Shetty,T.Leung,R.Sukthankar,L.Fei-Fei,
Large-scalevideowithconvolutionalneuralnetworks,in:Pro-
ceedingsoftheIEEEconferenceonComputerVisionandPatternRecog-
nition,1725{1732,2014.
[48]
T.Mikolov,K.Chen,G.Corrado,J.Dean,testimationofword
representationsinvectorspace,arXivpreprintarXiv:1301.3781.
[49]
V.Teh,G.E.Hinton,Rate-codedrestrictedBoltzmannmachinesforface
recognition,in:T.Leen,T.Dietterich,V.Tresp(Eds.),Advancesinneural
informationprocessingsystem,vol.13,908{914,2001.
27
[50]
F.Eyben,M.ollmer,B.Schuller,Opensmile:theMunichversatileand
fastopen-sourceaudiofeatureextractor,in:Proceedingsofthe18thACM
internationalconferenceonMultimedia,ACM,1459{1462,2010.
[51]
S.Ji,W.Xu,M.Yang,K.Yu,3Dconvolutionalneuralnetworksforhu-
manactionrecognition,IEEEtransactionsonpatternanalysisandmachine
intelligence35(1)(2013)221{231.
[52]
N.Majumder,MultimodalSentimentAnalysisinSocialMediausingDeep
LearningwithConvolutionalNeuralNetworks,Master'sthesis,CIC,Insti-
tutoPolitecnicoNacional,2017.
[53]
D.P.Kingma,J.Ba,Adam:AMethodforStochasticOptimization,CoRR
abs/1412.6980,URL
http://arxiv.org/abs/1412.6980
.
[54]
A.Zadeh,R.Zellers,E.Pincus,L.-P.Morency,Multimodalsentimentin-
tensityanalysisinvideos:Facialgesturesandverbalmessages,IEEEIn-
telligentSystems31(6)(2016)82{88.
[55]
C.Busso,M.Bulut,C.-C.Lee,A.Kazemzadeh,E.Mower,S.Kim,J.N.
Chang,S.Lee,S.S.Narayanan,IEMOCAP:Interactiveemotionaldyadic
motioncapturedatabase,Languageresourcesandevaluation42(4)(2008)
335{359.
[56]
V.Perez-Rosas,R.Mihalcea,L.-P.Morency,Utterance-LevelMultimodal
SentimentAnalysis,in:ACL(1),973{982,2013.
28
"
43,GILE: A Generalized Input-Label Embedding for Text Classification,http://arxiv.org/pdf/1806.06219v3.pdf,https://github.com/idiap/gile,"GILE:AGeneralizedInput-LabelEmbeddingforText
NikolaosPappasJamesHenderson
IdiapResearchInstitute,Martigny1920,Switzerland
{nikolaos.pappas,james.henderson@idiap.ch}
Abstract
Neuraltextmodelstypically
treatoutputlabelsascategoricalvariables
whichlackdescriptionandsemantics.This
forcestheirparametrizationtobedepen-
dentonthelabelsetsize,and,hence,they
areunabletoscaletolargelabelsetsand
generalizetounseenones.Existingjoint
input-labeltextmodelsovercometheseis-
suesbyexploitinglabeldescriptions,but
theyareunabletocapturecomplexlabelre-
lationships,haverigidparametrization,and
theirgainsonunseenlabelshappenoften
attheexpenseofweakperformanceonthe
labelsseenduringtraining.Inthispaper,
weproposeanewinput-labelmodelwhich
generalizesoverprevioussuchmodels,ad-
dressestheirlimitations,anddoesnotcom-
promiseperformanceonseenlabels.The
modelconsistsofajointnon-linearinput-
labelembeddingwithcontrollablecapacity
andajoint-space-dependent
unitwhichistrainedwithcross-entropyloss
tooptimizeperformance.We
evaluatemodelsonfull-resourceandlow-or
zero-resourcetextofmultilin-
gualnewsandbiomedicaltextwithalarge
labelset.Ourmodeloutperformsmonolin-
gualandmultilingualmodelswhichdonot
leveragelabelsemanticsandpreviousjoint
input-labelspacemodelsinbothscenarios.
1Introduction
TextisafundamentalNLPtaskwith
numerousreal-worldapplicationssuchastopic
recognition(
Tangetal.
,
2015
;
Yangetal.
,
2016
),
sentimentanalysis(
PangandLee
,
2005
;
Yang
etal.
,
2016
),andquestionanswering(
Chenetal.
,
2015
;
Kumaretal.
,
2015
).alsoap-
pearsasasubtaskforsequencepredictiontasks
suchasneuralmachinetranslation(
Choetal.
,
2014
;
Luongetal.
,
2015
),andsummarization
(
Rushetal.
,
2015
).Despitethenumerousstud-
ies,existingmodelsaretrainedonaedlabel
setusingk-hotvectorsand,therefore,treattarget
labelsasmereatomicsymbolswithoutanypartic-
ularstructuretothespaceoflabels,ignoringpo-
tentiallinguisticknowledgeaboutthewordsused
todescribetheoutputlabels.Giventhatseman-
ticrepresentationsofwordshavebeenshownto
beusefulforrepresentingtheinput,itisreason-
abletoexpectthattheyaregoingtobeusefulfor
representingthelabelsaswell.
Previousworkhasleveragedknowledgefrom
thelabeltextsthroughajointinput-labelspace,
initiallyforimage(
Westonetal.
,
2011
;
Mensinketal.
,
2012
;
Fromeetal.
,
2013
;
Socheretal.
,
2013
).Suchmodelsgeneralizeto
labelsbothseenandunseenduringtraining,and
scalewellonverylargelabelsets.However,aswe
explaininSection
2
,existinginput-labelmodels
fortext(
YazdaniandHenderson
,
2015
;
Nametal.
,
2016
)havethefollowinglimitations:(i)theirem-
beddingdoesnotcapturecomplexlabelrelation-
shipsduetoitsbilinearform,(ii)theiroutputlayer
parametrizationisrigidbecauseitdependsonthe
dimensionalityoftheencodedtextandlabels,and,
(iii)theyareoutperformedonseenlabelsbyclas-
baselinestrainedwithcross-entropyloss
(
Fromeetal.
,
2013
;
Socheretal.
,
2013
).
Inthispaper,weproposeanewjointinput-label
modelwhichgeneralizesoverprevioussuchmod-
els,addressestheirlimitations,anddoesnotcom-
promiseperformanceonseenlabels(seeFigure
1
).Theproposedmodeliscomprisedofajoint
non-linearinput-labelembeddingwithcontrol-
lablecapacityandajoint-space-dependentclassi-
unitwhichistrainedwithcross-entropy
losstooptimizeperformance.
1
The
needforcapturingcomplexlabelrelationships
isaddressedbytwonon-lineartransformations
whichhavethesametargetjointspacedimension-
1
Ourcodeisavailableat:
github.com/idiap/gile
arXiv:1806.06219v3  [cs.CL]  30 Jan 2019ality.Theparametrizationoftheoutputlayeris
notconstrainedbythedimensionalityofthein-
putorlabelencoding,butisinsteadxiblewitha
capacitywhichcanbeeasilycontrolledbychoos-
ingthedimensionalityofthejointspace.Training
isperformedwithcross-entropyloss,whichisa
suitablesurrogatelossforproblems,
asopposedtoarankinglosssuchasWARPloss
(
Westonetal.
,
2010
)whichismoresuitablefor
rankingproblems.
Evaluationisperformedonfull-resourceand
low-orzero-resourcescenariosoftwotextclas-
tasks,namelyonbiomedicalsemantic
indexing(
Nametal.
,
2016
)andonmultilingual
news(
PappasandPopescu-Belis
,
2017
)againstseveralcompetitivebaselines.In
bothscenarios,weprovideacomprehensiveabla-
tionanalysiswhichhighlightstheimportanceof
eachmodelcomponentandthedifferencewith
previousembeddingformulationswhenusingthe
sametypeofarchitectureandlossfunction.
Ourmaincontributionsarethefollowing:
(i)
Weidentifykeytheoreticalandpracticallim-
itationsofexistingjointinput-labelmodels.
(ii)
Weproposeanoveljointinput-labelembed-
dingwithxibleparametrizationwhichgen-
eralizesovertheprevioussuchmodelsand
addressestheirlimitations.
(iii)
Weprovideempiricalevidenceofthesupe-
riorityofourmodelovermonolingualand
multilingualmodelswhichignorelabelse-
mantics,andoverpreviousjointinput-label
modelsonbothseenandunseenlabels.
Theremainderofthispaperisorganizedasfol-
lows.Section2providesbackgroundknowledge
andexplainslimitationsofexistingmodels.Sec-
tion3describesthemodelcomponents,training
andrelationtopreviousformulations.Section
4describesourevaluationresultsandanalysis,
whileSection5providesanoverviewofprevious
workandSection6concludesthepaperandpro-
videsfutureresearchdirections.
2Background:NeuralText

Wearegivenacollection
D
=
f
(
x
i
;y
i
)
;i
=
1
;:::;N
g
madeof
N
documents,whereeach
document
x
i
isassociatedwithlabels
y
i
=
f
y
ij
2f
0
;
1
gj
j
=1
;:::;k
g
,andkisthe
totalnumberoflabels.Eachdocument
x
i
=
f
w
11
;w
12
;:::;w
K
i
T
K
i
g
isasequenceofwords
groupedintosentences,with
K
i
beingthenum-
berofsentencesindocument
i
and
T
j
beingthe
numberofwordsinsentence
j
.Eachlabel
j
has
atextualdescriptioncomprisedofmultiplewords,
c
j
=
f
c
j
1
;c
j
2
;:::;c
jL
j
j
j
=1
;:::;k
g
with
L
j
beingthenumberofwordsineachdescription.
Giventheinputtextsandtheirassociatedlabels
seen
duringthetrainingportionof
D
,ourgoalis
tolearnatextwhichisabletopredict
labels
both
intheseen,
Y
s
,orunseen,
Y
u
,label
sets,asthesetsofuniquelabelswhich
havebeenseenornotduringtrainingrespectively
and,hence,
Y\Y
u
=
;
and
Y
=
Y
s
[Y
u
.
2
2.1InputTextRepresentation
Toencodetheinputtext,wefocusonhierarchical
attentionnetworks(HANs),whicharecompetitive
formonolingual(
Yangetal.
,
2016
)andmultilin-
gualtext(
PappasandPopescu-Belis
,
2017
).Themodeltakesasinputadocument
x
and
outputsadocumentvector
h
.Theinputwordsand
labelwordsarerepresentedbyvectorsin
IR
d
from
thesame
3
embeddings
E
2
IR
jV
d
,where
V
is
thevocabularyand
d
istheembeddingdimension;
E
canbepre-trainedorlearnedjointlywiththe
restofthemodel.Themodelhastwolevelsof
abstraction,wordandsentence.Thewordlevelis
madeofanencodernetwork
g
w
andanattention
network
a
w
,whilethesentencelevelsimilarlyin-
cludesanencoderandanattentionnetwork.
Encoders
.Thefunction
g
w
encodesthesequence
ofinputwords
f
w
it
j
t
=1
;:::;T
i
g
foreachsen-
tence
i
ofthedocument,notedas:
h
(
it
)
w
=
g
w
(
w
it
)
;t
2
[1
;T
i
]
;
(1)
andatthesentencelevel,aftercombiningthein-
termediatewordvectors
f
h
(
it
)
w
j
t
=1
;:::;T
i
g
to
asentencevector
s
i
2
IR
d
w
(seebelow),where
d
w
isthedimensionofthewordencoder,thefunc-
tion
g
s
encodesthesequenceofsentencevectors
f
s
i
j
i
=1
;:::;K
g
,notedas
h
(
i
)
s
.The
g
w
and
g
s
functionscanbeanyfeed-forward(D
ENSE
)or
recurrentnetworks,e.g.GRU(
Choetal.
,
2014
).
Attention
.The

w
and

s
attentionmechanisms,
whichestimatetheimportanceofeachhidden
2
Notethatdependingonthenumberoflabelsperdocu-
menttheproblemcanbeamulti-labelormulti-classproblem.
3
Thisstatementholdstrueformultilingual
problemstooiftheembeddingsarealignedacrosslanguages.
statevector,areusedtoobtainthesentence
s
i
and
documentrepresentation
h
respectively.Thesen-
tencevectoristhuscalculatedasfollows:
s
i
=
T
i
X
t
=1

(
it
)
w
h
(
it
)
w
=
T
i
X
t
=1
exp(
v
>
it
u
w
)
P
j
exp(
v
>
ij
u
w
)
h
(
it
)
w
;
(2)
where
v
it
=
f
w
(
h
(
it
)
w
)
isafully-connectednet-
workwith
W
w
parameters.Thedocumentvector
h
2
IR
d
h
,where
d
h
isthedimensionofthesen-
tenceencoder,iscalculatedsimilarly,byreplacing
u
it
with
v
i
=
f
s
(
h
(
i
)
s
)
whichisafully-connected
networkwith
W
s
parameters,and
u
w
with
u
s
,
whichareparametersoftheattentionfunctions.
2.2LabelTextRepresentation
Toencodethelabeltextweuseanencoderfunc-
tionwhichtakesasinputalabeldescription
c
j
and
outputsalabelvector
e
j
2
IR
d
c
8
j=1
;:::;
k
.For
efyreasons,weuseasimple,parameter-
freefunctiontocompute
e
j
,namelytheaverage
ofwordvectorswhichdescribelabel
j
,namely
e
j
=
1
L
j
P
L
j
t
=1
c
jt
,andhence
d
c
=
d
inthis
case.Bystackingalltheselabelvectorsintoama-
trix,weobtainthelabelembedding
E2
IR
jY
d
.
Inprinciple,wecouldalsousethesameencoder
functionsastheonesforinputtext,butthiswould
increasethecomputationhence,we
keepthisdirectionasfuturework.
2.3OutputLayerparametrizations
2.3.1TypicalLinearUnit
Themosttypicaloutputlayer,consistsofalinear
unitwithaweightmatrix
W
2
IR
d
h
j
anda
biasvector
b
2
IR
jYj
followedbyasoftmaxor
sigmoidactivationfunction.Giventheencoder's
hiddenrepresentation
h
withdimensionsize
d
h
,
theprobabilitydistributionofoutput
y
giveninput
x
isproportionaltothefollowingquantity:
p
(
y
j
x
)
/
exp
(
W
>
h
+
b
)
:
(3)
Theparametersin
W
canbelearnedseparatelyor
betiedwiththeparametersoftheembedding
E
by
setting
W
=
E
T
iftheinputdimensionof
W
is
restrictedtobethesameasthatoftheembedding
E
(
d
=
d
h
)andeachlabelisrepresentedbya
singleworddescriptioni.e.when
Y
corresponds
to
V
and
E
=
E
.Inthelattercase,Eq.
3
becomes:
p
(
y
j
x
)
/
exp
(
Eh
+
b
)
:
(4)
Eitherway,theparametersofsuchmodelsaretyp-
icallylearnedwithcross-entropyloss,whichis
suitableforproblems.However,in
bothcasestheycannotbeappliedtolabelswhich
arenotseenduringtraining,becauseeachlabel
haslearnedparameterswhicharetothat
label,sotheparametersforunseenlabelscannot
belearned.Wenowturnourfocustoaclassof
modelswhichcanhandleunseenlabels.
2.3.2BilinearInput-LabelUnit
Jointinput-outputembeddingmodelscangener-
alizefromseentounseenlabelsbecausethepa-
rametersofthelabelencoderareshared.The
previouslyproposedjointinput-outputembedding
modelsby
YazdaniandHenderson
(
2015
)and
Nametal.
(
2016
)arebasedonthefollowingbi-
linearrankingfunction
f
(

)
:
f
(
x;y
)=
EW
h;
(5)
where
E2
IR
jY
d
isthelabelembeddingand
W2
IR
d

d
h
isthebilinearembedding.Thisfunc-
tionallowsonetotherankofagivenlabel
y
withrespectto
x
andistrainedusinghingeloss
torankpositivelabelshigherthannegativeones.
Butnotethattheuseofthisrankinglossmeans
thattheydonotmodeltheconditionalprobability,
asdothetraditionalmodelsabove.
Limitations
.Firstly,theaboveformulacanonly
capturelinearrelationshipsbetweenencodedtext
(
h
)andlabelembedding(
E
)through
W
.Weargue
thattherelationshipsbetweendifferentlabelsare
non-linearduetothecomplexinteractionsofthe
semanticrelationsacrosslabelsbutalsobetween
labelsanddifferentencodedinputs.Amoreap-
propriateformforthispurposewouldincludea
non-lineartransformation
˙
(

)
,e.g.witheither:
(a)
˙
(
EW
)
|
{z
}
Labelstructure
h
or(b)
E
˙
(
W
h
)
|
{z
}
Inputstructure
:
(6)
Secondly,itishardtocontroltheiroutputlayer
capacityduetotheirbilinearform,whichusesa
matrixofparameters(
W
)whosesizeisbounded
bythedimensionalitiesofthelabelembedding
andthetextencoding.Thirdly,theirlossfunction
optimizesrankinginsteadofperfor-
manceandthustreatstheground-truthasaranked
listwheninrealityitconsistsofoneormoreinde-
pendentlabels.
Summary
.Wehypothesizethatthesearetherea-
sonswhythesemodelsdonotyetperformwellon
seenlabelscomparedtomodelswhichmakeuse
ofthetypicallinearunit,andtheydonottakefull
advantageofthestructureoftheproblemwhen
testedonunseenlabels.Ideally,wewouldliketo
haveamodelwhichwilladdresstheseissuesand
willcombinethefromboththetypicallin-
earunitandthejointinput-labelmodels.
3TheProposedOutputLayerParamet-
rizationforText
Weproposeanewoutputlayerparametrizationfor
neuraltextwhichiscomprisedofa
generalizedinput-labelembeddingwhichcaptures
thestructureofthelabels,thestructureoftheen-
codedtextsandtheinteractionsbetweenthetwo,
followedbyaunitwhichisindepen-
dentofthelabelsetsize.Theresultingmodel
hasthefollowingproperties:(i)itisabletocap-
turecomplexoutputstructure,(ii)ithasaxi-
bleparametrizationwhichallowsitscapacityto
becontrolled,and(iii)itistrainedwithaclassi-
surrogatelosssuchascross-entropy.The
modelisdepictedinFigure
1
.Inthissection,we
describethemodelindetail,showinghowitcan
betrainedefforarbitrarilylargelabelsets
andhowitisrelatedtopreviousmodels.
3.1AGeneralizedInput-LabelEmbedding
Let
g
in
(
h
)
and
g
out
(
e
j
)
betwonon-linearprojec-
tionsoftheencodedinput,i.e.thedocument
h
,
andanyencodedlabel
e
j
,where
e
j
isthej
th
row
vectorfromthelabelembeddingmatrix
E
,which
havethefollowingform:
e
0
j
=
g
out
(
e
j
)=
˙
(
e
j
U
+
b
u
)
(7)
h
0
=
g
in
(
h
)=
˙
(
Vh
+
b
v
)
;
(8)
where
˙
(

)
isanonlinearactivationfunctionsuch
asReLUorTanh,thematrix
U
2
IR
d

d
j
andbias
b
u
2
IR
d
j
arethelinearprojectionofthelabels,
andthematrix
V
2
IR
d
j

d
h
andbias
b
v
2
IR
d
j
are
thelinearprojectionoftheencodedinput.Note
thattheprojectionsfor
h
0
and
e
0
j
couldbehigh-
rankorlow-rankdependingontheirinitialdimen-
sionsandthetargetjointspacedimension.Also
let
E
0
2
IR
jY
d
j
bethematrixresultingfrompro-
jectingalltheoutputs
e
j
tothejointspace,i.e.
g
out
(
E
)
.
Theconditionaloutputprobabilitydistribution
Figure1:Eachencodedtextandlabelareprojectedto
ajointinput-labelmultiplicativespace,theoutputof
whichisprocessedbyaunitwithlabel-
set-sizeindependentparametrization.
cannowbere-writtenas:
p
(
y
j
x
)
/
exp

E
0
h
0

/
exp

g
out
(
E
)
g
in
(
h
)

/
exp

˙
(
E
U
+
b
u
)
|
{z
}
LabelStructure
˙
(
Vh
+
b
v
)
|
{z
}
InputStructure

:
(9)
Crucially,thisfunctionhasnolabel-set-sizede-
pendentparameters,unlike
W
and
b
inEq.
3
.In
principle,thisparametrizationcanbeusedforboth
multi-classandmulti-labelproblemsby
theexponentialintermsofasoftmaxandsigmoid
functionsrespectively.However,inthispaperwe
willfocusonthelatter.
3.2Unit
Werequirethatourunitparameters
dependonlyonthejointinput-labelspaceabove.
Torepresentthecompatibilitybetweenanyen-
codedinputtext
h
i
andanyencodedlabel
e
j
for
thistask,wetheirjointrepresentationbased
onmultiplicativeinteractionsinthejointspace:
g
(
ij
)
joint
=
g
in
(
h
i
)

g
out
(
e
j
)
;
(10)
where

iscomponent-wisemultiplication.
Theprobabilityfor
h
i
tobelongtooneofthe
k
knownlabelsismodeledbyalinearunitwhich
mapsanypointinthejointspaceintoascore
whichindicatesthevalidityofthecombination:
p
(
ij
)
val
=
g
(
ij
)
joint
w
+
b;
(11)
where
w
2
IR
d
j
and
b
areascalarvariables.We
computetheoutputofthislinearunitforeach
knownlabelwhichwewouldliketopredictfor
agivendocument
i
,namely:
P
(
i
)
val
=
2
6
6
6
4
p
(
i
1)
val
p
(
i
2)
val
:::
p
(
ik
)
val
3
7
7
7
5
=
2
6
6
6
4
g
(
i
1)
joint
w
+
b
g
(
i
2)
joint
w
+
b
:::
g
(
ik
)
joint
w
+
b
3
7
7
7
5
:
(12)
Foreachrow,thehigherthevaluethemorelikely
thelabelistobeassignedtothedocument.Toob-
tainvalidprobabilityestimatesandbeabletotrain
withbinarycross-entropylossformulti-labelclas-
weapplyasigmoidfunctionasfollows:
^
y
i
=^
p
(
y
i
j
x
i
)=
1
1+
e

P
(
i
)
val
:
(13)
Summary
.Byaddingtheabovechangestothe
generalformofEq.
9
theconditionalprobabil-
ity
p
(
y
i
j
x
i
)
isnowproportionaltothefollowing
quantity:
exp

˙
(
E
U
+
b
u
)(
˙
(
Vh
+
b
v
)

w
)+
b

:
(14)
Notethatthenumberofparametersinthisequa-
tionisindependentofthesizeofthelabelset,
giventhat
U
,
V
,
w
and
b
dependonlyon
d
j
,and
k
canvaryarbitrarily.Thisallowsthemodelto
scaleuptolargelabelsetsandgeneralizetoun-
seenlabels.Lastly,theproposedoutputlayerad-
dressesallthelimitationsofthepreviousmodels,
asfollows:(i)itisabletocapturecomplexstruc-
tureinthejointinput-outputspace,(ii)itprovides
ameanstoeasilycontrolitscapacity
d
j
,and(iii)
itistrainablewithcross-entropyloss.
3.3TrainingObjectives
Thetrainingobjectiveforthemulti-label
cationtaskisbasedonbinarycross-entropyloss.
Assuming

containsalltheparametersofthe
model,thetraininglossiscomputedasfollows:
L
(

)=

1
Nk
N
X
i
=1
k
X
j
=1
H
(
y
ij
;
^
y
ij
)
;
(15)
where
H
isthebinarycross-entropybetweenthe
goldlabel
y
ij
andpredictedlabel
^
y
ij
foradocu-
ment
i
andacandidatelabel
j
.
Wehandlemultiplelanguagesaccordingto
Fi-
ratetal.
(
2016
)and
PappasandPopescu-Belis
(
2017
).Assumingthat
=
f

1
;
2
;:::;
M
g
are
alltheparametersrequiredforeachoftheMlan-
guages,weuseajointmultilingualobjectivebased
onthesumofcross-entropylosses:
L
=

1
Z
N
e
X
i
M
X
l
k
X
j
=1
H
(
y
(
l
)
ij
;
^
y
(
l
)
ij
)
;
(16)
where
Z
=
N
e
Mk
with
N
e
beingthenum-
berofexamplesperepoch.Ateachiteration,
adocument-labelpairforeachlanguageissam-
pled.Inaddition,multilingualmodelssharea
certainsubsetoftheencoderparametersduring
trainingwhiletheoutputlayerparametersarekept
asdescribedby
Pappasand
Popescu-Belis
(
2017
).Inthispaper,weshare
mostoftheoutputlayerparameters,namelythe
onesfromtheinput-labelspace(U,V,
b
v
,
b
u
),and
wekeeponlytheunitparameters(
w
,
b)
3.4ScalingUptoLargeLabelSets
Foraverylargenumber
d
j
ofjoint-spacedi-
mensionsinourparametrization,thecomputa-
tionalcomplexityincreasesprohibitivelybecause
ourprojectionrequiresalargematrixmultiplica-
tionbetween
U
and
E
,whichdependson
jYj
.In
suchcases,weresorttosampling-basedtraining,
byadoptingthecommonlyusednegativesampling
methodproposedby
Mikolovetal.
(
2013
).Let
x
i
2
IR
d
and
y
ik
2f
0
;
1
g
beaninput-labelpair
and
^
y
ik
theoutputprobabilitiesfromourmodel
(Eq.
14
).Byintroducingthesets
k
p
i
and
k
n
i
,which
containtheindicesofthepositiveandnegativela-
belsrespectivelyforthe
i
-thinput,theloss
L
(

)
in
Eq.
15
canbere-writtenasfollows:
=

1
Z
N
X
i
=1
k
X
j
=1
h
y
ij
log^
y
ij
+
y
ij
log(1

^
y
ij
)
i
=

1
Z
N
X
i
=1
h
k
p
i
X
j
=1
log^
y
ij
+
k
n
i
X
j
=1
log(1

^
y
ij
)
i
;
(17)
where
Z
=
Nk
and

y
ij
is(
1

y
ij
).Toreduce
thecomputationalcostneededtoevaluate
^
y
ij
for
allthenegativelabelset
k
n
i
,wesample
k

la-
belsfromthenegativelabelsetwithprobability
p
=
1
j
k
n
i
j
tocreatetheset
k
n
i
.Thisenablestraining
onarbitrarilybiglabelsetswithoutincreasingthe
computationrequired.Bycontrollingthenumber
ofsampleswecandrasticallyspeedupthetrain-
ingtime,aswedemonstrateempiricallyinSec-
tion
4.2.2
.Exploringmoreinformativesampling
methods,e.g.importancesampling,wouldbean
interestingdirectionoffuturework.
3.5RelationtoPreviousParametrizations
Theproposedembeddingformcanbeseenas
ageneralizationovertheinput-labelembeddings
withabilinearform,becauseitsdegenerateform
isequivalenttothebilinearformofEq.
5
.Inpar-
ticular,thiscanbesimplyderivedifwesetone
ofthetwonon-linearprojectionfunctionsinthe
secondlineofEq.
9
tobetheidentityfunction,
e.g.
g
out
(

)=
I
,setallbiasestozero,andmake
the
˙
(
:
)
activationfunctionlinear,asfollows:
˙
(
E
U
+
b
u
)
˙
(
Vh
+
b
v
)=(
E
I
)(
Vh
)
=
E
Vh

;
(18)
where
V
byconsequencehasthesamenumber
ofdimensionsas
W2
IR
d

d
h
fromthebilinear
input-labelembeddingmodelofEq.
5
.
4Experiments
Theevaluationisperformedonlarge-scale
biomedicalsemanticindexingusingtheBioASQ
dataset,obtainedby
Nametal.
(
2016
),andon
multilingualnewsusingtheDWcor-
pus,whichconsistsofeightlanguagedatasetsob-
tainedby
PappasandPopescu-Belis
(
2017
).The
statisticsofthesedatasetsarelistedinTable
1
.
4.1BiomedicalText
Weevaluateonbiomedicaltext
todemonstratethatourgeneralizedinput-label
modelscalestoverylargelabelsetsandperforms
betterthanpreviousjointinput-labelmodelson
bothseenandunseenlabelpredictionscenarios.
4.1.1Settings
Wefollowtheexactevaluationprotocol,dataand
settingsof
Nametal.
(
2016
),asdescribedbelow.
WeusetheBioASQTask3adataset,whichisa
collectionofpublicationsinbiomedical
research.Thedatasetcontainsabout12Mdocu-
mentslabeledwitharound11labelsoutof27,455,
whichareaccordingtotheMedicalSub-
jectHeadings(MESH)hierarchy.Thedatawas
minimallypre-processedwithtokenization,num-
berreplacements(NUM),rarewordreplacements
(UNK),andsplitwiththeprovidedscriptbyyear
sothatthetrainingsetincludesalldocumentsuntil
2004andtheonesfrom2005to2015werekeptfor
thetestset,thiscorrespondedto6,692,815docu-
mentsfortrainingand4,912,719fortesting.For
validation,asetof100,000documentswereran-
domlysampledfromthetrainingset.Wereport
thesameranking-basedevaluationmetricsas
Nam
etal.
(
2016
),namelyrankloss(RL),averagepre-
cision(AvgPr)andone-errorloss(OneErr).
DatasetDocumentsLabels
abbrev.#count#words

w
d
#count

w
l
BioASQ11,705,534528,15621426,10435.0
DW598,304884,2724365,6372.3
Œen112,816110,9715161,3852.1
Œde132,709261,2804241,1761.8
Œes75,827130,6614128434.7
Œpt39,47458,8495713961.8
Œuk35,423105,2403422881.7
Œru108,076123,4933309161.8
Œar57,69758,9223574352.4
Œfa36,28234,8565381982.5
Table1:Datasetstatistics:#countisthenumberof
documents,#wordsarethenumberofuniquewordsin
thevocabulary
V
,

w
d
and

w
l
aretheaveragenumberof
wordsperdocumentandlabelrespectively.
Ourhyper-parameterswereselectedonvalida-
tiondatabasedonaverageprecisionasfollows:
100-dimensionalwordembeddings,encoder,at-
tention(samedimensionsasthebaselines),joint
input-labelembeddingof500,batchsizeof64,
maximumnumberof300wordsperdocumentand
50wordsperlabel,ReLUactivation,0.3%nega-
tivelabelsampling,andoptimizationwithADAM
untilconvergence.Thewordembeddingswere
learnedend-to-endonthetask.
4
Thebaselinesarethejointinput-labelmodels
from
Nametal.
(
2016
),notedas[N16],namely:

WSABIE+
:Thismodelisanextensionof
theoriginalWSABIEmodelby
Westonetal.
(
2011
),whichinsteadoflearningaranking
modelwitheddocumentfeatures,itjointly
learnsfeaturesfordocumentsandwords,and
istrainedwiththeWARPrankingloss.

AiTextML
:Thismodelistheoneproposed
by
Nametal.
(
2016
)withthepurpose
oflearningjointlyrepresentationsofdocu-
ments,labelsandwords,alongwithajoint
input-labelspacewhichistrainedwiththe
WARPrankingloss.
ThescoresoftheWSABIE+andAiTextMLbase-
linesinTable
2
aretheonesreportedby
Nametal.
(
2016
).Inaddition,wereportscoresofaword-
levelattentionneuralnetwork(WAN)withD
ENSE
encoderandattentionfollowedbyasigmoidout-
4
Here,thewordembeddingsareincludedintheparameter
statisticsbecausetheyarevariablesofthenetwork.
ModelLayerformDimSeenlabelsUnseenlabelsParams
abbrev.output#countRLAvgPrOneErrRLAvgPrOneErr#count
[N16]
WSABIE+
EW
h
t
1005.2136.6441.7248.810.3799.94722.10M
AiTextMLavg
EW
h
t
1003.5432.7825.9952.890.3999.94724.47M
AiTextMLinf
EW
h
t
1003.5432.7825.9921.622.6698.61724.47M
Baselines
WAN
W
>
h
t
Œ1.5342.37
11.23
ŒŒŒ55.60M
BIL-WAN[YH15]
˙
(
EW
)
W
h
t
1001.2140.6817.5218.729.5093.8952.85M
BIL-WAN[N16]
EW
h
t
1001.1241.9116.9416.2610.5593.2352.84M
Ours
GILE-WAN
˙
(
E
U
)
˙
(
Vh
t
)
500
0.7844.39
11.60
9.0612.9591.90
52.93M

constrained
d
j
˙
(
EW
)
˙
(
W
h
t
)
1001.0137.7116.1610.3411.2193.3852.85M

onlylabel(Eq.
6
a)
˙
(
EW
)
h
t
1001.0640.8113.779.7714.7190.5652.84M

onlyinput(Eq.
6
b)
E
˙
(
W
h
t
)
1001.0739.7815.6719.287.1895.9152.84M
Table2:Biomedicalsemanticindexingresultscomputedoverlabelsseenandunseenduringtraining,i.e.the
full-resourceversuszero-resourcesettings.Bestscoresamongthecompetingmodelsaremarkedin
bold
.
putlayer,trainedwithbinarycross-entropyloss.
5
OurmodelreplacesWAN'soutputlayerwitha
generalizedinput-labelembeddinglayerandits
variations,notedGILE-WAN.Forcomparison,we
alsocomparetobilinearinput-labelembedding
versionsofWANforthemodelby
Yazdaniand
Henderson
(
2015
),notedasBIL-WAN[YH16],
andtheoneby
Nametal.
(
2016
),notedasBIL-
WAN[N16].NotethattheAiTextMLparameter
spaceishugeandmakeslearningdifforour
models(linearwrt.labelsanddocuments).In-
stead,wemakesurethatourmodelshavefarfewer
parametersthanthebaselines(Table
2
).
4.1.2Results
Theresultsonbiomedicalsemanticindexingon
seenandunseenlabelsareshowninTable
2
.We
observethattheneuralbaseline,WAN,outper-
formsWSABIE+andAiTextMLontheseenla-
bels,namelyby
+
5.73and
+
9.59pointsinterms
ofAvgPrrespectively.Thedifferencesareeven
morepronouncedwhenconsideringtheranking
lossandoneerrormetrics.Thisresultiscompati-
blewithpreviousthatexistingjointinput-
labelmodelsarenotabletooutperformstrongsu-
pervisedbaselinesonseenlabels.However,WAN
isnotabletogeneralizeatalltounseenlabels,
hencetheWSABIE+andAiTextMLhaveaclear
advantageinthezero-resourcesetting.
Incontrast,ourgeneralizedinput-labelmodel,
5
Inourpreliminaryexperiments,wealsotrainedtheneu-
ralmodelwithahingelossasWSABIE+andAiTextML,but
itperformedsimilarlytothemandmuchworsethanWAN,
sowedidnotfurtherexperimentwithit.
GILE-WAN,outperformsWANevenonseenla-
bels,whereourmodelhashigheraveragepreci-
sionby
+
2.02points,betterrankinglossby
+
43%
andcomparableOneErr(

3%).Andthisgainis
notattheexpenseofperformanceonunseenla-
bels.GILE-WAN,outperformsWSABIE+,Ai-
TextMLvariants
6
byalargemargininbothcases,
e.g.by
+
7.75,
+
11.61pointsonseenlabelsandby
+
12.58,
+
10.29pointsintermsofaveragepreci-
siononunseenlabels,respectively.Interestingly,
ourGILE-WANmodelalsooutperformsthetwo
previousbilinearinput-labelembeddingformula-
tionsof
YazdaniandHenderson
(
2015
)and
Nam
etal.
(
2016
),namelyBIL-WAN[YH15]andBIL-
WAN[N16]by
+
3.71,
+
2.48pointsonseenla-
belsand
+
3.45and
+
2.39pointsonunseenla-
bels,respectively,evenwhentheyaretrainedwith
thesameencodersandlossasours.Thesemod-
elsarenotabletooutperformtheWANbaseline
whenevaluatedontheseenlabels,namelythey
have

1.68and

0.46pointsloweraveragepreci-
sionthanWAN,buttheyoutperformWSABIE+
andAiTextMLonbothseenandunseenlabels.
Overall,theresultsshowaclearadvantageofour
generalizedinput-labelembeddingmodelagainst
previousmodelsonbothseenandunseenlabels.
4.1.3AblationAnalysis
Toevaluatetheeffectivenessofindividualcom-
ponentsofourmodel,weperformedanablation
study(lastthreerowsinTable
2
).Notethatwhen
weuseonlythelabeloronlytheinputembedding
6
Namely,
avg
whenusingtheaverageofwordvectorsand
inf
whenusinginferredlabelvectorstomakepredictions.
inourgeneralizedinput-labelformulation,thedi-
mensionalityofthejointspaceisconstrainedto
bethedimensionalityoftheencodedlabelsand
inputsrespectively,thatis
d
j
=
100
inourexperi-
ments.
Allthreevariantsofourmodeloutperform
previousembeddingformulationsof
Nametal.
(
2016
)and
YazdaniandHenderson
(
2015
)inall
metricsexceptfromAvgPronseenlabelswhere
theyscoreslightlylower.ThedecreaseinAvgPrec
forourmodelvariantswith
d
j
=
100
comparedto
theneuralbaselinescouldbeattributedtothedif-
inlearningtheparametersofahighlynon-
linearspacewithonlyafewhiddendimensions.
Indeed,whenweincreasethenumberofdimen-
sions(
d
j
=
500
),ourfullmodeloutperformsthem
byalargemargin.Recallthatthisincreaseinca-
pacityisonlypossiblewithourfullmodel
tioninEq.9andnoneoftheothervariantsallow
ustodothiswithoutinterferingwiththeoriginal
dimensionalityoftheencodedlabels(
E
)andinput
(
h
t
).Inaddition,ourmodelvariantswith
d
j
=
100
exhibitconsistentlyhigherscoresthanbaselinesin
termsofmostmetricsonbothseenandunseenla-
bels,whichsuggeststhattheyareabletocapture
morecomplexrelationshipsacrosslabelsandbe-
tweenencodedinputsandlabels.
Overall,thebestperformanceamongourmodel
variantsisachievedwhenusingonlythelabelem-
beddingand,hence,itisthemostcom-
ponentofourmodel.Surprisingly,ourmodelwith
onlythelabelembeddingachieveshigherperfor-
mancethanourfullmodelonunseenlabelsbutit
isfarbehindourfullmodelwhenweconsiderper-
formanceonbothseenandunseenlabels.When
weconstrainourfullmodeltohavethesamedi-
mensionalitywiththeothervariants,i.e.
d
j
=
100
,
itoutperformstheonethatusesonlytheinputem-
beddinginmostmetricsanditisoutperformedby
theonethatusesonlythelabelembedding.
4.2MultilingualNewsText
Weevaluateonmultilingualnewstext
tiontodemonstratethatouroutputlayerbased
onthegeneralizedinput-labelembeddingoutper-
formspreviousmodelswithatypicaloutputlayer
inawidevarietyofsettings,evenforlabelswhich
havebeenseenduringtraining.
4.2.1Settings
Wefollowtheexactevaluationprotocol,dataand
settingsof
PappasandPopescu-Belis
(
2017
),as
describedbelow.Thedatasetissplitperlanguage
into80%fortraining,10%forvalidationand10%
fortesting.Weevaluateonbothtypesoflabels
(general
Y
g
,and
Y
s
)ina
full-resource
scenario
,andweevaluateonlyonthegeneralla-
bels(
Y
g
)ina
low-resourcescenario
.Accuracyis
measuredwiththemicro-averagedF1percentage
scores.
Thewordembeddingsforthistaskarethe
alignedpre-trained40-dimensionalmulti-CCA
multilingualwordembeddingsby
Ammaretal.
(
2016
)andarekeptedduringtraining.
7
The
sentencesarealreadytruncatedatalengthof30
wordsandthedocumentsatalengthof30sen-
tences.Thehyper-parameterswereselectedon
validationdataasfollows:100-dimensionalen-
coderandattention,
ReLU
activation,batchsize
of16,epochsizeof25k,nonegativesampling
(alllabelsareused)andoptimizationwithADAM
untilconvergence.Toensureequalcapacityto
baselines,weuseapproximatelythesamenumber
ofparameters
n
tot
withthebaseline
layers,bysetting:
d
j
'
d
h
j
k
(
i
)
j
d
h
+
d
;i
=1
;:::;M;
(19)
inthemonolingualcase,andsimilarly,
d
j
'
(
d
h

P
M
i
=1
j
k
(
i
)
j
)
=
(
d
h
+
d
)
inthemultilingual
case,where
k
(
i
)
isthenumberoflabelsinlan-
guage
i
.
Thehierarchicalmodelshave
Dense
encoders
inallscenarios(Tables
3
,
6
,and
7
),exceptfrom
thevaryingencoderexperiment(Table
4
).For
thelow-resourcescenario,thelevelsofdataavail-
abilityare:
tiny
from0.1%to0.5%,
small
from
1%to5%and
medium
from10%to50%ofthe
originaltrainingset.Foreachlevel,theaver-
ageF1acrossdiscreteincrementsof0.1,1and
10arereportedrespectively.Thedecisionthresh-
olds,whichweretunedonvalidationdataby
Pap-
pasandPopescu-Belis
(
2017
),aresetasfollows:
forthefull-resourcescenarioitissetto0.4for
j
Y
s
j
<
400
and0.2for
j
Y
s
j
400
,andforthe
low-resourcescenarioitissetto0.3forallsets.
Thebaselinesareallthemonolingualandmulti-
lingualneuralnetworksfrom
PappasandPopescu-
Belis
(
2017
)
8
,notedas[PB17],namely:
7
Thewordembeddingsarenotincludedintheparameters
statisticsbecausetheyarenotvariablesofthenetwork.
8
Forreference,inTable
4
wealsocomparetoalogistic
regressiontrainedwithunigramsoverthefullvocabularyand
Models
Languages(en+aux
!
en)
Languages(en+aux
!
aux)
Stat.
Y
g
abbrev.
de
es
pt
uk
ru
ar
fa
de
es
pt
uk
ru
ar
fa
avg
[PB17]
Mono
NN(Avg)50.7..................................53.170.057.280.959.364.466.657.6
HNN(Avg)70.0..................................67.982.570.586.877.479.076.673.6
HAN(Att)71.2..................................71.882.871.385.379.880.576.674.7
Multi
MHAN-Enc71.069.969.2
70.8
71.570.071.369.782.969.786.880.379.076.074.1
MHAN-Att74.074.274.172.973.973.873.372.582.570.887.780.582.176.376.3
MHAN-Both72.871.270.565.671.168.969.270.482.871.687.580.879.177.174.2
Ours
Mono
GILE-NN(Avg)
60.1
..................................
60.376.662.182.065.777.468.665.2
GILE-HNN(Avg)
74.8
..................................
71.383.372.688.381.581.977.177.1
GILE-HAN(Att)
76.5
..................................
74.283.471.986.182.781.077.278.0
Multi
GILE-MHAN-Enc
75.174.072.7
70.7
74.473.573.272.783.473.088.782.883.377.476.7
GILE-MHAN-Att
76.5
76.5
76.3
75.3
76.1
75.6
75.2
74.5
83.5
72.7
88.0
83.4
82.1
76.778.0
GILE-MHAN-Both
75.373.772.167.272.573.869.772.6
84.0
73.5
89.0
81.982.0
77.7
76.0
Y
s
Models
de
es
pt
uk
ru
ar
fa
de
es
pt
uk
ru
ar
fa
avg
[PB17]
Mono
NN(Avg)24.4..................................21.822.124.333.026.024.132.125.3
HNN(Avg)39.3..................................39.637.933.642.239.334.643.138.9
HAN(Att)43.4..................................44.846.341.946.445.841.249.444.2
Multi
MHAN-Enc45.445.944.341.142.1
44.9
41.043.946.239.347.445.037.948.643.8
MHAN-Att46.346.0
45.9
45.646.446.4
46.1
46.546.743.347.945.841.348.045.8
MHAN-Both45.745.641.541.245.6
44.643.0
45.946.440.346.3
46.1
40.7
50.3
44.5
Ours
Mono
GILE-NN(Avg)
27.5
..................................
27.528.429.236.831.632.135.629.5
GILE-HNN(Avg)
43.1
..................................
43.442.037.743.042.936.644.142.2
GILE-HAN(Att)
45.9
..................................
47.347.442.646.646.941.9
48.6
45.9
Multi
GILE-MHAN-Enc
46.046.6
41.2
42.546.4
43.4
41.847.247.741.549.546.641.450.745.1
GILE-MHAN-Att
47.347.045.845.546.246.5
45.5
47.647.943.549.146.542.250.346.5
GILE-MHAN-Both
47.046.742.842.045.6
42.839.3
48.047.643.148.5
46.0
42.1
49.0
45.0
Table3:Full-resourceresultsongeneral(upperhalf)and(lowerhalf)labelsusingmono-
lingualandbilingualmodelswithD
ENSE
encodersonEnglishastarget(left)andtheauxiliarylanguageastarget
(right).TheaveragebilingualF1-score(%)isnoted
avg
andthetoponesperblockareunderlined
.Themonolin-
gualscoresontheleftcomefromasinglemodel,henceasinglescoreisrepeatedmultipletimes;therepetitionis
markedwithconsecutivedots.

NN
:Aneuralnetworkwhichfeedstheav-
eragevectoroftheinputwordsdirectlytoa
layer,astheoneusedby
Kle-
mentievetal.
(
2012
).

HNN
:Ahierarchicalnetworkwithencoders
andaveragepoolingateverylevel,followed
byalayer,astheoneusedby
Tangetal.
(
2015
).

HAN
:Ahierarchicalnetworkwithencoders
andattention,followedbya
layer,astheoneusedby
Yangetal.
(
2016
).

MHAN
:Threemultilingualhierarchicalnet-
workswithsharedencoders,notedMHAN-
Enc,sharedattention,notedMHAN-Att,
andsharedattentionandencoders,noted
MHAN-Both,astheonesusedby
Pappasand
Popescu-Belis
(
2017
).
Toensureacontrolledcomparisontotheabove
baselines,foreachmodelweevaluateaver-
sionwheretheiroutputlayerisreplacedbyour
generalizedinput-labelembeddingoutputlayer
overthetop-10%mostfrequentwordsby
Mrinietal.
(
2017
),
notedas[M17],whichusethesamesettingsanddata.
usingthesamenumberofparameters;these
havetheabbreviationﬁGILEﬂprependedintheir
name(e.g.GILE-HAN).ThescoresofHANand
MHANmodelsinTables
3
,
6
and
7
aretheonesre-
portedby
PappasandPopescu-Belis
(
2017
),while
forTable
4
wetrainthemourselvesusingtheir
code.Lastly,thebestscoreforeachpairwisecom-
parisonbetweenajointinput-labelmodelandits
counterpartismarkedin
bold
.
4.2.2Results
Table
3
displaystheresultsoffull-resourcedocu-
mentusingD
ENSE
encodersforboth
generalandlabels.Ontheleft,wedisplay
theperformanceofmodelsontheEnglishsub-
corpuswhenEnglishandanauxiliarylanguage
areusedfortraining,andontheright,theperfor-
manceontheauxiliarylanguagesub-corpuswhen
thatlanguageandEnglishareusedfortraining.
Theresultsshowthatin98%ofcomparisonson
generallabels(tophalfofTable
3
)thejointinput-
labelmodelsimproveconsistentlyoverthecor-
respondingmodelsusingatypicalsigmoidclas-
layer.Thisvalidatesourmain
hypothesisthatthejointinput-labelmodelssuc-
ModelsLanguagesStatistics
abbrev.endeesptukruarfa
n
l
f
l
[M17]
LogReg-
BOW
75.872.981.474.3
91.0
79.282.077.026M79.19
LogReg-
BOW
-10%74.770.180.671.1
89.5
76.580.875.55M77.35
[PB17]
HAN-
BIGRU
76.3
74.1
84.5
72.9
87.7
82.9
81.775.3377K
79.42
HAN-
GRU
77.1
72.584.070.886.683.082.976.0138K79.11
HAN-D
ENSE
71.271.882.871.385.379.880.576.650K77.41
Ours
GILE-HAN-
BIGRU
78.1
73.6
84.9
72.5
89.0
82.4
82.5
75.8
377K
79.85
GILE-HAN-
GRU
77.1
72.684.772.488.683.683.476.0
138K
79.80
GILE-HAN-D
ENSE
76.5
74.2
83.471.986.182.782.677.2
50K
79.12
Table4:Full-resourceresultsongeneral(
Y
g
)topiclabelswithD
ENSE
and
GRU
encoders.Reported
arealsotheaveragenumberofparametersperlanguage(
n
l
),andtheaverage
F
1
perlanguage(
f
l
).
cessfullyexploitthesemanticsofthelabels,which
provideusefulcuesforclasopposed
tomodelswhichareagnostictolabelsemantics.
Theresultsforlabels(bottomhalfofTa-
ble
3
)demonstratethesametrend,withthejoint
input-labelmodelsperformingbetterin87%of
comparisons.
InTable
5
,wealsodirectlycompareourem-
beddingtopreviousbilinearinput-labelembed-
dingformulationswhenusingthebestmonolin-
gual(HAN)fromTable
3
,exactly
asdoneinSection
4.1
.Theresultsonthegeneral
labelsshowthatGILEoutperformstheprevious
bilinearinput-labelmodels,BIL[YH15]andBIL
[N16],by+1.62and+3.3percentagepointsonav-
eragerespectively.Thisdifferenceismuchmore
pronouncedonthelabels,wherethelabel
setismuchlarger,namely+6.5and+13.5percent-
agepointsrespectively.Similarly,ourmodelwith
constraineddimensionalityisalsoasgoodorbet-
teronaveragethanthebilinearinput-labelmodels,
by+0.9and+2.2ongenerallabelsandby-0.5and
+6.1onlabelsrespectively,whichhigh-
lightstheimportanceoflearningnon-linearrela-
tionshipsacrossencodedlabelsanddocuments.
Amongourablatedmodelvariants,asinprevious
section,thebestistheonewithonlythelabelpro-
jectionbutitstillworsethanourfullmodelby-
5.2percentagepoints.TheimprovementsofGILE
againsteachbaselineisandconsistent
onbothdatasets.Hence,inthefollowingexperi-
mentswewillonlyconsiderthebestoftheseal-
ternatives.
Thebestbilingualperformanceonaverageis
thatoftheGILE-MHAN-Attmodel,forbothgen-
eralandlabels.Thisimprovementcan
beattributedtotheeffectivesharingbetweenla-
HANLanguages
Y
g
outputlayerendeesptukruarfa
Linear[PB17]71.271.882.871.385.379.880.576.6
BIL[YH15]71.770.582.071.186.680.680.476.0
BIL[N16]69.869.180.967.4
87.5
79.978.475.1
GILE(Ours)
76.5
74.2
83.471.9
86.1
82.782.677.2
-constrained
d
j
73.673.183.371.087.181.680.476.4
-onlylabel71.469.682.170.386.280.681.176.2
-onlyinput55.154.280.666.585.660.878.974.0
Y
s
outputlayerendeesptukruarfa
Linear[PB17]43.444.846.341.946.445.841.2
49.4
BIL[YH15]40.737.838.133.544.638.139.142.6
BIL[N16]34.430.234.433.631.422.835.638.9
GILE(Ours)
45.9
47.3
47.442.646.646.941.9
48.6
-constrained
d
j
38.538.036.835.142.136.136.748.7
-onlylabel38.441.542.938.344.039.337.243.4
-onlyinput12.110.88.820.511.87.812.024.6
Table5:Directcomparisonwithpreviousbilin-
earinput-labelmodels,namelyBIL[YH15]andBIL
[N16],andwithourablatedmodelvariantsusingthe
bestmonolingual(HAN)fromTable
3
onbothgeneral(upperhalf)and(lowerhalf)
labels.Bestscoresamongthecompetingmodelsare
markedin
bold
.
belsemanticsacrosslanguagesthroughthejoint
multilingualinput-labeloutputlayer.Effectively,
thismodelhasthesamemultilingualsharing
schemewiththebestmodelreportedby
Pappas
andPopescu-Belis
(
2017
),MHAN-Att,namely
sharingattentionateachlevelofthehierarchy,
whichagreeswellwiththeirmainIn-
terestingly,theimprovementholdswhenusing
differenttypesofhierarchicalencoders,namely
ModelsGenerallabelslabels
abbrev.
#lang.
n
l
f
l
n
l
f
l
[PB17]
HAN150K77.4190K44.90
MHAN240K78.3080K45.72
MHAN832K77.9172K45.82
Ours
GILE-HAN150K
79.12
90K
45.90
GILE-MHAN240K
79.68
80K
46.49
GILE-MHAN832K
79.48
72K
46.32
Table6:Multilinguallearningresults.Thecolumns
aretheaveragenumberofparametersperlanguage
(
n
l
),average
F
1
perlanguage(
f
l
).
D
ENSE
GRU,andbiGRU,asshowninTable
4
,
whichdemonstratethegeneralityoftheapproach.
Inaddition,ourbestmodelsoutperformlogis-
ticregressiontrainedeitherontop-10%mostfre-
quentwordsoronthefullvocabulary,eventhough
ourmodelsutilizemanyfewerparameters,namely
377K/138Kvs.26M/5M.Increasingthecapacity
ofourmodelsshouldleadtoevenfurtherimprove-
ments.
Multilinguallearning.
Sofar,wehaveshown
thattheproposedjointinput-labelmodelsoutper-
formtypicalneuralmodelswhentrainingwithone
andtwolanguages.Doestheimprovementremain
whenincreasingthenumberoflanguageseven
more?ToanswerthequestionwereportinTable
6
theaverageF1-scoreperlanguageforthebest
baselinesfromthepreviousexperiment(HANand
MHAN-Att)withtheproposedjointinput-label
versionsofthem(GILE-HANandGILE-MHAN-
Att)whenincreasingthenumberoflanguages(1,
2and8)thatareusedfortraining.Overall,weob-
servethatthejointinput-labelmodelsoutperform
allthebaselinesindependentlyofthenumberof
languagesinvolvedinthetraining,whilehaving
thesamenumberofparameters.Wealsoreplicate
thepreviousresultthatasecondlanguagehelps
butbeyondthatthereisnoimprovement.
Low-resourcetransfer.
Weinvestigatehere
whetherjointinput-labelmodelsareusefulfor
low-resourcelanguages.Table
7
showsthelow-
resourceresultsfromEnglishto
sevenotherlanguageswhenvaryingtheamountof
theirtrainingdata.Ourmodelwithbothshareden-
codersandattention,GILE-MHAN,outperforms
previousmodelsinaverage,namelyHAN(
Yang
etal.
,
2016
)andMHAN(
PappasandPopescu-
Belis
,
2017
),forlow-resourceinthe
majorityofthecases.
Levels[PB17]Ours
range
HAN
MHAN
GILE-MHAN
en
!
de
0.1-0.5%29.939.4
42.9
1-5%51.3
52.6
51.6
10-50%63.563.8
65.9
en
!
es
0.1-0.5%39.5
41.5
39.0
1-5%45.650.1
50.9
10-50%74.275.2
76.4
en
!
pt
0.1-0.5%30.933.8
39.6
1-5%44.647.3
48.9
10-50%60.962.1
62.3
en
!
uk
0.1-0.5%60.460.9
61.1
1-5%68.269.0
69.4
10-50%76.4
76.7
76.5
en
!
ru
0.1-0.5%27.6
29.1
27.9
1-5%39.3
40.2
40.2
10-50%69.269.4
70.4
en
!
ar
0.1-0.5%35.436.6
46.1
1-5%45.646.6
49.5
10-50%48.947.8
61.8
en
!
fa
0.1-0.5%36.041.3
42.5
1-5%55.0
55.5
55.4
10-50%69.2
70.0
69.7
Table7:Low-resourceresultswithvari-
oussizesoftrainingdatausingthegenerallabels.
Thesharedinput-labelspaceappearstobehelp-
fulespeciallywhentransferringfromEnglishto
German,PortugueseandArabiclanguages.GILE-
MHANisbehindMHANontrans-
ferringknowledgefromEnglishtoSpanishandto
Russianinthe0.1-0.5%resourcesetting,butinthe
restofthecasestheyhaveverysimilarscores.
Labelsampling.
Tospeedupcomputationitis
possibletotrainourmodelbysamplinglabels,in-
steadoftrainingoverthewholelabelset.How
muchspeed-upcanweachievefromthislabel
samplingapproachandstillretaingoodlevelsof
performance?InFigure
2
,weattempttoanswer
thisquestionbyreportingtheperformanceofour
GILE-HNNmodelwhenvaryingtheamountof
labels(%)thatitusesfortrainingoverEnglish
generalandlabelsoftheDWdataset.In
bothcases,theperformanceofGILE-HNNtends
toincreaseasthepercentageoflabelssampledin-
creases,butitlevelsoffforthehigherpercentages.
Forgenerallabels,topperformanceisreached
witha40%to50%samplingrate,whichtranslates
toa22%to18%speedup,whileforthe
labels,itisreachedwitha60%to70%sampling
rate,whichtranslatestoa40%to36%speedup.
Figure2:Varyingsamplingpercentageforgeneraland
Englishlabels.
(Top)
GILE-HNNiscompared
againstHNNintermsofF1(%).
(Bottom)
Theruntime
speedupoverGILE-HNNtrainedonthefulllabelset.
Thespeedupiscorrelatedtothesizeofthelabel
set,sincetherearemanyfewergenerallabelsthan
labels,namely327vs1,058here.Hence,
weexpectevenhigherspeedupsforbiggerlabel
sets.Interestingly,GILE-HNNwithlabelsam-
plingreachestheperformanceofthebaselinewith
a25%and60%sampleforgeneraland
labelsrespectively.Thistranslatestoaspeedupof
30%and50%respectivelycomparedtoaGILE-
HNNtrainedoveralllabels.Overall,theseresults
showthatourmodeliseffectiveandthatitcanalso
scaletolargelabelsets.Thelabelsamplingshould
alsobeusefulintaskswherethecomputationre-
sourcesmaybelimitedorbudgeted.
5RelatedWork
5.1Neuraltext
Researchin
neuraltext
wasinitially
basedonfeed-forwardnetworks,whichrequired
unsupervisedpre-training(
Collobertetal.
,
2011
;
Mikolovetal.
,
2013
;
LeandMikolov
,
2014
)and
laterontheyfocusedonnetworkswithhierarchi-
calstructure.
Kim
(
2014
)proposedaconvolu-
tionalneuralnetwork(CNN)forsentenceclas-

JohnsonandZhang
(
2015
)proposed
aCNNforhigh-dimensionaldata
while
Zhangetal.
(
2015
)adoptedacharacter-level
CNNfortext
Laietal.
(
2015
)pro-
posedarecurrentCNNtocapturesequentialinfor-
mation,whichoutperformedsimplerCNNs.
Lin
etal.
(
2015
)and
Tangetal.
(
2015
)proposedhi-
erarchicalrecurrentneuralnetworksandshowed
thattheyweresuperiortoCNN-basedmodels.
Yangetal.
(
2016
)demonstratedthatahierarchi-
calattentionnetworkwithbi-directionalgateden-
codersoutperformspreviousalternatives.
Pappas
andPopescu-Belis
(
2017
)adaptedsuchnetworks
tolearnhierarchicaldocumentstructureswith
sharedcomponentsacrossdifferentlanguages.
Theissueofscalingtolargelabelsetshas
beenaddressedpreviouslybyoutputlayerapprox-
imations(
MorinandBengio
,
2005
)andwiththe
useofsub-wordunitsorcharacter-levelmodeling
(
Sennrichetal.
,
2016
;
Leeetal.
,
2017
)whichis
mainlyapplicabletostructuredpredictionprob-
lems.Despitethenumerousstudies,mostofthe
existingneuraltexttionmodelsignore
labeldescriptionsandsemantics.Moreover,they
arebasedontypicaloutputlayerparametrizations
whicharedependentonthelabelsetsize,and
thusarenotabletoscalewelltolargelabelsets
nortogeneralizetounseenlabels.Ouroutput
layerparametrizationaddressestheselimitations
andcouldpotentiallyimprovesuchmodels.
5.2OutputRepresentationLearning
Thereexiststudieswhichaimtolearnoutputrep-
resentationsdirectlyfromdatawithoutanyse-
manticgroundingtowordembeddings(
Srikumar
andManning
,
2014
;
Yehetal.
,
2018
;
Augen-
steinetal.
,
2018
).Suchmethodshavealabel-
set-sizedependentparametrization,whichmakes
themdatahungry,lessscalableonlargelabelsets
andincapableofgeneralizingtounseenclasses.
Wangetal.
(
2018
)addressedthelackofseman-
ticgroundingtowordembeddingsbyproposing
anefmethodbasedonlabel-attentivetext
representationswhicharehelpfulfortextclassi-
However,incontrasttoourstudy,their
parametrizationisstilllabel-set-sizedependent
andthustheirmodelisnotabletoscalewellto
largelabelsetsnortogeneralizetounseenlabels.
5.3Zero-shotText
Severalstudieshavefocusedonlearningjoint
input-labelrepresentationsgroundedtowordse-
manticsforunseenlabelpredictionforimages
(
Westonetal.
,
2011
;
Socheretal.
,
2013
;
Norouzi
etal.
,
2014
;
Zhangetal.
,
2016
;
Fuetal.
,
2018
),
calledzero-shotHowever,thereare
fewersuchstudiesfortext
Dauphin
etal.
(
2014
)predictedsemanticutterancesoftext
bymappingtheminthesamesemanticspacewith
theclasslabelsusinganunsupervisedlearningob-
jective.
YazdaniandHenderson
(
2015
)proposed
azero-shotspokenlanguageunderstandingmodel
basedonabilinearinput-labelmodelabletogen-
eralizetopreviouslyunseenlabels.
Nametal.
(
2016
),proposedabilinearjointdocument-label
embeddingwhichlearnssharedwordrepresenta-
tionsbetweendocumentsandlabels.Morere-
cently,
Shuetal.
(
2017
)proposedanapproachfor
open-worldwhichaimstoidentify
noveldocumentsduringtestingbutitisnotable
togeneralizetounseenclasses.Perhaps,themost
similarmodeltooursisfromtherecentstudyby
Pappasetal.
(
2018
)onneuralmachinetranslation,
withthedifferencethattheyhavesingle-wordla-
beldescriptionsandtheyusealabel-set-dependent
biasinasoftmaxlinearpredictionunit,whichis
designedforstructuredprediction.Hence,their
modelcanneitherhandleunseenlabelsnormulti-
labelaswedohere.
Comparedtopreviousjointinput-labelmodels,
theproposedmodelhasamoregeneralandxi-
bleparametrizationwhichallowstheoutputlayer
capacitytobecontrolled.Moreover,itisnotre-
strictedtolinearmappings,whichhavelimited
expressivity,butusesnonlinearmappings,similar
toenergy-basedlearningnetworks(
LeCunetal.
,
2006
;
BelangerandMcCallum
,
2016
).Thelinkto
thelattercanbemadeifweregard
P
(
ij
)
val
inEq.
11
asanenergyfunctionforthe
i
-thdocumentandthe
j
-thlabel,thecalculationofwhichusesasimple
multiplicativetransformation(Eq.
10
).Lastly,the
proposedmodelperformswellonbothseenand
unseenlabelsetsbyleveragingthebinarycross-
entropyloss,whichisthestandardlossforclassi-
problems,insteadofarankingloss.
6Conclusion
Weproposedanoveljointinput-labelembedding
modelforneuraltextwhichgener-
alizesoverexistinginput-labelmodelsandad-
dressestheirlimitationswhilepreservinghighper-
formanceonbothseenandunseenlabels.Com-
paredtobaselineneuralmodelswithatypicalout-
putlayer,ourmodelismorescalableandhasbet-
terperformanceontheseenlabels.Comparedto
previousjointinput-labelmodels,itperformssig-
betteronunseenlabelswithoutcompro-
misingperformanceontheseenlabels.Theseim-
provementscanbeattributedtothetheabilityof
ourmodeltocapturecomplexinput-labelrelation-
ships,toitscontrollablecapacityandtoitstraining
objectivewhichisbasedoncross-entropyloss.
Asfuturework,thelabelrepresentationcould
belearnedbyamoresophisticatedencoder,and
thelabelsamplingcouldfromimportance
samplingtoavoidrevisitinguninformativelabels.
Anotherinterestingdirectionwouldbetoa
morescalablewayofincreasingtheoutputlayer
capacity,forinstanceusingadeepratherthan
widenetwork.Moreover,adapting
theproposedmodeltostructuredprediction,for
instancebyusingasoftmaxunitin-
steadofasigmoidone,wouldtaskssuch
asneuralmachinetranslation,languagemodel-
ingandsummarization,inisolationbutalsowhen
trainedjointlywithmulti-tasklearning.
Acknowledgments
WearegratefulforthesupportfromtheEuro-
peanUnionthroughitsHorizon2020program
intheSUMMAprojectn.688139,see
http:
//www.summa-project.eu
.Wewouldalso
liketothankouractioneditor,EnekoAgirre,and
theanonymousreviewersfortheirinvaluablesug-
gestionsandfeedback.
References
WaleedAmmar,GeorgeMulcaire,YuliaTsvetkov,
GuillaumeLample,ChrisDyer,andNoahA.
Smith.2016.
Massivelymultilingualwordem-
beddings
.
CoRR
,abs/1602.01925.v2.
IsabelleAugenstein,SebastianRuder,andAn-
dersSøgaard.2018.
Multi-tasklearningof
pairwisesequencetasksoverdis-
paratelabelspaces
.In
Proceedingsofthe
2018ConferenceoftheNorthAmericanChap-
teroftheAssociationforComputationalLin-
guistics:HumanLanguageTechnologies,Vol-
ume1(LongPapers)
,pages1896Œ1906,New
Orleans,Louisiana.
DavidBelangerandAndrewMcCallum.2016.
Structuredpredictionenergynetworks
.In
Pro-
ceedingsofThe33rdInternationalConference
onMachineLearning
,volume48of
Proceed-
ingsofMachineLearningResearch
,pages983Œ
992,NewYork,NewYork,USA.PMLR.
JianshuChen,JiHe,YelongShen,LinXiao,Xi-
aodongHe,JianfengGao,XinyingSong,and
LiDeng.2015.
End-to-endlearningofLDA
bymirror-descentbackpropagationoveradeep
architecture
.In
AdvancesinNeuralInforma-
tionProcessingSystems28
,pages1765Œ1773,
Montreal,Canada.
KyunghyunCho,BartvanMerrienboer,Caglar
Gulcehre,DzmitryBahdanau,FethiBougares,
HolgerSchwenk,andYoshuaBengio.2014.
LearningphraserepresentationsusingRNN
encoderŒdecoderforstatisticalmachinetransla-
tion
.In
Proceedingsofthe2014Conferenceon
EmpiricalMethodsinNaturalLanguagePro-
cessing
,pages1724Œ1734,Doha,Qatar.
RonanCollobert,JasonWeston,LéonBottou,
MichaelKarlen,KorayKavukcuoglu,andPavel
Kuksa.2011.
Naturallanguageprocessing(al-
most)fromscratch
.
JournalofMachineLearn-
ingResearch
,12:2493Œ2537.
YannN.Dauphin,GökhanTür,DilekHakkani-
Tür,andLarryP.Heck.2014.
Zero-shotlearn-
ingandclusteringforsemanticutteranceclassi-

.In
InternationalConferenceonLearn-
ingRepresentations
,Banff,Canada.
OrhanFirat,BaskaranSankaran,YaserAl-
Onaizan,FatosT.YarmanVural,and
KyunghyunCho.2016.
Zero-resource
translationwithmulti-lingualneuralmachine
translation
.In
Proceedingsofthe2016Con-
ferenceonEmpiricalMethodsinNatural
LanguageProcessing
,pages268Œ277,Austin,
USA.
AndreaFrome,GregS.Corrado,JonShlens,
SamyBengio,JeffDean,MarcAurelioRan-
zato,andTomasMikolov.2013.
DeViSE:A
deepvisual-semanticembeddingmodel
.In
C.J.C.Burges,L.Bottou,M.Welling,
Z.Ghahramani,andK.Q.Weinberger,edi-
tors,
AdvancesinNeuralInformationProcess-
ingSystems26
,pages2121Œ2129.CurranAs-
sociates,Inc.
YanweiFu,TaoXiang,Yu-GangJiang,Xi-
angyangXue,LeonidSigal,andShaogang
Gong.2018.
Recentadvancesinzero-shot
recognition:Towarddata-efunderstand-
ingofvisualcontent
.
IEEESignalProcessing
Magazine
,35(1):112Œ125.
RieJohnsonandTongZhang.2015.
Effective
useofwordorderfortextcategorizationwith
convolutionalneuralnetworks
.In
Proceedings
ofthe2015ConferenceoftheNorthAmeri-
canChapteroftheAssociationforComputa-
tionalLinguistics:HumanLanguageTechnolo-
gies
,pages103Œ112,Denver,Colorado.
YoonKim.2014.
Convolutionalneuralnetworks
forsentence
.In
Proceedingsof
the2014ConferenceonEmpiricalMethods
inNaturalLanguageProcessing
,pages1746Œ
1751,Doha,Qatar.
AlexandreKlementiev,IvanTitov,andBinod
Bhattarai.2012.
Inducingcrosslingualdis-
tributedrepresentationsofwords
.In
Pro-
ceedingsofCOLING2012
,pages1459Œ1474,
Mumbai,India.
AnkitKumar,OzanIrsoy,JonathanSu,James
Bradbury,RobertEnglish,BrianPierce,Pe-
terOndruska,IshaanGulrajani,andRichard
Socher.2015.
Askmeanything:Dynamic
memorynetworksfornaturallanguageprocess-
ing
.In
ProceedingsofThe33rdInternational
ConferenceonMachineLearning
,pages334Œ
343,NewYorkCity,USA.
SiweiLai,LihengXu,KangLiu,andJunZhao.
2015.
Recurrentconvolutionalneuralnetworks
fortext
.In
Proceedingsofthe
29thAAAIConferenceonIntelligence
,
pages2267Œ2273,Austin,USA.
QuocV.LeandTomasMikolov.2014.
Distributed
representationsofsentencesanddocuments
.In
ProceedingsofThe31stInternationalConfer-
enceonMachineLearning
,pages1188â

A¸SŒ
1196,Beijing,China.
YannLeCun,SumitChopra,RaiaHadsell,FuJie
Huang,andetal.2006.
Atutorialonenergy-
basedlearning
.In
PredictingStructuredData
.
MITPress.
JasonLee,KyunghyunCho,andThomasHof-
mann.2017.
Fullycharacter-levelneuralma-
chinetranslationwithoutexplicitsegmentation
.
TransactionsoftheAssociationforComputa-
tionalLinguistics
,5:365Œ378.
RuiLin,ShujieLiu,MuyunYang,MuLi,Ming
Zhou,andShengLi.2015.
Hierarchicalrecur-
rentneuralnetworkfordocumentmodeling
.In
Proceedingsofthe2015ConferenceonEmpir-
icalMethodsinNaturalLanguageProcessing
,
pages899Œ907,Lisbon,Portugal.
ThangLuong,HieuPham,andChristopherD.
Manning.2015.
Effectiveapproachesto
attention-basedneuralmachinetranslation
.In
Proceedingsofthe2015ConferenceonEmpir-
icalMethodsinNaturalLanguageProcessing
,
pages1412Œ1421,Lisbon,Portugal.
ThomasMensink,JakobVerbeek,FlorentPer-
ronnin,andGabrielaCsurka.2012.Metric
learningforlargescaleimage
Generalizingtonewclassesatnear-zerocost.
In
ComputerVisionŒECCV2012
,pages488Œ
501,Berlin,Heidelberg.SpringerBerlinHei-
delberg.
TomasMikolov,IlyaSutskever,KaiChen,GregS
Corrado,andJeffDean.2013.
Distributed
representationsofwordsandphrasesandtheir
compositionality
.InC.J.C.Burges,L.Bottou,
M.Welling,Z.Ghahramani,andK.Q.Wein-
berger,editors,
AdvancesinNeuralInformation
ProcessingSystems26
,pages3111Œ3119.Cur-
ranAssociates,Inc.
FredericMorinandYoshuaBengio.2005.
Hier-
archicalprobabilisticneuralnetworklanguage
model
.In
ProceedingsoftheTenthInterna-
tionalWorkshoponIntelligenceand
Statistics
,pages246Œ252.
KhalilMrini,NikolaosPappas,andAndrei
Popescu-Belis.2017.
Cross-lingualtransferfor
newsarticlelabeling:Benchmarkingstatistical
andneuralmodels
.In
IdiapResearchReport
,
Idiap-RR-26-2017.
JinseokNam,EneldoLozaMencía,andJohannes
Fürnkranz.2016.
All-intext:Learningdocu-
ment,label,andwordrepresentationsjointly
.
In
Proceedingsofthe13thAAAIConferenceon
Intelligence
,AAAI'16,pages1948Œ
1954,Phoenix,Arizona.
MohammadNorouzi,TomasMikolov,SamyBen-
gio,YoramSinger,JonathonShlens,Andrea
Frome,GregCorrado,andJeffreyDean.2014.
Zero-shotlearningbyconvexcombinationof
semanticembeddings.
In
InternationalCon-
ferenceonLearningRepresentations
,Banff,
Canada.
BoPangandLillianLee.2005.
Seeingstars:Ex-
ploitingclassrelationshipsforsentimentcate-
gorizationwithrespecttoratingscales
.In
Pro-
ceedingsofthe43rdAnnualMeetingonAs-
sociationforComputationalLinguistics
,pages
115Œ124,AnnArbor,Michigan.
NikolaosPappas,LeslyMiculicich,andJames
Henderson.2018.
Beyondweighttying:Learn-
ingjointinput-outputembeddingsforneural
machinetranslation
.In
Proceedingsofthe
ThirdConferenceonMachineTranslation:Re-
searchPapers
,pages73Œ83,Belgium,Brussels.
AssociationforComputationalLinguistics.
NikolaosPappasandAndreiPopescu-Belis.2017.
Multilingualhierarchicalattentionnetworksfor
document
.In
Proceedingsofthe
EighthInternationalJointConferenceonNatu-
ralLanguageProcessing(Volume1:LongPa-
pers)
,pages1015Œ1025.
AlexanderM.Rush,SumitChopra,andJason
Weston.2015.
Aneuralattentionmodelfor
abstractivesentencesummarization
.In
Pro-
ceedingsofthe2015ConferenceonEmpiri-
calMethodsinNaturalLanguageProcessing
,
pages379Œ389,Lisbon,Portugal.
RicoSennrich,BarryHaddow,andAlexandra
Birch.2016.
Neuralmachinetranslationofrare
wordswithsubwordunits
.In
Proceedingsof
the54thAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1:LongPa-
pers)
,pages1715Œ1725,Berlin,Germany.
LeiShu,HuXu,andBingLiu.2017.
DOC:Deep
openionoftextdocuments
.In
Pro-
ceedingsofthe2017ConferenceonEmpiri-
calMethodsinNaturalLanguageProcessing
,
pages2911Œ2916,Copenhagen,Denmark.As-
sociationforComputationalLinguistics.
RichardSocher,MilindGanjoo,ChristopherD.
Manning,andAndrewY.Ng.2013.
Zero-
shotlearningthroughcross-modaltransfer
.In
Proceedingsofthe26thInternationalConfer-
enceonNeuralInformationProcessingSys-
tems
,NIPS'13,pages935Œ943,LakeTahoe,
Nevada.
VivekSrikumarandChristopherD.Manning.
2014.
Learningdistributedrepresentationsfor
structuredoutputprediction
.In
Proceedings
ofthe27thInternationalConferenceonNeu-
ralInformationProcessingSystems-Volume2
,
NIPS'14,pages3266Œ3274,Cambridge,MA,
USA.MITPress.
DuyuTang,BingQin,andTingLiu.2015.
Doc-
umentmodelingwithgatedrecurrentneural
networkforsentiment
.In
Pro-
ceedingsofthe2015ConferenceonEmpiri-
calMethodsinNaturalLanguageProcessing
,
pages1422Œ1432,Lisbon,Portugal.Associa-
tionforComputationalLinguistics.
GuoyinWang,ChunyuanLi,WenlinWang,Yizhe
Zhang,DinghanShen,XinyuanZhang,Ricardo
Henao,andLawrenceCarin.2018.
Jointem-
beddingofwordsandlabelsfortext
tion
.In
Proceedingsofthe56thAnnualMeet-
ingoftheAssociationforComputationalLin-
guistics(Volume1:LongPapers)
,pages2321Œ
2331.AssociationforComputationalLinguis-
tics.
JasonWeston,SamyBengio,andNicolasUsunier.
2010.
Largescaleimageannotation:Learn-
ingtorankwithjointword-imageembeddings
.
Mach.Learn.
,81(1):21Œ35.
JasonWeston,SamyBengio,andNicolasUsunier.
2011.
WSABIE:Scalinguptolargevocab-
ularyimageannotation
.In
Proceedingsof
theTwenty-SecondInternationalJointConfer-
enceonIntelligence(Volume3)
,pages
2764Œ2770,Barcelona,Spain.
ZichaoYang,DiyiYang,ChrisDyer,Xiaodong
He,AlexSmola,andEduardHovy.2016.
Hier-
archicalattentionnetworksfordocumentclas-

.In
Proceedingsofthe2016Confer-
enceoftheNorthAmericanChapteroftheAs-
sociationforComputationalLinguistics:Hu-
manLanguageTechnologies
,pages1480Œ1489,
SanDiego,California.
MajidYazdaniandJamesHenderson.2015.
A
modelofzero-shotlearningofspokenlanguage
understanding
.In
Proceedingsofthe2015
ConferenceonEmpiricalMethodsinNatural
LanguageProcessing
,pages244Œ249,Lisbon,
Portugal.
Chih-KuanYeh,Wei-ChiehWu,Wei-JenKo,and
Yu-ChiangFrankWang.2018.
Learningdeep
latentspacesformulti-label
.In
In
Proceedingsofthe32ndAAAIConferenceon
Intelligence
,NewOrleans,USA.
XiangZhang,JunboZhao,andYannLeCun.
2015.
Character-levelconvolutionalnetworks
fortext
.In
AdvancesinNeural
InformationProcessingSystems28
,pages649Œ
657,Montreal,Canada.
YangZhang,BoqingGong,andMubarakShah.
2016.
Fastzero-shotimagetagging
.In
Pro-
ceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition
,LasVegas,
USA.
"
44,EARL: Joint Entity and Relation Linking for Question Answering over Knowledge Graphs,http://arxiv.org/pdf/1801.03825v4.pdf,https://github.com/AskNowQA/EARL,"EARL:JointEntityandRelationLinkingfor
QuestionAnsweringoverKnowledgeGraphs
MohnishDubey
1
;
2
,DebayanBanerjee
1
,DebanjanChaudhuri
1
;
2
,andJensLehmann
1
;
2
1
SmartDataAnalyticsGroup(SDA),UniversityofBonn,Germany
{dubey,chaudhur,jens.lehmann}@cs.uni-bonn.de
debayan@uni-bonn.de
2
FraunhoferIAIS,Bonn,Germany
jens.lehmann@iais.fraunhofer.de
Abstract.
Manyquestionansweringsystemsoverknowledgegraphsrelyon
entityandrelationlinkingcomponentsinordertoconnectthenaturallanguage
inputtotheunderlyingknowledgegraph.Traditionally,entitylinkingandrelation
linkinghavebeenperformedeitherasdependentsequentialtasksorasindependent
paralleltasks.Inthispaper,weproposeaframeworkcalledEARL,whichperforms
entitylinkingandrelationlinkingasajointtask.EARLimplementstwodifferent
solutionstrategiesforwhichweprovideacomparativeanalysisinthispaper:The
strategyisaformalisationofthejointentityandrelationlinkingtasksasan
instanceoftheGeneralisedTravellingSalesmanProblem(GTSP).Inorderto
becomputationallyfeasible,weemployapproximateGTSPsolvers.Thesecond
strategyusesmachinelearninginordertoexploittheconnectiondensitybetween
nodesintheknowledgegraph.Itreliesonthreebasefeaturesandre-rankingsteps
inordertopredictentitiesandrelations.Wecomparethestrategiesandevaluate
themonadatasetwith5000questions.Bothstrategiesoutperform
thecurrentstate-of-the-artapproachesforentityandrelationlinking.
Keywords:
EntityLinking,RelationLinking,GTSP,QuestionAnswering
1Introduction
Questionansweringoverknowledgegraphs(KGs)isanactiveresearchareaconcerned
withtechniquesthatallowobtaininginformationfromknowledgegraphsbasedon
naturallanguageinput.,SemanticQuestionAnswering(SQA)as
in[
8
]isthetaskofusersaskingquestionsinnaturallanguage(NL)towhichtheyreceive
aconciseanswergeneratedbyaformalqueryoveraKG.
Semanticquestionansweringsystemscanbeafullyrulebasedsystems[
4
]orend-
to-endmachinelearningbasedsystems[
19
].ThemainchallengesfacedinSQAare(i)
entityandlinking,(ii)relationandlinking,(iii)queryintent
and(iv)formalquerygeneration.
SomeQAsystemshaveachievedgoodperformanceonsimplequestions[
11
],
i.e.thosequestionswhichcanbeansweredbylinkingtoatmostonerelationandatmost
oneentityintheKG.Recently,thefocushasshiftedtowardscomplexquestions[
30
],
comprisingofmultipleentitiesandrelations.
arXiv:1801.03825v4  [cs.AI]  25 Jun 2018Fig.1.
Anexcerptofthesubdivisionknowledgegraphfortheexamplequestion""Wherewasthe
founderofTeslaandSpaceXborn?"".Notethatbothentitiesandrelationsarenodesinthegraph.
Usually,allentitiesandrelationsneedtobecorrectlylinkedtotheknowledgegraph
inordertogeneratethecorrectformalqueryandsuccessfullyanswerthequestionof
auser.Hence,itiscrucialtoperformthelinkingprocesswithhighaccuracyandthis
isamajorbottleneckforthewidespreadadoptionofcurrentSQAsystems.Inmost
entitylinkingsystems[
12
,
26
],disambiguationisperformedbylookingatotherentities
presentintheinputtext.However,inthecaseofnaturallanguagequestions(shorttext
fragments)thenumberofotherentitiesfordisambiguationisnothigh.Therefore,itis
potentiallytoconsiderentityandrelationcandidatesfortheinputquestions
incombination,tomaximisetheusableevidenceforthecandidateselectionprocess.
Toachievethis,weproposeEARL(
E
ntity
a
nd
R
elation
L
inker),asystemforjointly
linkingentitiesandrelationsinaquestiontoaknowledgegraph.EARLtreatsentity
linkingandrelationlinkingasasingletaskandthusaimstoreducetheerrorcausedby
thedependentsteps.
EARLusestheknowledgegraphtojointlydisambiguateentityandrelations:It
obtainsthecontextforentitydisambiguationbyobservingtherelationssurrounding
theentity.Similarly,itobtainsthecontextforrelationdisambiguationbylookingat
thesurroundingentities.Thesystemsupportsmultipleentitiesandrelationsoccurring
incomplexquestions.EARLimplementstwodifferentsolutionstrategies:The
strategyisaformalisationofthejointentityandrelationlinkingtasksasaninstanceof
theGeneralisedTravellingSalesmanProblem(GTSP).SincetheproblemisNP-hard,
weemployapproximateGTSPsolvers.Thesecondstrategyusesmachinelearningin
ordertoexploittheconnectiondensitybetweennodesintheKG.Itreliesonthreebase
featuresandre-rankingstepsinordertopredictentitiesandrelations.Wecomparethe
strategiesandevaluatethemonadatasetwith5000questions.Bothstrategiesoutperform
thecurrentstate-of-the-artapproachesforentityandrelationlinking.
Letusconsideranexampletoexplaintheunderlyingidea:
""Wherewasthefounder
ofTeslaandSpaceXborn?""
.Here,theentitylinkerneedstoperformdisambiguationfor
thekeyword""Tesla""betweenthescientist""NikolaTesla""andthecarcompany""Tesla
Motors"".EARLusesallotherentitiesandrelations(
SpaceX,founder,born
)presentinthe
query.Itdoesthisbyanalysingthesubdivisiongraphoftheknowledgegraphfragment
containingthecandidatesforrelevantentitiesandrelations.Whileperformingthejoint
analysis(Figure1),EARLdetectsthatthereisnolikelycombinationofcandidates,
whichsupportsthedisambiguationof""Tesla""as""NikolaTesla"",whereasthereisa
plausiblecombinationofcandidatesforthecarcompany""TeslaMotors"".
Overall,ourcontributionsinthispaperareasfollows:
1.
TheframeworkEARL,whereGTSPsolverorConnectionDensitycanbeusedfor
jointlinkingofentitiesandrelations(Sec.4).
2.
Aformalisationofthejointentityandrelationlinkingproblemasaninstanceofthe
GeneralisedTravellingSalesman(GTSP)problem(Sec.4.2).
3.
AnimplementationoftheGTSPstrategyusingapproximateGTSPsolvers.
4.
A""ConnectionDensity""formalisationandimplementationofthejointentityand
relationlinkingproblemasamachinelearningtask(Sec.4.3).
5.
AnadaptiveE/Rlearningmodule,whichcancorrecterrorsoccurringacrossdifferent
modules(Sec.4.4).
6.
Acomparativeanalysisofbothstrategies-GTSPandconnectiondensity(Table2).
7.
Afullyannotatedversionofthe5000questionLC-QuADdata-set,whereentityand
relationsarelinkedtotheKG.
8.
AlargesetoflabelsforDBpediapredicatesandentitiescoveringthesyntacticand
semanticvariations.
3
Thepaperisorganisedintothefollowingsections:(2)RelatedWorkoutliningsome
ofthemajorcontributionsinentityandrelationlinkingusedinquestionanswering;
(3)ProblemStatement,wherewediscusstheproblemindepthandourhypothesesfor
thesolution;(4)thearchitectureofEARLincludingpreprocessingstepsfollowedby
(i)aGTSPsolveror(ii)aconnectiondensityapproach;(5)Evaluation,withvarious
evaluationcriteriaandresults;(6)Discussion;and(7)Conclusion.
2RelatedWork
Theentityandrelationlinkingchallengehasattractedawidevarietyofsolutionsover
time.LinkingnaturallanguagephrasestoDBpediaresources,Spotlight[
12
]breaks
downtheprocessofentityspottingintofourphases.Ittheentityusingalistof
surfaceformsandthengeneratesDBpediaresourcescandidates.Itthendisambiguates
theentitybasedonsurroundingcontext.AGDISTIS[
26
]followstheinherentstructure
ofthetargetknowledgebasemorecloselytosolvetheproblem.Beingagraph-based
disambiguationsystem,AGDISTISperformsdisambiguationbasedonthehop-distance
betweenthecandidatesfortheentitiesinagiventext,wheremultipleentitiesarepresent.
Babelfy[
13
]useswordsensedisambiguationforentitylinking.Ontheotherhand,
S-MART[
29
]isoftenappropriatedasanentitylinkingsystemoverFreebaseresources.
Itgeneratesmultipleregressiontreesandthenappliessophisticatedstructuredprediction
techniquestolinkentitiestoresources.
Asrelationlinkingisgenerallyconsideredtobeatask,onlyafew
generalpurposerelationlinkingsystemsareinuse.Iterativebootstrappingstrategiesfor
extractingRDFresourcesfromunstructuredtexthavebeenexploredinBOA[
5
]and
PATTY[
15
].Itconsistsofnaturallanguagepatternscorrespondingtorelationspresentin
theknowledgegraph.Wordembeddingmodelsarealsofrequentlyusedtoovercomethe
linguisticgapforrelationlinking.RelMatch[
20
]improvestheaccuracyofthePATTY
datasetforrelationlinking.TherearetoolssuchasReMatch[
14
]whichuseswordnet
similarityforrelationlinking.
3
datasetavailableat
https://github.com/AskNowQA/EARL
Linking
Approach
QASystemAdvantageDisadvantage
Sequential
[4][2][21]
-Reducescandidatesearch
spaceforRelationLinking
-RelationLinkinginformation
cannotbeexploitedinEntity
Linkingprocess
-Allowsschemav
-ErrorsinEntityLinkingcannot
beovercome
Parallel
[27][28][16]-Lowerruntime
-EntityLinkingprocesscannot
useinformationfromRelation
Linkingprocessandviceversa
-Re-rankingofEntitiespossible
basedonRelationLinking
-Doesnotallowschemav
cation
Joint
[1][30]-Potentiallyhighaccuracy-Complexityincrease
(with-Reduceserrorpropagation-Largersearchspace
limited-Betterdisambiguation
candidate-Allowsschemav
set)-Allowsre-ranking
Table1.
StateoftheartforEntityandRelationlinkinginQuestionAnswering
ManyQAsystemsuseanout-of-the-boxentitylinker,oftenoneoftheaforemen-
tionedones.Thesetoolsarenottailor-madeforquestionsandareinsteadtrainedon
largetextcorpora,typicallydevoidofquestions.Thismaycreateseveralproblemsas
questionsdonotspanovermorethanonesentence,therebyrenderingcontext-based
disambiguationrelativelyineffective.Further,graphbasedsystemsrelyonthepresence
ofmultipleentitiesinthesourcetextanddisambiguatethembasedoneachother.This
becomesdifwhendealingwithquestions,astheyseldomconsistofmultipleentity.
Thus,toavoidtheissuesmentioned,avarietyofapproacheshavebeenemployed
forentityandrelationlinkingforquestionanswering.Semanticparsing-basedsystems
suchasAskNow[
4
]andTBSL[
25
]linktheentitiesandgeneratealistofcandidate
relationsbasedontheresources.Theyuseseveralstringandsemanticsimilarity
techniquestoselectthecorrectentityandrelationcandidatesforthequestion.In
thesesystems,theprocessofrelationlinkingdependsonlinkingtheentities.Generating
entityandrelationcandidateshasalsobeenexploredby[
30
],whichusesthesecandidates
tocreatestagedquerygraphs,andlaterre-ranksthembasedontextualsimilaritybetween
thequeryandthetargetquestion,computedbyaSiamesearchitecture-basedneural
network.TherearesomeQAsystemssuchasXser[
28
],whichperformsrelationlinking
independentofentitylinking.STAGG[
30
]takesthetop10entitiesgivenbytheentity
linkerandtriestobuildquery-subgraphchainscorrespondingtothequestion.This
approachconsidersarankedlistofentitycandidatesfromtheentitylinkerandchooses
thebestcandidatebasedonthequerysubgraphformed.Generally,semanticparsing
basedsystemstreatentityandrelationlinkingasseparatetaskswhichcanbeobserved
inthegeneralisedpipelineofFrankenstein[21]andOKBQA
www.okbqa.org/
.
3OverviewandPreliminaries
3.1OverviewandResearchQuestions
Asdiscussedpreviously,inquestionansweringthetasksofentityandrelationlinking
areperformedeithersequentiallyorinparallel.Insequentialsystems,usuallythe
entitylinkingtaskisperformedfollowedbyrelationlinking.Asaconsequence,
informationintherelationlinkingphasecannotbeexploitedduringentitylinkinginthis
case.Inparallelsystems,entityandrelationlinkingareperformedindependently.While
thisisefintermsofruntimeperformance,theentitylinkingprocesscannot
fromfurtherinformationobtainedduringrelationlinkingandviceversa.Weillustrate
theadvantagesanddisadvantagesofbothapproaches,aswellasthesystemsfollowing
them,inTable1.Ourmaincontributioninthispaperistheprovisionofasystem,which
takescandidatesforentityandrelationlinkingasinputandperformsajointoptimisation
selectingthebestcombinationofentityandrelationcandidates.
Postulates
Wehavethreepostulates,whichwewanttoverifybasedonourapproach:
H1
:Givencandidatelistsofentitiesandrelationsfromaquestion,thecorrectsolution
isacycleofminimalcostthatvisitsexactlyonecandidatefromeachlist.
H2
:Givencandidatelistsofentitiesandrelationsfromaquestion,thecorrect
candidatesexhibitrelativelydenseandshort-hopconnectionsamongthemselvesinthe
knowledgegraphcomparedtowrongcandidatesets.
H3
:Jointlylinkingentityandrelationleadstohigheraccuracycomparedtoperform-
ingthesetasksseparately.
Wewillre-visitallofthesepostulatesintheevaluationsectionofthepaper.
3.2Preliminaries
Wewillintroducebasicnotionsfromgraphtheory:
1
(Graph).
A(simple,undirected)graphisanorderedpair
G
=(
V;E
)
where
V
isasetwhoseelementsarecalled
vertices
and
E
isasetofpairsofvertices
whichiscalled
edges
.
2
(KnowledgeGraph).
Withinthescopeofthispaper,wea
knowl-
edgegraph
asalabelleddirectedmulti-graph.Alabelleddirectedmulti-graphisa
tuple
KG
=(
V;E;L
)
where
V
isasetcalledvertices,
L
isasetofedgelabelsand
E

V

L

V
isasetoforderedtriples.
Itshouldbenotedthatourofknowledgegraphscapturesbasicaspectsof
RDFdatasetsaswellaspropertygraphs[
6
].Theknowledgegraphverticesrepresent
entitiesandtheedgesrepresentrelationshipsbetweenthoseentities.
3
(SubdivisionGraph).
Thesubdivisiongraph[
24
]
S
(
G
)
ofagraph
G
is
thegraphobtainedfrom
G
byreplacingeachedge
e
=(
u;v
)
of
G
byanewvertex
w
e
and2newedges
(
u;w
e
)
and
(
v;w
e
)
.
Fig.2.
EARLArchitecture:InthedisambiguationphaseonemaychooseeitherConnectionDensity
orGTSP.IncaseswheretrainingdataisnotavailablebeforehandGTSPworksbetter.
4EARL
Ingeneral,entitylinkingisatwostepprocess.Thestepistoidentifyandspotthe
spanoftheentity.Thesecondstepistodisambiguateorlinktheentitytotheknowledge
graph.Forlinking,thecandidatesaregeneratedforthespottedspanoftheentityandthen
thebestcandidateischosenforthelinking.Thesetwostepsaresimilarlyfollowedin
standardrelationlinkingapproaches.Inourapproach,wespotthespansofentities
andrelations.Afterthat,the(disambiguation)linkingtaskisperformedjointlyforboth
entitiesandrelations.
Inthissectionwediscussthestepofspandetectionofentityandrelationin
naturallanguagequestionandcandidatelistgeneration.Weperformthedisambiguation
bytwodifferentapproaches,whicharediscussedlaterinthissection.
4.1CandidateGenerationSteps
4.1.1ShallowParsing
:Givenaquestion,extractallkeywordphrasesout.EARL
usesSENNA[
3
]asthekeywordextractor.Wealsoremovestopwordsfromthequestion
atthisstage.Inexamplequestion""WherewasthefounderofTeslaandSpaceXborn?""
weidentify
<founder,Tesla,SpaceX,born>
asourkeywordphrases.
4.1.2E/RPrediction
Oncekeywordphrasesareextractedfromthequestions,the
nextstepinEARListopredictwhethereachoftheseisanentityorarelation.Weusea
characterembeddingbasedlong-shorttermmemorynetwork(LSTM)todothesame.
Thenetworkistrainedusinglabelsforentityandrelationintheknowledgegraph.For
handlingoutofvocabularywords[
17
],andalsotoencodetheknowledgegraphstructure
inthenetwork,wetakeamulti-tasklearningapproachwithhardparametersharing.Our
modelistrainedonacustomlossgivenby:
E
=(1


)
E
BCE
+

E
ED
(1)
Where,
E
BCE
isthebinarycrossentropylossforthelearningobjectiveofaphrase
beinganentityorarelationand
E
Ed
isthesquaredeuclediandistancebetweenthepre-
dictedembeddingandthecorrectembeddingforthatlabel.Thevalueof

isempirically
selectedas0.25.Weusepre-trainedlabelembeddingsfromRDF2Vec[
18
]whichare
trainedonknowledgegraphs.RDF2Vecprovideslatentrepresentationforentitiesand
relationsinRDFgraphs.Itefcientlycapturesthesemanticrelatednessbetweenentities
andrelations.
Weuseahiddenlayersizeof128fortheLSTM,followedbytwodenselayersof
sizes512and256respectively.Adropoutvalueof0.5isusedinthedenselayers.The
networkistrainedusingAdamoptimizer[9]withalearningrateof0.0001andabatch
sizeof128.Goingbacktotheexample,thismodule""founder""and""born""as
relations
,""Tesla""and""SpaceX""as
entities
.
4.1.3CandidateListGeneration
Thismoduleretrievesacandidatelistforeach
keywordinthenaturallanguagequestionbytheshallowparser.Toretrieve
thetopcandidatesforakeywordwecreateanElasticsearch
4
indexofURI-labelpairs.
SinceEARLrequiresanexhaustivelistoflabelsforaURIintheknowledgegraph,we
expandthelabels.WeusedWikidatalabelsforentitieswhichareinsame-asrelationin
theknowledgebase.Forrelationswerequirelabelswhichweresemanticallyequivalent
(suchaswriter,author)forwhichwetooksynonymsfromtheOxfordDictionaryAPI
5
.
Tocovergrammaticalvariationsofaparticularlabel,weaddedfromfastText
6
.
Weavoidanybiasheldtowardsoragainstpopularentitiesandrelations.
Theoutputofthesepre-processingstepsarei)setofkeywordsfromthequestion,ii)
everykeywordisieitherasrelationorentity,iii)foreverykeywordthereisa
setofcandidateURIsfromtheknowledgegraph.
4.2UsingGTSPforDisambiguation
AtthispointwemayuseeitheraGTSPbasedsolutionorConnectionDensity(later
explainedin4.3)fordisambiguation.WestartwiththeformalisationforGTSPbased
solution.
Theentityandrelationlinkingprocesscanbeformalisedviaspottingandcandidate
generationfunctionsasfollows:Let
S
bethesetofallstrings.Weassumethatthere
isafunction
spot
:
S
!
2
S
whichmapsastring
s
(theinputquestion)toaset
K
ofsubstringsof
s
.Wecallthisset
K
the
keywords
occurringinourinput.Moreover,
weassumethereisafunction
cand
KG
:
K!
2
V
[
L
whichmapseachkeywordto
asetofcandidatenodeandedgelabelsforourknowledgegraph
G
=(
V;E;L
)
.
Thegoalofjointentityandrelationlinkingistocombinationsofcandidates,
whicharecloselyrelated.Howcloselynodesarerelatedismodelledbyacostfunction
cost
KG
:(
V
[
L
)

(
V
[
L
)
!
[0
;
1]
.Lowervaluesindicatecloserrelationships.
Accordingtoourpostulate,weaimtoencodegraphdistancesinthecostfunction
torewardthosecombinationsofentitiesandrelations,whicharelocatedclosetoeach
otherintheinputknowledgegraph.Tobeabletoconsiderdistancesbetweenboth
relationsandentities,wetransformtheknowledgegraphintoitssubdivisiongraph(see
3).Thissubdivisiongraphallowsustoelegantlythedistancefunction
asillustratedinFigure4.
4
https://www.elastic.co/products/elasticsearch
5
https://developer.oxforddictionaries.com/
6
https://fasttext.cc/
Fig.3.
UsingGTSPfordisambiguation:TheboldlinerepresentsthesolutionofferedbytheGTSP
solver.Eachedgerepresentsanexistingconnectionintheknowledgegraph.Theedgeweightis
equaltothenumberofhopsbetweenthetwonodesintheknowledgegraph.Wealsoaddtheindex
searchranksofthetwonodestheedgesconnecttotheedgeweightwhensolvingforGTSP.
Giventheknowledgegraph
KG
andthefunctions
spot
,
cand
and
cost
,wecan
casttheproblemofjointentityandrelationlinkingasaninstanceoftheGeneralised
TravellingSalesman(GTSP)problem:Weconstructagraph
G
with
V
=
S
k
2
K
cand
(
k
)
.
Eachnodeset
cand
(
k
)
iscalledaclusterinthisvertexset.TheGTSPproblemisto
asubset
V
0
=(
v
1
;:::;v
n
)
of
V
whichcontainsexactlyonenodefromeachclusterand
thetotalcost
P
n

1
i
=1
cost
(
v
i
;v
i
+1
)
isminimalwithrespecttoallsuchsubsets.Please
notethatinourformalisationoftheGTSP,wedonotrequire
V
0
tobeacycle,i.e.
v
1
and
v
n
canbedifferent.Moreover,wedonotrequireclusterstobedisjoint,i.e.different
keywordscanhaveoverlappingcandidatesets.
Figure3illustratestheproblemformulation.Eachcandidatesetforakeywordforms
aclusterinthegraph.Theweightofeachedgeinthisgraphisgivenbythecostfunction,
whichincludesthedistancebetweenthenodesinthesubdivisiongraphoftheinput
knowledgegraphaswellasthescoresofthecandidates.TheGTSPrequires
thesolutiontovisitoneelementperclusterandminimisestheoveralldistance.
ApproximateGTSPSolvers
Inordertosolvethejointentityandrelationlinking
problem,thecorrespondingGTSPinstanceneedstobesolved.Unfortunately,theGTSP
isNP-hard[
10
]andhenceitisintractable.However,sinceGTSPcanbereducedto
standardTSP,severalpolynomialapproximationalgorithmsexisttosolveGTSP.The
state-of-the-artapproximateGTSPsolveristheLinŒKernighanŒHelsgaunalgorithm[
7
].
Here,aGTSPinstanceistransformedintostandardasymmetricTSPinstancesusing
theNoon-Beantransformation.ItallowstheheuristicTSPsolverLKHtobeusedfor
solvingtheinitialGTSP.AmongLKH'scharacteristics,itsuseof1-treeapproximation
fordeterminingacandidateedgeset,theextensionofthebasicsearchstep,andeffective
rulesfordirectingandpruningthesearchcontributetoitsefy.
WhileaGTSPbasedsolutionwouldbesuitableforsolvingthejointentityand
relationlinkingproblem,ithasthedrawbackthatitcanonlyprovidethebestcandidate
foreachkeywordgiventhelistofcandidates.MostapproximateGTSPsolutionsdo
notexploreallpossiblepathsandnodesandhenceacomprehensivescoringandre-
rankingofnodesisnotpossible.Ideally,wewouldliketogobeyondthisandre-rank
allcandidatesforagivenkeyword.ThiswouldopenupnewopportunitiesfromaQA
perspective,i.e.ausercouldbepresentedwithasortedlistofmultiplepossibleanswers
toselectfrom.
4.3UsingConnectionDensityforDisambiguation
GTSPConnectionDensity
RequiresnotrainingdataRequiresdatatotraintheXGBoost
TheapproximateGSTPLKHsolutionisonly
abletoreturnthetopresultasnotallpossible
pathsareexplored.
Returnsalistofallpossiblecandidatesinorder
ofscore
TimecomplexityofLKHis
O
(
nL
2
)
wheren
=numberofnodesingraph,L=numberof
clustersingraphof
Timecomplexityis
O
(
N
2
L
2
)
whereN=num-
berofnodespercluster,L=numberofclusters
ingraph
Reliesonidentifyingthepathwithminimum
cost
Dependsonidentifyingdenseandshort-hop
connections
Table2.
ComparisonofGTSPbasedapproachandConnectiondensityforDisambiguation
Asdiscussedearlier,oncethecandidatelistgenerationisachieved,EARLofferstwo
independentmodulesfortheentityandrelationlinking.Intheprevioussubsection4.2
wediscussedoneapproachusingGTSP.Inthissubsectionwewilldiscussthesecond
approachfordisambiguationusingConnectionDensity,whichworksasanalternativeto
theGTSPapproach.Wehavealsocomparedthetwomethodsintable2.
4.3.1FormalisationofConnectionDensity:
Forkeywordsinaquestion
wehavetheset
K
asearlier.Foreachkeyword
K
i
wehavelist
L
i
whichconsists
ofallthecandidateurisgeneratedbytextsearch.Wehave
n
suchcandidatelistsfor
eachquestiongivenby,
L
=
f
L
1
;L
2
;L
3
;:::;L
n
g
.Weconsideraprobablecandidate
c
i
m
2
L
i
,where
m
isthetotalnumberofcandidatestobeconsideredperkeyword,whichis
thesameasthenumberofitemsineachlist.
Fig.4.
ConnectionDensitywithexample:Thedottedlinesrepresentcorrespondingconnections
betweenthenodesintheknowledgebase.
Thehopdistance
dKGhops
(
c
k
i
;c
o
j
)
2
Z
+
isnumberofhopsbetween
c
k
i
and
c
o
j
inthesubdivisionknowledgegraph.Iftheshortestpathfrom
c
k
i
and
c
o
j
requiresthe
traversalof
h
edgesthen
dKGhops
(
c
k
i
;c
o
j
)
=
h
.
ConnectionDensityisbasedonthethreefeatures:TextsimilaritybasedinitialRank
oftheListitem(
R
i
)Connection-Count(
C
)andHop-Count(
H
)
InitialRankoftheList(
R
i
),isgeneratedbyretrievingthecandidatesfromthesearch
indexviatextsearch.Thisisachievedinthepreprocessingstepsasmentionedinthe
section4.Further,to
C
weintroduce
dConnect
.
dConnect
(
c
k
i
;c
o
j
)=
(
1
if
dKGhops
(
c
k
i
,
c
o
j
)
6
2
0
otherwise
(2)
TheConnection-Count
C
forancandidate
c
,isthenumberofconnectionsfrom
c
to
candidatesinalltheotherlistsdividedbythetotalnumber
n
ofkeywordsspotted.We
considernodesathopcountsofgreaterthan2disconnectedbecausenodestoofaraway
fromeachotherintheknowledgebasedonotcarrymeaningfulsemanticconnectionto
eachother.
C
(
c
k
i
)=1
=n
X
o
j
o
6
=
k
j
=
m
X
j
=1
dConnect
(
c
k
i
;c
o
j
)
(3)
TheHop-Count
H
foracandidate
c
,isthesumofdistancesfrom
c
toalltheother
candidatesinalltheotherlistsdividedbythetotalnumberofkeywordsspotted.
H
(
c
k
i
)=1
=n
X
o
j
o
6
=
k
j
=
m
X
j
=1
dKGhops
(
c
k
i
;c
o
j
)
(4)
4.3.2CandidateRe-ranking:
H
;
C
and
R
i
constituteourfeaturespace
X
.This
featurespaceisusedtothemostrelevantcandidategivenasetofcandidatesfor
ankeywordinthequestion.Weuseamachinelearningtolearnthe
probabilityofbeingthemostsuitablecandidate

c
i
giventhesetofcandidates.The
list
R
f
isobtainedbyre-rankingthecandidatelistsbasedontheprobabilityassignedby
the.Ideally,

c
i
shouldbethetop-mostcandidatein
R
f
.
Thetrainingdataconsistsofthefeatures
H
;
C
and
R
i
andalabel1ifthecandidateis
thecorrect,0otherwise.Forthetesting,weapplythelearnedfunctionfromthe
f
on
X
foreverycandidate
2
c
i
andgetaprobabilityscoreforbeingthemostsuitable
candidate.Weperformexperimentswiththreedifferentnamelyextreme
gradientboosting(xgboost),SVM(withalinearkernel)andlogisticregressiontore-rank
thecandidates.Theexperimentsaredoneusinga5-foldcross-validationstrategywhere,
foreachfoldwetraintheonthetrainingsetandobservethemeanreciprocal
rank(MRR)of

c
i
onthetestingsetafterre-rankingthecandidatelistsbasedonthe
assignedprobability.TheaverageMRRon5-foldcross-validationforthethree
are0.905,0.704and0.794respectively.Hence,weusexgboostasthein
oursubsequentexperimentsforre-ranking.
4.3.3Algorithm
Wenowpresentapseudo-codeversionofthealgorithmtocalculate
thetwofeatures:ConnectionDensityalgorithmisusedforhopcountand
connectioncountforeachcandidatenode.Wethenpassthesefeaturestoa
forscoringandrankingThisalgorithm(Algorithm1ConnectionDensity)hasatime
complexitygivenby
O
(
N
2
L
2
)
whereNisthenumberofkeywordsandListhenumber
ofcandidatesforeachkeyword.
Algorithm1:
ConnectionDensity
function:
ConnectionDensity()
input:
L
,with
n
numberofkeywords
//
anarrayofarrays
output:
Hop-Count
H
,Connection-Count
C
1
dConnectCounter={}
//
Countforconnectionsfromandtoeachnode
2
dHopCounter={}
//
Similarlyhopcountsforeachnode
3
foreach
L
a
2L
do
4
foreach
c
a
i
2
L
a
do
5
dConnectCounter
[
c
a
i
]=0
//
Initialisingthedictionary
6
dHopCounter
[
c
a
i
]=0
7
foreach
(
L
a
;L
b
)
2L
do
8
foreach
c
a
i
2
L
a
do
9
foreach
c
b
j
2
L
b
do
10
if
dKGhops
(
c
a
i
,
c
b
j
)<=2
then
11
dConnectCounter
[
c
a
i
]+=1
12
dConnectCounter
[
c
b
j
]+=1
13
dHopCounter
[
c
a
i
]+=dKGhops(
c
a
i
,
c
b
j
)
14
dHopCounter
[
c
b
j
]+=dKGhops(
c
a
i
,
c
b
j
)
15
foreach
(
c
i
;score
)
2
dConnectCounter
do
16
C
(
c
i
)
=
dConnectCounter
(
c
i
)
=n
//
Normalisationwithrespectto
numberofkeywordsspotted
17
foreach
(
c
i
;score
)
2
dHopCounter
do
18
H
(
c
i
)
=
dHopCounter
(
c
i
)
=n
19
return
(Hop-Count
H
,Connection-Count
C
)
4.4AdaptiveE/RLearning
EARLusesaseriesofsequentialmoduleswithlittletonofeedbackacrossthem.Hence,
theerrorsinonemodulepropagatedowntheline.Totrammelthis,weimplementan
adaptiveapproachespeciallyforcurbingtheerrorsmadeinthepre-processingmodules.
Whileconductingexperiments,itwasobservedthatmostoftheerrorsareintheshallow
parsingphase,mainlybecauseofgrammaticalerrorsinLC-QuADwhichdirectlyaffects
theconsecutiveE/Rpredictionandcandidateselectionsteps.IftheE/Rpredictionis
erroneous,itwillsearchinawrongElasticsearchindexforprobablecandidatelist
generation.Insuchacasenoneofthecandidates
2
c
i
forakeywordwouldcontain

c
i
as
isbytheprobabilitiesassignedto
c
i
bythere-rankermodule.Ifthemaximum
probabilityassignedto
c
i
islessthanaverysmallthresholdvalue,empiricallychosenas
Fig.5.
AdaptiveE/Rlearning
0.01,were-dothestepsfromERpredictionafteralteringtheoriginalprediction.Ifthe
initialassignedprobabilityis
entity
,wechangeitto
relation
andvice-versa,example
5.Thismoduleisempiricallyevaluatedintable5.
5Evaluation
DataSet
:LC-QuAD[
23
]isthelargestcomplexquestionsdatasetavailableforQAover
KGs.Wehaveannotatedthisdatasettocreateagoldlabeldatasetforentityandrelation
linking,i.e.eachquestionnowcontainsthecorrectKGentityandrelationURIswith
theirrespectivetextspansinthequestion.Thisannotationwasdoneinasemi-automated
processandsubsequentlymanuallyvTheannotateddatasetof5000questionsis
publiclyavailableat
https://figshare.com/projects/EARL/28218
5.1Experiment1:ComparisonofGTSP,LKHandConnectionDensity
Aim:
Weevaluatehypotheses(
H1
and
H2
)thattheconnectiondensityandGTSPcanbe
usedforjointlinkingtask.WealsoevaluatetheLKHapproximationsolutionofGTSP
fordoingthistask.Wecomparethetimecomplexityofthethreedifferentapproaches.
Results:
ConnectiondensityresultsinasimilaraccuracyasthatofanexactGTSP
solutionwithabettertimecomplexity(seeTable3).Connectiondensityhasworsetime
complexitythanapproximateGTSPsolverLKHifweassumethebestcaseofequal
clustersizesforLKH.However,itprovidesabetteraccuracy.Moreover,theaverage
timetakeninEARLusingconnectiondensity(includingthecandidategenerationstep)
is0.42secondsperquestion.FurtherobservingTable3,wecanseethatthebruteforce
GTSPsolutionandConnectionDensityhavesimilaraccuracy,butthebruteforceGTSP
solutionhasexponentialtimecomplexity.TheapproximatesolutionLKHhaspolynomial
runtime,butitsaccuracydropscomparedtothebruteforceGTSPsolution.Moreover,
fromaquestionansweringperspectivetherankedlistofferedbytheConnectionDensity
approachisusefulsinceitcanbepresentedtotheuserasalistofpossiblecorrect
ApproachAccuracy(K=30)Accuracy(K=10)TimeComplexity
BruteForceGTSP0.610.62
O
(
n
2
2
n
)
LKH-GTSP0.590.58
O
(
nL
2
)
ConnectionDensity0.610.62
O
(
N
2
L
2
)
Table3.
EmpiricalcomparisonofConnectionDensityandGTSP:n=numberofnodesingraph;
L=numberofclustersingraph;N=numberofnodespercluster;topKresultsretrievedfrom
ElasticSearch.
solutionsorusedbysubsequentprocessingstepsofaQAsystem.Hence,forfurther
experimentsinthissectionweusedtheconnectiondensityapproach.
5.2Experiment2:EvaluatingJointConnectivityandRe-ranker
Aim
:EvaluatingtheperformanceofConnectionDensityforpredictingthecorrect
entityandrelationcandidatesfromasetofpossibleE-Rcandidates.Hereweevaluate
hypothesis
H2
,thecorrectcandidatesexhibitrelativelydenseandshort-hopconnections.
Valueofk
R
f
basedon
R
i
R
f
basedon
C
;
HR
f
basedon
R
i
;
C
;
H
k
=100.5430.6890.708
k
=300.5440.6660.735
k
=500.5430.617
0.737
k
=1000.5400.5340.733
k

=100.5680.864
0.905
k

=300.5540.7790.864
k

=500.5490.7080.852
k

=500.5450.6030.817
Table4.
Evaluationofjointlinkingperformance
Metrics
:Weusethemeanreciprocalrankofthecorrectcandidate

c
i
foreachentity/re-
lationinthequery.Fromtheprobablecandidatelistgenerationstep,wefetchalistof
topcandidatesforeachphraseinaquerywitha
k
valueof10,30,50and100,
wherekisthenumberofresultsfromtextsearchforeachkeywordspotted.Toevaluate
therobustnessofourandfeaturesweperformtwotests.i)Onthetophalf
ofTable4were-rankthetopkcandidatesreturnedfromthepreviousstep.ii)Onthe
bottomhalfofTable4weinsertthecorrectcandidateintoeachlisttopurely
testre-rankingabilitiesofoursystem(thisportionofthetablecontains
k

asthenumber
ofitemsineachcandidatelist).Weinjectthecorrecturisatthelowestrank(see
k

),ifit
wasnotretrievedinthetop
k
resultsfrompreviousstep.
Results
:TheresultsinTable4depictthatouralgorithmisabletosuccessfullyre-rank
thecorrectURIsifthecorrectonesarealreadypresent.IncasecorrectURIswere
missinginthecandidatelist,weinsertedURIsasthelastcandidate.The
MRRthenincreasedfrom0.568to0.905.
5.3Experiment3:EvaluatingEntityLinking
Aim
:ToevaluatetheperformanceofEARLwithotherstate-of-the-artsystemsonthe
entitylinkingtask.Thisalsoevaluatesourhypothesis
H3
.
Metrics
:Wearereportingtheperformanceonaccuracy.Accuracyisbytheratio
ofthecorrectlyentitiesoverthetotalnumberofentitiespresent.
Result
:EARLperformsbetterentitylinkingthantheothersystems(Table5),namely
Babelfy,DBpediaSpotlight,TextRazorandAGDISTIS+FOX(limitedtoentitytypes-
LOC,PER,ORG).WeconductedthistestontheLC-QuADandQALD-7dataset
7
.The
valueof
k
issetto30whilere-rankingandfetchingthemostprobableentity.
7
https://project-hobbit.eu/challenges/qald2017/
SystemAccuracyLC-QuADAccuracy-QALD
FOX[22]+AGDISTIS[26]0.360.30
DBpediaSpotlight[12]0.400.42
TextRazor
8
0.520.53
Babelfy[13]0.560.56
EARLwithoutadaptivelearning0.610.55
EARLwithadaptivelearning
0.650.57
Table5.
EvaluatingEARL'sEntityLinkingperformance
5.4Experiment4:EvaluatingRelationLinking
Aim
:Givenaquestion,thetaskistotheperformrelationlinkinginthequestion.This
alsoevaluatesourhypothesis
H3
.
Metrics
:WeusethesameaccuracymetricasintheExperiment3
Results
:AsreportedinTable6,EARLoutperformsotherapproacheswecouldrun
onLC-QuADandQALD.Thelargedifferenceinaccuracyofrelation-linkingover
LC-QuADoverQALD,isduetothefacethatLC-QuADhas82%questionswithmore
thanonerelation,thusdetectingrelationphrasesinthequestionwasdif
SystemAccuracyLC-QuADAccuracy-QALD
ReMatch[14]0.120.31
RelMatch[20]0.150.29
EARLwithoutadaptivelearning0.320.45
EARLwithadaptivelearning
0.360.47
Table6.
EvaluatingEARL'sRelationLinkingperformance
6Discussion
Ouranalysisshowsthatwehaveprovidedtwotractable(polynomialwithrespectto
thenumberofclustersandtheelementspercluster)approachesofsolvingthejoint
entityandrelationlinkingproblem.Weexperimentlyachievesimilaraccuracyasthe
exactGTSPsolutionwithbothLKH-GTSPandConnectionDensitywithbettertime
complexity,whichallowsustousethesysteminQAenginesinpractice.Itmustbe
notedthatoneofthesalientfeaturesofLKH-GTSPisthatitrequiresnotrainingdatafor
thedisambiguationmodulewhileontheotherhandConnectionDensityperformsbetter
giventrainingdataforitsXGBoost.WhilethesystemwastestedonDBpedia,
itisnotrestrictedtoaparticularknowledgegraph.
Therearesomelimitations:Thecurrentapproachdoesnottacklequestionswith
hiddenrelations,suchas""HowmanyshowsdoesHBOhave?"".Heretheseman-
ticunderstandingofthecorrespondingSPARQLqueryistocountallTVshows
(
dbo:TelevisionShow
)whichareownedby(
dbo:company
)theHBO(
dbr:HBO
).Here
dbo:company
isthehiddenrelationwhichwedonotattempttolink.However,itcould
bearguedthatthisproblemgoesbeyondthescopeofrelationlinkingandcouldbebetter
handledbythequerygenerationphaseofasemanticQAsystem.
AnotherlimitationisthatEARLcannotbeusedasinferencetoolforentitiesas
requiredbysomequestions.ForexampleTaikonautisanastronautwithChinesenation-
ality.Thesystemcanonlylinktaikonautto
dbr:Astronaut
,butadditionalinformation
cannotbecaptured.Itshouldbenoted,however,thatEARLcantackletheproblemof
the""lexicalgap""toagreatextentasitusessynonymsviathegrammarforms.
OurapproachesofLKH-GTSPandConnectionDensitybothhavepolynomialand
approximatelysimilartimecomplexities.EARLwitheitherConnectionDensityor
LKH-GTSPcanprocessaquestioninafewhundredmillisecondsonastandarddesktop
computeronaverage.Theresultlogs,experimentalsetupandsourcecodeofoursystem
arepubliclyavailableat:https://github.com/AskNowQA/EARL.
7ConclusionsandFutureWork
HereweproposeEARL,aframeworkforjointentityandrelationlinking.Weprovided
twostrategiesforjointlinking-onebasedonreducingtheproblemtoaninstanceofthe
GeneralisedTravellingSalesmanproblemandtheotherbasedonaconnectiondensity
basedmachinelearningapproach.OurexperimentsonQAbenchmarksresultedin
accuracieswhichareabovetheresultsofcurrentstate-of-the-artapproaches
forentityandrelationlinking.Infuture,wewillimprovethecandidategenerationphase
toensurethatahigherproportionofcorrectcandidatesareretrieved.
Acknowledgement
ThisworkissupportedbythefundingreceivedfromtheEU
H2020projectsWDAqua(ITN,GA.642795)andHOBBIT(GA.688227).
References
1.
J.Berant,A.Chou,R.Frostig,andP.Liang.Semanticparsingonfreebasefromquestion-
answerpairs.In
EMNLP
,volume2,page6,2013.
2.
A.Both,D.Diefenbach,K.Singh,S.Shekarpour,D.Cherix,andC.Lange.QanaryŒa
methodologyforvocabulary-drivenopenquestionansweringsystems.In
International
SemanticWebConference
,pages625Œ641.Springer,2016.
3.
R.Collobert,J.Weston,L.Bottou,M.Karlen,K.Kavukcuoglu,andP.Kuksa.Natu-
rallanguageprocessing(almost)fromscratch.
JournalofMachineLearningResearch
,
12(Aug):2493Œ2537,2011.
4.
M.Dubey,S.Dasgupta,A.Sharma,K.Höffner,andJ.Lehmann.Asknow:Aframeworkfor
naturallanguagequeryformalizationinsparql.In
InternationalSemanticWebConference
,
pages300Œ316.Springer,2016.
5.
D.GerberandA.-C.N.Ngomo.Bootstrappingthelinkeddataweb.In
1stWorkshoponWeb
ScaleKnowledgeExtraction@ISWC
,volume2011,2011.
6.
A.GubichevandM.Then.Graphpatternmatching:Dowehavetoreinventthewheel?In
ProceedingsofWorkshoponGRAphData
.ACM,2014.
7.
K.Helsgaun.SolvingtheequalitygeneralizedtravelingsalesmanproblemusingthelinŒ
kernighanŒhelsgaunalgorithm.
MathematicalProgrammingComputation
,2015.
8.
K.Höffner,S.Walter,E.Marx,R.Usbeck,J.Lehmann,andA.-C.NgongaNgomo.Survey
onchallengesofquestionansweringinthesemanticweb.
SemanticWeb
,8(6):895Œ920,2017.
9.
D.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization.
arXivpreprint
arXiv:1412.6980
,2014.
10.
G.Laporte,H.Mercure,andY.Nobert.Generalizedtravellingsalesmanproblemthroughn
setsofnodes:theasymmetricalcase.
DiscreteAppliedMathematics
,18(2):185Œ197,1987.
11.
D.Lukovnikov,A.Fischer,J.Lehmann,andS.Auer.Neuralnetwork-basedquestionan-
sweringoverknowledgegraphsonwordandcharacterlevel.In
Proceedingsofthe26th
InternationalConferenceonWorldWideWeb
,pages1211Œ1220,2017.
12.
P.N.Mendes,M.Jakob,A.García-Silva,andC.Bizer.Dbpediaspotlight:sheddinglight
onthewebofdocuments.In
Proceedingsofthe7thinternationalconferenceonsemantic
systems
,pages1Œ8.ACM,2011.
13.
A.Moro,A.Raganato,andR.Navigli.Entitylinkingmeetswordsensedisambiguation:a
approach.
TransactionsoftheAssociationforComputationalLinguistics
,2014.
14.
I.O.Mulang,K.Singh,andF.Orlandi.Matchingnaturallanguagerelationstoknowledge
graphpropertiesforquestionanswering.In
Proceedingsofthe13thInternationalConference
onSemanticSystems
,pages89Œ96.ACM,2017.
15.
N.Nakashole,G.Weikum,andF.Suchanek.Patty:Ataxonomyofrelationalpatternswith
semantictypes.In
ProceedingsoftheEMNLP2012
,pages1135Œ1145.Associationfor
ComputationalLinguistics,2012.
16.
S.Park,S.Kwon,B.Kim,andG.G.Lee.Isoftatqald-5:Hybridquestionansweringsystem
overlinkeddataandtextdata.In
CLEF(WorkingNotes)
,2015.
17.
Y.Pinter,R.Guthrie,andJ.Eisenstein.Mimickingwordembeddingsusingsubwordrnns.In
EMNLP
,pages102Œ112,2017.
18.
P.RistoskiandH.Paulheim.Rdf2vec:Rdfgraphembeddingsfordatamining.In
International
SemanticWebConference
,pages498Œ514.Springer,2016.
19.
I.V.Serban,A.García-Durán,C.Gulcehre,S.Ahn,S.Chandar,A.Courville,andY.Bengio.
Generatingfactoidquestionswithrecurrentneuralnetworks:The30mfactoidquestion-answer
corpus.
arXivpreprintarXiv:1603.06807
,2016.
20.
K.Singh,I.O.Mulang,I.Lytra,M.Y.Jaradeh,A.Sakor,M.-E.Vidal,C.Lange,andS.Auer.
Capturingknowledgeinsemantically-typedrelationalpatternstoenhancerelationlinking.In
ProceedingsoftheKnowledgeCaptureConference
,page31.ACM,2017.
21.
K.Singh,A.S.Radhakrishna,A.Both,S.Shekarpour,I.Lytra,R.Usbeck,A.Vyas,A.Khik-
matullaev,D.Punjani,etal.Whyreinventthewheel:Let'sbuildquestionansweringsystems
together.In
Proceedingsofthe2018WorldWideWebConferenceonWorldWideWeb
,pages
1247Œ1256.InternationalWorldWideWebConferencesSteeringCommittee,2018.
22.
R.SpeckandA.-C.NgongaNgomo.Ensemblelearningfornamedentityrecognition.In
TheSemanticWebŒISWC2014
,volume8796of
LectureNotesinComputerScience
,pages
519Œ534.SpringerInternationalPublishing,2014.
23.
P.Trivedi,G.Maheshwari,M.Dubey,andJ.Lehmann.Lc-quad:Acorpusforcomplex
questionansweringoverknowledgegraphs.In
InternationalSemanticWebConference
,pages
210Œ218.Springer,2017.
24.
R.J.Trudeau.Introductiontographtheory(corrected,enlargedrepublication.ed.),1993.
25.
C.Unger,L.Bühmann,J.Lehmann,A.-C.NgongaNgomo,D.Gerber,andP.Cimiano.
Template-basedquestionansweringoverrdfdata.In
Proceedingsofthe21stinternational
conferenceonWorldWideWeb
,pages639Œ648.ACM,2012.
26.
R.Usbeck,A.-C.N.Ngomo,M.Röder,D.Gerber,S.A.Coelho,S.Auer,andA.Both.
Agdistis-graph-baseddisambiguationofnamedentitiesusinglinkeddata.In
International
SemanticWebConference
,pages457Œ471.Springer,2014.
27.
A.P.B.Veyseh.Cross-lingualquestionansweringusingcommonsemanticspace.In
TextGraphs@NAACL-HLT
,pages15Œ19,2016.
28.
K.Xu,S.Zhang,Y.Feng,andD.Zhao.Answeringnaturallanguagequestionsviaphrasal
semanticparsing.In
NaturalLanguageProcessingandChineseComputing
.Springer,2014.
29.
Y.YangandM.-W.Chang.S-mart:Noveltree-basedstructuredlearningalgorithmsapplied
totweetentitylinking.In
ACL2015
,2015.
30.
W.-t.Yih,M.-W.Chang,X.He,andJ.Gao.Semanticparsingviastagedquerygraph
generation:Questionansweringwithknowledgebase.In
Proceedingsofthe53rdACLConf.
,
volume1,pages1321Œ1331,2015.
"
45,Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning,http://arxiv.org/pdf/1806.06193v1.pdf,https://github.com/richardaecn/cvpr18-inaturalist-transfer,"LargeScaleFine-GrainedCategorizationandTransfer
Learning
YinCui
1
;
2

YangSong
3
ChenSun
3
AndrewHoward
3
SergeBelongie
1
;
2
1
DepartmentofComputerScience,CornellUniversity
2
CornellTech
3
GoogleResearch
Abstract
Transferringtheknowledgelearnedfromlargescale
datasets(
e.g
.,ImageNet)viaoffersaneffective
solutionforainedvisualcategoriza-
tion(FGVC)tasks(
e.g
.,recognizingbirdspeciesorcar
make&model).Insuchscenarios,dataannotationoften
callsforspecializeddomainknowledgeandthusisdif
toscale.Inthiswork,westtackleaprobleminlargescale
FGVC.OurmethodwonstplaceiniNaturalist2017large
scalespecieschallenge.Centraltothesuc-
cessofourapproachisatrainingschemethatuseshigher
imageresolutionanddealswiththelong-taileddistribu-
tionoftrainingdata.Next,westudytransferlearningvia
fromlargescaledatasetstosmallscale,domain-
FGVCdatasets.Weproposeameasuretoestimate
domainsimilarityviaEarthMover'sDistanceanddemon-
stratethattransferlearningfrompre-trainingona
sourcedomainthatissimilartothetargetdomainbythis
measure.OurproposedtransferlearningoutperformsIm-
ageNetpre-trainingandobtainsstate-of-the-artresultson
multiplecommonlyusedFGVCdatasets.
1.Introduction
Fine-grainedvisualcategorization(FGVC)aimstodis-
tinguishsubordinatevisualcategories.Examplesinclude
recognizingnaturalcategoriessuchasspeciesofbirds[
58
,
54
],dogs[
28
]andplants[
39
,
59
];orman-madecategories
suchascarmake&model[
32
,
63
].AsuccessfulFGVC
modelshouldbeabletodiscriminatecategorieswithsubtle
differences,whichpresentsformidablechallengesforthe
modeldesignyetalsoprovidesinsightstoawiderangeof
applicationssuchasrichimagecaptioning[
3
],imagegen-
eration[
5
],andmachineteaching[
27
,
37
].
RecentadvancesonConvolutionalNeuralNetworks
(CNNs)forvisualrecognition[
33
,
48
,
51
,
20
]havefu-
eledremarkableprogressonFGVC[
36
,
11
,
69
].Ingen-
eral,toachievereasonablygoodperformancewithCNNs,

WorkdoneduringinternshipatGoogleResearch.
Figure1.Overviewoftheproposedtransferlearningscheme.
Giventhetargetdomainofinterest,wepre-trainaCNNonthe
selectedsubsetfromthesourcedomainbasedontheproposeddo-
mainsimilaritymeasure,andthenonthetargetdomain.
oneneedstotrainnetworkswithvastamountsofsupervised
data.However,collectingalabeleddatasetof-
tenrequiresexpert-leveldomainknowledgeandtherefore
isdiftoscale.Asaresult,commonlyusedFGVC
datasets[
58
,
28
,
32
]arerelativelysmall,typicallycontain-
ingaround10koflabeledtrainingimages.Insuchasce-
nario,thenetworksthatarepre-trainedonlarge
scaledatasetssuchasImageNet[
12
]isoftenadopted.
Thiscommonsetupposestwoquestions:1)Whatare
theimportantfactorstoachievegoodperformanceonlarge
scaleFGVC?Althoughotherlargescalegenericvisual
datasetslikeImageNetcontainsomecate-
gories,theirimagesareusuallyiconicwebimagesthat
containobjectsinthecenterwithsimilarscaleandsimple
backgrounds.Withthelimitedavailabilityoflargescale
FGVCdatasets,howtodesignmodelsthatperformwell
onlargescalenon-iconicimageswithcate-
goriesremainsanunderdevelopedarea.2)Howdoesone
effectivelyconducttransferlearning,bytrainingthe
networkonalargescaledatasetandthenit
ondatasets?ModernFGVC
methodsoverwhelminglyuseImageNetpre-trainednet-
worksforGiventhefactthatthetarget
graineddomainisknown,canwedobetterthanImageNet?
Thispaperaimstoanswerthetwoaforementionedprob-
lems,withtherecentlyintroducediNaturalist2017large
scaleddataset(iNat)[
55
].iNatcontains675,170
1
arXiv:1806.06193v1  [cs.CV]  16 Jun 2018trainingandvalidationimagesfrom5,089cate-
gories.Allimageswerecapturedinnaturalconditionswith
variedobjectscalesandbackgrounds.Therefore,iNatof-
fersagreatopportunitytoinvestigatekeyfactorsbehind
trainingCNNsthatperformwellonlargescaleFGVC.In
addition,alongwithImageNet,iNatenablesustostudy
thetransferofknowledgelearnedonlargescaledatasets
tosmallscaledomains.
Inthiswork,weproposeatrainingschemefor
largescalecategorization,achievingtopper-
formanceoniNat.UnlikeImageNet,imagesiniNathave
muchhigherresolutionsandawiderangeofobjectscales.
WeshowinSec.
3.1
thatperformanceoniNatcanbeim-
provedwithhigherinputimageresolution.
Anotherissueweaddressinthispaperisthelong-tailed
distribution,whereafewcategorieshavemostoftheim-
ages[
71
,
56
].Todealwiththis,wepresentasimpleyet
effectiveapproach.Theideaistolearngoodfeaturesfrom
alargeamountoftrainingdataandthenona
moreevenly-distributedsubsettobalancethenetwork'sef-
fortsamongallcategoriesandtransferthelearnedfeatures.
Ourexperimentalresults,showninSec.
3.2
,revealthatwe
cangreatlyimprovetheunder-representedcategoriesand
achievebetteroverallperformance.
Secondly,westudyhowtotransferfromknowledge
learnedonlargescaledatasetstosmallscale
domains.Datasetsareoftenbiasedintermsoftheirstatis-
ticsoncontentandstyle[
53
].OnCUB200Birds[
58
],iNat
pre-trainednetworksperformmuchbetterthanImageNet
pre-trainedones;whereasonStanford-Dogs[
28
],ImageNet
pre-trainednetworksyieldbetterperformance.Thisisbe-
causetherearemorevisuallysimilarbirdcategoriesiniNat
anddogcategoriesinImageNet.Inlightofthis,wepro-
poseanovelwaytomeasurethevisualsimilaritybetween
sourceandtargetdomainsbasedonimage-levelvisualsim-
ilaritywithEarthMover'sDistance.Bythenet-
workstrainedonselectedsubsetsbasedonourproposed
domainsimilarity,weachievebettertransferlearningthan
ImageNetpre-trainingandstate-of-the-artresultsoncom-
monlyuseddatasets.Fig.
1
givesanoverview
oftheproposedtrainingscheme.
WebelieveourstudyonlargescaleFGVCanddomain-
transferlearningcouldofferusefulguidelinesfor
researchersworkingonsimilarproblems.
2.RelatedWork
Fine-GrainedVisualCategorization(FGVC)
.Recent
FGVCmethodstypicallyincorporateuseful
informationintoaCNNandtrainthenetworkend-to-
end.Notably,secondorderbilinearfeatureinteractionswas
showntobeveryeffective[
36
].Thisideawaslaterex-
tendedtocompactbilinearpooling[
17
],andthenhigher
orderinteractions[
11
,
9
,
47
].Tocapturesubtlevisual
differences,visualattention[
60
,
16
,
69
]anddeepmetric
learning[
45
,
10
]areoftenused.Beyondpixels,wealso
leverageotherinformationincludingparts[
66
,
7
,
67
],at-
tributes[
57
,
18
],humaninteractions[
8
,
13
]andtextde-
scriptions[
42
,
22
].Todealwiththelackoftrainingdatain
FGVC,additionalwebimagescanbecollectedtoaugment
theoriginaldataset[
10
,
31
,
62
,
18
].Ourapproachdiffers
fromthembytransferringthepre-trainednetworkonexist-
inglargescaledatasetswithoutcollectingnewdata.
Usinghigh-resolutionimagesforFGVChasbecamein-
creasinglypopular[
26
,
36
].Thereisalsoasimilartrend
inImageNetvisualrecognition,fromoriginally
224

224
inAlexNet[
33
]to
331

331
inrecentlyproposedNAS-
Net[
72
].However,nopriorworkhassystematicallystud-
iedtheeffectofimageresolutiononlargescale
datasetsaswedointhispaper.
Howtodealwithlong-taileddistributionisanimpor-
tantprobleminrealworlddata[
71
,
56
].However,itis
aratherunexploredareamainlybecausecommonlyused
benchmarkdatasetsarepre-processedtobeclose-toevenly
distributed[
12
,
34
].VanHorn
etal
.[
56
]pointedoutthatthe
performanceoftailcategoriesaremuchpoorerthanhead
categoriesthathaveenoughtrainingdata.Wepresentasim-
pletwo-steptrainingschemetodealwithlong-taileddistri-
butionthatworkswellinpractice.
TransferLearning
.ConvolutionalNeuralNetworks
(CNNs)trainedonImageNethavebeenwidelyusedfor
transferlearning,eitherbydirectlyusingthepre-trained
networkasafeatureextractor[
46
,
14
,
70
],or
thenetwork[
19
,
40
].Duetotheremarkablesuccessof
usingpre-trainedCNNsfortransferlearning,extensive
effortshavebeenmadeonunderstandingtransferlearn-
ing[
64
,
4
,
24
,
49
].Inparticular,somepriorworkloosely
demonstratedtheconnectionbetweentransferlearningand
domainsimilarity.Forexample,transferlearningbetween
tworandomsplitsiseasierthannatural/man-madeob-
jectsplitsinImageNet[
64
];manuallyadding
512
addi-
tionalrelevantcategoriesfromallavailableclassesimprove
uponthecommonlyused
1000
ImageNetclassesonPAS-
CALVOC[
15
];transferringfromacombinedImageNet
andPlacesdatasetyieldsbetterresultsonalistofvisual
recognitiontasks[
70
].Azizpour
etal
.[
4
]conductedause-
fulstudyonalistoftransferlearningtasksthathavediffer-
entsimilaritywiththeoriginalImageNettask
(
e.g
.,imageisconsideredtobemoresimilar
thaninstanceretrieval,
etc
.).Ourmajordifferencesbetween
theirworkaretwo-fold:Firstly,weprovideawaytoquan-
tifythesimilaritybetweensourceandtargetdomainand
thenchooseamoresimilarsubsetfromsourcedomainfor
bettertransferlearning.Secondly,theyallusepre-trained
CNNsasfeatureextractorsandonlytraineitherthelast
layerorusealinearSVMontheextractedfeatures,whereas
weallthelayersofthenetwork.
3.LargeScaleFine-GrainedCategorization
Inthissection,wepresentourtrainingschemethat
achievestopperformanceonthechallengingiNaturalist
2017dataset,especiallyfocusingonusinghigherimageres-
olutionanddealingwithlong-taileddistribution.
3.1.TheEffectofImageResolution
WhentrainingaCNN,fortheeaseofnetworkdesignand
traininginbatches,theinputimageisusuallypre-processed
tobesquarewithacertainsize.Eachnetworkarchitecture
usuallyhasadefaultinputsize.Forexample,AlexNet[
33
]
andVGGNet[
48
]takethedefaultinputsizeof
224

224
andthisdefaultinputsizecannotbeeasilychangedbe-
causethefully-connectedlayerafterconvolutionsrequires
aedsizefeaturemap.Morerecentnetworksincluding
ResNet[
20
]andInception[
51
,
52
,
50
]arefullyconvolu-
tional,withaglobalaveragepoolinglayerrightaftercon-
volutions.Thisdesignenablesthenetworktotakeinput
imageswitharbitrarysizes.Imageswithdifferentresolu-
tioninducefeaturemapsofdifferentdown-sampledsizes
withinthenetwork.
Inputimageswithhigherresolutionsusuallycontain
richerinformationandsubtledetailsthatareimportantto
visualrecognition,especiallyforFGVC.Therefore,ingen-
eral,higherresolutioninputimageyieldsbetterperfor-
mance.FornetworksoptimizedonImageNet,thereisa
trendofusinginputimageswithhigherresolutionformod-
ernnetworks:fromoriginally
224

224
inAlexNet[
33
]to
331

331
inrecentlyproposedNASNet[
72
],asshownin
Table
3
.However,mostimagesfromImageNethaveares-
olutionof
500

375
andcontainobjectsofsimilarscales,
limitingthewecangetfromusinghigherresolu-
tioninputs.Weexploretheeffectofusingawiderange
ofinputimagesizesfrom
299

299
to
560

560
iniNat
dataset,showinggreatlyimprovedperformancewithhigher
resolutioninputs.
3.2.ailedDistribution
Thestatisticsofrealworldimagesislong-tailed:afew
categoriesarehighlyrepresentativeandhavemostofthe
images,whereasmostcategoriesareobservedrarelywith
onlyafewimages[
71
,
56
].Thisisinstarkcontrasttothe
evenimagedistributioninpopularbenchmarkdatasetssuch
asImageNet[
12
],COCO[
34
]andCUB200[
58
].
Withhighlyimbalancednumbersofimagesacrosscat-
egoriesiniNaturalistdataset[
55
],weobservepoorperfor-
manceonunderrepresentedtailcategories.Wearguethat
thisismainlycausedbytworeasons:1)Thelackoftraining
data.Around1,500categoriesiniNattraining
sethavefewerthan30images.2)Theextremeclassim-
balanceencounteredduringtraining:theratiobetweenthe
numberofimagesinthelargestclassandthesmallestoneis
InputRes.
Networks
224

224
AlexNet[
33
],VGGNet[
48
],ResNet[
20
]
299

299
Inception[
51
,
52
,
50
]
320

320
ResNetv2[
21
],ResNeXt[
61
],SENet[
23
]
331

331
NASNet[
72
]
Table1.Defaultinputimageresolutionfordifferentnetworks.
Thereisatrendofusinginputimageswithhigherresolutionfor
modernnetworks.
Figure2.Thedistributionofimagefrequencyofeachcategoryin
thewholetrainingsetweusedinthestagetrainingandthe
selectedsubsetweusedinthesecondstage
about435.Withoutanyre-samplingofthetrainingimages
orre-weightingoftheloss,categorieswithmoreimagesin
theheadwilldominatethoseinthetail.Sincethereisvery
littlewecandofortheissueoflackoftrainingdata,
weproposeasimpleandeffectivewaytoaddressthesec-
ondissueoftheclassimbalance.
Theproposedtrainingschemehastwostages.Inthe
stage,wetrainthenetworkasusualontheoriginalimbal-
anceddataset.Withlargenumberoftrainingdatafromall
categories,thenetworklearnsgoodfeaturerepresentations.
Then,inthesecondstage,wethenetworkona
subsetcontainingmorebalanceddatawithasmalllearning
rate.Theideaistoslowlytransferthelearnedfeatureandlet
thenetworkre-balanceamongallcategories.Fig.
2
shows
thedistributionofimagefrequencyiniNattrainingsetthat
wetrainedoninthestageandthesubsetweusedinthe
secondstage,respectively.ExperimentsinSec.
5.2
verify
thattheproposedstrategyyieldsimprovedoverallperfor-
mance,especiallyforunderrepresentedtailcategories.
4.TransferLearning
Thissectiondescribestransferlearningfromthenet-
workstrainedonlargescaledatasetstosmallscale
graineddatasets.Weintroduceawaytomeasurevisualsim-
ilaritybetweentwodomainsandthenshowhowtoselecta
subsetfromsourcedomaingiventhetargetdomain.
4.1.DomainSimilarity
Supposewehaveasourcedomain
S
andatargetdomain
T
.Wethedistancebetweentwoimages
s
2S
and
t
2T
astheEuclideandistancebetweentheirfeaturerep-
resentations:
d
(
s;t
)=
k
g
(
s
)

g
(
t
)
k
(1)
where
g
(

)
denotesafeatureextractorforanimage.Tobet-
tercapturetheimagesimilarity,thefeatureextractor
g
(

)
needstobecapableofextractinghigh-levelinformation
fromimagesinageneric,unbiasedmanner.Therefore,in
ourexperiments,weuse
g
(

)
asthefeaturesextractedfrom
thepenultimatelayerofaResNet-101trainedonthelarge
scaleJFTdataset[
49
].
Ingeneral,usingmoreimagesyieldsbettertransfer
learningperformance.Forthesakeofsimplicity,inthis
studyweignoretheeffectofdomainscale(numberofim-
ages).,wenormalizethenumberofimages
inbothsourceandtargetdomain.AsstudiedbyChen
et
al
.[
49
],transferlearningperformanceincreaseslogarith-
micallywiththeamountoftrainingdata.Thissuggeststhat
theperformancegainintransferlearningresultingfromthe
useofmoretrainingdatawouldbewhenweal-
readyhavealargeenoughdataset(
e.g
.,ImageNet).There-
fore,ignoringthedomainscaleisareasonableassumption
thattheproblem.Ourtionofdomainsimi-
laritycanbegeneralizedtotakedomainscaleintoaccount
byaddingascalefactor,butwefoundignoringthedomain
scalealreadyworkswellinpractice.
Underthisassumption,transferlearningcanbeviewed
asmovingasetofimagesfromthesourcedomain
S
tothe
targetdomain
T
.Theworkneededtobedonebymoving
animagetoanothercanbeastheirimagedistance
inEqn.
1
.Thenthedistancebetweentwodomainscanbe
astheleastamountoftotalworkneeded.Thisdef-
initionofdomainsimilaritycanbecalculatedbytheEarth
Mover'sDistance(EMD)[
41
,
43
].
Tomakethecomputationsmoretractable,wefurther
makeanadditionaltorepresentallimagefea-
turesinacategorybythemeanoftheirfeatures.Formally,
wedenotesourcedomainas
S
=
f
(
s
i
;w
s
i
)
g
m
i
=1
andtarget
domainas
T
=
f
(
t
j
;w
t
j
)
g
n
j
=1
,where
s
i
isthe
i
-thcate-
goryin
S
and
w
s
i
isthenormalizednumberofimagesin
thatcategory;similarlyfor
t
j
and
w
t
j
in
T
.
m
and
n
are
thetotalnumberofcategoriesinsourcedomain
S
andtar-
getdomain
T
,respectively.Sincewenormalizethenumber
ofimages,wehave
P
m
i
=1
w
s
i
=
P
n
j
=1
w
t
j
=1
.
g
(
s
i
)
de-
notesthemeanofimagefeaturesincategory
i
fromsource
domain,similarlyfor
g
(
t
j
)
intargetdomain.Usingthede-
notations,thedistancebetween
S
and
T
isas
theirEarthMover'sDistance(EMD):
d
(
S
;
T
)=
EMD
(
S
;
T
)=
P
m;n
i
=1
;j
=1
f
i;j
d
i;j
P
m;n
i
=1
;j
=1
f
i;j
(2)
Figure3.TheproposeddomainsimilaritycalculatedbyEarth
Mover'sDistance(EMD).Categoriesinsourcedomainandtar-
getdomainarerepresentedbyredandgreencircles.Thesizeof
thecircledenotesthenormalizednumberofimagesinthatcate-
gory.Bluearrowsrepresentwsfromsourcetotargetdomainby
solvingEMD.
where
d
i;j
=
k
g
(
s
i
)

g
(
t
j
)
k
andtheoptimalw
f
i;j
correspondstotheleastamountoftotalworkbysolvingthe
EMDoptimizationproblem.Finally,thedomainsimilarity
isas:
sim
(
S
;
T
)=
e

d
(
S
;
T
)
(3)
where

issetto0.01inallexperiments.Fig.
3
illustrates
calculatingtheproposeddomainsimilaritybyEMD.
4.2.SourceDomainSelection
WiththedomainsimilarityinEqn.
2
,weareable
toselectasubsetfromsourcedomainthatismoresimilar
totargetdomains.Weusegreedyselectionstrategytoin-
crementallyincludethemostsimilarcategoryinthesource
domain.Thatis,foreachcategory
s
i
insourcedomain
S
,
wecalculateitsdomainsimilaritywithtargetdomainby
sim
(
f
(
s
i
;
1)
g
;
T
)
asinEqn.
3
.Thentop
k
cat-
egorieswithhighestdomainsimilaritieswillbeselected.
Noticethatalthoughthisgreedywayofselectionhasno
guaranteeontheoptimalityoftheselectedsubsetofsize
k
intermsofdomainsimilarity,wefoundthissimplestrategy
workswellinpractice.
5.Experiments
TheproposedtrainingschemeforlargescaleFGVCis
evaluatedontherecentlyproposediNaturalist2017dataset
(iNat)[
55
].Wealsoevaluatetheeffectivenessoftheour
proposedtransferlearningbyusingImageNetandiNatas
sourcedomains,and7categorizationdatasets
astargetdomains.Sec.
5.1
introducesexperimentsetup.
ExperimentresultsoniNatandtransferlearningarepre-
sentedinSec.
3
andSec.
5.3
,respectively.
5.1.Experimentsetup
5.1.1Datasets
iNaturalist
.TheiNatrualist2017dataset(iNat)[
55
]
contains675,170trainingandvalidationimagesfrom5,089
naturalcategories.Thosecategoriesbelongto
13super-categoriesincludingPlantae(Plant),Insecta(In-
sect),Aves(Bird),Mammalia(Mammal),andsoon.The
iNatdatasetishighlyimbalancedwithdramaticallydiffer-
entnumberofimagespercategory.Forexample,thelargest
super-categoryﬁPlantae(Plant)ﬂhas196,613imagesfrom
2,101categories;whereasthesmallestsuper-categoryﬁPro-
tozoaﬂonlyhas381imagesfrom4categories.Wecombine
theoriginalsplitoftrainingsetand90%ofthevalidationset
asourtrainingset(iNattrain),andusetherestof10%vali-
dationsetasourminivalidationset(iNatminival),resulting
intotalof665,473trainingand9,697validationimages.
ImageNet
.WeusetheILSVRC2012[
44
]splitsof
1,281,167training(ImageNettrain)and50,000validation
(ImageNetval)imagesfrom1,000classes.
Fine-GrainedVisualCategorization
.Weevaluateour
transferlearningapproachon7visualcate-
gorizationdatasetsastargetdomains,whichcoverawide
rangeofFGVCtasksincludingnaturalcategorieslikebird
andwerandman-madecategoriessuchasaircraft.Table
2
summarizesnumberofcategories,togetherwithnumber
ofimagesintheiroriginaltrainingandvalidationsplits.
5.1.2NetworkArchitectures
Weuse3typesofnetworkarchitectures:ResNet[
20
,
21
],Inception[
51
,
52
,
50
]andSENet[
23
].
ResidualNetwork(ResNet)
.Originallyintroducedby
He
etal
.[
20
],networkswithresidualconnectionsgreatly
reducedtheoptimizationdifandenabledthetrain-
ingofmuchdeepernetworks.ResNetswerelaterimproved
bypre-activationthatusesidentitymappingastheskipcon-
nectionbetweenresidualmodules[
21
].Weusedthelatest
versionofResNets[
21
]with50,101and152layers.
Inception
.TheInceptionmodulewasproposed
bySzegedy
etal
.inGoogleNet[
51
]thatwasdesigned
tobeveryefintermsofparametersandcomputa-
tions,whileachievingstate-of-the-artperformance.Incep-
tionmodulewasthenfurtheroptimizedbyusingBatchNor-
malization[
25
],factorizedconvolution[
52
,
50
]andresidual
connections[
50
]asintroducedin[
20
].WeuseInception-
v3[
52
],Inception-v4andInception-ResNet-v2[
50
]asrep-
resentativesforInceptionnetworksinourexperiments.
Squeeze-and-Excitation(SE)
.Recentlyproposedby
Hu
etal
.[
23
],Sequeeze-and-Excitation(SE)modules
achievedthebestperformanceinILSVRC2017[
44
].SE
modulesqueezesresponsesfromafeaturemapbyspatial
averagepoolingandthenlearnstore-scaleeachchannelof
FGVCDataset
#class
#train
#val
Flowers-102[
39
]
102
2,040
6,149
CUB200Birds[
58
]
200
5,994
5,794
Aircraft[
38
]
100
6,667
3,333
StanfordCars[
32
]
196
8,144
8,041
StanfordDogs[
28
]
120
12,000
8,580
NABirds[
54
]
555
23,929
24,633
Food101[
6
]
101
75,750
25,250
Table2.Weuse7visualcategorizationdatasetsto
evaluatetheproposedtransferlearningmethod.
Inc-v3299
Inc-v3448
Inc-v3560
Top-1(%)
29.93
26.51
25.37
Top-5(%)
10.61
9.02
8.56
Table3.Top-5errorrateoniNatminivalusingInception-v3with
variousinputsizes.Higherinputsizeyieldbetterperformance.
thefeaturemap.Duetoitssimplicityindesign,SEmodule
canbeusedinalmostanymodernnetworkstoboosttheper-
formancewithlittleadditionaloverhead.WeuseInception-
v3SEandInception-ResNet-v2SEasbaselines.
Forallnetworkarchitectures,wefollowstrictlytheir
originaldesignbutwiththelastlinearlayer
replacedtomatchthenumberofcategoriesinourdatasets.
5.1.3Implementation
Weusedopen-sourceTw[
2
]toimplementand
trainallthemodelsasynchronouslyonmultipleNVIDIA
TeslaK80GPUs.Duringtraining,theinputimagewas
randomlycroppedfromtheoriginalimageandre-sizedto
thetargetinputsizewithscaleandaspectratioaugmenta-
tion[
51
].WetrainedallnetworksusingtheRMSPropopti-
mizerwithmomentumof0.9,andthebatchsizeof32.The
initiallearningratewassetto0.045,withexponentialdecay
of0.94afterevery2epochs,sameas[
51
];for
intransferlearning,theinitiallearningrateisloweredto
0.0045withthelearningratedecayof0.94afterevery4
epochs.Wealsousedlabelsmoothingasintroducedin[
52
].
Duringinference,theoriginalimageiscentercroppedand
re-sizedtothetargetinputsize.
5.2.LargeScaleVisualRecognition
Toverifytheproposedlearningschemeforlargescale
categorization,weconductextensiveexperi-
mentsoniNaturalist2017dataset.Forbetterperformance,
wefromImageNetpre-trainednetworks.Iftrain-
ingfromscratchoniNat,thetop-5errorrateis
ˇ
1%
worse.
WetrainInception-v3with3differentinputresolutions
(299,448and560).Theeffectofimageresolutionispre-
sentedinTable
3
.Fromthetable,wecanseethatusing
higherinputresolutionsachievebetterperformanceoniNat.
Figure4.Top-5errorrateoniNatminivalbeforeandafter
tuningonamorebalancedsubset.Thissimplestrategyimproves
theperformanceonlong-tailediNatdataset.
Theevaluationofourproposedschemefor
dealingwithlong-taileddistributionispresentedinFig.
4
.
Betterperformancecanbeobtainedbyfurther
onamorebalancedsubsetwithsmalllearningrate(
10

6
inourexperiments).Table
4
showsperformanceimprove-
mentsonheadandtailcategorieswithIm-
provementsonheadcategorieswith

100
trainingimages
are1.95%oftop-1and0.92%oftop-5;whereasontailcat-
egorieswith
<
100
trainingimages,theimprovementsare
5.74%oftop-1and2.71%oftop-5.Theseresultsverify
thattheproposedschemegreatlyimprovesthe
performanceonunderrepresentedtailcategories.
Table
5
presentsthedetailedperformancebreakdownof
ourwinningentryintheiNaturalist2017challenge[
1
].Us-
inghigherimageresolutionandfurtherona
morebalancedsubsetarethekeytooursuccess.
5.3.DomainSimilarityandTransferLearning
Weevaluatetheproposedtransferlearningmethodby
pre-trainingthenetworkonsourcedomain
fromscratch
,
andthenontargetdomainsforvi-
sualcategorization.OtherthantrainingseparatelyonIm-
ageNetandiNat,wealsotrainnetworksonacombinedIm-
ageNet+iNatdatasetthatcontains1,946,640trainingim-
agesfrom6,089categories(
i.e
.,1,000fromImageNetand
5,089fromiNat).Weuseinputsizeof
299

299
forall
networks.Table
6
showsthepre-trainingperformanceeval-
uatedonImageNetvalandiNatminival.Notably,asingle
networktrainedonthecombinedImageNet+iNatdataset
achievescompetitiveperformancecomparedwithtwomod-
elstrainedseparately.Ingeneral,combinedtrainingisbet-
terthantrainingseparatelyinthecaseofInceptionandIn-
ceptionSE,butworseinthecaseofResNet.
Basedontheproposeddomainselectionstrategy
inSec.
4.2
,weselectthefollowingtwosubsetsfromthe
combinedImageNet+iNatdataset:
SubsetA
waschosen
byincludingtop200ImageNet+iNatcategoriesforeach
ofthe7FGVCdataset.Removingduplicatedcategoriesre-
sultedinasourcedomaincontaining832categories.
Subset
B
wasselectedbyaddingmostsimilar400categoriesfor
BeforeFT
AfterFT
Top-1
Top-5
Top-1
Top-5
Head:

100
imgs
19.28
5.79
17.33
4.87
Tail:
<
100
imgs
29.89
9.12
24.15
6.41
Table4.Top-1andtop-5errorrates(%)oniNatminivalfor
Inception-v4560.Theproposedschemegreatlyim-
provestheperformanceonunderrepresentedtailcategories.
Network
Top-1(%)
Top-5(%)
Inc-v3299
29.9
10.6
Inc-v3560
25.4(+4.5)
8.6(+2.0)
Inc-v3560FT
22.7(+2.7)
6.6(+2.0)
Inc-v4560FT
20.8(+1.9)
5.4(+1.2)
Inc-v4560FT12-crop
19.2(+1.6)
4.7(+0.7)
Ensemble
18.1(+1.1)
4.1(+0.6)
Table5.PerformanceimprovementsoniNatminival.Thenumber
insidethebracketsindicatestheimprovementoverthemodelin
thepreviousrow.FTdenotesusingtheproposedto
dealwithlong-taileddistribution.Ensemblecontainstwomodels:
Inc-v4560FTandInc-ResNet-v2560FTwith12-crop.
CUB200,NABirds,top100categoriesforStanfordDogs
andtop50categoriesforStanfordCarsandAircraft,which
gaveus585categoriesintotal.Fig.
6
showstop10most
similarcategoriesinImageNet+iNatforallFGVCdatasets
calculatedbyourproposeddomainsimilarity.It'sclearto
seethatforCUB200,Flowers-102andNABirds,mostsim-
ilarcategoriesarefromiNat;whereasforStanfordDogs,
StanfordCars,AircraftandFood101,mostsimilarcate-
goriesarefromImageNet.Thisindicatesthestrongdataset
biasinbothImageNetandiNat.
Thetransferlearningperformancebyan
Inception-v3ondatasetsarepresentedinTable
7
.WecanseethatbothImageNetandiNatarehighlybi-
ased,achievingdramaticallydifferenttransferlearningper-
formanceontargetdatasets.Interestingly,whenwetrans-
fernetworkstrainedonthecombinedImageNet+iNat
dataset,performancearein-betweenImageNetandiNat
pre-training,indicatingthatwecannotachievegoodper-
formanceontargetdomainsbysimplyusingalargerscale,
combinedsourcedomain.
Further,inFig.
5
,weshowtherelationshipbetween
transferlearningperformanceandourproposeddomain
similarity.Weobservebettertransferlearningperformance
whenunedfromamoresimilarsourcedomain,except
Food101,onwhichthetransferlearningperformanceal-
moststayssameasdomainsimilaritychanges.Webelieve
thisislikelyduetothelargenumberoftrainingimagesin
Food101(750trainingimagesperclass).Therefore,thetar-
getdomaincontainsenoughdatathustransferlearninghas
verylittlehelp.Insuchascenario,ourassumptiononig-
noringthescaleofdomainisnolongervalid.
ImageNetval
iNaturalistminival
Original
SeparateTrain
CombinedTrain
SeparateTrain
CombinedTrain
top-1
top-5
top-1
top-5
top-1
top-5
top-1
top-5
top-1
top-5
ResNet-50[
20
,
21
]
24.70
7.80
24.33
7.61
25.23
8.06
36.23
15.67
36.93
16.49
ResNet-101[
20
,
21
]
23.60
7.10
23.08
7.09
23.39
7.06
34.15
14.58
33.97
14.53
ResNet-152[
20
,
21
]
23.00
6.70
22.34
6.81
22.59
6.64
31.04
12.52
32.58
13.20
Inception-v3[
52
]
21.20
5.60
21.73
5.97
21.52
5.87
31.18
11.90
30.29
11.10
Inception-ResNet-v2[
50
]
19
:
90

4
:
90

20.33
5.16
20.20
5.18
27.53
9.87
27.78
9.12
Inception-v3SE[
23
]
-
-
20.98
5.76
20.75
5.69
30.15
11.69
29.79
10.64
Inception-ResNet-v2SE[
23
]
19.80
4.79
19.77
4.79
19.56
4.61
27.30
9.61
26.01
8.18
Table6.Pre-trainingperformanceondifferentsourcedomains.NetworkstrainedonthecombinedImageNet+iNatdatasetwith6,089
classesachievecompetitiveperformanceonbothImageNetandiNatcomparedwithnetworkstrainedseparatelyoneachdataset.

indicates
themodelwasevaluatedonthenon-blacklistedsubsetofImageNetvalidationsetthatmayslightlyimprovetheperformance.
CUB200
StanfordDogs
Flowers-102
StanfordCars
Aircraft
Food101
NABirds
ImageNet
82.84
84.19
96.26
91.31
85.49
88.65
82.01
iNat
89.26
78.46
97.64
88.31
82.61
88.80
87.91
ImageNet+iNat
85.84
82.36
97.07
91.38
85.21
88.45
83.98
SubsetA(832-class)
86.37
84.69
97.65
91.42
86.28
88.78
84.79
SubsetB(585-class)
88.76
85.23
97.37
90.58
86.13
88.37
87.89
Table7.Transferlearningperformanceon7FGVCdatasetsbytheInception-v3299pre-trainedondifferentsourcedomains.
Eachrowrepresentsanetworkpre-trainedonasourcedomain,andeachcolumnshowsthetop-1imageaccuracyby
differentnetworksonatargetdataset.RelativegoodandpoorperformanceoneachFGVCdatasetaremarkedby
greenandred,respectively.TwoselectedsubsetsbasedondomainsimilarityachievegoodperformanceonallFGVCdatasets.
Figure5.Therelationshipbetweentransferlearningperformance
anddomainsimilaritybetweensourceandtargetdomain.Each
linerepresentsatargetFGVCdatasetandeachmarkerrepresents
thesourcedomain.Bettertransferlearningperformancecanbe
achievedbythenetworkthatispre-trainedonamore
similarsourcedomain.Twoselectedsubsetsbasedonourdomain
similarityachievegoodperformanceonallFGVCdatasets.
FromTable
7
andFig.
5
,weobservethattheselected
SubsetBachievesgoodperformanceamongallFGVC
datasets,surpassingImageNetpre-trainingbyalargemar-
ginonCUB200andNABirds.InTable
8
,wecompareour
approachwithexistingFGVCmethods.Resultsdemon-
stratestate-of-the-artperformanceoftheprposedtransfer
learningmethodoncommonlyusedFGVCdatasets.Notice
thatsinceourofdomainsimilarityisfasttocom-
pute,wecaneasilyexploredifferentwaystoselectasource
domain.Thetransferlearningperformancecanbedirectly
estimatedbasedondomainsimilarity,withoutconducting
anypre-trainingandPriortoourwork,the
onlyoptionstoachievegoodperformanceonFGVCtasks
areeitherdesigningbettermodelsbasedonImageNet
tuning[
36
,
11
,
69
]oraugmentingthedatasetbycollecting
moreimages[
62
,
31
].Ourwork,however,providesanovel
directionofusingamoresimilarsourcedomaintopre-train
thenetwork.Weshowthatwithproperlyselectedsubsets
insourcedomain,itisabletomatchorexceedthoseperfor-
mancegainbysimplyoff-the-shelfnetworks.
6.Conclusions
Inthiswork,wehavepresentedatrainingschemethat
achievestopperformanceonlargescaleiNaturalistdataset,
byusinghigherresolutioninputimageandto
dealwithlong-taileddistribution.Wefurtherproposed
anovelwayofcapturingdomainsimilaritywithEarth
Mover'sDistanceandshowedbettertransferlearningper-
formancecanbeachievedbyfromamoresim-
ilardomain.Inthefuture,weplantostudyotherimportant
factorsintransferlearningbeyonddomainsimilarity.
Acknowledgments.
Thisworkwassupportedinpartbya
GoogleFocusedResearchAward.Wewouldliketothank
ourcolleaguesatGoogleforhelpfuldiscussions.
Figure6.Examplesshowingtop10mostsimilarcategoriesinthecombinedImageNet+iNatforeachFGVCdataset,calculatedwithour
proposeddomainsimilarity.Theleftcolumnrepresents7FGVCtargetdomains,eachbyarandomlychosenimagefromthedataset.Each
rowshowstop10mostsimilarcategoriesinImageNet+iNatforaFGVCtargetdomain.Werepresentacategorybyonerandomly
chosenimagefromthatcategory.ImageNetcategoriesaremarkedinblue,whereasiNatcategoriesareinred.
Method
CUB200
StanfordDogs
StanfordCars
Aircrafts
Food101
SubsetB(585-class):Inception-v3
89.6
86.3
93.1
89.6
90.1
SubsetB(585-class):Inception-ResNet-v2SE
89.3
88.0
93.5
90.7
90.4
Krause
etal
.[
30
]
82.0
-
92.6
-
-
Bilinear-CNN[
36
]
84.1
-
91.3
84.1
82.4
CompactBilinearPooling[
17
]
84.3
-
91.2
84.1
83.2
Zhang
etal
.[
68
]
84.5
72.0
-
-
-
Low-rankBilinearPooling[
29
]
84.2
-
90.9
87.3
-
KernelPooling[
11
]
86.2
-
92.4
86.9
85.5
RA-CNN[
16
]
85.3
87.3
92.5
-
-
ImprovedBilinear-CNN[
35
]
85.8
-
92.0
88.5
-
MA-CNN[
69
]
86.5
-
92.8
89.9
-
DLA[
65
]
85.1
-
94.1
92.6
89.7
Table8.Comparisontoexistingstate-of-the-artFGVCmethods.Asaconvention,weusesame
448

448
inputsize.Sincewedidn't
recentproposedFGVCmethodsappliedtoFlowers-102andNABirds,weonlyshowcomparisonsontherestof5datasets.Ourproposed
transferlearningapproachisabletoachievestate-of-the-artperformanceonallFGVCdatasets,especiallyonCUB200andNABirds.
References
[1]
Theinaturalist2017largescalespeciesclas
tionchallenge.
https://www.kaggle.com/c/
inaturalist-challenge-at-fgvc-2017
.
6
[2]
M.Abadi,P.Barham,J.Chen,Z.Chen,A.Davis,J.Dean,
M.Devin,S.Ghemawat,G.Irving,M.Isard,etal.Tensor-
w:Asystemforlarge-scalemachinelearning.In
OSDI
,
2016.
5
[3]
L.AnneHendricks,S.Venugopalan,M.Rohrbach,
R.Mooney,K.Saenko,andT.Darrell.Deepcomposi-
tionalcaptioning:Describingnovelobjectcategorieswith-
outpairedtrainingdata.In
CVPR
,2016.
1
[4]
H.Azizpour,A.S.Razavian,J.Sullivan,A.Maki,and
S.Carlsson.Factorsoftransferabilityforagenericconvnet
representation.
PAMI
,2016.
2
[5]
J.Bao,D.Chen,F.Wen,H.Li,andG.Hua.Cvae-gan:Fine-
grainedimagegenerationthroughasymmetrictraining.In
ICCV
,2017.
1
[6]
L.Bossard,M.Guillaumin,andL.VanGool.Food-101Œ
miningdiscriminativecomponentswithrandomforests.In
ECCV
,2014.
5
[7]
S.Branson,G.VanHorn,P.Perona,andS.Belongie.Im-
provedbirdspeciesrecognitionusingposenormalizeddeep
convolutionalnets.In
BMVC
,2014.
2
[8]
S.Branson,C.Wah,F.Schroff,B.Babenko,P.Welinder,
P.Perona,andS.Belongie.Visualrecognitionwithhumans
intheloop.
ECCV
,2010.
2
[9]
S.Cai,W.Zuo,andL.Zhang.Higher-orderintegrationof
hierarchicalconvolutionalactivationsfordvisual
categorization.In
ICCV
,2017.
2
[10]
Y.Cui,F.Zhou,Y.Lin,andS.Belongie.Fine-grainedcate-
gorizationanddatasetbootstrappingusingdeepmetriclearn-
ingwithhumansintheloop.In
CVPR
,2016.
2
[11]
Y.Cui,F.Zhou,J.Wang,X.Liu,Y.Lin,andS.Belongie.
Kernelpoolingforconvolutionalneuralnetworks.In
CVPR
,
2017.
1
,
2
,
7
,
8
[12]
J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-
Fei.Imagenet:Alarge-scalehierarchicalimagedatabase.In
CVPR
,2009.
1
,
2
,
3
[13]
J.Deng,J.Krause,M.Stark,andL.Fei-Fei.Leveraging
thewisdomofthecrowdforrecognition.
PAMI
,
2016.
2
[14]
J.Donahue,Y.Jia,O.Vinyals,J.Hoffman,N.Zhang,
E.Tzeng,andT.Darrell.Decaf:Adeepconvolutionalacti-
vationfeatureforgenericvisualrecognition.In
ICML
,2014.
2
[15]
M.Everingham,L.VanGool,C.K.Williams,J.Winn,and
A.Zisserman.Thepascalvisualobjectclasses(voc)chal-
lenge.
IJCV
,2010.
2
[16]
J.Fu,H.Zheng,andT.Mei.Lookclosertoseebetter:recur-
rentattentionconvolutionalneuralnetworkfor
imagerecognition.In
CVPR
,2017.
2
,
8
[17]
Y.Gao,O.Beijbom,N.Zhang,andT.Darrell.Compact
bilinearpooling.In
CVPR
,2016.
2
,
8
[18]
T.Gebru,J.Hoffman,andL.Fei-Fei.Fine-grainedrecogni-
tioninthewild:Amulti-taskdomainadaptationapproach.
In
ICCV
,2017.
2
[19]
R.Girshick,J.Donahue,T.Darrell,andJ.Malik.Richfea-
turehierarchiesforaccurateobjectdetectionandsemantic
segmentation.In
CVPR
,2014.
2
[20]
K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearning
forimagerecognition.In
CVPR
,2016.
1
,
3
,
5
,
7
[21]
K.He,X.Zhang,S.Ren,andJ.Sun.Identitymappingsin
deepresidualnetworks.In
ECCV
,2016.
3
,
5
,
7
[22]
X.HeandY.Peng.Fine-graindimageviacom-
biningvisionandlanguage.In
CVPR
,2017.
2
[23]
J.Hu,L.Shen,andG.Sun.Squeeze-and-excitationnet-
works.
arXivpreprintarXiv:1709.01507
,2017.
3
,
5
,
7
[24]
M.Huh,P.Agrawal,andA.A.Efros.Whatmakesimagenet
goodfortransferlearning?In
NIPSWorkshop
,2016.
2
[25]
S.IoffeandC.Szegedy.Batchnormalization:Accelerating
deepnetworktrainingbyreducinginternalcovariateshift.In
ICML
,2015.
5
[26]
M.Jaderberg,K.Simonyan,A.Zisserman,etal.Spatial
transformernetworks.In
NIPS
,2015.
2
[27]
E.Johns,O.MacAodha,andG.J.Brostow.Becomingthe
expert-interactivemulti-classmachineteaching.In
CVPR
,
2015.
1
[28]
A.Khosla,N.Jayadevaprakash,B.Yao,andF.-F.Li.Novel
datasetforfgvc:Stanforddogs.In
CVPRWorkshop
,2011.
1
,
2
,
5
[29]
S.KongandC.Fowlkes.Low-rankbilinearpoolingfor
grainedIn
CVPR
,2017.
8
[30]
J.Krause,H.Jin,J.Yang,andL.Fei-Fei.Fine-grained
recognitionwithoutpartannotations.In
CVPR
,2015.
8
[31]
J.Krause,B.Sapp,A.Howard,H.Zhou,A.Toshev,
T.Duerig,J.Philbin,andL.Fei-Fei.Theunreasonableeffec-
tivenessofnoisydataforrecognition.In
ECCV
,
2016.
2
,
7
[32]
J.Krause,M.Stark,J.Deng,andL.Fei-Fei.3dobjectrep-
resentationsforcategorization.In
ICCVWork-
shop
,2013.
1
,
5
[33]
A.Krizhevsky,I.Sutskever,andG.E.Hinton.Imagenet
withdeepconvolutionalneuralnetworks.In
NIPS
,2012.
1
,
2
,
3
[34]
T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra-
manan,P.Doll
´
ar,andC.L.Zitnick.Microsoftcoco:Com-
monobjectsincontext.In
ECCV
,2014.
2
,
3
[35]
T.-Y.LinandS.Maji.Improvedbilinearpoolingwithcnns.
In
BMVC
,2017.
8
[36]
T.-Y.Lin,A.RoyChowdhury,andS.Maji.Bilinearcnnmod-
elsforvisualrecognition.In
ICCV
,2015.
1
,
2
,
7
,
8
[37]
O.MacAodha,S.Su,Y.Chen,P.Perona,andY.Yue.Teach-
ingcategoriestohumanlearnerswithvisualexplanations.In
CVPR
,2018.
1
[38]
S.Maji,E.Rahtu,J.Kannala,M.Blaschko,andA.Vedaldi.
Fine-grainedvisualcofaircraft.
arXivpreprint
arXiv:1306.5151
,2013.
5
[39]
M.-E.NilsbackandA.Zisserman.Automatedwerclassi-
overalargenumberofclasses.In
ICVGIP
,2008.
1
,
5
[40]
M.Oquab,L.Bottou,I.Laptev,andJ.Sivic.Learningand
transferringmid-levelimagerepresentationsusingconvolu-
tionalneuralnetworks.In
CVPR
,2014.
2
[41]
S.T.Rachev.ThemongeŒkantorovichmasstransference
problemanditsstochasticapplications.
TheoryofProba-
bility&ItsApplications
,1985.
4
[42]
S.Reed,Z.Akata,H.Lee,andB.Schiele.Learningdeep
representationsofvisualdescriptions.In
CVPR
,
2016.
2
[43]
Y.Rubner,C.Tomasi,andL.J.Guibas.Theearthmover's
distanceasametricforimageretrieval.
IJCV
,2000.
4
[44]
O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,
S.Ma,Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,
etal.Imagenetlargescalevisualrecognitionchallenge.
IJCV
,2015.
5
[45]
F.Schroff,D.Kalenichenko,andJ.Philbin.Facenet:Auni-
embeddingforfacerecognitionandclustering.In
CVPR
,
2015.
2
[46]
A.SharifRazavian,H.Azizpour,J.Sullivan,andS.Carls-
son.Cnnfeaturesoff-the-shelf:anastoundingbaselinefor
recognition.In
CVPRWorkshops
,2014.
2
[47]
M.Simon,Y.Gao,T.Darrell,J.Denzler,andE.Rod-
ner.Generalizedorderlesspoolingperformsimplicitsalient
matching.In
ICCV
,2017.
2
[48]
K.SimonyanandA.Zisserman.Verydeepconvolutional
networksforlarge-scaleimagerecognition.
arXivpreprint
arXiv:1409.1556
,2014.
1
,
3
[49]
C.Sun,A.Shrivastava,S.Singh,andA.Gupta.Revisiting
unreasonableeffectivenessofdataindeeplearningera.In
ICCV
,2017.
2
,
4
[50]
C.Szegedy,S.Ioffe,V.Vanhoucke,andA.A.Alemi.
Inception-v4,inception-resnetandtheimpactofresidual
connectionsonlearning.In
AAAI
,2017.
3
,
5
,
7
[51]
C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,
D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich.
Goingdeeperwithconvolutions.In
CVPR
,2015.
1
,
3
,
5
[52]
C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna.
Rethinkingtheinceptionarchitectureforcomputervision.In
CVPR
,2016.
3
,
5
,
7
[53]
A.TorralbaandA.A.Efros.Unbiasedlookatdatasetbias.
In
CVPR
,2011.
2
[54]
G.VanHorn,S.Branson,R.Farrell,S.Haber,J.Barry,
P.Ipeirotis,P.Perona,andS.Belongie.Buildingabird
recognitionappandlargescaledatasetwithcitizenscientists:
Theprintindatasetcollection.In
CVPR
,
2015.
1
,
5
[55]
G.VanHorn,O.MacAodha,Y.Song,Y.Cui,C.Sun,
A.Shepard,H.Adam,P.Perona,andS.Belongie.Theinat-
uralistspeciesclassianddetectiondataset.In
CVPR
,
2018.
1
,
3
,
4
,
5
[56]
G.VanHornandP.Perona.Thedevilisinthetails:
Fine-grainedclasinthewild.
arXivpreprint
arXiv:1709.01450
,2017.
2
,
3
[57]
A.Vedaldi,S.Mahendran,S.Tsogkas,S.Maji,R.Girshick,
J.Kannala,E.Rahtu,I.Kokkinos,M.B.Blaschko,D.Weiss,
etal.Understandingobjectsindetailwithat-
tributes.In
CVPR
,2014.
2
[58]
C.Wah,S.Branson,P.Welinder,P.Perona,andS.Belongie.
Thecaltech-ucsdbirds-200-2011dataset.
CaliforniaInsti-
tuteofTechnology
,2011.
1
,
2
,
3
,
5
[59]
J.D.Wegner,S.Branson,D.Hall,K.Schindler,andP.Per-
ona.Catalogingpublicobjectsusingaerialandstreet-level
images-urbantrees.In
CVPR
,2016.
1
[60]
T.Xiao,Y.Xu,K.Yang,J.Zhang,Y.Peng,andZ.Zhang.
Theapplicationoftwo-levelattentionmodelsindeepconvo-
lutionalneuralnetworkforimage
In
CVPR
,2015.
2
[61]
S.Xie,R.Girshick,P.Doll
´
ar,Z.Tu,andK.He.Aggregated
residualtransformationsfordeepneuralnetworks.In
CVPR
,
2017.
3
[62]
Z.Xu,S.Huang,Y.Zhang,andD.Tao.Webly-supervised
visualcategorizationviadeepdomainadapta-
tion.
PAMI
,2016.
2
,
7
[63]
L.Yang,P.Luo,C.C.Loy,andX.Tang.Alarge-scalecar
datasetforcategorizationandvIn
CVPR
,2015.
1
[64]
J.Yosinski,J.Clune,Y.Bengio,andH.Lipson.Howtrans-
ferablearefeaturesindeepneuralnetworks?In
NIPS
,2014.
2
[65]
F.Yu,D.Wang,E.Shelhamer,andT.Darrell.Deeplayer
aggregation.In
CVPR
,2018.
8
[66]
N.Zhang,J.Donahue,R.Girshick,andT.Darrell.Part-
basedr-cnnsforcategorydetection.In
ECCV
,
2014.
2
[67]
N.Zhang,E.Shelhamer,Y.Gao,andT.Darrell.Fine-grained
poseprediction,normalization,andrecognition.In
ICLR
Workshops
,2016.
2
[68]
X.Zhang,H.Xiong,W.Zhou,W.Lin,andQ.Tian.Picking
deepresponsesforimagerecognition.In
CVPR
,2016.
8
[69]
H.Zheng,J.Fu,T.Mei,andJ.Luo.Learningmulti-attention
convolutionalneuralnetworkforimagerecog-
nition.In
ICCV
,2017.
1
,
2
,
7
,
8
[70]
B.Zhou,A.Lapedriza,A.Khosla,A.Oliva,andA.Torralba.
Places:A10millionimagedatabaseforscenerecognition.
PAMI
,2017.
2
[71]
X.Zhu,D.Anguelov,andD.Ramanan.Capturinglong-tail
distributionsofobjectsubcategories.In
CVPR
,2014.
2
,
3
[72]
B.Zoph,V.Vasudevan,J.Shlens,andQ.V.Le.Learning
transferablearchitecturesforscalableimagerecognition.In
CVPR
,2018.
2
,
3
"
46,Orthogonal Machine Learning: Power and Limitations,http://arxiv.org/pdf/1711.00342v6.pdf,https://github.com/IliasZadik/double_orthogonal_ml,"OrthogonalMachineLearning:PowerandLimitations
LesterMackey
1
VasilisSyrgkanis
1
IliasZadik
12
Abstract
Doublemachinelearningprovides
p
n
-consistent
estimatesofparametersofinterestevenwhen
high-dimensionalornonparametricnuisancepa-
rametersareestimatedatan
n

1
=
4
rate.Thekey
istoemploy
Neyman-orthogonal
momentequa-
tionswhichareinsensitivetopertur-
bationsinthenuisanceparameters.Weshow
thatthe
n

1
=
4
requirementcanbeimprovedto
n

1
=
(2
k
+2)
byemployinga
k
-thordernotionof
orthogonalitythatgrantsrobustnesstomorecom-
plexorhigher-dimensionalnuisanceparameters.
Inthepartiallylinearregressionsetting,popular
incausalinference,weshowthatwecancon-
structsecond-orderorthogonalmomentsifand
onlyifthetreatmentresidualisnotnormallydis-
tributed.OurproofreliesonStein'slemmaand
maybeofindependentinterest.Weconcludeby
demonstratingtherobustnessofanex-
plicitdoubly-orthogonalestimationprocedurefor
treatmenteffect.
1.Introduction
Theincreasedavailabilityoflargeandcomplexobserva-
tionaldatasetsisdrivinganincreasingdemandtoconduct
accuratecausalinferenceoftreatmenteffectsinthepres-
enceofhigh-dimensionalconfoundingfactors.Wetakeas
ourrunningexampledemandestimationfrompricingand
purchasedatainthedigitaleconomywheremanyfeatures
oftheworldthatsimultaneouslyaffectpricingdecisions
anddemandareavailableinlargedatastores.Oneoften
appealstomodernstatisticalmachinelearning(ML)tech-
niquestomodelandthehigh-dimensionalornonparamet-
ricnuisanceparametersintroducedbytheseconfounders.
However,mostsuchtechniquesintroducebiasintotheir
estimates(e.g.,viaregularization)andhenceyieldinvalidor
1
MicrosoftResearchNewEngland,USA
2
Operations
ResearchCenter,MIT,USA.Correspondenceto:Lester
Mackey
<
lmackey@microsoft.com
>
,VasilisSyrgkanis
<
vasy@microsoft.com
>
,IliasZadik
<
izadik@mit.edu
>
.
Proceedingsofthe
35
th
InternationalConferenceonMachine
Learning
,Stockholm,Sweden,PMLR80,2018.Copyright2018
bytheauthor(s).
inaccurateinferencesconcerningtheparametersofinterest
(thetreatmenteffects).
Severalrecentlinesofhavebegunaddresstheproblemof
debiasingMLestimatorstoperformaccurateinferenceona
lowdimensionalcomponentofmodelparameters.Promi-
nentexamplesincludeLassodebiasing(
Zhang&Zhang
;
vandeGeeretal.
,
2014
;
Javanmard&Montanari
,
2015
)and
post-selectioninference(
Bellonietal.
;
Berketal.
,
2013
;
Tibshiranietal.
,
2016
).Therecentdouble/debiasedML
workof
Chernozhukovetal.
(
2017
)describesageneral-
purposestrategyforextractingvalidinferencesfortarget
parametersfromsomewhatarbitraryandrelativelyinaccu-
rateestimatesofnuisanceparameters.
,
Chernozhukovetal.
(
2017
)analyzeatwo-
stageprocesswhereinthestageoneestimatesnuisance
parametersusingarbitrarystatisticalMLtechniquesona
stagedatasampleandinthesecondstageestimatesthe
lowdimensionalparametersofinterestviathegeneralized
methodofmoments(GMM).Crucially,themomentsinthe
secondstagearerequiredtosatisfya
Neymanorthogonality
condition,grantingthemrobustnesstoerrors
inthenuisanceparameterestimation.Amainconclusion
isthatthesecondstageestimatesare
p
n
-consistentand
asymptoticallynormalwheneverthestageestimatesare
consistentlyestimatedata
o
(
n

1
=
4
)
rate.
Toillustratethisresult,letusconsiderthepartiallylinear
regression(PLR)model,popularincausalinference.Inthe
PLRmodelweobservedatatriplets
Z
=(
T;Y;X
)
,where
T
2
R
representsatreatmentorpolicyapplied,
Y
2
R
representsanoutcomeofinterest,and
X
2
R
p
isavector
ofassociatedcovariates.Theseobservationsarerelatedvia
theequations
Y
=

0
T
+
f
0
(
X
)+

E
[

j
X;T
]=0
a:s:
T
=
g
0
(
X
)+
;
E
[

j
X
]=0
a:s:
where

and

representunobserveddisturbanceswithdis-
tributionsindependentof
(

0
;f
0
;g
0
)
.Theequation
featuresthetreatmenteffect

0
,ourobjectofinference.The
secondequationdescribestherelationbetweenthetreat-
ment
T
andtheassociatedcovariates
X
.Thecovariates
X
affecttheoutcome
Y
throughthenuisancefunction
f
0
and
thetreatment
T
throughthenuisancefunction
g
0
.Using
theNeyman-orthogonalmomentof(
Chernozhukovetal.
,
arXiv:1711.00342v6  [cs.LG]  1 Aug 2018OrthogonalMachineLearning:PowerandLimitations
2017
,Eq.4.55),theauthorsshowthatitsufcestoesti-
matethenuisance
(
f
0
;g
0
)
atan
o
(
n

1
=
4
)
ratetoconstruct
a
p
n
-consistentandasymptoticallynormalestimatorof

0
.
Inthiswork,weprovideaframeworkforachievingstronger
robustnesstostageerrorswhilemaintainingsecond
stagevalidity.Inparticular,weintroduceanotionof
higher-orderorthogonalityandshowthatifthemoment
is
k
-thorderorthogonalthenaestimationrateof
o
(
n

1
=
(2
k
+2)
)
suffor
p
n
-asymptoticnormalityofthe
secondstage.
Wethenprovideaconcreteapplicationofourapproachto
thecaseofestimatingtreatmenteffectsinthePLRmodel.
Interestingly,weshowanimpossibilityresultwhenthetreat-
mentresidualfollowsaGaussiandistribution:nohigher-
orderorthogonalmomentswithasymptoticvariance
exist,soNeymanorthogonalityappearstobe
thelimitofrobustnesstostageerrorsunderGaussian
treatmentresidual.However,conversely,wealsoshow
howtoconstructappropriatesecond-orderorthogonalmo-
mentswheneverthetreatmentresidualisnotGaussian.As
aresult,whenthenuisancefunctionsarelinearinthehigh-
dimensionalconfounders,oursecond-orderorthogonalmo-
mentsprovidevalidinferenceswheneverthenumberof
relevantconfoundersis
o
(
n
2
=
3
log
p
)
;meanwhilethet-order
orthogonalityanalysesof(
Chernozhukovetal.
,
2017
)ac-
commodateonly
o
(
p
n
log
p
)
relevantconfounders.
Weapplythesetechniquesinthesettingofdemandesti-
mationfrompricingandpurchasedata,wherehighlynon-
Gaussiantreatmentresidualsarestandard.Inthissetting,
thetreatmentisthepriceofaproduct,andcommonly,con-
ditionalonallobservablecovariates,thetreatmentfollowsa
discretedistributionrepresentingrandomdiscountsoffered
tocustomersoverabaselinepricelinearintheobservables.
InFigure
1
weportraytheresultsofasyntheticdemand
estimationproblemwithdensedependenceonobservables.
Here,thestandardorthogonalmomentestimationhaslarge
bias,comparabletovariance,whileoursecond-orderor-
thogonalmomentsleadtonearlyunbiasedestimation.
Notationalconventions
Foreach
n
2
N
,weintroduce
theshorthand
[
n
]
for
f
1
;:::;n
g
.Welet
p
!
and
d
!
rep-
resentconvergenceinprobabilityandconvergenceindis-
tributionrespectively.Whenrandomvariables
A
and
B
areindependent,weuse
E
A
[
g
(
A;B
)]
,
E
[
g
(
A;B
)
j
B
]
torepresentexpectationonlyoverthevariable
A
.Fora
sequenceofrandomvectors
(
X
n
)
1
n
=1
andadeterministic
sequenceofscalars
(
a
n
)
1
n
=1
,wewrite
X
n
=
O
P
(
a
n
)
to
mean
X
n
=a
n
isstochasticallybounded,i.e.,forany
>
0
thereis
R

;N

>
0
with
Pr(
k
X
n
=a
n
k
>R

)


forall
n>N

.Welet
N
(


representamultivariateGaussian
distributionwithmean

andcovariance

.
2.
Z
-EstimationwithNuisanceFunctionsand
Orthogonality
Ouraimistoestimateanunknowntargetparameter

0
2


R
d
givenaccesstoindependentreplicates
(
Z
t
)
2
n
t
=1
of
arandomdatavector
Z
2
R
ˆ
drawnfromadistribution
satisfying
d
momentconditions,
E
[
m
(
Z;
0
;h
0
(
X
))
j
X
]=0
a:s:
(1)
Here,
X
2
R
p
isasub-vectoroftheobserveddatavector
Z
,
h
0
2Hf
h
:
R
p
!
R
`
g
isavectorof
`
unknown
nuisancefunctions,and
m
:
R
ˆ

R
d

R
`
!
R
d
isa
vectorof
d
knownmomentfunctions.Weassumethatthese
momentconditionsexactlyidentifytheparameter

0
,and
weallowforthedatatobehigh-dimensional,with
ˆ
and
p
potentiallygrowingwiththesamplesize
n
.However,
thenumberofparametersofinterest
d
andthenumberof
nuisancefunctions
`
areassumedtobeconstant.
Wewillanalyzeatwo-stageestimationprocesswherewe
estimatethenuisanceparametersusinghalfofoursam-
ple
1
andthenforma
Z
-estimateofthetargetparameter

0
usingtheremainderofthesampleandourtage
estimatesofthenuisance.This
sample-splitting
procedure
proceedsasfollows.
1.
Firststage.
Formanestimate
^
h
2H
of
h
0
using
(
Z
t
)
2
n
t
=
n
+1
(e.g.,byrunninganonparametricorhigh-
dimensionalregressionprocedure).
2.
Secondstage.
Computea
Z
-estimate
^

SS
2

of

0
usinganempiricalversionofthemomentconditions
(
1
)and
^
h
asaplug-inestimateof
h
0
:
^

SS
solves
1
n
P
n
t
=1
m
(
Z
t
;;
^
h
(
X
t
))=0
:
(2)
Relegatingonlyhalfofthesampletoeachstagerepresentsa
statisticallyinefcientuseofdataand,inmanyapplications,
detrimentallyimpactsthequalityoftheestimate
^
h
.Aformofrepeatedsamplesplittingcalled
K
-foldcross-

(see,e.g.,
Chernozhukovetal.
,
2017
)addressesboth
oftheseconcerns.
K
-foldpartitionstheindex
setofthedatapoints
[2
n
]
into
K
subsets
I
1
;:::;I
K
ofcar-
dinality
2
n
K
(assumingforsimplicitythat
K
divides
2
n
)and
producesthefollowingtwo-stageestimate:
1.
Firststage.
Foreach
k
2
[
K
]
,formanestimate
^
h
k
2H
of
h
0
usingonlythedatapoints
(
Z
t
)
t
2
I
c
k
cor-
respondingto
I
c
k
=[2
n
]
n
I
k
.
2.
Secondstage.
Computea
Z
-estimate
^

SS
2

of

0
usinganempiricalversionofthemomentconditions
1
Unequaldivisionsofthesamplecanalsobeused;wefocus
onanequaldivisionforsimplicityofpresentation.
OrthogonalMachineLearning:PowerandLimitations
(a)Orthogonalestimates(
^

=2
:
78
,
^
˙
=
:
022
)
(b)Second-orderorthogonalestimates(
^

=3
:
,
^
˙
=
:
032
)
Figure1.
Weportraythedistributionofestimatesbasedonorthogonalmomentsandsecond-orderorthogonalmoments.Thetruetreatment
effect

0
=3
.Samplesize
n
=5000
,dimensionofconfounders
d
=1000
,supportsizeofsparselinearnuisancefunctions
s
=100
.
ThedetailsofthisexperimentcanbefoundinSection
5
.
and
(
^
h
k
)
k
2
[
K
]
asplug-inestimatorsof
h
0
:
^

CF
solves
1
2
n
K
X
k
=1
X
t
2
I
k
m
(
Z
t
;;
^
h
k
(
X
t
))=0
:
(3)
Throughout,weassume
K
isaconstantindependentofall
problemdimensions.AswewillseeinTheorem
1
,achief
advantageofoversamplesplittingisimproved
relativeefywithanasymptoticvariancethat
theuseofthefulldatasetinestimating

.
MainQuestion.
Ourprimaryinferentialgoalistoestablish
conditionsunderwhichtheestimators
^

SS
in
(
2
)
and
^

CF
(
3
)enjoy
p
n
-asymptoticnormality,thatis
p
n
(
^

SS


0
)
d
!
N
(0
;

and
p
2
n
(
^

CF


0
)
d
!
N
(0
;

forsomeconstantcovariancematrix

.Coupledwitha
consistentestimatorof

,asymptoticnormalityenablesthe
constructionofasymptoticallyvalidintervalsfor

basedonGaussianorStudent'stquantilesandasymptot-
icallyvalidhypothesistests,liketheWaldtest,basedon
chi-squaredlimits.
2.1.Higher-orderOrthogonality
Wewouldlikeourtwo-stageprocedurestoproduceac-
curateestimatesof

0
evenwhenthestagenuisance
estimatesarerelativelyinaccurate.Withthisgoalinmind,
Chernozhukovetal.
(
2017
)thenotionofNeyman-
orthogonalmoments,inspiredbytheearlyworkof
Neyman
(
1979
).Inoursetting,theorthogonalityconditionof(
Cher-
nozhukovetal.
,
2017
)isimpliedbythefollowingcondition,
whichwewillcall
st-orderorthogonality
:
1
(First-orderOrthogonalMoments)
.
Avectorof
moments
m
:
R
ˆ

R
d

R
`
!
R
d
is
orthogonal
withrespecttothenuisance
h
0
(
X
)
if
E

r

m
(
Z;
0
;
)
j

=
h
0
(
X
)
j
X

=0
:
Here,
r

m
(
Z;
0
;
)
isthegradientofthevectorofmo-
mentswithrespecttoits
`
arguments.
Intuitively,orthogonalmomentsareinsensitive
tosmallperturbationsinthenuisanceparametersandhence
robusttosmallerrorsinestimatesoftheseparameters.A
mainresultof(
Chernozhukovetal.
,
2017
)isthat,ifthe
moments
m
areorthogonal,then
o
(
n

1
=
4
)
error
rates
2
inthestageestimationof
h
0
aresufcientfor
p
n
-asymptoticnormalityoftheestimates
^

SS
and
^

CF
.
Ouraimistoaccommodateslowerratesofconvergencein
thestageofestimationbydesigningmomentsrobustto
largernuisanceestimationerrors.Toachievethis,wewill
introduceageneralizednotionoforthogonalitythatrequires
higher-ordernuisancederivativesof
m
tobeconditionally
meanzero.Wewillmakeuseofthefollowinghigher-order
differentialnotation:
2
(Higher-orderDifferentials)
.
Givenavector
ofmoments
m
:
R
ˆ

R
d

R
`
!
R
d
andavector

2
N
`
wedenoteby
D

m
(
Z;;
)
the

-differentialof
m
with
respecttoits
`
arguments:
D

m
(
Z;;
)=
r

1

1
r

2

2
:::
r

`

`
m
(
Z;;
)
(4)
Wearenowequippedtoournotionof
S
-orthogonal
moments
:
3
(
S
-OrthogonalMoments)
.
Avectorofmo-
ments
m
:
R
ˆ

R
d

R
`
!
R
d
is
S
-orthogonal
with
respecttothenuisance
h
0
(
X
)
forsome
orthogonalityset
S

N
`
,ifforany

2
S
:
E
[
D

m
(
Z;
0
;h
0
(
X
))
j
X
]=0
:
(5)
Wewilloftenbeinterestedinthespecialcaseof
3
inwhich
S
iscomprisedofallvectors

2
N
`
with
k

k
1

2
Inthesenseofrootmeansquarederror:
n
1
=
4
q
E
[
k
h
0
(
X
)

^
h
(
X
)
k
2
2
j
^
h
]
p
!
0
.
OrthogonalMachineLearning:PowerandLimitations
k
.Thisimpliesthatallmixednuisancederivativesofthe
momentoforder
k
orlessareconditionallymeanzero.We
willrefertothisspecialcaseas
k
-orthogonality
or
k
-th
orderorthogonality
.
4
(
k
-OrthogonalMoments)
.
Avectorofmo-
ments
m
:
R
ˆ

R
d

R
`
!
R
d
is
k
-orthogonal
ifitis
S
k
-orthogonalforthe
k
-orthogonalityset
,
S
k
,
f

2
N
`
:
k

k
1

k
g
.
Thegeneralnotionof
S
-orthogonalityallowsforourmo-
mentstobemorerobusttoerrorsinsomenuisancefunc-
tionsandlessrobusttoerrorsinothers.Thisisparticularly
valuablewhensomenuisancefunctionsareeasiertoesti-
matethanothers;wewillencountersuchanexamplein
Section
4.2
.
3.Higher-orderOrthogonalityandRoot-
n
Consistency
Wewillnowshowthat
S
-orthogonalitytogetherwithappro-
priateconsistencyratesforthestageestimatesofthe
nuisancefunctionsimply
p
n
-consistencyandasymptotic
normalityofthetwo-stageestimates
^

SS
and
^

CF
.Beyond
orthogonalityandconsistency,ourmainAssumption
1
de-
mands,non-degeneracy,andregularityofthe
moments
m
,allofwhicharestandardforestablishingthe
asymptoticnormalityof
Z
-estimators.
Assumption1.
Foranon-emptyorthogonalityset
S

N
`
and
k
,
max

2
S
k

k
1
,weassumethefollowing:
1.
S
-Orthogonality.
Themoments
m
are
S
-orthogonal.
2.
.
E
[
m
(
Z;;h
0
(
X
))]
6
=0
when

6
=

0
.
3.
Non-degeneracy.
Thematrix
E
[
r

m
(
Z;
0
;h
0
(
X
))]
isinvertible.
4.
Smoothness.
r
k
m
existsandiscontinuous.
5.
ConsistencyofFirstStage.
Theststageestimates
satisfy
E
[
Q
`
i
=1
j
^
h
i
(
X
)

h
0
;i
(
X
)
j
4

i
j
^
h
]
p
!
0
;
8

2
S;
wheretheconvergenceinprobabilityiswithrespectto
theauxiliarydatasetusedto
^
h
.
6.
RateofFirstStage.
Theststageestimatessatisfy
n
1
=
2

q
E
[
Q
`
i
=1
j
^
h
i
(
X
)

h
0
;i
(
X
)
j
2

i
j
^
h
]
p
!
0
;
8

2f
a
2
N
`
:
k
a
k
1

k
+1
gn
S
,wherethecon-
vergenceinprobabilityiswithrespecttotheauxiliary
datasetusedto
^
h
.
7.
RegularityofMoments.
Thereexistsan
r>
0
such
thatthefollowingregularityconditionshold:
(a)
E
[sup

2B

0
;r
kr

m
(
Z;;h
0
(
X
))
k
F
]
<
1
for
B

0
;r
,
f

2
:
k



0
k
2

r
g
:
(b)
sup
h
2B
h
0
;r
E
[sup

2B

0
;r
kr

r

m
(
Z;;h
(
X
))
k
2
]
<
1
for
B
h
0
;r
,
f
h
2H
:
max

:
k

k
1

k
+1
E
[
Q
`
i
=1
j
h
i
(
X
)

h
0
;i
(
X
)
j
2

i
]

r
g
:
(c)
max

:
k

k
1

k
+1
sup
h
2B
h
0
;r
E

j
D

m
(
Z;
0
;h
(
X
))
j
4




(

0
;h
0
)
<
1
.
(d)
E
[sup

2
A;h
2B
h
0
;r
k
m
(
Z;;h
(
X
))
k
2
]
<
1
,
foranycompact
A


,
(e)
sup

2
A;h
2B
h
0
;r
E
[
kr

m
(
Z;;h
(
X
))
k
2
]
<
1
,
foranycompact
A


.
Wearenowreadytostateourmaintheoremontheimpli-
cationsof
S
-orthogonalityforsecondstage
p
n
-asymptotic
normality.TheproofcanbefoundinSection
A
.
Theorem1
(MainTheorem)
.
UnderAssumption
1
,if
^

SS
and
^

CF
areconsistent,then
p
n
(
^

SS


0
)
d
!
N
(0
;

and
p
2
n
(
^

CF


0
)
d
!
N
(0
;

where
=
J

1
VJ

1
for
J
=
E
[
r

m
(
Z;
0
;h
0
(
X
))]
and
V
=
Cov
(
m
(
Z;
0
;h
0
(
X
)))
.
Avarietyofstandardsufcientconditionsguaranteethe
consistencyof
^

SS
and
^

CF
.Ournextresult,provedin
Section
B
,establishesconsistencyundereitheroftwocom-
monlyassumptions.
Assumption2.
Oneofthefollowingsetsofconditionsis

1.
Compactnessconditions:

iscompact.
2.
Convexityconditions:

isconvex,

0
isintheinte-
riorof

,and,withprobabilityapproaching1,the
mapping

7!
1
n
P
n
t
=1
m
(
Z
t
;;
^
h
(
X
t
))
isthegradi-
entofaconvexfunction.
Remark
Acontinuouslydifferentiablevector-valued
function

7!
F
(

)
onaconvexdomain

isthegradi-
entofaconvexfunctionwheneverthematrix
r

F
(

)
is
symmetricandpositiveforall

.
Theorem2
(Consistency)
.
IfAssumptions
1
and
2
hold,
then
^

SS
and
^

CF
areconsistent.
3.1.ConditionsforFirstStageRates
Ourassumptiononthestageestimationrates,i.e.,that
8

2f
a
2
N
`
:
k
a
k
1

k
+1
gn
S
n
1
=
2

r
E
h
Q
`
i
=1
j
^
h
i
(
X
)

h
0
;i
(
X
)
j
2

i
j
^
h
i
p
!
0
OrthogonalMachineLearning:PowerandLimitations
mayseemcomplex,asitinvolvestheinteractionoftheer-
rorsofmultiplenuisancefunctionestimates.Inthissection
wegivesufcientconditionsthatinvolveonlytheratesof
individualnuisancefunctionestimatesandwhichimplyour
stagerateassumptions.Inparticular,weareinterested
informulatingconsistencyrateconditionsforeachnuisance
function
h
i
withrespecttoan
L
p
norm,
k
^
h
i

h
0
;i
k
p
=
E
[
k
^
h
i
(
X
)

h
0
;i
(
X
)
k
p
p
j
^
h
]
1
=p
:
Wewillmakeuseofthesesufconditionswhenap-
plyingourmaintheoremtothepartiallylinearregression
modelinSection
4.2
.
Lemma3.
Let
k
=max
a
2
S
k
a
k
1
.Then
(1)
Assumption
1
.
6
holdsifanyofthefollowingholds
8

2f
a
2
N
`
:
k
a
k
1

k
+1
gn
S
:

p
n
Q
`
i
=1
k
^
h
i

h
0
;i
k

i
2
k

k
1
p
!
0
(6)
8
i;n
1

i
k

k
1
k
^
h
i

h
0
;i
k
2
k

k
1
p
!
0
(7)
forsome

i
2
(0
;
2]
where
1
k

k
1
P
`
i
=1

i

i

1
2
8
i;n
1

i
k

k
1
k
^
h
i

h
0
;i
k
2
k

k
1
p
!
0
(8)
forsome

i
2
(0
;
2]
:
(2)
Assumption
1
.
5
holdsif
8
i
,
k
^
h
i

h
0
;i
k
4
k
p
!
0
.
Asimplerdescriptionofthesufconditionsarises
under
k
-orthogonality
4
),sincetheset
f
a
2
N
`
:
k
a
k
1

k
+1
gn
S
k
containsonlyvectors

with
k

k
=
k
+1
.
Corollary4.
If
S
isthecanonical
k
-orthogonalityset
S
k

4
),thenAssumption
1
.
6
holdswhenever
8
i;n
1
2(
k
+1)
k
^
h
i

h
0
;i
k
2(
k
+1)
p
!
0
;
andAssumption
1
.
5
holdswhenever
8
i
,
k
^
h
i

h
0
;i
k
4
k
p
!
0
.
Inthecaseoforthogonality,Corollary
4
requires
thatthestagenuisancefunctionsbeestimatedata
o
(
n

1
=
4
)
ratewithrespecttothe
L
4
norm.Thisisalmost
butnotexactlythesameastheconditionpresentedin(
Cher-
nozhukovetal.
,
2017
),whichrequire
o
(
n

1
=
4
)
consistency
rateswithrespecttothe
L
2
norm.Ignoringtheexpectation
over
X
,thetwoconditionsareequivalent.
3
Moreover,inthe
caseof
k
-orthogonality,Corollary
4
requires
o
(
n

1
=
2(
k
+1)
)
rateswithrespecttothe
L
2(
k
+1)
norm.Moregenerally,
S
-orthogonalityallowsforsomefunctionstobeestimated
slowerthanothersaswewillseeinthecaseofthesparse
linearmodel.
3
Wewouldrecovertheexactconditionin(
Chernozhukovetal.
,
2017
)ifwereplacedAssumption
1
.
7c
withthemorestringent
assumptionthat
j
D

m
(
Z;;h
(
X
))
j


a.s.
4.Second-orderOrthogonalityforPartially
LinearRegression
Whensecond-orderorthogonalmomentssatisfyingAssump-
tion
1
areemployed,Corollary
4
impliesthatan
o
(
n

1
=
6
)
rateofnuisanceparameterestimationissufcientfor
p
n
-
consistencyof
^

SS
and
^

CF
.Thisasymptoticimprovement
overorthogonalityholdsthepromiseofaccom-
modatingmorecomplexandhigher-dimensionalnuisance
parameters.Inthissection,wedetailboththelimitations
andthepowerofthisapproachinthepartiallylinearregres-
sion(PLR)modelsettingpopularincausalinference(see,
e.g,
Chernozhukovetal.
,
2017
).
5
(PartiallyLinearRegression(PLR))
.
Inthe
partiallylinearregressionmodelofobservations
Z
=
(
T;Y;X
)
,
T
2
R
representsatreatmentorpolicyapplied,
Y
2
R
representsanoutcomeofinterest,and
X
2
R
p
is
avectorofassociatedcovariates.Theseobservationsare
relatedviatheequations
Y
=

0
T
+
f
0
(
X
)+

E
[

j
X;T
]=0
a:s:
T
=
g
0
(
X
)+
;
E
[

j
X
]=0
a:s:
where

and

representunobservednoisevariableswith
distributionsindependentof
(

0
;f
0
;g
0
)
.
4.1.Limitations:theGaussianTreatmentBarrier
Ourresultshowsthat,underthePLRmodel,ifthe
treatmentnoise,

,isconditionallyGaussiangiven
X
,then
nosecond-orderorthogonalmomentcansatisfyAssump-
tion
1
,becauseeverytwicecontinuouslydifferentiable
2
-
orthogonalmomenthas
E
[
r

m
(
Z;
0
;h
0
(
X
))]=0
(avi-
olationofAssumption
1
.3).TheproofinSection
D
relies
onStein'slemma.
Theorem5.
UnderthePLRmodel,supposethat

iscondi-
tionallyGaussiangiven
X
(a.s.X).Ifatwicedifferentiable
momentfunction
m
issecond-orderorthogonalwithre-
specttothenuisanceparameters
(
f
0
(
X
)
;g
0
(
X
))
,thenit
mustsatisfy
E
[
r

m
(
Z;
0
;h
0
(
X
))]=0
andhencevio-
lateAssumption
1
.3.Thereforenosecond-orderorthogonal
momentAssumption
1
.
Inthefollowingresult,provedinSection
E
,weestablish
thatundermildconditionsAssumption
1
.
3
isnecessaryfor
the
p
n
-consistencyof
^

SS
inthePLRmodel.
Proposition6.
UnderthePLRmodel,supposethat
j

j
2
andthattheconditionaldistributionof
(

)
given
X
has
fullsupporton
R
2
(a.s.X).Thennomomentfunction
m
simultaneously
1.
Assumption
1
,exceptforAssumption
1
.
3
,
2.
E
[
r

m
(
Z;
0
;h
0
(
X
))]=0
,and
3.
^

SS


0
=
O
P
(1
=
p
n
)
.
OrthogonalMachineLearning:PowerandLimitations
4.2.Power:Second-orderOrthogonalityunder
Non-GaussianTreatment
Wenextshowthat,inversely,second-orderorthogonalmo-
mentsareavailablewhenevertheconditionaldistributionof
treatmentnoisegiven
X
isnota.s.Gaussian.Ourproofsrely
onastandardcharacterizationofaGaussiandistribution,
provedinSection
F
:
Lemma7.
If
E
[

j
X
]=0
a.s.,theconditionaldistribution
of

given
X
isa.s.Gaussianifandonlyifforall
r
2
N
;r

2
itholdsthat,
E


r
+1
j
X

=
r
E
[

2
j
X
]
E


r

1
j
X

a.s.
Wewillfocusonestimatingthenuisancefunctions
q
0
=
f
0
+

0
g
0
and
g
0
insteadofthenuisancefunctions
f
0
and
g
0
,
sincetheformertaskismorepracticalinmanyapplications.
Thisisbecauseestimating
q
0
canbeaccomplishedbycar-
ryingoutanarbitrarynon-parametricregressionof
Y
onto
X
.Incontrast,estimating
f
0
typicallyinvolvesregressing
Y
onto
(
X;T
)
,where
T
isconstrainedtoenterlinearly.
ThelattermightbecumbersomewhenusingarbitraryML
regressionprocedures.
Ourresult,establishedinSection
G
,produces
variance2-orthogonalmomentswhenanappropriatemo-
mentofthetreatmentnoise

isknown.
Theorem8.
UnderthePLRmodel,supposethatweknow
E
[

r
j
X
]
andthat
E
[

r
+1
]
6
=
r
E
[
E
[

2
j
X
]
E
[

r

1
j
X
]]
for
some
r
2
N
,sothattheconditionaldistributionof

given
X
is
not
a.s.Gaussian.Thenthemoments
m
(
Z;;q
(
X
)
;g
(
X
)
;
r

1
(
X
))
,
(
Y

q
(
X
)


(
T

g
(
X
)))

((
T

g
(
X
))
r

E
[

r
j
X
]

r
(
T

g
(
X
))

r

1
(
X
))
satisfyeachofthefollowingproperties

2-orthogonality
withrespecttothenuisance
h
0
(
X
)=
(
q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])
,


When

6
=

0
;
E
[
m
(
Z;;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])]
6
=0
;

Non-degeneracy:
E
[
r

m

Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
]

]
6
=0
;

Smoothness:
r
k
m
iscontinuousforall
k
2
N
.
Ournextresult,provedinSection
H
,addressesthemore
realisticsettinginwhichwedonothaveexactknowledge
of
E
[

r
j
X
]
.Weintroduceanadditionalnuisanceparameter
andstillsatisfyanorthogonalityconditionwithrespectto
theseparameters.
Theorem9.
UnderthePLRmodel,supposethat
E
[

r
+1
]
6
=
r
E
[
E
[

2
j
X
]
E
[

r

1
j
X
]]
for
r
2
N
,sothattheconditional
distributionof

given
X
is
not
a.s.Gaussian.Then,if
S
,
f

2
N
4
:
k

k
1

2
gnf
(1
;
0
;
0
;
1)
;
(0
;
1
;
0
;
1)
g
;
themoments
m
(
Z;;q
(
X
)
;g
(
X
)
;
r

1
(
X
)
;
r
(
X
))
,
(
Y

q
(
X
)


(
T

g
(
X
)))

((
T

g
(
X
))
r


r
(
X
)

r
(
T

g
(
X
))

r

1
(
X
))
satisfyeachofthefollowingproperties

S
-orthogonality
withrespecttothenuisance
h
0
(
X
)=(
q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
]
;
E
[

r
j
X
])
,


When

6
=

0
,
E
[
m
(
Z;;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
]
;
E
[

r
j
X
])]
6
=0
;

Non-degeneracy:
E
[
r

m

Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
]
;
E
[

r
j
X
]

]
6
=0
;

Smoothness:
r
k
m
continuousforall
k
2
N
.
Inwords,
S
-orthogonalityheremeansthat
m
theor-
thogonalityconditionforallmixedderivativesoftotalorder
atmost2withrespecttothefournuisanceparametersex-
ceptthemixedderivativeswithrespectto
(
q
0
(
X
)
;
E
[

r
j
X
])
and
(
g
0
(
X
)
;
E
[

r
j
X
])
.
4.3.ApplicationtoHigh-dimensionalLinearNuisance
Functions
WenowconsiderdeployingthePLRmodelinthehigh-
dimensionallinearregressionsetting,where
f
0
(
X
)=
h
X;
0
i
and
g
0
(
X
)=
h
X;
0
i
fortwo
s
-sparsevectors

0
;
0
2
R
p
,
p
tendstoas
n
!1
,and
(
;X
)
aremutuallyindependent.
q
0
=

0

0
+

0
.Inthis
high-dimensionalregressionsetting,
Chernozhukovetal.
(
2017
,Rem.4.3)showedthattwo-stageestimationwith
orthogonalmoments
m
(
Z;;
h
X;q
i
;
h
X;
i
)=
(9)
(
Y
h
X;q
i

(
T
h
X;
i
))(
T
h
X;
i
)
andLassoestimatesofthenuisanceprovidesa
p
n
-
asymptoticallynormalestimatorof

0
when
s
=
o
(
n
1
2
=
log
p
)
.Ournextresult,establishedinAppendix
I
,
showsthatwecanaccommodate
s
=
o
(
n
2
3
=
log
p
)
withan
explicitsetofhigher-orderorthogonalmoments.
OrthogonalMachineLearning:PowerandLimitations
Theorem10.
Inthehigh-dimensionallinearregression
setting,supposethateither
E
[

3
]
6
=0
(non-zeroskew-
ness)or
E
[

4
]
6
=3
E
[

2
]
2
(excesskurtosis),that
X
has
i.i.d.mean-zerostandardGaussianentries,that

and

arealmostsurelyboundedbytheknownvalue
C
,andthat

0
2
[

M;M
]
forknown
M
.If
s
=
o
(
n
2
=
3
=
log
p
)
;
andin
theststageofestimationwe
(a)
createestimates
^
q;
^

of
q
0
;
0
viaLassoregressionof
Y
on
X
and
T
on
X
respectively,withregularization
parameter

n
=2
CM
p
3log(
p
)
=n
and
(b)
estimate
E
[

2
]
and
E
[

3
]
using
^

t
,
T
0
t
h
X
0
t
;
^

i
,
^

2
=
1
n
P
n
t
=1
^

2
t
;
and
^

3
=
1
n
P
n
t
=1
(^

3
t

3^

2
^

t
)
;
for
(
T
0
t
;X
0
t
)
n
t
=1
ani.i.d.sampleindependentof
^

,
then,usingthemoments
m
ofTheorem
9
with
r
=2
in
thecaseofnon-zeroskewnessor
r
=3
inthecaseofex-
cesskurtosis,
^

SS
and
^

CF
are
p
n
-asymptoticallynormal
estimatorsof

0
.
5.Experiments
Weperformanexperimentalanalysisofthesecondorder
orthogonalestimatorofTheorem
10
with
r
=3
forthecase
ofestimatingtreatmenteffectsinthePLRmodelwithhigh-
dimensionalsparselinearnuisancefunctions.Wecompare
ourestimatorwiththedoubleMLestimator(labeled`dml'
inourbasedontheorthogonalmoments
(
9
)
of(
Chernozhukovetal.
,
2017
).Ourexperimentsare
designedtosimulatedemandestimationfrompricingand
purchasedata,wherenon-Gaussiantreatmentresidualsare
standard.Here,ourcovariates
X
correspondtoallcollected
variablesthatmayaffectapricingpolicy.Atypicalran-
domizedexperimentinapricingpolicytakestheformof
randomdiscountsfromabaselinepriceasacompanyof-
fersrandomdiscountstocustomersperiodicallytogauge
demandlevel.Inthiscase,thetreatmentresidualŒtheunex-
plainedinpriceŒisdecidedlynon-Gaussianand
followsadiscretedistributionoverasmallnum-
berofpricepoints.Pythoncoderecreatingallexperiments
isavailableat
https://github.com/IliasZadik/
double_orthogonal_ml
.
Experiment
Wegenerated
n
independent
replicatesofoutcome
Y
,treatment
T
,andconfoundingco-
variates
X
.Theconfounders
X
havedimension
p
andhave
independentcomponentsfromthe
N
(0
;
1)
distribution.The
treatmentisasparselinearfunctionof
X
,
T
=
h

0
;X
i
+

,
whereonly
s
ofthe
p
coefof

0
arenon-zero.The
x
-axisoneachplotisthenumberofnon-zerocoef
s
.Moreover,

isdrawnfromadiscretedistributionwith
values
f
0
:
5
;
0
;

1
:
5
;

3
:
5
g
takenrespectivelywithprob-
abilities
(
:
65
;:
2
;:
1
;:
05)
.Here,thetreatmentrepresents
thepriceofaproductorservice,andthisdatagenerat-
ingprocesssimulatesrandomdiscountsoverabaseline
price.Finally,theoutcomeisgeneratedbyalinearmodel,
Y
=

0
T
+
h

0
;X
i
+

,where

0
=3
isthetreatmentef-
fect,

0
isanothersparsevectorwithonly
s
non-zeroentries,
and

isdrawnindependentlyfromauniform
U
(

˙

;˙

)
distribution.Importantly,thecoordinatesofthe
s
non-zero
entriesofthecoef

0
arethesameasthecoordinates
ofthe
s
non-zeroentriesof

0
.Thelatterensuresthat
variables
X
createatrueendogeneityproblem,i.e.,that
X
affectsboththetreatmentandtheoutcomedirectly.In
suchsettings,controllingfor
X
isimportantforunbiased
estimation.
Togenerateaninstanceoftheproblem,thecommonsup-
portofboth

0
and

0
wasgenerateduniformlyatrandom
fromthesetofallcoordinates,andeachnon-zerocoef-
cientwasgeneratedindependentlyfromauniform
U
(0
;
5)
distribution.Thestagenuisancefunctionswere
forbothmethodsbyrunningtheLassoonasubsampleof
n=
2
samplepoints.Forthemethodallremaining
n=
2
pointswereusedforthesecondstageestimationof

0
.
Forthesecond-ordermethod,themoments
E
[

2
]
and
E
[

3
]
wereestimatedusingasubsampleof
n=
4
pointsasdescribed
inTheorem
10
,andtheremaining
n=
4
samplepointswere
usedforthesecondstageestimationof

0
.Foreachmethod
weperformedacrosstheandsecondstages,
andforthesecond-ordermethodweperformednestedcross-
betweenthe
n=
4
subsampleusedforthe
E
[

2
]
and
E
[

3
]
estimationandthe
n=
4
subsampleusedforthesecond
stageestimation.Theregularizationparameter

n
ofeach
Lassowaschosentobe
p
log(
p
)
=n
.
Foreachinstanceoftheproblem,i.e.,eachrandomreal-
izationofthecoefwegenerated
2000
independent
datasetstoestimatethebiasandstandarddeviationofeach
estimator.Werepeatedthisprocessover
100
randomlygen-
eratedprobleminstances,eachtimewithadifferentdraw
ofthecoef

0
and

0
,toevaluatevariabilityacross
differentrealizationsofthenuisancefunctions.
DistributionofErrorswithFixedSparsity
InFigure
1
,
wedisplaythedistributionofestimatesbasedonorthog-
onalmomentsandsecond-orderorthogonalmomentsfor
aparticularsparsitylevel
s
=100
andfor
n
=5000
and
p
=1000
.Weobservethatbothestimatesareapproxi-
matelynormallydistributed,buttheorthogonalmoment
estimationexhibitsbias,anorderofmagnitude
largerthanthevariance.
Bias-VarianceTradeoffwithVaryingSparsity
Figure
2
portraysthemedianquantities(solidlines)andmaximum
andminimumofthesequantities(errorbars)acrossthe
OrthogonalMachineLearning:PowerandLimitations
Figure2.
Comparisonofestimates
^

CF
basedonorthogonalmomentsandsecondorderorthogonalmomentsunderthePLRmodelasa
functionofthenumberofnon-zerocoefinthenuisancevectors

0
and

0
.SeeSection
5
formoredetails.Theparametersusedfor
thiswere
n
=5000
,
p
=1000
,
˙

=1
.Thefourthdisplaysthe
`
2
errorinthecoefcientsdiscoveredbythestage
estimatesforeachofthenuisancefunctions:model
tisthemodelfor
E
[
T
j
X
]
andmodel
yisthemodelfor
E
[
Y
j
X
]
.
100
differentnuisancefunctiondrawsasafunctionofthe
supportsizefor
n
=5000
,
p
=1000
,and
˙

=1
.
Varying
n
and
p
InFigure
3
,wedisplayhowperfor-
mancevarieswith
n
and
p
.Duetocomputationalconsid-
erations,forthisparameterexploration,weonlyuseda
singleprobleminstanceforeach
(
n;p;s
)
tripletratherthan
100
instancesasintheexplorationabove.Wenotethatfor
n
=2000
;p
=5000
thebreakingpointofourmethodis
around
s
=100
,whilefor
n
=5000
;p
=2000
itisaround
s
=550
.For
n
=10000
;p
=1000
ourmethodperforms
exceptionallywellevenuntil
s
=800
.
(a)
n
=2000
;p
=1000
(b)
n
=2000
;p
=2000
(c)
n
=2000
;p
=5000
(d)
n
=5000
;p
=1000
(e)
n
=5000
;p
=2000
(f)
n
=10000
;p
=1000
Figure3.
MSEofbothestimatorsasthesparsityvariesfordifferent
samplesizeanddimensionpairs
(
n;p
)
.Notethattherangeofthe
supportsizesislargerforlarger
n
.
˙

=1
.
Varying
˙

FinallyFigure
4
displaysperformanceasthe
variance
˙

ofthenoise

grows.
(a)
˙

=3
(b)
˙

=10
(c)
˙

=20
Figure4.
MSEofbothestimatorsasthesparsityvariesfordifferent
varianceparameters
˙

.
n
=5000
;p
=1000
.
6.Conclusion
Ouraiminthisworkwastoconductaccurateinferencefor
ed-dimensionaltargetparametersinthepresenceofhigh-
dimensionalornonparametricnuisance.Toachievethis,
weintroducedanotionof
k
-thorderorthogonalmoments
fortwo-stage
Z
-estimation,generalizingtherNey-
manorthogonalitystudiedin(
Chernozhukovetal.
,
2017
).
Given
k
-thorderorthogonalmoments,weestablishedthat
estimatingnuisanceatan
o
(
n

1
=
(2
k
+2)
)
ratesuffor
p
n
-consistentandasymptoticallynormalestimatesoftar-
getparameters.WethenstudiedthePLRmodelpopularin
causalinferenceandshowedthatavalidsecond-orderor-
thogonalmomentexistsifandonlyifthetreatmentresidual
isnotnormallydistributed.Inthehigh-dimensionallinear
nuisancesetting,theseexplicitsecond-orderorthogonalmo-
mentstoleratedensernuisancevectorsthan
thoseaccommodatedby(
Chernozhukovetal.
,
2017
).We
complementedourresultswithsyntheticdemandestimation
experimentsshowingtheofsecond-orderorthogo-
nalmomentsoverstandardNeyman-orthogonalmoments.
OrthogonalMachineLearning:PowerandLimitations
References
Belloni,A.,Chernozhukov,V.,Val,I.F.,andHansen,C.
Programevaluationandcausalinferencewithhighdi-
mensionaldata.
Econometrica
,85(1):233Œ298.
Berk,R.,Brown,L.,Buja,A.,Zhang,K.,andZhao,L.Valid
post-selectioninference.
Ann.Statist.
,41(2):802Œ837,04
2013.doi:10.1214/12-AOS1077.
Chernozhukov,V.,Chetverikov,D.,Demirer,M.,E.,
Hansen,C.,andNewey,W.Double/debiased/neymanma-
chinelearningoftreatmenteffects.
AmericanEconomic
Review
,107(5):261Œ65,May2017.
Durrett,R.
Probability:TheoryandExamples
.Cambridge
UniversityPress,NewYork,NY,USA,4thedition,2010.
ISBN0521765390,9780521765398.
Flanders,H.Differentiationundertheintegralsign.
The
AmericanMathematicalMonthly
,80(6):615Œ627,1973.
Hastie,T.,Tibshirani,R.,andWainwright,M.
Statistical
learningwithsparsity:thelassoandgeneralizations
.
CRCpress,2015.
Javanmard,A.andMontanari,A.De-biasingtheLasso:
OptimalSampleSizeforGaussianDesigns.
ArXive-
prints
,August2015.
Newey,W.andMcFadden,D.l.Chapter36largesample
estimationandhypothesistesting.
HandbookofEcono-
metrics
,4:2111Œ2245,1994.ISSN1573-4412.doi:
http://dx.doi.org/10.1016/S1573-4412(05)80005-4.
Neyman,J.C()testsandtheiruse.
Sankhy:TheIndian
JournalofStatistics,SeriesA(1961-2002)
,41(1/2):1Œ21,
1979.ISSN0581572X.
Stein,C.M.Estimationofthemeanofamultivariatenormal
distribution.
Ann.Statist.
,9(6):1135Œ1151,111981.
Tibshirani,R.J.,Taylor,J.,Lockhart,R.,andTibshirani,R.
Exactpost-selectioninferenceforsequentialregression
procedures.
JournaloftheAmericanStatisticalAssocia-
tion
,111(514):600Œ620,2016.
vandeGeer,S.,Buhlmann,P.,Ritov,Y.,andDezeure,R.
Onasymptoticallyoptimalregionsandtests
forhigh-dimensionalmodels.
Ann.Statist.
,42(3):1166Œ
1202,062014.doi:10.1214/14-AOS1221.
vanderVaart,A.W.
Asymptoticstatistics
.CambridgeSeries
inStatisticalandProbabilisticMathematics.Cambridge
UniversityPress,1998.ISBN0-521-49603-9.
Zhang,C.H.andZhang,S.intervalsfor
lowdimensionalparametersinhighdimensionallinear
models.
JournaloftheRoyalStatisticalSociety:Se-
riesB(StatisticalMethodology)
,76(1):217Œ242.doi:
10.1111/rssb.12026.
OrthogonalMachineLearning:PowerandLimitations
A.ProofofTheorem
1
Weprovetheresultforthesample-splittingestimator
^

SS
in
(
2
)
andthendiscusshowtogeneralizeforthe
K
-foldcross
estimator
^

CF
in(
3
)with
p
2
n
scaling.
Foreachcoordinatemomentfunction
m
i
,themeanvaluetheoremandtheof
^

SS
implythat
1
n
n
X
t
=1
hr

m
i
(
Z
t
;
~

(
i
)
;
^
h
(
X
t
))
;
0

^

SS
i
=
1
n
n
X
t
=1
(
m
i
(
Z
t
;
0
;
^
h
(
X
t
))

m
i
(
Z
t
;
^

SS
;
^
h
(
X
t
)))=
1
n
n
X
t
=1
m
i
(
Z
t
;
0
;
^
h
(
X
t
))
(10)
forsomeconvexcombination,
~

(
i
)
,of
^

SS
and

0
.Hence,
p
n
(

0

^

SS
)
I
[det
^
J
(
^
h
)
6
=0]=
^
J
(
^
h
)

1
I
[det
^
J
(
^
h
)
6
=0]
1
p
n
n
X
t
=1
m
(
Z
t
;
0
;
^
h
(
X
t
))
|
{z
}
B
for
^
J
(
h
)
,
1
n
n
X
t
=1
2
4
r

m
1
(
Z
t
;
~

(1)
;h
(
X
t
))

r

m
d
(
Z
t
;
~

(
d
)
;h
(
X
t
))
3
5
2
R
d

d
:
WewillshowinSection
A.1
that
^
J
(
^
h
)
convergesinprobabilitytotheinvertiblematrix
J
=
E
[
r

m
(
Z;
0
;h
0
(
X
))]
.
Hence,wewillhave
I
[det
^
J
(
^
h
)
6
=0]
p
!
I
[det
J
6
=0]=1
and
^
J
(
^
h
)

1
I
[det
^
J
(
^
h
)
6
=0]
p
!
J

1
bythecontinuousmapping
theorem(
vanderVaart
,
1998
,Thm.2.3).WewillnextshowinSection
A.2
that
B
convergesindistributiontoamean-zero
multivariateGaussiandistributionwithconstantcovariancematrix
V
=
Cov
(
m
(
Z;
0
;h
0
(
X
)))
.Slutsky'stheorem(
vander
Vaart
,
1998
,Thm.2.8)willthereforeimplythat
p
n
(

0

^

SS
)
I
[det
^
J
(
^
h
)
6
=0]
convergesindistributionto
N
(0
;J

1
VJ

1
)
.
Finally,thefollowinglemma,provedinSection
J.1
,willimplythat
p
n
(

0

^

SS
)
alsoconvergesindistributionto
N
(0
;J

1
VJ

1
)
,asdesired.
Lemma11.
Considerasequenceofbinaryrandomvariables
Y
n
2f
0
;
1
g
satisfying
Y
n
p
!
1
.If
X
n
Y
n
p
!
X
,then
X
n
p
!
X
.Similarly,if
X
n
Y
n
d
!
X
,then
X
n
d
!
X
.
A.1.Convergenceof
^
J
(
^
h
)

J
.
Foreachcoordinate
j
andmoment
m
i
and
r>
0
inAssumption
1
.
7
,themeanvaluetheoremandCauchy-Schwarz
implythat
E
h



^
J
ij
(
^
h
)

^
J
ij
(
h
0
)



I
[
~

(
i
)
2B

0
;r
]
j
^
h
i
I
[
^
h
2B
h
0
;r
]

E
h



r

j
m
i
(
Z
t
;
~

(
i
)
;
^
h
(
X
t
))
r

j
m
i
(
Z
t
;
~

(
i
)
;h
0
(
X
t
))



I
[
~

(
i
)
2B

0
;r
]
j
^
h
i
I
[
^
h
2B
h
0
;r
]
=
E
h



h
^
h
(
X
t
)

h
0
(
X
t
)
;
r

r

j
m
i
(
Z
t
;
~

(
i
)
;
~
h
(
j
)
(
X
t
))
i



I
[
~

(
i
)
2B

0
;r
]
j
^
h
i
I
[
^
h
2B
h
0
;r
]

v
u
u
t
E
h
k
^
h
(
X
t
)

h
0
(
X
t
)
k
2
2
j
^
h
i
sup
h
2B
h
0
;r
E
""
sup

2B

0
;r
kr

r

j
m
i
(
Z
t
;;h
(
X
t
))
k
2
2
#
for
~
h
(
j
)
(
X
t
)
aconvexcombinationof
h
0
(
X
t
)
and
^
h
(
X
t
)
.Theconsistencyof
^
h
(Assumption
1
.
6
)andtheregularity
conditionAssumption
1
.
7b
thereforeimplythat
E
[
j
^
J
ij
(
^
h
)

^
J
ij
(
h
0
)
j
I
[
~

(
i
)
2B

0
;r
]
j
^
h
]
I
[
^
h
2B
h
0
;r
]
p
!
0
andhencethat
j
^
J
ij
(
^
h
)

^
J
ij
(
h
0
)
j
I
[
^
h
2B
h
0
;r
;
~

(
i
)
2B

0
;r
]
p
!
0
bythefollowinglemma,provedinSection
J.2
.
Lemma12.
Considerasequenceoftworandomvariables
X
n
;Z
n
,where
X
n
isa
d
-dimensionalrandomvector.
Supposethat
E

k
X
n
k
p
p
j
Z
n

p
!
0
forsome
p

1
.Then
X
n
p
!
0
.
NowAssumptions
1
.
6
and
1
.
5
andthecontinuousmappingtheoremimplythat
I
[
^
h
2B
h
0
;r
]
p
!
1
.Therefore,byLemma
11
,
wefurtherhave
j
^
J
ij
(
^
h
)

^
J
ij
(
h
0
)
j
I
[
~

(
i
)
2B

0
;r
]
p
!
0
.
OrthogonalMachineLearning:PowerandLimitations
TheregularityAssumptions
1
.
4
and
1
.
7a
additionallyimplytheuniformlawoflargenumbers,
sup

2B

0
;r
k
1
n
P
n
t
=1
r

m
i
(
Z
t
;;h
0
(
X
t
))

E
Z
[
r

m
i
(
Z;;h
0
(
X
))]
k
2
p
!
0
foreachmoment
m
i
(see,e.g.,
Newey&McFadden
,
1994
,Lem.2.4).Takentogether,theseconclusionsyield
h
^
J
i
(
^
h
)

E
Z
[
r

m
i
(
Z;
~

(
i
)
;h
0
(
X
))]
i
I
[
~

(
i
)
2B

0
;r
]
p
!
0
;
foreach
m
i
,where
^
J
i
(
^
h
)
denotesthe
i
-throwof
^
J
(
^
h
)
.
Since
~

(
i
)
isaconvexcombinationof
^

SS
and

0
,theconsistencyof
^

SS
impliesthat
~

(
i
)
p
!

0
andthere-
forethat
I
[
~

(
i
)
2B

0
;r
]
p
!
1
and
E
Z
[
r

m
i
(
Z;
~

(
i
)
;h
0
(
X
))]
p
!
E
Z
[
r

m
i
(
Z;
0
;h
0
(
X
))]
bythecontinuousmap-
pingtheorem.Lemma
11
thereforeimpliesthat
^
J
i
(
^
h
)

E
Z
[
r

m
i
(
Z;
~

(
i
)
;h
0
(
X
))]
p
!
0
andhencethat
^
J
i
(
^
h
)
p
!
E
Z
[
r

m
i
(
Z;
0
;h
0
(
X
))]
,asdesired.
A.2.AsymptoticNormalityof
B
.
Foravector

2
R
`
andavector

2
N
`
,wetheshorthand


,
Q
`
i
=1


`
`
.
Toestablishtheasymptoticnormalityof
B
,welet
k
=max

2
S
k

k
1
andapplyTaylor'stheoremwith
k
+1
-order
remainderaround
h
0
(
X
t
)
foreach
X
t
:
B
=
1
p
n
n
X
t
=1
m
(
Z
t
;
0
;h
0
(
X
t
))
|
{z
}
C
+
1
p
n
n
X
t
=1
X

:

2
S
1
k

k
1
!
D

m
(
Z
t
;
0
;h
0
(
X
t
))

^
h
(
X
t
)

h
0
(
X
t
)


|
{z
}
G
+
1
p
n
n
X
t
=1
X

:
k

k
1

k
62
S
1
k

k
1
!
D

m
(
Z
t
;
0
;h
0
(
X
t
))

^
h
(
X
t
)

h
0
(
X
t
)


|
{z
}
E
+
1
p
n
n
X
t
=1
X

:
k

k
1
=
k
+1
1
(
k
+1)!
2
4
D

m
1
(
Z
t
;
0
;
~
h
(1)
(
X
t
))

D

m
d
(
Z
t
;
0
;
~
h
(
d
)
(
X
t
))
3
5

^
h
(
X
t
)

h
0
(
X
t
)


|
{z
}
F
;
(11)
where
~
h
(
i
)
(
X
t
)
;i
=1
;
2
;:::;d
arevectorswhichare(potentiallydistinct)convexcombinationsof
^
h
(
X
t
)
and
h
0
(
X
t
)
.Note
that
C
isthesumof
n
i.i.d.mean-zerorandomvectorsdividedby
p
n
andthatthecovariance
V
=
Cov
(
m
(
Z;
0
;h
0
(
X
)))
ofeachvectorisbyAssumption
1
.
7d
.Hence,thecentrallimittheoremimpliesthat
C
!
d
N
(0
;V
)
.Itremainsto
showthat
G;E;F
p
!
0
.
Firstwearguethattheratesofstageconsistency(Assumption
1
.
6
)implythat
E;F
p
!
0
.Toachievethiswewillshow
that
E
[
j
E
i
jj
^
h
]
;
E
[
j
F
i
jj
^
h
]
p
!
0
,where
E
i
and
F
i
representthe
i
-thentriesof
E
and
F
respectively.Sincethenumberof
entries
d
isaconstant,Lemma
12
willthenimplythat
E;F
p
!
0
.Firstwehave
E
[
j
E
i
jj
^
h
]

X

:
k

k
1

k
62
S
p
n
k

k
1
!
E
Z
t
[
j
D

m
i
(
Z
t
;
0
;h
0
(
X
t
))(
^
h
(
X
t
)

h
0
(
X
t
))

j
]
(triangleinequality)

X

:
k

k
1

k
62
S
p
n
k

k
1
!
p
E
[
j
D

m
i
(
Z
t
;
0
;h
0
(
X
t
))
j
2
]
q
E
X
t
[
j
^
h
(
X
t
)

h
0
(
X
t
)
j
2

]
(Cauchy-Schwarz)

X

:
k

k
1

k
62
S
p
n
k

k
1
!


(

0
;h
0
)
1
=
4
q
E
X
t
[
j
^
h
(
X
t
)

h
0
(
X
t
)
j
2

]
(Assumption
1
.
7c
)

max

:
k

k
1

qk
62
S


(

0
;h
0
)
1
=
4
p
n
q
E
X
t
[
j
^
h
(
X
t
)

h
0
(
X
t
)
j
2

]
p
!
0
:
(Assumption
1
.
6
)
OrthogonalMachineLearning:PowerandLimitations
Since
~
h
(
i
)
isaconvexcombinationof
^
h
and
h
0
,parallelreasoningyields
E
[
j
F
i
jj
^
h
]
I
[
^
h
2B
h
0
;r
]

max

:
k

k
1
=
k
+1
I
[
^
h
2B
h
0
;r
]
q
E
Z
t
[
j
D

m
i
(
Z
t
;
0
;
~
h
(
i
)
(
X
t
))
j
2
]
p
n
q
E
X
t
[
j
^
h
(
X
t
)

h
0
(
X
t
)
j
2

]

max

:
k

k
1
=
k
+1


(

0
;h
0
)
1
=
4
p
n
q
E
X
t
[
j
^
h
(
X
t
)

h
0
(
X
t
)
j
2

]
p
!
0
:
(Assumptions
1
.
7c
and
1
.
6
)
AsinSection
A.1
,theconsistencyof
^
h
(Assumption
1
.
6
)furtherimpliesthat
E
[
j
F
i
jj
^
h
]
p
!
0
.
Finally,wearguethatorthogonalityandtheratesofthestageimplythat
G
p
!
0
.By
S
-orthogonalityofthemoments,
for

2
S
,
E
[
D

m
(
Z
t
;
0
;h
0
(
X
t
))
j
X
t
]=0
andinparticular
E
h
D

m
(
Z
t
;
0
;h
0
(
X
t
))

^
h
(
X
t
)

h
0
(
X
t
)


j
^
h
i
=
E
h
E
[
D

m
(
Z
t
;
0
;h
0
(
X
t
))
j
X
t
]

^
h
(
X
t
)

h
0
(
X
t
)


j
^
h
i
=0
:
(12)
Wenowshowthat
E
h
G
2
i
j
^
h
i
p
!
0
.Wehave
E
h
G
2
i
j
^
h
i
=
1
n
X
t;t
0
=1
;
2
;:::;n;t
6
=
t
0
E
2
4
X

:
k

k
1

k
2
S
1
k

k
1
!
D

m
i
(
Z
t
;
0
;h
0
(
X
t
))

^
h
(
X
t
)

h
0
(
X
t
)


j
^
h
3
5
2
+
1
n
n
X
t
=
t
0
=1
E
2
6
4
0
@
X

:
k

k
1

k
2
S
1
k

k
1
!
D

m
i
(
Z
t
;
0
;h
0
(
X
t
))

^
h
(
X
t
)

h
0
(
X
t
)


1
A
2
j
^
h
3
7
5
Allthecrosstermsarezerobecauseof(
12
).Therefore:
E
h
G
2
i
j
^
h
i
=
E
2
4
 
X

:

2
S
1
k

k
1
!
D

m
i
(
Z
t
;
0
;h
0
(
X
t
))

^
h
(
X
t
)

h
0
(
X
t
)


!
2
j
^
h
3
5

E
""
X

:

2
S
1
k

k
1
!

D

m
i
(
Z
t
;
0
;h
0
(
X
t
))

^
h
(
X
t
)

h
0
(
X
t
)



2
j
^
h
#
(Jensen'sinequality)

max

:

2
S
E


D

m
i
(
Z
t
;
0
;h
0
(
X
t
))

^
h
(
X
t
)

h
0
(
X
t
)



2
j
^
h


max

:

2
S
r
E
h
(
D

m
i
(
Z
t
;
0
;h
0
(
X
t
)))
4
i
s
E


^
h
(
X
t
)

h
0
(
X
t
)

4

j
^
h

(Cauchy-Schwarz)
=max

:

2
S
p


(

0
;h
0
)
s
E


^
h
(
X
t
)

h
0
(
X
t
)

4

j
^
h

(Assumption
1
.
7c
)
GivenAssumption
1
.
5
wegetthatthelatterconvergestozeroinprobability.Giventhatthenumberofmoments
d
isalsoa
constant,wehaveshownthat
E
[
k
G
k
2
2
j
^
h
]
p
!
0
.ByLemma
12
thelatterimpliesthat
G
p
!
0
.
Theproofforthe
K
-foldcrossestimator
^

CF
followspreciselythesamestepsasthe
^

SS
proof(with
p
2
n
scaling
insteadof
p
n
scaling)exceptfortheargumentconcerning
G
p
!
0
.Inthiscase
G
=
P
K
k
=1
G
k
,where,for
k
=1
;:::;K
.
G
k
=
1
p
2
n
X
t
2
I
k
X

:

2
S
1
k

k
1
!
D

m
(
Z
t
;
0
;h
k
(
X
t
))

^
h
k
(
X
t
)

h
k
(
X
t
)


:
K
istreatedasconstantwithrespecttotheotherproblemparameters,andthereforeitsufcestoshow
G
k
p
!
0
,forall
k
=1
;
2
;:::;K
.Fix
k
2
[
K
]
.ByLemma
12
itsuftoshow
E
h
G
2
k
j
^
h
k
i
p
!
0
.Theproofofthisfollowsexactlythe
samestepsasproving
E
h
G
2
j
^
h
i
p
!
0
inthe
^

SS
case.Thediagonaltermscanbeboundedinanidenticalwayandthe
crosstermsarezeroagainbecause
^
h
k
istrainedinthestageondata
(
X
t
)
t
2
I
c
k
andthereforethedata
(
X
t
)
t
2
I
k
remain
independentgiven
^
h
k
.Ourproofiscomplete.
OrthogonalMachineLearning:PowerandLimitations
B.ProofofTheorem
2
Weprovetheresultforthesample-splittingestimator
^

SS
in
(
2
)
.Theproofforthe
K
-foldcrossestimator
^

CF
in
(
3
)
isanalogousandfollowsasin(
Chernozhukovetal.
,
2017
).
Fixanycompact
A


.Ourinitialgoalistoestablishtheuniformconvergence
sup

2
A
j
1
n
P
n
t
=1
m
i
(
Z
t
;;
^
h
(
X
t
))

E
[
m
i
(
Z;;h
0
(
X
))]
j
p
!
0
(13)
foreachmoment
m
i
.Tothisend,wenotethatthecontinuity(Assumption
1
.
4
)anddomination(Assumption
1
.
7d
)of
m
i
implytheuniformlawoflargenumbers
sup

2
A;h
2B
h
0
;r
j
1
n
P
n
t
=1
m
i
(
Z
t
;;h
(
X
t
))

E
Z
[
m
i
(
Z;;h
(
X
))]
j
p
!
0
foreachmoment
m
i
(see,e.g.,
Newey&McFadden
,
1994
,Lem.2.4).Moreover,themeanvaluetheoremandtwo
applicationsofCauchy-Schwarzyield
j
E
[
m
i
(
Z;;
^
h
(
X
))
j
^
h
]

E
[
m
i
(
Z;;h
0
(
X
))]
jj
E
[
hr

m
i
(
Z;;
~
h
(
i
)
(
X
))
;
^
h
(
X
)

h
0
(
X
)
ij
^
h
]
j
j
E
[
kr

m
i
(
Z;;
~
h
(
i
)
(
X
))
k
2
k
^
h
(
X
)

h
0
(
X
)
k
2
j
^
h
]

q
E
[
kr

m
i
(
Z;;
~
h
(
i
)
(
X
))
k
2
2
j
^
h
]
E
[
k
^
h
(
X
)

h
0
(
X
)
k
2
2
j
^
h
]
for
~
h
(
i
)
aconvexcombinationof
h
0
and
^
h
.Hence,theuniformboundonthemomentsof
r

m
i
(Assumption
1
.
7e
)andthe
consistencyof
^
h
(Assumption
1
.
5
)imply
sup

2
A
j
E
[
m
i
(
Z;;
^
h
(
X
))
j
^
h
]

E
[
m
i
(
Z;;h
0
(
X
))]
j
p
!
0
,andtherefore
I
[
^
h
2B
h
0
;r
]sup

2
A
j
1
n
P
n
t
=1
m
i
(
Z
t
;;
^
h
(
X
t
))

E
[
m
i
(
Z;;h
0
(
X
))]
j
p
!
0
bythetriangleinequality.Since
I
[
^
h
2B
h
0
;r
]
p
!
1
bytheassumedconsistencyof
^
h
,theuniformconvergence(
13
)follows
fromLemma
11
.Giventheuniformconvergence
(
13
)
,standardargumentsnowimplyconsistencygiven
(Assumption
1
.
2
)andeitherthecompactnessconditionsofAssumption
2
.
1
(see,e.g.,
Newey&McFadden
,
1994
,Thm.2.6)
ortheconvexityconditionsofAssumption
2
.
2
(see,e.g.,
Newey&McFadden
,
1994
,Thm.2.7).
C.ProofofLemma
3
Wewillusetheinequalitythatforanyvectorofrandomvariables
(
W
1
;:::;W
K
)
,
E
h
Q
K
i
=1
j
W
i
j
i

Q
K
i
=1
E

j
W
i
j
K

1
K
;
whichfollowsfromrepeatedapplicationofH
¨
older'sinequality.Inparticular,wehave
E
X

Q
`
i
=1



^
h
i
(
X
)

h
0
;i
(
X
)



2

i


Q
`
i
=1
E
X




^
h
i
(
X
)

h
0
;i
(
X
)



2
k

k
1


i
=
k

k
1
=
Q
`
i
=1
k
^
h
i

h
0
;i
k
2

i
2
k

k
1
Thusthepartfollowsbytakingtherootofthelatterinequalityandmultiplyingby
p
n
.Forthesecondpartofthe
lemma,observethatundertheconditionforeachnuisancefunctionwehave:
p
n
`
Y
i
=1
k
^
h
i

h
0
;i
k

i
2
k

k
1
=
n
1
2

P
`
i
=1

i

i
k

k
1
`
Y
i
=1

n
1

i
k

k
1
k
^
h
i

h
0
;i
k
2
k

k
1


i
If
1
2

P
`
i
=1

i

i
k

k
1

0
,thenallpartsintheaboveproductconvergeto
0
inprobability.
Forthesecondpartforall

2
S
wesimilarlyhave
E
X

Q
`
i
=1



^
h
i
(
X
)

h
0
;i
(
X
)



4

i


Q
`
i
=1
E
X




^
h
i
(
X
)

h
0
;i
(
X
)



4
k

k
1


i
=
4
k

k
1
=
Q
`
i
=1
k
^
h
i

h
0
;i
k
4

i
4
k

k
1
OrthogonalMachineLearning:PowerandLimitations
HencetosatisfyAssumption
1
.
5
itsuftosatisfy
8

2
S;
8
i
,
k
^
h
i

h
0
;i
k
4
k

k
1
p
!
0
.ButbyHolderinequalityandour
hypothesiswehave
k
^
h
i

h
0
;i
k
4
k

k
1
k
^
h
i

h
0
;i
k
4[max

2
S
k

k
1
]
p
!
0
;
aswewanted.
D.ProofofTheorem
5
SupposethatthePLRmodelholdswiththeconditionaldistributionof

given
X
Gaussian.Consideragenericmoment
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
,where
h
0
(
X
)
representsanyadditionalnuisanceindependentof
f
0
(
X
)
;g
0
(
X
)
.Wewill
provetheresultbycontradiction.Assumethat
m
is
2
-orthogonalwithrespectto
(
f
0
(
X
)
;g
0
(
X
))
andAssumption
1
.
By
0
-orthogonality,wehave
E
[
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
(14)
foranychoiceoftruemodelparameters
(

0
;f
0
;g
0
;h
0
)
,so
r
f
0
(
X
)
E
[
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=
r
g
0
(
X
)
E
[
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
:
Since
m
iscontinuouslydifferentiable(Assumption
1
.
4
),wemaydifferentiateundertheintegralsign(
Flanders
,
1973
)to
that
0=
r
f
0
(
X
)
E
[
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
=
r
f
0
(
X
)
E
[
m
(
T;
0
T
+
f
0
(
X
)+

0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
=
E
[
r
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))+
r
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
and
0=
r
g
0
(
X
)
E
[
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
=
r
g
0
(
X
)
E
[
m
(
g
0
(
X
)+
;
0
(
g
0
(
X
)+

)+
f
0
(
X
)+
;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
=
E
[
r
1
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))+
r
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))

0
j
X
]
+
E
[
r
5
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
:
Moreover,by
1
-orthogonality,wehave
E
[
r
i
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
for
i
2f
4
;
5
g
,so
E
[
r
i
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
;
8
i
2f
1
;
2
;
4
;
5
g
and
8
(

0
;f;g;h
)
:
(15)
Hence,
r
g
0
(
X
)
E
[
r
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=
r
f
0
(
X
)
E
[
r
1
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
;
andweagainexchangederivativeandintegralusingthecontinuityof
r
2
m
(Assumption
1
.
4
)(
Flanders
,
1973
)to
E
[
r
1
;
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))+
r
5
;
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))]
+
E
[

0
r
2
;
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
=
E
[
r
4
;
1
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))+
r
2
;
1
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
:
Sincethepartialderivativesof
m
aredifferentiablebyAssumption
1
.
4
,wehave
r
1
;
4
m
=
r
4
;
1
m
andtherefore
E
[
r
5
;
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]+

0
E
[
r
2
;
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
=
E
[
r
2
;
1
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
By
2
-orthogonality,
E
[
r
5
;
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
,andhence

0
E
[
r
2
;
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=
E
[
r
2
;
1
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
:
(16)
Notethatequality(
15
)alsoimplies
0=
r
f
0
(
X
)
E
[
r
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=
r
f
0
(
X
)
E
[
r
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
:
OrthogonalMachineLearning:PowerandLimitations
Weagainexchangederivativeandintegralusingthecontinuityof
r
2
m
(Assumption
1
.
4
)(
Flanders
,
1973
)to
0=
E
[
r
2
;
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))+
r
2
;
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
(17)
=
E
[
r
4
;
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))+
r
4
;
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
:
Sincethepartialderivativesof
m
arecontinuousbyAssumption
1
.
4
,wehave
r
2
;
4
m
=
r
4
;
2
m
andtherefore
E
[
r
2
;
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=
E
[
r
4
;
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
By
2
-orthogonality,
E
[
r
4
;
4
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
,andhence
E
[
r
2
;
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
(18)
Combiningtheequalities(
16
),(
17
),and(
18
)wethat
E
[
r
2
;
1
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
:
(19)
Now,the
0
-orthogonalitycondition
(
14
)
,thecontinuityof
r
m
(Assumption
1
.
4
),anddifferentiationundertheintegralsign
(
Flanders
,
1973
)implythat
0=
r

0
E
[
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=
r

0
E
[
m
(
T;
0
T
+
f
0
(
X
)+

0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
=
E
[
r
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))

T
+
r
3
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
:
Since
T
=
g
0
(
X
)+

and
E
[
r
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
byequality
14
,
E
[
r
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))


+
r
3
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
(20)
Since

isconditionallyGaussiangiven
X
,Stein'slemma(
Stein
,
1981
),thesymmetryofthepartialderivativesof
m
,and
theequality
19
implythat
E
[
r
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))


j
X
]=
E
[
r
2
m
(
g
0
(
X
)+
;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))


j
X
]
=
E
[
r
;
2
m
(
g
0
(
X
)+
;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]
=
E
[
r
1
;
2
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=
E
[
r
2
;
1
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
:
Hencetheequality(
20
)gives
E
[
r
3
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
j
X
]=0
whichcontradictsAssumption
1
.
3
.
E.ProofofProposition
6
Fixanymomentoftheform
m
(
T;Y;;f
(
X
)
;g
(
X
)
;h
(
X
))
,where
h
representsanynuisanceinadditionto
(
f;g
)
.Let
F
bethespaceofallvalidnuisancefunctions
(
f;g;h
)
and
F
(
X
)=
f
(
f
(
X
)
;g
(
X
)
;h
(
X
)):(
f;g;h
)
2
F
g
.
Weprovethelemmabycontradiction.Suppose
m
thethreehypothesisofourlemma.Westartbyestab-
lishingthat
Var
(
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
)))=0
forall
(

0
;f
0
;g
0
;h
0
)
.Fixany
(

0
;f
0
;g
0
;h
0
)
,andsuppose
Var
(
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
)))
>
0
.AsinthebeginningoftheproofofTheorem
1
themeanvaluetheorem
implies
^
J
(
^
f;
^
g;
^
h
)
p
n
(

0

^

SS
)=
1
p
n
n
X
t
=1
m

T
t
;Y
t
;
0
;
^
f
(
X
t
)
;
^
g
(
X
t
)
;
^
h
(
X
t
)

|
{z
}
B
(21)
where
^
J
(
f;g;h
)
,
1
n
P
n
t
=1
r

m
(
T
t
;Y
t
;
~
;f
(
X
t
)
;g
(
X
t
)
;h
(
X
t
))
;
forsome
~

whichisaconvexcombinationof
^

SS
;
0
.
IntheproofofTheorem
1
weonlyuseAssumption
1
.
3
toinvert
J
=
E
[
r

m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
)]
whichis
thein-probabilitylimitof
^
J
(
^
f;
^
g;
^
h
)
.Inparticular,bothofthefollowingresultsestablishedintheproofoftheTheorem
1
remaintrueinoursetting:

B
tendstoanormaldistributionwithmeanzeroandvarianceVar
(
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
)))
>
0
.

^
J
(
^
f;
^
g;
^
h
)
convergesinprobabilityto
J
.
OrthogonalMachineLearning:PowerandLimitations
Sinceinthiscase
J
=0
,asAssumption
1
.3isviolated,and
p
n
(

0

^

SS
)
isboundedinprobability,wegetthat
^
J
(
^
f;
^
g;
^
h
)
p
n
(

0

^

SS
)
convergestozeroinprobability.By
(
21
)
,thiscontradictsthefactthat
B
convergestoadistribution
withnon-zerovariance.Hence,Var
(
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
)))=0
asdesired.
Nowrecallingthat,forall
(

0
;f
0
;g
0
;h
0
)
,
E
[
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))]=0
,weconcludethatforall
(

0
;f
0
;g
0
;h
0
)
,
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))=0
,almostsurelywithrespecttotherandomvariables
X;
.Now

(

0
;f
0
;g
0
;h
0
)
.Nowsupposethatforsome
(
a;b
)
2
R
2
,
m
(
a;b;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
6
=0
.Then,since
m
is
continuous,thereexistsaneighborhood
N
suchthat
m
(
a
0
;b
0
;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
6
=0
forall
(
a
0
;b
0
)
2N
.Sincethe
conditionaldistributionof

hasfullsupport(a.s.X)and,given
X
,
(
T;Y
)
isaninvertiblelinearfunctionof
(

)
,thecon-
ditionaldistributionof
(
T;Y
)
given
X
alsohasfullsupporton
R
2
(a.s.X).Hence,
Pr(
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
6
=
0)

Pr((
T;Y
)
2N
)
>
0
.Thisisacontradictionas
m
(
T;Y;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))
isa.s.zero.Therefore,for
almostevery
X
andall
a;b
2
R
and
(

0
;f
0
;g
0
;h
0
)
,
m
(
a;b;
0
;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))=0
.Sincethedistributionof
X
is
independentof

0
and
j

j
2
,wethereforehave
E
[
m
(
Y;T;;f
0
(
X
)
;g
0
(
X
)
;h
0
(
X
))]=0
forsome

6
=

0
,whichcontradicts.
F.ProofofLemma
7
SincethecharacteristicfunctionofaGaussiandistributionisandonthewholerealline,Levy'sInversion
FormulaimpliesthattheGaussiandistributionisuniquelycharacterizedbyitsmoments(
Durrett
,
2010
,Sec.3.3.1).
G.ProofofTheorem
8
Smoothnessfollowsfromthefactthat
m
isapolynomialin
(
;q
(
X
)
;g
(
X
)
;
r

1
(
X
))
.Non-degeneracyfollowsfromthe
PLRequations
5
),theproperty
E
[

j
X
]=0
,andourchoiceof
r
as
E
[
r

m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])]=

E
[(
T

g
0
(
X
))(

r

E
[

r
j
X
]

r
E
[

r

1
j
X
])]
=

E
[

(

r

E
[

r
j
X
]

r
E
[

r

1
j
X
]]
=

E
[
E
[

r
+1
j
X
]

r
E
[

2
j
X
]
E
[

r

1
j
X
]]
6
=0
:
Wenextestablish
0
-orthogonalityusingtheproperty
E
[

j
X;T
]=0
of
5
:
E

m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])
j
X

=
E
[



r

E
[

r
j
X
]

r
E
[

r

1
j
X
]
j
X

]=0
:
Ourchoiceof
r
furtherimpliesas,for

6
=

0
,
E

m
(
Z;;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])

=(

0


)
E
[
E
[

r
+1
j
X
]

E
[

j
X
]
E
[

r
j
X
]

rE
[

2
j
X
]
E
[

r

1
j
X
]]
=(

0


)
E
[
E
[

r
+1
j
X
]

rE
[

2
j
X
]
E
[

r

1
j
X
]]
6
=0
:
Weinvoketheproperties
E
[

j
X
]=0
and
E
[

j
X;T
]=0
of
5
toderive
1
-orthogonalityvia
E

r
q
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])
j
X

=

E


r

E
[

r
j
X
]

r
E
[

r

1
j
X
]
j
X

=0
;
E

r
g
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])
j
X

=

0
E


r

E
[

r
j
X
]

r
E
[

r

1
j
X
]
j
X


E


(
r
r

1

r
E
[

r

1
j
X
])
j
X

=0
;
and
E

r

r

1
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])

=

E
[
r
j
X
]=0
:
Thesamepropertiesalsoyield
2
-orthogonalityforthesecondpartialderivativesof
q
(
X
)
via
E
h
r
2
q
(
X
)
;q
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])
j
X
i
=0
;
E
h
r
2
q
(
X
)
;g
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])
j
X
i
=
E

r
r

1

r
E
[

r

1
j
X
]
j
X

=0
;
and
E
h
r
2
q
(
X
)

r

1
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])
j
X
i
=
E
[
r
j
X
]=0
;
OrthogonalMachineLearning:PowerandLimitations
forthesecondpartialderivativesof
g
(
X
)
via
E
h
r
2
g
(
X
)
;g
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])
j
X
i
=
E


(
r
r

1

r
E
[

r

1
j
X
])+
r
(
r

1)

r

2
j
X

=
r
(
r

2)
E

E
[

j
X;T
]

r

2
j
X

=0
and
E
h
r
2
g
(
X
)

r

1
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])
j
X
i
=


0
E
[
r
j
X
]+
E
[
r
j
X
]=0
;
andforthesecondpartialderivativesof

r

1
(
X
)
via
E
h
r
2

r

1
(
X
)

r

1
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
])
j
X
i
=0
:
Thisestablishes
2
-orthogonality.
H.ProofofTheorem
9
ThemajorityoftheproofisidenticaltothatofTheorem
8
;itonlyremainstoshowthattheadvertisedpartialderivatives
withrespectto

r
(
X
)
arealsomeanzerogiven
X
.Theseequalitiesfollowfromtheproperty
E
[

j
X
]=0
of
5
:
E

r

r
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
]
;
E
[

r
j
X
]))
j
X

=

E
[

j
X
]=0
;
E
h
r
2

r
(
X
)

r
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
]
;
E
[

r
j
X
]))
j
X
i
=0
;
and
E
h
r
2

r
(
X
)

r

1
(
X
)
m
(
Z;
0
;q
0
(
X
)
;g
0
(
X
)
;
E
[

r

1
j
X
]
;
E
[

r
j
X
]))
j
X
i
=0
:
I.ProofofTheorem
10
Weprovetheresultexplicitlyfortheexcesskurtosissettingwith
E
[

4
]
6
=3
E
[

2
]
2
.Aparallelargumentyieldstheresultfor
non-zeroskewness(
E
[

3
]
6
=0
).
Toestablish
p
n
-consistencyandasymptoticnormality,itsuftocheckeachofthepreconditionsofTheorems
1
and
2
.Since

isindependentof
X
and
E
[

4
]
6
=3
E
[

2
]
2
,theconditionsofTheorem
9
arewith
r
=3
.
Hence,themoments
m
ofTheorem
9
satisfy
S
-orthogonality(Assumption
1
.
1
)for
S
=
f

2
N
4
:
k

k
1

2
gn
f
(1
;
0
;
0
;
1)
;
(0
;
1
;
0
;
1)
g
withrespecttothenuisance
(
h
q
0
;X
i
;
h

0
;X
i
;
E
[

2
]
;
E
[

3
])
,(Assumption
1
.
2
),non-
degeneracyof
E
[
r

m
(
Z;
0
;h
0
(
X
))]
(Assumption
1
.
3
),andcontinuityof
r
m
2
(Assumption
1
.
4
).Theformof
m
,the
standardGaussiani.i.d.componentsof
X
,andthealmostsureboundednessof

and

furtherimplythattheregularity
conditionsofAssumption
1
.
7
areallforanychoiceof
r>
0
.Hence,itonlyremainstoestablishthestage
consistencyandrateassumptions(Assumptions
1
.
5
and
1
.
6
)andtheconvexityconditions(Assumption
2
.
2
).
I.1.CheckingRateofFirstStage(Assumption
1
.
6
)
WebeginwithAssumption
1
.
6
.Since
f

2
N
4
:
k

k
1

3
gn
S
=
f

2
N
4
:
k

k
1
=3
g[f
(1
;
0
;
0
;
1)
;
(0
;
1
;
0
;
1)
g
by
Lemma
3
,itsuftoestablishthesufcientcondition
(
6
)
for

=(0
;
1
;
0
;
1)
and

=(1
;
0
;
0
;
1)
andthecondition
(
7
)
for
the

with
k

k
1
=3
.Hence,itsuftosatisfy
(1)
n
1
2
E
X
[
jh
X;
^
q

q
0
ij
4
]
1
4
j
^

3

E
[

3
]
j
p
!
0
,whichcorrespondsto

=(1
;
0
;
0
;
1)
andcondition(
6
),
(2)
n
1
2
E
X
[
jh
X;
^



0
ij
4
]
1
4
j
^

3

E
[

3
]
j
p
!
0
,whichcorrespondsto

=(0
;
1
;
0
;
1)
andcondition(
6
),
(3)
n
1
2
E
X
[
jh
X;
^
q

q
0
ij
6
]
1
2
p
!
0
,
(4)
n
1
2
E
X
[
jh
X;
^



0
ij
6
]
1
2
p
!
0
,
(5)
n
1
2
j
^

2

E
[

2
]
j
3
p
!
0
,and
(6)
n
1
2
j
^

3

E
[

3
]
j
3
p
!
0
,
OrthogonalMachineLearning:PowerandLimitations
where
X
avectorofi.i.d.mean-zerostandardGaussianentries,independentfromthestage,andtheconvergenceto
zeroisconsideredinprobabilitywithrespecttothestagerandomvariables.
Wewillestimate
q;
0
usinghalfofoursampleanduseourestimate
^

toproduceanestimateofthesecondand
thirdmomentsof

basedontheotherhalfofthesampleandthefollowinglemma.
Lemma13.
Supposethatanestimator
^

2
R
p
basedon
n
samplepoints
E
X
[
jh
X;
^



0
ij
6
]
1
2
=
O
P

1
p
n

for
X
independentof
^

.If
^

2
:=
1
n
P
n
t
=1
(
T
0
t
h
X
0
t
;
^

i
)
2
and
^

3
:=
1
n
P
n
t
=1
(
T
0
t
h
X
0
t
;
^

i
)
3

3
1
n
P
n
t
=1
(
T
0
t
h
X
0
t
;
^

i
)^

2
for
(
T
0
t
;X
0
t
)
n
t
=1
i.i.d.replicatesof
(
T;X
)
independentof
^

,then
j
^

2

E
[

2
]
j
=
O
P

1
n
1
3

and
j
^

3

E
[

3
]
j
=
O
P

1
p
n

:
(22)
Asaresult,
n
1
2
j
^

2

E
[

2
]
j
3
p
!
0
and
n
1
2
j
^

3

E
[

3
]
j
3
p
!
0
:
Proof.
Webeginwiththethirdmomentestimation.Foranewdatapoint
(
T;X
)
independentof
^

,

,
h
X;
0

^

i
sothat
T
h
X;
^

i
=

+

.Since

isindependentof
(
X;
^

)
,and
E
[

]=0
,wehave
E
X
[(

+

)
3
]

3
E
X
[(

+

)]
E
X
[(

+

)
2
]=
E
[

3
]+
E
X
[

3
]

3
E
X
[

2
]
E
X
[

]
orequivalently
E
[

3
]=
E
X
[(

+

)
3
]

3
E
X
[(

+

)]
E
X
[(

+

)
2
]

E
X
[

3
]+3
E
X
[

2
]
E
X
[

]
:
(23)
Since
E
X
[
j

j
3
]

E
X
[

6
]
1
2
=
O
P
(1
=
p
n
)
byCauchy-Schwarzandourassumptionon
^

,and
j
E
X
[

]
E
X
[

2
]
j
E
X
[
j

j
3
]
byHolder'sinequality,theequality(
23
)impliesthat
j
E
[

3
]

(
E
X
[(

+

)
3
]

3
E
X
[

+

]
E
X
[(

+

)
2
])
j
=
O
P
(1
=
p
n
)
:
Since
E
X
[(

+

)
6
]=
O
(1)
,thecentrallimittheorem,thestronglawoflargenumbers,andSlutsky'stheoremimplythat
^

3

(
E
X
[(

+

)
3
]

3
E
X
[

+

]
E
X
[(

+

)
2
])=
O
P
(1
=
p
n
)
:
Therefore,
j
^

3

E
[

3
]
j
=
O
P
(1
=
p
n
)
:
Thesecondmomentestimationfollowssimilarlyusingtheidentity,
E
[

2
]=
E
X
[(

+

)
2
]

E
X
[

2
]
;
andthefactthat
E
X
[

2
]

E
X
[
j

j
3
]
2
3
=
O
P
(
n

1
3
)
byHolder'sinequality.
InlightofLemma
13
itsuftoestimatethevectors
q
0
and

0
using
n
samplepointssothat

n
1
2
E
X
[
jh
X;
^
q

q
0
ij
4
]
1
4
n

1
2
p
!
0
,
E
X
[
jh
X;
^
q

q
0
ij
4
]
1
4
p
!
0
,

n
1
2
E
X
[
jh
X;
^



0
ij
4
]
1
4
n

1
2
p
!
0
,
E
X
[
jh
X;
^



0
ij
4
]
1
4
p
!
0
,

n
1
2
E
X
[
jh
X;
^
q

q
0
ij
6
]
1
2
p
!
0
,and

n
1
2
E
X
[
jh
X;
^



0
ij
6
]
1
2
p
!
0
,
andtherestoftheconditionswillfollow.Toachievetheseconclusionsweusethefollowingresultontheperformanceof
theLasso.Thefollowingtheoremisdistilledfrom(
Hastieetal.
,
2015
,Chapter11).
OrthogonalMachineLearning:PowerandLimitations
Theorem14.
Let
p;s
2
N
with
s

p
and
s
=
o
(
n
2
=
3
=
log
p
)
and
˙>
0
,andsupposethatweobservei.i.d.datapoints
(
~
Y
i
;
~
X
i
)
n
i
=1
distributedaccordingtothemodel
~
Y
=
h
~
X;
0
i
+
w
foran
s
-sparse

0
2
R
p
,
~
X
2
R
p
withstandard
Gaussianentries,and
w
2
R
p
independentmean-zeronoisewith
k
w
k
1

˙
.Supposethat
p
growstowith
n
.Thenwithachoiceoftuningparameter

n
=2
˙
p
3log
p=n
,theLassoestimate
^

0
tothisdataset
k
^

0


0
k
2
=
O
P
(
p
s
log
p=n
)
.
Proof.
UsingTheorem11.1andExample11.2of(
Hastieetal.
,
2015
),weknowthatsince
~
X
hasiid
N
(0
;
1)
entries,if

n
=2
˙
p
3log(
p
)
=n
,wehave
Pr
""
k
^

0


0
k
2
˙
p
3
s
log
p=n
>
1
#

2exp
ˆ

1
2
log(
p
)
˙
:
(24)
Since
p
growsunboundedlywith
n
,foranyed
>
0
,wehavethatforsomesome
N

,if
n>N

,therighthand
sideisatmost

.Thuswecanconcludethat:
k
^

0


0
k
2
=
O
p

p
s
log
p=n

.
Noticethatfor
q
0
weknow
Y
=

0
T
+
h
X;
0
i
+

=

0
h
X;
0
i
+

0

+
h
X;
0
i
+

(fromtheof
T
)
=
h
X;q
0
i
+

0

+

(since
q
0
=

0

0
+

0
)
Hence,
Y
=
h
X;q
0
i
+

+

0
;
andweknowthatthenoiseterm,

+

0

isalmostsurelyboundedby
C
+
CM
=
C
(
M
+1)
.Hence,byTheorem
14
,ourLassoestimate
^
q

k
^
q

q
0
k
2
=
O
P
(
p
s
log
p=n
)
.Similarly,ourLassoestimate
^


k
^



0
k
2
=
O
P
(
p
s
log
p=n
)
.
Now,since
X
hasiidmean-zerostandardGaussiancomponents,weknowthatforallvectors
v
2
R
p
and
a
2
N
itholds
E
[
jh
X;v
ij
a
]=
O

p
a
a
k
v
k
a
2

.Applyingthisto
v
=^
q

q
and
v
=^



0
for
a
2f
4
;
6
g
wehave
E
X
[
jh
X;
^
q

q
0
ij
4
]=
O
(
k
^
q

q
0
k
4
2
)=
O
P
 

q
s
log
p
n

4
!
E
X
[
jh
X;
^



0
ij
4
]=
O
(
k
^



0
k
4
2
)=
O
P
 

q
s
log
p
n

4
!
E
X
[
jh
X;
^
q

q
0
ij
6
]=
O
(
k
^
q

q
0
k
6
2
)=
O
P
 

q
s
log
p
n

6
!
E
X
[
jh
X;
^



0
ij
6
]=
O
(
k
^



0
k
6
2
)=
O
P
 

q
s
log
p
n

6
!
:
Nowforthesparsitylevel
s
=
o

n
2
=
3
log
p

wehave
q
s
log
p
n
=
o
(
n

1
6
)
whichimpliesallofthedesiredconditionsfor
Assumption
1
.
6
.
I.2.CheckingConsistencyofFirstStage(Assumption
1
.
5
)
NextweprovethatAssumption
1
.
5
isSince
max

2
S
k

k
1
=2
itsufcesbyLemma
3
toshowthatforour
choicesof
^
;
^
q;
^

2
,and
^

3
wehave
E
X
[
jh
X;
^
q

q
0
ij
8
]
1
8
p
!
0
(25)
E
X
[
jh
X;
^



0
ij
8
]
1
8
p
!
0
(26)
j
^

2

E
[

2
]
j
p
!
0
(27)
j
^

3

E
[

3
]
j
p
!
0
:
(28)
OrthogonalMachineLearning:PowerandLimitations
Parts
(
27
)
and
(
28
)
followdirectlyfromLemma
13
.Since
X
consistsofstandardGaussianentries,ananalogousargument
tothataboveimpliesthat
E
X
[
jh
X;
^
q

q
0
ij
8
]
1
8
=
O
(
k
^
q

q
0
k
2
)=
O
P

q
s
log
p
n

E
X
[
jh
X;
^



0
ij
8
]
1
8
=
O
(
k
^



0
k
2
)=
O
P

q
s
log
p
n

:
Nowforthesparsitylevel
s
=
o
(
n
2
3
(
M
+1)
2
log
p
)
wehave
q
s
log
p
n
=
o
(1)
whichimpliesalsoconditions(
25
)and(
26
).
I.3.CheckingConvexityConditions(Assumption
2
.
2
)
Finally,weestablishtheconvexityconditions(Assumption
2
.
2
).Weconsider
=
R
,whichisconvex.Withoutloss
ofgenerality,assume
3
E
[

2
]
2
>
E
[

4
]
;otherwise,onecanestablishtheconvexityconditionsfor

m
.Let
F
n
(

)=
1
n
P
n
t
=1
m
(
Z
t
;;
^
h
(
X
t
))
.Since
F
n
iscontinuouslydifferentiable,
F
n
isthederivativeofaconvexfunctionwhenever
r
F
n
(

)

0
,forall

2

.Since
F
n
islinearin

wehaveforall

2

r
F
n
(

)=
1
n
n
X
t
=1

(
T
t
h
^
;X
t
i
)
4
+(
T
t
h
^
;X
t
i
)^

3
+3(
T
t
h
^
;X
t
i
)
2
^

2
;
theestablishedconsistencyof
(^
;
^

3
;
^

2
)
andSlutsky'stheoremimplythat
r
F
n
(

)

1
n
n
X
t
=1

(
T
t
h
;X
t
i
)
4
+(
T
t
h
;X
t
i
)
E
[

3
]+3(
T
t
h
;X
t
i
)
2
E
[

2
]
p
!
0
:
Thestronglawoflargenumbersnowyields
r
F
n
(

)

(3
E
[

2
]
2

E
[

4
])
p
!
0
:
Hence,
Pr(
r
F
n
(

)
<
0)

Pr(
jr
F
n
(

)

(3
E
[

2
]
2

E
[

4
])
j
>
3
E
[

2
]
2

E
[

4
])
!
0
;
verifyingAssumption
2
.
2
.Theproofiscomplete.
J.ProofsofAuxiliaryLemmata
J.1.ProofofLemma
11
Sinceeach
Y
n
isbinary,and
Y
n
p
!
1
,forevery
>
0
,
Pr[
j
X
n
(1

Y
n
)
j
>
]

Pr[
Y
n
=0]=Pr[
j
1

Y
n
j
>
1
=
2]
!
0
:
Hence,
X
n
(1

Y
n
)
p
!
0
.BothadvertisedclaimsnowfollowbySlutsky'stheorem(
vanderVaart
,
1998
,Thm.2.8).
J.2.ProofofLemma
12
Let
X
n;i
denotethe
i
-thcoordinateof
X
n
,i.e.
k
X
n
k
p
p
=
P
d
i
=1
X
p
n;i
.Bytheassumptionofthelemma,wehavethatfor
every

,thereexists
n
(

)
,suchthatforall
n

n
(

)
:
Pr
h
max
i
E
[
j
X
n;i
j
p
j
Z
n
]
>
i
<
Let
E
n
denotetheevent
f
max
i
E
[
j
X
n;i
j
p
j
Z
n
]


g
.Hence,
Pr[
E
n
]

1


,forany
n

n
(

)
.ByMarkov'sinequality,
forany
n

n
(

p

=
2
d
;

=
2
d
)
,theevent
E
n
impliesthat:
Pr[
j
X
n;i
j
p
>
p
j
Z
n
]

E
[
j
X
n;i
j
p
j
Z
n
]

p


2
d
OrthogonalMachineLearning:PowerandLimitations
Thus,wehave:
Pr[
j
X
n;i
j
>
]=
E
[Pr[
j
X
n;i
j
p
>
p
j
Z
n
]]
=
E
[Pr[
j
X
n;i
j
p
>
p
j
Z
n
]
jE
n
]

Pr[
E
n
]+
E
[Pr[
j
X
n;i
j
p
>
p
j
Z
n
]
j:E
n
]

Pr[
:E
n
]


d
Byaunionboundover
i
,wehavethat
Pr[max
i
j
X
n;i
j
>
]


.Hence,wealsohavethatforany

,forany
n

n
(

p

=
2
d
;

=
2
d
)
,
Pr[
k
X
n
k
1
>
]


,whichimplies
X
n
p
!
0
.
"
47,Riemannian kernel based Nyström method for approximate infinite-dimensional covariance descriptors with application to image set classification,https://arxiv.org/pdf/1806.06177v2.pdf,https://github.com/Kai-Xuan/AidCovDs,"Riemannian kernel based 
Nystr
ö
m
 
method 
for
 
approximat
e
 
infinite
-
dimensional covariance 
descriptors 
with application to
 
image set 
classification
 
 
Kai
-
Xuan Chen
1
, Xiao
-
Jun Wu
1
,
*
,
 
Rui Wang
1
,
 
Josef Kittler
2
 
1 
School of I
o
T 
Engineering
, Jiangnan University,
 
214122, 
Wuxi, China
 
2
 
Center for Vision, Speech and Signal Processing(CVSSP)
,
 
University of Surry,
 
GU2 7XH
,
 
G
uildford, UK
 
{kaixuan_chen_jnu
, 
xiaojun_wu_jnu}@163.com
, 
RunningWang@outlook.com
,
 
j.kittler@surrey.ac.uk
 
 
 
 
Abstract

In the 
domain of
 
pattern recognition
, using the 
CovDs
 
(Covariance Descriptors)
 
to represent 
data 
and taking the 
metrics of 
the 
resulting Riemannian manifold into account have 
been widely 
adopted
 
for the task of image set cl
assification
. 
Recently, 
it has been proven that infinite
-
dimensional CovDs 
are 
more discriminative than 
the
ir
 
low
-
dimensional 
counterparts
.
 
However, 
the form of 
infinite
-
dimensional CovDs 
is implicit and 
the 
computation
al load
 
is high
. 
W
e 
propose
 
a novel framework for
 
represent
ing 
image set
s
 
by 
approximating 
infinite
-
dimensional 
CovDs
 
in the paradigm of
 
the
 




 
m
ethod based on 
a 
Riemannian kernel. 
We start by modeling
 
the images
 
via 
CovDs, 
which lie 
on the 
Riemannian manifold 
spanned by
 
SPD
 
(Symmetric Positive Definite)
 
matrices
.
 
W
e
 
then 
extend the 
Nystr
ö
m
 
method
 
to 
the SPD manifold 
and 
obtain the 
approximation
s
 
of 
CovDs in RKHS
 
(
Reproducing Kernel Hilbert 
Space
)
.
 
Finally
,
 
we 
approximate infinite
-
dimensional CovDs 
via 
the
se
 
approximations.
 
E
mpirically
, we 
apply
 
our framework 
to
 
the 
task of image set classification.
 
The 
experiment
al results
 
obtained 
on three benchmark datasets
 
show that 
our
 
proposed approximat
e
 
in
finite
-
dimensional CovDs outperform the original CovDs
.
 
  
(
Source 
Co
de
: 
https://github.com/Kai
-
Xuan/AidCovDs
)
 
Keywords

image set classification; 
Covariances Descriptors
; 
Riemannian manifold
;
 
Riemannian kernel;
 
Nystr
ö
m 
method;
 
Reproducing Kernel Hilbert Space
 
 
I.
 
 
I
NTRODUCTION
 
I
mage set 
classification
 
has received 
a lot of
 
attention in 
artificial intelligence 
and pattern recognition
 
[1,2,3,4,5,6,7,8]
.
 
In
 
the domain of 
image set 
classification
, each set contains a 
number of images
,
 
possibly 
acquired in
 
different environments, 
which 
have
 
the same 
label
. 
To this end,
 
image sets
 
can offer
 
more discriminative and robust 
information 
than 
a
 
single
-
shot 
image
 
[9,10,11,12,13]. 
The 
common
l
y
 
used
 
representation
s
 
of
 
image set
s
 
include
,
 
linear subspaces [1], 
CovDs
(
covariance 
descriptors
)
 
[7,8,15], 
Gaussian mixture model
 
[14]
, among 
which,
 
CovDs
 
defined
 
by second
-
order statistics of image 
features have been widely applied in object detection
 
[4], 
gesture classification
 
[5], 
and 
virus 
recognition [13]
. 
 
The 
CovDs
 
are
 
in the form of
 
SPD(Symmetric Positive 
Definite) matrices 
which
 
lie on a 
non
-
line
ar
 
manifold
 
known as
 
the
 
SPD manifold
 
[7,8]
.
 
The space spanned by the SPD matrices 
i
s not a vector space because 
it does
 
not 
satisf
y
 
the 
scalar 
multiplication axiom
. F
or example, multiply
ing
 
an SPD matrix 
by a negative scalar will 
result
 
in 
a negative definite matrix
 
instead of SPD
 
matrix
 
[11]
.
 
As a consequence
, Euclidean metric 
can
not
 
be used 
to analyze SPD matrices
.
 
Instead,
 
R
iemannian 
metrics have been 
shown
 
to 
be
 
a better 
tool
 
for operating on
 
SPD 
matrices.
 
To this end, 
the SPD manifold
, which forms the 
interior of a convex cone in the Euclidean space
,
 
is one kind of 
Riemannian manifold.
 
A
 
variety of Riemannian metrics of SPD 
manifold have been proposed. In particular, the AIRM(Affine 
Invariant Riemannian Metric) [2,8]
,
 
which 
measures 
the 
geodesic distance 
on SPD manifold
, and has the property of 
affine 
invariance
, 
is the most 
popular
. The 
Stein divergence and 
Jeffrey divergence
 
[2,10]
 
are efficient metrics 
to 
measure 
geodesic distance between two SPD matrices, 
which 
are
 
derived
 
from
 
Bregman divergence for some special seed function
s
. The 
LEM(Log
-
Euclidean Metric)
 
[7,8]
 
views the SPD matrices on 
the Lie group
.
 
It measures
 
the similarity between two SPD 
matrices through computing the distance in 
the SPD matrix 
logarithm domain
,
 
which is 
a
 
flat
 
surface
 
at the point of identity 
matrix.
 
 
R
ecently, infinite
-
dimensional CovDs 
have 
becom
e 
increasingly popular because of their robustness for the task of
 
recognition
 
as 
argued
 
in
 
[16,17,18]
.
 
In these 
three
 
papers, 
the 
features of the samples are mapped to 
the
 
RKHS(Reproducing 
Kernel 
Hilbert Space)
 
through 
a 
kernel function
,
 
i.e., 







. Note,
 
the infinite
-
dimensional CovDs in RKHS
 
do not have 
an 
explicit 
representation 
form
.
 
To tackle this 
problem,
 
i
n [16],
 
the 
authors
 
extend the Bregman divergence to 
analyze 
the 
infinite
-
dimensional CovDs
 
in RKHS
 
via the kernel trick
.
 
Similari
l
y,
 
in [17] and [18],
 
the
 
LEM and AIRM 
were extended 
to 
the infinite
-
dimensional 
setting.
 
However, the dimension of 
features in RKHS is infinite and the
 
number of independent 
observations is 
finite, which inevitably results in the rank of 
infinite
-
dimensional CovDs being deficient. To overcome this 
drawback,
 
in [13] and [19], the authors propo
se to approximate 
infinite
-
dimensional CovDs in RKHS with an explicit mapping. 
The resulting CovDs are effective for image classification by 
virtue of approximating explicit forms of infinite
-
dimensional 
CovDs in RKHS. 
 
 
In this paper, we prop
ose a novel framework to approximate 
infinite
-
dimensional CovDs, 
for
 
use 
as
 
data descriptors
 
of 
image 
set
s
.
 
Different from [13], 
we approximate infinite
-
dimensional 
CovDs by
 
the
 




 
method based on
 
a
 
Riemannian 
kernel 
and tackle the task of image set classification instead of image 
classification.
 
In the traditional CovDs 
for
 
image set
s
, 
the 
images need to be resized and
 
vectorized for CovDs, which will 
result
 
in 
the loss of information. 
Compared with the 
traditional 
data 
descriptor
s
 
for
 
image sets, 
the 
representation
 
obtained by 
o
ur proposed framework has the following 
three
 
advantages:
 
(
1)
 
we use the CovDs to 
represent the images 
without
 
resizing 
and 
vectorizing
 
them
,
 
which
 
leads to more 
robust 
data 
descriptors 
for
 
each
 
single
 
image
 
in the sets
.
 
(
2) 
our
 
proposed framework 
gives 
an
 
explicit
 
form of 
CovDs to approximate infinite
-
dimensional CovDs in RKHS
, which 
is
 
a very
 
competitive 
descriptor 
for
 
image set
s
.
 
(
3)
 
F
or the image sets, the 
dimensionality of traditional 
CovDs
 
is
 
usually
 



 
by 
resizing the images to
 



. I
n 
this
 
paper, 
the 
dimensionality 
of CovDs 
obtained 
by
 
the proposed framework 
is 



 
or
 



,
 
which 
are
 
far lower than 



.
 
F
ig.1 gives the 
flow chart of 
the 
proposed framework.
 
For a given image set
 
(Fig.1 
S
tep 1)
,
 
we firstly extract features of images via Gabor 
filter [10,11,13] or SIFT [2
2
]
 
(Fig.1 
S
tep 2) 
and use the CovDs 
of these features 
to represent the images
 
(Fig.1 
S
tep 3)
. Secondl
y,
 
w
e
 
m
ap the CovDs 
of images 
into RKHS via Riemannian kernel 
and compute
 
the
 
associated parameters for
 
the
 




 
method 
(
Fig.1 
S
tep 4)
,
 
and use them to 
approximate 
CovDs
 
in RKHS
 
(Fig.1 
S
tep 5)
.
  
Finally
, we combine the 
approximations
 
of 
the 
images
 
to 
form a
 
feature matrix and compute
 
its
 
covariance 
matrix to represent
 
the
 
given
 
image set (
Fig.1
 
S
tep 6 and 
S
tep 
7).
 
 
The 
rest
 
of this paper is organized as follows: In Section 

w
e give a brief overview 
of
 
CovDs
 
for image sets 
and infinite
-
dimensional CovDs, 
and 
introduce 
some classical related 
Riemannian metrics
 
of SPD manifold
. In Section 

the model of 




 
me
thod
 
and our proposed 
framework
, 
and introduce some SPD manifold
-
based classification 
algorithms which are used in the experiments of this paper. In 

in terms of
 
average accuracies and standard deviations
. To t
his end,
 
the 
results of 
the 
experiment
s
 
show that 
the descriptors of 
our 
proposed framework
 
offer
 
more discriminative information than 
the 
traditional ones for 
the 
task of 
image set classification. 
In 
Section 

express 
our conclusions and 
suggest 
future 
work
.
 
II.
 
R
ELATED
 
W
ORK
 
I
n this section, we
 
give an overview
 
of
 
the 
traditional 
CovDs and 
infinite
-
dimensional CovDs
 
for image sets
,
 
and 
introduce 
some classical Riemannian metrics of SPD manifold.
 
In this paper, we will 
adopt
 
the following notation
s
: 



 
is the 
space spanned by real
 
 



 
SPD matrices
.
 






 
is the 
tangent space at the point 





,
 
where 
the 
dimensionality of 
each 
point
s
 
is
 



.
 


 
is the 
flat surface
 
spanned by real 



 
symmetric
 
matrices
,
 
which is the tangent space 
at the 
point of identity matrix 







.
 
 
A.
 
Traditional CovDs and Infinite
-
D
im
e
nsional CovDs
 
Given 
an image set with 
n 
images
,
 













,
 
where 





 
is 
obtained by resizing and vectorizing the 
i
-
th 
image sample of
 
image set
. 
Let 
C 
be
 
the covariance matrix
 
[2,7,8
,13
] computed from the raw intensity
 
of 
resized 
sample
s
 
in the image sets: 
 






















































 
where 













 
is the mean
 
vector
 
of image
 
samples
 
in the 
set 
S
,
 
and 
C
 
is a 
D
pd
D
 
covariance matrix
.
 
 



















 
is the centering matrix and 


 
is a column vector
 
of
 
N
 
ones 
[2
,13
]
.
 
 
F
ig.1. The flow chart of our proposed framework. Given an image set (
S
tep 1). Extract the features of images (
S
tep 2). Use the robust CovDs to represent the 
images in image set (
S
tep 3). Map the CovDs into RKHS via Riemannian kernel and compute associated p
arameters for 




 
method (
S
tep 4). Obtain the 
approximations
 
of CovDs in RKHS (
S
tep 5). Combine the 
approximations
 
to form a feature matrix and compute its covariance matrix to represent the given 
image set (
S
tep 6 and 
S
tep 7).
 
 
According to Eq.(1), 
an
 
infinite
-
dimensional CovDs in 
RKHS can be defined as:
 






















































 
where 


























,
 







 
is the 
implicit
 
kernel mapping
 
to 

 
and the dimensionality of samples 
in 

 
approaches 

.
 
Thus,  


 
is
 
usually semi
-
defin
ite
 
and on 
the boundary of the positive cone.
 
B.
 
Affine Invariant Riemannian Metric
 
 
T
he 



 
is the Riemannian manifold 
spanned by
 
SPD 
matrices 
and 
can be viewed as a 
convex
 
cone in the 








 
dimensional Euclidean space
 
[2]
.
 
The similarity between two 
SPD matrices
 
can be
 
obtained by computing
 
the length of 
geodesic curve
 
on the SPD manifold
, which is analogous to 
computing 
the
 
length
 
of 
straight line 
between
 
two 
points in the 
vector space. The AIRM
 
[2,8] is one of the most popular 
Riemannian metrics
 
which 
analyze
s
 
the points 
on the SPD 
manifold
. For
 
a 
Riemannian
 
point
 
P
 
on the SPD manifold, The 
AIRM can be defined through two 
associated 
tangent vectors 









:
 













































 
According to Eq.(
3
), t
he geodesic distance 


 
between 
two 
SPD matrices
 


 
and
 


 
via
 
AIRM
 
[
2,8]
 
can be written 
as:
 












































 
where
 






 
is
 
the 
matrix 
Frobenius norm,
 
and
 




 
denotes
 
the matrix logarithm
 
operator.
 
C.
 
Log
-
Euclidean 
M
etric
 
LEM(
Lo
g
-
Euclidean metric)
 
[4,7,8,11] is a bi
-
variant 
Riemannian metric
,
 
which 
takes
 
the Riemannian geometry of 
SPD manifold
 
into account
.
 
T
he 
distance 


 
between 
these two SPD matrices
,
 


 
and
 


, defined
 
via
 
this metric 
can be written as:
 



































 
The meaning of 






 
and 
 




 
 
is
 
the 
same as that in Eq.(4)
.
 
LEM can be viewed as the distance 
between
 
the points in the 
domain of matrix logarithm
, which is the 
tangent space 


 
projected 
from SPD manifold 



 
by 
logarithm mapping
 
[7,8]
:
 










































 
where
 


 
is a vector space
. 
Figure 2 gives 
an
 
illustration of 
the 
logarithm mappin
g
.
 
Furthermore,
 
the 
associated
 
Riemannian 
kernel 
function can be 
represented by the inner product of points 
in
 
the
 
tangent space [7,11]:
 

































 
(
7)
 
For all 
the 
points
 











,
 
the final kernel matrix is 
a 
symmetric
 
matrix
 
owing
 
to
 



















.
 
F
or
 
any 









,
 
we have:
 







































 
 


































 
 

















 
Eq. (8) 
proves
 
that 
the 
Log
-
Euclidean kernel guarantees the 
positive definite
ness
 
property of the Riemannian kernel and 

 
III.
 
A
PPROXIMAT
E
 
INFINITE
-
DIMENSIONAL COVARIAN
CE 
DESCRIPTORS FOR IMAG
E SETS
 
In this section,
 
we 
give an overview 
of
 
the
 




 
meth
od
 
which
 
can
 
approximate the SPD matrices in RKHS
 
and 
introduce our proposed 
framework
 
to estimate
 
infinite
-
dimensional CovDs 
for image sets
.
 
W
e
 
then
 
introduce some 
classification algorithms 
for data
 
on SPD manifold which will 
be used in our experiments.
 
A.
 
Nystr
ö
m
 
Method
 
As 
shown 
i
n [
13
]
 
and [20],
 
the
 




 
method 
is a data
-
dependent estimation
 
method
 
for
 
the
 
vectorial
 
features in RKHS.
 
Here, we 
extend
 
the
 




 
method to approximate SPD 
matrices in RKHS
 
via Log
-
Euclidean kernel
.
 
Now, 
given a 
training set
 














,
 
where 






,
 
c
onsist
ing
 
of 
M
 
SPD matrice
s, then the Riemannian kernel 
matrix 















 
can be obtained via Eq.(7).
 
The 
approximation of 
K
 
can be written as 

















.
 
H
ere,
 













 
with 
E
 
being the 
diagonal matrix of 
top 
D
 
eigenvalues
 
of the Riemannian kernel 
matrix 
K
 
and 
V
 
being the
 
matrix
 
of 
corresponding 
eigenvectors. 
Based on 
this
 
approximation, a random 
SPD matrix 
Y
 
in RKHS 
can be approximated as a 
D
-
dimensional vector
:
 












































 
where
 




 
is the
 
D
-
dimensional 
vector 
approximat
ion
 
of the 
SPD matrix
 
Y
 
in RKHS.
 
For the given training set 
S
,
 
it can 
be 
estimate
d
 
as
 
a
 



 
matrix 


























,
 
where 








 
is 
the 
D
-
dimensional
 
vector
 
approximat
ion
 
of 


 
in RKHS.
 
Algorithm 
1 summarizes the 




 
method on SPD manifold.
 
B.
 
Approximate infinite
-
dimensional CovDs for image sets
 
O
ur proposed framework,
 
which approximates the infinite
-
dimensional CovDs
 
in RKHS, offer
s
 
more discriminative 
and 
lower
-
dimensional
 
descriptors for image sets
. 
 
We firstly
 
 
Fig 2.
 
Logarithm mapping
 
extract the features of the
 
single
 
images via 
G
abor
 
filter
 
[10,11,13]
 
or 
SIFT
 
[2
2
] 
(Fig.1 
S
tep2) 
and 
use the CovDs of 
these features to represent the
m
 
(Fig.1 
S
tep 3)
.
 
This
 
offers 
more 
robust information for 
single 
images
,
 
compared to resizing and 
vectorizing
 
them
.
 
Secondly, we map the CovDs into RKHS
 
(Fig.1
 
S
tep 
4
)
,
 
where the form is implicit and the dimensionality 
approaches 
 

.
 
Though
 
the infinite
-
dimensional CovDs in 
RKHS
 
offer
 
more
 
discriminative information, 
its form is 
implicit
 
and 
consequently 
not 
amenable
 
to extending to 
infinite
-
dimensional
 
manifold
.
 
 
To this end, we 
estimate
 
the
 
infinite
-
dimensional CovDs
 
in RKHS
 
(Fig.1 
S
tep 5) via 
an
 
explicit mapping and obtain the 
finite
-
dimensional
 
(Fig.1 
S
tep 
7) 
approximations
,
 
which 
offers
 
more discriminative 
descriptors for image sets.
 
Consider
 
a set
 
of CovD
s
,
 





















 
corresponding to 
N
 
images in an image set. 
According to 
Eq.(2), the infinite
-
dimensional CovDs for this image set can 
be written as:
 













































 
where 


























,
 

 
is a 
Riemannian kernel mapping
.
 
Then, 




 
in RKHS 
can be 
approximated
 
as:
 


























 
via 
Eq.(9). 
T
he
 
resulting
 
approximate 
in
finite
-
dimensional CovDs 
for image set can be written as:
 













































 
where 


 
is the final descriptor
 
extracted
 
for
 
the
 
image set 
by
 
our
 
proposed
 
framework
. 
T
he proposed framework is 
s
ummarized in
 
Algorithm 2.
 
C.
 
Classification algor
ithms based on SPD manifold
 
The NN(
nearest neighbor) algorithm is one of the simplest 
methods 
of
 
classification in the domain of computer vision and 
pattern recognition. 
In this paper
,
 
we use
 
two
 
NN classification 
algorithms 
separately 
based on AIRM and LEM 
to 
classify 
points on
 
the
 
SPD manifold
 
to demonstrate the advantages
 
of 
our proposed framework for image set classification.
 
I
n contrast to
 
[7], where the CDL(covariance discriminative 
learning) was proposed for image set classification
, the 
classical classification algorithms 
can
 
be
 
directly utilized on the 
SPD manifold
. In this paper, the Riemannian geometr
y
 
of the 
SPD manifold 
is
 
fully considered.
 
It derives a kernel function 
that maps the SPD matrices to the Euclidean space through the 
LEM metric. As a result, the classical cla
ssification algorithms 
applicable in the linear space can be exploited in the kernel 
formulation. LDA (linear discriminant analysis) and PLS 
(partial least squares) devoted to the linear space are considered 
in [7] for the task of classification. 
 
Lastly, 
we introduce the Riemannian sparse coding 
algorithm LogEKSR [11], which takes Riemannian geometry 
of SPD manifold 
into account and applies the sparse 
representation in 
RKHS
. In [11], the derivation of the Log
-
Euclidean kernel is presented to map SPD matric
es from SPD 
manifold to RKHS (Reproducing Kernel Hilbert Space). Note 
that the Log
-
Euclidean kernels in this algorithm can be derived 
from Eq.(7). 
 
Log
-
EK.poly
 
 
 
 
 
 
   
 
    


























 
Log
-
EK.exp
 
  
     




























 
Log
-
EK.gau
 
 































 
 
where 
p
n 
is a polynomial of degree 
n

1 with positive 
coefficients. 
These three kernels are respectively the polynomial
 
kernel
, exponential
 
kernel
, and gaussian kernel 
associated with 
LEM. According to [11], t
hese 
kernels are positive definite and 

 
 
IV.
 
EXPERIMENTS
 
AND
 
ANALYSIS
 
 
We 
evaluate
 
the effectiveness of our proposed framework 
for image set classification.
 
We 
carry out
 
experiments
 
relating 
to
 
three different tasks, namely object categorization, hand 
gesture recognition, and virus cell classification.
 
The three 
benchmark datasets are ETH
-
80 [
4
,
21
], Cambridge hand gesture 
dataset (CG) [
5
] and Virus cell dataset [
13
] respectively.
 
For
 
benchmarking, we compare the accuracies and standard 
deviations of traditional CovDs 
with
 
approximate infinite
-
dimensional CovDs(proposed) 
using
 
the same classification 
al
g
orithm.
 
To this end, 
we 
use
 
the nearest neighbor 
classifier
(NN)
 
based on LEM[10] and AIRM[7]
,
 
which are 
introduced in Section 

.  Beside these two NN classifiers, we 
also make use of another two classification algorithms based on 
SPD manifold
,
 
namely 
LDA
-
based CDL (
covariance 
discri
minative learning
) [7] and LogEK.poly
-
based LogEKSR 
Algorithm 1: 
Nystr
ö
m 
method on SPD manifold
 
Input:  
 

 
training set
: 





















.
 

 
D
, target dimensionality
.
 

 
a random SPD matrix 
Y
.
 
Output: 
 

 
the approximation of 
Y
 
in RKHS
.
 
1
: compute the Riemannian kernel matrix
:
















 
via 
Eq.(7)
.
 
2
: obtain the diagonal matrix 
E
 
of top
 
D
 
eigenvalues of 
K
.
 
3
: obtain the corresponding eigenvectors 
V
.
 
4: compute the approximation 
Z(
Y
) 
via Eq.(9)
.
 
 
Algorithm 
2
: 
Approximate infinite
-
dimensional CovDs for image sets
 
Input:  
 

 
image
 
set
: 
















 
is a
n
 
image matrix
.
 

 
D
, target dimensionality
.
 

 
training set: 


 
= 













.
 
Output: 
 

 






,
 
approximate infinite
-
dimensional CovDs
 
1
:
 
obtain the 
CovDs of
 
image set
  

 
and  


:
 

 
and



 
via 
computing covariance matrix of Gabor or SIFT features.
 
2
: obtain 
associated parameters of Nystr
ö
m
 
method
 
via 
Algorithm 1
.
 
3
: obtain 
the approximations





 
via Eq.(9).
 
4: compute the approximat
e infinite
-
dimensional 


 
via Eq.(11).
 
 
(Log
-
Euclidean Kernels for Sparse Representation) [11], which 
are the state
-
of
-
the
-
art method on SPD manifold. The different 
methods used in our experiments are 
referred to
 
as:
 

 



: AIRM
-
based NN classifier on the SPD 
manifold spanned by traditional CovDs for image sets.
 

 




:
 
AIRM
-
based NN classifier on the SPD 
manifold spanned by approximate infinite
-
dimensional 
CovDs via our framework for image sets.
 

 



: LEM
-
based 
NN classifier on the SPD 
manifold spanned by traditional CovDs for image sets.
 

 




:
 
LEM
-
based NN classifier on the SPD 
manifold spanned by approximate infinite
-
dimensional 
CovDs via our framework for image sets.
 

 

 
:
 
CDL on the SPD manifold spa
nned by traditional 
CovDs for image sets.
 

 


:
 
CDL on the SPD manifold spanned by 
approximate infinite
-
dimensional CovDs via our 
framework for image sets.
 

 

 
:
 
LogEKSR on the SPD manifold spanned by 
traditional CovDs for image sets.
 

 



:
 
LogEKSR on the SPD manifold spanned 
by approximate infinite
-
dimensional CovDs via our 
framework for image sets.
 
 
 
 
To generate the CovDs for
 
single
 
images in image sets, the 
feature vectors are extracted 
using
 
G
abor filter
 
[10,11,13]
 
or 
SIFT
 
[2
2
]
.
 
For the Virus cell dataset
, the features are extracted 
using
 
G
abor filter
 
with 5
 
scale
s 
and 8 orientation
s
. 
For
 
the ETH
-
80 and CG
 
datasets, 
the features are extrac
ted 
using 
SIFT.
 
For 
the proposed framework, the dimensionality of the final CovDs 
is determined by the target dimensionality 
D
 
in Algorithm 1. 
W
e 
set 
the 
target dimensionality 
D=
40 and 
choose 60 
training 
samples
 
at 
random
 
for
 
the
 



m
 
method
 
on 
the 
ETH
-
80 and 
Virus datasets
,
 
and set 
the 
target dimensionality 
D=
80 
with
 
104 
training samples for
 
the
 



m
 
method 
on
 
the
 
CG dataset
.
 
Thus, the dimensionality of the CovDs obtained by our proposed 
framework is 



 
for the CG dataset and  



 
for the 
ETH
-
80 and Virus datasets. In the traditional CovDs in our 
experiments, the dimensionality of the CovDs is 



 
via 
resizing the imag
es to 



.
 
 
A.
 
Object Cate
gorization on dataset ETH
-
80
 
 
In 
the ETH
-
80
 
dataset, there are eight
 
categories
 
of
 
images
:
  
pears, tomatoes, dogs, cows, apples, cars, horses and cups. Each 
class consists of 10 image sets, and each set has 41 images from 
different views
 
(see 
the top line of 
Fig.3 for example)
.
 
For each 
class, 2 image sets are chosen 
as
 
training samples
 
randoml
y
 
and 
the 
remaining
 
8
 
image sets are used as test samples.
 
The 
average 
recognition rate and standard deviation of the 10 cross validation 
experiments are 
presented
 

 
B.
 
Hand Gesture Recognition
 
For this task, we 
use
 
the 
Cam
bridge hand 
gesture
 
dataset
, 
which is an image sequence of hand gesture
s
 
defined by 3 
motions and 3 hand shapes.
 
In this dataset, there are 
9
 
categories 
of 
images 
and
 
900 image sets
.
 
E
ach class has 100 image sets 
(see the middle line of Fig.3 for example). For each cl
ass of 
samples, 
20
 
image sets are
 
randomly
 
chosen 
as
 
training samples 
and the 
remaining
 
80
 
image sets are used as test samples. The 
average recognition rate and standard deviation of the 10 cross 
validation experiments are
 
shown
 
in 

.
 
C.
 
Virus Vell 
Classification
 
T
he Virus dataset contains 15 categories, each consist
ing
 
of 
5
 
image sets, and each set has 
20
 
images (see the 
bottom
 
line of 
Fig.3 for example). For each class, 
3
 
image sets are 
randomly 
chosen 
as
 
training samples
 
and the 
remaining
 
2
 
image sets are 
used as test samples. The average recognition rate and standard 
deviation of the 10 cross validation experiments are used as the 
final results in 
TABLE
 

.
 
D.
 
R
esult and Analysis
 

approximate in
finite
-
dimensional CovDs 
obtained by
 
our proposed framework
, as 
well as the performance
 
of 
traditional CovDs 
using
 
four
 
classification algorithms. 
On 
all three
 
datasets, t
he 
results
 
achieved by the
 
four
 
classifiers show that the approximate 
infinite
-
dimensional CovDs of 
our 
proposed framework are 
more discriminative
 
and robust
 
than the traditional CovDs. 
Especially on 
the 
ETH
-
80 and Virus datasets
, the 
accura
cy
 
of 
the 
two NN classifiers
,


-


 
an
d 

-


,
 
based 
on our proposed CovDs
,
 
is
 
higher than that of 
the 
two state
-
of
-
the
-
art classification algorithms
,
 
CDL and LogEKSR
,
 
based on 
traditional CovDs
.
 
This
 
fully illustrates that our proposed
 
framework offers more 
discriminative
 
features
 
than the 
traditional ones. Also, the accurac
y
 
of 
the 
two classifiers
,
 
CDL 
 
 
F
ig.3: images in three datasets. Top line: ETH
-
80 [4]. Middle line: CG [5]. 
Bottom line: Virus [13].
 
TABLE 
I
.  Results for the ETH
-
80 [4], C
G [5], and Virus [13] datasets.
 
 
M
ethod
 
E
TH
-
80 [4]
 
C
G [5]
 
V
irus [13]
 
N
N
-
AIRM
 







 







 







 
N
N
-
AIRM
pro
 







 







 







 
N
N
-
LogED
 







 







 







 
N
N
-
LogED
pro
 







 







 







 
C
DL
 







 







 







 
C
DL
pro
 







 







 







 
L
ogEKSR
 







 







 







 
L
ogEKSR
pro
 







 







 







 
 
and LogEKSR
,
 
show that our proposed framework 
is
 
more 
discriminative. A
t
 
last, 


 
achieves the best 
recognition rates of 
89.61
%
 
and 63.33
%.
 
For the CG dataset,
 
the 
advantages 
are 
not as obvious
.
 
However, 


 
still 
achieves 
the best 
result with accuracy of 91.57% and standard deviation 
of 0.73.
 
V.
 
CONCLUSION
 
AND
 
FUTURE
 
WORK
 
In this paper, we proposed a novel framework to 
approximate infinit
e
-
dimensional CovDs 
to 
extract
 
novel
 
descriptors of 
image sets.
 
Our experimental results show that 
the CovDs
 
obtained by our proposed framework are more 
discriminative than the traditional CovDs 
in different
 
task
s
 
of
 
image set classification.
 
More important
ly
,
 
we take the 
Riemannian geometry of SPD manifold into account because 
the dimensionality of our
 
propose
d
 
CovDs is finite
, and our 
dimensionality is lower than the traditional ones.
 
For the future 
work,
 
we will consider 
how 
to extend our proposed framework 
to other types of Riemannian manifold
s
.
 
VI.
 
A
CKNOWLEDGMENTS
 
THE PAPER IS SUPPORTED BY THE NATIONAL 
NATURAL 
 
SCIENCE 
 
FOUNDATION
 
 
OF 
 
CHINA 
  
(GRANT NO.61373055

61672265), 
 
UK EPSRC GRANT 
EP/N007743/1, MURI/EPSRC/DSTL GRANT EP/R018456/1, 
AND THE 111 PROJECT OF MINISTRY OF EDUCATION 
OF CHINA (GRANT NO. B12018).
 
R
EFER
ENCES
 
[1]
 
Huang Z, Wang R, Shan S, et al. Projection metric learning on Grassmann 
manifold with application to video based face 
recognition[C]//Proceedings of the IEEE Conference on Computer Vision 
and Pattern Recognition. 2015: 140
-
149. 
 
[2]
 
Harandi M, Salzmann M
, Hartley R. Dimensionality reduction on SPD 
manifolds: The emergence of geometry
-
aware methods[J]. IEEE 
transactions on pattern analysis and machine intelligence, 2017.
 
[3]
 
Feng F, Wu X J, Xu T. Object tracking with kernel correlation filters 
based on mean sh
ift[C]//Smart Cities Conference (ISC2), 2017 
International. IEEE, 2017: 1
-
7. 
 
[4]
 
Huang Z, Wang R, Shan S, et al. Log
-
euclidean metric learning on 
symmetric positive definite manifold with application to image set 
classification[C]//International conference on
 
machine learning. 2015: 
720
-
729.
 
[5]
 
Faraki M, Harandi M T, Porikli F. Image set classification by symmetric 
positive semi
-
definite matrices[C]//Applications of Computer Vision 
(WACV), 2016 IEEE Winter Conference on. IEEE, 2016: 1
-
8.
 
[6]
 
Li H, Wu X J. Multi
-
focus
 
Image Fusion Using Dictionary Learning and 
Low
-
Rank Representation[J]. 2017. 
 
[7]
 
Wang R, Guo H, Davis L S, et al. Covariance discriminative learning: A 
natural and efficient approach to image set classification[C]//Computer 
Vision and Pattern 
Recognition (CVPR), 2012 IEEE Conference on. 
IEEE, 2012: 2496
-
2503.
 
[8]
 
Ren J, Wu X. Bidirectional Covariance Matrices: A Compact and 
Efficient Data Descriptor for Image Set Classification[C]//International 
Conference on Intelligent Science and Big Data 
Engineering. Springer, 
Cham, 2015: 186
-
195.
 
[9]
 
Cherian A, Sra S. Riemannian dictionary learning and sparse coding for 
positive definite matrices[J]. IEEE transactions on neural networks and 
learning systems, 2017.
 
[10]
 
Harandi M T, Hartley R, Lovell B, et al. Spar
se coding on symmetric 
positive definite manifolds using bregman divergences[J]. IEEE 
transactions on neural networks and learning systems, 2016, 27(6): 1294
-
1306.
 
[11]
 
Li P, Wang Q, Zuo W, et al. Log
-
Euclidean kernels for sparse 
representation and dictionary l
earning[C]//Proceedings of the IEEE 
International Conference on Computer Vision. 2013: 1601
-
1608.
 
[12]
 
Wang Q, Li P, Zuo W, et al. RAID
-
G: Robust estimation of approximate 
infinite dimensional Gaussian with application to material 
recognition[C]//Proceedings of
 
the IEEE Conference on Computer Vision 
and Pattern Recognition. 2016: 4433
-
4441.
 
[13]
 
Faraki M, Harandi M T, Porikli F. Approximate infinite
-
dimensional 
region covariance descriptors for image classification[C]//Acoustics, 
Speech and Signal Processing (ICASSP)
, 2015 IEEE International 
Conference on. IEEE, 2015: 1364
-
1368.
 
[14]
 
Arandjelovic O, Shakhnarovich G, Fisher J, et al. Face recognition with 
image sets using manifold density divergence[C]//Computer Vision and 
Pattern Recognition, 2005. CVPR 2005. IEEE Computer
 
Society 
Conference on. IEEE, 2005, 1: 581
-
588.
 
[15]
 
Tuzel O, Porikli F, Meer P. Region covariance: A fast descriptor for 
detection and classification[J]. Computer Vision

ECCV 2006, 2006: 
589
-
600.
 
[16]
 
Harandi M, Salzmann M, Porikli F. Bregman divergences for infini
te 
dimensional covariance matrices[C]//Proceedings of the IEEE 
Conference on Computer Vision and Pattern Recognition. 2014: 1003
-
1010.
 
[17]
 
Quang M H, San Biagio M, Murino V. Log
-
Hilbert
-
Schmidt metric 
between positive definite operators on Hilbert spaces[C]//A
dvances in 
Neural Information Processing Systems. 2014: 388
-
396.
 
[18]
 
Minh H Q. Affine
-
invariant Riemannian distance between infinite
-
dimensional covariance operators[C]//International Conference on 
Networked Geometric Science of Information. Springer, Cham, 20
15: 30
-
38.
 
[19]
 
Wang Q, Li P, Zuo W, et al. RAID
-
G: Robust estimation of approximate 
infinite dimensional Gaussian with application to material 
recognition[C]//Proceedings of the IEEE Conference on Computer Vision 
and Pattern Recognition. 2016: 4433
-
4441.
 
[20]
 
Willi
ams C K I, Seeger M. Using the Nyström method to speed up kernel 
machines[C]//Advances in neural information processing systems. 2001: 
682
-
688.
 
[21]
 
Wang R, Wu X J. Structure Maintaining Discriminant Maps (SMDM) for 
Grassmann Manifold Dimensionality Reduction w
ith Applications to the 
Image Set Classification[C]// International Symposium on Distributed 
Computing and Applications To Business, Engineering and Science. 
IEEE Computer Society, 2017:105
-
108.
 
[22]
 
Lowe D G. Distinctive image features from scale
-
invariant key
points[J]. 
International journal of computer vision, 2004, 60(2): 91
-
110.
 
 
"
48,Ensemble Pruning based on Objection Maximization with a General Distributed Framework,https://arxiv.org/pdf/1806.04899v3.pdf,https://github.com/eustomaqua/EPFD,"arXiv:1806.04899v3  [cs.LG]  30 Sep 2019IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,
VOL.,NO.,MONTHYEAR
1
EnsemblePruningbasedonObjectionMaximization
withaGeneralDistributedFramework
YijunBian,YijunWang,YaqiangYao,andHuanhuanChen,
SeniorMember,IEEE
Abstract
ŠEnsemblepruning,selectingasubsetofindividual
learnersfromanoriginalensemble,alleviatesthedecien
ciesof
ensemblelearningonthecostoftimeandspace.Accuracyand

diversityserveastwocrucialfactorswhiletheyusuallyco
nict
witheachother.Tobalancebothofthem,weformalizethe

ensemblepruningproblemasanobjectionmaximizationprob
lem
basedoninformationentropy.Thenweproposeanensemble

pruningmethodincludingacentralizedversionandadistri
buted
version,inwhichthelatteristospeeduptheformer.Atlast
,we
extractageneraldistributedframeworkforensemblepruni
ng,
whichcanbewidelysuitableformostoftheexistingensembl
e
pruningmethodsandachievelesstimeconsumingwithout

muchaccuracydegradation.Experimentalresultsvalidate
the
efciencyofourframeworkandmethods,particularlyconce
rning
aremarkableimprovementoftheexecutionspeed,accompani
ed
bygratifyingaccuracyperformance.
IndexTerms
Šensemblelearning,ensemblepruning,diversity,
composablecore-sets,informationentropy.
I.I
NTRODUCTION
T
HANKStoitsremarkablepotential,ensemblelearning

hasattractedanamountofinterestinthemachinelearn-
ingcommunity[1]andhasbeenappliedwidelyinmanyreal-

worldtaskssuchasobjectdetection,objectrecognition,a
nd
objecttracking[2]Œ[5].Asitisalsoknownascommittee-ba
sed
learning,multipleclassiersystems,ormixturesofexper
ts
[1],[6],[7],anensembleisasetoflearnedmodelsthat

makedecisionscollectivelyratherthanrelyingononesing
le
model.Thevarietyoftypesofindividuallearnerscategori
zes
anensembleasheterogeneousensemblesandhomogeneous

ensembles.Andmostoftheensemblemethodsconcentrate

onthelattersuchasbagging[8]andboosting[9],[10].
Thesuccessofensemblemethodsiscommonlyattributable
totwokeyissues:theaccuracyofindividuallearnersandth
e
diversityamongthem[11].Forclassicationproblems,one

classierisaccurateifitserrorrateisbetterthanrandom

guessingonnewinstances;twoclassiersarediverseifthe
y
makedifferenterrorsonnewinstances[11].Unfortunately
,
thereisstillnoconsensusinthecommunityonthedenition

ormeasurementfordiversity,unliketheapparentaccuracy
.
Besides,thediversityamongindividualclassiersusuall
y
Y.Bian,Y.Wang,Y.Yao,andH.ChenwerewiththeSchoolof
ComputerScienceandTechnology,UniversityofScienceand
Technology
ofChina(USTC),Hefei230027,China(e-mails:yjbian@mail
.ustc.edu.cn;
wyjun@mail.ustc.edu.cn;yaoyaq@mail.ustc.edu.cn;hche
n@ustc.edu.cn).
ThisworkwassupportedinpartbytheNationalKeyResearcha
nd
DevelopmentProgramofChinaunderGrant2016YFB1000905,a
ndinpart
bytheNationalNaturalScienceFoundationofChinaunderGr
ants91846111
and91746209.Correspondingauthor:HuanhuanChen.
ManuscriptreceivedJanuary07,2019;revisedJune23,2019
;accepted
September25,2019.
decreaseswhentheseindividualsapproachahigherlevelof

accuracy.Thushowtohandlethetrade-offbetweenthetwo

criteriaisanessentialissueinensemblelearning.
Althoughensemblemethodsareeffectual,onesignicant
drawbackhereisthatboththerequiredmemoryandpro-

cessingtimeincreasevisiblywiththenumberofindivid-

uallearnersintheensemble.Tomitigatethisshortcoming

motivatesensemblepruningthataimstoselectasubset

ofindividuallearnersinanensemble,namedasensemble

selectionorensemblethinningaswell[12]Œ[19].Itcoulde
ven
improvethegeneralizationperformanceofanensemblewith

asmallersize[20].Therehasbeenanumerousprogressionon

ensemblepruningmethodsinthelasttwodecades.Mostofthe

existingpruningmethods,however,arecentralizedinwhic
h
allindividualclassiershavetobestoredandprocessedon

onesinglemachine.Asthescaleofdataandanensembleitsel
f
enlargesrapidlyinthecontextofbigdata,theperformance
of
centralizedmethodsisbecomingthebottleneckinexecutio
n
time,whichiswhydistributedapproachesneedtoemerge.
Todealwithensemblepruningproblemsfastwithbal-
ancingdiversityandaccuracyappropriately,werstlytre
at
ensemblepruningasanobjectionmaximizationproblemusin
g
informationentropytoreectdiversityandaccuracy.The

objectivefunctionthatweaimtomaximizeisatrade-off

betweendiversityandaccuracyfromaninformationentropy

perspective.Secondly,wetransformthisapproachtoone

distributedversiontospeeduptheexecution,inspiredby

theemergingconceptofﬁcomposablecore-setsﬂinrecent

years.Itadoptsthesameideaasatwo-rounddivide-and-

conquerstrategy,whichisparticularlysuitablefordistr
ibuted
settings.Thirdly,weextractageneraldistributedframew
ork
forensemblepruningfromourmethod'sdistributedversion
.
Itcouldbewidelyapplicabletovariousexistingmethodsfo
r
ensemblepruningandachievelesstimeconsumingwithout

muchaccuracydegradation.
Ourcontributioninthispaperisfour-fold:

Weformalizetheensemblepruningproblemasanobjec-

tionmaximizationproblembasedoninformationentropy,

inordertobalancediversityandaccuracy.
Weproposeanensemblepruningmethodincludinga

centralizedversionandadistributedversion,utilizing

accuracyanddiversityconcurrently.
Weproposeageneraldistributedframeworkforensemble

pruning,whichcouldbewidelyutilizedandachieveless

timeconsumingwithoutmuchaccuracydegradation.
Wedesigndetailedexperimentstovalidatetheeffective-

nessofourdistributedframeworkandapproaches.
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,
VOL.,NO.,MONTHYEAR
2
II.R
ELATED
W
ORK
Inthissection,weintroducediversity(i.e.,akeyconcept
inensemblelearning)andexistingresearchonitrstly.Th
en
wedescribethedifcultyandexistingmethodsinensemble

pruning.Thirdly,weintroduceaconceptofﬁcomposablecor
e-
setsﬂanditsdevelopmentwhichshedssomelightonour

work.Finally,weexplainthedistinctionbetweenourpropo
sed
methodsandotherexistingensemblepruningmethodand

summarizeourcontributionspecically.

A.DiversityinEnsembleLearning
Diversity,intuitivelyconsideredasthedifferenceamong
individuallearnersinanensemble,isafundamentalissue

inensemblemethods[1],withseveralalternativenamesas

dependence,orthogonalityorcomplementarityoflearners
[6].
Practically,individualclassiersareusuallytrainedon
the
subsetsofthesametrainingdata,whichdrivesthemhighly

correlated,breakstheassumptionabouttheindependenceo
f
individualclassiers,andmakesithardtoseekdiversity.

Numerousensemblemethodsattempttoencouragediversity

implicitlyorheuristically[21].Forinstance,boostinga
nd
baggingpromotediversitybyre-weightingandsub-samplin
g
existingtrainingsamples,respectively[8]Œ[10],[22],[
23];
neuralnetworks(NN)ensemblesalsocreatediversityusing

differentinitialweights,differentarchitecturesofthe
net-
works,anddifferentlearningalgorithms.
Unfortunately,researchersstillhavenotreachedaconsen
sus
yetonanofcialmeasureofdiversity.Severalmeasures

havebeenproposedtorepresentdiversity,whichcouldbe

generallydividedintopairwiseandnon-pairwisediversit
y
measures[6],whilenosuperiorexists[7].Basedoncoin-

cidenterrorsbetweenapairofindividualclassiers,pair
wise
diversityrepresentsthebehaviorwhetherbothofthempred
ict
aninstanceidenticallyordisagreewitheachother,includ
ing
Q
-statistic[24],

-statistic[25],disagreementmeasure[26],
[27],correlationcoefcient[28],anddouble-fault[29].
In
contrast,non-pairwisediversityistheaverageofallposs
i-
blepairs,directlymeasuringasetofclassiersusingthe

variance,entropy,ortheproportionofindividualclassi
ers
thatfailoninstanceschosenrandomly.Itincludesinterra
ter
agreement[30],Kohavi-Wolpertvariance[31],theentropy

ofthevotes[32],[33],thedifcultyindex[6],[34],the

generalizeddiversity[35],andthecoincidentfailurediv
er-
sity[35].Besides,therearealsotwoothermeasuresthat

donotfallintothesetwomentionedcategoriespreviously:

Oneisthecorrelationpenaltyfunction[36],measuringthe

diversityofeachmemberagainsttheentireensembleinthe

negativecorrelationlearning(NCL)[37],[38];Theotheri
s
ambiguity[39],measuringtheaverageoffsetofeachmember

againsttheentireensembleoutput.
Moreover,fewresearcherscouldtellhowdiversityworks
exactlyalthoughthecrucialroleofdiversityhasbeenwide
ly
acceptedinensemblemethods.Inthelastdecadeorso,

Brown[40]claimedthatfromaninformationtheoreticper-

spective,diversitywithinanensembleexistedindeedon

numerouslevelsofinteractionbetweentheclassiers.His

workinspiredZhouandLi[41]toproposethatthemutual
informationshouldbemaximizedtominimizetheprediction

errorofanensemblefromtheviewofmulti-information.

Subsequently,Yu
etal.
[21]claimedthatthediversityamong
individuallearnersinapairwisemanner,usedintheirdive
rsity
regularizedmachine(DRM),couldreducethehypothesis

spacecomplexity,whichimpliedthatcontrollingdiversit
y
playedtheroleofregularizationinensemblemethods.

B.EnsemblePruning
Ensemblepruningdealswiththereductionofanen-
semblewhileimprovingitsefciencyandpredictiveper-

formance[42].MargineantuandDietterich[43]showedthe

possibilitytoobtainnearlythesamelevelofperformance

astheentiresetbyselectingasubsetoflearnersfroman

ensembleintherststudyonensemblepruning.Zhou
et
al.
[20]providedthebias-variancedecompositionoferroras
theprincipalfactorofthesuccessoftheirapproachnameda
s
ﬁGeneticAlgorithmbasedonSelectiveEnsemble(GASEN)ﬂ,

andclaimedthatpruningcouldleadtosmallerensembles

withbettergeneralizationperformance.Itisdifcult,ho
w-
ever,toselectthesub-ensembleswiththebestgeneralizat
ion
performance.Onetroubleistoestimatethegeneralization

performanceofasub-ensemble,andtheotheristhatnding

theoptimalsubsetisacombinatorialsearchproblemwith

exponentialcomputationalcomplexity[44].Notethatsele
cting
thebestcombinationofclassiersfromanensembleisNP-

completehardandevenintractabletoapproximate[45].
Numerousensemblepruningmethodshavebeenproposed
toovercomeshortcomingsofensemblelearningoverthelast

twodecades,whichcouldbecategorizedintothreegeneral

families:ranking-based,clustering-based,andoptimiza
tion-
based.Ranking-basedpruningmethods,thesimplestconcep
tu-
ally,orderthelearnersintheensembleandselecttherstf
ew
ofthemaccordingtodifferentevaluationfunctions[42],i
n-
cludingminimizingtheerror(e.g.,OrientationOrdering[
46]),
maximizingthediversity(e.g.,KL-divergencePruningand

KappaPruning[43]),orcombiningthemboth(e.g.,Diversit
y
RegularizedEnsemblePruning[44]).Clustering-basedpru
n-
ingmethodsemployaclusteringalgorithmtodetectgroups

oflearnersthatmakesimilarpredictionsinitiallyandthe
n
pruneeachclusterseparatelytoincreasetheoveralldiver
sity
oftheensemble[42].Notethatanintrinsicpropertythatth
ose
methodscouldbeexecutedinaparallelmannerisignored

frequentlyinthesecondphase.Optimization-basedprunin
g
methodsposeensemblepruningasanoptimizationproblem

whichistondthesubsetoftheoriginalensemblethat

optimizesameasureindicatingitsgeneralizationperform
ance.
Searchingexhaustivelyinthespaceofensemblesubsetsis

unfeasibleevenforamoderateensemblesizesincethisprob
-
lemisNP-completehard[44],[45].Thusvarioustechniques

areutilizedtoalleviatethispredicamentincludinggenet
ic
algorithm[47],greedyalgorithm[48],hillclimbing[49],
and
bi-objectiveevolutionaryoptimization[50].

C.ComposableCore-sets
Overthelastfewyears,aneffectivetechnique,captured
viatheconceptofﬁcomposablecore-setsﬂ,arisesinorder
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,
VOL.,NO.,MONTHYEAR
3
tosolveoptimizationproblemsoverlargedatasetsinthe

distributedcomputingliterature.Itseffectivenesshasb
een
conrmedempiricallyformanymachinelearningapplicatio
ns,
suchasdiversenearestneighborsearch[51],diversitymax
i-
mization[52],andfeatureselection[53].
Thenotionofﬁcomposablecore-setsﬂisintroducedexplic-
itlybyIndyk
etal.
[51]fortheveryrsttime,whilethe
notionofﬁcore-setsﬂcanbedatedbackto[54].Acore-set

foranoptimizationproblem,informally,isasubset(witha

guaranteedapproximationfactor)ofthatdataonwhichsolv
ing
theunderlyingproblemcouldyieldanapproximatesolution

fortheoriginaldata.Composablecore-setsareacollectio
nof
core-setsinwhichtheunionofthemgivesacore-setforthe

unionoftheoriginaldatasubsets[51].Besides,acomposab
le
core-setwith

approximatefactoryieldsasolutionwhichis
anapproximationoftheoptimalsolutionfortheoptimizati
on
problem,andtheapproximationisguaranteedbyafactor

,
whichis
1
=
12
[52]andcouldbeimprovedto
8
=
25
[53].
D.OurContribution
Anessentialdistinctionbetweenourproposedmethodsand
otherexistingensemblepruningmethodsisthatourmeth-

odscouldtackletheensemblepruningtaskinadistributed

way,acceleratingthepruningprocesssubstantially.Inst
ead
ofusinganypreviousmeasureofdiversity,ourmethods

utilizetheconceptofthemutualinformation
I(
;)
[55],the
normalizedmutualinformation
MI(
;)
,andthenormalized
variationinformation
VI(
;)
[53]fromaninformationentropy
perspective,toconductanobjectivefunctionandtakedive
rsity
andaccuracyintoconsiderationimplicitlyandsimultaneo
usly.
Besides,ourproposedframework(
EPFD
)couldbewidely
appliedtovariousexistingensemblepruningmethods,to

achievelesstimeconsumingwithoutmuchaccuracydegra-

dation,whichisanimpressiveadvantageofourmethod.
III.M
ETHODOLOGY
Inthissection,werstlyelaborateourobjectionmaxi-
mizationbasedoninformationentropyforensemblepruning

inacentralizedway,thenattainadistributedversionby

introducingtheconceptofcomposablecore-sets,andnall
y
extractageneraldistributedframeworkforensemblepruni
ng.
A.ObjectionMaximizationBasedonInformationEntropyfor

EnsemblePruning
Givenalargedataset
D
withthesize
d
oflabeled
instancesobtainedgraduallyfromstreamdata,andtheirla
bels
representedbya
d
-dimensionalvector
c
.Asetof
n
trained
individualclassiers
H
=
f
h
i
g
n

i
=1
isconsideredastheoriginal
ensemble,inwhicheachonemapsthefeaturespaceof

instancestothelabelspace.Theclassicationresultvect
or
ofanyindividualclassier
h
i
fromtheensemble
H
onthe
dataset
D
,similartotheclasslabelvector
c
,isrepresented
bya
d
-dimensionalvector
h
i
.Theensemblepruningtaskaims
tondacompactsubsetoftheoriginalensemblewhichwill

predictthelabelswithhighaccuracy.Thesechosenindivid
ual
classiersneedtobediverseandaccuratesimultaneouslyt
o
achievethisgoal.Tothisend,weselectsomediversied

individualclassiersfromtheoriginalensemblewhichare

relevanttothevectorofclasslabels.Hencewestrivetode
ne
ametricdistancebetweenindividualclassiersinconside
ra-
tionofdiversityandaccuracyconcurrentlyinspiredby[53
]
sothattheensemblepruningproblemwouldbereducedtoan

objectionmaximizationproblem.
Giventwodiscreterandomvariables
X
and
Y
,Coverand
Thomas[55]denedthemutualinformation
I(
;)
between
them,i.e.,
I(
X
;
Y
)=H(
X
)

H(
X
j
Y
)
=
X
x
2
X;y
2
Y
p
(
x;y
)log
p
(
x;y
)
p
(
x
)
p
(
y
)
;
(1)
andthenZadeh
etal.
[53]denedthenormalizedmutualin-
formation
MI(
;)
andthenormalizedvariationofinformation
VI(
;)
ofthem,i.e.,
MI(
X;Y
)=
I(
X
;
Y
)
p
H(
X
)H(
Y
)
;
(2)
VI(
X;Y
)=1

I(
X
;
Y
)
H(
X;Y
)
;
(3)
wherein
p
(
;)
,
H(
)
,and
H(
;)
arethejointprobability,the
entropyfunction,andthejointentropyfunction,respecti
vely.
Considertheclasslabelvector
c
andtwoclassicationresult
vectors(
h
i
and
h
j
)generatedoverthedataset
D
byanytwo
individualclassiers(
h
i
and
h
j
).Inthiscase,thenormalized
mutualinformation
MI(
h
i
;c
)
exhibitstherelevancebetween
thisindividualclassier
h
i
andtheclasslabelvector
c
,im-
plyingtheaccuracyofthisindividualclassieronthetrai
ning
dataset;thenormalizedvariationofinformation
VI(
h
i
;h
j
)
re-
vealstheredundancybetweenthesetwoindividualclassie
rs,
indicatingthediversitybetweenthem.Sinceclasslabelsh
ave
alreadybeendiscretevaluesandthesevaluesareonlyrelev
ant
tothenumberofclassesinthoseuseddatasets,wedonotneed

todiscretizecontinuousvariablestocalculatetheprobab
ilities
usedin
MI(
;)
and
VI(
;)
,whileZadeh
etal.
[53]haveto
dealwithit.
Inordertotakebothdiversityandaccuracyintoconsidera-
tionconcurrently,theobjectivefunctionbetweentwoindi
vid-
ualclassiers(
atrade-offbetweendiversityandaccuracyof
twoindividualclassiers
,
TDAC
)isdenednaturallyas
TDAC(
h
i
;h
j
)
=
(

VI(
h
i
;
h
j
)+(1


)
MI(
h
i
;
c
)+MI(
h
j
;
c
)
2
;
if
h
i
6=
h
j
;
0
;
otherwise
;
(4)
wherearegularizationfactor

isintroducedtobalance
betweenthesetwocriteria,indicatingtheirimportanceas
well.
Therstcriterionistoraisediversitybyavoidingredunda
ncy,
andthesecondoneistopromoteaccuracybymaximizing

theirrelevance.Notethat
VI(
;)
ismetric[56]and
MI(
;c
)
isnon-negative[53].Consequently
TDAC(
;)
ismetric
aswell,whichmeans
TDAC(
h
i
;h
j
)+TDAC(
h
j
;h
k
)
>
TDAC(
h
i
;h
k
)
.
Subsequently,foranensemble
H
thatisasetcomposed
of
n
individualclassiers,theobjection(
atrade-offbetween
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,
VOL.,NO.,MONTHYEAR
4
diversityandaccuracyofasetofanensemble
,
TDAS
)is
denednaturallyas
TDAS(
H
)=
1
2
X
h
i
2H
X
h
j
2H
TDAC(
h
i
;h
j
)
:
(5)
Notethat
TDAS(
H
)
inEq.(5)couldbereformulatedas
TDAS(
H
)
=
1
2

X
h
i
2H
X
h
j
2H
VI(
h
i
;
h
j
)+
n

1
2
(1


)
X
h
i
2H
MI(
h
i
;
c
)
;
(6)
where
VI(
;)
oftwosimilarindividualclassierswillbenear
tozero.TherstterminEq.(6)preventstoselectsimilar

individualclassiers,andthesecondtermensuresthattho
se
selectedindividualclassiersarerelevanttotheclassla
bels.
Thereforetheensemblepruningtaskisreformulatedasan

objectivefunctionmaximizationproblem,whichaimstond

asubset
PˆH
withaspeciedcondition
jPj
=
k
torestrict
thesizeoftheprunedsub-ensemble,
max
PˆH
;
jPj
=
k
TDAS(
P
)=max
PˆH
;
jPj
=
k
1
2
X
h
i
2P
X
h
j
2P
TDAC(
h
i
;h
j
)
:
(7)
Itisabi-objectiveoptimizationproblemtocombinediver-
sityandaccuracyconcurrentlyforensemblepruningtasks,

butwecouldmanagetoreformulateitasasingle-objective

optimizationproblembyintroducing
TDAC(
;)
inEq.(4).
Thatisonewaytosolvetheproblembyusingtheobjective

weighting[57].Anotherwaycouldbeintroducingaconcept

ofdomination[50],[58]toobtainaParetooptimalsolution
,
whichwewillleaveitforfuturework.Besides,oursolution

managestoxthesizeoftheprunedsub-ensemblewhilethe

lattersolutioncannot,whichusuallyleadstooversizesub
-
ensemblesandaffectsthespacecost.
Algorithm1
CentralizedObjectionMaximizationforEnsemble
Pruning(
COMEP
)
Input:
Setofanoriginalensemble
H
,threshold
k
asthesizeofthe
prunedsub-ensemble.
Output:
Setoftheprunedsub-ensemble
P
satisfyingthat
PˆH
and
jPj
6
k
.
1:
P 
anarbitraryindividualclassier
h
i
2H
.
2:
for
2
6
i
6
k
do
3:
h

 
argmax
h
i
2HnP
Ph
j
2P
TDAC(
h
i
;h
j
)
.
4:
Move
h

from
H
to
P
.
5:
endfor
Uptonow,wehavemodeledtheensemblepruningtask
throughanobjectivefunctionmaximizationproblemasshow
n
inEq.(7),whichisenoughtoformacentralizedalgorithm

toaccomplishthisgoalofensemblepruning.Thiscentraliz
ed
method,namedasﬁ
CentralizedObjectionMaximizationfor
EnsemblePruning(COMEP)
ﬂshowninAlgorithm1,selects
greedilythecurrentoptimalclassierateachstep,andcou
ld
achievea
1
=
2
approximationfactorforobjectivefunction
maximizationproblemaccordingto[59].

B.DistributedObjectionMaximizationforEnsemblePrunin
g
ﬁ
DistributedObjectionMaximizationforEnsemblePruning
(DOMEP)
ﬂ,thedistributedversionof
COMEP
shownin
Algorithm2,adoptsatwo-rounddivide-and-conquerstrate
gy
andcomposablecore-sets[60]asguidelines,whicharepar-

ticularlysuitablefordistributedsettings.Itpartition
saset
ofindividualclassiersofanensembleintosmallerpieces
,
solvestheensemblepruningproblemoneachpieceseparatel
y,
andeventuallyobtainsasubsetfromtheunionofthese

representativesubsetsforallpieces.
Considerasetof
n
trainedindividualclassiers
H
=
f
h
i
g
n

i
=1
astheoriginalensemble.Intherstphase,aprimary
machinepartitionsallindividualclassiersintheorigin
al
ensembleinto
m
groups
fH
i
g
m

i
=1
randomlyandallocatesthem
todifferentmachines.Notethat
[
m

i
=1
H
i
=
H
where
m
isthe
numberofmachines,andthattheprimarymachinecouldbe

anyoneoftheseavailablemachines.Foreach
i
(1
6
i
6
m
)
,
machine
i
runs
COMEP
onitsallocatedset
H
i
independently
andselectsasubset
P
i
fromitinparallel.Inthesecondphase,
theprimarymachinegathersallsubsets,runs
COMEP
ontheir
union
[
m

i
=1
P
i
toproduceasubset
P
0
,andeventuallyoutputs
thebestoneofthembycomparing
P
0
with
P
i
(1
6
i
6
m
)
accordingtoEq.(7).Itsufcestooutputthesatisfyingsub
set
P
afterthesetwophases(Lines1Œ5inAlgorithm2)in
practiceandtheadditionalcomparisonpurposestogetahig
her
approximationfactorwhichis
1
=
4
theoreticallyandcouldeven
reach
8
=
25
undersomespecialconditions[53].
Algorithm2
DistributedObjectionMaximizationforEnsemble
Pruning(
DOMEP
)
Input:
Setofanoriginalensemble
H
,threshold
k
asthesizeofthe
prunedsub-ensemble,numberofmachines
m
.
Output:
Setoftheprunedsub-ensemble
P
meetingthat
PˆH
and
jPj
6
k
.
1:
Partition
H
randomlyinto
m
groupsasequallyaspossible,i.e.,
H
1
;:::;
H
m
.
2:
for
1
6
i
6
m
do
3:
P
i
 
COMEP
(
H
i
,
k
).
4:
endfor
5:
P
0
 
COMEP
(
[
m

i
=1
P
i
,
k
).
6:
P 
argmax
T2fP
1
;:::;
P
m
;
P
0
g
TDAS(
T
)
.
Atlast,theremightbesomeconfusionoverhowtopartition
n
individualclassiersin
H
randomlyinto
m
groupsas
equallyaspossible.Therststepistoshufetheseindivid
ual
classierswithadifferentrandomorder,andthesecondste
p
istoarrangethemtoget
m
groups.Therewouldbenodoubt
when
n
isamultipleof
m
inwhicheachgroupwouldcontain
n=m
individualclassiers.Butif
n
isnotamultipleof
m
,
therewouldbe
(
n
mod
m
)
groupsofthemthateachcontains
d
n=m
e
individualclassiersand
(
n
mumble
m
)
groupsof
themthateachcontains
b
n=m
c
individualclassiers.Notice
that
n
mumble
m
=
m
d
n=m
e
n
accordingto[61].
C.AGeneralDistributedFrameworkforEnsemblePruning
Ageneraldistributedframeworkisextractedfrom
DOMEP
,
namedasﬁ
EnsemblePruningFrameworkinaDistributed
Setting(EPFD)
ﬂshowninAlgorithm3,whichlikewiseadopts
thetwo-rounddivide-and-conquerstrategyandcomposable

core-sets[60].Itenablestheensemblepruningproblemtob
e
solvedfastinadistributedway.Ensemblepruningisusuall
y
describedasaprocesstoacquiretheoptimumsubsetfromthe

originalensemble.Let
ALG
denoteanarbitraryalgorithmto
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,
VOL.,NO.,MONTHYEAR
5
M
1
M
1
M
2
M
3

M
m
M
1
P
1
=
ALG
(
H
1
)
P
2
=
ALG
(
H
2
)
P
3
=
ALG
(
H
3
)
P
i
=
ALG
(
H
i
)
P
m
=
ALG
(
H
m
)
P
0
=
ALG
(
[
m

i
=1
P
i
)
P
=
Best
(
P
1
;:::;
P
m
;P
0
)
Partition
Gather
Compare
(a)TheFirstPhase
(b)TheSecondPhase
Fig.1.DiagramofAlgorithm3:EnsemblePruningFrameworki
naDistributedSetting(
EPFD
).(a)Intherstphase,aprimarymachine(e.g.,
M
1
)partitions
allindividualclassiersintheoriginalensembleinto
m
groups
fH
i
g
m

i
=1
randomlyandallocatesthemtodifferentmachines.Foreach
i
(1
6
i
6
m
)
,the
machine
M
i
runs
ALG
onitsallocatedset
H
i
independentlyandselectedasubset
P
i
fromitinparallel.(b)Inthesecondphase,theprimarymach
ine
(e.g.,
M
1
)gathersallsubsets,runs
ALG
ontheirunion
[
m

i
=1
P
i
toproduceasubset
P
0
,andeventuallyoutputsthebestoneofthembycomparing
P
0
with
P
i
(1
6
i
6
m
)
.Remark:(1)
ALG
couldbeanyoneofexistingpruningmethod,including
COMEP
.(2)Thenal
P
ischosenby
Best
(

)
accordingtosome
certaincriteriasuchasaccuracyor
TDAS(

)
in
DOMEP
.
Algorithm3
EnsemblePruningFrameworkinaDistributedSetting
(
EPFD
)
Input:
Setofanoriginalensemble
H
,numberofmachines
m
,a
pruningmethod
ALG
.
Output:
Setoftheprunedsub-ensemble
P
meetingthat
PˆH
.
1:
Partition
H
into
fH
i
g
m

i
=1
randomly.
2:
for
1
6
i
6
m
do
3:
P
i
 
outputfromanypruningmethod
ALG
on
H
i
.
4:
endfor
5:
P
0
 
outputfrom
ALG
on
[
m

i
=1
P
i
.
6:
P 
thebestoneamong
P
i
;:::;
P
m
;
and
P
0
accordingtosome
certaincriteriasuchasaccuracy.
performthistaskand
H
theoriginalensemble.
EPFD
consists
oftwomainphasesjustlike
DOMEP
thatcouldberegarded
asaspecialcaseof
EPFD
where
COMEP
ischosenasthe
usedpruningmethod(Lines3,5inAlgorithm3).Anotherkey

differenceisthatthecriterion(Line6inAlgorithm3)here
is
notlimitedtoEq.(7).Forinstance,itcoulduseaccuracyor

anyothermeasurescorrespondingtodatatocomparediffere
nt
subsets.
EPFD
isasimpleyetpowerfultooltoaccelerate
theoriginalmethodsforensemblepruningwithoutmuch

performancedegradation,whichiselaboratedinSectionIV
-C.
IV.E
XPERIMENTS
Inordertoevaluateourproposedmethods,inthissection,
weelaborateourexperimentson17binaryand12multi-class

datasetsincludinganimagedatasetwith12,500pictures

(Dogsvs.Cats
1
)and28datasetsfromUCIrepository[62].
Standard5-foldcross-validationisusedintheseexperime
nts
wheretheentiredatasetissplitintothreepartsineach

iteration,with60%asthetrainingset,20%asthevalidatio
n
set,and20%asthetestset.Besides,weconstructhomoge-

neousensemblesusingBaggingonvarioustypesofclassier
s
includingdecisiontrees(DT),naiveBayesian(NB)classi
ers,
k
-nearestneighbors(KNN)classiers,linearmodel(LM)
1
http://www.kaggle.com/c/dogs-vs-cats
classiers,andlinearSVMs(LSVM).Anensembleisrstly

trainedonthetrainingset,thenprunedbyapruningmethodo
n
thevalidationset,andnallytestedonthetestset.Thebas
e-
linesthatweconsiderareavarietyofranking-basedmethod
s
aswellasoptimization-basedmethods.Theseranking-base
d
methodsincludeKL-divergencePruning(KL),KappaPruning

(KP)[43],OrientationOrderingPruning(OO)[46],Reduce-

ErrorPruning(RE)[63],DiversityRegularizedEnsemble

Pruning(DREP)[44],andOrdering-basedEnsemblePruning

(OEP);Theseoptimization-basedmethodsareSingle-objec
tive
EnsemblePruning(SEP)andParetoEnsemblePruning(PEP)

[50].Notethatseveralmethodscannotxthenumberof

learnersafterensemblepruning(suchasOO,DREP,SEP,

OEP,andPEP),whileotherscouldxitbygivingapruning

ratewhichistheuplimitofthepercentageofthosediscarde
d
individualclassiersintheoriginalensemble.Thosemeth
ods
thatcannotxthesizemightleadtooversizeorundersizesu
b-
ensemblesandaffecttheirspacecost.Duetospaceconstrai
nts,
weonlyreportthecomparisonsofthetimecostandthetest

accuracyhereinafter.

A.Comparisonof
COMEP
and
DOMEP
totheState-of-the-
artEnsemblePruningMethods
Inthissubsection,wecomparethequalityofvarious
ensemblepruningmethods(theoriginalcentralizedversio
n)
includingKL,KP,OO,RE,DREP,SEP,OEP,andPEPwith

ourproposedcentralized(
COMEP
)anddistributed(
DOMEP
)
methods.
ExperimentalresultsreportedinTableIcontaintheaverag
e
testaccuracyofeachmethodandthecorrespondingstandard

deviationunder5-foldcross-validationoneachdataset.E
ach
rowinTableIcomparestheclassicationaccuracyusing

baggingwiththesametypeofindividualclassiers.The

resultswithhigheraccuracyandlowerstandarddeviationa
re
indicatedwithboldfontsforeachdataset(row).Besides,

weexaminethesignicanceofthedifferenceintheaccuracy
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,
VOL.,NO.,MONTHYEAR
6
TABLEI
C
OMPARISONOFTHESTATE
-
OF
-
THE
-
ARTMETHODSWITH
COMEP
AND
DOMEP
USING
B
AGGINGTOPRODUCEANENSEMBLEWITH
DT
SAS
INDIVIDUALCLASSIFIERS
.
DatasetKLKPOOREDREPSEPOEPPEP
COMEPDOMEP
Iono89.43

3.4490.29

3.4190.57

4.6988.57

3.0389.71

3.7090.86

2.9689.71

4.33
91.71

3.41
91.14

3.4191.71

3.56
Liver61.45

8.2462.32

5.1264.06

7.3561.16

4.96
z
56.23

8.4160.87

4.35
z
64.64

4.5462.32

4.2362.90

4.18
64.64

4.30
Spam93.28

1.1393.21

0.9393.30

1.4393.54

1.1692.08

1.4893.43

0.82
93.56

1.24
93.38

0.8093.41

0.9093.45

1.00
Wisconsin94.37

3.5894.67

3.25
z
95.56

4.0695.41

3.2593.93

3.6894.96

3.86
z
95.26

3.5095.41

3.7995.56

3.70
95.70

3.45
Credit100.00

0.00100.00

0.00100.00

0.00100.00

0.00100.00

0.00100.00

0.00100.00

0.00100.00

0.00100.00

0.00100.00

0.00
Landsat97.34

0.4597.22

0.6697.29

0.3797.31

0.2896.08

1.06
z
97.26

0.2697.34

0.3597.29

0.5697.15

0.43
97.42

0.34
Wilt98.24

0.3298.24

0.2698.37

0.4298.28

0.3298.01

0.5198.37

0.5198.16

0.1898.39

0.53
98.41

0.33
98.35

0.38
Shuttle99.96

0.0399.96

0.03
99.97

0.0299.97

0.0299.97

0.02
99.96

0.0399.96

0.03
99.97

0.02
99.96

0.0399.97

0.03
Ecoli94.55

3.1494.85

2.0393.64

2.7195.15

2.4991.52

3.49
z
94.85

2.7593.03

2.3094.24

2.7195.15

2.49
95.15

1.98
SensorReadings99.25

0.4099.19

0.5099.41

0.47
99.52

0.24
99.28

0.2599.45

0.2399.50

0.1999.36

0.4399.43

0.3399.38

0.41
t
-Test(W/T/L)0/10/01/9/00/10/01/9/02/8/02/8/00/10/00
/10/00/10/0Š
AverageRank7.157.204.804.608.405.555.204.754.402.95
1
Thereportedresultsaretheaveragetestaccuracyofeachme
thodandthecorrespondingstandarddeviationunder5-fold
cross-validationoneachdata
set.Thebestresultswithhigheraccuracyandlowerstandar
ddeviationarehighlightedinboldface.
2
Bytwo-tailedpaired
t
-testat5%signicancelevel,
z
and
y
denotethattheperformanceof
DOMEP
issuperiortoandinferiortothatofthecomparative
method,respectively.
3
Thelasttworowsshowtheresultsof
t
-testandaveragerank,respectively.TheﬁW/T/Lﬂin
t
-testindicatesthat
DOMEP
issuperiorto,notsignicantly
differentfrom,orinferiortothecorrespondingcomparati
vemethods.TheaveragerankiscalculatedaccordingtotheF
riedmantest[64].
TABLEII
C
OMPARISONOFTHESTATE
-
OF
-
THE
-
ARTMETHODSWITH
COMEP
AND
DOMEP
USING
B
AGGINGTOPRODUCEANENSEMBLEWITH
SVM
SAS
INDIVIDUALCLASSIFIERS
.
DatasetKLKPOOREDREPSEPOEPPEP
COMEPDOMEP
Liver
58.84

1.6558.84

1.65
58.26

1.21
58.84

1.6558.84

1.65
58.55

1.65
58.84

1.65
58.55

1.65
58.84

1.6558.84

1.65
Ringnorm98.43

0.37
98.53

0.30
y
98.51

0.36
y
98.47

0.2998.43

0.2998.47

0.3698.39

0.2598.44

0.3198.43

0.3198.43

0.31
Waveform91.45

1.6091.23

1.5091.35

1.5891.17

1.6790.83

1.52
z
91.27

1.5391.13

1.6191.37

1.63
91.45

1.49
91.23

1.63
Credit78.02

0.0778.01

0.1178.03

0.0878.02

0.0877.96

0.12
78.04

0.12
78.02

0.1178.02

0.1378.00

0.0978.01

0.11
Landsat65.24

0.0065.24

0.0065.24

0.0065.24

0.0065.24

0.0065.24

0.0065.24

0.0065.24

0.0065.24

0.0065.24

0.00
Page91.26

0.4191.26

0.4591.28

0.4491.24

0.4691.28

0.5291.22

0.3291.24

0.44
91.30

0.53
91.26

0.4591.22

0.44
Wilt94.68

0.0994.68

0.0994.68

0.0994.68

0.0994.68

0.0994.68

0.0994.68

0.0994.68

0.0994.68

0.0994.68

0.09
SensorReadings89.17

0.7889.24

0.8889.28

0.5289.35

0.5188.91

0.6089.11

0.91
z
89.26

0.50
89.51

0.94
89.42

0.8389.37

1.04
EEGEyeState55.13

0.0055.13

0.0055.13

0.0055.13

0.0055.13

0.0055.13

0.0055.13

0.0155.13

0.0055.13

0.0055.13

0.00
WaveformNoise86.35

0.7486.11

0.8386.37

0.7986.21

0.9485.99

0.8386.49

1.0286.11

0.9886.29

0.77
86.55

0.68
86.43

0.81
t
-Test(W/T/L)0/10/00/9/10/9/10/10/01/9/01/9/00/10/00
/10/00/10/0Š
AverageRank5.205.604.605.507.055.506.604.554.655.75
0123456789101112
KL
KPOOREDREPSEPOEPPEPCOMEPDOMEP(a)
KLKPOOREDREPSEPOEPPEPCOMEPDOMEP020406080100120aggr.rank.accuracy(b)
Fig.2.Comparisonofthestate-of-the-artmethodswith
COMEP
and
DOMEP
onthetestaccuracy.(a)Friedmantestchart(non-overlapp
ingmeans
signicantdifference)[64].(b)Theaggregatedrankforea
chmethod(the
smallerthebetter)[50].

performancebetweentwoensemblepruningmethodsbytwo-

tailedpaired
t
-testat5%signicanceleveltotellwhetherthese
twomethodshavesignicantlydifferentresults.Twometho
ds
endinatieifthereisnosignicantstatisticaldifference
;
otherwise,theonewithhighervaluesofaccuracywillwin.

InthelasttworowsofTableI,theaveragerankofeach

methodiscalculatedbasedontheFriedmantest[64],and

theperformanceofeachmethodiscomparedwith
DOMEP
intermsofthenumberofdatasetsthat
DOMEP
haswon,
tied,orlost,respectively.Itcanbeinferredthat
DOMEP
doesnotunderperformcentralizedmethodsinmanydata

setseventhoughitonlyutilizeslocalinformationunlike

others,whichconrmsthereasonablenessof
DOMEP
(and
COMEP
)utilizingaccuracyanddiversitysimultaneously.De-
spiteslightlylowervaluesofaccuracyinsomecases,
DOMEP
remainsacceptableresults.Similarresultsarereportedi
n
TableIIandTableIIIusingdifferentindividualclassier
s.
Moreover,Figure2reportsthecomparisonofthestate-of-t
he-
artmethodswith
COMEP
and
DOMEP
onthetestaccuracy
usingstatisticaltestmethods[50],[64].Figure2(a)show
s
that
COMEP
and
DOMEP
havesignicantsuperiorityover
othercomparedcentralizedmethods,utilizingtheFriedma
n
testchart[64];Figure2(b)representstheaggregatedrank
for
eachmethod,depictingthesameresults.

B.
DOMEP
vs.
COMEP
overtheTimeCost
Inthissubsection,wegivethecomplexityanalysisofthe
proposedmethodsandpresentthecorrespondingexperiment
al
results.AccordingtotheAlgorithm1,thecomputational

complexityof
COMEP
isanalyzedasfollows:
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,
VOL.,NO.,MONTHYEAR
7
TABLEIII
C
OMPARISONOFTHESTATE
-
OF
-
THE
-
ARTMETHODSWITH
COMEP
AND
DOMEP
USING
B
AGGINGTOPRODUCEANENSEMBLEWITH
KNN
SAS
INDIVIDUALCLASSIFIERS
.
DatasetKLKPOOREDREPSEPOEPPEP
COMEPDOMEP
Ames58.83

5.4858.54

7.20
60.29

7.45
57.81

7.1457.52

7.2959.71

6.1859.56

6.9658.69

5.9060.15

6.9059.12

5.95
Card67.59

2.1066.86

1.6866.72

1.6867.01

2.2764.38

2.2767.88

2.8765.99

1.2265.55

2.39
z
68.32

3.33
68.03

3.44
Sonar80.98

7.9880.00

8.5281.95

7.0381.95

5.6279.02

5.6281.46

5.8780.00

5.2978.05

5.72
82.44

7.19
80.98

8.69
Page95.87

0.3595.87

0.4995.81

0.4595.76

0.3595.76

0.3995.80

0.5795.87

0.5595.81

0.4695.87

0.64
95.89

0.64
Wilt97.93

0.2897.89

0.32
98.01

0.24
97.99

0.2097.77

0.3697.93

0.3597.89

0.4098.01

0.3297.97

0.1998.01

0.26
Landsat90.28

0.8890.22

1.1890.30

0.9590.40

0.8390.31

0.94
90.47

0.78
90.37

0.8590.37

0.7690.47

1.2190.44

0.87
Shuttle99.82

0.0399.81

0.0399.82

0.04
99.82

0.02
99.82

0.0499.81

0.0499.81

0.0399.81

0.0399.80

0.03
z
99.82

0.04
Ecoli95.15

2.9194.55

3.14
95.76

2.91
95.45

3.0393.33

3.1494.85

2.9594.24

3.1194.85

3.6595.45

3.03
95.76

2.91
WaveformNoise84.34

1.0584.20

1.4684.48

2.0984.36

2.2483.54

0.7484.36

1.3384.48

2.2484.16

1.5884.48

1.81
84.62

1.77
EEGEyeState95.99

0.5596.06

0.5296.16

0.5095.98

0.5995.03

0.37
z
96.11

0.5096.08

0.66
96.28

0.55
96.19

0.6296.01

0.46
SensorReadings96.29

0.4796.11

0.3996.33

0.6296.37

0.5396.00

0.5196.29

0.6496.39

0.4296.28

0.60
96.42

0.64
96.31

0.31
t
-Test(W/T/L)0/11/00/11/00/11/00/11/01/10/00/11/00/1
1/01/10/01/10/0Š
AverageRank5.827.453.775.278.955.095.776.553.053.27
Experiment00.511.522.533.5Speedup(a)
Experiment00.511.52Speedup(b)
Experiment00.511.5Efficiency(c)
Experiment00.20.40.60.81Efficiency(d)
Fig.3.Comparisonofspeedupandefciencybetween
COMEP
and
DOMEP
.(a)Speedupwithtwomachines.(b)Speedupwiththreemachi
nes.
(c)Efciencywithtwomachines.(d)Efciencywiththreema
chines.
Firstly,thecomplexityofanessentialtermof

TDAC(
h
i
;h
j
)
is
O
(
d
2
n
2

c
)
,where
d
and
n
c
arethe
numberofinstancesandlabels,respectively.
Secondly,thechosenindividualclassierisobtainedwith

thecomplexityof
O
(

1
3
k
3
+
1
2
nk
2
)
timesthatofthe
TDAC(
h
i
;h
j
)
termsince
P
k

i
=2
(
i

1)(
n

i
+2)=

1
3
k
3
+
n
+2
2
k
2

3
n
+4
6
k
,where
k
isthesizeofthepruned
sub-ensemble.
Therefore,theoverallcomputationalcomplexityof
COMEP
is
O

(

1
3
k
3
+
1
2
nk
2
)
d
2
n
2

c

.Besides,since
H
ispartitionedinto
m
groupsin
DOMEP
,theoverallcomputationalcomplexity
of
DOMEP
is
O

(

1
3
k
3
+
n
2
m
k
2
)
d
2
n
2

c

.
Inthisexperiment,weemploydifferentnumbersofma-
chinesin
DOMEP
inordertotestitsspeedupincomparison
with
COMEP
.Undertheidealconditions,Zadeh
etal.
[53]
pointedthatthespeedupbetweenthedistributedandcentra
l-
izedversionwasalmostlinearintermsofthenumberof

usedmachinessincetherewasnooverheadofinformation-

sharingbetweenthosemachines.Toverifywhether
DOMEP
KLKPOOREDREPSEPOEPPEP9092949698Accuracy (%)CentralizedDistributed(a)
KLKPOOREDREPSEPOEPPEP020406080Time Cost (s)CentralizedDistributed(b)
Fig.4.Comparisonbetweenthestate-of-the-artensemblep
runingmethods
andtheirdistributedversionsontheSensorReadingsdatas
etusingBagging
withDTsasindividualclassiersforbinaryclassication
.(a)Accuracy.(b)
TimeCost.
KLKPOOREDREPSEPOEPPEP9092949698Accuracy (%)CentralizedDistributed(a)
KLKPOOREDREPSEPOEPPEP020406080100Time Cost (s)CentralizedDistributed(b)
Fig.5.Comparisonbetweenthestate-of-the-artensemblep
runingmethods
andtheirdistributedversionsontheSensorReadingsdatas
etusingBagging
withKNNsasindividualclassiersforbinaryclassicatio
n.(a)Accuracy.(b)
TimeCost.

couldachievecompetitiveperformancefasterthan
COMEP
or
not,weintroducetwoperformanceindicatorsfromparallel

processing,i.e.,speedupandefciency
2
,wherespeedupis
denedasaquotientoftheexecutiontimeof
COMEP
and
thatof
DOMEP
,toreporthowmuchspeedupof
DOMEP
over
COMEP
.Constrainedbythecapabilityofmachinesonwhich
wetest,severalexperimentsforensemblepruningconducte
d
ontwoorthreemachinesareusedasatypicalexampleto

presenttheperformanceof
DOMEP
.Theresultsusingvarious
2
Efciencyisameasureofhoweffectivelyparallelcomputin
gcouldbeused
tosolveaparticularproblem.Aparallelalgorithmisconsi
deredcostefcient
ifitsasymptoticrunningtimemultipliedbythenumberofpr
ocessingunits
involvedinthecomputationiscomparabletotherunningtim
eofthebest
sequentialalgorithm[65].
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,
VOL.,NO.,MONTHYEAR
8
KLKPOOREDREPSEPOEPPEP9092949698100Accuracy (%)CentralizedDistributed(a)
KLKPOOREDREPSEPOEPPEP010203040506070Time Cost (s)CentralizedDistributed(b)
Fig.6.Comparisonbetweenthestate-of-the-artensemblep
runingmethods
andtheirdistributedversionsontheSensorReadingsdatas
etusingBagging
withDTsasindividualclassiersformulti-classclassic
ation.(a)Accuracy.
(b)TimeCost.
KLKPOOREDREPSEPOEPPEP868890929496Accuracy (%)CentralizedDistributed(a)
KLKPOOREDREPSEPOEPPEP020406080Time Cost (s)CentralizedDistributed(b)
Fig.7.Comparisonbetweenthestate-of-the-artensemblep
runingmethods
andtheirdistributedversionsontheSensorReadingsdatas
etusingBagging
withKNNsasindividualclassiersformulti-classclassi
cation.(a)Accuracy.
(b)TimeCost.

settingsofthisexperimentaresummarizedinFigure3,whic
h
indicatesthat
DOMEP
runsfasterthan
COMEP
evenreaching
super-linearspeedup
3
onatinyminorityofexperiments.
C.ComparisonBetweentheState-of-the-artEnsemblePrun-

ingMethodsandTheirCorrespondingDistributedVersions

Generatedwith
EPFD
Inthissubsection,toverifytheeffectivenessof
EPFD
,we
comparethequalityofvariouscentralizedensembleprunin
g
methodsandtheirrespectivedistributedversionsthatare

generatedusing
EPFD
intermsofaccuracyandtimecost.To
testthequalityoftheselectedsub-ensemblesofeachmetho
d,
wecontrolthemunderthesameconditions(includingtheem-

ployedensemblemethodsortypesofindividualclassiers)
in
eachexperiment.Figure4showsthecomparisonresultswhen

individualclassiersaredesignatedasDTsandassembled

bybaggingforbinaryclassication.Itcanbeinferredthat
,
foreachpruningmethod(eachgrouponthehorizontalaxis),

theaccuracyofthedistributedversionissuperiororequal
to
thatofitscorrespondingcentralizedversion.Inconsider
ation
ofthelesstimecostittakes,webelievethatthedistribute
d
versionofeachmethodoutperformsitsoriginalcentralize
d
version.Besides,wecantellthattheeffectivenessof
EPFD
isexceptionallyevidentonPEP,i.e.,acomplicatedmethod

utilizinganevolutionaryParetooptimizationcombinedwi
th
alocalsearchsubroutine.Moreover,Figure5reportssimil
ar
resultsofthatwithKNNsasindividualclassiersforbinar
y
3
Sometimesaspeedupofmorethan
m
whenusing
m
processorsis
observedinparallelcomputing,whichiscalledsuper-line
arspeedup[65].
1.451.51.551.6
Objective Function Value55606570Accuracy (%)Correlation = 0.7349(a)
2.92.9533.053.13.153.2
Objective Function Value505560657075Accuracy (%)Correlation = 0.7271(b)
Fig.8.Relationofbinaryclassicationaccuracyandobjec
tivefunctionvalue
for3-combinationsand4-combinationsintheAmesdatasetu
singBagging
withDTsasindividualclassiersforbinaryclassication
.(a)3-combinations.
(b)4-combinations.
1.681.71.721.741.761.781.8
Objective Function Value83848586878889Accuracy (%)Correlation = 0.7770(a)
3.43.453.53.553.6
Objective Function Value83848586878889Accuracy (%)Correlation = 0.7581(b)
Fig.9.Relationofbinaryclassicationaccuracyandobjec
tivefunction
valuefor3-combinationsand4-combinationsintheWavefor
mdatasetusing
BaggingwithDTsasindividualclassiersforbinaryclassi
cation.(a)3-
combinations.(b)4-combinations.
1.681.71.721.741.76
Objective Function Value75767778798081Accuracy (%)Correlation = 0.7154(a)
3.383.43.423.443.463.483.53.52
Objective Function Value74757677787980Accuracy (%)Correlation = 0.7306(b)
Fig.10.Relationofmulti-classclassicationaccuracyan
dobjectivefunction
valuefor3-combinationsand4-combinationsintheWavefor
mdatasetusing
BaggingwithDTsasindividualclassiersformulti-classi
cation.(a)3-
combinations.(b)4-combinations.

classication;Figures6Œ7reportsimilarresultsofthatw
ith
DTsandKNNsasindividualclassiersformulti-classclass
i-
cation,respectively.

D.ValidatingtheObjectiveFunction
Regardinganewobjectivefunction,itsrelationwithclas-
sicationaccuracyisoneofthefundamentalquestions.We

selecttwosmall-sizedensembles(smallinthenumberof

individualclassiers)andevaluateallpossiblecombinat
ions
oftheseindividualclassiersinordertotestthisissue.I
nthis
experiment,wecomparetheclassicationaccuracyforallt
he
3-combinationsand4-combinationsofindividualclassie
rsin
theoriginalensembleagainsttheircorrespondingobjecti
ve
valuewiththe

parameterequalto0.5,whichmeansthat
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,
VOL.,NO.,MONTHYEAR
9
123456789
Size of the Pruned Sub-Ensemble9797.59898.5Accuracy (%) = 0.1 = 0.5 = 0.9(a)
0.10.20.30.40.50.60.70.8
97.89898.298.498.6Accuracy (%)3579(b)
Fig.11.Effectof

valueontheclassicationaccuracyintheRingnorm
datasetusingBaggingwithSVMsasindividualclassiersfo
rbinaryclassi-
cation.(a)Accuracyofeachcriterionindividually.(b)S
lightdifferencesof

valuewhileselectingthedifferentsizeoftheprunedsub-e
nsemble(3,5,
7,9).
123456789
Size of the Pruned Sub-Ensemble60657075Accuracy (%) = 0.1 = 0.5 = 0.9(a)
0.10.20.30.40.50.60.70.8
60657075Accuracy (%)3579(b)
Fig.12.Effectof

valueontheclassicationaccuracyintheHeartdataset
usingBaggingwithKNNsasindividualclassiersforbinary
classication.
(a)Accuracyofeachcriterionindividually.(b)Slightdif
ferencesof

value
whileselectingthedifferentsizeoftheprunedsub-ensemb
le(3,5,7,9).
123456789
Size of the Pruned Sub-Ensemble70727476788082Accuracy (%) = 0.1 = 0.5 = 0.9(a)
0.10.20.30.40.50.60.70.8
70727476788082Accuracy (%)3579(b)
Fig.13.Effectof

valueontheclassicationaccuracyintheWaveform
datasetusingBaggingwithDTsasindividualclassiersfor
multi-class
classication.(a)Accuracyofeachcriterionindividuall
y.(b)Slightdifferences
of

valuewhileselectingthedifferentsizeoftheprunedsub-e
nsemble(3,
5,7,9).

twocriteriainEq.(4)areequallyimportant.Eachsmallblu
e
dotinFigure8representstheclassicationaccuracyona

3-combinationor4-combinationoftheindividualclassie
rs
withthesize8ofanensembleintheAmesdatasetforbinary

classication,andthelineistheregressionline.Similar
results
areshowninFigures9Œ10intheWaveformdatasetforbinary

classicationandmulti-classclassication,respective
ly.We
observethattheobjectivevalueandtheclassicationaccu
racy
arehighlycorrelatedfromFigures8Œ10,whichmeansthat

maximizingthisobjectivefunctionleadstoourtarget(i.e
.,
thehighlyaccuratesub-ensembles).

E.Effectof

Value
Crucialasotherissues,therelationoftwocriterianeedst
o
beinvestigatedinthedenedobjectivefunction.Toreveal
how
theclassicationresultsareaffectedwiththeregulariza
tion
factor
;
different

values(from0.1to0.9with0.2steps)
aretestedintheexperimentsofthispart.Figure11exempli
es
theeffectof

ontheRingnormdataset.Figure11(a)illustrates
thatthelinearcombinationconcurrentlyconsideringthem
both
performsbetterthanfocusingmoreon
MI
term(

=0
:1
)
or
VI
term(

=0
:9
)inEq.(6)individually,although
ndingtheoptimalvalueofthe

isanotherchallenge.
Figure11(b)presentsthataglobalmaximumaroundthe

optimal

existsindeedregardlessofthesizeofthepruned
sub-ensemble,whichsuggeststhatitmightberelatedtothe

intrinsicpropertiesofthedataset.Similarresultsarere
ported
inFigures12Œ13forbinaryclassicationandmulti-class

classication,respectively.Althoughproperresultsfor
alldata
setshavebeenbroughtwith

beingsetto0.5(inTablesIŒIII)
forconvenience,
DOMEP
wouldachieveabetterperformance
inpracticewhenthe

isadjustedforeachdatasetseparately.
V.C
ONCLUSION
Inthiswork,weformalizetheensemblepruningproblem
asanobjectionmaximizationproblembasedoninformation

entropytoconsiderdiversityandaccuracysimultaneously
.
Then,weproposeanensemblepruningmethodaccordingto

thisobjectionmaximizationproblem(includingtwoversio
ns,
COMEP
and
DOMEP
)forensemblepruning.Wealsopresent
thatourmethods(
COMEP
and
DOMEP
)areconsistentlycom-
petitivewithvariousexistingmethodsforensembleprunin
g,
whichcouldhandlelarge-scaleensemblesfastyetefcient
ly
throughhandlingtheaccuracyanddiversityoftheensemble
s
properly.Atlast,weproposeageneraldistributedframewo
rk
(
EPFD
)forensemblepruning,whichcouldbewidelyapplied
tovariousexistingmethodsforensemblepruning,toachiev
e
lesstimeconsumingwithoutmuchaccuracydegradation.The

remarkableeffectivenessof
EPFD
ispositivelyvaluablefor
enormousdataintherealworld.Forfuturework,itseems

likeapromisingdirectiontoexplorethedeepertheoretica
l
basisandtotryotherobjectivefunctionstoachievebetter

performance.
R
EFERENCES
[1]Z.-H.Zhou,
EnsembleMethods:FoundationsandAlgorithms
.CRC
press,2012.
[2]R.Girshick,J.Donahue,T.Darrell,andJ.Malik,ﬁRichf
eature
hierarchiesforaccurateobjectdetectionandsemanticseg
mentation,ﬂ
in
CVPR
,2014,pp.580Œ587.
[3]J.Wang,Z.Liu,Y.Wu,andJ.Yuan,ﬁMiningactionletense
mblefor
actionrecognitionwithdepthcameras,ﬂin
CVPR
,2012,pp.1290Œ1297.
[4]X.Zhou,L.Xie,P.Zhang,andY.Zhang,ﬁAnensembleofdee
pneural
networksforobjecttracking,ﬂin
ICIP
,Oct2014,pp.843Œ847.
[5]H.YkhlefandD.Bouchaffra,ﬁAnefcientensemblepruni
ngapproach
basedonsimplecoalitionalgames,ﬂ
InformFusion
,vol.34,pp.28Œ42,
2017.
[6]L.KunchevaandC.Whitaker,ﬁMeasuresofdiversityincl
assier
ensemblesandtheirrelationshipwiththeensembleaccurac
y,ﬂ
Mach
Learn
,vol.51,no.2,pp.181Œ207,2003.
[7]E.Tang,P.Suganthan,andX.Yao,ﬁAnanalysisofdiversi
tymeasures,ﬂ
MachLearn
,vol.65,no.1,pp.247Œ271,jul2006.
[8]L.Breiman,ﬁBaggingpredictors,ﬂ
MachLearn
,vol.24,no.2,pp.123Œ
140,1996.
[9]Y.Freund,ﬁBoostingaweaklearningalgorithmbymajori
ty,ﬂ
Inform
Comput
,vol.121,no.2,pp.256Œ285,1995.
[10]Y.Freund,R.E.Schapire
etal.
,ﬁExperimentswithanewboosting
algorithm,ﬂin
ICML
,vol.96.Bari,Italy,1996,pp.148Œ156.
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,
VOL.,NO.,MONTHYEAR
10
[11]T.Dietterich,ﬁEnsemblemethodsinmachinelearning,
ﬂin
MCS
,vol.
1857,2000,pp.1Œ15.
[12]Z.Liu,Q.Dai,andN.Liu,ﬁEnsembleselectionbygrasp,
ﬂ
ApplIntell
,
vol.41,no.1,pp.128Œ144,2014.
[13]Y.KokkinosandK.G.Margaritis,ﬁCondenceratioafn
itypropaga-
tioninensembleselectionofneuralnetworkclassiersfor
distributed
privacy-preservingdatamining,ﬂ
Neurocomputing
,vol.150,pp.513Œ
528,2015.
[14]C.-X.Zhang,J.-S.Zhang,andQ.-Y.Yin,ﬁAranking-bas
edstrategyto
prunevariableselectionensembles,ﬂ
Knowl-BasedSyst
,vol.125,pp.
13Œ25,2017.
[15]R.Lysiak,M.Kurzynski,andT.Woloszynski,ﬁOptimals
electionof
ensembleclassiersusingmeasuresofcompetenceanddiver
sityofbase
classiers,ﬂ
Neurocomputing
,vol.126,pp.29Œ35,2014.
[16]I.Partalas,G.Tsoumakas,andI.P.Vlahavas,ﬁFocused
ensemble
selection:Adiversity-basedmethodforgreedyensemblese
lection.ﬂin
ECAI
,2008,pp.117Œ121.
[17]R.Caruana,A.Niculescu-Mizil,G.Crew,andA.Ksikes,
ﬁEnsemble
selectionfromlibrariesofmodels,ﬂin
ICML
.ACM,2004,p.18.
[18]R.E.Baneld,L.O.Hall,K.W.Bowyer,andW.P.Kegelmey
er,
ﬁEnsemblediversitymeasuresandtheirapplicationtothin
ning,ﬂ
Inform
Fusion
,vol.6,no.1,pp.49Œ62,2005.
[19]H.Chen,P.Tino,andX.Yao,ﬁPredictiveensembleprun
ingbyex-
pectationpropagation,ﬂ
IEEETKnowlDataEn
,no.7,pp.999Œ1013,
2009.
[20]Z.-H.Zhou,J.Wu,andW.Tang,ﬁEnsemblingneuralnetwo
rks:many
couldbebetterthanall,ﬂ
ArtifIntell
,vol.137,no.1-2,pp.239Œ263,
2002.
[21]Y.Yu,Y.-F.Li,andZ.-H.Zhou,ﬁDiversityregularized
machine,ﬂin
IJCAI
,2011,p.1603.
[22]P.MelvilleandR.Mooney,ﬁConstructingdiverseclass
ierensembles
usingarticialtrainingexamples,ﬂin
IJCAI
,2003,pp.505Œ510.
[23]R.Soares,H.Chen,andX.Yao,ﬁAcluster-basedsemisup
ervised
ensembleformulticlassclassication,ﬂ
TETCI
,vol.1,no.6,pp.408Œ
420,2017.
[24]G.U.Yule,ﬁOntheassociationofattributesinstatist
ics:withillustra-
tionsfromthematerialofthechildhoodsociety,&c,ﬂ
Philosophical
TransactionsoftheRoyalSocietyofLondon.SeriesA,Conta
ining
PapersofaMathematicalorPhysicalCharacter
,vol.194,pp.257Œ
319,1900.
[25]J.Cohen,ﬁAcoefcientofagreementfornominalscales
,ﬂ
EducPsychol
Meas
,vol.20,no.1,pp.37Œ46,1960.
[26]D.B.Skalak
etal.
,ﬁThesourcesofincreasedaccuracyfortwoproposed
boostingalgorithms,ﬂin
AAAI
,vol.1129,1996,p.1133.
[27]T.K.Ho,ﬁTherandomsubspacemethodforconstructingd
ecision
forests,ﬂ
IEEETPatternAnal
,vol.20,no.8,pp.832Œ844,1998.
[28]P.SneathandR.Sokal,
Numericaltaxonomy:Theprinciplesand
practiceofnumericalclassication.
WHFreeman,1973.
[29]G.GiacintoandF.Roli,ﬁDesignofeffectiveneuralnet
workensembles
forimageclassicationpurposes,ﬂ
ImageVisionComput
,vol.19,no.9,
pp.699Œ707,2001.
[30]J.L.Fleiss,B.Levin,andM.C.Paik,
Statisticalmethodsforratesand
proportions
.JohnWiley&Sons,2013.
[31]R.Kohavi,D.H.Wolpert
etal.
,ﬁBiasplusvariancedecompositionfor
zero-onelossfunctions,ﬂin
ICML
,vol.96,1996,pp.275Œ83.
[32]P.CunninghamandJ.Carney,ﬁDiversityversusquality
inclassication
ensemblesbasedonfeatureselection,ﬂin
ECML
.Springer,2000,pp.
109Œ116.
[33]C.A.ShippandL.I.Kuncheva,ﬁRelationshipsbetweenc
ombination
methodsandmeasuresofdiversityincombiningclassiers,
ﬂ
Inform
Fusion
,vol.3,no.2,pp.135Œ148,2002.
[34]L.K.HansenandP.Salamon,ﬁNeuralnetworkensembles,
ﬂ
IEEET
PatternAnal
,vol.12,no.10,pp.993Œ1001,1990.
[35]D.PartridgeandW.Krzanowski,ﬁSoftwarediversity:p
racticalstatistics
foritsmeasurementandexploitation,ﬂ
InformSoftwareTech
,vol.39,
no.10,pp.707Œ717,1997.
[36]Y.LiuandX.Yao,ﬁEnsemblelearningvianegativecorre
lation,ﬂ
Neural
Networks
,vol.12,no.10,pp.1399Œ1404,1999.
[37]H.ChenandX.Yao,ﬁRegularizednegativecorrelationl
earningfor
neuralnetworkensembles,ﬂ
IEEETNeuralNetwor
,vol.20,no.12,
pp.1962Œ1979,2009.
[38]ŠŠ,ﬁMultiobjectiveneuralnetworkensemblesbasedon
regularized
negativecorrelationlearning,ﬂ
IEEETKnowlDataEn
,vol.22,no.12,
pp.1738Œ1751,2010.
[39]G.ZenobiandP.Cunningham,ﬁUsingdiversityinprepar
ingensembles
ofclassiersbasedondifferentfeaturesubsetstominimiz
egeneraliza-
tionerror,ﬂin
ECML
.Springer,2001,pp.576Œ587.
[40]G.Brown,ﬁAninformationtheoreticperspectiveonmul
tipleclassier
systems,ﬂin
MCS
,2009,pp.344Œ353.
[41]Z.-H.ZhouandN.Li,ﬁMulti-informationensembledive
rsity,ﬂin
MCS
,
2010,pp.134Œ144.
[42]G.Tsoumakas,I.Partalas,andI.Vlahavas,ﬁAnensembl
epruning
primer,ﬂin
SUEMA
.Springer,2009,pp.1Œ13.
[43]D.MargineantuandT.Dietterich,ﬁPruningadaptivebo
osting,ﬂin
ICML
,
vol.97,1997,pp.211Œ218.
[44]N.Li,Y.Yu,andZ.-H.Zhou,ﬁDiversityregularizedens
emblepruning,ﬂ
in
ECMLPKDD
,2012,pp.330Œ345.
[45]G.Mart´nez-MuŸnozandA.Su´arez,ﬁUsingboostingto
prunebagging
ensembles,ﬂ
PatternRecognLett
,vol.28,no.1,pp.156Œ165,2007.
[46]Mart´nez-MuŸnozandSu´arez,ﬁPruninginorderedbag
gingensembles,ﬂ
in
ICML
,2006,pp.609Œ616.
[47]Z.-H.ZhouandW.Tang,ﬁSelectiveensembleofdecision
trees,ﬂin
RSFDGrC
,2003,pp.476Œ483.
[48]I.Partalas,G.Tsoumakas,andI.Vlahavas,ﬁAstudyong
reedyal-
gorithmsforensemblepruning,ﬂ
AristotleUniversityofThessaloniki,
Thessaloniki,Greece
,2012.
[49]H.Guo,F.Sun,J.Cheng,Y.Li,andM.Xu,ﬁAnovelmargin-
based
measurefordirectedhillclimbingensemblepruning,ﬂ
MathProblEng
,
2016.
[50]C.Qian,Y.Yu,andZ.-H.Zhou,ﬁParetoensemblepruning
,ﬂin
AAAI
,
2015,pp.2935Œ2941.
[51]P.Indyk,S.Mahabadi,M.Mahdian,andV.Mirrokni,ﬁCom
posable
core-setsfordiversityandcoveragemaximization,ﬂin
PODS
,2014,pp.
100Œ108.
[52]S.Aghamolaei,M.Farhadi,andH.Zarrabi-Zadeh,ﬁDive
rsitymaximiza-
tionviacomposablecoresets,ﬂin
CCCG
,2015,p.43.
[53]S.Zadeh,M.Ghadiri,V.Mirrokni,andM.Zadimoghaddam
,ﬁScalable
featureselectionviadistributeddiversitymaximization
,ﬂin
AAAI
,2017,
pp.2876Œ2883.
[54]P.Agarwal,S.Har-Peled,andK.Varadarajan,ﬁApproxi
matingextent
measuresofpoints,ﬂ
JACM
,vol.51,no.4,pp.606Œ635,2004.
[55]T.CoverandJ.Thomas,
Elementsofinformationtheory
.JohnWiley
&Sons,2012.
[56]N.Vinh,J.Epps,andJ.Bailey,ﬁInformationtheoretic
measuresfor
clusteringscomparison:Variants,properties,normaliza
tionandcorrec-
tionforchance,ﬂ
JMachLearnRes
,vol.11,no.Oct,pp.2837Œ2854,
2010.
[57]N.SrinivasandK.Deb,ﬁMuiltiobjectiveoptimization
usingnondom-
inatedsortingingeneticalgorithms,ﬂ
EvolComput
,vol.2,no.3,pp.
221Œ248,1994.
[58]K.Deb,A.Pratap,S.Agarwal,andT.Meyarivan,ﬁAfasta
ndelitist
multiobjectivegeneticalgorithm:Nsga-ii,ﬂ
IEEETEvolutComput
,
vol.6,no.2,pp.182Œ197,2002.
[59]B.BirnbaumandK.Goldman,ﬁAnimprovedanalysisforag
reedy
remote-cliquealgorithmusingfactor-revealingLPs,ﬂ
Algorithmica
,
vol.55,no.1,pp.42Œ59,2009.
[60]V.MirrokniandM.Zadimoghaddam,ﬁRandomizedcomposa
blecore-
setsfordistributedsubmodularmaximization,ﬂin
STOC
,2015,pp.153Œ
162.
[61]R.Graham,D.Knuth,andO.Patashnik,
ConcreteMathematics:AFoun-
dationforComputerScience
.Addison-WesleyLongmanPublishing
Co.,Inc.,2012.
[62]M.Lichman,ﬁUCImachinelearningrepository,ﬂ2013.[
Online].
Available:http://archive.ics.uci.edu/ml
[63]G.Mart´nez-MuŸnoz,D.Hern´andez-Lobato,andA.Su´
arez,ﬁAnanalysis
ofensemblepruningtechniquesbasedonorderedaggregatio
n,ﬂ
IEEET
PatternAnal
,vol.31,no.2,pp.245Œ259,2009.
[64]J.Demsar,ﬁStatisticalcomparisonsofclassiersov
ermultipledatasets,ﬂ
JMachLearnRes
,vol.7,no.Jan,pp.1Œ30,2006.
[65]Wikipedia,ﬁSpeedup,ﬂWebsite,accessedonJune14,20
19.[Online].
Available:https://en.wikipedia.org/wiki/Speedup
"
49,BaRC: Backward Reachability Curriculum for Robotic Reinforcement Learning,http://arxiv.org/pdf/1806.06161v2.pdf,https://github.com/StanfordASL/BaRC,"BaRC:BackwardReachabilityCurriculumforRoboticReinforcementLearning
BorisIvanovic
1
,JamesHarrison
2
,ApoorvaSharma
1
,MoChen
3
,MarcoPavone
1
Abstract
ŠModel-freeReinforcementLearning(RL)offers
anattractiveapproachtolearncontrolpoliciesforhigh-
dimensionalsystems,butitsrelativelypoorsamplecomplexity
oftennecessitatestraininginsimulatedenvironments.Evenin
simulation,goal-directedtaskswhosenaturalrewardfunction
issparseremainintractableforstate-of-the-artmodel-free
algorithmsforcontinuouscontrol.Thebottleneckinthese
tasksistheprohibitiveamountofexplorationrequiredto
obtainalearningsignalfromtheinitialstateofthesystem.
Inthiswork,weleveragephysicalpriorsintheformofan
approximatesystemdynamicsmodeltodesignacurriculum
foramodel-freepolicyoptimizationalgorithm.OurBackward
ReachabilityCurriculum(BaRC)beginspolicytrainingfrom
statesthatrequireasmallnumberofactionstoaccomplishthe
task,andexpandstheinitialstatedistributionbackwardsina
dynamically-consistentmanneroncethepolicyoptimizational-
gorithmdemonstratesperformance.BaRCisgeneral,
inthatitcanacceleratetrainingofanymodel-freeRLalgorithm
onabroadclassofgoal-directedcontinuouscontrolMDPs.
Itscurriculumstrategyisphysicallyintuitive,easy-to-tune,
andallowsincorporatingphysicalpriorstoacceleratetraining
withouthinderingtheperformance,,andapplicability
ofthemodel-freeRLalgorithm.Weevaluateourapproachon
tworepresentativedynamicroboticlearningproblemsand
substantialperformanceimprovementrelativetopreviouscur-
riculumgenerationtechniquesandna
¨
eexplorationstrategies.
I.I
NTRODUCTION
Reinforcementlearning(RL)isapowerfultoolfortrain-
ingagentstomaximizerewardaccumulationinsequential
decision-makingproblems[1]Œ[4].Inparticular,model-free
approachestoroboticRLmayallowforthecompletionof
tasksthatarediffortraditionalcontroltheoretictools
tosolve,suchaslearningpoliciesmappingobservations
directlytoactions[3].Inspiteofprevioussuccesses,RL
hasnotseenwidespreadadoptioninreal-worldsettings,
especiallyinthecontextofrobotics.Oneofthefundamental
barrierstoapplyingRLinroboticsystemsistheextremely
highsamplecomplexityassociatedwithtrainingpolicies.
Oneapproachtoovercomingsamplecomplexityistotrain
insimulationandporttothetrueenvironment,possibly
withaﬁsim-to-realﬂalgorithmtomitigatetheimpactsof
simulator-realitymismatch[4]Œ[6].Whilethisisapromising
approachthatreducesthenumberoftrialsthatarerequired
torunonaphysicalrobot,itstillrequirespossiblymillions
oftrialsonsimulatedsystems[7].Arootcauseofthis
extremelyhighsamplecomplexityistheuseofna
¨
eexplo-
rationstrategiessuchas

-greedy[8],Ornstein-Uhlenbeck
1
DepartmentofAeronauticsandAstronautics,
StanfordUniversity,Stanford,CA,USA94305.
f
borisi,apoorva,pavone
g
@stanford.edu
2
DepartmentofMechanicalEngineering,StanfordUniversity,Stanford,
CA,USA94305.
jharrison@stanford.edu
3
SchoolofComputingScience,SimonFraserUniversity,Burnaby,BC,
CanadaV5A1S6.
mochen@cs.sfu.ca
noise[9],orinthecaseofstochasticpolicygradiental-
gorithmssuchasProximalPolicyOptimization(PPO)[7],
relianceonthestochasticityofthepolicy.Thesedithering
explorationstrategiesarewell-knowntotakeexponential
timeforproblemswithsparsereward[10].Unfortunately,
inroboticlearning,smoothrewardfunctionscanleadto
undesirablebehaviorandsparserewardscorrespondingdi-
rectlytoaccomplishingataskareoftennecessary[11],[12].
Moreover,ifanothercomponentoftherewardfunctionis
acostassociatedwithcontrol,na
¨
eexplorationstrategies
mayactuallyguidethesystemawayfromthegoalregionin
anefforttominimizecontrolcosts.
Inthecontextofrobotics,explorationefyhasthe
potentialtobesubstantiallyimprovedbyleveragingphysical
priors[13].Model-basedapproachestoroboticRLhave
demonstratedexceptionalsampleefy[14],butare
oftenlimitedtogeneratinglocalmodelsandpolicies,and
arenotcapableofsolvingaswideavarietyofproblemsas
model-freemethods.Techniquestoleveragemodelstoguide
explorationoftenrequireeithersubstantialof
theunderlyingmodel-freelearningalgorithm[13],[15],or
leveragecontroltheoretictoolswhichpotentiallyprevents
applicationtomanyinterestingproblems[16],[17].
Contributions:
WeproposeBackwardReachableCurricu-
lum(BaRC),amethodofimprovingexplorationefy
inmodel-freeRLbyseamlesslyleveragingapproximate
physicalmodelswithoutalteringtheRLalgorithm.Our
approachworksbyalteringtheinitialstatedistribution
usedwhentraininganarbitrarymodel-freeRLagent.We
starttrainingfromstatesnearthesparsegoalregion,and
iterativelyexpandtheinitialstatedistributionbycomputing
approximatebackwardreachablesets(BRSs):thesetofall
pointsinthestatespacecapableofreachingacertainregion
inaed,shortamountoftime.
Byexplicitlymovingtheinitialstatedistributionback-
wardsintimefromthegoalaccordingtoapproximatedy-
namicsofthesystem,weinitiallytrainonsimpleproblems,
ensuringthelearningagentreceivesastrongrewardsignal.
Astheagentdemonstratesmastery,weincreaseproblem
difuntilarrivingattheoriginallearningproblem.Our
approachrequiresonlyapproximateBRSs,soalo
approximatemodelcanbeusedtocomputethem.During
forwardlearning,theoriginalmodelisused,so
ourmethodissimplyawrapperthataugments
any
model-
freeRLalgorithm.
OnekeyfeatureofourapproachisthatBRSsexpandinall
dynamicallyfeasibledirectionsofthestatespace,providing
aﬁfrontierﬂfromwhichstatesaresampled.Byusingprior
knowledgeofthesystemthroughapproximatedynamicsand
performingbackwardreachability,wedrasticallyimprove
sampleefywhilestilllearningamodel-freecontrol
arXiv:1806.06161v2  [cs.RO]  17 Sep 2018policy,therebymaintainingthepotentialtolearnhighquality
policieswitharbitraryinputs(suchasimagesorothersensors
inputs).Additionally,ourapproachiscapableofhandling
highlydynamicsystemsinwhichtheforwardandbackward
timedynamicsvastlydiffer,andismodularinthesensethat
anymodel-freeRLmethodandanyreachabilitymethodcan
becombined.
Wedemonstrateourapproachonacarenvironmentanda
dynamicplanarquadrotorenvironment,andobservesubstan-
tialimprovementsinlearningefyoverbothstandard
model-freeRLaswellasexistingcurriculumapproaches.We
furtherinvestigateavarietyofscenariosinwhichthereexists
mismatchbetweenthemodelusedforcurriculumgeneration
andforforwardlearning,andgoodperformanceeven
withmodelmismatch.
II.R
ELATED
W
ORK
Onegrowingbodyofworkthataimstoimproveexplo-
rationefyingeneralRLproblemshasbeenonthe
topicof
deepexploration
[10].Ditheringstrategiessuchas

-greedyaimtoexplorethestatespacebytakingsuboptimal
actionswithsomelowprobability.However,thisdoesnotac-
countfortherelativeuncertaintyofvalueestimates,andthus
anagentinanMDPwithsparserewardwillcontinuously
explorearegionaroundtheinitialstate,asopposedtodelib-
eratelysearchingthestatespace[18].Generalpurposedeep
explorationstrategiesaddressthisproblemby,forexample,
maintainingapproximatedistributionsovervaluefunctions
andsamplingatthestartofeachepisode[10],oraugmenting
theMDPrewardtowardsreducingtheagent'suncertaintyin
environmentdynamics[19].Inthecontextofrobotic
RLinsimulation,theavailabilityofphysicalmodelsandthe
abilitytoadjusttheparametersoftheRLtaskwhiletraining
shouldenableguidingexplorationinamoredirectmanner;
thisisthehigh-levelmotivationforourwork.
Ourapproachisbasedonideasfromcurriculumlearning,
whichaimstoimprovetherateoflearningbytraining
oneasierexamples,andincreasingthedifastraining
progressesuntiltheoriginaltaskbecomesappropriatetotrain
ondirectly[20].Insupervisedlearning,avarietyofhand-
designedcurriculamethodsaswellasautomaticcurriculum
generationmethodshavebeeneffectiveincertainscenarios
[21],[22].Severalauthorshavealsoappliedcurriculum
schemestoroboticsystems[23],[24];however,theseap-
proachesarebasedonlearningcontrolprimi-
tivesoridentifyinginversedynamicsmodels,asopposedto
directlysolvingagivenMDP.Acommoncurriculummethod
inRLisrewardﬁsmoothingﬂ,inwhichasparserewardsignal
isreplacedwithasmoothfunction,thusprovidingreward
signaleverywhereinthestatespace.Acommonexample
ofthisisprovidingacostthatisquadraticinthedistance
tothegoalstate.Thisapproachhasbeenshowntoyield
highlysub-optimalpoliciesinnumerousroboticcontroltasks
[11],[12].Forexample,inapeginsertiontask,[11]found
thattherobotwouldinsteadplacethepegbesidethehole,
asopposedtosuccessfullyinsertingit.Wecompareagainst
rewardsmoothinginournumericalexperiments.
Theworkof[12]offersanattractive,purelymodel-free
approachtocurriculumgenerationforroboticRL,which
adjustsproblemcomplexitybyadjustingtheinitialstate
distributionoftheRLtaskduringtraining.Theauthorsargue
thatinitialstateswhichyieldamediumsuccessrateleadto
goodlearningperformance,andthusselectnewstartstates
bysamplingrandomactionsfromthesemediumsuccess
ratestates.Thisapproachwasappliedsuccessfullyina
varietyofsystems,butcanbreakdownforhighlydynamic
orunstablesystems.Insuchsystems,thestatereachedby
takingrandomactionsfromaﬁgoodstartstateﬂislikelyto
befarfromtheoriginalstateandthusunlikelytoitselfalso
beaﬁgoodstartstate.ﬂFurther,forsystemswheredynamics
movingforwardintimeareverydifferentfromthosemoving
backwardintime,e.g.underactuatedsystemswithdrift,
thisrandomactionsamplingcurriculummaynevertrain
fromstartsthatbelongtothetrueinitialstatedistribution.
Comparedtotheworkin[12],ourmethodcanbeviewed
asageneralizationthatexplicitlyutilizespriorknowledgeof
asystem.ByexpandingBRSsoutwardfromthegoaland
generatingamoreuniformcoverageofthestatespace,our
approacheffectivelyprioritizesexplorationofthefrontierof
theBRS.
Backwardreachabilityhasbeenextensivelyusedforveri-
fyingperformanceandsafetyofsystems[25],[26].Thereare
aplethoraoftoolsforcomputingBRSsformanydifferent
classesofsystemmodels[27]Œ[29].Sincethetraditional
focusofbackwardreachabilityhasbeenonprovidingper-
formanceandsafetyguarantees,computationsofBRSsare
expensive.Recentsystemdecompositiontechniquesareef-
fectiveinalleviatingthecomputationalburdeninavarietyof
problemsetups[30]Œ[32].Aswewilldemonstrate,approxi-
mateBRSscomputedusingthesedecompositiontechniques
aresufforthepurposeofguidingpolicylearningand
donotimpacttrainingtime.
III.P
ROBLEM
F
ORMULATION
Thegoalofthisworkistoseamlesslyleveragephysi-
calpriorstoprovideamethodforgeneratingcurriculato
acceleratemodel-freeRLforsparse-rewardrobotictasksin
simulation.Inthefollowingsubsectionsweformalizethe
notionofasparse-rewardroboticRLtaskandspecifythe
physicalpriorsweleverage.
A.SparseRewardMDP
WeassumetheroboticRLtaskisasadiscretetime
MDPoftheform
(
S
;
A
;F;r;ˆ
0
;;
S
g
;
S
f
)
,inwhich
Sˆ
R
n
and
Aˆ
R
m
arethestateandactionsetsrespectively,
ˆ
0
isthedistributionofinitialstates,and
F
:
SA!S
is
thetransitionfunctionwhichmaybestochastic.
TheMDPisanhorizonprocesswithdiscount
factor

.Thereexisttwosetsofabsorbingstateswhichwe
refertoas
goalstates
,
S
g
ˆS
,and
failurestates
,
S
f
ˆS
.
Thisdistinctionisfortechnicalreasons,butisanintuitive
onethatiscommoninroboticmotionplanning[33].For
example,agoalstatemaybearoboticarmsuccessfully
placinganobjectinthecorrectlocation,whereasafailure
statemaybetherobotdroppingtheobjectoutofreach.
Thefunction
r
:
SA!
R
isarewardfunction,
composedofastrictlynegativerunningcost
1
,andthecost
orrewardassociatedwiththegoalandfailureregions.
Therunningcostimpliesthatarobotcanonlyaccumulate
negativerewardduringnormaloperation,andthusmakesit
desirabletoreachthegoalregionasopposedtocontinuously
accumulatingreward.Finally,wemakethetechnicalassump-
tionthatfortheoptimalpolicy,forall
s
2S
,trajectories
terminatinginthegoalregionhavehigherexpectedreward
thanthoseterminatinginthefailureregion.Thisissimply
sayingthatitisalwaysdesirabletotrytoreachthegoal
regionandtoavoidfailure.Therewardstructurepresented
hereinisageneralizationofthatpresentedin[12],andallows
ustoincorporatecostcomponentsthatarecommonand
desirableinroboticssuchascontroleffortpenalties.
Weassumewearetraininginsimulation,andthuswecan
settheinitialstateofeachepisodearbitrarily.Furthermore,
weassumewehaveknowledgeofthegoalregion.
B.ApproximateDynamicsModelasaPhysicalPrior
Weassumeaccesstophysicalpriorknowledgeinthe
formofaapproximatedynamicsmodelofthe
system,henceforthreferredtoasthe
curriculummodel
^
M
,
withwhichapproximateBRSscanbeefcomputedto
guidethepolicylearningprocess.Dependingonthemethod
usedtocomputeBRSs,anappropriatechoiceofcurriculum
modelcouldtaketheformofanODE,adifferenceequation,
oranMDP.Inthispaper,wechoosetousetheHamilton-
Jacobi(HJ)reachabilityformulation,andhenceassumethat
thiscurriculummodeltakestheformofacontinuous-time
ODE.InaccordancewithHJreachability,letthecurriculum
statebedenoted
^
s
2
^
Sˆ
R
^
n
,withdynamicsgivenby
_
^
s
(
t
)=
^
f
(^
s
(
t
)
;
^
a
(
t
))
;
^
a
(
t
)
2
^
A
;
(1)
wherethesimulatorstatedimension
n
isnotnecessarily
equalto
^
n
.,weainjectivemap
˚
(

):
R
n
!
R
^
n
thatmapsthesimulatormodelstatetothecur-
riculummodelstate.Ingeneral,
˚
(

)
isnonlinearandoftena
projection.Thecurriculumdynamicsshouldbechosentobe
aminimallysufrepresentationofthedynamics,andis
onlyneedforacoarseapproximationoftheBRSs.
Thecurriculumdynamics
^
f
:
^
S
^
A!
^
S
areassumedto
beuniformlycontinuous,bounded,andLipschitzcontinuous
in
^
s
fored
^
a
,sothatgivenameasureablecontrolfunction
^
a
(

)
,thereexistsauniquetrajectorysolving(1)[34].
IV.A
PPROACH
OurfullapproachissummarizedinAlgorithm1
2
.The
intuitionbehindourmethodstemsfrompedagogy,where
learnersaretaughtfoundationaltopicsthatarelaterlayered
withadvancedstudy.BRSsofferaconvenientchoiceasfoun-
dationaltopicsinthecontextofcontinuouscontroltasksŠin
ordertoreachastate
s
intime
T
,atrajectorymustpass
throughtheBRSwithtimehorizon
T
ofstate
s
,whichwe
1
Weusethetermrunningcosttodenoteacostassociatedwithanon-
terminalstate-actionpair.
2
Thecodeusedinthispaperisavailableat
https://github.com/
StanfordASL/BaRC
denoteas
R
(
T
;
s
)
.Thus,ouralgorithmbeginsbyrequiring
thelearner(anRLpolicy)tolearnhowtotransitionfrom
statesinthegoal'sBRStothegoal.Oncethelearnercan
successfullyreachthegoalfromthesestates,thecurriculum
computestheirBRSandusesthisexpandedsetasnewinitial
statesfromwhichtotrain,thusincrementallyincreasing
thedifofthetask.Thisprocessofexpandingthe
initial-statedistributioninadynamically-informedmanner
continuesiterativelyuntiltheBRSspansthestatespaceora
givenstartstateisreached.Therestofthissectiondescribes
ouralgorithmindetail.
Everystageofthecurriculumisbydetermining
thesetofinitialstatestotrainfrombycallingE
XPAND
-
B
ACKWARDS
tocomputestarts
set,theunionof
R
(
T
;^
s
)
forevery
^
s
instarts,whichinitiallyjustcontainsastatein
thegoalregion.Theinnerloopofthealgorithmrepresents
trainingapolicyonthisstageofthecurriculum.Rather
thanonlytrainingonstatesfromthisexpandedset,we
sample
N
new
statesfromthestart
setandmixin
N
old
states
sampledfromold
starts,alistofstatesfromwhichthe
policyhaspreviouslydemonstratedmasteryofthetask.In
thisway,weavoidcatastrophicforgettingduringthepolicy
training.Wetrainthepolicyusingamodel-freepolicyopti-
mizationalgorithmfor
N
TP
iterationsviatheT
RAIN
P
OLICY
subroutine.Thesubroutinealsoreturnsthesuccessrateof
thepolicyfromthedifferentinitialstatessampledduring
training.Thestartslistisupdatedtocontaininitialstates
fromwhichthepolicycanreliablyreachthegoal.Thisis
doneviatheS
ELECT
subroutine,whichreturnsinitialstates
insuccess
mapwithsuccessrategreaterthan
C
select
.These
statesarealsoaddedtotheold
startsbuffer.
ThisinnerlooprepeatsuntilthecalltoE
VALUATE
deter-
minesthatthefractionofstatesinthisstageofthecurriculum
fromwhich
ˇ
canreachthegoalexceeds
C
pass
.Atthispoint,
ˇ
isconsideredtohaveadequatelymasteredthiscurriculum
stage,andwemovetothenextcurriculumstage.
Hyperparameters:
Thealgorithmhassixhyperparam-
eters.
C
select
and
C
pass
anotionofmasteryfroma
particularstateandacurriculumstagerespectively.The
horizon
T
usedintheBRScomputationcontrolshowmuch
thetaskdifincreasesbetweenstagesofthecurriculum.
Theratioof
N
old
to
N
new
balancestrainingonnewscenarios
withpreventingforgetting.Thenumberoftrainingiterations
betweenevaluations
N
TP
shouldideallybesettoavaluesuch
thatthepolicyhasahighchanceofmasteringastageofthe
curriculumafter
N
TP
iterationsofthepolicyoptimization
algorithm,inordertoavoidunnecessaryevaluationchecks.
Empirically,thealgorithmisrobusttothesettingsofthese
hyperparameters.
ComputingBRSs:
ToactuallyobtainBRSs,weuse
methodsfromreachabilityanalysis.,weusethe
HJformulationofreachabilitysinceourapproachfocuses
oncapturingkeynonlinearbehaviorsofsystems(through
thecurriculumdynamicsmodel).TheBRSofatargetset
Fˆ
R
^
n
representsthesetofstatesofthecurriculummodel
^
s
2
R
^
n
fromwhichthecurriculumsystemcanbedriven
into
F
attheendofashorttimehorizonofduration
T
.In
ourwork,theBRSofatargetset
Fˆ
R
^
n
,whichwedenote
Algorithm1
ThefullBaRCalgorithm.
Require:
Hyperparameters
N
new
;N
old
;T;C
pass
;N
TP
;C
select
function
B
A
RC(
s
g
,
ˆ
0
,
^
M
)
ˇ
 
ˇ
0
starts
 f
s
g
g
oldstarts
 f
s
g
g
for
i
=1,2...
do
starts
set
 
E
XPAND
B
ACKWARDS
(starts,
^
M
,
T
)
frac
successful
 
0.0
while
frac
successful
<C
pass
do
ˆ
i
 
Unif(
starts
set
;N
new
)
[
Unif(
oldstarts
;N
old
)
ˇ
,success
map
 
T
RAIN
P
OLICY
(
ˆ
i
,
ˇ
,
N
TP
)
starts
 
S
ELECT
(success
map,
C
select
)
oldstarts
 
oldstarts
[
starts
frac
successful
 
E
VALUATE
(
ˆ
i
,
ˇ
)
endwhile
iter
result
 
P
ERFORMANCE
M
ETRIC
(
ˆ
0
,
ˇ
)
endfor
return
ˇ
endfunction
R
(
T
;
F
)
,isformallytobe
f
^
s
0
:
9
^
a
(

)
;
^
s
(

)
(1)
;
^
s
(

T
)=^
s
0
;
^
s
(0)
2Fg
;
whichisobtainedasthezerosublevelsetofavaluefunction
V
(

T;
^
s
)
thatisthesolutiontoanHJpartialdifferential
equation(PDE)[25],[26]:
R
(
T
;
F
)=
f
^
s
:
V
(

T;
^
s
)

0
g
.
Moredetailsaregivenintheappendix.Whenthetargetset
consistsofasinglestate
^
s
,wedenotetheBRS
R
(
T
;^
s
)
.
SolvingthePDEiscomputationallyexpensiveingeneral.
However,sincehereweareonlyusingBRSstoguidepolicy
searchratherthantheirtraditionalapplicationofverifying
systemperformanceandsafety,wecanmakeapproxima-
tions.,weutilizesystemdecompositionmethods
[30]Œ[32]withthecurriculummodelin(1)to
obtainapproximateBRSswithoutimpactingthe
overallpolicytrainingtime.Thetechniquesin[30]Œ[32]pro-
videouterapproximations,althoughanouterapproximation
isnotnecessaryforguidingpolicysearch;anyapproxima-
tionthatcaptureskeynonlinearsystembehaviorsuf
Eachsystemweexperimentonisdecomposedinto
overlappingsubsetsofoneortwocomponents,whichcan
eachbesolvedefandrunonlineinouralgorithm.
,weusetheopensourcehelperOC
3
andLevel
SetMethods
4
toolboxesinMATLAB.
SamplingfromaBRS:
Weperformrejectionsampling
overthecomponentsofthedecomposedBRSapproximation,
meaningeachcomponenthaslowdimensionality(typically
1-Dor2-D)andthusitisinexpensivetoevaluatemem-
bership.Wedetermineatightboundingboxaround
eachcomponent'sBRS(thiscanbecomputedef
fromwidelyavailablelow-levelcontourplottingmethods,
e.g.
contourc
inMATLAB).Then,weuniformlysample
pointsintheboundingboxandrejectanythatfailmem-
bershipchecks.Theinitialboundingboxcalculationenables
ustoefperformrejectionsamplingincaseswhere
thesizeoftheBRSismuchsmallerthanthestatespace.
3
Foundat
https://github.com/HJReachability/helperOC
4
Foundat
http://www.cs.ubc.ca/
Ÿ
mitchell/ToolboxLS/
Fig.1.PerformanceofthecarmodelunderBaRC,theapproachof
[12](referredtoasRandomCurriculum),andstandardPPO[7].
Upper:
Percentageofstartstatesinwhichthelearnedpolicycanreacha
goalstate.Thiscurveisequivalenttoexpectedrewardoverawideinitial
statedistribution.Meanand95%intervalswerecomputedfrom
theresultsof5differentruns.
Lower:
Averagerewardduringpolicytraining.
Notethatitis
not
desirabletohaveexpectedrewardbeapproximately1,
asthisimpliesthatthecurriculumisnotprovidingsufchallenging
problems.Instead,itisdesirabletoensuretherewardisbetween0and1
toprovidealearningsignalfortheagentduringtraining.
VisualizationsofourrejectionsamplingmethodandofBRSs
canbefoundintheappendix.
Whileamoreformaltheoreticalanalysisisoutofthe
scopeofthispaper,thetheoreticalofsampling
basedonBRSscanbegleanedfromtheofthe
BRS,whichinvolvesanexistentialforthecontrol
function
^
a
(

)
.Thisimpliesthatastatethatcanpossiblyreach
thegoalusing
any
policyisincludedintheBRS.Thisis
incontrasttomethodsthatsamplethestatespacethrough
applyingrandomcontrols.
V.E
XPERIMENTAL
R
ESULTS
Weperformnumericalexperimentsontwodynamicalsys-
tems,andevaluateavarietyofmodelmismatchscenariosto
investigatetherobustnessofBaRCtoinaccuratecurriculum
models.AllexperimentswereperformedusingthePPO
algorithmasthemodel-freepolicyoptimizationmethod.
Thehyperparametersforbothexperimentsarelistedinthe
appendix.
A.CarModel
Weuseavedimensionalcarthatisastandardtest
environmentinmotionplanning[35]andRL[5].Thecar
Fig.2.PerformanceofBaRC,randomcurriculum,andstandardPPOontheplanarquadrotortask.
Left:
Meanrewardovertenexperimentsforthe
threeapproaches(95%intervals).Notethatbetween4-8iterations,thedifferentinstantiationsofBaRClearntoreachthegoal.ThePPOpolicy
simplylearnshowtohowever,whereastherandomcurriculumneverlearnsanynon-collidingbehavior.
Center:
Theaveragerewardduringtrainingof
thepolicy.TheiterativeincreasinganddecreasingofaveragerewardfortheBaRCagentshowsthepolicylearningforagivenBRS,followedbyBaRC
expandingtheBRS.ThisalternatinglearningandexpansionisrepeateduntiltheBaRCagentreachestheinitialstateandthealgorithmachievesmaximal
rewardineveryiteration.
Right:
Themeanrewardovertenexperiments,plottedversuswallclocktime.
hasstate
s
=[
x;y;;v;
]
T
,where
x
and
y
denoteposition
intheplane,

denotesheadingangle,
v
denotesspeed,and

denotestrajectorycurvature.Thesystemtakesasinput
a
=[
a
v
;a

]
T
,where
a
v
islongitudinalaccelerationand
a

isthederivativeofthecurvature.Therewardfunction
isanindicatorvariableforasmallregionaroundthegoal
state.Thegoalstateisapointattheoriginwithvelocity
sampleduniformlyfrom0.1to1.0m/sandheadingsampled
uniformlyfrom

ˇ
to
ˇ
radians.Weconsiderstatesthat
arewithin0.1mindistance,0.1m/sinvelocity,and0.1
radiansofthegoalstatetohavereachedthegoal.Theagent
receivesarewardof1.0uponreachingthegoal,and0.0
otherwise.Thesimulatormodelwasusedasacurriculum
modelfortheexperimentspresentedinthebodyofthepaper.
Theappendixcontainsacollectionofresultsforcaseswhere
thereexistsamismatchbetweenthesimulatormodelandthe
curriculummodel,andthesesystemsperformedsimilarlyto
thecasespresentedhere.
TotestcoverageofthestatespaceviatheBaRCalgorithm,
weinitializedasetof100pointsﬁbehindﬂthegoalstate.
Here,ﬁbehindﬂmeansthatanysampledpoint'sprojection
ontothegoalstate'svelocityvectorisnegative.Further,
themaximumangleallowedbetweenanysampledstateand
thegoalstate'snegativevelocityvectoris
ˇ=
4
.Finally,we
orientthesampledstatesrandomlywithin
ˇ=
4
radiansofthe
goalvector'sorientation.Eachofthesampledstateshaszero
initialvelocityandcurvature.WethenranBaRC,evaluating
ateachiterationthenumberoforiginalsampledstatesfrom
whichthelearnedpolicycouldtraversetothegoal.Wechose
thissetasitisrepresentativeofstatesfromwhichapolicy
couldrealisticallytraversetoagoalwithnonzerovelocity.
Figure1showstheperformanceofBaRC,aswellasthe
curriculummethodpresentedin[12](whichwerefertoas
RandomCurriculum
)andstandardPPO[7].Thetopplot
showsthenumberofstartstatesfromwhichthealgorithm
successfullyreachesthegoalstate.Whenthesuccessrate
exceeded95%,thetaskwasconsideredsolvedandtraining
wasterminated.BaRCachievesrapidcoverageofthestate
spaceandasuccessrateofgreaterthan95%forallve
experimentsinunderthirtyiterationsoftheouterloop.In
contrast,PPOobtainsalmostnoreward.Therandomcurricu-
lumapproachsuccessfullyconnectsforbetweentwentyand
fortypercentofstartstatesandshowslittleimprovement
beyondthe40iterations.Onthebottom,theaverage
rewardduringtrainingisplotted.Theserewardreturnsare
fortrainingfromcurriculum-selectedstartstatesasopposed
tothetrueinitialstatedistributionoftheproblem.Asaresult,
counter-intuitively,itisdesirabletohaveneitheranaverage
rewardclosetozero(implyinganineffectiveexploration
scheme),noranaveragerewardclosetoone(implyingthe
curriculumisexpandingtooslowly).Theaveragerewardfor
BaRCvariedbetweenapproximately0.4and0.9,demon-
stratingthatthealgorithmcreatesawell-pacedcurriculum
thatensuresgood,continuouslearningprogress.
B.PlanarQuadrotorModel
ToevaluatetheperformanceofBaRConahighlydy-
namic,unstablesystem,wetestedthealgorithmonaplanar
simulationofaquadrotorinclutter.Thisplanar
quadrotorisastandardtestprobleminthecontrolliterature
[36],[37].Thissystemhasstate
s
=[
x;v
x
;y;v
y
;˚;!
]
,
where
x;y;˚
denotetheplanarcoordinatesandroll,and
v
x
;v
y
;!
denotetheirtimederivatives.Theactionsavailable
totheagentarethetworotorthrusts,
T
1
and
T
2
.The
learningagentreceivesanobservationthatisaugmentedwith
8lasersensors(seeFigure3foravisualization).
Thesesensorsareplacedatevery45degreesandprovide
thedistancefromthevehicletothenearestobstacle.This
observationallowsthelearningagenttomapdirectlyfrom
sensorinputstoactions.
Inthisproblem,theagentreceivesarewardof1000for
reachingthegoalregionas
S
g
=
f
s
:
x

4
;y

4
g
)andaccumulatesacontrolcostof
0
:
01(
T
2
1
+
T
2
2
)
per
timestep.Theepisoderunsforatmost200timesteps,and
themaximumthrustissettobetwicetheweightofthe
quadrotor.Iftheagentcollideswithanobstacleorthewalls
itreceivesacostequaltothemaximumcontrolcostoverthe
entireepisode.Assuch,itisalwaysdesirablefortheagent
toavoidcollisions.Becauseoftheextremelysparsereward
associatedwithreachingthegoal,theunstabledynamics,and
absorbingcollisionstates,thisproblemisextremelydif
tosolveforstandardexplorationschemes.
Figure2showstheperformanceofBaRCaswellasthe
randomcurriculumandstandardPPO.PPO,asexpected,
Fig.3.
Left:
VisualizationofatrajectoryfromaBaRC-trainedpolicy.Thepolicywastrainedfor10BaRCiterations.Theredlinesdenotelaser
measurementswhichwereincludedintheobservation.Theyareexcludedfromtheothertwoplotsforclarity.Magentaindicatesthegoalregionandgrey
indicatesobstacles.
CenterLeft:
Atrajectoryfromapolicytrainedusingtherandomcurriculumapproachfrom[12].Itwastrainedfor10curriculum
iterations.
CenterRight:
AtrajectoryfromapolicytrainedusingstandardPPO[7].Itwastrainedfor200PPOiterations(equivalentto10outerloops
of20PPOiterations).
Right:
AtrajectoryfromapolicytrainedusingstandardPPO,withaquadratizedrewardfunction.
neverreachesthegoalandthuscannotlearntodeliberately
movetowardthegoalregion.Instead,itlearnsalocally
optimalpolicywhichhoversinplacetoavoidcollision.
WealsocomparetostandardPPOwithasmoothedreward
function.Inthisversionoftheproblem,therewardfunction
isaugmentedwithaquadraticcostterm,penalizingdistance
fromthegoal.Asaresult,thelearnedpolicyisahighlysub-
optimallocalminimum,inwhichtheagenthoversslightly
closertotheobstacleaboveit,andtotheright.Itdoesnot,
however,learnapolicythatallowsittoreachthegoal.
Therandomcurriculum,becauseittakesrandomac-
tionsforwardandbackward,rapidlybecomesunstableand
achievesonlyinfrequentrewards.Itdoesnotadvancefar
enoughinthestatespacetoproduceanydeliberategoal-
seekingbehaviorfromthetruestartstateoftheproblem.
Incontrast,theBaRCagentlearnstomovetothegoal
everytimewithin4-8iterationsofthecurriculum.Ascan
beseenintheaveragerewardplot(Figure2,center),there
arediscretewavesofpolicyimprovement(averagereward
increase)followedbyiterativeexpansionoftheBRS(visible
bytheassociatedsharprewarddecrease).Thiscontinuesuntil
thecurriculumreachestheinitialstate.Itthenrapidlytrains
fromtheinitialstate,quicklyimprovingtobeabletoconsis-
tentlysolvetheproblem.Sampletrajectoriesarevisualized
inFigure3.Becausewedonotenforceahoverconditionin
thegoalregion,theBaRCpolicylearnsextremelyaggressive
trajectoriestowardthegoal.
SincePPOexplorationstepsareinterleavedwithexpan-
sionoftheBRS,theaveragetimeperiterationisslowerthan
standardPPOortherandomcurriculum.Theperformance
versuswallclocktimeforboththecarsystemandtheplanar
quadrotorispresentedinFigure2(right).Eachiteration
ofBaRCtakesroughly2-3timesaslongasstandardPPO
iterations.Evenconsideringperformanceversusclocktime,
BaRCdemonstratesdramaticperformanceimprovementver-
suspreviousapproaches.Moreover,oncetheBRSexpands
toincludetheinitialstate,expansionoftheBRSmaybe
haltedandtrainingcancontinuewithoutanycurriculum
computationoverhead.
VI.D
ISCUSSIONAND
C
ONCLUSIONS
Inthispaper,weproposedbackwardreachabilitycur-
riculum(BaRC),whichaddressesthechallengeofsample
complexityinmodel-freereinforcementlearning(RL)by
usingbackwardreachablesets(BRSs)toprovidecurricula
forthepolicylearningprocess.Inourapproach,weinitially
trainthepolicystartingclosetothegoalregion,beforeiter-
ativelycomputingBRSsusingasimplidynamicmodel
totrainthepolicyfrommoredifinitialstates.BaRCis
aneffectivewaytoleveragephysicalpriorstodramatically
increasetherateoftrainingofmodel-freealgorithmswithout
affectingtheirxibilityintermsofapplicabilitytoalarge
varietyofRLproblems,andenablesthesolutionofsparse
rewardroboticproblemsthatwerepreviouslyintractablewith
standardRLexplorationschemes.Thehyperparametersof
BaRC,suchasthetimehorizonofBRSexpansionsand
themasterythreshold,areintuitivetoasystemdesignerand
maybeeasilyadjusted.Inournumericalexamples,since
weusedcontinuoustimeBRSmethods,thetimehorizonof
BRSexpansionmaybechosentobeanypositivenumber,
asopposedtobeinglimitedtothesametimediscretization
ofthesimulator.DiscretetimeBRSmethodscanalsobe
usedtoasimilareffect,sincethetimediscretizationof
thecurriculummodelneednotbethesameasthatofthe
simulatormodel.
Whilewehaveseenstrongexplorationefygains
fromdirectlycomputingBRSs,itisunclearifourapproach
couldberelaxedtoasamplingbasedscheme.Ourapproach
mimicsdynamicprogramming,tsolvingthecontrolsub-
problemclosetothegoalregion,anditerativelyincreasing
theproblemcomplexitywhilerelyingonthesolutionstothe
previouslysolvedsubproblems.Samplingbasedanalogues
todynamicprogrammingexistin,forexample,motionplan-
ning[38],[39].Thesesamplingbasedalgorithms,usingthe
curriculumdynamics,maybeapromisingwaytoincrease
theefyofapproximatingBRSs.
Thereareseveraladditionalavenuesoffuturework.First,
incorporatingbothforwardandbackwardreachabilityto
speeduppolicytrainingwhenaparticularinitialstate
distributionisofinteresthastopotentialtoimprovetraining
efy.Investigatingpotentialperformanceof
tuningtheBRStimehorizonandquantifyingthesample
complexityofusingBRSsversussimulatingtrajec-
toriespotentiallywithrespecttothequalityofthecurriculum
modelareimportantcharacterizationsofBaRC.Finally,
performingefsystem[40]Œ[42]during
portionsofthetrainingprocessthatoccuronthetrueto
obtainacurriculummodelinsituationswhenanapproximate
modelisnotknownapriorimaybeinvestigatedtoincrease
thesetofproblemsBaRCmaybeappliedto.
A
CKNOWLEDGMENT
TheauthorswerepartiallysupportedbytheOfof
NavalResearch,ONRYIPProgram,underContractN00014-
17-1-2433andtheToyotaResearchInstitute(ﬁTRIﬂ).James
HarrisonwassupportedinpartbytheStanfordGraduateFel-
lowshipandtheNationalSciencesandEngineeringResearch
Council(NSERC).Thisarticlesolelytheopinions
andconclusionsofitsauthorsandnotONR,TRIorany
otherToyotaentity.
R
EFERENCES
[1]
D.Silver,A.Huang,C.J.Maddison,A.Guez,L.Sifre,G.Van
DenDriessche,J.Schrittwieser,I.Antonoglou,V.Panneershelvam,
M.Lanctot
etal.
,ﬁMasteringthegameofgowithdeepneural
networksandtreesearch,ﬂ
Nature
,2016.
[2]
V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,
D.Wierstra,andM.Riedmiller,ﬁPlayingatariwithdeepreinforcement
learning,ﬂ
NeuralInformationProcessingSystems(NIPS)
,2013.
[3]
S.Levine,C.Finn,T.Darrell,andP.Abbeel,ﬁEnd-to-endtraining
ofdeepvisuomotorpolicies,ﬂ
JournalofMachineLearningResearch
,
2016.
[4]
J.Tan,T.Zhang,E.Coumans,A.Iscen,Y.Bai,D.Hafner,S.Bo-
hez,andV.Vanhoucke,ﬁSim-to-real:Learningagilelocomotionfor
quadrupedrobots,ﬂ
Robotics:ScienceandSystems(RSS)
,2018.
[5]
J.Harrison,A.Garg,B.Ivanovic,Y.Zhu,S.Savarese,L.Fei-Fei,and
M.Pavone,ﬁAdaPT:Zero-shotadaptivepolicytransferforstochastic
dynamicalsystems,ﬂ
InternationalSymposiumonRoboticsResearch
(ISRR)
,2017.
[6]
J.Tobin,R.Fong,A.Ray,J.Schneider,W.Zaremba,andP.Abbeel,
ﬁDomainrandomizationfortransferringdeepneuralnetworksfrom
simulationtotherealworld,ﬂ
IEEEInternationalConferenceon
IntelligentRobotsandSystems(IROS)
,2017.
[7]
J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov,
ﬁProximalpolicyoptimizationalgorithms,ﬂ
arXiv:1707.06347
,2017.
[8]
V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.
Bellemare,A.Graves,M.Riedmiller,A.K.Fidjeland,G.Ostrovski
etal.
,ﬁHuman-levelcontrolthroughdeepreinforcementlearning,ﬂ
Nature
,2015.
[9]
T.P.Lillicrap,J.J.Hunt,A.Pritzel,N.Heess,T.Erez,Y.Tassa,
D.Silver,andD.Wierstra,ﬁContinuouscontrolwithdeepreinforce-
mentlearning,ﬂ
InternationalConferenceonLearningRepresentations
(ICLR)
,2016.
[10]
I.Osband,C.Blundell,A.Pritzel,andB.VanRoy,ﬁDeepexplo-
rationviabootstrappedDQN,ﬂ
NeuralInformationProcessingSystems
(NIPS)
,2016.
[11]
G.Thomas,M.Chien,A.Tamar,J.A.Ojea,andP.Abbeel,ﬁLearning
roboticassemblyfromCAD,ﬂ
IEEEInternationalConferenceon
RoboticsandAutomation(ICRA)
,2018.
[12]
C.Florensa,D.Held,M.Wulfmeier,M.Zhang,andP.Abbeel,ﬁRe-
versecurriculumgenerationforreinforcementlearning,ﬂ
Conference
onRobotLearning(CoRL)
,2017.
[13]
S.Bansal,R.Calandra,S.Levine,andC.Tomlin,ﬁMBMF:Model-
basedpriorsformodel-freereinforcementlearning,ﬂ
Conferenceon
RobotLearning(CoRL)
,2017.
[14]
M.DeisenrothandC.E.Rasmussen,ﬁPILCO:Amodel-basedand
data-efapproachtopolicysearch,ﬂ
InternationalConferenceon
MachineLearning(ICML)
,2011.
[15]
Y.Chebotar,K.Hausman,M.Zhang,G.Sukhatme,S.Schaal,and
S.Levine,ﬁCombiningmodel-basedandmodel-freeupdatesfor
trajectory-centricreinforcementlearning,ﬂ
InternationalConferenceon
MachineLearning(ICML)
,2017.
[16]
S.LevineandP.Abbeel,ﬁLearningneuralnetworkpolicieswith
guidedpolicysearchunderunknowndynamics,ﬂ
NeuralInformation
ProcessingSystems(NIPS)
,2014.
[17]
S.Gu,T.Lillicrap,I.Sutskever,andS.Levine,ﬁContinuousdeepQ-
learningwithmodel-basedacceleration,ﬂ
InternationalConferenceon
MachineLearning(ICML)
,2016.
[18]
I.Osband,D.Russo,Z.Wen,andB.VanRoy,ﬁDeepexplorationvia
randomizedvaluefunctions,ﬂ
arXiv:1703.07608
,2017.
[19]
R.Houthooft,X.Chen,Y.Duan,J.Schulman,F.DeTurck,and
P.Abbeel,ﬁVime:Variationalinformationmaximizingexploration,ﬂ
NeuralInformationProcessingSystems(NIPS)
,2016.
[20]
Y.Bengio,J.Louradour,R.Collobert,andJ.Weston,ﬁCurriculum
learning,ﬂ
InternationalConferenceonMachineLearning(ICML)
,
2009.
[21]
A.Graves,M.G.Bellemare,J.Menick,R.Munos,and
K.Kavukcuoglu,ﬁAutomatedcurriculumlearningforneuralnet-
works,ﬂ
InternationalConferenceonMachineLearning(ICML)
,2017.
[22]
W.ZarembaandI.Sutskever,ﬁLearningtoexecute,ﬂ
arXiv:1410.4615
,
2014.
[23]
A.KarpathyandM.VanDePanne,ﬁCurriculumlearningformotor
skills,ﬂ
CanadianConferenceonIntelligence
,2012.
[24]
A.BaranesandP.-Y.Oudeyer,ﬁActivelearningofinversemodels
withintrinsicallymotivatedgoalexplorationinrobots,ﬂ
Roboticsand
AutonomousSystems
,2013.
[25]
S.Bansal,M.Chen,S.Herbert,andC.J.Tomlin,ﬁHamilton-Jacobi
reachability:Abriefoverviewandrecentadvances,ﬂ
IEEEConference
onDecisionandControl(CDC)
,2017.
[26]
M.ChenandC.J.Tomlin,ﬁHamilton-JacobiReachability:Some
RecentTheoreticalAdvancesandApplicationsinUnmannedAirspace
Management,ﬂ
AnnualReviewofControl,Robotics,andAutonomous
Systems
,2018.
[27]
I.M.Mitchell,ﬁTheFlexible,ExtensibleandEfToolboxof
LevelSetMethods,ﬂ
J.Computing
,2008.
[28]
M.Althoff,ﬁAnIntroductiontoCORA2015,ﬂ
Proc.Workshopon
AppliedVforContinuousandHybridSystems
,2015.
[29]
C.Fan,B.Qi,S.Mitra,M.Viswanathan,andP.S.Duggirala,
ﬁAutomaticReachabilityAnalysisforNonlinearHybridModelswith
C2E2,ﬂ
ComputerAidedV
,2016.
[30]
I.M.MitchellandC.J.Tomlin,ﬁOverapproximatingReachableSets
byHamilton-JacobiProjections,ﬂ
J.Computing
,2003.
[31]
M.Chen,S.Herbert,andC.J.Tomlin,ﬁFastreachablesetapproxi-
mationsviastatedecouplingdisturbances,ﬂ
IEEEConf.Decisionand
Control
,2016.
[32]
M.Chen,S.L.Herbert,M.Vashishtha,S.Bansal,andC.J.Tomlin,
ﬁDecompositionofReachableSetsandTubesforaClassofNonlinear
Systems,ﬂ
IEEETransactionsonAutomaticControl
,2018.
[33]
S.M.LaValle,
Planningalgorithms
.Cambridgeuniversitypress,
2006.
[34]
E.A.CoddingtonandN.Levinson,
TheoryofOrdinaryDifferential
Equations
.McGraw-HillInc.,1955.
[35]
D.J.WebbandJ.vandenBerg,ﬁKinodynamicRRT*:Asymptotically
optimalmotionplanningforrobotswithlineardynamics,ﬂ
IEEE
InternationalConferenceonRoboticsandAutomation(ICRA)
,2013.
[36]
J.H.Gillula,H.Huang,M.P.Vitus,andC.J.Tomlin,ﬁDesignof
guaranteedsafemaneuversusingreachablesets:Autonomousquadro-
toraerobaticsintheoryandpractice,ﬂ
IEEEInternationalConference
onRoboticsandAutomation(ICRA)
,2010.
[37]
S.Singh,A.Majumdar,J.-J.Slotine,andM.Pavone,ﬁRobustonline
motionplanningviacontractiontheoryandconvexoptimization,ﬂ
IEEEInternationalConferenceonRoboticsandAutomation(ICRA)
,
2017.
[38]
L.Janson,E.Schmerling,A.Clark,andM.Pavone,ﬁFastmarching
tree:Afastmarchingsampling-basedmethodforoptimalmotion
planninginmanydimensions,ﬂ
InternationalJournalofRobotics
Research
,2015.
[39]
B.Ichter,E.Schmerling,andM.Pavone,ﬁGroupmarchingtree:
Sampling-basedapproximatelyoptimalmotionplanningongpus,ﬂ
IEEEInternationalConferenceonRoboticComputing(IRC)
,2017.
[40]
L.Ljung,
System
.Springer,1998.
[41]
S.LevineandV.Koltun,ﬁGuidedpolicysearch,ﬂ
InternationalCon-
ferenceonMachineLearning(ICML)
,2013.
[42]
J.Harrison,A.Sharma,andM.Pavone,ﬁMeta-learningpriorsfor
efonlinebayesianregression,ﬂ
WorkshopontheAlgorithmic
FoundationsofRobotics(WAFR)
,2018.
[43]
M.G.CrandallandP.-l.Lions,ﬁViscositysolutionsofHamilton-
Jacobiequations,ﬂ
Trans.AmericanMathematicalSociety
,1983.
A
PPENDIX
A.Computingbackwardreachablesets(BRSs)
Intuitively,aBRSrepresentsthesetofstatesofthe
curriculummodel
^
s
2
R
^
n
fromwhichthecurriculumsystem
canbedrivenintoatargetset
Fˆ
R
^
n
attheendofatime
horizonofduration
T
.
Inthispaper,wefocusontheﬁMaximalBRS
5
ﬂ;inthis
casethesystemseekstoenter
F
usingsomecontrolfunction,
andtheMaximalBRS(referredtoassimplyﬁBRSﬂinthis
paper)representsthesetofstatesfromwhichthesystemis
guaranteedtoreach
F
.
1:
BackwardReachableSet(BRS)
.
R
(
T
;
F
)=
f
^
s
0
:
9
^
a
(

)
;
^
s
(

)
(1)
;
^
s
(

T
)=^
s
0
;
^
s
(0)
2Fg
HJformulationsoutlinedinthereviewpapers[25],[26]
castthereachabilityproblemasanoptimalcontrolproblem
anddirectlycomputeBRSsinthefullstatespaceofthe
system.Thenumericalmethodssuchasthoseimplemented
in[27]forobtainingtheoptimalsolutionallinvolvesolving
anHJPDEonagridthatrepresentsadiscretizationofthe
statespace.
Letthetargetset
Fˆ
R
^
n
berepresentedbytheimplicit
surfacefunction
V
0
(
s
)
as
F
=
f
^
s
:
V
0
(^
s
)

0
g
.Consider
theoptimizationproblem
V
R
(
t;s
)=inf
^
a
(

)
V
0
(^
s
(0))
subjectto(1)
(2)
Thevaluefunction
V
R
(

T;
^
s
)
isgivenbytheimplicit
surfacefunctionrepresentingthemaximalBRS:
R
(
T
;
F
)=
f
s
:
V
R
(

T;s
)

0
g
,andistheviscositysolution[43]ofthe
followingHJPDE,derivedthroughdynamicprogramming:
@V
(
t;
^
s
)
@t
+
H
(^
s;
r
V
(
t;
^
s
))=0
;t
2
[

T;
0]
;
(3)
V
(0
;
^
s
)=
V
0
(^
s
)
;
(4)
wheretheHamiltonian
H
(
s;
)
isgivenby
H
(^
s;
)=min
^
a
2
^
A

(^
s;
^
a
)
:
(5)
B.Systemdecomposition
SolvingtheHJPDE(3)iscomputationallyexpensive;
BRSsforsystemswithmorethan5statedimensionsare
computationallyintractableusingtheHJformulation.In
ordertotakeadvantageoftheHJformulation'sabilityto
computeBRSsfornonlinearsystemswithout
impactingtheoverallpolicytrainingtime,wewillutilize
systemdecompositionmethodstoobtainapproximateBRSs.
Inparticular,in[32],theauthorsproposeddecomposing
systemsintoﬁself-containedsubsystemsﬂ,andin[30]and
[31],theauthorsproposedwaystocomputeprojections
ofBRSs.Wewillcombinethesemethodstoobtainover-
approximationsofBRSsforthecurriculummodel.
detailsofeachcurriculummodelwillbedescribedinSection
B.
5
Thetermﬁmaximalﬂreferstotheroleoftheoptimalcontrol,whichis
tryingtomaximizethesizeoftheBRS.
ItisimportanttonotethatinthispaperBRSsareusedto
guidepolicylearning,andaswewilldemonstrate,approxi-
mateBRSscomputedwiththecurriculummodel
in(1)aresufforthepurposesofthispaper.
C.CarModel
Forthecarenvironment,weusedthefollowingve-state
simulatormodel:
_
s
=
2
6
6
6
6
4
_
x
_
y
_

_
v
_

3
7
7
7
7
5
=
2
6
6
6
4
(
v
+
d
v
)cos

(
v
+
d
v
)sin

!
a
v
+
d
v

ctrl
d
a

(
a

+
d


ctrl
)
3
7
7
7
5
;
j
a
v
j

a
v
;
j
a

j

a

;
(6)
wheretheposition
(
x;y
)
,heading

,speed
v
,andangular
speed

arethecar'sstates.Thecontrolvariablesarethe
linearacceleration
a
v
andangularacceleration
a

.The
variables
d
v
;d
a
v
;d
a

representnoiseinthespeed,linear
acceleration,andangularacceleration,andarestochastic.We
assumethatthelearningagenthasfullaccesstothecarstate.
Forthecurriculummodel,weusedthesamestatevariable
anddynamics,butwithoutthedisturbancevariables:
_
^
s
=
2
6
6
6
6
6
4
_
^
x
_
^
y
_
^

_
^
v
_
^

3
7
7
7
7
7
5
=
2
6
6
6
6
4
^
v
cos
^

^
v
sin
^

^

^
a
v
^
a

3
7
7
7
7
5
;
j
^
a
v
j
^

a
v
;
j
^
a

j
^

a

:
(7)
InordertoensurethattheapproximateBRScomputation
canbedonequickly,wedecomposed(7)asfollows:
""
_
^
x
_
^

#
=

^
v
cos
^

^


;

^



v

^
v


v
(8a)
""
_
^
y
_
^

#
=

^
v
sin
^

^


;

^



v

^
v


v
(8b)
_
^
v
=^
a
v
;
j
^
a
v
j
^

a
v
;
(8c)
_
^

=^
a

;
j
^
a

j
^

a

(8d)
Thedecompositionoccursintwosteps.First,the5Dcur-
riculumdynamicsaredecomposedintotwo4Dsubsystems
withstates
(^
x;
^
;
^
v;
^

)
and
(^
y;
^
;
^
v;
^

)
.Thisresultsinanover-
approximationoftheBRS[32].Next,
^
v
and
^

areseparated
from
(^
x;
^

)
and
(^
y;
^

)
,resultinginthesubsystemsin(8).In
(8a)and(8b),
^
v
and
^

becomecontrolvariables
thataredependentonthesubsystemBRSsin(8c)and(8d),
whichresultsinover-approximationoftheBRSaccording
to[30],[31].
HyperparameterSettings:
Inallexperimentsinvolving
thecarenvironment,wechose
N
new
=200
;N
old
=100
;T
=0
:
1
s;
C
pass
=
C
select
=0
:
5
;N
TP
=20
Thesevalueswerechosenwithalmostnotuningeffort,as
BaRCisrobusttohyperparamterchoice.
D.PlanarQuadrotorModel
Fortheplanarquadrotorenvironment,weusedthesame
dynamicsforthesimulatorandcurriculummodels,givenby
2
6
6
6
6
6
6
4
_
x
_
v
x
_
y
_
v
y
_
˚
_
!
3
7
7
7
7
7
7
5
=
2
6
6
6
6
6
6
4
v
x

1
m
C
v
D
v
x

T
1
m
sin
˚

T
2
m
sin
˚
v
y

1
m
(
mg
+
C
v
D
v
y
)+
T
1
m
cos
˚
+
T
2
m
cos
˚
!

1
I
yy
C
˚
D
!

l
I
yy
T
1
+
l
I
yy
T
2
3
7
7
7
7
7
7
5
(9a)
with
T

T
1
;T
2


T
,andwherethestateisgivenbythe
positionintheverticalplane
(
x;y
)
,velocity
(
v
x
;v
y
)
,pitch
˚
,
andpitchrate
!
.Thecontrolvariablesarethethrusts
T
1
;T
2
.
Inaddition,thequadrotorhasmass
m
,momentofinertia
I
y
y
,half-length
l
.Furthermore,
g
denotestheacceleration
duetogravity,
C
v
D
thetranslationaldragcoefand
C
˚
D
therotationaldragcoef
Thequadrotorreceivedanobservationcontainingitsfull
state,aswellaseightsensormeasurementsfromlaser
Thesesensorswerelocatedevery45degrees
onthebodyofthequad,andreturneddistancetothenearest
obstacle.Thesesensorswereedtothebodyframeofthe
quadrotor.Thetaskofincorporatingthesesensorsoutputs
toavoidobstacleswouldrequireasubstantialeffortusing
traditionalcontrol,estimation,andmappingtools.
ForefcomputationofapproximateBRSs,wede-
composedthequadrotormodelasfollows:

_
v
x
_
˚

=


1
m
C
v
D
v
x

T
1
m
sin
˚

T
2
m
sin
˚
!

;
(10a)

_
v
y
_
˚

=


1
m
(
mg
+
C
v
D
v
y
)+
T
1
m
cos
˚
+
T
2
m
cos
˚
!

;
(10b)
_
x
=
v
x
;
(10c)
_
y
=
v
y
;
(10d)
_
!
=

1
I
yy
C
˚
D
!

l
I
yy
T
1
+
l
I
yy
T
2
(10e)
with
T

T
1
;T
2


T;!

!


!
,
v
x

v
x


v
x
,
v
y

v
y


v
y
,andwhere
!
isacontrolin(10a)and
(10b),and
v
x
and
v
y
arecontrolsin(10c)and(10d)
respectively.
HyperparameterSettings:
Inallexperimentsinvolving
thequadrotorenvironment,thehyperparametersweresetto
exactlymatchtheparametersusedpreviously,forthe5-D
carenvironment.
E.CarModel
WallClockRuntime:
Inadditiontotheanalysesper-
formedinthemainbodyofthepaper,wealsocomparedthe
wallclockruntimeofBaRCtothatofarandomcurriculum
andstandardPPO.Thisexperimentwasrunonservers
withocta-coreCPUs,buteachindividualrunwaslimited
torunninginonethread.Figure4illustratesourresults.
Wecanseethatouralgorithmiscompetitiveinruntime,
Fig.4.Percentageofsuccessfulstartsforthecarmodel,plottedversus
wallclocktime.
evenwiththeadditionalcostofcomputingBRSsateach
iteration.Inthisenvironment,thelongestiterationsofBaRC
takes2-3timesaslongasstandardPPOiterations.However,
thismultiplerapidlyshrinksasthepolicylearnstoreach
thegoalquicklyandlearnstogeneralizeacrossthestate
space.OncetheBRShasreachedtheinitialstate,theBRS
expansioncanbeterminatedandeachiterationofBaRCruns
forapproximatelythesameamountoftimeasstandardPPO.
RobustnesstoModel-SystemMismatch:
Todetermine
howaccuratetheapproximatedynamicsmodelmustbe
(andthereforehowrobustourapproachistomodelerror),
weevaluatedBaRC'sperformanceinthepresenceofthree
commonmodelmismatchscenarios:additivevelocitynoise,
nonzero-meanadditivecontrolnoise,andoversteer.
Toaddvelocitynoise,wesampleadisturbancevalue
d
v
fromastandardGaussiandistribution,addthistothe
currentstate'svelocity,numericallyintegratethedynamics
equationstoproducethenextstate,andthenremovethe
addedvelocityfromtheresultingstate.Thus,theadded
velocitynoiseaffectsthepropagationofthesystem,butis
notobservedbythepolicy.Thisvelocitynoiseisroughly
equivalenttowheelsliporincreasedtractionduringa
timestepofthediscretedynamics.Fornonzero-meanadditive
controlnoise,wesamplethedisturbancevalues
d
v

ctrl
and
d


ctrl
independentlyfromaGaussiancenteredat0.3with
standarddeviation0.2,whicharethenaddedtothecontrol
inputs.Finally,toproduceoversteerweset
d
a

=1
:
5
whichmultipliesthecommandedrateofcurvature.Figure5
showstheperformanceofBaRC,arandomcurriculum,and
standardPPOoneachofthesemodelmismatchscenarios.As
canbeseen,ourapproachoutperformstheothersinallcases.
Interestingly,thesemodelmismatchesgenerallyimproved
theperformanceoftherandomcurriculummethod.
F.PlanarQuadrotorModel
BRSVisualization:
Whilenotstrictlyanexperiment,
fortheofthereaderweshowexampleBRSsforthe
planarquadrotorsysteminFigure6toillustratehowitis
sampledfromandwhatitsevolutionperiterationlookslike.
Fig.5.Corruptedcarmodelresults.
Left:
TheperformanceofBaRC,randomcurricula,andPPOaloneonasystemwithadditivenonzero-meanGaussian
noisetothecontrolinput.
Middle:
Theperformanceofthethreealgorithmsonasystemwithoversteer(steeringcontrolinputsaremultipliedbyaconstant
factorgreaterthan1).
Right:
Thealgorithms'performanceonasystemwithadditiveGaussianlongitudinalvelocitynoise.
Fig.6.AnexampleBRSforourplanarquadsystemandhowitevolvesovertimeasmorepointsareaddedtothestartsvariable.Plotsadvanceforward
fromtop-lefttobottom-right.Greendotsshowtrainingstartstatesobtainedviarejectionsampling.ThethreeBRSsshowninredcorrespondtotime
horizonsof
T
=
f
0
:
05
;
0
:
10
;
0
:
15
g
s,respectively,fromtheinnermost(closesttothetargetset)totheoutermostcontour.
"
50,Object Level Visual Reasoning in Videos,http://arxiv.org/pdf/1806.06157v3.pdf,https://github.com/fabienbaradel/object_level_visual_reasoning,"ObjectLevelVisualReasoninginVideos
FabienBaradel
1
,NataliaNeverova
2
,ChristianWolf
1
;
3
,
JulienMille
4
,andGregMori
5
1
UniversiteLyon,INSALyon,CNRS,LIRIS,F-69621,Villeurbanne,France,
firstname.lastname@liris.cnrs.fr
2
FacebookAIResearch,Paris,France,
nneverova@fb.com
3
INRIA,CITILaboratory,Villeurbanne,France
4
Laboratoired'Informatiquedel'Univ.deTours,INSACentreValdeLoire,
41034,Blois,France,
julien.mille@insa-cvl.fr
5
SimonFraserUniversity,Vancouver,Canada,
mori@cs.sfu.ca
https://fabienbaradel.github.io/eccv18_object_level_visual_reasoning/
Abstract.
Humanactivityrecognitionistypicallyaddressedbydetect-
ingkeyconceptslikeglobalandlocalmotion,featuresrelatedtoobject
classespresentinthescene,aswellasfeaturesrelatedtotheglobalcon-
text.Thenextopenchallengesinactivityrecognitionrequirealevelof
understandingthatpushesbeyondthisandcallformodelswithcapa-
bilitiesfordistinctionanddetailedcomprehensionofinteractions
betweenactorsandobjectsinascene.Weproposeamodelcapableof
learningtoreasonaboutsemanticallymeaningfulspatio-temporalinter-
actionsinvideos.Thekeytoourapproachisachoiceofperforming
thisreasoningattheobjectlevelthroughtheintegrationofstateofthe
artobjectdetectionnetworks.Thisallowsthemodeltolearndetailed
spatialinteractionsthatexistatasemantic,object-interactionrelevant
level.Weevaluateourmethodonthreestandarddatasets(Twenty-BN
Something-Something,VLOGandEPICKitchens)andachievestateof
theartresultsonallofthem.Finally,weshowvisualizationsofthein-
teractionslearnedbythemodel,whichillustrateobjectclassesandtheir
interactionscorrespondingtotactivityclasses.
Keywords:
Videounderstanding

Human-objectinteraction
1Introduction
Theofvideounderstandingisextremelydiverse,rangingfromextract-
inghighlydetailedinformationcapturedbyspdesignedmotioncap-
turesystems[33]tomakinggeneralsenseofvideosfromtheWeb[1].Asin
thedomainofimagerecognition,thereexistanumberoflarge-scalevideo
datasets[6,26,13,12,23,14],whichallowthetrainingofhigh-capacitydeeplearn-
ingmodelsfrommassiveamountsofdata.Thesemodelsenabledetectionofkey
cuespresentinvideos,suchasglobalandlocalmotion,variousobjectcategories
andglobalscene-levelinformation,andoftenachieveimpressiveperformancein
recognizinghigh-level,abstractconceptsinthewild.
arXiv:1806.06157v3  [cs.CV]  20 Sep 20182Baradel
etal.
Fig.1.
Humanscanunderstandwhathappenedinavideo(\theleftmostcarrotwas
choppedbytheperson"")givenonlyapairofframes.Alongtheselines,thegoalofthis
workistoexplorethecapabilitiesofhigher-level
reasoning
inneuralmodelsoperating
atthesemanticlevelofobjectsandinteractions.
However,recentattentionhasbeendirectedtowardamorethoroughun-
derstandingofhuman-focusedactivityindiverseinternetvideos.These
rangefromatomichumanactions[14]toobjectinteractions[13]
toeveryday,commonlyoccurringhuman-objectinteractions[12].Thisreturns
ustoahuman-centricviewpointofactivityrecognitionwhereitisnotonlythe
presenceofcertainobjects/scenesthatdictatetheactivitypresent,butthe
manner,order,andofhumaninteractionwiththesesceneelementsthat
arenecessaryforunderstanding.Inasense,thisisakintotheproblemsincurrent
3Dhumanactivityrecognitiondatasets[33],butrequiresthemorechallenging
reasoningandunderstandingofdiverseenvironmentscommontointernetvideo
collections.
Humansareabletoinferwhathappenedinavideogivenonlyafewsam-
pleframes.Inparticular,theycaninfercomplexactivitieshappeningbetween
pairsofframes.Thisfacultyiscalled
reasoning
andisakeycomponentofhu-
manintelligence.AsanexamplewecanconsiderthepairofimagesinFigure1,
whichshowsacomplexsituationinvolvingarticulatedobjects(human,carrots
andknife),thechangeoflocationandcompositionofobjects.Forhumansitis
straightforwardtodrawaconclusiononwhathappened(acarrotwaschopped
bythehuman).Humanshavethisextraordinaryabilityofperformingvisualrea-
soningonverycomplicatedtaskswhileitremainsunattainableforcontemporary
computervisionalgorithms[37,11].
Theabilitytoperform
visualreasoning
incomputervisionalgorithmsisstill
anopenproblem.Attemptshavebeenmadeforlearninginteractionsbetween
tentitiesinimageswithpromisingresultsonVisualQuestionAnswering.
Therehavebeenanumberofattemptstoequipneuralmodelswithreasoning
abilitiesbytrainingthemtosolveVisualQuestionAnswering(VQA)problems.
Amongproposedsolutionsareprior-lessdatanormalization[27],structuring
networkstomodelrelationships[32,43]aswellasmorecomplexattentionbased
ObjectLevelVisualReasoninginVideos3
mechanisms[18].Atthesametime,itwasshownthathighperformanceonexist-
ingVQAdatasetscanbeachievedbysimplydiscoveringbiasesinthedata[21].
Weextendtheseto
objectlevelreasoninginvideos
.Sinceavideois
atemporalsequence,weleveragetimeasanexplicitcausalsignaltoidentify
causalobjectrelations.Ourapproachisrelatedtotheconceptofthe
\arrowof
thetime""
[28]involvingthe\one-waydirection""or\asymmetry""oftime.Causal
eventoccursbeforetheeventit(
A
!
B
).InFigure1theknifewasused
beforethecarrotswitchedovertothechopped-upstateontherightside.For
avideoproblem,wewanttoidentifyacausalevent
A
happening
inavideothatitslabel
B
.Butinsteadofidentifyingthiscausalevent
directlyfrompixelswewanttoidentifyitfromanobjectlevelperspective.We
believethatsuchanapproachwouldbeabletolearncausalsignals.
Followingthishypothesisweproposetomakeabridgebetweenobjectde-
tectionandactivityrecognition.Objectdetectionallowsustoextractlow-level
informationfromascenewithallthepresentobjectinstancesandtheirsemantic
meanings.However,detailedactivityunderstandingrequiresreasoningoverthese
semanticstructures,determiningwhichobjectswereinvolvedininteractions,of
whatnature,andwhatweretheresultsofthese.Tocompoundproblems,the
semanticstructureofascenemaychangeduringavideo(e.g.anewobjectcan
appear,apersonmaymakeamovefromonepointtoanotheroneofthescene).
Weproposean
ObjectRelationNetwork
(ORN),aneuralnetworkmod-
uleforreasoningbetweendetectedsemanticobjectinstancesthroughspaceand
time.TheORNhaspotentialtoaddresstheseissuesandconductrelational
reasoningoverobjectinteractionsforthepurposeofactivityrecognition.Aset
ofobjectdetectionmasksrangingovertobjectcategoriesandtemporal
occurrencesisinputtotheORN.TheORNisabletoinferpairwiserelationships
betweenobjectsdetectedatvaryingtmomentsintime.
Codeandobjectmaskspredictionswillbepubliclyavailable
6
.
2Relatedwork
ActionRecognition.
Actionrecognitionhasalonghistoryincomputervi-
sion.Pre-deeplearningapproachesinactionrecognitionfocusedonhandcrafted
spatio-temporalfeaturesincludingspace-timeinterestpointslikeSIFT-3D,HOG3D,
IDTandaggregatedthemusingbag-of-wordstechniques.Somehand-craftedrep-
resentations,likedensetrajectories[42],stillgivecompetitiveperformanceand
arefrequentlycombinedwithdeeplearning.
Intherecentpast,workhasshiftedtodeeplearning.Earlyattemptsadapt
2Dconvolutionalnetworkstovideosthroughtemporalpoolingand3Dconvolu-
tions[2,40].3Dconvolutionsarenowwidelyadoptedforactivityrecognitionwith
theintroductionoffeaturetransferbypre-trained2Dconvolutionalker-
nelsfromimagemodelstrainedonImageNet/ILSVRC[31]through
3Dkernels[6].Thedownsideof3Dkernelsistheircomputationalcomplexity
6
https://github.com/fabienbaradel/object_level_visual_reasoning
4Baradel
etal.
andthelargenumberoflearnableparameters,leadingtotheintroductionof
2.5Dkernels,i.e.separableintheformofa2Dspatialkernelfollowedby
atemporalkernel[44].AnalternativetotemporalconvolutionsareRecurrent
NeuralNetworks(RNNs)intheirvariousgatedforms(GRUs,LSTMs)[17,9].
Karpathy
etal.
[20]presentedawidestudyontwaysofconnecting
informationinspatialandtemporaldimensionsthroughconvolutionsandpool-
ing.Onverygeneraldatasetswithcoarseactivityclassestheyhaveshowedthat
therewasasmallmarginbetweenclassifyingindividualframesandclassifying
videoswithmoresophisticatedtemporalaggregation.
Simoyan
etal.
[35]proposedawidelyadoptedtwo-streamarchitecturefor
actionrecognitionwhichextractstwotstreams,oneprocessingrawRGB
inputandoneprocessingpre-computedopticalwimages.Themethodout-
performedthestateoftheart,butreliesonrathersmallscaleopticalwcom-
putations.
Inslightlynarrowersettings,priorinformationonthevideocontentcanallow
moremodels.Articulatedposeiswidelyusedincaseswherehumans
areguaranteedtobepresent[33].Poseestimationandactivityrecognitionasa
joint(multi-task)problemhasrecentlyshowntoimprovebothtasks[25].Some-
whatrelatedtoourwork,StructuralRNNs[19]performactivityrecognition
byintegratingfeaturesfromsemanticobjectsandtheirrelationships.However,
theyhandlethetemporalevolutionoftrackedobjectsinvideoswithasetof
RNNs,eachofwhichcorrespondstocliquesinagraphwhichmodelsthespatio-
temporalrelationshipsbetweentheseobjects.Thisgraphishand-craftedman-
uallyforeachapplication,thoughrelatedworkprovideslearnableconnections
viagatingfunctions[8].
Attentionmodelsareawaytostructuredeepnetworksinanoftengeneric
way.Theyareabletoiterativelyfocusattentiontosppartsinthedatawith-
outrequiringpriorknowledgeaboutpartorobjectpositions.Inactivityrecog-
nition,theyhavegainedsometractioninrecentyears,eitherassoft-attention
onarticulatedpose(joints)[36],onfeaturemapcells[34,39],ontime[45]oron
partsinrawRGBinputthroughtiablecrops[3].
Whenrawvideodataisgloballyfedintodeepneuralnetworks,theyfocus
onextractingspatio-temporalfeaturesandperformaggregations.Ithasbeen
shownthatthesetechniquesfailonchallengingdatasets,whichre-
quirelearninglongtemporaldependenciesandhuman-objectinteractions.A
concentratedehasbeenmadetocreatelargescaledatasetstoovercome
theseissues[13,12,23,14].
RelationalReasoning.
Relationalreasoningisawellstudiedformany
applicationsrangingfromvisualreasoning[32]toreasoningaboutphysical
systems[4].Battaglia
etal.
[4]introduceatiablenetworkphysics
enginecalledInteractionNetwork(IN).INlearnstopredictseveralphysical
systemssuchasgravitationalsystems,rigidbodydynamics,andmass-spring
systems.Itshowsimpressiveresults;however,itlearnsfromavirtualenviron-
ment,whichprovidesaccesstovirtuallyunlimitedtrainingexamples.Following
thesameperspective,Santoro
etal.
[32]introducedRelationNetwork(RN),
ObjectLevelVisualReasoninginVideos5
aplug-inmoduleforreasoningindeepnetworks.RNshowshuman-levelper-
formanceinVisualQuestionAnswering(VQA)byinferringpairwise\object""
relations.However,incontrasttoourwork,theterm\object""in[32]doesnot
refertosemanticallymeaningfulentities,buttodiscretecellsinfeaturemaps.
Thenumberofinteractionsthereforegrowswithfeaturemapresolutions,which
makesittoscale.Furthermore,arecentstudy[21]hasshownthatsome
oftheseresultsaresubjecttodatasetbiasanddonotgeneralizewelltosmall
changesinthesettingsofthedataset.
Inthesameline,arecentwork[38]hasshownpromisingresultsondiscov-
eringobjectsandtheirinteractionsinanunsupervisedmannerusingtraining
examplesfromvirtualenvironments.In[41],attentionandrelationalmodules
arecombinedonagraphstructure.Fromatperspective,[27]showthat
relationalreasoningcanbelearnedforvisualreasoninginadatadrivenwaywith-
outanypriorusingconditionalbatchnormalizationwithafeature-wise
transformationbasedonconditioninginformation.Inanoppositeapproach,a
strongstructuralpriorislearnedintheformofacomplexattentionmecha-
nism:in[18],anexternalmemorymodulecombinedwithattentionprocesses
overinputimagesandtextquestions,performingiterativereasoningforVQA.
WhilemostofthediscussedworkhasbeendesignedforVQAandforpre-
dictionsonphysicalsystemsandenvironments,extensionshavebeenproposed
forvideounderstanding.Reasoninginvideosonamaskorsegmentationlevel
hasbeenattemptedforvideoprediction[24],wherethegoalwastoleveragese-
manticinformationtobeablepredictfurtherintothefuture.Zhouetal[5]have
recentlyshownstate-of-the-artperformanceonchallengingdatasetsbyextend-
ingRelationNetworktovideoTheirchosenentitiesareframes,on
whichtheyemployRNtoreasononatemporallevelonlythroughpairwiseframe
relations.Theapproachispromising,butrestrictedtotemporalcontextualin-
formationwithoutanunderstandingonalocalobjectlevel,whichisprovided
byourapproach.
Reasoningoversetsofobjectsissomewhatrelatedtoreasoningfromunstruc-
tureddatapoints,asdoneinPointNet[29],designedtolearnfromunorderedsets
ofpoints.PointNetsharesmanypropertieswithDeepSet[46]whichisamore
generalframeworkforextractinginformationfromsetsofobjects.Tosomeex-
tent,ourworkisrelatedtoPointNet,aswehandleunorderedsetsofobjectsina
permutationinvariantway.However,wehaveanobjectrelationviewpointthat
directlyreasonsoverrelationshipsbetweenthesesemanticentities.
3Object-levelVisualReasoninginSpaceandTime
Ourgoalistoextractmultipletypesofcuesfromavideosequence:interactions
betweenpredictedobjectsandtheirsemanticclasses,aswellaslocalandglobal
motioninthescene.Weformulatethisobjectiveasaneuralarchitecturewith
twoheads:an
activityhead
andan
objecthead
.Figure2givesafunctional
overviewofthemodel.Bothheadssharecommonfeaturesuptoacertainlayer
showninredintheThe
activityhead
,showninorangeinthe
6Baradel
etal.
Fig.2.
Afunctionaloverviewofthemodel.Aglobalconvolutionalmodelextracts
featuresandsplitsintotwoheadstrainedtopredict,respectivelyactivityclassesand
objectclasses.Thelatterarepredictedbypoolingoverobjectinstancemasks,which
arepredictedbyanadditionalconvolutionalmodel.Theobjectinstancesarepassed
throughavisualreasoningmodule.
isaCNN-basedarchitectureemployingconvolutionallayers,includingspatio-
temporalconvolutions,abletoextractglobalmotionfeatures.However,itisnot
abletoextractinformationfromanobjectlevelperspective.Weleveragethe
objecthead
toperformreasoningontherelationshipsbetweenpredictedobject
instances.
Ourmaincontributionisanewstructuredmodulecalled
ObjectRelation
Network
(ORN),whichisabletoperformspatio-temporalreasoningbetween
detectedobjectinstancesinthevideo.ORNisabletoreasonbymodelinghow
objectsmove,appearanddisappearandhowtheyinteractbetweentwoframes.
Inthissection,wewilldescribeourmaincontribution,theORNnetwork.
Wethenprovidedetailsaboutobjectinstancefeatures,abouttheactivityhead,
andaboutthenalrecognitiontask.Inwhatfollows,lowercaseletters
denote1Dvectorswhileuppercaselettersareusedfor2Dand3Dmatricesor
higherordertensors.Weassumethattheinputofoursystemisavideoof
T
framesdenotedby
X
1:
T
=(
X
t
)
T
t
=1
where
X
t
istheRGBimageattimestep
t
.
Thegoalistolearnamappingfrom
X
1:
T
toactivityclasses
y
.
3.1ObjectRelationNetwork
ORN(ObjectRelationNetwork)isamoduleforreasoningbetweensemantic
objectsthroughspaceandtime.Itcapturesobjectmoves,arrivalsandinterac-
tionsinantmanner.Wesupposethatforeachframe
t
,wehaveaset
ofobjects
k
withassociatedfeatures
o
k
t
.Objectsandfeaturesaredetectedand
computedbytheobjectheaddescribedinSection3.2.
Reasoningaboutactivitiesinvideosisinherentlytemporal,asactivitiesfol-
lowthe
arrowoftime
[28],i.e.thecausalityofthetimedimensionimposesthat
pastactionshaveconsequencesinthefuturebut
not
vice-versa.Wehandlethis
bysampling:runningaprocessovertime
t
,andforeachinstant
t
,samplinga
ObjectLevelVisualReasoninginVideos7
secondframe
t
0
with
t
0
<t
.Ournetworkreasonsonobjectswhichinteractbe-
tweenpairsofframesandtheircorrespondingsetsofobjects
O
t
0
=

o
k
t
0

K
0
k
=1
and
O
t
=

o
k
t

K
k
=1
.Thegoalistolearnageneralfunctiononthesetof
allinputobjectsfromthecombinedsetofbothframes:
g
t
=
g
(
o
1
t
0
;:::;
o
K
0
t
0
;
o
1
t
;:::;
o
K
t
)
:
(1)
Theobjectsinthissetareunordered,asidefortheframetheybelongto.
ThistaskisrelatedtoaproblemraisedinthePointNetalgorithm[29]dis-
cussedinSection2.PointNetapproximatesageneralfunction
g
overanitem
set
S
=
f
x
1
;x
2
;:::;x
N
g
asasymmetricfunction
g
0
ontransformedelementsof
theset
g
(
x
1
;x
2
;:::;x
N
)
ˇ
g
0
(
h
(
x
1
)
;h
(
x
2
)
;:::;h
(
x
N
))
:
(2)
In[29],
g
0
isamaxpoolingoperation.Theauthorsshowthatitallowsuniversal
approximationofcontinuoussetsoffunctionsgiventhatthehiddenrepresenta-
tion(theoutputofthemapping
h
(

))isoftlyhighdimension.
Wearguethattheapproximationin(2)canbeextendedasfollows:
g
(
x
1
;x
2
;:::;x
N
)
ˇ
f
˚
 
X
c
2C
h
c
(
[
i
2
c
x
i
)
!
(3)
where
C
isthesetofcliquesofagraphovertheitemset
S
,
[
isthe
concatenationoperatorandwechosethesumoperatorassymmetricfunction.
Theinputdimensionofthenon-linearity
h
c
(

)dependsonthesizeofclique
c
butmapstoaoutputdimension
H
.Inthecasewhere
C
iscomposedof
unarycliquesonly,form(3)decomposeslike(2)withtheexceptionofat
symmetryoperator(suminsteadofmaxpooling).Choosingtgraphical
structuresthrough
S
willleadtottermsinthesummationandallows
modelingttypesofinteractionsbetweenitemsintheitemset.
Notethattheinteractionsbetweenitemsin(3)arenotexclusivelymodeled
through
h
c
(

).Indeed,itisinterestingtonote,thatthegraphicaldecomposi-
tionprovidedby
C
leadstointeractionswhicharederentfromtheinteractions
thesamedecompositionwouldprovidewhenusedinaprobabilisticgraphical
model,likeforinstanceaMarkovRandomField(MRF).Inparticular,ade-
compositionintounarytermsonly,asgiveninequation(2),does
not
leadto
independencebetweenitems,whereasanMRFwithunarytermsonlyisequiva-
lenttoadistributionoverindependentrandomvariables.Thisisaconsequence
oftheglobalmapping
f
˚
(

),whichisonthesumoveralldirectinter-
actions.Higher-orderinteractionsbetweenseveralitemsnotdirectlymodeled
throughanon-linearity
h
c
(

)caneventuallybelearnedbythemodelthrough
thejointoutputspaceofall
h
c
(

),providedthatthedimensionality
H
ofthis
spaceishighenoughtoincorporateallinteractions.However,whereasthemap-
ping
h
c
(

)providesadirectmodelofinteractionsbetweenpairsofitems,learning
interactionsbetweentwoitems(
j;k
),whicharenotdirectlycapturedthrougha
8Baradel
etal.
Fig.3.
ORNintheobjectheadoperatingondetectedinstancesofobjects.
clique
c
anditscorresponding
h
c
(

),requireslearningacorrespondingsubspace
inthecommonoutputspacespannedbyall
h
c
(

).
Thisleadstothequestionofhowtothebetweenthecom-
plexityofthedecomposition
C
andtheoutputdimensionality
H
ofthemapping
h
c
(

),bothofwhichwilldeterminethecomplexityofthemodeledinteractions.
Increasingthesizeofcliquesin
C
willincreasetheinputdimension(andthere-
forethecapacity)ofthemapping
h
c
(

)aswellasthecomputationalcomplexity
ofthesumoperation.
Inspiredbyrelationalnetworks[32],wechosetodirectlymodelinter-frame
interactionsbetweenpairsofobjects(
j;k
)andleavemodelingofhigher-order
interactionstotheoutputspaceofthemappings
h

andtheglobalmapping
f
˚
:
g
t
=
X
j;k
h

(
o
j
t
0
;
o
k
t
)(4)
Inordertobetterdirectlymodellong-rangeinteractions,wemaketheglobal
mapping
f
˚
(

;

)recurrent,whichleadstothefollowingform:
r
t
=
f
˚
(
g
t
;
r
t

1
)(5)
where
r
t
representstherecurrent
objectreasoningstate
attime
t
and
g
t
isthe
globalinter-frameinteractioninferredattime
t
suchasdescribedinEquation4.
Inpractice,thisisimplementedasaGRU,butforsimplicityweomittedthe
gatesinEquation(5).Thepairwisemappings
h

(

;

)areimplementedasan
MLP.Figure3providesavisualexplanationoftheobjecthead'soperating
throughtime.
OurproposedORNfrom[32]inthreemainpoints:
Objectshaveasemantic
|wemodelrelationshipswithrespect
tosemanticallymeaningfulentities(objectinstances)insteadoffeaturemap
cellswhichdonothaveasemanticallymeaningfulspatialextent.Wewillshow
intheexperimentalsectionthatthisisakey
ObjectLevelVisualReasoninginVideos9
Objectsareselectedfromtframes
|weinferobjectpairwise
relationsonlybetweenobjectspresentintwotsets.Thisisakeydesign
choicewhichallowsourmodeltoreasonaboutchangesinobjectrelationships
overtime.
Longrangereasoning
|integrationoftheobjectrelationsovertimeisre-
currentbyusingaRNNfor
f
˚
(

).Sincereasoningfromafullsequencecannot
bedonebyinferringtherelationsbetweentwoframes,
f
˚
(

)allowslongrange
reasoningonsequencesofvariablelength.
3.2Objectinstancefeatures
Theobjectfeatures
O
t
=

o
k
t

K
k
=1
foreachframe
t
usedfortheORNmodule
describedabovearecomputedandcollectedfromlocalregionspredictedby
amaskpredictor.Independentlyforeachframe
X
t
oftheinputdatablock,
wepredictobjectinstancesasbinarymasks
B
k
t
andassociatedobjectclass
predictions
c
k
t
,adistributionover
C
classes.WeuseMask-RCNN[15],which
isabletodetectobjectsinaframeusingregionproposalnetworks[30]and
producesahighqualitysegmentationmaskforeachobjectinstance.
Theobjectiveistocollectfeaturesforeachobjectinstance,whichjointly
describeitsappearance,thechangeinitsappearanceovertime,anditsshape,
i.e.theshapeofthebinarymask.Intheory,appearancecouldalsobedescribed
bypoolingthefeaturerepresentationlearnedbythemaskpredictor(MaskR-
CNN).However,inpracticewechoosetopoolfeaturesfromthededicated
object
head
suchasshowninFigure2,whichalsoincludemotionthroughthespatio-
temporalconvolutionssharedwiththeactivityhead:
u
k
t
=ROI-Pooling(
U
t
;
B
k
t
)(6)
where
U
t
isthefeaturemapoutputbythe
objecthead
,
u
k
t
isa
D
-dimensional
vectorofappearanceandappearancechangeofobject
k
.
Shapeinformationfromthebinarymask
B
k
t
isextractedthroughthefollow-
ingmappingfunction:
b
k
t
=
g
˚
(
B
k
t
)
;
where
g
˚
(

)isaMLP.Informationabout
object
k
inimage
X
t
isgivenbyaconcatenationofappearance,shape,and
objectclass:
o
k
t
=[
b
k
t
u
k
t
c
k
t
].
3.3GlobalMotionandContext
Currentapproachesinvideounderstandingfocusonmodelingthevideofrom
ahigh-levelperspective.Byastackofspatio-temporalconvolutionandpooling
theyfocusonlearningglobalscenecontextinformation.eactivityrecog-
nitionrequiresintegrationofbothofthesesources:globalinformationaboutthe
entirevideocontentinadditiontorelationalreasoningformakingdistinc-
tionsregardingobjectinteractionsandproperties.
Inourmethod,locallow-levelreasoningisprovidedthroughobjecthead
andtheORNmodulesuchasdescribedaboveinSection3.1.Wecomplement
10Baradel
etal.
thisrepresentationbyhigh-levelcontextinformationdescribedby
V
t
whichare
featureoutputsfromtheactivityhead(orangeblockinFigure2).
Weusespatialglobalaveragepoolingover
V
t
tooutput
TD
-dimensional
featurevectorsdenotedby
v
t
,where
v
t
correspondstothecontextinformation
ofthevideoattimestep
t
.
Wemodelthedynamicsofthecontextinformationthroughtimebyemploy-
ingaRNN
f

(

)givenby:
s
t
=
f

(
v
t
;
s
t

1
)(7)
where
s
isthehiddenstateof
f

(

)andgivescuesabouttheevolutionofthe
contextthoughtime.
3.4Recognition
Givenaninputvideosequence
X
1:
T
,thetwotstreamscorrespondingto
theactivityheadandtheobjectheadresultinthetworepresentations
h
and
r
,
respectivelywhere
h
=
P
t
h
t
and
r
=
P
t
r
t
.Eachrepresentationisthehidden
stateoftherespectiveGRU,whichweredescribedintheprecedingsubsections.
Recallthat
h
providestheglobalmotioncontextwhile
r
providestheobject
reasoningstateoutputbytheORNmodule.Weperformindependentlinear
foreachrepresentation:
y
1
=
Wh
(8)
y
2
=
Zr
(9)
where
y
1
;
y
2
correspondtothelogitsfromthe
activityhead
andthe
objecthead
,
respectively,and
W
and
Z
aretrainableweights(includingbiases).The
predictionisdonebyaveraginglogits
y
1
and
y
2
followedbysoftmaxactivation.
4NetworkArchitecturesandfeaturedimensions
TheinputRGBimages
X
t
areofsize
R
3

W

H
where
W
and
H
correspondto
thewidthandheightandareofsize224each.Theobjectandactivityheads
(orangeandgreeninFigure2)areajointconvolutionalneuralnetworkwith
Resnet50architecturepre-trainedonImageNet/ILSVRC[31],withConv1and
Conv5blocksbeingto2.5Dconvolutions[44](3Dconvolutionswitha
separabletemporaldimension).Thischoicehasbeenoptimizedonthevalidation
set,asexplainedinSection6andshowninTable5.
Thelast
conv5
layershavebeensplitintotwotheads(activityhead
andobjecthead).Theintermediatefeaturerepresentations
U
t
and
V
t
areofdi-
mensions2048

T

14

14and2048

T

7

7,respectively.Weprovideahigher
spatialresolutionforthefeaturemaps
U
t
oftheobjectheadtogetmoreprecise
localdescriptors.Thiscanbedonebychangingthestrideoftheinitial
conv5
layersfrom2to1.Temporalconvolutionshavebeentokeepthesame
timetemporaldimensionthroughthenetwork.
ObjectLevelVisualReasoninginVideos11
Globalspatialpoolingofactivityfeaturesresultsina2048dimensionalfea-
turevectorfedintoaGRUwith512dimensionalhiddenstate
s
t
.ROI-Pooling
ofobjectfeaturesresultsin2048dimensionalfeaturevectors
u
k
t
.Theencoderof
thebinarymaskisaMLPwithonehiddenlayerofsize100andoutputsamask
embedding
b
k
t
ofdimension100.Thenumberofobjectclassesis80,whichleads
intotaltoa2229dimensionalobjectfeaturevector
o
k
t
.
Thenon-linearity
h

(

)isimplementedasanMLPwith2hiddenlayerseach
with512unitsandproducesan512dimensionaloutputspace.
f
˚
(

)isimple-
mentedasaGRUwitha256dimensionhiddenstate
r
t
.WeuseReLUasthe
activationfunctionaftereachlayerforeachnetwork.
5Training
Wetrainthemodelwithalosssplitintotwoterms:
L
tot
=
L

^
y
1
+
^
y
2
2
;
y

+
X
t
X
k
L
(
^
c
k
t
;
c
k
t
)
:
(10)
where
L
isthecross-entropyloss.Thetermcorrespondstosupervisedac-
tivityclasslossescomparingtwotactivityclasspredictionstotheclass
groundtruth:
^
y
1
isthepredictionoftheactivityhead,whereas
^
y
2
isthepre-
dictionoftheobjecthead,asgivenbyEquations(8)and(9),respectively.
Thesecondtermisalosswhichpushesthefeatures
U
oftheobjecttowards
representationsofthesemanticobjectclasses.Thegoalistoobtainfeatures
relatedto,both,motion(throughthelayerssharedwiththeactivityhead),as
wellasasobjectclasses.Asground-truthobjectclassesarenotavailable,we
thelossasthecross-entropybetweentheclasslabel
c
k
t
predictedbythe
maskpredictorandadedicatedlinearclassprediction
^
c
k
t
basedonfeatures
u
k
t
,
which,aswerecall,areRoI-pooledfrom
U
:
c
k
t
=
Ru
k
t
(11)
where
R
trainableparameters(biasesintegrated)learnedend-to-endtogether
withtheotherparametersofthemodel.
Wefoundthattrainingtheobjectheadonlyandthenthefullnetwork
wasperformingbetter.AResNet50networkpretrainedonImageNetismo
bysomeofitsto2.5convolutions(3Dconvolutionswiththetime
dimensionseparated),asdescribedinSection4;thenby
WetrainthemodelusingtheAdamoptimizer[22]withaninitiallearning
rateof10

4
on30epochsanduseearly-stoppingcriteriononthevalidationset
forhyper-parameteroptimization.Trainingtakes
˘
50minutesperepochon4
TitanXPGPUswithclipsof8frames.
12Baradel
etal.
6Experimentalresults
Weevaluatedthemethodonthreestandarddatasets,whichrepresent
activityrecognitiontasks:theSomething-Somethingdataset,the
VLOGdatasetandtherecentlyreleasedEPICKitchensdataset.
Something-Something(SS)
isarecentvideodatasetwith
108,000examplevideosand157classes[13].Itshowshumansperforming
entactionswithtobjects,actionsandobjectsbeingcombinedint
ways.SolvingSSrequirescommonsensereasoningandthestate-of-the-artmeth-
odsinactivityrecognitiontendtofail,whichmakesthisdatasetchallenging.
VLOG
isamulti-labelbinaryofhuman-objectinteractions
recentlyreleasedwith114,000videosand30classes[12].Classescorrespond
toobjects,andlabelsofaclassare1ifapersonhastouchedacertainobject
duringthevideo,otherwisetheyare0.Ithasrecentlybeenshown,thatstate-
of-the-artvideobasedmethods[6]areoutperformedonVLOGbyimagebased
methodslikeResNet-50[16],althoughthesevideomethodsoutperformimage
basedResNet-50onlarge-scalevideodatasetsliketheKineticsdataset[6].This
suggestsagapbetweentraditionaldatasetslikeKineticsandthe
datasetVLOG,makingitparticularly
EPICKitchens(EPIC)
isanegocentricvideodatasetrecentlyreleased
containing55hoursrecordingofdailyactivities[7].Thisisthelargestin
personvisionandtheactivitiesperformedarenon-scripted,whichmakesthe
datasetverychallengingandclosetorealworlddata.Thedatasetisdensely
annotatedandseveraltasksexistsuchasobjectdetection,actionrecognitionand
actionprediction.Wefocusonactionrecognitionwith39'594actionsegments
intotaland125actionsclasses(i.everbs).Sincethetestsetisnotavailableyet
weconductedourexperimentsonthetrainingset(28'561videos).Weusethe
videosrecordedbyperson01toperson25fortraining(22'675videos)and
thevalidationsetastheremainingvideos(5'886videos).
Foralldatasetswerescaletheinputvideoresolutionto256

256.While
training,wecropspace-timeblocksof224

224spatialresolutionand
L
frames,
with
L
=8fortheSSdatasetand
L
=4forVLOGandEPIC.Wedonotperform
anyotherdataaugmentation.Whiletrainingweextract
L
framesfromtheentire
videobysplittingthevideointo
L
sub-sequencesandrandomlysamplingone
framepersub-sequence.Theoutputsequenceofsize
L
iscalleda
clip
.Aclip
aimstorepresentthefullvideowithlessframes.Fortestingweaggregateresults
of10clips.Weuse
lintel
[10]fordecodingvideoonthey.
Theablationstudyisdonebyusingthetrainsetastrainingdataandwe
reporttheresultonthevalidationset.Wecompareagainstotherstate-of-the-
artapproachesonthetestset.Fortheablationstudies,weslightlydecreased
thecomputationalcomplexityofthemodel:thebasenetwork(includingactivity
andobjectheads)isaResNet-18insteadofResNet-50,asingleclipof4frames
isextractedfromavideoattesttime.
Comparisonwithotherapproaches.
Table1showstheperformanceof
theproposedapproachontheVLOGdataset.Weoutperformthestateoftheart
ObjectLevelVisualReasoninginVideos13
Table1.
ResultsonHand/SemanticObjectInteraction(Averagepre-
cisionin%onthetestset)onVLOGdataset.R50andI3Dimplementedby[12].
mAP
bag
bed
bedding
book/papers
bottle/tube
bowl
box
brush
cabinet
cell-phone
clothing
cup
door
drawers
food
fork
knife
laptop
microwave
oven
pen/pencil
pillow
plate
refrigerator
sink
spoon
animal
table
toothbrush
towel
R50[16]
40.529.768.965.864.558.233.122.119.0
23.9
54.045.528.649.2
28.7
49.619.437.562.948.823.036.939.212.5
55.9
58.831.1
57.4
26.839.622.9
I3D[6]
39.724.971.7
71.4
62.557.127.119.2
33.9
20.750.645.824.754.719.1
50.8
19.3
41.9
54.027.521.437.442.912.642.560.433.946.023.559.634.7
Ours
44.730.272.3
70.7
64.959.838.224.6
26.322.4
64.547.235.457.9
25.248.5
24.5
40.2
72.054.126.539.948.615.2
53.5
60.736.8
52.8
27.964.037.6
onthischallengingdatasetbyamarginof
ˇ
4.2points(44.7%accuracyagainst
40.5%by[16]).Asmentionedabove,traditionalvideoapproachestendtofail
onthischallengingdataset,providinginferiorresults.Table3shows
performanceonSSwhereweoutperformthestateoftheartgivenbyveryrecent
methods(+2.3points).OnEPICwere-implementstandardbaselinesandreport
resultsonthevalidationset(Table4)sincethetestsetisnotavailable.Ourfull
methodreportsanaccuracyof40.89andoutperformsbaselinesbyalargemargin
(
ˇ
+6.4and
ˇ
+7.9pointsrespectivelyforagainstCNN-2DandI3Dbasedona
ResNet-18).
ofobject-levelreasoning.
Table2showstheimportanceofrea-
soningontheperformanceofthemethod.Thebaselinecorrespondstotheper-
formanceobtainedbytheactivityheadtrainedaloneResNet,inthe
ResNet-18versionforthistable).Noobjectlevelreasoningispresentinthis
baseline.Theproposedapproach(thirdline)includinganobjectheadandthe
ORNmodulegains0.8,2.5and2.4pointscomparedtoourbaselinerespectively
onSS,onEPICandonVLOG.Thisindicatesthatthereasoningmoduleisable
toextractcomplementaryfeaturescomparedtotheactivityhead.
Using
semanticallydobjects
provedtobeimportantandledtoagainof
2pointsonEPICand2.3pointsonVLOGforthefullmodel(6/12.7pointsusing
theobjectheadonly)comparedtoanextensionofSantoro
etal.
[32]operating
onpixellevel.Thisindicatesimportanceofobjectlevelreasoning.Thegainon
SSissmaller(0.7pointwiththefullmodeland7.8pointswiththeobjecthead
only)andcanbeexplainedbythederenceinspatialresolutionofthevideos.
Objectdetectionsandpredictionsofthebinarymasksaredoneusingtheinitial
videoresolution.ThemeanvideoresolutionforVLOGis660

1183andforEPIC
is640

480against100

157forSS.Mask-RCNNhasbeentrainedonimagesof
resolution800

800andthusperformsbestonhigherresolutions.Thequalityof
theobjectdetectorisimportantforleveragingobjectlevelunderstandingthen
fortherestoftheablationstudywefocusonEPICandVLOGdatasets.
Thefunction
f
˚
inEquation(5)isanimportantdesignchoiceinourmodel.
Inourproposedmodel,
f
˚
isrecurrentovertimetoensurethattheORNmodule
captureslongrangereasoningovertime,asshowninEquation(5).Removing
therecurrenceinthisequationleadstoanMLPinsteadofa(gated)RNN,as
evaluatedinrow4ofTable2.Performancedecreasesby1.1pointonVLOG
and1.4pointsonEPIC.ThelargergapforEPICcomparedtoVLOGand
canarguablybeexplainedbythefactthatinSSactionscoverthewholevideo,
14Baradel
etal.
Table2.
AblationstudywithResNet-18backbone.Resultsin%:Top-1accuracyfor
EPICandSSdatasets,andmAPforVLOGdataset.
Method
Objecttype
EPIC
VLOG
SS
obj.head2heads
obj.head2heads
obj.head2heads
Baseline
-
-
38.33
-
35.03
-
31.31
ORN
pixel
23.7138.83
14.4035.18
2.5131.43
ORN
COCO
29.9440.89
27.1437.49
10.26
32.12
ORN-mlp
COCO
28.1539.41
25.4036.35
--
ORN
COCO-visual
28.4538.92
22.9235.49
--
ORN
COCO-shape
21.9237.16
7.1835.39
--
ORN
COCO-class
21.9637.75
13.4035.94
--
ORN
COCO-intra
29.2538.10
26.7836.28
--
ORNclique-1
COCO
28.2540.18
26.4836.71
--
ORNclique-3
COCO
22.6137.67
27.0536.04
--
Table3.
Experimentalresultsonthe
Something-Somethingdataset(classi-
accuracyin%onthetestset).
Methods
Top1
C3D+Avg[13]
21.50
I3D[13]
27.63
MultiScaleTRN[5]
33.60
Ours
35.97
Table4.
Experimentalresultsonthe
EPICKitchensdataset(accuracyin%
onthevalidationset{methodswith

havebeenre-implemented).
Methods
Top1
R18[16]

32.05
I3D-18[6]

34.20
Ours
40.89
whilesolvingVLOGrequiresdetectingtherightmomentwhenthehuman-object
interactionoccursandthuslongrangereasoningplaysalessimportantrole.
Visualfeaturesextractedfromobjectregionsarethemostdiscriminative,
howeverobjectshapesandlabelsalsoprovidecomplementaryinformation.Fi-
nally,thelastpartofTable2evaluatestheofthecliquessizeformodel-
ingtheinteractionsbetweenobjectsandshowthatpairwisecliquesoutperform
cliquesofsize1and3.Wewouldliketorecall,thatevenwithunarycliques
only,interactionsbetweenobjectsarestillmodeled.However,themodelneeds
tosubspacesinthehiddenrepresentationsassociatedtoeachinteraction.
CNNarchitectureandkernel
Theconvolutionalarchitecture
ofthemodelwasoptimizedoverthevalidationsetoftheSSdataset,asshown
inTable5.Thearchitectureitself(intermsofnumbersoflayers,etc.)
isdeterminedbypre-trainingonimageWeoptimizedthechoice
offrom2Dto2.5Dor3Dforseveralconvolutionalblocks.This
hasbeenoptimizedforthesingleheadmodelandusingaResNet-18variant
tospeedupcomputation.Addingtemporalconvolutionsincreasesperformance
upto100%w.r.t.topure2Dbaselines.Thisindicates,withoutsurprise,that
motionisastrongcue.kernelsto2.5Dontheinputsideandonthe
ObjectLevelVisualReasoninginVideos15
Table5.
oftheCNNarchitecture(choiceofkernelinonasinglehead
ResNet-18network.Accuracyin%onthevalidationsetofSomething-Somethingis
shown.2.5Dkernelsareseparablekernels:2Dfollowedbya1Dtemporal.
Conv1Conv2Conv3Conv4Conv5Aggreg
SS
2D3D2.5D
2D3D2.5D
2D3D2.5D
2D3D2.5D
2D3D2.5D
GAPRNN
X
--
X
--
X
--
X
--
X
--
X
-
15.73
X
--
X
--
X
--
X
--
X
--
-
X
15.88
-
X
-
-
X
-
-
X
-
-
X
-
-
X
-
X
-
31.42
--
X
--
X
--
X
--
X
--
X
X
-
27.58
X
--
X
--
X
--
X
--
-
X
-
X
-
31.28
X
--
X
--
X
--
-
X
-
-
X
-
X
-
32.06
X
--
X
--
-
X
-
-
X
-
-
X
-
X
-
32.25
X
--
X
--
X
--
X
--
--
X
X
-
31.31
X
--
X
--
X
--
--
X
--
X
X
-
32.79
X
--
X
--
--
X
--
X
--
X
X
-
33.77
-
X
-
X
--
X
--
X
--
X
--
X
-
28.71
-
X
-
-
X
-
X
--
X
--
X
--
X
-
31.42
--
X
X
--
X
--
X
--
X
--
X
-
20.05
--
X
--
X
X
--
X
--
X
--
X
-
22.52
outputsideprovidedbestperformances,suggestingthattemporalintegration
isrequiredataverylowlevel(motionestimation)aswellasonaveryhigh
level,closetoreasoning.Ourstudyalsocorroboratesrecentresearchinactivity
recognition,indicatingthat2.5Dkernelsprovideagoodbetweenhigh-
capacityandlearnablenumbersofparameters.Finallytemporalintegrationvia
RNNoutperformsglobalaveragepoolingoverspaceandtime.Thechoiceof
a(gated)RNNfortemporalintegrationoftheactivityheadfeaturesproved
important(seeTable5)comparedtoglobalaveragepooling(GAP)overspace
andtime.
Visualizingthelearnedobjectinteractions.
Figure4showsvisualiza-
tionsofthepairwiseobjectrelationshipsthemodellearnedfromdata,inpartic-
ularfromtheVLOGdataset.Eachgraphiscomputedforagivenactivityclass,
andstrongarcsbetweentwonodesinthegraphindicatestrongrelationships
betweentheobjectclasses,i.e.themodeldetectsahighcorrelationbetween
theserelationshipsandtheactivity.Thegraphswereobtainedbythresholding
thesummedactivationsofeachpairwiserelationship(
j;k
)inequation(4).Each
pair(
j;k
)canbeassignedapairofobjectclasses
c
j
t
and
c
k
t
throughthepredic-
tionsoftheobjectinstancemaskpredictor.Integratingoverallsamplesofthe
datasetforagivenclassleadstothevisualizationsinFigure4.Wecanseethat
theobjectinteractionsarehighlyrelevanttothedetectedactivities:the
person-
touches-bed
activityiscorrelatedtointeractionsbetweenrelevantobjectclasses
person
and
bed
.Similarly,activities
human-bowlinteraction
and
human-cupin-
teraction
showinteractionswiththerespectiveobjects
bowl
and
cup
.Moreover,
otherrecoveredrelationshipsarehighlycorrelatedtothescene(forexample,
dining-table
and
bowl
foractivity
human-bowlinteraction
).
16Baradel
etal.
Fig.4.
ExampleofobjectpairwiseinteractionslearnedbyourmodelonVLOGfor
fourtclasses.Objectsco-occurrencesareatthetopandlearnedpairwiseobjects
interactionsareatthebottom.Linethicknessindicateslearnedimportanceofagiven
relation.Interactionshavebeennormalizedbytheobjectco-occurrences.
Fig.5.Examplesoffailurecases
{a)smallsizedobjects(ontheleft).Ourmodel
detectsa
cellphone
anda
person
butfailstodetect
hand-cell-phonecontact
;b)con-
fusionbetweensemanticallysimilarobjects(ontheright).Themodelfalslypredicts
hand-cupcontact
insteadof
hand-glass-contact
eventhoughthe
wineglass
isdetected.
Finally,Figure5showssomefailurecases,whichareeitherduetoerrors
donebytheobjectmaskprediction(MaskR-CNN)orbytheORNitself.
7Conclusion
Wepresentedamethodforactivityrecognitioninvideoswhichleveragesob-
jectinstancedetectionsforvisualreasoningonobjectinteractionsovertime.
Thechoiceofreasoningoversemanticallywobjectsiskeytoourap-
proachandoutperformsstateoftheartmethodswhichreasonongrid-levels,
suchascellsofconvolutionalfeaturemaps.Temporaldependenciesandcausal
relationshipsaredealtwithbyintegratingrelationshipsbetweenttime
instants.Weevaluatedthemethodonthreedatasets,onwhichstandard
approachesdonotperformwell,andreportstate-of-the-artresults.
Acknowledgements.
ThisworkwasfundedbygrantDeepvision(ANR-15-
CE23-0029,STPGP-479356-15),ajointFrench/CanadiancallbyANR&NSERC.
ObjectLevelVisualReasoninginVideos17
References
1.
Abu-El-Haija,S.,Kothari,N.,Lee,J.,Natsev,P.,Toderici,G.,Varadarajan,B.,
Vijayanarasimhan,S.:Youtube-8m:Alarge-scalevideobenchmark.
arXivpreprintarxiv:1609.08675(2016)
2.
Baccouche,M.,Mamalet,F.,Wolf,C.,Garcia,C.,Baskurt,A.:Sequentialdeep
learningforhumanactionrecognition.In:HBU(2011)
3.
Baradel,F.,Wolf,C.,Mille,J.,Taylor,G.:Glimpseclouds:Humanactivityrecog-
nitionfromunstructuredfeaturepoints.In:CVPR(2018)
4.
Battaglia,P.W.,Pascanu,R.,Lai,M.,Rezende,D.J.,Kavukcuoglu,K.:Interaction
networksforlearningaboutobjects,relationsandphysics.In:NIPS(2016)
5.
Bolei,Z.,Zhang,A.A.,Torralba,A.:Temporalrelationalreasoninginvideos.In:
ECCV(2018)
6.
Carreira,J.,Zisserman,A.:Quovadis,actionrecognition?anewmodelandthe
kineticsdataset.In:CVPR(2017)
7.
Damen,D.,Doughty,H.,Farinella,G.M.,Fidler,S.,Furnari,A.,Kazakos,E.,
Moltisanti,D.,Munro,J.,Perrett,T.,Price,W.,Wray,M.:Scalingegocentric
vision:Theepic-kitchensdataset.In:ECCV(2018)
8.
Deng,Z.,Vahdat,A.,Hu,H.,Mori,G.:Structureinferencemachines:Recurrent
neuralnetworksforanalyzingrelationsingroupactivityrecognition.In:CVPR
(2016)
9.
Donahue,J.,Hendricks,L.A.,Guadarrama,S.,Rohrbach,M.,Venugopalan,S.,
Saenko,K.,Darrell,T.:Long-termrecurrentconvolutionalnetworksforvisual
recognitionanddescription.In:CVPR(2015)
10.
Duke,B.:Lintel:Pythonvideodecoding.
https://github.com/dukebw/lintel
(2018)
11.
Fleuret,F.,Li,T.,Dubout,C.,Wampler,E.K.,Yantis,S.,Geman,D.:Comparing
machinesandhumansonavisualcategorizationtest.ProceedingsoftheNational
AcademyofSciencesoftheUnitedStatesofAmerica
10843
,17621{5(2011)
12.
Fouhey,D.F.,Kuo,W.,Efros,A.A.,Malik,J.:Fromlifestylevlogstoeveryday
interactions.In:CVPR(2018)
13.
Goyal,R.,EbrahimiKahou,S.,Michalski,V.,Materzynska,J.,Westphal,S.,Kim,
H.,Haenel,V.,Fruend,I.,Yianilos,P.,Mueller-Freitag,M.,Hoppe,F.,Thurau,
C.,Bax,I.,Memisevic,R.:The""somethingsomething""videodatabaseforlearning
andevaluatingvisualcommonsense.In:ICCV(2017)
14.
Gu,C.,Sun,C.,Ross,D.A.,Vondrick,C.,Pantofaru,C.,Li,Y.,Vijayanarasimhan,
S.,Toderici,G.,Ricco,S.,Sukthankar,R.,Schmid,C.,Malik,J.:Ava:A
videodatasetofspatio-temporallylocalizedatomicvisualactions.arXivpreprint
arXiv:1705.08421(2017)
15.
He,K.,Gkioxari,G.,Dollar,P.,Girshick,R.:Maskr-cnn.In:ICCV(2017)
16.
He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.
In:CVPR(2016)
17.
Hochreiter,S.,Schmidhuber,J.:Longshort-termmemory.NeuralComputation
9
(8),1735{1780(1997)
18.
Hudson,D.,Manning,C.:Compositionalattentionnetworksformachinereasoning.
In:ICLR(2018)
19.
Jain,A.,Zamir,A.R.,Savarese,S.,Saxena,A.:Structural-RNN:DeepLearning
onSpatio-TemporalGraphs.In:CVPR(2016)
20.
Karpathy,A.,Toderici,G.,Shetty,S.,Leung,T.,Sukthankar,R.,Fei-Fei,L.:Large-
scalevideowithconvolutionalneuralnetworks.In:CVPR(2014)
18Baradel
etal.
21.
Kim,J.,Ricci,M.,Serre,T.:Not-so-CLEVR:Visualrelationsstrainfeedforward
neuralnetworks.arXivpreprintarXiv:1802.03390(2018)
22.
Kingma,D.,Ba,J.:Adam:Amethodforstochasticoptimization.In:ICML(2015)
23.
Krishna,R.,Zhu,Y.,Groth,O.,Johnson,J.,Hata,K.,Kravitz,J.,Chen,S.,Ka-
landitis,Y.,Li,L.J.,Shamma,D.A.,Bernstein,M.,Fei-Fei,L.:Visualgenome:
Connectinglanguageandvisionusingcrowdsourceddenseimageannotations.In-
ternationalJournalofComputerVision(IJCV)
123
,32{73(2017)
24.
Luc,P.,,Neverova,N.,Couprie,C.,Verbeek,J.,LeCun,Y.:Predictingdeeper
intothefutureofsemanticsegmentation.In:ICCV(2017)
25.
Luvizon,D.,Picard,D.,Tabia,H.:2d/3dposeestimationandactionrecognition
usingmultitaskdeeplearning.In:CVPR(2018)
26.
Monfort,M.,Zhou,B.,Bargal,S.A.,Andonian,A.,Yan,T.,Ramakrishnan,
K.,Brown,L.,Fan,Q.,Gutfruend,D.,Vondrick,C.,Oliva,A.:Moments
intimedataset:onemillionvideosforeventunderstanding.arXivpreprint
arXiv:1801.03150(2018)
27.
Perez,E.,Vries,H.D.,Strub,F.,Dumoulin,V.,Courville,A.:Learningvisualrea-
soningwithoutstrongpriors.In:ICMLMachineLearninginSpeechandLanguage
ProcessingWorkshop(2017)
28.
Pickup,L.C.,Pan,Z.,Wei,D.,Shih,Y.,Zhang,C.,Zisserman,A.,Scholkopf,B.,
Freeman,W.T.:Seeingthearrowoftime.In:CVPR(2014)
29.
Qi,C.R.,Su,H.,Mo,K.,Guibas,L.J.:Pointnet:Deeplearningonpointsetsfor
3dationandsegmentation.In:CVPR(2017)
30.
Ren,S.,He,K.,Girshick,R.,Sun,J.:FasterR-CNN:Towardsreal-timeobject
detectionwithregionproposalnetworks.In:NIPS(2015)
31.
Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,
Karpathy,A.,Khosla,A.,Bernstein,M.,Berg,A.,Fei-Fei,L.:Imagenetlargescale
visualrecognitionchallenge.IJCV
115
(3),211{252(2015)
32.
Santoro,A.,Raposo,D.,Barrett,D.G.,Malinowski,M.,Pascanu,R.,Battaglia,
P.,Lillicrap,T.:Asimpleneuralnetworkmoduleforrelationalreasoning.In:NIPS
(2017)
33.
Shahroudy,A.,Liu,J.,Ng,T.T.,Wang,G.:NTURGB+D:ALargeScaleDataset
for3DHumanActivityAnalysis.In:CVPR(2016)
34.
Sharma,S.,Kiros,R.,Salakhutdinov,R.:Actionrecognitionusingvisualattention.
In:ICLRWorkshop(2016)
35.
Simonyan,K.,Zisserman,A.:Two-streamconvolutionalnetworksforactionrecog-
nitioninvideos.In:NIPS(2014)
36.
Song,S.,Lan,C.,Xing,J.,Zeng,W.,Liu,J.:AnEnd-to-EndSpatio-Temporal
AttentionModelforHumanActionRecognitionfromSkeletonData.In:AAAI
(2016)
37.
Stabinger,S.,Rodranchez,A.,Piater,J.:25yearsofCNNs:Canwecompare
tohumanabstractioncapabilities?In:ICANN(2016)
38.
vanSteenkiste,S.,Chang,M.,K.,Schmidhuber,J.:Relationalneuralex-
pectationmaximization:Unsuperviseddiscoveryofobjectsandtheirinteractions.
In:ICLR(2018)
39.
Sun,L.,Jia,K.,Chen,K.,Yeung,D.,Shi,B.E.,Savarese,S.:Latticelongshort-
termmemoryforhumanactionrecognition.In:ICCV(2017)
40.
Tran,D.,Bourdev,L.,Fergus,R.,Torresani,L.,Paluri,M.:Learningspatiotem-
poralfeatureswith3dconvolutionalnetworks.In:ICCV(2015)
41.
Velikovi,P.,Cucurull,G.,Casanova,A.,Romero,A.,Li,P.,Bengio,Y.:Graph
attentionnetworks.In:ICLR(2018)
ObjectLevelVisualReasoninginVideos19
42.
Wang,H.,aser,A.,Schmid,C.,Liu,C.L.:ActionRecognitionbyDenseTrajec-
tories.In:CVPR(2011)
43.
Watters,N.,Zoran,D.,Weber,T.,Battaglia,P.,Pascanu,R.,Tacchetti,A.:Visual
interactionnetworks:Learningaphysicssimulatorfromvideo.In:NIPS(2017)
44.
Xie,S.,Sun,C.,Huang,J.,Tu,Z.,Murphy,K.:Rethinkingspatiotemporalfeature
learningforvideounderstanding.arXivprepringarxiv:1712.04851(2017)
45.
Yeung,S.,Russakovsky,O.,Jin,N.,Andriluka,M.,Mori,G.,Fei-Fei,L.:Everymo-
mentcounts:Densedetailedlabelingofactionsincomplexvideos.arXivpreprint
arXiv:1507.05738(2015)
46.
Zaheer,M.,Kottur,S.,Ravanbakhsh,S.,Poczos,B.,Salakhutdinov,R.R.,Smola,
A.J.:Deepsets.In:NIPS.pp.3391{3401(2017),
http://papers.nips.cc/paper/
6931-deep-sets.pdf
"
51,New Techniques for Preserving Global Structure and Denoising with Low Information Loss in Single-Image Super-Resolution,http://arxiv.org/pdf/1805.03383v2.pdf,https://github.com/nikhilvravi/DukeSR,"NewTechniquesforPreservingGlobalStructureandDenoisingwithLow
InformationLossinSingle-ImageSuper-Resolution
YijieBeiAlexDamianShijiaHuSachitMenonNikhilRaviCynthiaRudin

DukeUniversity
Abstract
Thisworkandaddressestwoimportanttechni-
calchallengesinsingle-imagesuper-resolution:(1)howto
upsampleanimagewithoutmagnifyingnoiseand(2)howto
preservelargescalestructurewhenupsampling.Wesum-
marizethetechniqueswedevelopedforoursecondplace
entryinTrack1(BicubicDownsampling),seventhplaceen-
tryinTrack2(RealisticAdverseConditions),andseventh
placeentryinTrack3(Realisticdifinthe2018NTIRE
Super-ResolutionChallenge.Furthermore,wepresentnew
neuralnetworkarchitecturesthataddressthe
twochallengeslistedabove:denoisingandpreservationof
large-scalestructure.
1.Introduction
Super-resolution(SR)isaclassicprobleminimagepro-
cessingwherethegoalistogenerateahighresolutionim-
agefromoneormorelowresolutionimages.Applica-
tionsofsuper-resolutionarewide-ranging.Forinstance,
SRisimportantforallowingmoderndis-
playstofunctionproperlywhenshowingvideorecordedat
lowerresolutions.SRalsohasmanyapplicationsinmed-
icalimaging,suchasreducingnoiseinimagesstemming
fromuncontrollablepatientmotions[11].Thisworkfo-
cusesonsingleimagesuper-resolution,whichisusefulfor
photographicenhancement,licenseplaterecognition,satel-
liteimaging,andotherremotesensingapplicationssuchas
recognitionofamilitarytarget[16].
Deeplearningtechniquescanlearnamappingdirectly
fromlowresolutiontohighresolutionimages,whereall
featureconstructionisautomated.Thismakessometypes
ofcomplexpreprocessingmucheasierthanpreviousap-
proaches,forexample,wenolongerneedtoexplicitly
chooseadictionaryoflow-levelfeatures(e.g.,edgedetec-
tors)toconvolvewiththeimage.Thefactthattraining
deepneuralnetworkshasbecomemucheasierwithinthe

Allauthorscontributedequally.ThankstoothermembersofDuke
DataScienceTeam
pastfewyearshasledtomorereliableautomatedtraining.
Ontheotherhand,thefactthatthesedeeplearningmeth-
odsuserecursivemathematicalformulasthatarenowmuch
morecomplicatedthanbeforemakesitmorediftode-
terminehowtobesttroubleshootthemtoachievehigher-
qualityperformance.
Inthisworkwediscussseveralinsightsintotheprob-
lemofsingle-imagesuper-resolutionŒmanyofwhichhave
ledtohigherqualityperformancebeyondentriesfromlast
year'sNTIREsingle-imageSRcompetition.Theseinsights
concerntheofnoisewhenupsamplingand
thepreservationoflargescalestructureinenhancedim-
ages.Weintroduceneuralnetworkarchitecturesforboth
thedenoisingproblem(DeNoisingforSuper-ResolutionŒ
DNSR)andtheproblemofpreservinglarge-scalesstructure
(AutomatedDecompositionandReconstructionforSuper-
ResolutionŒADRSR).Additionallywepresentasetof
tricksthatprovidedboostsinSRperformance.
Fordenoisingwhileupsampling,wepresenttheDNSR
(andmorebasicDNISR)architecturethatconcatenatestwo
networks,wherethetnetworkisfordenoisingandthe
secondisabaselinemethodforSR.Thisleveragesdo-
mainknowledgethatthenoiseshouldnothavebeeninthe
low-resolutionimageintheplaceandthusweshould
notamplifyit.Trainingtheseconcatenatednetworksled
toimprovementsinperformanceinTrack2(realisticmild
adverseconditions)andTrack3(realisticdift)ofthe
NTIRESR2018challenge.
ModernmethodsforSRhavetroublepreservinglarge
scalestructure.Evenifthehighresolutionimageslookre-
alisticinlocalpatches,theglobalstructure(suchasstripes
thatreachacrossthefullimage)canhaveseriousvisible
faults.Wepresentanarchitectureforpreservingstructureat
multiplescales.Inournetwork,ADRSR,theoriginalimage
isdownsampledmultipletimes,convolutionsareperformed
oneachofthedownsampledimages,andcombinedtoform
thehigh-resolutionimage.Thisallowsamultiscalere-
constructionoftheimagethatincludesinformationabout
thelargerscalesbeforemodelinginformationatthesmaller
scales.
Thearchitecturesfordenoisingandpreservinglarge-
4321
arXiv:1805.03383v2  [cs.CV]  16 Jun 2018scalestructurecanbeusedwithanynetworkblocksused
forSR;weusedconvolutionalblocksfromEDSR[9]within
ourimplementations,butthesecanbechangedtoanyother
blocks.DNISRorDNSRcombineanynetworkfordenois-
ingwithanynetworkforSR.
Mostoftheideasdiscussedherewerenotimplemented
intimefortheNTIRE2018SRcompetitiondeadline.How-
ever,wepresentasetoftricksthatwerehelpfulinachiev-
inghigherlevelperformanceduringthecompetition.For
instance,anideausedinourTrack1(classicbicubicdown-
sampling)entrywastorandomlyshufthered,green,
andbluelayersoftheimageduringtraining,whichhelps
asaformofself-ensembling.Wealsodiscussdifferent
upsamplingtechniques,andthatforx8
weshouldlearnthefullyimagedirectly,because
learningax4followedbyax2tendtolead
tothespuriousadditionofdetailsthatdonotexistinthe
originalhigh-resolutionimage.
Alloftheseideasweredevelopedoverthecourseofap-
proximately8weeksbyateamof5undergraduateswithno
previousexperienceinimageprocessing.
Ourentriesinthe2018NTIREsuperresolutioncompe-
tition[13]achievedseventhplaceinTrack2(realisticmild
adverseconditions),seventhplaceinTrack3(realisticdif-
andsecondplaceinTrack1(classicbicubicdown-
sampling).
Track1
Track2
Track3
PSNR
25.433
23.374
21.658
SSIM
0.7067
0.6252
0.5400
Table1:CompetitionResult
2.PreviousWork
Manyapproachestosingle-imagesuper-resolutionare
basedondifferentmethodsofimageupsampling.Inpar-
ticular,nearest-neighborsupsampling(inwhicheachun-
knownpixelintheupsampledimageisassignedthevalue
ofitsnearestknownneighbor)andbicubicupsampling(in
whicheachunknownpixelintheupsampledimageisas-
signedavalueinterpolatedfromitsnearestknownneigh-
bors)arepopularmethodsforbasicupsampling[2,3].
Thesemethods,whilesimpleandcomputationallyef
donotproviderealistichigh-resolutionimages.Moread-
vancedmethodsattempttobuildamapbetweenlowresolu-
tionimagesandhighresolutionimagesthroughavarietyof
differenttechniques.Sometechniquesincludefrequency-
domainmethodssuchasaliasremoval[14],recursiveleast
squares[7],andmultichannelsamplingtheoremmethods
[15],aswellasspatial-domainmethods,suchasiterated
back-projection[6],jointMAPrestoration[4],andadaptive
[10].
Neuralnetworkshaverecentlybeensuccessfulforim-
ageprocessingtasks,andthroughapplicationofclassi-
calResNetarchitectures,Ledigetal
:
createdonesuccess-
fulexampleofaconvolutionalneuralnetworkforsuper-
resolution,calledSRResNet[8].Theirworkshowedthat
theuseofresidualblocksimprovedperformanceonsuper-
resolutiontasksovermoretraditionalconvolutionalneural
networkarchitectures,andhasbecomethebasisformany
futurearchitecturesforsuper-resolution.Limetal
:
thenim-
provedonthiswiththeirEDSRmethodbyremovingbatch
normalization,usinganL1ratherthanL2lossfunction,and
addingdepthtothenetwork[9].Whilethesemodelshave
seensomesuccessinthesuper-resolutiontaskfor`clean'
images(thatis,imagesthathavebeenbicubicallydown-
scaledwithnofurtherdegradations),theydonotshowgood
resultsforimageswithnoise,blur,orotherdegradations.
Afewrecentinterestingsuper-resolutiontechniques
havebeensuggestedfordegradedimages.Zhangetal
:
[18]
suggestedusingCNNdenoisersasamodularpartofmodel-
basedoptimizationmethodstoperformvariouscomputer
visiontasksincludingsuperresolution.Shocheretal
:
[12]
proposedanunsupervisedapproachthattrainsanimage-
CNNattesttimethatlearnstousetherepetitive
structureofimagestoindetailswheretherepreviously
werenone.
Otherneural-networkbasedmethods,suchasgenera-
tiveadversarialnetworks[8],haveshownsuccessinsuper-
resolutionasmeasuredbyhumanviewers.However,these
networksachievevisualeffectssuitableforhumanviewing
by`hallucinating'featuresfromthelowresolutionimage
thatarenotnecessarilyintheoriginalimage,butwouldbe
believablegiventhelowresolutionimage.Assuch,theyare
notaswellsuitedfortasksthatmaximizesimilaritytothe
originalhighresolutionimage,suchasPSNRandSSIM.
Themethodsintroducedintothisworkaredifferentin
thattheyheavilyleveragepriorknowledge:DNSRlever-
agestheknowledgethatdenoisingbeforeupsamplingis
helpful,whileADRSRusesapyramidofdownsampledim-
agestoborrowinformationatbroaderscales.Theideas
withinADRSRandDNSRcanbecombinedwithanyneu-
ralnetworkapproachestodenoisingandsuper-resolutionin
ordertoincludedomainknowledge.
3.Challenges
Whenapproachingallthreesuper-resolutiontracks(cor-
respondingtonon-noisyandnoisyimages),weencountered
multiplechallenges.
First,therewerechallengesthatweretothe
competitionitself.Onesuchchallengewasthatof
model
validation
,becausethePSNRvaluesofouralgorithmvar-
iedwildlybetweenimages(seeFigure1).Dependingon
which100-imagesubsetweusedforvalidation,average
PSNRvaluesrangedfrom22to27.Thismadeitdif
4322
tocompareourresultstoothers'andrequiredustoa
validationsetof100imagesthroughouttraining.
Figure1:VaryingPSNRbetweenouralgorithm'simages
forTrack2
Particularlyfornoisyimages,itisverydiftoavoid
amplifyingthenoisewhileupsampling
.Severalofthetech-
niquesweintroduceherewereusefulforthis,particularly
thedenoisingandupsamplingnetworkDNSRforTracks2
and3.Evenwithoutnoise,artifactstendtoappearwhen
upsamplingbyafactorofeight.
Mosttraditionaldenoisersrequiresomeknowledgeof
thenoiseitself,normallythestandarddeviation.Touse
anyofthesedenoisers,itwasimperativeto
reverseengi-
neer
thenoise.Wetookapproximatelyareasofvarious
imagesandconsideredthedifferencebetweenthedegraded
lowresolutionimagesanddown-scaledversionsofthehigh
resolutionimages.Becauseablurkernelhasnoeffecton
regionsofanimage,thisdifferenceshouldbeagood
approximationofthenoise(seeFigure2).
Mostpriorconvolutionalnetworksforsuper-resolution
tendtofocusonincreasingtheresolutioninlocalareas;
however,thisapproachdoesnot
takeintoaccountmore
globalpatterns
(suchaszebrastripes).Somerecentwork
[5,12]haveaimedtosolvethisprobleminotherpromis-
ingways,andwepresentanewmethodforhandlingthis
(ADRSR)inwhatfollows.
Figure2:Histogramofnoisefromtwoimages
4.ADRSR:Atypeofarchitecturethatpre-
servesglobalstructure
Figure10showsthetypesofproblemsthatcanarise
fromEDSRandsimilarSRalgorithms.Thesealgorithms
considerlocalimagepatches,anddonotaimtoreconcile
themwithlarger-scalepatternsthatcrosscutintodifferent
patches.Bothincreasingthedepthofthenetworkandin-
creasingthesizeofeachkernelallowsthenetworktoin-
cludelargerscalepatterns.However,theseapproachesare
eitherhardtotrain,ordonotconvergeatall.Thus,we
reasonedthattheselargerpatternscouldbedetectedeven
byusingasmallerkernelonadownsampledimagewith-
outlossofinformation;thexibilityafforded
byalargenumberoflargerkernelsmaybeunnecessaryto
capturethisinformation.
Thearchitecturethatweintroduceforpreservingglobal
structureispresentedinFigure3,calledAutomatedDe-
compositionandReconstructionforSR(ADRSR).The
originalimageisdownsampledseveraltimes,witheach
downsampledimagebeingfedthroughaparallelsuper-
resolutionnetwork.Thispyramidrepresentationforthein-
putallowsustocreatethatcapturepatternsfromthe
originalimageatvariousscales.Wetheniterativelycom-
binetheinformationfromthevariousupscaledimagesto
produceamoreaccurateimagethatrespectsglobal
structure.Whenrunningthenetworkforwardonanewim-
age,itwouldstartfromthecoarsestscale,anditeratively
addmoredetailonthescales.
InFigure3,theSRnetworklabeledinthecanbe
replacedwithanySRnetwork.
WetrainedADRSRtoperformx8upscalingusinga
baselineEDSRupscalerasthemodularsuperresolution
network.WeinitializedthebottomcopyofEDSRwith
ourfullytrainedbaselinemodel,andtheniterativelytrained
eachsuccessivelevelbytemporarilyremovingalllevels
aboveit,anddirectlyoutputtingtheresultofthatlevel
(seeFigure3).Whiletrainingalevel,wefrozeallweights
exceptforthoseinthesuperresolutionnetworkinthatlevel
andtheweightsoftheconvolutionallayerthatcombinesthe
resultsofthecurrentlevelwiththeresultfromtheprevious
level.Aftertrainingeachleveltoconvergence,weunfroze
alloftheweightsandtrainedtheentirenetworkatonce,
whicheliminatedsomeblockyartifactsthatappearedasa
resultoftheupscalingprocess(seeFigure4).Theresultsof
thiscanbeseeninTable2andFigure5.
Algorithm
EDSR
ADRSR
PNSR
25.49
25.38
SSIM
0.6930
0.6898
Table2:ComparisonofADRSRtobaselineEDSR
WhiletheresultsdonotshowanydifferencefromEDSR
4323
Figure3:ADRSRNetworkwithax4superresolutionnetwork
intermsofnumericalperformancemetrics,thenetwork's
multiscalereconstructionwasintuitiveandinteresting.It
ispossiblethatthisarchitecturecouldbeusefulforother
applicationsbesidesPSNR/SSIMoptimization.
5.DNSR:Atypeofarchitecturefordenoising
withlowinformationlossforSR
AstheprincipalchallengeforTracks2and3isnoise,
weconsideredthreepossibleapproachesfordealingwith
thenoise:

(Baselinesimpleapproach).Thesimplestapproachis
tomanuallypreprocesstheimageswithanoisereduc-
tionalgorithm,andthentrainasuperresolutioncon-
volutionalnetworkonthedenoisedimages.

(SRwithoutdenoising).Allowtheresidualblocksin
asuper-resolutionnetwork(suchasEDSR)tosimulta-
neouslydenoisetheinputimagesandextractfeatures.
Thatis,wedirectlytrainEDSRonthenoisydata.

(DNISR,DNSR)Aftertrainingthedenoisingnetwork
andSRnetworkseparately,weconcatenatethem,and
thencontinuetotrainthemtogetherasasinglenet-
work.DNISRandDNSRdifferinthewaythatthey
concatenatethetwonetworksduringthetraining
stage,seeFigure6.
Thebaselineapproachallowedustoincorporatedo-
mainknowledgeaboutthenoise,butperformedpoorlydue
totheinformationlosscausedbythedenoiser.Thesec-
ondapproach,ontheotherhand,didnottendtosufferfrom
informationloss.However,itwasnotpossibletoincorpo-
rateanydomainknowledgeabouttheproblem(forinstance
thattheimageneedstobedenoised)intothenetwork.The
thirdandfourthapproachessolvedbothproblems.They
allowedustoincorporatedomainknowledgeintothenet-
work,sincewecouldexplicitlytrainthedenoisingnetwork.
DNSRtrainsthedenoiserandsuper-resolutionnetworkto-
getherattheendtominimizeinformationlossoftheover-
allprocedure.Thisapproachisalsoadvantageouswhen
4324
Figure4:Beforeunfreezingalloftheweights,ADRSR
tendedtoproduceblockyartifacts(top-right),howeveraf-
terunfreezingalloftheweightsandtrainingforafewmore
epochs,theartifactsdisappeared(bottom-right).
givenasmallnumberofimageswiththesamedegrada-
tionsapplied.Afterreverse-engineeringthenoise,external
datacanbeusedtotrainthedenoisingandsuper-resolution
networks,andthentheentireconcatenatednetworkcanbe
trainedonthedatasettoallowthenetworktocorrectany
additionaldegradations.
Basedonourapproach,weconstructedtwomod-
els,whichperformtheconcatenationintwodifferentways.
ThewasDNISR(DeNoisingIntoSuper-Resolution),
whichrantheimagethroughadenoisingnetwork(weused
DNCNN[17]),producingalow-resolutionnoise-reduced
image,andthenrantheresultthroughthesuper-resolution
network(weusedEDSR)toproduceahigh-resolutionim-
age.
Wefoundausefultricktofurtherminimizeinforma-
tionlossinDNISR:wefedtheoriginalimageintothe
super-resolutionnetworkalongsidethenoise-freeimage
withweightsinitializedto
0
.
Thesecondapproach(DeNoisingandSuper-Resolution
ŒDNSR)usedamorecomplicatedconcatenationproce-
dure.Itremovedtheinformationbottleneckbetweenthe
twonetworksbycombiningthetaillayerofthedenoiser
(whichmapped
256
channelsto
3
)andtheheadlayerof
theSRnetwork(whichmapped
3
channelsto
256
)intoa
singlebridgeconvolutionallayerthatmappeddirectlyfrom
thenumberoffeaturemapsinthedenoisertothenumber
offeaturemapsintheSRnetwork.UnlikeDNISR,there
isnodenoisedimageproducedbeforeenteringthesuper-
resolutionnetwork.SeeFigure6forthearchitecture.We
submittedthesamemodelforTracks2and3oftheNTIRE
2018competition.
Table3andFigures7and8showaPSNRcomparison
forEDSR,DNISR,andDNSR.Foravisualcomparisonof
theimagesproducedbyeachalgorithm,seeFigure14.
Algorithm
BICUBIC
EDSR
DNISR
DNSR
PNSR
23.47
24.49
24.52
24.90
SSIM
0.7333
0.7925
0.7940
0.7956
Table3:ComparisonofresultsfromEDSRandourdenois-
ingnetworks.Thenumbersreportedwerecomputedonthe
DIV2K[1]validationdataset.
6.GeneralTricksandInsights
Wediscoveredseveraltricksthatcanbeusedanytime,
withalmostanynetworkarchitecture.Toseetheresults
thesetrickshadonupscalingimagesbyafactorof
8
,see
Figure14.

RGBLayerShufInadditiontoandrotat-
ingtheimagepatchesduringtrainingandgeneration,
werandomlyshufthered,green,andbluelayers.
Thisimprovedouroverallmodelbyasmallamount.
Thistrickisapplicabletoanyconvolutionalstructure.
Figure13showstheeffectoftest-timeRGBShuf

Per-ImageMeanShift:Insteadofcalculatingtheav-
eragemeanthroughoutalloftheimagesandnormal-
izingbythatvalue,asintheoriginalEDSRpaper,we
insteadnormalizedeachindividualimagepatchduring
trainingbysubtractingitsmean.

DifferentUpsamplingTechniques:ForTrack1,we
startedbyusingsub-pixelshifttoupscaletheimage.
Inaddition,toupsamplebyafactorof
8
,weconcate-
natedthree

2
upsamplers,asintheoriginalEDSR
paper.Usingthisapproach,weranintoartifactsin-
ducedbytheupscaling(seeFigure9).Thesearti-
factswerediminishedbyswitchingtheupsampling
methodtoTransposedConvolutionupsampling.How-
ever,evenwiththesub-pixelshiftupscaler,theprob-
lemwentawaywhenweswitchedtodirectlylearning
a

8
upscalerinsteadofthreeconcatenated

2
up-
scalers.
Inourmethod,wefoundthatdirect

8
upscaling
combinedwiththesub-pixelshiftupscalerproduced
imageswithhigherPSNRvalues.However,thecon-
catenated

2
upscalersseemedlesspronetocreating
artifactsduetoantialiasing(seeFigure10).
4325
Figure5:ComparisonofbaselineEDSRwithADRSR.AlthoughthePSNRvaluesofEDSRandADRSRaresimilar,ADRSR
tendstoproducesharperlinesandedgesthroughoutthevalidationset.

ResidualScalingFactor:InEDSR,eachresiduallayer
ismultipliedby
0
:
1
attheend.Insteadofhardcoding
thisparameter,weallowedittobeafreevariablethat
couldbetrained.

EdgeLoss:Weattemptedtoaddanedge-losscom-
ponenttothelossbyapplyingaSobeltoboth
theupscaledandground-truthimages,andcomparing
those.However,thisdidnotimproveonourprevious
model.

KernelSize:Wetriedvariouskernelsizes,however
2

2
producedworseresultsandwecouldnotsuc-
cessfullytrainthenetworkwiththe
4

4
and
5

5
kernelsizes.
Figure11showsacomparisonofconvergenceratesfor
baselineEDSR,EDSRwithperimageintensityshift,and
EDSRwithdynamicresidualscalingfactors.Alltraining
startedwithrandomlyinitializedweights.Usingper-image
meanshiftgivesahigherinitialPSNRandfasterconver-
gence.
Table4andFigure12showaPSNRcomparisonfor
VDSR,EDSR,andourimprovedEDSRmodel.Foravi-
sualcomparisonoftheimagesproducedbyeachalgorithm,
seeFigure14.
Algorithm
EDSR
ImprovedEDSR
VDSR
BICUBIC
PNSR
25.49
25.60
24.70
23.69
SSIM
0.6930
0.6974
0.6580
0.6291
Table4:ComparisonofourimprovedEDSRalgorithmto
severalbaselines.Theimprovementscouldhavebeenmade
toanySRalgorithmbesidesEDSR.Thenumbersreported
werecomputedontheDIV2Kvalidationdataset.
Figure13showsthePSNRincrementacrossthe100
imagesfromtheDIV2Kvalidationsetafterapplyingonly
RGBShuftoEDSR.In97outofthe100cases,there
wasaboostinPSNR.
4326
Figure6:ArchitecturesforDNSRandDNISR.
Figure7:PSNRdifferencebetweenDNISRandEDSR
(sortedbydifferenceinPSNR)onthe100imagevalidation
setfromDIV2K[1].
7.Conclusion
Wediscussedtwonewnetworkarchitecturesfordenois-
ingandpreservinggeneralstructureinimagesduringsuper-
resolution,aswellasatoolboxoftricks.Thevastmajority
ofthedescribedherewerenotimplementedintime
forthecompetitiondeadline.Ourhigh-scoringentriesare
mostlyaresultofthetoolboxoftricksdiscussedabove.We
havenoticedsubstantialimprovementfromourcompetition
entriestotheresultsreportedinthispaper.
Ourcodeisavailableat:
https://github.
com/websterbei/EDSR_tensorflow
and
https:
//github.com/nikhilvravi/DukeSR
.
Figure8:PSNRdifferencebetweenDNSRandEDSR
(sortedbydifferenceinPSNR)onthe100imagevalidation
setfromDIV2K[1]
Figure9:Theupscalerusingsub-pixelshift(top-right)has
clearchromaticartifacts,whiletheupscalerusingtrans-
posedconvolutionalupscaling(bottom-right)doesnot.
References
[1]
E.AgustssonandR.Timofte.Ntire2017challengeonsin-
gleimagesuper-resolution:Datasetandstudy.In
TheIEEE
ConferenceonComputerVisionandPatternRecognition
(CVPR)Workshops
,July2017.
[2]
R.S.BabuandK.S.Murthy.Asurveyonthemethodsof
super-resolutionimagereconstruction.
InternationalJournal
ofComputerApplications
,15(2),February2011.
4327
Figure10:Diagonallinescreatedbysomeupscalingmeth-
odsduetoanti-aliasing
Figure11:ConvergenceratesforbaselineEDSR,EDSR
withperimageshiftanddynamicscalingfactor.The
isplottedwithrollingmeanof50.
[3]
A.GilmanandD.G.Bailey.Nearoptimalnon-uniformin-
terpolationforimagesuper-resolutionfrommultipleimages.
ImageandVisionComputingNewZealand,GreatBarrier
Island,NewZealand
,pages31Œ35,2006.
[4]
R.C.Hardie,K.J.Barnard,andE.E.Armstrong.Jointmap
registrationandhigh-resolutionimageestimationusingase-
quenceofundersampledimages.
IEEEtransactionsonIm-
ageProcessing
,6(12):1621Œ1633,1997.
[5]
M.Haris,G.Shakhnarovich,andN.Ukita.Deepback-
projectionnetworksforsuper-resolution.
Conferenceon
ComputerVisionandPatternRecognition
,2018.
[6]
M.IraniandS.Peleg.Motionanalysisforimageenhance-
ment:Resolution,occlusion,andtransparency.
Journalof
VisualCommunicationandImageRepresentation
,4(4):324Œ
335,1993.
[7]
S.Kim,N.K.Bose,andH.Valenzuela.Recursiverecon-
structionofhighresolutionimagefromnoisyundersampled
multiframes.
IEEETransactionsonAcoustics,Speech,and
SignalProcessing
,38(6):1013Œ1027,1990.
[8]
C.Ledig,L.Theis,F.Huszar,J.Caballero,A.Cunning-
Figure12:PSNRdifferencebetweenourimprovedEDSR
andbaselineEDSR(sortedbydifferenceinPSNR)onthe
100imagevalidationsetfromDIV2K[1].
Figure13:PSNRincrementfromtesttimeRGBShuf
(sortedbydifferenceinPSNR)
ham,A.Acosta,A.Aitken,A.Tejani,J.Totz,Z.Wang,and
W.Shi.Photo-realisticsingleimagesuper-resolutionusing
agenerativeadversarialnetwork.
CoRR
,abs/1609.04802,
September2016.
[9]
B.Lim,S.Son,H.Kim,S.Nah,andK.M.Lee.Enhanced
deepresidualnetworksforsingleimagesuper-resolution.
CoRR
,abs/1707.02921,2017.
[10]
A.J.Patti,A.M.Tekalp,andM.I.Sezan.Anewmotion-
compensatedreduced-ordermodelkalmanforspace-
varyingrestorationofprogressiveandinterlacedvideo.
IEEE
TransactionsonImageProcessing
,7(4):543Œ554,1998.
[11]
M.Robinson,S.Chiu,J.Lo,C.Toth,J.Izatt,andS.Farsiu.
Newapplicationsofsuper-resolutioninmedicalimaging.In
Super-ResolutionImaging
,chapter1,pages383Œ412.CRC
Press,2010.
[12]
A.Shocher,N.Cohen,andM.Irani.Zero-shot
4328
Figure14:Comparisonofdifferenttechniquesforupscalingimagesbyafactorof8(left)andupscalingnoisyimagesbya
factorof4(right).Thevisualdifferencesbetweentheimagesareespeciallypronouncedinthelastimagesofeachcolumn,
wherethefoldsintheleavesaremuchclearer.
super-resolutionusingdeepinternallearning.
CoRR
,
abs/1712.06087,2017.
[13]
R.Timofte,S.Gu,J.Wu,L.VanGool,L.Zhang,M.-H.
Yang,etal.Ntire2018challengeonsingleimagesuper-
resolution:Methodsandresults.In
TheIEEEConference
onComputerVisionandPatternRecognition(CVPR)Work-
shops
,June2018.
[14]
R.Tsai.Multiframeimagerestorationandregistration.
Ad-
vanceComputerVisualandImageProcessing
,1:317Œ339,
1984.
4329
[15]
H.UrandD.Gross.Improvedresolutionfromsubpixel
shiftedpictures.
CVGIP:GraphicalModelsandImagePro-
cessing
,54(2):181Œ186,1992.
[16]
L.Yue,H.Shen,J.Li,Q.Yuan,H.Zhang,andL.Zhang.
Imagesuper-resolution:Thetechniques,applications,and
future.
SignalProcessing
,128:389Œ408,November2016.
[17]
K.Zhang,W.Zuo,Y.Chen,D.Meng,andL.Zhang.Be-
yondagaussiandenoiser:ResiduallearningofdeepCNN
forimagedenoising.
CoRR
,abs/1608.03981,2016.
[18]
K.Zhang,W.Zuo,S.Gu,andL.Zhang.Learningdeepcnn
denoiserpriorforimagerestoration.
CoRR
,abs/1704.03264,
2017.
4330
"
52,Anticipation in Human-Robot Cooperation: A Recurrent Neural Network Approach for Multiple Action Sequences Prediction,http://arxiv.org/pdf/1802.10503v3.pdf,https://github.com/pschydlo/ActionAnticipation,"AnticipationinHuman-RobotCooperation:ARecurrentNeural
NetworkApproachforMultipleActionSequencesPrediction
PaulSchydlo
1
,MirkoRakovic
1
;
2
,LorenzoJamone
3
andJos
´
eSantos-Victor
1
Abstract
ŠClosehuman-robotcooperationisakeyenabler
fornewdevelopmentsinadvancedmanufacturingandassistive
applications.Closecooperationrequirerobotsthatcanpredict
humanactionsandintent,understandinghumannon-verbal
cues.Recentapproachesbasedonneuralnetworkshaveled
toencouragingresultsinthehumanactionpredictionproblem
bothincontinuousanddiscretespaces.Ourapproachextends
theresearchinthisdirection.
Ourcontributionsarethree-fold.First,wevalidatetheuse
ofgazeandbodyposecuesasameansofpredictinghuman
actionthroughafeatureselectionmethod.Next,weaddress
twoshortcomingsofexistingliterature:predictingmultipleand
variable-lengthactionsequences.Thisisachievedbyapplying
anencoder-decoderrecurrentneuralnetworktopologyinthe
discreteactionpredictionproblem.
Inaddition,wetheoreticallydemonstratetheimportanceof
predictingmultipleactionsequencesasameansofestimating
thestochasticrewardinahumanrobotcooperationscenario.
Finally,weshowtheabilitytoeffectivelytraintheprediction
modelonanactionpredictiondataset,involvinghumanmotion
data,andexploretheofthemodel'sparameterson
itsperformance.
I.INTRODUCTIONANDRELATEDWORK
Inaworldwithagrowingnumberofautonomoussys-
temsandmovingtowardsthecoexistenceandcooperation
betweenhumansandsophisticatedrobots,itiscrucialto
enablesystemstounderstandandpredicthuman
behaviour.Thisabilityapplicationsinareassuchas
cooperativerobotics[1],[2],auto-mobilesafety[3],elderly
care[4],amongmanyothers[5].
Inadditiontotheuseofspeechforcommunicatingand
coordinatingtheirnextactions,humansrelyextensivelyon
non-verbalcuesforactionandmovementprediction[6].
Situationswherefastcooperationisessential,forexample
cooperativeassembly,requiretheunderstandingofsubtle
non-verbalcues[2]aboutthehumanintentionandfutureac-
tion.Inthesescenarios,itisnotenoughtomerelyrecognize
thecurrentaction.Instead,itisfundamentaltopredictactions
andanticipatetheintentinordertoguaranteeseamless
cooperation[7].
A.Non-verbalcues
Thereareseveralnon-verbalcuesthatenablehumanaction
prediction[8],[9].Thispapertakesintoaccounttwoof
them:gazeandbodyposture.Gazeisimportant,asithas
botharoleinsocialcommunicationinconveyingturntaking
behaviour[10]orattentioninconversation,butatthesame
1
InstituteforSystemsandRobotics,InstitutoSuperiorT
´
ecnico,Univer-
sityLisbon,Portugal
2
FacultyofTechnicalSciences,UniversityofNoviSad,NoviSad,Serbia
3
QueenMaryUniversity,London,UnitedKingdom
timeitisdeeplyrelatedtotheagent'sTheoryofMind[11]
aboutthecollaborationpartnerandtheactiongoals
throughbothvisuo-motorcoupling[12]andattention[9].
Bodyposture,similarlytogaze,canservebothasocial
andintentionconveyingsignalwhilealsoindicatingpossible
actiontargets.
Pastworkshavefocusedoneithergaze[1]Œ[3]orbody
pose[13]cuesandtheirrelationtoactionrecognitionand
prediction.Bothareimportantinunderstandinghumanbe-
haviourandgiveinformationaboutthehuman'sactiongoal.
Researchonnon-verbalcuesinhuman-robotcooperation
hasalonghistory,includingthebulkofworkonmirror
neurons[14]anditscomputationalandroboticmodelsand
implementations[15].RelevantworkincludeAdmoni[5]
useofhumangazeasameansofestimatingthehuman
intent,modellingtherelationbetweenthegazeandtheaction
goalbytheirrelativedistance.Huang[1]the
importanceofgazefeatures,successfullydemonstratingthe
importanceofgazebyproactivelyplanningactionsaccording
tothehumanintent.
B.Predictionmodels
Humanactionpredictioncanbesolvedatdifferentlevels
ofabstractionandisconcernedwithestimatingaprobability
distributionoverthesetofnextpossibleactions.
Atahigherlevelofabstraction,modelscanpredictactions
inadiscretespace[3],[16]wheretheactionsaresymbolicin
natureandcanrepresentunderlyingmovementpatterns,e.g.
ﬁpress-buttonﬂorﬁgrab-objectﬂ.Onalowerlevelofabstrac-
tion,movementcanbedirectlyanticipatedinacontinuous
space[17],e.g.humanwalkingtrajectories.
Predictingincontinuousspacehasbeenaddressedin
thecontextofbodyposeandhumantrajectoryprediction.
RelevantworkincludetheuseofRecurrentNeuralNetworks
byMartinez[17]asameansofpredictingcoherentfuture
jointtrajectories.
Thedualproblemisactionpredictionindiscreteout-
comespace.RelevantworkincludeaConditionalRandom
FieldbasedapproachbyKoppula[18]tocapturetemporal
dependenciesandSaponaro'sHiddenMarkovModelbased
approach[19].Recently,RecurrentNeuralNetworks,with-
outlimitingMarkovianassumptions,haveshownexcellent
results[16],[17],[20].Relevantworkinclude,thestructural
RNNasameansofencodingpastcontextualinformation
andpredictingaednumberofstepsinthefuturebyJain
[16].Whilethehashadarapidevolutioninthelast
coupleofyears,therearetwoshortcomingsintheliterature
thispaperaddresses.
arXiv:1802.10503v3  [cs.HC]  18 Feb 2019Theisconcernedwithpredictingaedversusa
variablenumberofstepsintothefuture.Whilemodelslike
[16]havearemarkableabilitytocondensecontextualpast
information,theirscopeislimitedtoedstepaheadpre-
dictionlength.Thispaperaimsatextendingdiscriminative
recurrentmodelsinasettingwithvariable
lengthactionsequenceprediction.
Thesecondshortcomingisrelatedtothesinglefuture
actionsequenceversusmultiplefutureactionsequences.
Whilemodelsliketheoneintroducedin[17]areableto
effectivelyuserecurrentmodelstopredictavariablenumber
ofstepsintothefuture,theirscopeislimitedtoaregression
setting,wheresamplingmultiplefutureactionsequencesis
anon-trivialproblem.Thispaperexploresamultiplefuture
actionsequencepredictorinthesetting.
C.Contributions
Themaincontributionsofthispaperarethefollowing:

Quantifyingthe
relativeimportanceofposeandgaze
featuresinanintentionrecognitionscenario.

Extendingrecurrentneuralnetworkedstepaction
predictionwith
variablelengthactionprediction
.

Introducingthesimultaneouspredictionof
multiple
futureactionsequences
.
II.APPROACH
Ourworklooksattheactionpredictionproblemfroman
end-to-endperspective,startingwiththeproblemofnon-
verbalcuesselectionandmovingontodevelopanaction
sequencepredictionmodel.Keepinginmindthegoal,
predictingfuturehumanactiongivenasequenceofpastnon-
verbalcuessuchasgazeandpose,thissectionisorganized
inasequentialbottom-uporder.
First,weaddresstheissueofestablishingaquantitative
metricforassessingtherelativeimportanceofposeandgaze
features.Then,inSectionII.B,weintroducethemultiple
actionsequencepredictionmodelwhichisoneofthekey
contributionsofthispaper.Predictingactionsequencesintro-
ducescomplexityissueswhicharehandledinSectionII.C.
Finally,inSectionIIIweusethedistributionoverfuture
actionsequencessampledfromthemodel,introducedin
SectionII,toestimatetheexpectedfuturerewardinahuman-
robotcooperationscenario.
A.Featureimportance
Thissectionseekstointroduceaquantitativemetricforthe
relativegazeandbodyposecuesimportance,twocommonly
usedfeaturesinnon-verbalcommunication[8].Selectingthe
rightfeaturesisanimportantsteptoreducethecomplexity
andincreasetherobustnessofourmodels.
Therearedifferentfeatureselectionmethodswhichcanbe
categorizedinto

,
wrapper
and
embedded
classes[20].
Sincetherelationbetweenthefeaturesisunknown,itis
assumedtobenon-linearinnature.Followingthenon-
linearityassumption,thefocusofthissectionwillbeon
the
wrapper
classoffeatureselectionmethods.Thisclassof
methodscapturesnon-linearrelationbetweenthevariables
throughablack-boxmodel.Itstartsbytrainingthemodel
onsubsetsofthefeaturespaceandthenranksthefeatures
accordingtothemodel'saccuracy[20].
Inthecaseofthispaper,theblack-boxmodelisthe
intentionrecognitionmodel,aRecurrentNeuralNetwork
(RNN)sequencetosequencemodel.Thestructureofthe
modelisbyanembeddinglayer,whichateverystep
transformsthefeaturevectorintoanintermediaryrepresenta-
tion,actingasaninputtothemodel'sRNN.Foreveryinput,
thisRNNreturnsadiscretedistributionoverintentions.This
distributionisobtainedbyprojectingtherecurrentneural
network'sinternalstateandnormalizingitthroughasoftmax
layer.
Fig.1:
Intentionrecognitionmodel.
Thismodelmaps
asequenceofinputfeaturestoasequenceofdiscrete
distributionsovertheactionvocabulary.
Thepredictionaccuracyofthemodelwithandwithout
agivenfeaturecanbeconsideredaproxyforthefeature's
addedinformation.Havingestablishedaquantitativemeasure
ofthegazeandposefeatures'importance,thenextsection
introducesthepredictionmodel.
B.Predictionmodel
Thissectionintroducesthediscreteencoder-decoderre-
currentneuralnetworktopologywhichseekstosolvethe
shortcomingsenumeratedinsectionII.Thepartof
themodelisacontextualinformationencoder.Theencoder
condensespastinformationintoaedlengthcontextvector
throughaLongShortTermMemory(LSTM)cell.Theem-
beddingisafullyconnectedlayer(FeatureVectorDim

50),
whereFeatureVectorDimisthesizeofthefeaturevector.
Theembeddinglayerincludesdropoutswhichactasa
regularizationtothemodel[21].TheencoderLSTM'shidden
statedimensionis20.Thiscontextvector,theinternalstate
oftheencodingLSTM,istheinitialstateofthesecondpart
ofthemodel,thedecoder.
Thedecoderisresponsibleforgeneratingacoherentfuture
sequenceofactions.Ateachstepthedecoder,anLSTMcell,
returnsadiscretedistributionoverpossiblefutureactions.
Thisdistributionisobtainedbyprojectingthedecoder's
internalstateandnormalizingitusingasoftmaxlayer.The
decodingprocesssamplesanactionfromthedistribution
andfeedsitbackasaninputtothenextdecodingiteration.
Theprojectionisafullyconnectedlayer(HiddenStateDimx
VocabDim),whereHiddenStateDimisthesizeofthehidden
state,20,andVocabDimthedimensionoftheactiondiscrete
possibleactionsvocabulary,11.ThedecoderLSTM'shidden
statedimensionis20.
Fig.2:
Encoder-decodermodel.
Theleftpartsummarises
pastinformationintoaedlengthcontextvector.Rightpart
expandsthiscontextvectorintofutureactionsequences.
ThemodelistrainedwiththeAdamalgorithmusinga
sequentialcrossentropyloss.Thecrossentropycost(1)isa
measureofdifferencebetweentwodistributions:predicted
distribution,p,andreferencedistribution,r.Thediscrete
distributionisoverthelimitedsetofpossibleactions,
A,whereeverypossibleaction,a,isaninstanceofthisset,
p(a)andr(a)respectivelythepredictedandreference
probabilityoftheaction,a.Thesequentialcrossentropyis
obtainedbysummingthecrossentropy,H,costoverthe
predictionsteps:
H
(
p;r
)=

X
a
2
A
p
(
a
)log(
r
(
a
))
:
(1)
Aftertraining,thedecodingprocessallowsforvariable
lengthactionsequenceprediction.Expandingeverypossible
futureactionsequencesisNPhardandcomputationally
intractable.Thenextsectionlooksmorecloselyatthisissue
andintroducesonepossiblesolutiontotheproblem.
C.Complexityissues
Theprevioussectionhintsatthecomplexityunderlying
thedecodingprocess.Ateverydecodingstep,thedecoder
samplesoneormoreactionsfromtheoutputdistributionas
possibleactionsatagiventimestep;itthenexpandsthese
actionsbybranchingandfeedingthemindividuallyasinput
tothenextdecoderiteration.Therearetwostrategiesthat
couldbeappliedtothisdecodingprocess.
Naivelyexpandingthespaceofallpossibleactionse-
quencesandselectingthemostprobableactionsequenceat
theendseemslikeareasonableidea.Nevertheless,expand-
ingtheactionsateachstepresultsinavocabularysized
multiplierinthenumberofpossibleactionsequencesat
everypredictionstep.Intermsofcomplexitythismeansthat
thenumberofactionsequencesincreasesexponentiallywith
thenumberofpredictionsteps.Consideringa10actions
vocabularysize,thedecodingstepresultsin10action
sequences,expandingthe10actionsequencesresultsin100
possibleactionsequencesforatwostepaheadprediction,a
Nstepaheadpredictionwouldresultin
N
10
possibleaction
sequences.
(a)
(b)
(c)
Fig.3:
Searchmethodscomparison.
a)Exhaustivesearch
expandsallpossibleactionsequences.b)Greedysearchpicks
themostprobableactionateverystep.c)Beamsearchkeeps
asetofthebestKactionsequences,expandingandpruning
thesetateverystep.
Greedilyexpandingonlythebestoption,couldbea
solutiontotheexponentiallyexpandingtrajectoryspace,
neverthelessithastheshortcomingthatthismethodonly
returnsoneactionsequenceprediction.
Acommonsolutiontothesetwoproblemsistheimple-
mentationofa
beamsearch
baseddecoder[22].Thismethod
keepsasetofthetopKbestfutureactionsequencesat
everydecodingstep,expandingbytheactionvocabulary
sizeandpruningtheactionsequencesetbacktothetopK
futureactionsequences.TheresultisasampleofthetopK
mostprobablefutureactionsequencesorderedbylikelihood.
ThesetrajectoriesarecalledbeamsandKisthebeamwidth
parameter.
III.APPLICATIONSCENARIO
Anticipatingasetofpossiblefutureactionsisimportant
incooperativeassemblyscenarios,wheretwoagentswork
togetherinafastpacedjointactionsetting.Thisscenario
aimstoclarifytheimportanceandsomecaveatsoftheaction
predictionprobleminhumanrobotcooperationscenarios.
Thissettingisbyasetofpossibleworldstates,
S
,
humanandrobotactionpairs,
A
:
(
a
H
;a
R
)
,transitionbetween
statesasafunctionofthecurrentstateandjointactionpair,
T
(
S;A
)
,andajointimmediaterewardfunction,
R
(
S;A
)
.
Forthesakeofexample,theworldstatecouldbeasetof
pre-conditions,Tasetofaction-effectaxiomsandRareward
functiononthesub-goalcompletion.
Givenaninitialstate,
S
0
,andanactionsequence,
A
,
i.e.aseriesofactionpairs
(
a
H
;a
R
)
atNequidistanttime
steps,thetotalreward,
R
t
,isgivenby(2),where
A
i
and
S
i
correspondrespectivelytothehuman-robotactionpairand
worldstateattimestepiand,Nthenumberoftimesteps:
R
t
(
S
0
;
A
)=
N
X
i
=0
R
(
S
i
;
A
i
)
:
(2)
Inthissetting,therobotselectsanactionsequence,
A
R
,
maximisingthejointreward,
R
,andthehumanaction
sequence,
A
H
,isunknownandnon-deterministicfromthe
perspectiveoftherobot.Therefore,thefuturerewardassoci-
atedtoachosenrobotactionsequence,
A
R
canbeestimated
asanexpectationoverthesetofpossiblehumanactions,
A
H
,
givenby(3),where
p
(
A
H;k
)
representstheprobabilityofa
humanactionsequence,
A
H;K
,
R
(
S
i
;
(
a
H
;a
R
))
,thereward
associatedtothehuman-robotactionpairintheworldstate
S
i
,
#
H
thecardinalityofthesetofpossiblehumanactions
andNthenumberoftimestepsintothefuture:
E

R
t
(
S
0
;A
R
)

=
#
H
X
k
=0
""
N
X
i
=0
R

S
i
;

A
H;k
i
;A
R
i

#
p
(
A
H;k
)
:
(3)
Computingtheexpectation,requiresexpandingallpossi-
bleactionsequences,whichiscomputationallyintractable,
i.e.NPhard.Wewillnowseehowthebeamsearch,
introducedearlier,enablestheestimationofthisreward.
Consideringthesetofmostprobableactionsequencesas
representativeofthefuturehumanbehaviour,thatis,the
distributionhasvariance,wecanapproximatetheex-
pectedrewardthroughabiasedMonteCarloestimation.This
isachievedbysummingandweightingtherewardofagiven
human-robotactionsequencebythehumanactionsequence
probability(4).Increasingthenumberofpredictedhuman
actionsequences,K,approximatestherewardbetterbut
iscomputationallymoredemanding.Here
p
(
b
k
)
represents
theprobabilityofthekthbeam(predictedactionsequence),
while
S
i
,
b
k
i
and
A
R
i
representrespectivelytheworldstate,
theactionperformedbythehumaninthebeamkandthe
robotintheactionsequence
A
R
attimestepi:
E

R
(
S
0
;A
R
)

=
P
K
k
=0
h
P
N
i
=0
R
(
S
i
;
(
b
k
i
;A
R
i
))
i
p
(
b
k
)
P
K
k
=0
p
(
b
k
)
:
(4)
Asthebeamcounttendstothetotalnumberofpossible
actionsequencecombinations,thisexpressionapproximates
theexpectedreward(3).
IV.EXPERIMENTSANDRESULTS
Westartbydescribingthedatasetsusedintheevaluation,
wemoveontocomparethenon-verbalcuesimportanceand
byevaluatingtheactionsequencepredictionmodelon
adatasetthatincludesbodyposeinformation.
A.Datasets
Thefeatureimportanceisevaluatedonacombinedgaze
andskeletondatasetwhichwasacquiredandpublishedin
theISRVislabACTICIPATE
1
project(Fig.4a).Thisdataset
consistsofahumanactor'sgazeandskeletonmovement
whileperformingeitheroneofsixactions(PlaceLeft,Place
Center,PlaceRight,GiveLeft,GiveCenter,GiveRight).
ThisdatasetwasrecordedusingtheOptitrackmotioncapture
system,andPupilLabsbinoculareyegazetrackingsystem,
synchronisedata120Hzfrequency.Thetotalnumberof
actionsequencesis120.Thesequenceshaveanaverage
lengthof220frames.Everysequencecorrespondstoone
actionandislabelledaccordingly.
Themultipleactionsequencepredictionmodelisevalu-
atedontheCAD120dataset(Fig.4b,[23]).Thisdataset
consistsofahumanactor'sskeletonmovementwhileper-
formingasequenceofactionslikeﬁpouringﬂandﬁeatingﬂ.
1
TheACTICIPATEdatasetcanbedownloadedfromthefollowingweb
page:http://vislab.isr.tecnico.ulisboa.pt/datasets/
(a)
(b)
Fig.4:
Datasets.
a)ACTICIPATEmotionandeyegaze
dataset.b)CAD120RGB-Dmotiondataset.
Thisdatasetisofspecialinterestsinceitcoversthescope
ofactionsequencesanditisnotlimitedtooneactionper
videosegment.Itisoneofthefewdatasetswhichhas
avaryingorderofactionsequences.Thisdatasetconsists
ofjointpositionandorientationfeaturesequencestogether
withtherespectiveactionlabelsatasamplefrequencyof
5Hz.Thetotalnumberofactionsequencesis120andthe
sequenceshaveanaveragelengthof25timesteps.
B.FeatureImportance
Inourexperiment,wetrainthemodelonthecom-
binedbodyposeandgazefeaturestothatityields
theexpectedbehaviour.Asthemovementprogresses,the
modelreceivesmoreinformationandtheintention,
correctlyconvergingtothetruelabel,5.Thewholemove-
menttakes220frames(about2seconds).Themodelisable
topredicttheintentiontargetafterseeinglessthanhalfof
thetotaltrajectory,about100frames.
Fig.5:
Actionprobabilitytemporalevolution.
Themodel
startswithuniformprobabilityandafterabout100frames
convergestothecorrectlabel.
Thesecondexperimentisconcernedwithquantifying
therelativeimportanceofthedifferentnon-verbalcuesin
predictinghumanintent.Themodelistrainedontwosets
offeatures:(i)combinedgazeandposecues,and(ii)body
poseonly.Fig.6showsthemodelperformanceunderthese
twoconditionsandtheimportanceofthegazeinformation
forthecorrectpredictionofhumanaction.
Thedifferenceinaccuracybetweenthetwosetsofcues
hintsattheimportanceofgaze.Despitethemodelperform-
ingsimilarlywithandwithoutgaze,theresultsshowthat
gazehasanimportantroleinearlypredictionofhuman
activity.Themodeltrainedonbothgazeandbodyposecues
predictsthecorrectaction92msbeforethemodelwithonly
Fig.6:
Gazeandposeaccuracy.
Accuracyofamodel
trainedon(i)poseonlyfeatures,and(ii)trainedoncombined
gazeandbodyposefeatures.
bodycues.Aninterestingresultisthatthisdelaycoincides
withtherangeofdelaysbetweeneyeandhandmovement
observedinresearchoneye-armmovementcoupling[24].
Havingestablishedtherelativeimportanceofbothgaze
andbodyposefeaturesinactionprediction,inthenextsec-
tionwewillevaluatethemultipleactionsequenceprediction
modelonamulti-actionposefeaturedataset.
C.PredictionModel
Themodeltakestheposefeatures,observedoverthree
timesteps,asinputinordertopredictfutureactionsas
accuratelyaspossible.Wewillinvestigatehowtheprediction
model'sparametersaffecttheperformance.Themodelis
evaluatedontheCAD120dataset,introducedbefore.
PerformancewillbeassessedwiththeF1score[25].The
F1-scoreisevaluatedona4-foldcrossvalidationscheme,
withthescorebeinganaverageoverthefolds'results.
Astherearefoldswithoutinstancesofsomelabel,theF1
scoreiscalculateddirectlyonthetruepositive,falsenegative
andfalsepositiverate(5):
F
1=
2

TruePos
2

TruePos
+
FalseNeg
+
FalsePos
:
(5)
Whilethemodelisdynamicinitsabilitytopredict
variablelengthactionsequences,theaccuracyoftheaction
sequencepredictionisbythepredictionlengththe
modelistrainedon(Fig.7).Thiscorrelationisrelatedtothe
abilityofthedecodertomanageitsinternalstate.Whenthe
networkistrainedonalongfutureactionsequence,itlearns
tokeepandmanagethedecoder'sinternalstate,predicting
longersequenceswithmoreaccuracy.
Thesecondparametertoanalyseisthenumberofbeams
(actionsequences)whichdeterminethespaceofaction
sequencesthatthemodelisabletocapture(Fig.8).The
cumulativesumofthebeams'probabilitiesisameasureof
thesolutionspacethatweareabletocoverwithagiven
numberofbeams.
Thespaceofpossiblesolutionsgrowsexponentiallywith
thenumberofpredictionsteps.Whileabeamwidthof11
beamsisabletocapture100%oftheoutcomeprobability
spaceinaone-stepaheadpredictionscenario,thesame
Fig.7:
Accuracyasafunctionofpredictionlength.
Predictionaccuracyacrosstimestepsispositivelycorrelated
withthepredictionlengththemodelistrainedon.(N
correspondstothepredictionlengthusedfortrainingthe
model,Stepthepositioninthepredictedsequence.)
numberofbeamsonlycapturesaround75%oftheoutcome
probabilityspaceinthetwo-stepaheadpredictionscenario.
Asthesolutionspacegrows,aednumberofbeams
capturesacumulativeprobabilityoutcomespacethatdecays
withthenumberofpredictionsteps.
Fig.8:
Beamcumulativeprobability.
Cumulativeproba-
bilityoftheoutcomespacethemodelisabletocapture.
ﬂNﬂrepresentsthelengthofthepredictedtrajectory,and
ﬂ#Beamsﬂthelengthofthepredictedactionsequences.
Itiswellknownthatthegeneralizationerrorisrelated
tothemodel'scapacity,theabilitytolearncomplexpatterns
[26].Thedimensionalityofthecontextvectorsisaparameter
whichthemodel'scapacity.Increasingthisdimension
reducestheinformationalbottleneck,increasingthemodel's
capacityandasaconsequencethegeneralizationerror.
Increasingthegeneralizationerrormakesthemodelprone
toovertothetrainingsetandnotgeneralizingtonew
samples(Fig.9).Hence,thecontextvectordimensionality
actsasaregularizerofthemodel.
V.CONCLUSIONS
Weshowedtheimportanceofbothbodyposeandgaze
cuesfortheaccuratepredictionofhumanintent.More
,theexperimentsdemonstratedthatamodel
trainedonbothbodyandgazecuespredictsthecorrectaction
about92msbeforeamodeltrainedonlyonbodyposecues.
Fig.9:
Validationlossasafunctionofthecontext
dimensionality.
Theiterationrepresentsthenumberoftrain-
ingsteps,while#Crepresentsthedimensionalityofthe
contextvectorparameter.Asthedimensionalityparameteris
increased,thenetworkstartstoovtothetrainingdata.
Weintroducedarecurrentneuralnetworktopologyde-
signedtopredictmultipleandvariablelengthactionse-
quences.Predictingactionsequencesintroducescombina-
torialcomplexityissueswhichweresuccessfullymitigated
usingapruningmethod.
Wedemonstratedthetheoreticalvalueofpredictingmulti-
pleandvariableactionsequencesforestimatingtheexpected
futurerewardinahumanrobotcooperationscenario.
Westudiedhowdifferenttrainingprocedureandparameter
combinationsaffectthemodelperformance.Alltestswere
carriedoutonrealisticpubliclyavailabledatasets.
Ourapproachextendsthestateoftheartindirectionsthat
arekeytoenablemoreefhuman-robotcooperation,
particularlyinvolvingnon-verbalcommunication.
VI.FUTUREWORK
Possibledirectionsincludeextendingthemodelbyexplor-
ingtheconnectionbetweennon-verbalcuesandsemantic
featuresrelatedtothecontext,throughcomposingthemodel
withadditionalinformationusingprobabilisticmethods.
Furthermore,thisworkestablishesastrongbaseforthe
implementationofajointactionscenarioonahumanoid
roboticsplatformsuchastheiCub.
ACKNOWLEDGMENTS
ResearchsupportedbythePortugueseFoundationforSci-
enceandTechnology(FCT)project[UID/EEA/50009/2013],
EUH2020projectunderGrant752611-ACTICIPATEand
RBCog-Labresearchinfrastructure.
R
EFERENCES
[1]C.M.HuangandB.Mutlu,ﬁAnticipatoryrobotcontrolforef
human-robotcollaboration,ﬂ
ACM/IEEEInternationalConferenceon
Human-RobotInteraction(HRI)
,vol.2016-April,no.SectionV,
pp.83Œ90,2016.
[2]K.Sakita,K.Ogawara,S.Murakami,K.Kawamura,andK.Ikeuchi,
ﬁFlexiblecooperationbetweenhumanandrobotbyinterpretinghuman
intentionfromgazeinformation,ﬂ
IEEE/RSJInternationalConference
onIntelligentRobotsandSystems(IROS)
,vol.1,pp.846Œ851,2004.
[3]A.Jain,A.Singh,H.S.Koppula,S.Soh,andA.Saxena,ﬁRecurrent
NeuralNetworksfordriveractivityanticipationviasensory-fusionar-
chitecture,ﬂ
Proceedings-IEEEInternationalConferenceonRobotics
andAutomation(ICRA)
,vol.2016-June,2016.
[4]T.Yonezawa,H.Yamazoe,A.Utsumi,andS.Abe,ﬁAttractive,
Informative,andCommunicativeRobotSystemonGuidePlateas
anAttendantwithAwarenessofUser'sGaze,ﬂ
Paladyn,Journalof
BehavioralRobotics
,vol.4,no.2,pp.113Œ122,2013.
[5]H.AdmoniandS.Srinivasa,ﬁPredictinguserintentthrougheye
gazeforsharedautonomy,ﬂ
AAAIFallSymposium-TechnicalReport
,
vol.FS-16-01-,pp.298Œ303,2016.
[6]C.-M.Huang,S.Andrist,A.Saupp
´
e,andB.Mutlu,ﬁUsinggazepat-
ternstopredicttaskintentincollaboration.,ﬂ
Frontiersinpsychology
,
vol.6,no.July,p.1049,2015.
[7]N.SebanzandG.Knoblich,ﬁPredictioninJointAction:What,When,
andWhere,ﬂ
TopicsinCognitiveScience
,vol.1,no.2,pp.353Œ367,
2009.
[8]C.Breazeal,C.D.Kidd,A.L.Thomaz,G.Hoffman,andM.Berlin,
ﬁEffectsofNonverbalCommunicationonEfciencyandRobustness
ofHuman-RobotTeamwork,ﬂ
InternationalConferenceonIntelligent
RobotsandSystems(IROS)
,2005.
[9]M.Argyle,R.Ingham,F.Alkema,andM.McCallin,ﬁTheDifferent
FunctionsofGaze,ﬂ1973.
[10]S.Ho,T.Foulsham,andA.Kingstone,ﬁSpeakingandlisteningwith
theeyes:Gazesignalingduringdyadicinteractions,ﬂ
PLoSONE
,
vol.10,no.8,pp.1Œ18,2015.
[11]S.Baron-Cohen,
Mindblindness:AnEssayonAutismandTheoryof
Mind
.MITPress,1997.
[12]L.Lukic,J.Santos-Victor,andA.Billard,ﬁLearningroboticeye-arm-
handcoordinationfromhumandemonstration:Acoupleddynamical
systemsapproach,ﬂ
BiologicalCybernetics
,vol.108,no.2,pp.223Œ
248,2014.
[13]C.Perez-D'ArpinoandJ.A.Shah,ﬁFastTargetPredictionofHu-
manReachingMotionforCooperativeHuman-RobotManipulation
TasksusingTimeSeriesﬂ
InternationalConferenceon
RoboticsandAutomation(ICRA)
,pp.6175Œ6182,2015.
[14]V.GalleseandA.Goldman,ﬁMirrorneuronsandthesimulationtheory
ofmind-reading,ﬂ
TrendsinCognitiveSciences
,vol.2,no.12,pp.493
Œ501,1998.
[15]M.LopesandJ.Santos-Victor,ﬁVisuallearningbyimitationwith
motorrepresentations,ﬂ
IEEETransactionsonSystems,Man,and
Cybernetics,PartB(Cybernetics)
,vol.35,no.3,pp.438Œ449,2005.
[16]A.Jain,A.R.Zamir,S.Savarese,andA.Saxena,ﬁStructural-RNN:
DeepLearningonSpatio-TemporalGraphs,ﬂin
IEEEConferenceon
ComputerVisionandPatternRecognition(CVPR)
,2016.
[17]J.Martinez,M.J.Black,andJ.Romero,ﬁOnhumanmotionprediction
usingrecurrentneuralnetworks,ﬂin
IEEEConferenceonComputer
VisionandPatternRecognition(CVPR)
,2017.
[18]H.S.KoppulaandA.Saxena,ﬁAnticipatingHumanActivitiesUsing
ObjectAffordancesforReactiveRoboticResponse,ﬂ
IEEETransac-
tionsonPatternAnalysisandMachineIntelligence(TPAMI)
,vol.38,
no.1,pp.14Œ29,2016.
[19]G.Saponaro,G.Salvi,andA.Bernardino,ﬁRobotanticipationofhu-
manintentionsthroughcontinuousgesturerecognition,ﬂ
Proceedings
ofthe2013InternationalConferenceonCollaborationTechnologies
andSystems,CTS2013
,no.Cts,pp.218Œ225,2013.
[20]Y.Saeys,I.Inza,andP.Larra
Ÿ
naga,ﬁAreviewoffeatureselectiontech-
niquesinbioinformatics,ﬂ
Bioinformatics
,vol.23,no.19,pp.2507Œ
2517,2007.
[21]N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhut-
dinov,ﬁDropout:ASimpleWaytoPreventNeuralNetworks
fromOvﬂ
JournalofMachineLearningResearch
,vol.15,
pp.1929Œ1958,2014.
[22]K.Cho,B.vanMerrienboer,D.Bahdanau,andY.Bengio,ﬁOn
thePropertiesofNeuralMachineTranslation:Encoder-DecoderAp-
proaches,ﬂ2014.
[23]H.S.Koppula,R.Gupta,andA.Saxena,ﬁLearningHumanActivities
andObjectAffordancesfromRGB-DVideos,ﬂ
InternationalJournal
ofRoboticsResearch
,2012.
[24]R.W.Angel,W.Alston,andH.Garland,ﬁFunctionalrelationsbetween
themanualandoculomotorcontrolsystems,ﬂ
ExperimentalNeurology
,
vol.27,no.2,pp.248Œ257,1970.
[25]C.J.VanRijsbergen,
InformationRetrieval
.Butterworth-Heinemann,
1979.
[26]Y.Bengio,I.J.Goodfellow,andA.C.Courville,
DeepLearning
.The
MITPress,2016.
"
53,Kernel machines that adapt to GPUs for effective large batch training,http://arxiv.org/pdf/1806.06144v3.pdf,https://github.com/EigenPro/EigenPro2,"K
ERNELMACHINESTHATADAPTTO
GPU
SFOR
EFFECTIVELARGEBATCHTRAINING
SiyuanMa
1
MikhailBelkin
1
A
BSTRACT
ModernmachinelearningmodelsaretypicallytrainedusingStochasticGradientDescent(SGD)onmassively
parallelcomputingresourcessuchasGPUs.Increasingmini-batchsizeisasimpleanddirectwaytoutilizethe
parallelcomputingcapacity.Forsmallbatchanincreaseinbatchsizeresultsintheproportionalreductioninthe
trainingtime,aphenomenonknownas
linearscaling
.However,increasingbatchsizebeyondacertainvalue
leadstonofurtherimprovementintrainingtime.Inthispaperwedeveloptheanalyticalframeworkthat
extendslinearscalingtomatchtheparallelcomputingcapacityofaresource.Theframeworkisdesignedfora
classofclassicalkernelmachines.Itautomaticallyastandardkernelmachinetooutputamathemati-
callyequivalentpredictionfunction,yetallowingforextendedlinearscaling,i.e.,highereffectiveparallelization
andfastertrainingtimeongivenhardware.
Theresultingalgorithmsareaccurate,principledandveryfast.Forexample,usingasingleTitanXpGPU,
trainingonImageNetwith
1
:
3

10
6
datapointsand
1000
labelstakesunderanhour,whilesmallerdatasets,
suchasMNIST,takeseconds.Astheparametersarechosenanalytically,basedonthetheoreticalbounds,little
tuningbeyondselectingthekernelandthekernelparameterisneeded,furtherfacilitatingthepracticaluseof
thesemethods.
1I
NTRODUCTION
Modernmachinelearningmodelsaretrainedusing
StochasticGradientDescent(SGD)onparallelcomputing
resourcessuchasGPUs.Duringtrainingweaimtomin-
imizethe(wallclock)trainingtime
T
train
givenacompu-
tationalresource,e.g.,abankofGPUs.Althoughusing
largerbatchsize
m
improvesresourceutilization,itdoes
notnecessarilyleadtoareductionintrainingtime.In-
deed,wecandecomposethetrainingtime
T
train
(
m
)
into
twoparts,
T
train
(
m
)=
N
epoch
(
m
)

T
epoch
(
m
)
where
N
epoch
(
m
)
isthenumberoftrainingepochsrequired
forconvergenceand
T
epoch
(
m
)
isthewallclocktimeto
trainforoneepoch.Itiseasytoseethatincreasing
m
alwaysleadstohigherresourceutilization,thusdecreas-
ing
T
epoch
(
m
)
.However,
N
epoch
(
m
)
mayincreasewith
m
.
Infact,forageneralclassofconvexproblemsitcanbe
shown(
Maetal.
,
2017
)
N
epoch
(
m
)
isapproximatelycon-
stantfor
m
nomorethanacertaincriticalsize
m

and
N
epoch
(
m
)
/
m
for
m>m

.Ontheotherhand
T
epoch
(
m
)
1
DepartmentofComputerScienceandEngineering,Ohio
StateUniversity,Columbus,Ohio,UnitedStates.Correspondence
to:SiyuanMa
<
masi@cse.ohio-state.edu
>
,MikhailBelkin
<
mbelkin@cse.ohio-state.edu
>
.
isatbest
1
decreasesproportionallyto
1
=m
.Thusthetrain-
ingtimeisatleast
T
train
(
m
)=
ˆ
1
=m;
for
m

m

const
;
for
m>m

Inotherwords,weobtainlinearspeedup(ﬁlinearscalingﬂ)
forbatchsizesupto
m

,beyondwhichthetrainingtime
cannotbeimprovedbyfurtherincreasing
m
.Furthermore,
animportantpropertyof
m

isitsnearindependencefrom
thenumberoftrainingsamplesasitisprimarilydetermined
bythemodelandthedatadistribution.
Similarrelationshipbetweenthebatchsizeandthetraining
timehasbeenobservedempiricallyintrainingdeepneural
networks(
Krizhevsky
,
2014
).Aheuristiccalledtheﬁlinear
scalingruleﬂhasbeenwidelyusedindeeplearningprac-
tice(
Goyaletal.
,
2017
;
Youetal.
,
2017
;
Jiaetal.
,
2018
).
Moreover,inparalleltotheconvexcaseanalyzedin(
Ma
etal.
,
2017
),recentwork(
Golmantetal.
,
2018
;
McCan-
dlishetal.
,
2018
)empiricallydemonstratesthat
m

isin-
dependentofthedatasizefordeepneuralnetworks.
Manybestpracticesofmodernlarge-scalelearning(
Goyal
etal.
,
2017
;
Youetal.
,
2017
;
Jiaetal.
,
2018
)startwith
estimating
m

byeitherheuristicrulesorexperiments.The
optimaltrainingtimeforthemodelisthencappedbythe
1
Assumingperfectparallelcomputation.
arXiv:1806.06144v3  [stat.ML]  3 Mar 2019KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
estimatedbatchsize
m

whichisedgiventhemodel
architectureandweight,aswellasthelearningtask.
Inthisworkweproposeaprincipledframework(EigenPro
2.0)thatincreases
m

foraclassofmodelscorresponding
toclassicalkernelmachines.Ourframeworka
kernelmachinetooutputamathematicallyequivalentpre-
dictionfunction,yetallowingforextendedlinearscaling
adaptiveto(potentially)arbitraryparallelcomputational
resource.Furthermore,theoptimizationparameterselec-
tionisanalytic,makingiteasyandeftouseinprac-
ticeandappropriateforﬁinteractiveﬂexploratorymachine
learningandautomaticmodelselection.Theresultingal-
gorithmsshowspeedupfortrainingonGPUs
overthestate-of-the-artmethodsandexcellentgeneraliza-
tionperformance.
Kernelmachines.
Kernelmachinesareapowerfulclassof
methodsforandregression.Giventhetrain-
ingdata
f
(
x
x
x
i
;y
i
)
;i
=1
;:::;n
g2
R
d

R
,andaposi-
tivekernel
k
:
R
d

R
d
!
R
,kernelmachines
constructfunctionsoftheform
f
(
x
x
x
)=
P
i

i
k
(
x
x
x;x
x
x
i
)
.
Thesemethodsaretheoreticallyattractive,showexcellent
performanceonsmalldatasets,andareknowntobeuni-
versallearners,i.e.,capableofapproximatinganyfunc-
tionfromdata.However,makingkernelmachinesfastand
scalabletolargedatahasbeenachallengingproblem.Re-
centlargescaleeffortstypicallyinvolvedparal-
lelcomputationalresources,suchasmultiple(sometimes
thousands)AWSvCPU's(
Tuetal.
,
2016
;
Avronetal.
,
2016
)orsuper-computernodes(
Huangetal.
,
2014
).Very
recently,FALKON(
Rudietal.
,
2017
)andEigenPro(
Ma&
Belkin
,
2017
)showedstrongresultsonlarge
datasetswithmuchlowercomputationalrequirements,a
fewhoursonasingleGPU.
Themainproblemandourcontribution.
Themain
problemaddressedinthispaperistominimizethetraining
timeforakernelmachine,givenaccesstoaparallel
computationalresource
G
.Ourmaincontributionisthat
givenastandardkernel,weareabletolearnanewdataand
computationalresourcedependentkerneltominimizethe
resourcetimerequiredfortrainingwithoutchangingthe
mathematicalsolutionfortheoriginalkernel.Ourmodel
fora
computationalresource
G
isbasedonamodern
graphicsprocessingunit(GPU),adevicethatallowsfor
veryefhighlyparallel
2
matrixmultiplication.
Theoutlineofourapproachisshowninthediagramonthe
right.Wenowoutlinethekeyingredients.
Theinterpolationframework.
Inrecentyearswehave
seenthatinferencemethods,notablyneuralnetworks,that
2
Forexample,thereare3840CUDAcoresinNvidiaGTX
TitanXp(Pascal).
interpolateornearlyinterpolatethetrainingdatagen-
eralizeverywelltotestdata(
Zhangetal.
,
2016
).It
hasbeenobservedin(
Belkinetal.
,
2018
)thatmini-
mumnormkernelinterpolants,i.e.,functionsoftheforms
f
(
x
x
x
)=
P
i

i
k
(
x
x
x;x
x
x
i
)
,suchthat
f
(
x
x
x
i
)=
y
i
,achieveop-
timalornearoptimalgeneralizationperformance.While
themathematicalfoundationsofwhyinterpolationpro-
ducesgoodtestresultsarenotyetfullyunderstood,the
simplicityoftheframeworkcanbeusedtoaccelerate
andscalethetraining
ofclassicalkernelmeth-
ods,whileimproving
theirtestaccuracy.In-
deed,constructingthese
interpolatingfunctionsis
conceptuallyandmathe-
maticallysimple,requir-
ingapproximatelysolv-
ingasinglesystemof
linearequationswitha
uniquesolution,same
forbothregressionand

cantcomputationalsav-
ingsand,whenneces-
sary,regularization(
Yao
etal.
,
2007
)areprovided
byearlystopping,i.e.,
stoppingiterationswell
beforenumericalconver-
gence,oncesuccessive
iterationsfailtoimprove
validationerror.
Adaptivitytodataandcomputationalresource:choos-
ingoptimalbatchsizeandstepsizeforSGD.
Wewill
trainkernelmethodsusingStochasticGradientDescent
(SGD),amethodwhichiswell-suitedtomodernGPU's
andhasshownimpressivesuccessintrainingneuralnet-
works.Importantly,intheinterpolationframework,depen-
denceofconvergenceonthebatchsizeandthestepsize
canbederivedanalytically,allowingforfullanalysisand
automaticparameterselection.
Wenotethatintheparallelmodeleachiterationof
SGD(essentiallyamatrixmultiplication)takesthesame
timeforanymini-batchsizeupto
m
max
G
,asthe
mini-batchsizewheretheparallelcapacityoftheresource
G
isfullyutilized.Itisshownin(
Maetal.
,
2017
)thatin
theinterpolationframeworkconvergenceperiteration(us-
ingoptimalstepsize)improvesnearlylinearlyasafunc-
tionofthemini-batchsize
m
uptoacertain
criticalsize
m

(
k
)
andrapidlysaturatesafterthat.Thequantity
m

(
k
)
isrelatedtothespectrumofthekernel.Forkernelsused
inpracticeitistypicallyquitesmall,lessthan
10
,dueto
KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
theirrapideigenvaluedecay.Yet,dependingonthenum-
berofdatapoints,featuresandlabels,amodernGPUcan
handlemini-batchesofsize
1000
orlarger.Thisdisparity
presentsanopportunityformajorimprovementsintheef-
yofkernelmethods.Inthispaperweshowhowto
constructdataandresourceadaptivekernel
k
G
,bymodi-
fyingthespectrumofthekernelbyusingEigenProalgo-
rithm(
Ma&Belkin
,
2017
).Theresultingiterativemethod
withthenewkernelhassimilarorbetterconvergenceperit-
erationthantheoriginalkernel
k
forsmallmini-batchsize.
Howeveritsconvergenceimproveslinearlytomuchlarger
mini-batchsizes,matching
m
max
G
,themaximumthatcan
beutilizedbytheresource
G
.Importantly,SGDforeither
kernelconvergetothesameinterpolatedsolution.
Figure1:Adaptiveandoriginalkernel
Thus,weaimtomodifythekernelbyconstructingakernel
k
G
,suchthat
m

(
k
G
)=
m
max
G
withoutchangingtheop-
timal(interpolating)solution.Thisisshownschematically
inFigure
1
.Weseethatforsmallmini-batchsizeconver-
genceofthesetwokernels
k
and
k
G
issimilar.However,
valuesof
m>m

(
k
)
donothelptheconvergenceofthe
originalkernel
k
,whileconvergenceof
k
G
keepimproving
upto
m
=
m
max
G
,wheretheresourceutilizationissatu-
rated.Forempiricalresultsonrealdatasets,paralleltothe
schematicshownabove,seeFigure
2
inSection
5
.
Weconstructandimplementthesekernels(see
github.com/EigenPro/EigenPro2
forthecode),and
showhowtoanalyticallychooseparameters,including
thebatchsizeandthestepsize.Asasecondarycon-
tributionofthisworkwedevelopanimprovedversion
ofEigenPro(
Ma&Belkin
,
2017
)reducing
thememoryrequirementsandmakingthecomputational
overheadoverthestandardSGDnegligible.
Comparisontorelatedwork.
Inrecentyearstherehas
beenprogressonscalingandacceleratingkernel
methodsincluding(
Tak
´
acetal.
,
2013
;
Huangetal.
,
2014
;
Luetal.
,
2014
;
Tuetal.
,
2016
;
Avronetal.
,
2016
;
May
etal.
,
2017
).Mostofthesemethodsareabletoscaleto
largedatasetsbyutilizingmajorcomputationalresources
suchassupercomputersormultiple(sometimeshundreds
orthousands)AWSvCPU's
3
.Tworecentmethodswhich
allowforhighefykerneltrainingwithasingleCPU
orGPUisEigenPro(
Ma&Belkin
,
2017
)(usedaasbasis
fortheadaptivekernelsinthispaper)andFALKON(
Rudi
etal.
,
2017
).Themethoddevelopedinthispaperis
cantlyfasterthaneitherofthem,whileachievingsimilaror
bettertestsetaccuracy.Additionally,itiseasiertouseas
muchoftheparameterselectionisdoneautomatically.
Mini-batchSGD(usedinouralgorithm)hasbeenthedomi-
nanttechniqueintrainingdeepmodels.Therehasbeensig-
empiricalevidence(
Krizhevsky
,
2014
;
Youetal.
,
2017
;
Smithetal.
,
2017
)showingthatlinearlyscalingthe
stepsizewiththemini-batchsizeuptoacertainvalueleads
toimprovedconvergence.Thisphenomenonhasbeenuti-
lizedtoscaledeeplearningindistributedsystemsbyadopt-
inglargemini-batchsizes(
Goyaletal.
,
2017
).
Theadvantageofoursettingisthattheoptimalbatch
andstepsizescanbeanalyzedandexpressedanalytically.
Moreover,theseformulascontainvariableswhichcan
beexplicitlycomputedanddirectlyusedforparameter
selectioninouralgorithms.Goingbeyondbatchsizeand
stepsizeselection,thetheoreticalinterpolationframework
allowsustoconstructnewadaptivekernels,suchthatthe
mini-batchsizerequiredforoptimalconvergencematches
thecapacityofthecomputationalresource.
Thepaperisstructuredasfollows:InSection
3
,wepresent
ourmainalgorithmtolearnakerneltofullyutilizeagiven
computationalresource.InSection
4
,wepresentanim-
provedversionofEigenProiterationusedbythemainal-
gorithm.Wethenprovidecomparisonstostate-of-the-art
kernelmethodsonseverallargedatasetsinSection
5
.We
furtherdiscussexploratorymachinelearninginthecontext
ofourmethod.
2S
ETUP
Westartbydiscussingthebasicsettingandkernel
methodsusedinthispaper.
Kernelinterpolation.
Wearegiven
n
labeledtraining
points
(
x
x
x
1
;y
1
)
;:::;
(
x
x
x
n
;y
n
)
2
R
d

R
.Weconsidera
ReproducingKernelHilbertSpace(RKHS)
H
(
Aronszajn
,
1950
)correspondingtoapositivekernelfunction
k
:
R
d

R
d
!
R
.Thereisaunique(minimumnorm)
3
See
http://aws.amazon.com/ec2
fordetails.
KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
interpolatedsolutionin
H
oftheform
f

(

)=
n
X
i
=1


i
k
(
x
x
x
i
;

)
;
where(


1
;:::;

n
)
T
=
K

1
(
y
1
;:::;y
n
)
T
Here
K
denotesan
n

n
kernelmatrix,
K
ij
=
k
(
x
x
x
i
;x
x
x
j
)
.
Itiseasytocheckthat
8
i
f

(
x
i
)=
y
i
.
Remark2.1
(Squareloss)
.
Whiletheinterpolatedsolu-
tion
f

in
H
doesnotdependonanylossfunction,itis
theuniqueminimizerin
H
fortheempiricalsquareloss
L
(
f
)
,
1
n
P
n
i
=1
(
f
(
x
x
x
i
)

y
i
)
2
.
Gradientdescent.
Itcanbeshownthatgradientdescent
iterationfortheempiricalsquaredlossinRKHS
H
isgiven
by
f
 
f



2
n
n
X
i
=1
(
f
(
x
x
x
i
)

y
i
)
k
(
x
x
x
i
;

)
(1)
Mini-batchSGD.
Insteadofcalculatingthegradientwith
n
trainingpoints,eachSGDiterationupdatesthesolution
f
using
m
subsamples
(
x
x
x
t
1
;y
t
1
)
;:::;
(
x
x
x
t
m
;y
t
m
)
,
f
 
f



2
m
(
m
X
i
=1
(
f
(
x
x
x
t
i
)

y
t
i
)
k
(
x
x
x
t
i
;

)
)
(2)
Itisequivalenttorandomizedcoordinatedescent(
Leven-
thal&Lewis
,
2010
)for
K


=
y
y
y
on
m
coordinatesof



,

t
i
 

t
i



2
m
f
f
(
x
x
x
t
i
)

y
t
i
g
for
i
=1
;:::;m
(3)
Criticalmini-batchsizeaseffectiveparallelism.
Theo-
rem4in(
Maetal.
,
2017
)showsthatformini-batchitera-
tion(
2
)withkernel
k
thereisadata-dependentbatchsize
m

(
k
)
suchthat

Convergenceperiterationimproveslinearlywithin-
creasingbatchsize
m
for
m

m

(
k
)
(usingoptimal
constantstepsize).

Trainingwithanybatchsize
m>m

(
k
)
leadstothe
sameconvergenceperiterationastrainingwith
m

(
k
)
uptoasmallconstantfactor.
Wecancalculate
m

(
k
)
explicitlyusingkernelmatrix
K
(dependingonthedata),
m

(
k
)=

(
K
)

1
(
K
)
where

(
K
)
,
max
i
=1
;:::;n
k
(
x
x
x
i
;x
x
x
i
)
Foranyshiftinvariantkernel
k
,afternormalization,we
have

(
K
)=max
n
i
=1
k
(
x
x
x
i
;x
x
x
i
)

1
.
EigenProiteration(
Ma&Belkin
,
2017
).
Toachieve
fasterconvergence,EigenProiterationperformsspec-
tralonthekerneloperator
K
(
f
)
,
2
n
P
n
i
=1
h
k
(
x
x
x
i
;

)
;f
i
H
k
(
x
x
x
i
;

)
usingoperator,
P
(
f
)
,
f

q
X
i
=1
(1


q

i
)
h
e
i
;f
i
H
e
i
(4)
where

1


n
areorderedeigenvaluesof
K
and
e
i
isitseigenfunctioncorrespondingto

i
.Theiterationuses
P
torescalea(stochastic)gradientin
H
,
f
 
f



P
(
2
m
m
X
i
=1
(
f
(
x
x
x
t
i
)

y
t
i
)
k
(
x
x
x
t
i
;

)
)
(5)
Remark2.2
(Dataadaptivekernelforfastoptimization)
.
EigenProiterationfortargetfunction
y
andkernel
k
is
equivalenttoRichardsoniteration/randomized(block)
coordinatedescentforlinearsystem
K
P



=
y
y
y
P
,
(
P
f

(
x
x
x
1
)
;:::;
P
f

(
x
x
x
n
))
T
.Here
K
P
isthekernelma-
trixcorrespondingtoadata-dependentkernel
k
P
.When
n
!1
,ithasthefollowingexpansionaccordingtoMer-
cer'stheorem,
k
P
(
x
x
x;z
z
z
)=
q
X
i
=1

q
e
i
(
x
x
x
)
e
i
(
z
z
z
)+
1
X
i
=
q
+1

i
e
i
(
x
x
x
)
e
i
(
z
z
z
)
(6)
For
n<
1
,itisaoftheoriginalkernel
k
,
k
P
(
x
x
x;z
z
z
)=
P
f
k
(
x
x
x;

)
g
(
z
z
z
)
ˇ
k
(
x
x
x;z
z
z
)

q
X
i
=1
(

i


q
)
e
i
(
x
x
x
)
e
i
(
z
z
z
)
Remark2.3
(Preconditionedlinearsystem/gradientde-
scent)
.
K
P



=
y
y
y
P
isequivalenttothepreconditioned
linearsystem
PK


=
Py
y
y
where
P
isaleftmatrixpre-
conditionerrelatedto
P
.Accordingly,
P
istheoperator
preconditionerforpreconditioned(stochastic)gradientde-
scent(
5
).
Abstractionforparallelcomputationalresources.
To
constructaresourceadaptivekernel,weconsiderthefol-
lowingabstractionforgivencomputationalresource
G
,

C
G
:Parallelcapacityof
G
,i.e.,thenumberofparallel
operationsthatisrequiredtofullyutilizethecomput-
ingcapacityof
G
.

S
G
:Internalresourcememoryof
G
.
Tofullyutilize
G
,oneSGD/EigenProiterationmustexe-
cuteatleast
C
G
operationsusinglessthan
S
G
memory.In
thispaper,weprimarilyadaptkerneltoGPUdevices.Fora
GPU
G
,
S
G
equalsthesizeofitsdedicatedmemoryand
C
G
isproportionaltothenumberofthecomputingcores(e.g.,
3840CUDAcoresinTitanXp).Noteforcomputational
resourceslikeclusterandsupercomputer,weneedtotake
intoaccountadditionalfactorssuchasnetworkbandwidth.
KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
3M
AIN
A
LGORITHM
Ourmainalgorithmaimstoreducethetrainingtimeby
constructingadata/resourceadaptivekernelforanygiven
kernelfunction
k
tofullyutilizeacomputationalresource
G
.Itsdetailedwwispresentedontheright.
cally,weusethefollowingsteps:
Step1.
Calculatetheresource-dependentmini-batchsize
m
max
G
tofullyutilizeresource
G
.
Step2.
Identifytheparametersandconstructanewkernel
k
G
suchthat
m

(
k
G
)=
m
max
G
.
Step3.
Selectoptimalstepsizeandtrainusingimproved
EigenPro(seeSection
4
).
EigenPro2.0
Notethatduetoprop-
ertiesofEigenPro
iteration,trainingwith
thisadaptivekernel
convergestothesame
solutionastheoriginal
kernel.
Tocalculate
m
max
G
for100%resource
utilization,we
estimatetheopera-
tionparallelismand
memoryusageofone
EigenProiteration.The
improvedversionof
EigenProiteration(in-
troducedinSection
4
)
makescomputation
andmemoryoverhead
overthestandardSGD
negligible(seeTa-
ble
1
).Thusweassume
thatEigenProhasthe
samecomplexityas
thestandardSGDper
iteration.
CostofoneEigenPro
iterationwithbatch
size
m
.
Weconsider
trainingdata
(
x
x
x
i
;y
y
y
i
)
2
R
d

R
l
;i
=1
;:::;n
.
Hereeachfeaturevec-
tor
x
x
x
is
d
dimensional,
andeachlabel
y
y
y
is
l
di-
mensional.

Computationalcost.
Ittakes
(
d
+
l
)

m

n
oper-
ationstoperformoneSGDiterationon
m
pointsas
inIteration(
2
).Thesecomputationsreducetomatrix
multiplicationandcanbedoneinparallel.

Spaceusage.
Ittakes
d

n
memorytostorethetraining
data(askernelcenters)and
l

n
memorytomaintain
themodelweight.Additionallyweneedtostorea
m

n
kernelmatrixforthepredictiononthemini-
batch.Intotal,weneed
(
d
+
l
+
m
)

n
memory.
Wecannowcalculate
m
max
G
fortheparallelcomputational
resource
G
withparameters
C
G
;S
G
andintroducedinSec-
tion
2
.
Step1:Determiningbatchsize
m
max
G
for100%re-
sourceutilization.
Wetwomini-batchnota-
tions:

m
C
G
:batchsizeforfullyutilizingparallelismin
G
suchthat
(
d
+
l
)

m
C
G

n
ˇ
C
G
.

m
S
G
:batchsizeformaximummemoryusageof
G
suchthat
(
d
+
l
+
m
S
G
)

n
ˇ
S
G
.
Tobestutilize
G
withoutexceedingitsmemory,weset
m
max
G
=min
f
m
C
G
;m
S
G
g
.Notethatinpractice,itismore
importanttofullyutilizethememorysothat
m
max
G
.
m
S
G
.
Step2:Learningthekernel
k
G
given
m
max
G
.
Next,we
showhowtoconstruct
k
G
=
k
P
q
usingEigenProiteration
suchthat
m

(
k
G
)=
m
max
G
.Thecorresponding
q
is
as
q
,
max
f
i
2
N
;
s.t.
m

(
k
P
i
)

m
max
G
g
(7)
Tocompute
q
recallthat
m

(
k
P
q
)=

(
K
P
q
)

1
(
K
P
q
)
,where
K
P
q
isthekernelmatrixcorrespondingtothekernelfunction
k
P
q
.Usingtheof
P
q
and

inSection
2
,we
have

1
(
K
P
q
)=

q
(
K
)

(
K
P
q
)
ˇ
max
i
=1
;:::;n
k
P
q
(
x
x
x
i
;x
x
x
i
)
=max
i
=1
;:::;n
f
k
(
x
x
x
i
;x
x
x
i
)

q
X
j
=1
(

j


q
)
e
i
(
x
x
x
i
)
2
g
Inpractice,

(
K
P
q
)
canbeaccuratelyestimatedusingthe
maximumof
k
P
q
(
x
x
x;x
x
x
)
onasmallnumberofsubsamples.
Similarly,wecanestimate

q
(
K
)
onasubsamplekernel
matrix.Knowingtheapproximatetopeigenvaluesof
K
,
allowsustoefcompute
m

(
k
P
p
)
foreach
p
,thus
allowingtochoose
q
from(
7
).
Step3:Trainingwithadaptivekernel
k
G
=
k
P
q
.
Weuse
thelearnedkernel
k
G
withimprovedEigenPro(Section
4
).
Itsoptimizationparameters(batchandstepsize)arecalcu-
latedasfollows:
m
=
m
max
G
;
=
m
max
G

(
K
G
)
KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
Claim(Acceleration).
Usingtheadaptivekernel
k
G
de-
creasestheresourcetimerequiredfortraining(assumingan
idealizedmodeloftheGPUandworkload)overtheorigi-
nalkernel
k
byafactorof
accelerationof
k
G
over
k
=

(
K
)

(
K
G
)

m
max
G
m

(
k
)
SeetheAppendix
C
forthederivationandadiscussion.
Wenotethatempirically,

(
K
G
)
ˇ

(
K
)
,while
m
max
G
m

(
k
)
is
between
50
and
500
,whichisinlinewiththeacceleration
observedinpractice.
Remark3.1
(Choiceof
q
)
.
Notethatitisnotimportantto
select
q
exactly,accordingtoEq.
7
.Infact,choosing
k
P
p
forany
p>q
allowsforthesameaccelerationas
k
P
q
as
longasthemini-batchsizeischosentobe
m
max
G
andthe
stepsizeischosenaccordingly.Thus,wecanchooseany
value
p>q
forouradaptivekernel
k
P
p
.However,choos-
ing
p
largerthan
q
incursanadditionalcomputationcost
as
p
eigenvaluesandeigenvectorsof
K
needtobeapproxi-
matedaccurately.Inparticular,largersubsamplesize
s
(see
Section
4
maybeneededforapproximatingeigenvectors.
4I
MPROVED
E
IGEN
P
RO
I
TERATION
USING
N
YSTR
¨
OM
E
XTENSION
Inthissection,wepresentanimprovementfortheEigen-
Proiterationoriginallyproposedin(
Ma&Belkin
,
2017
).
WereducethememoryoverheadofEigen-
ProoverstandardSGDandnearlyeliminatecomputa-
tionaloverheadperiteration.Theimprovementisbasedon
anefrepresentationofthepreconditioner
P
q
using
Nystr
¨
omextension.
WestartbyrecallingtheEigenProiterationinRKHSand
itspreconditionerconstructedbythetop-
q
eigensystem

i
;e
i
ofthekerneloperator
K
:
f
 
f



P
q
(
2
m
m
X
i
=1
(
f
(
x
x
x
t
i
)

y
t
i
)
k
(
x
x
x
t
i
;

)
)
where
P
q
(
f
)=
f

q
X
i
=1
(1


q

i
)
h
e
i
;f
i
H
e
i
Thekeytoconstructtheaboveiterationistoobtainanaccu-
rateandcomputationallyefapproximationof

i
;e
i
suchthat
K
e
i
ˇ

i
e
i
.TheoriginalEigenProiteration
learnsanapproximate
e
i
oftheform
P
n
j
=1
w
j
k
(
x
x
x
j
;

)
.In
contrast,ourimprovedversionofEigenprousesonlya
smallnumberofsubsamples
x
x
x
r
1
;:::;x
x
x
r
s
tolearnan
e
i
oftheform
P
s
j
=1
w
j
k
(
x
x
x
r
j
;

)
.Thiscompactrepresenta-
tion(
s
versus
n
)nearlyeliminatesper-iterationoverhead
ofEigenProoverSGD.Importantly,thereisnoassociated
accuracyreductionasthisisthesamesubsetusedinthe
originalEigenProtoapproximate
P
q
.
Algorithm1
ImprovedEigenProiteration
(doublecoordinateblockdescent)
Input:
Kernelfunction
k
(
x
x
x;z
z
z
)
,EigenProparameter
q
,
mini-batchsize
m
,stepsize

,sizeofedcoordinate
block
s
initialize
modelparameter



=(

1
;:::;
n
)
T
 
0
subsample
s
coordinateindices
r
1
;:::;r
s
2f
1
;:::;n
g
forconstructing
P
q
,whichformedcoordinateblock



r
,
(

r
1
;:::;
r
s
)
T
compute
top-
q
eigenvalues

,
diag
(
˙
1
;:::;˙
q
)
and
correspondingeigenvectors
V
,
(
e
e
e
1
;:::;e
e
e
q
)
ofsub-
samplekernelmatrix
K
s
=[
k
(
x
x
x
r
i
;x
x
x
r
j
)]
s
i;j
=1
for
t
=1
;:::
do
1.sampleamini-batch
(
x
x
x
t
1
;y
t
1
)
;:::;
(
x
x
x
t
m
;y
t
m
)
2.calculatepredictionsonthemini-batch
f
(
x
x
x
t
j
)=
n
X
i
=1

i
k
(
x
x
x
i
;x
x
x
t
j
)
for
j
=1
;:::;m
3.update
sampledcoordinateblock
corresponding
tothemini-batch



t
,
(

t
1
;:::;
t
m
)
,



t
 



t



2
m
(
f
(
x
x
x
t
1
)

y
t
1
;:::;f
(
x
x
x
t
m
)

y
t
m
)
T
4.evaluatethefollowingfeaturemap
˚
(

)
onthemini-
batchfeatures
x
x
x
t
1
;:::;x
x
x
t
m
:
˚
(
x
x
x
)
,
(
k
(
x
x
x
r
1
;x
x
x
)
;:::;k
(
x
x
x
r
s
;x
x
x
))
T
5.update
coordinateblock



r
toapply
P
q
,



r
 



r
+


2
m
m
X
i
=1
(
f
(
x
x
x
t
i
)

y
t
i
)

VDV
T
˚
(
x
x
x
t
i
)
where
D
,
(1

˙
q



1


1
endfor
Next,weshowhowtoapproximate

i
;e
i
.Weconsider
arelatedlinearsystemforsubsamples
x
x
x
r
1
;:::;x
x
x
r
s
2
R
d
:
K
s
e
e
e
i
=
˙
i
e
e
e
i
where
K
s
,
[
k
(
x
x
x
r
i
;x
x
x
r
j
)]
s
i;j
=1
isasubsample
kernelmatrixand
˙
i
;e
e
e
i
isitseigenvalue/eigenvector.This
rank-
s
linearsystemisinfactadiscretizationof
K
e
i
=

i
e
i
intheRKHS.
Thetwoeigensystems,
˙
i
;e
e
e
i
and

i
;e
i
areconnected
through
Nystr
¨
omextension
.,theNystr
¨
omex-
tensionof
e
i
onsubsamples
x
x
x
r
1
;:::;x
x
x
r
s
approximates
e
i
asfollows:
e
i
(

)
ˇ
1
˙
i
s
X
j
=1
e
i
(
x
x
x
r
j
)
k
(
x
x
x
r
j
;

)
Evaluatingbothsideon
x
x
x
r
1
;:::;x
x
x
r
s
,wehave

i
ˇ
˙
i
s
;e
i
(

)
ˇ
1
p
˙
i
e
e
e
T
i
˚
(

)
KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
where
˚
(

)
,
(
k
(
x
x
x
r
1
;

)
;:::;k
(
x
x
x
r
s
;

))
T
isakernelfeature
map.Thusweapproximatethetop-
q
eigensystemof
K
us-
ingthetop-
q
eigensystemof
K
s
.These(low-rank)approx-
imationsfurtherallowustoapply
P
q
forefEigenPro
iterationonmini-batch
(
x
x
x
t
1
;y
t
1
)
:::;
(
x
x
x
t
m
;y
t
m
)
,
f
 
f



2
m
m
X
i
=1
(
f
(
x
x
x
t
i
)

y
t
i
)
k
(
x
x
x
t
i
;

)
+


2
m
m
X
i
=1
(
f
(
x
x
x
t
i
)

y
t
i
)

˚
(
x
x
x
t
i
)
T
VDV
T
˚
(

)
where
D
,


1
(1

˙
q



1
)
(8)
where

,
diag
(
˙
1
;

;˙
q
)
and
V
,
(
e
e
e
1
;

;e
e
e
q
)
are
top-
q
eigensystemof
K
s
.
Recallingthat
f
=
P
n
i
=1

i
k
(
x
x
x
i
;

)
,theaboveiteration
canbeexecutedbyupdatingtwocoordinateblocksofthe
parametervector



asinAlgorithm
1
.
Computation/memoryperiteration.
InAlgorithm
1
,the
costofeachiterationrelatestoupdatingtwocoordinate
blocks.Notably,Steps2-3isexactlythestandardSGD.
ThustheoverheadofourmethodcomesfromSteps4-5.
WecompareourimprovedEigenPrototheoriginalEigen-
ProandtostandardSGDinTable
1
.Weseethattheover-
headoforiginalEigenPro(inbold)scaleswiththedatasize
n
.Incontrast,improvedEigenProdependsonlyonthe
edcoordinateblocksize
s
whichisindependentof
n
.
Hence,when
n
becomeslarge,theoverheadofouritera-
tionbecomesnegligible(bothincomputationandmemory)
comparedtothecostofSGD.
Computation
Memory
ImprovedEigenPro
s

mq
s

mq
s

mq
+
n

m
(
d
+
l
)
s

q
s

q
s

q
+
n

(
m
+
d
+
l
)
OriginalEigenPro
n

mq
n

mq
n

mq
+
n

m
(
d
+
l
)
n

q
n

q
n

q
+
n

(
m
+
d
+
l
)
SGD
n

m
(
d
+
l
)
n

(
m
+
d
+
l
)
Table1:OverheadoverSGDis
bolded
.
n
:trainingdatasize,
m
:batchsize,
d
:featuredim.,
s
:edcoordinateblocksize,
q
:
EigenProparameter,
l
:numberoflabels.
Togivearealisticexample,formanyofourexperiments
n
=10
6
,while
s
ischosentobe
10
4
.Wetypicallyhave
d;m
ofthesameorderofmagnitude
10
3
,while
q
and
l
around
10
2
.ThisresultsinoverheadofEigenProofless
than
1%
overSGDforbothcomputationandmemory.
5E
XPERIMENTAL
E
VALUATION
Computingresource.
Werunallexperimentsonasingle
workstationequippedwith128GBmainmemory,twoIntel
Xeon(R)E5-2620processors,andoneNvidiaGTXTitan
Xp(Pascal)GPU.
Datasets.
Wereducemulticlasslabelstomultiplebi-
narylabels.ForimagedatasetsincludingMNIST(
Le-
Cunetal.
,
1998
),CIFAR-10(
Krizhevsky&Hinton
,
2009
),
andSVHN(
Netzeretal.
,
2011
),colorimagesare
transformedtograyscaleimages.Wethenrescalethe
rangeofeachfeatureto
[0
;
1]
.ForImageNet(
Dengetal.
,
2009
),weusethetop500PCAcomponentsofsome
convolutionalfeaturesextractedfromInception-ResNet-
v2(
Szegedyetal.
,
2017
).ForTIMIT(
Garofoloetal.
,
1993
),wenormalizeeachfeaturebyz-score.
Choosingthesizeofthecoordinateblock
s
.We
choose
s
accordingtothesizeofthetrainingdata,
n
.When
n

10
5
,wechoose
s
=2

10
3
;when
n>
10
5
,wechoose
s
=1
:
2

10
4
.
5.1Comparisontostate-of-the-artkernelmethods
InTable
2
,wecompareourmethodtothestate-of-the-art
kernelmethodsonseverallargedatasets.Foralldatasets,
ourmethodisfasterthanothermethodswhile
stillachievingbetterorsimilarresults.Moreover,our
methodusesonlyasingleGPUwhilemanystate-of-the-
artkernelmethodsusemuchlessaccessiblecomputingre-
sources.
Amongallthecomparedmethods,FALKON(
Rudietal.
,
2017
)andEigenPro(
Ma&Belkin
,
2017
)standoutfor
theircompetitiveperformanceandfasttrainingonasingle
GPU.Notably,ourmethodstillachieves5X-6Xaccelera-
tionoverFALKONand5X-14XaccelerationoverEigen-
Prowithmostlybetteraccuracy.Importantly,ourmethod
hastheadvantageofautomaticallyinferringparametersfor
optimization.Incontrast,parametersrelatedtooptimiza-
tionforFALKONandEigenProneedtobeselectedby
cross-validation.
5.2ConvergencecomparisontoSGDandEigenPro
InFigure
2
,wetrainthreekernelmachineswithEigen-
Pro2.0,standardSGDandEigenPro(
Ma&Belkin
,
2017
)
forvariousbatchsizes.ThestepsizesforSGDandEigen-
Proaretunedforbestperformance.ThestepsizeforEigen-
Pro2.0iscomputedautomaticallyaccordingtoSection
3
.
ConsistentwiththeschematicFigure
1
intheintroduction,
theoriginalkernel
k
hasacriticalbatchsize
m

(
k
)
ofsize
4
and
6
respectively,whichistoosmalltofullyutilizethe
parallelcomputingcapacityoftheGPUdevice.Incontrast,
ouradaptivekernel
k
G
hasamuchlargercriticalbatchsize
m

(
k
G
)
ˇ
6500
,whichleadstomaximumGPUutiliza-
tion.WeseethatEigenPro2.0outperforms
originalEigenProduetobetterresourceutilizationandpa-
rameterselection,aswellasloweroverhead(seeTable
1
).
KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
Table2:ComparisonofEigenPro2.0andstate-of-the-artkernelmethods
Dataset
Size
EigenPro2.0
(use1GTXTitanXp)
ResultsofOtherMethods
error
GPUtime
resourcetime
error
reference
MNIST
6
:
7

10
6
0.72%
19m
4.8hon
1GTXTitanX
0.70%
EigenPro(
Ma&Belkin
,
2017
)
1.1hon
1344AWSvCPUs
0.72%
PCG(
Avronetal.
,
2016
)
lessthan37.5hours
on1TeslaK20m
0.85%
(
Luetal.
,
2014
)
ImageNet
y
1
:
3

10
6
20.6%
40m
-
19.9%
Inception-ResNet-v2(
Szegedyetal.
,
2017
)
4hon
1TeslaK40c
20.7%
FALKON(
Rudietal.
,
2017
)
TIMIT
z
1
:
1

10
6
/
2

10
6
31.7%
32.1%
24m
(3epochs)
8m
(1epoch)
3.2hon
1GTXTitanX
31.7%
EigenPro(
Ma&Belkin
,
2017
)
1.5hon
1TeslaK40c
32.3%
FALKON(
Rudietal.
,
2017
)
512IBM
BlueGene/Qcores
33.5%
Ensemble(
Huangetal.
,
2014
)
7.5hon
1024AWSvCPUs
33.5%
BCD(
Tuetal.
,
2016
)
multipleAWS
g2.2xlargeinstances
32.4%
DNN(
Mayetal.
,
2017
)
multipleAWS
g2.2xlargeinstances
30.9%
SparseKernel(
Mayetal.
,
2017
)
(uselearnedfeatures)
SUSY
4

10
6
19.7%
58s
6mon
1GTXTitanX
19.8%
EigenPro(
Ma&Belkin
,
2017
)
4mon
1TeslaK40c
19.6%
FALKON(
Rudietal.
,
2017
)
36mon
IBMPOWER8
ˇ
20%
Hierarchical(
Chenetal.
,
2016
)
y
OurmethodusestheconvolutionalfeaturesfromInception-ResNet-v2andFalkonusestheconvolutionalfeaturesfromInception-v4.
Bothneuralnetworkmodelsarepresentedin(
Szegedyetal.
,
2017
)andshownearlyidenticalperformance.
z
TherearetwosamplingratesforTIMIT,whichresultintwotrainingsetsofdifferentsizes.
(a)MNIST(
10
5
subsamples),stopwhentrainmse
<
1

10

4
(b)TIMIT(
10
5
subsamples),stopwhentrainmse
<
2

10

4
Figure2:Timetoconvergewithdifferentbatchsizesandoptimalstepsizes
5.3BatchsizeandGPUutilization
ThenumberofoperationrequiredforoneiterationofSGD
islinearinthebatchsize.Thusweexpectthattimere-
quiredperiterationforapuresequentialmachinewould
scalelinearlywithbatchsize.Ontheotherhandanideal
paralleldevicewithnooverheadrequiresthesameamount
oftimetoprocessanymini-batch.InFigure
3a
,weshow
KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
(a)Timepertrainingiterationofdifferentbatchsizesonactual
andidealdevices(TIMIT,
n
=10
5
,
d
=440
)
(b)TimepertrainingepochonGPUwithdifferentsizesoftrain
set(
n
,whichisalsothemodelsize)andbatchthatintothe
GPUmemory
Figure3:Timeperiteration/epochfortrainingwithdifferentbatchsizes
howthetrainingtimeperiterationforactualGPUdepends
onthebatchsize.Weseethatforsmallbatchsizestime
periterationisnearlyconstant,likethatofanidealparallel
device,andstarttoincreaseforlargerbatches.
Notethatinadditiontotimeperiterationweneedtocon-
sidertheoverheadassociatedtoeachiteration.Larger
batchsizesincurlessoverheadperepoch.Thisphe-
nomenonisknowninthesystemsliteratureasAmdahl's
law(
Rodgers
,
1985
).InFigure
3b
weshowGPUtime
per
epoch
fordifferentmodel(trainingset)size(
n
).Wesee
consistentspeed-upsbyincreasingmini-batchsizeacross
modelsizesuptomaximumGPUutilization.
5.4ﬁInteractiveﬂtrainingforexploratorymachine
learning
Dataset
Size
Feature
EigenPro
(GPU)
ThunderSVM
(GPU)
LibSVM
(CPU)
TIMIT
1

10
5
440
15s
480s
1.6h
SVHN
7

10
4
1024
13s
142s
3.8h
MNIST
6

10
4
784
6s
31s
9m
CIFAR-10
5

10
4
1024
8s
121s
3.4h
Table3:Comparingtrainingtimeofkernelmachines
Mostpracticaltasksofmachinelearningrequiremultiple
trainingrunsforparameterandfeatureselection,evaluating
appropriatenessofdataorfeaturestoagiventask,andvar-
iousotherexploratorypurposes.Whileusinghours,days
orevenmonthsofmachinetimemaybenecessarytoim-
proveonthestateoftheartinlarge-scalecertainproblems,
itistootime-consumingandexpensiveformostdataanal-
ysiswork.Thus,itisverydesirabletotrainin
closetorealtime.Oneoftheadvantagesofourapproachis
thecombinationofitsspeedonsmallandmediumdatasets
usingstandardhardwaretogetherwiththeautomaticopti-
mizationparameterselection.
Wedemonstratethisonseveralsmallerdatasets(
10
4
˘
10
5
points)usingaTitanXpGPU(seeTable
3
).Weseethatin
everycasetrainingtakesnomorethan
15
seconds,mak-
ingmultiplerunsforparameterandfeatureselectioneasily
feasible.
Forcomparison,wealsoprovidetimingsforLibSVM,
apopularandwidelyusedkernellibrary(
Chang&Lin
,
2011
)andThunderSVM(
Wenetal.
,
2018
),afastGPU
implementationforLibSVM.WeshowtheresultsforLib-
SVM
4
andThunderSVMusingthesamekernelwiththe
sameparameter.Westoppediterationofourmethodwhen
theaccuracyontestexceededthatofLibSVM,whichour
methodwasabletoachieveoneverydataset.Whilenot
intendedasacomprehensiveevaluation,theofour
methodfortypicaldataanalysistasksareevident.
5
Fast
trainingalongwiththeﬁworry-freeﬂoptimizationcreate
anﬁinteractive/responsiveﬂenvironmentforusingkernel
methodsinmachinelearning.Furthermore,thechoiceof
kernel(e.g.,LaplacianorGaussian)anditssingleband-
widthparameterisusuallyfarsimplerthanthemultiplepa-
rametersinvolvedintheselectionofarchitectureinneural
networks.
4
Weusethesvmpackageinscikit-learn0.19.0.
5
OuralgorithmisstillmuchfasterthanLibSVMwhenrun-
ningonCPU.Forexample,trainingondatasetsshowninTable
3
takesbetweenoneandthreeminutes.
KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
5.5PracticalTechniquesforAcceleratingInference
Wewouldliketopointouttwosimpleandpracticaltech-
niquestoaccelerateandsimplifykerneltraining.Theuse
oftheLaplaciankernelisnotcommonintheliteratureand
inouropiniondeservesmoreattention.WhilePCAisfre-
quentlyusedtospeeduptraining(andsometimestoim-
provethetestresults),itisusefultostatethetechniqueex-
plicitly.
Choiceofkernelfunction.
InmanycasesLaplace(expo-
nential)kernel
k
(
x
x
x;z
z
z
)=
e

k
x
x
x

z
z
z
k
˙
producesresultscompa-
rableorbetterthanthoseforthemorestandardGaussian
kernel.MoreovertheLaplaciankernelhasseveralpracti-
caladvantagesovertheGaussian(consistentwiththe
ingsreportedin(
Belkinetal.
,
2018
)).(1)Laplaciangen-
erallyrequiresfewerepochsfortrainingtoobtainthesame
qualityresult.(2)Thebatchvalue
m

istypicallylarger
fortheLaplaciankernelallowingformoreeffectiveparal-
lelization.(3)TestperformancefortheLaplaciankernelis
empiricallymorerobusttothebandwidthparameter
˙
,sig-
reducingtheneedforcarefulparametertuningto
achieveoptimalperformance.
DimensionalityreductionbyPCA.
Recallthatthepri-
marycostofoneEigenProiterationis
n

md
forthenum-
berofoperationsand
n

(
m
+
d
)
formemorywhere
d
is
thenumberoffeatures.Thusreducingthedimensionof
thefeaturesresultsincomputationalsavings.It
isoftenpossibletoreducedimensionalityof
thedatawithoutperceptiblychangingclass(orre-
gression)accuracybyapplyingthePrincipalComponents
Analysis(PCA).Forexample,usingPCAtoreducethe
featuredimensionalityfrom
1536
to
500
forImageNetde-
creasestheaccuracybylessthan
0
:
2%
.
6C
ONCLUSIONAND
F
UTURE
D
IRECTIONS
Bestpracticesfortrainingmodernlarge-scalemodelsare
concernedwithlinearscaling.Mostoftheworkisbased
onanimplicitbutwidelyheldassumptionthatthelimit
oflinearscaling,
m

,cannotbecontrolledorchangedin
practice.Incontrast,thispapershowsthatthelimitoflin-
earscalingcanbeanalyticallyandautomaticallyadapted
toagivencomputingresource.Thisaddsanewdi-
mensionforpotentialimprovementsintraininglarge-scale
models.
Themaintechnicalcontributionofthispaperisanew
learningframework(EigenPro2.0)thatextendslinearscal-
ingtomatchtheparallelcapacityofacomputationalre-
source.Theframeworkisbasedonextractinglimited
secondorderinformationtomodifytheoptimizationpro-
cedurewithoutchangingthelearnedpredictorfunction.
Whileourpaperdealswithkernelmachines,similarideas
areapplicabletoamuchbroaderclassoflearningarchitec-
turesincludingdeepneuralnetworks.
Thealgorithmsdevelopedinthispaperallowforveryfast
kernellearningonsmallerdatasetsandeasyscalingtosev-
eralmilliondatapointsusingamodernGPU.Itislikely
thatmoreeffectivememorymanagementtogetherwithbet-
terhardwarewouldallowscalingupto
10
7
datapointswith
reasonabletrainingtime.Goingbeyondthatto
10
8
ormore
datapointsusingmulti-GPUsetupsisthenextnaturalstep
forkernelmethods.
A
CKNOWLEDGEMENTS
WethankRaefBassilyfordiscussionsandhelpfulcom-
mentsandAlexLeeforrunningThunderSVMcompar-
isons.WethankLorenzoRosascoandLuigiCarratinofor
sharingpreprocessedImageNetfeatures.WeusedaTitan
XpGPUprovidedbyNvidia.Weacknowledge
supportfromNSF.
R
EFERENCES
Aronszajn,N.Theoryofreproducingkernels.
Transactions
oftheAmericanmathematicalsociety
,68(3):337Œ404,
1950.
Avron,H.,Clarkson,K.,andWoodruff,D.Fasterker-
nelridgeregressionusingsketchingandprecondition-
ing.
arXivpreprintarXiv:1611.03220
,2016.
Belkin,M.,Ma,S.,andMandal,S.Tounderstanddeep
learningweneedtounderstandkernellearning.
arXiv
preprintarXiv:1802.01396
,2018.
Chang,C.-C.andLin,C.-J.Libsvm:alibraryforsupport
vectormachines.
ACMtransactionsonintelligentsys-
temsandtechnology(TIST)
,2(3):27,2011.
Chen,J.,Avron,H.,andSindhwani,V.Hierarchicallycom-
positionalkernelsforscalablenonparametriclearning.
arXivpreprintarXiv:1608.00860
,2016.
Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,and
Fei-Fei,L.Imagenet:Alarge-scalehierarchicalimage
database.In
ComputerVisionandPatternRecognition,
2009.CVPR2009.IEEEConferenceon
,pp.248Œ255.
IEEE,2009.
Garofolo,J.S.,Lamel,L.F.,Fisher,W.M.,Fiscus,J.G.,
andPallett,D.S.Darpatimitacoustic-phoneticconti-
nousspeechcorpuscd-rom.
NISTspeechdisc
,1-1.1,
1993.
Golmant,N.,Vemuri,N.,Yao,Z.,Feinberg,V.,Gho-
lami,A.,Rothauge,K.,Mahoney,M.W.,andGonza-
lez,J.Onthecomputationalinefyoflargebatch
sizesforstochasticgradientdescent.
arXivpreprint
arXiv:1811.12941
,2018.
KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
Goyal,P.,Doll
´
ar,P.,Girshick,R.,Noordhuis,P.,
Wesolowski,L.,Kyrola,A.,Tulloch,A.,Jia,Y.,andHe,
K.Accurate,largeminibatchsgd:Trainingimagenetin
1hour.
arXivpreprintarXiv:1706.02677
,2017.
Huang,P.-S.,Avron,H.,Sainath,T.N.,Sindhwani,V.,and
Ramabhadran,B.Kernelmethodsmatchdeepneural
networksontimit.In
ICASSP
,pp.205Œ209.IEEE,2014.
Jia,X.,Song,S.,He,W.,Wang,Y.,Rong,H.,Zhou,F.,
Xie,L.,Guo,Z.,Yang,Y.,Yu,L.,etal.Highlyscal-
abledeeplearningtrainingsystemwithmixed-precision:
Trainingimagenetinfourminutes.
arXivpreprint
arXiv:1807.11205
,2018.
Krizhevsky,A.Oneweirdtrickforparallelizingconvolu-
tionalneuralnetworks.
arXivpreprintarXiv:1404.5997
,
2014.
Krizhevsky,A.andHinton,G.Learningmultiplelayersof
featuresfromtinyimages.Master'sthesis,Universityof
Toronto,2009.
LeCun,Y.,Bottou,L.,Bengio,Y.,andHaffner,P.Gradient-
basedlearningappliedtodocumentrecognition.In
Pro-
ceedingsoftheIEEE
,pp.2278Œ2324,1998.
Leventhal,D.andLewis,A.S.Randomizedmethodsfor
linearconstraints:convergenceratesandconditioning.
MathematicsofOperationsResearch
,35(3):641Œ654,
2010.
Lu,Z.,May,A.,Liu,K.,Garakani,A.B.,Guo,D.,Bellet,
A.,Fan,L.,Collins,M.,Kingsbury,B.,Picheny,M.,and
Sha,F.Howtoscaleupkernelmethodstobeasgoodas
deepneuralnets.
arXivpreprintarXiv:1411.4000
,2014.
Ma,S.andBelkin,M.Divingintotheshallows:acompu-
tationalperspectiveonlarge-scaleshallowlearning.In
AdvancesinNeuralInformationProcessingSystems
,pp.
3781Œ3790,2017.
Ma,S.,Bassily,R.,andBelkin,M.Thepowerof
interpolation:Understandingtheeffectivenessofsgd
inmodernover-parametrizedlearning.
arXivpreprint
arXiv:1712.06559
,2017.
May,A.,Garakani,A.B.,Lu,Z.,Guo,D.,Liu,K.,Bellet,
A.,Fan,L.,Collins,M.,Hsu,D.,Kingsbury,B.,etal.
Kernelapproximationmethodsforspeechrecognition.
arXivpreprintarXiv:1701.03577
,2017.
McCandlish,S.,Kaplan,J.,Amodei,D.,andTeam,O.D.
Anempiricalmodeloflarge-batchtraining.
arXiv
preprintarXiv:1812.06162
,2018.
Netzer,Y.,Wang,T.,Coates,A.,Bissacco,A.,Wu,B.,and
Ng,A.Readingdigitsinnaturalimageswithunsuper-
visedfeaturelearning.In
NIPSworkshop
,volume2011,
pp.4,2011.
Rodgers,D.P.Improvementsinmultiprocessorsystemde-
sign.In
SIGARCH
,1985.
Rudi,A.,Carratino,L.,andRosasco,L.Falkon:Anop-
timallargescalekernelmethod.In
AdvancesinNeural
InformationProcessingSystems
,pp.3891Œ3901,2017.
Smith,S.L.,Kindermans,P.-J.,andLe,Q.V.Don'tdecay
thelearningrate,increasethebatchsize.
arXivpreprint
arXiv:1711.00489
,2017.
Szegedy,C.,Ioffe,S.,Vanhoucke,V.,andAlemi,A.A.
Inception-v4,inception-resnetandtheimpactofresidual
connectionsonlearning.In
AAAI
,volume4,pp.12,
2017.
Tak
´
ac,M.,Bijral,A.S.,Richt
´
arik,P.,andSrebro,N.Mini-
batchprimalanddualmethodsforSVMs.In
ICML(3)
,
pp.1022Œ1030,2013.
Tu,S.,Roelofs,R.,Venkataraman,S.,andRecht,B.Large
scalekernellearningusingblockcoordinatedescent.
arXivpreprintarXiv:1602.05310
,2016.
Wen,Z.,Shi,J.,Li,Q.,He,B.,andChen,J.Thunder-
svm:afastsvmlibraryongpusandcpus.
TheJournal
ofMachineLearningResearch(JMLR)
,19(1):797Œ801,
2018.
Yao,Y.,Rosasco,L.,andCaponnetto,A.Onearlystop-
pingingradientdescentlearning.
ConstructiveApprox-
imation
,26(2):289Œ315,2007.
You,Y.,Gitman,I.,andGinsburg,B.Largebatch
trainingofconvolutionalnetworks.
arXivpreprint
arXiv:1708.03888
,2017.
Zhang,C.,Bengio,S.,Hardt,M.,Recht,B.,andVinyals,
O.Understandingdeeplearningrequiresrethinkinggen-
eralization.
arXivpreprintarXiv:1611.03530
,2016.
KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
Appendices
AD
ATASETS
Wereducemulticlasslabelstomultiplebinarylabels.For
imagedatasetsincludingMNIST(
LeCunetal.
,
1998
),
CIFAR-10(
Krizhevsky&Hinton
,
2009
),andSVHN(
Net-
zeretal.
,
2011
),colorimagesaretransformedto
grayscaleimages.Wethenrescaletherangeofeachfea-
tureto
[0
;
1]
.ForImageNet(
Dengetal.
,
2009
),weuse
thetop800PCAcomponentsofsomeconvolutionalfea-
turesextractedfromInception-ResNet-v2(
Szegedyetal.
,
2017
).ForTIMIT(
Garofoloetal.
,
1993
),wenormalize
eachfeaturebyz-score.
BS
ELECTIONOF
K
ERNELANDITS
B
ANDWIDTH
WeuseGaussiankernel
k
(
x;y
)=exp(

k
x

y
k
2
2
˙
2
)
and
Laplacekernel
k
(
x;y
)=exp(

k
x

y
k
˙
)
inourexperi-
ments.Notethatthekernelbandwidth
˙
isselectedthrough
cross-validationonasmallsubsampleddataset.InTable
4
,
wereportthekernelanditsbandwidthselectedforeach
datasettoachievethebestperformance.Wealsoreport
theparametersthatarecalculatedautomaticallyusingour
method.Notethatinpracticewechooseavalue
q
(inthe
parenthesis)thatislargerthanthe
q
correspondingto
m
G
.
Increasing
q
appearstoleadtofasterconvergence.Weuse
asimpleheuristictoautomaticallyobtainsuch
q
basedon
theeigenvalueandthesizeoftheedcoordinateblock
6
.
CA
NALYSISOF
A
CCELERATION
Claim
(Acceleration).Usingtheadaptivekernel
k
G
decreasestheresourcetimerequiredfortrainingoverthe
originalkernel
k
byafactorof
a
ˇ

(
K
)

(
K
G
)

m
max
G
m

(
k
)
.
Wewillnowgiveaderivationofthisaccelerationfactor
a
,
basedontheanalysisofSGDintheinterpolatingsetting
in(
Maetal.
,
2017
).
Asbefore,let
(
x
x
x
1
;y
1
)
;:::;
(
x
x
x
n
;y
n
)
bethedata,and
let
K
bethecorresponding(normalized)kernelmatrix
K
ij
=
k
(
x
x
x
i
;x
x
x
j
)
=n
.WestartbyrecallingtheSGDit-
erationinthekernelsettingforamini-batchofsize
m
,
(
x
x
x
t
1
;y
t
1
)
;:::;
(
x
x
x
t
m
;y
t
m
)
,
f
 
f



2
m
(
m
X
i
=1
(
f
(
x
x
x
t
i
)

y
t
i
)
k
(
x
x
x
t
i
;

)
)
6
ForSUSYwedirectlyspecifyalarge
q
foroptimalperfor-
mance.
Whenstepsize

ischosenoptimally,wecanapplyThe-
orem4in(
Maetal.
,
2017
)tobounditsconvergenceper
iterationtowardtheoptimal(interpolating)solution
f

as
follows:
E
h
k
f
t

f

k
2
K
i

g

K
(
m
)

E
h
k
f
t

1

f

k
2
K
i
Here
g

K
(
m
)
isakernel-dependentupperboundonthecon-
vergencerate.
Thefastest(uptoasmallconstantfactor)convergence
rateperiterationisobtainedwhenusingmini-batchsize
m

(
K
)=

(
K
)

1
(
K
)


n
(
K
)
(orlarger).Kernelsusedinprac-
tice,suchasGaussiankernels,haverapideigendecay(
Ma
&Belkin
,
2017
),i.e.,

1
(
K
)
˛

n
(
K
)
.Hencewehave
m

(
k
)
ˇ

(
K
)

1
(
K
)
.
Thuswecanwriteanaccurateapproximationofconver-
gencerate
g

K
(
m

(
K
))
asfollows:


K
,
g

K
(
m

(
K
))=1

m

(
K
)


n
(
K
)

(
K
)+(
m

1)

n
(
K
)
ˇ
1


n
(
K
)

1
(
K
)
1+(
m

1)

n
(
K
)

(
K
)
Wenowobservethat

=max
i
=1
;:::;n
k
(
x
x
x
i
;x
x
x
i
)

tr
(
K
)
.
Henceforthemini-batchsize
m
muchsmallerthan
n
we
have
(
m

1)

n
(
K
)

(
K
)

(
m

1)

n
(
K
)
tr
(
K
)

m

1
n
˝
1
Thatallowsustowrite


K
ˇ
1


n
(
K
)

1
(
K
)
Wewillnowapplythisformulatotheadaptivekernel
k
G
.
Recallthatitscorrespondingkernelmatrix
K
G
the
top-
q
eigenspectrumof
K
suchthat

i
(
K
G
)=
(

q
(
K
)
if
i

q

i
(
K
)
if
i>q
Thustheconvergenceratefor
k
G
is


K
G
ˇ
1


n
(
K
G
)

1
(
K
G
)
=1


n
(
K
)

q
(
K
)
Next,wecomparethenumberofiterationsneededtocon-
vergetoerror

usingtheoriginalkernel
k
andtheadaptive
kernel
k
G
.
First,weseethatforkernel
k
ittakes
t
=
log

log


K
iterations
togobelowerror

suchthat
E
h
k
f
t

f

k
2
K
i



E
h
k
f
0

f

k
2
K
i
KernelmachinesthatadapttoGPUsforeffectivelargebatchtraining
Table4:Selectedkernelbandwidthandcorrespondingoptimizationparameters
Dataset
Sizeof(Subsampled)
TrainSet
Kernel
Bandwidth
Trainepochs
CalculatedParameters
q
(adjusted
q
)
m
=
m
G

MNIST
1

10
6
Gaussian
5
4
93(330)
735
379
TIMIT
1
:
1

10
6
Laplacian
15
3
52(128)
682
343
ImageNet
1
:
3

10
6
Gaussian
16
1
2(321)
294
149
SUSY
6

10
5
Gaussian
4
1
106(850)
1687
849
Noticethat

n
(
K
)

tr
(
K
)
n
=
1
n
fornormalizedkernel
matrix
K
.Thusforlarge
n
,wehave
log

log


K
=
log

log(1


n
(
K
)

1
(
K
)
)
ˇ
log



1
(
K
)

n
(
K
)
Inotherwords,thenumberofiterationsneededtoconverge
withkernel
k
isproportionalto

1
(
K
)

n
(
K
)
.
Bythesametoken,toachieveaccuracy

,theadaptiveker-
nel
k
G
needs
log

log


K
G
ˇ
log



q
(
K
)

n
(
K
)
iteration.
Therefore,toachieveaccuracy

,trainingwiththeadaptive
kernel
k
G
needs

q
(
K
)

1
(
K
)
asmanyiterationsastrainingwith
theoriginalkernel
k
.
Tounpackthemeaningoftheratio

q
(
K
)

1
(
K
)
,werewriteitas

q
(
K
)

1
(
K
)
=

1
(
K
G
)

1
(
K
)
=

(
K
G
)

(
K
)

m

(
K
)
m

(
K
G
)
=

(
K
G
)

(
K
)

m

(
K
)
m
max
G
Recallthatbytheassumptionsmadeinthepaper(1)any
iterationforkernel
K
withmini-batchsize
m

m
max
G
re-
quiresthesameamountofresourcetimetocompleteon
G
,
(2)iterationofkernels
K
and
K
G
requirethesameresource
timeforany
m
(negligibleoverhead).
Since
m

(
K
)

m

(
K
G
)
ˇ
m
max
G
,weseethatoneiter-
ationofbatchsize
m

(
K
)
andoneiterationofbatchsize
m

(
K
G
)
takethesameamountoftimeforeitherkernel.
Wethusconcludethattheadaptivekernelacceleratesover
theoriginalkernelbyafactorofapproximately

(
K
)

(
K
G
)

m
max
G
m

(
K
)
Remark.
Noticethatouranalysisisbasedonusingupper
boundsforconvergence.Whiletheseboundsaretight((
Ma
etal.
,
2017
),Theorem3),thereisnoguaranteesoftight-
nessfordataandchoiceofkernelusedinpractice.
Remarkably,thevaluesofparametersobtainedbyusing
theseboundsworkverywellinpractice.Moreover,accel-
erationpredictedtheoreticallycloselymatchesacceleration
observedinpractice.
"
54,Teaching Machines to Code: Neural Markup Generation with Visual Attention,http://arxiv.org/pdf/1802.05415v2.pdf,https://github.com/untrix/im2latex,"TeachingMachinestoCode:
NeuralMarkupGenerationwithInterpretable
Attention
SumeetS.Singh
IndependentResearcher
Saratoga,CA95070
sumeet@singhonline.info
Abstract
Wepresentaneuraltransducermodelwithvisualattentionthatlearnstogenerate
L
A
T
E
Xmarkupofareal-worldmathformulagivenitsimage.Applyingsequence
modelingandtransductiontechniquesthathavebeenverysuccessfulacrossmodal-
itiessuchasnaturallanguage,image,handwriting,speechandaudio;weconstruct
animage-to-markupmodelthatlearnstoproducesyntacticallyandsemantically
correctL
A
T
E
Xmarkupcodeover150wordslongandachievesaBLEUscoreof
89%;improvinguponthepreviousstate-of-artfortheIm2Latexproblem.Wealso
demonstratewithheat-mapvisualizationhowattentionhelpsininterpretingthe
modelandcanpinpoint(localize)symbolsontheimageaccuratelydespitehaving
beentrainedwithoutanyboundingboxdata.
1Introduction
Inthepastdecade,deepneuralnetworkmodelsbasedonRNNs
1
,CNNs
2
and`attention'[
29
]have
beenshowntobeverypowerfulsequencemodelersandtransducers.Theirabilitytomodeljoint
distributionsofreal-worlddatahasbeendemonstratedthroughremarkableachievementsinabroad
spectrumofgenerativetaskssuchas;imagesynthesis[
27
,
28
,
22
,
25
],imagedescription[
16
,
31
,
14
,
21
,
30
],videodescription[
7
],speechandaudiosynthesis[
26
],handwritingrecognition[
12
,
2
],
handwritingsynthesis[
9
],machinetranslation[
5
,
1
,
15
,
24
],speechrecognition[
10
,
4
,
11
],etc.[
8
,
29
]
Oneclassofsequencemodelsemploytheso-called
encoder-decoder
[
5
]or
sequence-to-sequence
[
24
]architecture,whereinan
encoder
encodesasourcesequenceintofeaturevectors,whicha
decoder
employstoproducethetargetsequence.Thesourceandtargetsequencesmayeitherbelongtothe
samemodality(e.g.inmachinetranslationuse-cases)ordifferentmodalities(e.g.inimage-to-text,
text-to-image,speech-to-text);theencoder/decodersub-modelsbeingconstructedaccordingly.
Theentiremodelistrainedend-to-endusingsupervised-learningtechniques.Inrecentyears,this
architecturehasbeenaugmentedwithan
attentionandalignment
modelwhichselectsasubsetof
thefeaturevectorsfordecoding.Ithasbeenshowntohelpwithlongersequences[
1
,
19
].Among
otherthings,thisarchitecturehasbeenusedforimage-captioning[
31
].Inourworkweemploya
encoder-decoderarchitecturewithattention,tomapimagesofmathformulasintocorresponding
L
A
T
E
Xmarkupcode.Thecontributionsofthispaperare:1)SolvestheIm2Latexproblem
100
and
improvesoverthepreviousbestreportedBLEUscoreby1.27%BLEU,2)Pushestheboundariesof
theneuralencoder-decoderarchitecturewithvisualattention,3)Analysesvariationsofthemodel
andcostfunction.wenotethechangestothebasemodel[
31
]andwhatimpactthose
hadonperformance,4)Demonstratestheuseofattentionvisualizationformodelinterpretationand
1
RecurrentNeuralNetwork.
2
ConvolutionalNeuralNetworksandvariantssuchasdilatedCNNs[32].
Preprint.Workinprogress.
arXiv:1802.05415v2  [cs.LG]  15 Jun 20185)Demonstrateshowattentioncanbeusedtolocalizeobjects(symbols)inanimagedespitehaving
beentrainedwithoutboundingboxdata.
1.1TheI
M
2L
ATEX
problem
The
I
M
2L
ATEX
ProblemisarequestforresearchproposedbyOpenAI.Thechallengeistobuild
aNeuralMarkupGenerationmodelthatcanbetrainedend-to-endtogeneratetheL
A
T
E
Xmarkup
ofamathformulagivenitsimage.Dataforthisproblemwasproducedbyrenderingsingle-line
real-worldL
A
T
E
XformulasobtainedfromtheKDDCup2003dataset.Theresultinggrayscale
imageswereusedastheinputsampleswhiletheoriginalmarkupwasusedasthelabel/target
sequence.Eachtraining/testsample(Figure1)iscomprisedofaninputimage
x
andacor-
S
0
=
X
l
1

2
l
Tr
˚
a
l
˚
a

l
+
X
l
1
2

2
l
Tr
f
a
l
f
a

l
+
X
r
1
g
r
Tr

 
a
r
 
a
r
:
S_{0}=\
sum
_{l}\frac{1}{2\
Delta
_{l}^{2}}\mathrm{Tr}\,\
phi
_{l}^{a}\
phi
_{

l}^{a}+\
sum
_{l}
\frac{1}{2\
epsilon
_{l}^{2}}\mathrm{Tr}\,f_{l}^{a}f_{

l}^{a}+\
sum
_{r}\frac{1}{g_{r}}\
mathrm{Tr}\,\
bar
{\
psi
}_{r}^{a}\
psi
_{r}^{a}\,.
S_{0}=\
sum
_{l}{\frac{1}{2\
Delta
_{l}^{2}}}\mathrm{Tr}\,\
phi
_{l}^{a}\
phi
_{

l}^{a}+\
sum
_{l}
{\frac{1}{2\
epsilon
_{l}^{2}}}\mathrm{Tr}\,f_{l}^{a}f_{

i}^{a}+\
sum
_{r}{\frac{1}{g_{r}}
}\mathrm{Tr}\,\
psi
_{r}^{a}\
psi
_{r}^{a}\,.
Figure1:Atrainingsample:Atthetopistheinputimage
x
,middlethetargetsequence
y
(
˝
=145
)
andbottomthepredictedsequence
^
y
(
˝
=148
).Eachspace-separatedwordin
y
and
^
y
2
V
respondingtargetL
A
T
E
X-sequence
y
oflength
˝
.Eachword
y
ofthetargetsequence,belongs
tothevocabularyofthedatasetplustwospecialtokens:beginning-of-sequence<bos>andend-
of-sequence<eos>.Denotingimagedimensionsas
H
I
;W
I
and
C
I
andthevocabularyasaset
V
of
K
words,werepresent
x
2
R
H
I

W
I

C
I
,
V
:=
f
L
A
T
E
Xtokens
;
<eos>
;
<bos>
g
;
j
V
j
=
K
and
y
:=(
y
1
;:::;
y
˝
);
y
t
2f
1
;:::;K
g
.ThetaskistogeneratemarkupthataL
A
T
E
Xcompilerwill
renderbacktotheoriginalimage.Therefore,ourmodelneedstogeneratesyntacticallyandseman-
ticallycorrectmarkup,bysimply`looking'attheimage:i.e.itshouldjointlymodelvisionand
language.
2Imagetomarkupmodel
Ourmodel(Figure2a)hasthesamebasicarchitectureas[
31
](whichwecallourbaselinemodel)in
thewaytheencoder,decoderandavisualattentioninteract.Howevertherearesignidifferences
inthesub-modelswhichwenotateintheremainderofthispaperandintheappendix.
2.1Encoder
Allimagesarestandardizedtoaedsizebycenteringandpaddingwithwhite
pixels.Thentheyarelinearlytransformed(whitened)tolieintherange[-
0.5,0.5].AdeepCNNthenencodesthewhitenedimageintoa
visualfeaturegrid

A
,having

H


W
(i.e.height

width)
visualfeaturevectors
a
(

h;

w
)
2
R

D
.
A
:=
2
6
4
a
(1
;
1)
:::
a
(1
;W
)
.
.
.
.
.
.
.
.
.
a
(
H;
1)
:::
a
(
H;W
)
3
7
5
(1)
Thevisualfeaturevectorsarethenconcatenated
(pooled)togetherinstridesofshape
[
S
H
;S
W
]
;beget-
ting
pooledfeaturevectors
a
(
h;w
)
2
R
D
,where
D
=

D

S
H

S
W
.Theresultingfeaturemap
A
,hasa
correspondinglyshrunkenshape
[
H;W
]
;where
H
=

H=S
H
and
W
=
S
W
=

W
.
Eachpooledfeaturevectorcanbeviewedasarectangularwindowintotheimage,boundedby
itsreceptive
3
Theideabehindthisistopartitiontheimageintospatiallylocalizedregional
encodingsandsetupadecoderarchitecture(Section2.2)thatselects/emphasizesonlytherelevant
3
Neighboringregionsoverlapbuteachregionisdistinctoverall.
2
(a)
(b)
Figure2:(a)Modeloutlineshowingmajorpartsofthemodel.Beamsearchdecoderisonlyused
duringinferencing,nottraining.LSTM-StackandAttentionmodeljointlyformaConditioned
AttentiveLSTMstack(CALSTM)whichcanitselfbestacked.(b)ExpandedviewofDecoderRNN
showingitssub-models.TherearethreenestedRNNcellsinall:ThedecoderRNN(DRNN)atthe
toplevel,nestingtheCALSTMwhichneststheLSTM-Stack.TheInitModeldoesnotparticipatein
recurrence,thereforeitsisshownoutsidethebox.
regionsateachtime-step
t
,whiletherest.Bahdanauetal.
1
showed
thatsuchpiecewiseencodingenablesmodelinglongersequencesasopposedtomodelsthatencode
theentireinputintoasinglefeaturevector[
24
,
5
].
4
Poolingallowsustoconstructencoderswith
differentreceptivesizes.Weshareresultsoftwosuchmodels:
I
2
L
-
NOPOOL
withnofeature
poolingandpooledfeaturegridshape[4,34]and
I
2
L
-
STRIPS
havingstride[4,1]andpooledfeature
gridshape[1,34].Finally,forconveniencewerepresent
A
asasequence
a
(Equation2).
Seetheappendixformoredetails.
a
:=(
a
1
;:::;
a
L
);
a
l
2
R
D
;
l
=
H
(
h

1)+
w
;
L
=
HW
(2)
2.2Decoder
p
t
:
f
1
;:::;K
g!
[0
;
1]
y
t
˘
p
t
p
t
(
y
t
):=
P
r
(
y
t
j
y
<t
;
a
)
(3)
P
r
(
y
j
a
)=
˝
Y
t
=1
p
t
(
y
t
)
(4)
Thedecoderisalanguagemodelerandgenerator.Itis
aRecurrentNeuralNetwork(DRNNinFigure2b)that
modelsthediscreteprobablydistribution
p
t
,oftheoutput
word
y
t
,conditionedonthesequenceofpreviouswords
y
<t
andrelevantregionsoftheencodedimage
a
5
(Equa-
tions3).Probabilityoftheentireoutputsequence
y
given
image
a
isthereforegivenbyEquation4.
4
Thatsaid,Bahdanauetal.
1
employabidirectional-LSTM[
8
]encoderwhosereceptivedoesencompass
theentireinputanyway!(Althoughthatdoesnotnecessarilymeanthatthebi-LSTMwillencodetheentire
image).LikewiseDengetal.
6
whoalsosolvethe
I
M
2L
ATEX
problemalsoemployabi-directionalLSTM
stackedontopofaCNN-encoderinordertogetfullviewoftheimage.Incontrast,ourvisualfeaturevectors
holdonlyspatiallylocalinformationwhichwefoundaresuftoachievegoodaccuracy.Thisisprobably
owingtothenatureoftheproblem;i.e.transcribingaone-linemathformulaintoL
A
T
E
Xsequencerequiresonly
localinformationateachstep.
5
Thisisnowaverystandardwaytomodelsequence(sentence)probabilitiesinneuralsequence-generators.
See[24]forexample.
3
DRNN
:
f
a
;
y
t

1
;
C
t

1
g!f
p
t
;
C
t
g
(5)
TheDRNNreceivesthepreviousword
y
t

1
andencodedimage
a
asinputs.Inaddition,itmaintains
aninternalstate
C
t
thatpropagatesinformation(features)extractedfromaninitialstate,theoutput
sequenceunrolledthusfarandimageregionsattendedtothusfar(Equation5).Itisascomplex
model,comprisedofthefollowingsub-models(Figure2b):1)ALSTM-Stack[
13
]responsible
formemorizing
C
t
andproducingarecurrentactivation
H
t
,2)AVisualattentionandalignment
modelresponsibleforselectingrelevantregionsoftheencodedimageforinputtotheLSTM-Stack,
6
3)ADeepOutputLayer[
20
]thatproducestheoutputprobabilities
p
t
,4)InitModel:Amodelthat
generatestheinitialstate
C
0
and5)Anembeddingmatrix
E
(learnedbytraining)thattransforms
y
t
intoadenserepresentation
2
R
m
.
2.2.1Inferencing
Afterthemodelistrained,theoutputsequenceisgeneratedbystartingwiththeword`bos'andthen
repeatedlysamplingfrom
p
t
until<eos>isproduced.Thesequenceofwordsthussampledisthe
predictedsequence:
^
y
:=(
^
y
1
;:::;
^
y
^
˝
);
^
y
t
2
R
K
.Forthisprocedureweusebeamsearchdecoding
[
8
]withabeamwidthof10.Figure1showsanexamplepredictedsequenceandFigures5and6
showexamplesofpredictionsrenderedintoimagesbyaL
A
T
E
X2
""
compiler.
2.2.2Visualattentionandalignmentmodel

t
:=(

t;
1
;:::;
t;L
)



0


t;l

1
P
L
l

t;l
=1
(6)

t
=
f
att
(
a
;
H
t

1
)
(7)
z
t
=

t
a
>
(8)
Aspreviouslyalluded,thedecodersoft
relevant(encoded)imageregionsateachstep.This
isimplementedvia.a`softattention'mechanism
7
whichcomputesaweightedsum
z
t
ofthepooled
featurevectors
a
l
.Thevisualattentionmodel
f
att
,
computestheweightdistribution

t
(Equations6,7and8).
f
att
ismodeledbyanMLP(details
intheappendix).Whileitisapossiblefor

t
toendupuniformlydistributedover
(
a
1
:::
a
L
)
,in
practiceweseeaunimodalshapewithmostoftheweightconcentratedon1-4neighborhood(see
Figure3)aroundthemode.Wecallthisneighborhoodthe
focal-region
-i.e.thefocusofattention.In
otherwordsweempiricallyobservethattheattentionmodel'sfocusis`sharp';convergingtowards
the`hardattention'formulationdescribedbyXuetal.
[31]
.Alsonotethat(Figure3),theattention
modelisabletoutilizetheextragranularityavailabletoitintheI2L-NOPOOLcaseandconsequently
generatesmuchsharperfocal-regionsthanI2L-STRIPS.
Furthermore,themodelalignsthefocal-regionwiththeoutputwordandthusscanstextontheimage
left-to-right(I2L-STRIPS)orleft-rightandup-down(I2L-NOPOOL)justlikeapersonwouldreadit
(Figure3).Wealsoobservethatitdoesn'tfocusonemptymarginsoftheimageexceptatthe
andlast(<eos>)stepswhichisquiteintuitivefordeterminingthebeginningorendoftext.
2.2.3LSTMstack
LSTM
q
:
f
x
q
t
;
h
q
t

1
;
c
q
t

1
g!f
h
q
t
;
c
q
t
g
1

q

Q
;
h
q
t
;
c
q
t
2
R
n
x
q
t
=
h
q

1
t
;
q
6
=1
(9)
x
1
t
=
f
z
t
;
Ey
t

1
g
ThecoresequencegeneratoroftheDRNNisamul-
tilayerLSTM[
9
](Figure2b).OurLSTMcellimple-
mentationfollowsGravesetal.
[11]
.TheLSTMcells
arestackedinamulti-layer[
33
,
20
]as
inEquation9.
LSTM
q
istheLSTMcellatposition
q
with
x
q
t
,
h
q
t
and
c
q
t
beingitsinput,hiddenactiva-
tionandcellstaterespectively.
LSTM
1
receivesthe
stack'sinput:softattentioncontext
z
t
andpreviousoutputword
Ey
t

1
.
LSTM
Q
producesthe
stack'soutput
H
t
=
h
Q
t
,whichissentuptotheDeepOutputLayer.Accordingly,thestack's
activation(
H
t
)andstate(
C
t
)areas:
H
t
=
h
Q
t
and
C
t
:=(
c
1
t
;:::;
c
Q
t
;
h
1
t
;:::;
h
Q
t
)
.We
6
TheLSTM-StackandVisualAttentionandAlignmentmodeljointlyformaConditionedAttentiveLSTM
(CALSTM);
H
t
and
C
t
beingitsactivationandinternalstaterespectively.Oursource-codeimplementsthe
CALSTMasaRNNcellwhichmaybeusedasadrop-inreplacementforaRNNcell.
7
`Soft'attentionasbyXuetal.[31]andoriginallyproposedbyBahdanauetal.[1].
4
Figure3:Focal-regionslearntbytheattentionmodel:totheleftbyI2L-STRIPSandtotheright
byI2L-NOPOOL.Imagedarknessisproportionalto

t
.Noticehow

t
concentratesontheimage
regioncorrespondingtotheoutputword(shownabovetheimage).The\fraccommandstartsa
fraction,\mathrmsetsafontand\eosisthe<eos>token.
donotuseskiporresidualconnectionsbetweenthecells.BothofourmodelshavetwoLSTMlayers
with
n
=1500
.Furtherdiscussionanddetailsofthismodelcanbefoundintheappendix.
2.2.4Deepoutputlayer
WeuseaDeepOutputLayer[
20
]toproducetheoutputprobabilities:
p
t
=
f
out
(
H
t
;
z
t
;
Ey
t

1
)
.
f
out
ismodeledbyanMLP.Notethattheoutputlayerreceivesskipconnec-
tionsfromtheLSTM-Stackinput(Equation9).Detailsofthismodelcanbefoundintheappendix.
2.2.5Initmodel
Figure4:InitModel.FC
=FullyConnectedLayer.
TheInitModel
f
init
,producestheinitialstate
C
0
oftheLSTM-Stack.
f
init
isintendedto`look'attheentireimage(
a
)andsetupthedecoder
appropriatelybeforeitstartsgeneratingtheoutput.
f
init
:
a
!
(
c
1
0
;:::;
c
Q
0
;
h
1
0
;:::;
h
Q
0
)
(10)
h
q
0
;
c
q
0
2
R
n
Thatsaid,sinceitonlyprovidesaverysmallimprovementinperformance
inexchangeforover7millionparameters,itsneedcouldbequestioned.
f
init
ismodeledasanMLPwithcommonhiddenlayersand
2
Q
distinct
outputlayers,oneforeachelementof
C
0
,connectedasinFigure4.See
theappendixformoredetailanddiscussion.
2.3Training
J
=

1
˝
log
(
P
r
(
y
j
a
))+

R
R
(11)
R
=
1
2
X


2
(11a)
Theentiremodelwastrainedend-to-endbymini-
mizingtheobjectivefunction
J
(Equation11)using
backpropagationthroughtime.Thetermin
Equation11istheaverage(per-word)logperplexity
ofthepredictedsequence
8
andisthemainobjective.
R
istheL2-regularizationterm,equaltoL2-normofthemodel'sparameters

(weightsandbiases)
8
i.e.Averagecross-entropy,negativelog-likelihoodornegativelog-probability.
5
and

R
isahyperparameterrequiringtuning.FollowingXuetal.
[31]
atwehadincluded
apenaltytermintendedtobiasthedistributionofthecumulativeattentionplacedonanimage-
location

l
:=
P
˝
t
=1

t;l
.Howeverweremoveditforvariousreasonswhicharediscussedinthe
appendixalongwithotherdetailsandanalyses.
Wesplitthedatasetintotwoedparts:1)trainingdataset=90-95%ofthedataand2)testdataset
5-10%.Atthebeginningofeachrun,5%ofthetrainingdatasetwasrandomlyheldoutasthe
validation-setandtheremainderwasusedfortraining.Therefore,eachsuchrunhadadifferent
training/validationdata-split,thusnaturallycross-validatingourlearningsacrossthedurationofthe
project.Wetrainedthemodelinminibatchesof56usingtheADAMoptimizer[
17
];periodically
evaluatingitoverthevalidationset
9
.Forefywebatchedthedatasuchthateachminibatch
hadsimilarlengthsamples.Fortheevaluationhowever,weedthetrainingandvalidation
datasetsplitandretrainedourmodelsforabout100epochs(
˘
2
1
2
days).Wethenpickedthe
model-snapshotswiththebestvalidationBLEUscoreandevaluatedthemodeloverthetest-dataset
forpublication.Table1liststhetrainingparametersandmetricsofvariousTraining
sequencepredictions(
^
y
)wereobtainedbyCTC-decoding[
10
]
p
t
.TrainingBLEUscorewasthen
calculatedover100consecutivemini-batches.WeusedtwoNvidiaGeForce1080Tigraphicscardsin
aparalleltowersOurimplementationusestheTwtoolkitandisdistributed
underAGPLlicense.
Table1:Trainingmetrics.

R
=0
:
00005
and

2
=0
:
9
forallruns.Thenumberafter@signisthe
trainingepochoftheselectedmodel-snapshot.

denotesthattherowcorrespondstoTable2.
DatasetModelInit

1
TrainTrain
ValidationValid'n
Model?
EpochsBLEU
BLEUED
I2L-140KI2L-STRIPSYes
0.51040.9361
0.8900@
72

0.0677
I2L-STRIPSNo
0.5750.9300
0.8874@
62
0.0691
I2L-NOPOOLYes
0.51040.9333
0.8909@
72

0.0684
I2L-NOPOOLNo
0.11190.9348
0.8820@
92
0.0738
Im2latex-90kI2L-STRIPSYes
0.51100.9366
0.8886@
77

0.0688
I2L-STRIPSNo
0.51610.9386
0.8810@
118
0.0750
3Results
GiventhattherearemultiplepossibleL
A
T
E
Xsequencesthatwillrenderthesamemathimage,ideally
weshouldperformavisualevaluation.However,sincethereisnowidelyacceptedvisualevaluation
metric,wereportcorpusBLEU(1,2,3&4grams)andper-wordLevensteinEditDistance
10
scores(see
Table2).Wealsoreporta(non-standard)exactvisualmatchscore
103
whichreportsthepercentage
ofexactvisualmatches,discardingallpartialmatches.Whilethepredictedandtargetedimages
matchinatleast70%
103
ofthecases,themodelgeneratesdifferentbutcorrectsequences(i.e.
y
6
=
^
y
)
inabout40%ofthecases(Figure5).Forthecaseswheretheimagesdonotexactlymatch,the
differencesinmostcasesareminor(Figure6).Overall,ourmodelsproducesyntacticallycorrect
sequences
11
foratleast99.85%ofthetestsamples(Table2).Pleasevisitourwebsitetoseehundreds
ofsamplevisualizations,analysesanddiscussions,data-setandsource-code.
3.1ModelInterpretabilityviaattention
SincetheLSTMstackonlyseesateredview(i.e.focal-region)oftheinput,itcanonlybase
itspredictionsonthefocal-regionsseenthusfarandinitial-state
C
0
.Furthersincetheinit-model
hasanegligibleimpactonperformancewecandropitfromthemodel(Table1)andtherebythe
9
Evaluationcyclewasrunonceortwiceperepochand/orwhenatrainingBLEUscorecalculatedon
sequencesdecodedusingCTC-Decoding[10]jumped.
10
i.e.Editdistancedividedbynumberofwordsinthetargetsequence.
11
i.e.ThosethatweresuccessfullyrenderedbyL
A
T
E
X2
""
.
103
Weusethe'matchwithoutwhitespace'algorithmprovidedbyDengetal.
[6]
whereintwoimagescountas
matchediftheymatchpixel-wisediscardingwhitecolumnsandallowingforupto5pixelimagetranslation(a
xquirk).Itoutputsabinarymatch/no-matchverdictforeachsample-i.e.partialmatcheshoweverclose,
areconsideredanon-match.
6
InputImage/RenderedSequence
y
len
^
y
len
0
T
+
q
+2

2
=
1
2

i
q
_
p
(

+2
i
+2
 
1


2_
p


+2
i

2
 
1

+2_
p
)
;T
+
q
+
p

2
=
1
2

i
q
_
p

+2
i
+
p
 
1


2_
p
;
147155
1
˙
ij
(
x

;y

;
x
+
)=
R
dP

4
ˇ
dp
+
4
ˇ
dk
+
4
ˇ
e

i
2
P

x
+
e

i
2
(
p
+
x

k
+
y

)
˙
ij
(
p
+
;k
+
;
P

)
;
150151
2
G
(
f
)
(
n
)


n
m
=0
f
~

(
n

m
)

;
~
f
(
m
)
g
(
q
)

(
n

2)
m
=0
f
~

(
n

m
)

;
~
f
(
m
+2)
g
(
˚
)
+
f
~

(
n
+1)

;
~
f
(1)
g
(
˚
)
150150
3
S
0
=
P
l
1

2
l
Tr
˚
a
l
˚
a

l
+
P
l
1
2

2
l
Tr
f
a
l
f
a

l
+
P
r
1
g
r
Tr

 
a
r
 
a
r
:
145148
4
ds
2
=

t
2
(
t
2
+
r
2

)(
t
2

r
2
+
)
dt
2
+
t
2
(
d˚
+
r
+
r

t
2
dr
)
2
+
(
t
2
+
r
2

)(
t
2

r
2
+
)
t
2
dr
2
:
147147
5
H
=
1
2
E
U
0
B
B
@
000
0
m
2
21
0
00
m
2
31
1
C
C
A
U
y
+
1
2
E
0
B
B
@
ab
0


b
0
b
0
000
1
C
C
A
;
147147
6
D
ab

(
p;p
3
)=

ab

a
3
p
2

p
2
3
+



g

+
p

p


(1


p
3
;
0
)
1
p
2
3
+

p
3
;
0
(1

˘
)
1
p
2
+


139145
7
V
(
H
1
;H
2
)=
1
8
(
g
2
2
+
g
2
1
)
(
j
H
1
j
2

H
2
j
2
)
2
+
m
2
1
j
H
1
j
2
+
m
2
2
j
H
2
j
2

m
2
3
(
H
1
H
2
+h
:
c
:
)
144145
8
A
3
0
(

0
!
0)=2
g
d
""
(1)

""
(2)

""
(3)

f


(
p

1

p

2
)+


(
p

3

p

1
)
+


(
p

2

p

3
)
g
:
146145
9
U
y
L
M
l
U
l
R
=
M

l
;U
y
L
M
L
U

L
=
M

L
;U
y
L
M
D
U

R
=
M

D
;U

R
T
M
R
U

R
=
M

R
:
149145
10
p

gg

1

1
g

2

2

g

d

p

d

p
~
F

1

2

d

p
=
1
p
!


1

2

d

p

1

2

p
F

1

2

p
;
147145
11
dE
dz
=
dE
el
dz
+
dE
rad
dz
ˇ
C
2

s
ˇ

2
ln
3
ET
2

2

ln
9
E
ˇ
3
T
+
3
ˇ
2

s
2

2
T
2

:
130144
12
L
0
=(2
n
+1)
j
h
j
2
+
j
h
j
d
y
0
d
0

j
h
j
2

h
j
P
1
k
=1
(
d
y
k
d
k

~
d
y
k
~
d
k
+
a
y
k
a
k

b
y
k
b
k
)
+
L
free
0
:
149144
13
Q
7

=
e
8
ˇ
2
m
b

q

˙

(1+

5
)
b

F

;Q
8
G
=
g
8
ˇ
2
m
b

q

˙

t
a

b

G
a

;
(
q
=
d
or
s
)
:
141143
14
ds
2
=

0

u
2
h
(
u
)
R
2
e
A
dx
2
0
+
u
2
R
2
e
C
dx
2
i
+
R
2
u
2
h
(
u
)
e
B
du
2
+
R
2
e
D
d

2
5

;
143143
15
sin
(
~
p
1

k
2
)
sin
(
~
p
2

k
2
)
sin
(
~
p
3

k
2
)
=

1
4
(sin~
p
1

k
+sin~
p
2

k
+sin~
p
3

k
)
133143
16
[
P
0
;X
0
]=
i;
[
P
i
;X
j
]=

i
ij

1

~
P
2

2

e
P
0

;
[
P
0
;X
i
]=

2
i

P
i
e
P
0

139143
17
h
J
a
1

1
(
P
1
)
:::J
a
n

n
(
P
n
)
i
T
ˇ
(

i
)
n
NT
2
12


a
1
:::a
n

1

n
(
P
1
;:::;P
n
)+
O
(
1
f
2
ˇ
)
;
143143
18
~
u
ˇ
(
~
k
0
1
)
y
~
u
ˇ
(
~
k
1
)=
N
2
ˇ

cos
2
˜
(
~
k
)

j
~p
j
2
4
(
cos˜
(
~
k
)
c
2
(
~
k
)+
1
3
~
k
2
b
1
(
~
k
)
2
)

:
142142
19
˝
0
(
y
)=
P
1
˙
1
=0
::
P
1
˙
4
=0
Y
(0)
˙
1
::˙
4
(
t
1
Z
1
)
˙
1
(
t
2
Z
2
)
˙
2
(
t
5
Z
5
)
˙
3
(
t
8
Z
8
)
˙
4
143142
20
sin(2

)
32
ˇ
2
I
(
~

!
sin(2

)
32
ˇ
2
(
I
(
~

c
2
2
c
2
3

1
I
(
M
G
;M
G
1
)+
c
2
2
s
2
3

2
I
(
M
G
;M
G
2
))
142142
21
W
=
Y
e
L
j
E
c
H
i
1

ij
+
Y
d
Q
ja
D
c
a
H
i
1

ij
+
Y
u
Q
ja
U
c
a
H
i
2

ij
+

i
1
H
j
2

ij
141141
22

i


(

)



(

)
@

=

i
~
C


s
(
T
I
)



Adj
(
u

1
)
a
I
e
a
=

i
~
C


s
(
T
I
)


k
(
I
)
;
145141
Figure5:Asampleofcorrectpredictionsby
I
2
L
-
STRIPS
.We'veshownthelongpredictionshence
lengthsaretouching150.Notethatattimesthetargetlengthisgreaterthanthepredictedlengthand
attimesthereverseistrue(thoughtheoriginalandpredictedimageswereidentical).Allsuchcases
wouldevaluatetoalessthanperfectBLEUscoreoredit-distance.Thishappensinabout40%ofthe
cases.Formoreexamplesvisitourwebsite.
y
^
y
0

~
S
2
!fM
1
;
M
2
g

~
S
2
!fM
1
;
M
2
g
1
ln
E
+
p
E
2

m
2
l

m
ˇ
E

p
E
2

m
2
l

m
ˇ
:
ln
E
+
p
E
2

m
2
t

m
ˇ
E

p
E
2

m
2
l

m
ˇ
:
2
(
r
0
`
s
)
~
d
˘
g
2

k
s
:
(
r
0
`
s
)

d
˘
g
2

k
s
:
3
_
p
a
+

b
a
p
b
!

_
xI
=0_
p
a
+

b
a
p
b
!

_
x

=0
5
^
T
=
^
V
+
^
t
^
G
^
V;
^
T
=
^
V
+
^
f
^
G
^
V;
7

A
=
(
#

;
e
#
_

)
;@
A
=
(
@

;
e
@
_

)
;
f
@
A
;

B
g
=

B
A

A
=(
#

;
e
v
_

)
;@
A
=
(
@

;
e
@
_

)
;
f
@
A
;

B
g
=

B
A
8
^
Y
˝
(
M
Z
)
j
DR
=
m
pole
˝

e

˝
(
m
pole
˝
)
j
DR
^
v
(
M
Z
)
j
DR

(
M
Z
)
^
Y
˝
(
M
Z
)
j
DG
=
m
vot
r

ke
z
˝
(
m
pele
r
)
j
D
H

(
M
Z
)
j
DE
(
M
Z
)
9
3
:
4

[2
j
2]
(
y
)=

9
y
2
h
1

22
:
21
y
+36
:
93
y
2
1

28
:
21
y
+143
:
2
y
2
i
3
:
4

[2
j
2]
(
y
)=

9
y
2
h
1

22
:
2
y
+36
:
93
y
2
1

28
:
2
y
+143
:
2
y
2
i
10
b
D


=
(

2
+2
eBS
3
)


b
D


=
(

2
+2
eBS
3
)


12
R
d
zdzd
bdb
n

(
f
)=0
R
d
~
zdz

dbd
n

(
f
)=0
13
L
(
p
4
)

S
=1
=
G
8
F
2
P
37
i
=1
N
i
W
i
L
(
s
0
)

S
2
=1
=
G
8
F
2
P
37
i
=1
N
i
W
i
14
det(
M
(0)
(
N
0
))=

Q
(0)
Q
r;s
[

(
h

h
r;s
)]
P
`
(
N
0

rs=K
)
;
det(
M
(0)
(
N
0
))=

Q
(0)
Q
r;s
[

(
h

h
r;s
)]
P
f
(
N
0

r
s
=K
)
;
17


e
2
N
2
T
:

e
2
N
2
T
:
Figure6:Arandomsampleofmistakesmadeby
I
2
L
-
STRIPS
.Observethatusuallythemodelgets
mostoftheformularightandthemistakeisonlyinasmallportionoftheoverallformula(e.g.sample
#1;generatingonesubscript
t
insteadofan
l
).Insomecasesthemistakeisinthefontandinsome
casestheimagesareidenticalbutwereincorrectlybytheimage-matchevaluationsoftware
(e.g.sample#0&#17).Insomecasesthepredictedformulaappearsmorecorrectthantheoriginal!
(sample#10wherepositionofthesubscript

hasbeen`corrected'by
I
2
L
-
STRIPS
).
7
Table2:Testresults.Im2latex-100kresultsarefromDengetal.
[6]
.Thelastcolumnisthepercentage
ofsuccessfullyrenderingpredictions.
DatasetModelBLEUEditVisualCompiling
ScoreDistanceMatch
103
Predictions
I2L-140K
I
2
L
-
NOPOOL
89.0
%0.067670.37%99.94%
I
2
L
-
STRIPS
89.0
%0.067169.24%99.85%
Im2latex-90k
I
2
L
-
STRIPS
88.19
%0.072568.03%99.81%
Im2latex-100kI
M
2T
EX
87.73%-
79.88
%-
dependencyon
C
0
(nowrandomlyinitialized).Thereforeif
I
t
isthefocal-regionatstep
t

bythepredicate

t;l
>
0
,then
p
t
(
^
y
t
)=
f
L
(
I
t
;
I
t

1
:::
I
0
)
where
f
L
representstheLSTM-stack
andDeepOutputLayer.Thisfactaidsconsiderablyininterpretingthepredictionsofthemodel.We
foundheat-maptypevisualsofthefocal-regions(Figure3)veryusefulininterpretingthemodeleven
asweweredevelopingit.
Objectdetectionviaattention:
Additionally,weobservethatthemodelsettlesonastep-by-step
alignmentof
I
t
withtheoutput-word'slocationontheimage:i.e.
p
t
(
^
y
t
)
ˇ
f
L
(
I
t
)
.Inotherwords
I
t
marksthebounding-boxof
^
y
t
eventhoughwetrainedwithoutanybounding-boxdata.Therefore
ourmodel-whoseencoderhasanarrowreceptivecanbeappliedtotheobjectdetectiontask
withoutrequiringboundingboxtrainingdata,bottom-upregionproposalsorpretrained
Notethatthisisnotpossiblewithencoderarchitectureshavingwidereceptivee.g.those
thatemployaRNN[
6
,
1
]becausetheirreceptiveencompasstheentireinput.Afuturework
willquantifytheaccuracyofobjectdetection[
18
]usingmoregranularreceptivePedersoli
etal.
[21]
havealsousedattentionforobjectdetectionbuttheirmodelismorecomplexinthatit
modelsbounding-boxesalthoughitdoesn'trequirethemfortraining.
3.2Dataset
Datasetswerecreatedfromsingle-lineL
A
T
E
Xmathformulasextractedfrompapersand
subsequentlyprocessedasfollows:1)Normalizetheformulastominimizespuriousambiguity.
12
2)Renderthenormalizedformulasusingxanddiscardonesthatdidn'tcompileorrender
successfully.3)Removeduplicates.4)Removeformulaswithlow-frequenceywords(frequen-
cy-threshold=24forIm2latex-90kand50forI2l-140K).5)Removeimagesbiggerthan
1086

126
andformulaslongerthan150.ProcessingtheIm2latex-100kdataset
104
(103559samples)asabove
resultedinthe
Im2latex-90k
datasetwhichhas93741samples.Ofthese,4648weresetasideasthe
testdatasetandtheremaining89093weresplitintotraining(95%)andvalidation(5%)setsbefore
eachrun(section2.3).WefoundtheIm2latex-90kdatasettoosmallforgoodgeneralizationand
thereforeaugmenteditwithadditionalsamplesfromKDDCup2003.Thisresultedinthe
I2L-140K
datasetwith114406(training),14280(validation)and14280(test)samples.Sincethenormalized
formulasarealreadyspaceseparatedtokensequences,noadditionaltokenizationstepwasnecessary.
Thevocabularywasthereforeproducedbysimplyidentifyingthesetofuniquespace-separatedwords
inthedataset.
Ancillarymaterial
Allancillarymaterial:Bothdatasets,ourmodelanddata-processingsource
code,visualizations,resultsamplesetc.isavailableatourwebsite.Appendixisprovidedalongside
thispaper.
References
[1]
Bahdanau,D.,Cho,K.,andBengio,Y.(2014).Neuralmachinetranslationbyjointlylearningto
alignandtranslate.
CoRR
,abs/1409.0473.
[2]
Bluche,T.(2016).Jointlinesegmentationandtranscriptionforend-to-endhandwrittenparagraph
recognition.In
NIPS
.
12
Normalizationwasperformedusingthemethodandsoftwareusedby[
6
]whichparsestheformulasintoan
ASTandthenconvertsthembacktonormalizedsequences.
104
Im2latex-100kdatasetisprovidedby[6].
8
[3]
Bluche,T.,Ney,H.,andKermorvant,C.(2014).Acomparisonofsequence-traineddeepneural
networksandrecurrentneuralnetworksopticalmodelingforhandwritingrecognition.In
SLSP
.
[4]
Chan,W.,Jaitly,N.,Le,Q.V.,andVinyals,O.(2015).Listen,attendandspell.
CoRR
,
abs/1508.01211.
[5]
Cho,K.,vanMerrienboer,B.,ÇaglarGülçehre,Bahdanau,D.,Bougares,F.,Schwenk,H.,and
Bengio,Y.(2014).Learningphraserepresentationsusingrnnencoder-decoderforstatistical
machinetranslation.In
EMNLP
.
[6]
Deng,Y.,Kanervisto,A.,Ling,J.,andRush,A.M.(2017).Image-to-markupgenerationwith
attention.In
ICML
.
[7]
Donahue,J.,Hendricks,L.A.,Guadarrama,S.,Rohrbach,M.,Venugopalan,S.,Saenko,K.,
andDarrell,T.(2015).Long-termrecurrentconvolutionalnetworksforvisualrecognitionand
description.
CoRR
,abs/1411.4389.
[8]
Graves,A.(2008).Supervisedsequencelabellingwithrecurrentneuralnetworks.In
Studiesin
ComputationalIntelligence
.
[9]
Graves,A.(2013).Generatingsequenceswithrecurrentneuralnetworks.
CoRR
,abs/1308.0850.
[10]
Graves,A.,Fernández,S.,Gomez,F.J.,andSchmidhuber,J.(2006).Connectionisttemporal
labellingunsegmentedsequencedatawithrecurrentneuralnetworks.In
ICML
.
[11]
Graves,A.,Mohamed,A.,andHinton,G.E.(2013).Speechrecognitionwithdeeprecurrent
neuralnetworks.
CoRR
,abs/1303.5778.
[12]
Graves,A.andSchmidhuber,J.(2008).Ofinehandwritingrecognitionwithmultidimensional
recurrentneuralnetworks.In
NIPS
.
[13]
Hochreiter,S.andSchmidhuber,J.(1997).Longshort-termmemory.
NeuralComput.
,
9(8):1735Œ1780.
[14]
Johnson,J.,Karpathy,A.,andFei-Fei,L.(2016).Densecap:Fullyconvolutionallocalization
networksfordensecaptioning.
CoRR
,abs/1511.07571.
[15]
Kalchbrenner,N.,Espeholt,L.,Simonyan,K.,vandenOord,A.,Graves,A.,andKavukcuoglu,
K.(2016).Neuralmachinetranslationinlineartime.
CoRR
,abs/1610.10099.
[16]
Karpathy,A.andfeiLi,F.(2015).Deepvisual-semanticalignmentsforgeneratingimage
descriptions.
2015IEEEConferenceonComputerVisionandPatternRecognition(CVPR)
,pages
3128Œ3137.
[17]
Kingma,D.P.andBa,J.(2014).Adam:Amethodforstochasticoptimization.
CoRR
,
abs/1412.6980.
[18]
Liu,C.,Mao,J.,Sha,F.,andYuille,A.L.(2017).Attentioncorrectnessinneuralimage
captioning.In
AAAI
.
[19]
Luong,M.,Pham,H.,andManning,C.D.(2015).Effectiveapproachestoattention-based
neuralmachinetranslation.
CoRR
,abs/1508.04025.
[20]
Pascanu,R.,ÇaglarGülçehre,Cho,K.,andBengio,Y.(2013).Howtoconstructdeeprecurrent
neuralnetworks.
CoRR
,abs/1312.6026.
[21]
Pedersoli,M.,Lucas,T.,Schmid,C.,andVerbeek,J.(2016).Areasofattentionforimage
captioning.
CoRR
,abs/1612.01033.
[22]
Salimans,T.,Karpathy,A.,Chen,X.,andKingma,D.P.(2017).Pixelcnn++:Improvingthepix-
elcnnwithdiscretizedlogisticmixturelikelihoodandother
CoRR
,abs/1701.05517.
[23]
Simonyan,K.andZisserman,A.(2014).Verydeepconvolutionalnetworksforlarge-scale
imagerecognition.
CoRR
,abs/1409.1556.
9
[24]
Sutskever,I.,Vinyals,O.,andLe,Q.V.(2014).Sequencetosequencelearningwithneural
networks.
CoRR
,abs/1409.3215.
[25]Theis,L.andBethge,M.(2015).Generativeimagemodelingusingspatiallstms.In
NIPS
.
[26]
vandenOord,A.,Dieleman,S.,Zen,H.,Simonyan,K.,Vinyals,O.,Graves,A.,Kalchbrenner,
N.,Senior,A.W.,andKavukcuoglu,K.(2016a).Wavenet:Agenerativemodelforrawaudio.
CoRR
,abs/1609.03499.
[27]
vandenOord,A.,Kalchbrenner,N.,andKavukcuoglu,K.(2016b).Pixelrecurrentneural
networks.In
ICML
.
[28]
vandenOord,A.,Kalchbrenner,N.,Vinyals,O.,Espeholt,L.,Graves,A.,andKavukcuoglu,K.
(2016c).Conditionalimagegenerationwithpixelcnndecoders.
CoRR
,abs/1606.05328.
[29]
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,L.,and
Polosukhin,I.(2017).Attentionisallyouneed.In
NIPS
.
[30]
Vinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2015).Showandtell:Aneuralimage
captiongenerator.In
2015IEEEConferenceonComputerVisionandPatternRecognition(CVPR)
.
[31]
Xu,K.,Ba,J.,Kiros,J.R.,Cho,K.,Courville,A.C.,Salakhutdinov,R.,Zemel,R.S.,and
Bengio,Y.(2015).Show,attendandtell:Neuralimagecaptiongenerationwithvisualattention.
In
ICML
.
[32]
Yu,F.andKoltun,V.(2015).Multi-scalecontextaggregationbydilatedconvolutions.
CoRR
,
abs/1511.07122.
[33]
Zaremba,W.,Sutskever,I.,andVinyals,O.(2014).Recurrentneuralnetworkregularization.
CoRR
,abs/1409.2329.
AQualitativeanalysesanddetails
Thissectionisanappendixtothepaper.Wepresentherefurtherdetails,analysesanddiscussionof
ourexperimentsandcomparisonwithrelatedwork.
A.1Encoder
Table3showstheoftheEncoderCNN.Allconvolutionkernelshaveshape(3,3),stride
(1,1)and
tanh
non-linearity,whereasallmaxpoolingwindowshaveshape(2,2)andstride(2,2).
WeinitiallyexperimentedwiththeoutputoftheVGG16model[
23
]-perXuetal.
[31]
.However
Table3:oftheEncoderCNN.
LayerOutputShapeChannels
Input(Image)
128

1088
1
Convolution
128

1088
64
Maxpool
64

544
64
Convolution)
64

544
128
Maxpool
32

272
128
Convolution
32

272
256
Maxpool
16

136
256
Convolution
16

136
512
Maxpool
8

68
512
Convolution
8

68
512
Maxpool
4

34=(

H


W
)512=(

D
)
(presumablysinceVGG16wastrainedonadifferentdatasetandadifferentproblem)theBLEU
scoredidn'timprovebeyond40%.ThenwestartedtrainingVGG16alongwithourmodelbutthe
end-to-endmodeldidn'tevenstartlearning(thelog-losscurvewas-possiblyduetothelarge
overalldepthoftheend-to-endmodel.Reducingthenumberofconvolutionlayersto6andchanging
10
y
^
y
0
@A
0

@t
=

i
[
A
0

;H
F
0
]
;
@A
0

@t
=

i
[
A
0

;H
F
0
]
;
1
f

i
(
x
)
;

j
(
y
)
g
=

ij

2
(
x

y
)
:
f

i
(
x
)
;

j
(
y
)
g
=

ij

2
(
x

y
)
:
2
V
total
=
P
i



@W
@z
i



2
+
V
D
+
V
soft
V
total
=
P
i



@W
@z
i



2
+
V
D
+
V
soft
3

y
a

(
p
)=
R
d
3
xe

ip

x
[
e


(
!A
a

iE
a
)+
R

(
f
1

a
+
f
2
˚
a
)
]

y


(
p
)=
R
d
3
xe

ip

x
[
e


(
!A
a

iE
a
)+
R

(
f
1

a
+
f
2
˚
a
)
]
4
H
stat
(
k
)=
P
+
i
vk
+

;H
stat
(
k
)=
P
+
i
vk
+

;
5
(
˚

P
s
+
˚P

s
)

2
M
2
;
(
˚

P
s
+
˚P

s
)

2
M
2
;
6
H
G=H
=
1
2
(
ˇ


i
~
2


)
g

(
ˇ

+
i
~
2


)
=
1
2
ˇ

g

ˇ

+
V
G=H
;H
G=H
=
1
2
(
ˇ


i
~
2


)
g

(
ˇ

+
i
~
2


)
=
1
2
ˇ

g

ˇ

+
V
G=H
;
7
S

S
[
˚
]+
S
[
'
]+
S
int
[
˚;'
]
S

S
[
˚
]+
S
[
'
]+
S
int
[
˚;'
]
8

1
=

4
ˇ

2
=

4

1
=

4
ˇ
9

1

r
2
8
ˇM
B
(
j
M
S
j
2
+
j
M
P
j
2
)
:

1

r
2
8
ˇM
B
(
j
M
S
j
2
+
j
M
P
j
2
)
:
10
E
(
r
)=

(
2
GE

m
2
r

q
1
q
2
r
+
m

S
1
S
2
r
)
E
(
r
)=

(
2
GE

m
2
r

q
1
q
2
r
+
m

S
1
S
2
r
)
11
˜
(
x
1
;x
2
)=
h
0
j
T 
(
x
1
)

 
(
x
2
)
j
P
i
:˜
(
x
1
;x
2
)=
h
0
j
T 
(
x
1
)

 
(
x
2
)
j
P
i
:
12

(2)

=
~
h
(2)
1
v
2
˚
j
(

i

)(

ij

)+
h
(2)
1
v
i
v
j
˚
k
(

i

)(

jk

)
;
(2)

=
~
h
(2)
1
v
2
˚
j
(

i

)(

ij

)+
h
(2)
1
v
i
v
j
˚
k
(

i

)(

jk

)
;
13
sin
2
2
#
=sin
2
2
#
sun
=
4
j
U
e
1
j
2
j
U
e
2
j
2
(
j
U
e
1
j
2
+
j
U
e
2
j
2
)
2
:
sin
2
2
#
=sin
2
2
#
sun
=
4
j
U
e
1
j
2
j
U
e
2
j
2
(
j
U
e
1
j
2
+
j
U
e
2
j
2
)
2
:
14
F
1
=
g
2
192
ˇ
5
=
2
M
Pl
'
1
:
5

10
15
GeV
:F
1
=
g
2
192
ˇ
5
=
2
M
Pl
'
1
:
5

10
15
GeV
:
15
b
<i>
=
Q
0

p<q

p
i
X
˝
p
(
i
)
˝
q
(
i
)
(
z;z
)
:b
<i>
=
P
0

p<q

p
X
r
(
i
)
i˝
(
i
)
(
z;z
)
:
16
F
W
+
D
1
(
x
)=[
d
p
(
x
)+
u
p
(
x
)+
d
n
(
x
)+
u
n
(
x
)+2
s
(
x
)+2
c
(
x
)]
=
2
:F
W
+
D
1
(
x
)=[
d
p
(
x
)+
u
p
(
x
)+
d
n
(
x
)+
u
n
(
x
)+2
s
(
x
)+2
c
(
x
)]
=
2
:
17
u
=
q
2
b
2
=
2
orQ
=
Q
0
u;Q
0
=
1
Am
N
b
2
u
=
q
2
b
2
=
2
orQ
=
Q
0
u;Q
0
=
1
Am
N
b
2
18

h
v

h
v
=
1
2
Tr


P
v


h
v
h
v

1
2
Tr




5
P
v

P
v


h
v



5
h
v
;

h
v

h
v
=
1
2
T
P
v
)

h
v
h
v

1
2
Tr(



5
P
v

P
v
)

h
v



5
h
v
;
19
~
P
g
(
z

ns
1
z
+
1
1

z
:
~
P
g
(
z

n
1
z
+
1
1

z
:
20
S

S
H
;T

T
H
;E
c

E
BH
;
for
HR

1
S

S
H
;T

T
H
;E
c

E
BH
;
for
HR

1
21

˘
N

1
=
2
10
23
s

1
exp
h

8
p
2
3

137
(

E
m
e
)
3
=
2
B
0
NB
A
1
=
2
(
m
p
m
e
)
1
=
2
i
;

˘
N

1
=
2
10
23
e

1
exp
h

8
p
2
3

137
(

E
m
e
)
3
=
2
B
0
NB
A
1
=
2
(
m
p
m
e
)
1
=
2
i
;
22
2
j
J
j
2
m
0
2
˘
=
m
b
2
m
d
2
˘
2
:
5

10
5
:
2
R
j
2
m
0
2
˘
=
m
2
b
m
2
d
˘
2
:
5

10
5
:
23
u
=
z
`
U

1
=
2
@
@t
;u
=
z
`
U

1
=
2
@
@t
;
24

ˆ
ˆ
c

ˆ
ˆ
c
25
e
(2
r
+1)
ˇiL
(0)
Y
1
(
v;x
)
e

(2
r
+1)
ˇiL
(0)
=
Y
1
((

1)
L
(0)
v;

x
)
;e
(2
r
+1)
ˇiL
(0)
Y
1
(
v;x
)
e

(2
r
+1)
ˇiL
(0)
=
Y
1
((

1)
L
(0)
v;

x
)
;
26
A
2
=
R
d
2
x
A
2
(
x
)


(
x
)
;
A
2
=
R
d
2
x
A
2
(
x
)


(
x
)
;
27
ds
2
=(
k
+
f
0
R
2
0
R
2
)

1
dR
2
+
R
2
d

2
k

(
k
+
f
0
R
2
0
R
2
)[
dx
5
+
A
R
(
R
)
dR
]
2
ds
2
=(
k
+
f
0
R
2
0
R
2
)

1
dR
2
+
R
2
d

2
k

(
k
+
f
0
R
2
0
R
2
)[
dx
5
+
A
R
(
R
)
dR
]
2
28
[^
ˆ
0
;
^
ˆ
0
]=0
;
[
^
S
A
0
;
[^
ˆ
0
;
^
ˆ
0
]=0
;
[
^
S
A
0
;
29
L
4
=(
F
1

@
5
A
2
)
dW
dA
1
+

:
L
4
=(
F
1

@
5
A
2
)
dW
dA
1
+

:
30

j

i
=

ij

q

1
^
R
ikjl

l

k

j

i
=

ij

q

1
^
R
iklj

l

k
31

i
=0
;

k
=
@
k

0
+
cm
2
h
k
=0
;

i
=0
;

k
=
@
k

0
+
cm
2
h
k
=0
;
32
sin

=
s
23
c
23
s
2
c
2
sin

13
sin

=
s
23
c
23
s
2
c
2
sin

13
33

A
=

2
Hs
AA
;

A
=

2
Hs
AA
;
34
^
D

1

=
^
D

1






+
P
1
n
=1
A
n
(
D

1
)
n


;
^
D

1

=
^
D

1






+
P
1
n
=1
A
n
(
D

1
)
n


;
35
H
G
(
x
2
)=

1
8
ˇGe
2
(
[
B
(
x
2
)]

2

1
)
;H
G
(
x
2
)=

1
8
ˇGe
2
(
[
B
(
x
2
)]

2

1
)
;
36
T

=
T
+

+
a
2

a
2
+
T


:T

=
T
+

+
a
2

a
2
+
T


:
37
G
H
d
+1
=
l
1

d

d
R
+
1
˙
l
dx
sinh
d
x
:G
H
d
+1
=
l
l

d

d
R
+
1
q
dx
sinh
d
x
:
38

(
n
)
f

g
=

(
n
)

A
0
)
A
0

1
(
x
1
)

A
0

j
(
x
j
)

A
0

n
(
x
n
)
;

(
n
)
f

g
=
@
n
(
n
)

A
0
)
A
0

1
(
x
1
)

A
0


(
x
j
)

A
o

n
(
x
n
)
;
39
_
ˇ
ab
=_
ˇ
ab
=
40
ˇ
0
a
=0
;ˇ
i
a
=
f
b
ac
ˇ
i
b

c
2
;ˇ
0
i
=0
;ˇ
ij
=0
;ˇ
0
a
=0
;ˇ
i
a
=
f
b
ac
ˇ
i
b

c
2
;ˇ
0
i
=0
;ˇ
ij
=0
;
41
j
Z
1
j
2
=
j
Z
2
j
2
=
1
(4
G
)
2
e


0
[(
Q
R
1
)
2
+(
Q
R
2
)
2
]
;
j
Z
1
j
2
=
j
Z
2
j
2
=
1
(4
G
)
2
e


0
[(
Q
R
1
)
2
+(
Q
R
2
)
2
]
;
Figure7:Arandomsampleofpredictionsof
I
2
L
-
STRIPS
containingbothgoodandbadpredictions.
Notethatthoughthisisarandomsample,predictionmistakesarenotobviousandittakessomeeffort
topointthemout!Formoreexamplesvisitourwebsite.
105
11
thenon-linearityto
tanh
(tokeeptheactivationsincheck)gotusgoodresults.Furtherreducing
numberoflayersto5yieldedthesameperformance,thereforewestuckwiththat(Table
3).Inadditon,weexperimentedwith
I
2
L
-
STRIPS
becauseitreducestherectangularimage-maptoa
linearmap,therebypresumablymakingthealignmentmodel'staskeasierbecausenowitwouldonly
needtoscaninone-dimension.However,itperformedaroundthesameas
I
2
L
-
NOPOOL
andtherefore
thathypothesiswasdebunked.InfactwepreferI2L-NOPOOLsinceithasfewerparametersandits
attentionmodelhassharperfocal-regionswhichhelpswithmodelinterpretation.
A.2Attentionmodel
Table4:oftheVisualAttention
ModelMLP.L=34forI2L-STRIPSandand
136forI2L-NOPOOL.
LayerNumUnitsActivation
3(output)Lsoftmax
2max(128,L)tanh
1max(256,L)tanh
Table4theoftheattention
modelMLP.Xuetal.
31
'sformulationofattention
model(

t;l
=
MLP
(
a
l
;
H
t

1
)
)receivesinputs
fromonlyasingleimagelocation.Incomparison,our
formulation(

t
=
f
att
(
a
;
H
t

1
)
)receivesthefull
encodedimage
a
initsinput.Thischangewasneeded
becausethepreviousformulationdidnotprogressbe-
yondapoint,presumablybecausethisproblemwar-
rantedawiderreceptiveThenewformulation
worksequallywellwithdifferentpoolingstrides(and
correspondinglydifferentvaluesofL).
Also,Xuetal.
31
'sformulationof
z
t
=

t


t

a
includesascalar

t
=
MLP
(
H
t

1
)
which
informstheLSTMhowmuchemphasistoplaceontheimagev/sthelanguagemodel.Experimentally
wefoundthatithadnoimpactonend-to-endperformance,thereforewedroppeditfromourmodel.
Xuetal.
31
alsouseasimplerformulafor
A
=
P
L
l
=1
(
P
˝
t
=1

t;l

1)
2
whichtheycall`doubly
stochasticoptimization'.Ourformulationusesthetruemeanof

l
,
˝=L
insteadof1,normalizesitto
aedrangesothatitcanbecomparedacrossmodelsandmoreimportantly,includesatarget-ASE
term
ASE
T
.Withoutthisterm,i.e.with
ASE
T
=0
,
A
wouldbiastheattentionmodeltowards
uniformlyscanningalltheLimagelocations.Thisisundesirablesincetherearemanyemptyregions
oftheimageswhereitmakesnosensefortheattentionmodeltospendmuchtime.Conversely,there
aresomedenselypopulatedregions(e.g.asymbolwithcomplexsuperscriptandsubscripts)where
themodelwouldreasonablyspendmoretimebecauseitwouldhavetoproducealongeroutput
sequence.Inotherwords,theoptimalscanningpatternwouldhavetobenon-uniform-
ASE
T
6
=0
.
Also,thescanningpatternwouldvaryfromsampletosample,but
ASE
T
issettoasinglevalue(even
ifzero)forallsamples.Thereforewepreferredtoremovetheattention-modelbiasaltogetherfrom
theobjectivefunctionbysetting

A
=0
inallsituationsexceptwhentheattentionmodelneededa
'nudge'inorderto`getofftheground'.Insuchcasesweset
ASE
T
basedonobservedvaluesof
ASE
N
(Table8).
A.3LSTMstack
Figure8:LSTMCell
i
t
=
˙
(
W
xi
x
t
+
W
hi
h
t

1
+
W
ci
c
t

1
+
b
i
)
f
t
=
˙
(
W
xf
x
t
+
W
hf
h
t

1
+
W
cf
c
t

1
+
b
f
)
c
t
=
f
t
c
t

1
+
i
t
tanh(
W
xc
x
t
+
W
hc
h
t

1
+
b
c
)
o
t
=
˙
(
W
xo
x
t
+
W
ho
h
t

1
+
W
co
c
t
+
b
o
)
h
t
=
o
t
tanh(
c
t
)
i
t
;
f
t
;
o
t
;
c
t
;
h
t
2
R
n
(12)
OurLSTMcellimplementation(Figure.8andequation12)followsGravesetal.
[11]
,Zarembaetal.
[33]
.Inequation12
˙
isthelogisticsigmoidfunctionand
i
t
,
f
t
,
o
t
,
c
t
and
h
t
arerespectivelythe
inputgate
,
forgetgate
,
outputgate
,
cell
and
hidden
activationvectorsofsize
n
.
12
DuringexperimentationourpenultimateLSTM-stackwhichhad3LSTMlayerswith1000units
each,gaveusavalidationscoreof87.45%.Atthatpointexperimentalobservationssuggestedthat
theLSTMstackwastheaccuracy'bottleneck'becauseothersub-modelswereperformingverywell.
IncreasingthenumberofLSTMunitsto1500gotusbettervalidationscore-butaworseov
Reducingthenumberoflayersdownto2gotusthebestoverallvalidationscore.Incomparison,Xu
etal.[31]haveusedasingleLSTMlayerwith1000cells.
A.4Deepoutputlayer
Table5:oftheDeepOutputLayer
MLP.
K
=339and358forI2L-140KandIm2latex-
90kdatasetsrespectively.
LayerNumUnitsActivation
3(output)Ksoftmax
2max(358,K)tanh
1max(358,K)tanh
Notethattheoutputlayerreceivesskipcon-
nectionsfromtheLSTM-Stackinput(
p
t
=
f
out
(
H
t
;
z
t
;
Ey
t

1
)
).Weobserveda2%im-
pactontheBLEUscorewiththeadditionof
input-to-outputskip-connections.Thisleadsus
tobelievethataddingskip-connectionswithin
theLSTM-stackmayhelpfurtherimprove
modelaccuracy.Overallaccuracyalsoimproved
byincreasingthenumberoflayersfrom2to3.
Lastly,observethatthissub-modelisdifferent
fromXuetal.
[31]
whereinthethreeinputsareafinto
D
dimensions,summedand
thenpassedthroughonefully-connectedlayer.Afterexperimentingwiththeirmodelweultimately
chosetoinsteadfeedtheinputs(concatenated)toafully-connectedlayertherebyallowingtheMLP
tonaturallylearntheinput-to-outputfunction.Wealsoincreasedthenumberoflayersto3,changed
activationfunctionofhiddenunitsfromrelutotanh
101
andensuredthateachlayerhadatleastas
manyunitsasthesoftmaxlayer(
K
).
A.5Initmodel
Table6:InitModellayers.
LayerNumUnitsActivation
Function
Output2Qntanh
Hidden1100tanh
TheinitmodelMLPisinTable6.We
questionedtheneedfortheInitModelandex-
perimentedjustusingzerovaluesfortheinitial
state.Thatcausedaslightbutconsistentdecline
(
<
1%)inthevalidationscore,indicatingthat
theinitialstatelearntbyourInitialStateModel
didcontributeinsomewaytowardslearning
andgeneralization.NotehoweverthatourInit
Modelisdifferentthan
31
,inthatourversionusesall
L
featurevectorsof
a
whiletheirstakesthe
average.Wealsoaddedahiddenlayerandused
tanh
activationfunctioninsteadof
relu
.Wedidstart
offwiththeirversionbutthatdidnotprovideanappreciableimpacttothebottomline(validation).
Thismadeushypothesizethatperhapstakinganaverageofthefeaturevectorswascausingalossof
information;andwemitigatedthatbytakinginallthe
L
featurevectorswithoutsummingthem.After
makingallthesechanges,theInitModelyieldsaconsistentalbietsmallperformanceimprovement
(Table.7).Butgiventhatitconsumes
˘
7.5millionparameters,itsusefulnessremainsinquestion.
Table7:ImpactoftheInitModelonoverallperformance.Sinceitcomprises10-12%ofthetotal
params,itmayaswellbeomittedinexchangeforasmallperformancehit.
ModelInitModelValidationNum
Present?BLEUParams
I
2
L
-
NOPOOL
Yes89.09%7,569,300
I
2
L
-
NOPOOL
No88.20%0
I
2
L
-
STRIPS
Yes89.00%7,569,300
I
2
L
-
STRIPS
No88.74%0
101
Wechangedfromrelutotanhpartlyinordertoremedy`activation-explosions'whichwerecausing
ovwerrors.
13
A.6Traininganddataset
A.6.1Alphapenalty
Pleaseseeequations13through13e.ThelossfunctionequationstatedinthepaperisEquation
13butwith

A
setto0.Thatwasthecasewhentrainingmodelswho'sresultswehavepublished,
howeveratothertimeswehadincludedapenaltyterm

A
A
whichwediscussnext.Observe
thatwhile
P
L
l

t;l
=1
,thereisnoconstraintonhowtheattentionisdistributedacrossthe
L
locationsoftheimage.Theterm

A
A
servestosteerthevarianceof

l
bypenalizinganydeviation
fromadesiredvalue.
ASE
(AlphaSquaredError)isthesumofsquared-differencebetween

l
anditsmean
˝=L
;and
ASE
N
isitsnormalizedvalue
13
2
[0,100]
14
.Therefore
ASE
N
/
ASE
/
˙
2

l
.
ASE
T
whichisthedesiredvalueof
ASE
N
,isahyperparameterthatneedstobe
discoveredthroughexperimentation
15
.Table8showstrainingresultswithalpha-penaltydetails.
Table8:Trainingmetrics.

R
=0
:
00005
and

2
=0
:
9
forallruns.
DatasetModelInit

A

1
Training
TrainingValidation
ASE
N
Model?
Epochs
BLEUED
I2L-140KI2L-STRIPSYes
0.00.5104
0.93610.06775.3827
I2L-STRIPSNo
0.00.575
0.93000.06914.9899
I2L-NOPOOLYes
0.00.5104
0.93330.06844.5801
I2L-NOPOOLNo
0.00.1119
0.93480.07384.7099
Im2latex-90kI2L-STRIPSYes
0.00.5110
0.93660.06885.1237
I2L-STRIPSNo
0.00050.5161
0.93860.07504.8291
J
=

1
˝
log
(
P
r
(
y
j
a
))+

R
R
+

A
A
(13)
R
=
1
2
X


2
(13a)
A
=(
ASE
N

ASE
T
)
(13b)
ASE
N
=
100
˝
2

L

1
L


ASE
(13c)
ASE
=
L
X
l
=1


l

˝
L

2
(13d)

l
:=
˝
X
t
=1

t;l
(13e)
Defaultvaluesof

1
and

2
oftheADAMopti-
mizer-0.9and0.99-yieldedverychoppyval-
idationscorecurveswithfrequentdown-spikes
wherethevalidationscorewouldfalltovery
lowlevels,ultimatelyresultinginlowerpeak
scores.Reducingtheandsecondmoments
(i.e.

1
and

2
)edtheproblemsuggestingthat
thedefaultmomentumwastoohighforour`ter-
rain'.Wedidnotusedropoutforregulariza-
tion,howeverincreasingthedata-setsize(I2L-
140K)andraisingtheminimum-word-frequency
thresholdfrom24(Im2latex-90k)to50((I2L-
140K))didyieldbettergeneralizationandover-
alltestscores(Table8).Finally,normalizingthe
data
16
yieldedabout25%moreaccuracythan
without.
13
Itcanbeshownthat
˝
2

L

1
L

isthemaximumpossiblevalueof
ASE
.
14
Wenormalize
ASE
sothatitmaybecomparedacrossbatches,runsandmodels.
15
Startwith
ASE
T
=0
,observewhere
ASE
N
settlesaftertraining,thenset
ASE
T
tothatvalueandrepeat
untilapproximateconvergence.
16
Normalizationwasperformedusingthemethodandsoftwareusedby[
6
]whichparsestheformulasintoan
ASTandthenconvertsthembacktonormalizedsequences.
14
"
55,On the Relationship between Data Efficiency and Error for Uncertainty Sampling,http://arxiv.org/pdf/1806.06123v1.pdf,https://worksheets.codalab.org/worksheets/0x8ef22fd3cd384029bf1d1cae5b268f2d,"OntheRelationshipbetweenDataandError
forUncertaintySampling
StephenMussmann
1
PercyLiang
1
Abstract
Whileactivelearningofferspotentialcostsav-
ings,theactualdataefyŠthereduction
inamountoflabeleddataneededtoobtainthe
sameerrorrateŠobservedinpracticeismixed.
Thispaperposesabasicquestion:whenisac-
tivelearningactuallyhelpful?Weprovidean
answerforlogisticregressionwiththepopular
activelearningalgorithm,uncertaintysampling.
Empirically,on21datasetsfromOpenML,we
astronginversecorrelationbetweendata
efyandtheerrorrateoftheclassi-
.Theoretically,weshowthatforavariant
ofuncertaintysampling,theasymptoticdataef-
yiswithinaconstantfactoroftheinverse
errorrateofthelimiting.
1.Introduction
Activelearningofferspotentiallabelcostsavingsbyadap-
tivelychoosingthedatapointstolabel.Overthepasttwo
decades,alargenumberofactivelearningalgorithmshave
beenproposed(
Seungetal.
,
1992
;
Lewis&Gale
,
1994
;
Freundetal.
,
1997
;
Tong&Koller
,
2001
;
Roy&McCal-
lum
,
2001
;
Brinker
,
2003
;
Hoietal.
,
2009
).Muchofthe
community'sfocusisoncomparingthemeritsofdifferent
activelearningalgorithms(
Schein&Ungar
,
2007
;
Yang&
Loog
,
2016
).
Thispaperismotivatedbytheobservationthatevenfor
a

activelearningalgorithm,itseffectivenessvaries
widelyacrossdatasets.
Tong&Koller
(
2001
)showa
datasetwhereuncertaintysamplingachieves5xdataef
ciency,meaningthatactivelearningachievesthesameer-
rorrateasrandomsamplingwithofthelabeled
data.Forthissamealgorithm,differentdatasetsyielda
mixedbagofresults:worseperformancethanrandomsam-
1
StanfordUniversity,Stanford,CA.Correspondenceto:
StephenMussmann
<
mussmann@stanford.edu
>
.
Proceedingsofthe
35
th
InternationalConferenceonMachine
Learning
,Stockholm,Sweden,PMLR80,2018.Copyright2018
bytheauthor(s).
pling(
Yang&Loog
,
2016
),nogains(
Schein&Ungar
,
2007
),gainsof2x(
Tong&Koller
,
2001
),andgainsof3x
(
Brinker
,
2003
).
Inwhatcasesandtowhatextentisactivelearningsuperior
tonaiverandomsampling?Thisisanimportantquestion
toaddressforactivelearningtobeeffectivelyusedinprac-
tice.Inthispaper,weprovidebothempiricalandtheoret-
icalanswersforthecaseoflogisticregressionanduncer-
taintysampling,ﬁthesimplestandmostcommonlyusedﬂ
activelearningalgorithminpractice(
Settles
,
2010
)andthe
bestalgorithmgiveninthebenchmarkexperimentsof
Yang
&Loog
(
2016
).
Empirically,inSection
3
,westudy21binary
tiondatasetsfromOpenML.Wefoundthatthedataef
ciencyforuncertaintysamplingandinverseerrorachieved
bytrainingonthefulldatasetarecorrelatedwithaPear-
soncorrelationof
0
:
79
andaSpearmanrankcorrelationof
0
:
67
.
Theoretically,inSection
4
,weanalyzeatwo-stagevariant
ofuncertaintysampling,whichlearnsaroughclassi-
viarandomsamplingandthensamplesnearthedeci-
sionboundaryofthat.Weshowthattheasymp-
toticdataefyofthisalgorithmcomparedtorandom
samplingiswithinasmallconstantfactoroftheinverse
limitingerrorrate.Theargumentfollowsbycomparing
theFisherinformationofthepassiveandactiveestimators,
formalizingtheintuitionthatinlowerrorregimes,random
samplingwastesmanysamplesthatthemodelisalready
about.Notethatthisresultisdifferentinkind
thanthe
log(1

)
versus
1

ratesoftenstudiedinstatis-
ticalactivelearningtheory(
Balcanetal.
,
2009
;
Hanneke
,
2014
),whichfocusesonconvergenceratesasopposedto
thedependenceonerror.Together,ourempiricalandthe-
oreticalresultsprovideastronglinkbetweenthedataef
ciencyandthelimitingerrorrate.
2.Setup
Considerabinaryproblemwherethegoal
istolearnapredictor
f
frominput
x
2
R
d
tooutput
y
2
1
;
+1
g
thathaslowexpectederror(0-1loss),
Err
(
f
)=Pr[
f
(
x
)
6
=
y
]
withrespecttoanunderlying
arXiv:1806.06123v1  [cs.LG]  15 Jun 2018OntheRelationshipbetweenDataandErrorforUncertaintySampling
datadistribution.Inpool-basedactivelearning,westart
withasetofunlabeledinputpoints
X
U
.Anactivelearn-
ingalgorithmqueriesapoint
x
2X
U
,receivesitslabel
y
,
andupdatesthemodelbasedon
(
x;y
)
.Apassivelearning
algorithm(randomsampling)simplysamplespointsfrom
X
U
uniformlyrandomlywithoutreplacement,queriestheir
labels,andtrainsamodelonthisdata.
2.1.LogisticRegression
Inthiswork,wefocusonlogisticregression,where
p
w
(
y
j
x
)=
˙
(
yx

w
)
,
w
isaweightvector,and
˙
(
z
)=
1
1+exp(

z
)
isthelogisticfunction.Aweight
vector
w
characterizesapredictor
f
w
(
x
)=
sgn
(
x

w
)
.
Givenasetoflabeleddatapoints
D
(gatheredeitherpas-
sivelyoractively),themaximumlikelihoodestimateis
^
w
=argmin
w
P
(
x;y
)
2
D

log
p
w
(
y
j
x
)
.thelimit-
ingparametersastheanalogousquantityonthepopulation:
w

=argmin
w
E
[

log
p
w
(
y
j
x
)]
.Acentralquantityin
thisworkisthe
limitingerror
,denotedErr
=
Err
(
f
w

)
.
Notethatweareinterestedin0-1loss(ascapturedbyErr),
thoughtheestimator
w

minimizesthelogisticloss.
2.2.UncertaintySampling
Inthiswork,wefocusonﬁthesimplestandmostcom-
monlyusedqueryframeworkﬂ(
Settles
,
2010
),uncertainty
sampling(
Lewis&Gale
,
1994
).Thisiscloselyrelated
tomargin-basedactivelearninginthetheoreticalliterature
(
Balcanetal.
,
2007
).
Uncertaintysamplingsamples
n
seed
datapointsran-
domlyfrom
X
U
,labelsthem,andusesthattotrainaninitial
model.Foreachofthenext
n

n
seed
iterations,itchooses
andatapointfrom
X
U
thatthecurrentmodelismostuncer-
tainabout(i.e.,closesttothedecisionboundary),queries
itslabel,andretrainsthemodelusingalllabeleddatapoints
collectedsofar.SeeAlgorithm
1
forthepseudocode(note
wewillchangethisslightlyforthetheoreticalanalysis).
2.3.Data
Let
^
w
passive
and
^
w
active
bethetwoestimatorsobtainedby
performingpassivelearning(randomsampling)andactive
learning(uncertaintysampling),respectively.Tocompare
thesetwoestimators,weuse
dataef
(alsoknownas
statisticalrelativeefy(
vanderVaart
,
1998
)orsam-
plecomplexityratio),whichisthereductioninnumberof
labeledpointsthatactivelearningrequirestoachieveerror

comparedtorandomsampling.
Moreprecisely,considerthenumberofsamplesforeach
estimatortoreacherror

:
n
active
(

)
def
=max
f
n
:
E
[
Err
(^
w
active
)]


g
;
(1)
n
passive
(

)
def
=max
f
n
:
E
[
Err
(^
w
passive
)]


g
;
(2)
wheretheexpectationiswithrespecttotheunlabeledpool,
thelabels,andanyrandomnessfromthealgorithm.Then
thedataefyisastheratio:
DE
(

)
def
=
n
passive
(

)
n
active
(

)
:
(3)
2.4.DataDependenceonDataset
Thedataefydependsonpropertiesoftheunder-
lyingdatadistribution.Intheexperiments(Section
3
),
weillustratethisdependenceonshowavarietyofreal-
worlddatasets.Asasimpleillustration,weshowthisphe-
nomenononasimplesyntheticdatadistribution.Suppose
datapointsaresampledaccordingto
y
˘
Uniform
(

1
;
1
g
)
;x
˘N
(
y
1
;I
)
;
(4)
where
e
1
=[1
;
0
;:::
]
.Thisdistributionover
(
x;y
)
isthe
standardGaussianNaiveBayesmodelwithmeans


1
and

1
andcovariance
I
.SeeFigure
1
forthelearning
curveswhen

=0
:
8
andFigure
2
forwhen

=2
:
3
.We
notethatthedataefydoesn'tevenreach
1
:
1
when

=0
:
8
,andthecurvesgetcloserwithmoredata.Onthe
otherhand,when

=2
:
3
,thedataefyexceeds
5
andincreasesdramatically.Thisillustratesthewildlydif-
ferentgainsofactivelearning,dependingonthedataset.
Inparticular,thedataefyishigherforthelessnoisy
dataset,asthethesisofthisworkpredicts.
Algorithm1
UncertaintySampling
Input:
Probabilisticmodel
p
w
(
y
j
x
)
,unlabeled
X
U
,
n
seed
Randomlysample
n
seed
pointswithoutreplacement
from
X
U
andcallthem
X
seed
.
X
U
=
X
U
nX
seed
D
=
;
for
each
x
in
X
seed
do
Query
x
togetlabel
y
D
=
D[f
(
x;y
)
g
endfor
for
n

n
seed
iterations
do
^
w
=argmin
w
P
(
x;y
)
2D

log
p
w
(
y
j
x
)
Choose
x
=argmin
x
2X
U
j
P
^
w
(
y
j
x
)

1
2
j
Query
x
togetlabel
y
X
U
=
X
U
nf
x
g
D
=
D[f
(
x;y
)
g
endfor
^
w
=argmin
w
P
(
x;y
)
2D

log
p
w
(
y
j
x
)
andreturn
^
w
OntheRelationshipbetweenDataandErrorforUncertaintySampling
Figure1.
Activelearningyieldsmeagergainswhentheclusters
areclosertogether(

=0
:
8
).Thedataefyisabout1xto
getto23%error;bothalgorithmsrequireapproximatelythesame
amountofdatatoachievethaterror.
3.Experiments
3.1.Datasets
Wewishtostudythedataefyofactivelearningver-
suspassivelearningacrossacomprehensivesetofdatasets
whichareﬁtypicalﬂofreal-worldsettings.Capturingarep-
resentativesetofdatasetsischallenging,andwewantedto
beasobjectiveandtransparentabouttheprocessaspossi-
ble,sowedetailthedatasetselectionprocessbelow.
WecuratedasetofdatasetssystematicallyfromOpenML,
avoidingsyntheticordegeneratecases.InAugust2017,we
downloadedall7968datasets.Weremoveddatasetswith
missingfeaturesorover1milliondatapoints.Wewanted
alargeunlabeledpool(relativetothenumberoffeatures)
sowekeptdatasetswherethenumberoffeatureswasless
than100andthenumberofdatapointswasatleast10,000.
Inourexperiments,wealloweachalgorithmtoquerythe
labelof
n
=1000
points,sothisstepensuresthat
d

n=
10
and
n
pool

10
n
.Weremarkthatmorethan98%
ofthedatasetswereoutbecausetheyweretoosmall
(hadfewerthan10,000points).138datasetsremained.
Wefurtherremoveddatasetsthatweresynthetic,hadun-
cleardescriptions,orwereduplicates.Wealsoremoved
datasets.Formulticlassdatasets,wepro-
cessedthemtobinarybypredictingmajority
classversustherest.Ofthe138datasets,36survived.
Weranstandardlogisticregressionontrainingsplitsof
thesedatasets.In11cases,logisticregressionwaslessthan
1%betterthanthethatalwayspredictsthemajor-
ityclass.Sincelogisticregressionwasnotmeaningfulfor
thesedatasets,weremovedthem,resultingin25datasets.
Figure2.
Activelearningyieldsspectaculargainswhentheclus-
tersarefartherapart(

=2
:
3
).Thedataefyisabout5xto
getto16%error;passivelearningrequiresabout1000datapoints
toachievethaterror,whileactivelearningonlyrequiresabout
200.
Ononeofthesedatasets,logisticregressionachieved0%
errorwithfewerthan
40
datapoints.Onanotherdataset,
theperformanceofrandomsamplingbecame
worse
asthe
numberoflabelsincreased.Ontwodatasets,activelearn-
ingachievedatleast1%errorlowerthantheerrorwith
the
fulltrainingset
,aphenomenonthat
Schohn&Cohn
(
2000
)callsﬁlessismoreﬂ;thisisbeyondthescopeofthis
work.Weremovedthesefourcases,resultingatotalof21
datasets.
The21datasetshasalargeamountofvariability,from
healthcare,gameplaying,control,ecology,economics,
computervision,security,andphysics.
3.2.Methodology
Weusedarandomsamplingseedofsize
n
seed
=100
and
plottedthelearningcurvesupuntilalabelingbudgetof
n
=1000
.Wecalculatedthedataefyatthelower
oftheerrorsachievedwiththe
n
=1000
budgetbyactive
andpassivelearning.Asaproxyforthelimitingerror,we
usetheerroronthetestsetobtainedbyaclassifertrained
onthefulltrainingset.
3.3.Results
Figure
3
plotstherelationshipbetweendataefyand
theinverseerroracrossalldatasets.Toremoveoutliers,we
cappedtheinverseerrorat
50
;thistruncatedtheinverseer-
rorofthreedatasetswhichhadinverseerrorof
190
,
3200
,
and
27000
whichcorrespondstoerrorslessthanaround
0.5%.Thecorrelation(
R
2
ofthebestlinearis
0
:
789
.
Further,thedataefyandtheinverseerrorhavea
Spearmanrankcorrelationof
0
:
669
.
OntheRelationshipbetweenDataandErrorforUncertaintySampling
Figure3.
Scatterplotofthedataefyofuncertaintysam-
plingversustheinverseerrorusingallthedata.Lineofbest
has0.789
R
2
,alsoknownasthePearsonCorrelation.
Insummary,wenotethatdataefyiscloselytiedto
theinverseerror.Inparticular,whentheerrorisbelow
10%,thedataefyisatleast3xandcanbemuch
higher.
4.TheoreticalAnalysis
Inthissection,weprovidetheoreticalinsightintothein-
verserelationshipbetweendataefyandlimitinger-
ror.Fortractability,westudytheasymptoticbehavioras
thenumberoflabels
n
tendsto.
Let
p
(
x;y
)
betheunderlyingdatadistributionover
R
d


1
;
1
g
.Foruncertaintysampling,therearethreedata
quantities:
n
seed
,thenumberofseeddatapoints;
n
,the
amountoflabeleddata(thebudget);and
n
pool
,thenumber
ofunlabeledpointsinthepool.Wewillassumethat
n
seed
and
n
pool
arefunctionsof
n
,andwelet
n
goto.In
particular,wewishtoboundthevalueof
lim

!
Err
DE
(

)
whereErristhelimitingerrorinSection
2.1
.
BoundingDE
(

)
forsmall

iscloselyrelatedtothesta-
tisticalasymptoticrelativeefy(
vanderVaart
,
1998
).
Weusedataefyasitappliesfor
n
.
Theasymptoticdataefyonlymakessenseiftheran-
domsamplinganduncertaintysamplingbothconvergeto
thesameerror.Otherwise,theasymptoticdataefy
wouldeitherbe
0
or
1
.Whilebiasinactivelearningisan
importanttopicofstudy(
Liuetal.
,
2015
),itisbeyondthe
scopeofthiswork.Wewillmakeanassumptionthaten-
suresthisisifthemodelisinsome
smallslabaroundthedecisionboundary.
4.1.Two-stageVariantofUncertaintySampling
Becauseofthecomplicatedcouplingbetweenuncertainty
samplingdrawsandupdates,weanalyzeatwo-stagevari-
ant:wegatheraninitialseedsetusingrandomsampling
fromtheunlabeleddataset,andthengatherthepointsclos-
esttothedecisionboundarylearnedfromtheseeddata.
Thistwo-stageapproachissimilartootheractivelearning
work(
Chaudhurietal.
,
2015
).
Thus,weonlyupdatetheparameterstwice:aftertheseed
roundwetrainontheseeddata,andafterwehavecollected
allthedata,wetrainonthedatathatwascollectedafterthe
seeddata.Wedonotupdatetheparametersbetweendraws
closesttothedecisionboundary.
Also,insteadofalwayschoosingthepointclosesttodeci-
sionboundarywithoutreplacementduringtheuncertainty
samplingphase,with
>
0
probabilitywerandomlysam-
plefromtheunlabeledpoolandwith
1


probabilitywe
choosethepointclosesttothedecisionboundary.Theran-
domsamplingproportion

ensuresthattheempiricaldata
covarianceisnon-singularforuncertaintysampling.
4.2.SketchofMainResult
Underassumptionsthatwillbedescribedlater,ourmain
resultisthatthereexistssome

0
suchthatforanyErr
<
<
0
,
DE
(

)
>
s
4
Err
;
(5)
where
s
isaconstantboundingaratioofconditionalco-
variancesinthedirectionsorthogonalto
w

.Inparticular,
ifthepdffactorizesintotwomarginaldistributions(decom-
positionof
x
intotwoindependentcomponents),onealong
thedirectionof
w

andoneinthedirectionsorthogonalto
w

,thentheconditionalcovariancesorthogonalto
w

are
equal,and
s
=1
.Ifthedistributionisadditionallysym-
metricacrossthedecisionboundary,weobtain
1
4
Err
<DE
(

)
<
1
2
Err
:
(6)
Wenowgivearoughproofsketch.Thecoreideaistocom-
paretheFisherinformationofactiveandpassivelearning,
similartootherworkintheliterature(
Souratietal.
,
2017
).
ItisknownthattheFisherinformationmatrixforlogistic
regressionis
I
=
E
[
˙
(1

˙
)
xx
>
]
;
(7)
where
˙
=
˙
(
yx

w

)
.Notethat
˙
onlydependsonthe
partof
x
parallelto
w

.Ifthedatadecomposesintotwo
independentcomponentsasmentionedabove,then
I
passive
=
E
[
˙
(1

˙
)]
E
[
xx
>
]
(8)
OntheRelationshipbetweenDataandErrorforUncertaintySampling
ifweignorethedimensionoftheFisherinformationalong
w

whichdoesn'tendupmattering(itonlychangesthe
magnitudeof
w

whichisindependentofthe0-1loss).Ad-
ditionally,sinceuncertaintysamplingsamplesatthedeci-
sionboundarywhere
w

x

=0
,wehave
˙
=
1
2
andthus
activelearningachieves:
I
active
=
1
4
E
[
xx
>
]
:
(9)
TheFisherinformationdeterminestheasymptoticrateof
convergenceoftheparameters:
p
n
(
w
n

w

)
d
!N
(0
;
I

1
)
:
(10)
Intuitively,thisconvergencerateismonotonicwith
I

1
=n
whichmeanstheratio(abuseofnotation,buttrueforany
linearfunctionoftheinverseFisherinformation)ofthein-
verseFisherinformationmatricesgivestheasymptoticrel-
ativerate,
DE
(

)
ˇ
I

1
passive
I

1
active
ˇ
1
=
4
E
[
˙
(1

˙
)]
:
(11)
Iftheoptimallogisticmodelis
calibrated
,meaningthe
model'spredictedprobabilitiesareonaveragecorrect,then
Err
2

E
[
˙
(1

˙
)]

Err
:
(12)
Puttingthesetogether,weget:
1
4
Err
/
DE
(

)
/
1
2
Err
:
(13)
Havinggiventheroughintuition,wenowgothroughthe
argumentsmoreformally.
4.3.Notation
Let
w

bethelimitingparameters,Let
w
0
betheweights
aftertheseedroundforactivelearning,and
w
n
bethe
weightsattheendoflearningwith
n
labels.
Weincludeabiastermforlogisticregressionbyinserting
acoordinateatthebeginningof
x
thatisalways
1
.Thus,
x
0
=1
and
w

0
isthebiastermoftheoptimalparame-
ters.Asaofnotation,thepdf
p
(
x
)
isonlya
functionofthenon-biascoordinates(otherwise,suchapdf
wouldn'texist).
Sincelogisticregressionisinvarianttotranslations(wecan
appropriatelychangethebias)androtations(wecanrotate
thenon-biasweights),withoutlossofgenerality,wewill
assumethat
w

=
k
w

k
e
1
,thatthebiastermis
0
,and
thatthedataismean
0
foralldirectionsorthogonalto
w

,
(
E
[
x
2:
]=0
).
4.4.Assumptions
Wehavefourtypesofassumptions:assumptionsonthe
valuesof
n
seed
and
n
pool
,assumptionsonthedistribu-
tionof
x
,assumptionsonthedistributionof
y
,andnon-
degeneracyassumptions.Asanexample,alltheseassump-
tionsareif
n
seed
=
p
n
,
n
pool
=
n
p
n
,
x
isa
mixtureoftruncated,molGaussians,and
y
iswell-
fornon-zeroweights.
4.4.1.A
SSUMPTIONSRELATING
n
SEED
;n;n
POOL
Recallthat
n
seed
isthenumberoflabelsfortheseedround,
n
isthelabelingbudget,and
n
pool
isthenumberofunla-
beleddatapoints.
Assumption1
(DataPoolSize)
.
n
pool
=
!
(
n
)
.
Assumption2
(SeedSize)
.
n
seed
=
n
ˆ
)
forsome
ˆ>
0
and
n
seed
=
o
(
n
)
.
Weneedthesize
n
pool
oftheunlabeledpoolhastobelarge
enoughsothatuncertaintysamplingcanselectpointsclose
tothedecisionboundary.Werequirethattheseedforun-
certaintysamplingislargeenoughtomakethedecision
boundaryaftertheseedroundconvergetothetruedeci-
sionboundary,andwerequirethatitissmallenoughso
thatitdoesn'tdetractfromtheadvantagesofuncertainty
sampling.
4.4.2.A
SSUMPTIONON
x
DISTRIBUTION
Weassumethatthedistributionon
x
hasapdf(ﬁcontinuous
distributionﬂ),andthefollowingtwoconditionshold:
Assumption3
(BoundedSupport)
.
9
B>
0:Pr[
k
x
k
>B
]=0
(14)
Assumption4
(Lipschitz)
.
Thepdfsandconditionalpdfs
p
(
x
)
;p
(
x
j
w

x
=
b
)
;p
(
x
j
w
1

x
1
=
b
1
;w
2

x
2
=
b
2
)
are
allLipschitz.
4.4.3.A
SSUMPTIONSON
x;y
DISTRIBUTION
Thesenextthreeassumptions(Assumptions
5
Œ
7
)areim-
pliedifthelogisticregressionmodelis
(
Pr[
y
j
x
]=
˙
(
yx

w

)
),buttheyarestrictlyweaker.Ifthe
readeriswillingtoassumethissection
canbeskipped.
Assumption5
(LocalExpectedLossisZero)
.
Thereexists

suchthatfor
k
w

w

k

,
E
w

x
=0
[
r
w
(

log
p
w

(
x;y
))]=0
(15)
Assumption
5
isifmodelisinany
thinslabaroundthedecisionboundaryby
w

.We
needthisassumptiontoconcludethatourtwo-stageuncer-
taintysamplingalgorithmconvergesto
w

.
OntheRelationshipbetweenDataandErrorforUncertaintySampling
Figure4.
Exampleofdistributionwithdeterministiclabelswhich
iscalibratedbutnotforlogisticregression.
Assumption6
(ConditionsonZero-OneLoss)
.
Let
Z
(
w
)=Pr
x;y
[
yx

w<
0]
bethezero-onelossofthe
bytheweights
w
.Then,

Z
istwice-differentiableat
w

,

Z
hasalocaloptimumat
w

,and

r
2
Z
(
w

)
6
=0
.
Inordertoconcludethatconvergencetotheoptimalparam-
etersimpliesconvergenceinerror,weneedAssumption
6
.
Thestrongestrequirementisthelocaloptimumpart.The
twicedifferentiableconditionisaregularityconditionand
theHessianconditionisgenerically
Assumption7
(Calibration)
.
Pr[
y
j
w


x
=
a
]=
˙
(
ya
)
(16)
Wesayamodelis
calibrated
iftheprobabilityofaclass,
conditionedonthemodelpredictingprobability
p
,is
p
.As-
sumption
7
amountstoassumingthatthelogisticmodel
withtheoptimalparameters
w

iscalibrated.Notethat
thisisweakerthanassumingthatthemodel
is(
Pr[
y
j
x
]=
˙
(
yx

w

)
).Forexample,
thedatadistributioninFigure
4
iscalibratedbutnotwell-

Thesethreeassumptionsallholdifthelogisticdistribution
ismeaning
Pr[
y
j
x
]=
˙
(
yx

w

)
.
4.4.4.N
ON
-
DEGENERACY

p
0
=
R
w


x
=0
p
(
x
)
asthemarginalprobability
den-
sity
ofselectingapointatthedecisionboundary.More
precisely,
p
0
istheprobabilitydensity
p
(
x
)
integratedover
the
d

1
dimensionalhyperplanemanifoldby
w


x
=0
.Equivalently,
p
0
istheprobabilitydensityof
therandomvariable
w

k
w

k

x
at
0
.
Assumption8
(Non-degeneracies)
.
p
0
6
=0
;
k
w

k6
=0
;
Err
6
=0
;
det(
E
[
xx
>
])
6
=0
(17)
Letusinterpretthesefourconditions.Weassumethatthe
probability
density
atthedecisionboundaryisnon-zero,
p
0
6
=0
,otherwiseuncertaintysamplingwillnotselect
pointsclosetothedecisionboundary(notethisisnotan
assumptionabouttheprobability
mass
).Weassumethat
k
w

k6
=0
,meaningthattheisnotdegener-
ate,withallpointsonthedecisionboundary.Weassume
Err
6
=0
,meaningthelogisticparametersdonotachieve
0%error.Finally,weassume
det(
E
[
xx
>
])
6
=0
,mean-
ingthatthedatacovarianceisnon-singular,orequivalently,
thattheparametersare
4.5.Proofs
Wewillproveaconditionontheconvergencerateof
theerrorbasedonaquantity

closelyrelatedtotheFisher
Information.However,wecan'trelyontheusualFisherin-
formationanalysis,whichdoesnotconnecttothezero-one
loss,butrathertotheasymptoticnormalityoftheparame-
ters.Thus,ourconditionsforthiskeylemmaareslightly
strongerthantheasymptoticnormalityresultofFisherIn-
formation.
4.5.1.R
ATES
L
EMMAIN
T
ERMSOF

Thelogisticloss(negativelog-likelihood)forasingledata
pointunderlogisticregressionis
`
w
(
x;y
)=log(1+exp(

w

yx
))
:
(18)
Further,thegradientandHessianare,
r
`
w
(
x;y
)=

yx˙
(

w

yx
)
(19)
r
2
`
w
(
x;y
)=
˙
(
w

yx
)
˙
(

w

yx
)
xx
>
(20)
Notethat
˙
(

x
)=1

˙
(
x
)
.
FollowingtheFisherInformationasymptoticnormality
analysis,notethat
p
n
(
w
n

w

)=
A

1
n
b
n
;
(21)
where
A
n
=
1
n
X
i
r
2
`
w
0
(
x
i
;y
i
)
;
(22)
b
n
=
1
p
n
X
i
r
`
w

(
x
i
;y
i
)
;
(23)
with
k
w
0

w

kk
w
n

w

k
.ThisisjustbyTaylor's
theoremsincethelogisticlossissmooth.
OntheRelationshipbetweenDataandErrorforUncertaintySampling
Fromthese,wecanthekeyquantity

thatisequiv-
alenttotheinverseFisherInformationunderstrongercon-
ditions.
4.1.
If
A
n
P
!
A
(non-singularandsymmetric)
and
E
[
b
n
b
T
n
]
!
B
exists,then
=
A

1
BA

1
:
(24)
Thisquantityisimportantbecauseofthefollowinglemma,
whichtranslatescomparisonsintheasymptoticvariances
tocomparisonsofdataefy.Recallthatwithoutloss
ofgenerality,welet
w

=
k
w

k
e
1
.
A

1
asthe
matrix
A
withouttherowandcolumn.
Lemma4.1.
Ifwehavetwoestimatorswithasymptotic
variances

a
and

b
,andforany
>
0
andbothesti-
mators,
n
Pr[
k
A
n

A
k

]
!
0
and
n
Pr[
k
w
n

w

k

]
!
0
,then

a;

1
˜
c

b;

1
(25)
impliesthatforsome

0
andanyErr
<<
0
,
n
a
(

)

cn
b
(

)
:
(26)
Theproofisintheappendix.Thislemmaonlyrequires
Assumption
6
,theconditionon
Z
at
w

,andispossiblyof
independentinterest.
Notethatwiththebiasterm,ourweightvectoris
d
+1
di-
mensional,so

isasquare
d
+1
dimensionalmatrix.How-
ever,withouttherowandcolumn,


1
isasquare
d
dimensionalmatrix.Thefactthattheratesdependon


1
insteadof

isnecessaryforourresults.Intuitively,the
coordinate(indirectionof
w

)hasslowconvergence
foruncertaintysamplingsinceweareselectingpointsnear
thedecisionboundarywhichhavesmallprojectiononto
w

andthuswegainlittleinformationaboutthedependenceof
y
on
x
1
.However,becauseouranalysisisintermsofthe
convergenceofthe0-1errorratherthanconvergenceofthe
parameters,theabovelemmadoesn'tdependonthecon-
vergencerateofthecoordinate.
Fromthislemma,itfollowsthatif
c
1

active
;

1
˚

passive
;

1
˚
c
2

active
;

1
;
(27)
thenforsufsmallerror,
c
1

DE
(

)

c
2
:
(28)
4.5.2.S
PECIFIC
C
ALCULATIONSFOR
A
LGORITHMS
Inprovingthelaterresults,it'susefultoestablishthe
consistencyofouralgorithms.Assumption
5
isusedhere.
Lemma4.2.
Bothtwo-stageuncertaintysamplingand
randomsamplingconvergeto
w

.
Next,weneedourtwoalgorithmssatisfytheconditionsof
Lemma
4.1
.
Lemma4.3.
Forouractiveandpassivelearningalgo-
rithms,forany
>
0
,
n
Pr[
k
A
n

A
k

]
!
0
and
n
Pr[
k
w
n

w

k

]
!
0
.
Now,wearereadyforthecomputationof


4.1
),thequantitycloselyrelatedtotheinverseFisherIn-
formation.
Lemma4.4.

passive
=
E
[
˙
(1

˙
)
xx
>
]

1
(29)
Theproofisintheappendix.Theproofreliesoncal-
ibration,Assumption
7
,toensurethat
E
[
r
2
`
w
(
x;y
)]=
Cov
(
r
`
w
(
x;y
))
,whichisalwaystruefor
models.
Thislemmagives

asexactlytheinverseFisherinforma-
tionthatwasmentionedearlier.Itistheexpectedvalueof
r
2
`
w

(
x;y
)=
˙
(1

˙
)
xx
>
.
Lemma4.5.

active
=
(30)

(1


)
E
x
1
=0
[
˙
(1

˙
)
xx
>
]+

E
[
˙
(1

˙
)
xx
>
]


1
Theproofisintheappendix.Theproofreliesontheas-
sumptionsofboundedsupportandLipshitzpdf,Assump-
tions
3
and
4
.
Becausewerandomlysamplefor

proportion,afactorof

times

passive
showsup.Additionally,wegeta
1


fac-
torfortheexpectedvalueof
r
2
`
w

(
x;y
)=
˙
(1

˙
)
xx
>
atthedecisionboundary.Wewillalmostsurelyneversam-
pleexactlyatthedecisionboundary,butas
n
!1
,the
seedroundweights
w
0
!
w

and
n
pool
=n
!1
,wesam-
plecloserandclosertothedecisionboundary.
4.5.3.R
ESULTS
Here,we
s
thathowmuchthecovariance
atthedecisionboundarydiffersfromthecovarianceforthe
restofthedistribution,whichisakeydependencyofour
mostgeneraltheorem.Denote
x

1
asthevector
x
without
theindex.Recallthatwithoutlossofgenerality,
w

=
k
w

k
e
1
.
4.2.
We
s
intermsof
C
0
and
C
1
,
C
0
=
E
x
1
=0
[
x

1
x
>

1
]
(31)
C
1
=
E
[
˙
(1

˙
)
x

1
x
>

1
]
E
[
˙
(1

˙
)]
(32)
1
s
=
k
C

1
=
2
0
C
1
C

1
=
2
0
k
2
(33)
OntheRelationshipbetweenDataandErrorforUncertaintySampling
Wecangiveaninterpretationtotheseconstants.
C
(
a
)=
E
[
x

1
x
T

1
j
x
1
=
a
]
asthecovarianceofthedirec-
tionsorthogonalto
w

attheslice
x
1
=
a
.Then,
C
0
is
simply
C
(0)
,thecovarianceatthedecisionboundary.
Further,avariable
B
thatis
x
1
weightedby
˙
(1

˙
)
:
p
(
B
=
b
)
/
˙
(
k
w

k
b
)(1

˙
(
k
w

k
b
))
p
(
x
1
=
b
)
:
(34)
Then,
C
1
=
E
[
C
(
B
)]
,thecovarianceoverthewholedis-
tribution,butweightedhighernearthedecisionboundary
withexponentialtails.Finally,
s
compareshowmuchthese
twocovariancesdiffer.
Intuitively,weneedthisparametertohandlethecasewhere
thecovarianceatthedecisionboundary(afactorofthe

foractivelearning)issmallrelativetotheaveragecovari-
ance.
Hereisourmaintheoremwhichisprovedbyshowingthat

passive
;

1
˜
s
4
Err

active
;

1
(35)
andthenusingLemma
4.1
.
Theorem4.1.
Forsufsmallconstant

(thatde-
pendsonthedataset)andfor
Err<<
0
,
DE
(

)
>
s
4
Err
:
(36)
Wecanalsogetanupperboundonthedataefyifwe
makeanadditionalassumptionthatthepdfof
x
factorizes
intotwomarginaldistributions(decompositionof
x
into
twoindependentcomponents),onealongthedirectionof
w

andoneinthedirectionsorthogonalto
w

.
Theorem4.2.
If
p
(
x
)=
p
(
x
1
)
p
(
x

1
)
and
p
(
x
1
)=
p
(

x
1
)
,thenforsufsmallconstant

(thatdepends
onthedataset),andforErr
<<
0
,
1
4
Err
<DE
(

)
<
1
2
Err
:
(37)
Wecanthereforeseefromtheseresultsthatthereisanin-
verserelationshipbetweentheasymptoticdataefy
andthepopulationerror,sheddinglightandgivingathe-
oreticalexplanationtotheempiricalobservationmadein
Section
3
.
5.DiscussionandRelatedWork
Theconclusionofthiswork,thatdataefyisin-
verselyrelatedtolimitingerror,hasbeenhintedatbya
couplesentencesinempiricalsurveypapers.
Schein&Un-
gar
(
2007
)statesﬁthedatasetssortneatlybynoise,with
[uncertainty]samplingfailingonmorenoisydata...and
performingatleastaswellasrandom[sampling]for[less
noisy]datasets.ﬂ
Yang&Loog
(
2016
)statesﬁForthe[less
noisy]datasets,randomsamplingdoesnotachievethebest
performance...,whichmayindicatethatweneedonlycon-
siderrandomsamplingonrelatively[noisy]tasksﬂ.
Additionally,thisconclusionhasevidencefromstatistical
activelearningtheory(
Hanneke
,
2014
).Whilenotmen-
tionedinthework,theratiobetweenthepassiveandac-
tiveboundspointstoa
1
=
Errfactor(thoughwithErrbeing
theoptimalerrorovernottheMLE
More,theratiobetweenthepassiveandactive
lowerboundsconvergesto

=
Err
)
as

!
Err.Addition-
ally,theratioofactiveandpassivealgorithmsconvergeto

=
Err
)
;howeverwithafactorofadisagreementcoef
cientwhichhasadimensiondependenceforlinearclassi-
anda
loglog1

factorwhichﬁissometimespossible
toremoveﬂ(
Hanneke
,
2014
).
Thisconclusioncanbeusedinpracticeinatleasttwopos-
sibleways.First,apilotstudyordomainknowledgecanbe
usedtogetaroughestimateoftheerrorandiftheer-
rorislowenough(lessthanaround10%),uncertaintysam-
plingcanbeused.Additionally,randomsamplingcouldbe
rununtilthetesterrorisbelow10%andthenaswitchbe
madetouncertaintysampling.
Doesourconclusionholdforothermodels?Becauseof
themathematicalsimilaritytoSVM,it'slikelyitalsoholds
forhingeloss.Itispossiblethatitalsoholdsforneu-
ralnetworkswithaasoftmaxlayer,sincethesoftmax
layerismathematicallyequivalenttologisticregression.
Infact,
Geifman&El-Yaniv
(
2017
)performsexperiments
withdeepneuralnetworksandmulticlasson
MNIST(1%error,6xdataefy),CIFAR-10(10%,
2x),andCIFAR-100(35%,1x)andresultsthatare
explainedwellbyourconclusion.
Inconclusion,wemakeanobservation,clearlya
phenomenon,demonstrateitempirically,andanalyzeit
theoretically.Thethesisofthiswork,thatthedataef
ciencyofuncertaintysamplingonlogisticregressionisin-
verselyproportionaltothelimitingerror,shedslightonthe
appropriateuseofactivelearning,enablingmachinelearn-
ingpractitionerstointelligentlychoosetheirdatacollection
techniques,whetheractiveorpassive.
Reproducibility
Thecode,data,andexperimentsforthispaperareavailable
ontheCodaLabplatformat
https://worksheets.codalab.org/worksheets/
0x8ef22fd3cd384029bf1d1cae5b268f2d/
.
OntheRelationshipbetweenDataandErrorforUncertaintySampling
Acknowledgments
ThisresearchwassupportedbyNSFgrantDGE-
1656518
.
References
Balcan,M.-F.,Broder,A.,andZhang,T.Marginbased
activelearning.In
InternationalConferenceonCompu-
tationalLearningTheory
,pp.35Œ50.Springer,2007.
Balcan,M.-F.,Beygelzimer,A.,andLangford,J.Agnostic
activelearning.
JournalofComputerandSystemSci-
ences
,75(1):78Œ89,2009.
Brinker,K.Incorporatingdiversityinactivelearningwith
supportvectormachines.In
Proceedingsofthe20th
InternationalConferenceonMachineLearning(ICML-
03)
,pp.59Œ66,2003.
Chaudhuri,K.,Kakade,S.M.,Netrapalli,P.,andSanghavi,
S.Convergenceratesofactivelearningformaximum
likelihoodestimation.In
AdvancesinNeuralInforma-
tionProcessingSystems
,pp.1090Œ1098,2015.
Freund,Y.,Seung,H.S.,Shamir,E.,andTishby,N.Selec-
tivesamplingusingthequerybycommitteealgorithm.
Machinelearning
,28(2):133Œ168,1997.
Geifman,Y.andEl-Yaniv,R.Deepactivelearningoverthe
longtail.
arXivpreprintarXiv:1711.00941
,2017.
Hanneke,S.
StatisticalTheoryofActiveLearning
.Now
PublishersIncorporated,2014.
Hoi,S.C.,Jin,R.,Zhu,J.,andLyu,M.R.Semisupervised
svmbatchmodeactivelearningwithapplicationstoim-
ageretrieval.
ACMTransactionsonInformationSystems
(TOIS)
,27(3):16,2009.
Lewis,D.D.andGale,W.A.Asequentialalgorithm
fortrainingtextIn
Proceedingsofthe17th
annualinternationalACMSIGIRconferenceonRe-
searchanddevelopmentininformationretrieval
,pp.3Œ
12.Springer-VerlagNewYork,Inc.,1994.
Liu,A.,Reyzin,L.,andZiebart,B.D.Shift-pessimistic
activelearningusingrobustbias-awareprediction.In
AAAI
,pp.2764Œ2770,2015.
Roy,N.andMcCallum,A.Towardoptimalactivelearn-
ingthroughmontecarloestimationoferrorreduction.
ICML,Williamstown
,pp.441Œ448,2001.
Schein,A.I.andUngar,L.H.Activelearningforlogis-
ticregression:anevaluation.
MachineLearning
,68(3):
235Œ265,2007.
Schohn,G.andCohn,D.Lessismore:Activelearning
withsupportvectormachines.In
ICML
,pp.839Œ846,
2000.
Settles,B.Activelearningliteraturesurvey.
Computer
SciencesTechnicalReport
,1648,2010.
Seung,H.S.,Opper,M.,andSompolinsky,H.Queryby
committee.In
Proceedingsoftheannualworkshop
onComputationallearningtheory
,pp.287Œ294.ACM,
1992.
Sourati,J.,Akcakaya,M.,Leen,T.K.,Erdogmus,D.,and
Dy,J.G.Asymptoticanalysisofobjectivesbasedon
informationinactivelearning.
JournalofMachine
LearningResearch
,18(34):1Œ41,2017.
Tong,S.andKoller,D.Supportvectormachineactive
learningwithapplicationstotext
Journal
ofmachinelearningresearch
,2(Nov):45Œ66,2001.
vanderVaart,A.W.
Asymptoticstatistics
.Cambridge
UniversityPress,1998.
Yang,Y.andLoog,M.Abenchmarkandcomparisonof
activelearningforlogisticregression.
arXivpreprint
arXiv:1611.08618
,2016.
OntheRelationshipbetweenDataandErrorforUncertaintySampling
6.Appendix
6.1.Notation
w
0
istheweightsaftertheseedround.
A

1
isthematrixwithouttherowandcolumn.
A
1
;

1
isthevectorfromtherowandallcolumnsexceptthe
column.
Generally,the
O
(
f
(
n
))
notationhidesconstantsthatonlydependonthedataset,suchas
k
w

k
,
s
,
B
,etc.
Fortheorderofthingsgoingtozero,wechoose

tobesmall,then
r
tobesmall,then
n
tobelarge.
w
0
isweightvectorafterseedround

active
(
n
)=
E
f
˘
active
;n
points
[
Err
(
f
)]

passive
(
n
)=
E
f
˘
passive
;n
points
[
Err
(
f
)]
DE
(

)=
max
f
n
:

passive
(
n
)


g
max
f
n
:

active
(
n
)


g
=
n
passive
(

)
n
active
(

)
Withoutlossofgenerality,assume
w

=
k
w

k
e
1
,
w

0
=0
,and
E
[
x
2:
]=0
.
Withanabuseofnotation,let
˙
=
˙
(
w


x
)=
˙
(
k
w

k
x
1
)
.
6.2.Losses

˙
(
x
)=
1
1+

exp(
x
)
.
Theloss(negativelog-likelihood)forasingledatapointunderlogisticregressionis
l
w
(
x;y
)=log(1+exp(

w

yx
))
andsothegradientis
r
l
w
(
x;y
)=

yx
exp(

w

yx
)
1+exp(

w

yx
)
=

yx˙
(

w

yx
)
andtheHessianis
r
2
l
w
(
x;y
)=
(
yx
)(
yx
)
T
exp(
w

yx
)
(1+exp(
w

yx
))
2
=
xx
T
(1+exp(
w

yx
))(1+exp(

w

yx
))
=
˙
(
w

yx
)
˙
(

w

yx
)
xx
T
Notethat
˙
(

x
)=1

˙
(
x
)
.
6.3.DecisionBoundary
Lemma6.1.
Forsufsmall
r
,if
k
w
0

w

k
2

2
r
,then
j
Z
w
0

x
=0
p
(
x
)

Z
w


x
=0
p
(
x
)
k
=
O
(
r
)
OntheRelationshipbetweenDataandErrorforUncertaintySampling
Proof.
Withoutlossofgenerality(rotationandtranslation),let
w

0
=0
,
w

=
k
w

k
e
1
andlet
w
0
=
w
0
1
e
1
+
w
0
2
e
2
.
Wesamplefromplaceswhere
w
0
0
+
w
0
1
x
1
+
w
0
2
x
2
=0
whichoccurswhen
x
1
=
w
0
2
w
0
1
x
2
+
w
0
0
w
0
1
=
ax
2
+
b
.Fromthe
theoremassumption,weknowthat
j
w
0
0
j
;
j
w
0
2
j
r
and
j
w
0
1
jk
w

k
r

1
2
k
w

k
(forsufsmall
r
)soweknow
that
j
a
j
;
j
b
j
O
(
r
)
Notethat
j
Z
w
0

x
=0
p
(
x
)

Z
w


x
=0
p
(
x
)
k
=
k
Z
x
p
(
x
1
=
ax
+
b;x
2
=
x
)

p
(
x
1
=0)
j
(NotethattheJacobianofthechangeofvariableshasthefollowingmatrixwhichhasdeterminant
1
)

10

a
1

j
Z
w
0

x
=0
p
(
x
)

Z
w


x
=0
p
(
x
)
k
Z
x
j
p
(
x
1
=
ax
+
b
j
x
2
=
x
)
p
(
x
2
=
x
)

p
(
x
1
=0
j
x
2
=
x
)
p
(
x
2
=
x
)
j
WiththeassumptionthattheconditionalprobabilitiesareLipschitz,

Z
x
L
j
ax
+
b
j
p
(
x
2
=
x
)

aLB
+
bL
=
O
(
r
)
Lemma6.2.
Forsufsmall
r
,if
k
w
0

w

k
2

r
,thenwithprobabilitygoingto
1
exponentiallyfast,allpoints
fromtwo-stageuncertaintysamplingarefromsomehyperplane
w
0
suchthat
k
w
0

w

k
2
r
.
Proof.
Forsmallenough
r
,then
R
w
0

x
=0
p
(
x
)
>p
0
=
2
fromtheabovelemmaif
k
w
0

w

k
2

2
r
.Thus,theprobability
ofanunlabeledpointwithintheparallelplanewithbiaslessthan
r
differentfrom
w
0
suchthat
k
w
0

w
0
k
2

r
isatleast
2
r
k
w
0
k
(
p
0
=
2)

rp
0
2
k
w

k
=
r
)
(forsufsmall
r
).
Recallthat
n
pool
=
!
(
n
)
and
n
seed
=
o
(
n
)
.
Forsuflarge
n
,theprobabilityofatleast
n
pointsfromthe
n
pool

n
seed
unlabeledpointsfallinginthisrangeis
Pr[
Binomial
(
n
pool

n
seed
;
probabilityoffalling
)

n
]

Pr[
Binomial
(
n
pool
=
2
;C
1
r
))

n
]
forsomeconstant
C
1
.
WecanuseaChernoffbound(standardwith

=1
=
2
)since
n
pool
=
!
(
n
)
toboundby
exp(

!
(
n
))
.Thustheprobability
thattheplaneswechoosefromarefartherthan
r
awayfrom
w
0
goesto
0
withratefasterthan
exp(

n
)
.
6.4.Convergence
Lemma
4.2
.
Bothtwo-stageuncertaintysamplingandrandomsamplingconvergeto
w

.
Proof.
Forpassivelearning,theHessianofthepopulationlossispositivebecausethedatacovarianceisnon-
singular(Assumption
8
).Thus,thepopulationlosshasauniqueoptimum.Bytheof
w

,
w

istheminimizer.
Sincethesamplelossconvergestothepopulationloss,theresultofpassivelearningconvergesto
w

.
OntheRelationshipbetweenDataandErrorforUncertaintySampling
Byasimilarargument,theweightvector
w
0
aftertheseedroundconvergesto
w

since
n
seed
issuper-constant(Assumption
2
).Thus,forany
r>
0
,withprobabilityconvergingto
1
as
n
!1
,
k
w
0

w

k
r


2
.ByLemma
6.2
,with
probabilitygoingto
1
,allpointsselectedarefromhyperplanes
w
where
k
w

w

k
2
r


.Thus,byAssumption
5
,
E
w

x
=0
[
r
l
w

(
x;y
)]=0
.Inthesecondstage,becauseofthe

proportionofrandomlyselectedpoints,thelossfrom
thenewuncertaintysamplingpopulationhasauniqueoptimum.Andbecausetheexpectationofthegradientoftheloss
is
0
forthepointsnearthedecisionboundary(withprobabilitygoingto
1
),theresultoftwo-stageuncertaintysampling
convergesinprobabilityto
w

.
6.5.Rates
Lemma.
If

exists,andforany
>
0
,
n
Pr[
k
A
n

A
k

]
!
0
and
n
Pr[
k
w
n

w

k

]
!
0
,thenthereexistvectors
c
k
6
=0
thatdependonlyonthedatadistributionsuchthat,
n
(

(
n
)

Err
)
!
X
k
c
T
k


1
c
k
Proof.
Thezero-oneerroris
Z
(
w
n
)=Pr[
yx

w
n
<
0]
Since
Z
istwicedifferentiableat
w

,byTaylor'stheorem,
Z
(
w
n
)=
Z
(
w

)+(
r
Z
(
w

))
T
(
w
n

w

)+(
w
n

w

)
T
(
1
2
r
2
Z
(
w

))(
w
n

w

)+(
w
n

w

)
T
R
(
w
n

w

)(
w
n

w

)
T
where
R
(
w
)
!
0
as
w
!
0
.
Since
Z
hasalocaloptimumat
w

,
r
Z
(
w

)=0
.Also
Z
(
w

)=
Err
.Additionally,denote
H
=
1
2
r
2
Z
(
w

)
,
Z
(
w
n
)=
Err
+(
w
n

w

)
T
(
H
+
R
(
w
n

w

))(
w
n

w

)
Chooseany
>
0
.Since
R
(
w
)
!
0
as
w
!
0
,thereis


suchthat
k
w
k


=
)k
R
(
w
)
k

.
near
(
n
)
tobe
theeventthat
k
A
n

A
k

^k
w
n

w

k


.Notethatfromthetheoremassumption,
n
Pr[
:
near
(
n
)]
!
0
.

(
n
)=
E
[
Z
(
w
n
)]=Pr[
:
near
(
n
)]
E
[
Z
(
w
n
)
j:
near
(
n
)]+Pr[
near
(
n
)]
E
[
Z
(
w
n
)
j
near
(
n
)]
j

(
n
)

n
E
[
Z
(
w
n
)
j
near
(
n
)]
j
n
Pr[
:
near
(
n
)]
j
E
[
Z
(
w
n
)
j:
near
(
n
)]

E
[
Z
(
w
n
)
j
near
(
n
)]
j

n
Pr[
:
near
(
n
)]
!
0
Thus,
n
(

(
n
)

Err
)
!
n
(
E
[
Z
(
w
n
)
j
near
(
n
)]

Err
)
Soweneedtojustworryabouttheconvergenceoftherightside,
E
[
Z
(
w
n
)
j
near
(
n
)]=
Err
+
1
n
E
[(
A

1
n
b
n
)
T
(
H
+
R
(
w
n

w

))(
A

1
n
b
n
)
j
near
(
n
)]
n
(
E
[
Z
(
w
n
)
j
near
(
n
)]

Err
)=
E
[
b
T
n
A

1
n
(
H
+
R
(
w
n

w

))
A

1
n
b
n
j
near
(
n
)]
OntheRelationshipbetweenDataandErrorforUncertaintySampling
Becauseweconditionedon
near
(
n
)
,
k
A
n

A
k

and
k
w
n

w

k


andtherefore
k
R
(
w
n

w

)
k

.So
k
A

1
n
(
H
+
R
(
w
n

w

))
A

1
n

A

1
HA

1
k
=
O
(

)
.Usingthis,weget,
k
n
(
E
[
Z
(
w
n
)
j
near
(
n
)]

Err
)

E
[
b
T
n
A

1
HA

1
b
n
j
near
(
n
)]
kk
E
[
b
T
n
O
(

)
b
n
j
near
(
n
)]
k

O
(

)
k
E
[
k
b
n
k
2
j
near
(
n
)]
k

O
(

)
k
E
[
b
n
b
T
n
j
near
(
n
)]
k
Notethat,
E
[
b
n
b
T
n
]=
E
[
b
n
b
T
n
j
near
(
n
)]Pr[
near
(
n
)]+
E
[
b
n
b
T
n
j:
near
(
n
)]Pr[
:
near
(
n
)]
andthelatertwoexpectationsexistsincetheleftexistsandthematricesarepositivePassingthroughthe
limit,weseethat
E
[
b
n
b
T
n
j
near
(
n
)]
!
B
.
Thus,notingthatwecandrive

!
0
,
n
(
E
[
Z
(
w
n
)
j
near
(
n
)]

Err
)
!
E
[
b
T
n
A

1
HA

1
b
n
j
near
(
n
)]
!
X
i;j
[
A

1
HA

1
]
i;j
E
[
b
n
b
T
n
j
near
(
n
)]
i;j
!
X
i;j
[
A

1
HA

1
]
i;j
B
i;j
Thus,puttingthistogether,weseethat
n
(

(
n
)

Err
)
!
X
i;j
[
A

1
HA

1
]
i;j
B
i;j
Doingmanipulationsontheindices,we
X
i;j
[
A

1
HA

1
]
i;j
B
i;j
=
X
i;j
H
i;j
(
A

1
BA

1
)
i;j
=
X
i;j
H
i;j

i;j
Therefore,
n
(

(
n
)

Err
)
!
X
i;j
H
i;j

i;j
andwearemostofthewaythere,justneedtousesomepropertiestoshowtheform.
Since
w

isalocaloptimum,
H

0
(andsymmetric)andsincetheHessianisnotidenticallyzeroat
w

,
H
6
=0
.
Withoutlossofgenerality,let
w

=
k
w

k
e
1
and
w

0
=0
asassumedbefore.Notethat
Z
(
w

+
e
1
)=
Z
(
w

)
for

2
(

w

k
=
2
;
1
)
.Sinceitisconstantalongthisline,
(
r
2
Z
(
w

))
1
;
1
=0
,andso
H
1
;
1
=0
So
H

0
,
H
issymmetric,
H
6
=0
,and
H
1
;
1
=0
.Since
H

0
and
H
1
;
1
=0
,
H
1
;i
=0
forall
i
.
Since
H

0
and
H
6
=0
,
H
=
P
k
c
k
c
T
k
forsomevectors
c
k
(wherethereisatleastone).Andfurther,
(
c
k
)
1
=0
.
OntheRelationshipbetweenDataandErrorforUncertaintySampling
X
i;j
H
i;j

i;j
=
X
i;j
(
X
k
c
k
c
T
k
)
i;j

i;j
=
X
k
c
T
k

c
k
Wecanremovetheelementsof
c
k
andtherowandcolumnof

withoutchanginganything,so
X
i;j
H
i;j

i;j
=
X
k
c
T
k


1
c
k
Andthusthetheoremisproved.
Lemma.
Ifwehavetwoalgorithms
a
and
b
thatsatisfytheconditionsofLemma
2
,and

a;

1
˜
c

b;

1
thenthereexists

0
suchthatfor
Err<<
0
,
n
a
(

)

cn
b
(

)
Proof.

a;

1
˜


b;

1
X
k
c
T
k

a;

1
c
k
>
X
k
c
T
k

b;

1
c
k
so,for
n>n
0
;n
0
>n
0
,
n
(

a
(
n
)

Err
)
>n
0
(

b
(
n
0
)

Err
)
setting
n
0
=

andfor
n>
max(
n
0
;n
0

)
,
n
(

a
(
n
)

Err
)
>n
(

b
(

)

Err
)
Soforsuflarge
n
,

a
(
n
)
>
b
(

)
Forany
>Err
suchthat
n
a
(

)
issuflarge,(weknowthisexistssince
n
a
(

)=
1


Err
)
)

a
(
n
)


for
n

n
a
(

)

b
(

)


for
n

n
a
(

)

b
(
n
0
)


for
n
0

1

n
a
(

)
n
b
(

)

1

n
a
(

)
n
a
(

)

n
b
(

)
OntheRelationshipbetweenDataandErrorforUncertaintySampling
Lemma
4.1
.
Ifwehavetwoalgorithmswith

a
and

b
,andforany
>
0
andbothestimators,
n
Pr[
k
A
n

A
k

]
!
0
and
n
Pr[
k
w
n

w

k

]
!
0
,then

a;

1
˜
c

b;

1
impliesthatforsome

0
andany
Err<<
0
,
n
a
(

)

cn
b
(

)
Proof.
Thisisastraightforwardapplicationoftheabovelemmas,Lemma
2
andLemma
3
.
6.6.Conditions
Lemma
4.3
.
Forouractiveandpassivelearningalgorithms,forany
>
0
,
n
Pr[
k
A
n

A
k

]
!
0
and
n
Pr[
k
w
n

w

k

]
!
0
Proof.
Recallthat
A
n
=
1
n
X
i
r
2
l
w
0
(
x
i
;y
i
)
b
n
=
1
p
n
X
i
r
l
w

(
x
i
;y
i
)
where
k
w
0

w

kk
w
n

w

k
.
Forpassivelearning,byCLT,forany

,
Pr[
k
w
n

w

k
>
]=
O
(
e


n
)
p
n
)
.Thus,wethat
n
Pr[
k
w
n

w

k

]
!
0
.
Wealsoneedthisfacttobound
w
0
.Then,withaHoeffdingboundonthesumof
A
n
,wecangetthat
Pr[
k
A
n

A
k

]=
O
(
e


n
)
p
n
)
andthus
n
Pr[
k
A
n

A
k

]
!
0
.
Foractivelearning,weneedtobecarefulbecauseif
k
w
0

w

k
>
2
,wearenotevenguaranteedthattheresult
converges(seeLemma
6.2
).However,bytheCLT,wethat
Pr[
k
w
0

w

k
>
2]=
O
(
e


n
seed
)
p
n
seed
)
.Because
n
seed
=
n
ˆ
)
(seeAssumption
2
),thisconvergesexponentiallyfastand
n
Pr[
k
w
0

w

k
>
2]
!
0
.
Becauseofthe

randomsampling,andconditionedontheprobabilitythat
k
w
0

w

k
<
2
,wecangetthesame
resultsforactivelearningasforpassivelearning.NotethatfromLemma
6.2
,thereisexponentiallysmallprobabilityof
notsamplingallpointsfrom
w
0
where
k
w
0

w

k
<
.
6.7.
COV
calculationforpassive
Lemma6.3.
Forpassivelearning,
E
[
r
l
w

(
x;y
)(
r
l
w

(
x;y
))
T
]=
E
[
˙
(1

˙
)
xx
T
]
.
Proof.
Sincethemeanofthederivativeofthelossis
0
at
w

,
E
[
r
l
w

(
x;y
)(
r
l
w

(
x;y
))
T
]
i;j
=
E
[
x
i
x
j
˙
(

w

k
yx
1
)
2
]
=
E
x
1
[
E
[
x
i
x
j
j
x
1
]
E
[
˙
(
k
w

k
yx
1
)
2
j
x
1
]]
=
E
x
1
[
E
[
x
i
x
j
j
x
1
][
P
(
y
=1
j
x
1
)
˙
(

w

k
x
1
)
2
+
P
(
y
=1
j
x
1
)
˙
(
k
w

k
x
1
)
2
]]
fromthecalibratedassumption,
OntheRelationshipbetweenDataandErrorforUncertaintySampling
=
E
x
1
[
E
[
x
i
x
j
j
x
1
][
˙
(
k
w

k
x
1
)
˙
(

w

k
x
1
)
2
+
˙
(

w

k
x
1
)
˙
(
k
w

k
x
1
)
2
]]
=
E
x
1
[
E
[
x
i
x
j
j
x
1
]
˙
(
k
w

k
x
1
)
˙
(

w

k
x
1
)[
˙
(
j
w

k
x
1
)+
˙
(
k
w

k
x
1
)]]
=
E
x
1
[
E
[
x
i
x
j
j
x
1
]
˙
(
k
w

k
x
1
)
˙
(

w

k
x
1
)]
=
E
[
x
i
x
j
˙
(
k
w

k
x
1
)
˙
(

w

k
x
1
)]
=
E
[
˙
(1

˙
)
xx
T
]
i;j
Lemma
4.4
.

passive
=[
E
[
˙
(1

˙
)
xx
T
]]

1
Proof.
Forpassivelearning,bytheconvergenceof
w
n
!
w

andbythelawoflargenumbers,
A
n
!
A
=
E
[
˙
(1

˙
)
xx
T
]
Further,byindependenceofdraws,
E
[
b
n
b
T
n
]=
E
[
r
l
w

(
x;y
)(
r
l
w

(
x;y
))
T
]
sobyLemma
6.3
,
E
[
b
n
b
T
n
]=
E
[
˙
(1

˙
)
xx
T
]
B
=
E
[
˙
(1

˙
)
xx
T
]
B
=
A
Thus,

passive
=
A

1
BA

1
=
A

1
=[
E
[
˙
(1

˙
)
xx
T
]]

1
6.8.
COV
calculationforactive
Lemma6.4.
Forsufsmall
r
(smallwithrespecttodataset-onlydependentconstants),if
k
w
0

w

k
2

2
r
,then
k
E
w
0

x
=0
[
˙
(1

˙
)
xx
T
]

E
w


x
=0
[
˙
(1

˙
)
xx
T
]
k
=
O
(
r
)
and
k
E
w
0

x
=0
[
˙
(

yx
1
k
w

k
)
2
xx
T
]

E
w


x
=0
[
˙
(

yx
1
k
w

k
)
2
xx
T
]
k
=
O
(
r
)
Proof.
Withoutlossofgenerality(rotationandtranslation),let
w

0
=0
,
w

=
k
w

k
e
1
andlet
^
w
=
c
1
e
1
+
c
2
e
2
.
Wesamplefromplaceswhere
w
0
0
+
w
0
1
x
1
+
w
0
2
x
2
=0
whichoccurswhen
x
1
=
w
0
2
w
0
1
x
2
+
w
0
0
w
0
1
=
ax
2
+
b
.Fromthe
theoremassumption,weknowthat
j
w
0
0
j
;
j
w
0
2
j
r
and
j
w
0
1
jk
w

k
r

1
2
k
w

k
(forsufsmall
r
)soweknow
that
j
a
j
;
j
b
j
O
(
r
)
OntheRelationshipbetweenDataandErrorforUncertaintySampling

Q
(
x
1
)=
˙
(
k
w

k
x
1
)
˙
(

w

k
x
1
)
or
Q
(
x
1
)=
˙
(

yx
1
k
w

k
)
2
(abuseofnotation).Boththesefunctionsare
Lipschitzaround
x
1
=0
,andbounded(sincesupportboundedby
B
).
First,wecomputethejoint(nottheconditionals)andthenwecandividebythemarginalsfromthepreviouslemma,
Let
i
1
;i
2
;:::;i
d
beindicatorsfortheindices
i;j
thatarenon-zero.Thus,
i
1
+
i
2
+
:::
+
i
d

2
,
E
w
0

x
=0
[
˙
(1

˙
)
xx
T
]
i;j
=
=
E
w
0

x
=0
[
Q
(
x
1
)(
x
1
)
i
1
(
x
2
)
i
2
(
x
3
)
i
3
:::
]=
(Asbefore,theJacobianofthechangeofvariableshasdeterminant
1
)
Z
x
p
(
x
1
=
ax
+
b;x
2
=
x
)
Q
(
ax
+
b
)(
ax
+
b
)
i
1
(
x
)
i
2
E
[
x
i
3
3
:::
j
x
1
=
ax
+
b;x
2
=
x
]=
=
Z
x
p
(
x
2
=
x
)(
x
)
i
2
F
(
ax
+
b;x
)
where
F
(
x
1
;x
2
)=
p
(
x
1
j
x
2
)(
Q
(
x
1
)
x
i
1
1
)
mathbbE
[
x
i
3
3
:::
j
x
1
;x
2
]
Allthreecomponentsof
F
arebounded,sincesupportbounded,Assumption
3
.Further,allthreecomponentsareLipschitz,
becauseofAssumption
4
andboundedsupportaswell.Therefore,
F
isLipschitz.
j
Z
x
p
(
x
2
=
x
)(
x
)
i
2
F
(
ax
+
b;x
)

Z
x
p
(
x
2
=
x
)(
x
)
i
2
F
(0
;x
)

Z
x
p
(
x
2
=
x
)
j
x
j
i
2
L
j
ax
+
b
j

aLB
i
2
+1
+
bLB
i
2
=
O
(
r
)
Thus,forany
i;j
,
k
E
w
0

x
=0
[
Qxx
T
]
i;j

E
w


x
=0
[
Qxx
T
]
i;j
k
=
O
(
r
)
Wecanusethistoboundthematrixnorm,
k
E
w
0

x
=0
[
Qxx
T
]

E
w


x
=0
[
Qxx
T
]
k
=
O
(
r
)
Sincetheprobabilities(seeLemma
6.1
)andconditionalsarebothoffbyonly
O
(
r
)
(fromabove)andsincetheprobabilities
areboundedawayfrom
0
(seeLemma
6.1
andAssumption
8
),theconditionaldistributionisoffby
O
(
r
)
.Wecanplugin
bothfunctionsof
Q
togetthestatementofthetheorem.
Lemma
4.5
.

active
=[(1


)
E
x
1
=0
[
˙
(1

˙
)
xx
T
]+

E
[
˙
(1

˙
)
xx
T
]]

1
Proof.
Because
w
n
!
w

,andbythelawoflargenumbers,
A
n
!
(1


)
E
w
0
[
E
w
0

x
=0
[
˙
(

yx
1
k
w

k
)
2
xx
T
]]+

E
[
˙
(

yx
1
k
w

k
)
2
xx
T
]
FromLemma
6.4
,
OntheRelationshipbetweenDataandErrorforUncertaintySampling
k
E
w
0

x
=0
[
˙
(1

˙
)
xx
T
]

E
w


x
=0
[
˙
(1

˙
)
xx
T
]
k
=
O
(
r
)
and
k
w
0

w

k
<
2
r
withprobabilitygoingto
1
,
A
n
!
n

n
seed
n
[(1


)
E
w


x
=0
[
˙
(1

˙
)
xx
T
]+
O
(
r
)+

E
[
˙
(1

˙
)
xx
T
]]
Since
w
0
!
w

,
r
!
0
,andsince
n
seed
=
o
(
n
)
(seeAssumption
2
)so
A
n
!
A
=(1


)
E
w


x
=0
[
˙
(1

˙
)
xx
T
]+

E
[
˙
(1

˙
)
xx
T
]
ThesamelineofargumentwithusingLemma
6.4
andLemma
6.3
yields
B
=
A
So

active
=
A

1
BA

1
=
A

1
=[(1


)
E
x
1
=0
[
˙
(1

˙
)
xx
T
]+

E
[
˙
(1

˙
)
xx
T
]]

1
6.9.InversesWithoutFirstCoordinate
Lemma6.5.

a~a
T
~aA


1
=
""
b
~
b
T
~
bB
#
Where
b
=
1
a

~a
T
A

1
~a
~
b
=

bA

1
~a
B
=
A

1
+
b
(
A

1
~a
)(
A

1
~a
)
T
Proof.
Matrixalgebra.
Lemma6.6.
(
A

1
)

1
=(
A

1
)

1
+
((
A

1
)

1
A

1
;
1
)((
A

1
)

1
A

1
;
1
)
T
A
1
;
1

A
T

1
;
1
(
A

1
)

1
A

1
;
1
Proof.
Usetheabovetheoremandnotethat
b>
0
so
b
(
A

1
~a
)(
A

1
~a
)
T

0
OntheRelationshipbetweenDataandErrorforUncertaintySampling
6.10.RelatingErrtoexpectationofsigmoid
Lemma6.7.
Err
2
<
E
[
˙
(1

˙
)]
<Err
Proof.
Err
=
P
(
yx
1
k
w

k
<
0)
=
P
(
x
1
<
0
^
y
=1)+
P
(
x
1
>
0
^
y
=

1)
FromAssumption
7
,
=
Z
0

p
x
1
(
x
1
)
˙
(

w

1
x
1
)+
Z
0
1
0
p
x
1
(
x
1
)
˙
(
w

1
x
1
)
=
Z
1
0
[
p
x
1
(

x
1
)+
p
x
1
(
x
1
)]
˙
(
w

1
x
1
)
Additionally,
E
[
˙
(1

˙
)]=
E
[
˙
(
yx
1
k
w

k
)
˙
(

yx
1
k
w

k
)]
=
E
[
˙
(
k
w

k
x
1
)
˙
(

w

k
x
1
)]
=
Z
0

p
x
1
(
x
1
)
˙
(
k
w

k
x
1
)
˙
(

w

k
x
1
)+
Z
1
0
p
x
1
(
x
1
)
˙
(
k
w

k
x
1
)
˙
(

w

k
x
1
)
=
Z
1
0
[
p
x
1
(

x
1
)+
p
x
1
(
x
1
)]
˙
(
k
w

k
x
1
)
˙
(

w

k
x
1
)
Notethatfor
x
1
>
0
,
1
2
<˙
(

w

k
x
1
)
<
1
.Comparingequations,weget,
Err
2
<
E
[
˙
(1

˙
)]
<Err
6.11.MainDEbound
Theorem
4.1
.
Forsufsmallconstant

(thatdependsonthedataset)andfor
Err<<
0
,
DE
(

)
>
s
4
Err
Proof.
Forconvenience,
Q
=
E
x
1
=0
[
˙
(1

˙
)
xx
T
]
R
=
E
[
˙
(1

˙
)
xx
T
]=
COV
passive
S
=
R
+(1


)
Q
=
COV
active
Bytheof
s
,
E
x
1
=0
[
x

1
x
T

1
]

s
E
[
˙
(1

˙
)
x

1
x
T

1
]
E
[
˙
(1

˙
)]
OntheRelationshipbetweenDataandErrorforUncertaintySampling
ByLemma
6.7
,
4
Q

1
˜
s
Err
R

1
Forsmallenough

,
Q

1
˜
s=
(4
Err
)


1


R

1
R

1
+(1


)
Q

1
˜
s
4
Err
R

1
S

1
˜
s
4
Err
R

1
s
4
Err
(
S

1
)

1
˚
(
R

1
)

1

(
R

1
)

1
ThelaststepcomesfromnotingthattherighthandsideofLemma
6.6
positivefor
A
positive
Additionally,notethattherowandcolumnof
Q
is
0
,
so
S

1
;
1
=
R

1
;
1
and
S
1
;
1
=
R
1
;
1
.
Anexaminationyields,
(
S

1
)

1
S

1
;
1
)(
S

1
)

1
S

1
;
1
)
T
S
1
;
1

S
T

1
;
1
(
S

1
)

1
S

1
;
1
=
O
(

)
UsingLemma
6.6
,wethatwecanmake

smallenoughsothat
s
4
Err
(
S

1
)

1
˚
(
R

1
)

1
s
4
Err
COV
active;

1
˚
COV
passive;

1
sobyLemma
4.1
,for
Err<<
0
,
DE
(

)
>
s
4
Err
6.12.DEBoundGivenDecomposition
Weactuallygetaslightlymoregeneralresultfromthefollowinglemma.
Lemma6.8.
If
p
(
x
)=
p
(
x
1
)
p
(
x

1
)
,thenforsufsmallconstant

(thatdependsonthedataset),andfor
Err<
<
0
,
1
4
Err
<DE
(

)
<
1
2
Err
(1+
E
[
e
X
]
Var
(
e
X
)
)
where
p
(
e
X
=
x
)
/
˙
(
k
w

k
x
)(1

˙
(
k
w

k
x
))
p
(
x
1
=
x
)
OntheRelationshipbetweenDataandErrorforUncertaintySampling
Proof.
Withthedecomposition,intheTheorem
4.1
,
s
=1
.Sowegetforfreethatfor
Err<<
0
,
DE
(

)
>
1
4
Err
Asbefore,forconvenience,
Q
=
E
x
1
=0
[
˙
(1

˙
)
xx
T
]
R
=
E
[
˙
(1

˙
)
xx
T
]=
COV
passive
S
=
R
+(1


)
Q
=
COV
active
Becauseofthedecomposition,
R
2:
;
2:
=
E
[
˙
(1

˙
)]
E
[
x
2:
x
T
2:
]
˜
Err
2
E
[
x
2:
x
T
2:
]
Q
2:
;
2:
=
1
4
E
[
x
2:
x
T
2:
]
Q
2:
;
2:
˚
1
2
Err
R
2:
;
2:
Forsufsmall

,
Q
2:
;
2:
˚
1
=
(2
Err
)


1


R
2:
;
2:
R
2:
;
2:
+(1


)
Q
2:
;
2:
˚
1
2
Err
R
2:
;
2:
S
2:
;
2:
˚
1
2
Err
R
2:
;
2:
Becauseofthedecomposition,andbecause
E
[
x
2:
]=0
(withoutlossofgeneralitybytranslation),
R
0:1
;
2:
=0
Q
0:1
;
2:
=0
1
2
Err
(
A

1
)
2:
;
2:
˜
(
R

1
)
2:
;
2:
Now,letusexaminetheupperleftcorners,
R
0:1
;
0:1
=

E
[
˙
(1

˙
)]
E
[
˙
(1

˙
)
x
1
]
E
[
˙
(1

˙
)
x
1
]
E
[
˙
(1

˙
)
x
2
1
]

S
0:1
;
0:1
=

(1


)
=
4+

E
[
˙
(1

˙
)]

E
[
˙
(1

˙
)
x
1
]

E
[
˙
(1

˙
)
x
1
]

E
[
˙
(1

˙
)
x
2
1
]

Denote
D
=
E
[
˙
(1

˙
)]
E
[
˙
(1

˙
)
x
2
1
]

E
[
˙
(1

˙
)
x
1
]
2
Then,
OntheRelationshipbetweenDataandErrorforUncertaintySampling
(
R

1
)
0
;
0
=
E
[
˙
(1

˙
)
x
2
1
]
D
(
S

1
)
0
;
0
=

E
[
˙
(1

˙
)
x
2
1
]

(1


)(1
=
4)
E
[
˙
(1

˙
)
x
2
1
]+

2
D
(
R

1
)
0
;
0
=
(
S

1
)
0
;
0
=
1


4
E
[
˙
(1

˙
)]
(1+
E
[
˙
(1

˙
)
x
1
]
2
D
)+

Forsmallenough

,
(
R

1
)
0
;
0
=
(
S

1
)
0
;
0
<
1
2
Err
(1+
E
[
˙
(1

˙
)
x
1
]
2
D
)
Combiningtheboundsonthetwoblocksofthematrices,wegetthat
1
2
Err
(1+
E
[
˙
(1

˙
)
x
1
]
2
D
)(
S

1
)

1
˜
(
R

1
)

1
1
2
Err
(1+
E
[
˙
(1

˙
)
x
1
]
2
D
)
COV
active;

1
˜
COV
passive;

1
Sofor
<
0
,
DE
(

)
<
1
2
Err
(1+
E
[
˙
(1

˙
)
x
1
]
2
D
)
ifwe
e
X
suchthat
p
e
X
(
x
)
/
˙
(1

˙
)
p
x
1
(
x
)
,
DE
(

)
<
1
2
Err
(1+
E
[
e
X
]
2
Var
(
e
X
)
)
Theorem
4.2
.
If
p
(
x
)=
p
(
x
1
)
p
(
x

1
)
and
p
(
x
1
)=
p
(

x
1
)
,thenforsufsmallconstant

(thatdependsonthe
dataset),andforErr
<<
0
,
1
4
Err
<DE
(

)
<
1
2
Err
Proof.
If
p
(
x
1
)=
p
(

x
1
)
,then
p
(
e
X
)=
p
(

e
X
)
andso
E
[
e
X
]=0
.
UsingLemma
6.8
,wearriveattheconclusion.
"
56,Stochastic WaveNet: A Generative Latent Variable Model for Sequential Data,http://arxiv.org/pdf/1806.06116v1.pdf,https://github.com/laiguokun/SWaveNet,"StochasticWaveNet:AGenerativeLatentVariableModelforSequentialData
GuokunLai
1
BohanLi
1
GuoqingZheng
1
YimingYang
1
Abstract
Howtomodeldistributionofsequentialdata,in-
cludingbutnotlimitedtospeechandhumanmo-
tions,isanimportantongoingresearchproblem.
Ithasbeendemonstratedthatmodelcapacitycan
beenhancedbyintroducingstochas-
ticlatentvariablesinthehiddenstatesofrecur-
rentneuralnetworks.Simultaneously,WaveNet,
equippedwithdilatedconvolutions,achievesas-
tonishingempiricalperformanceinnaturalspeech
generationtask.Inthispaper,wecombinethe
ideasfrombothstochasticlatentvariablesand
dilatedconvolutions,andproposeanewarchitec-
turetomodelsequentialdata,termedasStochas-
ticWaveNet,wherestochasticlatentvariablesare
injectedintotheWaveNetstructure.Weargue
thatStochasticWaveNetenjoyspowerfuldistri-
butionmodelingcapacityandtheadvantageof
paralleltrainingfromdilatedconvolutions.Inor-
dertoefcientlyinfertheposteriordistribution
ofthelatentvariables,anovelinferencenetwork
structureisdesignedbasedonthecharacteristics
ofWaveNetarchitecture.State-of-the-artperfor-
mancesonbenchmarkdatasetsareobtainedby
StochasticWaveNetonnaturalspeechmodeling
andhighqualityhumanhandwritingsamplescan
begeneratedaswell.
1.Introduction
Learningtocapturecomplexdistributionofsequentialdata
isanimportantmachinelearningproblemandhasbeen
extensivelystudiedinrecentyears.Theautoregressiveneu-
ralnetworkmodels,includingRecurrentNeuralNetwork
(
HochreiterandSchmidhuber
,
1997
;
Chungetal.
,
2014
),
PixelCNN(
Oordetal.
,
2016
)andWaveNet(
VanDenOord
etal.
,
2016
),haveshownstrongempiricalperformancein
modelingnaturallanguage,imagesandhumanspeeches.
Allthesemethodsareaimedatlearningadeterministic
1
LanguageTechnologyInstitute,CarnegieMellonUniversity,
Pittsburgh,PA15213,USA.Correspondenceto:GuokunLai
<guokun@cs.cmu.edu>.
mappingfromthedatainputtotheoutput.Recently,evi-
dencehasbeenfound(
FabiusandvanAmersfoort
,
2014
;
Ganetal.
,
2015
;
Guetal.
,
2015
;
Goyaletal.
,
2017
;
Sha-
banianetal.
,
2017
)thatprobabilisticmodelingwithneural
networkscanfromuncertaintyintroducedtotheir
hiddenstates,namelyincludingstochasticlatentvariables
inthenetworkarchitecture.Withoutsuchuncertaintyinthe
hiddenstates,RNN,PixelCNNandWaveNetwouldparam-
eterizetherandomnessonlyinthelayerbyshaping
aoutputdistributionfromthedistributionfamily.
Hencetheoutputdistribution(whichisoftenassumedto
beGaussianforcontinuousdata)wouldbeunimodalorthe
mixtureofunimodalsgiventheinputdata,whichmaybe
insuftocapturethecomplextruedatadistribution
andtodescribethecomplexcorrelationsamongdifferent
outputdimensions(
Boulanger-Lewandowskietal.
,
2012
).
Evenforthenon-parametrizeddiscreteoutputdistribution
modeledbythesoftmaxfunction,aphenomenonreferred
toassoftmaxbottleneck(
Yangetal.
,
2017a
)stilllimitsthe
familyofoutputdistributions.Byinjectingthestochastic
latentvariablesintothehiddenstatesandtransformingtheir
uncertaintytooutputsbynon-linearlayers,thestochastic
neuralnetworkisequippedwiththeabilitytomodelthe
datawithamuchricherfamilyofdistributions.
Motivatedbythis,numerousvariantsofRNN-basedstochas-
ticneuralnetworkhavebeenproposed.STORN(
Bayerand
Osendorfer
,
2014
)isthetointegratestochasticlatent
variablesintoRNN'shiddenstates.InVRNN(
Chungetal.
,
2015
),thepriorofstochasticlatentvariablesisassumed
tobeafunctionoverhistoricaldataandstochasticlatent
variables,whichallowsthemtocapturetemporaldependen-
cies.SRNN(
Fraccaroetal.
,
2016
)andZ-forcing(
Goyal
etal.
,
2017
)offermorepowerfulversionswithaugmented
inferencenetworkswhichbettercapturethecorrelationbe-
tweenthestochasticlatentvariablesandthewholeobserved
sequence.ometrainingtricksintroducingin(
Goyaletal.
,
2017
;
Shabanianetal.
,
2017
)wouldeasetrainingprocess
forthestochasticrecurrentneuralnetworkswhichleadto
betterempiricalperformance.Byintroducingstochastic-
itytothehiddenstates,theseRNN-basedmodelsachieved
improvementsovervanillaRNNmodelsonlog-
likelihoodevaluationsonmultiplebenchmarkdatasetsfrom
variousdomains(
Goyaletal.
,
2017
;
Shabanianetal.
,
2017
).
InparallelwithRNN,WaveNet(
VanDenOordetal.
,
2016
)
arXiv:1806.06116v1  [cs.LG]  15 Jun 2018StochasticWaveNet:AGenerativeLatentVariableModelforSequentialData
providesanotherpowerfulwayofmodelingsequentialdata
withdilatedconvolutions,especiallyinthenaturalspeech
generationtask.WhileRNN-basedmodelsmustbetrained
inasequentialmanner,trainingaWaveNetcanbeeasily
parallelized.Furthermore,theparallelWaveNetproposed
in(
Oordetal.
,
2017
)isabletogeneratenewsequencesin
parallel.WaveNet,ordilatedconvolutions,hasalsobeen
adoptedastheencoderordecoderintheVAEframework
andproducesreasonableresultsinthetext(
Semeniutaetal.
,
2017
;
Yangetal.
,
2017b
)andmusic(
Engeletal.
,
2017
)
generationtask.
Inlightoftheadvantageofintroducingstochasticlatent
variablestoRNN-basedmodels,itisnaturaltoraiseaprob-
lemwhetherthiscarriestoWaveNet-basedmodels.
Tothisend,inthispaperweproposeStochasticWaveNet,
whichassociatesstochasticlatentvariableswitheveryhid-
denstatesintheWaveNetarchitecture.Comparedwiththe
vanillaWaveNet,StochasticWaveNetisabletocapturea
richerfamilyofdatadistributionsviatheaddedstochastic
latentvariables.Italsoinheritstheeaseofparalleltrain-
ingwithdilatedconvolutionsfromtheWaveNetarchitec-
ture.Becauseoftheaddedstochasticlatentvariables,an
inferencenetworkisalsodesignedandtrainedjointlywith
StochasticWaveNettomaximizethedatalog-likelihood.
Webelievethataftermodeltraining,themulti-layerstruc-
tureoflatentvariablesleadsthemtobothhierarchical
andsequentialstructuresofthedata.Thishypothesisisval-
idatedempiricallybycontrollingthenumberoflayersof
stochasticlatentvariables.
Therestofthispaperisorganizedasfollows:we
reviewthebackgroundinSection
2
.Theproposedmodel
andoptimizationalgorithmareintroducedinSection
3
.We
evaluateandanalyzetheproposedmodelonmultiplebench-
markdatasetsinSection
4
.Finally,thesummaryofthis
paperisincludedinSection
5
.
2.Preliminary
2.1.Notation
Wethemathematicalsymbolsusedintherestof
thispaper.Wedenoteasetofvectorsbyaboldsymbol,such
as
x
,whichmayutilizeoneortwodimensionsubscripts
asindex,suchas
x
i
or
x
i;j
.
f
(

)
representsthegeneral
functionthattransformsaninputvectortoaoutputvector.
And
f

(

)
isaneuralnetworkfunctionparametrizedby

.
Forasequentialdatasample
x
,
T
representsitslength.
2.2.AutoregressiveNeuralNetwork
Autoregressivenetworkmodelisdesignedtomodelthe
jointdistributionofthehigh-dimensionaldatawithsequen-
tialstructure,byfactorizingthejointdistributionofadata
sampleas
p
(
x
)=
T
Y
t
=1
p

(
x
t
j
x
<t
)
(1)
where
x
=
f
x
1
;x
2
;

x
T
g
;x
t
2
R
d
,
t
indexesthetem-
poraltimestamps,and

representsthemodelparameters.
Thentheautoregressivemodelcancomputethelikelihood
ofasampleandgenerateanewdatasampleinasequential
manner.
Inordertocapturericherstochasticitiesofthesequential
generationprocess,stochasticlatentvariablesforeachtime
stamphavebeenintroduced,referredtoasstochasticneural
network(
Chungetal.
,
2015
;
Fraccaroetal.
,
2016
;
Goyal
etal.
,
2017
).Thenthejointdistributionofthedatatogether
withthelatentvariablesisfactorizedas,
p
(
x
;
z
)=
T
Y
t
=1
p

(
x
t
;z
t
j
x
<t
;z
<t
)
=
T
Y
t
=1
p

(
x
t
j
x
<t
;z

t
)
p

(
z
t
j
x
<t
;z
<t
)
(2)
where
z
=
f
z
1
;z
2
;

z
T
g
;z
t
2
R
d
0
hasthesamesequence
lengthasthedatasample,
d
0
isitsdimensionforonetime
stamp.
z
isalsogeneratedsequentially,namelythepriorof
z
t
isconditionalprobabilitygiven
x
<t
and
z
<t
.
2.3.WaveNet
WaveNet(
VanDenOordetal.
,
2016
)isaconvolutional
autoregressiveneuralnetworkwhichadoptsdilatedcausal
convolutions(
YuandKoltun
,
2015
)toextractthesequen-
tialdependencyinthedatadistribution.Differentfrom
recurrentneuralnetwork,dilatedconvolutionlayerscan
becomputedinparallelduringthetrainingprocess,which
makesWaveNetmuchfasterthanRNNinmodelingsequen-
tialdata.AtypicalWaveNetstructureisvisualizedinFigure
1
.Besidethecomputationadvantage,WaveNethasshown
thestart-of-the-artresultinspeechgenerationtask(
Oord
etal.
,
2017
).
Figure1.
VisualizationofaWaveNetstructurefrom(
Van
DenOordetal.
,
2016
)
StochasticWaveNet:AGenerativeLatentVariableModelforSequentialData
3.StochasticWaveNet
Inthissection,weintroduceasequentialgenerativemodel
(StochasticWaveNet),whichimposesstochasticlatentvari-
ableswiththemulti-layerdilatedconvolutionstructure.
WeintroducethegenerationprocessofStochas-
ticWaveNet,andthendescribethevariationalinference
method.
3.1.GenerativeModel
Similarasstochasticrecurrentneuralnetworks,weinject
thestochasticlatentvariableineachWaveNethiddennode
inthegenerationprocess,whichisillustratedinFigure
2a
.More,forasequentialdatasample
x
with
length
T
,weintroduceasetofstochasticlatentvariables
z
=
f
z
t;l
j
1

t

T;
1

l

L
g
;z
t;l
2
R
d
0
,where
L
is
thenumberofthelayersofWaveNetarchitecture.Thenthe
generationprocesscanbedescribedas,
p
(
x
;
z
)=
T
Y
t
=1
p

(
x
t
;z
t;
1:
L
j
x
<t
;z
<t;
1:
L
)
=
T
Y
t
=1
[
p

(
x
t
j
z

t;
1:
L
;x
<t
)
L
Y
l
=1
p

(
z
t;l
j
z
t;<l
;x
<t
;z
<t;
1:
L
)]
(3)
Thegenerationprocesscanbeinterpretedasthis.Ateach
timestamp
t
,wesamplethestochasticlatentvariables
z
t;l
fromapriordistributionwhichareconditionedonthelower
levellatentvariablesandhistoricalrecordsincludingthe
datasamples
x
<t
andlatentvariables
z
<t
.Thenwesample
thenewdatasample
x
t
accordingtoallsampledlatent
variablesandhistoricalrecords.Throughthisprocess,new
sequentialdatasamplesaregeneratedinarecursiveway.
InStochasticWaveNet,thepriordistribution
p

(
z
t;l
j
x
<t
;z
t;<l
;z
<t;
1:
L
)=
N
(
z
t;l
;

t;l
;v
t;l
)
isde-
asaGaussiandistributionwiththediagonal
covariancematrix.Thesequentialandhierarchical
dependencyamongthelatentvariablesaremodeledbythe
WaveNetarchitecture.Inordertosummarizeallhistorical
information,weintroducetwostochastichiddenvariables
h
and
d
,whicharecalculatedas,
h
t;l
=
f

1
(
d
t

2
l
;l

1
;d
t;l

1
)
d
t;l
=
f

2
(
h
t;l
;z
t;l
)
(4)
Where
f

1
mimicsthedesignofthedilatedconvolution
inWaveNet,and
f

2
isafullyconnectedlayertosum-
marizethehiddenstatesandthesampledlatentvariable.
DifferentfromthevanillaWaveNet,thehiddenstates
h
are
stochastic
becauseoftherandomsamples
z
.Wepa-
rameterizethemeanandvarianceofthepriordistributions
bythehiddenrepresentations
h
,whichis

t;l
=
f

3
(
h
t;l
)
and
log
v
t;l
=
f

4
(
h
t;l
)
.Similarly,weparameterizethe
emissionprobability
p

(
x
t
j
x
<t
;z
t;
1:
L
;z
<t;
1:
L
)
asaneural
networkfunctionoverthehiddenrepresentations.
3.2.VariationalInferenceforStochasticWaveNet
Insteadofdirectlymaximizinglog-likelihoodforasequen-
tialsample
x
,weoptimizeitsvariationalevidencelower
bound(ELBO)(
Jordanetal.
,
1999
).Exactposteriorinfer-
enceofthestochasticvariables
z
ofStochasticWaveNet
isintractable.Hence,wedescribeavariationalinference
methodforStochasticWaveNetbyutilizingthereparame-
terizationtrickintroducedin(
KingmaandWelling
,
2013
).
Firstly,wewritetheELBOas,
log
p
(
x
)

Z
q
˚
(
z
j
x
)log
p

(
x
;
z
)
q
˚
(
z
j
x
)
dz
=
E
q
˚
(
z
j
x
)
[
T
X
t
=1
log
p

(
x
t
j
z

t;
1:
L
;x
<t
)]

D
KL
(
q
˚
(
z
j
x
)
jj
p

(
z
j
x
))
=
L
(
x
)
wherep

(
z
j
x
)=
T
Y
t
=1
L
Y
l
=1
p

(
z
t;l
j
z
t;<l
;x
<t
;z
<t;
1:
L
)
(5)
WecanderivethesecondequationbytakingEq.
3
intothe
equation,and
L
(
x
)
denotesthelossfunctionforthe
sample
x
.Hereanotherproblemneedstobeaddressedis
howtotheposteriordistribution
q
˚
(
z
j
x
)
.Inorderto
maximizetheELBO,wefactorizetheposterioras,
q
˚
(
z
j
x
)=
T
Y
t
=1
L
Y
l
=1
q
˚
(
z
t;l
j
x
;z
t;<l
;z
<t;
1:
L
)
(6)
Heretheposteriordistributionfor
z
t;l
isconditionedonthe
stochasticlatentvariablessampledbeforeitandtheentire
observeddata
x
.Byutilizingthefuturedata
x

t
,wecan
bettermaximizethetermin
L
(
x
)
,thereconstruction
lossterm.Inopposite,thepriordistributionof
z
t;l
isonly
conditionedon
x
<t
,soencoding
x

t
informationmayin-
creasethedegreeofdistributionmismatchbetweentheprior
andposteriordistribution,namelyenlargingtheKLtermin
lossfunction.
ExploringthedependencystructureinWaveNet.
How-
ever,byanalyzingthedependencyamongtheoutputsand
hiddenstatesofWaveNet,wewouldthatthestochastic
latentvariablesattimestampt,
z
t;
1:
L
wouldnot
StochasticWaveNet:AGenerativeLatentVariableModelforSequentialData
wholeposterioroutputs.Sotheinferencenetworkwould
onlyrequirepartialposterioroutputstomaximizethere-
constructionlossterminthelossfunction.Denotetheset
ofoutputsthatwouldbeby
z
l;t
as
s
(
l;t
)
.The
posteriordistributioncanbeas,
q
˚
(
z
j
x
)=
T
Y
t
=1
L
Y
l
=1
q
˚
(
z
t;l
j
x
<t
;x
s
(
l;t
)
;z
t;<l
;z
<t;
1:
L
)
(7)
Theposteriordistributionremovesunnecessary
conditionalvariables,whichmakestheoptimizationmore
efcient.Tosummarizetheinformationfromposterior
outputs
x
s
(
l;t
)
,wedesignareversedWaveNetarchitecture
tocomputethehiddenfeature
b
,illustratedinFigure
2b
,
and
b
t;l
isformulatedas,
b
t;l
=
f
(
x
s
(
t;l
)
)=
f
˚
1
(
b
t;l
+1
;b
t
+2
l
+1
;l
+1
)
(8)
wherewedethat
b
t;L
=
f
˚
2
(
x
t
;x
t
+2
L
+1
)
,and
f
˚
1
and
f
˚
2
isthedilatedconvolutionlayer,whosestructure
isareverseversionof
f

1
inEq.
3
.Finally,weinfer-
encetheposteriordistribution
q
˚
(
z
t;l
j
x
;z
t;<l
;z
<t;
1:
L
)=
N
(
z
t;l
;

0
t;l
;v
0
t;l
)
by
b
and
h
,whichis

0
t;l
=
f
˚
3
(
h
t;l
;b
t;l
)
and
log
v
0
t;l
=
f
˚
4
(
h
t;l
;b
t;l
)
.Here,wereusethestochastic
hiddenstates
h
inthegenerativemodelinordertocompress
thenumberofthemodelparameters.
KLAnnealingTrick.
Itiswellknownthatthedeepneural
networkswithmulti-layersstochasticlatentvariablesisdif-
totrain,ofwhichoneimportantreasonisthattheKL
terminthelossfunctionlimitedthecapacityofthestochas-
ticlatentvariabletocompressthedatainformationinearly
stagesoftraining.TheKLAnnealingisacommontrickto
alleviatethisissue.Theobjectivefunctionisas,
L

(
x
)=
E
q
˚
(
z
j
x
)
[
T
X
t
=1
log
p

(
x
j
z
)]


KL
(
q
˚
(
z
j
x
)
jj
p

(
z
j
x
))
(9)
Duringthetrainingprocess,the

isannealedfrom0to
1.Inpreviousworks,researchersusuallyadoptthelinear
annealingstrategy(
Fraccaroetal.
,
2016
;
Goyaletal.
,
2017
).
Inourexperiment,wethatitstillincreases

toofastfor
StochasticWaveNet.Weproposetousecosineannealing
strategyalternatively,namelythe

isfollowingthefunction


=1

cos
(

)
,where

scansfrom
0
to
ˇ
2
.
4.Experiment
Inthissection,weevaluatetheproposedStochastic
WaveNetonseveralbenchmarkdatasetsfromvariousdo-
mains,includingnaturalspeech,humanhandwritingand
(a)GenerativeModel
(b)InferenceModel
Figure2.
ThisillustratesatoysampleofStochastic
WaveNet,whichhastwolayers.Theleftoneisthegenerative
model,andtherightoneistheinferencemodel.Boththesolid
lineanddashlinerepresenttheneuralnetworkfunctions.The
h
intherightisidenticaltotheoneintheleft.The
z
inthe
generativemodelaresampledfrompriordistributions,andthe
onesintheinferencemodelarefromposterior.
humanmotionmodelingtasks.WeshowthatStochastic
WaveNet,orSWaveNetinshort,achievesstate-of-the-art
results,andvisualizesthegeneratedsamplesforthehuman
handwritingdomain.Theexperimentcodesarepublicly
accessible.
1
Baselines
.Thefollowingsequentialgenerativemodelspro-
posedinrecentyearsaretreatedasthebaselinemethods:

RNN
:Theoriginalrecurrentneuralnetworkwiththe
LSTMcell.

VRNN
:Thegenerativemodelwiththerecurrentstruc-
tureproposedin(
Chungetal.
,
2015
).Itfor-
mulatesthepriordistributionof
z
t
asaconditional
probabilitygivenhistoricaldata
x
<t
andlatentvari-
ables
z
<t
.

SRNN
:Proposedin(
Fraccaroetal.
,
2016
),anditaug-
mentstheinferencenetworkbyabackwardRNNto
1
https://github.com/laiguokun/SWaveNet
StochasticWaveNet:AGenerativeLatentVariableModelforSequentialData
betteroptimizetheELBO.

Z-forcing
:Proposedin(
Goyaletal.
,
2017
),whosear-
chitectureissimilartoSRNN,anditeasesthetraining
ofthestochasticlatentvariablesbyaddingauxiliary
costwhichforcesmodeltousestochasticlatentvari-
ables
z
toreconstructthefuturedata
x
>t
.

WaveNet
:Proposedin(
VanDenOordetal.
,
2016
)and
producestate-of-the-artresultinthespeechgeneration
task.
Weevaluatedifferentmodelsbycomparingthelog-
likelihoodonthetestset(RNN,WaveNet)oritslower
bound(VRNN,SRNN,Z-forcingandourmethod).For
faircomparison,amultivariateGaussiandistributionwith
thediagonalcovariancematrixisusedastheoutputdistri-
butionforeachtimestampinallexperiments.TheAdam
optimizer(
KingmaandBa
,
2014
)isusedforallmodels,
andthelearningrateisscheduledbythecosineannealing.
Followingtheexperimentsettingin(
Fraccaroetal.
,
2016
),
weuse1sampletoapproximatethevariationalevidence
lowerboundtoreducethecomputationcost.
4.1.NaturalSpeechModeling
Inthenaturalspeechmodelingtask,wetrainthemodel
tothelog-likelihoodfunctionoftherawaudiosignals,
followingtheexperimentsettingin(
Fraccaroetal.
,
2016
;
Goyaletal.
,
2017
).Therawsignals,whichcorrespondto
thereal-valuedamplitudes,arerepresentedasasequence
of200-dimensionalframes.Eachframeis200consecutive
samples.Thepreprocessinganddatasetsegmentationare
identicalto(
Fraccaroetal.
,
2016
;
Goyaletal.
,
2017
).We
evaluatetheproposedmodelinthefollowingbenchmark
datasets:

Blizzard(
Prahalladetal.
,
2013
):TheBlizzardChal-
lenge2013,whichisatext-to-speechdatasetcontain-
ing300hoursofEnglishfromasinglefemalespeaker.

TIMIT
2
:TIMITrawaudiodatasets,whichcontains
6,300Englishsentence,readby630speakers.
ForBlizzarddatasets,wereporttheaveragelog-likelihood
overthehalf-secondsegmentsofthetestset.ForTIMIT
datasets,wereporttheaveragelog-likelihoodovereach
sequenceofthetestset,whichisfollowingthesettingin
(
Fraccaroetal.
,
2016
;
Goyaletal.
,
2017
).Inthistask,we
use5-layerSWaveNetarchitecturewith1024hiddendimen-
sionsforBlizzardand512forTIMIT.Andthedimensions
ofthestochasticlatentvariablesare100forbothdatasets.
TheexperimentresultsareillustratedinTable
1
.Thepro-
posedmodelhasproducedthebestresultforbothdatasets.
2
https://catalog.ldc.upenn.edu/ldc93s1
MethodBlizzardTIMIT
RNN741326643
VRNN

9392

28982
SRNN

11991

60550
Z-forcing(+kla)

14226

68903
Z-forcing(+kla,aux)*

15024

70469
WaveNet-577726074
SWaveNet

15708

72463
(

274)(

639)
Table1.
TestsetLog-likelihoodsonthenaturalspeechmodeling
task.ThegroupisallRNN-basedmodels,whilethesecond
groupisWaveNet-basedmodels.Bestresultsarehighlightedin
bold.

denotesthatthetrainingobjectiveisequippedwithan
auxiliarytermwhichothermethodsdon'thave.ForSWaveNet,we
reportthemean(

standarddeviation)producedby10different
runs.
Sincetheperformancegapisnotenough,we
alsoreportthevarianceoftheproposedmodelperfor-
mancebyrerunningthemodelwith10randomseeds,which
showstheconsistenceperformance.Comparedwiththe
WaveNetmodel,theonewithoutstochastichiddenstates,
SWaveNetgetsaperformanceboost.Simultane-
ously,SWaveNetstillenjoystheadvantageoftheparallel
trainingcomparedwithRNN-basedstochasticmodels.One
commonconcernaboutSWaveNetisthatitmayrequire
largerhiddendimensionofthestochasticlatentvariables
thanRNNbasedmodelduetoitsmulti-layerstructure.How-
ever,thetotaldimensionofstochasticlatentvariablesfor
onetimestampofSWaveNetis500,whichistwiceasthe
numberintheSRNNandtheZ-forcingpapers(
Fraccaro
etal.
,
2016
;
Goyaletal.
,
2017
).Wewillfurtherdiscussthe
relationshipbetweenthenumberofstochasticlayersand
themodelperformanceinsection
4.3
.
4.2.HandwritingandHumanMotionGeneration
Next,weevaluatetheproposedmodelbyvisualizinggen-
eratedsamplesfromthetrainedmodel.Thedomainwe
chooseishumanhandwriting,whosewritingtracksarede-
scribedbyasequentialsamplepoints.Thefollowingdataset
isusedtotrainthegenerativemodel:
IAM-OnDB
(
LiwickiandBunke
,
2005
):Thehumanhand-
writingdatasetscontains13,040handwritinglineswritten
by500writers.Thewritingtrajectoriesarerepresentedasa
sequenceof3-dimensionframes.Eachframeiscomposed
oftworeal-valuenumbers,whichis
(
x;y
)
coordinatefor
thissamplepoint,andabinarynumberindicatingwhether
thepenistouchingthepaper.Thedatapreprocessingand
divisionaresameas(
Graves
,
2013
;
Chungetal.
,
2015
).
ThequantitativeresultsarereportedinTable
2
.SWaveNet
achievessimilarresultcomparedwiththebestone,and
StochasticWaveNet:AGenerativeLatentVariableModelforSequentialData
(a)GroundTruth(b)RNN(c)VRNN(d)SWaveNet
Figure3.
Generatedhandwritingsample:(a)arethesamplesfromthegroundtruthdata.(b)(c),(d)arefromRNN,VRNNandSWaveNet,
respectively.Eachlineisonehandwritingsample.
MethodIAM-OnDB
RNN(
Chungetal.
,
2015
)1016
VRNN(
Chungetal.
,
2015
)

1334
WaveNet1021
SWaveNet

1301
Table2.
Log-likelihoodresultsonIAM-OnDBdataset.Thebest
resultarehighlightedinbold.
stillshowsimprovementtothevanillaWaveNet
architecture.InFigure
3
,weplotthegroundtruthsamples
andtheonesrandomlygeneratedfromdifferentmodels.
ComparedwithRNNandVRNN,SWaveNetshowsclearer
result.Itiseasytodistinguishtheboundaryofthecharacters,
andwecanobversethatmoreofthemaresimilartothe
English-characters,suchasﬁisﬂinthefourthlineandﬁherﬂ
inthelastline.
4.3.ofStochasticLatentVariables
ThemostprominentdistinctionbetweenSWaveNetand
RNN-basedstochasticneuralnetworksisthatSWaveNet
utilizesthedilatedconvolutionlayerstomodelmulti-layer
stochasticlatentvariablesratherthanonelayerlatentvari-
ablesintheRNNmodels.Here,weperformtheempirical
studyaboutthenumberofstochasticlayersinSWaveNet
modeltodemonstratetheefyofthedesignofmulti-
layersstochasticlatentvariables.Theexperimentisde-
signedasfollows.Firstly,weretainthetotalnumberof
layersandonlychangethenumberofstochasticlayers,
namelythelayercontainsstochasticlatentvariables.More
,ForaSWaveNetwith
L
layersand
S
stochastic
layers,
(
S

L
)
,weeliminatethestochasticlatentvariables
inthebottompart,whichis
f
z
1:
T;l
;
l<L

S
g
inEq.
4
.
Thenforeachtimestamp,whenthemodelhas
D
dimension
stochasticvariablesintotal,eachlayerwouldhave
b
D
S
c
dimensionstochasticvariables.Inthisexperiment,weset
D
=500
.
WeplottheexperimentresultsinFigure
4
.Fromtheplots,
(a)Blizzard
(b)TIMIT
(c)IAM-OnDB
Figure4.
Theofthenumberofstochasticlayersof
SWaveNet.
wethatSWaveNetcanachievebetterperformancewith
multiplestochasticlayers.Thisdemonstratesthatitishelp-
fultoencodethestochasticlatentvariableswithahier-
achicalstructure.AndintheexperimentonBlizzardand
IAM-OnDB,weobservethattheperformancewilldecrease
whenthenumberofstochasticlayersislargeenough.Be-
causetoolargenumberofstochasticlayerswouldresultin
toosmallnumberoflatentvariablesforalayertomemorize
valuableinformationindifferenthierarchylevels.
Wealsostudyhowthemodelperformancewouldbe
encedbythenumberofstochasticlatentvariables.Similar
topreviousone,weonlytunethetotalnumberofstochastic
latentvariablesandkeeprestsettingsunchanged,whichis
4stochasticlayers.TheresultsareplottedinFigure
5
.They
demonstratethatStochasticWaveNetwouldbe
fromevenasmallnumberofstochasticlatentvariables.
5.Conclusion
Inthispaper,wepresentanovelgenerativelatentvariable
modelforsequentialdata,namedasStochasticWaveNet,
whichinjectsstochasticlatentvariablesintothehiddenstate
ofWaveNet.Anewinferencenetworkstructureisdesigned
StochasticWaveNet:AGenerativeLatentVariableModelforSequentialData
(a)Blizzard
(b)TIMIT
(c)IAM-OnDB
Figure5.
Theofthenumberofstochasticlatentvariables
ofSWaveNet.
basedonthecharacteristicofWaveNetarchitecture.Empir-
icallyresultsshowstate-of-the-artperformancesonvarious
domainsbyleveragingadditionalstochasticlatentvariables.
Simultaneously,thetrainingprocessofWaveNetisgreatly
acceleratedbyparallellcomputationcomparedwithRNN-
basedmodels.Forfuturework,apotentialresearchdi-
rectionistoadopttheadvancedtrainingstrategies(
Goyal
etal.
,
2017
;
Shabanianetal.
,
2017
)designedforsequential
stochasticneuralnetworks,toStochasticWaveNet.
References
Bayer,J.andOsendorfer,C.(2014).Learningstochastic
recurrentnetworks.
arXivpreprintarXiv:1411.7610
.
Boulanger-Lewandowski,N.,Bengio,Y.,andVincent,
P.(2012).Modelingtemporaldependenciesinhigh-
dimensionalsequences:Applicationtopolyphonic
musicgenerationandtranscription.
arXivpreprint
arXiv:1206.6392
.
Chung,J.,Gulcehre,C.,Cho,K.,andBengio,Y.(2014).
Empiricalevaluationofgatedrecurrentneuralnetworks
onsequencemodeling.
arXivpreprintarXiv:1412.3555
.
Chung,J.,Kastner,K.,Dinh,L.,Goel,K.,Courville,A.C.,
andBengio,Y.(2015).Arecurrentlatentvariablemodel
forsequentialdata.In
Advancesinneuralinformation
processingsystems
,pages2980Œ2988.
Engel,J.,Resnick,C.,Roberts,A.,Dieleman,S.,Eck,D.,
Simonyan,K.,andNorouzi,M.(2017).Neuralaudio
synthesisofmusicalnoteswithwavenetautoencoders.
arXivpreprintarXiv:1704.01279
.
Fabius,O.andvanAmersfoort,J.R.(2014).Variational
recurrentauto-encoders.
arXivpreprintarXiv:1412.6581
.
Fraccaro,M.,Sønderby,S.K.,Paquet,U.,andWinther,O.
(2016).Sequentialneuralmodelswithstochasticlayers.
In
Advancesinneuralinformationprocessingsystems
,
pages2199Œ2207.
Gan,Z.,Li,C.,Henao,R.,Carlson,D.E.,andCarin,L.
(2015).Deeptemporalsigmoidbeliefnetworksforse-
quencemodeling.In
AdvancesinNeuralInformation
ProcessingSystems
,pages2467Œ2475.
Goyal,A.,Sordoni,A.,Côté,M.-A.,Ke,N.R.,andBen-
gio,Y.(2017).Z-forcing:Trainingstochasticrecurrent
networks.In
AdvancesinNeuralInformationProcessing
Systems
,pages6716Œ6726.
Graves,A.(2013).Generatingsequenceswithrecurrent
neuralnetworks.
arXivpreprintarXiv:1308.0850
.
Gu,S.,Ghahramani,Z.,andTurner,R.E.(2015).Neural
adaptivesequentialmontecarlo.In
AdvancesinNeural
InformationProcessingSystems
,pages2629Œ2637.
Hochreiter,S.andSchmidhuber,J.(1997).Longshort-term
memory.
Neuralcomputation
,9(8):1735Œ1780.
Jordan,M.I.,Ghahramani,Z.,Jaakkola,T.S.,andSaul,
L.K.(1999).Anintroductiontovariationalmethodsfor
graphicalmodels.
Machinelearning
,37(2):183Œ233.
Kingma,D.P.andBa,J.(2014).Adam:Amethodfor
stochasticoptimization.
arXivpreprintarXiv:1412.6980
.
Kingma,D.P.andWelling,M.(2013).Auto-encoding
variationalbayes.
arXivpreprintarXiv:1312.6114
.
Liwicki,M.andBunke,H.(2005).Iam-ondb-anon-line
englishsentencedatabaseacquiredfromhandwrittentext
onawhiteboard.In
DocumentAnalysisandRecognition,
2005.Proceedings.EighthInternationalConferenceon
,
pages956Œ961.IEEE.
Oord,A.v.d.,Kalchbrenner,N.,andKavukcuoglu,K.
(2016).Pixelrecurrentneuralnetworks.
arXivpreprint
arXiv:1601.06759
.
Oord,A.v.d.,Li,Y.,Babuschkin,I.,Simonyan,K.,Vinyals,
O.,Kavukcuoglu,K.,Driessche,G.v.d.,Lockhart,
E.,Cobo,L.C.,Stimberg,F.,etal.(2017).Paral-
lelwavenet:Fastspeechsynthesis.
arXiv
preprintarXiv:1711.10433
.
Prahallad,K.,Vadapalli,A.,Elluru,N.,Mantena,G.,Pu-
lugundla,B.,Bhaskararao,P.,Murthy,H.,King,S.,
Karaiskos,V.,andBlack,A.(2013).Theblizzardchal-
lenge2013Œindianlanguagetask.In
Blizzardchallenge
workshop
,volume2013.
StochasticWaveNet:AGenerativeLatentVariableModelforSequentialData
Semeniuta,S.,Severyn,A.,andBarth,E.(2017).Ahybrid
convolutionalvariationalautoencoderfortextgeneration.
arXivpreprintarXiv:1702.02390
.
Shabanian,S.,Arpit,D.,Trischler,A.,andBengio,
Y.(2017).Variationalbi-lstms.
arXivpreprint
arXiv:1711.05717
.
VanDenOord,A.,Dieleman,S.,Zen,H.,Simonyan,K.,
Vinyals,O.,Graves,A.,Kalchbrenner,N.,Senior,A.,and
Kavukcuoglu,K.(2016).Wavenet:Agenerativemodel
forrawaudio.
arXivpreprintarXiv:1609.03499
.
Yang,Z.,Dai,Z.,Salakhutdinov,R.,andCohen,W.W.
(2017a).Breakingthesoftmaxbottleneck:ahigh-rank
rnnlanguagemodel.
arXivpreprintarXiv:1711.03953
.
Yang,Z.,Hu,Z.,Salakhutdinov,R.,andBerg-Kirkpatrick,
T.(2017b).Improvedvariationalautoencodersfortext
modelingusingdilatedconvolutions.
arXivpreprint
arXiv:1702.08139
.
Yu,F.andKoltun,V.(2015).Multi-scalecontextag-
gregationbydilatedconvolutions.
arXivpreprint
arXiv:1511.07122
.
"
57,Non-Negative Networks Against Adversarial Attacks,http://arxiv.org/pdf/1806.06108v2.pdf,https://github.com/endgameinc/malware_evasion_competition,"Non-NegativeNetworksAgainstAdversarialAttacks
WilliamFleshman,
1
EdwardRaff,
1,2
JaredSylvester,
1,2
StevenForsyth,
3
MarkMcLean
1
1
LaboratoryforPhysicalSciences,
2
BoozAllenHamilton,
3
Nvidia
{william.fleshman,edraff,jared,mrmclea}@lps.umd.edu
,
sforsyth@nvidia.com
Abstract
Adversarialattacksagainstneuralnetworksareaproblemof
considerableimportance,forwhicheffectivedefensesarenot
yetreadilyavailable.Wemakeprogresstowardthisproblem
byshowingthatnon-negativeweightconstraintscanbeused
toimproveresistanceinscenarios.Inparticular,we
showthattheycanprovideaneffectivedefenseforbinaryclas-
problemswithasymmetriccost,suchasmalwareor
spamdetection.Wealsoshowthepotentialfornon-negativity
tobehelpfultonon-binaryproblemsbyapplyingittoimage

1Introduction
Recently,therehasbeenanincreasedresearcheffortinex-
ploringadversarialexampleswhichfoolmachinelearning
(Goodfellow,Shlens,andSzegedy,2015;Kurakin,
Goodfellow,andBengio,2017a;Szegedyetal
.
,2014;Yuan
etal
.
,2017).Themajorityoftheexistingresearchfocuseson
theimagedomain,whereanexampleisgeneratedbymaking
smallperturbationstoinputpixelsinordertomakealarge
changeinthedistributionofpredictedclassprobabilities.We
areparticularlyinterestedinadversarialattacksfor
malware
detection
,whichisthetaskofdeterminingifaisbenign
ormalicious.Thisinvolvesareal-lifeadversary(themalware
author)whoisattemptingtosubvertdetectiontools,such
asanti-virusprograms.Withmachinelearningapproaches
tomalwaredetectionbecomingmoreprevalent(Raffetal
.
,
2018;Pascanuetal
.
,2015;SaxeandBerlin,2015;Sahsand
Khan,2012),thisisanareathaturgentlyrequiressolutions
totheadversarialproblem.Becauseanadversaryisactively
attemptingtosubvertoutputs,smalldecreasesinaccuracy
whennotunderattackareanacceptablecostforremediating
targetedattacks.Inthisscenario,theeffectiveaccuracyof
thesystemwouldbetheaccuracyunderattack,whichwill
beatornearzerowithoutproperdefenses.
Forexample,Raffetal
.
(2018)trainedaconvolutional
neuralnetworkcalledMalConvtodistinguishbetweenbe-
nignandmaliciousWindowsexecutableWhenworking
withimages,anypixelcanbearbitrarilyaltered,butthis
freedomdoesnotcarryovertothemalwarecase.Theex-
ecutableformatfollowsstricterruleswhichconstrainthe
optionsavailabletotheattacker(Kreuketal
.
,2018;Russu
etal
.
,2016;Grosseetal
.
,2016;Suciu,Coull,andJohns,
2018).Perturbinganarbitrarybyteofanexecutablewill
mostlikelychangethefunctionalityoftheorpreventit
fromexecutingentirely.Thispropertyisusefulfordefending
againstanadversarialattack,asamalwareauthorneedsto
evadedetectionwitha
working
malicious
Kreuketal
.
(2018)wereabletobypasstheselimitations
byapplyinggradient-basedattackstocreateperturbations
whichwererestrictedtobyteslocatedinunusedsections
ofmaliciousexecutableTheadversarialexamplesre-
mainedjustasmalicious,butthewasfooledbythe
introductionofoverwhelminglybenignyetunusedsections
oftheThisispossiblebecausetheadversarycontrolsthe
input,andtheEXEformatallowsunusedsections.Because
ofthecomplicationsandobfuscationsthatareavailableto
malwareauthors,itisnotnecessarilypossibletotellthata
sectionisunused,evenifitscontentsappearrandom.Thisis
an
additiveonly
adversaryŠi.e.,theattackercanonlyadd
featuresŠwhichhasbeenwidelyusedandwillbethefocus
ofourstudy.
Ananalogytotheimagedomainwouldbeanattackerthat
couldcreatenewpixelswhichrepresentthedesiredclassand
putthemoutsideofthecroppingboxoftheimage,suchthat
theywouldbeinthedigitalbutneverbeseenbyahuman
observer.Thiscontrastswithastandardadversarialattackon
images,sincetheattackeristypicallylimitedtochangingthe
valuesofexistingpixelsintheimageratherthanintroducing
newpixelsentirely.
Giventheseuniquecharacteristicsandcosts,wenotethat
themalwarecaseisonewherewecare
only
abouttargeted
adversarialattacks.Theadversaryalwayswantstofoolde-
tectorsintocallingmaliciousbenign.Assuch,wein-
troduceanapproachtotackletargetedadversarialattacksby
exploitingnon-negativelearningconstraints.Wewillhigh-
lightrelatedworkinsection2.Insection3wewilldetailour
motivationfornon-negativelearningformalware,aswellas
howwegeneralizeitsusetomulti-classproblemslikeimage
Theattackscenarioandexperimentsonmalware,
spam,andimagedomainswillbedetailedinsection4.In
section5wewilldemonstratehowourapproachreduces
evasionstoalmost0%formalwareandexactly0%spam
detection.Onimagesweshowimprovementstorobustness
againstadversarialattacksagainstimages,showing
thatthereispotentialfornon-negativitytoaidinnon-binary
problems.Wewillendwithourconclusionsinsection6.
arXiv:1806.06108v2  [stat.ML]  3 Jan 20192RelatedWork
Theissuesoftargetedadversarialbinaryprob-
lems,aswellastheadditiveadversary,wasbrought
upbyDalvietal
.
(2004),whonoteditsimportanceina
numberofdomainslikefrauddetection,counterterrorism,
surveillance,andothers.Therehavebeenseveralattempts
atcreatingmachinelearningwhichcandefend
againstsuchadversarialexamples.Yuanetal
.
(2017)provide
athoroughsurveyofbothattacksanddefenses
fordeeplearningsystems.Someoftheseattackswillbeused
tocomparetherobustnessofourtechniquetopriormethods.
Inourcasewearelearningagainstareallifeadversary
inabinaryclatask,similartotheinitialworkin
thisspaceonevadingspam(LowdandMeek,2005b;
Dalvietal
.
,2004;LowdandMeek,2005a).Ourmalware
casegivesthedefenderaslightcomparativeadvantagein
constrainingtheattacktoproduceaworkingbinary,where
spamauthorscaninsertmorearbitrarycontent.
Priorworkshavelookedatsimilarweightconstraintap-
proachestoadversarialrobustness.KandTeo(2009)uses
atechniquetokeepthedistributionoflearnedweightsasso-
ciatedwithfeaturesasevenaspossibleduringtraining.By
preventinganyonefeaturefrombecomingoverwhelmingly
predictive,theyforcetheadversarytomanipulatemanyfea-
turesinordertocauseaSimilarly,Grosseet
al
.
(2016)testedasuiteoffeaturereductionmethodsspecif-
icallyinthemalwaredomain(Grosseetal
.
,2016).First,
theyusedthemutualinformationbetweenfeaturesandthe
targetclassinordertolimittherepresentationofeach
tothosefeatures.LikeKandTeo,theycreatedanalter-
nativefeatureselectionmethodtolimittrainingtofeatures
whichcarriednearequalimportance.Theyfoundbothof
thesetechniquestobeineffective.
Ourapproachisalsoafeaturereductiontechnique.The
differenceisthatwetrainonallfeatures,butonlyretainthe
capacitytodistinguishareducednumberoffeaturesattest
timeŠnamely,onlythoseindicativeofthepositiveclass.
Trainingonallfeaturesallowsthemodeltoautomatically
determinewhichareimportantforthetargetclassandutilizes
theotherfeaturestoaccuratelysetathreshold,represented
bythebiasterm,fordeterminingwhenarequisitequantityof
featuresarepresentforassigningsamplestothetargetclass.
ChorowskiandZurada(2015)usednon-negativeweight
constraintsinordertotrainmoreinterpretableneuralnet-
works.Theyfoundthattheconstraintscausedtheneuronsto
isolatefeaturesinmeaningfulways.Webuildonthistech-
niqueinordertoisolatefeatureswhilealsopreventingour
modelsfromusingthefeaturespredictiveofthenegative
class.
Goodfellow,Shlens,andSzegedy(2015)usedRBFnet-
workstoshowthatlowcapacitymodelscanberobustto
adversarialperturbationsbutfoundtheylacktheabilityto
generalize.Withourmethodsweweareabletoachieve
generalizationwhilealsoproducinglowpredic-
tionsduringtargetedattacks.
3IsolatingClasseswithNon-Negative
WeightConstraints
Wewillstartbybuildinganintuitiononhowlogisticregres-
sionwithnon-negativeweightconstraintsassignspredictive
powertoonlyfeaturesindicativeofthepositive(
+
)class
whileignoringthoseassociatedwiththenegative(

)class.
Let
C
(

)
beatrainedlogisticregressionbinary
oftheform
C
(
x
)=sign

w
T
x
+
b

,where
w
isthevector
ofnon-negativelearnedcoefof
C
(

)
,
x
isavectorof
booleanfeaturesforagivensample,and
b
isascalarbias.The
decisionboundaryof
C
(

)
existswhere
w
T
x
+
b
=0
,and
because
w
T
x

0
8
x
,thebias
b
mustbestrictlynegativein
orderfor
C
(

)
tohavethecapacitytoassignsamplestoboth
classes.Thedecisionfunctioncanthenberewrittenas:
C
(
x
)=
ˆ
(+)
w
T
x
j
b
j
(

)
w
T
x
<
j
b
j
(1)
Because
w
isnon-negative,thepresenceofanyfeature
x
i
2
x
canonlyincreasetheresultofthedotproduct,thus
pushingthetoward
(+)
.Weightsassociated
tofeaturesthatarepredictiveofclass
(

)
willtherefore
bepushedtoward
0
duringtraining.Whennofeaturesare
present
(
x
=
~
0)
themodeldefaultstoaof
(

)
duetothenegativebias
b
.Unlessasufnumberof
featurespredictiveofclass
(+)
arepresentinthesample,the
decisionwillremainunchanged.Atrainedinthis
waywillusefeaturesindicativeofthe
(

)
classtosetthe
biasterm,butwillnotallowthosefeaturestoparticipatein
attesttime.Thesamelogicfollowsforlogistic
regressionwithnon-booleanfeaturesifthefeaturesarealso
non-negativeorscaledtobenon-negativebeforetraining.
Givenaproblemwithasymmetricgoals,
wecanleveragethisbehaviortobuildadefenseagainstad-
versarialattacks.Formalwaredetection,themalwareauthor
wishestoavoiddetectionasmalware
(+)
,andinsteadinduce
afalsedetectionasbenign
(

)
.However,thereisnodesire
fortheauthorofabenignprogramtomaketheirapplica-
tionsbedetectedasmalicious.Thus,ifwemodelmalware
asthepositiveclasswithnon-negativeweights,
nothingcan
beadded
tothetomakeitseemmorebenigntothe

C
(

)
.Becauseexecutableprogramsmustmaintain
functionality,themalwareauthorcannottriviallyremove
contenttoreducethemaliciousscoreeither.Thisleavesthe
attackerwithnorecoursebuttore-writetheirapplication,or
performmorenon-trivialactssuchaspackingtoobscurein-
formation.Suchobfuscationscanthenberemediatedthrough
existingapproacheslikedynamicanalysis(Ugarte-Pedrero
etal.,2016;Chistyakovetal.,2017).
Notably,thismethodalsoappliestoneuralnetworkswith
asigmoidoutputneuronaslongastheinputtothe
layerandthelayer'sweightsareconstrainedtobenon-
negative.Theoutputlayerofsuchanetworkisidenticalto
ourlogisticregressionexample.Thecumulativeoperation
oftheintermediatelayers
˚
(

)
canbeinterpretedasare-
representationofthefeaturesbeforeapplyingthelogistic
regressionsuchas
C
(
x
)=sign

w
T
˚
(
x
)+
b

.Wewill
denotewhenamodelistrainedinanon-negativefashionby
appendingﬁ
+
ﬂtoitsname.
TheReLUfunctionisagoodchoiceforintermediatelayers
asitmaintainstherequirednon-negativerepresentationand
isalreadyfoundinmostmodernneuralnetworks.
Forbuildingintuition,inFigure1weprovideanexample
ofhowthisworksforneuralnetworksusingMNIST.Tofool
thenetworkintopredictingthepositiveclass(one)asthe
negativeclass(zero),theadversarymustnowmakelarger
removalsofcontentŠtothepointthatthenon-negative
attackisnolongerarealisticinput.
Figure1:Left:OriginalImage;Middle:Gradientattackon
LeNet;Right:Gradientattackonnon-negativeLeNet
+
.The
attackonthestandardmodelwasabletoaddpixelintensityin
around,zero-shapedareatofooltheintothinking
thiswasazero.Theattackontheconstrainedmodelwas
forcedtoremovepixelintensityfromtheoneratherthan
addinginnewvalueselsewhere.
Itshouldbenotedthatconstrainingamodelinthisway
doesreducetheamountofinformationavailablefordiscrimi-
natingsamplesatinferencetime,andadropin
accuracyislikelytooccurformostproblems.Thetradeoff
betweenadversarialrobustnessandperformanceshouldbe
analyzedforthedomainandusecase.
Apracticaltoourapproachisthatitissimpleto
implement.Inthegeneralcase,oncansimplyusegradient
decentwithaprojectionstepthatclipsnegativevaluesto
zeroaftereachstepupdate.Weimplementedourapproach
inKeras(Chollet,2015)bysimplyaddingtheﬁNonNegﬂ
constrainttoeachlayerinthemodel.
1
3.1Non-NegativityandMulti-Class
Theprimaryfocusofourworkisonbinarytaskslikemal-
wareandspamdetection,itisalsoworthaskingifitcanbe
appliedtomulti-classproblems.Inthisworkweshowthat
non-negativitycanstillhavesomeinthisscenario,
butweitnecessarytore-phrasehowsuchtasksarehan-
dled.Normally,onewouldusethesoftmax(
softmax
(
v
)
i
=
exp(
v
i
)
=
P
n
j
=1
exp(
v
j
)
)ontheun-normalizedprobabilities
v
givenbythelayer.Theprobabilityofaclass
i
is
thentakenas
softmax
(
v
)
i
.Howeverwethatthesoftmax
activationmakesiteasiertoattacknetworks.
Takethenonattackedactivationpattern
v
,where
v
i
>v
j
8
j
6
=
i
.Nowconsiderthenewactivationpattern
^
v
,whichis
producedbyanadversariallyperturbedinputwiththegoalof
inducingapredictionasclass
q
insteadof
i
.Thenitisneces-
sarytoforce
^
v
q
>
^
v
i
.Yetevenif
^
v
i
ˇ
v
i
,theprobabilityof
class
i
canbemadearbitrarilylowbycontinuingtomaximize
theresponseof
^
v
q
.Thismeansweareabletodiminishthe
1
https://keras.io/constraints/
apparentprobabilityofclass
i
withouthavingimpactedthe
model'sresponsetoclass
i
.Phrasedanalogouslyasanimage
problem,adversariesdon'tneedtoremovethe
amountofﬁcatﬂinaphototoinduceadecisionofﬁpotato,ﬂ
butonlyincreasetheamountofﬁpotato.ﬂ
InadditionChorowskiandZurada(2015)provedthata
non-negativenetworktrainedwithsoftmaxactivationcan
betransformedintoanequivalentunconstrainednetwork.
Thismeansthereislittlereasontoexpectournon-negative
approachtoprovideifwestaywiththesoftmaxacti-
vation,asithasanequivalentunconstrainedformandshould
beequallysusceptibletoalladversarialattacks.Assuchwe
mustmoveawayfromsoftmaxtogettheofour
non-negativeapproachinamulti-classscenario.
Insteadwecanlookattheprobleminaone-
vs-allfashionbyreplacingthesoftmaxactivationover
K
classeswith
K
independenttrainedwiththe
binarycross-entropylossandusingthesigmoidactivation
˙
(
z
)=1
=
(1+exp(

z
))
.Finalprobabilitiesaftertraining
areobtainedbynormalizingthesigmoidresponsestosumto
one.Wethatthisstrategycombinedwithnon-negative
learningprovidessomerobustnessagainstanadversarypro-
ducingtargetedattacks(e.g.,thenetworkis
99%surethecatisapotato).Theone-vs-allcomponentmake
itsuchthatincreasingtheofanewclasseven-
tuallyrequiresreducingtheoftheoriginalclass.
Thenon-negativityincreasesthedifofthisremoval
step,resultingindestructivechangestotheimage.
Wemaketwoimportantnotesonhowweapplynon-
negativetrainingforimageFirst,wepre-
trainthenetworkusingthestandardsoftmaxactivation,and
thenre-traintheweightswithourone-vs-allstyleandnon-
negativeconstraintsonthefullyconnectedlayers.Doing
soweonlyasmalldifferenceinaccuracybetweenre-
sults,wheretrainingnon-negativenetworksfromscratch
oftenhasreducedaccuracy.Second,wecontinuetousebatch
normalizationwithoutconstraints.Thisisbecausebatchnor-
malizationcanberolledintothebiastermandasare-scaling
oftheweights,andsodoesnotbreakthenon-negativecon-
straintinanyway.Weitspositiveimpactonconvergence
greaterwhentrainingwiththenon-negativeconstraints.
4ExperimentalMethodology
Havingthemechanismbywhichwewilldefend
againsttargetedadversarialattacks,wewillinvestigateits
applicationtotwomalwaredetectionmodels,onespamdetec-
tiontask,andfourimagetasks.Wewillspend
moretimeintroducingthemalwareattacks,asreadersmay
nothaveasmuchexperiencewiththisdomain.
Formalware,wewilllookatMalConv(Raffetal
.
,2018),a
recentlyproposedneuralnetworkthatlearnsfromrawbytes.
WewillalsoconsideranN-Grambasedmodel(Raffetal
.
,
2016).Bothofthesetechniquesareappliedtotherawbytes
ofaWeusethesame2,000,000trainingand80,000
testingdatumsasusedinRaffandNicholas(2017).
FollowingrecommendationsbyBiggio,Fumera,andRoli
(2014)wewillspecifythethreatmodelunderwhichwe
performourevaluations.Inallcases,ourthreatmodelwill
assumeawhite-boxadversarythathasfullknowledgeofthe
models,theirweights,andthetrainingdata.Forourbinary
problems,weassumeinthethreatmodelthat
ouradversarycanonlyaddnewfeaturestothemodel(i.e.,in
thefeaturevectorspacetheycanchangeazerovaluedfeature
tonon-zero,butcannotalteranalreadynon-zerovalue).
Werecognizethatthisthreatmodeldoesnotencompass
allpossibleadversaries,butnotethatitisoneofthemost
commonlyusedadversarialmodelsspanningmanydomains.
TheﬁGoodWordﬂattackonspammessagesisitselfanex-
ampleofthisthreatmodel'sactionspace,andoneofthe
initialworksinadversariallearningnoteditswideapplica-
bility(LowdandMeek,2005a).Inarecentsurvey,Maiorca,
Biggio,andGiacinto(2018)foundthat9outof10works
inevadingmaliciousPDFdetectorsusedtheadditiveonly
threatmodel,andtheseadditiveadversariessucceededin
bothwhite-boxandblack-boxattacks.Demontisetal
.
(2017)
consideredboththeadditiveonlyadversary,aswellasone
whichcouldaddorremovefeatures,asappliedtoandroid
malwaredetection.OntheirAndroiddata,theydemonstrate
alearningapproachwhichprovidesboundsonadversarysuc-
cessunderbothadversaryactionmodels,makingitrobustbut
stillvulnerable.Underthewhite-boxadditiveattackscenario,
theirSecure-SVMdetectionratedropsfrom95%onnormal
testdatadownto60%whenattacked.Finally,forthecase
ofWindowsPEdata,threedifferentworkshaveattackedthe
MalConvmodelusingtheadditiveadversary(Kreuketal
.
,
2018;Kolosnjajietal
.
,2018;Suciu,Coull,andJohns,2018).
Theadditivethreatmodelmakessensetostudy,asitis
easiertoimplementfortheadversaryandcurrentlysuccess-
fulinpractice.Forthisreason,itmakeslittlesenseforthe
adversarytoconsideramorepowerfulthreatmodel(e.g.,
addingandremovingfeatures)whichwouldincreasetheir
costsandeffort,whenthesimplerandcheaperalternative
works.Wewillshowinsection5thatwhilenotperfect,our
non-negativedefenseisthetodemonstrablythwartthe
additiveadversarywhilestillobtainingreasonableaccuracies.
Thisforcesapotentialadversarytoﬁstepupﬂtoamorepow-
erfulmodel,whichincreasestheireffortandcost.Wecontend
thisisofintrinsicvalue
eoipso
.Belowwewillreviewaddi-
tionaldetailsregardingthethreatmodelforeachdata-type
weconsider(WindowsPE,emails,andimages).Thisisfol-
lowedbyonhowtheattacksarecarriedoutforeach
algorithmasthedetailsaredifferentinallcases
duetomodelandproblemdiversity.
WindowsPEThreatModelSp
ForPEmalwarewe
usetheappendingofanunusedsectionastheattackvectorfor
technicalsimplicity.Theadversarywillbeallowedtoappend
anydesirednumberofbytesintoanaddedandunusedsection
ofthebinaryuntilnochangeintheevasionrateoccurs.
Ourapproachshouldstillworkiftheadversaryperformed
insertionsbetweenfunctionsratherthanattheendofthe
Realmalwareauthorsoftenemploypackingtoobfuscate
theentirebinary.Thisworkdoesnotconsiderdefenseagainst
packingobfuscation,excepttonotethatthecommonde-
fensivetechniqueistoemploydynamicanalysis.Ournon-
negativeapproachcanbeappliedtothefeaturesderivedfrom
dynamicanalysisaswell,butbeyondthescopeofthispa-
per.Thepossibilityofevadingnon-negativityondynamic
featuresrequiresaddressingthecat-and-mousegamearound
VMdetection,stealthymalware,andthenatureoffeatures
used.Thisdiscussionisimportant,butbeyondthecurrentam-
bit,whichwelimittostaticanalysis.Weareinterestedhere
inwhetherornotnon-negativityhasbenetotheadditive
adversary,notmoresophisticatedones.
SpamThreatModel
Forspamdetectionthead-
versarywillberestrainedtotheinsertionofnewcontentinto
anexistingspammessage.Thisisbecauseweareinterested
inthelower-effortﬁgoodwordﬂattackscenario.Despitebe-
inglesssophisticated,itremainseffectivetoday.Tackling
whollychangedandnewlycraftedspammessagesisbeyond
ourcurrentpurview.
ImageThreatmodel
Imagedoes
notexhibittheasymmetricerrorcoststhatmalwareandspam
do.Thepurposeofstudyingitistodetermineifournon-
negativitycanhavetomulti-classproblems.Itis
intuitivethattheanswerwouldbeﬁno,ﬂbutwenevertheless
thatsomelimitedexists.
Inthisthreatmodel,thereisnoﬁaddingﬂorﬁremovingﬂ
features,duetotheintrinsicnatureofimages.Assuchwe
considerthe
L
1
distancebetweenaoriginalimage
x
,and
itsadversariallyperturbedcounterpart
^
x
.Theadversarymay
arbitrarilyalteranypixels,solongas
k
x

^
x
k
1
<
,where

isaproblem-dependentmaximumdistance.
4.1AttackingMalConv
MalConvistheprimaryfocusofourinterest,asgradient
basedattackscannotnaivelybeappliedtoitsarchitecture.
Onlyrecentlyhaveattacksbeenproposed(Kolosnjajiet
al
.
,2018;Kreuketal
.
,2018),andwewillshowthatnon-
negativityallowsustothwarttheseadversaries.InMalConv,
rawbytesofanexecutablearepassedthroughalearned
embeddinglayerwhichactsasalookuptabletotransform
eachbyteintoan8-dimensionalvectorofrealvalues.This
representationisthenpassedthrougha1-dimensionalgated
convolution,globalmaxpooling,andthenafullyconnected
layerwithsigmoidoutput.Tohandlevaryingsizes,all
sequencesofbytesarepaddedtoaedlengthof2,100,000
usingaspecialﬁEndofFileﬂvalue(256)fromoutsideofthe
normalrangeofbytes(0Œ255).
Therawbytesarebothdiscreteandnon-ordinal,which
preventsgradientbasedattacksfrommanipulatingthemdi-
rectly.Kreuketal
.
(2018)(andindependentlyKolosnjajiet
al
.
(2018))devisedacleverwayofmodifyinggradientbased
attackstoworkonEXEs,evenwithanon-differentiableem-
beddinglayer,andwewillrecaptheirapproach.This
isdonebyperformingthegradientsearchofanadversar-
ialexampleinthe8-dimensionalvectorspaceproducedby
theembeddinglayer.Aperturbedvectoristhenmappedto
thebytewhichproducesthenearestneighborintheembed-
dingspace.KeepingwiththenotationofKreuketal
.
,let
M
2
R
n

d
bethelookuptablefromtheembeddinglayer
suchthat
M
:
X
!
Z
where
X
isthesetof
n
possible
bytesand
Z

R
d
istheembeddingspace.Thenforsome
sequenceofbytes
x
=(
x
0
;x
1
;:::;x
L
)
,wegeneratease-
quenceofvectors
z
=(
M
[
x
0
]
;
M
[
x
1
]
;:::;
M
[
x
L
])
were
M
[
x
i
]
indicatesrow
x
i
of
M
.Nowwegenerateanewvec-
tor
e
z
=
z
+

where

isaperturbationgeneratedfrom
anadversarialattack.Wemapeachelement
e
z
i
2
e
z
backto
bytespacebythenearestneighborof
e
z
i
amongthe
rowsof
M
.Byapplyingthistechniquetoonlysafe
regionsofabinary,theexecutionofgradientbasedattacks
againstMalConvarepossiblewithoutbreakingthebinary.
Toensurethataﬁsafeﬂareaexists,theyappendanunused
sectiontothebinary.Thelargerthisappendedsectionis,the
morespacetheadversaryhastodevelopastrongenough
signalofﬁbenign-nessﬂtofoolthealgorithm.
WereplicatetheattackdonebyKreuketal
.
(2018)which
usesthe
fastgradientsignmethod
(FGSM)(Goodfellow,
Shlens,andSzegedy,2015)togenerateanadversarialex-
ampleintheembeddingspace.Weour
e
z
bysolving:
e
z
=
z
+


sign

r
z
e
`
(
z
;y
;

)

,where
e
`
(

)
isthelossfunc-
tionofourmodelparameterizedby

and
z
istheembedded
representationofsomeinputwithlabel
y
.Thenew
e
z
isthen
mappedbackintobytespaceusingthemethodpreviouslydis-
cussed.Weperformedtheattackon1000randomlyselected
maliciousvaryingthesizeoftheappendedsectionused
togeneratetheadversarialexamples.
ForMalConv,addinganunusedsectionallowsanattacker
toaddbenignfeatureswhichoverwhelmthe
OurhypothesisisthatMalConv
+
shouldbeimmunetothe
attacksinceitonlylearnstolookformaliciousness,default-
ingtoadecisionofbenignwhennootherevidenceispresent.
Wealsonotethatthiscorrespondswellwithhowanti-virus
programsprefertohavelowerfalsepositiveratestoavoid
interferingwithusers'applications.
4.2AttackingN-Gram
TheN-Grammodelwastrainedusinglassoregularizedlo-
gisticregressiononthetopmillionmostfrequent6-byte
n-gramsfoundinour2milliontrainingset.The6-byte
gramsareusedasbooleanfeatures,wherea1representsthe
n-gram'spresenceinaLassoperformedfeatureselection
byassigningaweightof0tomostofthen-grams.Theresult-
ingmodelhadnon-zeroweightsassignedtoapproximately
67,000ofthefeatures.
Wedeviseawhite-boxattacksimilartotheattackKreuk
etal
.
(2018)usedagainstMalConvinthatweinjectbenign
bytesintoanunusedsectionappendedtomalicious
,wetakethemostbenign6-gramsbysorting
thembasedontheirlearnedlogisticregressioncoef
Weaddbenign6-gramsoneatatimetothemalicious
untilaoccurs.Thisendsupbeingthesame
kindofapproachLowdandMeek(2005b)usedtoperform
ﬁGoodWordﬂattacksonspamexceptweassumethe
adversaryhasperfectknowledgeofthemodel.Thesimplicity
oftheN-Grammodelallowsustodothistargetedattack,
andlookattheevasionrateasafunctionofthe
numberofinsertedfeatures.
Topreventtheseattacks,wetrainN-Gram
+
usingnon-
negativeweightconstraintsonthesamedata.Thismodelis
preventedfromassigningnegativeweightstoanyofthefea-
tures.WealsoremovethelassoregularizationfromN-Gram
+
astheconstraintsarealreadyperformingfeatureselectionby
pushingtheweightsofbenignfeaturestozero.
4.3SpamFiltering
Asmentionedintheprevioussection,LowdandMeek
(2005b)createdﬁGoodWordﬂattackstosuccessfullyevade
spamwithoutaccesstothemodel.Theseattacksap-
pendcommonwordsfromnormalemailsintospaminorder
tooverwhelmthespamintothinkingtheemailislegiti-
mate.Intheirseminalwork,theynotedthatitwasunrealistic
toassumethatanadversarywouldhaveaccesstothespam
,andwouldthusneedtosomehowguessatwhichwords
aregoodwords,ortosomehowquerythespamtosteal
informationaboutwhichwordsaregood.Othershavesimply
usedthemostfrequentwordsfromthehammessagesasa
proxytogoodwordselectionthatanadversarycouldrepli-
cate(Jorgensen,Zhou,andInge,2008;Zhou,Jorgensen,and
Inge,2007).Wetakethemorepessimisticapproachthatthe
adversaryhasfullaccesstoourmodel,andcansimplyselect
thewordsthathavethelargestnegativecoef(i.e.the
mostgood-lookingwords)fortheirattack.Thisisthesame
assumptionwemakeinattackingthen-grammodel.
Byshowingthatournon-negativelearningapproachelimi-
natesthepossibilityofgoodwordattacksinthispessimistic
case,weintrinsicallycoverallweakercasesofanadversary's
ability.WenoteaswellthatLowdandMeekspeculatedthe
onlyeffectivesolutiontostoptheGoodWordattackwould
betotoperiodicallyre-trainthemodel.Byeliminatingthe
possibilityofperformingGoodWordattacks,weincrease
thecosttooperatefortheadversary,astheymustnowexert
moreeffortintocraftingnovelspamtoavoid
detection.Byeliminatingthelowest-effortapproachthead-
versarycantake,weremediateasub-componentofthespam
problem,butnotspamasawhole.
WetraintwologisticregressionmodelsontheTREC2006
and2007SpamCorpora.
2
The2006datasetcontains37,822
emailswith24,912beingspam.The2007datasetcontains
75,419messageswith50,199ofthembeingspam.Weper-
formedverylittletextpreprocessingandrepresentedeach
emailasavectorofbooleanfeaturescorrespondingtothetop
10,000mostcommonwordsinthecorpus.Themodelis
trainedwithlassoregularizationinatraditionalmanner.The
secondmodelistrainedwithnon-negativeconstraintsonthe
coefinordertoisolateonlythefeaturespredictiveof
spamduringinference.
4.4TargetedAttacksonImage
Forourimageexperimentswefollowtherec-
ommendationsofCarliniandWagner(2017a)forevaluating
anadversarialdefense.InadditiontotheFGSMattack,we
willalsouseastrongeriteratedgradientattack.
weusetheIteratedGradientAttack(IGA)introducedin(Ku-
rakin,Goodfellow,andBengio,2017b),usingKerasforour
modelsandFoolbox(Rauber,Brendel,andBethge,2017)
fortheattackimplementations.Weevaluatedthe
atwhichsuchattackscansucceedagainstthestandardand
2
See
https://trec.nist.gov/data/spam.html
ournon-negativemodelsonMNIST,CIFAR10and100,and
TinyImageNet.
WenoteexplicitlythattheIGAattackisnotthemost
powerufuladversarywecoulduse.OtherattackslikePro-
jectedGradientDecent(PGD)andtheC&Wattack(Carlini
andWagner,2017b)aremoresuccessfully,anddefeatour
multi-classgeneralizationofnon-negativelearning.Westudy
IGAtoshowthatthereissomebutthatoverallthe
multi-classcaseisaweaknessofourapproach.Wethe
resultsinterestingandinformativebecauseourpriorbelief
wouldhavebeenthatnon-negativitywouldproducenobene-
tothedefenderatall,whichisnotthecase.
Weareinterestedindefendingagainstanad-
versarycreatingahightargetedattack(e.g.,a
labelwaspreviouslyasﬁcatﬂ,butnowis
asﬁpotatoﬂwithaprobabilityof99%).Assuchwewilllook
attheevasionrateforanadversaryalteringanimagetoother
classesoverarangeoftargetprobabilities
p
.Thegoalisto
seethenon-negativetrainednetworkhavealowerevasion
rate,especiallyfor
p

90%
.
ForMNISTandCIFAR10,sincethereareonly10classes,
wecalculatetheevasionrateatacertaintargetprobability
p
astheaveragerateatwhichanadversarycansuccessfully
alterthenetworkspredictiontoeveryotherclassandreacha
minimumprobability
p
.ForCIFAR100andTinyImageNet,
thelargernumberofclassesprohibitsthisexhaustivepairwise
comparison.Insteadweevaluatetheevasionrateagainsta
randomlyselectedalternativeclass.
OnMNIST,CIFAR10,andCIFAR100,duetotheirsmall
imagesizes(

32

32
),wefoundthatadversarialattacks
wouldoftenﬁsucceedﬂbychangingtheimagetoanunrecog-
nizabledegree.Forthisreasonwesetathresholdof60onthe
L
1
distancebetweentheoriginalimageandtheadversarial
Iftheadversarialimageexceededthis
threshold,wecountedtheattackasafailure.Thisthreshold
wasdeterminedbyexaminingseveralimages;moreinforma-
tioncanbefoundintheappendix.ForTinyImageNetthis
issuewasnotobserved,andFoolbox'sdefaultthresholdwas
used.
5Results
Havingreviewedthemethodbywhichwewilltargeted
adversarialattacks,andhowthemalwareattackswillbeap-
plied,wewillnowpresenttheresultsofournon-negative
networks.Firstwewillreviewthoserelatedtomalwareand
spamdetection,showingthatnon-negativelearningeffec-
tivelyneutralizesevasionbyamalwareauthor.Thenwewill
showhownon-negativelearningcanimproverobustnesson
severalimagebenchmarks.
5.1MalwareDetection
Usingthemethodoutlinedinsection4,Kreuketal
.
(2018)
reporteda100%evasionrateoftheirmodel.Asshownin
Figure2,ourreplicationoftheattackyieldedsimilarresults
forMalConv,whichwasevadedsuccessfullyfor95.4%of
theTheother4.6%ofwereallpreviouslyclassi-
asmalwarewithasigmoidactivationof1.0atmachine
precisionduetorounding.Theattackfailsfor
thesecasessincethereisnovalidgradientforthisoutput.
Apersistentadversarycouldstillcreateasuccessfuladver-
sarialexamplebyreplacingthesigmoidoutputwithalinear
activationfunctionbeforerunningtheattack.
0
20
40
60
80
100
0
20
40
60
80
100
AppendedSectionSizeasPercentofFile
EvasionRate(%)
EvasionRateasSizeofAppendedSectionIncreases
MalConv
MalConv
+
0
10
20
30
40
0
20
40
60
80
100
NumberofBenignN-GramsAdded
EvasionRate(%)
EvasionRateasTopBenignN-GramsareAdded
N-Gram
N-Gram+
Figure2:Evasionrate(y-axis)forMalConvandN-Gram
basedmodels.TopshowsMalConvevasionasthe
appendedsectionsizeincreases,andbottomshows
theN-Gramevasionasthenumberofbenignn-gramsare
added.Thenumberofthatevadeincreaseasthesizeof
theappendedsectionincreases.Theevasionratesremained
edforallsectionsizesgreaterthan25%ofthesize.
Ournon-negativelearningprovidesaneffectivedefense,
withonly0.6%ofabletoevadeMalConv
+
.Theoreti-
callywewouldexpectanevasionrateof0.0%.Investigating
thesesuccessfulevasionsuncoveredahiddenweaknessin
theMalConvarchitecture.WefoundthatbothMalConvand
MalConv
+
learnedtogiveasmallamountofmaliciouspre-
Table1:Outofsampleperformanceonmalwaredetectionin
theabsenceofattack.
Accuracy%PrecisionRecallAUC%
MalConv
94.10.9130.97298.1
MalConv
+
89.40.9080.88895.3
N-Gram
95.50.9260.98799.6
N-Gram
+
91.10.9150.88595.5
dictivepowertothespecialEndofFile(EOF)paddingvalue.
Thisismostlikelyabyproductoftheaveragemalicious
sizebeinglessthantheaveragebenignsizeinourtraining
set,whichcausesthesupposedlyneutralEOFvalueitself
tobeseenasanindicatorofmaliciousness.Theprocessof
addinganunusedsectionnecessarilyreducestheamount
ofEOFpaddingtokensgiventothenetwork,astheis
increasedinsize(pushingitclosertothe2.1MBprocessing
limit)thenewsectionreplacestheEOFtokens.Replacingthe
slightlymaliciousEOFtokenswithbenigncontentreduces
thenetwork'sinthebeingmalicious.
The0.6%ofthatevadedMalConv
+
onlydidsowhen
weresmall,andtheappendedsectionendedupcom-
prising50%oftheresultingbinary.Theslightmaliciousness
fromtheEOFwastheneededfeaturetopushthenetwork
intoadecisionofﬁmalicious.ﬂHowever,theremovalofEOFs
bytheunusedsectionremovedthisslightsignal,andpushed
thedecisionbacktoﬁbenign.ﬂIfweinsteadreplacethebytes
oftheunusedsectionwithrandombytesfromtheuniform
distribution,thestillevadedetection.Thismeansthe
evasionisnotafunctionoftheattackitself,butthe
tionofthebinarythatremovesEOFtokens.Asimpleto
thispaddingissueistoforcetherowoftheembeddingtable
correspondingtothespecialbytetobethezerovectorduring
training.ThiswouldpreventtheEOFtokenfromproviding
anypredictivepowerduringinference.
WeobservedsimilarresultsfortheN-Grammodel.The
evasionrateincreasesrapidlyasbenignfeaturesareadded
tothemaliciousWefoundthatappendingthetop41
mostbenignfeaturesresultedina100%evasionrate.This
attackiscompletelymitigatedbyN-Gram
+
sincenoneofits
featureshavenegativeweightssupportingthebenignclass.
Theonlywaytoalterthewouldbetoremove
maliciousn-gramsfromtheOurresultsforbothmodels
aredepictedinFigure2.
AccuracyvsDefense
Theonlydrawbackofthisapproachisthepossiblereduc-
tioninoverallaccuracy.Limitingtheavailableinformation
atinferencetimewilllikelyreduceperformanceformost
tasks.Alas,manysecurityrelatedapplications
existbecauseadversariesarepresentinthedomain.Wehave
shownthatunderattackournormalcompletelyfail
Šthereforeareductioninoverallaccuracymaybewellworth
theincreaseinmodeldefensibility.Table1showsmetrics
fromourmodelsundernormalconditionsforcomparison.
Sincedifferentmembersofthesecuritycommunityhave
differentdesireswithrespecttothetruepositivevs.false
0
0
:
2
0
:
4
0
:
6
0
:
8
1
0
0
:
2
0
:
4
0
:
6
0
:
8
1
FalsePositiveRate
TruePositiveRate
MalConv
MalConv+
N-Gram
N-Gram+
Figure3:ROCcurvesforMalConvandN-Grammalware
withandwithoutnon-negativerestraints,inthe
absenceofattack.
positivetrade-off,wealsoreporttheROCcurvesforthe
MalConv,MalConv
+
,N-Gram,andN-Gram
+
in
Figure3.
Whileournon-negativeapproachhaspaidapenaltyinac-
curacy,wenotethatwecanseethishaspredominatelycome
fromareductioninrecall.Becausefeaturescanonlyindicate
maliciousness,somemaliciousbinariesarelabeledasbenign
duetoalackofinformation.Thisscenariocorrespondsto
thepreferreddeploymentscenarioofsecurityproductsin
general,whichistohavealowerfalsepositiverate(benign
markedmalicious)attheexpenseoffalsenegatives(ma-
liciousmarkedbenign)(FerrandandFiliol,2016;Zhou
andInge,2008;Yih,Goodman,andHulten,2006).Assuch
thecostofnon-negativityinthisscenarioiswellaligned
withitsintendedusecase,makingthecostpalatableandthe
trade-offespeciallyeffective.
Tousitseemsreasonabletoacceptasmalllossinaccuracy
whennotunderattackinexchangeforalargeincreasein
accuracywhenunderattack.Aninterestingsolutioncouldbe
employingapairofmodels,oneconstrainedandonenot,in
additiontosomeheuristicindicatinganadversarialattackis
underway.Theunconstrainedmodelwouldgeneratelabels
duringnormaloperationsandfailovertotheconstrained
modelduringattack.Theoftheconstrainedmodel
couldbeusedasthisswitchingheuristicasweempirically
observethattheduringattackaremuchlower.
Thosewhoworkinanindustryenvironmentandproduce
commercialgradeAVproductsmayobjectthatouraccuracy
numbersdonotthesamelevelsobtainedtoday.We
remindthesereadersthatwedonothaveaccesstothesame
amountofdataoraccesstotheresourcesnecessarytopro-
ducetrainingcorporaofsimilarquality,andsoitshouldnot
beexpectedthatwewouldobtainthesamelevelsofaccu-
racyasproductionsystems.Thepurposeofthisworkisto
showthatalargeclassofmodelsthathavebeenusedand
attackedinpriorworkscanbesuccessfullydefendedagainst
thiscommonthreatmodel.Thiscomesatminorprice,asjust
discussed,butthisisthetechniquetoshowthatitcanbe
whollyprotective.
5.2SpamFiltering
Theaccuraciesforourtraditional,unconstrainedmodelswere
highonbothdatasets,butbothweresusceptibletoourversion
ofLowdandMeek'sﬁGoodWordﬂattack.Both
wereevaded100%ofthetimebyappendingonly7words
toeachmessageinthe2006caseandonly4wordsinthe
2007case.Thesewordscorrespondtothefeatureswiththe
lowestregressioncoefcients(i.e.,negativevalueswithhigh
magnitude)foreachmodel.
Useofthenon-negativeconstraintlowersouraccuracyfor
bothdatasetswhennotunderattack,but
completelyelimi-
nates
susceptibilitytotheseattacksasallﬁGoodWordsﬂhave
coefof0.0.Thespamauthorwouldonlybeableto
evadedetectionbyremovingwordsindicativeofspamfrom
theirmessage.Acomparisonofperformanceisshownin
Table2.
Table2:Outofsampleperformanceonspaminthe
absenceofattack.
Accuracy%PrecisionRecallAUC%F1Score
2006Lasso
96.50.9740.99397.10.983
2006Non-Neg.
82.60.9120.82083.50.864
2007Lasso
99.70.9990.99999.70.999
2007Non-Neg.
93.60.9620.94093.00.951
Despitethedropsinaccuracyimposedbyournon-negative
constraint,theresultsarebetterthanpriorworksindefending
againstweakerversionsoftheﬁGoodWordﬂattack.Forex-
ample,Jorgensen,Zhou,andInge(2008)developedadefense
basedonmultipleinstancelearning.Theirapproachwhen
attackedwithalloftheirselectedgoodwordshadaprecision
of0.772andarecallof0.743onthe2006TRECcorpus.
Thiswasthebestresultofalltheirtestedmethods,butour
non-negativeapproachachievesasuperior0.912and0.820
precisionandrecallrespectively.Whilespamauthorsarenot
asrestrictedtomodifytheirinputs,ourapproachforcesthem
tomoveuptoamoreexpensivethreatmodel(removingand
modifyingfeatures,ratherthanjustadding)Šwhichwe
argueisofintrinsicvalue.
Demontisetal
.
(2017)hadconcludedthatthereexistedan
ﬁimplicittrade-offbetweensecurityandsparsityﬂinbuilding
asecuremodelintheirAndroidmalwarework.Atleastfor
theadditiveadversary,weprovideevidencewithourbyte
n-gramsandspammodelsthatthisisnotanabsolute.Inboth
caseswebeginwithafullfeaturesetandthenon-negative
approachlearnsasparsemodel,whereallﬁgoodwordsﬂ(or
bytes)aregivencoefcientvaluesofzero.Assuchwesee
thatsparsityandsecurityoccurtogethertodefendagainstthe
additiveadversary.
5.3Image
Havinginvestigatedtheperformanceofnon-negativelearn-
ingformalwaredetection,wenowlookatitspotentialfor
imageInparticular,weitispossibleto
leveragenon-negativelearningasdiscussedinsubsection3.1
toproviderobustnessagainsttargetedattacks.That
istosayifthepredictedclassis
y
i
,theadversarywantsto
trickthemodelintopredictingclass
y
j
;j
6
=
i
andthatthe
ofthepredictionbe

p
.
ForMNISTwewilluseLeNet.Ouroutofsampleaccu-
racyusinganormalmodelis99.2%,whilethemodelwith
non-negativeconstraineddenselayersachieves98.6%.For
CIFAR10and100weuseaResNetbasedarchitecture.
3
ForCIFAR10weget92.3%accuracynormally,and91.6%
withournon-negativeapproach.OnCIFAR100thesame
architecturegets72.2%accuracynormally,and71.7%with
ournon-negativeapproach.ForTinyImageNet,wealsousea
ResNetarchitecturewiththeweightsofallbutthedense
layersinitializedpretrainedfromImageNet.
4
Thenormal
modelhasanaccuracyof56.6%,andtheconstrainedmodel
56.3%.Theresultsasafunctionofthetarget
p
canbeseeninFigure4.
Aninterestingartifactofourapproachisthatthenon-
negativenetworksareeasiertofoolforloerrors.
Wepositthisisduetotheprobabilitydistributionoverclasses
becomingnearuniformunderattack.OnCIFAR100they-
axisistruncatedforlegibilitysincetheevasionrateofFGSM
is93%andIGAis99%.Similarlyfornon-negativeTiny
ImageNet,FGSMandIGAachieve14%and17%evasion
rateswhen
p
=0
:
005
.
Despitetheseinitialhighevasionrates,wecanseein
allcasesthesuccessoftargetedadversarialattacksreaches
0%asthedesiredprobability
p
increases.ForMNISTand
CIFAR10,whichhaveonly10classes,thisoccursatupto
atarget30%Asmoreclassesareadded,the
difoftheattackincreases.ForTinyImageNetand
CIFAR100,targetedattacksfailby

2%
.
If
targeted
attacksfromIGAweretheonlytypeofattack
weneededtoworryabout,theseresultswouldalsoallow
ustousetheasamethodofdetectingattacks.
Forexample,CIFAR10hadtheweakestresults,needinga
targetof30%beforetargetedattacksfailed.The
averagepredictedofthenon-negativenetworkon
thetestsetwas93.8%.Thismeanswecanusethe
itselfasameasureofnetworkrobustness.Ifwedefaultto
aﬁno-answerﬂforeverythingwithaof40%or
lessonCIFAR10,andassumeanythingbelowthatlevelisan
attackanderror,theaccuracywouldhaveonlygonedown
1.2%.
Inordertodetermineifnon-negativeconstraintsaremerely
actingasagradientobfuscationtechnique(Athalye,Carlini,
andWagner,2018),wealsoattemptedablackboxattackby
attackingasubstitutemodelwithoutthenon-negativecon-
straintsandassessingwhethertheperturbedimagescreated
bythisattackwereabletofooltheconstrainedmodel.Inor-
dertomaketheattackasstrongaspossible,theunconstrained
networkwasthesameasthenetworkthatwasusedtowarm-
3
v1modeltakenfrom
https://tinyurl.com/
keras-cifar10-restnet
4
ResNet50built-inapplicationfrom
https://keras.io/
applications/#resnet50
20
40
60
80
100
0
10
20
30
Target
EvasionRate
MNIST
SoftmaxFGSM
SoftmaxIGA
Non-NegFGSM
Non-NegIGA
20
40
60
80
100
0
10
20
30
Target
CIFAR10
10

1
10
0
10
1
10
2
0
2
4
6
8
Target
CIFAR100
10
0
10
1
10
2
0
2
4
6
8
Target
TinyImageNet
Figure4:Targetedevasionrate(y-axis)asafunctionofthedesiredm
p
(x-axis)forfourdatasets.Due
tothedifferingrangesofinterest,righttwoshowninlogscaleforthex-axis.
startthenon-negativetraining.Thisshouldmaximizethe
transferabilityofattacksfromonetotheother.Despitethis
similarity,transferedattackshadonlya1.042%successrate,
whichisonereasonwebelievethatnon-negativeconstraints
arenotmerelyaformofgradientobfuscation.
Weemphasizethattheseresultsareevidencethatwecan
extendnon-negativitytoprovideinthemulti-class
case.Ourapproachappearstohavelowercostinmulti-class
casethaninthebinary,asaccuracydropsbylessthan1%
ineachdataset.Whilethecostislower,itsutilityislower
aswell.Ourmulti-classnon-negativeapproachprovidesno
in
untargeted
attacksŠwhereanyerrorbythemodel
isacceptabletotheattackerŠevenundertheweakerFGSM
attack.WhenevenstrongerattackslikeProjectedGradient
Descentareused,ourapproachisalsodefeatedinthetar-
getedscenario.Underthemoderate-strengthIGAattack,we
alsoseethatsusceptibilitytoevasionisincreasedforlow-
evasions.Intotal,weviewtheseresultsasindica-
tivethatnon-negativitycanhaveutilityforthemulti-class
caseandprovidesomelevelofthatisintrinsically
interesting,butmoreworkisneededtodetermineabetter
waytoapplythetechnique.
6Conclusion
Wehaveshownthatanincreasedrobustnesstoadversar-
ialexamplescanbeachievedthroughnon-negativeweight
constraints.Constrainedbinarycanonlyidentify
featuresassociatedwiththepositiveclassduringtesttime.
Therefore,theonlymethodforfoolingthemodelistoremove
featuresassociatedwiththatclass.Thismethodisparticu-
larlyusefulinsecurity-centricdomainslikemalwaredetec-
tion,whichhavewell-knownadversarialmotivation.Forcing
adversariestoremovemaliciousnessinthesedomainsisthe
desiredoutcome.Wehavealsodescribedatechniquetogen-
eralizethisrobustnesstomulti-classdomainssuchasimage
Weshowedaincreaseinrobustness
totargetedadversarialattackswhileminimizingtheamount
ofaccuracylostindoingso.
References
Athalye,A.;Carlini,N.;andWagner,D.2018.Obfuscated
GradientsGiveaFalseSenseofSecurity:Circumvent-
ingDefensestoAdversarialExamples.In
International
ConferenceonMachineLearning(ICML)
.
Biggio,B.;Fumera,G.;andRoli,F.2014.Securityevaluation
ofpatternunderattack.
IEEETransactionson
KnowledgeandDataEngineering
26(4):984Œ996.
Carlini,N.,andWagner,D.2017a.AdversarialExamplesAre
NotEasilyDetected:BypassingTenDetectionMethods.
In
Proceedingsofthe10thACMWorkshopon
IntelligenceandSecurity
,AISec'17,3Œ14.NewYork,
NY,USA:ACM.
Carlini,N.,andWagner,D.2017b.TowardsEvaluatingthe
RobustnessofNeuralNetworks.In
2017IEEESymposium
onSecurityandPrivacy(SP)
,39Œ57.IEEE.
Chistyakov,A.;Lobacheva,E.;Kuznetsov,A.;andRoma-
nenko,A.2017.SemanticEmbeddingsforProgrambe-
haviorPatterns.In
ICLRWorkshop
.
Chollet,F.2015.Keras.
Chorowski,J.,andZurada,J.M.2015.LearningUn-
derstandableNeuralNetworksWithNonnegativeWeight
Constraints.
IEEETransactionsonNeuralNetworksand
LearningSystems
26(1):62Œ69.
Dalvi,N.;Domingos,P.;Mausam;Sanghai,S.;andVerma,
D.2004.AdversarialIn
Proceedings
oftheTenthACMSIGKDDInternationalConferenceon
KnowledgeDiscoveryandDataMining
,KDD'04,99Œ108.
NewYork,NY,USA:ACM.
Demontis,A.;Melis,M.;Biggio,B.;Maiorca,D.;Arp,D.;
Rieck,K.;Corona,I.;Giacinto,G.;andRoli,F.2017.Yes,
MachineLearningCanBeMoreSecure!ACaseStudy
onAndroidMalwareDetection.
IEEETransactionson
DependableandSecureComputing
1Œ1.
Ferrand,O.,andFiliol,E.2016.Combinatorialdetection
ofmalwarebyIATdiscrimination.
JournalofComputer
VirologyandHackingTechniques
12(3):131Œ136.
Goodfellow,I.J.;Shlens,J.;andSzegedy,C.2015.Explain-
ingandHarnessingAdversarialExamples.In
International
ConferenceonLearningRepresentations(ICLR)
.
Grosse,K.;Papernot,N.;Manoharan,P.;Backes,M.;and
McDaniel,P.D.2016.Adversarialperturbationsagainst
deepneuralnetworksformalware
CoRR
abs/1606.04435.
Jorgensen,Z.;Zhou,Y.;andInge,M.2008.AMultiple
InstanceLearningStrategyforCombatingGoodWord
AttacksonSpamFilters.
J.Mach.Learn.Res.
9:1115Œ
1146.
KA.,andTeo,C.H.2009.FeatureWeightingfor
ImprovedRobustness.In
6thConferenceon
EmailandAnti-Spam(CEAS'09)
.
Kolosnjaji,B.;Demontis,A.;Biggio,B.;Maiorca,D.;Gi-
acinto,G.;Eckert,C.;andRoli,F.2018.Adversarial
MalwareBinaries:EvadingDeepLearningforMalware
DetectioninExecutables.In
26thEuropeanSignalPro-
cessingConference(EUSIPCO'18)
.
Kreuk,F.;Barak,A.;Aviv-Reuven,S.;Baruch,M.;Pinkas,
B.;andKeshet,J.2018.AdversarialExamplesonDiscrete
SequencesforBeatingWhole-BinaryMalwareDetection.
arXivpreprint
.
Kurakin,A.;Goodfellow,I.;andBengio,S.2017a.Adver-
sarialexamplesinthephysicalworld.In
International
ConferenceonLearningRepresentations(ICLR)
.
Kurakin,A.;Goodfellow,I.;andBengio,S.2017b.Adversar-
ialMachineLearningatScale.In
InternationalConference
onLearningRepresentations(ICLR)
.
Lowd,D.,andMeek,C.2005a.AdversarialLearning.In
Pro-
ceedingsoftheEleventhACMSIGKDDInternationalCon-
ferenceonKnowledgeDiscoveryinDataMining
,KDD
'05,641Œ647.NewYork,NY,USA:ACM.
Lowd,D.,andMeek,C.2005b.GoodWordAttackson
StatisticalSpamFilters.In
Conferenceonemailandanti-
spam(CEAS)
,125Œ132.
Maiorca,D.;Biggio,B.;andGiacinto,G.2018.TowardsRo-
bustDetectionofAdversarialInfectionVectors:Lessons
LearnedinPDFMalware.
arXivpreprint
.
Pascanu,R.;Stokes,J.W.;Sanossian,H.;Marinescu,M.;and
Thomas,A.2015.MalwareWithRecurrent
Networks.IEEE-InstituteofElectricalandElectronics
Engineers.
Raff,E.,andNicholas,C.2017.Malwareand
ClassImbalanceviaStochasticHashedLZJD.In
Proceed-
ingsofthe10thACMWorkshoponIntelligence
andSecurity
,AISec'17,111Œ120.NewYork,NY,USA:
ACM.
Raff,E.;Zak,R.;Cox,R.;Sylvester,J.;Yacci,P.;Ward,R.;
Tracy,A.;McLean,M.;andNicholas,C.2016.Aninves-
tigationofbyten-gramfeaturesformalware
JournalofComputerVirologyandHackingTechniques
.
Raff,E.;Barker,J.;Sylvester,J.;Brandon,R.;Catanzaro,B.;
andNicholas,C.2018.MalwareDetectionbyEatinga
WholeEXE.In
AAAIWorkshoponIntelligence
forCyberSecurity
.
Rauber,J.;Brendel,W.;andBethge,M.2017.Foolbox:A
Pythontoolboxtobenchmarktherobustnessofmachine
learningmodels.
Russu,P.;Demontis,A.;Biggio,B.;Fumera,G.;andRoli,F.
2016.SecureKernelMachinesAgainstEvasionAttacks.
In
Proceedingsofthe2016ACMWorkshoponal
IntelligenceandSecurity
,AISec'16,59Œ69.NewYork,
NY,USA:ACM.
Sahs,J.,andKhan,L.2012.AMachineLearningApproach
toAndroidMalwareDetection.In
2012EuropeanIntel-
ligenceandSecurityInformaticsConference
,141Œ147.
IEEE.
Saxe,J.,andBerlin,K.2015.DeepNeuralNetworkBased
MalwareDetectionUsingTwoDimensionalBinaryPro-
gramFeatures.
CoRRabs/1508.03096
.
Suciu,O.;Coull,S.E.;andJohns,J.2018.ExploringAdver-
sarialExamplesinMalwareDetection.In
AAAI2018Fall
SymposiumSeries:Adversary-AwareLearningTechniques
andTrendsinCybersecurity(ALEC)
.
Szegedy,C.;Zaremba,W.;Sutskever,I.;Bruna,J.;Erhan,D.;
Goodfellow,I.;andFergus,R.2014.Intriguingproperties
ofneuralnetworks.In
ICLR
.
Ugarte-Pedrero,X.;Balzarotti,D.;Santos,I.;andBringas,
P.G.2016.RAMBO:Run-TimePackerAnalysiswith
MultipleBranchObservation.In
Proceedingsofthe13th
InternationalConferenceonDetectionofIntrusionsand
Malware,andVulnerabilityAssessment-Volume9721
,
DIMVA2016,186Œ206.NewYork,NY,USA:Springer-
VerlagNewYork,Inc.
Yih,S.W.-t.;Goodman,J.;andHulten,G.2006.Learning
atLowFalsePositiveRates.In
Proceedingsofthe3rd
ConferenceonEmailandAnti-Spam
.CEAS.
Yuan,X.;He,P.;Zhu,Q.;Bhat,R.R.;andLi,X.2017.
AdversarialExamples:AttacksandDefensesforDeep
Learning.
arXiv
.
Zhou,Y.,andInge,W.M.2008.MalwareDetectionUsing
AdaptiveDataCompression.In
Proceedingsofthe1st
ACMWorkshoponWorkshoponAISec
,AISec'08,53Œ60.
NewYork,NY,USA:ACM.
Zhou,Y.;Jorgensen,Z.;andInge,M.2007.CombatingGood
WordAttacksonStatisticalSpamFilterswithMultiple
InstanceLearning.In
19thIEEEInternationalConference
onToolswithIntelligence(ICTAI2007)
,298Œ305.
IEEE.
AppendixAThresholdforCIFAR
Adversaries
Whencreatinganadversarialattack,itisnecessarythatsome
portionoftheimagemustbechangedasanintrinsicpartof
theattack.Thereiscurrentlyconsiderabledebateaboutthe
natureofhowlargethatchangeoughttobe,howitshouldbe
measured,andhowmuchweshouldcareaboutthenatureof
changestotheoriginalimage.Allofthesecouldbetopicsof
researchintheirownright,andwedonotclaimtosolvethem
inthiswork.Weusethe
L
1
distanceasameasuresimply
becauseithasbeenusedbypriorworks,evenifitisnotideal.
Wealsotakethestancethatitisimportantthatnoper-
ceptibledifferencebetweentheinputandattackedresultis
important,whilerecognizingthatisnotanagreeduponpro-
cedurebyeveryoneinthecommunity.Intuitivelywe
thelackofperceptibledifferenceimportantbecauseitleaves
noambiguityaboutthegroundtruthlabel.Iftheinputis
noticeablyperturbedbyanattack,thetruelabelofthenew
attackedimagemaycomeintoquestion.OurCIFAR10&
100resultsagainstthenon-negativenetworksoftenproduced
largedifferences,briningustoaneedtoimposeathreshold
atwhichwewillconsideranattackaﬁfailure.ﬂ
Figure5highlightstheneedtochooseathresholdbypro-
vidingexamplesofthespectrumof
L
1
distancesweobserved
betweentheoriginalandtheadversarial-generatedimages.
Itstartswithsmalldistances,suchasFigure5a,whichhas
onlyan
L
1
distanceof10andisclearlystillatruck.Atthe
extremeend,wealsohadresultslikeFigure5fwhichhadan
L
1
distanceof1000,andiswhollyunrecognizable.Weargue
thatsuchanattackmustbeafailure,astheinputdoesnot
evenresemblethedistributionofimagesfromCIFAR.
Thequestionis,wheredoesonedrawtheline?Whilewe
arguethatanimperceptibledifferenceisimportanttoavoid
labelambiguity,wehaveattemptedtogivedeferenceinal-
lowinglargemagnitudeattackswhilealsorecognizingthat
L
1
isnottheidealmethodtomeasurevisualperceptualdif-
ference.Assuchwehaveexperimentallyselectedathreshold
of60asonethatallowsforperceptibledifferences,andat
theedgeofnolongerbeingrecognizableasitsoriginalclass.
Ourdecisiontouseathresholdof60isbestshownin
Figure5cwherethe
L
1
differencestartstodemonstratean
obviousperceptibledifference.Wefeelthisimagerepresents
abalancebetweenthesubjectiveabilityofbeingableto
stilltellthatitisatypeofcar/truck,andhavingdifculty
recognizingwhatisintheimage(orifitisstillvalid)without
thecontextoftheoriginalimagenexttoit.
AllowinglargerthresholdsfortheCIFARattacksbeginsto
enteraterritorywhereitisnotcleartousthatthetruelabel
oftheimagehasbeenretained.Figure5dshowsonesuch
examplewithadeer,wheretheadversarialimagehasthe
samecolorsbutisuncleartouswhattheadversarialimage
shouldbelabeledas.
(a)
L
1
differenceof10,noperceptibledifference.
(b)
L
1
differenceof30,minuteperceptibledifference.
(c)
L
1
differenceof60,obviousperceptibledifference,thoughobjects
appearmostlyﬁthesameﬂ.
(d)
L
1
differenceof150,artifactsemerginginattack,origi-
nalobjectisgenerallyunrecognizable.
(e)
L
1
differenceof400,originalobjectisnolongerrecognizable.
(f)
L
1
differenceof1000,theimagehasbeencompletelydestroyed.
Figure5:ExamplesofIGAattacksagainstCIFAR10imageswithournon-negativenetwork.Ineachsubtheleftmost
imageistheoriginalimage,themiddleistheattackedresult,andtherightshowsthedifference.Movingfrom(a)to
(f),the
L
1
differencebetweentheoriginalandadversarialimageincreases.
"
58,Training VAEs Under Structured Residuals,http://arxiv.org/pdf/1804.01050v3.pdf,https://github.com/Garoe/tf_mvg,"TrainingVAEsUnderStructuredResiduals
GaroeDorta
1
;
2
SaraVicente
2
LourdesAgapito
3
NeillD.F.Campbell
1
IvorSimpson
2
1
UniversityofBath
2
AnthropicsTechnologyLtd.
3
UniversityCollegeLondon
1
{g.dorta.perez,n.campbell}@bath.ac.uk
2
{sara,ivor}@anthropics.com
3
l.agapito@cs.ucl.ac.uk
Abstract
Variationalauto-encoders(VAEs)areapopularandpowerfuldeepgenerative
model.PreviousworksonVAEshaveassumedafactorizedlikelihoodmodel,
wherebytheoutputuncertaintyofeachpixelisassumedtobeindependent.This
approximationisclearlylimitedasdemonstratedbyobservingaresidualimage
fromaVAEreconstruction,whichoftenpossessahighlevelofstructure.This
paperdemonstratesanovelschemetoincorporateastructuredGaussianlikeli-
hoodpredictionnetworkwithintheVAEthatallowstheresidualcorrelationsto
bemodeled.Ournovelarchitecture,withminimalincreaseincomplexity,incor-
poratesthecovariancematrixpredictionwithintheVAE.Wealsoproposeanew
mechanismforallowingstructureduncertaintyoncolorimages.Furthermore,we
provideaschemeforeffectivelytrainingthismodel,andincludesomesugges-
tionsforimprovingperformanceintermsofefyormodelinglongerrange
correlations.
Input



+

VAE
Ours
Figure1:Givenaninputimage,reconstructionsfromaVAEandourmodelareshown.TheVAE
modelstheoutputdistributionasafactorizedGaussian,whileourmodelusesastructuredGaussian
likelihood.Weshowthemeans

andasample

fromthecorrespondingcovariances.Thecorrelated
noisesampleofourmodelbettercapturesthestructurepresentinnaturalimages.
1Introduction
Generativeprobabilisticmodelsareapopulartoolforestimatingdatadensitydistributionsofimages.
Asidefromreconstructionandinterpolation,deepgenerativemodelsallowforsynthesizingnovel
examplesfromthelearneddistribution.Thereareanumberofinterestingapplicationsforsuch
models;forexample,imagesuper-resolution[
1
],editingimagesbasedonattributes[
2
],disentangling
shadingandalbedo[3]ornoiseremoval[4].
Weareinterestedingenerativemodelswithexplicitlikelihoodfunctions.Thesetypesofmodels
are,ingeneral,sufcientlyxibletolearncomplexdistributionsandareefcientatinference
andlearning.Theycanalsobewellsuitedtoreconstructingimages,whichisusefulincertain
applications.
arXiv:1804.01050v3  [stat.ML]  31 Jul 2018VariationalAutoEncoders(VAEs)[
5
,
6
]areapowerfulfamilyofdeepgenerativemodelsthatperform
variationalinferenceinordertolearnhigh-dimensionaldistributionsonlargedatasets.Inthismodel,
amappingislearnedfromalatentrepresentationtoimagespaceusingasetoftrainingexamples.
Tobeabletocomputethevariationalexpressions,theassociateddistributionsforboththelatent
parametersandtheresidualdistributionmusthaveexplicitanalyticforms.Ingeneral,factorized
Gaussianlikelihoodsaretheusualchoiceforthedistributionsduetotheirsimplicity.
Forimagedata,factorizedlikelihoodsimplythattheresidualerrorateachpixelisdistributed
independently.Incontrast,correlatedlikelihoodmodelscanaccountforsomespatialstructureinthe
uncertaintydistribution.Thefactorizedassumptionisdemonstrablyfalseinpracticalapplications,
wheretheresidualimageoftenexhibitsaclearspatialstructure.Thedifferencebetweenfactorized
andcorrelatedlikelihoodscanbehighlightedbycomparingsamplesfromtheoutputofuncertainty
distributions,asshowninFig.1.
Recentwork[
7
]demonstratedthatitispossibletopredictthestructuredoutputuncertaintyofa
generatedimagegivenalearnedlatentrepresentation.Inessenceestimatingacovariancematrixfrom
asinglesample.Giventheclearlimitationsofthefactorizedmodel,wehypothesizethatmodeling
thestructureofresidualsshouldbetotrainingVAEs.Toinvestigatethis,weproposeto
extendtheVAEbyusingastructuredGaussianlikelihooddistribution.
ProvidingtheVAEwithastructurednoisemodelendowsitwiththecapabilityofmodelingcomplex,
high-frequencyfeatures,whichdonotreliablyoccurinthesamelocation(
e.g
.hair),stochastically
ratherthandeterministically.ThisallowstheVAEtoconcentrateonstructurethatitcanmodel
correctly.
Tothebestofourknowledge,thisworkshowstheapproachfortrainingVAEswithacorrelated
Gaussianlikelihood.Anaiveapproachwouldintroduce
(
n
2
p

1)
=
2+
n
p
parameters,where
n
p
is
thenumberofpixelsintheimage.Thisisinfeasiblewithstandardstrategiesfortrainingdeepneural
networks.
Inthispaperweshowhowtoefovercometheselimitations.Wehavethreemaincontributions.
(1)ProvidinganovelarchitecturethatcombinestheVAEandstructuredcovariancepredictionmodels,
whilelimitingthenumberofadditionalparametersovertheoriginalVAE.Theproposedarchitecture
alsoallowsforstructureduncertaintypredictionincolorimages.(2)Aninvestigationintoeffective
trainingstrategiestoavoidpoorlocalminima.(3)Enhancementstothecovariancepredictionmodel
of[
7
]forimprovedefy(particularlyforhighdimensionaldata)andallowinglongerrange
correlationstobemodeled.WeshowexperimentsontheCelebAandLSUNChurches[
8
]datasets
anddemonstratethat,intermsofthepredictedresidualdistribution,ourmodeloffers
improvementsoverthefactorizedGaussianVAE.
2Relatedwork
ThisworkconsidersVAE[
5
],whichemploysamaximumlikelihoodapproach,withavariational
approximationontheposteriordistribution.VAEscommonlyusefactorizedGaussianlikelihoodsfor
modelingimagedata.Previousworkcanbeinto:methodsthatimprovethexibilityof
theposteriordistribution,andmethodsthatimprovethexibilityofthelikelihooddistribution.
ThemajorityofrecentresearchintoVAEshasbeenfocusedonimprovingthexibilityofthe
posteriorlatentdistribution.Suchmethodsinclude:parametrizedtransformationsofthedistribution
withtractableJacobians[
9
,
10
],andemployingadversarialtraining[
11
],distributionswithout
tractableprobabilitydensityfunctionscanbeadopted[
12
,
13
].Implicitrepresentationsviamultiple
sampling[
14
]andSteinparticle-basedrepresentations[
15
]havealsobeenexplored.Thisapproaches
seektoreducetheerrorinducedbythevariationalapproximationontheposteriordistribution.
However,theystillemploythesimplefactorizedassumptionontheresidualdistribution.Theyare
orthogonaltoourwork,andanyofthemcouldinprinciplebeusedinconjunctionwithourcorrelated
Gaussianlikelihood.
Morerelatedtoourworkaremethodsthatimprovethexibilityofthelikelihooddistribution.Anim-
plicitrepresentationbasedonGenerativeAdversarialNetworks(GANs)[
11
]hasbeenexplored[
16
].
However,thismethodsuffersfromknownissuesinGANtraining,suchasmodecollapseandunstable
training[
17
].Muchwork[
18
,
19
]hasbeendonetoattempttoaddressthoseissueswithGANs
models,yetasatisfactorysolutionisstilltobefound.Autoregressivedistributionsallowforcomplex
2
correlationsinthedatatobemodeled.PixelCNN[
20
]offersatractableneuralapproximationtolearn
thesedistributions.PreviousworkhasusedPixelCNNdistributionsforVAEs[
21
,
22
,
23
],achieving
impressiveresults.Thesequentialnatureofthedistribution,requiresaforwardpassofthenetwork
foreachpixelthatisgenerated,whichmakesitcomputationallydemanding.Incontrastourmethod
isabletomodelcorrelationsandgeneratesamplesinasingleforwardpassofthenetwork.
Mostworkonstructureduncertaintyisonlyapplicabletorestrictedsettings,suchassmalldata
scenarios[
24
],temporallycorrelatednoisemodels[
25
]andinGaussianprocesses[
26
].However
thesetechniquesingeneraldonotscalewelltodeeplearningsettingswhichtypicallyuselarge
quantitiesofhigh-dimensionaldata.MethodsapplicabletodeeplearningincludefactorizedGaussian
heteroscedasticnoisepredictioninimagesegmentationnetworks[
27
],andcorrelatedGaussiannoise
predictionfromalatentimagerepresentation[7].
3VariationalAutoencoder
WestartbyreviewingtheVAE[
5
]model.AVAEconsistsof(i)adecoder,
p

(
x
j
z
)
,whichmodels
theprobabilitydistributionoftheinputdata,
x
,conditionedonalow-dimensionalrepresentation,
z
,
inalatentspaceand(ii)anencoder,
q
˚
(
z
j
x
)
,whichmodelsthereverseprocess.Neuralnetworks
parametrizedby

and
˚
,areusedtomodelthedistributions.
Theparametersoftheneuralnetworksareestimatedsuchthatthemarginallikelihoodoftheinput
data,
p

(
x
)
,ismaximizedunderavariationalBayesianapproximation:
log
p

(
x
)=
D
KL
[
q
˚
(
z
j
x
)
jj
p

(
z
j
x
)]+
L
VAE
;
(1)
wherethevariationallowerboundis
L
VAE
=
E
z
˘
q
˚
(
z
j
x
)
[log
p

(
x
j
z
)]

D
KL
[
q
˚
(
z
j
x
)
jj
p

(
z
)]
:
(2)
Intheright-handsideofEq.1,thetermmeasuresthedistancebetweentheapproximateandthe
trueunknownposterior.Intheright-handsideofthevariationallowerbound,thettermisthe
reconstructionerror,andthesecondtermistheKLdivergencebetweentheencoderdistributionand
aknownprior.Maximizingtheboundwillbeapproximatelyequivalenttomaximizingthemarginal
likelihood,aslongastheapproximateposteriordistributionisxibleenough.
Forcontinuousdata,theapproximateposteriorandthedatalikelihoodusuallytaketheformof
multivariateGaussiandistributionswithfactorizedcovariancematrices
q
˚
(
z
j
x
)=
N

ˆ
(
x
)
;
!
(
x
)
2
I

;
(3)
p

(
x
j
z
)=
N


(
z
)
;
˙
(
z
)
2
I

;
(4)
where
x
istheimageasacolumnvector,themeans

(
z
)
;
ˆ
(
x
)
andvariances
˙
(
z
)
2
;
!
(
x
)
2
are
(non-linear)functionsoftheinputsorthelatentvariables.
4Methodology
WeproposetoextendtheVAEtouseacorrelatedGaussianlikelihood
p

(
x
j
z
)=
N


(
z
)
;

(
z
)

;
(5)
where

(
z
)
isadensecovariancematrix.Thecovariancematrixcapturesthecorrelationsbetween
pixels,thusallowingthemodeltopredictcorrelateduncertaintyoveritsoutputs.
Thelearningtaskforthemodelisill-posed,asafullcovariancematrix

(
z
)
mustbepredictedfrom
eachinputusingitsencodedlatentrepresentation.Moreover,theinitialestimatesarefarawayfrom
anyreasonablesolution,astheweighsofthenetworksareinitializedrandomly.
Theissuecanbepartlyovercomebyleveragingpreviousworkonstructureduncertaintypredic-
tionfordeepneuralnetworks[
7
].Inthatwork,theauthorstackledtheproblembyrestrictingthe
uncertaintypredictionnetworktoonlybeabletomodelcovariancematricesthathavesparseinverses.
Formally,theserestrictedcovariancematricesareas

(
z
)

1
=

(
z
)=
L
(
z
)
L
(
z
)
T
;
(6)
3
InputReconstructionResidualInputReconstructionResidual
Y
R
Cb
G
Cr
B
Figure2:Input,reconstructionsandresidualsinYCbCrandRGBcolorspacesforaVAEwith
diagonalcovariancetrainedwithRGBimages.IntheYCbCrspacetheresidualsoftheYchannel
arehighlystructured,whiletheonesforthecolorchannelsarenot.InRGBspaceallthechannels
containhighlystructuredresidualsandtheinformationishighlycorrelatedbetweenthechannels.
wherethe

(
z
)
isasparseprecisionmatrix,and
L
(
z
)
L
(
z
)
T
isitsCholeskydecomposition.
Theadvantagesofmodelingthecovarianceinthisway,isthattheuncertaintypredictionnetworkis
onlyrequiredtoestimatethenon-zerovaluesin
L
(
z
)
,anditistrivialtoevaluateallthetermsofa
Gaussianlikelihoodfrom
L
(
z
)
.Moreover,despite

(
z
)
beingsparse,

(
z
)
remainsadensematrix,
whichallowsmodelinglongrangecorrelationsintheresiduals.Thesparsitypatternproposedbythe
authorsissuchthat,forapatchsize
n
f
,pixelsthatareinsidethe
n
f
-neighborhoodinimage
spacehavenon-zeroentriesin
L
(
z
)
.Foraninputimagewith
n
p
pixelsthereare
n
p

(
n
2
f

1)
=
2+1
non-zeroentriesin
L
(
z
)
,as
L
(
z
)
isalowertriangularmatrix.Thenumberofparametersofthe
covariancepredictionnetworkisproportionaltothesquareoftheneighborhoodsize
n
f
.
The
L
(
z
)
networkasdescribedin[
7
],canonlybeusedtoestimatesparseprecisionmatricesfor
smallgray-scaleimages.Inthefollowingsectionswewillshowhowtoapplythemethodwithinthe
contextofVAEs,andhowtohandlecolorimagesaswellaslargerresolutioninputs.
4.1Colorimages
Wepresentastructureuncertaintyapproachthatcanmodelcolorimageswithaminimalincrementin
thenumberofparametersovermodelinggray-scaleimages.Toachievethis,weobservethatina
luminancecolorspace,suchasYCbCr,thehigh-frequencydetailsoftheimagearemostlyencodedin
theluminancechannel.ThisfacthasbeenusedbyimagecompressionalgorithmslikeJPEG,where
thecolorchannelsCbandCrarequantizedwithminimallossofqualityintheresultingimages.Itis
knownthatVAEsstruggletomodelhigh-frequencydetails,leadingtohighlystructuredresiduals
fortheluminancechannel,incontrasttheCbandCr,whicharesmoothbynature,leadtomostly
uncorrelatedresiduals,asshowninFig.2.
Therefore,theluminancechannelismodeledusingacorrelatedGaussiandistribution,whilethe
remainingchannelswillusefactorizedGaussians
x
=[
x
Y
;
x
Cb
;
x
Cr
]
;
(7)
p

(
x
j
z
)=
p

(
x
Y
j
z
)
p

(
x
Cb
j
z
)
p

(
x
Cr
j
z
)
;
(8)
p

(
x
Y
j
z
)=
N


Y
(
z
)
;

(
z
)

;
(9)
p

(
x
Cb
j
z
)=
N


Cb
(
z
)
;
˙
Cb
(
z
)
2
I

;
(10)
p

(
x
Cr
j
z
)=
N


Cr
(
z
)
;
˙
Cr
(
z
)
2
I

:
(11)
ManydatasetscontainimagescompressedinJPEG,andasaforementionedthisformatquantizesthe
CbCrchannels.Thelossofinformationduetoquantizationcanbeproblematic,aswithdifferent
amountofinformationthecolorchannelsshouldbetreateddifferently.Inordertoequalizethe
4
amountofinformationperpixelacrosschannels,theCbCrchannelsaredownsampledwithafactor
proportionaltothequantizationfactor.
4.2Priors
Asourgenerativemodelestimatesbothameanandacovariancematrixperinputimage,wemust
employsomeregularizationtoavoidpoorsolutions.Intuitively,littlevarianceisdesiredonthe
predictedcovariancematrix,i.e.themodelshouldbecertainaboutitspredictions.Anotherissueis
thepredictionofspuriouscorrelations,whicharenotwellsupportedbythedata.Experimentally,we
thatwithoutanyregularizationarandomlyinitializednetworkhasatendencytomodelmostof
theinformationinthecovariancematrix

(
z
)
,whichisundesirable.
TheBayesianapproachforregularizationistoaddapriordistributionoverthepredictedparameters.
ThestandardpriorusedforcovariancematricesisaninverseWishartdistribution,asthisisaconjugate
priorforthenormaldistribution.Alternatively,wecouldconsideraGammadistributionforthe
diagonalvaluesin
L
(
z
)
andaGaussianfortheoff-diagonalones.
Inpractice,agoodsetofparametersforthepriorsthatencouragedlowvarianceandsparse
correlationstructurewasdiftoThetotallikelihoodofthemodelwasindominatedbythe
costofthepriors.BoththeWishartandtheGamma-Gaussianpriorsmadetheposteriortoostrongly
diagonal.Empirically,weobtainedgoodresultswithavarianceminimizerandan
L
1
regularizerfor
theoff-diagonalelementsin
L
(
z
)
,wherethelossisas
L
=
L
VAE
+

jj
˙
(
z
)
2
jj
1
+

X
i;j
j
L
(
z
)
i;j
j
;
(12)
where

and

arescalarhyper-parametersand
i
6
=
j
.Evaluatingtheestimatedvariance
˙
(
z
)
2
given
L
(
z
)
impliesperformingacostlymatrixinverse.Insteadweapproximatethetermusingtheempirical
variance
jj
˙
(
z
)
2
jj
1
ˇjj
x


(
z
)
jj
2
2
.
4.3Scalingtolargerimages
Tomodellargerimages,thesizeoftheneighborhoodshouldbeincreasedaccordingly,sothatrelevant
correlationsarestillmodeled.However,thedimensionalityofthecovariancematrixincreases
quadraticallywiththesizeoftheneighborhood.Ourproposedsolutionistoreducethedimensionality
of
L
(
z
)
byapproximatingitwithalearnablebasis
L
(
z
)=
s
(
BW
(
z
))
;
(13)
where
B
isa
(
n
2
f

1)
=
2+1

n
b
matrixcontainingthebasis,
W
(
z
)
isa
n
b

n
p
matrixofweights,
n
p
isthenumberofpixelsintheinput,
n
b
isthenumberofbasisvectorsand
n
f
istheneighborhood
size.Eachcolumninthematrix
BW
(
z
)
containsadenserepresentationofthecorrespondingcolumn
in
L
,andtheoperator
s
(

)
padswithzeroesitsinput,convertingfromthedenserepresentationof
BW
(
z
)
tothesparseoneof
L
(
z
)
.
Thecovariancenetworkoutputbecomes
W
(
z
)
,whilethebasis
B
islearnedattraintimeanditis
sharedforalltheimages.Tofurtherboostthereductionindimensionalityin
L
(
z
)
,thebases
B
can
beconstructedsuchthattheneighboringstructureissimilartodilatedconvolutions.
Experimentally,wesawonlymarginalgainswhenusingthebasis,thedilatedsparsitypatternand
bothtogether.However,dilatedconvolutionshavebeenshowntobeagoodapproximationtolarge
denseTherefore,webelievesuchdilated-likesparsitypatternsmightbeusefulforlarger
resolutionimages.
4.4
OurapproachcanbeimplementedefentlyinmodernGPUarchitectures.Duringlearningthe
conditionalloglikelihoodofthedataisevaluatedas
log
p

(
x
j
z
)=

1
2

log
j

(
z
)
j
+(
x


(
z
))
T

(
z
)(
x


(
z
))+
n
p
log(2
ˇ
)

:
(14)
Thebasisderivationintheprevioussectionisused,where
L
(
z
)=
s
(
BW
(
z
))
.Thesquareerror
termcanbeevaluatedas
yy
T
,where
y
=(
x


)
T
L
(
z
)
,whichavoidsexplicitlyconstructing

.
5
ModelNLLKLFLOPs
VAE(Sph)[5]

3647

1033
=

4517

1308326
:
31
=
322
:
946
:
67
e
6
=
6
:
68
e
6
VAE(Diag)[5]

4016

813
=

4598

2747339
:
69
=
341
:
926
:
66
e
6
=
6
:
67
e
6

-VAE(Diag)[31]

3123

998
=

5574

108342
:
48
=
199
:
166
:
66
e
6
=
6
:
67
e
6
SUPN[7]

8308

1455
=

269
:
76
=

9
:
78
e
6
=

Ours

8297

1455
=

8669

1424269
:
76
=
281
:
916
:
72
e
6
=
6
:
70
e
6
Table1:Quantitativecomparisonofdensityestimationerrormeasuredasthenegativeloglikelihood
(NLL)forthe(grayscale/YCbCr)CelebAdataset,lowerisbetter.FLOPsmeasuresmodels
complexity,andKLdenotestheKLdivergenceoftheapproximateposteriortotheprior.Ourmodel
isabletoachievealikelihoodsimilartopreviouswork[
7
],withareductioninmodel
complexity.
Eachcolumninthematrix
B
isreshapedandzeropaddedtoan
n
f

n
f
kernel.Thesquarederroris
computedbyconvolvingtheresidualwitheachkernel,andperformingalinearcombinationwiththe
weights
W
(
z
)
.Thelogdeterminanttermcanbeevaluatedas
2
P
i
log(
B
0
;i
W
(
z
))
,where
B
0
;i
isa
vector.Ifnobasismatrixisused,thesameconvolutionalapproachcanbeappliedbysetting
B
=
I
.
Duringinference,thesamplingapproachdescribedin[7]isused.
5Results
WeevaluateourmodelontheCelebA[
28
]andLSUNOutdoorChurches[
8
]datasets.Thenetworks
areimplementedinTw[
29
]andtheyaretrainedonasingleTitanXGPUusingtheAdam[
30
]
optimizer.Allexperimentsuseimagesof
64

64
pixels,wheretheCbandCrchannelsareblurred
anddownsampledto
16

16
pixels.Thepatchsize
n
f
forourcovariancepredictionissetto3.
Additionaldetailsonthemodelarchitecturecanbefoundinthesupplementalmaterial.Fordata
augmentationweemploysimpleleft-rightoftheimages.
AllmodelsaretrainedwithimagesontheYCbCrspace,whichallowsadirectcomparison.Two
factorizationsaretestedforVAEs,asphericalcovarianceforboththeYandCbandCrchannels,
denotedasVAE(Sph),andadiagonalcovariancefortheYchannelandsphericalforCbandCr,
denotedVAE(Diag).ResultsforVAEstrainedwithRGBdataareshowninthesupplement.
Abatchsizeof
64
andalearningrateof
0
:
0005
isused.VAE(Sph)isusedaspretrainingforour
model,additionallyforthe
5
epochsonlythestructureduncertaintybranchofthenetworkis
trained.Thehyper-parametersvaluesfortheregularizersare

=10
and

=0
:
001
.
5.1CelebA
Weusethealignedandcroppedversionofthedataset,andwefurthercroptheimageinasquare
centeredattheface,followingthesameprocedureas[
16
].Allthemodelsaretrainedfor110epochs.
VAEswithfactorizedGaussianlikelihoodscanovtothereconstructionerror,thusproducing
lowqualitysamples.

-VAE[
31
]addressthisissuebyincreasingtheweightoftheKLtermin
theloglikelihood.WeshowresultsforbothaVAEand

-VAEtrainedwithadiagonalGaussian
likelihood.For

-VAEtheauthorsrecommendavalueof

of
62
:
5
forthisdataset,howeverwe
foundexperimentallythatavalueof
5
performedbetter.ForafaircomparisonwithSUPN[
7
],the
pretrainedVAEthatisneededforthemethodistrainedusingourapproach.
QuantitativeresultsareshowninTable1.Thelowerboundofthenegativeloglikelihoodonthetest
set,evaluatedwith500
z
samplesperimageasdescribedin[
14
].For

-VAEwereportthelikelihood
aftersetting

=1
.ValuesarereportedforbothgrayscaleandYCbCrimages.Ourmethodachieves
lowerlikelihoodthancompetingmethods,withtheexceptionof[
7
].However,our
modelislesscomplexaswedonotrequireaseparatedecodernetworkforthestructuredcovariance
prediction.
TheKLdivergencebetweentheapproximateposteriorandthepriorshowthatourcorrelatedresidual
distributionpredictionisalsoinordertoachievealatentdistributionthatbetterfollowsthe
prior.
6
Input
VAE(Sph)VAE(Diag)

-VAE(Diag)Ours

+


+


+


+

Figure3:Comparisonofimagereconstructionsforthedifferentmodels.Incontrasttopreviouswork,
ourmodelisabletolearnstructuredresiduals.
VAE(Sph)VAE(Diag)

-VAE(Diag)Ours

+


+


+


+

Figure4:Samplesforallmodels,whereourmethodistheonlyabletoincludehigh-frequency
detailssuchaswrinkles.
ReconstructionsareshowninFig.3.Themeans,

,obtainedwithourmodelarecomparableto
competingmethodsandthesamples,

,fromthenoisemodeladdplausibledetails,likehair.
SamplesofallthemodelsareshowninFig.4.TheVAEisoverinitspredictions,as
denotedbythehighvalueintheKLdivergenceshowninTable.1.Consequently,thisleadstoalatent
spacethatdoesnotfollowthepriordistributionandthustothepoorsamplesobserved.

-VAEisable
toproducesamplesthatareofsimilarqualitytoitsreconstructions.Ourmethodisabletoproduce
goodqualitysamples,wherethestructureduncertaintypredictionbranchisagainabletomodelhigh
frequencydetails.
5.2LSUN
Weusethe
churchoutdoors
categoryofthisdataset,asthetestdataisnotavailableweusethe
validationsetinstead.Allthemodelsaretrainedfor150epochs.
QuantitativeresultsforreconstructionsarepresentedinTable2,wherewemeasurethemeansquared
error(MSE)withrespecttotheinput,aswellasthenegativeloglikelihood.TheMSEismeasured
usingtheRGBimagesbeforetheconversiontoYCbCrcolorspace,anditiscomputedusingonlythe
predictedmeans,i.e.

issettozero.Thevaluescorrespondtoimagesinthe
[0
;
255]
range,andwe
showmeanandstandarddeviationsacrossthedataset.Ourmodelisabletoofferanimprovements
overaVAEwithdiagonalGaussian,whilethe

-VAEshowsadropinthereconstruction
quality.AVAEwithsphericalnoiseisabletoachievethelowestreconstructioncost.Ourmodelis
abletooutperformcompetingmethodsintermsofmarginalloglikelihood.
7
ModelNLLKLMSE
VAE(Sph)[5]

2440

1176303
:
83566

285
VAE(Diag)[5]

4464

1924331
:
06728

327

-VAE(Diag)[31]

4213

2055205
:
88800

354
Ours

6918

2423313
:
92614

286
Table2:Quantitativecomparisonofdensityestimationerrormeasuredasthenegativeloglikelihood
(NLL)fortheLSUNdataset,lowerisbetter.Ourmodelisabletoachieveaimprovement
inlikelihoodincomparisonwithVAEswithfactorizednoisemodels.
Input
VAE(Sph)VAE(Diag)

-VAE(Diag)Ours

+


+


+


+

Figure5:Comparisonofimagereconstructionsforthedifferentmodels.Ourmodelisabletoimprove
overtheoversmoothedimagesgeneratedbypreviouswork.
VAE(Sph)VAE(Diag)

-VAE(Diag)Ours

+


+


+


+

Figure6:Samplesdrawnfromthemodels.VAEfailstolearnanusefullatentspaceand

-VAE
producesblurrierimagesthanourmodel.
ReconstructionsareshowninFig.5,whereweagainthatourmodelisabletoproducebetter
reconstructionsthancompetingmethods,withtheexceptionofaVAEwithsphericalnoise.However,
theresidualsmodeledbyasphericalVAEarequitelimited,astheyareforcedtohavehighlevels
ofnoiseofnoisethroughouttheimage,includingareaswhicharetrivialtomodellikethesky.Our
structuredresidualsadddetail,howeverasthisdatasetismorecomplexthanfacesallthemodels
struggletoreconstructtheinput.
SamplesfromthemodelsareshowninFig.6,wherewethattheVAEwithdiagonalcovariance
strugglestogenerateanythingmeaningful.

-VAEisabletogeneraterecognizableshapes,whichare
blurry,whileourmodelisabletoproducemeanssimilartoVAE(Sph)withbetternoise
samples.
8
6Conclusions
ThispaperproposestheapproachforendowingVariationalAutoEncoderswithastructured
likelihoodmodel.OurresultshavedemonstratedthatVAEscanbesuccessfullytrainedtopredict
structuredoutputuncertainty,andthatsuchmodelshavesimilarreconstructionsthanthoseobtained
withafactorizedlikelihoodmodel.
Inthispaper,wehaveproposedasimpleschemeforensuringtheVAEreducesthevarianceof
themodel,ratherthanattemptingtodescribetheresidualerrorthroughthecovariance.Thisraises
interestingavenuesforfuturework,particularlyintermsofaddinghierarchicalpriorstopreventthe
structuredresidualmodeladdingspuriouscorrelations.Furtherworkwillalsoincludeinvestigations
onhigherresolutionimagesandtheefyofthebasissetanddilatedsparsitypatternswhen
modelingsuchdata.
References
[1]
Ledig,C.,Theis,L.,Huszar,F.,Caballero,J.,Cunningham,A.,Acosta,A.,Aitken,A.,Tejani,
A.,Totz,J.,Wang,Z.,Shi,W.:Photo-realisticsingleimagesuper-resolutionusingagenerative
adversarialnetwork.In:CVPR.(July2017)
[2]
Yan,X.,Yang,J.,Sohn,K.,Lee,H.:Attribute2image:Conditionalimagegenerationfrom
visualattributes.In:ECCV.(2016)
[3]
Shu,Z.,Yumer,E.,Hadap,S.,Sunkavalli,K.,Shechtman,E.,Samaras,D.:Neuralfaceediting
withintrinsicimagedisentangling.In:CVPR.(July2017)
[4]
Vincent,P.,Larochelle,H.,Bengio,Y.,Manzagol,P.A.:Extractingandcomposingrobust
featureswithdenoisingautoencoders.In:ICML.(2008)1096Œ1103
[5]Kingma,D.P.,Welling,M.:Auto-encodingvariationalbayes.ICLR(2014)
[6]
Rezende,D.J.,Mohamed,S.,Wierstra,D.:Stochasticbackpropagationandapproximate
inferenceindeepgenerativemodels.ICML(2014)
[7]
Dorta,G.,Vicente,S.,Agapito,L.,Campbell,N.D.F.,Simpson,I.:Structureduncertainty
predictionnetworks.In:CVPR.(2018)
[8]
Yu,F.,Zhang,Y.,Song,S.,Seff,A.,Xiao,J.:Lsun:Constructionofalarge-scaleimagedataset
usingdeeplearningwithhumansintheloop.arXivpreprintarXiv:1506.03365(2015)
[9]Rezende,D.J.,Mohamed,S.:Variationalinferencewithnormalizingws.ICML(2015)
[10]
Kingma,D.P.,Salimans,T.,Welling,M.:Improvingvariationalinferencewithinverseautore-
gressivew.NIPS(2016)
[11]
Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,Courville,A.,
Bengio,Y.:Generativeadversarialnets.In:NIPS.(2014)2672Œ2680
[12]
Makhzani,A.,Shlens,J.,Jaitly,N.,Goodfellow,I.:Adversarialautoencoders.In:ICLR.(2016)
[13]
Mescheder,L.,Nowozin,S.,Geiger,A.:Adversarialvariationalbayes:Unifyingvariational
autoencodersandgenerativeadversarialnetworks.In:ICML.(2017)
[14]Burda,Y.,Grosse,R.,Salakhutdinov,R.:Importanceweightedautoencoders.ICLR(2016)
[15]
Pu,Y.,Gan,Z.,Henao,R.,Li,C.,Han,S.,Carin,L.:Vaelearningviasteinvariationalgradient
descent.NIPS(2017)
[16]
Larsen,A.B.L.,Sønderby,S.K.,Winther,O.:Autoencodingbeyondpixelsusingalearned
similaritymetric.In:ICML.Volume48.,JMLR(2016)1558Œ1566
[17]
Radford,A.,Metz,L.,Chintala,S.:Unsupervisedrepresentationlearningwithdeepconvolu-
tionalgenerativeadversarialnetworks.ICLR(2016)
[18]
Berthelot,D.,Schumm,T.,Metz,L.:Began:Boundaryequilibriumgenerativeadversarial
networks.CoRR(2017)
[19]Arjovsky,M.,Chintala,S.,Bottou,L.:Wassersteingan.CoRR(2017)
[20]Oord,A.,Kalchbrenner,N.,Kavukcuoglu,K.:Pixelrecurrentneuralnetworks.ICML(2016)
9
[21]
Chen,X.,Kingma,D.P.,Salimans,T.,Duan,Y.,Dhariwal,P.,Schulman,J.,Sutskever,I.,
Abbeel,P.:VariationalLossyAutoencoder.In:ICLR.(2017)
[22]
Gulrajani,I.,Kumar,K.,Ahmed,F.,Taiga,A.A.,Visin,F.,Vazquez,D.,Courville,A.:Pixel-
VAE:ALatentVariableModelforNaturalImages.In:ICLR.(2017)
[23]Makhzani,A.,Frey,B.J.:Pixelganautoencoders.In:NIPS.(2017)1975Œ1985
[24]
Nikias,C.L.,Pan,R.:Timedelayestimationinunknowngaussianspatiallycorrelatednoise.
IEEETransactionsonAcoustics,Speech,andSignalProcessing
36
(11)(1988)1706Œ1714
[25]
Woolrich,M.W.,Ripley,B.D.,Brady,M.,Smith,S.M.:Temporalautocorrelationinunivariate
linearmodelingoffmridata.NeuroImage
14
(6)(2001)1370Œ1386
[26]
Rasmussen,C.E.,Williams,C.K.:Gaussianprocessesformachinelearning.Volume1.MIT
pressCambridge(2006)
[27]
Kendall,A.,Gal,Y.:Whatuncertaintiesdoweneedinbayesiandeeplearningforcomputer
vision?In:NIPS.(2017)
[28]
Liu,Z.,Luo,P.,Wang,X.,Tang,X.:Deeplearningfaceattributesinthewild.In:ICCV.
(December2015)
[29]
Abadi,M.,Agarwal,A.,Barham,P.,Brevdo,E.,Chen,Z.,Citro,C.,Corrado,G.S.,Davis,
A.,Dean,J.,Devin,M.,etal.:TensorFlow:Large-scalemachinelearningonheterogeneous
systems(2015)Softwareavailablefromw.org.
[30]Kingma,D.,Ba,J.:Adam:Amethodforstochasticoptimization.ICLR(2015)
[31]
Higgins,I.,Matthey,L.,Pal,A.,Burgess,C.,Glorot,X.,Botvinick,M.,Mohamed,S.,Lerchner,
A.:beta-vae:Learningbasicvisualconceptswithaconstrainedvariationalframework.In:
ICLR.(2017)
10
"
59,"High-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach",http://arxiv.org/pdf/1802.07167v3.pdf,https://github.com/TeaPearce/Deep_Learning_Prediction_Intervals,arXiv:1802.07167v3  [stat.ML]  15 Jun 2018
60,Low-shot learning with large-scale diffusion,http://arxiv.org/pdf/1706.02332v3.pdf,https://github.com/facebookresearch/low-shot-with-diffusion,"Low-shotlearningwithlarge-scalediffusion
MatthijsDouze
y
,ArthurSzlam
y
,BharathHariharan
y
,Herv
´
eJ
´
egou
y
y
FacebookAIResearch
*
CornellUniversity
Abstract
Thispaperconsiderstheproblemofinferringimagela-
belsfromimageswhenonlyafewannotatedexamplesare
availableattrainingtime.Thissetupisoftenreferredto
aslow-shotlearning,whereastandardapproachistore-
trainthelastfewlayersofaconvolutionalneuralnetwork
learnedonseparateclassesforwhichtrainingexamplesare
abundant.Weconsiderasemi-supervisedsettingbasedon
alargecollectionofimagestosupportlabelpropagation.
Thisispossiblebyleveragingtherecentadvancesonlarge-
scalesimilaritygraphconstruction.
Weshowthatdespiteitsconceptualsimplicity,scaling
labelpropagationuptohundredmillionsofimagesleadsto
stateoftheartaccuracyinthelow-shotlearningregime.
1.Introduction
Large,diversecollectionsofimagesarenowcommon-
place;theseoftencontainaﬁlongtailﬂofvisualconcepts.
Someconceptslikeﬁpersonﬂorﬁcatﬂappearinmanyim-
ages,butthevastmajorityofthevisualclassesdonotoccur
frequently.Eventhoughthetotalnumberofimagesmay
belarge,itishardtocollectenoughlabeleddataformost
ofthevisualconcepts.Thusifwewanttolearnthem,we
mustdosowithfewlabeledexamples.Thistaskisnamed
low-shotlearning
intheliterature.
Inordertolearnnewclasseswithlittlesupervision,a
standardapproachistoleveragealreadylearned
forthemostfrequentclasses,employingaso-called
trans-
ferlearning
strategy.Forinstance,fornewclasseswithfew
labels,onlythefewlastlayersofaconvolutionalneuralnet-
workarere-trained.Thislimitsthenumberofparameters
thatneedtobelearnedandlimitsover
Inthispaper,weconsiderthelow-shotlearningprob-
lemdescribedabove,wherethegoalistolearntodetect
newvisualclasseswithonlyafewannotatedimagesper
class,butwealsoassumethatwehavemanyunlabelledim-
ages.Thisiscalledsemi-supervisedlearning[
39
,
37
](con-

ThisworkwascarriedoutwhileB.Hariharanwaspost-docatFAIR.
Figure1.Thediffusionsetup.Thearrowsindicatethedirection
ofdiffusion.Thereisnodiffusionperformedfromthetestim-
ages.Fortherestofthegraph,theedgesarebidirectional(
i.e.
,the
graphmatrixissymmetric).Exceptwhenmentionedotherwise,
theedgeshavenoweights.
sidered,
e.g.
,forfaceannotation[
14
]).Themotivationof
thisworkisthreefold.Firstwewanttoshowthatwithmod-
erncomputationaltools,classicalsemi-supervisedlearning
methodsscalegracefullytohundredsofmillionsofunla-
beledpoints.Alimitingfactorinpreviousevaluationswas
thatconstructingthesimilaritygraphsupportingthediffu-
sionwasslow.Thisisnolongerabottleneck:thanksto
advancesbothincomputingarchitecturesandalgorithms,
onecanroutinelycomputethesimilaritygraphfor100mil-
lionsimagesinafewhours[
21
].Second,wewanttoan-
swerthequestion:
Doesaverylargenumberofimageshelp
forsemi-supervisedlearning?
Finally,bycomparingthe
resultsofthesemethodsonImagenetandtheYFCC100M
dataset[
33
],wehighlighthowthesemethodsexhibitsome
aspectsofImagenetthatcantheperfor-
manceoflowshotlearningalgorithms.
Insummary,thecontributionofourpaperisastudyof
semi-supervisedlearninginthescenariowherewehavea
verylargenumberofunlabeledimages.Ourmainresults
arethatinthissetting,semi-supervisedlearningleadsto
stateoftheartlow-shotlearningperformance.Inmorede-
tail,wemakethefollowingcontributions:

Wecarryoutalarge-scaleevaluationfordiffusion
methodsforsemi-supervisedlearningandcompare
1
arXiv:1706.02332v3  [cs.CV]  15 Jun 2018ittorecentlow-shotlearningpapers.Ourexperi-
mentsareallcarriedoutonthepublicbenchmarkIm-
agenet[
11
]andtheYFC100Mdataset[
33
].

Weshowthatourapproachisefandthatthedif-
fusionprocessscalesuptohundredsofmillionsofim-
ages,whichisorder(s)ofmagnitudelargerthanwhat
weareawareintheliteratureonimage-baseddiffu-
sion[
19
,
18
].Thisismadepossiblebyleveragingthe
recentstateoftheartforefk-nearestneighbor
graphconstruction[
21
].

Weevaluateseveralvariantsandhypothesesinvolved
indiffusionmethods,suchasusingclassfrequency
priors[
38
].Thisscenarioisrealisticinsituations
wherethisstatisticisknownapriori.Weproposea
simplewaytoestimateitwithoutthispriorknowl-
edge,andextendthisassumptiontoamulticlassset-
tingbyintroducingaprobabilisticprojectionstepde-
rivedfromSinkhorn-Knoppalgorithm.

Ourexperimentalstudyshowsthatasimplepropaga-
tionprocessoutperformssomestate-of-
the-artapproachesinlow-shotvisuallearningwhen(i)
thenumberofannotatedimagesperclassissmalland
when(ii)thenumberofunlabeledimagesislargeor
theunlabeledimagescomeformthesamedomainas
thetestimages.
Thispaperisorganizedasfollows.Section
2
reviews
relatedworksandSection
3
describesthelabelpropagation
methods.TheexperimentalstudyispresentedinSection
4
.
Ourconclusioninsection
5
summarizesour
2.Relatedwork
Low-shotlearning
Recentlytherehasbeenarenewedin-
terestforlow-shotlearning,
i.e.
,learningwithfewexam-
plesthankstopriorstatisticsonotherclasses.Suchworks
includemetriclearning[
26
],learningkNN[
35
],regular-
izationandfeaturehallucination[
16
]orpredictingparam-
etersofthenetwork[
5
].RaviandLarochelleintroducea
meta-learnertolearntheoptimizationparametersinvovled
inthelow-shotlearningregime[
30
].Mostoftheworks
considersmalldatasetslikeOmniglot,CIFAR,orasmall
subsetofImagenet.Inourpaperwewillfocussolelyon
largedatasets,inparticulartheImagenetcollection[
31
]as-
sociatedwiththeILSVRCchallenge.
Diffusionmethods
Wereferthereaderto[
3
,
12
]fora
reviewofdiffusionprocessesandmatrixnormalizationop-
tions.Suchmethodsareanefwayofclusteringim-
agesgivenamatrixofinputsimilarity,orakNNgraph,and
havebeensuccessfullyusedinasemi-superviseddiscovery
setup[
14
].Theysharesomeconnectionswithspectralclus-
tering[
6
].In[
29
],akNNgraphisclusteredwithspectral
clustering,whichamountstocomputingthe
k
eigenvectors
associatedwiththe
k
largesteigenvaluesofthegraph,and
clusteringtheseeigenvectors.Sincetheeigenvaluesareob-
tainedviaLanczositerations[
15
,Chapter10],thebasicop-
erationissimilartoadiffusionprocess.Thisisalsorelated
topoweriterationclustering[
25
],asintheworkofCho
et
al.
[
8
]toclusters.
Semi-supervisedlearning
ThekNNgraphcanbeused
fortransductiveandsemi-supervisedlearning(see
e.g.
[
3
,
39
]foranintroduction).Intransductivelearning,arela-
tivelysmallnumberoflabelsareusedtoaugmentalarge
setofunlabeleddataandthegoalistoextendthelabeling
totheunlabeleddata(whichisgivenattraintime).Semi-
supervisedlearningissimilar,excepttheremaybeasepa-
ratesetoftestpointsthatarenotseenattraintime.Inour
work,weconsiderthesimpleproposalofZhu
etal.
[
38
],
wherepowersofthe(normalized)kNNgraphareusedto
smoothfunctionsonthekNNgraphwithdesiredval-
uesatthelabeledpoints.Thereexistmanyvariationsonthe
algorithms,
e.g.
,Zhou
etal.
[
37
]weighttheedgesbasedon
distancesandintroducealosstradingaation
constraintandasmoothnesstermenforcingconsistencyof
neighboringnodes.
Labelpropagationisatransductivemethod.Inorderto
evaluateonnewdata,weneedtoextendthesmoothfunc-
tionsoutofthetrainingdata.Whiledeepnetworkshave
beenusedbeforeforoutofsampleextension,
e.g.
,in[
7
]
and[
20
],inthespeechdomain,inthiswork,weusea
weightedsumofnearestneighborsfromthe(perhapsun-
labeled)trainingdata[
4
].
kNN-graphconstruction
Thediffusionmeth-
odsuseamatrixasinputcontainingthesimilaritybetween
allimagesofthedataset.Considering
N
images,
e.g.
,
N
=10
8
,itisnotpossibletostoreamatrixofsize
N
2
.
Howevermostoftheimagepairsarenotrelatedandhavea
similaritycloseto0.Thereforediffusionmethodsareusu-
allyimplementedwithsparsematrices.Thismeansthatwe
computeagraphconnectingeachimagetoitsneighbors,as
determinedbythesimilaritymetricbetweenimagerepre-
sentations.Inparticular,weconsiderthek-nearestneigh-
borgraph(kNN-graph)overasetofvectors.Severalap-
proximatealgorithms[
10
,
23
,
1
,
17
]havebeenproposedto
efproducethekNNgraphusedasinputofitera-
tive/diffusionmethods,sincethisoperationisofquadratic
complexityinthenumberofimages.Inthispaper,weem-
ploytheFaisslibrary,whichwasshowncapabletoconstruct
agraphconnectingupto1billionvectors[
21
].
3.Propagatinglabels
Thissectiondescribestheinitialstageofourproposal,
whichestimatestheclassoftheunlabelledimageswitha
diffusionprocess.Itincludesanimagedescriptionstep,the
constructionofakNNgraphconnectingsimilarimages,and
alabeldiffusionalgorithm.
3.1.Imagedescription
Ameaningfulsemanticimagerepresentationandanas-
sociatedmetricisrequiredtomatchinstancesofclassesthat
havenotbeenseenbeforehand.Whileearlyworksonsemi-
supervisedlabelling[
14
]wereusingad-hocsemanticglobal
descriptorslikeGIST[
27
],weextractactivationmapsfrom
aCNNtrainedonimagesfromasetof
base
classesthatare
independentfromthe
novel
classesonwhichtheevaluation
isperformed.Seetheexperimentalsectionformoredetails
aboutthetrainingprocessfordescriptors.
Themeanclassintroducedforlow-shotlearn-
ing[
26
]isanotherwaytoperformdimensionalityreduc-
tionwhileimprovingaccuracythankstoabettercompari-
sonmetric.Wedonotconsiderthisapproachsinceitcan
beseenaspartofthedescriptorlearning.
3.2.matrix:approximatekNNgraph
Asdiscussedintherelatedwork,mostdiffusionpro-
cessesuseasinputthekNNgraphrepresentingthe
N

N
sparsesimilaritymatrix,denotedby
W
,whichconnects
the
N
imagesofthecollection.Webuildthisgraphus-
ingapproximatek-nearestneighborsearch.Thankstore-
centadvancesinefsimilaritysearch[
10
,
21
],trading
someaccuracyagainstefydrasticallyimprovesthe
graphconstructiontime.Asanexample,withtheF
AISS
library[
21
],buildingthegraphassociatedwith600kim-
agestakes2minuteson1GPU.Inourpreliminaryexperi-
ments,theapproximationintheknn-graphdoesnotinduce
anysub-optimality,possiblybecausethediffusionprocess
compensatestheartifactsinducedbytheapproximation.
Differentstrategiesexisttosettheweightsoftheaf
itymatrix
W
.Wechoosetosearchthe
k
nearestneighbors
ofeachimage,andseta1foreachoftheneighborsinthe
correspondingrowofasparsematrix
W
0
.Thenwesym-
metrizethematrixbyaddingittoitstranspose.Wesubse-
quently
`
1
-normalizetherowstoproduceasparsestochas-
ticmatrix:
W
=
D

1
(
W
>
0
+
W
0
)
,with
D
thediagonal
matrixofrowsums.
Thehandlingforthetestpointsisdifferent:testpoints
donotparticipateinlabelpropagationbecauseweclassify
eachofthemindependentlyoftheothers.Therefore,there
arenooutgoingedgesontestpoints;theyonlygetincoming
edgesfromtheir
k
nearestneighbors.
3.3.Labelpropagation
Wenowgivedetailsaboutthediffusionprocessitself,
whichissummarizedinFigure
1
.Webuildonthestraight-
forwardlabelpropagationalgorithmof[
38
].Thesetof
imagesonwhichweperformdiffusioniscomposedof
n
L
labelled
seed
imagesand
n
B
unlabelled
background
images
(
N
=
n
L
+
n
B
).the
N

C
matrix
L
,where
C
isthe
numberofclassesforwhichwewanttodiffusethelabels,
i.e.
,thenewclassesnotseeninthetrainingset.Eachrow
l
i
in
L
isassociatedwithagivenimage,andrepresentsthe
probabilitiesofeachclassforthatimage.Agivencolumn
correspondstoagivenclass,andgivesitsprobabilitiesfor
eachimage.Themethodinitializes
l
i
toaone-hotvectorfor
theseeds.Backgroundimagesareinitializedwith0proba-
bilitiesforallclasses.Diffusingfromtheknownlabels,the
methoditeratesas
L
t
+1
=
WL
t
.
Wecanoptionallyresetthe
L
rowscorrespondingto
seedstotheir1-hotground-truthateachiteration.When
iteratingtoconvergence,all
l
i
wouldeventuallyconverge
totheeigenvectorof
W
withlargesteigenvalue(whennot
resetting),ortotheharmonicfunctionwithrespectto
W
withboundaryconditionsgivenbytheseeds(whenreset-
ting).Empirically,forlow-shotlearning,weobservethat
resettingisdetrimentaltoaccuracy.Earlystoppingper-
formsbetterinbothcases,sowecross-validatethenumber
ofdiffusioniterations.
decision&combinationwithlogisticre-
gression
Wepredicttheclassofatestexample
i
asthe
thecolumnthatmaximizesthescore
l
i
.SimilartoZhou
etal.
[
37
],wehavealsooptimizedalossbalancingthe
tingconstraintwiththediffusionsmoothingterm.However
wefoundthatasimplelatefusion(weightedmeanoflog-
probabilities,parametrizedbyasinglecross-validatedco-
efofthescoresproducedbydiffusionandlogistic
regressionachievesbetterresults.
3.4.Variations
Usingpriors
Thelabelpropagationcantakeintoaccount
severalpriorsdependingontheassumptionsoftheproblem,
whichareintegratedbyanormalizationoperator

andbymodifyingtheupdateequationas
L
t
+1
=

(
WL
t
)
:
(1)
Multiclassassumption.
Forinstance,intheILSVRCchal-
lengebuiltupontheImagenetdataset[
31
],thereisonlyone
labelperclass,thereforewecan

asafunctionthat
`
1
-normalizeseachrowtoprovideadistributionoverla-
bels(byconventionthenormalizationleavesall-0vectors
unchanged).
Classfrequencypriors.
Additionally,wepointoutthatla-
belsareevenlydistributedinImagenet.Ifwetranslatethis
setuptooursemi-unsupervisedsetting,itwouldmeanthat
wemayassumethatthedistributionoftheunlabelledim-
agesisuniformoverlabels.Thisassumptioncanbetaken
intoaccountby

asthefunctionperforminga
`
1
normalizationofcolumnsof
L
.
Whileonecouldarguethatthisisnotrealisticingen-
eral,amorerealisticscenarioistoconsiderthatweknow
themarginaldistributionofthelabels,asproposedbyZhu
etal.
[
38
],whoshowthatthepriorcanbesimplyenforced
(
i.e.
,applycolumn-wisenormalizationto
L
andmultiply
eachcolumnbythepriorclassprobability).Thisarisesin
situationssuchastagprediction,ifwecanempiricallymea-
suretherelativeprobabilitiesoftags,possiblyregularized
forlowestvalues.
CombinedMulticlassassumptionandclassfrequencypri-
ors.
Weproposeavariantwaytousebothamulticlass
settingandpriorclassprobabilitiesbyenforcingthematrix
L
tojointlysatisfythefollowingproperties:
L1
C
=
1
N
1
>
N
L
/
p
C
(2)
where
p
C
isthepriordistributionoverlabels.Forthispur-
pose,weadoptastrategysimilartothatofCuturi[
9
]in
hisworkonoptimaltransport,inwhichheshowsthatthe
Sinkhorn-Knoppalgorithm[
32
]providesanefand
theoreticallygroundedwaytoprojectamatrixsothatitsat-
suchmarginals.TheSinkhorn-Knoppalgorithmiter-
atesbyalternatelyenforcingthemarginalconditions,as
L
 
L
diag(
L1
C
)

1
diag(
p
C
)
(3)
L
 
diag(
1
>
N
L
)

1
L
(4)
untilconvergence.Hereweassumethatthealgorithmonly
operatesonrowsandcolumnswhosesumisstrictlyposi-
tive.AsdiscussedbyKnight[
24
],theconvergenceofthis
algorithmisfast.Thereforewestopafter5iterations.This
projectionisperformedaftereachupdatebyEqn.
1
.Note
thatZhu
etal.
[
38
]solelyconsideredthesecondconstraint
inEqn.
2
,whichcanbeobtainedbyenforcingtheprior,as
discussedbyBengio
etal.
[
3
].Weevaluatebothvariantsin
theexperimentalsection
4
.
Non-linearupdates.
TheMarkovClustering(MCL)[
13
]
isanotherdiffusionalgorithmwithnonlinearupdatesorig-
inallyproposedforclustering.Incontrasttotheprevious
algorithm,MCLiteratesdirectlyoverthesimilaritymatrix
as
W
0
t
 
W
t

W
t
W
t
+1
 

r
(
W
0
t
)
;
(5)
where

r
isanelement-wiseraisingtopower
r
ofthema-
trix,followedbyacolumn-wisenormalization[
13
].The
power
r
2
(1
;
2]
isabandwidthparameter:when
r
ishigh,
smalledgesquicklyvanishalongtheiterations.Asmaller
r
preservestheedgeslonger.Theclusteringisperformed
byextractingconnectedcomponentsfromthematrix.
InSection
4
weevaluatetheroleofthenon-linearupdate
ofMCLbyintroducingthe

r
non-linearityinthediffu-
sionprocedure.Moreprecisely,wemodifyEquation
1
as
L
t
+1
=
r
(

(
WL
t
))
:
3.5.Complexity
Forthecomplexityevaluation,wedistinguishtwo
stages.Inthe
off-line
stage,
(i)
theCNNistrainedon
thebaseclasses,
(ii)
descriptorsareextractedfortheback-
groundimages,and
(iii)
aknn-graphiscomputedforthe
backgroundimages.Inthe
on-line
stage,wereceivetrain-
ingandtestimagesfromnovelclasses,
(i)
computefeatures
forthem,
(ii)
complementtheknn-graphmatrixtoinclude
thetrainingandtestimages,and
(iii)
performthediffusion
iterations.Hereweassumethatthe
N

N
graphmatrix
W
0
isdecomposedinfourblocks
W
0
=

W
LL
W
LB
W
BL
W
BB

2f
0
;
1
g
(
n
L
+
n
B
)

(
n
L
+
n
B
)
(6)
Thelargestmatrix
W
BB
2f
0
;
1
g
n
B

n
B
iscomputedoff-
line.On-linewecomputethethreeothermatrices.Wecom-
bine
W
BL
and
W
BB
bymergingsimilaritysearchresult
lists,henceeachrowof
W
0
containsexactly
k
non-zero
values,requiringtostorethedistancesalongwith
W
BB
.
Wearemostlyinterestedinthecomplexityoftheon-
linephase.Thereforeweexcludethedescriptorextraction,
whichisindependentofthecomplexity,and
thecomplexityofhandlingthetestimages,whichisnegli-
giblecomparedtothetrainingoperations.Weconsiderthe
logisticregressionasabaselineforthecomplexitycompar-
ison:
Logisticregression
theSGDtrainingentails
O
(
I
logreg

B

C

d
)
multiply-adds,with
d
denotesthedescrip-
tordimensionalityand
C
thenumberofclasses.The
numberofiterationsandbatchsizeare
I
logreg
and
B
.
Diffusion
thecomplexityisdecomposedinto:comput-
ingthematrices
W
LL
,
W
LB
and
W
BL
,whichin-
volves
O
(
d

n
L

n
B
)
multiply-addsusingbrute-
forcedistancecomputations;andperforming
I
dif
it-
erationsofsparse-densematrixmultiplications,which
incurs
O
(
k

N

C

I
dif
)
multiply-adds(note,sparse
matrixoperationsaremorelimitedbyirregularmem-
oryaccesspatternsthanarithmeticoperations).There-
forethediffusioncomplexityislinearinthenumber
ofbackgroundimages
n
B
.Seethesupplementalfor
moredetails.
Memoryusage.
Oneimportantbottleneckofthealgo-
rithmisitsmemoryusage.Thesparsematrix
W
0
occu-
pies
8
Nk
bytesinRAM,and
W
almosttwicethisamount,
becausemostnearestneighborsarenotreciprocal;the
L
matrixis
4
CN
bytes.Fortunately,theiterationscanbeper-
formedonecolumnof
L
atatime,reducingthisto
2

4
N
bytesfor
L
t
and
L
t
+1
(inpractice,whenmemoryisanis-
sue,wegroupcolumnsbybatchesofsize
C
0
<C
).
4.Experiments
4.1.Datasetsandevaluationprotocol
Weuse
Imagenet
2012[
11
]andfollowarecent
setup[
16
]previouslyintroducedforlow-shotlearning.The
1000Imagenetclassesaresplitrandomlyintotwogroups,
eachcontainingbaseandnovelclasses.Group1(193base
and300novelclasses)isusedforhyper-parametertuning
andgroup2(196+311classes)fortestingwithedhyper-
parameters.WeassumethefullImagenettrainingdatais
availableforthebaseclasses.Forthenovelclasses,only
n
imagesperclassareavailablefortraining.Similarto[
16
]
thesubsetof
n
imagesisdrawnrandomlyandtherandom
selectionisperformed5timeswithdifferentrandomseeds.
Asalargesourceofunlabelledimages,weusethe
YFCC100M
dataset[
33
].Itconsistsof99millionrepre-
sentativeimagesfromtheFlickrphotosharingsite
1
.Note
thatsomeworkshaveusedthisdatasetwithtagsorGPS
metadataasweaksupervision[
22
].
Learningtheimagedescriptors.
Weusethe50-layer
ResnettrainedbyHariharan
etal.
[
16
]onallbaseclasses
(group1+group2),toensurethatthedescriptioncalcu-
lationhasneverseenanyimageofthenovelclasses.We
runtheCNNonallimages,andextracta2048-dimvec-
torfromthe49thlayer,justbeforethelastfullyconnected
layer.Thisdescriptorisuseddirectlyasinputforthelogis-
ticregression.Forthediffusion,wePCA-reducethefea-
turevectorto256dimensionsandL2-normalizeit,which
isstandardinpriorworksonunsupervisedimagematching
withpre-learnedimagerepresentations[
2
,
34
].
Performancemeasureandbaseline
Inagivengroup(1
or2),weclassifytheImagenetvalidationimagesfrom
both
thebaseandnovelclasses,andmeasurethetop-5accu-
racy.Thereforetheclassdistributionisheavilyunbalanced.
Sincetheseedimagesaredrawnrandomly,werepeatthe
randomdraws5timeswithdifferentrandomseedsandav-
eragetheobtainedtop-5accuracy(the

xx
notationgives
thestandarddeviation).
Thebaselineisalogisticregressionappliedonthela-
belledpoints.Weemployaper-classimagesamplingstrat-
egytocircumventtheunbalancednumberofexamplesper
class.Weoptimizethelearningrate,batchsizeandL2reg-
ularizationfactorofthelogisticregressiononthegroup1
1
Ofthe100Moriginalsomearevideosandsomearenotavailable
anymore.Wereplacethemwithuniformwhiteimages.
background
noneF1MImagenet
edgeweighting
constant
62.7

0.68
65.4

0.55
73.3

0.72
Gaussianweighting*
62.7

0.66
65.4

0.58
73.6

0.71
meaningfulneighbors*
62.7

0.68
40.0

0.20
73.6

0.62

operator
none
40.6

0.18
41.1

0.10
42.3

0.19
Sinkhorn
61.1

0.69
56.8

0.50
72.3

0.72
column-wise
62.7

0.68
65.4

0.55
73.3

0.72
non-lineartransform*

r
62.7

0.68
65.4

0.55
73.3

0.72
classfrequencyprior*
62.7

0.66
65.4

0.60
73.3

0.65
Table1.Variationsonweightingforedgesandnormalization
stepsoniteratesof
L
.Thetestsareperformedfor
n
=2
and
k
=30
,with5runsonthegroup1validationimages.Variants
thatrequireaparameter(
e.g.
,the
˙
oftheGaussianweighting)are
indicatedwithaﬁ*ﬂ.Inthiscasewereportonlythebestresult,
seethesupplementarymaterialforfullresults.Intherestofthe
paper,weusethevariantsindicated
inbold
,sincetheyaresimple
anddonotaddanyparameter.
images.Itisworthnoticingthatourbaselineoutperforms
thereportedstateoftheartinthissetting.
Backgroundimagesfordiffusion
Weconsiderthefol-
lowingsetsofbackgroundimages:
1.
None:
thediffusionisdirectlyfromtheseedimagesto
thetestimages;
2.
In-domainsetting:
thebackgroundimagesaretheIm-
agenettrainingimagefromthenovelclasses,butwith-
outlabels.Thiscorrespondstoausecasewhereall
imagesareknowntobelongtoasetofclasses,but
onlyasubsetofthemhavebeenlabelled;
3.
Out-of-domainsetting:
the
n
B
backgroundimagesare
takenfromYFCC100M.Wedenotethissettingby
F100k,F1M,F10MorF100M,dependingonthenum-
berofimagesweuse(
e.g.
,wenoteF1Mfor
n
B
=
10
6
).Thiscorrespondstoamorechallengingsetting
wherewehavenopriorknowledgeabouttheimage
usedinthediffusion.
4.2.Parametersofdiffusion
Wecompareafewsettingsofthediffusionalgorithmas
discussedinsection
3.4
.Inallcases,wesetthenumberof
nearestneighborsto
k
=30
andevaluatewith
n
=2
.The
nearestneighborsarecomputedwithFaiss[
21
],usingthe
IVFFlatindex.Itcomputesexactdistancesbutoccasionally
missesafewneighbors.
Graphedgeweighting.
Weexperimentedwithdifferent
weightingsfor
W
0
,thatwereproposedintheliterature.We
comparedaconstantweight,aGaussianweighting[
25
,
3
],
Figure2.performancewith
n
=2
,withvarious
settingsof
k
and
n
B
,orderedbytotalnumberofedges(averageof
5testruns,withcross-validatednumberofiterations).
Hariharanlogistic
in-domaindiffusion
n
etal.
[
16
]regression
diffusion+logistic
1
63.660.4

0.78
69.7

0.86
69.76

0.88
2
71.568.8

0.82
75.4

0.64
75.60

0.69
5
80.079.1

0.35
79.9

0.17
81.35

0.22
10
83.383.4

0.16
82.1

0.14
84.56

0.12
20
85.286.0

0.15
83.6

0.12
86.72

0.09
Table2.
In-domaindiffusiononImagenet
:Wecompareagainst
logisticregressionandarecentlow-shotlearningtechnique[
16
]
onthisbenchmark.Resultsarereportedwith
k
=30
fordiffusion.
(with
˙
ahyper-parameter),andaweightingbasedonthe
ﬁmeaningfulneighborsﬂproposal[
28
].
Table
1
showsthatresultsareremarkablyindependentof
theweightingchoice,whichiswhywesetitto1
2
.Thebest
normalizationthatcanbeappliedtothe
L
matrixisasimple
column-wiseL1normalization.Thankstothelinearitera-
tionformula,itcanbeappliedattheendoftheiterations.
4.3.Lardiffusion
Figure
2
reportsexperimentsbyvaryingthenumberof
backgroundimages
n
B
andthenumber
k
ofneighbors,for
n
=2
.Allthecurveshaveanoptimalpointintermsofac-
curacy
vs
computationalcostat
k
=30.Thismaybeaintrin-
sicpropertyofthedescriptormanifold.Anadditionalnum-
ber:beforestartingthediffusioniterations,with
k
=1000
andnobackgroundimages(thebestsetting)weobtainan
accuracyof60.5%.Thisisaknn-clasandthisisthe
fastestsettingbecausetheknn-graphdoesnotneedtobe
constructednorstored.
4.4.Comparisonwithlo
Wecomparetheperformanceofdiffusionagainstthelo-
gisticbaselineandarecentmethodofthestateof
theart[
16
],usingthesamefeatures.
In-domainscenario.
Forlow-shotlearning(
n

5
),the
in-domaindiffusionoutperformstheothermethodsbya
largemargin,seeTable
2
.Thecombinationwithlogistic
regressionisnotveryeffective.
Out-of-domaindiffusion.
Table
3
showsthattheperfor-
manceofdiffusioniscompetitiveonlywhen1or2images
areavailableperclass.AsstatedinSection
3.2
,wedonot
includethetestpointsinthediffusion,whichisstandard
foraclassisetting.However,ifweallowthis,as
inafullytransductivesetting,weobtainatop-5accuracy
of
69.6
%

0.68
with
n
=2
withdiffusionoverF1M,
i.e.
,on
parwithdiffusionoverF100M.
combination.
Weexperimentedwithavery
simplelatefusion:tocombinethescoresofthetwoclas-
wesimplytakeaweightedaverageoftheirpredic-
tions(log-probabilities),andcrossvalidatetheweightfac-
tor.Bothinthein-domain(Table
2
)andout-of-domain(Ta-
ble
3
)cases,theresultsareabovethebestof
thetwoinputThisshowsthatthelogisticre-
gressionclassandthediffusionaccessdif-
ferentaspectsofimagecollection.Wealsoexperimented
withmorecomplicatedcombinationmethods,likeusingthe
graphedgesasaregularizerduringthelogisticregression,
whichdidnotimprovethisresult.
Comparisonwiththestateoftheart.
Withthein-
domaindiffusion,wenoticethatourmethodoutperforms
thestate-of-the-artresultof[
16
]andwhich,itself,outper-
formsoriscloselycompetitivewith[
35
,
36
]inthissetting.
Intheout-of-domainsetting,outresultsarebetteronlyfor
n
=1.However,theirmethodisacomplementarycombi-
nationofalossandalearneddataaugmentation
procedurethatistailoredtotheexperimental
setupwithbaseandnovelclasses.Incontrast,ourdiffusion
procedureisgenericandhasonlytwoparameters(
n
B
and
k
).Notethattheout-of-domainsettingiscomparablewith
thestandardlow-shotsetting,becausetheunlabeledimages
fromF100Maregeneric,andhavenothingtodowithIma-
genet;andbecausetheneighborconstructionanddiffusion
areefenoughtoberunonasingleworkstation.
2
Notethatourparametricexperimentsusethesetofbaselineimage
descriptorsusedinthearXivversionof[
16
],andthetablecomparesall
methodsusingthoseunderlyingfeatures,sotheresultsarenotdirectly
comparablewiththerestofthepaper.
out-of-domaindiffusion
logistic
diffusion+logistic
Hariharan
n
noneF1MF10MF100M
regression
+F10M+F100M
etal.
[
16
]
1
58.5

0.52
61.4

0.61
62.7

0.76
63.6

0.61
60.4

0.78
63.3

0.73
64.0

0.70
63.6
2
63.6

0.60
66.8

0.71
68.4

0.74
69.5

0.60
68.8

0.82
70.6

0.80
71.1

0.82
71.5
5
69.0

0.46
72.5

0.27
74.0

0.35
75.2

0.40
79.1

0.35
79.4

0.34
79.7

0.38
80.0
10
73.9

0.15
76.2

0.19
77.4

0.31
78.5

0.34
83.4

0.16
83.6

0.13
83.9

0.10
83.3
20
78.0

0.15
79.1

0.23
80.0

0.27
80.8

0.18
86.0

0.15
86.2

0.12
86.3

0.17
85.2
Table3.
Out-of-domaindiffusion:
Comparisonoffordifferentvaluesof
n
,with
k
=30
forthediffusionresults.Theﬁnoneﬂ
columnindicatesthatthediffusionsolelyreliesonthelabelledimages.Theresultsoftherightmostcolumn[
16
]arestate-of-the-artonthis
benchmarktoourknowledge,generallyoutperformingtheresultsofmatchingnetworksandmodelregression[
35
,
36
]inthissetting.
background
noneF1MF10MF100M
optimaliteration
2345
timing:graphcompletion
2m57s8m36s40m41s4h08m
timing:diffusion
4.4s19s3m44s54m
Table4.Timingsforthedifferentstepsona24-core2.5GHz
machine,foravaryingnumberofunlabelledimagesfrom
YFCC100M.Note,thetimingof4h08mforgraphcompletionover
F100Mtakesonly23mwhenexecutedon8GPUs.
4.5.Complexity:Runtimeandmemory
Wemeasuredtherun-timesofthedifferentstepsin-
volvedindiffusionprocessandreporttheminTable
4
.The
graphconstructiontimeislinearin
n
B
,thankstothepre-
computationofthegraphmatrixforthebackgroundimages
(seeSection
3.5
).Forcomparison,trainingthelogisticre-
gressiontakesbetween2m27sand12m,dependingonthe
cross-validatedparameters.
Intermsofmemoryusage,thebiggestF100Mexperi-
mentsneedtosimultaneouslykeepinRAMa
W
matrix
of5.3billionnon-zerovalues(39.5GiB),and
L
t
and
L
t
+1
(35.8GiB,usingslicesof
C
0
=96
columns).Thisisthe
maindrawbackofusingdiffusion.HoweverTable
3
shows
thatrestrictingthediffusionto10millionimagesalready
providesmostofthegain,whiledividingbyanorderof
magnitudememoryandcomputationalcomplexity.
4.6.Analysisofthediffusionprocess
Wediscusshowfast
L
upﬂ(itisdenseafterafew
iterations).Weconsidertherateofnodesreachedbythe
diffusionprocess:weconsiderverylargegraphs,fewseeds
andarelativelysmallgraphdegree.Figure
3
measuresthe
sparsityofthematrix
L
(ononerunofvalidation),whichin-
dicatestherateof(label,image)tuplesthathavenotbeenat-
tainedbythediffusionprocessateachdiffusionstep.While
thegraphisnotnecessarilyfullyconnected,weobservethat
mostimagescanbereachedbyalllabelsinpractice.
Thefractionofnodesreachedbyalllabeledpointsgrows
rapidlyandconvergestoavaluecloseto1inafewiter-
ationswhen
k

10
.Inordertorelatethisobservation
withtheperformanceattainedalongiterations,itisinterest-
ingtocomparewhathappensinthisplottotheoneonthe
Figure3.Statisticsoveriterations,for
n
=2
.
Top
:Rateofnon-
zeroelementinthematrix
L
.
Bottom
:correspondingaccuracy.
right.Theplotontherightshowsthattheiterationnumber
atwhichthematrixcloseto1issimilartotheiterationat
whichaccuracyismaximal,asselectedbycross-validation.
Themaximumoccurslaterif
n
B
islargerandwhen
k
is
smaller.Notealsothatearlystoppingisimportant.
4.7.Qualitativeresults
Figure
4
showspathsbetweenaseedimageandtestim-
ages,whichgivesapartialviewofthediffusion.Givena
class,webacktrackthepath:foragivennode(image)and
iteration
i
,welookuptheprecedingnodethatcontributed
mosttotheweightin
L
i
thatnodeatthatiteration.Atit-
eration0,thebacktrackingprocessalwaysendsinasource
node.Eachrowoftheisonesuchpaths.Foratest
image(right),weshowthepathfortheground-truthclass
triumphalarch
BrianJ.Geiger
miguel77
Melirius
Melirius
eyllom
Tab59
(triumphalarch)
jack-o'-lantern
EricMMartin
jack-o'-lantern
UrbanSeaStar
breathedontbreathe
dalylab
abroadGuille
(jack-o'-lantern)
planetarium
pedromf
planetarium

kokeshi
jokke75
DexterPerrin
(planetarium)
mosque
mosque
mosque
snarlenarlen
mosque
NikaSmetana
qiv
(planetarium)
woolen
rchach
woolen
ladybugh
Swiv
sock
AmberStrocel
(woolen)
sock
sock
sock
LindaN.
LindaN.
teaandcakes
Catrijn
(woolen)
Figure4.Imagesvisitedduringthediffusionprocessfromaseed(
l
eft)tothetestimage(
right
).Wegiveground-truthclassforImagenet
images(testimagesmarkedbyparentheses).Thetworowsarecorrectly.Thetwobottomonesarefailurecases.Imagenet
imagesarenotshownforcopyrightreasons,butthelabelsareshown.ForYFCC100Mimages,weprovidetheFlickridoftheircreators.
andthatforthefoundclass,orasinglerowforbothwhen
theimageiscorrectly.Notethatthepreceding
nodecanbetheimageitself,sincethediagonalofthe
W
matrixisnotsetto0.Thankstothesizeofthedataset,the
pathsareﬁsmoothﬂ:theyevolvethroughsimilarimages.
5.Conclusion
Weexperimentedonlarge-scalelabelpropagationfor
low-shotlearning.Unsurprisingly,wehavefoundthatper-
formingdiffusionoverimagesfromthesamedomainworks
muchbetterthanimagesfromadifferentdomain.We
clearlyobservethat,asthenumberofimagesoverwhich
wediffusegrows,theaccuracysteadilyimprove.Themain
performancefactoristhetotalnumberofedges,whichalso
reasonablythecomplexity.Wealsoreportneutral
resultsformostsophisticatedvariants,forinstanceweshow
thatedgeweightsarenotuseful.Furthermore,labeledim-
agesshouldbeincludedinthediffusionprocessandnotjust
usedassources,
i.e.
,notenforcedtokeeptheirlabel.
Themainoutcomeofourstudyistoshowthatdiffusion
overalargeimagesetissuperiortostate-of-the-artmethods
forlow-shotlearningwhenveryfewlabelsareavailable.
Interestingly,late-fusionwithastandardsresultis
effective.Thisshowsthecomplementaryoftheapproaches,
andsuggeststhatitcouldbecombinedwithforthcoming
methodsforlow-shortlearning.
Whenmorelabelsareavailable,simplelogisticre-
gressionbecomessuperiortothemethodswedescribe
(andtootherstateoftheartlow-shotlearningmethods).
However,wenotethattherearemanycircumstances
whereevenafewlabelsperclassaremorediftoget
thanbuilding(andthenkeeping)agraphoverunlabeled
data.Forexample,iftherearealargenumberofﬁtailﬂ
classeswhichwewillneedtoclassify,afewexamples
perclasscanmultiplytomanylabels.Inthesecases
diffusioncombinedwithlogisticregressionisthebest
method.Thecodetoreproduceourresultsisavailableat
https://github.com/facebookresearch/low-shot-with-diffusion
.
References
[1]
Y.Avrithis,Y.Kalantidis,E.Anagnostopoulos,andI.Z.
Emiris.Web-scaleimageclusteringrevisited.In
ICCV
,
2015.
[2]
A.Babenko,A.Slesarev,A.Chigorin,andV.Lempitsky.
Neuralcodesforimageretrieval.In
ECCV
,September2014.
[3]
Y.Bengio,O.Delalleau,andN.L.Roux.Labelpropaga-
tionandquadraticcriterion.InO.Chapelle,B.Sch
¨
olkopf,
andA.Zien,editors,
Semi-SupervisedLearning
,chapter11,
pages195Œ216.MITPress,Boston,2006.
[4]
Y.Bengio,J.Paiement,P.Vincent,O.Delalleau,N.L.Roux,
andM.Ouimet.Out-of-sampleextensionsforlle,isomap,
mds,eigenmaps,andspectralclustering.In
NIPS
,pages
177Œ184,2003.
[5]
L.Bertinetto,J.F.Henriques,J.Valmadre,P.Torr,and
A.Vedaldi.Learningfeed-forwardone-shotlearners.In
NIPS
,2016.
[6]
R.R.C.BoazNadler,StephaneLafonandI.G.Kevrekidis.
Diffusionmaps,spectralclusteringandreactioncoordinates
ofdynamicalsystems.Technicalreport,Arxiv,2008.
[7]
T.Chin,L.Wang,K.Schindler,andD.Suter.Extrapolating
learnedmanifoldsforhumanactivityrecognition.In
ICIP
,
pages381Œ384,2007.
[8]
M.ChoandK.M.Lee.Mode-seekingongraphsviarandom
walks.In
CVPR
,June2012.
[9]
M.Cuturi.Sinkhorndistances:Lightspeedcomputationof
optimaltransport.In
NIPS
,pages2292Œ2300,2013.
[10]
W.Dong,M.Charikar,andK.Li.Efk-nearestneigh-
borgraphconstructionforgenericsimilaritymeasures.In
WWW
,March2011.
[11]
W.Dong,R.Socher,L.Li-Jia,K.Li,andL.Fei-Fei.Ima-
genet:Alarge-scalehierarchicalimagedatabase.In
CVPR
,
June2009.
[12]
M.DonoserandH.Bischof.Diffusionprocessesforretrieval
revisited.In
CVPR
,pages1320Œ1327,2013.
[13]
A.J.Enright,S.VanDongen,andC.A.Ouzounis.Anef
cientalgorithmforlarge-scaledetectionofproteinfamilies.
Nucleicacidsresearch
,30(7),2002.
[14]
R.Fergus,Y.Weiss,andA.Torralba.Semi-supervisedlearn-
ingingiganticimagecollections.In
NIPS
,pages522Œ530,
2009.
[15]
G.H.GolubandC.V.Loan.
Matrixcomputations
.John
HopkinksUniversityPress,2013.
[16]
B.HariharanandR.Girshick.Low-shotvisualrecognition
byshrinkingandhallucinatingfeatures.In
ICCV
,2017.
[17]
B.HarwoodandT.Drummond.Fanng:Fastapproximate
nearestneighbourgraphs.In
CVPR
,2016.
[18]
A.Iscen,Y.Avrithis,G.Tolias,T.Furon,andO.Chum.Fast
spectralrankingforsimilaritysearch.In
CVPR
,June2017.
[19]
A.Iscen,G.Tolias,Y.Avrithis,T.Furon,andO.Chum.
Efdiffusiononregionmanifolds:Recoveringsmall
objectswithcompactCNNrepresentations.In
CVPR
,June
2017.
[20]
A.Jansen,G.Sell,andV.Lyzinski.Scalableout-of-sample
extensionofgraphembeddingsusingdeepneuralnetworks.
CoRR
,abs/1508.04422,2015.
[21]
J.Johnson,M.Douze,andH.J
´
egou.Billion-scalesimilarity
searchwithGPUs.
arXivpreprintarXiv:1702.08734
,2017.
[22]
A.Joulin,L.vanderMaaten,A.Jabri,andN.Vasilache.
Learningvisualfeaturesfromlargeweaklysuperviseddata.
In
ECCV
,2016.
[23]
Y.Kalantidis,L.Kennedy,H.Nguyen,C.Mellina,andD.A.
Shamma.Lohandbehold:Web-scalevisualsearch,recom-
mendationandclusteringusinglocallyoptimizedhashing.
arXivpreprintarXiv:1604.06480
,2016.
[24]
P.A.Knight.TheSinkhorn-Knoppalgorithm:convergence
andapplications.
SIAMJournalonMatrixAnalysisandAp-
plications
,30(1):261Œ275,2008.
[25]
F.LinandW.W.Cohen.Poweriterationclustering.In
ICML
,2010.
[26]
T.Mensink,J.Verbeek,F.Perronnin,andG.Csurka.Metric
learningforlargescaleimageGeneralizingto
newclassesatnear-zerocost.In
ECCV
,December2012.
[27]
A.OlivaandA.Torralba.Modelingtheshapeofthescene:
aholisticrepresentationofthespatialenvelope.
IJCV
,
42(3):145Œ175,2001.
[28]
D.Omercevic,O.Drbohlav,andA.Leonardis.High-
dimensionalfeaturematching:employingtheconceptof
meaningfulnearestneighbors.In
ICCV
,October2007.
[29]
J.PhilbinandA.Zisserman.Objectminingusingamatching
graphonverylargeimagecollections.In
ComputerVision,
Graphics&ImageProcessing
,2008.
[30]
S.RaviandH.Larochelle.Optimizationasamodelforfew-
shotlearning.In
ICLR
,April2017.
[31]
O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,
S.Ma,Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,
A.C.Berg,andL.Fei-Fei.ImageNetLargeScaleVisual
RecognitionChallenge.
IJCV
,115(3):211Œ252,2015.
[32]
R.SinkhornandP.Knopp.Concerningnonnegativematrices
anddoublystochasticmatrices.
PJournalofMathemat-
ics
,21:343Œ348,1967.
[33]
B.Thomee,D.A.Shamma,G.Friedland,B.Elizalde,K.Ni,
D.Poland,D.Borth,andL.-J.Li.YFCC100M:Thenew
datainmultimediaresearch.
CommunicationsoftheACM
,
59(2):64Œ73,2016.
[34]
G.Tolias,R.Sicre,andH.J
´
egou.Particularobjectretrieval
withintegralmax-poolingofCNNactivations.In
ICLR
,
2016.
[35]
O.Vinyals,C.Blundell,T.Lillicrap,andD.Wierstra.Match-
ingnetworksforoneshotlearning.In
NIPS
,2016.
[36]
Y.-X.WangandM.Hebert.Learningtolearn:Modelre-
gressionnetworksforeasysmallsamplelearning.In
ECCV
,
2016.
[37]
D.Zhou,O.Bousquet,T.N.Lal,J.Weston,and
B.Sch
¨
olkopf.Learningwithlocalandglobalconsistency.
In
NIPS
,volume16,pages321Œ328,2003.
[38]
X.Zhu,Z.Ghahramani,andJ.D.Lafferty.Semi-supervised
learningusingGaussianandharmonicfunctions.In
ICML
,2003.
[39]
X.ZhuandA.B.Goldberg.
IntroductiontoSemi-Supervised
Learning
.SynthesisLecturesonIntelligenceand
MachineLearning.Morgan&ClaypoolPublishers,2009.
Appendix
Wepresentseveraladditionalresultsanddetailstocom-
plementthepaper.Section
A
reportsanotherevaluation
protocol,whichrestrictstheevaluationtonovelclasses.
Sections
B
and
C
areparametricevaluations.Section
D
givessomedetailsaboutthegraphcomputation.
A.Evaluationresultsonnovelclasses
Inthemainpaper,weevaluatedthesearchperformance
onallthetestimagesfromgroup2.Theperformancere-
strictedtoonlythenovelclassesisalsoreportedinprior
work[
16
]usingacombinationofTable
5
shows
theresultsinthissetting.
Astobeexpected,theresultsreportedinthesetablesare
inferiortothoseobtainedinthesetupwherealltestimages
areThisisbecausethenovelclassesareharder
toclassifythanthebaseclasses.Otherwisetheordering
ofthemethodsispreservedandtheconclusionsidentical.
Thediffusioniseffectiveinthelow-shotregimeandis,by
itself,betterthanthestateoftheartbyalargemarginwhen
onlyoneexampleisavailable.Thecombinationwithlate
fusionoutperformsthestateoftheart,evenin
theout-of-domainsetup.
B.Detailsoftheparametricevaluation
Inthepaperwereportedresultsfortheedgeweighting
andgraphnormalizationwiththebestparametersetting.
Here,wereportresultsforallparameters
3
.Weevaluatethe
followingedgeweightings(Figure
5
,row):

Gaussianweighting.Theedgeweightis
e

x
2
=˙
2
with
x
thedistancebetweentheedgenodes.Notethat
˙
!
1
correspondstoaconstantweighting;

Weightingbasedontheﬁmeaningfulneighborsﬂpro-
posal[
28
].Itreliesonanexponentialofneighbor
3
Notethatourparametricexperimentsusethesetofbaselineimage
descriptorsusedinthearXivversionofthepaperbyBarath
etal.
[
16
],and
thecomparesallmethodsusingthoseunderlyingfeatures.Therefore
theresultsarenotdirectlycomparablewiththerestofthepaper.
distances.Foragivengraphnode,fortheneighbor
i
of
itslistofresults,theweightis
s
(1

e


)
k
,where
s
is
thedistance,remappedlinearlyto
[0
;
1]
sothatthe
neighborhas
s
=1
andthe
k
th
neighborhas
s
=0
.
Wevaryparameter

intheplot.
Wealsoreportresultsfordifferentnormalizationsofthe
matrix
L
.InFigure
5
(secondrow),wecompare:

Thenon-linear

r
normalization,allelementsof
L
are
raisedtoapower
r
.Wevarytheparameter
r
,and
r
=
1
correspondstotheidentitytransform;

Weclassifyallimagesinagraphwithalogisticre-
gression.Weusethepredictedfrequencyof
eachclassoverthewholegraph,andraiseittosome
power(theparameter)toreduceorincreaseitspeaki-
ness.ThischoiceisinspiredbytheMarkovClustering
Algorithm[
13
].Thisgivesanormalizationfactorthat
weenforceforeachcolumnof
L
,insteadofthedefault
uniformdistribution.
Theconclusionoftheseexperimentsisthatthesevari-
antsdonotimproveoverconstantweightsandastandard
diffusion,mostofthemhavinganeutraleffect.Therefore,
weconcludethatthediffusionprocessmostlydependson
thetopologyofthegraph.
C.Latefusionweights
Letdenoteby
l
logreg
i
and
l
dif
i
2
[0
;
1]
C
thedistributions
overclassesreturnedbythetwoforimage
i
.We
fusetheloglikehoodbyaweightedaverage,whichamounts
toretrievingthetop-5classpredictionasthosemaximizing
a
n
log(
l
logreg
ic
)+(1

a
n
)log(
l
dif
ic
)
;
(7)
where
a
n
istheoptimalmixingcoeffor
n
seedpoints,
asfoundbycross-validation.
Figure
6
showstheseoptimalmixingfactors.Sincethe
logisticregressionisbetteratclassifyingwithmanytraining
examples,theparameter
a
n
increaseswith
n
.
D.Computationofthe
W
0
blocks
Asstatedinthepaper,weneedtocomputethe4blocks
ofthematrix
W
0
:
W
0
=

W
LL
W
LB
W
BL
W
BB

2f
0
;
1
g
(
n
L
+
n
B
)

(
n
L
+
n
B
)
;
(8)
where,usually,
n
L
˝
n
B
.Eachblockrequirestoperform
a
k
-nearestneighborsearch.WeemploytheFaisslibrary
4
optimizedforthistask[
21
],anduseitasfollows:
4
http://github.com/facebookresearch/faiss
out-of-domaindiffusion
in-domain
logistic
combined
bestreported
n
noneF1MF10MF100M
(Imagenet)
regression
+F10M+F100M
results[
16
]
1
39.4

0.85
43.9

0.96
46.3

1.28
47.6

1.09
57.7

1.28
42.6

1.31
46.5

1.23
47.9

1.18
45.1
2
47.8

0.94
52.7

1.14
55.2

1.21
57.0

1.05
66.9

1.06
54.4

1.29
57.5

1.34
58.4

1.29
58.8
5
56.8

0.73
62.2

0.44
64.6

0.57
66.3

0.68
73.8

0.29
71.4

0.54
71.9

0.55
72.3

0.58
72.7
10
64.9

0.28
68.0

0.33
69.9

0.47
71.7

0.54
77.6

0.23
78.6

0.27
78.7

0.21
79.2

0.14
79.1
20
71.4

0.26
72.7

0.40
74.1

0.38
75.3

0.29
80.0

0.21
82.9

0.20
83.0

0.15
83.2

0.22
82.6
Table5.Comparisonoffordifferentvaluesof
n
,with
k
=30
forthediffusionresults,evaluating
onlyonnovelclasses
using
thetwodifferentsetsoffeaturesthatweconsider.
Gaussianweighting
Meaningfulneighborsmodel

r
normalization
Normalizationwithclassweights
Figure5.Evaluationofedgeweighting(top)andmatrixnormalizations(bottom)usedinthediffusion.Thecommonsettingsare:
k
=30
,
n
=2
,evaluationisaveragedover5runsonthevalidationset(group1),andweselectthebestiteration.

W
BB
2f
0
;
1
g
n
B

n
B
:weuseaFaissindexreferred
toasﬁIVFFlatﬂ.Theaccuracy-speedcompromiseis
controlledbyaparametergivingthenumberofin-
vertedlistsvisitedatsearchtime.Weadoptedarel-
ativelyhighprobesetting(256)toguaranteethatmost
oftheactualneighborsareretrieved.Withtherecom-
mendedsettingsofFaiss,thecomplexityofonesearch
isproportionalto
d
p
n
B
,sothetotalcomplexityis
O
(
dn
1
:
5
B
)
.Thisissuper-linearwithrespectto
n
B
,but
itisstillrelativelyef(seeourtimings)andper-
formedoff-line;

W
LB
2f
0
;
1
g
n
L

n
B
:were-usethesameindextodo
n
L
similaritysearchoperations,thistimeusingonly
O
(
dn
L
p
n
B
)
;

W
BL
2f
0
;
1
g
n
B

n
L
:weneedtoindexontheseed
imagedescriptors.Wefoundthatinpractice,con-
structinganindexontheseimagesisatbest1.4

faster
thanbrute-forcesearch.Therefore,weusebrute-force
searchinthiscase,whichifoforder
O
(
dn
B
n
L
)
;

W
BB
2f
0
;
1
g
n
L

n
L
:ithasanegligiblecomplexity.
Thefusionoftheresultlists
[
W
LL
W
LB
]
and
Figure6.Performanceasafunctionofthelatefusionweight
a
n
onthevalidation(group1)images,averagedover5runs.A
weightof0ispurediffusion,1ispurelogisticregression.The
mixingfactorsthatareselectedfortestareindicactedwithcircles.
[
W
BL
W
BB
]
toget
k
resultsperrowof
W
0
isdonein
asinglepassandinanegligibleamountoftime.Therefore
thedominantcomplexityis
O
(
dn
B
n
L
)
.Atypicalbreak-
downofthetimingsforF100Mis(inseconds):
Timings(s)
W
BB
W
LB
W
BL
W
LL
onCPU
Š65+27831200332
on8GPUs
25929732+645331
For
W
LB
wedecomposethetiminginto:loadingofthe
precomputedIVFFlatindex(andmovingittoGPUifap-
propriate)andtheactualcomputationoftheneighbors.
"
61,Adapting Neural Text Classification for Improved Software Categorization,http://arxiv.org/pdf/1806.01742v2.pdf,https://github.com/paqs2020/paqs2020,"AdaptingNeuralTextfor
ImprovedSoftwareCategorization
AlexanderLeClair,ZacharyEberhart,andCollinMcMillan
UniversityofNotreDame,DepartmentofComputerScience,NotreDame,IN46656
Email:
f
aleclair,zeberhar,cmc
g
@nd.edu
Abstract
ŠSoftwareCategorizationisthetaskoforganizing
softwareintogroupsthatbroadlydescribethebehaviorofthe
software,suchasﬁeditorsﬂorﬁscience.ﬂCategorizationplaysan
importantroleinseveralmaintenancetasks,suchasrepository
navigationandfeatureelicitation.Currentapproachesattempt
tocasttheproblemastexttomakeuseoftherich
bodyofliteraturefromtheNLPdomain.However,aswewill
showinthispaper,textalgorithmsaregenerally
notapplicableoff-the-shelftosourcecode;wefoundthatthey
workwellwhenhigh-levelprojectdescriptionsareavailable,but
sufferverylargeperformancepenaltieswhenclassifyingsource
codeandcommentsonly.Weproposeasetofadaptationsto
astate-of-the-artneuralclasalgorithm,andperform
twoevaluations:onewithreferencedatafromDebianend-
userprograms,andonewithasetofC/C++librariesthat
wehiredprofessionalprogrammerstoannotate.Weshowthat
ourproposedapproachachievesperformanceexceedingthatof
previoussoftwaretechniquesaswellasastate-of-
the-artneuraltexttechnique.
I.I
NTRODUCTION
SoftwareCategorizationisthetaskoforganizingsoftware
intogroupsthatbroadlydescribethebehaviorofthesoftware,
forexample,
sound
or
web
[1].Categorizationhaslongbeen
understoodtoplayanimportantroleinsoftwaremaintenance,
fortaskssuchashelpingprogrammerslocateprogramsin
largerepositoriesofcode[2],[3],[4],[5],identifyingfeatures
toprioritize[6],[7],andsimilarprogramstoone's
own.Forexample,McMillan
etal.
[8]describeasituationin
whichstakeholdersmustsimilarprogramsduringfeature
elicitationinacompetitivecommercialbiddingprocess.
Theproblemofsoftwarecategorizationisusuallyas
acoarse-grainedcategorizationofwholeprojects.Sometimes
thisprocessissupervised,suchastheassignmentofaset
ofknowntagstocoderepositories[2],[9]orcategoriesto
adatasetofprojects[4],[1].Unsupervisedproceduresare
popularaswell,withLDAbeingaparticularlyimportanttool
forgroupingsimilarprograms[10],[11],[12],[13],[14],
[15].Inarichvarietyofways,theusefulnessofautomatic
andcalculationofsimilarityofprogramsiswell
establishedintheliterature.
Alogicalchoiceinattemptingautomaticsoftwarecatego-
rizationistoturntotheexistingliteratureintext
Textisawell-studiedresearcharea,andquite
excellentresultsarereportedfortaskssuchassentiment
analysisandnewsarticle(seeSectionIII).Butas
wewillshowinthispaper,theseapproachesaregenerallynot
effectiveoff-the-shelfforsoftwareThereason,
inanutshell,isthevocabularyprobleminsourcecodethat
hasbeenobservedfordecades[16],[17],[18]:thetermsused
atalow-levelinsourcecodetendtovarymorethantermsin
textdocumentsduetoprogrammer-generatednames,
thewidespreaduseofabbreviationsandcompoundwordsthat
maybehardtoexpandorsplit,andthemeaningsthat
manywordshaveincodethataredifferentthantheirEnglish
meanings(e.g.button,free,object).Inshort,thelow-level
codedetailsdonotalignwithhigh-levelconcepts[16].
Therearesignspriortothispaperthatthevocabularyprob-
leminsoftwareaffectsautomaticcategorization.Arevealing
resultbyWang
et.al
[5]atICSM'13showedquitegood
performanceinplacingsoftwareintohierarchicalcategories.
Theresultisinformativebecausetheyachieveditbymining
thesoftware'sincludingonlineresourcesandothertext
dataŒtheydidnotrelysolelyonthesourcecode.Incontrast,
Linares
et.al
[1]reportedatICSM'11andEMSEfarlower
performancewhenusingattributesfromsourcecodeonly.
Notably,bothsetsofauthorsusedthebag-of-wordsmodel
andalinear,andadatasetderivedfromJavaprojects
downloadedfromSourceForge.(However,thereareafew
importantcaveatswediscussinSectionIII.)
Unfortunately,itisoftennotrealistictoassumethatsoft-
warehasanonlinealongtextdescription,or
canthigh-leveldocumentation.Newprojectsaddedtoanopen-
sourcerepositorymayhaveonlythesourcecodeavailable(a
problemGitHubhasfacedduringautomatictagging[19]),and
commercialrepositoriesmayhavemanyprojectswithonly
limitedrelatedtextinformationsuchaslegacycode[8].We
cannotassumethatwewillhavehigh-leveltextdataavailable
inthesesituations;ifwewishtocategorizeaprojectthat
doesnothavethisinformation,wewillhavetorelyonthe
knowledgeencodedintothesource.
Inthispaper,wepresentanapproachforusingpre-
trainedwordembeddingsinaneuralarchitecturewithseveral
adaptationsforsoftwarecategorization.Essentiallywhatwe
proposeisaprocedureforpre-trainingwordembeddingsthat
wedesignedtoencodethesimilaritybetweenlow-levelterms
inthesourcecodetohigh-leveltermsinprojectdescriptions.
Thesehigh-leveldescriptionsaresometimesavailableinlarge
repositoriesofprojects.Thenwebuiltaneural
architecturethatusesthewordembedding,andtrainedthis
architectureonalargecorpusofsoftwareprojects(
with
descriptions).Wethenusethemodeltoclassifyatestset
ofprojectsthathavesourceonly(
no
textdescriptions).
arXiv:1806.01742v2  [cs.SE]  15 Jun 2018Ourapproachachievesresultssuperiortoabag-of-
words/linearregression(BoW+LR)baselineusedbyrelated
work[5],[1]whentextdescriptionsareavailable,andexhibits
farbetterperformancewhenhigh-leveltextinformationis
notprovided.Wefoundthatthisimprovementisduetoour
proposedprocedureforconstructingwordembeddingsfrom
descriptionsandcodeonthetrainingset:performancewas
limitedwhenwetestedanembeddingtrainedonnon-software
data(e.g.Wikipedia)asiscommonintheNLPresearcharea.
Theembeddingwerecommendconnectswordsinproject
descriptionstotermsinthesourcecode.
Togiveageneralpictureoftheproblem,considerthat
thebaselineBoW+LRusedbypreviousworkachieved68%
precision64%recallinasix-categoryproblem
inthispaper,whentextdescriptionswereavailableŒresults
similartothosereportedatICSM'13[5].Butwhenonlycode
isavailable,baselineperformancedropsto43%precision33%
recall.Thebaselineisnotapplicabletocodeoff-the-shelf.
Weobservethesameproblemusingastate-of-the-arttext
basedonrecurrentandconvolutionalneuralnet-
works:usingawordembeddingpre-trainedonaverylarge
Englishcorpus(providedbyPennington
et.al
[20])that
hasbeenshowntoperformwellonnaturallanguage,we
found80%precision77%recallwhenprojectdescriptions
wereavailable.But,only53%precision46%recallwhen
onlycodewasavailable.Thereisanimprovementoverthe
BoW+LRbaselineonthetextdata,asweexpectgiventhe
neuraltechnique'sbirthplaceinNLPliterature.Butweobserve
onlymarginallyimprovedperformanceonthesamedatasetof
projects,whenweuseonlythesourcecode.
Theapproachweproposeinthispaperachieves86%
precision80%recallwhentextdescriptionsareavailable,
whichdropsto61%precision52%recallwhenusingonly
sourcecode.Thisisstillasubstantialdrop,andwedonot
claimtoﬁsolveﬂthevocabularyproblem.However,weview
theacross-the-boardimprovements,including6-18%increases
inprecisionandrecall,asprogresstowardsadapting
textNLPtechniquestoasoftwaremaintenanceproblem.
Topromotereproducibilityandfurtherstudy,wemakeour
entiredataset,implementation,andevaluationmaterialsand
resultsavailableviaanonlineappendix(seeSectionIX).
II.P
ROBLEMAND
O
VERVIEW
Wetargettheproblemofsoftwarecategorization.Inkeeping
withthetraditionalproblemaﬁprojectﬂmaybeone
programorasuiteofprogramsintendedtoperformaparticular
task,andweclassifytheseintooneofseveralcategories
bythesoftwarerepository.Tolimitthescopeofthis
onepaper,weexploreonlysingle-classwedo
notconsidermulti-classorhierarchicalcategorizationatthis
time.Also,wefocusoncategoriesbyarepository,
insteadofunsupervisedlabelgeneration.
Asolutiontothisproblemwouldhaveseveralapplications
intheshortrun,asdiscussedintheIntroduction.Butinthe
longrun,wearepositioningthispapertoadvancethestate-of-
the-artinautomaticprogramcomprehensiongenerally:much
effortinthesoftwaremaintenancesubareahasbeendedicated
toautomatedunderstandingofcodechanges,artifacts,etc.,
withthehopeofﬁteachingthecomputerﬂtorecognizehigh-
levelrationalesimilartowhatahumanmight,ratherthan
low-leveldetailsonly.Inaddition,thispapercontributestoan
ongoingdebateinSoftwareEngineeringresearchastowhether
neuralarchitecturesareanappropriatetoolgiventheunique
constraintspresentinSEdata[21],[22].Wearguethatthey
arefortheproblemofcategorization,especiallygiventheir
abilitytomodelembeddings.
Fig.1:Anoverviewofthispaper.
Thispaperhasfourmajorcomponentsasshowninthe
overviewinFigure1.First,inSectionIV,weprepareacorpus
comprisedofC/C++projectsfromtheDebian
packages
repository,totalingover1.5millionand6.6millionfunc-
tions.Therepositorycontainsaspecialcategorylabeledﬁlibsﬂ,
whichweremoveandannotatemanuallyforaseparateevalua-
tion.Next,inSectionV,wedescribeourcustom
approachthatisbasedonneuraltextalgorithms
describedinrelevantNLPliterature.Third,inSectionsVI
andVII,wepresentourevaluationofour
approachincomparisonwithanalternatesoftware
approach,aswellasrecentworkfromtheareaof
textFinally,inSectionsVIIIandIX,wepresent
anexampleillustratingtheintuitionbehindourresults,and
informationforreproducingourwork.
III.B
ACKGROUNDAND
R
ELATED
W
ORK
Thissectiondiscussesbackgroundtechnologiesincluding
neuraltextpluskeyrelatedwork.
A.RelatedWork
Theprimaryareaofrelatedworkissoftwarecategorization,
ofprojectsintocate-
goriesinasoftwarerepository.Oneexampleofclosely-related
workisbyLinares
etal.
[1].Basically,thepaperusesterms
andAPIcallsasattributestoclassifysoftwareprojectsintoone
of22categoriesinaSourceForgerepositoryofabout3,200
projects.Thepaperstudiedseveralalgorithms,butbyfarthe
bestresultwasachievedbyalinearusingthebag-of-
wordsmodel,afterselectingfeaturesusingexpectedentropy
loss.LimitingtheattributestoAPIcallsallowsbinariestobe
categorizedwithoutcode,butwasfoundtoimposeacoston
performanceversusanolderapproachthattreatssourcecode
astextandusestextalgorithms[4].However,we
cautionthattheresultsfromthosepapersarenotcomparable
toourworkinthispaper,becausethefeatureselectionphase
usingexpectedentropylosswasperformedontheentire
dataset,includingthetestset.Eventhoughthatmaybeuseful
foransweringinterestingresearchquestionsaboutsoftware,it
isnotareal-worldcategorizationscenariobecauseonewould
nothavethecategoriesofthetestsetonwhichtoperform
featureselection,priortomakingthepredictions.
Severalapproacheshavebeenexploredinthisvein,suchas
bySharma
etal.
[2],Kawaguchi
etal.
[14],Sandhu
etal.
[23],
Kim
etal.
[24],andYang
etal.
[25].Wang
etal.
[5]explore
hierarchicalcategorization,inwhichwholesoftwareprojects
areplacedintoahierarchye.g.
players
or
editors
within
the
video
category.Theoverallmessagefromthesepapersis
thatalinearusingabag-of-wordsmodelofsoftware
issuftoobtainreasonableperformance,whenhigh-
leveltextdescriptionsareavailable.Infact,thisconclusionis
supportedbyrelevantworkinNLP,thatBoW+LR(bag
ofwords+linearregression)isastrongoverallbaseline[26].
Aftercategorizationintocategories,unsuper-
visedapproachesarethenext-mostsimilarareaofrelated
work.Unsupervisedapproachesarequitepopular:Chen
et
al.
[27]presentathoroughsurveyofusingtopicmodelsto
organizesoftwareinsoftwarerepositoriessuchasautomated
Githubtagging[28],[12],andthisworkiscomplementaryto
worktocalculatesoftwaresimilarityintermsoffeaturesthe
softwareimplements[8],[29],[30],[31](seeacomparisonof
varioussimilaritymetricsbyRagkhitwetsagul
etal.
[32]).
Broadlyspeaking,thispaperisrelatedtotheof
automatedprogramcomprehensionforsoftwaremaintenance.
Sourcecodesummarization,forexample,seekstogenerate
andupdatedocumentationbyautomaticunderstandingof
softwarecomponents[33],[34],[35],[36].Topicssuchas
codesummarization,deobfuscationofsourcecode[37],bug
localization[38],[39],[40],featurelocation[41],traceability
linkrecovery[42]areall,atahighlevel,instancesofthe
conceptassignmentproblem[43]inautomaticprogramcom-
prehension.Adistinguishingfactorofthispaperisthatwe
targetcategorizationoffunctionsinsourcecodetoprovidea
moreviewsoftware,butwithoutconnectingcode
functionalitytoonlyafewprojects.
B.NeuralText
Neuraltexttechniquesareusuallythought
ofasincontrasttoclassicaltechniques.Inaclassicaltext
technique,afeatureengineeringstepisfollowed
byfeatureselectionandthentrainingofamachinelearning
algorithm[44],[45].Thefeatureengineeringandselection
phasesarecharacterizedbyhumandesigneffort,suchas
choosingaBagofWords(BoW)modelinwhicheachword
isafeature,andoutwordsbasedonentropyloss[1].
TherearemanychoicesforMLalgorithm,with,forexample,
LinearRegression(LR)beingquitepopular[44],[46].
Neuralalgorithmshaveinrecentyearsbeen
proposedasalternatives.Akeyadvantagetoneuralalgorithms
isthattheyavoidmuchofthefeatureengineering/selection
workthatisdependentoneducatedguessworkbythedesigner.
Instead,anetworklearnsthefeaturerepresentationcalleda
wordembedding.Thewordembedding,inanutshell,isa
vectorspaceinwhicheachwordisavectorofrealnumbers.
Inaneuralstructure,theembeddingcanbecreatedeithervia
asupervisedtrainingprocess,orviaanunsupervised,pre-
trainedwordembeddingbasedonwordco-occurrences[20].
Supervisedembeddingshavetheadvantageofcustomization
toaparticulardataset,butthenarelimitedtothatdataset.
Unsupervisedembeddingshavetheadvantagetolearnfroma
muchlargerandmorediversedataset(weuseunsupervised
embeddingslaterinthispaperforthisreason).
Theliteraturedescribesahugevarietyofneuralarchitec-
turesfortext[47],[48],[49].Ingeneralthough,
mostarchitecturesincludeconvolutionand/orrecurrentlayers
combinedindifferentwaysfordifferenttasks.Atahigh
level,aconvolutionlayer(andassociatedpoolingstrategy)
isusedtoselectwhichwordsarethemostimportant,whilea
recurrentlayerisintendedtocapturethesemanticsofaword
bycapturingthecontextinwhichawordappears(i.e.,the
otherwordstotherightorleftofawordinasentence).
Inadaptingneuralarchitecturestosoftwareengineering
data,itisworthnotingthatalargenumberofthetext
algorithmsinexistencehavebeendesignedfor
andevaluatedondatasetsofsentencesthathavebeenmanually
placedintoasmallsetofrelativelydistinctcategories.For
example,theproblemofsentimentanalysisisquitecom-
mon:sentencesfrome.g.moviereviewsarelabeledaseither
positiveornegative[50].Or,newsgroupcategoriessuchas
rec.motorcylcesversuscomp.sys.ibm.pc.hardware[51].Itis
notsurprisingthatperformancelevelsarereportedtobe
quitehigh,inthe96%+range[45],sinceaconvolutionlayer
willquicklypickoutwordslikeﬁexcellentﬂforthepositive
category,andarecurrentlayercandetectimportantword
orderssuchasﬁnotbadﬂversusﬁverybadﬂ,amongother
semanticfactors.
Thesoftwarecategorizationproblemisdifferent,because
1)theartifactstendtobemuchlongerthanasentence(in
termsofnumberofwords),2)thevocabularytendstobe
differentamongdifferentprojects(duetospecialized
names),and3)thecategoriestendtooverlapandareoftennot
polaroppositeslikenegativeandpositive.Softwarecategories
suchas
database
and
web
aretechnicallydistinct,but
arelikelytobecloselyboundtogetherinpractice,andare
notmutuallyexclusive.Forthesereasonsandothers,
thereisdebateastowhetherneuralarchitectures
areappropriateforsoftwareengineeringdataatall[21],
despitetheirpopularity[22].InSectionV,wewillpresent
ourargumentthataneuralapproachisanappropriatetoolfor
softwareandouradaptations.
IV.C
ORPUS
P
REPARATION
WepreparedacorpusfromasnapshotoftheDebian
packages
repositorytakenSeptember19,2017.Thesnap-
shotincludes24,598softwareprojects.Fromthese,we
projectsthatdidnothaveanyCorC++source(some
projectscontainsourceofmultiplelanguages,butwe
usedonlytheC/C++components;wechoseC/C++because
itwasthelargestlanguagefamilyintherepository),leaving
9804projects.UsingasuiteofC/C++analysisscriptsprovided
bytheauthorsof[52],weextractedallfunctions,and
functioncalls.Wetookthecategoryfortheprojectfromthe
DebianpackageThe
allowpackagemaintainerstoplaceaprojectinmorethan
onecategory,butinpracticewefoundthistobequiterare,
withonlyatinypercentofprojectshavingmultiplecategories.
Sincemultiplecategoriesperprojectwasunusual,wedecided
totreattheproblemassingle-class.Forthe
fewprojectswithmultiplecategories,wechoseonlythe
categorylistedintheTherewere75
categoriesinthedataset,thoughmostareverysmall.
Wethencreatedvedatasets:
FullHoldout/TestCorpus
Werandomlyselected320
projectsfromthe16largestcategories(twentyfromeach
category)toholdoutfromallotherdatasets.Weusethese
projectsexclusivelyasatestingsetinourexperiments.The
holdoutsetisneitherusedtocreatethewordembeddingnor
totraintheneuralarchitecture.Wekepttheholdouttoaround
5%ofthechallengecorpus,tolimitremovaloftrainingdata.
StandardTrainingCorpus
Weintendthestandardcorpus
tothecommonpracticeintextclasVery
widespreadpracticeinthetextcliteratureisto
paredownlarge,complexdatasetsintosmaller,simplerones
bykeepingonlythetopfewcategories.Forexample,work
co-authoredbyYannLeCunatNIPS'15[53],alreadywith
over450citations,selectsthefourlargestoutof28categories
inanEnglishnewscorpustoensureatleast31,900samples
percategory,andveoutofover100categoriesinChineseto
ensure112,000samplespercategory.Tofollowthispractice,
wecreateaﬁstandardcorpusﬂfromthelargestsixcategories
(excluding
libs
,whichwetreatseparately).Admittedlythe
choiceofsixissomewhatarbitrary,butthetopsixcategories
coverabout33%oftheprojects(excludinglibs),makinga
subset.Thesixcategoriesare:
admin
,
games
,
net
,
science
,
sound
,and
utils
.Thereare1,443,408
functionsover3,119projectsinthestandardcorpus.Tocreate
abalancedset,followinganundersamplingprocesssimilar
to[53],werandomlyselected150kfunctions(roughlythe
sizeofthesmallestcategory)fromeachcategory,foratotal
of900kfunctions.
ChallengeTrainingCorpus
Inreality,softwarecatego-
rizationtakesplaceoveralargerandmorediversesetof
categoriesthanareinthestandardcorpus.Thereisalsomuch
morenoise,asmorecategoriesoverlap(aprojectcouldbe
both
net
and
web
,forexample,eveniftheyonlylabeled
asonecategoryinthedataset),andthecategoriesvarymuch
moreinsize.Theresultisamuchmoredif
problem.Tothisrealisticsituation,weselectthetop16
categories(top17minus
libs
),whichcoversabout50%of
theprojects,toformaﬁchallengecorpus.ﬂTheremaining58
categoriesthatwedonotusecoveraround20%ofthedataset.
Inourview,categorizationofthe58smallestcategoriesistoo
farfromthecapabilitiesofcurrenttechnologyatthistime.
Thechallengecorpuscontains2,951,529functionsover5,958
projects.Fromtheseweselected60kfunctions(roughlythe
sizeofthesmallestcategory)fromeachcategoryforatotal
of960kfunctions.
LibsHoldout/TestCorpus
Werandomlyselected100
projectsfromthe
libs
categorytoserveasaholdout/testset
forexperimentsonthatcategory.Wekepttheﬁlibsholdoutﬂ
setrelativelysmall(around5%ofthetrainingset)tofacilitate
manualannotationandinspection.
LibsTrainingCorpus
The
libs
categoryintheDebian
packagesrepositoryisaspecialcategorybecauseitcontains
projectsthatareverylikelytoalsoexistinanothercategory,
eventhoughinpracticetheothercategoryisnotlistedinthe
repository.Forexample,aprojectmaynotonlybealibrary,
butalibraryfordecodingmp3soitcanbeconsidered
bothin
libs
and
sound
.Asaresult,
libs
isadiverse
andlargecategorythatincludesover30%ofprojects.For
thisreason,weseparate
libs
fromtherestoftherepository,
annotateitmanuallywiththecategoriesfromtherestofthe
project,andstudyhowwellourmodelthelibraries
ase.g.asoundlibrary.SeeSectionVI-Efordetails.
V.O
UR
A
PPROACH
Thissectiondescribesourapproach,includinghowwe
representfunctions,theionmodelweimplement,
ourrationale,andimplementationdetails.Inanutshell,our
approachisto1)representeachfunctionasavectorofintegers
andassignalabel,2)trainawordembeddingforcode,
3)trainaneuralalgorithmusingthefunction
representationsandlabelsfromthestep,and4)toclassify
awholeproject,weclassifyeachofthefunctionsfromthat
projectandthenapplyavotingmechanismtopredictalabel
fortheproject.
A.FunctionRepresentation
Werepresenteachfunctionasavectorofintegers.
Toobtainthisvector,wecreateatextstringof
thefunctionintheform:
projectNamefunctionName
functionContents
.Weconverttheprojectandfunction
namestolowercasebutdonotsplitorotherwisepreprocess
them.Forthefunctioncontents,wetakethecodeofthe
functionincludingcomments,convertnon-wordsymbolse.g.
bracketsandcommastospaces,andconverttolowercase.
Thenwesplittheentirestringonwhitespacetoobtainavector
oftexttokens(whichmayormaynotbeEnglishwords).Then,
wecreateanindexwhereeachtokenhasauniqueinteger(to
avoidcollisions,wedonotusethehashingtrick).Weusethe
indextocreateavectorofintegersfromeachvectoroftokens.
Ourrationaleforincludingtheprojectnameisdescribedin
thenextsection,asitisconnectedtoouruseofarecurrent
layer.Notehoweverthatinourexperiments,wedividethe
trainingandtestingsetsbyproject,
not
byfunction,toavoid
asituationwherethemodelsimplylearnswhichprojectname
belongsinwhichcategory(i.e.functionsfromprojectsinthe
trainingsetcannotbeinthetestingorvalidationsets).
Ourrationaleforincludingthefunctionnameistwofold.
First,thefunctionnameoftenincludesvaluablesemantic
information[18].Second,manyprojectsinourdatasetare
dependenciesofotherprojects,andinthissituationfunction
namesfromoneprojectwilloccurinthefunctioncontents
offunctionsfromotherprojects(sinceaprojectwilltypically
callfunctionsintheprojectsonwhichitdepends;preserving
thesecallrelationshipsisalsowhywedonotdoaggressive
splittingduringpreprocessing).
Thefunctionrepresentationaboveinvolvesinformationonly
fromthesourcecode.Werefertoitasthe
code-only
or
co
representation.TheDebianpackagesrepositorycontainsa
short(usuallyabout3sentences)highlevelprojectdescription
formostprojects.Wherethatdescriptionisavailable,we
appendittothecodeonlyrepresentationofeachfunctionto
createasecond,
code-description
or
cd
representation.
Thisdistinctionisimportantbecauseweusebothinthe
trainingset:wetrainourmodelusingboththe
cd
examples
andthe
co
examples,toimprovetheversatilityofthemodel.
Themodelwilllearntoclassifybothsituationswheretext
descriptionsareavailable,andwheretheyarenot.
B.TrainingaWordEmbedding
Weusethreepre-trainedwordembeddingsinthispaper.We
usedtheGloVewordembeddingtechnique[20].Duetospace
limitationswedefertothepaperbyPennington
et.al
[20]fora
completeexplanation,butessentiallywhatthetechniquedoes
iscreateavectorspaceinwhichwordsaremodeledasvectors,
andwordsthatcooccurwithinawindowsize(typicaliswithin
15words)areconsideredmoresimilarandthusmadetobe
closerinthevectorspace.Weuseda100dimensionvector
spaceforallthreeembeddingsinthispaper.
WikipediaEmbedding
Theembeddingweuseisthe
100d,6Btokenwordvectorsprovidedbytheauthorsofthe
originalGloVepaper.Thesevectorsweretrainedusinggeneral
textdatafromtheentirecorpusofsentencesonWikipedia.It
representsatypicalembeddingforuseinclassifyingtextdata.
Code-OnlyEmbedding
Thesecondsetofwordvectors
weusedisonethatwetrainedourselves.Usingawindow
sizeof200tokensmeasuredasleft-contextofeachword,
wecomputedawordembeddingusingeachfunctioninthe
trainingsetasaﬁsentence.ﬂOurideawastoproducean
embeddinginwhichtermsfromcodearenearby,ifthey
cooccurinfunctions.Asimilarideahasbeenproposedas
Python2Vec[54],exceptthatweincreasethewindowsizeto
200duetothelonglengthoffunctionscomparedtosentences.
Code-DescriptionEmbedding
Thethirdembeddingwe
usedisalsoonewetrainedourselves,andis,toourknowledge,
anovelstrategy.Essentiallywhatwedois
prepend
theproject
description(thesameoneusedtocreatethe
cd
function
representation)toeveryfunctionbeforecomputingtheword
vectors.Thedescriptionappearsbeforethefunctionineach
sequence,sothatthewordsfromthedescriptionwilloccurin
theleft-contextofthetermsinthefunction'scode.Theidea
isthatthetermsfromcodewillbenearinthevectorspace
towordsinthehighleveldescription.Ourintentisforthis
topartiallyaddressthevocabularyproblem,becauselow-level
codetermswillclusteraroundhigh-leveldescriptivewords.
C.NeuralModel
Formaximumclarityandreproducibility,wedescribeour
neuralmodelinthecontextoftheactualKeras
codethatwewrotetoimplementourmodel(Figure2).The
rapidproliferationofalargevarietyofneural
algorithmsavailablemakesitquitediftoselectaﬁsingle
bestﬂalgorithm,sowedesignedourownmodelthatcharac-
terizestheadvancementsthatseemedbroadlyeffectiveduring
ourliteraturereview(seeSectionIII-B),centeringaround
convolutionandrecurrentlayers.Ourmodelissimilartothe
C-LSTMmodelthatwasshowntohaveperformanceinline
withcompetitivemodelsonseveraltextdatasets[55].
a)OurModel:
Ourmodelconsistsofthefollowing:
EmbeddingLayer
Firstweuseanembeddinglayerinwhich
everytokenintheentirevocabularyisrepresentedasareal-
valuedvectoroflength
embed_dims
.Weusedoneofthe
pre-trainedembeddingsdescribedintheprevioussubsection,
dependingontheexperimentinfuturesections,thoughin
generalwerecommendusingthecode-descriptionembedding.
Notethatwedonoteliminatetokensthatoccurrarely,since
thesetokensmayhaveusefulsemanticinformation,andthe
convolutionlayershoulddeemphasizelesserimportanttokens
anyway.Weusedasequencelengthof60asacompromise
betweenmaximizinginformationavailabletothemodeland
minimizingmodelsizeinmemory.88%offunctionswere
shorterthanoursequencelength;wetruncatedlongerse-
quences.Theoutputoftheembeddinglayerisarepresentation
ofafunctionthatisan
X
by
Y
matrixwhere
X
isthesequence
lengthand
Y
isthenumberofembeddingdimensions.
ConvolutionLayer
Thenextlayerisaone-dimensional
convolutionlayer.Ourrationaleforusingthislayerissothat
themodelwouldlearnwhichtokensandﬁphrasesﬂoftokens
arethemostimportantindeterminingwhetherafunction
belongsinacategory.Intextconvolutionmay
phrasessuchasﬁquitewelltoindicateapositive
moviereview.Insoftwareaphrasecouldbee.g.
01model=Sequential()
02model.add(Embedding(vocab_size,embed_dims,
03weights=[embed_matrix],
04input_length=seq_len,trainable=False))
05model.add(Conv1D(filters,kernel_size,
06padding='valid',activation='relu',
07strides=strides))
08model.add(MaxPooling1D())
09model.add(LSTM(lstm_units))
10model.add(Dense(hide_u,activation='relu'))
11model.add(Dropout(dropout_level))
12model.add(Dense(num_categories,
13activation='softmax'))
14model.compile(
15loss='categorical_crossentropy',
16optimizer='adam',metrics=['accuracy'])
Fig.2:Kerascodeimplementingourmodel,includedfor
maximumclarityandreproducibility.AlsoseeSectionIX.
ﬁifnotmutedplaysoundﬂ(wedonotselectthesephrases,the
convolutionlayerlearnsthem,seeSectionVIII).
MaxPooling
Weusedamaximumpoolingstrategyto
downsampletheoutputoftheconvolutionlayerandfocuson
onlythemore-importanttokens.
LongShortTermMemory
Nextweusedarecurrentlayerto
capturethesemanticsoftokensintermsoftheorderinwhich
thetokensappearinthefunctionrepresentations.Weused
LSTMduetoitsabilitytocapturesemanticsoverarelatively
longsequence,whichisimportantbecauseimportanttokensin
ourfunctionrepresentationmaynotbeneareachotherinthe
sequence.Forexample,considerasituationwherethemodel
ﬁseesﬂafunctionrepresentationwithaprojectname
projA
followedlaterinthesequenceby
volume
,andthatfunction
isinthesoundcategory.Iflaterduringtrainingthemodel
seesafunctionwith
projA
followedby
audio
,themodel
willlearnnotonlythatvolumeandaudioaretokensassociated
withthelabelsound,butthatvolumeandaudioareassociated
witheachotherbecauseoftheirco-occurrencewithprojA.The
outputoftheLSTMlayerisanembeddingrepresentationof
eachfunctionasareal-valuedvectoroflength
lstm_units
(notethatthisembeddingisofthefunction,nottheword
embeddinglayerabove).
DenseHidden
Weuseafully-connectedhiddenlayerafter
therecurrentlayerfollowingthestandardprocedureofmany
neuralarchitectures,toprovidealayerforlearninghowthe
vectorrepresentationsfromLSTMbelongtowhichcategories.
Asheavilyrecommendedinrelatedliterature[56],weapply
dropoutasregularizationtoresistov
DenseOutput
Finally,weincludeanoutputlayer,after
whichafunctionisrepresentedasareal-valuedvectorof
length
num_categories
.Theindexofthehighestvalue
inthisvectoristhepredictedcategoryforthefunction.
b)ParameterTuning:
Weusedgridsearchtotunethe
parametersofourmodel.Weusedalltheprojectsinour
standardcorpustrainingset,creatingavalidationsetfrom
5%ofthestandardcorpusduringgridsearchtoavoidusing
ourholdout/testsetduringtuning,acrosstheeightparameters
listedbelow.Inanygivenrun,wepickthemodelthatperforms
bestonthevalidationset(Test-for-best-Validationstrategy).
Ultimately,wesettledonthefollowing
epochs
best-of-3
filters
250
kernel_size
3
strides
1
lstm_units
100
hide_u
512
dropout_level
0.5
optimizer
adamax
c)ImplementationDetails:
Weimplementedourtech-
niqueusingKeras2.1.2runningTensorFlow1.4.0.Hardware
includedanE5-1650v4CPUandtwoGeforce1070GPUs.
D.VotingMechanism
Weuseapluralityvotingmechanismtopredictaproject
categoryfromalistoffunctionlabelspredictedbyour
model.Theconceptisverysimple:aproject
getsthelabelassignedtothemostnumberofitsfunctions
(onefunctiononevote).Morecomplexvotingmechanisms,
suchasthosebasedonfunctionsizeorimportance[57],are
anareaoffuturework.
VI.E
VALUATION
Thissectiondescribesourevaluationofourapproach,start-
ingwithourresearchquestions,methodology,and
forthebaselineswechose.
A.ResearchQuestions
Amajormotivationforthispaperistheperformancepenalty
associatedwithusingtextalgorithmsonsource
codedata.Wedesignedourproposedapproachtoreducethis
penaltybyseveraladaptationsoftexttosource
code,andseektoquantifytheaffectoftheseadaptations.
Therefore,weaskthefollowingResearchQuestions(RQs):
RQ
1
Whatisthedifferenceinbaselineperformancefrom
thecasewhentextdescriptionsareavailable,towhen
onlysourcecodeisavailable?
RQ
2
Whatisthedifferenceinourproposedapproach's
performancefromthecasewhentextdescriptions
areavailable,towhenonlysourcecodeisavailable?
RQ
3
Whatistheeffectofthewordembeddingsonthe
performanceoftheneuralnetwork-basedapproach?
Tobeclear,ﬁourproposedapproachﬂmeansthe
neural-basedapproachwiththecode-descriptionembedding
(
nn+cd
).Thebaselinesaredescribedinthenextsubsection.
TherationalebehindRQ
1
isthatthebaselinetextclassi-
algorithmsweredesignedfornaturallanguagetext,
andmaynotbeapplicableoff-the-shelftosourcecodedue
tothevocabularyproblemwhichhaslongbeenrecognizedin
softwareengineeringresearch(seeSectionI).Therationale
behindRQ
2
isthatourproposedapproachisalsolikelyto
sufferaperformancepenaltywhenclassifyingcode-onlydata,
butweseektoquantifythispenaltyandcompareittothe
baselineperformance.OnewaytoviewRQ
1
andRQ
2
isas
establishingalikelylowerandupperboundforperformance
expectations:inareal-worldscenario,ifapractitionerhasa
setofprojectstoclassify,someprojectsmayhavetextdescrip-
tionswhileotherdonot.Finally,therationalebehindRQ
3
is
thatonekeydifferencebetweenourapproachandoff-the-shelf
textishowwecreateawordembedding,andwe
seektoquantifytheeffectofthatembedding.
B.Baselines
Weusetwobaselinesinthispaper:1)abagofwordslinear
regression(BoW+LR),and2)aneuralnetwork-based
usingawordembeddingtrainedonWikipedia.
WeusetheBoW+LRfortworeasons.First,a
consensushasformedinthetextliteraturethat
itisastrongbaselineacrossawidevarietyofdatasets[26].
Second,thetwomostclosely-relatedpapersonthetopicof
softwarecategorization([1]and[5])bothusealinear
andabagofwordsrepresentationofsoftware.Weuseda
vocabularysizeofthetop1.8kterms,whichwasthehighest
thatwouldintothe64gbofmemoryinourworkstation,and
isslightlymorethanthe100percategoryusedbyLinares
et.
al
[1].Inthepreviouswork,theprojectwasrepresentedasa
matrixofallwordsintheproject.However,inpilotstudies,
wefoundthatperformanceinthiswasextremely
low(10-15%F1-measure)whenEELwasnotappliedtothe
testingset(pleasenotecaveatsinSectionIII-A).Sotoprovide
thebestapples-to-applescomparison,weﬁswappedinﬂthe
BoW+LRalgorithmfortheNN-basedoneweproposeinour
approach:theBoW+LRalgorithmreceivedexactlythesame
trainingdataastheNNapproaches,includingourfunction
representationandourvotingmechanism.Thenwetested
usingthesametestingdata.Wefeltthatthissetupwould
isolatetheeffectsofthealgorithmfromotherfactorssuchas
pre-processinganddatarepresentation.
WeuseanNN-basedwiththeWikipedia-trained
wordembedding(
nn+w
,SectionV-B)asasecondbaseline.
Theneuralarchitectureweproposeusingisuniqueonlyin
smallways:ahugevarietyofneuraltextexists,
andasnotedinSectionV,webuiltanarchitecturethat
characterizesarchitecturesthathavebeenshowntohave
goodperformance.Whatmakesourapproachnoveloverall
istheadaptationsforcodedata,oneofthemostimportant
ofwhichistheprocedureforcreatingacode-description
wordembedding.WeconsidertheNN-basedwith
theWikipediaembeddingtobearepresentativestate-of-the-
artNN-basedtext,andtoisolatetheeffectsofthe
embeddingversusotheradaptationssuchaspre-processing,we
conductexperimentsinwhichwechangeonlytheembedding.
C.Datasets
WeusethethreedatasetsdescribedinSectionIV:the
standarddatasetofsixcategories,thechallengedatasetof
sixteencategories,andthe
libs
datasetofsixcategories.
Thestandardandchallengedatasetsusethecategorypro-
videdbytheDebianpackagesrepositoryasagoldsetoflabels
fortheprojects.Wetrainallmodelsonthesametraining
sets,toisolatetheeffectsofthemodelsfromtheeffectsof
variationsinthedataset.WereiteratefromSectionIVthatthe
holdout/testsetsareusedonlyduringourexperimentsinthis
andthenextsection;weneverusethemduringtraining.
The
libs
datasetdoesnotincludecategorylabelsofits
own(theyarealllabeledintherepositoryas
libs
),despite
thediversenatureofthedataset.Toobtainlabelsforthe
libsholdoutset,wehiredsixprofessionalprogrammersvia
UpWorktolabeleveryprojectintheholdoutset.Webuilta
webinterfacethatshowedtheprogrammerstheprojectname,
description,andwebsitelink,aswellasalistofthetop19
(20minus
libs
)categoriesintherepository.Weallowedthe
programmerstochosefromalistof19categoriesinsteadof
onlythetopsix,tohelpensurethattheywerenotﬁforcedﬂ
toselectacategoryforaprojectwhentheprojectmaynot
actuallybeagoodforthosetopsix.Theprogrammers
selectedaprimaryanduptotwosecondarycategoriesforeach
project.Theprogrammersrequiredonaverage2.24hours.
Onceallsixprogrammershadcompletedlabelingthe
projects,weselectedacategorylabelforaprojectbytaking
themost-commonprimarylabel.Wethenchoseonlythe
projectswithalabelfromthetopsixcategories.For72%
ofprojects,atleastthreeofthesixprogrammerschosethe
sameprimarylabel(therewerenoties).Weconsiderthisan
encouragingsignforthereliabilityofourlabeling,considering
thattheprogrammerswerefromaroundtheworld,chosefrom
20categories,andworkedindependently.Wedonotusethe
secondarylabelatthistime,butmetricssuchastop-nprecision
oranalogstopyramidprecision[58]areanareaoffuturework.
D.Metrics
WeusePrecision,Recall,andF1Scoretoquantifyperfor-
mance.Ingeneral,weusetheF1Scoreasthekeyindicator
ofmodelquality,sinceitbothprecisionandrecall.
Technically,wecalculatedallthreemetricsusingthe
cation
reportfunctionalityin
sklearn
version0.19.1,which
computesallthreemetricsforeachcategory,aswellasan
overallscoreweightedbythesizeofeachcategoryinthetest
set.Thisoverallscoreisthemainonewereportinthispaper,
butfulloutputisavailableinouronlineappendix.
E.Methodology
OurmethodologytoanswerRQ
1
istotrainaBoW+LR
modelanda
nn+w
(neuralnetwork+Wikipediaembedding)
modelusingthesamedataset,andthenruntwoexperiments
withthismodel.Inone,weusethe
co
(code-only)testdata,
andinanotherweusethe
cd
(code-description)testdata.Note
thatbothtestsetshavethesamefunctions;theonlydifference
iswhetherthetextdescriptiondataisavailabletothemodel
duringtesting.Wealwaysusethecode-descriptiondataand
thecode-onlydatafromthetrainingsetduringtraining.
OurmethodologytoanswerRQ
2
issimilartoourmethodol-
ogyforRQ
1
,exceptthatweuseourproposedapproachinstead
ofthebaselines.Theideaistochangeonlyonevariableata
time;inthiscase,thealgorithm.
Finally,ourmethodologyforRQ
3
istousethesame
experimentalsetup,excepttochangeonlytheembedding.We
conductexperimentswiththesamedatasets,changingonly
betweentheWikipediaembedding,thecode-onlyembedding,
andthecode-descriptionembedding.
F.ThreatstoValidity
Likeanypaper,thisevaluationcarriesthreatstovalidity.
Onethreatinthispaperistheselectionofdataset.Due
tothecomputationalexpenseoftrainingandtestingeach
wordembeddingandmodelwepre-selecta
holdout/testandtrainingsetratherthanconductingane.g.
10-foldcross-validation.Whilethisisthetypicalstrategyin
text(suchasin[53]asmentionedinSectionIV),
itdoesopenupathreattovalidityinthatresultsmayvary
ifadifferentholdoutsetwereused.Weattemptedtomitigate
thisthreatbyconductingexperimentsoverthreedatasets.
Anotherthreattovalidityistheselectionoflabelsfor
projects.Thereislikelytobeoverlapinthecategories(e.g.,
between
net
and
web
),oraprojectcouldbemiscatego-
rizedinthegoldset,soitispossiblethatsomeprojectsare
categorizedcorrectlybythemodels,evenifthatprediction
iscalculatedasamiss.Wetriedtomitigatethisthreatby
usingbothreferencelabelsfromDebianpackages,aswellas
amanually-labeledlibsdatasetthatweheldoutcompletely
fromtrainingandtestingwiththereferencelabels.
VII.E
VALUATION
R
ESULTS
Wepresentouranswertoeachresearchquestioninthis
section,withthesupportingdataandrationale.Weofferan
interpretationofeachresult,whichwediscussfurtherinthe
followingsection.
A.RQ
1
:BaselinePerformance
Weobservethatbothbaselinessufferadropin
performance,whenfacedwithcode-onlydataversuscode-
descriptiondata,asclearlyvisibleinFigure3.Consider
lines1and5inTableI.Theselinescorrespondtothelinear
regressionforcode-only(line1)andcode-description(line5)
data.Inline5,theF1scoreonthestandarddatasetis65,and
precision/recallis68/64.Theseresultsarebroadlysimilarto
the67%F1scorereportedbyWang
et.al
[5]whenusingtext
descriptiondata(minedfromtheweb)withlinearregressionto
classifyJavaprojectsfromSourceForge(thoughtheresultsare
notdirectlycomparableduetodifferentcategories,datasets,
andprogramminglanguage).However,whentextdataisnot
available,weobserveadropinperformanceto34%F1score,
43/33precision/recallŒanearly50%performancepenaltyas
measuredbyF1score.
Thesamepatternisvisibleonthechallengeandlibs
datasets.Onthechallengedata,theF1scoreis48for
lr
with
descriptions,butonly20without.Andonthelibsdata,theF1
scoreis52withdescriptionsversus40without.Performance
isconsistentlymuchloweroncode-onlydata.
Thebaselineneural-basedapproachwiththeWikipedia
embedding(
nn+w
inTableI)exhibitssimilarbehavior.It
achieves78%F1scorewithdescriptions,comparedto46%
withoutŒovera40%performancedrop.Challengeresults
areevenmoreextreme,from40to17F1score.Interestingly,
performanceonthelibsdatasetisrelativelystable,evenwith
slightlyimprovedresultswhendescriptiondataisremoved:
from48to52.Onelikelyexplanationisthatthelibrary
functionscouldbecalledbyfunctionsinthetrainingset,
whichwouldleadtothemodellearningtorecognizethose
librariesaspartofacategory.Still,onthestandardand
challengedatasets,thereisevidenceforalargeperformance
gapbetweencode-onlyandcode-descriptiondata.
Fig.3:
F1Scoreforcode-onlyandcode-descriptionstandard
datasets.Weobserveadropinperformancefromcode-
descriptiontocode-onlyinallcases,thoughthisdropwaslowerfor
nn+cd
than
nn+w
.Bestoverallperformerwas
nn+cd
.
Thisresultmaynotbeallthatsurprising,inlightofthe
algorithms'originintheNLPliterature.Themore-recent
neural-basedapproachesweredesignedtoexceedbaselineLR
performanceontextdata,andweobservethattheydowith
textdescriptionsofsoftwareonthestandarddataset.Butthatis
basicallyatextproblemthathappenstobeusing
textdatafromthesoftwaredomain.Baselineperformance
of
lr
and
nn+w
tendstobelowonboththestandardand
challengedatasetsforcode-onlytestdata.Ourinterpretation
oftheseresultsisthatclassifyingsourcecodeisadifferent
problemthantexthighlylikelytobedueto
thevocabularyproblem.Inshort,wethatanoff-the-shelf
applicationoftextapproachestosourcecode
dataisnotlikelytobeeffective.
B.RQ
2
:ProposedApproachPerformance
Lines4and8correspondtothecode-onlyandcode-
descriptionresultsforourproposedapproach(
nn+cd
).We
observeadropfrom81F1scorefor
cd
to53for
co
on
thestandarddataset.Thisisroughlya35%dropŒcertainly
verybuttheeffectisnotaspronouncedasin
thebaseline.Infact,the
nn+cd
resultoncode-onlydatais
low-endcompetitivewiththebaselineperformanceoncode-
TABLEI:
Overviewoftheresults.Thecolumn
Train
indicatesthetrainingsetused:wealwaysusebothcode-onlyandcode-description
functionrepresentationsfromthetrainingset.
Test
isthetestset:
cd
meanscode-description,
co
meanscode-only.RecallfromSectionV-A
that
cd
simulatesthesituationwhenwehavedescriptions,and
co
simulateswhenweonlyhavecode,butboth
cd
and
co
testsetshave
thesamefunctionsfromtheholdoutset.Thetopfourcolumnsareresultssimulatingwhennodescriptiondataisavailable(
co
).The
algorithm
lr1.8k
isthelinearbaselineusingvocabsizeof1.8k,
nn+w
istheneuralarchitecturewithWikipediaembedding,
nn+co
is
thecode-onlyembedding,and
nn+cd
isthecode-descriptionembedding.
P
,
R
,and
F
correspondtoprecision,recall,andf-measure.
run
is
therunnumberassociatedwiththedownloadablemodelandfulloutputavailableinouronlineappendix;weincludeittoassistreplication
ofourexperimentsanduseofthemodelswecreated(seeSectionIX).
StandardChallengeLibs
TrainTestAlgo.PRFrunPRFrunPRFrun
cd+cocolr1.8k
43333411
36212021
52374031
cd+coconn+w
534646101
172117110
614952201
cd+coconn+co
585354102
262624120
675457202
cd+coconn+cd
615253103
342825130
756366203
cd+cocdlr1.8k
68646512
51514822
61495232
cd+cocdnn+w
807778104
444240140
584948204
cd+cocdnn+co
847576105
494744150
756669205
cd+cocdnn+cd
868081106
534746160
767172206
descriptiondata.Onthestandarddataset,F1scorefor
nn+cd
was53forcode-only,comparedto65for
lr
code-description.
Thechallengedatasetresultsaremoremixed.While
nn+cd
doesobtainthehighestF1scoreonboth
co
and
cd
groups
amongtheneural-basedapproaches,the
lr
baselineactually
hasthehighestperformance,thoughbyasmallmargin.Given
thethreattovaliditywementionintheprevioussectionof
avoidingcross-validationduetocomputationalexpense,itis
diftodrawastrongconclusionfromthechallengeset.
Also,wenotethattheperformancepenaltyfor
nn+cd
is
roughly50%onthechallengedataset,similartothebaseline.
Remarkably,weobservelittledifferencebetween
nn+cd
performanceonthelibsdataset.Aswithbaselineperformance,
wesurmisethatthereasonisthatthelibrariesareusedby
theend-userprograms,whichmeansthatthetrainingsetwill
includefunctionnamesofthelibrariesinthecode-onlydata;
thehigh-leveldescriptionsmaynotaddmuchinformation.
C.RQ
3
:EffectsofEmbeddings
Wefoundevidencethattheembeddingshadanimportant
impactontheperformanceoftheneural-basedapproachŒ
,that
nn+cd
wasthebestperformingapproach
overall.Comparing
nn+cd
to
nn+w
,weobserveanincrease
of3pointsonF1scoreonthe
cd
standarddataset,and
anincreaseof5pointsonthe
co
standarddataset.The
performanceincreasefor
nn+co
and
nn+cd
over
nn+w
is
largeronthecode-onlydata,probablybecausetheembeddings
includewordsfromthecode,whiletheWikipediaembedding
islesslikelytohavethesamevocabularyasisusedincode.
Wewilldemonstrateanexampleoftheconferredby
theembeddingsinthenextsection,butfornowweobserve
anincreaseinperformance,whichislargeroncode-onlydata.
Thedifferencebetweentheembeddingsiscleareronthe
libs
dataset.F1scorefor
nn+cd
is72withtextdescriptions,
comparedto48for
nn+w
.Withcode-only,
nn+cd
achieves
66F1scoreversus52for
nn+w
.
ButaswithRQ
2
,itisdiftomakestrongconclusions
fromthechallengedatasetwhencomparingtheembeddings.
The
nn+cd
(ourproposedapproach)doesper-
formslightlybetter,buttheimprovementislargeonlywhen
comparing
nn+cd
to
nn+w
.Whileanareaoffuturework
isverifyingthedifferenceonmultipleholdoutsets(time
permitting),ourinterpretationisthatthechallengedatasetis
neartheedgeofthecapabilityofthealgorithmswediscuss.
Onepossiblereasonisthatallapproaches(
lr
included)
arepickingupontheﬁeasyﬂcluesforeachclass,while
missingmoresubtledetails.Oneremedymaysimplybeto
includemoretrainingdata(perhapsbyoversamplinginstead
ofundersamplingduringcorpuspreparation,SectionIV),but
itseemslikelythatworkwillneedtobedoneindesigning
modelssensitivetotheofsoftware.
Overall,wethat
nn+cd
isthebestperformer,atleast
byasmallmargin,inallsituationsexceptcode-onlyonthe
standardset,whereitisaheadonprecisionbutlags
nn+co
onrecallbyonepoint(averysmalldifference).
VIII.D
ISCUSSION
/C
ONCLUSION
Inthispaper,weproposedseveraladaptationstooff-the-
shelfneural-basedtexttothedomainofsoftware
categorization.Tosummarize,theseadaptationsare:

Representprojectsasfunctions,andassigneachfunction
thelabeloftheprojectfromwhichitoriginated.

Modelfunctionsasasequenceoftokenswithminimal
pre-processing(SectionV-A).

Trainawordembeddingbasedonacombinationofcode
andhigh-leveldescriptions(SectionV-B).

Useconvolutionalandrecurrentneurallayerswiththe
parameterswefoundviaagridsearch.

Traintheneuralmodelwithbothcode-onlyandcode-
descriptionexamplesofthesamefunctions.

Useavotingmechanismtoproduceprojectpredictions
fromthemodel'sfunctionpredictions(SectionV-D).
Therearenumerousareasoffuturework,includingcus-
tomizedneuralarchitecturesfordifferentsoftwaredata,im-
provedvotingmechanisms,experimentswithdifferentpre-
processingtechniques,andvariationsonembeddingstrat-
egy.Nonetheless,inexperimentswithbothreferencedata
fromDebianuser-endprograms(thestandardandchallenge
datasets),plusmanually-annotatedprogramminglibraries,we
foundimprovementoveroff-the-shelfapplicationsoftext
aswellasabaselinefromprevioussoftware
maintenanceliterature(RQ
2
).Weobservedthatperformance
forallapproaches,butespeciallythebaselineapproaches,was
heavilybythepresenceofhigh-leveltextdata:
whenonlycodedatawasavailable,performancewasfarlower
(RQ
1
).Wealsofoundthatourproposedcode-description
embeddingwasanimportantcontributortothesuccessofour
approach(RQ
3
).
Tobeginunderstanding
why
thecode-descriptionembed-
dingachievedhigherperformance,considertheexamplefunc-
tioninFigure5.Thisfunctionbelongsto
stardict
,which
isadictionaryprogramcategorizedinthereferenceunder
utils
.The
nn+cd
approachcorrectlypredictedthecategory
forthisfunction,butthe
nn+w
approachpredictedthefunction
tobepartofthe
science
category.Keepinmindthat
theonlydifferencebetweenthesetwomodelsistheword
embedding:allparametersandtrainingdatawerethesame.
Figure4showsaheatmapoftheactivationoftheconvolu-
tionalneurallayerforbothapproaches.Thehorizontalaxisis
250wide:onecolumnforeach(theparametervalue250
fornumberofwasselectedaftergridsearchtuning).
Theverticalaxisis60tall:onerowforeachterm(recall
thatweusedasequencelengthof60).Area
A
inbothheat
mapscorrespondstoterms25,26,and27.The
nn+w
model
activatesheavilyacrosstheseterms,whichasseeninFigure5,
correspondtothephraseﬁcomputelevenshteindistance.ﬂIn
particular,thewordﬁlevenshteinﬂleadstoactiva-
tion,whichimpliesthatthatwordisanimportant
ofthenetwork'sdecision.
ﬁLevenshteinﬂisanunusualwordinthecodecorpus,
occurringonlyahandfuloftimes.However,intheWikipedia
(a)NN+W(Wikipediaembedding),predicting
science
category(incorrect).
(b)NN+CD(Code-Descriptionembedding),predicting
utils
category(correct).
Fig.4:
Heatmapsoftheattentionoftheconvolutionallayerintheneural-basedapproaches.Theonlydifferencebetween(a)and(b)isthe
embedding.Bothweretrainedwiththesametrainingdataandusingthesameparameters.Wegeneratedthemapsusingthefunctionin
Figure5.Theverticalaxisrepresentsthetokens(60sequencelength)withthetokenatthetop.Thehorizontalaxisrepresentsthe250
intheconvolutionlayer.ArrowsdenoteareasAandB,whicharediscussedinSectionVIIIandshowninFigure4.
embedding,thenearestwordsinthevectorspaceareﬁhyper-
focalﬂ,ﬁmahalanobisﬂ,andﬁcomoving.ﬂThetwodonot
appearanywhereinthedataset,butﬁcomovingﬂappearsinthe
function
normalizeData
intheproject
yt
.Thedescription
of
yt
isﬁanintegratedscienceenvironmentforcollaboratively
askingandansweringastrophysicalquestionsﬂŒthe
yt
project
isas
science
.Whathaslikelyhappenedisthatthe
network,viatheWikipediaembedding,connectedtheword
ﬁlevenshteinﬂtoaprojectinthesciencecategory,basedon
theword'sEnglishusage.Infact,theEnglishusageisnot
whatmatters:the
nn+cd
approachactivatesmoreonother
words,suchasthoseinarea
B
andbeyond(corresponding
tothetextﬁinternationaldictionaryﬂ),whicharelinkedvia
theembeddingtotermsinthe
utils
category.Thereisnot
stardictqqcaleditdistancensmtempitemp
currentelementskndkmdinjicostk
costd
computelevenshteindistance
betweens
andtthisisusingquick
algorithmdescrdelim
internationaldictionary
stardictisacross
platformandinternationaldictionarywritten
ingtk2xithaspowerfulfeaturessuchas
globstyle
Fig.5:
Code-DescriptionrepresentationofthefunctionCalEdit-
DistancefromStarDict,aprojectinthe
utils
category.Purple
textcorrespondstoareaAinFigure4,andgreentexttoarea
B.Thedescrdelimtermisadelimiterweaddedtodenotethe
separatebetweenwordsfromcodeandwordsaddedfromtheproject
description.Thecode-onlyrepresentationwouldonlyhaveterms
priortodescrdelim.
spaceinthispapertoshowalltheseterms,butweprovidea
Tensorboardinteractivevisualizationofbothembeddingsvia
ouronlineappendix.Thereisalsonotspacetodiscussthe
categorizationresultsatfunction-level(wepresentedproject-
levelpredictions,afterourvotingmechanism),butthese
resultsarealsoavailableintheappendix.
Animportantareaofourfutureworkistofurtherunder-
standhowcodeandnaturallanguagediffer,sothatwecan
makebetteradaptationsofNLPworktotheSEdomain,a
rapidlygrowingresearcharea[22].Weviewthispaperas
progressinthatdirection.
IX.R
EPRODUCIBILITY
Toencouragefurtherresearchandtofacilitatereplication
ofourresults,weprovideallscripts,rawandprocessed
data,experimentalinputandoutput,andotherinformationvia
anonlineappendix.Aswithanypaperrelyingonmachine
learningalgorithms,thenumberofparameters,softwarever-
sions,andothermodelfarexceedswhatcanbe
explainedinasinglemanuscript.Wehavetriedtocondense
theresultstoonlythosenecessarytotheobjectiveofthis
paper,butweincludeseveralothersupporting
http://173.255.251.249/codecat/
A
CKNOWLEDGMENT
ThisworkissupportedinpartbytheNSFCCF-1452959,
CCF-1717607,andCNS-1510329grants.Anyopinions,
ings,andconclusionsexpressedhereinaretheauthorsanddo
notnecessarilythoseofthesponsors.
R
EFERENCES
[1]
M.Linares-V
´
asquez,C.McMillan,D.Poshyvanyk,andM.Grechanik,
ﬁOnusingmachinelearningtoautomaticallyclassifysoftwareapplica-
tionsintodomaincategories,ﬂ
EmpiricalSoftwareEngineering
,vol.19,
no.3,pp.582Œ618,2014.
[2]
A.Sharma,F.Thung,P.S.Kochhar,A.Sulistya,andD.Lo,ﬁCataloging
githubrepositories,ﬂin
Proceedingsofthe21stInternationalConference
onEvaluationandAssessmentinSoftwareEngineering
,ser.EASE'17.
NewYork,NY,USA:ACM,2017,pp.314Œ319.[Online].Available:
http://doi.acm.org/10.1145/3084226.3084287
[3]
C.McMillan,M.Linares-Vasquez,D.Poshyvanyk,andM.Grechanik,
ﬁCategorizingsoftwareapplicationsformaintenance,ﬂin
Proceedingsof
the201127thIEEEInternationalConferenceonSoftwareMaintenance
,
ser.ICSM'11.Washington,DC,USA:IEEEComputerSociety,2011,
pp.343Œ352.[Online].Available:http://dx.doi.org/10.1109/ICSM.2011.
6080801
[4]
S.Ugurel,R.Krovetz,andC.L.Giles,ﬁWhat'sthecode?:Automatic
ofsourcecodearchives,ﬂin
ProceedingsoftheEighth
ACMSIGKDDInternationalConferenceonKnowledgeDiscoveryand
DataMining
,ser.KDD'02.NewYork,NY,USA:ACM,2002,
pp.632Œ638.[Online].Available:http://doi.acm.org/10.1145/775047.
775141
[5]
T.Wang,H.Wang,G.Yin,C.X.Ling,X.Li,andP.Zou,ﬁMining
softwareacrossmultiplerepositoriesforhierarchicalcategoriza-
tion,ﬂin
SoftwareMaintenance(ICSM),201329thIEEEInternational
Conferenceon
.IEEE,2013,pp.240Œ249.
[6]
W.Frakes,R.Prieto,C.Fox
etal.
,ﬁDare:Domainanalysisandreuse
environment,ﬂ
Annalsofsoftwareengineering
,vol.5,no.1,pp.125Œ
141,1998.
[7]
K.C.Kang,J.Lee,andP.Donohoe,ﬁFeature-orientedproductline
engineering,ﬂ
IEEEsoftware
,vol.19,no.4,pp.58Œ65,2002.
[8]
C.McMillan,M.Grechanik,andD.Poshyvanyk,ﬁDetectingsimilar
softwareapplications,ﬂin
Proceedingsofthe2012International
ConferenceonSoftwareEngineering
,ser.ICSE2012.Piscataway,
NJ,USA:IEEEPress,2012,pp.364Œ374.[Online].Available:
http://dl.acm.org/citation.cfm?id=2337223.2337267
[9]
F.Thung,D.Lo,andL.Jiang,ﬁDetectingsimilarapplicationswith
collaborativetagging,ﬂin
SoftwareMaintenance(ICSM),201228th
IEEEInternationalConferenceon
.IEEE,2012,pp.600Œ603.
[10]
T.Wang,G.Yin,X.Li,andH.Wang,ﬁLabeledtopicdetectionof
opensourcesoftwarefromminingmasstextualprojectﬂin
ProceedingsoftheFirstInternationalWorkshoponSoftwareMining
.
ACM,2012,pp.17Œ24.
[11]
Y.Wu,Y.Yao,F.Xu,H.Tong,andJ.Lu,ﬁTag2word:Usingtagsto
generatewordsforcontentbasedtagrecommendation,ﬂin
Proceedings
ofthe25thACMinternationalonconferenceoninformationand
knowledgemanagement
.ACM,2016,pp.2287Œ2292.
[12]
K.Tian,M.Revelle,andD.Poshyvanyk,ﬁUsinglatentdirichletal-
locationforautomaticcategorizationofsoftware,ﬂin
MiningSoftware
Repositories,2009.MSR'09.6thIEEEInternationalWorkingConfer-
enceon
.IEEE,2009,pp.163Œ166.
[13]
P.F.Baldi,C.V.Lopes,E.J.Linstead,andS.K.Bajracharya,ﬁAtheory
ofaspectsaslatenttopics,ﬂin
ACMSigplanNotices
,vol.43,no.10.
ACM,2008,pp.543Œ562.
[14]
S.Kawaguchi,P.K.Garg,M.Matsushita,andK.Inoue,ﬁMudablue:An
automaticcategorizationsystemforopensourcerepositories,ﬂ
Journal
ofSystemsandSoftware
,vol.79,no.7,pp.939Œ953,2006.
[15]
S.Vargas-Baldrich,M.Linares-V
´
asquez,andD.Poshyvanyk,ﬁAuto-
matedtaggingofsoftwareprojectsusingbytecodeanddependencies
(n),ﬂin
AutomatedSoftwareEngineering(ASE),201530thIEEE/ACM
InternationalConferenceon
.IEEE,2015,pp.289Œ294.
[16]
T.J.Biggerstaff,B.G.Mitbander,andD.Webster,ﬁTheconcept
assignmentprobleminprogramunderstanding,ﬂin
Proceedingsofthe
15thinternationalconferenceonSoftwareEngineering
,ser.ICSE'93.
LosAlamitos,CA,USA:IEEEComputerSocietyPress,1993,pp.
482Œ498.[Online].Available:http://dl.acm.org/citation.cfm?id=257572.
257679
[17]
A.MarcusandJ.I.Maletic,ﬁRecoveringdocumentation-to-source-code
traceabilitylinksusinglatentsemanticindexing,ﬂin
Proceedingsof25th
InternationalConferenceonSoftwareEngineering
,2003,pp.125Œ135.
[18]
E.Hill,L.Pollock,andK.Vijay-Shanker,ﬁAutomaticallycapturing
sourcecodecontextofnl-queriesforsoftwaremaintenanceand
reuse,ﬂin
Proceedingsofthe31stInternationalConferenceon
SoftwareEngineering
,ser.ICSE'09.Washington,DC,USA:
IEEEComputerSociety,2009,pp.232Œ242.[Online].Available:
http://dx.doi.org/10.1109/ICSE.2009.5070524
[19]
GitHub,ﬁIntroducingtopics,ﬂJan.2017,https://blog.github.com/2017-
01-31-introducing-topics/.
[20]
J.Pennington,R.Socher,andC.Manning,ﬁGlove:Globalvectors
forwordrepresentation,ﬂin
Proceedingsofthe2014conferenceon
empiricalmethodsinnaturallanguageprocessing(EMNLP)
,2014,pp.
1532Œ1543.
[21]
V.J.HellendoornandP.Devanbu,ﬁAredeepneuralnetworksthebest
choiceformodelingsourcecode?ﬂin
Proceedingsofthe201711th
JointMeetingonFoundationsofSoftwareEngineering
.ACM,2017,
pp.763Œ773.
[22]
M.Allamanis,E.T.Barr,P.Devanbu,andC.Sutton,ﬁAsurvey
ofmachinelearningforbigcodeandnaturalness,ﬂ
arXivpreprint
arXiv:1709.06182
,2017.
[23]
P.Sandhu,ﬁApproachesforcategorizationofreusablesoftwarecompo-
nents,ﬂ
JournalofComputerScience
,vol.3,no.5,pp.266Œ273,2007.
[24]
Y.Kim,S.-j.Cho,S.Han,andI.You,ﬁAsoftwarescheme
usingbinary-levelcharacteristicsforefsoftwareﬂ
Soft
Computing
,vol.22,no.2,pp.595Œ606,2018.
[25]
C.-Z.YangandM.-H.Tu,ﬁLacta:Anenhancedautomaticsoftwarecate-
gorizationonthenativecodeofandroidapplications,ﬂin
Proceedingsof
theinternationalmulticonferenceofengineersandcomputerscientists
(IMECS2012)
,vol.1,2012.
[26]
S.WangandC.D.Manning,ﬁBaselinesandbigrams:Simple,good
sentimentandtopicﬂin
Proceedingsofthe50thAnnual
MeetingoftheAssociationforComputationalLinguistics:ShortPapers-
Volume2
.AssociationforComputationalLinguistics,2012,pp.90Œ94.
[27]
T.-H.Chen,S.W.Thomas,andA.E.Hassan,ﬁAsurveyontheuseof
topicmodelswhenminingsoftwarerepositories,ﬂ
EmpiricalSoftware
Engineering
,vol.21,no.5,pp.1843Œ1919,2016.
[28]
X.Cai,J.Zhu,B.Shen,andY.Chen,ﬁGreta:Graph-basedtagassign-
mentforgithubrepositories,ﬂin
ComputerSoftwareandApplications
Conference(COMPSAC),2016IEEE40thAnnual
,vol.1.IEEE,2016,
pp.63Œ72.
[29]
X.Ye,H.Shen,X.Ma,R.Bunescu,andC.Liu,ﬁFromwordembeddings
todocumentsimilaritiesforimprovedinformationretrievalinsoftware
engineering,ﬂin
Proceedingsofthe38thinternationalconferenceon
softwareengineering
.ACM,2016,pp.404Œ415.
[30]
Y.Zhang,D.Lo,P.S.Kochhar,X.Xia,Q.Li,andJ.Sun,ﬁDetecting
similarrepositoriesongithub,ﬂin
SoftwareAnalysis,Evolutionand
Reengineering(SANER),2017IEEE24thInternationalConferenceon
.
IEEE,2017,pp.13Œ23.
[31]
X.Gu,H.Zhang,D.Zhang,andS.Kim,ﬁDeepapilearning,ﬂin
Proceedingsofthe201624thACMSIGSOFTInternationalSymposium
onFoundationsofSoftwareEngineering
.ACM,2016,pp.631Œ642.
[32]
C.Ragkhitwetsagul,J.Krinke,andD.Clark,ﬁAcomparisonofcode
similarityanalysers,ﬂ
EmpiricalSoftwareEngineering
,pp.1Œ56,2017.
[33]
J.Fowkes,P.Chanthirasegaran,R.Ranca,M.Allamanis,M.Lapata,
andC.Sutton,ﬁAutofoldingforsourcecodesummarization,ﬂ
IEEE
TransactionsonSoftwareEngineering
,vol.43,no.12,pp.1095Œ1109,
2017.
[34]
P.W.McBurneyandC.McMillan,ﬁAutomaticsourcecodesumma-
rizationofcontextforjavamethods,ﬂ
IEEETransactionsonSoftware
Engineering
,vol.42,no.2,pp.103Œ119,2016.
[35]
P.Rodeghero,C.Liu,P.W.McBurney,andC.McMillan,ﬁAneye-
trackingstudyofjavaprogrammersandapplicationtosourcecode
summarization,ﬂ
IEEETransactionsonSoftwareEngineering
,vol.41,
no.11,pp.1038Œ1054,2015.
[36]
P.Loyola,E.Marrese-Taylor,andY.Matsuo,ﬁAneuralarchitecturefor
generatingnaturallanguagedescriptionsfromsourcecodechanges,ﬂ
in
Proceedingsofthe55thAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume2:ShortPapers)
,vol.2,2017,pp.
287Œ292.
[37]
B.Vasilescu,C.Casalnuovo,andP.Devanbu,ﬁRecoveringclear,natural
fromobfuscatedjsnames,ﬂin
Proceedingsofthe201711th
JointMeetingonFoundationsofSoftwareEngineering
.ACM,2017,
pp.683Œ693.
[38]
A.N.Lam,A.T.Nguyen,H.A.Nguyen,andT.N.Nguyen,ﬁBuglo-
calizationwithcombinationofdeeplearningandinformationretrieval,ﬂ
in
ProgramComprehension(ICPC),2017IEEE/ACM25thInternational
Conferenceon
.IEEE,2017,pp.218Œ229.
[39]
J.Zhou,H.Zhang,andD.Lo,ﬁWhereshouldthebugsbeed?-
moreaccurateinformationretrieval-basedbuglocalizationbasedon
bugreports,ﬂin
Proceedingsofthe34thInternationalConferenceon
SoftwareEngineering
.IEEEPress,2012,pp.14Œ24.
[40]
C.Liu,X.Yan,L.Fei,J.Han,andS.P.Midkiff,ﬁSober:statistical
model-basedbuglocalization,ﬂin
ACMSIGSOFTSoftwareEngineering
Notes
,vol.30,no.5.ACM,2005,pp.286Œ295.
[41]
B.Dit,M.Revelle,M.Gethers,andD.Poshyvanyk,ﬁFeaturelocation
insourcecode:ataxonomyandsurvey,ﬂ
Journalofsoftware:Evolution
andProcess
,vol.25,no.1,pp.53Œ95,2013.
[42]
C.Mills,ﬁAutomatingtraceabilitylinkrecoverythroughﬂ
in
Proceedingsofthe201711thJointMeetingonFoundationsof
SoftwareEngineering
.ACM,2017,pp.1068Œ1070.
[43]
T.J.Biggerstaff,B.G.Mitbander,andD.E.Webster,ﬁProgram
understandingandtheconceptassignmentproblem,ﬂ
Communications
oftheACM
,vol.37,no.5,pp.72Œ82,1994.
[44]
F.Sebastiani,ﬁMachinelearninginautomatedtextcategorization,ﬂ
ACM
computingsurveys(CSUR)
,vol.34,no.1,pp.1Œ47,2002.
[45]
S.Lai,L.Xu,K.Liu,andJ.Zhao,ﬁRecurrentconvolutionalneural
networksfortextﬂin
AAAI
,vol.333,2015,pp.2267Œ
2273.
[46]
S.WangandC.D.Manning,ﬁBaselinesandbigrams:Simple,good
sentimentandtopicﬂin
Proceedingsofthe50thAnnual
MeetingoftheAssociationforComputationalLinguistics:ShortPapers
-Volume2
,ser.ACL'12.Stroudsburg,PA,USA:Association
forComputationalLinguistics,2012,pp.90Œ94.[Online].Available:
http://dl.acm.org/citation.cfm?id=2390665.2390688
[47]
V.John,ﬁAsurveyofneuralnetworktechniquesforfeatureextraction
fromtext,ﬂ
arXivpreprintarXiv:1704.08531
,2017.
[48]
M.Allahyari,S.Pouriyeh,M.S.Safaei,E.D.Trippe,J.B.
Gutierrez,andK.Kochut,ﬁAbriefsurveyoftextmining:
clusteringandextractiontechniques,ﬂ
arXivpreprintarXiv:1707.02919
,
2017.
[49]
M.K.Raju,S.T.Subrahmanian,andT.Sivakumar,ﬁAcomparative
surveyondifferenttextcategorizationtechniques,ﬂ
InternationalJournal
ofComputerScienceandEngineering
,vol.5,no.3,pp.1612Œ1618,
2017.
[50]
R.Socher,A.Perelygin,J.Wu,J.Chuang,C.D.Manning,A.Ng,and
C.Potts,ﬁRecursivedeepmodelsforsemanticcompositionalityovera
sentimenttreebank,ﬂin
Proceedingsofthe2013conferenceonempirical
methodsinnaturallanguageprocessing
,2013,pp.1631Œ1642.
[51]
K.Lang,ﬁNewsweeder:Learningtonetnews,ﬂin
Proceedingsof
theTwelfthInternationalConferenceonMachineLearning
,1995,pp.
331Œ339.
[52]
C.Mcmillan,D.Poshyvanyk,M.Grechanik,Q.Xie,andC.Fu,
ﬁPortfolio:Searchingforrelevantfunctionsandtheirusagesinmillions
oflinesofcode,ﬂ
ACMTransactionsonSoftwareEngineeringand
Methodology(TOSEM)
,vol.22,no.4,p.37,2013.
[53]
X.Zhang,J.Zhao,andY.LeCun,ﬁCharacter-levelconvolutional
networksfortextﬂin
Advancesinneuralinformation
processingsystems
,2015,pp.649Œ657.
[54]
A.Gude,ﬁPython2vec,ﬂMar.2016,https://gab41.lab41.org/python2vec-
word-embeddings-for-source-code-3d14d030fe8f.
[55]
C.Zhou,C.Sun,Z.Liu,andF.Lau,ﬁAc-lstmneuralnetworkfortext
ﬂ
arXivpreprintarXiv:1511.08630
,2015.
[56]
N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhut-
dinov,ﬁDropout:Asimplewaytopreventneuralnetworksfromover-
ﬂ
TheJournalofMachineLearningResearch
,vol.15,no.1,pp.
1929Œ1958,2014.
[57]
B.Neate,W.Irwin,andN.Churcher,ﬁCoderank:Anewfamilyofsoft-
waremetrics,ﬂin
SoftwareEngineeringConference,2006.Australian
.
IEEE,2006,pp.10Œpp.
[58]
G.MurrayandG.Carenini,ﬁSummarizingspokenandwrittencon-
versations,ﬂin
ProceedingsoftheConferenceonEmpiricalMethods
inNaturalLanguageProcessing
.AssociationforComputational
Linguistics,2008,pp.773Œ782.
"
62,Efficient Nearest Neighbors Search for Large-Scale Landmark Recognition,http://arxiv.org/pdf/1806.05946v1.pdf,https://github.com/fmaglia/BoI,"NearestNeighborsSearchforLarge-Scale
LandmarkRecognition
FedericoMagliani,TomasoFontanini,andAndreaPrati
IMPlab-UniversityofParma,43124Parma,Italy,
federico.magliani@studenti.unipr.it
,
http://implab.ce.unipr.it
Abstract.
Theproblemoflandmarkrecognitionhasachievedexcellentresults
insmall-scaledatasets.Whendealingwithlarge-scaleretrieval,issuesthatwere
irrelevantwithsmallamountofdata,quicklybecomefundamentalforanef
cientretrievalphase.Inparticular,computationaltimeneedstobekeptaslowas
possible,whilsttheretrievalaccuracyhastobepreservedasmuchaspossible.In
thispaperweproposeanovelmulti-indexhashingmethodcalledBagofIndexes
(BoI)forApproximateNearestNeighbors(ANN)search.Itallowstodrastically
reducethequerytimeandoutperformstheaccuracyresultscomparedtothestate-
of-the-artmethodsforlarge-scalelandmarkrecognition.Ithasbeendemonstrated
thatthisfamilyofalgorithmscanbeappliedondifferentembeddingtechniques
likeVLADandR-MACobtainingexcellentresultsinveryshorttimesondiffer-
entpublicdatasets:Holidays+Flickr1M,Oxford105kandParis106k.
Keywords:
Landmarkrecognition,Nearestneighborssearch,Large-scaleimage
retrieval,Approximatesearch
1Introduction
Landmarkrecognitionisanemergingofresearchincomputervision.Inanutshell,
startingfromanimagedatasetdividedintoclasses,witheachimagerepresentedbya
featurevector,theobjectiveistocorrectlyidentifytowhichclassaqueryimagebe-
longs.Thistaskpresentsseveralchallenges:reachinghighaccuracyintherecognition
phase,fastresearchtimeduringtheretrievalphaseandreducedmemoryoccupancy
whenworkingwithalargeamountofdata.Thelarge-scaleretrievalhasrecentlybe-
comeinterestingbecausetheresultsobtainedinthemajorityofsmall-scaledatasetsare
overthe90%oftheaccuracyretrieval(
e
.
g
.toGordo
etal
.[6]).Searchingthecorrectk
nearestneighborsofeachqueryisthecrucialproblemoflarge-scaleretrievalbecause,
duetothegreatdimensionofdata,alotofdistractorsarepresentandshouldnotbe
consideredaspossiblequeryneighbors.Inordertodealwithlarge-scaledatasets,an
efsearchalgorithm,thatretrievesqueryresultsfasterthann
¨
aivebruteforceap-
proach,whilekeepingahighaccuracy,iscrucial.Withanapproximatesearchnotall
thereturnedneighborsarecorrect,thensomeareapproximate,buttheyaretypically
stillclosetotheexactneighbors.Usually,obtaininggoodresultsintheimageretrieval
taskisstrictlycorrelatedwiththehighdimensionalityoftheglobalimagedescriptors,
arXiv:1806.05946v1  [cs.CV]  15 Jun 20182FedericoMagliani,TomasoFontanini,andAndreaPrati
butonalarge-scaleversionofthesameproblemisnotadvisabletousethesameap-
proach,duetothelargeamountofmemorythatwouldbeneeded.Apossiblesolution
istoreducethedimensionalityofthedescriptors,forexamplethroughPCA,and,
then,applytechniquesbasedonhashingfunctionsforanefretrieval.
Followingthisstrategy,thispaperintroducesanewmulti-indexhashingmethod
called
BagofIndexes
(BoI)forlarge-scalelandmarkrecognitionbasedonLocality-
SensitiveHashing(LSH)anditsvariants,whichallowstominimizetheaccuracyre-
ductionwiththegrowthofthedata.Theproposedmethodistestedondifferentpublic
benchmarksusingdifferentembeddingsinordertoprovethatisnotanad-hocsolution.
Thispaperisorganizedasfollows.Section2introducesthegeneraltechniquesused
inthestateoftheart.Next,Section3describestheproposed
BagofIndexes
(BoI)al-
gorithm.Finally,Section4reportstheexperimentalresultsonthreepublicdatasets:
Holidays+Flickr1M,Oxford105kandParis106k.Finally,concludingremarksarere-
ported.
2Relatedwork
Inthelastyears,theproblemoflandmarkrecognitionwasaddressedinmanydifferent
ways[12][19][22].Recently,withthedevelopmentofnewpowerfulGPUs,thedeep
learningapproachhasshownitssuperiorperformanceinmanytasksofimageretrieval
[24][5][1][26].
Wheneverthenumberofimagesinthedatasetbecomestoolarge,aNearestNeigh-
bor(NN)searchapproachtothelandmarkrecognitiontaskbecomesinfeasible,dueto
thewell-knownproblemofthecurseofdimensionality.Therefore,ApproximateNear-
estNeighbors(ANN)becomesuseful,sinceitconsistsinreturningapointthathasa
distancefromthequeryequalstoatmost
c
timesthedistancefromthequerytoits
nearestpoints,where
c>
1
.
OneoftheproposedtechniquesthatallowstoeftreattheANNsearchprob-
lemistheLocality-SensitiveHashing(LSH[9]),wheretheindexofthedescriptoris
createdthroughhashfunctions.LSHprojectspointsthatareclosetoeachotherintothe
samebucketwithhighprobability.TherearemanydifferentvariantsofLSH,suchas
E2LSH[3],multi-probeLSH[15],andmanyothers.
WhileLSHisadata-independenthashingmethod,thereexistalsodata-dependent
methodslikeSpectralHashing[25],which,however,isslowerthanLSHandtherefore
notappropriateforlarge-scaleretrieval.InPermutation-Pivotsindex[2],dataobjects
andqueriesarerepresentedasappropriatepermutationsofasetofrandomlyselected
referenceobjects,andtheirsimilarityisapproximatedbycomparingtheirrepresenta-
tionintermsofpermutations.ProductQuantization(PQ)[10]isusedforsearching
localdescriptors.Itdividesthefeaturespaceindisjointsubspacesandthenquantizes
eachsubspaceseparately.Itpre-computesthedistancesandsavestheminlook-upta-
blesforspeedingupthesearch.LocallyOptimizedProductQuantization(LOPQ)[13]
isanoptimizationofPQthattriestolocallyoptimizeanindividualproductquantizer
percellandusesittoencoderesiduals.Instead,FLANN[17]isanopensourcelibrary
forANNandoneofthemostpopularfornearestneighbormatching.Itincludesdif-
EfNearestNeighborsSearchforLarge-ScaleLandmarkRecognition3
ferentalgorithmsandhasanautomatedprocedureforthebest
algorithmtosearchinaparticulardataset.
3BagofIndexes
TheproposedBagofIndexes(BoI)borrowsconceptsfromthewell-knownBagof
Words(BoW)approach.Itisaformofmulti-indexhashingmethod[7][18]forthe
resolutionofANNsearchproblem.
Firstly,followingtheLSHapproach,
L
hashtablescomposedby
2

buckets,that
willcontaintheindexesofthedatabasedescriptors,arecreated.Theparameter

rep-
resentsthehashdimensioninbits.ThelistofparametersofBoIandchosenvaluesare
reportedinTable1inSection3.2.Secondly,thedescriptorsareprojected
L
timesusing
hashingfunctions.Itisworthtonotethatthisapproachcanbeusedincombination
withdifferentprojectionfunctions,notonlyhashingandLSHfunctions.Finally,each
indexofthedescriptorsissavedinthecorrespondingbucketthatistheonematching
theprojectionresult.
Atquerytime,foreachquery,aBoIstructureiscreated,thatisavectorof
n
weights
(eachcorrespondingtooneimageofthedatabase)initiliazedtozero.Everyelementof
thevectorwillbebasedontheweighingmethodexplainedinSection3.1.So,
attheendoftheprojectionphase,itispossibletomakeacoarse-grainevaluationof
thesimilaritybetweenthequeryimageandtheotherimageswithoutcalculatingthe
Euclideandistancebetweenthem,butconsideringonlytheirfrequenciesinthequery
buckets.Subsequently,attheendoftheretrievalphase,the
""
elementsofthevectorwith
thehighestweightsarere-rankedaccordingtotheirEuclideandistancefromthequery.
Thenearestneighboristhensearchedonlyinthisshortre-rankedlist.Bycomputing
theEuclideandistancesonlyattheendoftheretrievalphaseandonlyonthisshortlist
(insteadofcomputingthemoneachhashtablelikeinstandardLSH),thecomputational
timeisgreatlyreduced.Furthermore,thisapproach,unlikeLSH,doesnotrequireto
maintainarankinglistwithoutduplicatesforallthe
L
hashtables.Thedetailedanalysis
ofthememoryoccupationofBoIisreportedinSection4.
3.1Weighingmetric
Aspreviouslyreported,BoIcanbeusedincombinationwithdifferenthashingfunc-
tions.WhenusedwithbaselineLSH,thecorrespondingbucketofthequeryimage
willbechecked.Inthiscase,eventhoughtitisfasterthanLSH,theaccuracysuffers
aloss.Conversely,whenBoIiscombinedwithmulti-probeLSH,alsothe
l
-neighboringbucketsareconsidered.
The
l
-neighborsarethebucketsthathaveaHammingdistancelessthanorequal
to
l
fromthehashedvalueofthequery,whichcorrespondstothequerybucket.The
weightsfortheanyvalueof
l
arechosenasfollows:
w
(
i;q;l
)=
(
1
2
H
(
i;q
)
if
H
(
i;q
)

l
0
otherwise
(1)
4FedericoMagliani,TomasoFontanini,andAndreaPrati
Fig.1.
OverviewoftheretrievalthroughBoImulti-probeLSH.
where
i
isagenericbucket,
q
isthequerybucketand
H
(
i;q
)
istheHammingdistance
between
i
and
q
.TheBoImulti-probeLSHapproachincreasesthenumberofbuckets
consideredduringtheretrievaland,thus,theprobabilityofretrievingthecorrectresult,
byexploitingthemainprincipleofLSHthatsimilarobjectsshouldfallinthesame
bucketorintheonesthatareclosetoit.However,evenifwewanttoaccountforsome
uncertaintyintheselectionofthecorrectbucket,wealsowanttoweightlessassoonas
wemovefartherfromtheﬁcentralﬂbucket.
Fig.1showsanexemplaroverviewoftheBoIcomputation.With
L
=3
hash
tablesand1-neighbours(i.e.,
l
=1
),aquerycanbeprojectedindifferentbuckets.The
correspondingweights(seeeq.1)areaccumulatedintheBoI(seethegraphonthe
bottomoftheimage).Onlythe

imageswiththehighestweightsareconsideredforthe
laststep(re-ranking)forimprovingtherecall.
3.2BoIadaptivemulti-probeLSH
ThisBoImulti-probeLSHapproachhasthedrawbackofincreasingthecomputational
timesinceitalsoneedstosearchinneighboringbuckets(whichare
P
l
i
=0

log
2

i

,being

thehashdimension).Tomitigatethisdrawback,weintroduceafurthervariant,called
BoIadaptivemulti-probeLSH
.Themainideaofthisapproachistoiterativelythe
searchbucketspace,bystartingwithalargenumberofneighboringbuckets

0
(e.g.,10)
andslowlyreduce

whenthenumberofhashtablesincreases.Thisadaptiveincrease
offocuscan,ontheonehand,reducethecomputationaltimeand,ontheotherhand,
reducethenoise.Infact,ateachiteration,theretrievalresultsaresupposedtobemore
likelycorrectandthelastiterationsaremeanttojustthem,sothereisnoneed
tosearchonalargenumberofbuckets.Inordertoavoidcheckingthesameneighbors
duringdifferentexperiments,thelistofneighborstocheckisshufrandomlyateach
experiment.
Twodifferenttechniquesforthereductionofthenumberofhashtablesareevalu-
ated:
EfNearestNeighborsSearchforLarge-ScaleLandmarkRecognition5
Œ
linear
:thenumberofneighboringbuckets

isreducedby2every40hashtables,
i.e.:

i
=
(

i

1

2
if
i
=
f

1
;:::;k
1

1
g

i

1
otherwise
(2)
with
i
=
f
1
;:::;L
g
;
1
=40
and
k
1
:
k
1

1

L
Œ
sublinear
:thenumberofneighboringbuckets

isreducedby2every25hash
tables,butonlyafterthehalfofhashtables,i.e.:

i
=
8
>
<
>
:

i

1
if
i

L=
2

i

1

2
if
i
=
f
L=
2
;L=
2+

2
;:::;L=
2+
k
2

2
g

i

1
otherwise
(3)
with
i
=
f
1
;:::;L
g
;
2
=25
and
k
2
:
L=
2+
k
2

2

L
Symbol

Chosenvalue
n
numberofimages
-

hashdimension
2
8
=256
L
numberofhashtables
100

0
initialgap
10
l
neighborsbucket
1-neighbors
""
elementsinthere-rankinglist
250
-
reduction
sublinear
Table1.
Summaryofnotation.
Theproposedapproachcontainsseveralparameters.Theirvalueswerechosenafter
anextensiveparameteranalysis(outofthescopeofthispaper)andsummaryofnotation
isreportedinTable1.
L
,

and
l
shouldbeaslowaspossiblesincetheydirectlyaffect
thenumberofbuckets
N
l
q
tobecheckedandthereforethecomputationaltimeateach
query
q
,asfollows:
N
l
q
=
L
l
X
i
=0


i
i

=
L
l
X
i
=0
(

i
)!
i
!(

i

i
)!
(4)
where

i
=

0
=log
2
;
8
i
forstandardBoImulti-probeLSH,whereas,inthecaseof
BoIadaptivemulti-probeLSH,

i
canbecomputedusingtheeqs.2or3.
4Experimentalresults
Theproposedapproachhasbeenextensivelytestedonpublicdatasetsinordertoeval-
uatetheaccuracyagainstthestateoftheart.
6FedericoMagliani,TomasoFontanini,andAndreaPrati
Dataset
Size
Queryimages
Holidays[11]+Flickr1M
1001491
500
Oxford105k[20]
105063
55
Paris106k[21]
106392
55
Table2.
Datasetsusedintheexperiments
4.1Datasetsandevaluationmetrics
Theperformanceismeasuredonthreepublicimagedatasets:Holidays+Flickr1M,Ox-
ford105kandParis106kasshowninTable2.
Holidays
[11]iscomposedby1491imagesrepresentingtheholidaysphotosof
differentlocations,subdividedin500classes.Thedatabaseimagesare991andthe
queryimagesare500,oneforeveryclass.
Oxford5k
[20]iscomposedby5062imagesofOxfordlandmarks.Theclassesare
11andthequeriesare55(5foreachclass).
Paris
[21]iscomposedby6412imagesoflandmarksofParis,France.Theclasses
are11andthequeriesare55(5foreachclass).
Flickr1M
[8]contains1millionFlickrimagesusedasdistractorsforHolidays,Ox-
ford5kandParis6kgeneratingHolidays+Flickr1M,Oxford105kandParis106kdatasets.
Evaluation
.MeanAveragePrecision(mAP)wasusedasmetricsforaccuracy.
Distance
.
L
2
distancewasemployedtocomparequeryimageswiththedatabase.
Implementation
.Allexperimentshavebeenrunon4separatethreads.TheCNN
featuresusedforthecreationoflocVLAD[16]descriptorsarecalculatedonaNVIDIA
GeForceGTX1070GPUmountedonacomputerwith8-coreand3.40GHzCPU.
4.2ResultsonHolidays+Flickr1Mdatasets
Thissectionreportstheresultsofourapproach,byaddingtotheHolidaysdatasetadif-
ferentnumberofdistractors,obtainedfromtheFlickr1Mdataset.Alltheexperiments
havebeenconductedseveraltimesandameanhasbeencomputedinordertoeliminate
therandomnessoftheGaussiandistributionusedinthehashingfunction.Theembed-
dingsusedarelocVLADdescriptors[16],whilethefeaturesareextractedfromthe
layermixed8ofInceptionV3network[23]thatisaCNNpre-trainedontheImageNet
[4]dataset.ThevocabularyusedforthecreationoflocVLADdescriptorsiscalculated
onParis6k.
Table3summarizestheresultsonHolidays+Flickr1MdatasetintermsofmAPand
averageretrievaltime(msec).Theexperimentsevaluatedonlythetop

=250
nearestneighbors.
LSHandmulti-probeLSHachieveexcellentresults,butwithanhugeretrieval
time.AlsoPP-index[2]needsmorethan3secondsforaquerytoretrievetheresults.
LOPQ[13]andFLANN[17]reachpoorresultsonlarge-scaleretrieval.LOPQreached
36.37%,whileFLANNachieved83.97%.However,whilequerytimeforLOPQis
prettylow,FLANNisnotabletokeepthequerytimelow.Itisworthsayingthatboth
LOPQandFLANNhasbeentestedusingtheavailablecodesfromauthorsandreported
resultscorrespondtothebestfoundofparameters.Giventhey
EfNearestNeighborsSearchforLarge-ScaleLandmarkRecognition7
Method

Holidays+Flickr1M
mAP
avgretrievaltime
LSH*
250
86.03%
3103
Multi-probeLSH*(L=50)
250
86.10%
16706
PP-index*[2]
250
82.70%
2844
LOPQ[13]
250
36.37%
4
FLANN[17]
250
83.97%
995
BoILSH
250
78.10%
5
BoImulti-probeLSH
250
85.16%
12
BoIadaptivemulti-probeLSH
250
85.35%
8
PP-index*[2]
10k
85.51%
15640
LOPQ[13]
10k
67.22%
72
FLANN[17]
10k
85.66%
1004
BoIadaptivemulti-probeLSH
10k
86.09%
16
Table3.
ResultsintermsofmAPandaverageretrievaltimeinmseconHolidays+Flickr1M.*
indicatesourre-implementation.
low(especiallyforLOPQ)performanceinaccuracy,furtherexperimentshavebeencon-
ductedforLOPQ,FLANN,aswellasPP-indexandourmethodbyincreasing

from
250to10k.Asforeseeable,alltheaccuracyresultsimprovedwithrespectto

=250
(LOPQincreasesfrom36.37%to67.22%),buttheproposedBoIadaptivemulti-probe
LSHmethodstilloutperformsalltheothers.Moreover,ourmethodstillresultstobe
fasterthantheothers(LOPQisfastlikeours,butwithloweraccuracy,whilePP-index
andFLANNareslightlylowerinaccuracy,butmuchslower).
Overallspeaking,ourproposaloutperformsallthecomparedmethodsinthetrade-
offbetweenaccuracyandefy.Tobetterhighlightthis,Fig.2showsjointlythe
mAP(ony-axis)andtheaveragequerytime(onx-axis).Thebesttrade-offhastobe
foundintheupperleftcornerofthisgraph,i.e.correspondingtohighaccuracyandlow
querytime.AlltheBoI-basedmethodsclearlyoutperformtheothermethods.
Regardingthememoryfootprintofthealgorithmfor1Mimageswith1Mdescrip-
torsof128D=4bytes),brute-forceapproachrequires0.5Gb(1Mx128x4).
LSHneedsonly100Mb:1MindexesforeachoftheL=100hashtables,becauseeach
indexesisrepresentedbyabyte(8bit)andso1Mindexesx100hashtablesx1byte=
100Mb.TheproposedBoIonlyrequiresadditional4Mbtostore1Mweights.
4.3ResultsonOxford105kandParis106kdatasets
Sinceourgoalistoexecutelarge-scaleretrievalforlandmarkrecognition,wehavealso
usedtheOxford105kandParis106kdatasets.Inthiscase,allthemethodsaretested
usingR-MACdescriptors,byGordo
etal.
[6],sinceVLADdescriptorsare
demonstratedtobenotsuitedforthesedatasets[14].
Table4showthemAPandtheaverageretrievaltime.Using

=2500
,thepro-
posedapproachobtainedslightlyworseresultsthanPP-index,butresultedoneorderof
magnitudefasterinbothdatasets.Whenmoretop-rankedimagesareused(

=10
k
),
BoIadapativemulti-probeLSHobtainedthebestresultsandwithlowerquerytime.
8FedericoMagliani,TomasoFontanini,andAndreaPrati
Fig.2.
RelationshipbetweentimeandaccuracyonHolidays+Flickr1Mwithdifferentapproaches.
Furthermore,LOPQ[13]worksbetteronParis106kthanOxford105k,whileFLANN
[17]performspoorlyonbothdatasets.
5Conclusions
Inthispaper,anovelmulti-indexhashingmethodscalledBagofIndexes(BoI)for
approximatenearestneighborsearchproblemisproposed.Thismethoddemonstrated
anoverallbettertrade-offbetweenaccuracyandspeedw.r.t.state-of-the-artmethods
onseverallarge-scalelandmarkrecognitiondatasets.Also,itworkswellwithdifferent
embeddingtypes(VLADandR-MAC).Themainfuturedirectionsofourworkwillbe
Method

Oxford105k
Paris106k
mAP
avgret.time
mAP
avgret.time
LSH*
2500
80.83%
610
86.50%
607
PP-index*[2]
2500
81.89%
240
88.14%
140
LOPQ[13]
2500
71.90%
346
87.47%
295
FLANN[17]
2500
70.33%
2118
68.93%
2132
BoIadaptivemulti-probeLSH
2500
81.44%
12
87.90%
13
PP-index*[2]
10k
82.82%
250
89.04%
164
LOPQ[13]
10k
69.94%
1153
88.00%
841
FLANN[17]
10k
69.37%
2135
70.73%
2156
BoIadaptivemulti-probeLSH
10k
84.38%
25
92.31%
27
Table4.
ResultsintermsofmAPandaverageretrievaltime(msec)onOxford105kand
Paris106k.*indicatesourre-implementationofthemethod.
EfNearestNeighborsSearchforLarge-ScaleLandmarkRecognition9
relatedtoreducethedimensionofthedescriptorinordertospeedthecreationofbucket
structureandtoadapttheproposedmethodfordatasetwithbillionsofelements.
Acknowledgments
.ThisworkispartiallyfundedbyRegioneEmiliaRomagnaun-
dertheﬁPianotriennalealtecompetenzeperlaricerca,iltrasferimentotecnologicoe
l'imprenditorialit
˚
aﬂ.
References
1.
Babenko,A.,Slesarev,A.,Chigorin,A.,Lempitsky,V.:Neuralcodesforimageretrieval.In:
Europeanconferenceoncomputervision,Springer(2014)584Œ599
2.
Chavez,E.,Figueroa,K.,Navarro,G.:Effectiveproximityretrievalbyorderingpermuta-
tions.IEEEtransactionsonPatternAnalysisandMachineIntelligence
30
(2008)1647Œ1658
3.
Datar,M.,Immorlica,N.,Indyk,P.,Mirrokni,V.S.:Locality-sensitivehashingschemebased
onp-stabledistributions.In:ProceedingsofthetwentiethannualsymposiumonComputa-
tionalgeometry,ACM(2004)253Œ262
4.
Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:Imagenet:Alarge-scalehier-
archicalimagedatabase.In:ComputerVisionandPatternRecognition,2009.CVPR2009.
IEEEConferenceon,IEEE(2009)248Œ255
5.
Gordo,A.,Almaz
´
an,J.,Revaud,J.,Larlus,D.:Deepimageretrieval:Learningglobalrepre-
sentationsforimagesearch.In:EuropeanConferenceonComputerVision,Springer(2016)
241Œ257
6.
Gordo,A.,Almazan,J.,Revaud,J.,Larlus,D.:End-to-endlearningofdeepvisualrepresen-
tationsforimageretrieval.InternationalJournalofComputerVision
124
(2)(2017)237Œ254
7.
Greene,D.,Parnas,M.,Yao,F.:Multi-indexhashingforinformationretrieval.In:Foun-
dationsofComputerScience,1994Proceedings.,35thAnnualSymposiumon,IEEE(1994)
722Œ731
8.
Huiskes,M.J.,Lew,M.S.:Themirretrievalevaluation.In:Proceedingsofthe1st
ACMinternationalconferenceonMultimediainformationretrieval,ACM(2008)39Œ43
9.
Indyk,P.,Motwani,R.:Approximatenearestneighbors:towardsremovingthecurseof
dimensionality.In:ProceedingsofthethirtiethannualACMsymposiumonTheoryofcom-
puting,ACM(1998)604Œ613
10.
Jegou,H.,Douze,M.,Schmid,C.:Productquantizationfornearestneighborsearch.IEEE
transactionsonpatternanalysisandmachineintelligence
33
(1)(2011)117Œ128
11.
J
´
egou,H.,Douze,M.,Schmid,C.:Hammingembeddingandweakgeometryconsistency
forlargescaleimagesearch-extendedversion.(2008)
12.
J
´
egou,H.,Douze,M.,Schmid,C.,P
´
erez,P.:Aggregatinglocaldescriptorsintoacompact
imagerepresentation.CVPR(2010)3304Œ3311
13.
Kalantidis,Y.,Avrithis,Y.:Locallyoptimizedproductquantizationforapproximatenearest
neighborsearch.In:ProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition.(2014)2321Œ2328
14.
Liu,Y.,Zhang,D.,Lu,G.,Ma,W.Y.:Asurveyofcontent-basedimageretrievalwithhigh-
levelsemantics.Patternrecognition
40
(1)(2007)262Œ282
15.
Lv,Q.,Josephson,W.,Wang,Z.,Charikar,M.,Li,K.:Multi-probeLSH:efindexing
forhigh-dimensionalsimilaritysearch.In:Proceedingsofthe33rdinternationalconference
onVerylargedatabases,VLDBEndowment(2007)950Œ961
16.
Magliani,F.,Bidgoli,N.M.,Prati,A.:Alocation-awareembeddingtechniqueforaccurate
landmarkrecognition.ICDSC(2017)
17.
Muja,M.,Lowe,D.G.:Scalablenearestneighboralgorithmsforhighdimensionaldata.
IEEEtransactionsonpatternanalysisandmachineintelligence
36
(11)(2014)2227Œ2240
10FedericoMagliani,TomasoFontanini,andAndreaPrati
18.
Norouzi,M.,Punjani,A.,Fleet,D.J.:Fastsearchinhammingspacewithmulti-indexhash-
ing.In:ComputerVisionandPatternRecognition(CVPR),2012IEEEConferenceon,IEEE
(2012)3108Œ3115
19.
Perronnin,F.,Liu,Y.,S
´
anchez,J.,Poirier,H.:Large-scaleimageretrievalwithcompressed
vectors.In:ComputerVisionandPatternRecognition(CVPR),2010IEEEConference
on,IEEE(2010)3384Œ3391
20.
Philbin,J.,Chum,O.,Isard,M.,Sivic,J.,Zisserman,A.:Objectretrievalwithlargevo-
cabulariesandfastspatialmatching.In:ProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition.(2007)
21.
Philbin,J.,Chum,O.,Isard,M.,Sivic,J.,Zisserman,A.:Lostinquantization:Improving
particularobjectretrievalinalrgescaleimagedatabases.CVPR(2008)
22.
Sivic,J.,Zisserman,A.:Videogoogle:Atextretrievalapproachtoobjectmatchinginvideos.
ICCV
2
(2003)1470Œ1477
23.
Szegedy,C.,Vanhoucke,V.,Ioffe,S.,Shlens,J.,Wojna,Z.:Rethinkingtheinceptionarchi-
tectureforcomputervision.In:ProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognition.(2016)2818Œ2826
24.
Tolias,G.,Sicre,R.,J
´
egou,H.:Particularobjectretrievalwithintegralmax-poolingofCNN
activations.arXivpreprintarXiv:1511.05879(2015)
25.
Weiss,Y.,Torralba,A.,Fergus,R.:Spectralhashing.In:Advancesinneuralinformation
processingsystems.(2009)1753Œ1760
26.
Yue-HeiNg,J.,Yang,F.,Davis,L.S.:Exploitinglocalfeaturesfromdeepnetworksfor
imageretrieval.In:ProceedingsoftheIEEEConferenceonComputerVisionandPattern
RecognitionWorkshops.(2015)53Œ61
"
63,RepMet: Representative-based metric learning for classification and one-shot object detection,http://arxiv.org/pdf/1806.04728v3.pdf,https://github.com/jshtok/RepMet,"RepMet:Representative-basedmetriclearningforandfew-shot
objectdetection
LeonidKarlinsky

,JosephShtok

,SivanHarary

,EliSchwartz

,AmitAides,RogerioFeris
IBMResearchAI
Haifa,Israel
RajaGiryes
SchoolofElectricalEngineering,Tel-AvivUniversity
Tel-Aviv,Israel
AlexM.Bronstein
DepartmentofComputerScience,Technion
Haifa,Israel
Abstract
Distancemetriclearning(DML)hasbeensuccessfully
appliedtoobjectbothinthestandardregime
ofrichtrainingdataandinthefew-shotscenario,where
eachcategoryisrepresentedbyonlyafewexamples.In
thiswork,weproposeanewmethodforDMLthatsimul-
taneouslylearnsthebackbonenetworkparameters,theem-
beddingspace,andthemulti-modaldistributionofeachof
thetrainingcategoriesinthatspace,inasingleend-to-end
trainingprocess.Ourapproachoutperformsstate-of-the-
artmethodsforDML-basedobjectonava-
rietyofstandardaineddatasets.Furthermore,we
demonstratetheeffectivenessofourapproachontheprob-
lemoffew-shotobjectdetection,byincorporatingthepro-
posedDMLarchitectureasaheadintoastan-
dardobjectdetectionmodel.Weachievethebestresultson
theImageNet-LOCdatasetcomparedtostrongbaselines,
whenonlyafewtrainingexamplesareavailable.Wealso
offerthecommunityanewepisodicbenchmarkbasedon
theImageNetdatasetforthefew-shotobjectdetectiontask.
Codewillbereleaseduponacceptanceofthepaper.
1.Introduction
Duetothegreatsuccessofdeepneuralnetworks(DNNs)
inthetasksofimageanddetection[
7
,
11
,
12
,
14
,
32
,
44
],theyarenowwidelyacceptedasthe`feature
extractorsofchoice'foralmostallcomputervisionappli-
cations,mainlyfortheirabilitytolearngoodfeaturesfrom

Theauthorshavecontributedequallytothiswork
thedata.Itiswell-knownthattrainingaregularDNNmodel
fromscratchrequiresaamountoftrainingdata
[
26
].Yet,inmanypracticalapplications,onemaybegiven
onlyafewtrainingsamplesperclasstolearna.
Thisisknownasthefew-shotlearningproblem.
Figure1.
One-shotdetectionexample
.Surroundingimages:ex-
amplesofnewcategoriesunseenintraining.Centerimage:de-
tectionresultfortheone-shotdetectoronanimagecontainingin-
stancesof
partridge,whichisoneofthenewcategories.
Recentstudieshaveachievedadvancesinus-
ingDNNsforfew-shotlearning.Thishasbeendemon-
stratedfortasks,suchasfacerecogni-
tion[
28
]andfortheofgeneralcategories
[
6
,
10
,
33
,
38
,
40
,
43
].However,veryfewworkshavein-
vestigatedtheproblemoffew-shotobject
detection
,where
thetaskofrecognizinginstancesofacategory,represented
1
arXiv:1806.04728v3  [cs.CV]  18 Nov 2018Figure2.
Overviewofourapproach.
(a)
Traintime:
backbone,embeddingspaceandmixturemodelsfortheclassesarelearnedjointly,
classrepresentativesaremixturemodecentersintheembeddingspace;(b)
Testtime:
new(unseenduringtraining)classesareintroduced
tothedetectorinthelearnedembeddingspaceusingjustoneorafewexamples.Finetuningtherepresentativesandtheembedding(onthe
episodetraindata)canbeusedtofurtherimproveperformance(Section
5
).Forbrevity,onlytwonovelclassesareillustratedinthetest.
Theclassposteriorsarecomputedbymeasuringthedistancesoftheinputfeaturestotherepresentativesofeachoftheclasses.
byafewexamples,iscomplicatedbythepresenceofthe
imagebackgroundandtheneedtoaccuratelylocalizethe
objects.Recently,severalinterestingpapersdemonstrated
preliminaryresultsforthezero-shotobjectdetectioncase
[
1
,
23
]andforthefew-shottransferlearning[
5
]scenario.
Inthiswork,weproposeanovelapproachforDistance
MetricLearning(DML)anddemonstrateitseffectiveness
onbothfew-shotobjectdetectionandobject
Werepresenteachclassbyamixturemodelwithmultiple
modes,andconsiderthecentersofthesemodesasthe
rep-
resentative
vectorsfortheclass.Unlikepreviousmethods,
we
simultaneously
learntheembeddingspace,backbone
networkparameters,andtherepresentativevectorsofthe
trainingcategories,inasingleend-to-endtrainingprocess.
Forfew-shotobjectdetection,webuilduponmodernap-
proaches(e.g.,thedeformable-FPNvariantoftheFaster-
RCNN[
7
,
11
])thatrelyonaRegionProposalNetwork
(RPN)togenerateregionsofinterest,anda`head'
thattheseROIsintooneoftheobjectcategoriesor
abackgroundregion.Inordertolearnarobustdetectorwith
justafewtrainingexamples(seeFigure
1
foraone-shotde-
tectionexample),weproposetoreplacethehead
withasubnetthatlearnstocomputeclassposteriorsfor
eachROI,usingourproposedDMLapproach.Theinputto
thissubnetarethefeaturevectorspooledfromtheROIs,and
theclassposteriorsforagivenROIarecomputedbycom-
paringitsembeddingvectortothesetofrepresentativesfor
eachcategory.Thedetectiontaskrequiressolving`anopen
setrecognitionproblem',namelytoclassifyROIsintoboth
thestructuredforegroundcategoriesandtheunstructured
backgroundcategory.Inthiscontext,thejointend-to-end
trainingisimportant,sincesamplingbackgroundROIsfor
separatetrainingoftheDMLisveryinef(Section
5
).
Inthefew-shotdetectionexperiments,weintroduce
new
categories
intothedetector.Thisisdonebyreplacingthe
learnedrepresentatives(correspondingtooldcategories)
withembeddingvectorscomputedfromtheforeground
ROIsofthefewtrainingexamplesgivenforthesecategories
(
k
examplesfor
k
-shotdetection).Wealsoinvestigatethe
effectsofourproposedmodelandthebaselines
forfew-shotlearning.Promisingresults,comparedtobase-
linesandthepreviouswork,arereportedonthefew-shot
detectiontask(Section
5.2
)underliningtheeffectiveness
ofjointlyoptimizingthebackboneandtheembeddingfor
DML.Figure
2
schematicallyillustratesanoverviewofour
approachtofew-shotdetection.
Wealsodemonstratetheuseofourapproachforgen-
eralDML-basedbycomparingtotheMagnet
Loss[
25
]andotherstate-of-the-artDML-basedapproaches
[
42
,
22
].Insteadofthealternatingtrainingofembedding
andclusteringusedin[
25
],ourproposedapproachend-
to-endtrainsasingle(monolithic)networkarchitectureca-
pableoflearningtheDMLembeddingtogetherwiththe
representatives(modesofthemixturedistributions).Ef-
fectively,thisbringstheclusteringinsidetheend-to-end
networktraining.Usingthismethod,wewereableto
improveuponthestate-of-the-artresultsob-
tainedin[
22
,
25
,
42
]onavarietyof
tiondatasets(Section
5.1
).
Ourcontributionsarethreefold.
First
,weproposea
novelsub-netarchitectureforjointlytraininganembed-
dingspacetogetherwiththesetofmixturedistributionsin
thisspace,havingone(multi-modal)mixtureforeachof
thecategories.Thisarchitectureisshowntoimprovethe
currentstateoftheartforbothDML-basedobject
cationandfew-shotobjectdetection.
Second
,wepropose
amethodtoequipanobjectdetectorwithaDMLclassi-
headthatcanadmitnewcategories,andthustransform
itintoafew-shotdetector.Tothebestofourknowledge,
thishasnotbeendonebefore.Thisisprobablyduetode-
tectortrainingbatchesbeingusuallylimitedtooneimage
per-GPU,notallowingforbatchcontrolintermsofcate-
gorycontent.Thiscontrolisneededbyanyofthecurrent
few-shotlearnersthatuseepisode-basedtraining.This,in
turn,makesitchallengingtousethoseapproacheswithina
detectorthatisbeingtrainedend-to-end.Inourapproach,
thesetofrepresentativesservesasan`internalmemory'to
passinformationbetweentrainingbatches.
Third
,inthe
few-shotliterature,itisacommonpracticeto
evaluatetheapproachesbyaveragingtheperformanceon
multipleinstancesofthefew-shottask,calledepisodes.We
offersuchanepisodicbenchmarkforthefew-shotdetec-
tionproblem,builtonachallengingfew-shot
detectiontask.
2.Relatedwork
DistanceMetricLearning.
Theuseofmetriclearn-
ingforcomputervisiontaskshasalonghistory(see[
15
]
forasurvey).Inagrowingbodyofwork,themeth-
odsforimageationandretrieval,basedondeep
DML,haveachievedstate-of-the-artresultsonvarioustasks
[
22
,
25
,
34
,
42
].Rippeletal.[
25
]showedthatiftheem-
beddingandclusteringofthecategoryinstancesarealter-
natedduringtraining,thenonavarietyofchallenging
graineddatasets[
13
,
20
,
21
,
27
]theDML-based
tionimprovesthestate-of-the-artevenwithrespecttothe
non-DMLmethods.InDML,themetricbeinglearnedis
usuallyimplementedasan
L
2
distancebetweenthesam-
plesinanembeddingspacegeneratedbyaneuralnetwork.
Thebasiclossfunctionfortrainingsuchanembeddingis
thetripletloss[
41
],oroneofitsrecentgeneralizations
[
34
,
35
,
39
].Theselossesaredesignedtomaketheem-
beddingspacesemanticallymeaningful,suchthatobjects
fromthesamecategoryarecloseunderthe
L
2
distance,and
objectsfromdifferentcategoriesarefarapart.Thismakes
DMLanaturalchoiceforfew-shotvisualrecognition.Fol-
lowingtheDML,adiscriminativeclassposterioriscom-
putedattesttime.Tothatend,anon-parametricapproach
suchas
k
-Nearest-Neighbors(
k
-NN)iscommonlyused
tomodeltheclassdistributionsinthelearnedembedding
space[
38
,
33
,
41
],thoughinsomecasesparametricmodels
arealsoused[
4
].Inaddition,inmanyapproachessuchas
[
33
,
41
]thereisaninherentassumptionofthecategorydis-
tributionsbeinguni-modalintheembeddingspace.Ourap-
proachinsteadlearnsamulti-modalmixtureforeachcate-
gory,while
simultaneously
learningtheembeddingspacein
whichthedistancestotheserepresentativesarecomputed.
Few-shotLearning.
Animportantrecentworkinfew-
shothasintroducedMatchingNetworks[
38
],
wherebothtrainandtestdataareorganizedin`episodes'.
An
N
-way,
M
-shotepisodeisaninstanceofthefew-shot
taskrepresentedbyasetof
M
trainingexamplesfrom
eachofthe
N
categories,andonequeryimageofanob-
jectfromoneofthecategories.Thegoalistodetermine
thecorrectcategoryforthequery.In[
38
],thealgorithm
learnstoproduceadedicatedDMLembeddingto
theepisode.In[
33
],eachclassisrepresentedbyaProto-
type-acentroidofthebatchelementscorrespondingtothat
category.Recently,evenmorecompellingresultswereob-
tainedonthestandardfew-shotbenchmarks
usingmeta-learningmethods[
9
,
19
,
24
,
43
]andsynthesis
methods[
6
,
10
,
29
,
40
,
43
].Althoughgreatprogresswas
madetowardsfew-shotitisstilldifto
applythesemethodstofew-shotdetection.Thereasonis
thatadetectortrainingbatchtypicallyconsistsofjustone
image,withahighlyunbalancedforegroundtobackground
ROIratio(somewhatbalancedusingOHEM[
31
]andalike).
Thisisproblematicforexistingfew-shotlearners,which
usuallyrequireabalancedsetofexamplesfrommultiple
categoriesineachbatchandcommonlyhavedifcop-
ingwithunstructurednoise(backgroundROIsinourcase).
Thereareonlyahandfulofexistingworksonfew-shot
detection.AninterestingrecentworkbyChenatal.[
5
]
proposedusingregularizedonthefewgivenex-
amplesinordertotransferapre-traineddetectortothefew-
shottask.Theauthorsshowthatusingtheirproposedreg-
ularization,ofthestandarddetectors,suchas
FRCNN[
30
]andSSD[
18
],canbeimproved
inthefew-shottrainingscenario.Adifferentapproachby
Dongetal.[
8
]usesadditionalunlabeleddatainasemi-
supervisedsetting.Byusingtheclassicalmethodofenrich-
ingthetrainingdatawithsampleselection,
themethodof[
8
]producesresultscomparabletoweakly
supervisedmethodswithlotsoftrainingexamples.Un-
likepreviousmethods,weproposeaDML-basedapproach
forfew-shotobjectdetection,whichyieldssuperiorperfor-
mancecomparedtoexistingtechniques.
Figure3.
TheproposedRepMetDMLsub-netarchitecture
performsjointend-to-endtrainingoftheDMLembeddingtogetherwiththe
modesoftheclassposteriordistribution.
3.RepMetArchitecture
Weproposeasubnetarchitectureandcorresponding
lossesthatallowustotrainaDMLembeddingjointlywith
themulti-modalmixturedistributionusedforcomputingthe
classposteriorintheresultingembeddingspace.Thissub-
netthenbecomesaDML-basedhead,whichcan
beattachedontopofaoradetectionback-
bone.ItisimportanttonotethatourDML-subnetistrained
jointlywiththefeatureproducingbackbone.Thearchitec-
tureoftheproposedsubnetisdepictedinFigure
3
.
Thetrainingisorganizedinbatches,butforsimplicity
wewillrefertotheinputofthesubnetasasingle(pooled)
featurevector
X
2
R
f
computedbythebackboneforthe
givenimage(orROI).ExamplesforabackboneareIncep-
tionV3[
36
]oranFPN[
16
](withouttheRCNN).We
employaDMLembeddingmodule,whichconsistsofafew
fullyconnected(FC)layerswithbatchnormalization(BN)
andReLUnon-linearity(weused
2
-
3
suchlayersinourex-
periments).Theoutputoftheembeddingmoduleisavector
E
=
E
(
X
)
2
R
e
,wherecommonly
e
˝
f
.Asanaddi-
tionalsetoftrainedparameters,weholdasetof`represen-
tatives'
R
ij
2
R
e
.Eachvector
R
ij
representsthecenterof
the
j
-thmodeofthelearneddiscriminativemixturedistri-
butionintheembeddingspace,forthe
i
-thclassoutofthe
totalof
N
classes.Weassumeaednumberof
K
modes
(peaks)inthedistributionofeachclass,so
1

j

K
.
Inourimplementation,therepresentativesarerealized
asweightsofanFClayerofsize
N

K

e
receivingaed
scalarinput
1
.Theoutputofthislayerisreshapedtoan
N

K

e
tensor.Duringtraining,thissimpleconstruc-
tionwsthegradientstotheweightsoftheFClayerand
learnstherepresentatives.Foragivenimage(oranROI,in
thecaseofthedetector)anditscorrespondingembedding
vector
E
,ournetworkcomputesthe
N

K
distancematrix
whoseelements
d
ij
(
E
)=
d
(
E;R
ij
)
arethedistancesfrom
E
toeveryrepresentative
R
ij
.Thesedistancesareusedto
computetheprobabilityofthegivenimage(orROI)ineach
mode
j
ofeachclass
i
:
p
ij
(
E
)
/
exp
 

d
2
ij
(
E
)
2
˙
2
!
:
(1)
Hereweassumethatalltheclassdistributionsaremixtures
ofisotropicmulti-variateGaussianswithvariance
˙
2
.In
ourcurrentimplementation,wedonotlearnthemixingco-
efandsetthediscriminativeclassposteriortobe:
P
(
C
=
i
j
X
)=
P
(
C
=
i
j
E
)

max
j
=1
;:::;K
p
ij
(
E
)
:
(2)
where
C
=
i
denotesclass
i
andthemaximumistakenover
allthemodesofitsmixture.Thisconditionalprobability
isanupperboundontheactualclassposterior.Thereason
forusingthisapproximationisthatforone-shotdetection,
attesttime,therepresentativesarereplacedwithembedded
examplesofnovelclasses,unseenduringtraining(morede-
tailsarefoundinSection
5
).Mixturecoefareasso-
ciatedwithmodes,andsincethemodeschangeat
testtime,learningthemixturecoefbecomeshighly
non-trivial.Therefore,theuseoftheupperboundinEq.
2
eliminatestheneedtoestimatethemixturecoef
Aninterestingfutureextensiontoourapproachwouldbe
topredictthemixturecoefentsandthecovarianceofthe
modesasafunctionof
E
or
X
.
Havingcomputedtheclassposterior,wealsoestimate
a(discriminative)posteriorforthe`open'background(
B
)
class.Following[
2
],wedonotmodelthebackgroundprob-
ability,butinsteaditisestimatedviaitslowerboundusing
theforeground(class)probabilities:
P
(
Bj
X
)=
P
(
Bj
E
)=1

max
ij
p
ij
(
E
)
:
(3)
Having
P
(
C
=
i
j
X
)
and
P
(
Bj
X
)
computedinthenet-
work,weuseasumoftwolossestotrainourmodel(DML
subnet+backbone).Thelossistheregularcross-
entropy(CE)withthegroundtruthlabelsgivenfortheim-
age(orROI)correspondingto
X
.Theotherisintendedto
ensurethereisatleast

marginbetweenthedistanceof
E
totheclosestrepresentativeofthecorrectclass,andthe
distanceof
E
totheclosestrepresentativeofawrongclass:
L
(
E;R
)=




min
j
d
i

j
(
E
)

min
j;i
6
=
i

d
ij
(
E
)+





+
;
(4)
where
i

isthecorrectclassindexforthecurrentexample
and
jj
+
istheReLUfunction.Figure
4
illustrateshowthe
proposedDMLsub-netisintegratedwithinthefullnetwork
architecturesusedfortheDML-basedandthe
few-shotdetectionexperiments.
4.Implementationdetails
Inthissectionwelistadditionaldetailsofourimplemen-
tationoftheproposedapproachfortheDML-basedclassi-
(Section
4.1
)andfew-shotdetection(Section
4.2
)
tasks.Codewillbereleaseduponacceptance.
4.1.
FortheDML-basedexperiments,weused
theInceptionV3[
36
]backbone,attachingtheproposed
DMLsubnettothelayerbeforeitslastFClayer.Theem-
beddingmoduleofthesubnetconsistsoftwoFClayersof
sizes
2048
and
1024
,thewithBNandReLU,andthe
secondjustwithlinearactivation.Thisisfollowedbyan
L
2
normalizationoftheembeddingvectors.Alllayersare
initializedrandomly.InallofourDML-based
experiments,weset
˙
=0
:
5
anduse
K
=3
representatives
percategory.Eachtrainingbatchwasconstructedbyran-
domlysampling
M
=12
categoriesandsampling
D
=4
randominstancesfromeachofthosecategories.
InourDML-basedexperimentsonstan-
dardbenchmarks,thereisnobackgroundcategory
B
,hence
wedonotneedourclassmixturestohandlepointsthatare
outlierstoallofthemixtures.Therefore,weresorttoa
moreclassicalmixturemodelvariantwithequalyweighted
modes,replacingtheclassposteriorinEq.
2
withitssofter
normalizedversion,whichwehaveexperimentallyv
asmoreforDML-based
P
(
C
=
i
j
X
)=
P
(
C
=
i
j
E
)=
K
P
j
=1
p
ij
(
E
)
N
P
i
=1
K
P
j
=1
p
ij
(
E
)
(5)
4.2.detection
Forfew-shotdetection,weusedourDMLsub-netin-
steadoftheRCNN(the`head')ontopofthe
FPNbackbone[
16
]initsDeformableConvolutions(DCN)
variant[
7
].OurcodeisbasedontheoriginalMXNetbased
implemetationof[
7
].Thebackbonewaspre-trainedon
MS-COCO[
17
]fromscratch.OurDMLsubnet,includ-
ingtherepresentatives,wasinitializedrandomly.Theentire
networkwastrainedinanend-to-endfashionusingOHEM
[
31
]andSoftNMS[
3
].TheembeddingmoduleintheDML
subnetforone-shotdetectionconsistedoftwoFClayers
ofwidth
1024
withBNandReLU,andaFClayerof
width
256
withlinearactivation,followedby
L
2
normal-
ization.Weused
K
=5
representativesperclassduring
training,andset
˙
=0
:
5
.Asin[
7
],eachtrainingbatch
consistedofonerandomtrainingimage.
5.Results
WehaveevaluatedtheutilityofourproposedDMLsub-
netonaseriesofandfew-shotdetectiontasks.
5.1.
Fine-grained
Wetestedourapproachon
asetofdatasets,widelyadopted
inthestate-of-the-artDMLworks[
22
,
25
,
42
]:StanfordDogs[
13
],Oxford-IIITPet[
21
],Oxford102
Flowers[
20
],andImageNetAttributes[
25
].Theresultsre-
portedinTable
1
showthatourapproachoutperformsthe
state-of-the-artDMLmethods[
22
,
25
,
42
]on
alldatasetsexceptOxfordFlowers.Figure
5
showstheevo-
lutionofthet-SNE[
37
]plotofthetraininginstancesinthe
embeddingspaceoverthetrainingiterations.
Attributedistribution.
WevthatfollowingDML
trainingforimageswithsimilarattributesare
closertoeachotherintheembeddingspace(eventhough
attributeannotationswerenotusedduringtraining).We
usedthesameexperimentalprotocolas[
25
].,
wetrainedourDMLontheImageNetAttributes
datasetin[
25
],whichcontains
116236
imagesfrom
90
classes.Next,wemeasuredtheattributedistributionon
theObjectAttributesdataset[
27
],whichprovides
25
at-
tributesannotationsforabout
25
imagesperclassforthese
90
classes.Foreachimageinthisdataset,andforeach
attribute,wecomputethefractionofneighborsalsofea-
turingthisattribute,overdifferentneighborhoodcardinal-
ities.Figure
6
(a)showsimprovedresultsobtainedbyour
approachascomparedto[
25
]andtoothermethods.
HyperparameterrobustnessŒablationstudy.
We
evaluateddifferentvaluesofrepresentativesperclass(
1

K

8
),and
9
differentarchitecturesoftheembeddingnet-
work(varyingthenumberofdenselayersbetween
1
and
3
andusingthreedifferentwidthsforeach).Samerobustness
Figure4.
Networkarchitecturesused.
(a)NetworkforDMLbased(b)Networkforfew-shotdetection;itsbackboneis
FPN+DCNwithdeformableROI-align[
7
].
method
datasetMsML[
22
]Magnet[
25
]VMF[
42
]Ours
StanfordDogs29.724.924.0
13.7
OxfordFlowers10.58.6
4.4
11
OxfordPet18.810.69.9
6.9
ImageNetAttributesŒ15.9Š
13.2
Table1.Comparisonof
testerror(in%)
withthestate-of-the-artDMLapproachesondifferent
datasets(lowerisbetter).
testswerealsorepeatedforourimplementationof[
25
].We
havevthatourimplementationreproducedtheresults
reportedin[
25
],whosecodeisnotavailable.
Figures
6
(b)and
6
(c)showthatourmethodismorero-
busttochangesinthehyperparameterscomparedto[
25
].
Thesedepict,foreachmethodandateachtrainingit-
eration,thestandarddeviationoftheerrorob-
tainedbyvaryingtheembeddingnetworkarchitectureand
thenumberofrepresentativesperclass,respectively.
5.2.Fobjectdetection
Tothebestofourknowledge,theonlyfew-shotdetec-
tionbenchmarkavailableto-dateisreportedintheLSTD
work[
5
]byChenetal.,whoproposedtoapproachfew-
shotdetectionbyaregularizedInTable
2
,we
compareourapproachtotheresultsofLSTD[
5
]on'Task
1',whichistheirmostchallengingImageNetbased
50
-way
few-shotdetectionscenario.
1-shot5-shot10-shot
LSTD[
5
]19.237.444.3
ours
24.139.649.2
Table2.ComparisontoLSTD[
5
]ontheirTask1experiment:
50
-waydetectionon50ImageNetcategories(asmAP
%
).
Sinceforalloftheirproposedtasks,thebenchmarks
of[
5
]consistofjustoneepisode(train/testimagesse-
lection)pertask,wecreatedanadditionalbenchmarkfor
few-shotdetection.Ourproposedbenchmarkisbasedon
ImageNet-LOCdata.Thebenchmarkcontainsmultipleran-
domepisodes(instancesofthefew-shotdetectiontasks);
weused
500
randomepisodesinourbenchmark.Thisfor-
matisborrowedfromthefew-shotclassliterature.
Eachepisode,forthecaseofthe
n
-shot,
m
-wayfew-shot
detectiontask,containsrandom
n
trainingexamplesfor
eachofthe
m
randomlychosenclasses,and
10

m
random
queryimagescontainingoneormoreinstancesbelonging
totheseclasses(thusatleast
10
instancesperclass).The
goalistodetectandcorrectlyclassifytheseinstances.For
consistency,foreach
n
2f
1
;
5
;
10
g
thesame
500
random
episodesareusedinallofthe
n
-shotexperiments.Please
seeFigure
1
foranillustrationofa
1
-shot,
5
-wayepisode.
Ontheproposedfew-shotdetectionbenchmark,wehave
comparedourapproachtothreebaselines.Forthede-
notedas
`baseline-FT'
,weastandarddetector
networkonjustthefew(
n

m
)availablesamplesofthe
(
m
)novelcategoriesineach(
n
-shot,
m
-way)testepisode.
,wethelineardecisionlayerofthe
headoftheFPN-DCNdetector[
7
],thesamede-
tectorweuseasabackboneforourapproach.Forthesec-
ondbaseline,denotedas
'baseline-DML'
,weattachour
DMLsub-netwithouttheembeddingmoduletotheregu-
lar(pre-trained)FPN-DCNdetector,effectivelyusingthe
FPN-DCNtwolastFClayersastheembeddingmodule.
TheFPN-DCNdetectorusedforthisbaselineispre-trained
asaregularFPN-DCNonthesamedataasourapproach,
hencewithoutbeingoptimizedforDMLbased
tionasopposedtoourfullmethod.Forthethirdbaseline,
denotedas
`baseline-DML-external'
,wetrainedtheDML
sub-netembeddingmoduleseparatelyfromthedetector,in
anoftrainingprocess.Theembeddingwastrainedon
sampledforegroundandbackgroundROIsusingthetriplet
loss[
41
].Wealsoobtainedasimilarperformanceforthis
baselinewhentrainingtheembeddingmodulewiththePro-
totypicalNetworks[
33
].
Allthebaselineswerepre-trainedonthesametrain-
Figure5.Evolutionofthet-SNEvisualizationoftheembeddingspacewhiletrainingontheOxfordFlowers.Differentcolorscorrespond
todifferentmixturemodes.(a)initial;(b)1200iterations;(c)4200iterations.NotetheseparationintolocalclusterscreatedbyourDML.
Figure6.(a)MeanattributeprecisionasafunctionofneighborhoodsizeontheImageNetAttributesdataset.The`Softmax',`Triplet'and
`Magnet'graphsareborrowedfrom[
25
].(b)TestingperformancestabilitytohyperparameterchangeofourmethodandtheMagnetloss
[
25
].WeplottheSTDoftheerror,measuredaccrossvariousdepthandwidthsizesoftheembeddingmodel,asafunctionof
theiterationnumber.Lowerisbetter.(c)sameas(b)forvariousnumberofmodesinthelearnedmixture.
ingsetasourmodelandtestedonthesamecollections
orrandomepisodes.Totrainthemodelsweusedthe
100
categoriesfromImageNet-LOC(mostlyanimalsand
birdsspecies).Fortesting,weusedalltheremaining
214
ImageNet-LOCanimalandbirdspeciescategories(unseen
attraining)toensurethatthetrainandthetestcategories
belongedtothesameconceptdomain.Forourmodeland
alltheDML-baselines,ineachepisode,thesetofcategories
beingdetectedwasresettothe
m
newonesbyreplacingthe
setofrepresentatives
R
intheDMLsubnetwiththeembed-
dingvectorscomputedfromtheROIscorrespondingtothe
trainingobjectsoftheepisode.TheseROIswereselected
amongthe
2
K
ROIsperimagereturnedbyRPNbycheck-
ingwhichROIspassedtheIoU

0
:
7
requirementwiththe
trainingobjectsboundingboxes.Inourapproach,theem-
beddingandthebackbonearejointlyoptimizedtobeused
withtherepresentatives-basedclassposterior.Thisoffers
anadvantagecomparedtothebaselines,assuggestedby
theperformancecomparisonreportedinTable
3
.
Theevaluationofourapproachandthebaselinesonthe
setofunseenclassesisreportedinTable
3
(inits
unseen
classessection).Themeanaverageprecision(mAP)in%is
calculatedon
5
-waydetectiontasks(
500
suchtasks).The
mAPiscomputedbycollectingandevaluatingjointly(in
termsofscorethresholdforcomputingprecisionandrecall)
theentiresetofboundingboxesdetectedinallthe
500
test
episodeswith
50
queryimageseach.
Inaddition,foreachofthetestedmethods(oursandthe
baselines),werepeatedtheexperimentswhile
thelastlayerofthenetworkjustontheepisodetraining
images(forourmodelandthebaselinesusingDML,thelast
embeddinglayerandtherepresentativeswere
TheresultswitharealsoreportedinTable
3
.
Figure
7
showsexamplesof
1
-shotdetectiontestresults.
Fromtherelativelylowperformanceof'baseline-DML-
external',wecanconcludethat,asstatedintheintroduc-
tion,jointtrainingoftheembeddingspacewiththeDML
iscrucialfortheperformance.Fromourclose
examination,thereductioninmAPof'baseline-DML-
external'ismostlyattributedtosignihigherFalse
Positivesratesthanintheothermethods.Althoughtheex-
ternalembeddingwastrainedonthesametrainingimages
asourmethodandtheotherbaselines,itwasinfeasibleto
sampletheentirecollectionofpossiblebackgroundROIs
thatarebeingprocessedbyourmethodwhentrainingasa
detectorend-to-end.Therefore,wehadtoresorttosam-
pling200ROIsperimage,whichreducedthebaseline's
abilitytorejectthebackground.
Totesttheinter-dependenceofthelearnedembedding
ontherepresentativeslearnedjointlywithitduring
noepisodewithepisode
datasetmethod1-shot5-shot10-shot1-shot5-shot10-shot
ImageNet-LOC
(214unseenanimalclasses)
baseline-FT(FPN-DCN[
7
])ŠŠŠ35.051.059.7
baseline-DML41.358.261.641.359.766.5
baseline-DML-external19.030.230.432.137.238.1
Ours
56.968.871.559.273.979.2
ImageNet-LOC
(100seenanimalclasses)
Ours-trainedrepresentativesŠ86.3ŠŠŠŠ
Ours-episoderepresentatives64.579.482.6ŠŠŠ
Table3.Few-shot
5
-waydetectiontestperformanceonImageNet-LOC.ReportedasmAPin%.
Figure7.
Exampleone-shotdetectionresults.
Greenframesindicatecorrectlydetectedobjectsandredframesindicatewrongdetections.
Athresholdof
0
:
5
onthedetectionscoreisusedthroughout.Detectionswithhigherscoresaredrawnontopofthosewithlowerscores.
training,werepeatedtheepisode-basedtestingontheset
ofclassesseenduringtraining(usingonlyvalidationim-
agesnotusedfortraining).Theresultsofthisevaluation
arealsoreportedinTable
3
inthe
seen
classessection.We
repeatedtheseenclassestestingtwice:onceusingtherep-
resentativestakenfromthetrainingobjectsofeachepisode
(sameasforunseenclasses)andonceusingtheoriginally
trainedrepresentatives(astheycorrespondtothesetofseen
classes).Sinceduringtraining,welearn
K
=5
represen-
tativesperclass,wereporttheresultofthesecondtestin
the
5
-shotcolumn.Wecanseethat(i)thetrainedrepresen-
tativesperformbetterthanembeddingofrandomclassex-
amples,underliningagaintheofjointtraining;(ii)
theperformancedropfromtrainedrepresentativestoran-
domclassmembersisnotthatbig(
˘
7
points),hintingthat
thelearnedembeddingisrobusttochangeofrepresentatives
andislikelytoperformwellonthenewunseencategories
(aswasvaboveinourfew-shotexperiments).
In[
1
]Recall@100wasusedastheirperformancemea-
sure(Recall%taking
100
topdetectionsineachtestim-
age).Wealsoimplementedthismeasureinour
1
-shottest,
achieving88.2%Recall@100and65.9%Recall@10calcu-
latedoverourentiresetof
500
testepisodes.Thisdemon-
stratesthatourapproachworkswellonanindividualimage
basis,andillustratestheimportanceofconsideringallthe
boxesfromallthetestimagessimultaneouslywhencom-
putingtheAP,aswedidinourbenchmark.
Inordertocheckiftheintroducedbyre-
placingtheRCNNwithourDMLsub-nethinders
thedetectionperformanceontheseenclasses,wetestedthe
detectionperformanceofourmodelandthevanillaFPN-
DCNmodel(usingtheiroriginalcode)onthevalidation
setsofthe100Imagenet-LOCtrainingcategoriesand
ofPASCALVOC.AsshowninTable
4
,ourdetectoris
slightlyinferiortotheoriginalFPN-DCNmodelonthePas-
calVOC,butcomparesfavorablyonthe100Imagenet-
LOC(morecategories.
PASCALVOCImageNet(LOC)
method/IoU0.70.50.30.70.50.3
FPN-DCN[
7
]
74.683.585.3
46.955.260.2
Ours73.782.984.9
60.761.770.7
Table4.Regulardetectionperformance(inmAP[%])perdifferent
acceptanceIoU.FPN-DCNevaluatedusingtheiroriginalcode.
6.Summary&Conclusions
Inthiswork,weproposedanewmethodforDML,
achievingstate-of-the-artperformanceforobject
tioncomparedtootherDML-basedapproaches.Usingthis
method,wedesignedoneofthefew-shotdetectionap-
proaches,whichcomparesfavorablytothecurrentfew-shot
detectionstate-of-the-art.Wealsoproposedabenchmark
forthefew-shotobjectdetection,basedontheImagenet-
LOCdataset,inthehopesthatitwillencourageresearchers
tofurtherinvestigateintothisproblem,whichhassofar
beenalmostuntouched.Futureworkdirectionsincludepre-
dictingthemixingcoefientsandcovariancesfortheclass
mixtureslearnedwithinourDMLsub-netasafunctionof
theinput.Wealsointendtouseexamplesynthesismethods
togeneratenewsamplesautomatically,thereby
enrichingthesetofestimatedrepresentatives.
References
[1]
A.Bansal,K.Sikka,G.Sharma,R.Chellappa,andA.Di-
vakaran.Zero-ShotObjectDetection.
arXiv:1804.04340,
2018.
2
,
8
[2]
A.BendaleandT.Boult.TowardsOpenWorldRecogni-
tion.
IEEE
Conference
on
Computer
Vision
and
Pattern
Recognition
(CVPR),pages1893Œ1902,2015.
4
[3]
N.Bodla,B.Singh,R.Chellappa,andL.S.Davis.Soft-
NMSŠImprovingObjectDetectionWithOneLineofCode.
IEEE
International
Conference
on
Computer
Vision
(ICCV),
pages5562Œ5570,2017.
5
[4]
D.Chen,X.Cao,L.Wang,F.Wen,andJ.Sun.Bayesian
facerevisited:Ajointformulation.In
European
Conference
on
Computer
Vision
(ECCV),pages566Œ579,2012.
3
[5]
H.Chen,Y.Wang,G.Wang,andY.Qiao.LSTD:ALow-
ShotTransferDetectorforObjectDetection.
AAAI,2018.
2
,
3
,
6
[6]
Z.Chen,Y.Fu,Y.Zhang,Y.-G.Jiang,X.Xue,andL.Si-
gal.SemanticFeatureAugmentationinFew-shotLearning.
arXiv:1804.05298v2,2018.
1
,
3
[7]
J.Dai,H.Qi,Y.Xiong,Y.Li,G.Zhang,H.Hu,and
Y.Wei.DeformableConvolutionalNetworks.
The
IEEE
International
Conference
on
Computer
Vision
(ICCV),pages
764Œ773,2017.
1
,
2
,
5
,
6
,
8
[8]
X.Dong,L.Zheng,F.Ma,Y.Yang,andD.Meng.Few-shot
ObjectDetection.
Arxiv:1706.08249,pages1Œ11,2017.
3
[9]
C.Finn,P.Abbeel,andS.Levine.Model-Agnostic
Meta-LearningforFastAdaptationofDeepNetworks.
arXiv:1703.03400,2017.
3
[10]
B.HariharanandR.Girshick.Low-shotVisualRecognition
byShrinkingandHallucinatingFeatures.
IEEE
International
Conference
on
Computer
Vision
(ICCV),2017.
1
,
3
[11]
K.He,G.Gkioxari,P.Doll
´
ar,andR.Girshick.MaskR-
CNN.
arXiv:1703.06870,2017.
1
,
2
[12]
G.Huang,Z.Liu,L.v.d.Maaten,andK.Q.Weinberger.
DenselyConnectedConvolutionalNetworks.
2017
IEEE
Conference
on
Computer
Vision
and
Pattern
Recognition
(CVPR),pages2261Œ2269,2017.
1
[13]
A.Khosla,N.Jayadevaprakash,B.Yao,andL.Fei-Fei.
NovelDatasetforFine-GrainedImageCategorization.
IEEE
Conference
on
Computer
Vision
and
Pattern
Recognition
(CVPR)
Workshop,2011.
3
,
5
[14]
A.Krizhevsky,I.Sutskever,andG.E.Hinton.ImageNet
withDeepConvolutionalNeuralNetworks.
Advances
In
Neural
Information
Processing
Systems,pages
1Œ9,2012.
1
[15]
B.Kulis.MetricLearning:ASurvey.
Foundations
and
Trends
in
Machine
Learning,5(4):287Œ364,2013.
3
[16]
T.-Y.Lin,P.Doll
´
ar,R.Girshick,K.He,B.Hariharan,and
S.Belongie.FeaturePyramidNetworksforObjectDetec-
tion.
IEEE
Conference
on
Computer
Vision
and
Pattern
Recognition
(CVPR),2017.
4
,
5
[17]
T.Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra-
manan,P.Doll
´
ar,andC.L.Zitnick.MicrosoftCOCO:
Commonobjectsincontext.In
Lecture
Notes
in
Computer
Science
(including
subseries
Lecture
Notes
in

Intelligence
and
Lecture
Notes
in
Bioinformatics),volume
8693LNCS,pages740Œ755,2014.
5
[18]
W.Liu,D.Anguelov,D.Erhan,C.Szegedy,S.Reed,C.Y.
Fu,andA.C.Berg.SSD:Singleshotmultiboxdetec-
tor.
Lecture
Notes
in
Computer
Science
(including
subseries
Lecture
Notes
in

Intelligence
and
Lecture
Notes
in
Bioinformatics),9905LNCS:21Œ37,2016.
3
[19]
N.Mishra,M.Rohaninejad,X.Chen,andP.Abbeel.Meta-
LearningwithTemporalConvolutions.
arXiv:1707.03141,
2017.
3
[20]
M.E.NilsbackandA.Zisserman.Automatedwerclas-
overalargenumberofclasses.
Proceedings
-
6th
Indian
Conference
on
Computer
Vision,
Graphics
and
Image
Processing,
ICVGIP,pages722Œ729,2008.
3
,
5
[21]
O.M.Parkhi,A.Vedaldi,A.Zisserman,andC.V.Jawahar.
Catsanddogs.
2012
IEEE
Conference
on
Computer
Vision
and
Pattern
Recognition
(CVPR),pages3498Œ3505,2012.
3
,
5
[22]
Q.Qian,R.Jin,S.Zhu,andY.Lin.Fine-grainedvisualcate-
gorizationviamulti-stagemetriclearning.
IEEE
Conference
on
Computer
Vision
and
Pattern
Recognition
(CVPR),07-
12-June:3716Œ3724,2015.
2
,
3
,
5
,
6
[23]
S.Rahman,S.Khan,andF.Porikli.Zero-ShotObjectDe-
tection:LearningtoSimultaneouslyRecognizeandLocalize
NovelConcepts.
arXiv:1803.06049,2018.
2
[24]
S.RaviandH.Larochelle.OptimizationAsaModelfor
Few-ShotLearning.
International
Conference
on
Learning
Representations
(ICLR),pages1Œ11,2017.
3
[25]
O.Rippel,M.Paluri,P.Dollar,andL.Bourdev.
MetricLearningwithAdaptiveDensityDiscrimination.
arXiv:1511.05939,pages1Œ15,2015.
2
,
3
,
5
,
6
,
7
[26]
O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,
S.Ma,Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,
A.C.Berg,andL.Fei-Fei.ImageNetLargeScaleVisual
RecognitionChallenge.
IJCV,92015.
1
[27]
O.RussakovskyandL.Fei-Fei.Attributelearningin
large-scaledatasets.
Lecture
Notes
in
Computer
Science
(including
subseries
Lecture
Notes
in

Intelligence
and
Lecture
Notes
in
Bioinformatics),6553LNCS(PART
1):1Œ14,2010.
3
,
5
[28]
F.Schroff,D.Kalenichenko,andJ.Philbin.FaceNet:Auni-
embeddingforfacerecognitionandclustering.
IEEE
Conference
on
Computer
Vision
and
Pattern
Recognition
(CVPR),pages815Œ823,2015.
1
[29]
E.Schwartz,L.Karlinsky,J.Shtok,S.Harary,M.Marder,
A.Kumar,R.Feris,R.Giryes,andA.M.Bronstein.-
Encoder:anEffectiveSampleSynthesisMethodforFew-
ShotObjectRecognition.
NIPS,2018.
3
[30]
R.G.J.S.ShaoqingRen,KaimingHe.WeaklySuper-
visedOne-ShotDetectionwithAttentionSiameseNetworks.
NIPS,pages1Œ9,2015.
3
[31]
A.Shrivastava,A.Gupta,andR.Girshick.TrainingRegion-
basedObjectDetectorswithOnlineHardExampleMin-
ing.
IEEE
Conference
on
Computer
Vision
and
Pattern
Recognition
(CVPR),2016.
3
,
5
[32]
K.SimonyanandA.Zisserman.VeryDeepConvolu-
tionalNetworksforLarge-ScaleImageRecognition.
CoRR
arXiv:1409.1556,abs/1409.1:1Œ14,2014.
1
[33]
J.Snell,K.Swersky,andR.S.Zemel.PrototypicalNetworks
forFew-shotLearning.
Advances
In
Neural
Information
Processing
Systems
(NIPS),2017.
1
,
3
,
6
[34]
K.Sohn.ImprovedDeepMetricLearningwithMulti-
classN-pairLossObjective.
Neural
Information
Processing
Systems
(NIPS),pages1Œ9,2016.
3
[35]
H.O.Song,Y.Xiang,S.Jegelka,andS.Savarese.DeepMet-
ricLearningviaLiftedStructuredFeatureEmbedding.
IEEE
Conference
on
Computer
Vision
and
Pattern
Recognition
(CVPR),pages4004Œ4012,2016.
3
[36]
C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna.
RethinkingtheInceptionArchitectureforComputerVision.
arXiv:1512.00567,2015.
4
,
5
[37]
L.J.P.VanDerMaatenandG.E.Hinton.Visualizinghigh-
dimensionaldatausingt-sne.
Journal
of
Machine
Learning
Research,9:2579Œ2605,2008.
5
[38]
O.Vinyals,C.Blundell,T.Lillicrap,K.Kavukcuoglu,and
D.Wierstra.MatchingNetworksforOneShotLearn-
ing.
Advances
In
Neural
Information
Processing
Systems
(NIPS),2016.
1
,
3
[39]
J.Wang,F.Zhou,S.Wen,X.Liu,andY.Lin.DeepMetric
LearningwithAngularLoss.In
Proceedings
of
the
IEEE
International
Conference
on
Computer
Vision,volume2017-
Octob,pages2612Œ2620,2017.
3
[40]
Y.-X.Wang,R.Girshick,M.Hebert,andB.Hariharan.Low-
ShotLearningfromImaginaryData.
arXiv:1801.05401,
2018.
1
,
3
[41]
K.Q.WeinbergerandL.K.Saul.DistanceMetricLearn-
ingforLargeMarginNearestNeighbor
The
Journal
of
Machine
Learning
Research,10:207Œ244,2009.
3
,
6
[42]
X.Zhe,S.Chen,andH.Yan.DirectionalStatistics-based
DeepMetricLearningforImagecationandRe-
trieval.
arXiv:1802.09662,pages1Œ12,2018.
2
,
3
,
5
,
6
[43]
F.Zhou,B.Wu,andZ.Li.DeepMeta-Learning:Learning
toLearnintheConceptSpace.
arXiv:1802.03596,22018.
1
,
3
[44]
B.Zoph,V.Vasudevan,J.Shlens,andQ.V.Le.Learning
TransferableArchitecturesforScalableImageRecognition.
arXiv:1707.07012,2017.
1
"
64,Robust Bayesian Model Selection for Variable Clustering with the Gaussian Graphical Model,http://arxiv.org/pdf/1806.05924v1.pdf,https://github.com/andrade-stats/robustBayesClustering,"RobustBayesianModelSelectionforVariable
ClusteringwiththeGaussianGraphicalModel
DanielAndrade

GraduateUniversityofAdvancedStudies(SOKENDAI)
10-3Midoricho,Tachikawa,Tokyo,190-8562,Japan
andrade@ism.ac.jp
AkikoTakeda
DepartmentofCreativeInformatics
TheUniversityofTokyo
7-3-1Hongo,Bunkyo-ku,Tokyo,113-8656,Japan
takeda@mist.i.u-tokyo.ac.jp
KenjiFukumizu
TheInstituteofStatisticalMathematics
10-3Midoricho,Tachikawa,Tokyo,190-8562,Japan
fukumizu@ism.ac.jp
June18,2018
Abstract
Variableclusteringisimportantforexplanatoryanalysis.However,only
fewdedicatedmethodsforvariableclusteringwiththeGaussiangraphical
modelhavebeenproposed.Evenmoresevere,smallinsigni˝cantpartial
correlationsduetonoisecandramaticallychangetheclusteringresultwhen
evaluatingforexamplewiththeBayesianInformationCriteria(BIC).In
thiswork,wetrytoaddressthisissuebyproposingaBayesianmodelthat
accountsfornegligiblesmall,butnotnecessarilyzero,partialcorrelations.
Basedonourmodel,weproposetoevaluateavariableclusteringresult
usingthemarginallikelihood.Toaddresstheintractablecalculationofthe
marginallikelihood,weproposetwosolutions:onebasedonavariational
approximation,andanotherbasedonMCMC.Experimentsonsimulated
datashowsthattheproposedmethodissimilarlyaccurateasBICintheno
noisesetting,butconsiderablymoreaccuratewhentherearenoisypartial
correlations.Furthermore,onrealdatatheproposedmethodprovides
clusteringresultsthatareintuitivelysensible,whichisnotalwaysthecase
whenusingBICoritsextensions.

The˝rstauthorisalsoa˚liatedwithNECCorporation.
1
arXiv:1806.05924v1  [stat.AP]  15 Jun 20181Introduction
TheGaussiangraphicalmodel(GGM)hasbecomeaninvaluabletoolfordetecting
partialcorrelationsbetweenvariables.Assumingthevariablesarejointlydrawn
fromamultivariatenormaldistribution,thesparsitypatternoftheprecision
matrixrevealswhichpairsofvariablesareindependentgivenallothervariables
[Anderson,2004].Inparticular,wecan˝ndclustersofvariablesthataremutually
independent,bygroupingthevariablesaccordingtheirentriesintheprecision
matrix.
However,inpractice,itcanbedi˚cultto˝ndameaningfulclusteringdue
tothenoiseoftheentriesinthepartialcorrelations.Thenoisecanbedueto
thesampling,thisisinparticularthecasewhen
n
thenumberofobservationsis
small,orduetosmallnon-zeropartialcorrelationsinthetrueprecisionmatrix
thatmightbeconsideredasinsigni˝cant.Hereinthiswork,weareparticularly
interestedinthelattertypeofnoise.Intheextreme,smallpartialcorrelations
mightleadtoaconnectedgraphofvariables,wherenogroupingofvariablescan
beidenti˝ed.Foranexploratoryanalysissucharesultmightnotbedesirable.
Asanalternative,weproposeto˝ndaclusteringofvariables,suchthatthe
partialcorrelationbetweentwovariablesindi˙erentgroupsisnegligiblysmall,
butnotnecessarilyzero.Theopenquestion,whichwetrytoaddresshere,is
whetherthereisaprincipledmodelselectioncriteriaforthisscenario.
Forexample,theBayesianInformationCriteria(BIC)[Schwarz,1978]isa
popularmodelselectioncriteriafortheGaussiangraphicalmodel.However,
inthenoisesettingitdoesnothaveanyformalguarantees.Asasolution,
weproposehereaBayesianmodelthatexplicitlyaccountsforsmallpartial
correlationsbetweenvariablesindi˙erentclusters.
Underourproposedmodel,themarginallikelihoodofthedatacanthenbe
usedtoidentifythecorrect(ifthereisagroundtruthintheory),oratleasta
meaningfulclustering(inpractice)thathelpsanalysis.Themarginallikelihood
ofourmodeldoesnothaveananalyticsolution.Therefore,weprovidetwo
approximations.The˝rstisavariationalapproximation,thesecondisbasedon
MCMC.
Experimentsonsimulateddatashowthattheproposedmethodissimilarly
accurateasBICinthenonoisesetting,butconsiderablymoreaccuratewhen
therearenoisypartialcorrelations.Theproposedmethodalsocomparesfavorable
totwopreviouslyproposedmethodsforvariableclusteringandmodelselection,
namelytheClusteredGraphicalLasso(CGL)[Tanetal.,2015]andtheDirichlet
ProcessVariableClustering(DPVC)[Pallaetal.,2012]method.
Ourpaperisorganizedasfollows.InSection2,wediscusspreviousworks
relatedtovariableclusteringandmodelselection.InSection3,weintroducea
basicBayesianmodelforevaluatingvariableclusterings,whichwethenextend
inSection4tohandlenoiseontheprecisionmatrix.Fortheproposedmodel,
whichcanhandlenoiseintheprecisionmatrix,thecalculationofthemarginal
likelihoodisinfeasibleandwedescribeourapproximationstrategyinSection5.
Sinceenumeratingallpossibleclusteringsisintractable,wedescribeinSection
6anheuristicbasedonspectralclusteringtolimitthenumberofcandidate
2
clusterings.Weevaluatetheproposedmethodonsyntheticandrealdatain
Sections7and8,respectively.Finally,wediscussour˝ndingsinSection9.
2RelatedWork
Findingaclusteringofvariablesisequivalentto˝ndinganappropriateblock
structureofthecovariancematrix.Recently,Tanetal.[2015]andDevijverand
Gallopin[2016]suggestedtodetectblockdiagonalstructurebythresholdingthe
absolutevaluesofthecovariancematrix.Theirmethodsperformmodelselection
usingthemeansquarederrorofrandomlyleftoutelementsofthecovariance
matrix[Tanetal.,2015],andaslopeheuristic[DevijverandGallopin,2016].
AlsoseveralBayesianlatentvariablemodelshavebeenproposedforthistask
[MarlinandMurphy,2009,Sunetal.,2014,Pallaetal.,2012].Eachclustering,
includingthenumberofclusters,iseitherevaluatedusingthevariationallower
bound[MarlinandMurphy,2009],orbyplacingaDirichletProcesspriorover
clusterings[Pallaetal.,2012,Sunetal.,2014].However,alloftheabovemethods
assumethatthepartialcorrelationsofvariablesacrossclustersareexactlyzero.
Anexceptionistheworkin[Marlinetal.,2009]whichproposestoregularize
theprecisionmatrixsuchthatpartialcorrelationsofvariablesthatbelongto
thesameclusterarepenalizedlessthanthosebelongingtodi˙erentclusters.
Forthatpurposetheyintroducethreehyper-parameters,

1
(forwithincluster
penalty),

0
(foracrossclusters),with

0
>
1
,and

D
forapenaltyofthe
diagonalelements.Theclustersdonotneedtobeknowna-prioriandare
estimatedbyoptimizingalowerboundonthemarginallikelihood.Assuchtheir
methodcanalso˝ndvariableclusterings,evenwhenthetruepartialcorrelation
ofvariablesindi˙erentclustersisnotexactlyzero.However,theclustering
resultisin˛uencedbythreehyperparameters

0
;
1
,and

D
whichhavetobe
determinedusingcross-validation.
Recently,theworkin[Sunetal.,2015,HosseiniandLee,2016]relaxesthe
assumptionofacleanblockstructurebyallowingsomevariablestocorrespond
totwoclusters.Themodelselectionissue,inparticular,determiningthenumber
ofclusters,iseitheraddressedwithsomeheuristics[Sunetal.,2015]orcross-
validation[HosseiniandLee,2016].
3TheBayesianGaussianGraphicalModelfor
Clustering
OurstartingpointforvariableclusteringisthefollowingBayesianGaussian
graphicalmodel.Letusdenoteby
p
thenumberofvariables,and
n
thenumber
ofobservations.Weassumethateachobservation
x
2
R
p
isgeneratedi.i.d.from
amultivariatenormaldistributionwithzeromeanandcovariancematrix

.
Assumingthatthereare
k
groupsofvariablesthataremutuallyindependent,we
knowthat,afterappropriatepermutationofthevariables,

hasthefollowing
blockstructure
3
=
0
B
@

1
00
0
.
.
.
0
00
k
1
C
A
;
where

j
2
R
p
j

p
j
,and
p
j
isthenumberofvariablesincluster
j
.
ByplacinganinverseWishartpriorovereachblock

j
,wearriveatthe
followingBayesianmodel
p
(
x
1
;:::;
x
n
;

jf

j
g
j
;
f

j;
0
g
j
;
C
)
=
n
Y
i
=1
Normal
(
x
i
j
0
;

k
Y
j
=1
InvW

j
j

j
;

j;
0
)
;
(1)
where

j
and

j;
0
,arethedegreesoffreedomandthescalematrix,respectively.
Weset

j
=
p
j
+1
;

j
=
I
p
j
leadingtoanon-informativeprioron

j
.
C
denotes
thevariableclusteringwhichimposestheblockstructureon

.Wewillreferto
thismodelasthebasicinverseWishartpriormodel.
Assumingwearegivenasetofpossiblevariableclusterings
C
,wecanthen
choosetheclustering
C

thatmaximizestheposteriorprobabilityoftheclustering,
i.e.
C

=argmax
C2
C
p
(
Cj
X
)=argmax
C2
C
p
(
X
jC
)

p
(
C
)
;
wherewedenoteby
X
theobservations
x
1
;:::;
x
n
,and
p
(
C
)
isaprioroverthe
clusteringswhichweassumetobeuniform.Here,wereferto
p
(
X
jC
)
asthe
marginallikelihood(giventheclustering).ForthebasicinverseWishartprior
modelthemarginallikelihoodcanbecalculatedanalytically,seee.g.[Lenkoski
andDobra,2011].
4ProposedModel
Inthissection,weextendtheBayesianmodelfromEquation
(1)
toaccount
fornon-zeropartialcorrelationsbetweenvariablesindi˙erentclusters.For
thatpurposeweintroducethematrix


2
R
p

p
thatmodelsthenoiseonthe
precisionmatrix.Thefulljointprobabilityofourmodelisgivenasfollows:
p
(
x
1
;:::;
x
n
;

;


j


;


0
;
f

j
g
j
;
f

j;
0
g
j
;
C
)
=
n
Y
i
=1
Normal
(
x
i
j
0
;


InvW


j


;


0
)
k
Y
j
=1
InvW

j
j

j
;

j;
0
)
;
(2)
4
where
:=

1
+



1

)

1
,and
:=
0
B
@

1
00
0
.
.
.
0
00
k
1
C
A
:
Asbefore,theblockstructureof

isgivenbytheclustering
C
.Theproposed
modelisthesamemodelasinEquation
(1)
,withthemaindi˙erencethatthe
noiseterm



1

isaddedtotheprecisionmatrixofthenormaldistribution.
1
˛
>
0
isahyper-parameterthatis˝xedtoasmallpositivevalue
accountingforthedegreeofnoiseontheprecisionmatrix.Furthermore,we
assumenon-informativepriorson

j
and


bysetting

j
=
p
j
+1
;

j
=
I
p
j
and


=
p
+1
;


0
=
I
p
.
Remarkontheparameterization
Wenotethatasanalternativeparam-
eterization,wecouldhavede˝ned
:=

1
+

1

)

1
,andinsteadplacea
prioron


thatencourages


1

tobesmallintermsofsomematrixnorm.For
example,wecouldhaveset


0
=
1

I
p
.
5EstimationoftheMarginalLikelihood
Themarginallikelihoodofthedatagivenourproposedmodelcanbeexpressed
asfollows:
p
(
x
1
;:::;
x
n
j


;


0
;
f

j
g
j
;
f

j;
0
g
j
;
C
)
=
Z
Normal
(
x
1
;:::;
x
n
j
0
;


k
Y
j
=1
InvW

j
j

j
;

j;
0
)
d

j
˜
0)

InvW


j


;


0
)
d


˜
0)
:
where
:=

1
+



1

)

1
.
Clearly,if

=0
,werecoverthebasicinverseWishartpriormodel,as
discussedinSection3,andthemarginallikelihoodhasaclosedformsolutiondue
totheconjugacyofthecovariancematrixoftheGaussianandtheinverseWishart
prior.However,if
>
0
,thereisnoanalyticsolutionanymore.Therefore,we
proposetoeitheruseanestimatebasedonavariationalapproximation(Section
5.2)oronMCMC(Section5.3).Bothofourestimatesrequirethecalculationof
themaximumaposteriorsolutionwhichweexplain˝rstinSection5.1.
RemarkonBICtypeapproximationofthemarginallikelihood
We
notethatforourproposedmodelanapproximationofthemarginallikelihood
usingBICisnotsensible.Toseethis,recallthatBICconsistsoftwoterms:the
datalog-likelihoodunderthemodelwiththemaximumlikelihoodestimate,and
5
apenaltydependingonthenumberoffreeparameters.Themaximumlikelihood
estimateis
^

;
^


=argmax

;


n
X
i
=1
log
Normal
(
x
i
j
0
;


1
+



1

)

1
)
;
where
S
isthesamplecovariancematrix.Notethatwithoutthespeci˝cation
ofaprior,itisvalidthat
^

;
^


arenotpositivede˝niteaslongasthematrix
^


1
+

^


1

ispositivede˝nite.Therefore
^


1
+

^


1

=
S

1
,andthedata
likelihoodunderthemodelwiththemaximumlikelihoodestimateissimply
P
n
i
=1
log
Normal
(
x
i
j
0
;S
)
,whichisindependentoftheclustering.Thenumber
of
free
parametersis
(
p
2

p
)
=
2
whichisalsoindependentoftheclustering.That
means,foranyclusteringweendupwiththesameBIC.
Furthermore,aLaplacianapproximationasusedinthegeneralizedBayesian
informationcriterion[Konishietal.,2004]isalsonotsuitable,sinceinourcase
theparameterspaceisoverthepositivede˝nitematrices.
5.1Calculationofmaximumaposteriorsolution
Firstnotethat
p

;


j
x
1
;:::;
x
n
;

;


0
;
f

j
g
j
;
f

j;
0
g
j
;
C
)
/
Normal
(
x
1
;:::;
x
n
j
0
;


k
Y
j
=1
InvW

j
j

j
;

j;
0
)

InvW


j


;


0
)
where
:=

1
+



1

)

1
.
Therefore,
log
p

;


j
x
1
;:::;
x
n
;

;


0
;
f

j
g
j
;
f

j;
0
g
j
;
C
)=

n
2
log
j

j
n
2
trace
(
S


1
)



+
p
+1
2
log
j


j
1
2
trace


0


1

)
+
k
X
j
=1



j
+
p
j
+1
2
log
j

j
j
1
2
trace

j;
0


1
j
)

+
const
=
1
2

n

log
j


1
j
n

trace
(
S


1
)
+(


+
p
+1)

log
j


1

j
trace


0


1

)
+
k
X
j
=1

(

j
+
p
j
+1)

log
j


1
j
j
trace

j;
0


1
j
)

+
const;
6
wheretheconstantiswithrespectto


;

1
;:::

k
,and
p
j
denotesthenumber
ofvariablesincluster
j
.
Solutionusinga3-BlockADMM
FindingtheMAPcanbeformulatedas
aconvexoptimizationproblembyachangeofparameterization:byde˝ning
X
:=

1
,
X
j
:=

1
j
,and
X

:=

1

,wegetthefollowingconvexoptimization
problem:
minimize
X
˜
0
;X

˜
0
n

trace
(
S
(
X
+
X

))

n

log
j
X
+
X

j
+
trace
(
A

X

)

a


log
j
X

j
+
k
X
j
=1

trace
(
A
j
X
j
)

a
j

log
j
X
j
j

;
(3)
where,forsimplifyingnotation,weintroducedthefollowingconstants:
A

:=

0
;
a

:=


+
p
+1
;
A
j
:=
j;
0
;
a
j
:=

j
+
p
j
+1
:
Fromthisform,weseeimmediatelythattheproblemisstrictlyconvexjointly
in
X

and
X
.
1
WefurtherreformulatetheproblembyintroducinganadditionalvariableZ:
minimize
f
(
X

;X
1
;:::;X
k
;Z
)
subjectto
Z
=
X
+
X

;
X

;X
1
;:::;X
k
;Z

0
;
with
f
(
X

;X
1
;:::;X
k
;Z
):=
n

trace
(
SZ
)

n

log
j
Z
j
+
trace
(
A

X

)

a


log
j
X

j
+
k
X
j
=1

trace
(
A
j
X
j
)

a
j

log
j
X
j
j

:
Itistemptingtousea2-BlockADMMalgorithm,likee.g.in[Boydetal.,
2011],whichleadstotwooptimizationproblems:updateof
X;X

andupdate
of
Z
.However,unfortunately,inourcasetheresultingoptimizationproblem
forupdating
X;X

doesnothaveananalyticsolution.Therefore,instead,we
suggesttheuseofa3-BlockADMM,whichupdatesthefollowingsequence:
1
Since

log
j
X
j
isastrictlyconvexfunctionand
trace
(
XS
)
isalinearfunction.
7
X
t
+1
:=argmin
X
1
;:::;X
k
˜
0
k
X
j
=1

trace
(
A
j
X
j
)

a
j

log
j
X
j
j

+
trace
(
U
t
(
X
+
X
t


Z
t
))
+
ˆ
2
jj
X
+
X
t


Z
t
jj
2
F
;
X
t
+1

:=argmin
X

˜
0
trace
(
A

X

)

a


log
j
X

j
+
trace
(
U
t
(
X
t
+1
+
X


Z
t
))
+
ˆ
2
jj
X
t
+1
+
X


Z
t
jj
2
F
;
Z
t
+1
:=argmin
Z
˜
0
n

trace
(
SZ
)

n

log
j
Z
j
+
trace
(
U
t
(
X
t
+1
+
X
t
+1


Z
))
+
ˆ
2
jj
X
t
+1
+
X
t
+1


Z
jj
2
F
;
U
t
+1
:=
ˆ
(
X
t
+1
+
X
t
+1


Z
t
+1
)+
U
t
;
where
U
istheLagrangemultiplier,and
X
t
;Z
t
;U
t
,denotes
X;Z;U
at
iteration
t
;
ˆ>
0
isthelearningrate.
2
Eachoftheabovesub-optimizationproblemcanbesolvede˚cientlyviathe
followingstrategy.Thezerogradientconditionforthe˝rstoptimizationproblem
withvariable
X
is

X

1
j
+
ˆ
a
j
X
j
=

1
a
j
(
A
j
+
U
j
+
ˆ
(
X


Z
j
))
:
Thezerogradientconditionforthesecondoptimizationproblemwithvariable
X

is

X

1

+

2
a

X

=

1
a

(
A

+
U
+

(
X

Z
))
:
Thezerogradientconditionforthethirdoptimizationproblemwithvariable
Z
is

Z

1
+
ˆ
n
Z
=
1
n
(
U

nS
+
ˆ
(
X
+
X

))
:
Eachoftheabovethreeoptimizationproblemcanbesolvedviaaneigenvalue
decompositionasfollows.Weneedtosolve
V
suchthatitsatis˝es:

V

1
+

=
R
^
V

0
Since
R
isasymmetricmatrix(notnecessarilypositiveornegativesemi-de˝nite),
wehavetheeigenvaluedecomposition:
QLQ
T
=
R;
2
Inourexperiments,wesetthelearningrate
ˆ
initiallyto1.0,andincreaseitevery100
iterationsbyafactorof
1
:
1
.Wefoundexperimentallythatthisspeeds-uptheconvergenceof
ADMM.
8
where
Q
isanorthonormalmatrixand
L
isadiagonalmatrixwithrealvalues.
Denoting
Y
:=
Q
T
VQ
,wehave

Y

1
+

=
L;
(4)
Sincethesolution
Y
mustalsobeadiagonalmatrix,wehave
Y
ij
=0
,for
j
6
=
i
,
andwemusthavethat

(
Y
ii
)

1
+

ii
=
L
ii
:
(5)
Then,Equation(5)isequivalentto

2
ii

L
ii
Y
ii

1=0
;
andthereforeonesolutionis
Y
ii
=
L
ii
+
p
L
2
ii
+4

2

:
Notethatfor
>
0
,wehavethat
Y
ii
>
0
.Therefore,wehavethattheresulting
Y
solvesEquation(4)andmoreover
V
=
QYQ
T
˜
0
:
Thatmeans,wecansolvethesemi-de˝niteproblemwithonlyoneeigenvalue
decomposition,andthereforeisin
O
(
p
3
)
.
Finally,wenotethatincontrasttothe2-blockADMM,ageneral3-block
ADMMdoesnothaveaconvergenceguaranteeforany
ˆ>
0
.However,using
arecentresultfrom[Linetal.,2015],wecanshowinAppendixAthatinour
casetheconditionsforconvergencearemetforany
ˆ>
0
.
5.2VariationalApproximationoftheMarginalLikelihood
Hereweexplainourstrategyforthecalculationofavariationalapproximationof
themarginallikelihood.Forsimplicity,let

denotethevectorofallparameters,
X
theobserveddata,and

thevectorofallhyper-parameters.
Let
^

denotetheposteriormode.Furthermore,let
g
(

)
beanapproximation
oftheposteriordistribution
p
(

j
X
;

;
C
)
thatisaccuratearoundthemode
^

.
Thenwehave
p
(
X
j

;
C
)=
p
(

;
X
j

;
C
)
p
(

j
X
;

;
C
)
=
p
(
^

;
X
j

;
C
)
p
(
^

j
X
;

;
C
)
ˇ
p
(
^

;
X
j

;
C
)
g
(
^

)
:
(6)
NotethatfortheLaplaceapproximationwewoulduse
g
(

)=
N
(

j
^

;V
)
,
where
V
isanappropriatecovariancematrix.However,heretheposterior
p
(

j
X
;

;
C
)
isaprobabilitymeasureoverthepositivede˝nitematricesandnot
over
R
d
,whichmakestheLaplaceapproximationinappropriate.
9
Instead,wesuggesttoapproximatetheposteriordistribution
p


;

1
;:::

k
j
x
1
;:::;
x
n
;

;


0
;
f

j
g
j
;
f

j;
0
g
j
;
C
)
bythefactorizeddistribution
g
:=
g



)

k
Y
j
=1
g
j

j
)
:
Wede˝ne
g



)
and
g
j

j
)
asfollows:
g



):=
InvW


j

g
;

g
)
;
with

g
:=(

g
+
p
+1)

^


;
where
^


isthemodeoftheposteriorprobability
p


j
X
;

;
C
)
(ascalculated
intheprevioussection).Notethatthischoiceensuresthatthemodeof
g

isthe
sameasthemodeof
p


j
x
1
;:::;
x
n
;

;
C
)
.Analogously,weset
g
j

j
):=
InvW

j
j

g;j
;

g;j
)
;
with

g;j
:=(

g;j
+
p
j
+1)

^

j
;
where
^

j
isthemodeoftheposteriorprobability
p

j
j
X
;

;
C
)
.Theremain-
ingparameters

g
2
R
and

g;j
2
R
areoptimizedbyminimizingtheKL-
divergencebetweenthethefactorizeddistribution
g
andtheposteriordistribu-
tion
p


;

1
;:::

k
j
x
1
;:::;
x
n
;

;
C
)
.Thedetailsofthefollowingderivationsare
10
giveninAppendixB.Forsimplicityletusdenote
g
J
:=
Q
k
j
=1
g
j
,thenwehave
KL
(
g
jj
p
)=

Z
g



)

k
Y
j
=1
g
j

j
)
log
p


;

1
;:::

k
;
x
1
;:::;
x
n
j

;
C
)
g



)

Q
k
j
=1
g
j

j
)
d


d

+
c
=

1
2
n
E
g
J
;g

[log
j


1
+



1

j
]
+
1
2
(


+
p
+1)
E
g

[log
j


j
]
+
1
2
trace


0
+
nS
)
E
g



1

])

Entropy
[
g

]
+
1
2
k
X
j
=1
(

j
+
p
j
+1)
E
g
j
[log
j

j
j
]
+
1
2
k
X
j
=1
trace

j;
0
+
nS
j
)
E
g
j


1
j
])

k
X
j
=1
Entropy
[
g
j
]+
c;
where
c
isaconstantwithrespectto
g

and
g
j
.However,theterm
E
g
J
;g

[
log
j


1
+



1

j
]
cannotbesolvedanalytically,thereforeweneedtoresorttosomesortof
approximation.
Weassumethat
E
g
J
;g

[log
j


1
+



1

j
]
ˇ
E
g
J
;g

[log
j


1
j
]
.Thisway,weget
KL
(
g
jj
p
)
ˇ
KL
(
g

jj
InvW
(


;


0
+
nS
))
+
k
X
j
=1
KL
(
g
j
jj
InvW
(

j
+
n;

j;
0
+
nS
j
))
+
c
0
;
whereweusedthat
E
g
J
;g

[log
j


1
j
]=

k
X
j
=1
E
g
j
[log
j

j
j
]
;
and
c
0
isaconstantwithrespectto
g

and
g
j
.
Fromtheaboveexpression,weseethatwecanoptimizetheparametersof
g

11
and
g
j
independentlyfromeachother.Theoptimalparameter
^

g
for
g

is
^

g
=argmin

g
KL
(
g

jj
InvW
(


;


0
+
nS
))
=argmin

g

g

g
+
p
+1
trace


0
+
nS
)
^


1

)

2log
p
(

g
2
)


g
p
+


log(

g
+
p
+1)
+(

g



)
p
X
i
=1
 


g

p
+
i
2

:
Andanalogously,wehave
^

g;j
=argmin

g;j

g;j

g;j
+
p
j
+1
trace

j;
0
+
nS
j
)
^


1
j
)

2log
p
j
(

g;j
2
)


g;j
p
j
+
p
j
(

j
+
n
)log(

g;j
+
p
j
+1)
+(

g;j


j

n
)
p
j
X
i
=1
 


g;j

p
j
+
i
2

:
Eachisaonedimensionalnon-convexoptimizationproblemthatwesolvewith
Brent'smethod[Brent,1971].
5.3MCMCEstimationofMarginalLikelihood
Asanalternativetothevariationalapproximation,weinvestigateanMCMC
estimationbasedonChib'smethod[Chib,1995,ChibandJeliazkov,2001].
Tosimplifythedescription,weintroductionthefollowingnotations

1
:=

;

2
;:::;

k
+1
:=
1
;:::;

k
:
Furthermore,wede˝ne

<i
:=
f

1
;:::;

i

1
g
and

>i
:=
f

i
+1
;:::;

k
+1
g
.For
simplicity,wealsosuppressinthenotationtheexplicitconditioningonthe
hyper-parameters

andtheclustering
C
,whichareboth˝xed.
FollowingthestrategyofChib[1995],themarginallikelihoodcanbeexpressed
as
p
(
X
)=
p
(
^

1
;:::;
^

k
+1
;
X
)
p
(
^

1
;:::;
^

k
+1
j
X
)
=
p
(
^

1
;:::;
^

k
+1
;
X
)
Q
k
+1
i
=1
p
(
^

i
j
X
;
^

1
:::;
^

i

1
)
(7)
Inordertoapproximate
p
(
X
)
withEquation
(7)
,weneedtoestimate
p
(
^

i
j
X
;
^

1
;:::
^

i

1
)
.First,notethatwecanexpressthevalueoftheconditional
12
posteriordistributionat
^

i
,asfollows(seeChibandJeliazkov[2001],Section
2.3):
p
(
^

i
j
X
;
^

1
;:::
^

i

1
)
=
E


i
˘
p
(


i
j
X
;
^

<i
)
[

(

i
;
^

i
j
^

<i
;

>i
)
q
i
(
^

i
)]
E


i
˘
p
(

>i
j
X
;
^


i
)
q
(

i
)
[

(
^

i
;

i
j
^

<i
;

>i
)]
;
(8)
where
q
i
(

i
)
isaproposaldistributionfor

i
,andtheacceptanceprobabilityof
movingfromstate

i
tostate

0
i
,holdingtheotherstates˝xedisde˝nedas

(

i
;

0
i
j

<i
;

>i
):=min
f
1
;
p
(
X
;

<i
;

>i
;

0
i
)

q
i
(

i
)
p
(
X
;

<i
;

>i
;

i
)

q
i
(

0
i
)
g
:
(9)
Next,usingEquation(8),wecanestimate
p
(
^

i
j
X
;
^

1
;:::
^

i

1
)
withaMonteCarloapproximationwith
M
samples:
p
(
^

i
j
X
;
^

1
;:::
^

i

1
)
ˇ
1
M
P
M
m
=1

(

i;m
i
;
^

i
j
^

<i
;

i;m
>i
)
q
i
(
^

i
)
1
M
P
M
m
=1

(
^

i
;

q;m
i
j
^

<i
;

i
+1
;m
>i
)
(10)
where

a;m
i
˘
p
(

i
j
X
;
^

<a
)
,

a;m
>i
˘
p
(

>i
j
X
;
^

<a
)
,and

q;m
i
˘
q
(

i
)
.
Finally,inordertosamplefrom
p
(


i
j
X
;
^

<i
)
,weproposetousethe
Metropolis-HastingswithinGibbssamplerasshowninAlgorithm1.
MH
j
(

t
j
;
 
)
denotestheMetropolis-Hastingsalgorithmwithcurrentstate

t
j
,andacceptance
probability

(

j
;

0
j
j
 
)
,Equation
(9)
,and

0

i
isasampleaftertheburn-in.For
theproposaldistribution
q
i
(

i
)
,weuse
q
i
:=
8
>
>
<
>
>
:
InvW
(
;
^



(

+
p
+1))
with

=


n
+


if
i
=1
;
InvW
(
;
^

i

1

(

+
p
i

1
+1))
with

=(1


)


n
+

i

1
else.
(11)
Here
>
0
isahyper-parameteroftheMCMCalgorithmthatischosentocontrol
theacceptanceprobability.Notethatifwechoose

=1
and

is0,thenthe
proposaldistribution
q
i
(

i
)
equalstheposteriordistribution
p
(

i
j
X
;
^

1
;:::
^

i

1
)
.
However,inpractice,wefoundthattheacceptanceprobabilitiescanbetoosmall,
leadingtounstableestimatesanddivisionby0inEquation
(10)
.Therefore,for
ourexperimentswechose

=10
.
6Restrictingthehypothesesspace
ThenumberofpossibleclusteringsfollowtheBellnumbers,andthereforeitis
infeasibletoenumerateallpossibleclusterings,evenifthenumberofvariables
p
issmall.Itisthereforecrucialtorestrictthehypothesesspacetoasubsetofall
clusteringsthatarelikelytocontainthetrueclustering.Wedenotethissubset
as
C

.
13
Algorithm1
Metropolis-HastingswithinGibbssamplerforsamplingfrom
p
(


i
j
X
;
^

<i
)
.
for
t
from1to
M
do
for
j
from
i
to
k
+1
do
 
:=
f
^

<i
;

t
i
;:::;

t
j

1
;

t

1
>j
g

t
j
:=
MH
j
(

t

1
j
;
 
)
endfor
endfor
Wesuggesttousespectralclusteringondi˙erentestimatesoftheprecision
matrixtoacquirethesetofclusterings
C

.Amotivationforthisheuristicis
giveninAppendixC.
First,foranappropriate

,weestimatetheprecisionmatrixusing
X

:=argmin
X

0

log
j
X
j
+
trace
(
XS
)+

X
i
6
=
j
j
X
ij
j
q
:
(12)
Inourexperiments,wetake
q
=1
,whichisequivalenttotheGraphicalLasso
[Friedmanetal.,2008]withanl1-penaltyonallentriesof
X
exceptthediagonal.
Inthenextstep,wethenconstructtheLaplacian
L
asde˝nedinthefollowing.
L
ii
=
X
k
6
=
i
j
X

ik
j
q
;
L
ij
=

X

ij
j
q
for
i
6
=
j:
(13)
Finally,weusek-meansclusteringontheeigenvectorsoftheLaplacianL.The
detailsofacquiringthesetofclusterings
C

usingthespectralclusteringmethod
aresummarizedbelow:
InSection7.1wecon˝rmexperimentallythat,eveninthepresenceofnoise,
C

oftencontainsthetrueclustering,orclusteringsthatareclosetothetrue
clustering.
6.1Posteriordistributionovernumberofclusters
Inprinciple,theposteriordistributionforthenumberofclusterscanbecalculated
using
p
(
k
j
X
)
/
X
C2
C
k
p
(
X
jC
)
;
where
C
k
denotesthesetofallclusteringswithnumberofclustersbeingequalto
k
.Sincethisiscomputationallyinfeasible,weusethefollowingapproximation
P
(
k
j
X
)
/
X
C2
C
k
p
(
X
jC
)
ˇ
X
C2
C

k
p
(
X
jC
)
;
where
C

k
isthesetofallclusteringswith
k
clustersthatareintherestricted
hypothesesspace
C

.
14
Algorithm2
SpectralClusteringforvariableclusteringwiththeGaussian
graphicalmodel.
J
:=setofregularizationparametervalues.
K
max
:=maximumnumberofconsideredclusters.
C

:=
fg
for

2
J
do
X

:=
solveoptimizationproblemfromEquation(12).
(
e
1
;:::;
e
K
max
):=
determinetheeigenvectorscorrespondingtothe
K
max
lowesteigenvaluesoftheLaplacian
L
asde˝nedinEquations(13).
for
k
2f
2
;:::;K
max
g
do
C

:=
clusterallvariablesinto
k
partitionsusingk-meanswith
(
e
1
;:::;
e
k
)
.
C

:=
C

[C

endfor
endfor
return
restrictedhypothesesspace
C

7SimulationStudy
Inthissection,weevaluatetheproposedmethodonsimulateddataforwhich
thegroundtruthisavailable.Insub-section7.1,weevaluatethequalityofthe
restrictedhypothesesspace
C

,followedbysub-section7.2,whereweevaluated
theproposedmethod'sabilitytoselectthebestclusteringin
C

.
Forthenumberofclustersweconsidertherangefrom
2
to
15
.Forthesetof
regularizationparametersofthespectralclusteringmethodweuse
J
:=
f
0.0001,
0.0005,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01
g
(see
Algorithm2).
Inallexperimentsthenumberofvariablesis
p
=40
,andthegroundtruthis
4clusterswith10variableseach.
Forgeneratingpositive-de˝nitecovariancematrices,weconsiderthefollowing
twodistributions:
InvW
(
p
+1
;I
p
)
,and
Uniform
p
,withdimension
p
.Wedenote
by
U
˘
Uniform
p
thepositive-de˝nitematrixgeneratedinthefollowingway
U
=
A
+(0
:
001


min
(
A
))
I
p
;
where

min
(
A
)
isthesmallesteigenvalueof
A
,and
A
isdrawnasfollows
A
i;j
=
A
j;i
˘
Uniform
(

1
;
1)
;i
6
=
j
A
i;i
=0
:
Forgenerating

,weeithersampleeachblock
j
from
InvW
(
p
j
+1
;I
p
j
)
or
fromUniform
p
j
.
Forgeneratingthenoisematrix


,wesampleeitherfrom
InvW
(
p
+1
;I
p
)
orfromUniform
p
.The˝naldataisthensampledasfollows
x
˘
N
(0
;


1
+



1

)

1
)
;
15
where

de˝nesthenoiselevel.
Forevaluationweusetheadjustednormalizedmutualinformation(ANMI),
where
0
:
0
meansthatanycorrespondencewiththetruelabelsisatchancelevel,
and
1
:
0
meansthataperfectone-to-onecorrespondenceexists[Vinhetal.,2010].
Werepeatedallexperiments5timesandreporttheaverageANMIscore.
7.1Evaluationoftherestrictedhypothesesspace
First,independentofanymodelselectioncriteria,wecheckherethequality
oftheclusteringsthatarefoundwiththespectralclusteringalgorithmfrom
Section6.Wealsocomparetosingleandaveragelinkageclusteringasusedin
[Tanetal.,2015].
Thesetofallclusteringsthatarefoundisdenotedby
C

(therestricted
hypothesesspace).
Inordertoevaluatethequalityoftherestrictedhypothesesspace
C

,we
reporttheoracleperformancecalculatedby
max
C2
C

ANMI
(
C
;
C
T
)
,where
C
T
denotesthetrueclustering,and
ANMI
(
C
;
C
T
)
denotestheANMIscorewhen
comparingclustering
C
withthetrueclustering.Inparticular,ascoreof1.0
meansthatthetrueclusteringiscontainedin
C

.
Theresultsofallexperimentswithnoiselevel

2f
0
:
0
;
0
:
01
;
0
:
1
g
areshown
inTables1,forbalancedclusters,andTable2,forunbalancedclusters.
Fromtheseresultsweseethattherestrictedhypothesesspaceofspectral
clusteringisaround100,considerablysmallerthanthenumberofallpossible
clusterings.Moreimportantly,wealsoseethatthat
C

acquiredbyspectral
clusteringeithercontainsthetrueclusteringoraclusteringthatisclosetothe
truth.Incontrast,thehypothesesspacerestrictedbysingleandaveragelinkage
issmaller,butmoreoftenmissesthetrueclustering.
7.2Evaluationofclusteringselectioncriteria
Here,weevaluatetheperformanceofourproposedmethodforselectingthe
correctclusteringintherestrictedhypothesesspace
C

.Wecompareour
proposedmethod(variational)withseveralbaselinesandtwopreviouslyproposed
methods[Tanetal.,2015,Pallaetal.,2012].Exceptforthetwopreviously
proposedmethods,wecreated
C

withthespectralclusteringalgorithmfrom
Section6.
Asaclusterselectioncriteria,wecompareourmethodtotheExtended
BayesianInformationCriterion(EBIC)with

2f
0
;
0
:
5
;
1
g
[ChenandChen,
2008,FoygelandDrton,2010],AkaikeInformationCriteria[Akaike,1973],and
theCalinski-HarabazIndex(CHI)[Cali«skiandHarabasz,1974].NotethatEBIC
andAICarecalculatedbasedonthebasicGaussiangraphicalmodel(i.e.the
modelinEquation1,butignoringthepriorspeci˝cation).
3
Furthermore,wenote
thatEBICismodelconsistent,andtherefore,assumingthatthetrueprecision
matrixcontainsnon-zeroentriesineachelement,willchooseasymptotically
3
AsdiscussedinSection5,EBIC(andalsoAIC)cannotbeusedwithourproposedmodel.
16
theclusteringthathasonlyoneclusterwithallvariablesinit.However,asan
advantageforEBIC,weexcludethatclustering.Furthermore,wenotethatin
contrasttoEBICandAIC,theCalinski-HarabazIndexisnotamodel-based
clusterevaluationcriterion.TheCalinski-HarabazIndexisanheuristicthatuses
asclusteringcriteriontheratioofthevariancewithinandacrossclusters.As
suchitisexpectedtogivereasonableclusteringresultsifthenoiseisconsiderably
smallerinmagnitudethanthewithin-clustervariablepartialcorrelations.
WeremarkthatEBICandAICisnotwellde˝nedifthesamplecovariance
matrixissingular,inparticularif
n<p
or
n
ˇ
p
.Asanad-hodremedy,which
workswellinpractice
4
,wealwaysadd
0
:
001
timestheidentitymatrixtothe
covariancematrix(seealsoLedoitandWolf[2004]).
Finally,wealsocomparetheproposedmethodtotwopreviousapproaches
forvariableclustering:theclusteredgraphicallasso(CGL)asproposedin[Tan
etal.,2015],andtheDirichletprocessvariableclustering(DPVC)modelas
proposedin[Pallaetal.,2012],forwhichtheimplementationisavailable.DPVC
modelsthenumberofclustersusingaDirichletprocess.CGLusesformodel
selectionthemeansquarederrorforrecoveringrandomlyleft-outelementsofthe
covariancematrix.CGLusesforclusteringeitherthesinglelinkageclustering
(SLC)ortheaveragelinkageclustering(ALC)method.Forconciseness,weshow
onlytheresultsforALC,sincetheytendedtobebetterthanSLC.
Theresultsofallexperimentswithnoiselevel

2f
0
:
0
;
0
:
01
;
0
:
1
g
areshown
inTables3and4,forbalancedclusters,andTables5and6,forunbalanced
clusters.
Thetablesalsocontaintheperformanceoftheproposedmethodfor

2
f
0
;
0
:
01
;
0
:
02
;
0
:
03
g
.Notethat

=0
:
0
correspondstothebasicinverseWishart
priormodelforwhichwecancalculatethemarginallikelihoodanalytically.
Comparingtheproposedmethodwithdi˙erent

,weseethat

=0
:
02
o˙ersgoodclusteringperformanceinthenonoiseandnoisysetting.Incontrast,
modelselectionwithEBICandAICperforms,asexpected,wellinthenonoise
scenario,however,inthenoisysettingtheytendtoselectincorrectclusterings.In
particularforlargesamplesizesEBICtendstofailtoidentifycorrectclusterings.
TheCalinski-HarabazIndexperformswellinthenoisysettings,whereasin
thenonoisesettingitperformsunsatisfactory.
InFigures1and2,weshowtheposteriordistributionwithandwithoutnoise
ontheprecisionmatrix,respectively.
5
Inbothcases,giventhatthesamplesize
n
islargeenough,theproposedmethodisabletoestimatecorrectlythenumber
ofclusters.Incontrast,thebasicinverseWishartpriormodelunderestimates
thenumberofclustersforlarge
n
andexistenceofnoiseintheprecisionmatrix.
4
Inparticularforthemutualfundsdatainthenextsection,wherethecovariancematrix
wasbadconditioned.
5
Samesettingasbefore,
p
=40
,

j
˘
InvW
(
p
j
+1
;I
p
j
)
.Noiseis


˘
InvW
(
p
+1
;I
p
)
;
=
0
:
01
.Proposedmethod

=0
:
02
.
17
Figure1:Posteriordistributionofthenumberofclustersoftheproposedmethod
(toprow)andthebasicinverseWishartpriormodel(bottomrow).Ground
truthis4clusters;thereisnonoiseontheprecisionmatrix.
7.3ComparisonofvariationalandMCMCestimate
Here,wecompareourvariationalapproximationwithMCMConasmallscale
simulatedproblemwhereitiscomputationallyfeasibletoestimatethemarginal
likelihoodwithMCMC.Wegeneratedsyntheticdataasintheprevioussection,
onlywiththedi˙erencethatwesetthenumberofvariables
p
to12.
Thenumberofsamples
M
forMCMCwassetto10000,whereweused10%
asburnin.Fortworandomlypickedclusteringsfor
n
=12
,and
n
=1200000
,we
checkedtheacceptanceratesandconvergenceusingthemultivariateextension
oftheGelman-Rubindiagnostic[BrooksandGelman,1998].Theaverage
acceptancerateswerearound
80%
andthepotentialscalereductionfactorwas
1.01.
TheruntimeofMCMCwasaround40minutesforevaluatingoneclustering,
whereasforthevariationalapproximationtheruntimewasaround2seconds.
6
TheresultsareshowninTable7,suggestingthatthequalityoftheselected
clusteringsusingthevariationalapproximationissimilartoMCMC.
8RealDataExperiments
Inthissection,weinvestigatethepropertiesoftheproposedmodelselection
criteriononthreerealdatasets.Inallcases,weusethespectralclustering
algorithmfromAppendixCtocreateclustercandidates.Allvariableswere
normalizedtohavemean0andvariance1.Forallmethods,exceptDPVC,the
numberofclustersisconsideredtobein
f
2
;
3
;
4
;:::;
min
(
p

1
;
15)
g
.DPVC
automaticallyselectsthenumberofclustersbyassumingaDirichletprocess
6
RuntimeononecoreofIntel(R)Xeon(R)CPU2.30GHz.
18
Figure2:Posteriordistributionofthenumberofclustersoftheproposedmethod
(toprow)andthebasicinverseWishartpriormodel(bottomrow).Ground
truthis4clusters;noisewasaddedtotheprecisionmatrix.
prior.Weevaluatedtheproposedmethodwith

=0
:
02
usingthevariational
approximation.
8.1MutualFunds
Hereweusethemutualfundsdata,whichhasbeenpreviouslyanalyzedin[Scott
andCarvalho,2008,Marlinetal.,2009].Thedatacontains59mutualfunds
(p=59)groupedinto4clusters:U.S.bondfunds,U.S.stockfunds,balanced
funds(containingU.S.stocksandbonds),andinternationalstockfunds.The
numberofobservationsis86.
TheresultsofallmethodsarevisualizedinTable8.Itisdi˚culttointerpret
theresultsproducedbyEBIC(

=1
:
0
),AICandtheCalinski-HarabazIndex.
Incontrast,theproposedmethodandEBIC(

=0
:
0
)produceresultsthatare
easiertointerpret.Inparticular,ourresultssuggestthatthereisaconsiderable
correlationbetweenthebalancedfundsandtheU.S.stockfundswhichwasalso
observedinMarlinetal.[2009].
InFigure3weshowatwodimensionalrepresentationofthedata,thatwas
foundusingLaplacianEigenmaps[BelkinandNiyogi,2003].The˝guresupports
theclaimthatbalancedfundsandtheU.S.stockfundshavesimilarbehavior.
8.2GeneRegulations
Wetestedourmethodalsoonthegeneexpressiondatathatwasanalyzedin
[Hiroseetal.,2017].Thedataconsistsof11geneswith445geneexpressions.
ThetruegeneregularizationsareknowninthiscaseandshowninFigure4,
adaptedfrom[Hiroseetal.,2017].Themostimportantfactisthatthereare
19
Figure3:Twodimensionalrepresentationofthemutualfundsdata.
twoindependentgroupsofgenesandanyclusteringthatmixesthesetwocanbe
consideredaswrong.
WeshowtheresultsofallmethodsinFigure5,wherewemarkeachcluster
withadi˙erentcolorsuperimposedonthetrueregularizationstructure.Here
onlytheclusteringselectedbytheproposedmethod,EBIC(

=1
:
0
)and
Calinski-Harabazcorrectlydividethetwogroupofgenes.
Figure4:GeneregulationsofE.coli.asgivenin[Hiroseetal.,2017,Albersts
etal.,2014]
20
8.3AviationSensors
Asathirddataset,weusethe˛ightaviationdatasetfromNASA
7
.Thedata
setcontainssensorinformationsampledfromairplanesduringoperation.We
extractedtheinformationof16continuous-valuedsensorsthatwererecordedfor
di˙erent˛ightswithintotal25032364samples.
TheclusteringresultsareshowninTable9.Thedatasetdoesnothaveany
groundtruth,buttheclusteringresultofourproposedmethodisreasonable:
Cluster9groupssensorsthatmeasureora˙ectaltitude
8
,Cluster8correctly
clusterstheleftandrightsensorsformeasuringtherotationaroundtheaxis
pointingthroughthenoiseoftheaircraft,inCluster2allsensorsthatmeasure
theanglebetweenchordand˛ightdirectionaregroupedtogether.Italsoappears
reasonablethattheyellowhydraulicsystemoftheleftpartoftheplanehaslittle
directinteractionwiththegreenhydraulicsystemoftherightpart(Cluster1
andCluster4).Andthesensorfortherudder,in˛uencingthedirectionofthe
plane,ismostlyindependentoftheothersensors(Cluster5).
Incontrast,theclusteringselectedbythebasicinverseWishartprior,EBIC,
andAICisdi˚culttointerpret.WenotethatwedidnotcomparetoDPVC,since
thelargenumberofsamplesmadetheMCMCalgorithmofDPVCinfeasible.
9DiscussionandConclusions
Wehaveintroducedanewmethodforevaluatingvariableclusteringsbasedon
themarginallikelihoodofaBayesianmodelthattakesintoaccountnoiseonthe
precisionmatrix.Sincethecalculationofthemarginallikelihoodisanalytically
intractable,weproposedtwoapproximations:avariationalapproximationandan
approximationbasedonMCMC.Experimentally,wefoundthatthevariational
approximationisconsiderablyfasterthanMCMCandalsoleadstoaccurate
modelselections.
Wecomparedourproposedmethodtoseveralstandardmodelselection
criteria.Inparticular,wecomparedtoBICandextendedBIC(EBIC)which
areoftenthemethodofchoiceformodelselectioninGaussiangraphicalmodels.
However,weemphasizethatEBICwasdesignedtohandlethesituationwhere
p
isintheorderof
n
,andhasnotbeendesignedtohandlenoise.Asaconsequence,
ourexperimentsshowedthatinpracticeitsperformancedependshighlyon
thechoiceofthe

parameter.Incontrast,theproposedmethod,with˝xed
hyper-parameters,showsbetterperformanceonvarioussimulatedandrealdata.
Wealsocomparedourmethodtoothertwopreviouslyproposedmethods,
namelyClusterGraphicalLasso(CGL)[Tanetal.,2015],andDirichletProcess
VariableClustering(DPVC)[Pallaetal.,2012]thatperformsjointlyclustering
andmodelselection.However,itappearsthatinmanysituationsthemodel
selectionalgorithmofCGLisnotabletodetectthetruemodel,evenifthere
7
https://c3.nasa.gov/dashlink/projects/85/
whereweuseallrecordsfromTail687.
8
Theelevatorpositionofanairplanein˛uencesthealtitude,andthestaticpressuresystem
ofanairplanemeasuresthealtitude.
21
isnonoise.Ontheotherhand,theDirichletprocessassumptionbyDPVC
appearstobeveryrestrictive,leadingagaintomanysituationswherethetrue
model(clustering)ismissed.Overall,ourmethodperformsbetterintermsof
selectingthecorrectclusteringonsyntheticdatawithgroundtruth,andselects
meaningfulclustersonrealdata.
Thepythonsourcecodeforvariableclusteringandmodelselectionwith
theproposedmethodandallbaselinesisavailableat
https://github.com/
andrade-stats/robustBayesClustering
.
AConvergenceof3-blockADMM
Wecanwritetheoptimizationproblemin(3)as
minimize
f
1
(
X

)+
f
2
(
X
1
;:::;X
k
)+
f
3
(
Z
)
subjectto

X

X

+
Z
=0
;
X

;X
1
;:::;X
k
˜
0
;
with
f
1
(
X

):=
trace
(
A

X

)

a


log
j
X

j
;
f
2
(
X
1
;:::;X
k
):=
k
X
j
=1

trace
(
A
j
X
j
)

a
j

log
j
X
j
j

;
f
3
(
Z
):=
n

trace
(
SZ
)

n

log
j
Z
j
:
Firstnotethatthefunctions
f
1
;f
2
and
f
3
areconvexproperclosedfunctions.
Since
X

;X
1
;:::;X
k
˜
0
,wehaveduetotheequalityconstraintthat
Z
˜
0
.
Assumingthattheglobalminimaisattained,wecanassumethat
Z

˙I
,
forsomelargeenough
˙>
0
.Asaconsequence,wehavethat
r
2
f
3
(
Z
)=
Z

1

Z

1

˙

2
I
,andtherefore
f
3
isastronglyconvexfunction.Analogously,
wehavethat
f
1
and
f
2
arestronglyconvexfunctions,andthereforealsocoercive.
ThisallowsustoapplyTheorem3.2in[Linetal.,2015]whichguaranteesthe
convergenceofthe3-blockADMM.
BDerivationofvariationalapproximation
Here,wegivemoredetailsoftheKL-divergenceminimizationfromSection5.2.
Recall,thattheremainingparameters

g
2
R
and

g;j
2
R
areoptimizedby
minimizingtheKL-divergencebetweenthethefactorizeddistribution
g
andthe
22
posteriordistribution
p


;

1
;:::

k
j
x
1
;:::;
x
n
;

;
C
)
.Wehave
KL
(
g
jj
p
)=

Z
g



)

k
Y
j
=1
g
j

j
)
log
p


;

1
;:::

k
;
x
1
;:::;
x
n
j

;
C
)
g



)

Q
k
j
=1
g
j

j
)
d


d

+
c
=

1
2
E
g
J
;g

[
n

log
j


1
+



1

)
j
]

1
2
E
g

[(


+
p
+1)

log
j


1

j

trace


0
+
nS


1

)]

Entropy
[
g

]
+
k
X
j
=1


1
2
E
g
j
[(

j
+
p
j
+1)

log
j


1
j
j

trace

j;
0
+
nS
j


1
j
)]

Entropy
[
g
j
]

+
c
=

1
2
n
E
g
J
;g

[log
j


1
+



1

j
]
+
1
2
(


+
p
+1)
E
g

[log
j


j
]
+
1
2
trace


0
+
nS
)
E
g



1

])

Entropy
[
g

]
+
1
2
k
X
j
=1
(

j
+
p
j
+1)
E
g
j
[log
j

j
j
]
+
1
2
k
X
j
=1
trace

j;
0
+
nS
j
)
E
g
j


1
j
])

k
X
j
=1
Entropy
[
g
j
]+
c;
where
c
isaconstantwithrespectto
g

and
g
j
.However,theterm
E
g
J
;g

[
log
j


1
+



1

j
]
cannotbesolvedanalytically,thereforeweneedtoresorttosomesortof
approximation.Assumingthat
E
g
J
;g

[log
j


1
+



1

j
]
ˇ
E
g
J
;g

[log
j


1
j
]
;
23
weget
KL
(
g
jj
p
)
ˇ
1
2
n
E
g
J
;g

[log
j


1
j
]
+
1
2
(


+
p
+1)
E
g

[log
j


j
]
+
1
2
trace


0
+
nS
)
E
g



1

])

Entropy
[
g

]
+
1
2
k
X
j
=1
(

j
+
p
j
+1)
E
g
j
[log
j

j
j
]
+
1
2
k
X
j
=1
trace

j;
0
+
nS
j
)
E
g
j


1
j
])

k
X
j
=1
Entropy
[
g
j
]+
c
=

E
g

[log

j


j

1
2
(


+
p
+1)
e

1
2
trace


0
+
nS


1

)

]

Entropy
[
g

]

k
X
j
=1
E
g
j
[log

j

j
j

1
2
(

j
+
n
+
p
j
+1)
e

1
2
trace

j;
0
+
nS
j


1
j
)

]+
Entropy
[
g
j
]+
c
=

E
g

[log
InvW
(


;


0
+
nS
)]

Entropy
[
g

]

k
X
j
=1
E
g
j
[log
InvW
(

j
+
n;

j;
0
+
nS
j
)]
+
Entropy
[
g
j
]+
c
0
=
KL
(
g

jj
InvW
(


;


0
+
nS
))
+
k
X
j
=1
KL
(
g
j
jj
InvW
(

j
+
n;

j;
0
+
nS
j
))
+
c
0
;
whereweusedthat
E
g
J
;g

[log
j


1
j
]
=

P
k
j
=1
E
g
j
[log
j

j
j
]
,and
c
0
isaconstantwithrespectto
g

and
g
j
.
Fromtheaboveexpression,weseethatwecanoptimizetheparametersof
g

24
and
g
j
independentlyfromeachother.Theoptimalparameter
^

g
for
g

is
^

g
=argmin

g
KL
(
g

jj
InvW
(


;


0
+
nS
))
=argmin

g
(


+
p
+1)
E
g

[log
j


j
]
+
trace


0
+
nS
)
E
g



1

])

2

Entropy
[
g

]
=argmin

g
(


+
p
+1)


p
log2+
p
log(

g
+
p
+1)
+log
j
^


j
p
X
i
=1
 


g

p
+
i
2

+

g

g
+
p
+1
trace


0
+
nS
)
^


1

)

2log
p
(

g
2
)


g
p

p
(
p
+1)log(

g
+
p
+1)
+(

g
+
p
+1)
p
X
i
=1
 


g

p
+
i
2

=argmin

g
p
(


+
p
+1)log(

g
+
p
+1)

(


+
p
+1)
p
X
i
=1
 


g

p
+
i
2

+

g

g
+
p
+1
trace


0
+
nS
)
^


1

)

2log
p
(

g
2
)


g
p

p
(
p
+1)log(

g
+
p
+1)
+(

g
+
p
+1)
p
X
i
=1
 


g

p
+
i
2

=argmin

g

g

g
+
p
+1
trace


0
+
nS
)
^


1

)

2log
p
(

g
2
)


g
p
+


log(

g
+
p
+1)
+(

g



)
p
X
i
=1
 


g

p
+
i
2

:
25
Andanalogously,wehave
^

g;j
=argmin

g;j

g;j

g;j
+
p
j
+1
trace

j;
0
+
nS
j
)
^


1
j
)

2log
p
j
(

g;j
2
)


g;j
p
j
+
p
j
(

j
+
n
)log(

g;j
+
p
j
+1)
+(

g;j


j

n
)
p
j
X
i
=1
 


g;j

p
j
+
i
2

:
CSpectralClusteringforvariableclusteringwith
theGaussiangraphicalmodel
Let
S
2
R
p

p
denotethesamplecovariancematrixoftheobservedvariables.
Undertheassumptionthattheobservationsaredrawni.i.d.fromamultivariate
normaldistribution,withmean
0
andprecisionmatrix
X
+
X

,thelog-
likelihood
9
ofthedataisgivenby
n
2
(log
j
X
+
X

j
trace
((
X
+
X

)
S
))
;
where
n
isthenumberofobservations.Weassumethat
X
isblocksparse,i.e.a
permutationmatrix
P
existssuchthat
P
T
XP
isblockdiagonal.Ifweknewthe
numberofblocks
m
,thenwecouldestimatetheblockmatrix
X
(andthusthe
variableclustering)bythefollowingoptimizationproblem.
OptimizationProblem1:
minimize
X
˜
0

log
j
X
+
X

j
+
trace
((
X
+
X

)
S
)
subjectto
Xisblocksparsewithexactly
m
blocks
;
where
X

isassumedtobeaconstantmatrixwithsmallentries.Weclaimthat
thiscanbereformulated,forany
q>
0
,asfollowing.
OptimizationProblem2:
minimize
X
˜
0

log
j
X
+
X

j
+
trace
((
X
+
X

)
S
)
subjectto
L
ii
=
X
k
6
=
i
j
X
ik
j
q
;
L
ij
=

X
ij
j
q
for
i
6
=
j;
rank
(
L
)=
p

m:
9
Uptoaconstantthatdoesnotdependon
X
.
26
Proposition1.
Optimizationproblem1and2havethesamesolution.Moreover,
the
m
dimensionalnullspaceof
L
canbechosensuchthateachbasisvectoris
theindicatorvectorforonevariableblockof
X
.
Proof.
Firstletusde˝nethematrix
~
X
,by
~
X
ij
:=
j
X
ij
j
q
.Thenclearly,i˙
X
isblocksparsewith
m
blocks,sois
~
X
.Furthermore,
~
X
ij

0
,and
L
isthe
unnormalizedLaplacianasde˝nedin[VonLuxburg,2007].Wecantherefore
applyProposition(2)of[VonLuxburg,2007],to˝ndthatthedimensionofthe
eigenspaceofLcorrespondingtoeigenvalue0,isexactlythenumberofblocksin
~
X
.AlsofromProposition(2)of[VonLuxburg,2007]itfollowsthateachsuch
eigenvector
e
k
2
R
p
canbechosensuchthatitindicatesthevariablesbelonging
tothesameblock,i.e.
e
k
(
i
)
6
=0
,i˙variableibelongstoblockk.
Usingthenuclearnormasaconvexrelaxationfortherankconstraint,we
have
minimize
X

0

log
j
X
+
X

j
+
trace
((
X
+
X

)
S
)+

m
jj
L
jj

subjectto
L
ii
=
X
k
6
=
i
j
X
ik
j
q
;
L
ij
=

X
ij
j
q
for
i
6
=
j:
withanappropriatelychosen

m
.Bythede˝nitionof
L
,wehavethat
L
is
positivesemi-de˝nite,andtherefore
jj
L
jj

=
trace
(
L
)
.Asaconsequence,wecan
rewritetheaboveproblemas
X

:=argmin
X

0

log
j
X
+
X

j
+
trace
((
X
+
X

)
S
)
+

m
X
i
6
=
j
j
X
ij
j
q
:
Finally,forthepurposeoflearningtheLaplacian
L
,weignoretheterm
X

andsetittozero.Thiswillnecessarilyleadtoanestimateof
X

thatisnota
cleanblockmatrix,buthassmallnon-zeroentriesbetweenblocks.Nevertheless,
spectralclusteringisknowntoberobusttosuchviolations[Ngetal.,2002].
ThisleadstoAlgorithm2inSection6.
References
HirotoguAkaike.Informationtheoryandanextensionofthemaximumlikelihood
principle.In
Reprint
in
Breakthroughs
in
statistics,
1992
,pages
Springer,1973.
BAlbersts,AJohnson,JLewis,DMorgan,MRa˙,KRoberts,andPWalter.
Molecularbiologyofthecell:theproblemsbook.GarlandScience,2014.
27
TheodoreWilburAnderson.
An
introduction
to
multivariate
statistical
analysis
,
volume3.WileyNewYork,2004.
MikhailBelkinandParthaNiyogi.Laplacianeigenmapsfordimensionality
reductionanddatarepresentation.
Neural
computation
,
2003.
StephenBoyd,NealParikh,EricChu,BorjaPeleato,andJonathanEckstein.
Distributedoptimizationandstatisticallearningviathealternatingdirection
methodofmultipliers.
Foundations
and
Trends
in
Machine
Learning
,3(1):
2011.
RichardPBrent.Algorithmsfor˝ndingzerosandextremaoffunctionswithout
calculatingderivatives.Technicalreport,StanfordUniversity,Departmentof
ComputerScience,1971.
StephenPBrooksandAndrewGelman.Generalmethodsformonitoring
convergenceofiterativesimulations.
Journal
of
computational
and
graphical
statistics,1998.
TadeuszCali«skiandJerzyHarabasz.Adendritemethodforclusteranalysis.
Communications
in
Statistics-theory
and
Methods,1974.
JiahuaChenandZehuaChen.ExtendedBayesianinformationcriteriaformodel
selectionwithlargemodelspaces.
Biometrika,2008.
SiddharthaChib.MarginallikelihoodfromtheGibbsoutput.
Journal
of
the
american
statistical
association,1995.
SiddharthaChibandIvanJeliazkov.MarginallikelihoodfromtheMetrop
Hastingsoutput.
Journal
of
the
American
Statistical
Association
,96(453):
2001.
EmilieDevijverandMélinaGallopin.Block-diagonalcovarianceselectionforhigh-
dimensionalGaussiangraphicalmodels.
Journal
of
the
American
Statistical
Association,2016.
RinaFoygelandMathiasDrton.ExtendedBayesianinformationcriteriafor
Gaussiangraphicalmodels.In
Advances
in
neural
information
processing
systems,pages2010.
JeromeFriedman,TrevorHastie,andRobertTibshirani.Sparseinversecovari-
anceestimationwiththegraphicallasso.
Biostatistics,2008.
KeiHirose,HironoriFujisawa,andJunSese.RobustsparseGaussiangraphical
modeling.
Journal
of
Multivariate
Analysis,2017.
SeyedMohammadJavadHosseiniandSu-InLee.LearningSparseGaussian
GraphicalModelswithOverlappingBlocks.In
Advances
in
Neural
Information
Processing
Systems,pages2016.
28
SadanoriKonishi,TomohiroAndo,andSeiyaImoto.Bayesianinformation
criteriaandsmoothingparameterselectioninradialbasisfunctionnetworks.
Biometrika,,2004.
OlivierLedoitandMichaelWolf.Awell-conditionedestimatorforlarge-
dimensionalcovariancematrices.
Journal
of
multivariate
analysis
,88(2):
2004.
AlexLenkoskiandAdrianDobra.Computationalaspectsrelatedtoinferencein
gaussiangraphicalmodelswiththeg-wishartprior.
Journal
of
Computational
and
Graphical
Statistics,2011.
TianyiLin,ShiqianMa,andShuzhongZhang.Globalconvergenceofunmodi˝ed
3-blockADMMforaclassofconvexminimizationproblems.
Journal
of
Scienti˝c
Computing,pages2015.
BenjaminMMarlinandKevinPMurphy.SparseGaussiangraphicalmodelswith
unknownblockstructure.In
Proceedings
of
the
26th
Annual
International
Conference
on
Machine
Learning,pages2.ACM,2009.
BenjaminMMarlin,MarkSchmidt,andKevinPMurphy.Groupsparsepriors
forcovarianceestimation.In
Proceedings
of
the
Twenty-Fifth
Conference
on
Uncertainty
in
Arti˝cial
Intelligence,pagesAUAIPress,2009.
AndrewYNg,MichaelIJordan,YairWeiss,andOthers.Onspectralclustering:
Analysisandanalgorithm.
Advances
in
neural
information
processing
systems
,
2002.
KonstantinaPalla,ZoubinGhahramani,andDavidAKnowles.Anonparametric
variableclusteringmodel.In
Advances
in
Neural
Information
Processing
Systems,pages2012.
GideonSchwarz.Estimatingthedimensionofamodel.
The
annals
of
statistics
,
1978.
JamesGScottandCarlosMCarvalho.Feature-inclusionstochasticsearch
forGaussiangraphicalmodels.
Journal
of
Computational
and
Graphical
Statistics,2008.
SiqiSun,YuanchengZhu,andJinboXu.AdaptiveVariableClusteringin
GaussianGraphicalModels.In
AISTATS,pages2014.
SiqiSun,HaiWang,andJinboXu.InferringBlockStructureofGraphical
ModelsinExponentialFamilies.In
AISTATS,2015.
KeanMingTan,DanielaWitten,andAliShojaie.Theclustergraphicallassofor
improvedestimationofGaussiangraphicalmodels.
Computational
statistics
&
data
analysis,2015.
29
NguyenXuanVinh,JulienEpps,andJamesBailey.Informationtheoretic
measuresforclusteringscomparison:Variants,properties,normalizationand
correctionforchance.
Journal
of
Machine
Learning
Research
,
2854,2010.
UlrikeVonLuxburg.Atutorialonspectralclustering.
Statistics
and
computing
,
2007.
30
Table1:Evaluationofrestrictedhypothesesspacefor
p
=40
,
n
2
f
20
;
40
;
400
;
4000
;
40000
;
4000000
g
.Groundtruthcontains4balancedclusters.
ShowstheoracleperformancemeasuredbyANMIforspectralclustering,average
linkageandsinglelinkage.NotethatthatanANMIscoreof1.0meansthat
thetrueclusteringiscontainedinthehypothesesspacefoundbytheclustering
method.Thesizeofthehypothesesspacerestrictedbyeachclusteringmethodis
denotedby
j
C

j
.Averageresultsover5runswithstandarddeviationinbrackets.

j
˘
InvW
(
p
j
+1
;I
p
j
)
,nonoise
20404004000400004000000
spectral
ANMI0.77(0.14)0.95(0.06)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
j
C

j
140.8(5.78)139.0(8.65)112.8(5.64)99.8(2.23)101.4(7.94)98.4(3.61)
average
ANMI0.38(0.09)0.38(0.06)0.45(0.05)0.45(0.03)0.45(0.07)0.45(0.03)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI0.32(0.08)0.34(0.09)0.39(0.08)0.39(0.08)0.42(0.14)0.41(0.08)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)

j
˘
InvW
(
p
j
+1
;I
p
j
)
;


˘
InvW
(
p
+1
;I
p
)
;
=0
:
01
spectral
ANMI0.49(0.03)0.9(0.03)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
j
C

j
143.2(7.25)144.4(3.32)108.6(9.89)105.4(9.79)103.6(5.0)97.0(6.57)
average
ANMI0.26(0.05)0.34(0.04)0.46(0.07)0.51(0.08)0.42(0.09)0.45(0.06)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI0.16(0.08)0.25(0.08)0.37(0.03)0.4(0.06)0.3(0.12)0.32(0.09)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)

j
˘
InvW
(
p
j
+1
;I
p
j
)
;


˘
InvW
(
p
+1
;I
p
)
;
=0
:
1
spectral
ANMI0.34(0.1)0.87(0.09)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
j
C

j
121.4(7.34)106.4(18.51)35.4(5.12)33.2(11.48)37.4(5.54)31.0(8.65)
average
ANMI0.1(0.05)0.15(0.03)0.34(0.08)0.37(0.1)0.26(0.11)0.28(0.09)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI0.04(0.03)0.08(0.04)0.19(0.11)0.21(0.06)0.11(0.03)0.13(0.02)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)

j
˘
Uniform
p
j
,nonoise
spectral
ANMI0.34(0.1)0.87(0.09)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
j
C

j
121.4(7.34)106.4(18.51)35.4(5.12)33.2(11.48)37.4(5.54)31.0(8.65)
average
ANMI0.1(0.06)0.26(0.07)0.92(0.11)1.0(0.0)1.0(0.0)0.99(0.03)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI0.04(0.02)0.13(0.08)0.82(0.25)1.0(0.0)1.0(0.0)0.99(0.03)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)

j
˘
Uniform
p
j
;


˘
Uniform
p
;
=0
:
01
spectral
ANMI0.28(0.06)0.81(0.1)0.94(0.06)0.99(0.03)0.99(0.03)0.97(0.03)
j
C

j
127.2(3.6)106.0(5.29)48.2(9.77)50.2(5.95)51.0(8.94)48.0(5.69)
average
ANMI0.14(0.05)0.22(0.04)0.81(0.16)0.89(0.1)0.87(0.12)0.94(0.12)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI0.04(0.02)0.1(0.04)0.78(0.13)0.71(0.23)0.78(0.11)0.79(0.17)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)

j
˘
Uniform
p
j
;


˘
Uniform
p
;
=0
:
1
spectral
ANMI0.3(0.03)0.72(0.08)0.88(0.07)0.9(0.07)0.87(0.11)0.88(0.04)
j
C

j
126.2(2.23)120.4(9.35)74.4(19.41)87.2(7.93)79.2(13.61)77.0(14.25)
average
ANMI0.08(0.04)0.26(0.11)0.83(0.15)0.88(0.12)0.87(0.11)0.94(0.12)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI0.05(0.03)0.13(0.07)0.7(0.14)0.69(0.15)0.76(0.12)0.76(0.14)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
31
Table2:SamesettingasinTable1butwithunbalancedclusters.Groundtruth
is4clusterswithsizes20,10,5,5.

j
˘
InvW
(
p
j
+1
;I
p
j
)
,nonoise
20404004000400004000000
spectral
ANMI0.52(0.13)0.85(0.11)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
j
C

j
141.2(6.62)133.2(8.03)80.8(8.21)73.4(8.89)62.0(7.38)62.6(7.23)
average
ANMI0.34(0.06)0.39(0.05)0.37(0.04)0.38(0.07)0.38(0.06)0.44(0.09)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI0.33(0.05)0.35(0.03)0.32(0.04)0.32(0.14)0.27(0.13)0.39(0.12)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)

j
˘
InvW
(
p
j
+1
;I
p
j
)
;


˘
InvW
(
p
+1
;I
p
)
;
=0
:
01
spectral
ANMI0.55(0.13)0.81(0.07)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
j
C

j
148.8(4.62)136.0(6.81)80.4(9.77)68.8(10.3)67.0(5.93)63.0(14.3)
average
ANMI0.34(0.06)0.37(0.08)0.53(0.12)0.5(0.1)0.46(0.1)0.52(0.1)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI0.29(0.07)0.29(0.08)0.41(0.17)0.4(0.14)0.37(0.11)0.32(0.12)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)

j
˘
InvW
(
p
j
+1
;I
p
j
)
;


˘
InvW
(
p
+1
;I
p
)
;
=0
:
1
spectral
ANMI0.26(0.04)0.5(0.06)0.93(0.07)0.93(0.07)0.99(0.02)0.91(0.08)
j
C

j
144.4(5.54)159.2(1.83)121.0(10.43)120.2(6.62)117.0(3.41)113.2(11.91)
average
ANMI0.2(0.03)0.22(0.06)0.37(0.09)0.36(0.08)0.41(0.13)0.44(0.07)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI0.2(0.08)0.2(0.07)0.24(0.04)0.29(0.05)0.33(0.07)0.32(0.05)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)

j
˘
Uniform
p
j
,nonoise
spectral
ANMI0.36(0.06)0.72(0.13)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
j
C

j
124.0(7.29)115.8(9.89)40.8(12.5)39.4(5.2)33.2(4.79)38.6(5.24)
average
ANMI0.09(0.04)0.05(0.08)0.12(0.07)0.29(0.07)0.37(0.07)0.34(0.14)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI0.01(0.04)-0.01(0.0)-0.01(0.01)0.06(0.1)0.17(0.19)0.13(0.12)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)

j
˘
Uniform
p
j
;


˘
Uniform
p
;
=0
:
01
spectral
ANMI0.39(0.04)0.67(0.11)0.85(0.05)0.89(0.07)0.87(0.07)0.89(0.06)
j
C

j
125.6(8.06)115.0(12.85)42.6(7.09)59.2(11.55)53.2(9.2)54.0(6.69)
average
ANMI0.04(0.03)0.06(0.05)0.12(0.06)0.21(0.08)0.18(0.09)0.21(0.13)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI-0.01(0.0)-0.01(0.0)-0.01(0.0)0.0(0.02)0.01(0.05)0.02(0.05)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)

j
˘
Uniform
p
j
;


˘
Uniform
p
;
=0
:
1
spectral
ANMI0.32(0.06)0.68(0.13)0.8(0.09)0.81(0.09)0.79(0.07)0.78(0.09)
j
C

j
124.2(9.33)109.6(12.63)66.6(10.71)74.2(7.14)62.8(5.11)65.2(13.85)
average
ANMI0.04(0.03)0.06(0.05)0.09(0.05)0.19(0.05)0.13(0.06)0.2(0.13)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
single
ANMI-0.01(0.0)-0.01(0.0)-0.01(0.0)-0.01(0.0)-0.01(0.0)0.0(0.02)
j
C

j
14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)14.0(0.0)
32
Table3:Evaluationofclusteringresultsfor
p
=40
,
n
2
f
20
;
40
;
400
;
4000
;
40000
;
4000000
g
.Groundtruthis4balancedclusters.Shows
theANMIoftheselectedmodels(standarddeviationinbrackets).Nonoiseis
added.

j
˘
InvW
(
p
j
+1
;I
p
j
)
,nonoise
20404004000400004000000
Proposed(

=0
:
01
)0.76(0.14)0.93(0.09)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
Proposed(

=0
:
02
)0.7(0.2)0.92(0.08)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
Proposed(

=0
:
03
)0.67(0.18)0.88(0.14)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
basicinverseWishartprior0.73(0.17)0.93(0.09)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=0
)0.12(0.15)0.92(0.08)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=0
:
5
)0.36(0.03)0.51(0.04)0.99(0.03)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=1
:
0
)0.35(0.02)0.39(0.05)0.96(0.05)1.0(0.0)1.0(0.0)1.0(0.0)
AIC0.12(0.15)0.6(0.49)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
Calinski-HarabazIndex0.32(0.03)0.19(0.16)0.84(0.13)0.73(0.0)0.73(0.0)0.73(0.0)
CGL(ALC)0.06(0.05)0.03(0.05)0.11(0.06)0.04(0.04)0.06(0.03)0.06(0.07)
DPVC0.53(0.07)0.61(0.17)0.82(0.06)0.93(0.09)NANA

j
˘
Uniform
p
j
,nonoise
Proposed(

=0
:
01
)0.12(0.04)0.48(0.07)0.94(0.06)1.0(0.0)1.0(0.0)1.0(0.0)
Proposed(

=0
:
02
)0.12(0.05)0.4(0.04)0.93(0.06)1.0(0.0)1.0(0.0)1.0(0.0)
Proposed(

=0
:
03
)0.12(0.05)0.39(0.03)0.93(0.06)1.0(0.0)1.0(0.0)1.0(0.0)
basicinverseWishartprior0.14(0.05)0.76(0.1)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=0
)0.07(0.04)0.87(0.09)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=0
:
5
)0.11(0.05)0.48(0.12)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=1
:
0
)0.11(0.05)0.38(0.05)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
AIC0.07(0.04)0.66(0.34)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
Calinski-HarabazIndex0.15(0.05)0.66(0.16)0.79(0.11)0.46(0.14)0.65(0.23)0.59(0.17)
CGL(ALC)0.03(0.02)0.02(0.02)0.37(0.03)0.39(0.0)0.39(0.0)0.51(0.25)
DPVC0.01(0.02)0.03(0.03)0.4(0.2)0.51(0.22)NANA
33
Table4:Evaluationofclusteringresultswith
p
=40
,
n
2
f
20
;
40
;
400
;
4000
;
40000
;
4000000
g
.Groundtruthis4balancedclusters.Shows
theANMIoftheselectedmodels(standarddeviationinbrackets).Noiseis
addedtotheprecisionmatrix.

j
˘
InvW
(
p
j
+1
;I
p
j
)
;


˘
InvW
(
p
+1
;I
p
)
;
=0
:
01
20404004000400004000000
Proposed(

=0
:
01
)0.44(0.07)0.86(0.06)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
Proposed(

=0
:
02
)0.41(0.06)0.86(0.06)1.0(0.0)1.0(0.0)1.0(0.0)0.99(0.03)
Proposed(

=0
:
03
)0.38(0.06)0.8(0.06)1.0(0.0)1.0(0.0)1.0(0.0)0.99(0.03)
basicinverseWishartprior0.45(0.07)0.89(0.02)1.0(0.0)1.0(0.0)0.41(0.04)0.39(0.0)
EBIC(

=0
)0.02(0.02)0.82(0.07)1.0(0.0)1.0(0.0)0.41(0.04)0.39(0.0)
EBIC(

=0
:
5
)0.25(0.08)0.32(0.07)0.98(0.04)1.0(0.0)0.48(0.13)0.39(0.0)
EBIC(

=1
:
0
)0.23(0.07)0.32(0.07)0.96(0.06)1.0(0.0)0.66(0.14)0.39(0.0)
AIC0.0(0.01)0.54(0.44)1.0(0.0)0.39(0.0)0.41(0.04)0.39(0.0)
Calinski-HarabazIndex0.26(0.09)0.3(0.16)0.93(0.1)0.95(0.11)0.89(0.13)0.84(0.13)
CGL(ALC)0.01(0.02)0.02(0.05)0.04(0.05)0.03(0.02)0.05(0.06)0.02(0.02)
DPVC0.33(0.07)0.42(0.08)0.59(0.16)0.21(0.18)NANA

j
˘
InvW
(
p
j
+1
;I
p
j
)
;


˘
InvW
(
p
+1
;I
p
)
;
=0
:
1
Proposed(

=0
:
01
)0.1(0.1)0.4(0.09)0.93(0.1)0.39(0.0)0.33(0.17)0.29(0.15)
Proposed(

=0
:
02
)0.13(0.09)0.41(0.07)0.97(0.04)0.95(0.11)1.0(0.0)0.99(0.03)
Proposed(

=0
:
03
)0.13(0.09)0.4(0.09)0.95(0.04)0.99(0.03)1.0(0.0)0.99(0.03)
basicinverseWishartprior0.1(0.1)0.4(0.09)0.93(0.1)0.23(0.19)0.18(0.21)0.23(0.19)
EBIC(

=0
)0.09(0.09)0.29(0.06)0.94(0.05)0.31(0.15)0.18(0.21)0.23(0.19)
EBIC(

=0
:
5
)0.12(0.05)0.2(0.02)0.87(0.02)0.41(0.04)0.18(0.21)0.23(0.19)
EBIC(

=1
:
0
)0.14(0.06)0.2(0.02)0.54(0.07)0.86(0.24)0.18(0.21)0.23(0.19)
AIC-0.0(0.0)0.0(0.01)0.09(0.15)0.23(0.19)0.18(0.21)0.23(0.19)
Calinski-HarabazIndex0.11(0.05)0.15(0.13)0.94(0.05)0.99(0.03)1.0(0.0)0.99(0.03)
CGL(ALC)0.02(0.03)0.0(0.01)0.01(0.01)0.01(0.02)0.0(0.0)0.0(0.0)
DPVC0.11(0.06)0.16(0.06)0.27(0.06)0.04(0.04)NANA

j
˘
Uniform
p
j
;


˘
Uniform
p
;
=0
:
01
Proposed(

=0
:
01
)0.1(0.04)0.45(0.05)0.92(0.06)0.99(0.03)0.99(0.03)0.93(0.1)
Proposed(

=0
:
02
)0.12(0.03)0.43(0.06)0.92(0.06)0.99(0.03)0.99(0.03)0.93(0.1)
Proposed(

=0
:
03
)0.13(0.02)0.39(0.03)0.89(0.07)0.99(0.03)0.99(0.03)0.93(0.1)
basicinverseWishartprior0.11(0.06)0.65(0.12)0.94(0.06)0.88(0.12)0.3(0.28)0.46(0.14)
EBIC(

=0
)0.06(0.04)0.78(0.14)0.92(0.1)0.81(0.23)0.3(0.28)0.46(0.14)
EBIC(

=0
:
5
)0.1(0.03)0.44(0.06)0.94(0.06)0.99(0.03)0.3(0.28)0.46(0.14)
EBIC(

=1
:
0
)0.1(0.03)0.39(0.03)0.94(0.06)0.99(0.03)0.3(0.28)0.46(0.14)
AIC0.06(0.04)0.24(0.33)0.35(0.43)0.44(0.15)0.3(0.28)0.46(0.14)
Calinski-HarabazIndex0.14(0.06)0.54(0.33)0.57(0.35)0.76(0.21)0.59(0.29)0.66(0.14)
CGL(ALC)0.0(0.01)0.01(0.01)0.24(0.18)0.39(0.0)0.35(0.08)0.39(0.0)
DPVC-0.01(0.01)0.06(0.07)0.29(0.22)0.44(0.2)NANA

j
˘
Uniform
p
j
;


˘
Uniform
p
;
=0
:
1
Proposed(

=0
:
01
)0.11(0.02)0.45(0.05)0.88(0.07)0.79(0.21)0.56(0.34)0.64(0.22)
Proposed(

=0
:
02
)0.14(0.04)0.4(0.02)0.86(0.07)0.9(0.07)0.56(0.34)0.64(0.22)
Proposed(

=0
:
03
)0.14(0.04)0.39(0.03)0.86(0.07)0.9(0.07)0.56(0.34)0.64(0.22)
basicinverseWishartprior0.13(0.04)0.52(0.07)0.88(0.07)0.42(0.33)0.15(0.19)0.23(0.19)
EBIC(

=0
)0.12(0.06)0.7(0.1)0.78(0.22)0.42(0.33)0.15(0.19)0.16(0.19)
EBIC(

=0
:
5
)0.13(0.04)0.44(0.05)0.88(0.07)0.48(0.26)0.15(0.19)0.16(0.19)
EBIC(

=1
:
0
)0.12(0.05)0.39(0.03)0.88(0.07)0.6(0.3)0.15(0.19)0.16(0.19)
AIC0.12(0.06)0.2(0.17)0.06(0.12)0.42(0.33)0.15(0.19)0.16(0.19)
Calinski-HarabazIndex0.17(0.06)0.48(0.29)0.28(0.34)0.9(0.07)0.49(0.27)0.63(0.22)
CGL(ALC)0.01(0.01)0.07(0.08)0.31(0.15)0.39(0.0)0.33(0.11)0.38(0.02)
DPVC-0.0(0.0)0.1(0.09)0.35(0.12)0.19(0.18)NANA
34
Table5:Evaluationofclusteringresultsfor
p
=40
,
n
2
f
20
;
40
;
400
;
4000
;
40000
;
4000000
g
.Groundtruthis4unbalancedclusterswith
sizes20,10,5,5.ShowstheANMIoftheselectedmodels(standarddeviation
inbrackets).Nonoiseisadded.

j
˘
InvW
(
p
j
+1
;I
p
j
)
,nonoise
20404004000400004000000
Proposed(

=0
:
01
)0.49(0.15)0.84(0.11)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
Proposed(

=0
:
02
)0.47(0.17)0.84(0.11)0.99(0.02)1.0(0.0)1.0(0.0)1.0(0.0)
Proposed(

=0
:
03
)0.42(0.19)0.82(0.13)0.99(0.02)1.0(0.0)1.0(0.0)1.0(0.0)
basicinverseWishartprior0.5(0.15)0.84(0.12)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=0
)0.2(0.17)0.8(0.12)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=0
:
5
)0.24(0.05)0.37(0.05)0.99(0.02)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=1
:
0
)0.23(0.06)0.32(0.04)0.99(0.02)1.0(0.0)1.0(0.0)1.0(0.0)
AIC0.15(0.19)0.16(0.12)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
Calinski-HarabazIndex0.17(0.09)0.17(0.23)0.46(0.27)0.45(0.23)0.47(0.19)0.4(0.14)
CGL(ALC)0.07(0.11)0.03(0.04)0.05(0.07)0.03(0.03)0.07(0.07)0.05(0.06)
DPVC0.57(0.13)0.66(0.07)0.64(0.14)0.87(0.17)NANA

j
˘
Uniform
p
j
,nonoise
Proposed(

=0
:
01
)0.15(0.03)0.33(0.03)0.87(0.1)0.98(0.03)1.0(0.0)0.98(0.03)
Proposed(

=0
:
02
)0.15(0.03)0.33(0.03)0.87(0.1)0.97(0.04)1.0(0.0)0.97(0.04)
Proposed(

=0
:
03
)0.16(0.03)0.31(0.03)0.67(0.18)0.97(0.04)0.98(0.03)0.97(0.04)
basicinverseWishartprior0.17(0.05)0.33(0.02)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=0
)0.08(0.09)0.6(0.23)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=0
:
5
)0.16(0.03)0.33(0.04)0.98(0.03)1.0(0.0)1.0(0.0)1.0(0.0)
EBIC(

=1
:
0
)0.16(0.03)0.31(0.03)0.91(0.12)1.0(0.0)1.0(0.0)1.0(0.0)
AIC0.08(0.08)0.52(0.33)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
Calinski-HarabazIndex0.16(0.06)0.53(0.3)0.64(0.15)0.63(0.28)0.88(0.17)0.96(0.08)
CGL(ALC)-0.01(0.01)-0.01(0.0)-0.0(0.01)0.15(0.16)0.15(0.21)0.12(0.06)
DPVC0.02(0.01)0.0(0.04)0.23(0.14)0.25(0.13)NANA
35
Table6:Evaluationofclusteringresultswith
p
=40
,
n
2
f
20
;
40
;
400
;
4000
;
40000
;
4000000
g
.Groundtruthis4unbalancedclusterswith
sizes20,10,5,5.ShowstheANMIoftheselectedmodels(standarddeviation
inbrackets).Noiseisaddedtotheprecisionmatrix.

j
˘
InvW
(
p
j
+1
;I
p
j
)
;


˘
InvW
(
p
+1
;I
p
)
;
=0
:
01
20404004000400004000000
Proposed(

=0
:
01
)0.45(0.14)0.75(0.15)1.0(0.0)1.0(0.0)1.0(0.0)1.0(0.0)
Proposed(

=0
:
02
)0.39(0.09)0.75(0.15)1.0(0.0)1.0(0.0)1.0(0.0)0.98(0.03)
Proposed(

=0
:
03
)0.39(0.09)0.7(0.18)1.0(0.0)0.97(0.06)1.0(0.0)0.98(0.03)
basicinverseWishartprior0.48(0.15)0.8(0.09)1.0(0.0)0.91(0.11)0.39(0.13)0.42(0.12)
EBIC(

=0
)0.12(0.08)0.67(0.12)1.0(0.0)0.91(0.11)0.48(0.17)0.42(0.12)
EBIC(

=0
:
5
)0.19(0.08)0.32(0.04)0.97(0.03)1.0(0.0)0.54(0.26)0.42(0.12)
EBIC(

=1
:
0
)0.17(0.07)0.28(0.07)0.96(0.03)1.0(0.0)0.68(0.24)0.42(0.12)
AIC0.06(0.09)0.3(0.34)1.0(0.0)0.4(0.1)0.39(0.13)0.42(0.12)
Calinski-HarabazIndex0.2(0.06)0.13(0.2)0.45(0.27)0.59(0.17)0.7(0.21)0.77(0.03)
CGL(ALC)0.08(0.06)0.05(0.03)0.04(0.03)0.03(0.02)0.03(0.02)0.04(0.04)
DPVC0.28(0.04)0.35(0.07)0.57(0.08)0.4(0.12)NANA

j
˘
InvW
(
p
j
+1
;I
p
j
)
;


˘
InvW
(
p
+1
;I
p
)
;
=0
:
1
Proposed(

=0
:
01
)0.09(0.11)0.42(0.12)0.84(0.1)0.42(0.16)0.18(0.22)0.24(0.18)
Proposed(

=0
:
02
)0.09(0.11)0.42(0.13)0.88(0.11)0.85(0.15)0.99(0.02)0.9(0.09)
Proposed(

=0
:
03
)0.15(0.06)0.42(0.13)0.89(0.09)0.92(0.07)0.99(0.02)0.9(0.09)
basicinverseWishartprior0.11(0.14)0.42(0.13)0.84(0.1)0.2(0.2)0.0(0.01)0.1(0.17)
EBIC(

=0
)0.04(0.05)0.24(0.06)0.88(0.11)0.2(0.2)0.0(0.01)0.1(0.17)
EBIC(

=0
:
5
)0.05(0.02)0.19(0.04)0.74(0.19)0.44(0.17)0.0(0.01)0.1(0.17)
EBIC(

=1
:
0
)0.05(0.02)0.19(0.04)0.41(0.06)0.78(0.12)0.0(0.01)0.1(0.17)
AIC-0.01(0.01)0.15(0.21)0.19(0.2)0.2(0.2)0.0(0.01)0.1(0.17)
Calinski-HarabazIndex0.06(0.03)0.17(0.11)0.68(0.25)0.67(0.2)0.83(0.17)0.76(0.04)
CGL(ALC)0.04(0.04)0.03(0.02)0.05(0.06)0.1(0.11)0.05(0.07)0.08(0.09)
DPVC0.13(0.05)0.16(0.05)0.3(0.13)0.07(0.03)NANA

j
˘
Uniform
p
j
;


˘
Uniform
p
;
=0
:
01
Proposed(

=0
:
01
)0.11(0.02)0.32(0.04)0.74(0.15)0.83(0.1)0.59(0.32)0.5(0.33)
Proposed(

=0
:
02
)0.11(0.02)0.32(0.04)0.61(0.17)0.83(0.1)0.59(0.32)0.59(0.32)
Proposed(

=0
:
03
)0.11(0.02)0.32(0.04)0.43(0.06)0.83(0.1)0.59(0.32)0.59(0.32)
basicinverseWishartprior0.11(0.02)0.32(0.04)0.84(0.05)0.28(0.0)0.11(0.14)0.17(0.23)
EBIC(

=0
)0.18(0.13)0.43(0.05)0.76(0.13)0.22(0.12)0.11(0.14)0.06(0.11)
EBIC(

=0
:
5
)0.11(0.02)0.32(0.04)0.84(0.05)0.51(0.3)0.11(0.14)0.06(0.11)
EBIC(

=1
:
0
)0.11(0.02)0.32(0.04)0.79(0.13)0.67(0.24)0.11(0.14)0.06(0.11)
AIC0.14(0.05)0.16(0.28)0.17(0.23)0.22(0.12)0.09(0.12)0.06(0.11)
Calinski-HarabazIndex0.14(0.08)0.32(0.3)0.34(0.33)0.68(0.22)0.25(0.27)0.41(0.32)
CGL(ALC)-0.01(0.0)-0.01(0.0)0.01(0.04)-0.01(0.01)0.02(0.02)0.01(0.01)
DPVC0.01(0.01)0.03(0.06)0.2(0.05)0.01(0.02)NANA

j
˘
Uniform
p
j
;


˘
Uniform
p
;
=0
:
1
Proposed(

=0
:
01
)0.1(0.02)0.34(0.07)0.68(0.18)0.6(0.31)0.09(0.12)0.06(0.11)
Proposed(

=0
:
02
)0.11(0.02)0.34(0.07)0.65(0.21)0.7(0.13)0.21(0.21)0.28(0.26)
Proposed(

=0
:
03
)0.11(0.02)0.32(0.06)0.58(0.2)0.7(0.13)0.32(0.22)0.28(0.26)
basicinverseWishartprior0.14(0.03)0.37(0.08)0.78(0.1)0.0(0.02)0.09(0.12)0.06(0.11)
EBIC(

=0
)0.16(0.05)0.49(0.21)0.71(0.14)0.0(0.02)0.09(0.12)0.06(0.11)
EBIC(

=0
:
5
)0.11(0.01)0.36(0.08)0.77(0.13)0.06(0.11)0.09(0.12)0.06(0.11)
EBIC(

=1
:
0
)0.11(0.01)0.31(0.05)0.7(0.16)0.12(0.14)0.09(0.12)0.06(0.11)
AIC0.15(0.05)0.05(0.12)0.06(0.11)0.0(0.02)0.09(0.12)0.06(0.11)
Calinski-HarabazIndex0.16(0.05)0.29(0.26)0.42(0.23)0.45(0.38)0.09(0.12)0.33(0.31)
CGL(ALC)-0.0(0.01)-0.0(0.01)-0.01(0.0)-0.01(0.0)-0.01(0.0)-0.0(0.01)
DPVC0.0(0.04)0.03(0.05)0.11(0.13)0.02(0.03)NANA
36
Table7:ComparisonofvariationalandMCMCestimate.Evaluationofclustering
resultsfor
p
=12
,
n
2f
12
;
120
;
1200
;
1200000
g
.Groundtruthis4balanced
clusters.

=0
:
02
.ShowstheANMIoftheselectedmodels(standarddeviation
inbrackets).

j
˘
InvW
(
p
j
+1
;I
p
j
)
,nonoise
1212012001200000
Proposed,variational0.39(0.23)0.89(0.09)0.96(0.07)0.82(0.11)
Proposed,MCMC0.37(0.23)0.89(0.09)0.96(0.07)0.9(0.14)
basicinverseWishartprior0.39(0.23)0.89(0.09)1.0(0.0)1.0(0.0)

j
˘
Uniform
p
j
,nonoise
Proposed,variational0.76(0.17)1.0(0.0)1.0(0.0)1.0(0.0)
Proposed,MCMC0.66(0.1)1.0(0.0)1.0(0.0)1.0(0.0)
basicinverseWishartprior0.76(0.17)1.0(0.0)1.0(0.0)1.0(0.0)

j
˘
InvW
(
p
j
+1
;I
p
j
)
;


˘
InvW
(
p
+1
;I
p
)
;
=0
:
01
Proposed,variational0.42(0.27)0.8(0.16)1.0(0.0)0.96(0.07)
Proposed,MCMC0.17(0.24)0.8(0.16)1.0(0.0)0.96(0.07)
basicinverseWishartprior0.42(0.27)0.94(0.12)0.93(0.13)0.34(0.04)

j
˘
InvW
(
p
j
+1
;I
p
j
)
;


˘
InvW
(
p
+1
;I
p
)
;
=0
:
1
Proposed,variational0.11(0.16)0.57(0.07)0.55(0.26)0.78(0.2)
Proposed,MCMC0.09(0.06)0.61(0.13)0.61(0.23)0.78(0.2)
basicinverseWishartprior0.16(0.15)0.54(0.1)0.28(0.15)0.21(0.18)

j
˘
Uniform
p
j
;


˘
Uniform
p
;
=0
:
01
Proposed,variational0.79(0.12)0.82(0.26)0.73(0.33)0.96(0.07)
Proposed,MCMC0.82(0.11)0.96(0.09)0.75(0.31)0.96(0.07)
basicinverseWishartprior0.79(0.12)0.48(0.15)0.28(0.09)0.28(0.09)

j
˘
Uniform
p
j
;


˘
Uniform
p
;
=0
:
1
Proposed,variational0.67(0.22)0.24(0.24)0.32(0.0)0.35(0.18)
Proposed,MCMC0.68(0.17)0.24(0.24)0.46(0.27)0.35(0.18)
basicinverseWishartprior0.69(0.21)0.13(0.11)0.26(0.13)0.28(0.09)
37
Table8:Evaluationofselectedclusteringsofthemutualfundsdata.Colors
highlightthetypeoffund.Numbersdenotetheclusteridassignedbythe
respectivemethod.Herethesizeoftherestrictedhypothesesspace
j
C

j
found
byspectralclusteringwas128.
ProposedandEBIC(

=0
:
0
)[numberofclusters=6,ANMI=0.48]
U.S.bondfunds
2222222422222
U.S.stockfunds
111111111111111111111111115146
balancedfunds
1111111
internationalstockfunds
131131331
basicinverseWishartprior[numberofclusters=3,ANMI=0.42]
U.S.bondfunds
2222222222222
U.S.stockfunds
111111111111111111111111113111
balancedfunds
1111111
internationalstockfunds
111111111
EBIC(

=0
:
5
)[numberofclusters=11,ANMI=0.32]
U.S.bondfunds
29292221109222
U.S.stockfunds
711711711771157115187115555555854886
balancedfunds
1178711711
internationalstockfunds
131131333
EBIC(

=1
:
0
)[numberofclusters=14,ANMI=0.25]
U.S.bondfunds
292921421149101010
U.S.stockfunds
12812612812128612863116857555561151115411
balancedfunds
812112867
internationalstockfunds
31333133131313
AICandCalinski-HarabazIndex[numberofclusters=2,ANMI=0]
U.S.bondfunds
1111111111111
U.S.stockfunds
111111111111111111111111112111
balancedfunds
1111111
internationalstockfunds
111111111
CGL(ALC)[numberofclusters=3,ANMI=0.36]
U.S.bondfunds
1111111311111
U.S.stockfunds
222222222222222222222222223333
balancedfunds
2222322
internationalstockfunds
222222232
DPVC[numberofclusters=2,ANMI=0.35]
U.S.bondfunds
1111111211111
U.S.stockfunds
222222222222222222222222222222
balancedfunds
2222222
internationalstockfunds
222222222
38
ProposedandEBIC(

=1
:
0
)
EBIC(

=0
:
0
),basicinverse
Wishartprior,AIC,CGL
EBIC(

=0
:
5
)
Calinski-HarabazIndex
DPVC
Figure5:ClusteringsofgeneregulationsnetworkofE.coli.Theclustering
resultsarevisualizedbydi˙erentcolors.Herethesizeoftherestrictedhypotheses
space
j
C

j
foundbyspectralclusteringwas18.
39
Table9:EvaluationofselectedclusteringsoftheAviationSensorDatawith16
variables.Herethesizeoftherestrictedhypothesesspace
j
C

j
foundbyspectral
clusteringwas28.
Proposed
Cluster1BRAKEPRESSURELHYELLOW
Cluster2INDICATEDANGLEOFATTACK,ANGLEOFATTACK2,ANGLEOFATTACK1
Cluster3ROLLSPOILERRIGHT
Cluster4BRAKEPRESSURERHGREEN
Cluster5RUDDERPOSITION
Cluster6AILERONPOSITIONRH,AILERONPOSITIONLH
Cluster7ROLLSPOILERLEFT
Cluster8PITCHTRIMPOSITION
Cluster9STATICPRESSURELSP,TOTALPRESSURELSP,AVARAGESTATICPRESSURELSP,
ELEVATORPOSITIONLEFT,ELEVATORPOSITIONRIGHT
basicinverseWishartprior,EBIC(

2f
0
:
0
;
0
:
5
;
1
:
0
g
),AIC
Cluster1STATICPRESSURELSP,INDICATEDANGLEOFATTACK,TOTALPRESSURELSP,
RUDDERPOSITION,AILERONPOSITIONRH,AVARAGESTATICPRESSURELSP,
ELEVATORPOSITIONLEFT,ELEVATORPOSITIONRIGHT,PITCHTRIMPOSITION,
ANGLEOFATTACK2,ANGLEOFATTACK1,AILERONPOSITIONLH,ROLLSPOILERLEFT,
BRAKEPRESSURELHYELLOW,ROLLSPOILERRIGHT
Cluster2BRAKEPRESSURERHGREEN
Calinski-HarabazIndex
Cluster1STATICPRESSURELSP,TOTALPRESSURELSP,AILERONPOSITIONRH,
AVARAGESTATICPRESSURELSP,ELEVATORPOSITIONLEFT,ELEVATORPOSITIONRIGHT,
BRAKEPRESSURERHGREEN,AILERONPOSITIONLH,BRAKEPRESSURELHYELLOW
Cluster2INDICATEDANGLEOFATTACK,ANGLEOFATTACK2,ANGLEOFATTACK1
Cluster3RUDDERPOSITION,PITCHTRIMPOSITION,ROLLSPOILERLEFT,ROLLSPOILERRIGHT
CGL(ALC)
Cluster1STATICPRESSURELSP,TOTALPRESSURELSP,AVARAGESTATICPRESSURELSP,
ELEVATORPOSITIONLEFT,ELEVATORPOSITIONRIGHT,BRAKEPRESSURELHYELLOW
Cluster2INDICATEDANGLEOFATTACK,RUDDERPOSITION,AILERONPOSITIONRH,
PITCHTRIMPOSITION,BRAKEPRESSURERHGREEN,ANGLEOFATTACK2,
ANGLEOFATTACK1,AILERONPOSITIONLH,ROLLSPOILERLEFT,ROLLSPOILERRIGHT
40
"
65,Learning Front-end Filter-bank Parameters using Convolutional Neural Networks for Abnormal Heart Sound Detection,http://arxiv.org/pdf/1806.05892v1.pdf,https://github.com/AhmedImtiazPrio/heartnet,"LearningFront-endFilter-bankParametersusingConvolutionalNeural
NetworksforAbnormalHeartSoundDetection
AhmedImtiazHumayun
1
,ShabnamGhaffarzadegan
2
,ZheFeng
2
andTHasan
1
Abstract
ŠAutomaticheartsoundabnormalitydetectioncan
playavitalroleintheearlydiagnosisofheartdiseases,partic-
ularlyinlow-resourcesettings.Thestate-of-the-artalgorithms
forthistaskutilizeasetofFiniteImpulseResponse(FIR)
band-passasafront-endfollowedbyaConvolutional
NeuralNetwork(CNN)model.Inthiswork,wepropounda
novelCNNarchitecturethatintegratesthefront-endband-
passwithinthenetworkusingtime-convolution(tConv)
layers,whichenablestheFIR-bankparameterstobecome
learnable.Differentinitializationstrategiesforthelearnable
ters,includingrandomparametersandasetofprFIR
-bankareexamined.UsingtheproposedtConv
layers,weaddconstraintstothelearnableFIRtoensure
linearandzerophaseresponses.Experimentalevaluationsare
performedonabalanced4-foldcross-validationtaskprepared
usingthePhysioNet/CinC2016dataset.Resultsdemonstrate
thattheproposedmodelsyieldsuperiorperformancecompared
tothestate-of-the-artsystem,whilethelinearphaseFIR-
bankmethodprovidesanabsoluteimprovementof9.54%over
thebaselineintermsofanoverallaccuracymetric.
I.I
NTRODUCTION
Cardiovasculardiseases(CVDs)areresponsibleforabout
17.7milliondeathseveryyear,representing31%ofthe
globalmortality[1].Cardiacauscultationisthemostpopular
non-invasiveandcost-effectiveprocedurefortheearlydiag-
nosisofvariousheartdiseases.However,effectivecardiac
auscultationrequirestrainedphysicians,aresourcewhichis
limitedespeciallyinlow-incomecountriesoftheworld[2].
Thus,machinelearningbasedautomatedheartsoundclassi-
systemsimplementedwithasmart-phoneattachable
digitalstethoscopeinthepoint-of-carelocationscanbeof
impactforearlydiagnosisofcardiacdiseases,
particularlyforcountriesthatsufferfromashortageand
geographicmal-distributionofskilledphysicians.
AutomatedofthePhonocardiogram(PCG),
i.e.,theheartsound,havebeenextensivelystudiedand
researchedinthepastfewdecades.AnalysisofthePCGcan
bebroadlydividedintotwoprincipalareas:(i)segmentation
ofthePCGsignal,i.e.,detectionoftheandsecond
heartsounds(S1andS2),and(ii)ofrecordings
aspathologicorphysiologic.Conventionallyheartsound
methodsemployedNeuralNetworks
1
THasanandAhmedImtiazHumayunareafwithmHealth
Laboratory,DepartmentofBiomedicalEngineering,BangladeshUniversity
ofEngineeringandTechnology(BUET),Dhaka-1205,Bangladesh.Email:
taufiq@bme.buet.ac.bd,imtiaz@mhealth.buet.ac.bd
2
ShabnamGhaffarzadeganandZheFengarewiththeHuman
MachineInteractionGroup-2,RobertBoschResearchand
TechnologyCenter(RTC),PaloAlto,CA-94304,USA.Email:
f
shabnam.ghaffarzadegan,zhe.feng2
g
@us.bosch.com
(ANN)[3],SupportVectorMachines(SVM)[4]andHid-
denMarkovModels(HMM)[5].The2016Physionet/CinC
Challengereleasedanarchiveof
4430
PCGrecordings,
whichisthemostextensiveopen-sourceheartsounddataset
todate.Time,frequencyandstatisticalfeatures[6],Mel-
frequencyCepstralCoef(MFCC)[7],andContinuous
WaveletTransform(CWT),weresomeofthecommonly
usedfeaturesbythePhysioNetchallengeentrants.Among
thetopscoringsystems,Maknickasetal.[8]extractedMel-
frequencySpectralCoef(MFSC)fromunsegmented
signalsanduseda2DCNN.Plesingeretal.[9]proposed
anovelsegmentationmethod,ahistogrambasedfeature
selectionmethodandparameterizedsigmoidfunctionsper
feature,todiscriminatebetweenclasses.Variousmachine
learningalgorithmsincludingSVM[10],k-NearestNeighbor
(k-NN)[7],MultilayerPerceptron(MLP)[11],[12],Random
Forest[6],1D[13]and2DCNNs[8],andRecurrentNeural
Network(RNN)[14]wereemployedinthechallenge.A
goodnumberofsubmissionsusedanensembleof
withavotingalgorithm[6],[11],[12],[13].Thebestper-
formingsystemwaspresentedbyPotesetal.[13]thatcom-
bineda1D-CNNmodelwithanAdaboost-Abstain
usingathresholdbasedvotingalgorithm.Filter-banksare
usedasastandardpre-processingstepduringaudiofeature
engineeringandarealsoincorporatedin[13]beforethe1D-
CNN.However,noparticularphysiologicalof
the-bankstructureandtheircutofffrequency
werepresented.Inthiswork,weproposeaCNNbasedFinite
ImpulseResponse(FIR)-bankfront-end,thatlearnsthe
frequencycharacteristicsoftheFIRsuchthattheyare
moreeffectiveindistinguishingpathologicheartsounds.
II.D
ATASET
A.PhysioNet/CinCChallengeDataset
The2016PhysioNet/CinCChallengedataset[15]isanac-
cumulationofPCGrecordingsfromsevendifferentresearch
groups,consistingofanopentrainingsetandahiddentest
set.Thedatasetcontainssixsubsets(a-f)correspondingto
thecontributinggroups.Thetrainingdatacontains
3153
heart
soundrecordingscollectedfrom
764
patientswithatotal
numberof
84
;
425
cardiaccyclesrangingfrom
35
to
159
bpm.Thedatasetisclassunbalancedwith
2488
Normal
and
665
Abnormal
heartsoundrecordings.
B.DatasetPreparationforCross-validation
Consideringthefactthatthedatasetisunbalancedand
thenumberofrecordingsissmall,wedividedthedataset
c

2018IEEE.Acceptedforpublicationinthe40thInternationalEngineeringinMedicineandBiologyConference(EMBC).
arXiv:1806.05892v1  [cs.CV]  15 Jun 2018Fig.1.ThePCGrecordingsintheprepared4-foldcrossvalidationdataset.
into4foldsforcross-validation,withbalancedvalidation
sets(equalnumberofnormalandabnormalrecordings).A
validationsetof
301
recordingswasalreadyprovidedby
PhysioNet(Fold0).Therestofthethreefoldsarecreated
byrandomsamplingwithoutreplacement.
III.P
ROPOSED
M
ETHOD
A.BaselineImplementation
OurbaselinesystemfollowstheCNNsystemproposed
inthetopscoringsolution[13]ofthePhysionet/CinC2016
challenge.Firstwepre-processthesignal,todecomposeit
intointofourfrequencybands(
25

45
,
45

80
,
80

200
,
200

500
Hz).Next,cardiaccyclesareextractedusingPCG
segmentation[16]andzeropaddedtobe
2
:
5
sinlength.
Fourdifferentbandsofeachcardiaccyclearefedinto
4
differentinputbranchesofthe1D-CNN.Eachbranchhas
twoconvolutionallayersofkernelsize
5
,followedbya
LinearUnit(ReLU)activationandamax-pooling
of
2
.Theconvolutionallayerhas
8
whilethe
secondhas
4
.Theoutputsofthefourbranchesarefed
toanMLPnetworkafterbeingandconcatenated.
TheMLPnetworkhasahiddenlayerof
20
neuronswith
ReLUactivationandasingleneuronasoutputwithsigmoid
activation.Adamoptimizationisusedwithbinarycross-
entropyasthelossfunction.Theresultingmodelprovides
predictionsoneverycardiaccycle,whichareaveragedover
theentirerecordingandroundedforinference.
B.LearnableFilter-banks:ThetConvLayer
Foracausaldiscrete-timeFIRoforder
N
with
coef
b
0
;b
1
;:::b
N
,theoutputsamples
y
[
n
]
are
obtainedbyaweightedsumofthemostrecentsamplesof
theinputsignal
x
[
n
]
.Thiscanbeexpressedas:
y
[
n
]=
b
0
x
[
n
]+
b
1
x
[
n

1]+
:::::
+
b
N
x
[
n

N
]
=
N
X
i
=0
b
i
x
[
n

i
]
:
(1)
Throughalocalconnectivitypatternofneuronsbetweenad-
jacentlayers,a1D-CNNperformscross-correlationbetween
it'sinputandit'skernel.Theoutputofaconvolutionallayer,
withakernelofoddlength
N
+1
,canbeexpressedas:
y
[
n
]=
b
0
x
[
n
+
N
2
]+
b
1
x
[
n
+
N
2

1]+
::::
+
b
N
2
x
[
n
]+
::::
+
b
N

1
x
[
n

N
2
+1]+
b
N
x
[
n

N
2
]
=
N
X
i
=0
b
i
x
[
n
+
N
2

i
]
(2)
Fig.2.OperationofatConvlayerasanFIR-bank.
where
b
0
;b
1
;:::b
N
arethekernelweights.Consideringa
causalsystemtheoutputoftheconvolutionallayerbecomes:
y
[
n

N
2
]=
˙
 

+
N
X
i
=0
b
i
x
[
n

i
]
!
(3)
where
˙
(

)
istheactivationfunctionand

isthebiasterm.
Therefore,a1Dconvolutionallayerwithlinearactivation
andzerobias,actsasanFIRwithanaddeddelay
of
N=
2
[17].Wedenotesuchlayersastime-convolutional
(tConv)layers(Fig.2)[18].Naturally,thekernelsofthese
layers(similarto-bankcoefcanbeupdated
withStochasticGradientDescent(SGD).Thisenablesthe
tConvlayerstolearncoefthatdecomposethesignal
intopathologicallymeaningfulsub-bands.Thefront-end
-banksusedin[13]werecoalescedwiththemodel
architectureusingtConvlayers.Theresultingarchitecture
isshowninFig.3.
DuringimplementationofthetConvnetwork,further
overthebaselinemodelwasmadethatinclude:
additionoftheinitializationschemepresentedbyHeetal.
[19],droppingoutactivationsbeforemax-pooling,andaddi-
tionofbatch-normalizationaftereveryconvolutionallayer.
Thehyper-parametersarere-tunedforoptimalperformance
asshowninTableI,usingTreeofParzenEstimators[20].
C.VariantsofthetConvLayer
1)LinearPhasetConv:
TheFIRintuitionoftConvlayers
institutesnewinsightsintothefrequencyandphaseresponse
ofthekernel.Especially,largekernelscanintroducesig-
phasedistortionintotheiractivations.Thephase
responseofaindicatesthephaseshiftinradiansthat
eachinputcomponentsinusoidwillundergo.Aconvolutional
kernelwithnon-linearphasewouldintroduceatemporal
shiftbetweenthehighfrequency(e.g.,murmurs)andlow
frequency(e.g.,systoleanddiastole)patternsinthePCG
signal.Tomitigatetheeffect,weproposeanovelconvolu-
tionalarchitecturetermedlinearphasetConv.LinearPhase
(LP)istheconditionwhenthephaseresponseofaisa
linearfunctionoffrequency(excludingphasewrapsat+/-
ˇ
radians).Akernelwithsymmetricweightsarounditscenter
wouldhavelinearphase,i.e.,itwouldintroduceanequal
TABLEI
H
YPERPARAMETERSOFTHE
1D-CNNM
ODEL
Parameter
Value
LearningRate
0.0012843784
LearningRateDecay
0.0001132885
DropoutafterConvLayers
50%
L2-regularizationinConvlayers
0.0486
Poolsize
2
Fig.3.ProposedCNNModelArchitectureincludingaLearnableFront-endFilter-bank(tConvLayers).
TABLEII
E
XPERIMENTAL
R
ESULTSONTHE
4-F
OLD
C
ROSS
-V
ALIDATION
D
ATA
Model
Fold
Sensitivity
(%)

(%)
Macc
(%)
Cross-fold
Sesitivity
(%)
Cross-fold

(%)
Cross-fold
Macc
(%)
Baseline[13]
0
63.76
81.11
72.44
64.06
(

2.5)
91.06
(

6.8)
77.56
(

3.4)
1
64.07
94.15
79.11
2
61.06
96.57
78.82
3
67.33
92.39
79.86
tConv
Non-Learn
0
89.30
56.26
72.78
88.74
(

1.53)
79.94
(

16)
84.34
(

7.85)
1
89.39
86.54
87.97
2
89.80
90.48
90.14
3
86.47
86.47
86.47
tConv-
FIRInit
0
91.57
57.14
74.36
87.72
(

2.9)
83.39
(

17.5)
85.55
(

7.5)
1
86.29
91.31
88.81
2
88.14
93.15
90.64
3
84.87
91.98
88.42
LP-tConv-
FIRInit
0
88.45
65.65
77.05
90.91
(

2.4)
83.29
(

11.8)
87.10
(

6.79)
1
93.81
88.38
91.10
2
91.72
90.97
91.35
3
89.64
88.14
88.89
ZP-tConv-
FIRInit
0
90.73
56.65
73.69
89.52
(

1.1)
81.41
(

16.6)
85.47
(

8)
1
89.22
90.31
89.77
2
89.97
91.81
90.89
3
88.14
86.88
87.51
LP-tConv-
RandInit
0
73.23
79.84
76.53
84.01
(

8)
86.79
(

5.8)
85.40
(

6.2)
1
92.40
86.13
89.26
2
86.13
93.98
90.06
3
84.29
87.22
85.76
delayforallofthepassingfrequencies/patterns,ensuringno
distortion.ResultsarefurtherdiscussedinSec.IV-B.
2)ZeroPhasetConv:
Zerophase(ZP)isaspecial
caseofalinearphaseFIR,wherethephaseresponse
isIncorporatingaforward-reverseconvolutioninto
tConvlayers[21],weproposeazerophasetConvlayer,the
operationofwhichisshowninFig4.If
x
[
n
]
istheinput
signal,
h
[
n
]
istheimpulseresponseofthekernel,and
X
(
e
j!
)
and
Y
(
e
j!
)
arefouriertransformsof
x
[
n
]
and
h
[
n
]
:
Y
(
e
j!
)=
X
(
e
j!
)
:H

(
e
j!
)
:H
(
e
j!
)
=
X
(
e
j!
)
j
H
(
e
j!
)
j
2
where
H
(
e
j!
)=
j
H
(
e
j!
)
j
6
H
(
e
j!
)
(4)
Notethat,theoperationintimedomainisequivalent
totakingthecomplexconjugateinthefrequencydomain.
Therefore,theeffectofaZP-tConvisjustamultiplication
bythesquaredmagnitudeinthefrequencydomain.
Fig.4.ForwardreverseinaZeroPhasetConv(ZP-tConv)layer.
IV.R
ESULTSAND
D
ISCUSSION
A.ExperimentalEvaluation
Theperformanceoftheproposedmethodsareevaluated
andcomparedwiththebaselineonour4-foldcrossvalidation
dataset(Sec.II-B).Thelossfunctionwasweightedduring
trainingtodrawemphasisonabnormalrecordingsasthey
representedonly21%ofthedata.Asperformancemetrics,
sensitivity,andMacc(meanofsensitivityand
arecalculatedandaveragedoverthe4-folds.
TheproposedtConvmodelisalsoevaluatedwiththeFIR
parametersedasdescribedinSec.III-A(tConvNon-
Learn).TheresultsaresummarizedinTableII.
Fromtheresults,weobservethatthebestsystemattained
anaveragedcross-foldMaccof87.10
(

6
:
79
)
%usingthe
proposedLP-tConvapproachwithFIRinitialization.This
representsanabsoluteimprovementof9.54%overthebase-
lineCNNsystem[13].OthervariantsoftheproposedtConv
systemsalsoprovidedsuperiorperformancecomparedtothe
baselineasseeninTableII.
B.KernelInitializationfortConvlayers
AsdiscussedinIII-C.1makingthekernelsymmetric
reducesphasedistortionwhichhasanadditionalof
requiringhalfthenumberoflearnableparametersinatConv
Fig.5.Inthepanelofeachrowrepresentinputbranches(1-4)of
theCNNmodel.Foreachinputbranch,thecolumnsrepresent:(a)Initial
FIRcoef(b)LearnedFIRcoefintConv,(c)LearnedFIR
coefinLP-tConv(d)Magnitude(blue)andphaseresponse(green)
ofthelearnedviaLP-tConv.
Fig.6.ValidationAccuracypercardiaccycleforLP-tConvondifferent
trainingsubsetsofthePhysionetHeartSoundDataset.
layer.ComparedtoZP-tConv,learningsymmetricpattern
improvestheMaccmetric(TableII).Wealsoexperimented
withzero,randomandFIRinitialization(initializedwith
FIRcoefasinSec.III-A)schemes.Visualizingthe
learnedcoefandtheirfrequencyresponses(Fig5),we
observethathigherfrequencycoefarelessaffected
bytrainingcomparedtolowerfrequencycoef
C.DatasetVariabilityandResultAnalysis
InFig.6,wecomparetheperformanceoftheproposed
LP-tConvsystemoverdifferentdatasubsets.Themodel
performedlowestontheSUAHSDB(training-f)subsetof
thePhysioNetdata.Performanceonfold0issubstandard
comparedtotheotherfolds(TableII).Wewereunableto
anycorrelationbetweensignalqualityandthemodel
performance.ALong-TermSpectralAverage(LTSA)[22]
overnormalheartsoundPCGshoweddifferencesinthe
frequencycharacteristicsofsensorsusedduringrecordingas
seeninFig.7.Here,adistinctdifferenceisvisiblebetween
thefrequencyenvelopeofJABESelectronicstethoscopeand
theotherstethoscopes.Approximately67%ofthetraining
databelongstotraining-e,whichcreatedadependencyof
themodeltowardsthecharacteristicsofthissubset.Besides,
training-ewasrecordedusingauniquepiezoelectricsensor
basedstethoscope[15],whichmayalsobecontributing
towardsthesuboptimalgeneralization.Fold0containeda
lowerpercentageoftraining-einitsvalidationset,which
explainsthepoorvalidationperformance.
V.C
ONCLUSION
Inthisstudy,wehaveproposednoveltConvlayerswith
CNNaslearnable-banksfornormal-abnormalheart
soundDifferentinitializationstrategieshave
beenexaminedforthetConvlayerswhileconstraintshave
beenaddedtoensureazeroandlinearphaseresponseinthe
resultingFIRlters.Experimentalresultsusingtheproposed
architectureshowsimprovementscomparedto
state-of-the-artsolutionswithrespecttovariousperformance
metricsonacross-validationtaskpreparedusingthePhys-
ioNetheartsoundchallengedataset.
R
EFERENCES
[1]W.H.O.factsheet317,
Cardiovasculardiseases(CVDs)
,May2017.
[2]U.Alam,O.Asghar,S.Q.Khan,S.Hayat,andR.A.Malik,ﬁCardiac
auscultation:anessentialclinicalskillindecline,ﬂ
Br.J.Cardiology
,
vol.17,no.1,p.8,2010.
[3]H.U

guz,ﬁAbiomedicalsystembasedonneuralnetworkand
principalcomponentanalysisfordiagnosisoftheheartvalvediseases,ﬂ
J.Med.Syst.
,vol.36,no.1,pp.61Œ72,2012.
Fig.7.Long-termSpectralAverage(LTSA)ofnormalheartsound
recordingscapturedusingdifferentsensors.
[4]A.Gharehbaghi,P.Ask,M.Lind
´
en,andA.Babic,ﬁAnovelmodelfor
screeningaorticstenosisusingphonocardiogram,ﬂin
Proc.NBCBME
.
Springer,2015,pp.48Œ51.
[5]R.Sarac¸O

gLu,ﬁHiddenmarkovmodel-basedofheart
valvediseasewithpcafordimensionreduction,ﬂ
Eng.Appl.Artif.
Intell.
,vol.25,no.7,pp.1523Œ1528,2012.
[6]M.N.HomsiandP.Warrick,ﬁEnsemblemethodswithoutliersfor
phonocardiogramclﬂ
Physiol.Meas.
,vol.38,no.8,p.
1631,2017.
[7]I.J.D.Bobillo,ﬁAtensorapproachtoheartsoundﬂin
Proc.IEEECinC
,2016,pp.629Œ632.
[8]V.MaknickasandA.Maknickas,ﬁRecognitionofnormalŒabnormal
phonocardiographicsignalsusingdeepconvolutionalneuralnetworks
andmel-frequencyspectralcoefﬂ
Physiol.Meas.
,vol.38,no.8,
p.1671,2017.
[9]F.Plesinger,I.Viscor,J.Halamek,J.Jurco,andP.Jurak,ﬁHeartsounds
analysisusingprobabilityassessment,ﬂ
Physiol.Meas.
,vol.38,no.8,
p.1685,2017.
[10]B.M.Whitaker,P.B.Suresha,C.Liu,G.D.Clifford,andD.V.
Anderson,ﬁCombiningsparsecodingandtime-domainfeaturesfor
heartsoundﬂ
Physiol.Meas.
,vol.38,no.8,p.1701,
2017.
[11]E.KayandA.Agarwal,ﬁDropconnectedneuralnetworkstrainedon
time-frequencyandinter-beatfeaturesforclassifyingheartsounds,ﬂ
Physiol.Meas.
,vol.38,no.8,p.1645,2017.
[12]M.Zabihi,A.B.Rad,S.Kiranyaz,M.Gabbouj,andA.K.Katsagge-
los,ﬁHeartsoundanomalyandqualitydetectionusingensembleof
neuralnetworkswithoutsegmentation,ﬂin
Proc.IEEECinC
,2016,
pp.613Œ616.
[13]C.Potes,S.Parvaneh,A.Rahman,andB.Conroy,ﬁEnsembleof
feature-basedanddeeplearning-basedfordetectionof
abnormalheartsounds,ﬂin
Proc.IEEECinC
,2016,pp.621Œ624.
[14]T.-c.I.YangandH.Hsieh,ofacousticphysiological
signalsbasedondeeplearningneuralnetworkswithaugmented
features,ﬂin
Proc.IEEECinC
,2016,pp.569Œ572.
[15]C.Liu,D.Springer,Q.Li,B.Moody,R.A.Juan,F.J.Chorro,
F.Castells,J.M.Roig,I.Silva,A.E.Johnson
etal.
,ﬁAnopenaccess
databasefortheevaluationofheartsoundalgorithms,ﬂ
Physiol.Meas.
,
vol.37,no.12,p.2181,2016.
[16]D.B.Springer,L.Tarassenko,andG.D.Clifford,ﬁLogisticregression-
HSMM-basedheartsoundsegmentation,ﬂ
IEEETrans.onBiomed.
Eng.
,vol.63,no.4,pp.822Œ832,2016.
[17]R.MateiandG.Liviu,ﬁAclassofcircularly-symmetricCNNspatial
linearﬂvol.19,pp.299Œ316,012006.
[18]T.N.Sainath,R.J.Weiss,A.Senior,K.W.Wilson,andO.Vinyals,
ﬁLearningthespeechfront-endwithrawwaveformCLDNNs,ﬂin
Proc.ISCAInterspeech
,2015.
[19]K.He,X.Zhang,S.Ren,andJ.Sun,ﬁDeepresiduallearningfor
imagerecognition,ﬂin
Proc.IEEECVPR
,2016,pp.770Œ778.
[20]J.S.Bergstra,R.Bardenet,Y.Bengio,andB.K
´
egl,ﬁAlgorithms
forhyper-parameteroptimization,ﬂin
Adv.NeuralInf.Process.Syst.
,
2011,pp.2546Œ2554.
[21]B.E.Shi,ﬁEstimatingtheCNNsteadystateusingforward-backward
recursions,ﬂin
Proc.IEEECNNA
,2006,pp.1Œ6.
[22]D.Byrne,H.Dillon,K.Tran,S.Arlinger,K.Wilbraham,R.Cox,
B.Hagerman,R.Hetu,J.Kei,C.Lui
etal.
,ﬁAninternational
comparisonoflong-termaveragespeechspectra,ﬂ
TheJ.Acoust.Soc.
Am.
,vol.96,no.4,pp.2108Œ2120,1994.
"
66,Automated Image Data Preprocessing with Deep Reinforcement Learning,https://arxiv.org/pdf/1806.05886v2.pdf,https://github.com/IBM/automation-of-image-data-preprocessing,"AutomatedImageDataPreprocessingwithDeep
ReinforcementLearning
TranNgocMinh
1
,MathieuSinn
2
,HoangThanhLam
3
,MartinWistuba
4
IBMResearchDublin,Ireland
1,4
{m.n.tran,martin.wistuba}@ibm.com
,
2,3
{mathsinn,t.l.hoang}@ie.ibm.com
Abstract
Datapreparation,i.e.theprocessoftransformingrawdataintoaformatthatcan
beusedfortrainingeffectivemachinelearningmodels,isatediousandtime-
consumingtask.Forimagedata,preprocessingtypicallyinvolvesasequence
ofbasictransformationssuchascropping,rotatingorimages.
Currently,datascientistsdecidemanuallybasedontheirexperiencewhichtrans-
formationstoapplyinwhichparticularordertoagivenimagedataset.Besides
constitutingabottleneckinreal-worlddatascienceprojects,manualimagedata
preprocessingmayyieldsuboptimalresultsasdatascientistsneedtorelyonin-
tuitionortrial-and-errorapproacheswhenexploringthespaceofpossibleimage
transformationsandthusmightnotbeabletodiscoverthemosteffectiveones.To
mitigatetheinefciencyandpotentialineffectivenessofmanualdatapreprocessing,
thispaperproposesadeepreinforcementlearningframeworktoautomatically
discovertheoptimaldatapreprocessingstepsfortraininganimage.The
frameworktakesasinputsetsoflabeledimagesandpreprocessing
transformations.Itjointlylearnstheandtheoptimalpreprocessingtrans-
formationsforindividualimages.Experimentalresultsshowthattheproposed
approachnotonlyimprovestheaccuracyofimagebutalsomakesthem
substantiallymorerobusttonoisyinputsattesttime.
1Introduction
Datapreprocessing,i.e.theprocessoftransformingrawdataintoaformatthatcanbeusedfortraining
effectivemachinelearningmodels,accountsfor50-80%ofthetimespentontypicaldatascience
projects[
3
,
11
].Besidesconstitutingabottleneck,manualdatapreprocessingisalsoineffectiveasit
onlyexploresasmallpartofthespaceofpossibletransformationsandthusmightnotdiscoverthe
mosteffectiveonesforremovingnoiseand/orextractingmeaningfulfeaturesfromagivensetof
rawdata.Unstructureddata
1
areparticularlychallenginginthisregardastheirpreparationrequires
deepexpertiseinsuchasComputerVisionorNaturalLanguagePreprocessing;moreover,
becauseofthehighcomplexityofmachinelearningmodelsdealingwithsuchdata,theeffectofdata
preprocessingisparticularlydiftounderstand.Hence,automatingdatapreprocessingishighly
desirableasitincreasestheproductivityofdatascientistsandmayleadtobetterperformanceofthe
resultingmachinelearningmodels.
Despiteofitshighpotentialvalue,theautomationofdatapreprocessinghasbeenmostlyoverlooked
bythemachinelearningcommunity,withonlyfewpriorworksonthissubject[
3
,
13
].Recently,Bilalli
etal.[
3
]suggestedamethodforautomatingdatapreprocessingviameta-learning.However,their
approachonlyfocusesonstructureddatawithalimitednumberofrelativelysimplepreprocessing
1
By
unstructureddata
wemeanimages,textandtimeseries,whileweuse
structureddata
torefertodatain
tabularformat,e.g.asinrelationaldatabases.
Preprint.Workinprogress.
arXiv:1806.05886v2  [cs.CV]  29 Apr 2021techniquessuchasnormalization,standardizationanddiscretization.Furthermore,preprocessingin
theircasedoesnotaddressindividualdatainstances,butinsteadappliestothewholedataset.The
studyin[13]developsatransformationpursuitforimagewhichisalsoappliedtothe
wholedataset.Whilethesestudiesprovideprincipledmethodsforaugmentingtrainingdata,they
stillsufferfromanumberofintrinsicdisadvantages.Firstly,givenalargesetoftransformations
2
,
applyingthemtoalldatainstancesresultsinanexcessivelylargetrainingdataset,increasing
prohibitivelytheamountoftrainingtime.Secondly,notethattheorderofapplyingtransformations
isimportant;forexample,rotatingandthenanimagecreatesadifferentresultthan
andthenrotatingtheimage.Withalargetransformationset,effectivechainsof
transformations
3
requiresanexhaustivesearchoveranexponentialsearchspace.Thirdly,applying
thesametransformationchaintoalldatainstancesisoftenineffectiveaseachdatainstancehasits
ownfeatureandhenceshouldbepreprocessedinadifferentway.Forexample,differentimagesmight
requirerotationswithdifferentanglesbecausetheyweretakenfromdifferentviews;clearly,applying
thesamerotationtothemintheaugmentedtrainingsetisinefLastly,anaugmentationwith
irrelevanttransformationsmayevenproducewronglylabeleddatainstances,e.g.a180-degree
rotationofanimageofthedigitﬁ6ﬂproducesanimageofthedigitﬁ9ﬂ.
Inthiswork,weproposeanautomatedmethodfordatapreprocessingthattransformseachdata
instanceindividually
4
.Althoughwepresentourapproachforimageinputs,iteasilygeneralizes
toothertypesofdata,suchastimeseriesortext.Theproposedapproachisbasedonadeep
reinforcementlearningframeworktoaddressthelimitationsofstate-of-the-artapproachesdiscussed
above:Firstly,ourpreprocessingapproachresultsinonevariantperimage,sotheaugmentedtraining
datasethasthesamesizeastheoriginalone.Secondlyandthirdly,wepreprocesseachimage
individuallywithitsowntransformationchainwhichisdiscovered,usingreinforcement
learningtoexplorethespaceofpotentialtransformationchains.Lastly,ourapproachdiscovers
optimaltransformationchains,againbyusingreinforcementlearningtoexploitthemosteffective
ones.Experimentalresultsonreal-worlddatasetsshowthatthemorenoiseadatasetcontains,
themoreeffectivetheframeworkistoimprovetheaccuracyofatrainedonthatdataset.
Furthermore,thealsobecomessubstantiallymorerobustagainstnoisyinputsattesttime
whenbeingtrainedusingourproposedframework.
Theremainderofthispaperisorganizedasfollows.WediscussrelatedworkinSection2andthen
presenttheaddressedchallengeaswellasourproposedsolutioninSection3.Inordertoevaluatethe
solution,wedescribeanevaluationmethodologyinSection4andexperimentsinSection5.Finally,
wediscussandconcludeourstudyinSections6and7.
2RelatedWork
Generalizationisthemainchallengeofimages,particularlywhentrainedonsmalland/or
noisytrainingdatasets.Therefore,numerousapproacheshavebeenproposedtoimprovethegener-
alization,suchasaddingaregularizationtermonthenormofweights[
4
],usingdropout[
18
,
19
]
orbatchnormalization[
20
].Dataaugmentationisanothereffectiveapproachthathelpsincrease
thegeneralizationofimagethroughapplyingsimpletransformationssuchasrotatingand
inputimages,andaddingthetransformedimagestothetrainingset[
10
].Thefullsetof
transformationsusedin[
10
]includesshifting,zoomingin/out,rotating,distorting,shading
andstyling.Dataaugmentationwithmorecomplicatedtransformationsisinvestigatedin[
9
],which
evaluatesthreeconcretepreprocessingtechniques,namelyZeroComponentAnalysis,MeanNormal-
izationandStandardization,ontheperformanceofdifferentconvolutionalneuralnetworks.Whilethe
approachesin[
9
,
10
]preprocessimagesaccordingtoapreselectedchainoftransformations,Paulinet
al.[
13
]suggestthatthetransformationsetshouldbechoseninprincipledwayinsteadofresortingto
(manual)trial-and-error,whichisfeasibleonlywhenthenumberofpossibletransformationsissmall.
Theirproposedapproachselectsasetoftransformations,possiblyordered,throughagreedysearch
strategy.Althoughthisapproachoffersamorecompetitivesetoftransformations,itstillhasseveral
limitations:Firstly,thesearchprocessisinefcientbecauseitinvolvesretrainingtheon
2
Throughoutthispaper,weusethewords
preprocessing
and
transformation
interchangeablytoindicatean
operationappliedtodatainstancessuchasanimage.
3
Theterm
chainoftransformations
isusedtoindicateanorderedsetoftransformations.
4
Animplementationofthemethodcanbefoundat
https://github.com/IBM/automation-of-image-data-
preprocessing
.
2
thewholeaugmenteddataseteverytimeacandidatetransformationinthesearchspaceisevaluated.
Secondly,thesamepreprocessingtransformationsareappliedtoallimages,whichhasanumberof
disadvantagesasdiscussedinSection1.Ourapproachusesareinforcementlearningframeworkto
addresspreciselythoseshortcomings.
Reinforcementlearning[
14
]anddeepreinforcementlearning[
7
]haverecentlydrawn
substantialattentionfromthemachinelearningresearchcommunity.However,thereareonlyfew
studies[
1
,
2
,
8
]applyingdeepreinforcementlearningtovisualrecognitiontasks,suchasedge
detection,segmentationandobjectdetection,oractiveobjectlocalization.However,noneofthese
worksconsidersautomatingthepreprocessingofimagesorlearningtransformationsets.Tothebest
ofourknowledge,ourworkisthestudyutilizingdeepreinforcementlearningtosearchfor
effectivechainsofpreprocessingtransformationsforindividualimages.
3ReinforcementLearningFramework
Wepresentinthissectionourdetailedsolutionforimagepreprocessingusingadeepreinforcement
learningframework.Theframeworkincludestwobasiccomponents,namely,anagentandan
environment.Givenaninputimage,thegeneralwwoftheframeworkisshowninFigure1.
Theagenthasitspolicyrepresentedbyadeepneuralnetworkwhoseoutputsareactionvaluesused
byadecisionmakertodecidewhetheranimageissufpreprocessedorwhetheradditional
transformationsneedtobeapplied.Theenvironmentisresponsiblefortransformingtheimage
uponrequestbytheagent,whereaconcretetransformationoperationisgiventogetherwiththe
request.Afterdoingso,theenvironmentcontinuestofeedthetransformedimageintothedeepneural
networkforanewevaluation.Thisprocessisiterateduntiltheagentmakesadecisiontostop
preprocessingtheimage.Inadditiontoperformingtheactualimagetransformations,theenvironment
alsoevaluatestheimpactofeachtransformationandproducesrewardsbasedonthat.Technical
detailsofeachcomponentoftheframeworkaregiveninthefollowingsubsections.
Figure1:Thewwofpreprocessingimages.
3.1StateandActionSpaces
Inthissectionwestatesandactionsusedinourreinforcementlearningframeworkand
introduceanimportantpropertyofpreprocessingtechniquesthatcanbeused.
3.1.1StateSpace
Weastateasanimage,thusanoriginalstatecorrespondstoanoriginalimage,anda
transformedstatecorrespondstoapreprocessedimage.Assuch,thestatespaceconsistsofall
originalimagesaswellasallimagestransformedbyanarbitrarychainoftransformations.Itiseasy
toseethatthesizeofthestatespacegrowsexponentiallyinthelengthofthetransformationchains.
3.1.2ActionSpace
InFigure2(a),weshowthetypicalarchitectureofaconvolutionalneuralnetwork(CNN)whose
outputisalogitvectorwith
k
valuesrepresentingthecorrespondingunnormalizedlikelihoodofthe
k
3
outputclasses.Inourstudy,weuseavariantofDeepQ-Network(DQN)[
16
,
17
]tomodelanetwork
policy.ThenetworkpolicyimplementedinaDQNasshowninFigure2(b)resemblestheCNNin
Figure2(a),exceptthattheoutputlayerisextendedtoformanactionspace.TheDQNoutputlayer
containingQ-valuesconsistsoftwopartsthatarecorrespondingtotwogroupsofactions.The
partisavectorplayingthesameroleasthelogitvectorintheCNN,i.e.itrepresentstheunnormalized
likelihoodofthe
k
classes.Wedenoteeachslotinthispartasastopaction
SAction
i
.Ifthedecision
makerdecidesonthenextactionasoneofthestopactions
SAction
i
,thepreprocessingofaninput
imagewillstopwithapredictionofclass
i
fortheinputimage.ThesecondpartoftheDQNoutput
layerisavectorrepresentingasetof
n
transformationactions.Ifadecisionismadeforanext
actionwithoneofthetransformationactions
TAction
j
,thecurrentimagewillbecontinuedto
bepreprocessedwiththetransformation
j
.Thetwosetsofstopandtransformationactionsform
anactionspacewhichhastotally
k
+
n
actionsincaseofdiscretetransformation.Notethatitis
straightforwardtoalsosupportcontinuousactions.Forexample,wecanmodelacontinuousrotation
bytwoslotsinthesecondpartoftheDQNoutput:onefortheQ-valueoftherotationaction
andoneforthevalueoftherotationangle.Likewise,wecanalsoadaptthepartoftheDQN
outputinordertoapplytheframeworktoaregressionproblem,e.g.whentheinputsaretimeseries
andthetaskistoforecastfuturevalues.
Figure2:Illustrationofdeepneuralnetworkpolicies
.
3.1.3SymmetryofaTransformationAction
Thereisanimportantpropertythatanyconsideredtransformationoperationshouldpossess,the
so-calledsymmetryproperty:Duetotheexploratorynatureofthereinforcementlearningapproach,
itisessentialtoallowanimagetoberecoveredafteratrialtransformationthatresultedinapoor
performance.Forexample,ifthereisarotationtransformationwithanangle

,thereshould
beanotherrotationtransformationwiththeangle


inthesetofpossibletransformations.For
transformationssuchasimagecropping,invertingtransformationsisnotasstraight-forward;it
requiresimplementingamemorymechanismtorememberstatesbeforeanytransformation.Asthis
mayleadtolargememoryusageforlongtransformations,amechanismtocontrolthemaximum
lengthofatransformationchainshouldbeused(seeSection3.4).
3.2DecisionMaker
Thedecisionmakeriswhereareinforcementlearningpolicyisdeployed.Itisresponsiblefor
selectingthenextactiontobeappliedtothecurrentstate.Theactionandthestatearethenpassedto
theenvironmentcomponentforfurtherprocessing.Inourstudy,weusethemaxpolicytoselectan
appropriateaction,giventheDQNoutputlayer.Furthermore,inordertoenabletheexplorationof
reinforcementlearning,weallowthedecisionmakertoselectalternativenextactionsrandomlywith
someprobability

,whichisknownas

-greedyexplorationstrategy[
14
].Theprobability

startsata
maximumof
1
:
0
andisannealeddowntoaminimumof
0
:
1
duringtraining.
3.3DeepNeuralNetwork
UsingaDQNasinFigure2(b)isasimplestartingpoint;performancegainscanbeachievedusing
othervariantsofDQN.Inourwork,weimplementedavariantofDQN,namelyDuelingDQN
4
(DDQN)[
21
]asshowninFigure2(c).TheideabehindDDQNsisthattheQ-valueswillbea
combinationofavaluefunctionandanadvantagefunction.Thevaluefunctionhowgoodit
istobeinagivenstatewhiletheadvantagefunctionindicateshowmuchbetterselectinganactionis
comparedtotheothers.Theofseparatingthetwofunctionsisthatthereinforcementlearning
frameworkdoesnotneedtolearnbothvalueandadvantageatthesametime,andthereforeaDDQN
isabletolearnthestate-valuefunctionefciently.Inordertoupdatethedeepneuralnetwork,weuse
theBellmanequation
Q
(
s;a
)=
r
+


max
a
0
(
Q
(
s
0
;a
0
))
,where
Q
(
s;a
)
istheDQNoutputvalue
ofaction
a
giveninputstate
s
,
r
and
s
0
aretherewardandthenextstatereturnedbytheenvironment
whentheaction
a
isappliedtothestate
s
,and

isthediscountedparameter.Wereferthereaderto
[21]formoredetailsonDDQNs.
3.4Environment
Theenvironmentiswhereactualtransformationsonimagesareperformed.Inaddition,itisalso
responsibleforcalculatingreturnrewardsduringtraining.Uponreceivinganimageandanaction
fromthereinforcementlearningagent,theenvironmentbehavesdifferentlydependingonthetype
oftheaction.Ifitisatransformationaction,theenvironmentwillapplythattransformationtothe
imageonlyifthelengthofthechainoftransformationsappliedparticularlytothatimageissmaller
thanaparameter
max
_
len
.Ifthechainislongerthan
max
_
len
,theimageisrecovered
toitsoriginalstateandthereinforcementlearningframeworkmustseekanothertransformationchain
fortheimage.Note,thisrecoverymechanismisonlyusedfortraining.Attesttime,wesimplypick
thestopactionwiththelargestQ-valueforthepredictionoftheimage.Therecoverymechanism
alsosolvesthememoryproblemdescribedinSection3.1.3.Regardlessofthelengthofthecurrent
transformationchain,theenvironmentwillreturnazerorewardtothereinforcementlearningagent
inthiscase.
Iftheenvironmentreceivesastopaction
SAction
i
,itdoesnotreturnanewimagebutarewardand
theoriginalimageasclass
i
.Thestrategytocomputereturnrewardsduringtrainingplays
animportantrolefortheconvergenceofthetraining.Theenvironmentusesthegroundtruelabelof
theoriginalimagetodeterminethereward.Asimplestrategyistoassignarewardof
+1
ifthelabel
isequalto
i
and

1
ifotherwise.However,thissimplestrategydoesnotworkwellwhenthenumber
ofclasses
k
islargerthan2sinceitcausesunbalancingrewards.Hence,wesuggestamorerobust
schemetocomputereturnrewards,whichistoassignarewardof
k

1
ifthelabelisequalto
i
and

1
ifotherwise.
Figure3:Themethodologyofourexperiments.
4Methodology
OurmethodologyinsettingupexperimentsisillustratedinFigure3.Inordertoevaluateourauto-
preprocessingframework,wetrainthreedifferentmodels,namelyNN,RLandCL,showninFigures
3(a),3(b)and3(c),respectively.Forallofthem,weusethesameneuralnetworkarchitecture.Figure
3(a)representsaCNNmodelwithanarbitraryarchitecture.Thissamearchitecturewillalsobeused
asthepolicynetworkinthereinforcementlearningframeworkasshowninFigure3(b).Sinceboth
modelsusethesamenetworkarchitecture,anyperformancedifferencebetweenthetwomodelsin
5
ourexperimentsiscausedbythereinforcementlearningsolution.InFigure3(c),wealsohaveaCNN
modelwiththesamenetworkarchitecture,butwedonottrainthenetworkfromscratch.Rather,
wecontinuetothenetworkobtainedfromthereinforcementlearningframework.The
N
originaltrainingimagesarepreprocessedbytheframeworktoproduce
N
newtrainingimageswhich
areusedasinputsoftheprocess.
Inourexperiments,weimplementthreedifferentCNNarchitectures,namelyArch1,Arch2and
Arch3asshowninFigures4(a),4(b)and4(c),respectively.Hence,wehavetotallyninemodelsfor
comparison.Thearchitecturesareselectedaccordingtotheircomplexityrangingfromsimplein
Figure4(a)tocomplexinFigure4(c).Notethatthehyperparametersandarchitecturesofthemodels
inFigure4arenotdesignedﬁoptimallyﬂ(e.g.using(hyper-)parametertuningorauto-architecture
search),butchosensuchthatthereissomelevelofcomplexitydifferencebetweenthem,theeffectof
whichisdiscussedinourevaluationbelow.
Figure4:CNNarchitecturesusedinourexperiments.LRN,BN,MPandFCstandforlocalresponse
normalization,batchnormalization,maxpoolingandfullyconnected,respectively.Allconvolutional
layersuseastrideof1x1andallmaxpoolinglayersuseastrideof2x2.
5ExperimentalResults
Inthissectionwewillpresentourexperimentstovalidateoursolutiontotheproblemofimage
preprocessingautomation.Westartourexperimentsbycomparingtheaccuracyperformanceof
imagewithandwithoutpreprocessing.Then,weevaluatetherobustnessofthe
withrespecttodistortedimagesattesttime.Inaddition,wealsoprovidesomeinsightsonthe
behaviourofthereinforcementlearningframework.
5.1ExperimentalSetups
Weselectforourstudyfourdatasetswithdifferentlevelsofcomplexityandnoise.MNIST[
12
]
isaveryclean10-classdatasetwith70K28x28x1imagesdividedinto55K/5K/10Kfortraining,
validationandtesting,respectively.SVHN[
5
]isa10-classdatasetthatisnoisierthanMNISTwith
˘
864K32x32x3imagesdividedinto
˘
598K/6K/26K.CIFAR[
15
]isa10-classdatasetthatisyet
noisierthanSVHNwith60K32x32x3imagesdividedinto45K/5K/10K.Finally,DOGCAT[
6
]isthe
noisiestofallfourdatasets;ithas2classeswith25K100x100x3imagesdividedinto20K/1K/4K.
Withrespecttothetransformationset,weimplementedtwooperations,namelyimagerotation
andforsimplicitybecausetheytriviallysatisfythesymmetrypropertyrequirement
withoutthenecessitytoimplementamemorymechanism.Concretely,thereare11transforma-
tionsconsistingof3(horizontally,vertically,andboth)and8rotations(withangles

1
;

2
;

4
;

8
;
+8
;
+4
;
+2
;
+1
degrees).Theparameter
max
_
len
specifyingthemaximumlength
ofatransformationchainissetto
10
inourexperiments.Othergeneralparametersinclude
optimizer
=
Adam
,
learning
_
rate
=0
:
0001
and
regularization
_
coefficient
=0
:
001
.For
eachexperiment,weperform5runswith5differentinitializationsandreportresultsas
mean

std
.
6
5.2PerformanceofImage
PerformanceresultsintermsofaccuracyareshowninTable1.Itcanbeseenthatinmostcases,
thebareconvolutionalneuralnetwork(NN)producestheworstperformancewhilethe
reinforcementlearning(RL)yieldshigheraccuracy.Theaccuracyperformanceisimproved
furtherbytheCNNthatiscontinuedtolearn(CL)fromthetrainedRL.Wenote
thattheaccuracyreportedinTable1doesnotachievestate-of-the-artperformanceasthenetworks
thatweusedinourexperimentswererelativelysimpleandnotadaptedforthedatasets;nevertheless
webelieveitisworthmentioningthattheRLframeworkresultsinimprovingtheaccuracyofthe
baselinemethods,nb:withoutincreasingthesizeofthetrainingset.Moreover,itisinteresting
toobservethattheaccuracydifferencebetweentheNNandtheRLincreases
fornoisierandmorecomplexdatasets.Ontheonehand,whileforMNISTsimplepreprocessing
techniquessuchasrotationanddonothelpimprovingtheaccuracy,theyevendecrease
accuracyassomedigitschangetheirmeaningwhenbeingrotatedorOntheotherhand,
onthemuchnoisierDOGCATdataset,theRLismuchmoresuccessfulinincreasingthe
accuracyofthebaselineCNN.
Table1:Performanceofimageintermofaccuracy.
MNIST
SVHN
CIFAR
DOGCAT
Arch1
NN
0
:
9926

0
:
0004
0
:
9429

0
:
0015
0
:
7018

0
:
0028
0
:
7461

0
:
0137
RL
0
:
9915

0
:
0010
0
:
9507

0
:
0031
0
:
7442

0
:
0046
0
:
8035

0
:
0062
CL
0
:
9935

0
:
0006
0
:
9509

0
:
0026
0
:
7410

0
:
0026
0
:
8222

0
:
0102
Arch2
NN
0
:
9941

0
:
0002
0
:
9636

0
:
0008
0
:
7910

0
:
0048
0
:
8669

0
:
0261
RL
0
:
9916

0
:
0004
0
:
9715

0
:
0016
0
:
8193

0
:
0015
0
:
9209

0
:
0124
CL
0
:
9936

0
:
0004
0
:
9716

0
:
0011
0
:
8175

0
:
0022
0
:
9044

0
:
0090
Arch3
NN
0
:
9954

0
:
0004
0
:
9734

0
:
0016
0
:
8391

0
:
0028
0
:
8647

0
:
0065
RL
0
:
9932

0
:
0007
0
:
9726

0
:
0029
0
:
8687

0
:
0080
0
:
9231

0
:
0081
CL
0
:
9955

0
:
0005
0
:
9760

0
:
0021
0
:
8700

0
:
0057
0
:
9243

0
:
0101
5.3RobustnessofImage
Inordertoevaluatetherobustnessofimagewedistorteachtestimagewith
50%
probability
byapplyingarandomchainoftransformations.Robustnessresultsintermofaccuracyareshown
inTable2.Aswecansee,theresultsareconsistentinallcasesinthesensethattheNN
islessrobust(itsaccuracydecreasesonthetestsetwithdistortions),comparedtothe
performanceoncleantestdatareportedinTable1.Ontheotherhand,theRLismuchmore
robustasitsperformanceonlyslightlydegradesonthedistortedtestdata.Notethat,only
50%
images
ofthetestsetweredistorted,hencetherobustnessdifferencebetweenthetwowouldbe
evenlargerifalltestimageshadbeendistorted.TherobustnessoftheCLisnotashighas
fortheRL,butstillsubstantiallyhigherthanthatoftheNN.Thisisatrade-off
betweentheaccuracyandtherobustnesswhenchoosingbetweentheRLandtheCL
Table2:Robustnessofimageintermofaccuracy.
MNIST
SVHN
CIFAR
DOGCAT
Arch1
NN
0
:
7442

0
:
0037
0
:
6991

0
:
0020
0
:
5099

0
:
0078
0
:
6687

0
:
0068
RL
0
:
9772

0
:
0024
0
:
8699

0
:
0033
0
:
7006

0
:
0085
0
:
7574

0
:
0203
CL
0
:
8400

0
:
0048
0
:
7945

0
:
0100
0
:
6645

0
:
0032
0
:
7723

0
:
0228
Arch2
NN
0
:
7516

0
:
0021
0
:
7242

0
:
0011
0
:
5872

0
:
0057
0
:
7936

0
:
0302
RL
0
:
9797

0
:
0016
0
:
8876

0
:
0020
0
:
7909

0
:
0027
0
:
8987

0
:
0151
CL
0
:
8301

0
:
0114
0
:
8207

0
:
0024
0
:
7709

0
:
0035
0
:
8524

0
:
0076
Arch3
NN
0
:
7563

0
:
0024
0
:
7400

0
:
0019
0
:
6402

0
:
0093
0
:
7791

0
:
0062
RL
0
:
9826

0
:
0014
0
:
9062

0
:
0015
0
:
8160

0
:
0046
0
:
8872

0
:
0074
CL
0
:
9549

0
:
0046
0
:
8462

0
:
0085
0
:
7199

0
:
0037
0
:
8845

0
:
0068
7
5.4ADeeperLookintotheOperationoftheFramework
Inordertovisualizehowthereinforcementlearningframeworkpreprocessesdistortedimages,we
runanotherexperimentonMNISTwithcoarserdistortions,inparticularrotationsareperformed
withlargeangles

90
degreesandoperationsasinthepreviousexperiment.Foreach
distortedimage,wetracetheoperationoftheframeworkandobtainthetransformationchainthat
theframeworkautomaticallygeneratesfortheimage.Anillustrationforafewimagesisshown
inFigure5.Itisinterestingthatmostimagesareeitherdirectlyortransformedtotheir
originalversionbeforebeingTheexactrecoveryispossiblethankstothesymmetry
propertyoftransformationactions.Althoughtheframeworkisabletorecoverdistortedimages,itis
notguaranteedtotheoptimalchainoftransformationsintermoftheshortestrecoverypath.In
addition,thereisasmallnumberofimageswhichareconfusedbytheframeworkasshowninthe
bottomrowofFigure5.Thesearethemainsourceoferrorsofthereinforcement
learning.
Figure5:Illustrationofhowthereinforcementlearningframeworkpreprocessesdistortedimages.
6Discussion
Thekeycontributionsofthispaperarethree-fold.Firstly,wedevelopedtheideaofautomateddata
preprocessingusingareinforcementlearningframework.Whilewedemonstratedandevaluatedit
forimagedata,itisapplicabletoothertypesofstructuredandunstructureddataaswell.Secondly,
theproposedsystemisiterativeandthereforeitprovidesexplainabledatapreprocessing,i.e.one
caninspectwhichtransformationswereappliedtoeachdatainstanceduringthepreprocessing.
Thirdly,comparedwithtraditionaldataaugmentationapproaches,oursystemfollowsamoreefcient
approachtoproduceacleantrainingdatasetthatcanbeusedeffectivelyfortraininghighlyaccurate
androbustmachinelearningmodels.
Despitebeingofhighpracticalvalue,theautomationofdatapreprocessinghasonlydrawnlittle
interestbythemachinelearningresearchcommunitysofar.Althoughwesuggestinthispapera
novelapproachforthisproblem,thereisstillalotofroomtoextendthiswork.Firstly,thesetof
transformationsmaycontainmoreadvancedpreprocessingtechniquessuchasrotationswithlearnable
angles,cropping/scalingwithlearnableratios,imagesegmentation,objectdetection,etc.Whileitis
easytointegratecontinuousactionswithlearnableparametersintotheframeworkasdescribedin
Section3.1.2,complicatedactionslikeimagesegmentationandobjectdetectionmayrequiremore
efforts.Forexample,onecouldselectonlyasmallnumberofsegmentsorobjectsasthe
representationofanimageforthenextiterationafterapplyingthoseactions.Secondly,onecould
boosttheperformanceofthereinforcementlearningframeworkbyreplacingthecurrentsimpleDQN
policynetwork.Inaddition,CNNsderivedfromthepolicynetwork(asdescribedinFigure3(c))may
beawaytoobtainbetterperformanceintermsofaccuracy.
7Conclusions
Wehavepresentedinthispaperanovelapproachtotheproblemofautomatingdatapreprocessing,
whichisofhighpotentialvalueforreal-worlddatascienceandmachinelearningprojects.The
approachisbasedonareinforcementlearningframeworktosequencesofpreprocessingtransfor-
mationsforeachdatainstanceindividually.Weshowedinourexperimentsthatevenwithsimple
preprocessingactionssuchasrotationandimagecanwith
respecttotheiraccuracyandparticularlytheirrobustness.Thankstotheiterativenatureofthe
framework,oursolutionalsoprovidesacertainlevelofexplanability,i.e.wecantraceexactlyhowan
imageispreprocessedviaachainoftransformations.Insummary,webelievethatthisisapromising
8
researchapproachtoaddresstheproblemofautomatingdatapreprocessing.Futureworkshouldaim
ataddressingcontinuousactions,transformationsthatrequirememorization,anddemonstratingthe
frameworkonothertypesofdatasuchastextortimeseries.
References
[1]
A.Gherega,M.Radulescu,M.Udrea,ﬁAQ-LearningApproachtoDecisionProblemsinImage
Processingﬂ,InternationalConferencesonAdvancesinMultimedia,Pages60-66,2012.
[2]
B.Bhanu,J.Peng,ﬁAdaptiveIntegratedImageSegmentationandObjectRecognitionﬂ,IEEE
TransactionsonSystems,ManandCybernetics,Pages427-441,2000.
[3]
B.Bilalli,A.Abello,T.Aluja-Banet,R.Wrembel,ﬁAutomatedDataPre-processingviaMeta-
learningﬂ,InternationalConferenceonModelandDataEngineering,Pages194-208,2016.
[4]
B.Wang,D.Klabjan,ﬁRegularizationforUnsupervisedDeepNeuralNetsﬂ,AAAIConference
onIntelligence,2017.
[5]CIFARDataSet,https://www.cs.toronto.edu/kriz/cifar.html,2018.
[6]
DOGCATDataSet,https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data,2018.
[7]I.Goodfellow,Y.Bengio,A.Courville,ﬁDeepLearningﬂ,MITPress,2017.
[8]
J.C.Caicedo,S.Lazebnik,ﬁActiveObjectLocalizationwithDeepReinforcementLearningﬂ,
IEEEInternationalConferenceonComputerVision,Pages2488-2496,2015.
[9]
K.K.Pal,K.S.Sudeep,ﬁPreprocessingforImagebyConvolutionalNeural
Networksﬂ,IEEEInternationalConferenceonRecentTrendsinElectronics,Information&
CommunicationTechnology,2016.
[10]
L.Perez,J.Wang,ﬁTheEffectivenessofDataAugmentationinImageusing
DeepLearningﬂ,arXiv:1712.04621,2017.
[11]
M.A.Munson,ﬁAStudyontheImportanceofandTimeSpentonDifferentModelingStepsﬂ,
ACMSIGKDDExplorations,Pages65-71,2011.
[12]MNISTDataSet,http://yann.lecun.com/exdb/mnist/,2018.
[13]
M.Paulin,J.Revaud,Z.Harchaoui,F.Perronnin,C.Schmid,ﬁTransformationPursuitfor
ImageIEEEConferenceonComputerVision&PatternRecognition,Pages
3646-3653,2014.
[14]R.S.Sutton,A.G.Barto,ﬁReinforcementLearning:AnIntroductionﬂ,MITPress,2017.
[15]SVHNDataSet,2018.
[16]
V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,D.Wierstra,M.Riedmiller,
ﬁPlayingAtariwithDeepReinforcementLearningﬂ,NIPSDeepLearningWorkshop,2013.
[17]
V.Mnihetal.,ﬁHuman-levelControlthroughDeepReinforcementLearningﬂ,NatureJournal,
Pages529-533,2015.
[18]
Y.Gal,Z.Ghahramani,ﬁATheoreticallyGroundedApplicationofDropoutinRecurrentNeural
Networksﬂ,AdvancesinNeuralInformationProcessingSystems,Pages1019-1027,2016.
[19]
Y.Kubo,G.Tucker,S.Wiesler,ﬁCompactingNeuralNetworkviaDropoutTrainingﬂ,
arXiv:1611.06148,2017.
[20]
Y.Ma,D.Klabjan,ﬁConvergenceAnalysisofBatchNormalizationforDeepNeuralNetsﬂ,
arXiv:1705.08011,2017.
[21]
Z.Wang,T.Schaul,M.Hessel,H.V.Hasselt,M.Lanctot,N.D.Freitas,ﬁDuelingNetworkAr-
chitecturesforDeepReinforcementLearningﬂ,InternationalConferenceonMachineLearning,
Pages1995-2003,2016.
9
"
67,Selfless Sequential Learning,http://arxiv.org/pdf/1806.05421v5.pdf,https://github.com/rahafaljundi/Selfless-Sequential-Learning,"PublishedasaconferencepaperatICLR2019
S
ELFLESS
S
EQUENTIAL
L
EARNING
RahafAljundi
KULeuven
ESAT-PSI,Belgium
rahaf.aljundi@gmail.com
MarcusRohrbach
FacebookAIResearch
mrf@fb.com
TinneTuytelaars
KULeuven
ESAT-PSI,Belgium
tinne.tuytelaars@esat.kuleuven.be
A
BSTRACT
Sequentiallearning,alsocalledlifelonglearning,studiestheproblemoflearning
tasksinasequencewithaccessrestrictedtoonlythedataofthecurrenttask.In
thispaperwelookatascenariowithedmodelcapacity,andpostulatethatthe
learningprocessshouldnotbei.e.itshouldaccountforfuturetaskstobe
addedandthusleaveenoughcapacityforthem.Toachieve
Sequential
Learning
westudydifferentregularizationstrategiesandactivationfunctions.We
thatimposingsparsityattheleveloftherepresentation(i.e.neuronactivations)
ismoreforsequentiallearningthanencouragingparametersparsity.
Inparticular,weproposeanovelregularizer,thatencouragesrepresentationspar-
sitybymeansofneuralinhibition.Itresultsinfewactiveneuronswhichinturn
leavesmorefreeneuronstobeutilizedbyupcomingtasks.Asneuralinhibition
overanentirelayercanbetoodrastic,especiallyforcomplextasksrequiringstrong
representations,ourregularizeronlyinhibitsotherneuronsinalocalneighbour-
hood,inspiredbylateralinhibitionprocessesinthebrain.Wecombineournovel
regularizerwithstate-of-the-artlifelonglearningmethodsthatpenalizechanges
toimportantpreviouslylearnedpartsofthenetwork.Weshowthatournewreg-
ularizerleadstoincreasedsparsitywhichtranslatesinconsistentperformance
improvementondiversedatasets
1
.
1I
NTRODUCTION
Sequentiallearning,alsoreferredtoascontinual,incremental,orlifelonglearning(LLL),studiesthe
problemoflearningasequenceoftasks,oneatatime,withoutaccesstothetrainingdataofprevious
orfuturetasks.Whenlearninganewtask,akeychallengeinthiscontextishowtoavoidcatastrophic
interferencewiththetaskslearnedpreviously(French,1999;Li&Hoiem,2016).Somemethods
exploitanadditionalepisodicmemorytostoreasmallamountofprevioustasksdatatoregularize
futuretasklearning(e.g.Lopez-Pazetal.(2017)).Othersstoreprevioustasksmodelsandattesttime,
selectonemodelormergethemodels(Rusuetal.,2016;Aljundietal.,2016;Leeetal.,2017).In
contrast,inthisworkweareinterestedinthechallengingsituationoflearningasequenceoftasks
withoutaccesstoanypreviousorfuturetaskdata
and
restrictedtoamodelcapacity
,asalso
studiedinKirkpatricketal.(2016);Aljundietal.(2017);Fernandoetal.(2017);Mallya&Lazebnik
(2017);Serràetal.(2018).Thisscenarionotonlyhasmanypracticalincludingprivacyand
scalability,butalsoresemblesmorecloselyhowthemammalianbrainlearnstasksovertime.
Themammalianbrainiscomposedofbillionsofneurons.Yetatanygiventime,informationis
representedbyonlyafewactiveneuronsresultinginasparsityof90-95%(Lennie,2003).In
neuralbiology,
lateralinhibition
describestheprocesswhereanactivatedneuronreducestheactivity
ofitsweakerneighbors.Thiscreatesapowerfuldecorrelatedandcompactrepresentationwith
minimuminterferencebetweendifferentinputpatternsinthebrain(Yuetal.,2014).Thisisinstark
1
Thecodeisavailableat
https://github.com/rahafaljundi/Selfless-Sequential-Learning
1
arXiv:1806.05421v5  [stat.ML]  12 Apr 2019PublishedasaconferencepaperatICLR2019
Figure1:
Thedifferencebetweenparametersparsity(a)andrepresentationsparsity(b)inasimpletwotasks
case.Firstlayerindicatesinputpatterns.Learningthetaskutilizespartsindicatedinred.Task2has
differentinputpatternsandusespartsshowningreen.Orangeindicateschangedneuronsactivationsasaresult
ofthesecondtask.In(a),whenanexamplefromthetaskisencounteredagain,theactivationsofthe
layerwillnotbeaffectedbythechanges,however,thesecondandlaterlayeractivationsarechanged.Such
interferenceislargelyreducedwhenimposingsparsityontherepresentation(b).
contrastwithneuralnetworks,whichtypicallylearndenserepresentationsthatarehighly
entangled(Bengioetal.,2009).Suchanentangledrepresentationisquitesensitivetochangesin
theinputpatterns,inthatitrespondsdifferentlytoinputpatternswithonlysmallvariations.French
(1999)suggeststhatanoverlappedrepresentationplaysacrucialroleincatastrophicforgettingand
reducingthisoverlapwouldresultinareducedinterference.Cogswelletal.(2015)showthatwhen
theamountofovinaneuralnetworkisreduced,therepresentationcorrelationisalsoreduced.
Assuch,learningadisentangledrepresentationismorepowerfulandlessvulnerabletocatastrophic
interference.However,ifthelearneddisentangledrepresentationatagiventaskisnotsparse,only
littlecapacityisleftforlearningnewtasks.Thiswouldinturnresultineitherantothe
newtasksoragainaforgettingofprevioustasks.Incontrast,asparseanddecorrelatedrepresentation
wouldleadtoapowerfulrepresentationandatthesametimeenoughfreeneuronsthatcanbechanged
withoutinterferencewiththeneuralactivationslearnedfortheprevioustasks.
Ingeneral,sparsityinneuralnetworkscanbethoughtofeitherintermsofthenetworkparameters
orintermsoftherepresentation(i.e.,theactivations).Inthispaperwepostulate,and
experimentally,thatasparseanddecorrelatedrepresentationispreferableoverparametersparsity
inasequentiallearningscenario.Therearetwoargumentsforthis:asparserepresentationis
lesssensitivetonewanddifferentpatterns(suchasdatafromnewtasks)andsecond,thetraining
procedureofthenewtaskscanusethefreeneuronsleadingtolessinterferencewiththeprevious
tasks,hencereducingforgetting.Incontrast,whentheeffectiveparametersarespreadamong
differentneurons,changingtheineffectiveoneswouldchangethefunctionoftheircorresponding
neuronsandhenceinterferewithprevioustasks(seealsoFigure1).Basedontheseobservations,
weproposeanewregularizerthatexhibitsabehaviorsimilartothelateralinhibitioninbiological
neurons.Themainideaofourregularizeristopenalizeneuronsthatareactiveatthesametime.
Thisleadstomoresparsityandadecorrelatedrepresentation.However,complextasksmayactually
requiremultipleactiveneuronsinalayeratthesametimetolearnastrongrepresentation.Therefore,
ourregularizer,
SparsecodingthroughLocalNeuralInhibitionandDiscounting(
SLNID
)
,only
penalizesneuronslocally.Furthermore,wedon'twantinhibitiontoaffectpreviouslylearnedtasks,
eveniflatertasksuseneuronsfromearliertasks.Animportantcomponentof
SLNID
isthus
todiscountinhibitionfrom/toneuronswhichhavehigh
neuronimportance
Œanewconceptthat
weintroduceinanalogytoparameterimportance(Kirkpatricketal.,2016;Zenkeetal.,2017;
Aljundietal.,2017).Whencombinedwithastate-of-the-artimportantparameterspreservation
method(Aljundietal.,2017;Kirkpatricketal.,2016),ourproposedregularizerleadstosparseand
decorrelatedrepresentationswhichimprovesthelifelonglearningperformance.
Ourcontributionisthreefold.First,wedirectattentionto
SequentialLearning
andstudy
adiversesetofrepresentationbasedregularizers,parameterbasedregularizers,aswellassparsity
inducingactivationfunctionstothisend.Thesehavenotbeenstudiedextensivelyinthelifelong
learningliteraturebefore.Second,weproposeanovelregularizer,
SLNID
,whichisinspiredby
lateralinhibitioninthebrain.Third,weshowthatourproposedregularizerconsistentlyoutperforms
alternativesonthreediversedatasets(PermutedMNIST,CIFAR,TinyImagenet)andwecompareto
andoutperformstate-of-the-artLLLapproachesonan8-taskobjectchallenge.
SLNID
canbeappliedtodifferentregularizationbasedLLLapproaches,andweshowexperimentswith
MAS(Aljundietal.,2017)andEWC(Kirkpatricketal.,2016).
Inthefollowing,wediscussrelatedLLLapproachesanddifferentregularizationcriteriafroma
LLLperspective(Section2).WeproceedbyintroducingSequentialLearninganddetailing
ournovelregularizer(Section3).Section4describesourexperimentalevaluation,whileSection5
concludesthepaper.
2
PublishedasaconferencepaperatICLR2019
2R
ELATED
W
ORK
Thegoalinlifelonglearningistolearnasequenceoftaskswithoutcatastrophicforgettingofprevi-
ouslylearnedones(Thrun&Mitchell,1995).Onecanidentifydifferentapproachestointroducing
lifelonglearninginneuralnetworks.Here,wefocusonlearningasequenceoftasksusingaed
modelcapacity,i.e.withaedarchitectureandednumberofparameters.Underthissetting,
methodseitherfollowapseudorehearsalapproach,i.e.usingthenewtaskdatatoapproximatethe
performanceoftheprevioustask(Li&Hoiem,2016;Trikietal.,2017),oraimatidentifyingthe
importantparametersusedbythecurrentsetoftasksandpenalizingchangestothoseparametersby
newtasks(Kirkpatricketal.,2016;Zenkeetal.,2017;Aljundietal.,2017;Chaudhryetal.,2018;
Liuetal.,2018).Toidentifytheimportantparametersforagiventask,ElasticWeightConsolida-
tion(Kirkpatricketal.,2016)usesanapproximationoftheFisherinformationmatrixcomputedafter
trainingagiventask.Liuetal.(2018)suggestanetworkreparameterizationtoobtainabetterdiagonal
approximationoftheFisherInformationmatrixofthenetworkparameters.PathIntegral(Zenke
etal.,2017)estimatestheimportanceofthenetworkparameterswhilelearningagiventaskby
accumulatingthecontributionofeachparametertothechangeintheloss.Chaudhryetal.(2018)
suggestaKL-divergencebasedgeneralizationofElasticWeightConsolidationandPathIntegral.
MemoryAwareSynapses(Aljundietal.,2017)estimatestheimportanceoftheparametersinan
onlinemannerwithoutsupervisionbymeasuringthesensitivityofthelearnedfunctiontosmall
perturbationsontheparameters.Thismethodislesssensitivetothedatadistributionshift,anda
localversionproposedbytheauthorsresemblesapplyingHebbrule(Hebb,2002)toconsolidatethe
importantparameters,makingitmorebiologicallyplausible.
Acommondrawbackofalltheabovemethodsisthatlearningataskcouldutilizeagoodportionof
thenetworkcapacity,leavingfew""free""neuronstobeadaptedbythenewtask.Thisinturnleadsto
inferiorperformanceonthenewlylearnedtasksorforgettingthepreviouslylearnedones,aswewill
showintheexperiments.Hence,westudytheroleofsparsityandrepresentationdecorrelationin
sequentiallearning.Thisaspecthasnotreceivedmuchattentionintheliteratureyet.Veryrecently,
(Serràetal.,2018)proposedtoovercomecatastrophicforgettingthroughlearnedhardattention
masksforeachtaskwithL1regularizationimposedontheaccumulatedhardattentionmasks.This
comesclosertoourapproachalthoughwestudyandproposearegularizationschemeonthelearned
representation.
Theconceptofreducingtherepresentationoverlaphasbeensuggestedbeforeinearlyattempts
towardsovercomingcatastrophicforgettinginneuralnetworks(French,1999).Thishasledtoseveral
methodswiththegoaloforthogonalizingtheactivations(French,1992;1994;Kruschke,1992;
1993;Sloman&Rumelhart,1992).However,theseapproachesaremainlydesignedfor
architecturesandactivationfunctions,whichmakesithardtointegratetheminrecentneuralnetwork
structures.
Theofneuralnetworkshasmostlybeenstudiedforcompression.SVDdecomposition
canbeappliedtoreducethenumberofeffectiveparameters(Xueetal.,2013).However,thereis
noguaranteethatthetrainingprocedureconvergestoalowrankweightmatrix.Otherworksiterate
betweenpruningandretrainingofaneuralnetworkasapostprocessingstep(Liuetal.,2015;Sun
etal.,2016;Aghasietal.,2017;Louizosetal.,2017).Whilecompressinganeuralnetworkby
removingparametersleadstoasparserneuralnetwork,thisdoesnotnecessarilyleadtoasparser
representation.Indeed,aweightvectorcanbehighlysparsebutspreadamongthedifferentneurons.
Thisreducestheeffectivesizeofaneuralnetwork,fromacompressionpointofview,butitwould
notbeforlatertasksasmostoftheneuronsarealreadyoccupiedbythecurrentsetoftasks.
Inourexperiments,weshowthedifferencebetweenusingasparsepenaltyontherepresentation
versusapplyingittotheweights.
3S
ELFLESS
S
EQUENTIAL
L
EARNING
Oneofthemainchallengesinsinglemodelsequentiallearningistohavecapacitytolearnnew
tasksandatthesametimeavoidcatastrophicforgettingofprevioustasksasaresultoflearningnew
tasks.Inordertopreventcatastrophicforgetting,importanceweightbasedmethodssuchasEWC
(Kirkpatricketal.,2016)orMAS(Aljundietal.,2017)introduceanimportanceweight

k
foreach
parameter

k
inthenetwork.Whilethesemethodsdifferinhowtoestimatetheimportantparameters,
3
PublishedasaconferencepaperatICLR2019
allofthempenalizechangestoimportantparameterswhenlearninganewtask
T
n
using
L
2
penalty:
T
n
:min

1
M
M
X
m
=1
L
(
y
m
;f
(
x
m
;
n
))+


X
k

k
(

n
k


n

1
k
)
2
(1)
where

n

1
=
f

n

1
k
g
aretheoptimalparameterslearnedsofar,i.e.beforethecurrenttask.
f
x
m
g
isthesetof
M
traininginputs,with
f
f
(
x
m
;
n
)
g
and
f
y
m
g
thecorrespondingpredictedanddesired
outputs,respectively.


isatrade-offparameterbetweenthenewtaskobjective
L
andthechanges
ontheimportantparameters,i.e.theamountofforgetting.
Inthisworkweintroduceanadditionalregularizer
R
SSL
whichencouragessparsityintheactivations
H
l
=
f
h
m
i
g
foreachlayer
l
.
T
n
:min

1
M
M
X
m
=1
L
(
y
m
;f
(
x
m
;
n
))+


X
k

k
(

n
k


n

1
k
)
2
+

SSL
X
l
R
SSL
(
H
l
)
(2)

SSL
and


aretrade-offparametersthatcontrolthecontributionofeachterm.Whentrainingthe
task(
n
=1
),

k
=0
.
3.1S
PARSECODINGTHROUGH
N
EURAL
I
NHIBITION
(
SNI
)
Nowwedescribehowweobtainasparseanddecorrelatedrepresentation.Intheliteraturesparsityhas
beenproposedbyGlorotetal.(2011)tobecombinedwiththeactivationfunction(ReLU)to
controlunboundedactivationsandtoincreasesparsity.Theyminimizethe
L
1
normoftheactivations
(sinceminimizingthe
L
0
normisanNPhardproblem).However,
L
1
normimposesanequalpenalty
onalltheactiveneuronsleadingtosmallactivationmagnitudeacrossthenetwork.
Learningadecorrelatedrepresentationhasbeenexploredbeforewiththegoalofreducingov
ThisisusuallydonebyminimizingtheFrobeniusnormofthecovariancematrixcorrectedbythe
diagonal,asinCogswelletal.(2015)orXiongetal.(2016).Suchapenaltyresultsinadecorrelated
representationbutwithactivationsthataremostlyclosetoanonzeromeanvalue.Wemergethetwo
objectivesofsparseanddecorrelatedrepresentationresultinginthefollowingobjective:
R
SNI
(
H
l
)=
1
M
X
i;j
X
m
h
m
i
h
m
j
;i
6
=
j
(3)
whereweconsiderahiddenlayer
l
withactivations
H
l
=
f
h
m
i
g
forasetofinputs
X
=
f
x
m
g
and
i;j
2
1
;::;N
runningoverall
N
neuronsinthehiddenlayer.Thisformuladiffersfromminimizing
theFrobeniusnormofthecovariancematrixintwosimpleyetimportantaspects:
(1)InthecaseofaReLUactivationfunction,usedinmostmodernarchitectures,aneuronisactiveif
itsoutputislargerthanzero,andzerootherwise.Byassumingaclosetozeromeanoftheactivations,

i
'
0
8
i
2
1
;::;N
,weminimizethecorrelationbetweenanytwoactiveneurons.
(2)Byevaluatingthederivativeofthepresentedregularizerw.r.t.theactivation,weget:
@R
SNI
(
H
l
)
@h
m
i
=
1
M
X
j
6
=
i
h
m
j
(4)
i.e.,eachactiveneuronreceivesapenaltyfromeveryotheractiveneuronthatcorrespondstothat
otherneuron'sactivationmagnitude.Inotherwords,ifaneuronwithahighactivationvalue,
foragivenexample,itwillsuppressofotherneuronsforthatsameexample.Hence,thisresults
inadecorrelatedsparserepresentation.
3.2S
PARSECODINGTHROUGH
L
OCAL
N
EURAL
I
NHIBITION
(
SLNI
)
Thelossimposedbythe
SNI
objectivewillonlybezerowhenthereisatmostoneactiveneuron
perexample.Thisseemstobetooharshforcomplextasksthatneedaricherrepresentation.Thus,
wesuggesttorelaxtheobjectivebyimposingaspatialweightingtothecorrelationpenalty.Inother
words,anactiveneuronpenalizesmostlyitscloseneighboursandthiseffectvanishesforneurons
furtheraway.Insteadofuniformlypenalizingallthecorrelatedneurons,weweightthecorrelation
penaltybetweentwoneuronswithlocations
i
and
j
usingaGaussianweighting.Thisgives
R
SLNI
(
H
l
)=
1
M
X
i;j
e

(
i

j
)
2
2
˙
2
X
m
h
m
i
h
m
j
;i
6
=
j
(5)
4
PublishedasaconferencepaperatICLR2019
Assuch,eachactiveneuroninhibitsitsneighbours,introducingalocalityinthenetworkinspired
bybiologicalneurons.Whilethenotionofneighbouringneuronsisnotwellestablishedinafully
connectednetwork,ouraimistoallowfewneuronstobeactiveandnotonlyone,thusthosefew
activationsdon'thavetobesmalltocompensateforthepenalty.
˙
2
isahyperparameterrepresenting
thescaleatwhichneuronscanaffecteachother.Notethatthisissomewhatmorexiblethan
decorrelatingneuronsinedgroupsasusedinXiongetal.(2016).Ourregularizerinhibitslocally
theactiveneuronsleadingtoasparsecodingthroughlocalneuralinhibition.
3.3N
EURONIMPORTANCEFORDISCOUNTINGINHIBITION
Ourregularizeristobeappliedforeachtaskinthelearningsequence.Inthecaseoftaskswith
completelydifferentinputpatterns,theactiveneuronsoftheprevioustaskswillnotbeactivatedgiven
thenewtasksinputpatterns.However,whenthenewtasksareofsimilarorsharedpatterns,neurons
usedforprevioustaskswillbeactive.Inthatcase,ourpenaltywoulddiscourageotherneuronsfrom
beingactiveandencouragethenewtasktoadaptthealreadyactiveneuronsinstead.Thiswould
interferewiththeprevioustasksandcouldincreaseforgettingwhichisexactlywhatwewantto
overcome.Toavoidsuchinterference,weaddaweightfactortakingintoaccounttheimportanceof
theneuronswithrespecttotheprevioustasks.Toestimatetheimportanceoftheneurons,weuseasa
measurethesensitivityofthelossattheendofthetrainingtotheirchanges.Thisisapproximatedby
thegradientsofthelossw.r.t.theneuronsoutputs(beforetheactivationfunction)evaluatedateach
datapoint.Togetanimportancevalue,wethenaccumulatetheabsolutevalueofthegradientsover
thegivendatapointsobtainingimportanceweight

i
forneuron
n
i
:

i
=
1
M
M
X
m
=1
j
g
i
(
x
m
)
j
;g
i
(
x
m
)=
@
(
L
(
y
m
;f
(
x
m
;
n
)))
@n
m
i
(6)
where
n
m
i
istheoutputofneuron
n
i
foragiveninputexample
x
m
,and

n
aretheparametersafter
learningtask
n
.ThisisinlinewiththeestimationoftheparametersimportanceinKirkpatricketal.
(2016)butconsideringthederivationvariablestobetheneuronsoutputsinsteadoftheparameters.
Insteadofrelyingonthegradientoftheloss,wecanalsousethegradientofthelearnedfunction,i.e.
theoutputlayer,asdoneinAljundietal.(2017)forestimatingtheparametersimportance.Duringthe
earlyphasesofthiswork,weexperimentedwithbothandobservedasimilarbehaviour.Forsakeof
consistencyandcomputationalefyweutilizethegradientofthefunctionwhenusingAljundi
etal.(2017)asLLLmethodandthegradientofthelosswhenexperimentingwithEWC(Kirkpatrick
etal.,2016).Then,wecanweightourregularizerasfollows:
R
SLNID
(
H
l
)=
1
M
X
i;j
e

(

i
+

j
)
e

(
i

j
)
2
2
˙
2
X
m
h
m
i
h
m
j
;i
6
=
j
(7)
whichcanbereadas:ifanimportantneuronforaprevioustaskisactivegivenaninputpatternfrom
thecurrenttask,itwillnotsuppresstheotherneuronsfrombeingactiveneitherbeaffectedbyother
activeneurons.Forallotheractiveneurons,localinhibitionisdeployed.Theobjectivefor
trainingisgiveninEq.2,setting
R
SSL
:=
R
SLNID
and

SSL
:=

SLNID
.Werefertoourfullmethod
asSparsecodingthroughLocalNeuralInhibitionandDiscounting(
SLNID
).
4E
XPERIMENTS
Inthissectionwestudytheroleofstandardregularizationtechniqueswithafocusonsparsityand
decorrelationoftherepresentationinasequentiallearningscenario.Wecomparedifferent
activationfunctionsandregularizationtechniques,includingourproposed
SLNID
,onpermuted
MNIST(Sec.4.1).Then,wecomparethetopcompetingtechniquesandourproposedmethodin
thecaseofsequentiallylearningCIFAR-100classesandTinyImagenetclasses(Sec.4.2).Our
SLNID
regularizercanbeintegratedinanyimportanceweight-basedlifelonglearningapproachsuch
as(Kirkpatricketal.,2016;Zenkeetal.,2017;Aljundietal.,2017).HerewefocusonMemory
AwareSynapses(Aljundietal.,2017)(
MAS
),whichiseasytointegrateandexperimentwithand
hasshownsuperiorperformance(Aljundietal.,2017).However,wealsoshowresultswithElastic
weightconsolidation(Kirkpatricketal.,2016)(
EWC
)inSec.4.3.Further,weablatethecomponents
ofourregularizer,bothinthestandardsetting(Sec.4.4)asinasettingwithouthardtaskboundaries
(Sec.4.5).Finally,weshowhowourregularizerimprovesthestate-of-the-artperformanceona
sequenceofobjectrecognitiontasks(Sec.4.6).
5
PublishedasaconferencepaperatICLR2019
Figure2:
Comparisonofdifferentregularizationtechniqueson5permutedMNISTsequence.Representation
basedregularizersaresolidbars,barswithlinesrepresentparametersregularizers,dottedbarsrepresentactivation
functions.Averagetestaccuracyoveralltasksisgiveninthelegend.Representationbasedregularizersachieve
higherperformancethanothercomparedmethodsincludingparametersbasedregularizers.Ourregularizer,
SLNID
,performsthebestonthelasttwotasksindicatingthatmorecapacityislefttolearnthesetasks.
4.1A
NIN
-
DEPTHCOMPARISONOFREGULARIZERSANDACTIVATIONFUNCTIONSFOR
S
ELFLESS
S
EQUENTIAL
L
EARNING
Westudypossibleregularizationtechniquesthatcouldleadtolessinterferencebetweenthedifferent
tasksinasequentiallearningscenarioeitherbyenforcingsparsityordecorrelation.Additionally,we
examinetheuseofactivationfunctionsthatareinspiredbylateralinhibitioninbiologicalneurons
thatcouldbeadvantageousinsequentiallearning.MASAljundietal.(2017)isusedinallcasesas
LLLmethod.
RepresentationBasedmethods:
-
L1-Rep
:Topromoterepresentationalsparsity,an
L
1
penaltyontheactivationsisused.
-
Decov
(Cogswelletal.,2015)aimsatreducingovbydecorrelatingneuronactivations.To
doso,itminimizestheFrobeniusnormofthecovariancematrixcomputedontheactivationsofthe
currentbatchaftersubtractingthediagonaltoavoidpenalizingindependentneuronactivations.
Activationfunctions:
-
Maxout
network(Goodfellowetal.,2013b)utilizesthemaxoutactivationfunction.Foreachgroup
ofneurons,basedonaedwindowsize,onlythemaximumactivationisforwardedtothenextlayer.
Theactivationfunctionguaranteesaminimumsparsityratebythewindowsize.
-
LWTA
(Srivastavaetal.,2013):similarideatotheMaxoutnetworkexceptthatthenon-maximum
activationsaresettozerowhilemaintainingtheirconnections.IncontrasttoMaxout,LWTAkeeps
theconnectionsoftheinactiveneuronswhichcanbeoccupiedlateroncetheyareactivatedwithout
changingthepreviouslyactiveneuronconnections.
-
ReLU
(Glorotetal.,2011)Theactivationfunction(ReLU)usedasabaselinehereand
indicatedinlaterexperimentsas
No-Reg
asitrepresentsthestandardsettingofsequentiallearning
onnetworkswithReLU.AllthestudiedregularizersuseReLUasactivationfunction.
Parametersbasedregularizers:
-
OrthReg
(Rodríguezetal.,2016):RegularizingCNNswithlocallyconstraineddecorrelations.It
aimsatdecorrelatingthefeaturedetectorsbyminimizingthecosineoftheanglebetweentheweight
vectorsresultingeventuallyinorthogonalweightvectors.
-
L2-WD
:Weightdecaywith
L
2
norm(Krogh&Hertz,1992)controlsthecomplexityofthelearned
functionbyminimizingthemagnitudeoftheweights.
-
L1-Param
:
L
1
penaltyontheparameterstoencourageasolutionwithsparseparameters.
Dropoutisnotconsideredasitsrolecontradictsourgoal.Whiledropoutcanimproveeachtask
performanceandreduceovitactsasamodelaveragingtechnique.Byrandomlymasking
neurons,dropoutforcesthedifferentneuronstoworkindependently.Assuchitencouragesa
redundantrepresentation.Asshownby(Goodfellowetal.,2013a)thebestnetworksizeforclassifying
MNISTdigitswhenusingdropoutwasabout
50%
morethanwithoutit.Dropoutsteersthelearning
ofatasktowardsoccupyingagoodportionofthenetworkcapacity,ifnotallofit,whichcontradicts
thesequentiallearningneeds.
Experimentalsetup.
WeusetheMNISTdataset(LeCunetal.,1998)asataskinasequenceof
5tasks,wherewerandomlypermutealltheinputpixelsdifferentlyfortasks2to5.Thegoalisto
classifyMNISTdigitsfromallthedifferentpermutations.Thecompleterandompermutationofthe
pixelsineachtaskrequirestheneuralnetworktoinstantiateanewneuralrepresentationforeach
pattern.AsimilarsetuphasbeenusedbyKirkpatricketal.(2016);Zenkeetal.(2017);Goodfellow
etal.(2013a)withdifferentpercentageofpermutationsordifferentnumberoftasks.
Asabasenetwork,weemployamultilayerperceptronwithtwohiddenlayersandaSoftmaxloss.
6
PublishedasaconferencepaperatICLR2019
Figure3:
Comparisonofdifferentregularizationtechniquesonasequenceoftentasksfrom(a)Cifarsplitand
(b)TinyImageNetsplit.Thelegendshowsaveragetestaccuracyoveralltasks.SimpleL1-normregularizer
(L1-Rep)doesn'thelpinsuchmorecomplextasks.Ourregularizer
SLNID
achievesanimprovementof
2%
overDecovand
4

8%
comparedtoNo-Reg.
Weexperimentwithdifferentnumberofneuronsinthehiddenlayers
f
128
;
64
g
.For
SLNID
we
evaluatetheeffectof

SLNID
ontheperformanceandtheobtainedsparsityinFigure4.Ingeneral,the
best

SLNID
istheminimumvaluethatmaintainssimilarorbetteraccuracyonthetaskcompared
totheunregularizedcase,andwesuggesttousethisasarule-of-thumbtoset

SLNID
.For


,we
haveusedahigh


valuethatensurestheleastforgettingwhichallowsustotesttheeffectonthe
latertasksperformance.Notethatbetteraverageaccuraciescanbeobtainedwithtuned


.Please
refertoAppendixAforhyperparametersandotherdetails.
Results:
Figure2presentsthetestaccuracyoneachtaskattheendofthesequence,achievedbythe
differentregularizersandactivationfunctionsonthenetworkwithhiddenlayerofsize
128
.Results
onanetworkwithhiddenlayersize
64
areshownintheAppendixB.Clearly,inallthedifferent
tasks,therepresentationalregularizersshowasuperiorperformancetotheotherstudiedtechniques.
Fortheregularizersappliedtotheparameters,
L2-WD
and
L1-Param
donotexhibitacleartrend
anddonotsystematicallyshowanimprovementovertheuseofthedifferentactivationfunctionsonly.
While
OrthReg
showsaconsistentlygoodperformance,itislowerthanwhatcanbeachievedby
therepresentationalregularizers.Itisworthnotingthe
L1-Rep
yieldssuperiorperformanceover
L1-Param
.Thisobservationisconsistentacrossdifferentsizesofthehiddenlayers(inAppendixB)
andshowstheadvantageofencouragingsparsityintheactivationscomparedtothatintheparameters.
Regardingtheactivationfunctions,
Maxout
and
LWTA
achieveaslightlyhigherperformancethan
ReLU
.Wedidnotobserveadifferencebetweenthetwoactivationfunctions.However,
theimprovementover
ReLU
isonlymoderateanddoesnotjustifytheuseofaedwindowsize
andspecialarchitecturedesign.Ourproposedregularizer
SLNID
achieveshighifnotthehighest
performanceinallthetasksandsucceedsinhavingastableperformance.Thisindicatestheabilityof
SLNID
todirectthelearningprocesstowardsusingminimumamountofneuronsandhencemore
xibilityforupcomingtasks.
Representationsparsity&importantparametersparsity.
Figure4:
Onthe5permutedMNIST
sequence,hiddenlayer=128,Top:per-
centageofunusedparametersinthe1st
layerusingdifferent

SLNID
;Bottom:
histogramofneuralactivationsonthe
task.
Herewewanttoexaminetheeffectofourregularizeronthe
percentageofparametersthatareutilizedaftereachtaskand
hencethecapacityleftforthelatertasks.Onthenetworkwith
hiddenlayersize
128
,wecomputethepercentageofparameters
with

k
<
10

2
,with

k
,seeAppendixA,theimportance
weightmultiplierestimatedandaccumulatedovertasks.Those
parameterscanbeseenasunimportantand""free""forlater
tasks.Figure4(top)showsthepercentageoftheunimportant
(free)parametersinthelayeraftereachtaskfordifferent

SLNID
valuesalongwiththeachievedaveragetestaccuracy
attheendofthesequence.Itisclearthatthelarger

SLNID
,
i.e.,themoreneuralinhibition,thesmallerthepercentageof
importantparameters.Apartfromthehighest

SLNID
where
taskscouldn'treachtheirtopperformanceduetotoostrong
inhibition,improvementoverthe
No-Reg
isalwaysobserved.
Theoptimalvalueforlambdaseemstobetheonethatremains
closetotheoptimalperformanceonthecurrenttask,while
utilizingtheminimumcapacityfeasible.Next,wecompute
theaverageactivationperneuron,inthelayer,overall
theexamplesandplotthecorrespondinghistogramfor
SLNID
,
DeCov
,
L1-Rep
,
L1-Param
and
No-Reg
inFigure4(bot-
tom)attheirsettingthatyieldedtheresultsshowninFigure2.
SLNID
hasapeakatzeroindicating
representationsparsitywhiletheothermethodsvaluesarespreadalongtheline.Thisseemstohintat
7
PublishedasaconferencepaperatICLR2019
theeffectivenessofourapproach
SLNID
inlearningasparseyetpowerfulrepresentationandinturn
inaminimalinterferencebetweentasks.
Permutedmnist
Cifar
h-layerdim.
128
64
256
128
No-Reg
92.67
90.72
55.06
55.3
SNI
95.79
94.89
55.30
55.75
SNID
95.90
93.82
61.00
60.90
SLNI
95.95
94.87
56.06
55.79
SLNID
95.83
93.89
63.30
61.16
Multi-TaskJointTraining*
97.30
96.80
70.99
71.95
Table1:
SLNID
ablation.Averagetestaccuracyper
taskaftertrainingthelasttaskin%.*denotesthatMulti-
TaskJointTrainingviolatestheLLLscenarioasithas
accesstoalltasksatonceandthuscanbeseenasan
upperbound.
Method
Avg-acc
Finetune
32.67
LWF(Li&Hoiem,2016)
49.49
EBLL(Trikietal.,2017)
50.29
IMM(Leeetal.,2017)
43.4
PathIntegral(Zenkeetal.,2017)
50.49
EWC(Kirkpatricketal.,2016)
50.00
MAS(Aljundietal.,2017)
52.69
SLNID
-fcPretrained(ours)
53.77
SLNID
-fcrandomlyinitialized(ours)
54.50
Table2:8tasksobjectrecognitionse-
quence.Averagetestaccuracypertask
aftertrainingthelasttaskin%.
4.210T
ASKSEQUENCESON
CIFAR-100
AND
T
INY
I
MAGENET
Whiletheprevioussectionfocusedonlearningasequenceoftaskswithcompletelydifferentinput
patternsandsameobjective,wenowstudythecaseoflearningdifferentcategoriesofonedataset.
ForthiswesplittheCIFAR-100andtheTinyImageNet(Yao&Miller,2015)datasetintotentasks,
respectively.Wehave10and20categoriespertaskforCIFAR-100andTinyImagNet,respectively.
FurtherdetailsabouttheexperimentalsetupcanbefoundinappendixA.
Wecomparethetopcompetingmethodsfromthepreviousexperiments,
L1-Rep
,
DeCov
andour
SLNID
,and
No-Reg
asabaseline,
ReLU
inpreviousexperiment.Similarly,MASAljundietal.
(2017)isusedinallcasesasLLLmethod.Figures3(a)and3(b)showtheperformanceoneachofthe
tentasksattheendofthesequence.Forbothdatasets,weobservethatour
SLNID
performsoverall
best.
L1-Rep
and
DeCov
continuetoimproveoverthenonregularizedcase
No-Reg
.Theseresults
ourproposalontheimportanceofsparsityanddecorrelationinsequentiallearning.
4.3
SLNID
WITH
EWC(K
IRKPATRICKETAL
.,2016)
Wehaveshownthatourproposedregularizer
SLNID
exhibitsstableandsuperiorperformanceon
thedifferenttestednetworkswhenusing
MAS
asimportanceweightpreservationmethod.Toprove
theeffectivenessofourregularizerregardlessoftheusedimportanceweightbasedmethod,we
havetested
SLNID
onthe5taskspermutedMNISTsequenceincombinationwithElasticWeight
Consolidation(
EWC
,Kirkpatricketal.(2016))andobtainedaboostintheaverageperformanceat
theendofthelearnedsequenceequalto
3
:
1%
onthenetworkwithhiddenlayersize128anda
boostof
2
:
8%
withhiddenlayersize64.DetailedaccuraciesareshowninAppendixB.Itisworth
notingthatwithboth
MAS
and
EWC
our
SLNID
wasableobtainbetteraccuracyusinganetworkwith
a
64
-dimensionalhiddensizethanwhentrainingwithoutregularization
No-Reg
onanetworkof
doublethatsize(
128
),indicatingthat
SLNID
allowstouseneuronsmuchmoreef.
4.4A
BLATION
S
TUDY
Ourmethodcanbeseenascomposedofthreecomponents:theneuralinhibition,thelocalityrelax-
ationandtheneuronimportanceintegration.Tostudyhowthesecomponentsperformindividually,
Table1reportstheaverageaccuracyattheendoftheCifar100andpermutedMNISTsequences
foreachvariant,namely,
SNID
withoutneuronimportance(
SNI
),
SNID
,
SLNID
withoutneuron
importance(
SLNI
)inadditiontoourfull
SLNID
regularizer.AsweexplainedinSection3,when
taskshavecompletelydifferentinputpatterns,theneuronsthatwereactivatedontheprevioustask
exampleswillnotfornewtasksamplesandexclusionofimportantneuronsisnotmandatory.
However,whensharingispresentbetweenthedifferenttasks,atermtoprevent
SLNID
fromcausing
anyinterferenceisrequired.Thisismanifestedinthereportedresults:forpermutedMNIST,all
thevariantsworknicelyalone,asaresultofthesimplicityandthedisjointnatureofthissequence.
However,intheCifar100sequence,theintegrationoftheneuronimportanceinthe
SNID
and
SLNID
8
PublishedasaconferencepaperatICLR2019
regularizersexcludeimportantneuronsfromtheinhibition,resultinginaclearlybetterperformance.
Thelocalityin
SLNID
improvestheperformanceintheCifarsequence,whichsuggeststhataricher
representationisneededandmultipleactiveneuronsshouldbetolerated.
4.5S
EQUENTIAL
L
EARNINGWITHOUTHARDTASKBOUNDARIES
Inthepreviousexperiments,weconsideredthestandardtaskbasedscenarioasin(Li&Hoiem,2016;
Zenkeetal.,2017;Aljundietal.,2017;Serràetal.,2018),whereateachtimestepwereceiveatask
alongwithitstrainingdataandanewlayerisinitiatedforthenewtask,ifneeded.Here,
weareinterestedinamorerealisticscenariowherethedatadistributionshiftsgraduallywithouthard
taskboundaries.
Method
Avg.acc-tasksmodels
No-Regw/oMAS
69.20%
SLNIw/oMAS
72.14%
SLNIDw/oMAS
73.03%
No-Reg
66.88%
SLNI
71.32%
SLNID
72.33%
Method
Avg.acc-lastmodel
No-Regw/oMAS
65.15%
SLNIw/oMAS
63.54%
SLNIDw/oMAS
70.75%
No-Reg
66.33%
SLNI
64.50%
SLNID
70.94%
Table3:
Notasksboundariestestcase
onCifar100.Topblock,avg.accon
eachgroupofclassesusingeachgroup
model.Bottomblock,avg.acc.oneach
groupattheendofthetraining.
Totestthissetting,weusetheCifar100dataset.Instead
ofconsideringasetof10disjointtaskseachcomposedof
10classes,asinthepreviousexperiment(Sec.4.2),wenow
startbysamplingwithhighprobability
(2
=
3)
fromthe
10classesandwithlowprobability
(1
=
3)
fromtherestofthe
classes.Wetrainthenetwork(samearchitectureasinSec.4.2)
forafewepochsandthenchangethesamplingprobabilities
tobehigh
(2
=
3)
forclasses
11

20
andlow
(1
=
3)
forthe
remainingclasses.Thisprocessisrepeateduntilsampling
withhighprobabilityfromthelast10classesandlowfromthe
rest.Weuseonesharedlayerthroughoutandesti-
matetheimportanceweightsandtheneuronsimportanceafter
eachtrainingstep(beforechangingthesamplingprobabilities).
Weconsider6variants:our
SLNID
,theablations
SLNI
and
withoutregularizer
No-Reg
,asinSection4.4,aswelleach
ofthesethreetrainedwithouttheMASimportanceweight
regularizerofAljundietal.(2017),denotedas
w/oMAS
.Ta-
ble3presentstheaccuracyaveragedoverthetengroupsoften
classes,usingeachgroupmodel(i.e.themodeltrainedwhenthisgroupwassampledwithhighprob-
ability)inthetopblockandtheaverageaccuracyoneachofthetengroupsattheendofthetraining
(middleandbottomblock).Wecandeducethefollowing:1)
SLNID
improvestheperformance
considerably(bymorethan4%)evenwithoutimportanceweightregularizer.2)Inthisscenario
withouthardtaskboundariesthereislessforgettingthaninthescenariowithhardtaskboundaries
studiedinSection4.2forCifar(differencebetweenrowsintopblocktocorrespondingrowsinmiddle
block).Asaresult,theimprovementobtainedbydeployingtheimportanceweightregularizeris
moderate:at
70
:
75%
,
SLNIDw/oMAS
isalreadybetterthan
No-Reg
reaching
66
:
33%
.3)While
SLNI
withoutMASimprovestheindividualmodelsperformance(
72
:
14%
comparedto
69
:
20%
),it
failstoimprovetheoverallperformanceattheendofthesequence(
63
:
54%
comparedto
65
:
15%
),as
importantneuronsarenotexcludedfromthepenaltyandhencetheyarechangedorinhibitedleading
totasksinterferenceandperformancedeterioration.
4.6C
OMPARISONWITHTHESTATEOFTHEART
Tocompareourproposedapproachwiththedifferentstate-of-the-artsequentiallearningmethods,
weuseasequenceof8differentobjectrecognitiontasks,introducedinAljundietal.(2017).The
sequencestartsfromAlexNet(Krizhevskyetal.,2012)pretrainedonImageNet(Russakovsky
etal.,2015)asabasenetwork,followingthesettingofAljundietal.(2017).Moredetailsarein
AppendixA.4.Wecompareagainstthefollowing:
LearningwithoutForgetting
(Li&Hoiem,2016)
(
LwF
),
IncrementalMomentMatching
(Leeetal.,2017)(
IMM
),
PathIntegral
(Zenkeetal.,2017)and
sequential(
FineTuning
),inadditiontothecaseof
MAS
(Aljundietal.,2017)alone,i.e.
our
No-Reg
before.ComparedmethodswererunwiththeexactsamesetupasinAljundietal.
(2017).Forourregularizer,wedisabledropout,sincedropoutencouragesredundantactivations
whichcontradictsourregularizer'srole.Also,sincethenetworkispretrained,thelocalityintroduced
in
SLNID
maywiththealreadypretrainedactivations.Forthisreason,wealsotest
SLNID
withrandomlyinitializedfullyconnectedlayers.Ourregularizerisappliedwith
MAS
asasequential
learningmethod.Table2reportstheaveragetestaccuracyattheendofthesequenceachievedby
eachmethod.
SLNID
improvesevenwhenstartingfromapretrainednetworkanddisablingdropout.
9
PublishedasaconferencepaperatICLR2019
Surprisingly,evenwithrandomlyinitializedfullyconnectedlayers,
SLNID
improves
1
:
8%
overthe
stateoftheartusingafullypretrainednetwork.
5C
ONCLUSION
InthispaperwestudytheproblemofsequentiallearningusinganetworkwithedcapacityŒa
prerequisiteforascalableandcomputationallyefsolution.Akeyinsightofourapproachis
thatinthecontextofsequentiallearning(asopposedtoothercontextswheresparsityisimposed,
suchasnetworkcompressionoravoidingovsparsityshouldbeimposedatthelevelofthe
representationratherthanatthelevelofthenetworkparameters.Inspiredbylateralinhibitionin
themammalianbrain,weimposesparsitybymeansofanewregularizerthatdecorrelatesnearby
activeneurons.Weintegratethisinamodelwhich
learns
anewtaskbyleavingcapacity
forfuturetasksandatthesametime
avoidsforgetting
previoustasksbytakingintoaccountneurons
importance.
Acknowledgment:
Theauthor'sPhDisfundedbyanFWOscholarship.
R
EFERENCES
AlirezaAghasi,AfshinAbdi,NamNguyen,andJustinRomberg.Net-trim:Convexpruningofdeepneural
networkswithperformanceguarantee.In
AdvancesinNeuralInformationProcessingSystems
,pp.3180Œ3189,
2017.
RahafAljundi,PunarjayChakravarty,andTinneTuytelaars.Expertgate:Lifelonglearningwithanetworkof
experts.In
IEEEConferenceonComputerVisionandPatternRecognition(CVPR)
,2016.
RahafAljundi,FrancescaBabiloni,MohamedElhoseiny,MarcusRohrbach,andTinneTuytelaars.Memory
awaresynapses:Learningwhat(not)toforget.
arXivpreprintarXiv:1711.09601
,2017.
YoshuaBengioetal.Learningdeeparchitecturesforai.
Foundationsandtrends
R

inMachineLearning
,2(1):
1Œ127,2009.
ArslanChaudhry,PuneetKDokania,ThalaiyasingamAjanthan,andPhilipHSTorr.Riemannianwalkfor
incrementallearning:Understandingforgettingandintransigence.
arXivpreprintarXiv:1801.10112
,2018.
MichaelCogswell,FarukAhmed,RossGirshick,LarryZitnick,andDhruvBatra.Reducingovindeep
networksbydecorrelatingrepresentations.
arXivpreprintarXiv:1511.06068
,2015.
T.E.deCampos,B.R.Babu,andM.Varma.Characterrecognitioninnaturalimages.In
Proceedingsofthe
InternationalConferenceonComputerVisionTheoryandApplications,Lisbon,Portugal
,February2009.
M.Everingham,L.VanGool,C.K.I.Williams,J.Winn,andA.Zisserman.ThePAS-
CALVisualObjectClassesChallenge2012(VOC2012)Results.http://www.pascal-
network.org/challenges/VOC/voc2012/workshop/index.html.
ChrisanthaFernando,DylanBanarse,CharlesBlundell,YoriZwols,DavidHa,AndreiARusu,Alexander
Pritzel,andDaanWierstra.Pathnet:Evolutionchannelsgradientdescentinsuperneuralnetworks.
arXiv
preprintarXiv:1701.08734
,2017.
RobertMFrench.Semi-distributedrepresentationsandcatastrophicforgettinginconnectionistnetworks.
ConnectionScience
,4(3-4):365Œ377,1992.
RobertMFrench.Dynamicallyconstrainingconnectionistnetworkstoproducedistributed,orthogonalrepresen-
tationstoreducecatastrophicinterference.
network
,1111:00001,1994.
RobertMFrench.Catastrophicforgettinginconnectionistnetworks.
Trendsincognitivesciences
,3(4):128Œ135,
1999.
XavierGlorot,AntoineBordes,andYoshuaBengio.Deepsparseneuralnetworks.InGeoffreyGordon,
DavidDunson,andMiroslavDudík(eds.),
ProceedingsoftheFourteenthInternationalConferenceon
IntelligenceandStatistics
,volume15of
ProceedingsofMachineLearningResearch
,pp.315Œ323,
FortLauderdale,FL,USA,11Œ13Apr2011.PMLR.URL
http://proceedings.mlr.press/v15/
glorot11a.html
.
IanJGoodfellow,MehdiMirza,DaXiao,AaronCourville,andYoshuaBengio.Anempiricalinvestigationof
catastrophicforgettingingradient-basedneuralnetworks.
arXivpreprintarXiv:1312.6211
,2013a.
10
PublishedasaconferencepaperatICLR2019
IanJGoodfellow,DavidWarde-Farley,MehdiMirza,AaronCourville,andYoshuaBengio.Maxoutnetworks.
arXivpreprintarXiv:1302.4389
,2013b.
DOHebb.Theorganizationofbehavior.1949.
NewYorkWiely
,2002.
JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,GuillaumeDesjardins,AndreiARusu,
KieranMilan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-Barwinska,etal.Overcomingcatastrophic
forgettinginneuralnetworks.
arXivpreprintarXiv:1612.00796
,2016.
JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei.3dobjectrepresentationsforcategoriza-
tion.In
ProceedingsoftheIEEEInternationalConferenceonComputerVisionWorkshops
,pp.554Œ561,
2013.
AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.Imagenetwithdeepconvolutional
neuralnetworks.InF.Pereira,C.J.C.Burges,L.Bottou,andK.Q.Weinberger(eds.),
AdvancesinNeural
InformationProcessingSystems25
,pp.1097Œ1105.CurranAssociates,Inc.,2012.
AndersKroghandJohnAHertz.Asimpleweightdecaycanimprovegeneralization.In
Advancesinneural
informationprocessingsystems
,pp.950Œ957,1992.
JohnKKruschke.Alcove:anexemplar-basedconnectionistmodelofcategorylearning.
Psychologicalreview
,
99(1):22,1992.
JohnKKruschke.Humancategorylearning:Implicationsforbackpropagationmodels.
ConnectionScience
,5
(1):3Œ36,1993.
YannLeCun,LéonBottou,YoshuaBengio,andPatrickHaffner.Gradient-basedlearningappliedtodocument
recognition.
ProceedingsoftheIEEE
,86(11):2278Œ2324,1998.
Sang-WooLee,Jin-HwaKim,Jung-WooHa,andByoung-TakZhang.Overcomingcatastrophicforgettingby
incrementalmomentmatching.
arXivpreprintarXiv:1703.08475
,2017.
PeterLennie.Thecostofcorticalcomputation.
Currentbiology
,13(6):493Œ497,2003.
ZhizhongLiandDerekHoiem.Learningwithoutforgetting.In
EuropeanConferenceonComputerVision
,pp.
614Œ629.Springer,2016.
BaoyuanLiu,MinWang,HassanForoosh,MarshallTappen,andMariannaPensky.Sparseconvolutionalneural
networks.In
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
,pp.806Œ814,
2015.
XialeiLiu,MarcMasana,LuisHerranz,JoostVandeWeijer,AntonioMLopez,andAndrewDBagdanov.Rotate
yournetworks:Betterweightconsolidationandlesscatastrophicforgetting.
arXivpreprintarXiv:1802.02950
,
2018.
DavidLopez-Pazetal.Gradientepisodicmemoryforcontinuallearning.In
AdvancesinNeuralInformation
ProcessingSystems
,pp.6470Œ6479,2017.
ChristosLouizos,KarenUllrich,andMaxWelling.Bayesiancompressionfordeeplearning.In
Advancesin
NeuralInformationProcessingSystems
,pp.3290Œ3300,2017.
S.Maji,J.Kannala,E.Rahtu,M.Blaschko,andA.Vedaldi.Fine-grainedvisualofaircraft.
Technicalreport,2013.
ArunMallyaandSvetlanaLazebnik.Packnet:Addingmultipletaskstoasinglenetworkbyiterativepruning.
arXivpreprintarXiv:1711.05769
,1(2):3,2017.
YuvalNetzer,TaoWang,AdamCoates,AlessandroBissacco,BoWu,andAndrewYNg.Readingdigitsin
naturalimageswithunsupervisedfeaturelearning.2011.
M-E.NilsbackandA.Zisserman.Automatedweroveralargenumberofclasses.In
Proceedings
oftheIndianConferenceonComputerVision,GraphicsandImageProcessing
,Dec2008.
AriadnaQuattoniandAntonioTorralba.Recognizingindoorscenes.In
ComputerVisionandPatternRecognition,
2009.CVPR2009.IEEEConferenceon
,pp.413Œ420.IEEE,2009.
PauRodríguez,JordiGonzalez,GuillemCucurull,JosepMGonfaus,andXavierRoca.Regularizingcnnswith
locallyconstraineddecorrelations.
arXivpreprintarXiv:1611.01967
,2016.
11
PublishedasaconferencepaperatICLR2019
OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,Andrej
Karpathy,AdityaKhosla,MichaelBernstein,AlexanderC.Berg,andLiFei-Fei.ImageNetLargeScale
VisualRecognitionChallenge.
InternationalJournalofComputerVision(IJCV)
,115(3):211Œ252,2015.doi:
10.1007/s11263-015-0816-y.
AndreiARusu,NeilCRabinowitz,GuillaumeDesjardins,HubertSoyer,JamesKirkpatrick,KorayKavukcuoglu,
RazvanPascanu,andRaiaHadsell.Progressiveneuralnetworks.
arXivpreprintarXiv:1606.04671
,2016.
JoanSerrà,DídacSurís,MariusMiron,andAlexandrosKaratzoglou.Overcomingcatastrophicforgettingwith
hardattentiontothetask.
arXivpreprintarXiv:1801.01423
,2018.
KarenSimonyanandAndrewZisserman.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.
arXivpreprintarXiv:1409.1556
,2014.
StevenASlomanandDavidERumelhart.Reducinginterferenceindistributedmemoriesthroughepisodic
gating.
EssaysinhonorofWKEstes
,1:227Œ248,1992.
RupeshKSrivastava,JonathanMasci,SohrobKazerounian,FaustinoGomez,andJürgenSchmidhuber.Compete
tocompute.InC.J.C.Burges,L.Bottou,M.Welling,Z.Ghahramani,andK.Q.Weinberger(eds.),
AdvancesinNeuralInformationProcessingSystems26
,pp.2310Œ2318.CurranAssociates,Inc.,2013.URL
http://papers.nips.cc/paper/5059-compete-to-compute.pdf
.
YiSun,XiaogangWang,andXiaoouTang.Sparsifyingneuralnetworkconnectionsforfacerecognition.In
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
,pp.4856Œ4864,2016.
SebastianThrunandTomMMitchell.Lifelongrobotlearning.
Roboticsandautonomoussystems
,15(1-2):
25Œ46,1995.
AmalRannenTriki,RahafAljundi,MathewBBlaschko,andTinneTuytelaars.Encoderbasedlifelonglearning.
arXivpreprintarXiv:1704.01920
,2017.
P.Welinder,S.Branson,T.Mita,C.Wah,F.Schroff,S.Belongie,andP.Perona.Caltech-UCSDBirds200.
TechnicalReportCNS-TR-2010-001,CaliforniaInstituteofTechnology,2010.
WeiXiong,BoDu,LefeiZhang,RuiminHu,andDachengTao.Regularizingdeepconvolutionalneuralnetworks
withastructureddecorrelationconstraint.In
ICDM
,pp.519Œ528,2016.
JianXue,JinyuLi,andYifanGong.Restructuringofdeepneuralnetworkacousticmodelswithsingularvalue
decomposition.In
Interspeech
,pp.2365Œ2369,2013.
LeonYaoandJohnMiller.Tinyimagenetwithconvolutionalneuralnetworks.
CS231N
,2015.
YuguoYu,MicheleMigliore,MichaelLHines,andGordonMShepherd.Sparsecodingandlateralinhibition
arisingfrombalancedandunbalanceddendrodendriticexcitationandinhibition.
JournalofNeuroscience
,34
(41):13701Œ13713,2014.
FriedemannZenke,BenPoole,andSuryaGanguli.Improvedmultitasklearningthroughsynapticintelligence.
In
ProceedingsoftheInternationalConferenceonMachineLearning(ICML)
,2017.
12
PublishedasaconferencepaperatICLR2019
A
PPENDIX
AD
ETAILSONTHEEXPERIMENTALSETUP
Inalldesignedexperiments,ourregularizerisappliedtotheneuronsofthefullyconnectedlayers.Asafuture
work,weplantointegrateitintheconvolutionallayers.
A.1P
ERMUTED
M
NIST
Theusednetworkiscomposedoftwofullyconnectedlayers.Alltasksaretrainedfor10epochswithalearning
rate
10

2
usingSGDoptimizer.ReLUisusedasanactivationfunctionunlessmentionedotherwise.Throughout
theexperiment,weusedascale
˙
fortheGaussianfunctionusedforthelocalinhibitionequalto
1
=
6
ofthe
hiddenlayersize.Forallcompetingregularizers,wetesteddifferenthyperparametersfrom
10

2
to
10

9
and
reportthebestone.For


,wehaveusedahigh


valuethatensurestheleastforgetting.Thisallowsusto
examinethedegradationintheperformanceonthelatertaskscomparedtothoselearnedpreviouslyasaresultof
lackingcapacity.Notethatbetteraverageaccuraciescanbeobtainedwithtuned


.
Insection4.1weestimatedthefreecapacityinthenetworkwiththepercentageof

k
<
10

2
,with

k
,the
importanceweightmultiplierestimatedandaccumulatedovertasks.Weconsider

k
<
10

2
ofnegligible
importancasinanetworktrainedwithoutasparsityregularizer,

ij
<
10

2
coversthe10percentiles.
A.2CIFAR-100
Asabasenetwork,weuseanetworksimilartotheoneusedbyZenkeetal.(2017)butwithoutdropout.We
evaluatetwovariantswithhiddensize
N
=
f
256
;
128
g
.Throughouttheexperiment,weagainusedascale
˙
fortheGaussianfunctionequalto
1
=
6
ofthehiddenlayersize.Wetrainthedifferenttasksfor50epochswitha
learningrateof
10

2
usingSGDoptimizer.
A.3T
INY
I
MAGE
N
ET
WesplittheTinyImageNetdataset(Yao&Miller,2015)intotentasks,eachcontainingtwentycategoriestobe
learnedatonce.Asabasenetwork,weuseavariantofVGG(Simonyan&Zisserman,2014).Forarchitecture
details,pleaserefertoTable4below.
Layer
#
Convolution
64
MaxPooling
-
Convolution
128
MaxPooling
-
Convolution
256
MaxPooling
-
Convolution
256
MaxPooling
-
Convolution
512
Convolution
512
Fullyconnected
500
Fullyconnected
500
Fullyconnected
20
Table4:ArchitectureofthenetworkusedintheTinyImagenetexperiment.
Throughouttheexperiment,weagainusedascale
˙
fortheGaussianfunctionequalto
1
=
6
ofthehiddenlayer
size.
A.48
TASKOBJECTRECOGNITIONSEQUENCE
The8taskssequenceiscomposedof:1.Oxford
Flowers
(Nilsback&Zisserman,2008),2.MIT
Scenes
(Quattoni
&Torralba,2009),3.Caltech-UCSD
Birds
(Welinderetal.,2010),4.Stanford
Cars
(Krauseetal.,2013);5.
FGVC-
Aircraft
(Majietal.,2013);6.VOC
Actions
(Everinghametal.);7.
Letters
(deCamposetal.,
2009);and8.
SVHN
(Netzeretal.,2011)datasets.Wehavererunthedifferentmethodsandobtainthesame
reportedresultsasinAljundietal.(2017).
13
PublishedasaconferencepaperatICLR2019
BE
XTRA
R
ESULTS
B.1P
ERMUTED
M
NIST
S
EQUENCE
Insection4.1,wehavestudiedtheperformanceofdifferentregularizersandactivationfunctionson5permuted
Mnisttasksinanetworkwithahiddenlayerofsize
128
.Figure5showstheaverageaccuraciesachievedbyeach
ofthestudiedmethodsattheendofthelearnedsequenceinanetworkwithahiddenlayerofsize
64
.Similar
conclusionscanbedrawn.
Maxout
and
LWTA
performsimilarlyandimproveslightlyover
ReLU
.Regularizers
appliedtotherepresentationaremorepowerfulforsequentiallearningthanregularizersapplieddirectlytothe
parameters.,
L1-Rep
(orange)isconsistentlybetterthanL1-Param(pink).Our
SLNID
isableof
maintainingagoodperformanceonallthetasks,achievingamongthetopaveragetestaccuracies.Admittedly,
theperformancesof
SLNID
isveryclosetoL1-Rep.Thedifferencebetweenthesemethodsstandsoutmore
clearlyforlargernetworksandmorecomplextasks.
Figure5:
Comparisonofdifferentregularizationtechniqueson5permutedMNISTsequenceoftasks,hidden
size=64.Representationbasedregularizersaresolidbars,barswithlinesrepresentparametersregularizers,
dottedbarsrepresentactivationfunctions.SeeFigure2forsize
128
.
B.2SLNI
WITH
EWC
Toshowthatourapproachisnotlimitedto
MAS
(Aljundietal.,2017),wehavealsoexperimentedwith
EWC(Kirkpatricketal.,2016)asanotherimportanceweightbasedmethodalongwithourregularize
SLNID
onthepermutedMnistsequence.Figure6showsthetestaccuracyofeachtaskattheendofthe5permuted
Mnistsequenceachievedbyour
SLNID
combinedwith
EWC
andby
No-Reg
(hereindicating
EWC
without
regularization).Itisclearthat
SLNID
succeedstoimprovetheperformanceonallthelearnedtaskswhich
validatestheutilityofourapproachwithdifferentsequentiallearningmethods.
Figure6:
(a)
SLNID
with
EWC
on5permutedMnistsequenceoftasks,hiddensize=128,(b)hiddensize=64.
B.3C
IFAR
100
SEQUENCE
Insection4.2wehavetested
SLNID
andotherrepresentationregularizersontheCifar100sequence.In
Figure3(a)wecomparetheirperformanceonanetworkwithhiddenlayersize
256
.Figure7repeatsthesame
experimentforanetworkwithhiddensize
128
.While
DeCov
and
SLNID
continuetoimproveover
No-Reg
,
L1-Rep
seemstosufferinthiscase.Ourinterpretationisthat
L1-Rep
hereinterfereswiththepreviously
learnedtaskswhilepenalizingactivationsandhencesuffersfromcatastophicforgetting.Inlinewithallthe
previousexperiments
SLNID
achievesthebestaccuraciesandmanagesheretoimproveover
6%
comparedto
No-Reg
.
14
PublishedasaconferencepaperatICLR2019
Figure7:
ComparisonofdifferentregularizationtechniquesonasequenceoftentasksfromCifarsplit.Hidden
size=128.SeeFigure3(a)forsize256.
B.4S
PATIAL
L
OCALITY
T
EST
Toavoidpenalizingalltheactiveneurons,our
SLNID
weightsthecorrelationpenaltybetweeneachtwoneurons
basedontheirspatialdistanceusingaGaussianfunction.Wewanttovisualizetheeffectofthisspatiallocality
ontheneuronsactivity.Toachievethis,wehaveusedthe3tasksofthePermutedMnistsequenceasatest
caseandvisualizedtheneuronsimportanceaftereachtask.Thisisdoneusingthenetworkofhiddenlayersize
64
.Figure8,Figure9andFigure10showtheneuronsimportanceaftereachtask.Theleftcolumniswithout
locality,i.e.
SLNID
,andtherightcolumnis
SLNID
.Bluerepresentsthetask,orangethesecondtaskand
greenthethirdtask.Whenusing
SLNID
,inhibitionisappliedinalocalmannerallowingmoreactiveneurons
whichcouldpotentiallyimprovetherepresentationpower.Whenlearningthesecondtask,newneuronsbecome
importantregardlessoftheirclosenesstotaskimportantneuronsasthoseneuronsareexcludedfromthe
inhibition.Assuch,newneuronsarebecomingactiveasnewtasksarelearned.For
SLNID
allneuralcorrelation
ispenalizedinthetask.Andforlatertasks,veryfewneuronsareabletobecomeactiveandimportantfor
thenewtaskduetothestrongglobalinhibition,wherepreviousneuronsthatareexcludedfromtheinhibition
areeasiertobere-used.
15
PublishedasaconferencepaperatICLR2019
Figure8:
Firstlayerneuronimportanceafterlearningthetask(blue).Left:
SNID
,Right:
SLNID
.More
activeneuronsaretoleratedin
SLNID
.
Figure9:
Firstlayerneuronimportanceafterlearningthesecondtask(orange),superimposedonFigure8.Left:
SNID
,Right:
SLNID
.
SLNID
allowsnewneurons,especiallythosethatwerecloseneighbourstoprevious
importantneurons,tobecomeactiveandtobeusedforthenewtask.
SNID
penalizesallunimportantneurons
equally.Asaresult,previousneuronsareadaptedforthenewtasksandlessnewneuronsaregettingactivated.
Figure10:
Firstlayerneuronimportanceafterlearningthethirdtask(green),superimposedonFigure9.Left:
SNID
,Right:
SLNID
.
SLNID
allowspreviousneuronstobere-usedforthethirdtask.Itavoidschangingthe
previousimportantneuronsbyaddingnewneurons.For
SNID
,veryfewneuronsarenewlydeployed.Thenew
taskislearnedmostlybyadaptingpreviousimportantneurons,causingmoreinterference.
16
PublishedasaconferencepaperatICLR2019
Figure11:
Firstlayerneuronimportanceafterlearningthetask,sortedindescendingorderaccordingtothe
taskneuronimportance(blue).Left:
SNID
,Right:
SLNID
.Moreactiveneuronsaretoleratedin
SLNID
.
Figure12:
Firstlayerneuronimportanceafterlearningthesecondtasksortedindescendingorderaccording
tothetaskneuronimportance(orange),superimposedontopof11.Left:
SNID
,Right:
SLNID
.
SLNID
allowsnewneuronstobecomeactiveandbeusedforthenewtask.
SNID
penalizesallunimportant
neuronsequallyandhencemoreneuronsarere-usedtheninitiatedforthetime.
Figure13:
Firstlayerneuronimportanceafterlearningthethirdtasksortedindescendingorderaccordingto
thetaskneuronimportance(green),superimposedontopof12.Left:
SNID
,Right:
SLNID
.
SLNID
allowspreviousneuronstobere-usedforthethirdtaskwhileactivatingnewneuronstocopewiththeneeds
ofthenewtask.For
SNID
,veryfewneuronsarenewlydeployedwhilemostpreviousimportantneuronsfor
previoustasksarere-adaptedtolearnthenewtask.
17
"
68,Temporal Stability in Predictive Process Monitoring,http://arxiv.org/pdf/1712.04165v3.pdf,https://github.com/irhete/stability-predictive-monitoring,"NonamemanuscriptNo.
(willbeinsertedbytheeditor)
TemporalStabilityinPredictiveProcessMonitoring
IreneTeinemaa

MarlonDumas

Anna
Leontjeva

FabrizioMariaMaggi
Received:date/Accepted:date
Abstract
Predictiveprocessmonitoringisconcernedwiththeanalysisofevents
producedduringtheexecutionofabusinessprocessinordertopredictasearly
aspossibletheoutcomeofanongoingcase.Traditionally,predictiveprocess
monitoringmethodsareoptimizedwithrespecttoaccuracy.However,inenviron-
mentswhereusersmakedecisionsandtakeactionsinresponsetothepredictions
theyreceive,itisequallyimportanttooptimizethestabilityofthesuccessivepre-
dictionsmadeforeachcase.Tothisend,thispaperanotionoftemporal
stabilityforbinarytasksinpredictiveprocessmonitoringandeval-
uatesexistingmethodswithrespecttobothtemporalstabilityandaccuracy.We
thatmethodsbasedonXGBoostandLSTMneuralnetworksexhibitthehigh-
esttemporalstability.Wethenshowthattemporalstabilitycanbeenhancedby
hyperparameter-optimizingrandomforestsandXGBoostwithrespect
tointer-runstability.Finally,weshowthattimeseriessmoothingtechniquescan
furtherenhancetemporalstabilityattheexpenseofslightlyloweraccuracy.
Keywords
PredictiveProcessMonitoring

EarlySequencetion

Stability
1Introduction
Modernorganizationsgenerallyexecutetheirbusinessprocessesontopofprocess-
awareinformationsystems,suchasEnterpriseResourcePlanning(ERP)systems,
CustomerRelationshipManagement(CRM)systems,andBusinessProcessMan-
agementSystems(BPMS),amongothers[9].Thesesystemsrecordarangeof
eventsthatoccurduringtheexecutionoftheprocessestheysupport,including
eventssignalingthecreationandcompletionofbusinessprocessinstances(herein
called
cases
)andthestartandcompletionofactivitieswithineachcase.
Eventrecordsproducedbyprocess-awareinformationsystemscanbeextracted
andpre-processedtoproducebusinessprocess
eventlogs
[1].Abusinessprocess
eventlogconsistsofasetof
traces
,eachtraceconsistingofthesequenceofevent
UniversityofTartu,JuhanLiivi2,50409Tartu,Estonia,
E-mail:
f
irene.teinemaa,marlon.dumas,anna.leontjeva,f.m.maggi
g
@ut.ee
arXiv:1712.04165v3  [cs.LG]  15 Jun 20182
IreneTeinemaaetal.
recordsproducedbyonecase.Eacheventrecordconsistsofanumberofattributes.
Threeoftheseattributesarepresentineveryeventrecord,namelythe
eventclass
(a.k.a.
activityname
)specifyingwhichactivitytheeventrefersto,the
timestamp
specifyingwhendidtheeventoccur,andthe
caseid
indicatingwhichcaseofthe
processgeneratedthisevent.Inotherwords,everyeventrepresentstheoccurrence
ofanactivityataparticularpointintimeandinthecontextofagivencase.An
eventrecordmaycarryadditionalattributes.Theseattributesmaybecategorical,
numerical,ortextual.Forexample,inasalesprocess,aneventcorrespondingto
activity
payment
couldrecordthe
amount
ofthepayment,the
type
ofpayment
(e.g.,cashorbycreditcard),andan
errormessage
containingthetypeoferror
incaseofafailingcreditcardtransaction.Someattributesvaryfromoneeventto
another.Thesearecalled
event-speattributes
(or
eventattributes
forshort).
Forexample,inasalesprocess,theamountofthepaymentisspofactivity
payment.Otherattributes,namely
caseattributes
,belongtothecaseandare
hencesharedbyalleventsgeneratedbythesamecase.Forexampleinasales
process,thecustomeridenislikelytobeacaseattribute.Ifso,thisattribute
willappearineveryeventofeverycaseofthesalesprocess,anditwillhavethe
samevalueforalleventsgeneratedbythesamecase.
Predictiveprocessmonitoring
[19]isafamilyoftechniquesthatuseeventlogs
topredicthowanongoingcase(a
casepr
)willunfolduptoitscompletion.A
predictiveprocessmonitoringtechniquemayprovidepredictionsontheremaining
executiontimeofeachongoingcaseofaprocess[29],thenextactivitythatwill
beexecutedineachcase[11],ortheoutcomeofacasewrt.asetofpossible
outcomes[21,35].Inthiswork,weconcentrateonthelattertypeofpredictions,
namelyon
outcome-orientedpredictiveprocessmonitoring
[35],wheretheoutcome
isassumedtobeabinaryvalue(multi-classoutcomesareoutofscopeofthis
paper).Inthiscontext,theoutcomeofacasecanbeintways,
dependingonthebusinessgoalsandtargetsoftheprocess.Forinstance,inasales
processadesirableoutcomeisthatthecustomerplacesapurchaseorder,whilea
negativeoutcomeoccurswhenthecustomerterminatestheprocessbeforeplacing
anorder.
Avarietyofoutcome-orientedpredictiveprocessmonitoringtechniqueshave
beenproposedintheliterature[35].Inexistingwork,thequalityofthesemethods
ismeasuredintermsofpredictionaccuracyusing,forexample,precision,recall,
andAreaUndertheROCCurve(AUC).However,wearguethattheseaccuracy
measuresarenotnttoassessapredictiveprocessmonitoringmethod.Con-
sider,forinstance,ahealthcareprocesswherethetargetistoestimatewhethera
patientwillneedintensiveorstandardcare.Anaccuratepredictioncouldhelpthe
patienttoreceivethesuitabletreatmentinatimelymanner,aswellashelpthe
hospitaltobetterallocateresourcestopatients.Supposethatwhenthepatient
arrivesatthehospital,thepredictorestimatesthatshewillneedintensive
care,sosheisadmittedtotheintensivecareprogram.Afterexecutingapro-
cedure,thepredictorchangesthepredictionandestimatesthatstandardcareis
tforthepatient,sothepatientisbroughttostandardcare.However,after
performinganotherprocedure,thechangesthepredictionagainandrec-
ommendstransferringthepatientbacktointensivecare.Thisexampleshowshow
thepracticalusabilityofapredictorislimitedifitoutputsunstablepredictions,
i.e.,ifittendstooftenchangethevalueofthepredictionsafterseeingnewdata
aboutthesamecase.Inthisexample,thetreatmentofthepatientcouldhave
TemporalStabilityinPredictiveProcessMonitoring3
beenmoretifthepersonnelhadnottrustedtheintermediateprediction
ofthepredictorandhadnotbroughtthepatienttothestandardcare.Another
exampleconcernsadebtencashmentprocess,whereapredictionenginecanbe
usedtodecidewhetherthedebtshouldbesenttoacreditcollectionagencyornot.
Inthiscase,volatilepredictionscanmisleadusersofthesystemtoprematurely
sendthedebttothecollectionagency,resultinginsmallerrevenueascompared
towaitingsomemoretimeforthedebttoberepaid.Similarly,incaseoffraud
detectioninainstitution,unstablepredictionsmaycausetheinstitution
tofrequentlyblockandunblockthecreditofauser,resultingininconveniences
andlossofrevenuerelatedtopotentialtransactionsthattheuserwasnotableto
complete.
Theaboveexamplesillustratetheimportanceofthestabilityofa
whenusedtomakesuccessivepredictionsinthecontextofpredictiveprocess
monitoring.Theconventionalnotionofstabilityinnon-deterministiclearningal-
gorithms(suchasrandomforest)indicateshowmuchthepredictionsmadefor
thesameongoingcaseacrosstrunsoftrainingthe[10].In
otherwords,ifwetrainmultiplewiththesameparametersettingbut
trandomizationparameters,wouldtheseagreeonthepredic-
tionsmadeforthesamesampleornot?Fromhereinafter,werefertothisnotion
ofstabilityasthe
inter-runstability
.Conversely,inthispaper,weareinterested
inanothertypeofquestion,i.e.,onhowtarethepredictionsmadebythe
same(oranensembleoffortofthesamecase.
Spe,wewanttomeasurewhethertheoftenchangesitsprediction
aboutthesamecasewhenmoreeventsinthecaseareperformed.Werefertothe
latternotionofstabilityasthe
temporalstability
.
Inthispaper,we:
1.
introduceameasureoftemporalstabilityforbinarytasksin
predictiveprocessmonitoring,
2.
performanevaluationofseveralexistingpredictiveprocessmonitoringmeth-
odswithrespecttobothpredictionaccuracyandtemporalstability,
3.
studytheontemporalstabilityofincreasinginter-runstabilityincom-
binationwithpredictionaccuracy,
4.
studytheontemporalstabilityandaccuracyofapplyingsmoothing
techniquestothetimeseriesofpredictionsmadeforagivencase.
Therestofthepaperisstructuredasfollows.Section2summarizestherelated
workonpredictiveprocessmonitoring,earlysequenceandlearning
algorithmstability.Section3sthenotionoftemporalstabilityandpro-
posesametricformeasuringit,aswellasapost-processingtechniquetocombine
predictionsmadeforofthesamecaseinordertoreducetheirvolatil-
ity.Section4describestheexperimentalset-upandtheresultsoftheevaluation.
Section5concludesthepaperanddiscussesavenuesforfuturework.
2RelatedWork
Inthissection,wediscusstherelatedworkonpredictiveprocessmonitoring,early
sequencecandstabilityinlearningalgorithms.
4
IreneTeinemaaetal.
2.1PredictiveProcessMonitoring
Avarietyofpredictiveprocessmonitoringmethodshavebeenproposedintheex-
istingliterature[20].Theseapproachescanbedividedaccordingtotheprediction
targetintothefollowingcategories:remainingtimeprediction(regressiontasks),
nextactivityprediction(multi-classandoutcome-orientedpredic-
tion(binaryOutcome-orientedprocessmonitoringtechniquesdif-
ferintermsofthreeaspects:sequenceencoding,bucketingof(howmany
arebuiltandwhicharegivenasinputtoeachr),and
algorithm[35].
Asequenceencodingcanbe
lossless
,meaningthattheoriginaltracecanbe
recoveredcompletelyfromtheencodedtrace.Anexampleofsuchencodingis
the
index-basedencoding
proposedbyLeontjevaetal.[16],whichconcatenates
thedatafromalleventsintoasinglevector,sothatthepositioncontains
theactivitynamefromtheevent,thesecondpositioncontainstheactivity
namefromthesecondeventandsoon.Adrawbackofthismethodisthatthe
sizeoftheencodedvectorincreaseswitheachevent,whichmeansthataseparate
isneededforeachlength.Alternatively,a
lossy
encodingapproach
aggregatestheeventdataforeachtrace,thusproducingfeaturevectorsofthe
samesizeindependentlyofthelength.Examplesoflossyencodingsare
laststateencoding
,whichusesonlydatafromthemostrecenteventperformed
ineachtraceand
aggregationencoding
,whichaggregatestheinformationfrom
alleventsexecutedsofarusing,forinstance,thefrequenciesofcategoricalevent
attributes(e.g.,activitynames),oraggregationfunctionssuchasminimum,mean,
ormaximumfornumericeventattributes.Usingalossyencoding,wecanfeedall
theencodedtoa
single
,asthelengthofthefeaturevectordoes
notdependonthelength.
Severalexistingworkshaveproposeddividingintobucketsandtraining
separateforeachbucket,resultingina

approach.An
exampleis[16],wheretarebuiltforeachlength.Other
methodsclusterthebasedontheirsimilarityintermsoftheperformed
activitiesandbuildonepercluster[7].Otherstrainaforevery
stateinaprocessmodelorinatransitionsystem[14].
Existingworkshaveexperimentedwithtalgorithms.The
mostpopularchoicesaretree-basedmethods,suchasdecisiontrees[7,14,15]and
randomforests[16,7].Toourknowledge,thereisnoexistingworkonusingrecur-
rentneuralnetworks(RNNs)foroutcome-orientedpredictiveprocessmonitoring.
However,RNNswithlongshorttermmemoryunits(LSTMs)havebeenusedin
otherpredictiveprocessmonitoringtasks,suchasforpredictingtheremaining
timeandthenextactivity[33,11].
2.2EarlySequencecation
Withrespecttothebroaderliteratureonmachinelearning,outcome-orientedpre-
dictiveprocessmonitoringisrelatedto
earlysequenceation
.Givenasetof
labeledsequences,thegoalistobuildamodelthatforasequencepredicts
thelabelthiswillgetwhencompleted.Asurveyonsequence
presentedin[30]givesanoverviewofthetechniquesfromthis
TemporalStabilityinPredictiveProcessMonitoring5
Xingetal.[36]introducedthenotionof
seriality
insequencere-
ferringtothepropertythatforeachsequence,thereexistsalengthstart-
ingfromwhichtheclaoutputs(almost)thesameprediction.Theworks
onearlysequencearegenerallyfocusedondeterminingsuch
lengththatyieldsagoodprediction,alsoreferredtoasthe
minimalprediction
length
(MPL)[37].ThemethodbyXingetal.[37]dstheearliesttimestamp
whenthenearestneighborrelationshipsinthetrainingdatabecomestable(i.e.,
remainthesameinthesubsequentParrishetal.proposedamethod
basedonthe
reliability
ofpredictions,i.e.,theprobabilitythatthelabelassigned
toagivenisthesameasthelabelassignedtothewholesequence[26].
Morerecently,Morietal.[22]designedanapproachtomakeanearlyprediction
whentheratioofaccuracybetweenthepredictionmadefortheandfor
thefullsequenceexceedsapredeterminedthreshold.Mostofthetechniquesfor
earlyaredesignedfornumericaltimeseriesorsimple(univariate)
symbolicsequences.However,theproblemofpredictiveprocessmonitoringcan
beseenasanearlyovercomplexsequenceswhereeachelementhas
atimestamp,adiscreteattributereferringtoanactivity,andapayloadmadeofa
heterogeneoussetofotherattributes.Oneofthefewworksonearly
oncomplexsequencesis[17],whereLinetal.proposeconstructing
serialdecision
trees
andmonitortheerrorrateinleafnodesinordertodeterminetheMPL.
TheworksondevelopingserialandndingtheMPLareclosely
relatedtothenotionoftemporalstabilitystudiedinthispaper.Infact,aserial
hasperfecttemporalstability.However,insteadofdeterminingMPLand
makingpredictionsonlyaftertheMPLisreached,weareinterestedinpredicting
theoutcomeforeverypreofthesequence.Thereasonforthisisthatina
predictiveprocessmonitoringsetting,itisnecessarytogivethebestestimateofthe
caseoutcomeevenwhentoofewdataisavailabletomakeaprediction.Inthis
respect,weaimfortemporalstabilityalsoonshortwhentheprediction
mightstilldierfromtheonethatwouldbemadefortheentiresequence.
2.3StabilityofLearningAlgorithms
Stabilityoflearningalgorithmshasbeenatopicofinterestformanyyears.Con-
ventionally,alearningalgorithmisconsideredunstableifsmallchanges(pertur-
bations)inthetrainingsetcancausetchangesinthepredictor[4].Such
instabilityofsinglepredictorsmotivatedBreimanetal.tointroduce
baggingpre-
dictors
,showingthatthestabilityandaccuracyofapredictorcanbeincreased
byaggregatingtheestimationsfrommultipleversionsofthepredictor[4].Inthis
context,increasingstabilityrelatestodecreasingthevariancebetweenprediction
estimates.Bousquetetal.studiedtherelationshipbetweenstabilityandgeneral-
ization[3].Inparticular,theirstudyisbasedon
sensitivityanalysis
,i.e.,howmuch
replacingordeletingatrainingsamplethepredictionloss.Theypropose
threeofstability,whichareallbasedonchangesinthetrainingset.
Thereasonforthisisthattheyfocusondeterministicalgorithms,sothatallthe
randomnesscomesfromthesamplingonthedatasets.etal.extended
thesenotionsofstabilitytonon-deterministicalgorithms[10]whererandomness
ispresentevenwhenthetrainingsetremainsunchanged.Theirstability
tionsaresupplementedwitharandomnessparameter.Morerecently,Liuetal.
6
IreneTeinemaaetal.
proposedametricformeasuringstabilityacrossseveralrunsofrandomforestand
incorporateditintoaframeworkforselectingthehyperparametersbasedona
goodnessmeasurecombiningAUC,stability,andcost[18].
Whileexistingnotionsofstabilityarerelatedtochangesmadeinthetraining
phase(eitherbychangingthetrainingsetorbychangingtherandomnesspa-
rameter),inthispaperwestudythecasewhereboththetrainingdatasetand
therandomnessarebuttheinputvectorchangesovertime.Inparticular,
westudythe
temporalstability
ofpredictionsinthesettingwherepredictionsare
madesuccessivelyfortofthesamesequence.Inotherwords,we
examinehowmuchincreasingthelengthofthechangesthepredictions.
3TemporalPredictionStability
Inthissection,westartwithintroducingthenotionofpredictionscoresinoutcome-
orientedpredictiveprocessmonitoring.Weproceedwithtemporalstabil-
ityandprovideametrictomeasurethisproperty.Lastly,wedescribeourapproach
forcombiningpredictionscoresobtainedforofthesamecaseinorderto
reducetheirvolatility.
3.1PredictionScoresOverTime
Inanoutcome-orientedpredictiveprocessmonitoringtask,thetargetforclassi-
isabinaryvalue,referringtoeitherapositiveoranegativeoutcome.
Despitethefactthattheistrainedtorecognizeabinarytarget,itcan
usuallyoutputareal-valued
predictionscore
indicatingthelikelihoodtowardsthe
positiveoutcome.
Inpredictiveprocessmonitoring,theisaskedtogiveanestimation
aboutthecaseoutcomeaftereachperformedevent.Therefore,theprediction
scoresestimatedaftereacheventofthesamecaseformatimeseries.Asanexam-
ple,considerthepinktimeseries(CaseB)plottedinFig.1(left).Duringthe
5events,theisunsureaboutwhatwillbetheoutcomeofthiscase(the
predictionscoresfortheseeventsareequalto0.5).Then,the6theventprovides
somerelevantsignal,sothatthebecomestthatthecasewillbe
positive(thepredictionscoresforthefollowingeventsare0.9).Thisseriesisrather
stableovertime,asthesuccessivepredictionscoreschangeonlyonce.Anexample
ofacompletelystableseriesofpredictionsisCaseA(theblackline),wherethe
predictionscoresremainthesameforallNowconsiderCasesCandD
(greenandblue).Wecanseethatthechangesthepredictionscoreafter
almosteveryevent,producinga
volatile
timeseriesforthesecases.Suchunstable
predictionshavelittlepracticalvalue,causinguserstobecautiousaboutacting
uponthepredictionanddecreasingtheoverallcredibilityofthe
3.2TemporalStability
Basedontheaboverationale,wesaythatais
temporallystable
ifit(gen-
erally)outputssimilarpredictionstosuccessivefromthesamesequence.
TemporalStabilityinPredictiveProcessMonitoring7
Fig.1:Examplesofpredictionscoresovertime:original(left)andsmoothed
(right).
Givenathresholdonthepredictionscoresthatdetermineswhetherthepre-
dictedoutcomeispositiveornegative,itwouldbenaturaltotemporal
instabilityasthenumberoftimestheitsprediction,andto
temporalstabilityasoneminusanormalizedmeasureofinstability.Thedraw-
backofthisapproachisthatitisdependentonthechosenthreshold.Instead,we
aimforamoregeneral,threshold-independentmeasurethatwouldcapturethe
stabilityoftheunderanythreshold.Accordingly,weproposetomeasure
stabilityasafunctionofthemagnitudeofthechangesbetweensuccessivepredic-
tionscores.Thislatterisrelatedtotheformer:Ifthebetween
successivescoresishigh,thereexistmanythresholdsthatwouldleadtoin
thepredictedoutcome.Conversely,iftheislow,onlyalownumberof
thresholdswouldtheprediction.
Thesimplestwaytoconsiderthemagnitudeofthechangeswouldbetomeasure
the(average)absolutebetweensuccessivepredictionscores.Notethat
thismetricdoesnotconsiderthedirectionofthechanges,i.e.,achangetowards
thecorrectdirection(theactualclass)themeasureinthesamewayas
achangetowardsthewrongdirection.Asaresult,athatconsistently
improvesitspredictionisassignedasimilarstabilityscoreasonethat
aroundthesamescore.Analternativewouldbetoconsideronlythechanges
thataremadetothewrongdirection,calculatingthe(average)absolute
onlyoverthesechanges.However,thismetricwouldthe
consistency
ofthe
ratherthanits
stability
.Forinstance,considerasequencewiththeactual
outcomebeing
positive
,andtwoOneoftheoutputsascoreof
1attheevent,i.e.,itis(correctly)certainthattheoutcomewillbepositive,
butthroughoutthecasebecomesonlyslightlylesscertainofit,outputting0.99
onsomeevents.Theothermakesacompletelywrongestimationatthe
beginningofthesequence,outputtingascoreof0,whilethroughouttherestofthe
case,itonlyimprovesitsestimate(sometimesbylargemagnitudes),producing
scoreslike0.1,0.5,andeven0.95.Accordingtothelattermetric,thesecond
whichmakeschangesinlargemagnitudes,wouldbeconsideredmore
stablethanthealthoughtheoneonlychangesitspredictionby
8
IreneTeinemaaetal.
asmallamount.Inasense,ameasurethatconsidersthedirectionofthechange
penalizesthatmaketherightpredictionfromtheonset,sincetheonly
waytomaintaintheirstabilitythroughoutthesequencewouldbetoalwaysoutput
exactlythesamescore.Basedontheseconsiderations,weproceedwithmeasuring
theaveragederencebetweenthesuccessivepredictionscoreswithouttakinginto
accountthedirectionofthechange.
Accordingly,wemeasurethetemporalstability(TS)ofaasoneminus
theaverageabsolutebetweenanytwosuccessivepredictionscores:
TS
=1

1
n
n
X
i
=1
1
T
i

1
T
i
X
t
=2
j
^
y
i
t

^
y
i
t

1
j
;
(1)
where
n
isthenumberofcasesusedfortheevaluation,
T
i
isthetotalnumberof
eventsinthe
i
-thcase,and^
y
i
t
isthepredictionscoreofthe
t
-theventofthe
i
-th
case.Thismetricevaluatestheaverageabsolutebetweensuccessive
predictionscoreswithineachcaseinordertoeliminatethebiastowardslong
sequences,andthenaveragesoverthecases.
3.3CombiningPredictionScoresviaSmoothing
Wecanadjustthepredictionscoresduringapost-processingphasetoreduce
volatilitywithoutthepre-trainedSp,insteadofusing
explicitlythescorethattheoutputsforacaseafterobserving
t
events,
wecombineitwithpredictionscoresmadeforshorterofthesamecase.
Tocombinepredictions,wecanusevarioustimeseries
smoothing
methods,
whichaverageoutthenoiseandThesimplestwaytosmoothatime
seriesisviaa
movingaverage
.Thesmoothedestimateateacheventiscomputed
astheaverageofthelast
k
observations.Atapproach,called
singleex-
ponentialsmoothing
,assignsweightsthatdecreaseexponentiallyovertime.The
smoothedestimateattime
t
isthecombinationoftheobservedvalueattime
t
and
thesmoothedestimateattime
t

1,usingasmoothingparameter

,0
<
=
<
=1:
s
t
=(1


)

^
y
t
+


s
t

1
.Parameter

controlstowhatextentthepreviousob-
servationsaretakenintoaccount.Thelargerthe

,thestrongerthesmoothing
Whileothersmoothingtechniquesareavailable,weusethesingleexponen-
tialsmoothingbecauseofitssimplicityandbecauseitallowsustodirectlycontrol
thelevelofsmoothing.Also,onlytechniquesthatenablesequentialsmoothing(as
opposedtosmoothingovertheentiresequence)areapplicableinourcase,asin
thepredictiveprocessmonitoringsetting,onlythepredictionscoresmadeuptoa
certainpointinthesequenceareknown.
Forexample,considerthetimeseriesplottedinFig.1(right).Thesetime
serieshavebeenderivedfromtheexamplesinFig.1(left)byapplyingexponential
smoothingwith

=0
:
8.WecannoticethattheinCasesCandD
havebeenreducedconsiderably.However,smoothingcanalsohaveanegative
onthepredictions,illustratedbyCaseB.Namely,changesinthescoresdo
nothaveanimmediatestrongastheadjustedscoreputssomeweighton
thepreviousestimates.Therefore,whenaneventcarryingarelevantsignalabout
thecaseoutcomearrives,thesmoothedestimateiscautiousabouttrustingit,
resultinginaloweraccuracy.
TemporalStabilityinPredictiveProcessMonitoring9
4Evaluation
Weconductedanempiricalevaluationtoaddressthefollowingquestions:
RQ1
Whatistherelativeperformanceoftpredictiveprocessmonitoring
methodsintermsoftemporalstability(inadditiontoaccuracy)?
RQ2
Howdoesmaximizingtheinter-runstabilityincombinationwithprediction
accuracyectthetemporalstability?
RQ3
Howdoesdecreasingpredictionvolatilityviaexponentialsmoothing
theaccuracyandthetemporalstability?
Below,wedescribetheapproachesanddatasetsemployed,weexplaintheex-
perimentalsetup,anddiscusstheresults.Thecodeusedforthisevaluationis
availableat
https://github.com/irhete/stability-predictive-monitoring
.
4.1Approaches
ToaddressRQ1,wechoose7predictiveprocessmonitoringapproaches(seeTable
1)asbasisfortheexperiments.Weemploy2existingsequenceencodingtech-
niques,theindex-basedandtheaggregationencoding.AsexplainedinSection2.1,
index-basedencodingconstructsalosslessrepresentationofabyconcate-
natingthedatafromeachexecutedevent.Intheaggregationencoding,a
ofarbitrarylengthistransformedintoalengthfeaturevectorbyapply-
ingtaggregationfunctions.Inparticular,forcategoricalfeatures,weuse
frequencies,i.e.,howmanytimeseachpossiblevalue(e.g.,eachactivityname)
hasoccurredinthegivenwhilethenumericalfeaturesareaggregatedus-
ingtheaverage,maximum,minimum,sum,andstandarddeviationofthevalues
observedsofar.Bothencodingsarecombinedwithtwocmethods,
randomforest[5](RF)andXGBoost[6].Wechoosethesebecausethey
haveshowntooutperformothermethodsinvariousapplications[12,24].Addi-
tionally,weadaptapredictiveprocessmonitoringmethodbasedonLSTMneural
networks[33]topredicttheoutcomeofacase.
Inalloftheapproaches,eachconstitutesaseparatetraininginstance.For
index-basedencoding,thefactthattconsistofdierentnumbersof
eventsraisesanissuewhentryingtoencodeallwithgthvectors.
Therearetwopossiblesolutionstothisissue.Firstly,itispossibletothe
maximumlengthand,forshorterpadthedataformissingevents
withzeros.Analternativesolutionistobuildmultipleoneforeach
length;givenaoflength
l
inthetestingset,thepredictionforthis
isderivedfromtheconstructedbasedon(inthetrainingset)of
length
l
.Inourexperiments,weapplybothsolutionstotheRFandXGBoostbased
approaches,markedas
RF
idx
pad
/
XGB
idx
pad
and
RF
idx
mul
/
XGB
idx
mul
,
respectively.Sincethesecond,msolutionisnotcommonlyusedwith
LSTMs,intheLSTM-basedapproachweonlyapplythepaddingsolution.
Predictionscoresreturnedbyrsareoftenpoorlycalibrated,meaning
thatthescoresdonotwelltheactualprobabilitiesofbelongingtoone
classortotheother[13].Forinstance,oneermayoutputscoresthatare
alwaysconcentratedaround0.5,whileanothermayreturnscoresthatarewell
distributedwithintherangebetween0and1.Thiscausesbiaswhencomparing
10IreneTeinemaaetal.
Table1:Approaches.
ApproachMulti/singleclsEncodingClaer
RF
aggsingleaggregationRF
RF
idx
padsingleindexRF
RF
idx
mulmultiindexRF
XGB
idx
padsingleindexXGBoost
XGB
idx
mulmultiindexXGBoost
XGB
aggsingleaggregationXGBoost
LSTMsingleindexLSTM
tintermsoftemporalstability.Indeed,thebetween
anytwopredictionscoresinthecaseoftheformerareverysmall,making
itseemaverystablewhiletherelativewithineachcasemight
belargerthaninthelatterer.Toaddressthisissue,weapplyawell-known
calibrationmethod,Plattscaling[27],toeachofthebeforecomparison.
Wechoosethistechniquebecauseitoutperformsothermethodswhendatais
scarce(e.g.,lessthan1000datapointsavailableforcalibration)[23],whichisthe
caseinmostofourdatasets.Notethatcalibrationdoesnotchangetheorderofthe
predictionscoresassignedbythesamesothattheAUCofeach
isnotbyit.
TotestRQ2,weadapttheapproachproposedin[18]toRFandXGBoost
hyperparameteroptimization.Namely,insteadofchoosingtheoptimalparameter
settingbasedonAUConasinglerunoftraining,weperform5runswith
eachsettingandchoosetheonethatachieves1)thebestaverageAUCoverall
runs,and2)thebestcombinedAUCandinter-runstability
1
overallruns.Forthe
latterscenario,wegivemoreweighttotheinter-runstability,assigningweights1
and5toAUCandstability,respectively.
Todecreasepredictionvolatility(RQ3),weexperimentwithexponentialsmooth-
ing,varyingthesmoothingparameter

2f
0
:
1
;
0
:
25
;
0
:
5
;
0
:
75
;
0
:
9
g
.
4.2Datasets
Weusereal-lifedatasetspubliclyavailableatthe4TUCentreforResearchData
2
.
Fromthe4TUCentredatasets,weleftoutthosethatarenotbusinessprocessevent
logs,butinsteadrelatedtosoftwaredevelopmentorwebservices.Moreover,we
excludedeventlogswhereanaturallabelingforthecaseoutcomewasnoteasily
derivable.Also,wediscardedthedatasetswheretheorderofeventsisnotclearly
duetotimegranularityissues.Foreachselectedlog,itispossibletocome
upwithmultipleofcaseoutcome(
labelings
),sothateach
1
Inter-runstabilityreferstotheMSPDmetricintroducedin[18]:
MSPD
(
f
)=
2
E
x
i
[
Var
(
f
(
x
i
))

Cov
(
f
j
(
x
i
)
;f
k
(
x
i
))]
;
where
E
x
i
istheexpectationoverallvalidationdata,
f
isamappingfromasample
x
i
toalabel
y
i
onagivenrun,
Var
(
f
(
x
i
))
isthevariance
ofthepredictionsofasingledatapointoverthemodelruns,and
Cov
(
f
j
(
x
i
)
;f
k
(
x
i
))
isthe
covarianceofpredictionsofasingledatapointovertwomodelruns.
2
Productionlog:
https://data.4tu.nl/repository/uuid:68726926-5ac5-4fab-b873-ee76ea412399
,
otherlogs:
https://data.4tu.nl/repository/collection:event_logs_real
TemporalStabilityinPredictiveProcessMonitoring11
constitutesaseparatepredictiveprocessmonitoringproblem.Inthefollowing,
wedescribethedomainofeachofthedatasetsandthelabelingsthat
wereconstructedforcarryingouttheexperiments.Then,wedescribethefeature
extractionandpreprocessingprinciplesappliedtothedatasetsandconcludewith
acomparisonofgeneralstatisticsofthedatasets.
BPIC2012.
Thisdataset,originallypublishedinrelationtotheBusinessProcess
IntelligenceChallenge(BPIC)in2012,containstheexecutionhistoryofaloan
applicationprocessinaDutchinstitute.Eachcaseinthislogrecords
theeventsrelatedtoaparticularloanapplication.Forpurposes,we
somelabelingsbasedontheoutcomeofacase,i.e.,whetherthe
applicationisaccepted,rejected,orcancelled.Intuitively,thiscouldbethoughtof
asamulti-classproblem.However,toremainconsistentwithprevious
workonoutcome-orientedpredictiveprocessmonitoring,weapproachitasthree
separatebinarytasks.Intheexperiments,thesetasksarereferredto
as
bpic2012
accepted
,
bpic2012
declined
,and
bpic2012
cancelled
.
BPIC2017.
Thiseventlogoriginatesfromthesameinstitutionasthe
BPIC2012one.However,thedatacollectionhasbeenimproved,resultingina
richerandcleanerdataset.Asinthepreviouscase,theeventlogrecordsex-
ecutiontracesofaloanapplicationprocess.SimilarlytoBPIC2012,we
threeseparatelabelingsbasedontheoutcomeoftheapplication,referredtoas
bpic2017
accepted
,
bpic2017
refused
,and
bpic2017
cancelled
.
Sepsiscases.
Thislogrecordstrajectoriesofpatientswithsymptomsofthelife-
threateningsepsisconditioninaDutchhospital.Eachcaselogseventssincethe
patient'sregistrationintheemergencyroomuntilherdischargefromthehospital.
Amongothers,laboratoryteststogetherwiththeirresultsarerecordedasevents.
Moreover,thereasonofthedischargeisavailableinthedatainanobfuscated
format.
Wecreatedthreetlabelingsforthislog:
{
sepsis
cases
1
:thepatientreturnstotheemergencyroomwithin28daysfrom
thedischarge,
{
sepsis
cases
2
:thepatientis(eventually)admittedtointensivecare,
{
sepsis
cases
3
:thepatientisdischargedfromthehospitalonthebasisofsome-
thingotherthan
ReleaseA
,whichisthemostcommonreleasetype.
Hospitalbilling.
ThisdatasetcomesfromanERPsystemofahospital.Eachcase
isanexecutionofabillingprocedureformedicalservices.Wecreatedalabeling
basedonwhetherthecaseisreopenedornot.
Roadtr
ThislogcomesfromanItalianlocalpoliceforce.Thedataset
containseventsaboutsentaboutaaswellas(partial)repay-
ments.Additionalinformationrelatedtothecaseandtotheindividualevents
include,forinstance,thereason,thetotalamount,andtheamountofrepayments
foreachWecreatedthelabeling(
tr

)basedonwhethertheis
repaidinfullorissentforcreditcollection.
12IreneTeinemaaetal.
Productionlog.
Thislogcontainsdatafromamanufacturingprocess.Eachtrace
recordsinformationabouttheactivities,workersand/ormachinesinvolvedin
producinganitem.Thelabeling(
production
)isbasedonwhetherornotthe
numberofrejectedworkordersislargerthanzero.
Beforeencodingthetracesforweapplysomepreprocessingon
therawdatasets
3
.Ingeneral,weusealltheavailablecaseandeventattributes
withoutdoinganyfeatureextractionbeforeencoding.Still,afewextrafeatures
areaddedtoeacheventbasedonthetimestamps,namely,
hour,weekday,month,
timesincecasestart
,and
timesincelastevent
.Additionally,weincludethe
event
number
,i.e.,howmanyeventshavebeenperformedinthecaseuptothecurrent
event.Whileallthesefeaturesarecalculated
intra-case
,i.e.,consideringonlydata
fromthegivencase,featurescouldalsobeextracted
inter-case
,i.e.,basedonall
casesthatwereactiveatthetimetheeventwasperformed.Accordingly,weextract
the
numberofopencases
(howmanycaseswereopenduringtheexecutionofthe
event)asanotherfeature.tstrategiesforextractinginter-casefeaturesare
discussedin[32].
Eachcategoricalattributehasanumberofpossiblevalues,called
levels
.
Forsomeattributes,thenumberofdistinctlevelscanbeverylarge,withsome
ofthelevelsappearingonlyinafewcases.Inordertoavoidthedimensionality
explosionoftheinputdataset,wesetthecategorylevelsthatappearin10orless
samplestoacommonlevel
other
.
Duetothefactthateventlogsconsistofdatathatarerecordedautomatically
byinformationsystemsduringtheexecutionoftasksofaprocess,thereisnone
orverylittle
missingdata
inthetraditionalsense.However,itiscommonthat
teventscarrytdatapayloads,resultinginasituationwheresome
attributevaluesforagiveneventcanbe\missing""duetothefactthattheyare
notapplicableforthatparticularevent.Thiscanbecausedbymainlytworeasons.
Firstly,inmosteventlogs,aneventrecordsonlythevaluesofdataattributesthat
werechangedduringthatparticularevent.Therefore,inordertodeterminethe
valueofanattributeatthepointwhereaneventoccurred,weneedtosearchfor
thelatesteventinthetrace(ortracewherethevalueoftheattributein
questionchanged(ortheeventifnochangepointisfound).Forinstance,the
nameoftheresourceinvolvedintheexecutionofanactivityinacaseisoften
loggedonlyiftheresourcehaschangedsincethepreviousevent.Insuchcases,
wesearchfortheclosestprecedingeventinthesamecasewheretheresource
namewaspresentandusethesamevalueinthefeaturevectorproducedforthe
currentevent.Secondly,tactivitiescanproducettypesofdata.
Forinstance,inaloanapplicationprocess,informationaboutthemadeto
thecustomerbecomesavailableonlywhenanismade(beforethat,no
norinformationaboutitexists).Similarly,inacollectionprocess,theamount
ofpaymentisonlyavailableforpaymentevents.Theseexamplesconstituteaform
of
legitimatelymissingdata
[25]or
missingdatathatisoutofscope
[31].Inour
experiments,wedecidedtoaddresssuchcasesbyaddinganadditionalfeature(for
eachdataattribute)tothedataset,indicatingwhetherthegivenvalueispresent
foragiveneventornot.Thevalueoftheattributeitselfwassetto0ifnotpresent.
Ineventlogswhereinformationisavailableaboutcasecompletion,we
outincompletecasesinordertonotmisleadtheAlso,wecuteachtrace
3
Preprocesseddata:
https://github.com/irhete/stability-predictive-monitoring
TemporalStabilityinPredictiveProcessMonitoring13
Table2:Datasetstatistics.
posclassmedmaxtrunc.
datasetname#tracesratiolengthlengthlength#events
bpic2012
accepted46850.483517540155783
bpic2012
declined46850.173517540155783
bpic2012
cancelled46850.353517540155783
bpic2017
accepted314130.413518020624352
bpic2017
refused314130.123518020624352
bpic2017
cancelled314130.473518020624352
sepsis
cases
17820.14141852912189
sepsis
cases
27820.141360139178
sepsis
cases
37820.14131852211056
hospital
billing775250.0562178404721

1296150.4642010460462
production2200.53978232275
beforetheeventthatwasusedtothelabel.Forinstance,inthe
production
log,thetracesarecutimmediatelybeforethenumberofrejectedworkorders
becomeslargerthanzero.
Thedatasets(afterpreprocessing)exhibitntcharacteristicspresented
inTable2.Firstly,thenumberofcasesvariesfrom220inthe
production
logto
129615inthe
tr

log.Classimbalanceisthemostsevereinthe
hospi-
tal
billing
dataset,whereonlyabout5%ofcasesareofthepositiveclass.On
theotherhand,theclassesarealmostperfectlybalancedinthe
production
,
traf-


,
bpic2017
cancelled
,and
bpic2012
accepted
datasets.Themediantrace
lengthisthesmallestin
tr

,wherehalfofthecasesconsistof4orless
events,whileBPIC2012andBPIC2017variantshavethelongesttraces(median
length35).Tracelengthscanbeveryheterogenous.Forinstance,whiletheme-
diantracelengthin
hospital
billing
is6,themaximumtracelengthis217.Our
experimentshaveshownthatusingtheoriginallengthforverylongtracescauses
theperformanceofthetodecrease,aswellashindersthereadabilityof
theplots(seeFigures9and10intheAppendix).Therefore,wehavedecidedto
usetruncatedversionsoflongsequences.Wedeterminedthe
truncatedlength
in-
dependentlyforeachdatasetbasedonthefollowingcriteria.Firstly,thesequence
wastruncatedfromthelengthwhere90%oftheminorityclasssequenceshaveal-
readycompleted(andnotavailableanymorefortrainingandevaluation),asboth
trainingandevaluationofthewouldbeunreliablewhenhavingveryfew
sequencesfromoneoftheclasses.Secondly,asintheBPIC2012andBPIC2017
variantsthesignalstartstoconvergearound40and20events,respectively,we
furthertruncatedthesequencestotheselengthsforcomputationalreasons.For
histogramsofcaselengthsinbothclasses,seeFig.7inAppendix.
4.3ExperimentalSetup
Weapplyatemporalsplitfordividingcasesintotrainingandtestsets.Namely,
thecasesareorderedaccordingtothestarttimeandthe80%isusedfor
14IreneTeinemaaetal.
trainingandvalidatingthemodels,whiletheremaining20%isusedtoevaluate
theperformance.Notethat,usingthisapproach,someeventsinthetrainingcases
mightstilloverlapwiththetestperiod.Asweareusinganinter-casefeature(the
numberofopencases),whichconsidersdatafromallcasesactiveatagiventime,
thiscouldintroduceabiasintoourevaluation.Inordertoavoidthat,wecutthe
trainingcasessothateventsthatoverlapwiththetestperiodarediscarded.
Toachievethebestperformancewitheachmethod,thehyperparametersof
theneedtobeoptimizedseparatelyforeachmethodanddataset.To
thisend,wefurthersplitthetrainingcasesrandomlyinto80%trainingand20%
validationdata.Wetrainthemodelswithtparametersettingsonthe
trainingsetandselectthemodelthatperformedbestonthevalidationset.Inthe
caseofRFandXGBoost,thebestmodelsareselectedbasedontheAUConthe
validationdata.Duringtraining,LSTMsoptimizebinarycrossentropy,whichis
whyweselectedthebestparametersaccordingtothismetric.
WhileRFtendstoperformwellevenwithlittleoptimization,XGBoostand
LSTMaremuchmoresensitivetohyperparameterselection.Also,thenumberof
hyperparametersislargeronthelasttwomethods,makinggridsearchinfeasible.
Inordertokeepthemethodscomparable,wedecidedtousethesameoptimization
procedureforallofthem,i.e.,randomsearch[2]with16iterations.Asabasis
forrandomsearch,wespforeachhyperparameteradistributiontosample
values,aswellastheboundsforthevalues(seeTable5inAppendix).Theselected
valuesforeachhyperparameterarepresentedinTables6{10inAppendix.The
activationfunctionforLSTMisalwaysto
sigmoid
inourexperimentsand
thenumberofepochsto50.
4.4Results
TheexperimentswereperformedusingPythonlibrariesScikit-Learn
4
(RFand
XGBoost)andKeras
5
withTheano
6
backend(LSTM).
4.4.1Generalcomparison
Figure2showsthepredictionaccuracy(AUC)acrosstlengths.For
instance,length10meansthatthepredictionsweremadebasedonthe
10eventsinacase.Oneobservationisthatthem(
RF
idx
mul
and
XGB
idx
mul
)canyieldahighaccuracyonsome(especiallyontheshorter
ones),butatthesametimetheresultsareveryvolatile,causingtheAUCtodrop
unexpectedly.Forinstance,see
XGB
idx
mul
with
prefix
=24in
sepsis
cases
1
or
RF
idx
mul
with
prefix
=15in
sepsis
cases
3
.Onlongtheindex-based
encodingapproaches(bothmandsinglewithpadding)tend
toperformworsethantheothermethods.Exceptionsaresomesmallerdatasets,
namely,
production
and
sepsis
cases
3
,where
XGB
idx
pad
performswelloverall
lengths.
4
http://scikit-learn.org/
5
https://github.com/fchollet/keras/
6
http://www.deeplearning.net/software/theano/
TemporalStabilityinPredictiveProcessMonitoring15
Fig.2:Predictionaccuracy(measuredintermsofAUC).
tpatternscanbeseenfor
LSTM
.Firstly,inthecaseof
bpic2012
vari-
ants,theaccuracyislowerforshorterbutaftertherelevantsignalcomes
in(aroundlengthbetween12to20),themodelisabletomakeuseofit
betterthantheothermethods,reachingthehighestAUConlongSec-
ondly,while
LSTM
oftendoesnotachievethehighestAUC,itisalwaysreasonably
stable,inthesensethatnosuddendropsinAUCoccurinanylength.
Thesinglewithaggregationencoding(
RF
agg
and
XGB
agg
)per-
formwellonbothshortandlongAlthoughinsomelengthsthey
areoutperformedbytheindex-basedencodingmethods,theyareoverallmore
stable.Inparticular,thesemethodsaresomewhatmorevolatilethan
LSTM
,but
theyusuallydonotundergostrongfallsinAUCasthemForex-
ample,see
sepsis
cases
1
and
sepsis
cases
3
where
RF
agg
and
XGB
agg
retain
highaccuracyonlongwhile
RF
idx
mul
and
XGB
idx
mul
becomemore
volatile.
16IreneTeinemaaetal.
Wecanalsoobserve,inFig.2,that,insomecases,theAUCstartstodecline
asthelengthincreases,whichiscounter-intuitivesincethelongerthepre-
themoreinformationthehastomakeadecision.Forinstance,this
happensinthe
bpic2012
variants,
sepsis
cases
2
,
sepsis
cases
3
,and
tr

.
Toinvestigatethisphenomenon,weouttheshortcases,leavingonlythose
thatreachthemaximumconsideredlength,andcalculatedtheAUConly
forthoselongcases.Weobserved(Fig.8inAppendix)thattheAUCdoesnotun-
dergoadecreasewhenconsideringonlythelongcases,butinsteadkeepsincreasing
(orstaysatthesamelevel).TheseresultssuggestthatthedecreaseinAUCisnot
duetothestartingtoperformworseonlongRather,thisde-
creaseisduetothefactthatforshortercases,itiseasiertomakepredictionssince
theyareinitiallyclosertocompletion.Therefore,afterthesecaseshavecompleted
andtheyareexcludedfromthecalculationoftheAUC,theperformanceofthe
seemstodecay.
ThetemporalstabilityisplottedinFig.3.In11outof12datasets,thehigh-
eststabilityisachievedby
XGB
idx
pad
,usuallyfollowedby
XGB
agg
andthen
either
LSTM
or
RF
idx
pad
.Ingeneral,RFachievesslightlylowerstabilitythan
itsXGBoostcounterparts.Themapproachesalwayshavelowertem-
poralstabilitythansinglewhichisnotsurprising.Namely,astheRF
andXGBoostdonotconsiderthetemporalrelationsbetweentheinput
featuresand,instead,assumethemtobei.i.d.,thevariancebetweenc
builtforoflength
l
and
l
+1canbeveryhighand,thus,thepredictions
madefortwosuccessivecanbecompletelyuncorrelated.Thisdiscussion
answersRQ1.
4.4.2Increasingtheinter-runstabilityduringvalidation
Tables3and4presenttheoverallAUC(weightedaverageoveralllengths)
andthetemporalstabilityforthesingleclawithaggregationencodingwith
RFandXGBoostusingthreehyperparameteroptimizationapproaches:1)valida-
tionbasedonAUCoverasinglerunwitheachparametersetting(
RF
,
XGB
),2)
validationbasedonaverageAUCover5runswitheachparametersetting(
RF
5
,
XGB
5
),and3)validationbasedonacombinedmeasureofmeanAUCandinter-
runstabilityover5runswitheachparametersetting(
RF
5
S
,
XGB
5
S
).
TheresultsshowthatselectingthebestparametersaccordingtoAUCover5
runsusually(in7outof12cases)increasestheAUConthetestsetascompared
toselectingthembasedonasinglerun,whilethetemporalstabilityisincreased
almostalways(theonlyexceptionsare
tr

and
sepsis
cases
3
).Optimiz-
ingthecombinedmetricover5runsfurtherimprovesthetemporalstability,but
achievesslightlylessconsistentimprovementinAUC.Thevalidationover5runs
increasesthetemporalstabilityalsoforXGBoost.Infact,thehighesttemporal
stabilityisachievedbyeither
XGB
5
or
XGB
5
S
inthemajorityofthedatasets
ascanbeseeninTable4.TheAUCinthecaseofXGBoostremainsatthesame
levelorevendecreasesascomparedtovalidatingoverasinglerun.ThebestAUC
isoftenachievedby
RF
5
or
RF
5
S
(Table3).
ToanswerRQ2,wefoundthatvalidatingover5runsinsteadofasinglerun,
ingeneral,resultsinimprovementofAUCand/ortemporalstability.However,
theimprovementsarerathersmallinvalueandcomeattheexpenseofrunning5
timesmoreexperimentsduringthehyperparameteroptimizationphase.
TemporalStabilityinPredictiveProcessMonitoring17
Fig.3:Temporalstability.
4.4.3Decreasingtheintra-casepredictionvolatilityduringprediction
Fig.4showsthatdecreasingthepredictionvolatilityviaexponentialsmoothing
consistentlyimprovesthetemporalstability.Thelargerthesmoothingparameter

,thelargertheincreaseintemporalstability.Themethodsthatbthe
mostfromsmoothingaremulticl(
RF
idx
mul
and
XGB
idx
mul
).Being
initiallylessstable,smoothinghelpsthesemethodstoachieveasimilarlevelof
temporalstabilityastheothermethods.Insomecases,themrseven
overtaketheothermethodsonlarge

(see
bpic2012
variants).Also,
RF
agg
gains
relativelymorefromsmoothingthanitsXGBoostcounterpartand
LSTM
.For
instance,see
bpic2012
variantsor
production
,where
RF
agg
bypasseseither
LSTM
or
XGB
agg
.
InFig.5,theoverallAUCisplottedagainstthe

parameter.Weobserve
thatinmostcasessmoothingdecreasestheAUC.Thereasonforthisisthat
asthesmoothedestimateiscautiousaboutthemostrecentprediction,thetrue
18IreneTeinemaaetal.
Table3:ofmaximizinginter-runstabilityandaccuracyonAUC.
datasetRFRF
5RF
5
SXGBXGB
5XGB
5
S
bpic2012
accepted
0
:
6900
:
690
0
:
6740
:
6800
:
6770
:
677
bpic2012
cancelled
0
:
700
0
:
6910
:
6880
:
6970
:
6900
:
695
bpic2012
declined
0
:
610
0
:
6090
:
6090
:
6050
:
5990
:
603
bpic2017
accepted0
:
834
0
:
843
0
:
8390
:
8340
:
8410
:
831
bpic2017
cancelled0
:
803
0
:
813
0
:
8120
:
8100
:
8110
:
812
bpic2017
refused0
:
8050
:
816
0
:
820
0
:
8020
:
8100
:
801
hospital
billing0
:
6710
:
6620
:
665
0
:
731
0
:
7270
:
724
production
0
:
707
0
:
5400
:
5400
:
5650
:
5630
:
563
sepsis
cases
10
:
611
0
:
6380
:
638
0
:
5120
:
4900
:
490
sepsis
cases
20
:
750
0
:
781
0
:
7630
:
7610
:
7420
:
683
sepsis
cases
30
:
693
0
:
7470
:
747
0
:
7380
:
7120
:
712

0
:
667
0
:
6810
:
681
0
:
6610
:
6610
:
660
Table4:ofmaximizinginter-runstabilityandaccuracyontemporalsta-
bility.
datasetRFRF
5RF
5
SXGBXGB
5XGB
5
S
bpic2012
accepted0
:
9710
:
9700
:
9740
:
9780
:
988
0
:
994
bpic2012
cancelled0
:
9720
:
9700
:
9770
:
9820
:
991
0
:
996
bpic2012
declined0
:
9890
:
9880
:
9880
:
9930
:
993
0
:
996
bpic2017
accepted0
:
9590
:
9740
:
9750
:
976
0
:
988
0
:
977
bpic2017
cancelled0
:
9600
:
9730
:
9740
:
975
0
:
989
0
:
976
bpic2017
refused0
:
9840
:
9910
:
9920
:
993
0
:
998
0
:
992
hospital
billing0
:
9780
:
9760
:
9770
:
9870
:
980
0
:
981
production0
:
9520
:
9390
:
9390
:
930
0
:
9990
:
999
sepsis
cases
10
:
9880
:
9930
:
9930
:
999
1
:
0001
:
000
sepsis
cases
20
:
9920
:
9900
:
9930
:
9950
:
994
1
:
000
sepsis
cases
30
:
9840
:
9820
:
982
0
:
992
0
:
9870
:
987


0
:
773
0
:
7690
:
7690
:
7150
:
6970
:
702
signalinthedataoccursafteralag.However,theAUCdoesnotalwaysdecrease
withsmoothing.Forsmallerlogs(
production
and
sepsis
cases
variants),theAUC
remainsalmostunchangedbysmoothingorevenincreases.Alsointhelargerlogs,
asmallamountofsmoothingcanhelptoincreasetheAUC(e.g.,see
XGB
idx
mul
in
bpic2017
refused
).Themethodsthatbthemostfromsmoothingareagain
themWhilenotthemostaccuratemethodsbeforepostprocessing,
theyoftenovertaketheothermethodswithhighlevelsofsmoothing.
TofurtherunderstandtherelationshipbetweenAUCandtemporalstability,let
uslookatFig.6,wherethesetwometricsareplottedagainsteachother(eachdot
correspondstoAUCandtemporalstabilityobtainedviasmoothingwithapartic-
ularvalueof

).Weseethat
RF
idx
mul
and
XGB
idx
mul
changeconsiderablyin
thedirectionfromlefttoright,indicatingthattheyareinitiallyunstablebutim-
provesubstantiallywithsmoothing.Atthesametime,theirchangeintheup-down
directionissmall,meaningthattheAUCisnotmuch.Theleast
bysmoothingisthe
XGB
idx
pad
method.Forinstance,in
bpic2012
declined
and
sepsis
cases
2
boththeaccuracyandthetemporalstabilityremainalmostcon-
stant.Wealsoobservethat,althoughthe
LSTM
methodinthesmallerlogsis
initiallystableanddoesnotgaininstabilitywhensmoothing,itdoesbin
termsofAUCinthecasesof
production
,
sepsis
cases
2
,and
sepsis
cases
3
.The
XGB
agg
methodoftenappearsinthetoprightcorner,dominatingtheothertech-
TemporalStabilityinPredictiveProcessMonitoring19
Fig.4:Temporalstabilityacrosstlevelsofsmoothing.
niquesintermsofbothaccuracyandstability(see,forinstance,
bpic2012
cancelled
,
bpic2017
cancelled
,
hospital
billing
,and
sepsis
cases
3
).
ToanswerRQ3,exponentialsmoothinghelpstoincreasethetemporalstability,
butusuallyattheexpenseofloweraccuracy.Exceptionsare
RF
idx
mul
and
XGB
idx
mul
,wheresmoothingoftenincreasesbothtemporalstabilityandAUC.
5ConclusionandFutureWork
Weintroducedthenotionoftemporalstabilityforpredictiveprocessmonitoring.
Temporalstabilitycharacterizeshowmuchsuccessivepredictionscoresobtained
forthesamecase(sequenceofevents)fromeachother.Foratemporally
stablesuchsuccessivepredictionscoresaresimilartoeachother,result-
inginasmoothtimeseries,whileincaseofanunstabletheresulting
timeseriesisvolatile.Weevaluatedthetemporalstabilityof7existingpredic-
20IreneTeinemaaetal.
Fig.5:Overallpredictionaccuracyacrosstlevelsofsmoothing.
tiveprocessmonitoringmethods,includingsingleandmusingRF,
XGBoost,andLSTM.Theexperimentsweredoneon12predictiontasksformu-
latedon6real-lifepubliclyavailabledatasets.Wefoundthatthehighesttemporal
stabilitywasachievedbyasingleapproachwithXGBoost(usingeither
aggregationorindex-basedencoding),followedbyLSTM.
Weinvestigatedtheofhyperparameteroptimizationontemporalsta-
bility.Wecomparedtheconstructedafterselectingthebestpa-
rametersbasedon1)AUCoverasinglerunforeachparametersetting,2)AUC
over5runsforeachsetting,3)combinedAUCandinter-runstabilityover5runs
foreachsetting.Theresultsshowthatchoosingtheparametersbasedon5runs
canincreasebothAUCandtemporalstability.However,theimprovementissmall
andissubjecttotheof5timesmorecomputationsduringvalidation.
Finally,weexploredhowexponentialsmoothingtheAUCandtemporal
stability.Weconcludedthatsmoothingcanbeareasonableapproachforadjusting
thepredictionsinapplicationswheretemporalstabilityisimportantattheexpense
TemporalStabilityinPredictiveProcessMonitoring21
Fig.6:Temporalstabilityvs.predictionaccuracy.
ofachievingslightlysmallerAUC.Moreover,weobservedthatthem
benthemostfromsmoothing,insomecasesevenincreasingboththetemporal
stabilityandtheAUCatthesametime.Therefore,whenhightemporalstability
isrequired,itmaybereasonabletouseamapproachwithsmoothing,
achievingstableresultswithlittleornolossinaccuracy.
Asfuturework,weplantodevelopmorerobustnotionsoftemporalstability
thatwouldstillrequiremostofthesuccessiveinpredictionstobesmall,
butnotpenalizetheforchangingthepredictionwhenaneventwitha
relevantsignalarrives.Wewillexamineiftheworksonearlysequence
tioncouldbehelpfulindevelopinganadaptivesmoothingmethodthatdecreases
volatilityonsubsequenceswithoutsuppressingtherelevantsignal.Furthermore,
thenotionoftemporalstabilitycouldbeextendedtootherpredictiontasks,such
asmulti-classpredictionsandregression.Forinstance,temporalstabilitycould
alsobeinvestigatedinthecontextofpredictingtheremainingtimeofanongo-
ingcase.Whileseveralmethodshavebeendevelopedwiththegoalofproviding
22IreneTeinemaaetal.
accurateremainingtimeestimations,using,e.g.,non-parametricregression[8],
supportvectorregression[28],orLSTMneuralnetworks[34],noneoftheseworks
hasconsideredthestabilityofthepredictions.Anotheravenueforfutureworkis
toincorporatethenotionofstabilityintothetrainingphaseoftheFor
instance,incaseofneuralnetworksthiscouldbeachievedbyadjustingtheloss
functiontotakeintoaccountboththeaccuracyandthestabilityofthepredictions.
Acknowledgments
.ThisresearchwaspartlyfundedbytheEstonianResearch
Council(grantIUT20-55).
References
1.
vanderAalstWM(2016)Processmining:datascienceinaction.Springer
2.
BergstraJ,BengioY(2012)Randomsearchforhyper-parameteroptimization.
JournalofMachineLearningResearch13(Feb):281{305
3.
BousquetO,A(2002)Stabilityandgeneralization.JournalofMa-
chineLearningResearch2(Mar):499{526
4.
BreimanL(1996)Baggingpredictors.Machinelearning24(2):123{140
5.
BreimanL(2001)Randomforests.Machinelearning45(1):5{32
6.
ChenT,GuestrinC(2016)XGBoost:Ascalabletreeboostingsystem.In:
Proceedingsofthe22ndACMSIGKDDinternationalconferenceonknowledge
discoveryanddatamining,ACM,pp785{794
7.
DiFrancescomarinoC,DumasM,MaggiFM,TeinemaaI(2017)Clustering-
basedpredictiveprocessmonitoring.IEEETransServicesComputing
8.
vanDongenBF,CrooyRA,vanderAalstWM(2008)Cycletimeprediction:
WhenwillthiscasebeIn:OTMConfederatedInternational
Conferences""OntheMovetoMeaningfulInternetSystems"",Springer,pp
319{336
9.
DumasM,LaRosaM,MendlingJ,ReijersHA(2013)FundamentalsofBusi-
nessProcessManagement.Springer
10.
A,EvgeniouT,PontilM(2005)Stabilityofrandomizedlearning
algorithms.JournalofMachineLearningResearch6(Jan):55{79
11.
EvermannJ,RehseJR,FettkeP(2017)Predictingprocessbehaviourusing
deeplearning.DecisionSupportSystems
12.
Fandez-DelgadoM,CernadasE,BarroS,AmorimD(2014)Doweneed
hundredsoftosolverealworldproblems.Journalof
MachineLearningResearch15(1):3133{3181
13.
GuoC,PleissG,SunY,WeinbergerKQ(2017)Oncalibrationofmodern
neuralnetworks.arXivpreprintarXiv:170604599
14.
LakshmananGT,DuanS,KeyserPT,CurberaF,KhalafR(2010)Predictive
analyticsforsemi-structuredcaseorientedbusinessprocesses.In:International
ConferenceonBusinessProcessManagement,Springer,pp640{651
15.
deLeoniM,vanderAalstWM,DeesM(2016)Ageneralprocessmining
frameworkforcorrelating,predictingandclusteringdynamicbehaviorbased
oneventlogs.InformationSystems56:235{257
16.
LeontjevaA,ConfortiR,DiFrancescomarinoC,DumasM,MaggiFM(2015)
Complexsymbolicsequenceencodingsforpredictivemonitoringofbusiness
processes.In:InternationalConferenceonBusinessProcessManagement,
Springer,pp297{313
TemporalStabilityinPredictiveProcessMonitoring23
17.
LinYF,ChenHH,TsengVS,PeiJ,etal(2015)Reliableearlyon
multivariatetimeserieswithnumericalandcategoricalattributes.In:PAKDD
(1),pp199{211
18.
LiuCB,ChamberlainBP,LittleDA,Cardoso
^
A(2017)Generalisingrandom
forestparameteroptimisationtoincludestabilityandcost.In:JointEuro-
peanConferenceonMachineLearningandKnowledgeDiscoveryinDatabases,
Springer,pp102{113
19.
MaggiFM,DiFrancescomarinoC,DumasM,GhidiniC(2014)Predictive
monitoringofbusinessprocesses.In:InternationalConferenceonAdvanced
InformationSystemsEngineering,Springer,pp457{472
20.
Marquez-ChamorroAE,ResinasM,Ruiz-CortesA(2017)Predictivemonitor-
ingofbusinessprocesses:asurvey.IEEETransactionsonServicesComputing
21.
MetzgerA,LeitnerP,IvanovicD,SchmiedersE,FranklinR,CarroM,Dust-
darS,PohlK(2015)Comparingandcombiningpredictivebusinessprocess
monitoringtechniques.IEEETransSystems,Man,andCybernetics:Systems
45(2):276{290
22.
MoriU,MendiburuA,KeoghE,LozanoJA(2017)Reliableearly
oftimeseriesbasedondiscriminatingtheclassesovertime.DataMiningand
KnowledgeDiscovery31(1):233{263
23.
Niculescu-MizilA,CaruanaR(2005)Predictinggoodprobabilitieswithsu-
pervisedlearning.In:Proceedingsofthe22ndinternationalconferenceon
Machinelearning,ACM,pp625{632
24.
OlsonRS,LaCavaW,MustahsanZ,VarikA,MooreJH(2017)Data-
drivenadviceforapplyingmachinelearningtobioinformaticsproblems.arXiv
preprintarXiv:170805070
25.
OsborneJ(2013)Dealingwithmissingorincompletedata:Debunkingthe
mythofemptiness.In:Bestpracticesindatacleaning:Acompleteguideto
everythingyouneedtodobeforeandaftercollectingyourdata,SagePubli-
cationsThousandOaks,CA,pp105{138
26.
ParrishN,AndersonHS,GuptaMR,HsiaoDY(2013)Classifyingwithcon-
fromincompleteinformation.TheJournalofMachineLearningRe-
search14(1):3561{3589
27.
PlattJ,etal(1999)Probabilisticoutputsforsupportvectormachinesand
comparisonstoregularizedlikelihoodmethods.Advancesinlargemarginclas-
10(3):61{74
28.
PolatoM,SperdutiA,BurattinA,deLeoniM(2014)Data-awareremaining
timepredictionofbusinessprocessinstances.In:NeuralNetworks(IJCNN),
2014InternationalJointConferenceon,IEEE,pp816{823
29.
Rogge-SoltiA,WeskeM(2013)Predictionofremainingserviceexecutiontime
usingstochasticpetrinetswitharbitrarydelays.In:InternationalCon-
ferenceonService-OrientedComputing(ICSOC),Springer,pp389{403
30.
SantosT,KernR(2016)Aliteraturesurveyofearlytimeseries
anddeeplearning.In:SAMI@iKNOW
31.
SchaferJL,GrahamJW(2002)Missingdata:ourviewofthestateoftheart.
Psychologicalmethods7(2):147
32.
SenderovichA,DiFrancescomarinoC,GhidiniC,JorbinaK,MaggiFM(2017)
Intraandinter-casefeaturesinpredictiveprocessmonitoring:Ataleoftwo
dimensions.In:InternationalConferenceonBusinessProcessManagement,
Springer,pp306{323
24IreneTeinemaaetal.
33.
TaxN,VerenichI,LaRosaM,DumasM(2017)Predictivebusinessprocess
monitoringwithlstmneuralnetworks.In:InternationalConferenceonAd-
vancedInformationSystemsEngineering,Springer,pp477{492
34.
TaxN,VerenichI,RosaML,DumasM(2017)Predictivebusinessprocess
monitoringwithLSTMneuralnetworks.In:InternationalConferenceonAd-
vancedInformationSystemsEngineering(CAiSE),Springer,pp477{492
35.
TeinemaaI,DumasM,LaRosaM,MaggiFM(2017)Outcome-oriented
predictiveprocessmonitoring:Reviewandbenchmark.arXivpreprint
arXiv:170706766
36.
XingZ,PeiJ,DongG,YuPS(2008)Miningsequenceforearly
prediction.In:Proceedingsofthe2008SIAMinternationalconferenceondata
mining,SIAM,pp644{655
37.
XingZ,PeiJ,PhilipSY(2012)Earlyontimeseries.Knowledge
andinformationsystems31(1):105{127
TemporalStabilityinPredictiveProcessMonitoring25
Appendix
Fig.7:Caselengthhistogramsforpositiveandnegativeclasses.
26IreneTeinemaaetal.
Fig.8:Predictionaccuracyonlongcasesonly.
Table5:Hyperparametersanddistributionsusedinoptimizationviarandom
search.
ParameterDistributionValues
RF
#estimators(
n
est
)Uniforminteger
x
2
[150
;
1000]
Maxfeatures(
mf
)Log-uniform
x
2
[0
:
01
;
0
:
9]
XGBoost
#estimators(
n
est
)Uniforminteger
x
2
[150
;
1000]
Learningrate(
lr
)Uniform
x
2
[0
:
01
;
0
:
07]
Subsample(
subs
)Uniform
x
2
[0
:
5
;
1]
Maxtreedepth(
md
)Uniforminteger
x
2
[3
;
9]
Colsamplebytree(
cb
)Uniform
x
2
[0
:
5
;
1]
Minchildweight(
mcw
)Uniforminteger
x
2
[1
;
3]
LSTM
#hiddenlayers(
n
lay
)Categorical
x
2f
1
;
2
;
3
g
#unitsinhiddenlayer(
n
hid
)Log-uniforminteger
x
2
[10
;
150]
Initiallearningrate(
lr
)Log-uniform
x
2
[0
:
000001
;
0
:
0001]
Batchsize(
batch
)Categorical
x
2f
8
;
16
;
32
;
64
g
Dropout(
drop
)Uniform
x
2
[0
;
0
:
3]
Optimizer(
opt
)Categorical
x
2f
RMSProp;NAdam
g
Table6:Optimizedhyperparameters(RF).
RF
aggRF
idx
padRF
idx
mulRF
idx
mulRF
idx
mulRF
idx
mul

datasetn
estmfn
estmfn
estmfn
estmfn
estmfn
estmf
production7690.027690.024680.355560.739440.068330.86
sepsis
cases
18730.025370.138440.038400.794080.257390.52
sepsis
cases
23130.258730.029900.65670.293160.07--
sepsis
cases
35370.133130.252690.66230.258860.326140.02

8470.278470.275930.629120.382060.22--
bpic2012
accepted8010.079580.014740.242730.852870.032730.85
bpic2012
cancelled3240.069580.012730.857510.376730.036350.75
bpic2012
declined8010.076750.356350.756350.752730.852870.03
bpic2017
accepted5110.148280.144450.176090.338050.416850.07
bpic2017
refused5110.148280.148630.075600.245370.45370.4
bpic2017
cancelled5110.148280.148050.411520.333620.447820.43
hospital
billing5490.132770.229690.019690.01----
TemporalStabilityinPredictiveProcessMonitoring27
Table7:Optimizedhyperparametersforsingle(XGBoost).
XGB
aggXGB
idx
pad
datasetn
estlrsubsmdcbmcwn
estlrsubsmdcbmcw
production2240.010.5350.9516990.070.7780.632
sepsis
cases
13550.020.5930.9123990.060.6880.872
sepsis
cases
29710.040.7380.7324760.040.5240.721
sepsis
cases
33550.020.5930.9129180.020.7880.971

7730.040.7570.7127730.040.7570.712
bpic2012
accepted1560.010.7880.6117100.010.5170.781
bpic2012
cancelled4450.030.950.6112910.050.8570.792
bpic2012
declined3630.050.830.6523630.050.830.652
bpic2017
accepted2150.030.7540.6818300.010.6250.842
bpic2017
refused2150.030.7540.6818300.010.6250.842
bpic2017
cancelled1870.040.7640.7918300.010.6250.842
hospital
billing2150.030.7540.6817350.060.7130.541
Table8:Optimizedhyperparametersform(XGBoost).
XGB
idx
mul,XGB
idx
mul,
datasetn
estlrsubsmdcbmcwn
estlrsubsmdcbmcw
production2280.030.6330.9814360.030.9830.852
sepsis
cases
19180.020.7880.9711870.060.8270.852
sepsis
cases
29710.040.7380.7321870.060.8270.852
sepsis
cases
37640.020.9340.5217120.010.8950.921

9770.020.5270.8216150.030.7360.592
bpic2012
accepted3940.060.9880.522910.050.8570.792
bpic2012
cancelled8190.060.9940.8811900.020.8351.01
bpic2012
declined3630.050.830.6524450.030.950.611
bpic2017
accepted7330.020.9130.5717330.020.9130.571
bpic2017
refused9240.050.9680.7617330.020.9130.571
bpic2017
cancelled2150.030.7540.6819240.050.9680.761
hospital
billing5840.020.8160.6112150.030.7540.681
XGB
idx
mul,XGB
idx
mul,
datasetn
estlrsubsmdcbmcwn
estlrsubsmdcbmcw
production9210.030.5260.8219090.060.8660.892
sepsis
cases
13990.060.6880.8724690.050.9170.512
sepsis
cases
29710.040.7380.732------
sepsis
cases
31870.060.8270.8523850.030.7750.831

9720.030.8330.851------
bpic2012
accepted7200.010.8970.9314450.030.950.611
bpic2012
cancelled5260.050.7970.8821900.020.8351.01
bpic2012
declined1560.010.7880.6114450.030.950.611
bpic2017
accepted2150.030.7540.6818310.020.5950.841
bpic2017
refused2150.030.7540.6812150.030.7540.681
bpic2017
cancelled7330.020.9130.5718300.010.6250.842
hospital
billing------------
28IreneTeinemaaetal.
Table9:Optimizedhyperparameters(LSTM).
LSTM
datasetn
layn
hidlrbatchdropopt
production2275e-05160.05adam
sepsis
cases
12273e-05320.19nadam
sepsis
cases
21804e-05160.18nadam
sepsis
cases
32464e-0580.15nadam

21007e-05160.27nadam
bpic2012
accepted3193e-0580.18nadam
bpic2012
cancelled2212e-05320.25nadam
bpic2012
declined1202e-05320.02nadam
bpic2017
accepted1142e-0580.03nadam
bpic2017
refused1104e-05320.09nadam
bpic2017
cancelled2309e-05640.11rmsprop
hospital
billing31445e-05640.04rmsprop
Table10:Optimizedhyperparameters(combinedinter-runstabilityandAUC).
RF
5RF
5
SXGB
5XGB
5
S
datasetn
estmfn
estmfn
estlrsubsmdcbmcwn
estlrsubsmdcbmcw
production9270.819270.812310.020.9230.512310.020.9230.51
sepsis
cases
18580.18580.15860.010.7630.9715860.010.7630.971
sepsis
cases
22530.095170.158120.060.7680.729640.040.8370.991
sepsis
cases
37640.117640.111540.030.5770.5621540.030.5770.562

9570.249570.244240.050.7180.7626690.020.9940.671
bpic2012
accepted5810.168820.414820.020.5530.7212860.010.6640.792
bpic2012
cancelled3640.089790.34550.020.7660.512160.020.6770.681
bpic2012
declined8200.068200.065050.060.5660.9412570.030.8630.671
bpic2017
accepted1690.413590.462840.020.7870.5714990.030.6930.971
bpic2017
refused3460.244100.549330.010.6560.8713250.060.9230.81
bpic2017
cancelled3010.263000.411610.010.8960.5122760.030.7940.721
hospital
billing9000.079690.089210.020.7570.9927300.010.6380.972
TemporalStabilityinPredictiveProcessMonitoring29
Fig.9:Predictionaccuracyonoriginal(nottruncated)traces.
Fig.10:Temporalstabilityonoriginal(nottruncated)traces.
"
69,SGM: Sequence Generation Model for Multi-label Classification,http://arxiv.org/pdf/1806.04822v3.pdf,https://github.com/lancopku/SGM,"SGM:SequenceGenerationModelforMulti-Label
PengchengYang
1,2
,XuSun
1,2
,WeiLi
2
,ShumingMa
2
,WeiWu
2
,HoufengWang
2
1
DeepLearningLab,BeijingInstituteofBigDataResearch,PekingUniversity
2
MOEKeyLabofComputationalLinguistics,SchoolofEECS,PekingUniversity
f
yang
pc,xusun,liweitj47,shumingma,wu.wei,wanghf
g
@pku.edu.cn
Abstract
Multi-labelonisanimportantyetchallengingtaskinnaturallanguageprocessing.It
ismorecomplexthansingle-labelinthatthelabelstendtobecorrelated.Existing
methodstendtoignorethecorrelationsbetweenlabels.Besides,differentpartsofthetextcan
contributedifferentlytopredictingdifferentlabels,whichisnotconsideredbyexistingmodels.
Inthispaper,weproposetoviewthemulti-labeltaskasasequencegeneration
problem,andapplyasequencegenerationmodelwithanoveldecoderstructuretosolveit.
Extensiveexperimentalresultsshowthatourproposedmethodsoutperformpreviousworkby
asubstantialmargin.Furtheranalysisofexperimentalresultsdemonstratesthattheproposed
methodsnotonlycapturethecorrelationsbetweenlabels,butalsoselectthemostinformative
wordsautomaticallywhenpredictingdifferentlabels.
1
1Introduction
Multi-label(MLC)isanimportanttaskintheofnaturallanguageprocessing(NLP),
whichcanbeappliedinmanyreal-worldscenarios,suchastextcategorization(SchapireandSinger,
2000),tagrecommendation(Katakisetal.,2008),informationretrieval(GopalandYang,2010),andso
on.ThetargetoftheMLCtaskistoassignmultiplelabelstoeachinstanceinthedataset.
Binaryrelevance(BR)(Boutelletal.,2004)isoneoftheearliestattemptstosolvetheMLCtaskby
transformingtheMLCtaskintomultiplesingle-labelproblems.However,itneglectsthe
correlationsbetweenlabels.chains(CC)proposedbyReadetal.(2011)convertstheMLC
taskintoachainofbinaryionproblemstomodelthecorrelationsbetweenlabels.However,it
iscomputationallyexpensiveforlargedatasets.OthermethodssuchasML-DT(ClareandKing,2001),
Rank-SVM(ElisseeffandWeston,2002),andML-KNN(ZhangandZhou,2007)canonlybeusedto
capturetheorsecondorderlabelcorrelationsorarecomputationallyintractablewhenhigh-order
labelcorrelationsareconsidered.
Inrecentyears,neuralnetworkshaveachievedgreatsuccessintheofNLP.Someneuralnetwork
modelshavealsobeenappliedintheMLCtaskandachievedimportantprogress.Forinstance,fully
connectedneuralnetworkwithpairwiserankinglossfunctionisutilizedinZhangandZhou(2006).
Kurataetal.(2016)proposetoperformusingtheconvolutionalneuralnetwork(CNN).
Chenetal.(2017)useCNNandrecurrentneuralnetwork(RNN)tocapturethesemanticinformationof
texts.However,theyeitherneglectthecorrelationsbetweenlabelsordonotconsiderdifferencesinthe
contributionsoftextualcontentwhenpredictinglabels.
Inthispaper,inspiredbythetremendoussuccessofthesequence-to-sequence(Seq2Seq)modelin
machinetranslation(Bahdanauetal.,2014;Luongetal.,2015;Sunetal.,2017),abstractivesumma-
rization(Rushetal.,2015;Linetal.,2018),styletransfer(Shenetal.,2017;Xuetal.,2018)andother
domains,weproposeasequencegenerationmodelwithanoveldecoderstructuretosolvetheMLC
task.Theproposedsequencegenerationmodelconsistsofanencoderandadecoderwiththeattention
1
Thedatasetsandcodeareavailableat
https://github.com/lancopku/SGM
ThisworkislicencedunderaCreativeCommonsAttribution4.0InternationalLicence.Licencedetails:
http:
//creativecommons.org/licenses/by/4.0/
arXiv:1806.04822v3  [cs.CL]  15 Jun 2018mechanism.ThedecoderusesanLSTMtogeneratelabelssequentially,andpredictsthenextlabelbased
onitspreviouslypredictedlabels.Therefore,theproposedmodelcanconsiderthecorrelationsbetween
labelsbyprocessinglabelsequencedependenciesthroughtheLSTMstructure.Furthermore,theatten-
tionmechanismconsidersthecontributionsofdifferentpartsoftextwhenthemodelpredictsdifferent
labels.Inaddition,anoveldecoderstructurewithglobalembeddingisproposedtofurtherimprovethe
performanceofthemodelbyincorporatingoverallinformativesignals.
Thecontributionsofthispaperarelistedasfollows:

WeproposetoviewtheMLCtaskasasequencegenerationproblemtotakethecorrelationsbetween
labelsintoaccount.

Weproposeasequencegenerationmodelwithanoveldecoderstructure,whichnotonlycaptures
thecorrelationsbetweenlabels,butalsoselectsthemostinformativewordsautomaticallywhen
predictingdifferentlabels.

Extensiveexperimentalresultsshowthatourproposedmethodsoutperformthebaselinesbyalarge
margin.Furtheranalysisdemonstratestheeffectivenessoftheproposedmethodsoncorrelation
representation.
Thewholepaperisorganizedasfollows.WedescribeourmethodsinSection2.InSection3,we
presenttheexperimentsandmakeanalysisanddiscussions.Section4introducestherelatedwork.Fi-
nallyinSection5weconcludethispaperandexplorethefuturework.
2ProposedMethod
Weintroduceourproposedmethodsindetailinthissection.First,wegiveanoverviewofthemodelin
Section2.1.Second,weexplainthedetailsoftheproposedsequencegenerationmodelinSection2.2.
Finally,Section2.3presentsournoveldecoderstructure.
2.1Overview
Firstofall,wesomenotationsanddescribetheMLCtask.Giventhelabelspacewith
L
labels
L
=
f
l
1
;l
2
;

;l
L
g
,atextsequence
x
containing
m
words,thetaskistoassignasubset
y
containing
n
labelsinthelabelspace
L
to
x
.Unliketraditionalsingle-labelclaswhereonlyonelabelis
assignedtoeachsample,eachsampleintheMLCtaskcanhavemultiplelabels.Fromtheperspective
ofsequencegeneration,theMLCtaskcanbemodeledasanoptimallabelsequence
y

that
maximizestheconditionalprobability
p
(
y
j
x
)
,whichiscalculatedasfollows:
p
(
y
j
x
)=
n
Y
i
=1
p
(
y
i
j
y
1
;y
2
;

;y
i

1
;
x
)
(1)
AnoverviewofourproposedmodelisshowninFigure1.First,wesortthelabelsequenceofeach
sampleaccordingtothefrequencyofthelabelsinthetrainingset.High-frequencylabelsareplacedin
thefront.Inaddition,the
bos
and
eos
symbolsareaddedtotheheadandtailofthelabelsequence,
respectively.
Thetextsequence
x
isencodedtothethehiddenstates,whichareaggregatedtoacontextvector
c
t
bytheattentionmechanismattime-step
t
.Thedecodertakesthecontextvector
c
t
,thelasthidden
state
s
t

1
ofthedecoderandtheembeddingvector
g
(
y
t

1
)
astheinputstoproducethehiddenstate
s
t
attime-step
t
.Here
y
t

1
isthepredictedprobabilitydistributionoverthelabelspace
L
attime-step
t

1
.Thefunction
g
takes
y
t

1
asinputandproducestheembeddingvectorwhichisthenpassedtothe
decoder.Finally,themaskedsoftmaxlayerisusedtooutputtheprobabilitydistribution
y
t
.
2.2SequenceGeneration
Inthissubsection,weintroducethedetailsofourproposedmodel.Thewholesequencegeneration
modelconsistsofanencoderandadecoderwiththeattentionmechanism.
Figure1:Theoverviewofourproposedmodel.MSdenotesthemaskedsoftmaxlayer.GEdenotesthe
globalembedding.
Encoder:
Let
(
w
1
;
w
2
;

;
w
m
)
beasentencewith
m
wordsand
w
i
istheone-hotrepresentationof
the
i
-thword.Weembed
w
i
toadenseembeddingvector
x
i
byanembeddingmatrix
E
2
R
k
j
.
Here
jVj
isthesizeofthevocabulary,and
k
isthedimensionoftheembeddingvector.
WeuseabidirectionalLSTM(HochreiterandSchmidhuber,1997)toreadthetextsequence
x
from
bothdirectionsandcomputethehiddenstatesforeachword,
!
h
i
=
!
LSTM(
!
h
i

1
;
x
i
)
(2)
 
h
i
=
 
LSTM(
 
h
i
+1
;
x
i
)
(3)
Weobtainthehiddenrepresentationofthe
i
-thwordbyconcatenatingthehiddenstatesfrom
bothdirections,
h
i
=[
!
h
i
;
 
h
i
]
,whichembodiestheinformationofthesequencecenteredaroundthe
i
-thword.
Attention:
Whenthemodelpredictsdifferentlabels,notalltextwordsmakethesamecontribution.
Theattentionmechanismproducesacontextvectorbyfocusingondifferentportionsofthetextse-
quenceandaggregatingthehiddenrepresentationsofthoseinformativewords.Specially,theattention
mechanismassignstheweight

ti
tothe
i
-thwordattime-step
t
asfollows:
e
ti
=
v
T
a
tanh(
W
a
s
t
+
U
a
h
i
)
(4)

ti
=
exp(
e
ti
)
P
m
j
=1
exp(
e
tj
)
(5)
where
W
a
,
U
a
,
v
a
areweightparametersand
s
t
isthecurrenthiddenstateofthedecoderattime-step
t
.
Forsimplicity,allbiastermsareomittedinthispaper.Thecontextvector
c
t
whichispassedtothe
decoderattime-step
t
iscalculatedasfollows:
c
t
=
m
X
i
=1

ti
h
i
(6)
Decoder:
Thehiddenstate
s
t
ofthedecoderattime-step
t
iscomputedasfollows:
s
t
=LSTM(
s
t

1
;
[
g
(
y
t

1
);
c
t

1
])
(7)
where
[
g
(
y
t

1
);
c
t

1
]
meanstheconcatenationofthevectors
g
(
y
t

1
)
and
c
t

1
.
g
(
y
t

1
)
istheembed-
dingofthelabelwhichhasthehighestprobabilityunderthedistribution
y
t

1
.
y
t

1
istheprobability
distributionoverthelabelspace
L
attime-step
t

1
andiscomputedasfollows:
o
t
=
W
o
f
(
W
d
s
t
+
V
d
c
t
)
(8)
y
t
=
softmax
(
o
t
+
I
t
)
(9)
where
W
o
,
W
d
,and
V
d
areweightparameters,
I
t
2
R
L
isthemaskvectorthatisusedtopreventthe
decoderfrompredictingrepeatedlabels,and
f
isanonlinearactivationfunction.
(
I
t
)
i
=
(

ifthelabel
l
i
hasbeenpredictedatprevious
t

1
timesteps.
0
otherwise.
(10)
Atthetrainingstage,thelossfunctionisthecross-entropylossfunction.Weemploythebeamsearch
algorithm(WisemanandRush,2016)tothetop-rankedpredictionpathatinferencetime.The
predictionpathsendingwiththe
eos
areaddedtothecandidatepathset.
2.3GlobalEmbedding
Inthesequencegenerationmodelmentionedabove,theembeddingvector
g
(
y
t

1
)
inEquation(7)isthe
embeddingofthelabelthathasthehighestprobabilityunderthedistribution
y
t

1
.However,thiscalcu-
lationonlytakesadvantageofthemaximumvalueof
y
t

1
greedily.Theproposedsequencegeneration
modelgenerateslabelssequentiallyandpredictsthenextlabelconditionedonitspreviouslypredicted
labels.Therefore,itislikelythatwewouldgetasuccessionofwronglabelpredictionsinthefollowing
timestepsifthepredictioniswrongattime-step
t
,whichisalsocalled
exposurebias
.Toacertainextent,
thebeamsearchalgorithmalleviatesthisproblem.However,itcannotfundamentallysolvetheproblem
becausethe
exposurebias
phenomenonislikelytooccurforallcandidatepaths.
y
t

1
representsthe
predictedprobabilitydistributionattime-step
t

1
,soitisobviousthatallinformationin
y
t

1
ishelpful
whenwepredictthecurrentlabelattime-step
t
.The
exposurebias
problemoughttoberelievedby
consideringallinformativesignalscontainedin
y
t

1
.
Basedonthismotivation,weproposeanewdecoderstructure,wheretheembeddingvector
g
(
y
t

1
)
at
time-step
t
iscapableofrepresentingtheoverallinformationat
(
t

1)
-thtimestep.Inspiredbytheideaof
theadaptivegateinhighwaynetwork(Srivastavaetal.,2015),hereweintroduceourglobalembedding.
Let
e
denotestheembeddingofthelabelwhichhasthehighestprobabilityunderthedistribution
y
t

1
.

e
istheweightedaverageembeddingattime
t
,whichiscalculatedasfollows:

e
=
L
X
i
=1
y
(
i
)
t

1
e
i
(11)
where
y
(
i
)
t

1
isthe
i
-thelementof
y
t

1
and
e
i
istheembeddingvectorofthe
i
-thlabel.Thentheproposed
globalembedding
g
(
y
t

1
)
passedtothedecoderattime-step
t
isasfollows:
g
(
y
t

1
)=(
1

H
)

e
+
H


e
(12)
where
H
isthetransformgatecontrollingtheproportionoftheweightedaverageembedding:
H
=
W
1
e
+
W
2

e
(13)
where
W
1
;
W
2
2
R
L

L
areweightmatrices.Theglobalembedding
g
(
y
t

1
)
istheoptimizedcombina-
tionoftheoriginalembeddingandtheweightedaverageembeddingbyusingtransformgate
H
,which
canautomaticallydeterminethecombinationfactorineachdimension.
y
t

1
containstheinformation
ofallpossiblelabels.Byconsideringtheprobabilityofeverylabel,themodeliscapableofreducing
damagecausedbymispredictionsmadeintheprevioustimesteps.Thisenablesthemodeltopredict
labelsequencesmoreaccurately.
Dataset
TotalSamples
LabelSets
Words/Sample
Labels/Sample
RCV1-V2
804
;
414
103
123.94
3.24
AAPD
55
;
840
54
163
:
42
2
:
41
Table1:Summaryofdatasets.
TotalSamples
,
LabelSets
denotethetotalnumberofsamplesand
labels,respectively.
Words/Sample
istheaveragenumberofwordspersampleand
Labels/Sample
is
theaveragenumberoflabelspersample.
3Experiments
Inthissection,weevaluateourproposedmethodsontwodatasets.Weintroducethedatasets,
evaluationmetrics,experimentaldetails,andallbaselines.Then,wecompareourmethodswiththe
baselines.Finally,weprovidetheanalysisanddiscussionsofexperimentalresults.
3.1Datasets
ReutersCorpusVolumeI(RCV1-V2)
2
:
ThisdatasetisprovidedbyLewisetal.(2004).Itconsists
ofover
800
;
000
manuallycategorizednewswirestoriesmadeavailablebyReutersLtdforresearchpur-
poses.Multipletopicscanbeassignedtoeachnewswirestoryandthereare103topicsintotal.
ArxivAcademicPaperDataset(AAPD)
3
:
Webuildanewlargedatasetforthemulti-labeltext
cation.Wecollecttheabstractandthecorrespondingsubjectsof
55
;
840
papersinthecomputerscience
fromthewebsite
4
.Anacademicpapermayhavemultiplesubjectsandthereare54subjectsin
total.Thetargetistopredictcorrespondingsubjectsofanacademicpaperaccordingtothecontentof
theabstract.
Wedivideeachdatasetintotraining,validationandtestsets.Thestatisticsofthetwodatasetsare
showninTable1.
3.2EvaluationMetrics
Followingthepreviouswork(ZhangandZhou,2007;Chenetal.,2017),weadopthamminglossand
micro-
F
1
scoreasourmainevaluationmetrics.Micro-precisionandmicro-recallarealsoreportedto
assisttheanalysis.

Hamming-loss
(SchapireandSinger,1999)evaluatesthefractionofinstance-label
pairs,wherearelevantlabelismissedoranirrelevantispredicted.

Micro-
F
1
(Manningetal.,2008)canbeinterpretedasaweightedaverageoftheprecisionandre-
call.Itiscalculatedgloballybycountingthetotaltruepositives,falsenegatives,andfalsepositives.
3.3Details
Weextractthevocabulariesfromthetrainingsets.FortheRCV1-V2dataset,thesizeofthevocabulary
is
50
;
000
andout-of-vocabulary(OOV)wordsarereplacedwith
unk
.Eachdocumentistruncatedatthe
lengthof500andthebeamsizeis5attheinferencestage.Besides,wesetthewordembeddingsize
to512.Thehiddensizesoftheencoderandthedecoderare256and512,respectively.Thenumberof
LSTMlayersofencoderanddecoderis2.
FortheAAPDdataset,thesizeofwordembeddingis256.TherearetwoLSTMlayersintheencoder
anditssizeis256.Forthedecoder,thereisoneLSTMlayerofsize512.Thesizeofthevocabularyis
30
;
000
andOOVwordsarealsoreplacedwith
unk
.Eachdocumentistruncatedatthelengthof500.
Thebeamsizeis9attheinferencestage.
WeusetheAdam(KingmaandBa,2014)optimizationmethodtominimizethecross-entropylossover
thetrainingdata.Forthehyper-parametersoftheAdamoptimizer,wesetthelearningrate

=0
:
001
,
twomomentumparameters

1
=0
:
9
and

2
=0
:
999
respectively,and

=1

10

8
.Additionally,
2
http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_
README.htm
3
https://github.com/lancopku/SGM
4
https://arxiv.org/
Models
HL(-)P(+)R(+)F1(+)
BR
0.00860.9040.8160.858
CC
0.00870.8870.8280.857
LP
0.00870.8960.8240.858
CNN
0.0089
0.922
0.7980.855
CNN-RNN
0.00850.8890.8250.856
SGM
0.00810.8870.8500.869
+GE
0.0075
0.897
0.8600.878
(a)PerformanceontheRCV1-V2testset.
Models
HL(-)P(+)R(+)F1(+)
BR
0.03160.6440.6480.646
CC
0.03060.6570.6510.654
LP
0.03120.6620.6080.634
CNN
0.0256
0.849
0.5450.664
CNN-RNN
0.02780.7180.6180.664
SGM
0.02510.7460.6590.699
+GE
0.0245
0.748
0.6750.710
(b)PerformanceontheAAPDtestset.
Table2:Comparisonbetweenourmethodsandallbaselinesontwodatasets.GEdenotestheglobal
embedding.HL,P,R,andF1denotehammingloss,micro-precision,micro-recall,andmicro-
F
1
,re-
spectively.Thesymbolﬁ+ﬂindicatesthatthehigherthevalueis,thebetterthemodelperforms.The
symbolﬁ-ﬂistheopposite.
wemakeuseofthedropoutregularization(Srivastavaetal.,2014)toavoidovandclipthe
gradients(Pascanuetal.,2013)tothemaximumnormof10.0.Duringtraining,wetrainthemodelfora
ednumberofepochsandmonitoritsperformanceonthevalidationset.Oncethetrainingis
weselectthemodelwiththebestmicro-
F
1
scoreonthevalidationsetasourmodelandevaluateits
performanceonthetestset.
3.4Baselines
Wecompareourproposedmethodswiththefollowingbaselines:

BinaryRelevance(BR)
(Boutelletal.,2004)transformstheMLCtaskintomultiplesingle-label
problemsbyignoringthecorrelationsbetweenlabels.

Chains(CC)
(Readetal.,2011)transformstheMLCtaskintoachainofbinary
cationproblemsandtakeshigh-orderlabelcorrelationsintoconsideration.

LabelPowerset(LP)
(TsoumakasandKatakis,2006)transformsamulti-labelproblemtoamulti-
classproblemwithonemulti-classtrainedonalluniquelabelcombinations.

CNN
(Kim,2014)usesmultipleconvolutionkernelstoextracttextfeatures,whicharethenin-
puttedtothelineartransformationlayerfollowedbyasigmoidfunctiontooutputtheprobability
distributionoverthelabelspace.Themulti-labelsoftmarginlossisoptimized.

CNN-RNN
(Chenetal.,2017)utilizesCNNandRNNtocaptureboththeglobalandlocaltextual
semanticsandmodelthelabelcorrelations.
Followingthepreviouswork(Chenetal.,2017),weadoptthelinearSVMasthebaseinBR,
CCandLP.WeimplementBR,CCandLPbymeansofScikit-Multilearn(Szyma
´
nski,2017),anopen-
sourcelibraryfortheMLCtask.Wetunehyper-parametersofallbaselinealgorithmsonthevalidation
setbasedonthemicro-
F
1
score.Inaddition,trainingstrategiesmentionedinZhangandWallace(2015)
areusedtotunehyper-parametersforthebaselinesCNNandCNN-RNN.
3.5Results
Forthepurposeofsimplicity,wedenotetheproposedsequencegenerationmodelas
SGM
.Wereport
theevaluationresultsofourmethodsandallbaselinesonthetestsets.
TheexperimentalresultsofourmethodsandthebaselinesondatasetRCV1-V2areshowninTable2a.
Resultsshowthatourproposedmethodsgivethebestperformanceinthemainevaluationmetrics.Our
proposedSGMmodelusingglobalembeddingachievesareductionof12.79%hamming-lossandan
improvementof2.33%micro-
F
1
scoreoverthemostcommonlyusedbaselineBR.Besides,ourmeth-
odsoutperformothertraditionaldeep-learningmodelsbyalargemargin.Forinstance,theproposed
SGMmodelwithglobalembeddingachievesareductionof15.73%hamming-lossandanimprovement
Figure2:TheperformanceoftheSGMmodelwhen
usingdifferent

.Thereddottedlinerepresentsthe
resultsofusingtheadaptivegate.Thesymbolﬁ+ﬂ
indicatesthatthehigherthevalueis,thebetterthe
modelperforms.Thesymbolﬁ-ﬂistheopposite.
Figure3:TheperformanceoftheSGMmodelon
differentsubsetsoftheRCV1-V2testset.LLSrep-
resentsthelengthoflabelsequenceofeachsample
inthesubset.Theexplanationsofsymbolﬁ+ﬂand
ﬁ-ﬂcanbefoundinFigure2.
of2.69%micro-
F
1
scoreoverthetraditionalCNNmodel.Evenwithouttheglobalembedding,our
proposedSGMmodelisstillabletooutperformallbaselines.
Inaddition,theSGMmodelisimprovedbyusingglobalembedding.TheSGMmodel
withglobalembeddingachievesareductionof7.41%hamminglossandanimprovementof1.04%
micro-
F
1
scoreonthetestsetcomparedwiththemodelwithoutglobalembedding.
Table2bpresentstheresultsoftheproposedmethodsandthebaselinesontheAAPDtestset.Similar
totheexperimentalresultsontheRCV1-V2testset,ourproposedmethodsstilloutperformallbaselines
byalargemargininmainevaluationmetrics.Thisfurtherthatourmethodshave
advantagesoverpreviousworkonlargedatasets.Besides,theproposedSGMachievesareductionof
2.39%hamminglossandanimprovementof1.57%micro-
F
1
scoreonthetestsetbyusingglobal
embedding.Thisfurtherthattheglobalembeddingiscapableofhelpingthemodeltopredict
labelsequencesmoreaccurately.
3.6AnalysisandDiscussion
Hereweperformfurtheranalysisonthemodelandexperimentalresults.Wereporttheevaluationresults
intermsofhamminglossandmicro-
F
1
score.
3.6.1ExplorationofGlobalEmbedding
AsisshowninTable2,globalembeddingcanimprovetheperformanceofthemodel.The
globalembedding
g
(
y
t

1
)
attime-step
t
takesadvantageofallinformationofpossiblelabelscontained
in
y
t

1
,soitisabletoenrichthesourceinformationwhenthemodelpredictsthecurrentlabel,which
leadstotheperformanceofthemodelimproved.Theglobalembeddingisthecombination
oforiginalembedding
e
andtheweightedaverageembedding

e
byusingthetransformgate
H
.Herewe
conductexperimentsontheRCV1-V2datasettoexplorehowtheperformanceofourmodelisaffected
bytheproportionbetweentwokindsofembeddings.Intheexploratoryexperiment,theembedding
vectorattime-step
t
iscalculatedasfollows:
g
(
y
t

1
)=(1


)

e
+



e
(14)
Theproportionbetweentwokindsofembeddingsiscontrolledbycoef

.

=0
denotesthe
proposedSGMmodelwithoutglobalembedding.Theproportionofweightedaverageembeddingin-
creaseswhenweincrease

.Theexperimentalresultsusingdifferent

valuesinthedecoderareshown
inFigure2.
AsisshowninFigure2,theperformanceofthemodelvarieswhendifferent

isused.Overall,
themodelusingtheadaptivegateperformsthebest,whichachievesthebestresultsinbothhamming
lossandmicro-
F
1
.Themodelswith

6
=0
outperformthemodelwith

=0
,whichshowsthatthe
weightedaverageembeddingcontainsricherinformation,leadingtotheimprovementintheperformance
Models
HL(-)F1(+)
SGM
0.00810.869
w/omask
0.0083(
#
2
:
47%
)0.866(
#
0
:
35%
)
w/osorting
0.0084(
#
3
:
70%
)0.858(
#
1
:
27%
)
(a)AblationstudyfortheSGMmodel.
Models
HL(-)F1(+)
SGM+GE
0.00750.878
w/omask
0.0078(
#
4
:
00%
)0.873(
#
0
:
57%
)
w/osorting
0.0083(
#
10
:
67%
)0.859(
#
2
:
16%
)
(b)AblationstudyforSGMmodelwithglobalembedding.
Table3:AblationstudyontheRCV1-V2testset.GEdenotestheglobalembedding.HLandF1denote
hamminglossandmicro-
F
1
,respectively.Thesymbolﬁ+ﬂindicatesthatthehigherthevalueis,the
betterthemodelperforms.Thesymbolﬁ-ﬂistheopposite.
""
meansthattheperformanceofthemodel
improvesand
#
istheopposite.
ofthemodel.Withoutusingtheadaptivegate,theperformanceofthemodelimprovesatandthen
deterioratesas

increases.Itrevealsthereasonwhythemodelwiththeadaptivegateperformsthe
best:theadaptivegatecanautomaticallydeterminethemostappropriate

valueaccordingtotheactual
condition.
3.6.2TheImpactofMaskandSorting
OurproposedmethodsaredevelopedbasedontraditionalSeq2Seqmodels.However,themaskmodule
isaddedtotheproposedmethods,whichisusedtopreventthemodelsfrompredictingrepeatedlabels.
Inaddition,wesortthelabelsequenceofeachsampleaccordingtothefrequencyofappearanceoflabels
inthetrainingset.Inordertoexploretheimpactofthemaskmoduleandsorting,weconductablation
experimentsontheRCV1-V2dataset.TheexperimentalresultsareshowninTable3.ﬁ
w/omask
ﬂmeans
thatwedonotperformmaskoperationandﬁ
w/osorting
ﬂmeansthatwerandomlyshufthelabel
sequenceinordertoperturbitsoriginalorder.
AsisshowninTable3,theperformancedeclineoftheSGMmodelwithglobalembeddingismore
comparedwiththatoftheSGMmodelwithoutglobalembedding.Inaddition,thedecline
intheperformanceofthetwomodelsismorewhenwerandomlyshufthelabelsequence
ofthesamplecomparedwithremovingmaskmodule.ThelabelcardinalityoftheRCV1-V2datasetis
small,soourproposedmethodsarelesspronetopredictingrepeatedlabels.Thisexplainsthereason
whyexperimentalresultsindicatethatthemaskmodulehaslittleimpactonthemodels'performance.
Inaddition,theproposedmodelsaretrainedusingthemaximumlikelihoodestimationmethodandthe
cross-entropylossfunction,whichrequireshumanstotheorderoftheoutputlabels.Therefore,
thesortingoflabelsisveryimportantforthemodels'performance.Besides,theperformanceofboth
modelsdeclineswhenwedonotusethemaskmodule.Thisshowsthattheperformanceofthemodel
canbeimprovedbyusingthemaskoperation.
3.6.3ErrorAnalysis
Intheexperiment,wethattheperformanceofallmethodsdeteriorateswhenthelengthofthelabel
sequenceincreases(forsimplicity,wedenotethelengthofthelabelsequenceas
LLS
).Inordertoexplore
theofthevalueofthe
LLS
,wedividethetestsetintodifferentsubsetsbasedondifferent
LLS
.
Figure3showstheperformanceoftheSGMmodelandthemostcommonlyusedbaselineBRondifferent
subsetsoftheRCV1-V2testset.AsisshowninFigure3,generally,theperformanceofbothmodels
deterioratesasthe
LLS
increases.Thisshowsthatwhenthelabelsequenceofthesampleisparticularly
long,itisdiftoaccuratelypredictalllabels.Becausemoreinformationisneededwhenthemodel
predictsmorelabels.Itiseasytoignoresometruelabelswhosefeatureinformationisinsuf
However,asisshowninFigure3,theproposedSGMmodeloutperformsBRwithanyvalueof
LLS
,
andtheadvantagesofourmodelaremorewhen
LLS
islarge.ThetraditionalBRmethod
predictsalllabelsatonceonlybasedonthesampleinput.Therefore,ittendstoignoresometruelabels
whosefeatureinformationcontainedinthesampleisinsufTheSGMmodelgenerateslabels
sequentially,andpredictsthenextlabelbasedonitspreviouslypredictedlabels.Therefore,evenifthe
samplecontainslessinformationofsometruelabels,theSGMmodeliscapableofgeneratingthesetrue
labelsbyconsideringrelevantlabelsthathavebeenpredicted.

Gen
er
at
ingdescriptionsfor
videoshasmanyap-
plicationsincludinghuman
robotinteraction.

Manymethodsfor
im
age
cap
tion
ingrelyonpre-
trained
ob
ject
clas
si

CNNandLongShortTerm
Mem
oryrecurrentnetworks.

Howtolearn
ro
bust
vi
sual
clas
si
fromthe
weakannotationsofthesentencedescriptions.
(a)VisualanalysiswhentheSGMmodelpredictsﬁ
CV
ﬂ.

Gen
er
at
ingdescriptionsforvideoshasmanyap-
plicationsincludinghumanrobotinteraction.

Manymethodsfor
im
age
cap
tion
ingrelyonpre-
trainedobject
clas
si
CNNand
LongShortTerm
Mem
ory
re
cur
rentnetworks.

Howtolearn
ro
bustvisual
clas
si
fromthe
weak
an
no
ta
tionsofthe
sen
tencedescriptions.
(b)VisualanalysiswhentheSGMmodelpredictsﬁ
CL
ﬂ.
Table4:AnexampleabstractintheAAPDdataset,fromwhichweextractthreeinformativesentences.
Thisabstractisassignedtwolabels:ﬁ
CV
ﬂandﬁ
CL
ﬂ.Theydenotecomputervisionandcomputational
language,respectively.
Reference
BR
SGM
SGM+GE
CCAT,
C15,C152
,C41,C411
CCAT,C15,C13
CCAT,
C15,C152
CCAT,
C15,C152
,C41,C411
CCAT,GCAT,ECAT,C31,
GDIP,C13,C21,
E51,E512
CCAT,GCAT,GDIP,E51
CCAT,ECAT,GDIP,
E51,
E512
CCAT,GCAT,ECAT,C31,
GDIP,
E51,E512
,C312
GCAT,ECAT,
G15,G154,
G151,G155
GCAT,ECAT,GENV,G15
GCAT,ECAT,E21,
G15,
G154,G156
GCAT,ECAT,E21,
G15,
G154,G155
Table5:SeveralexamplesofthegeneratedlabelsequencesontheRCV1-V2dataset.Theredboldlabels
ineachexampleindicatethattheyarehighlycorrelated.
3.6.4VisualizationofAttention
Whenthemodelpredictsdifferentlabels,thereexistdifferencesinthecontributionsofdifferentwords.
TheSGMmodelisabletoselectthemostinformativewordsbyutilizingtheattentionmechanism.
ThevisualizationoftheattentionlayerisshowninTable4.AccordingtoTable4,whentheSGM
modelpredictsthelabelﬁ
CV
ﬂ,itcanautomaticallyassignlargerweightstomoreinformativewords,
like
image
,
visual
,
captioning
,andsoon.Forthelabelﬁ
CL
ﬂ,theselectedinformativewordsare
sentence
,
memory
,
recurrent
,etc.Thisshowsthatourproposedmodelsareabletoconsiderthe
differencesinthecontributionsoftextualcontentwhenpredictingdifferentlabelsandselectthemost
informativewordsautomatically.
3.6.5CaseStudy
WegiveseveralexamplesofthegeneratedlabelsequencesontheRCV1-V2datasetinTable5,where
wecomparetheproposedmethodswiththemostcommonlyusedbaselineBR.Theredboldlabelsin
eachexampleindicatethattheyarehighlycorrelated.Forinstance,thecorrelationcoefbetween
E51andE512is0.7664.Therefore,thesehighlycorrelatedlabelsarelikelytoappeartogetherinthe
predictedlabelsequence.TheBRalgorithmfailstocapturethislabelcorrelation,leavingmanytrue
labelsunpredicted.However,ourproposedmethodsaccuratelypredictalmostallhighlycorrelatedtrue
labels.TheproposedSGMcapturesthecorrelationsbetweenlabelsbyutilizingLSTMtogeneratela-
belssequentially.Therefore,forsometruelabelswhosefeatureinformationisinsuftheproposed
SGMisstillabletogeneratethembyconsideringrelevantlabelsthathavebeenpredicted.Inaddition,
labelsequencesthataremoreaccuratearepredictedbyusingglobalembedding.TheSGMmodelwith
globalembeddingpredictsmoretruelabelscomparedwiththeSGMmodelwithoutglobalembedding.
Thereasonisthatthesourceinformationisfurtherenrichedbyincorporatingoverallinformativesignals
intheprobabilitydistribution
y
t

1
whenthemodelpredictsthelabelattime-step
t
.Enrichedinfor-
mationmakesglobalembeddingmoresmooth,whichenablesthemodeltoreducedamagecausedby
mispredictionsmadeintheprevioustimesteps.
4RelatedWork
TheMLCtaskstudiestheproblemwheremultiplelabelsareassignedtoeachsample.Therearefour
maintypesofmethodsfortheMLCtask:problemtransformationmethods,algorithmadaptationmeth-
ods,ensemblemethods,andneuralnetworkmodels.
ProblemtransformationmethodsmaptheMLCtaskintomultiplesingle-labellearningtasks.Binary
relevance(BR)(Boutelletal.,2004)decomposestheMLCtaskintoindependentbinary
problemsbyignoringthecorrelationsbetweenlabels.Inordertomodellabelcorrelations,labelpower-
set(LP)(TsoumakasandKatakis,2006)transformsamulti-labelproblemtoamulti-classproblemwith
atrainedonalluniquelabelcombinations.chains(CC)(Readetal.,2011)trans-
formstheMLCtaskintoachainofbinaryproblems,wheresubsequentbinary
inthechainarebuiltuponthepredictionsofprecedingones.However,thecomputationalefyand
performanceofthesemethodsarechallengedbyapplicationswithalargenumberoflabelsandsamples.
Algorithmadaptationmethodsextendlearningalgorithmstohandlemulti-labeldatadirectly.
ClareandKing(2001)constructdecisiontreebasedonmulti-labelentropytoperform
ElisseeffandWeston(2002)optimizetheempiricalrankinglossbyusingmaximummarginstrategyand
kerneltricks.Collectivemulti-label(CML)(GhamrawiandMcCallum,2005)adoptsmaximum
entropyprincipletodealwithmulti-labeldatabyencodinglabelcorrelationsasconstraintconditions.
ZhangandZhou(2007)adopt
k
-nearestneighbortechniquestodealwithmulti-labeldata.F
¨
urnkranzet
al.(2008)makerankingamonglabelsbyutilizingpairwisecomparison.Lietal.(2015)proposeanovel
jointlearningalgorithmthatallowsthefeedbackstobepropagatedfromtheforlatterlabelsto
theforthecurrentlabel.Mostmethods,however,canonlybeusedtocapturetheorsecond
orderlabelcorrelationsorarecomputationallyintractableinconsideringhigh-orderlabelcorrelations.
Amongensemblemethods,Tsoumakasetal.(2011)breaktheinitialsetoflabelsintoanumberof
smallrandomsubsetsandemploytheLPalgorithmtotrainacorresponding.Szyma
´
nskietal.
(2016)proposetoconstructalabelco-occurrencegraphandperformcommunitydetectiontopartition
thelabelset.
Inrecentyears,someneuralnetworkmodelshavealsobeenusedfortheMLCtask.ZhangandZhou
(2006)proposetheBP-MLLthatutilizesafully-connectedneuralnetworkandapairwiserankingloss
function.Nametal.(2013)proposeaneuralnetworkusingcross-entropylossinsteadofrankingloss.
BenitesandSapozhnikova(2015)increasespeedbyaddinganextraARTlayerforcluster-
ing.Kurataetal.(2016)utilizewordembeddingsbasedonCNNtocapturelabelcorrelations.Chenet
al.(2017)proposetorepresentsemanticinformationoftextandmodelhigh-orderlabelcorrelationsby
combiningCNNwithRNN.BakerandKorhonen(2017)initializethehiddenlayerwithrowsthat
maptoco-occurrenceoflabelsbasedontheCNNarchitecturetoimprovetheperformanceofthemodel.
Maetal.(2018)proposetousethemulti-labelalgorithmformachinetranslationtohandle
thesituationwhereasentencecanbetranslatedintomorethanonecorrectsentences.
5ConclusionsandFutureWork
Inthispaper,weproposetoviewthemulti-labeltaskasasequencegenerationproblem
tomodelthecorrelationsbetweenlabels.Asequencegenerationmodelwithanoveldecoderstructure
isproposedtoimprovetheperformanceofExtensiveexperimentalresultsshowthatthe
proposedmethodsoutperformthebaselinesbyasubstantialmargin.Furtheranalysisofexperimental
resultsdemonstratesthatourproposedmethodsnotonlycapturethecorrelationsbetweenlabels,but
alsoselectthemostinformativewordsautomaticallywhenpredictingdifferentlabels.
AsanalyzedinSection3.6.3,whenalargenumberoflabelsareassignedtoasample,howtopredict
allthesetruelabelsaccuratelyisanintractableproblem.Ourproposedmethodsalleviatethisproblemto
someextent,butmoreeffectivesolutionsneedtobefurtherexploredinthefuture.
6Acknowledgements
ThisworkissupportedinpartbyNationalNaturalScienceFoundationofChina(No.61673028,No.
61333018)andtheNationalThousandYoungTalentsProgram.XuSunisthecorrespondingauthorof
thispaper.
References
[Bahdanauetal.2014]
DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.2014.Neuralmachinetranslation
byjointlylearningtoalignandtranslate.
CoRR
,abs/1409.0473.
[BakerandKorhonen2017]
SimonBakerandAnnaKorhonen.2017.Initializingneuralnetworksforhierarchical
multi-labeltextIn
BioNLP
.
[BenitesandSapozhnikova2015]
FernandoBenitesandElenaSapozhnikova.2015.Haram:ahierarchicalaram
neuralnetworkforlarge-scaletextIn
DataMiningWorkshop(ICDMW),2015IEEEInternational
Conferenceon
,pages847Œ854.IEEE.
[Boutelletal.2004]
MatthewR.Boutell,JieboLuo,XipengShen,andChristopherM.Brown.2004.Learning
multi-labelscene
PatternRecognition
,37(9):1757Œ1771.
[Chenetal.2017]
GuibinChen,DehengYe,ZhenchangXing,JieshanChen,andErikCambria.2017.Ensemble
applicationofconvolutionalandrecurrentneuralnetworksformulti-labeltextcategorization.In
2017Inter-
nationalJointConferenceonNeuralNetworks,IJCNN2017,Anchorage,AK,USA,May14-19,2017
,pages
2377Œ2383.
[ClareandKing2001]
AmandaClareandRossDKing.2001.Knowledgediscoveryinmulti-labelphenotypedata.
In
EuropeanConferenceonPrinciplesofDataMiningandKnowledgeDiscovery
,pages42Œ53.Springer.
[ElisseeffandWeston2002]
Andr
´
eElisseeffandJasonWeston.2002.Akernelmethodformulti-labelled
cation.In
Advancesinneuralinformationprocessingsystems
,pages681Œ687.
[F
¨
urnkranzetal.2008]
JohannesF
¨
urnkranz,EykeH
¨
ullermeier,EneldoLozaMenc
´
andKlausBrinker.2008.
Multilabelviacalibratedlabelranking.
Machinelearning
,73(2):133Œ153.
[GhamrawiandMcCallum2005]
NadiaGhamrawiandAndrewMcCallum.2005.Collectivemulti-label
cation.In
Proceedingsofthe14thACMinternationalconferenceonInformationandknowledgemanagement
,
pages195Œ200.ACM.
[GopalandYang2010]
SiddharthGopalandYimingYang.2010.Multilabelwithmeta-levelfeatures.
In
Proceedingsofthe33rdinternationalACMSIGIRconferenceonResearchanddevelopmentininformation
retrieval
,pages315Œ322.ACM.
[HochreiterandSchmidhuber1997]
SeppHochreiterandJ
¨
urgenSchmidhuber.1997.Longshort-termmemory.
Neuralcomputation
,9(8):1735Œ1780.
[Katakisetal.2008]
IoannisKatakis,GrigoriosTsoumakas,andIoannisVlahavas.2008.Multilabeltext
cationforautomatedtagsuggestion.In
ProceedingsoftheECML/PKDD
,volume18.
[Kim2014]
YoonKim.2014.ConvolutionalneuralnetworksforsentenceIn
Proceedingsofthe
2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,EMNLP2014,October25-29,2014,
Doha,Qatar,AmeetingofSIGDAT,aSpecialInterestGroupoftheACL
,pages1746Œ1751.
[KingmaandBa2014]
DiederikP.KingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimization.
CoRR
,abs/1412.6980.
[Kurataetal.2016]
GakutoKurata,BingXiang,andBowenZhou.2016.Improvedneuralnetwork-basedmulti-
labelwithbetterinitializationleveraginglabelco-occurrence.In
NAACLHLT2016,The2016
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguage
Technologies,SanDiegoCalifornia,USA,June12-17,2016
,pages521Œ526.
[Lewisetal.2004]
DavidD.Lewis,YimingYang,TonyG.Rose,andFanLi.2004.RCV1:Anewbenchmark
collectionfortextcategorizationresearch.
JournalofMachineLearningResearch
,5:361Œ397.
[Lietal.2015]
LiLi,HoufengWang,XuSun,BaobaoChang,ShiZhao,andLeiSha.2015.Multi-labeltext
categorizationwithjointlearningpredictions-as-featuresmethod.In
Proceedingsofthe2015Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing
,pages835Œ839.
[Linetal.2018]
JunyangLin,XuSun,ShumingMa,andQiSu.2018.Globalencodingforabstractivesummariza-
tion.In
ACL2018
.
[Luongetal.2015]
Minh-ThangLuong,HieuPham,andChristopherD.Manning.2015.Effectiveapproachesto
attention-basedneuralmachinetranslation.
CoRR
,abs/1508.04025.
[Maetal.2018]
ShumingMa,XuSun,YizhongWang,andJunyangLin.2018.Bag-of-wordsastargetforneural
machinetranslation.In
ACL2018
.
[Manningetal.2008]
ChristopherDManning,PrabhakarRaghavan,HinrichSch
¨
utze,etal.2008.
Introductionto
informationretrieval
,volume1.CambridgeuniversitypressCambridge.
[Nametal.2013]
JinseokNam,JungiKim,IrynaGurevych,andJohannesF
¨
urnkranz.2013.Large-scalemulti-
labeltext-revisitingneuralnetworks.
CoRR
,abs/1312.5419.
[Pascanuetal.2013]
RazvanPascanu,TomasMikolov,andYoshuaBengio.2013.Onthedifoftraining
recurrentneuralnetworks.In
InternationalConferenceonMachineLearning
,pages1310Œ1318.
[Readetal.2011]
JesseRead,BernhardPfahringer,GeoffHolmes,andEibeFrank.2011.chainsfor
multi-label
Machinelearning
,85(3):333.
[Rushetal.2015]
AlexanderM.Rush,SumitChopra,andJasonWeston.2015.Aneuralattentionmodelfor
abstractivesentencesummarization.In
Proceedingsofthe2015ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,EMNLP2015,Lisbon,Portugal,September17-21,2015
,pages379Œ389.
[SchapireandSinger1999]
RobertESchapireandYoramSinger.1999.Improvedboostingalgorithmsusing
predictions.
Machinelearning
,37(3):297Œ336.
[SchapireandSinger2000]
RobertESchapireandYoramSinger.2000.Boostexter:Aboosting-basedsystemfor
textcategorization.
Machinelearning
,39(2-3):135Œ168.
[Shenetal.2017]
TianxiaoShen,TaoLei,ReginaBarzilay,andTommiS.Jaakkola.2017.Styletransferfrom
non-paralleltextbycross-alignment.
CoRR
,abs/1705.09655.
[Srivastavaetal.2014]
NitishSrivastava,GeoffreyE.Hinton,AlexKrizhevsky,IlyaSutskever,andRuslan
Salakhutdinov.2014.Dropout:asimplewaytopreventneuralnetworksfromov
JournalofMa-
chineLearningResearch
,15(1):1929Œ1958.
[Srivastavaetal.2015]
RupeshKumarSrivastava,KlausGreff,andJ
¨
urgenSchmidhuber.2015.Highwaynetworks.
CoRR
,abs/1505.00387.
[Sunetal.2017]
XuSun,BingzhenWei,XuanchengRen,andShumingMa.2017.Labelembeddingnetwork:
Learninglabelrepresentationforsofttrainingofdeepnetworks.
CoRR
,abs/1710.10393.
[Szyma
´
nskietal.2016]
PiotrSzyma
´
nski,TomaszKajdanowicz,andKristianKersting.2016.Howisadata-driven
approachbetterthanrandomchoiceinlabelspacedivisionformulti-label
Entropy
,18(8):282.
[Szyma
´
nski2017]
PiotrSzyma
´
nski.2017.Ascikit-basedpythonenvironmentforperformingmulti-label
cation.
arXivpreprintarXiv:1702.01460
.
[TsoumakasandKatakis2006]
GrigoriosTsoumakasandIoannisKatakis.2006.Multi-labelAn
overview.
InternationalJournalofDataWarehousingandMining
,3(3).
[Tsoumakasetal.2011]
GrigoriosTsoumakas,IoannisKatakis,andIoannisVlahavas.2011.Randomk-labelsets
formultilabel
IEEETransactionsonKnowledgeandDataEngineering
,23(7):1079Œ1089.
[WisemanandRush2016]
SamWisemanandAlexanderM.Rush.2016.Sequence-to-sequencelearningasbeam-
searchoptimization.
CoRR
,abs/1606.02960.
[Xuetal.2018]
JingjingXu,XuSun,QiZeng,XuanchengRen,XiaodongZhang,HoufengWang,andWenjieLi.
2018.Unpairedsentiment-to-sentimenttranslation:Acycledreinforcementlearningapproach.
ACL2018
.
[ZhangandWallace2015]
YeZhangandByronC.Wallace.2015.Asensitivityanalysisof(andpractitioners'
guideto)convolutionalneuralnetworksforsentence
CoRR
,abs/1510.03820.
[ZhangandZhou2006]
Min-LingZhangandZhi-HuaZhou.2006.Multilabelneuralnetworkswithapplications
tofunctionalgenomicsandtextcategorization.
IEEETransactionsonKnowledgeandDataEngineering
,
18(10):1338Œ1351.
[ZhangandZhou2007]
Min-LingZhangandZhi-HuaZhou.2007.ML-KNN:Alazylearningapproachtomulti-
labellearning.
Patternrecognition
,40(7):2038Œ2048.
"
70,Molecular generative model based on conditional variational autoencoder for de novo molecular design,http://arxiv.org/pdf/1806.05805v1.pdf,https://github.com/jaechanglim/CVAE,"Lim
etal.
RESEARCH
Moleculargenerativemodelbasedonconditional
variationalautoencoderfordenovomolecular
design
JaechangLim
1
,SeongokRyu
1
,JinWooKim
1
andWooYounKim
1
,
2
*
*
Correspondence:
wooyoun@kaist.ac.kr
1
DepartmentofChemistry,
KAIST,291Daehak-ro,34141
Daejeon,RepublicofKorea
Fulllistofauthorinformationis
availableattheendofthearticle
Abstract
Weproposeamoleculargenerativemodelbasedontheconditionalvariational
autoencoderfordenovomoleculardesign.Itisspecializedtocontrolmultiple
molecularpropertiessimultaneouslybyimposingthemonalatentspace.Asa
proofofconcept,wedemonstratethatitcanbeusedtogeneratedrug-like
moleculeswithtargetproperties.Wewerealsoabletoadjustasingle
propertywithoutchangingtheothersandtomanipulateitbeyondtherangeof
thedataset.
Keywords:
Moleculardesign;conditionalvariationalautoencoder;deeplearning
Introduction
Theultimategoalofmoleculardesignfornewmaterialsanddrugsistodirectly
generatemoleculeswiththedesiredproperties.Thisisapparentlychallengingwork
becauseamolecularspaceisextraordinarilyvast,discrete,anddisorganizedwith
diversetypesofmolecules.Forinstance,10
8
moleculeshavebeensynthesized,[
1
]
whereasitisestimatedthatthereare10
23
to10
60
drug-likemolecules.[
2
]Despite
advancesinexperimentaltechniques,itistoodemandingtomoleculessuitable
forspapplicationsonlythroughexperiments.
Computer-aidedmoleculardesignhasattractedmuchattentionasapromisingso-
lutiontoovercometheexperimentallimitation.[
3
,
4
,
5
,
6
]Fastcalculationmethods
alongwithreasonableaccuracyandverylowcostenablehigh-throughputvirtual
screeningtomoleculeswithtargetproperties.Acommonstrategyistose-
lectcomputationallytopmoleculesoutofmillionsofmoleculesinavirtuallibrary
andthenverifythemexperimentally,leadingtoasitreductionintimeand
Moleculesinthelibrarymaynotmeetthegivencriteria.Inthiscase,tra-
ditionaloptimizationmethodssuchasageneticalgorithmcanbeusedtofurther
improvemolecularpropertiesbeyondthecriteriabystructuralmo
7
,
8
,
9
]
However,theyhaveafundamentallimitationintermsofbecausemany
trialsanderrorsareinevitabletooptimizemolecularpropertiesinahugemolecular
space.
Recentlyemerginggenerativemodelsbasedondeeplearningtechniquesmayof-
feraviablesolutionformoretmoleculardesign.omez-Bombarellietal.
adoptedavariationalautoencoder[
10
]tooptimizethemolecularpropertiesinala-
tentspaceinwhichmoleculesareexpressedasarealvector.[
11
]Thekeyadvantage
ofthismethodisthatagradient-basedoptimizationbecomesfeasiblebecausethe
arXiv:1806.05805v1  [cs.LG]  15 Jun 2018Lim
etal.
Page2of
12
latentspaceiscontinuousandtiable.Ithasbeensuccessfullyappliedto
improvingthepartitioncotofdrugcandidatesandthedelayedt
emissionrateoforganiclightemittingdiodecandidates.Blaschkeetal.employed
theadversarialautoencoder[
12
](AAE)andtheBayesianoptimizationtogenerate
ligandssptothedopaminetype2receptor.[
13
]Kadurinetal.comparedthe
VAEandAAEasamoleculargenerationmodelintermsofthereconstructioner-
rorandvariabilityoftheoutputmolecularts.[
14
]Inadditiontothose
autoencoder-basedmodels,agenerativemodeldevelopedfornaturallanguagepro-
cessinghasalsobeenusedformoleculardesign.[
15
,
16
,
17
,
18
]Molecularstructures
canbeexpressedwithSMILES.Then,thismodellearnstheprobabilitydistribution
ofthenextcharacterofagivenpieceofSMILES.Yuanetal.designedpotential
inhibitorsforatargetproteinandtestedtheminexperiments.[
16
]Basedonthe
naturallanguageprocessingmodel,Segleretal.andGuptaetal.appliedtransfer
learningtomoleculardesignforcertainbiologicalactivities.[
17
,
18
]Thisapproach
isespeciallyusefulwhenthereisnotenoughdatatotrainneuralnet-
worksinthenormalway.Olivecronaetal.,Guimaraesetal.,andJaquesetal.
proposedareinforcementlearningmethodtomodifyapre-trainedmoleculargen-
erativemodelsoastoimposeseveralpropertiesinmoleculesgeneratedfromthe
generativemodel.[
19
,
20
,
21
]
Wenotethatvariousmolecularpropertiesarecorrelatedwitheachother.There-
fore,adjustingonetargetpropertybystructuralmomaycauseanunde-
siredchangeinotherproperties.Toavoidthisprobleminrationalmoleculardesign,
onehastocontrolseveralpropertiesatthesametime.Here,weproposeamolecular
generativemodelusingtheconditionalvariationalautoencoder(CVAE)[
22
]suitable
formultivariablecontrol.Inadditiontotheadvantagesofusingthelatentspace,
ourmethodcanincorporatetheinformationofmolecularpropertiesintheencoding
processandmanipulatetheminthedecodingprocess.
Asaproofofconcept,weusedtheCVAEtogeneratedrug-likemoleculessatisfying
etargetpropertiesatthesametime:molecularweight(MW),partitioncot
(LogP),numberofhydrogenbonddonor(HBD),numberofhydrogenacceptor
(HBA),andtopologicalpolarsurfacearea(TPSA).Wewereabletoproducea
numberofmoleculeswiththespvaluesoftheetargetpropertieswithin
agivenrange.Itwasalsopossibletoadjustasingletargetwithoutchangingthe
others.Furthermore,wewereabletogeneratemoleculeswithpropertiesbeyondthe
rangeofthedatabase.
Method
Conditionalvariationalautoencoder(CVAE)
WeselectedtheCVAEasamoleculargenerator.Itisoneofthemostpopular
generativemodelswhichgeneratesobjectssimilartobutnotidenticaltoagiven
dataset.Inparticular,itisdistinguishedfromtheVAEinthatitcanimposecer-
tainconditionsintheencodinganddecodingprocesses.Toelucidatethe
betweenthetwomodels,wecomparedtheirobjectivefunctionswithoneanother.
TheobjectivefunctionoftheVAEisgivenby
E
[log
P
(
X
j
z
)]

D
KL
[
Q
(
z
j
X
)
k
P
(
z
)]
;
Lim
etal.
Page3of
12
where
E
denotesanexpectationvalue,
P
and
Q
areprobabilitydistributions,
D
KL
istheKullback-Leiblerdivergence,and
X
and
z
indicatethedataandlatentspaces,
respectively.Theandsecondtermsarealsocalledthereconstructionerrorand
theKLterm,respectively.Inanautoencoder,
Q
(
z
j
X
)and
P
(
X
j
z
)areapproximated
byanencoderandadecoder,respectively.AkeyoftheCVAEfromthe
VAEistoembedtheconditionalinformationintheobjectivefunctionoftheVAE,
leadingtotherevisedobjectivefunctionasfollow:
E
[log
P
(
X
j
z;c
)]

D
KL
[
Q
(
z
j
X;c
)
k
P
(
z
j
c
)]
;
where
c
denotesaconditionvector.Theconditionvector
c
isdirectlyinvolvedin
theencodinganddecodingprocesses.Inourmodel,themolecularpropertieswe
wanttocontrolwererepresentedastheconditionvector.Asaresult,theCVAE
cangeneratemoleculeswiththetargetpropertiesimposedbytheconditionvector.
IncorporatingmolecularpropertiesintheVAEtogeneratemoleculeswithde-
sirablepropertiesarealsopossiblethroughatwo-stepmodelproposedbyomez-
Bombarellietal.TheVAEistrainedjointlywithanadditionalneuralnetwork
forpropertyprediction.Subsequently,aGaussianprocessmodelcreatesamapping
fromtheresultinglatentspacetotheassociatedmolecularpropertiesFinally,prop-
ertyoptimizationintheresultinglatentspaceisperformedbyagradientdescent
optimizationmethod.
ThekeyofourCVAEmodelfromthejointlytrainedVAEmodelis
thatthemolecularpropertiesaredirectlyincorporatedintoboththeencoderand
decoder.Theresultinglatentvectoriscomposedoftwoparts:thepartis
forthetargetmolecularproperties,whilethesecondpartinvolvesthemolecular
structuresandtheotherproperties.Therefore,thedesiredmolecularpropertiescan
beembeddedinatargetmolecularstructuresimplybysettingaconditionvector.In
otherwords,onecancontrolthestructureandthepropertiesindependentlyexcept
forsomecasesinwhichthepropertiesarestronglycoupledtoamolecular
Thisisparticularlyusefultoincorporateacertainpropertyinagivenmolecule
justwithamarginalstructuremo.Afterall,theCVAEislesssensitive
tothecontinuityandsmoothnessofthelatentspace,becauseitdoesnotrequire
thederivativeofthelatentspacewithrespecttothelatentvectorofthemolecular
structure.AnothertechnicaloftheCVAEfromthejointlytrainedVAE
isthatitdoesnotneedanyfurtheroptimizationprocess,whichisinevitableinthe
jointlytrainedVAEforeachtpropertyvalue.
Molecularrepresentationandmodelconstruction
WerepresentedmoleculeswithSMILEScodestotakeadvantageofstate-of-the-art
deeplearningtechniquesthatarespecializedindealingwithtextsandsequences.
EachSMILEScodewascanonicalizedforauniquemolecularrepresentation.One
'E'waspaddedontheendoftheSMILEScodetoindicatetheendofthestring.
Subsequently,eachcharacterincluding'E'isrepresentedwithaone-hotvector,re-
sultinginaninputmatrix.Eachone-hotvectoroftheinputmatrixistransformed
toanembeddingvectorwiththesizeof300,andthentheinputmatrixisconcate-
natedwithaconditionvector.Thesecond,andlastentriesofthe
Lim
etal.
Page4of
12
conditionvectorarewithinformationconsistingoftheMW,LogP,andTPSA,
respectively,whiletheremainingtwoentriesarelabeledbytheHBDandHBAas
showninFigure1.ThevaluesofMW,logP,andTPSAarenormalizedfrom-1.0to
1.0.HBDandHBAareexpressedwithaone-hotvector,becausetheyareinteger
numbers.
TheresultingmatrixissubjectedtotheencoderoftheCVAEtogeneratealatent
vector.Weadoptedtheso-calledrecurrentneuralnetwork(RNN)withanLSTM
cellforboththeencoderanddecoderoftheCVAE.[
23
]Theyweremadeofa3-
layerRNNwith500hiddennodesoneachlayer.Asoftmaxlayerwasusedineach
outputofthedecodercell,andacrossentropywasusedasthecostfunctionof
thereconstructionerror.Thelatentvectorconcatenatedwiththeconditionvector
becomesaninputofthedecoderateachtimestepoftheRNNcell.Finally,the
outputvectorofeachdecodercellistransformedtoavectorwhosesizeisequal
tothatoftheone-hotvectoroftheinputmatrix.Thesoftmaxactivationfunction
isappliedtoeachtransformedvector.Theencoderanddecoderareoptimizedto
minimizethecostfunctionoftheCVAE.Togenerateamoleculewiththetarget
propertiesimposedbytheconditionvector,thecelloftheRNNdecoderwasun-
rolledfor120times.Allcharactersbefore`E'weretakeninthestochasticwrite-out
process,andif`E'didnotappearinthe120characters,theresultwasconsideredas
invalid.Eachoutputvectorofthedecodercellrepresentstheprobabilitydistribu-
tionoftheSMILEScodecharactersand'E'.Finally,theoutputvectorisconverted
toaSMILEScode.Itshouldbenotedthatevenasinglewrongcharacterinthe
resultingSMILEScodegivesrisetoaninvalidmolecule.Toincreasetherateof
validSMILEScodes,weusedthestochasticwrite-outmethodwhichsampleseach
characterofSMILESaccordingtoaprobabilitydistribution.Asaresult,asingle
setoflatentandconditionvectorsmaygiveanumberoftmolecules.We
performed100timesthestochasticwrite-outperonelatentvectorandtookallvalid
moleculesexceptduplicatedonesforlateranalysis.
Datasetandhyperparameters
RDKit[
24
],anopensourcecheminformaticspackage,wasusedforcheckingoutthe
validityofthegeneratedSMILEScodesandcalculatingtheetargetproperties
ofthemolecules.
ThetotaldatasetismadeofmoleculesrandomlyselectedfromtheZINC
dataset.[
25
]Generally,withmoredata,theperformancebecomesbetter.Typical
deeplearningmodelsneedhundredsofthousandsofdatapoints.Wecheckedoutthe
convergenceoftheresultswithrespecttothesizeofthedatainourcase.Theuseof
5,000,000ZINCmoleculesdidnotincreaseboththevalidationandthesuccessrates
ofgeneratingmoleculeswiththetargetpropertiescomparedtothosefrom500,000
ZINCmolecules.Thus,weadoptedthedatasetofthe500,000molecules,80%of
whichwereusedfortraining,andtherestwasusedforthetest.Thedistribution
oftheetargetpropertiesinthetotaldatasetisshowninFigure2.Thelearning
ratewassetto0.0001andexponentiallydecayedatarateof0.97.Themodelwas
traineduntilconverged.IntheperformanceevaluationoftheCVAE,ifeachtarget
propertyofthegeneratedmoleculeswasdrentfromthegiventargetvaluewith
the10%errorrangeoftheaveragevalueofthetotaldataset,weregardedthose
moleculesassuccessful.
Lim
etal.
Page5of
12
Figure1
Schematicrepresentationofconditionalvariationalautoencoderformoleculardesign
Figure2
Distributionofmolecularweight,LogP,HBD,HBA,andTPSAinthetotaldataset
(500,000).
Result
Astheapplication,wedemonstratedthattheCVAEmethodcangenerate
moleculeswithspvaluesfortheetargetpropertiesbyapplyingittoAspirin
andTThevaluesofthe(MW,LogP,HBD,HBA,andTPSA)forAspirin
andTare(180.04,1.31,1,3,and63.6)and(312.2,1.285,2,5,and90.64),
respectively.Theconditionvectorofeachmoleculewasmadebythosevalues.Latent
vectorstobeconcatenatedwiththeconditionvectorweresampledbyaddinga
Gaussiantypenoisetothelatentvectorofamoleculeselectedrandomlyinthe
trainingset.Figures3Aand3Bshowninemoleculesproducedwiththecondition
vectorofAspirinandTrespectively.Allofthemhadsimilarpropertiesto
thoseofAspirinandTwithinanerrorrangeof10%,respectively.However,
themolecularstructuresinFigure3areconsiderablytfromthoseofthe
originalmoleculesbecauseofthelatentvectorschosenrandomlyfromthetraining
set.
Thesecondapplicationwastogeneratemoleculessimilarinbothpropertiesand
structuretothemothermoleculebysamplinglatentvectorsaroundthatofthe
mother.Figure4showsthemoleculesgeneratedinsuchawayfromAspirin.They
lookverysimilartoAspirinandalsohavesimilarpropertieswiththoseofAspirin
withinanerrorrangeof10%.
Asthethirdcasestudy,wetestedwhethertheCVAEmethodcanchangeonlya
singlepropertywithoutchangingtheothers.Theconditionvectorwasconstructed
withtheMW,HBD,HBA,andTPSAofTandwevariedLogPfrom0.0
Lim
etal.
Page6of
12
Figure3
MoleculesgeneratedbytheCVAEwiththeconditionvectormadeofthetarget
propertiesof(A)Aspirinand(B)T
Figure4
MoleculesgeneratedbytheCVAEwiththeconditionvectormadeofthetarget
propertiesofAspirinandthelatentvectorslightlymofromthatofAspirin.
Lim
etal.
Page7of
12
to3.0.LatentvectorsweresampledaroundthatofTFigure5showsthe
result.AllthemoleculeshavesimilarpropertiestotheoriginalonesexceptLogP
asdesired.Themoleculesfromthetoplefttothebottomrighthavegradually
increasingLogPvaluesfrom-0.23to3.55.Insomecases,however,suchadelicate
controlofindividualpropertieswasnotpossible.Forinstance,wecouldnotgenerate
moleculeswithaLogPbeyond4.0.ItisprobablybecauseLogPisnotcompletly
independentfromtheotherfourproperties,soasubstantialchangeinLogPentails
achangeintheothers.Moreover,itwastoadjusttheMWandTPSA
independentlybecausetheMWandTPSAarehighlycorrelatedwithoneanother.
Finally,weinvestigatedthepossibilitytochangeaspmolecularproperty
beyondtherangeofatrainingset.Latentvectorsweresampledaroundmolecules
inthetrainingset.Intheconditionvector,theepropertiesweregivenrandomly
exceptforasingletargetproperty.Thetargetpropertywassetto10%largerthan
itsmaximumvalueinthetrainingset(e.g.,5.5forLogPand165.5forTPSA).
Figure6showstheresultingmolecules.Indeed,itwasabletogeneratemolecules
withaLogPlargerthan5.5(Figure6A)andmoleculeswithaTPSAlargerthan
165.5(Figure6B).WecomparedthedistributionoftheLogPandTPSAfor1,000
randomlyselectedmoleculesfromthetrainingsetand1,000generatedmolecules
withpropertyvaluesoutsideoftherangeofthedataset(towardlargervalues).
Figure7showsthatthedistributionofthetargetpropertiesareshiftedtolarger
values,leadingtoanincreasedratioofmoleculeswithpropertyvaluesoutsideof
Figure5
MoleculesgeneratedbytheCVAEwiththeconditionvectormadeofMW,HBD,HBA,
andTPSAofTandcontinuouslychangingLogP.
Lim
etal.
Page8of
12
therange.Therateofvalidmoleculesisrelativelylowcomparedtothecaseof
generatingmoleculeswithpropertyvaluesintherangeofthedataset.
Figure6
(A)MoleculeswithLogPlargerthan5.5.(B)MoleculeswithTPSAlargerthan165.
Figure7
DistributionofA)LogPandB)TPSAfor1000randomlyselectedmoleculesintraining
setand1000generatedmoleculeswithLogPandTPSAoutsideoftherangeofthedataset,
respectively.
WeanalyzedthelatentspaceconstructedbytheCVAE.Twoprincipleaxeswere
extractedbyprincipalcomponentanalysis.Figure8showsthetwocomponentsof
thelatentvectorsof1000randomlyselectedmoleculesfromthetestsetwiththeir
LogPandTPSAvalues.Moleculeswithsimilarpropertiesarelikelylocatedaround
Lim
etal.
Page9of
12
Table1
Numbersofattemptsandvalidmoleculesforgenerating100moleculeswhoseveproperties
arethesamewiththoseofAspirin,TLenalidomide,Rivaroxaban,andPregabalin.
condition
random
attempts
molecules
valid
numberof
%)
(100/attempts,
successrate
attempts
molecules
valid
numberof
%)
(100/attempts,
successrate
Aspirin28,84032,5670.34758,199711,6600.014
T15,96034,6960.62798,183741,9600.013
Lenalidomide502,0089,2300.19865,695822,0600.012
Rivaroxaban92,62047,5740.11866,205817,8000.012
Pregabalin77,68084,3710.13782,010723,3600.014
asameregionofthelatentspaceinthejointly-trainedVAE.InourCVAEmodel,
thelatentvectoriscomprisedoftwopartsasexplainedinthemethodsection.
Therefore,aspregioninthelatentspacedoesnotnecessarilyhaveacorrelation
withthetargetmolecularpropertieswhicharecontrolledbytheconditionvector.
Thisisgoodbecausetheseparationofinformationenablesamorecontrol
ofthemolecularstructureandpropertieswhengeneratingnewmolecules.
Figure8
Thelatentspaceof1000randomlyselectedmoleculeswithMW,LogPandTPSA
values.
ApartfromthesuccessfulapplicationsoftheCVAEmethod,ithasadrawback
thatshouldberesolved.Thesuccessrateofgeneratingdesirablemoleculesisvery
low.Wetestedhowmanyattemptsarerequiredtogenerate100moleculeswith
theedesiredpropertiesandhowmanyvalidmoleculesaregeneratedduring
thoseattempts.Wealsocomparedwhentheconditionvectorissetrandomlyorto
targetpropertiestoshowtheoftheconditionvectorforgeneratingdesirable
molecules.
Table1summarizesthenumberofattemptsforgenerating100moleculeswhose
epropertiesaresameasthoseofaspirin,TLenalidomide,Rivaroxaban,
andPregabalin,respectively.Lenalidomide,Rivaroxaban,andPregabalinaretop
sellingsmallmoleculedrugsin2016.[
26
]InTable1,'condition'meansthatthecon-
ditionvectorwassetastheepropertiesofthetargetmolecules,whereas'random'
meansthattheconditionvectorwasrandomlymade.Thenumberofvalidmolecules
inTable1indicatesthenumberofvalidmoleculesgeneratedduringtheattempts
tocreatemoleculeswiththeedesiredproperties.Forexample,100aspirin-like
moleculesand32,567validmoleculceswereobtainedfrom28,840attemptstocreate
aspirin-likemolecules.Thereasonwhythenumberofvalidmoleculesislargerthan
thenumberofattemptsisthatthestochasticwrite-outprocessisperformed100
timesforeachattempt.Allsuccessfulmolecules(100pereachtargetmolecule)are
reportedintheSupportingInformation.Itshouldbenotedthatthesuccessrate
dramaticallydroppedwhentheconditionvectorisrandomlyset.Itclearlymani-
feststhatthesuccessfulmoleculesgeneratedbytheCVAEintheexamplestudies
werenottheresultofmanyrandomtrials.
Lim
etal.
Page10of
12
Table2
Numberofgenerationattemptsandnumberofvalidmoleculesforthreesampling
methodsoflatentvectors.Thegenerationprocesswascontinueduntil100moleculeswiththe
targetpropertiesweresuccessfullycreatedfromasingletargetmolecule,anditwasrepeatedfor100
targetmoleculesselectedrandomlyfromtheZINCdataset.Thetableshowstheaveragevaluesover
the100targetmolecules.
samplingmethod
validmolecules
averagenumberof
attempts
averagenumberof
(100/attempts,%)
successrate
aroundtargetmolecules67640.5199340.10.05
aroundknownmolecules31799.421659.90.46
random50316.478888.20.12
WefurtheranalyzedtheperformanceoftheCVAEbyinvestigatingthechangein
thesuccessrateandthenumberofvalidmoleculesaccordingtolatentvectorsam-
plingmethods.Weemployedthreetsamplingmethods:random,aroundthe
latentvectorsofknownmolecules,andaroundthelatentvectorsoftargetmolecules.
Forallthesamplingmethods,theconditionvectorwasconstructedusingthee
propertiesofthetargetmolecules.Thegenerationprocesswascontinueduntil100
moleculeswiththeetargetpropertiesweresuccessfullycreatedfromasingle
targetmolecule,anditwasrepeatedfor100targetmoleculesselectedrandomly
fromtheZINCdataset.Table2showstheaveragevaluesforthesuccessrateand
thenumberofvalidmoleculesoverthe100targetmolecules.Itwasunexpected
thatsamplinglatentvectorsaroundatargetmoleculewasthemostein
termsofthesuccessrateandvalidmoleculesbecauseofthehighrateofduplicated
molecules.Inthiscase,thestructureofthegeneratedmoleculeswasverysimilarto
thatofthetargetmoleculeshowninFigure4.Samplinglatentvectorsaroundthose
ofknownmoleculesperformedbest.Becausetheknownmoleculeswererandomly
selectedfromtheZINCset,theirstructuresandpropertieswouldbeconsiderably
tfromthoseofatargetmolecule.Nonetheless,wewereabletogenerate
moleculeswiththedesiredpropertiesfromthoselatentvectorswitharelatively
highsuccessrate.Itmanifeststhattheconditionvectorappropriatelymothe
molecularstructurestohavethetargetproperties.Finally,itwasalsopossibleto
generatedesirablemoleculesfromcompletelyrandomlatentvectorsbutwithalow
successrate.
Wesuspectthatatsomeparttheoveralllowsuccessratesregardlessofthela-
tentvectorsamplingmethodsareduetothestrongcorrelationbetweenthee
targetproperties.Inaddition,itisknownthatthediscretenatureofSMILES
causesahighrateofinvalidmoleculesinthedecodingprocessfromlatentvec-
torstomolecules.[
27
]Thestochasticwrite-outmethodcircumventsthisproblem,
butmorefundamentalsolutionsshouldbedevised.Moreseverely,SMILESdoes
nothavethe3Dconformationalinformationofmolecularstructures.Therefore,it
musthavelimitationsinapplicationsinwhichconformationalarecritical.
Moleculargraphrepresentationincorporatingconformationalinformationcanbe
apromisingalternative.Encodingmoleculargraphsseemstobestraightforward,
butdecodingfromalatentspacetomoleculargraphsisstillanopenproblem.Re-
cently,tprogressalongthislinehasbeenmade.[
28
,
29
,
30
]Suchabetter
molecularrepresentationmayalsoimprovethesuccessrateofmoleculargeneration.
Weexpectthatthesuccessratemaybefurtherimprovedbyusingthegrammar
variationalautoencoder[
27
]andthereinforcementlearning.[
19
,
20
]
Lim
etal.
Page11of
12
Conclusion
Weproposedanewmoleculardesignstrategybasedontheconditionalvariational
autoencoder.Insteadofhigh-throughputvirtualscreening,ourmethodasoneofthe
deeplearning-basedgenerativemodelsdirectlyproducesmoleculeswithdesirable
targetproperties.Inparticular,itsstrengthiscontrollingmultipletargetproperties
simultaneouslybyimposingthemonaconditionvector.Wedemonstratedthatit
waspossibletogeneratedrug-likemoleculeswithspvaluesfortheetarget
properties(MW,LogP,HBD,HBA,andTPSA)withinanerrorrangeof10%.
Inaddition,wewereabletoselectivelycontrolLogPwithoutchangingtheother
propertiesandtoincreaseasppropertybeyondtherangeofthetrainingset.
Thus,thisnewmethodhasattractiveapplicabilityfortmoleculardesign.
Competinginterests
Theauthorsdeclarethattheyhavenocompetinginterests.
Author'scontributions
JaechangLim,SeongokRyu,JinWooKim,andWooYounKimorganizedthiswork.JaechangLimandWooYoun
Kimwrotethepaper
Acknowledgements
ThisworkwassupportedbyBasicScienceResearchProgramsthroughtheNationalResearchFoundationofKorea
(NRF)fundedbytheMinistryofScience,ICTandFuturePlanning(NRF-2017R1E1A1A01078109).
Authordetails
1
DepartmentofChemistry,KAIST,291Daehak-ro,34141Daejeon,RepublicofKorea.
2
KIfor
Intelligence,KAIST,291Daehak-ro,34141Daejeon,RepublicofKorea.
References
1.
S.Kim,P.A.Thiessen,E.E.Bolton,J.Chen,G.Fu,A.Gindulyte,L.Han,J.He,S.He,B.A.Shoemaker,
J.Wang,B.Yu,J.Zhang,S.H.Bryant,NucleicAcidsResearch
44
(D1),D1202(2016).
2.
P.G.Polishchuk,T.I.Madzhidov,A.Varnek,JournalofComputer-AidedMolecularDesign
27
(8),675(2013).
3.
B.K.Shoichet,Nature
432
(7019),862(2004)..URL
http://www.nature.com/doifinder/10.1038/nature03197
4.
T.Scior,A.Bender,G.Tresadern,J.L.Medina-Franco,K.Martyorga,T.Langer,
K.Cuanalo-Contreras,D.K.Agraotis,JournalofChemicalInformationandModeling
52
(4),867(2012)..
URL
http://pubs.acs.org/doi/10.1021/ci200528d
5.
T.Cheng,Q.Li,Z.Zhou,Y.Wang,S.H.Bryant,TheAAPSJournal
14
(1),133(2012)..URL
http://www.springerlink.com/index/10.1208/s12248-012-9322-0
6.
J.L.Reymond,R.vanDeursen,L.C.Blum,L.Ruddigkeit,MedChemComm
1
(1),30(2010)..URL
http://xlink.rsc.org/?DOI=c0md00020e
7.
T.Miyao,H.Kaneko,K.Funatsu,MolecularInformatics
33
(11-12),764(2014).
8.
M.Hartenfeller,G.Schneider,WileyInterdisciplinaryReviews:ComputationalMolecularScience
1
(5),742
(2011)..URL
http://doi.wiley.com/10.1002/wcms.49
9.
C.Rupakheti,A.Virshup,W.Yang,D.N.Beratan,JournalofChemicalInformationandModeling
55
(3),529
(2015)..URL
http://pubs.acs.org/doi/10.1021/ci500749q
10.
D.P.Kingma,M.Welling,(2013).URL
http://arxiv.org/abs/1312.6114
11.
R.omez-Bombarelli,J.N.Wei,D.Duvenaud,J.M.Hernandez-Lobato,B.Sanchez-Lengeling,D.Sheberla,
J.Aguilera-Iparraguirre,T.D.Hirzel,R.P.Adams,A.Aspuru-Guzik,pp.1{26(2016).URL
http://arxiv.org/abs/1610.02415
12.
A.Makhzani,J.Shlens,N.Jaitly,I.Goodfellow,B.Frey,(2015).URL
http://arxiv.org/abs/1511.05644
13.
T.Blaschke,M.Olivecrona,O.Engkvist,J.Bajorath,H.Chen,(2017).URL
http://arxiv.org/abs/1711.07839
14.
A.Kadurin,S.Nikolenko,K.Khrabrov,A.Aliper,A.Zhavoronkov,MolecularPharmaceutics
14
(9),3098
(2017)..URL
http://pubs.acs.org/doi/abs/10.1021/acs.molpharmaceut.7b00346http:
//pubs.acs.org/doi/10.1021/acs.molpharmaceut.7b00346
15.
E.J.Bjerrum,R.Threlfall,(2017).URL
http://arxiv.org/abs/1705.04612
16.
W.Yuan,D.Jiang,D.K.Nambiar,L.P.Liew,M.P.Hay,J.Bloomstein,P.Lu,B.Turner,Q.T.Le,
R.Tibshirani,P.Khatri,M.G.Moloney,A.C.Koong,JournalofChemicalInformationandModeling
57
(4),875
(2017)..URL
http://arxiv.org/abs/1611.02796http://pubs.acs.org/doi/10.1021/acs.jcim.6b00754
17.
M.H.S.Segler,T.Kogej,C.Tyrchan,M.P.Waller,pp.1{17(2017).URL
http://arxiv.org/abs/1701.01329
18.
A.Gupta,A.T.Muller,B.J.H.Huisman,J.A.Fuchs,P.Schneider,G.Schneider,MolecularInformatics
1700111
(2017)..URL
http://doi.wiley.com/10.1002/minf.201700111
19.
M.Olivecrona,T.Blaschke,O.Engkvist,H.Chen,JournalofCheminformatics
9
(1),1(2017)..URL
http://arxiv.org/abs/1704.07555
20.
G.L.Guimaraes,B.Sanchez-Lengeling,C.Outeiral,P.L.C.Farias,A.Aspuru-Guzik,(2017).URL
http://arxiv.org/abs/1705.10843
Lim
etal.
Page12of
12
21.
N.Jaques,S.Gu,D.Bahdanau,J.M.Hernandez-Lobato,R.E.Turner,D.Eck,(2016).URL
http://arxiv.org/abs/1611.02796
22.
D.P.Kingma,D.J.Rezende,S.Mohamed,M.Welling,pp.1{9(2014).URL
http://arxiv.org/abs/1406.5298
23.
S.Hochreiter,J.UrgenSchmidhuber,NeuralComputation
9
(8),1735(1997)..URL
http://www7.informatik.tu-muenchen.de/{
~
}hochreit{%}5Cnhttp://www.idsia.ch/{
~
}juergen
24.
rdkit.URL
http://www.rdkit.org/
25.
J.J.Irwin,T.Sterling,M.M.Mysinger,E.S.Bolstad,R.G.Coleman,JournalofChemicalInformationand
Modeling
52
(7),1757(2012)..URL
http://pubs.acs.org/doi/10.1021/ci3001277
26.
Topsellingsmallmoleculedrugsin2016.URL
https://www.genengnews.com/the-lists/the-top-15-best-selling-drugs-of-2016/77900868
27.
M.J.Kusner,B.Paige,J.M.Hernandez-Lobato,(2017).URL
http://arxiv.org/abs/1703.01925
28.
H.Wang,J.Wang,J.Wang,M.Zhao,W.Zhang,F.Zhang,X.Xie,M.Guo,(2017).URL
http://arxiv.org/abs/1711.08267
29.
J.You,R.Ying,X.Ren,W.L.Hamilton,J.Leskovec,(2018).URL
http://arxiv.org/abs/1802.08773
30.
W.Jin,R.Barzilay,T.Jaakkola,
"
71,Weakly Supervised Deep Image Hashing through Tag Embeddings,http://arxiv.org/pdf/1806.05804v3.pdf,https://github.com/Vijetha1/WDHT,"WeaklySupervisedDeepImageHashingthroughTagEmbeddings
VijethaGattupalli,YaoxinZhuo,BaoxinLi
ArizonaStateUniversity
f
vijetha.gattupalli,yzhuo6,baoxin.li
g
@asu.edu
Abstract
Manyapproachestosemanticimagehashinghavebeen
formulatedassupervisedlearningproblemsthatutilizeim-
agesandlabelinformationtolearnthebinaryhashcodes.
However,large-scalelabelledimagedataisexpensiveto
obtain,thusimposingarestrictionontheusageofsuch
algorithms.Ontheotherhand,unlabelledimagedatais
abundantduetotheexistenceofmanyWebimagereposi-
tories.SuchWebimagesmayoftencomewithimagestags
thatcontainsusefulinformation,althoughrawtagsingen-
eraldonotreadilyleadtosemanticlabels.Motivatedby
thisscenario,weformulatetheproblemofsemanticimage
hashingasaweakly-supervisedlearningproblem.Weuti-
lizetheinformationcontainedintheuser-generatedtags
associatedwiththeimagestolearnthehashcodes.More
,weextracttheword2vecsemanticembeddings
ofthetagsandusetheinformationcontainedinthemfor
constrainingthelearning.Accordingly,wenameourmodel
WeaklySupervisedDeepHashingusingTagEmbeddings
(WDHT).WDHTistestedforthetaskofsemanticimage
retrievalandiscomparedagainstseveralstate-of-artmod-
els.Resultsshowthatourapproachsetsanewstate-of-art
intheareaofweeklysupervisedimagehashing.
1.Introduction
SemanticImageHashinghasbeenanactiveresearch
areaforthepastfewyearsduetoitsabilitytobothsearch
andstoremassiveimagedatabasesef.,itis
thetaskofsemanticallymappingimagestobinarycodes
suchthatsomenotionofsimilarityispreserved.Inthisset-
ting,similarityisdeterminedbythegroundtruthclassla-
bels,whichareexpensivetoobtain.Thisimposesarestric-
tionontheamountoftrainingdataavailable.Ontheother
hand,muchwebimagedataavailabletodayhaveassoci-
atedtextualmeta-data(tags).Suchtaginformationisoften
readilyavailableandisinexpensive.Owingtothesefacts,
inthispaper,weattempttheproblemofweeklysupervised
semanticimagehashingbyleveragingthetaginformation
associatedwiththeWebimages.
Sample1Sample2Sample3
Images
Tags
#india#cinema#movie#star
#still#handsome#bollywood
#actor#khan#shahrukh#srk
#omshantiom
#sunset#bali#r
#indonesia#mirror#asia
#boda#mariage#hochzeit
#indonesien#heirat#chappel
#conradhotel#50faves
#justimagine#weddingchappel
#perfectangle
#megashot#theroadtoheaven
#thegoldendreams
#wood#trees#fence#track
#derbyshire#farming
#wideangle
#agriculture#grassland
#sigma1020#autums#marlock
#holestone#holestonemoor
Labels
dancing
buildings,clouds,
sky,sunset
grass,sky,tree
Table1:Tableshowingtheimage-tag-labeltripletsforsome
randomsamplesfromNUS-WIDEdataset.
Thecurrentproblemisaddressedasweaklysupervised
mainlyduetothefollowingreasons.Tagsmaycontain
someinformationrelatedtothesemanticsoftheimages.
However,itisnon-trivialtoextractexplicitlabelinforma-
tionfromrawtags.Table1illustratesthreesamplesfrom
theNUS-WIDEdataset.Itcanbenoticedthatsample1has
notagsthataredirectlyassociatedwiththelabelﬁdancingﬂ.
Whilesamples2and3havesometagsthatconveylabel
information,theystillhavetheirownshortcomings.For
example,theyareassociatedwithtoomanyuninformative
tags.Theseuninformativetagsmaybeaconsequenceofthe
social-mediabehaviourofthepubliclikeopinionexpres-
sion,selfpresentation,attractingattentionetc[1].Thisre-
sultsintagsthatmaybesubjective(eg.
#thegoldendreams,
#handsome,#50faves
),purelycontextoriented(eg.
#india,
#conradhotel#katrina
),photographyrelated(
#wideangle
)
etc.Thusthesetagscontaininformationwhichisnotre-
latedtotheimagecontent,makingtheprocessofextract-
inglabelsfromtagsfurtherdifTherearesomeprior
works[1],[2]thatattemptedtoaddressthedifesin
extractinginformationfromrawtags.
1
arXiv:1806.05804v3  [cs.CV]  28 Jan 2019Eventhoughourworkfocusesonusingtaginforma-
tiontoassistinlearningthehashspace,ouralgorithmdoes
notfallunderthecategoryofcross-modalhashing(CMH).
CMHdealswithlearninghashspacesthataresharedfor
samplesfromvariousmodalities.Ideally,aspacethuslearnt
shouldbeabletoretrievesamplesfromonemodalitybyus-
ingquerysamplesfromadifferentmodality(e.g.,retrieving
images/videosusingtextqueriesandviceversa)[3].Our
workonlydealswithdirectimagehashingwherethequery
andretrievalsamplesareimages.Weonlyutilizetheinfor-
mationfromtagstolearnbetterhashspacesforsemantic
imageretrieval.Further,muchworkinCMHassumesthe
availabilityofimage-tag-labeltripletsandusethisinforma-
tiontolearnthesharedhashspace.Thustheycanbecalled
supervisedlearningapproaches,whileoursisaweaklysu-
pervisedapproach.
Akeycomponentofourmethodistheutilizationofthe
word2vec
model[4],whichisamethodforembeddingEn-
glishwordsontoavectorspacesuchthatthecosinesimilar-
itybetweenthevectorsofthewordsisinaccordancewith
theirsemanticsimilarity.Inourtask,the
<
image,tagset
>
pairsarefromtheWebimagedatasets,andthetagsgen-
erallybearsomerelevancetothesemanticsoftheimage
(albeitthisrelevancemaybeweak,noisy,andincomplete).
Henceweemploythe
word2vec
representationofthetagsin
ourmodel,andregularizethelearnedhashspaceinsucha
waythatimageshavingsimilartagvectorsshouldhavesim-
ilarhashcodes.Usingthewordvectorsofthetagsmaylead
toabettersemantichashspaceascomparedtousingonly
thebinarytagvectorsthemselves.Forexample,ifthetrain-
ingdatacontainsimagesofcatsanddogs,andseveralother
non-animalclasses,wewouldwantthehashsub-spacesof
thecatsandthedogstobeclosetoeachother.Further,an
animalinatestset(forexamplehorse),whosetrueclass
isnotinthetrainingsetwouldideallybemapped
toacodeclosertothecombinedsub-spaceofthecatand
thedog,thantoothernon-animalclasses.Suchdesired
arrangementofthesub-spacescouldbenaturallyattained
throughemployingtheword-vectorsimilaritiesofthetags
duringtraining.
Inthiswork,weproposeadeepneuralnetwork,com-
pletewithalearningalgorithm,forweaklysupervised
learningofasemantichashingmodelthroughusingthe
wordembeddingsoftheimagetags.Tothebestourknowl-
edge,thisistheworktouseanend-to-enddeepmodel
tolearnhashvectorsusingimagesandtagsalone(without
usinglabels).Ontheparticulartaskofimagehashing,our
methodappearstobetheworkonusingwordembed-
dingsoftagsinaweaklysupervisedsetting.Weevaluate
ourapproachandreportsystematiccomparisonwithrele-
vantstate-of-the-art,andourapproachisshowntooutper-
formexistingunsupervisedorweaklysupervisedhashing
methodsforsemanticimageretrieval.
2.RelatedWork
Mucheffortintheareaofsemanticimagehashinghas
beendirectedtowardsutilizingsupervisedmethodologies
tolearnthehashspace.Whilethereissomeworkinthe
areaofunsupervisedhashing,verylittleattemptwasmade
intheareaofweaklysupervisedhashing.Sincethenumber
ofweaklysupervisedhashingtechniquesareverylimitedin
number,wecompareourmodeltobothweaklysupervised
andunsupervisedmethodsduringevaluation.Onsimilar
lines,inthissection,wegiveabriefoverviewoftherelated
workfromboththeareas.
TheforemostimagehashingalgorithmcalledtheLocal-
itySensitiveHashing[5]worksontheprincipleofproject-
ingthedataontorandomhyperplanesandcomputingeach
bitbasedonwhichhalf-spacethesamplefallsinto.This
algorithmisdata-independentandthereforetheproduced
hashcodesdonotcapturethestructureinthedata.Several
variants([6],[7],[8])havebeenproposed,allproducing
hashcodesirrespectiveofthedistributionofthedata.
Anotherparadigmofimagehashingisthedata-
dependenthashingmethods.Traditionallydata-dependent
methodshavebeenformulatedasindependentfeaturelearn-
ingandhashcodingstages.However,withtheadventof
deeplearningandthehugeamountofdataavailable,lit-
eraturehasmovedtowardslearninghashcodesassingle
stagealgorithms,whichtakeinimagepixelsasinputsand
directlylearnthehashcodes.Thiscanalsobeinterpreted
asaninbuiltfeaturelearningtechniquethatdoesnotrequire
humanintervention.
Approachessuchas[9],[10],[11]aresomerepresenta-
tiveworksofnon-deeplearningbasedunsupervisedlearn-
ing.[9]triedtominimizethequantizationerrorbetween
thereal-valueduncorrelatedfeaturevectorandthebinary
codebyarotationofthezero-centereddata.[10]
showedtheanalogybetweentheproblemoftheop-
timalhashspacedistributionandgraphpartitioningalgo-
rithmandattemptedtheproblemusingspectralways.[11]
attemptedtheproblemoflearninghashspacesinasemi-
supervisedwaybybackpropagatingtheloss
overalimitedlabeleddata-setandanentropybasedloss
overtheentirelabelledandunlabelleddata-set.
Representativedeep-learning-basedunsupervisedhash-
ingalgorithmsinclude[12],[13],[14].Theworkof[12],
thoughbeingdeep-learning-based,isnotanend-to-end
frameworkthatcantakeinrawimagesandproducethe
hashes.TheyusedGISTfeaturesasinputstotheneuralnet-
workandlearnedthehashcodesbyminimizingthequan-
tizationloss,maximumvarianceloss,andtheindependent
bitloss.Thekeyideaof[13]istoproducerotationinvari-
antbinarycodesandshowedthattheyachievestate-of-art
performanceonthreedifferenttasksnamely,imagematch-
ing,imageretrievalandobjectrecognition.Theapproach
of[14]learnshashcodesastheoutputsofthehiddenlayer
2
ofabinaryauto-encoder.Thismakesthelearningproblem
NP-hardandtheyresorttoanalternateoptimizationscheme
tomovetowardsthedesiredhashspace.
Anothernote-worthymentionintheareaofuni-modal
imagehashingis[15].Theyutilizedthewordembedding
oflabelsasthesupervisiontolearnanimagehashspace.
Whilethisappearssimilartoourwork,theyusedvectorrep-
resentationsoflabels,renderingtheworktofallunderthe
categoryofsupervisedimagehashing,whereasourwork
usesvectorrepresentationsofrawtags.
Acommoncharacteristicamongmostdeeplearningand
non-deep-learningbasedsemantichashingmethodsisthat
theyrelyonlyontheinformationfromtheimagestolearn
thehashcodes,oftencompletelyignoringotherassociated
metadata.Severalworks([16],[17],[18])intheareaof
CrossModalHashing(CMH)attemptedutilizingtagin-
formationalongwithimagedatatolearnthehashspace.
However,asmentionedpreviously,theylearnacommon
hashspaceforvariousmodalitiesofinput(imageandtag
inthiscase),whichisdifferentfromwhatweintendtodo.
AmongalltheCMHmethods,[17]istheclosestapproach
toourwork.[17]intendstoalignthevisualspaceofim-
agesandthesemanticspaceofsentencesusinglanguage
(
word2vec
)andvision(CNNbased)models.Themaindif-
ferencebetweentheirworkandoursisthatweattemptto
usetaginformationwhichismuchnoisythantheactual
Englishsentencestheyusedintheirwork.Practically,such
cleanEnglishsentencesareashardtoobtainasthesuper-
visedlabelinformation.AnextensivediscussiononCMH
anduni-modalhashlearningcanbefoundin[3]and[19]
respectively.
UnlikeCMH,weaklysupervisedhashingmethodslever-
ageonlytheimage-taginformationduringtraining.[20],
[21],[22]aresomewell-knownworksinthisarea.Theau-
thorsof[20]proposedaframeworkwhichconsistoftwo
stages,weaklysupervisedpre-trainingandus-
ingsupervisedlabels.[21]usedcollaborativewith
hashinginpredictingtheimage-labelassociations,where
theground-truthlabelsareusedtogeneratethelabelmatrix.
Toourbestknowledge,[22]istheonlypriorapproachthat
attemptedtrulyweeklysupervisedhashing(i.e.,withoutus-
inglabelinformation).More,theyattempted
toexplorethediscriminativeinformationandthelocalgeo-
metricstructurefromthetagsandimages.Theythenformu-
latedthehashingproblemasaneigenvalueproblem.Con-
sideringthesefacts,weonlycompareourapproachto[22]
amongtheweekly-supervisedmethods.
Inthiswork,weintendtobuildanend-to-enddeeplearn-
inghashingmodelthatdoesnotrequireexpensivelabelsin
trainingbutcanstillgeneratesemanticallymeaningfulhash
codes.Intheexperimentssection,wecompareourmodel
tothefollowingunsupervisedandweaklysupervisedimage
hashingapproaches:[9],[10],[11],[11],[23],[24],[25],
[26],[14],[21],[22].Additionally,in-ordertoshowthe
oftheusageoftagembeddings,wedeveloped
adeeplearningbasedbaselinewhichintendstolearnase-
mantichashspaceusingonlythebinarytagvectors.More
detailsaboutourapproachandthebinarytagvectormodel
arepresentedinthenextsection.
3.ProposedApproach
3.1.ProblemFormulation
Inthisworkweassumethatthedatasetshavetripletsof
image-tags-labels(
x
i
,
T
i
,
l
i
).Here,
x
i
representstheimage
featurevectorforthe
i
th
sample,
T
i
representsthecorre-
spondingtagssetand
l
i
representsitsbinarylabelvector.
Inagenericscenario,eachsampleisassociatedwithmore
thanonetagandmorethanonesemanticlabel.Therefore,
thetagsarerepresentedasaset
T
i
andthelabelsarerepre-
sentedasabinaryvector
l
i
.Inthelabelvector,thevalueof
anelementis
1
ifthecorrespondinglabelisassociatedwith
thatimageandis
0
otherwise.Ourtaskistoafunc-
tion


)
thattakes(
x
i
,
T
i
)asinputsandproducesahash
vector
b
i
asoutput.Thehashspacethuslearntshouldmap
semanticallysimilarimages,bythelabelvectors,
tonearbyhashcodesanddissimilaronestofarthercodes.
Whilethelabelsassumedtobeunavailableduringthetrain-
ingphase,theyareemployedduringthetestingphaseto
measuretheperformanceofthelearntmodel.
3.2.TagProcessing
Let
˝
j
i
representataginthetagset
T
i
,.where
j
isthein-
dexofthetagintheset,i.e.,
j
2
[1
;m
]
where
m
isthetotal
numberoftagsassociatedwiththe
i
th
sample.Weconvert
eachtag
˝
j
i
intoa
d
-dimensionalvectorusingthe
word2vec
languagemodel[4].Thusforeachtag
˝
j
i
,weobtainavec-
torrepresentation
v
j
i
whichisthe
word2vec
representation
ofthetagword
˝
j
i
.Sinceeachimagehasmultipletagsasso-
ciatedwithit,weaggregateallthetagvectorsintoasingle
d
-dimensionalvectorforagivenimage.Inthiswork,we
adoptedbasicfunctionslike
tf
(tagfrequency),
itf
(inverse
tagfrequency)and
mean
tocomputetheaggregatedvec-
tor
w
i
.Inexperiments,wewillcomparetheseaggregation
techniquesbytheirperformance.
Theformulaeusedtocompute
w
i
aregivenbelow.
mean
:
w
i
=
1
m
m
X
j
=1
v
j
i
tf
:
w
i
=
1
m
m
X
j
=1
n
(
˝
j
i
)
N
v
j
i
itf
:
w
i
=
1
m
m
X
j
=1
log
N
n
(
˝
j
i
)
v
j
i
(1)
3
Figure1:NetworkArchitectureoftheproposedmodel.Theoutergreenboxrepresentsthepre-trainedAlexNetmodel;The
FC3,H1andH2layersarethenewlyappendedlayers.ThehashcodesareextractedfromtheH1layer.
Here,
N
representsthetotalnumberoftagsinthedatabase
and
n
(
˝
j
i
)
representsthenumberofimagesassociatedwith
thetag
˝
j
i
.
Thuswearriveatthe
image-tagvector
(
x
i
,
w
i
)pairs
fromtheinitial
image-tagset
(
x
i
,
T
i
)pairs.
3.3.DesigningaNetworkforHashing
Weusethepre-trainedAlexNetmodelasakeybuild-
ingblockforourhashingmodel.Thenetworktakes
227X227X3dimensionalimagesasinputandpassesthem
throughveconvolutionallayersandtwofullyconnected
layers,labelledasCONV
i
(
i
=1,...,5),FC1andFC2.Until
theFC2layer,thearchitectureisidenticaltotheAlexNet
[27]architectureandtheweightsareinitializedtothepre-
trainedImageNet[28]weights.TheFC2layerproducesa
4096
dimensionalvector,whichisgivenasinputtoanother
fullyconnectedlayerFC3.FC3outputs
256
dimensional
vectorwhichisfurtherfullyconnectedtotwolayersH1
andH2inalateralfashion.TheacronymsH1andH2rep-
resentthe
Head1
and
Head2
respectively.Theoutputsof
H1andH2are
b
(numberofbitsinthehashcode)and
d
(dimensionalityoftheaggregatedtagvector)dimensional
vectors,whicharethentoppedby
sigmoid
and
tanh
activa-
tionsrespectively.TheoverallmodelisshowninFigure1.
ThenewlayersbeyondtheAlexNetlayersareinitialized
with
glorotnormal
[29]weights.TheVGG-19network
wasalsoattempted,givingresultssimilartothoseofthe
AlexNetmodelbutwithmuch-increasedtrainingtime.We
thereforedecidedtotrainallourmodelsusingtheAlexnet
model.
Themodelistrainedonthreelosscomponentsback
propagatingfromthetwoheadsH1andH2intothenet-
work.More,webackpropagatepair-wisesim-
ilaritylossandquantizationlossfromH1,andmini-batch
wisehingelossfromH2.Thuswepresumethatthelosson
H2(hingeloss)forcesthenetworktoformfeaturespaces
(especiallyatthelaterlayers,H2andFC3)thatareinac-
cordancewiththesemanticinformationcontainedintheag-
gregatedtagvectors,
w
i
.Ontheotherhand,thepair-wise
lossonH1alignsthehashspacesuchthatsemanticallysim-
ilarimagepairsareclosebyanddissimilarpairsarefarther.
Thusthetwomainlosscomponentsaugmenteachotherand
guidethenetworktowardslearningasemanticallymean-
ingfulhashspace.Thethirdlosscomponent,thequantiza-
tionloss,forcestheoutputofH1tobecloseto
0
or
1
.
ThepairwiseEuclideanlossappliedonH1wasused
forhashingin[30]whilethequantizationlosswasused
in[9].ThehingelossontheheadH2isarankingloss
usedby[31]tolearnasemanticallymeaningfulrealvalued
imagerepresentationusingwordembeddingsof
tionlabels.Whilethehingelosscomponentdoesnotseem
toserveaclearpurposeinthisnetworkarchitecture,em-
piricalresultsshowthatthiscomponentcontributes
cantlytotheperformanceboostofourmodel.Also,[31]
mentionsthatusingsuchlossbooststheperformanceof
theirmodelinsteadofusinga
L
2
component.Theypre-
sumethatthiscouldbeduetothefactthattheproblem
offormingasemanticallymeaningfulimagerepresentation
spaceisarankingproblemingeneralandthereforesucha
rankinglosscouldbemorerelevant.Onsimilarlines,we
canarguethatthecurrentproblemoflearningimagehashes
isarankingproblemaswell,andthus,suchahingeloss
componentcouldboosttheperformanceofaretrievalsys-
tem.
Duringinference,onlyH1isusedtoextractthefeatures,
whicharethenquantizedtoobtainthehashcodeaccord-
ingtothefollowingscheme:
b
i
=
1
2
(
sgn
(
h
(1)
i

0
:
5
1
)+1)
.
Here,
h
(1)
i
representsthereal-valuedfeaturevectorobtained
4
attheoutputofH1,
sgn
representsasignfunctionthatout-
puts
1
/

1
basedoniftheinputtothesignfunctionispos-
itiveornegativeandlastly,
1
representsavectorofonesof
length
b
.Thus,weobtainbinarycodeswhichhaveavalue
of
1
/
0
fromarawtrain/testimages.
3.4.DesigningtheLossFunctions
Pair-wiseSimilarityLoss:
Moststate-of-artsuper-
visedlearningmethodsassumebinarysimilaritybetween
twoimages,i.e.,twoimagescanbeeithersimilar(
1
)or
dissimilar(
0
)dependingoniftheyshareacommonlabel
ornot.However,inthecurrentweaklysupervisedlearning
context,weintendtousecosinesimilaritybetweentheag-
gregatedtagvectorsasthegroundtruthsimilarity.Since
cosinesimilarityisreal-valuedandcantakevaluesbetween
-1and1,thegroundtruthsimilarityinourcaseisnotbinary
valued,i.e.,wecandeemanimagepairtobelesssimilaror
moresimilar,insteadofabsolutelydeclaringittobesimilar
ordissimilar.Weonlyconsiderthisnotionofgroundtruth
similarityduringtrainingandstickwiththe
0
/
1
similarity
duringevaluation.
Weformulatethepair-wisesimilaritylossfunctionas
follows.Foranyimagepair(
x
i
,
x
j
),thelossfunction
shouldpushthecorrespondinghashescloserifthecosine
distancebetweenthemissmallerandvice-versa.Theequa-
tionofthislossfunctionisgivenbelow,
L
1
=
k
X
i
=1
k
X
j
=1
[
1
b
(
h
(1)
i

h
(1)
j
)
T

(
h
(1)
i

h
(1)
j
)

1
2
(1
:
0

w
T
i

w
j
k
w
i
kk
w
j
k
)]
2
(2)
where
k
istheminibatchsizeandthetwosummations
signifycomputingpairwiselossesacrossallpossiblepairs.
Thevectors
h
(1)
i
and
h
(1)
j
representtheoutputvectorsofH1
forsample
x
i
and
x
j
respectively.Alowervalueof
L
1
is
obtainedwhenahighvalueof
1
:
0

w
T
i

w
j
k
w
i
kk
w
j
k
resultsina
highvalueof
(
h
(1)
i

h
(1)
j
)
T

(
h
(1)
i

h
(1)
j
)
andvice-versa.
Highervalueof
1
:
0

w
T
i

w
j
k
w
i
kk
w
j
k
isobtainedwhenthesam-
plesaredissimilar,thusthehashcodesshouldbepushed
apart.Similarly,lowervalueofthistermisobtainedwhen
thesamplesaresimilarandthereforethehashcodesshould
bepushedcloser.
Mini-batch-wiseHingeLoss:
Inadditiontothepair-
wisesimilarityloss,wealsointendtoback-propagatealoss
thatformsasemanticembeddingspaceattheoutputofH2.
Suchalossfunctionadjuststhefeaturespacesofnotonly
theH2layerbutalsosomeofthepreviouslayers(FC3,
FC2),thustransmittingthesemanticinformationfromthe
tagsbackintothenetwork.AsH1isconnectedtotheout-
putofFC3,thesemanticinformationcontainedinFC3will
aidinlearningthehashesattheoutputofH2,thusenhanc-
ingthemodel'sperformance.Tothisend,wethe
followingloss,
L
2
=
X
n
X
j
6
=
n
max
[0
;margin
+
w
j

h
(2)
n

w
n

h
(2)
n
]
(3)
where
h
(2)
n
representstheoutputoftheheadH2forthe
n
th
sampleinthemini-batch.Theloss
L
2
is
0
onlywhenthe
quantity
w
n

h
(2)
n
ismorethan
margin
+
w
j

h
(2)
n
.Thatis,
thevalueofthelossiszeroonlywhenthepredictionofhead
H
2
forthe
n
th
sampleisclosertothegroundtruthaggre-
gatedtagvector
w
n
thantoanyothergroundtruthtagvector
w
j
byamargin
margin
.Asimilarideawaspreviouslycon-
sideredin[32],wherethegoalwastosemanticallyembed
videosontoaspaceusingthe
word2vec
representationof
thevideolabels.Assuch,theirapproachissupervised(i.e.,
assumingthelabelinformation).
QuantizationLoss:
Wefurtherimposethequantization
lossontheH1outputtoforcetheoutputstobecloseto
0
or
1
,asfollows,
L
3
=

k
X
i
=1
1
b
(
h
(1)
n

0
:
5
1
)
T

(
h
(1)
n

0
:
5
1
)
(4)
Thisfunctionpenalizesthenetworkiftheoutputofaneuron
iscloseto0.5.
Duringtraining,weweighthethreelosscomponents
L
1
,
L
2
and
L
3
byfactors

1
,

2
and

3
respectively.There-
foretheresultantlossthatwillbeback-propagatedis:
L
=

1
L
1
+

2
L
2
+

3
L
3
3.5.Thebinaryectormodel
Inadditiontocomparingourmethodwithseveralstate-
of-artmodels,webuiltanotherdeepmodel,thatusesthe
binarytagvectorsforsupervision,unlikethe
word2vec
tag
embeddingsweusedinWDHT.Wecallthismodelthe
bi-
narytag-vectormodel
intherestofthetext.Toaccom-
modatethis,wemakeslighttoourmodel.
Firstly,wesupposethattwoimagesaresimilarifbothof
themshareatleastonetag.Suchkindofformulationhas
beenusedinvarioussupervisedlearningmethodswhere
theyconsidertwoimagestobesimilarifbothofthemshare
atleastonelabel.Sinceourproblemsettingisweaklysu-
pervised,weusetagvectorsinsteadoflabelvectors.Tag
vectorsarebinaryvectorswhoselengthisequaltothetotal
numberoftagsinthedata-setandwillhaveavalueof
1
if
thetagisassociatedwiththeimageand
0
otherwise.
Regardingthenetworkarchitecture,onlytheheadH1
iskeptandH2iscompletelyremoved.Wedothisowing
tothefactthatthereal-valuedvectors(likeaggregatedtag
vectorsintheabovescenario)arenotavailableinthiscase,
toregresstheoutputsto.Additionally,inthepreviouscase,
5
Method
12bits
24bits
32bits
48bits
itf
0.6124
0.6323
0.6531
0.6644
tf
0.6394
0.6836
0.6881
0.6835
mean
0.6709
0.6805
0.6955
0.6621
Table2:ComparingthemAPofthemodelwiththe
itf
,
tf
or
mean
aggregationfunctionsfortheNUS-WIDEdataset.
thelossappliedonH1,i.e.,the
L
1
componenthasareal-
valuedgroundtruthsimilarity,unlikethecurrentscenario.
Therefore,weuseadifferentlosscomponent(contrastive
loss)toaccommodatethebinaryvaluedgroundtruthsimi-
laritylabels.Theequationofthelossisasfollows,
L
4
=
k
X
i
=1
k
X
j
=1
S

(1


)

D
+
(1

S
)



(
max
(0
;margin

D
))
2
where
D
=
1
b
(
h
(1)
i

h
(1)
j
)
T

(
h
(1)
i

h
(1)
j
)
(5)
Here,
margin
representsthemarginassociatedwiththe
hingelosscomponentofthecontrastiveloss,
S
represents
thegroundtruthsimilaritylabel,and

representsthefrac-
tionofsimilarsamplepairspresentintheminibatch.
Weighingthelosssub-componentsby

and
1


respec-
tivelyareimportantduetothefactthatinanymini-batch
onlyasmallfractionoftheimagepairswillhaveatleast
onetagincommon,thusmakingthedatasethighlyimbal-
anced.Wethereforeincorporate

weightfactorintheloss.
Thusthelossforthebinarytag-vectormodelbe-
comes:
L
=

3
L
3
+

4
L
4
4.ExperimentsandResults
4.1.Datasets
NUS-WIDE
ThisisaWebimagedatasetwith269,648
imagescollectedfromFlickr.Eachimageisassociatedwith
asetoftags.[33]presentsthatthereisatotalof425,059
tagsassociatedwiththe269kimages.Further,theauthors
of[33]conductedmanualannotationoftheseimagestoa
setof81labels.Forourexperiments,weused
onlytheimagesthatareassociatedwithatleastoneofthe
21mostfrequentlabels.Thusweformedatrainingsetof
100,000imagesandatestingsetof2,000images.Weused
thewholetrainingsetasthedatabaseandthetestingsetas
thequerysetduringevaluation.
MIR-FLICKR25K
This
isacomparativelysmallerdatasetwith25,000imagescol-
lectedfromFlickrandcontains1386tagsassociatedwith
them.[34]manuallyassociatedtheimageswith38seman-
ticcategories.Forourexperiments,weusedtheimages
whichareassociatedwithatleastoneofthe38categories.
Thusweusedatotalof16,000imagesfortrainingand2,000
fortesting.Forboththedata-sets,werandomlypickedthe
testingsetwithoutconsideringthelabelsoftheimages.
4.2.Training
Wetrainedourmodelusingmini-batchgradientdescent
withalearningrateof0.001forthelastthreelayers(FC3,
H1,andH2)andalearningrateof0.0001forthepre-trained
layers(CONV1-FC2).Wealsousedthemomentumterm
withtherateofmomentumequalto0.9.Theweighingfac-
torsforthelosses,

1
,

2
,

3
and

4
,aresetto1.0,10.0,
1.0and1.0respectivelyforalltheexperiments,whichwere
determinedbyperformingagridsearchoverthehyper-
parameterspace.The
word2vec
modelthatweusedwas
trainedon1billionwordsfromtheWikipediadocuments
andoutputsa300-dimensionalvectorforagivenword.
ThereforethenumberofoutputneuronsonH2issetto300.
4.3.PerformanceEvaluation
Weevaluatedthelearnedhashcodesforthetaskof
semanticimageretrieval.Weusedthemean-Average-
Precision(mAP)metrictocompareourmodel'sperfor-
mancetotheexistingmethods.Weusedthesameprotocol
usedby[35],[36],[37]andseveralotherstocomputethe
mAPvalues.Theresultsarecomparedagainstelevenstate-
of-artapproachesITQ,PCAH,LSH,DSH,SpH,SH,AGH,
DH,UH-BDNN,DeepBitandWMH.Allthemethods,ex-
ceptWMHarerunusingthecodeprovidedbytheauthors
andforthesuggestedhyper-parametersettings.Asmostof
theworkspresentedherearebasedonthepre-determined
featurevectors,weextractedthe4096-dimensionalvectors
fromtheAlexNetmodel(i.etheoutputofFC2)andused
themasinputtothesemethods.ForWMHwedirectly
quotetheresultsfromtheoriginalpaper(thecodeisnot
publiclyavailable).Forafaircomparison,werunourmodel
withthesameexperimentalsettingasWMHandreportthe
results.WetheimagesandtagsinWMH'sstan-
dard,thenperformedanotherroundofexperimentsusing
only5,000trainingimagesand1,000queryimagesforthe
twodatasets.
Firstly,tozethetagaggregationscheme,wecom-
paredtheperformanceofourmodelusingthe
itf
,
tf
and
mean
functionsforaggregationonNUS-WIDEdata-set.
Wenoticedthat
mean
workedslightlybetterthanthe
idf
and
tf
ascanbeseenfromTable2.Further,weperformed
avarianceanalysisonthewordvectorsoftagsassociated
witheachimage.More,wecomputedthevari-
anceofthetagvectorsforeachimageandthenanalyzedthe
histogramofthevariancesforallimages.Itwasfoundthat
amajorityofthevariancesfallsbelow8.Notethatthemax-
imumdistancebetweenanytwowordvectorsinthisspace
canbe
2
p
300
(therangeofeachdimensionofthetagvector
is
[

1
;
1]
andthespaceis300-dimensional).Thisappears
tosuggestthatformostoftheimages,theirtagvectorsdo
6
Algorithm
NUS-WIDE
MIRFLICKR-25K
12bits
24bits
32bits
48bits
12bits
24bits
32bits
48bits
ITQ[9]
(non-deep)
0.5295
0.5227
0.4932
0.5275
0.6418
0.655
0.6253
0.6504
PCAH[11]
(non-deep)
0.4566
0.4209
0.4016
0.3971
0.6098
0.6033
0.6085
0.6169
LSH[5]
(non-deep)
0.3308
0.3682
0.3726
0.3918
0.5708
0.5885
0.5843
0.6015
DSH[23]
(non-deep)
0.5065
0.5118
0.4902
0.4807
0.6561
0.6593
0.644
0.6422
SpH[24]
(non-deep)
0.3829
0.3959
0.3907
0.3947
0.586
0.5785
0.5789
0.5789
SH[10]
(non-deep)
0.4503
0.4029
0.4006
0.3731
0.6251
0.6157
0.6044
0.596
AGH[26]
(non-deep)
0.535
0.5226
0.497
0.4791
0.6378
0.6484
0.6473
0.6346
DH[12]
(deep)
0.4036
0.3974
0.3932
0.4014
0.5833
0.5945
0.5932
0.5942
UH-BDNN[14]
(deep)
0.4982
0.4996
0.4823
0.4853
0.6324
0.6279
0.6274
0.6258
DeepBit[13]
(deep)
0.4225
0.4247
0.4359
0.431
0.5974
0.6032
0.6077
0.6115
BinaryTagVector
(deep)
0.4809
0.475
0.4793
0.4702
0.6064
0.6087
0.6077
0.6098
Proposed(WDHT)
(deep)
0.6258
0.6397
0.6606
0.647
0.687
0.695
0.6667
0.6621
WMH*
(non-deep)
0.299
0.306
0.307
0.309
0.585
0.590
0.582
0.573
Proposed(WDHT*)
(deep)
0.4910
0.4916
0.4835
0.485
0.626
0.6355
0.6326
0.6308
Table3:MAPvaluesofNUS-WIDEandMIR-FLICKR25kdata-setscomputedusingthetop50,000retrievedimages.
Algorithm
NUS-WIDE
MIRFLICKR-25K
12bits
24bits
32bits
48bits
12bits
24bits
32bits
48bits
ITQ[9]
(non-deep)
0.6329
0.6299
0.594
0.6478
0.6908
0.7064
0.6684
0.6996
PCAH[11]
(non-deep)
0.5766
0.5046
0.49
0.4904
0.643
0.6306
0.6372
0.6516
LSH[5]
(non-deep)
0.3501
0.4093
0.4169
0.4546
0.5736
0.6049
0.5954
0.6239
DSH[23]
(non-deep)
0.5919
0.5982
0.5713
0.5791
0.6955
0.7071
0.6834
0.6603
SpH[24]
(non-deep)
0.4645
0.4645
0.4465
0.4472
0.5966
0.5811
0.5828
0.579
SH[10]
(non-deep)
0.5623
0.5033
0.4896
0.4533
0.6605
0.6405
0.6291
0.6213
AGH[26]
(non-deep)
0.6551
0.6459
0.6274
0.6225
0.6862
0.7005
0.6998
0.6853
DH[12]
(deep)
0.4733
0.4601
0.462
0.4763
0.6033
0.6195
0.6135
0.618
UH-BDNN[14]
(deep)
0.5923
0.5915
0.5902
0.6097
0.6654
0.6684
0.6672
0.6699
DeepBit[13]
(deep)
0.5463
0.5548
0.5624
0.561
0.589
0.6027
0.609
0.6086
BinaryTagVector
(deep)
0.6202
0.627
0.6247
0.6249
0.6365
0.6326
0.6373
0.6352
Proposed(WDHT)
(deep)
0.6709
0.6805
0.6955
0.676
0.7346
0.743
0.7034
0.7054
Table4:MAPvaluesofNUS-WIDEandMIR-FLICKR25kdata-setscomputedusingthetop5,000retrievedimages
notspreadouttoomuch,whichmightexplainthatthesim-
ple
mean
aggregationfunctionisworkingreasonablywell.
Further,wecomputedthemAPfortwodifferentset-
tings,oneusingthetop50,000retrievedimagesandan-
otherusingthetop5,000retrievedimagesfortheunsu-
pervisedapproachesandreporttheresultsinTable3and
Table4respectively.Thesevenmethodspresented
herearenon-deep-learningmethodswhilethelastthree
aredeep-learning-based.Additionally,DH[12]andUH-
BDNN[14],eventhoughbeingdeep-learning-based,de-
pendonthehand-craftedfeatures.DeepBit[13]istheonly
workthattakesarawimageasinputandproducesabinary
code,butitsperformanceisinferiortomostothermeth-
ods.Incontrast,ourapproach(WDHT)isanend-to-end
frameworkandperformedsuperiorthanallthestate-of-art
methodsonbothdatasets.
Thenon-deep-learningbasedapproachesITQ[9]and
AGH[26]seemtostandinthesecondandthethirdplacesin
termsofthemAPvaluesintheexperiments.Thesemeth-
odsareperformingsuperiortotheexistingdeeplearning
basedmethods([12],[14],[13])aswell.Ontheotherhand,
theweaklysupervisedapproachWMHseemedtoperform
quiteinferiorascomparedtoWDHTwiththenewexperi-
mentsetting.Theresultsarepresentedasthebottom2rows
ofTable3.
Forfurtheranalysis,weplottedtheprecision-recall
curvesinFigure2.Thesecurvesarecomputedtakinginto
considerationalltheretrievedsamplesfromthedatabasefor
agivenqueryimage.More,wecomputedthe
averageprecisionforvariousvaluesofrecall(1000discrete
valuesofrecall)forallqueryimages.Thebigperformance
gainofourapproachontheNUS-WIDEdata-setcanbeno-
7
Figure2:PrecisionRecallcurvesforNUS-WIDEandMIR-FLICKRdatasets.
Figure3:MAPvaluesobtainedforvarioushyper-
parametersettingsfortheNUS-WIDEdataset
ticedfromthesegraphsaswell.
Thepresenceofthreelosscomponentsintheobjective
functionstriggerstheobviousquestionofcombiningthem
intherightproportions.Toanalyzethis,wethevalueof

1
to1.0andchangethevaluesof

2
and

3
between
0
:
01
and
100
:
0
.Weperformedagridsearchoverthisrangeand
chosethebesthyper-parametersforourmodel.Specif-
ically,wesetthreevaluestotheonesthatgavemaximum
mAPvalueoveravalidationsetduringthegridsearch.For
eachsettingofthehyper-parametervalues,weonlyused
10,000trainingsampleduetothehightrainingtimeofthese
experiments.AbarplotofthevalidationmAPsofNUS-
WIDEdatasetforvariousvaluesof

2
and

3
isgivenin
Figure3.Itcanbenoticedthathighervaluesof

2
and
lowervaluesof

3
gavebettermAPascom-
paredtoothercombinations.Asimilarbehaviourwasno-
ticedontheMIR-FLICKRdatasetaswell.Thisisinac-
cordancewiththerationalepresentedinSection3.3thata
rankinglossisbetteratformingsemanticallymeaningful
spacesascomparedtoEuclideanlosscomponents([31]).
Whilethisrationaleisyettobevalidatedmathematically,
ourresultssuggestthisseemstobethecaseempirically.
5.Conclusion
Inthispaper,weattemptedtheproblemofweaklysu-
perviseddeepimagehashingusingtagembedding.Our
methodisanend-to-endframeworkthattakesrawimages
andtagsasinputsandproduceshashcodes.Therefore,
ourmodelisapplicabletoWebimageswheresuchinfor-
mationisabundant.Throughextensiveexperimentswith
comparisonwithexistingstate-of-the-art,wedemonstrated
thattheproposedapproachwasabletodeliver
performanceboostwhenevaluatedontwowell-knownand
widely-testeddatasets.Futureworkincludespossiblebetter
aggregationschemesinthe
word2vec
spacethatmayleadto
improvedperformance.
8
References
[1]
ManishGupta,RuiLi,ZhijunYin,andJiaweiHan.Sur-
veyonsocialtaggingtechniques.
ACMSigkddExplorations
Newsletter
,12(1):58Œ72,2010.
[2]
ShiladSen,FMaxwellHarper,AdamLaPitz,andJohn
Riedl.Thequestforqualitytags.In
Proceedingsofthe2007
internationalACMconferenceonSupportinggroupwork
,
pages361Œ370.ACM,2007.
[3]
KaiyeWang,QiyueYin,WeiWang,ShuWu,andLiang
Wang.Acomprehensivesurveyoncross-modalretrieval.
arXivpreprintarXiv:1607.06215
,2016.
[4]
TomasMikolov,KaiChen,GregCorrado,andJeffreyDean.
Efestimationofwordrepresentationsinvectorspace.
arXivpreprintarXiv:1301.3781
,2013.
[5]
MosesSCharikar.Similarityestimationtechniquesfrom
roundingalgorithms.In
Proceedingsofthethiry-fourthan-
nualACMsymposiumonTheoryofcomputing
,pages380Œ
388.ACM,2002.
[6]
AnirbanDasgupta,RaviKumar,andTam
´
asSarl
´
os.Fast
locality-sensitivehashing.In
Proceedingsofthe17thACM
SIGKDDinternationalconferenceonKnowledgediscovery
anddatamining
,pages1073Œ1081.ACM,2011.
[7]
BrianKulisandKristenGrauman.Kernelizedlocality-
sensitivehashing.
IEEETransactionsonPatternAnalysis
andMachineIntelligence
,34(6):1092Œ1104,2012.
[8]
AniketChakrabarti,VenuSatuluri,AtreyaSrivathsan,and
SrinivasanParthasarathy.Abayesianperspectiveonlocality
sensitivehashingwithextensionsforkernelmethods.
ACM
TransactionsonKnowledgeDiscoveryfromData(TKDD)
,
10(2):19,2015.
[9]
YunchaoGong,SvetlanaLazebnik,AlbertGordo,andFlo-
rentPerronnin.Iterativequantization:Aprocrusteanap-
proachtolearningbinarycodesforlarge-scaleimagere-
trieval.
IEEETransactionsonPatternAnalysisandMachine
Intelligence
,35(12):2916Œ2929,2013.
[10]
YairWeiss,AntonioTorralba,andRobFergus.Spectral
hashing.In
Advancesinneuralinformationprocessingsys-
tems
,pages1753Œ1760,2009.
[11]
JunWang,SanjivKumar,andShih-FuChang.Semi-
supervisedhashingforlarge-scalesearch.
IEEETrans-
actionsonPatternAnalysisandMachineIntelligence
,
34(12):2393Œ2406,2012.
[12]
VeniceErinLiong,JiwenLu,GangWang,PierreMoulin,
andJieZhou.Deephashingforcompactbinarycodeslearn-
ing.In
ProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition
,pages2475Œ2483,2015.
[13]
KevinLin,JiwenLu,Chu-SongChen,andJieZhou.Learn-
ingcompactbinarydescriptorswithunsuperviseddeepneu-
ralnetworks.In
ProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognition
,pages1183Œ
1192,2016.
[14]
Thanh-ToanDo,Anh-DzungDoan,andNgai-ManCheung.
Learningtohashwithbinarydeepneuralnetwork.In
Eu-
ropeanConferenceonComputerVision
,pages219Œ234.
Springer,2016.
[15]
YueCao,MingshengLong,JianminWang,andShichen
Liu.Deepvisual-semanticquantizationforefimage
retrieval.In
CVPR
,volume2,page6,2017.
[16]
Qing-YuanJiangandWu-JunLi.Deepcross-modalhashing.
CoRR
,2016.
[17]
YueCao,MingshengLong,JianminWang,QiangYang,and
PhilipSYu.Deepvisual-semantichashingforcross-modal
retrieval.In
Proceedingsofthe22ndACMSIGKDDInterna-
tionalConferenceonKnowledgeDiscoveryandDataMin-
ing
,pages1445Œ1454.ACM,2016.
[18]
XingXu,FuminShen,YangYang,HengTaoShen,andXue-
longLi.Learningdiscriminativebinarycodesforlarge-scale
cross-modalretrieval.
IEEETransactionsonImageProcess-
ing
,26(5):2494Œ2507,2017.
[19]
JingdongWang,TingZhang,NicuSebe,HengTaoShen,
etal.Asurveyonlearningtohash.
IEEETransactionson
PatternAnalysisandMachineIntelligence
,2017.
[20]
ZiyuGuan,FeiXie,WanqingZhao,XiaopengWang,Long
Chen,WeiZhao,andJinyePeng.Tag-basedweakly-
supervisedhashingforimageretrieval.
[21]
HanwangZhang,NaZhao,XindiShang,Huan-BoLuan,and
Tat-sengChua.Discreteimagehashingusinglargeweakly
annotatedphotocollections.In
AAAI
,pages3669Œ3675,
2016.
[22]
JinhuiTangandZechaoLi.Weakly-supervisedmultimodal
hashingforscalablesocialimageretrieval.
IEEETransac-
tionsonCircuitsandSystemsforVideoTechnology
,2017.
[23]
ZhongmingJin,ChengLi,YueLin,andDengCai.Den-
sitysensitivehashing.
IEEEtransactionsoncybernetics
,
44(8):1362Œ1371,2014.
[24]
Jae-PilHeo,YoungwoonLee,JunfengHe,Shih-FuChang,
andSung-EuiYoon.Sphericalhashing.In
ComputerVision
andPatternRecognition(CVPR),2012IEEEConferenceon
,
pages2957Œ2964.IEEE,2012.
[25]
XiaofengZhu,LeiZhang,andZiHuang.Asparseembed-
dingandleastvarianceencodingapproachtohashing.
IEEE
transactionsonimageprocessing
,23(9):3737Œ3750,2014.
[26]
WeiLiu,CunMu,SanjivKumar,andShih-FuChang.Dis-
cretegraphhashing.In
AdvancesinNeuralInformationPro-
cessingSystems
,pages3419Œ3427,2014.
[27]
AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.
Imagenetwithdeepconvolutionalneuralnet-
works.In
Advancesinneuralinformationprocessingsys-
tems
,pages1097Œ1105,2012.
[28]
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,
andLiFei-Fei.Imagenet:Alarge-scalehierarchicalim-
agedatabase.In
ComputerVisionandPatternRecognition,
2009.CVPR2009.IEEEConferenceon
,pages248Œ255.
IEEE,2009.
[29]
XavierGlorotandYoshuaBengio.Understandingthedif
cultyoftrainingdeepfeedforwardneuralnetworks.In
Pro-
ceedingsoftheThirteenthInternationalConferenceonArti-
IntelligenceandStatistics
,pages249Œ256,2010.
9
[30]
BrianKulisandTrevorDarrell.Learningtohashwithbinary
reconstructiveembeddings.In
Advancesinneuralinforma-
tionprocessingsystems
,pages1042Œ1050,2009.
[31]
AndreaFrome,GregSCorrado,JonShlens,SamyBengio,
JeffDean,TomasMikolov,etal.Devise:Adeepvisual-
semanticembeddingmodel.In
Advancesinneuralinforma-
tionprocessingsystems
,pages2121Œ2129,2013.
[32]
Sheng-HungHu,YikangLi,andBaoxinLi.Video2vec:
Learningsemanticspatio-temporalembeddingsforvideo
representation.In
PatternRecognition(ICPR),201623rd
InternationalConferenceon
,pages811Œ816.IEEE,2016.
[33]
Tat-SengChua,JinhuiTang,RichangHong,HaojieLi,Zhip-
ingLuo,andYantaoZheng.Nus-wide:areal-worldwebim-
agedatabasefromnationaluniversityofsingapore.In
Pro-
ceedingsoftheACMinternationalconferenceonimageand
videoretrieval
,page48.ACM,2009.
[34]
MarkJHuiskesandMichaelSLew.Themirretrieval
evaluation.In
Proceedingsofthe1stACMinternationalcon-
ferenceonMultimediainformationretrieval
,pages39Œ43.
ACM,2008.
[35]
Wu-JunLi,ShengWang,andWang-ChengKang.Feature
learningbaseddeepsupervisedhashingwithpairwiselabels.
arXivpreprintarXiv:1511.03855
,2015.
[36]
XiaofangWang,YiShi,andKrisMKitani.Deepsupervised
hashingwithtripletlabels.In
AsianConferenceonComputer
Vision
,pages70Œ84.Springer,2016.
[37]
HanjiangLai,YanPan,YeLiu,andShuichengYan.Simulta-
neousfeaturelearningandhashcodingwithdeepneuralnet-
works.In
ComputerVisionandPatternRecognition(CVPR),
2015IEEEConferenceon
,pages3270Œ3278.IEEE,2015.
10
"
72,Disentangled Person Image Generation,http://arxiv.org/pdf/1712.02621v4.pdf,https://github.com/charliememory/Disentangled-Person-Image-Generation,"DisentangledPersonImageGeneration
LiqianMa
1
QianruSun
2

StamatiosGeorgoulis
1
LucVanGool
1
;
3
BerntSchiele
2
MarioFritz
2
1
KU-Leuven/PSI,ToyotaMotorEurope(TRACE)
3
ETHZurich
2
MaxPlanckInstituteforInformatics,SaarlandInformaticsCampus
f
liqian.ma,sgeorgou,luc.vangool
g
@esat.kuleuven.be
f
qsun,schiele,mfritz
g
@mpi-inf.mpg.de
Abstract
Generatingnovel,yetrealistic,imagesofpersonsisa
challengingtaskduetothecomplexinterplaybetweenthe
differentimagefactors,suchastheforeground,background
andposeinformation.Inthiswork,weaimatgenerat-
ingsuchimagesbasedonanovel,two-stagereconstruc-
tionpipelinethatlearnsadisentangledrepresentationof
theaforementionedimagefactorsandgeneratesnovelper-
sonimagesatthesametime.First,amulti-branchedrecon-
structionnetworkisproposedtodisentangleandencodethe
threefactorsintoembeddingfeatures,whicharethencom-
binedtore-composetheinputimageitself.Second,three
correspondingmappingfunctionsarelearnedinanadver-
sarialmannerinordertomapGaussiannoisetothelearned
embeddingfeaturespace,foreachfactor,respectively.Us-
ingtheproposedframework,wecanmanipulatethefore-
ground,backgroundandposeoftheinputimage,andalso
samplenewembeddingfeaturestogeneratesuchtargeted
manipulations,thatprovidemorecontroloverthegener-
ationprocess.ExperimentsontheMarket-1501andDeep-
fashiondatasetsshowthatourmodeldoesnotonlygenerate
realisticpersonimageswithnewforegrounds,backgrounds
andposes,butalsomanipulatesthegeneratedfactorsand
interpolatesthein-betweenstates.Anothersetofexperi-
mentsonMarket-1501showsthatourmodelcanalsobe
forthepersonrtask
1
.
1.Introduction
Theprocessofgeneratingrealistic-lookingimagesof
personshasseveralapplications,likeimageediting,person
(re-ID),inpaintingoron-demandgenerated
artformovieproduction.Therecentadventofimagegener-

Correspondingauthor
1
Projectpage:
https://homes.esat.kuleuven.be/
Ÿ
liqianma/CVPR18_DPIG/index.html
Figure1:Left:imagesamplingresultsonMarket-1501.
Threefactors,
i.e
.foreground,backgroundandpose,canbe
sampledindependently(1st-3rdrows)andjointly(4throw).
Right:similarjointsamplingresultsonDeepFashion.This
datasetcontainsalmostnobackground,soweonlydisen-
tangletheimageintoappearanceandposefactors.
ationmodels,suchasvariationalautoencoders(VAE)[
13
],
generativeadversarialnetworks(GANs)[
7
]andautoregres-
sivemodels(ARMs)(
e.g
.PixelRNN[
35
]),hasprovided
powerfultoolstowardsthisgoal.Severalpapers[
25
,
2
,
1
]
havethenexploitedtheabilityofthesenetworkstogenerate
sharpimagesinordertosynthesizerealisticphotosoffaces
andnaturalscenes.Recently,Ma
etal
.[
21
]proposedan
architecturetosynthesizenovelpersonimagesinarbitrary
posesgivenasinputanimageofthatpersonandanewpose.
Fromanapplicationperspectivehowever,theuseroften
wantstohavemorecontroloverthegeneratedimages(
e.g
.
changethebackground,aperson'sappearanceandclothing,
ortheviewpoint),whichissomethingthatexistingmeth-
arXiv:1712.02621v4  [cs.CV]  15 Jun 2018odsareessentiallyuncapableof.Wegobeyondthesecon-
straintsandinvestigatehowtogeneratenovelpersonim-
ageswithauserintentioninmind(
i.e
.foreground
(FG),background(BG),posemanipulation).Thekeyidea
istoexplicitlyguidethegenerationprocessbyanappropri-
aterepresentationofthatintention.Fig.
1
givesexamples
oftheintendedgeneratedimages.
Tothisend,wedisentangletheinputimageintointerme-
diateembeddingfeatures,
i.e
.personimagescanbereduced
toacompositionoffeaturesofforeground,background,and
pose.Comparedtoexistingapproaches,werelyonadiffer-
enttechniquetogeneratenewsamples.Inparticular,we
aimatsamplingfromastandarddistribution,
e.g
.aGaus-
siandistribution,togeneratenewembeddingfeatures
andfromthemgeneratenewimages.Toachievethis,
fake
embeddingfeatures
~
e
arelearnedinanadversarialmanner
tomatchthedistributionofthe
real
embeddingfeatures
e
,
wheretheencodedfeaturesfromtheinputimagearetreated
as
real
whilsttheonesgeneratedfromtheGaussiannoise
as
fake
(Fig.
2
).Consequently,thenewlysampledimages
comefromlearned
fake
embeddingfeatures
~
e
ratherthan
theoriginalGaussiannoiseasinthetraditionalGANmod-
els.Bydoingso,theproposedtechniqueenablesusnotonly
tosampleacontrollableinputforthegenerator,butalsoto
preservethecomplexityofthecomposedimages(
i.e
.real-
isticpersonimages).
Tosumup,ourfullpipelineproceedsintwostagesas
showninFig.
2
.Atstage-I,weuseaperson'simageas
inputanddisentangletheinformationintothreemainfac-
tors,namelyforeground,backgroundandpose.Eachdisen-
tangledfactorismodeledbyembeddingfeaturesthrougha
reconstructionnetwork.Atstage-II,amappingfunctionis
learnedtomapaGaussiandistributiontoafeatureembed-
dingdistribution.
Ourcontributionsare:1)Anewtaskofgeneratingnatu-
ralpersonimagesbydisentanglingtheinputintoweakly
correlatedfactors,namelyforeground,backgroundand
pose.2)Atwo-stageframeworktolearnmanipulatableem-
beddingfeaturesforallthreefactors.Instage-I,theen-
coderofthemulti-branchedreconstructionnetworkserves
conditionalimagegenerationtasks,whereasinstage-IIthe
mappingfunctionslearnedthroughadversarialtraining(
i.e
.
mappingnoise
z
to
fake
embeddingfeatures
emb
)serve
samplingtasks(
i.e
.theinputissampledfromastandard
Gaussiandistribution).3)Atechniquetomatchthedistri-
butionof
real
and
fake
embeddingfeaturesthroughadver-
sarialtraining,notboundtotheimagegenerationtask.4)
Anapproachtogeneratenewimagepairsforpersonre-ID.
Sec.
4
constructsaVirtualMarketre-IDdatasetby
foregroundfeaturesandchangingbackgroundfeaturesand
posekeypointstogeneratesamplesofoneidentity.
Figure2:Ourtwo-stageframework.Instage-I,weuseare-
constructionnetworktoobtainthe
real
embeddingfeatures
e
foreachfactor,
i.e
.foreground,backgroundandpose.The
architecturaldetailsofstage-IareshowninFigure
3
.In
stage-II,weproposeanovel,two-stepmappingtechnique
foradversarialembeddingfeaturelearningthatmap
Gaussiannoise
z
tointermediateembeddingfeatures
~
e
then
tothedata
~
x
.Weusethepre-trainedencoderanddecoder
ofstage-Itoguidethelearningofmappingfunctions

.
2.Relatedwork
Imagegenerationfromnoise.
Theabilityofgenerative
models,suchasGANs[
7
],adversarialautoencoders(AAE)
[
22
],VAEs[
13
]andARMs(
e.g
.PixelRNN[
35
]),tosyn-
thesizerealistic-looking,sharpimageshasledimagegen-
erationresearchlately.Traditionalimagegenerationworks
useGANs[
7
]orVAEs[
13
]tomapadistributiongenerated
bynoise
z
tothedistributionofrealdata.Convolutional
VAEsandAAEs[
22
]haveshownhowtotransformanauto-
encoderintoagenerator,butinthiscase,itisratherdif
totrainthemappingfunctionforcomplexdatadistributions,
suchaspersonimages(asalsomentionedinARAE-GAN
[
11
]).Assuch,traditionalimagegenerationmethodsare
notoptimalwhenitcomestothehumanbody.Forexam-
ple,Zheng
etal
.[
42
]directlyadoptedtheDCGANarchi-
tecture[
25
]togeneratepersonimagesfromnoise,butas
Fig.
7
(b)shows,vanillaDCGANleadstounrealisticresults.
Instead,weproposeatwo-stepmappingtechniqueinstage-
IItoguidethelearning,
i.e
.
z
!
e
!
x
(Fig.
2
).Similarto
[
11
],weuseadecodertoadversariallymapthenoisedistri-
butiontothefeatureembeddingdistributionlearnedbythe
reconstructionnetwork.
Conditionalimagegeneration.
Sincethehumanbody
hasacomplexnon-rigidstructurewithmanydegreesof
freedom[
23
],severalworkshaveusedstructureconditions
togeneratepersonimages.Reed
etal
.in[
26
]proposed
theGenerativeAdversarialWhat-WhereNetworkthatuses
posekeypointsandtextdescriptionsascondition,whereas
in[
27
]theyusedanextensionofPixelCNNinaddition
toconditioningonpartkeypoints,segmentationmasksand
texttogenerateimagesontheMPIHumanPosedataset,
2
Figure3:Stage-I:disentangledimagereconstruction.Thisframeworkiscomposedofthreebranches:foreground,back-
groundandpose.Notethatweuseafully-connectedauto-encodernetworktoreconstructthepose(incl.keypointcoordinates
andvisibility),sothatwecandecodetheembeddedposefeaturestoobtaintheheatmapsatthesamplingphase.
amongothers.Lassner
etal
.[
15
]generatedfull-bodyim-
agesofpersonsinclothingbyconditioningon
bodyandclothingsegments,
e.g
.pose,shapeorcolor.Zhao
etal
.[
39
]combinedthestrengthsofGANswithvariational
inferencetogeneratemulti-viewimagesofpersonsincloth-
inginamanner.Closertoourwork,Ma
etal
.[
21
]proposedtoconditiononimageandposekey-
pointstotransferthehumanposeinaxibleway.Facial
landmarkscanbetransferedaccordingly[
32
].Yet,their
methodsneedthetrainingsetofalignedpersonimagepairs
whichcostsexpensivehumanannotations.Mostrecently,
Zhu
etal
.[
43
]proposedtheCycleGANthatusescyclecon-
sistencytoachieveunpairedimage-to-imagetranslationbe-
tweendomains.Theyachievecompellingresultsinappear-
ancechangesbutshowlittlesuccessingeometricchanges.
Sinceimagesthemselvescontainabundantcontextinfor-
mation[
33
],someworkshavetriedtotackletheproblemin
anunsupervisedway.Doersch
etal
.[
4
]exploredtheuseof
spatialcontext,
i.e
.relativepositionbetweentwoneighbor-
ingpatchesinanimage,asasupervisorysignalforunsu-
pervisedvisualrepresentationlearning.Noroozi
etal
.[
24
]
extendedthetasktoajigsawpuzzlesolvedbyobserving
allthetilessimultaneously,whichcanreducetheambigu-
ityamongtheselocalpatchpairs.Lee
etal
.[
16
]utilized
contextinanimagegenerationtaskbyinferringthespa-
tialarrangementandgeneratingtheimageatthesametime.
Weusethesupervisioninadifferentway.Toextractpose-
invariantappearancefeatures,wearrangethebodypartfea-
tureembeddingsaccordingtotheregion-of-interest(ROI)
boundingboxesobtainedwithposekeypoints.Then,weex-
plicitlyutilizetheseposekeypointsasstructureinformation
toselectthenecessaryappearancefeaturesforeachbody
partandgeneratetheentirepersonimage.
Ingeneral,thispaperstudiesadifferentproblemthan
thesesupervisedorunsupervisedapproachesandtriesto
solvethedisentangledpersonimagegenerationtaskin
anunpaired,self-supervisedmanner,byleveragingfore-
ground,backgroundandposesamplingatthesametime,
inordertogainmorecontroloverthegenerationprocess.
Disentangledimagegeneration.
Fewpapershaveal-
readytriedtoworktowardsthisdirectionbylearningadis-
entangledrepresentationoftheinputimage.Chen
etal
.[
2
]
proposedInfoGAN,anextensiontoGANs,tolearndisen-
tangledrepresentationsusingmutualinformationinanun-
supervisedmanner,likewritingstylesfromdigitshapeson
theMNISTdataset,posefromlightingof3Drenderedim-
ages,andbackgrounddigitsfromthecentraldigitonthe
SVHNdataset.Cheung
etal
.[
3
]addedacross-covariance
penaltyinasemi-supervisedautoencoderarchitectureinor-
dertodisentanglefactors,likehand-writingstylefordigits
andsubjectidentityinfaces.Tran
etal
.[
34
]proposedDR-
GANtolearnbothagenerativeandadiscriminativerep-
resentationfromoneormultiplefaceimagestosynthesize
identity-preservingfacesattargetposes.Incontrast,our
methodgivesanexplicitrepresentationofthemain3axisof
variation(foreground,background,pose).Moreover,train-
ingisfacilitatedwithoutaneedforexpensiveidentityan-
notations-whichisnotreadilyavailableatscale.
3.Method
Ourgoalistodisentangletheappearanceandstructure
factorsinpersonimages,sothatwecanmanipulatethefore-
ground,backgroundandposeseparately.Toachievethis,
weproposeatwo-stagepipelineshowninFig.
2
.Instage-I,
wedisentangletheforeground,backgroundandposefac-
torsusingareconstructionnetworkinadivide-and-conquer
manner.Inparticular,wereconstructpersonimagesby
disentanglingintointermediateembeddingfeaturesof
3
thethreefactors,thenrecovertheinputimagebydecoding
thesefeatures.Instage-II,wetreatthesefeaturesas
real
to
learnmappingfunctions

formappingaGaussiandistri-
butiontotheembeddingfeaturedistributionadversarially.
3.1.
I
:Disentangledimagereconstruction
Atstage-I,weproposeamulti-branchedreconstruction
architecturetodisentangletheforeground,backgroundand
posefactorsasshowninFig.
3
.Notethat,toobtainthe
poseheatmapsandthecoarseposemaskweadoptthesame
procedureasin[
21
],butweinsteadusethemtoguidethe
informationwinourmulti-branchednetwork.
Foregroundbranch.
Toseparatetheforegroundandback-
groundinformation,weapplythecoarseposemasktothe
featuremapsinsteadoftheinputimagedirectly.Bydo-
ingso,wecanalleviatetheinaccuraciesofthecoarsepose
mask.Then,inordertofurtherdisentangletheforeground
fromtheposeinformation,weencodeposeinvariantfea-
tureswith7BodyRegions-Of-Interestinsteadofthewhole
imagesimilarto[
40
].,foreachROIweextract
thefeaturemapsresizedto48

48andpassthemintothe
weightsharingforegroundencodertoincreasethelearning
efy.Finally,theencoded7bodyROIembeddingfea-
turesareconcatenatedintoa224Dfeaturevector.Later,we
useBodyROI7todenoteourmodelwhichuses7bodyROIs
toextractforegroundembeddingfeatures,anduseWhole-
Bodytodenoteourmodelthatextractsforegroundembed-
dingfeaturesfromthewholefeaturemapsdirectlyinstead
ofextractingandresizingtheROIfeaturemap.
Backgroundbranch.
Forthebackgroundbranch,weap-
plytheinverseposemasktogetthebackgroundfeature
mapsandpassthemintothebackgroundencodertoob-
taina128-dimembeddingfeature.Then,theforeground
andbackgroundfeaturesareconcatenatedandtiledinto
128

64

352appearancefeaturemaps.
Posebranch.
Fortheposebranch,weconcatenatethe18-
channelheatmapswiththeappearancefeaturemapsand
passthemintotheaﬁU-Netﬂ-basedarchitecture[
29
],
i.e
.,
convolutionalautoencoderwithskipconnections,togen-
eratethepersonimagefollowingPG
2
(G1+D)[
21
].
Here,thecombinationofappearanceandposeimposesa
strongexplicitdisentanglingconstraintthatforcesthenet-
worktolearnhowtouseposestructureinformationtose-
lecttheusefulappearanceinformationforeachpixel.For
posesampling,weuseanextrafully-connectednetworkto
reconstructtheposeinformation,sothatwecandecodethe
embeddedposefeaturestoobtaintheheatmaps.Sincesome
bodyregionsmaybeunseenduetoocclusions,weintro-
duceavisibilityvariable

i
2f
0
;
1
g
;i
=1
;:::;
18
torepre-
sentthevisibilitystateofeachposekeypoint.Now,thepose
informationcanberepresentedbya54-dimvector(36-dim
keypointcoordinates

and18-dimkeypointvisibility

).
3.2.
II
:Embeddingfeaturemapping
Imagescanberepresentedbyalow-dimensional,con-
tinuousfeatureembeddingspace.Inparticular,in[
36
,
30
,
37
,
5
]ithasbeenshownthattheylieonorneara
low-dimensionalmanifoldoftheoriginalhigh-dimensional
space.Therefore,thedistributionofthisfeatureembedding
spaceshouldbemorecontinuousandeasiertolearncom-
paredtotherealdatadistribution.Someworks[
38
,
8
,
28
]
havethenattemptedtousetheintermediatefeaturerepre-
sentationsofapre-trainedDNNtoguideanotherDNN.
Inspiredbytheseideas,weproposeatwo-stepmapping
techniqueasillustratedinFig.
2
.Insteadofdirectlylearn-
ingtodecodeGaussiannoisetotheimagespace,we
learnamappingfunction

thatmapsaGaussianspace
Z
intoacontinuousfeatureembeddingspace
E
,andthen
usethepre-traineddecodertomapthefeatureembedding
space
E
intotherealimagespace
X
.Theencoderlearned
instage-IencodestheFG,BGandPosefactors
x
intolow-
dimensional
real
embeddingfeatures
e
.Then,wetreatthe
featuresmappedfromGaussiannoise
z
as
fake
embedding
features
~
e
andlearnthemappingfunction

adversarially.
Inthisway,wecansample
fake
embeddingfeaturesfrom
noiseandthenmapthembacktoimagesusingthedecoder
learnedinstage-I.Theproposedtwo-stepmappingtech-
niqueiseasytotraininapiecewisestyleandmostimpor-
tantlycanbeusefulforotherimagegenerationapplications.
3.3.Personimagesampling
Asexplained,eachimagefactorcannotonlybeencoded
fromtheinputinformation,butalsobesampledfromGaus-
siannoise.Astothelatter,tosampleanewforeground,
backgroundorpose,wecombinethedecoderslearnedin
stage-Iandmappingfunctionslearnedinstage-IItocon-
structa
z
!
~
e
!
~
x
samplingpipeline(Fig.
4
).Notethat,
forforegroundandbackgroundsamplingthedecoderisa
convolutionalﬁU-netﬂ-basedarchitecture,whileforpose
samplingthedecoderisafully-connectedone.Ourexper-
imentsshowthatourframeworkperformswellwhenused
inbothaconditionalandanunconditionalway.
3.4.Networkarchitecture
Here,wedescribetheproposedarchitecture.Forboth
stages,weuseresidualblockstomakethetrainingeasier.
Allconvolutionlayersconsistof3

3andthenum-
berofincreaseslinearlywitheachblock.Allfully-
connectedlayersconsistof512-dim,exceptforthebottle-
necklayers.Weapplylinearunits(ReLU)toeach
layer,exceptforthebottleneckandtheoutputlayers.
Fortheforegroundandbackgroundbranchesinstage-I,
theinputimageisfedintoaconvolutionalresidualblock
andtheposemaskisusedtoextracttheforegroundand
backgroundfeaturemaps.Then,themaskedforeground
andbackgroundfeaturemapsarepassedintoanencoder
4
Figure4:Samplingphase:Sampleforeground,backgroundandposefromGaussiannoisetocomposenewpersonimages.
consistingof
N
convolutionalresidualblocks,respectively,
where
N
dependsonthesizeoftheinput.Similarto[
21
],
eachresidualblockconsistsoftwoconvolutionlayerswith
stride=1,followedbyonesub-samplingconvolutionlayer
withstride=2,exceptforthelastblock.Forthedecoder,
anﬁU-Netﬂ-basedarchitecture[
29
]isusedwith
N
convo-
lutionalresidualblocksbeforeandafterthebottlenecks,re-
spectively,followingPG
2
(G1+D)[
21
].
Forposereconstruction,weuseanauto-encoderarchi-
tecturewherebothencoderanddecoderconsistof4fully-
connectedresidualblockswith32-dimbottlenecklayers.
Asin[
9
],weuseadensely-connected-likearchitecture,
i.e
.
eachresidualblockconsistsoftwofully-connectedlayers.
Foreachmappingfunctioninstage-II,weuseafully-
connectednetworkconsistingof4fully-connectedresidual
blockstomap
K
-dimGaussiannoise
z
to
K
-dimembed-
dingfeatures
e
.Forthediscriminator,weadoptafully-
connectednetworkwith4fully-connectedlayers.
3.5.Optimizationstrategy
Thetrainingproceduresofstage-Iandstage-IIaresep-
arated,sincethemappingfunctions

fg
,

bg
and

pose
in
stage-IIcanbetrainedinapiecewisestyle.Instage-I,we
usebothL1andadversariallosstooptimizetheimage(
i.e
.
foregroundandbackground)reconstructionnetwork.This
choiceisknowntoresultinsharperandmorerealisticim-
ages.Inparticular,weuse
G
1
and
D
1
todenotetheimage
reconstructionnetworkandthecorrespondingdiscriminator
instage-I.Theoveralllossesfor
G
1
and
D
1
areasfollows,
L
D
1
R
=
E
x
˘
p
data
(
x
)

log
D
1
(
x
)

+
E
x
˘
p
data
(
x
)

log(1

D
1
(
G
1
(
x;h
)))

;
(1)
L
G
1
R
=
E
x
˘
p
data
(
x
)

log(
D
1
(
G
1
(
x;h
)))

+

k
(
G
1
(
x;h
)

x
)
k
1
;
(2)
where
x
denotesthepersonimage,
h
denotesthepose
heatmaps,and

istheweightofL1losscontrollinghow
closethereconstructionlooksliketotheinputimageatlow
frequencies.Forposereconstruction,weusetheL2lossto
reconstructtheinputposeinformationincludingkeypoint
coordinates

andvisibility

mentionedinSec.
3.1
,
L
Pose
R
=
E
(

)
˘
p
data
(

)
k
(
G
1
(
;
)

(
;
)
k
2
2
;
(3)
Aftertrainingthereconstructionnetworkinstage-I,we
itandusetheWassersteinGAN[
1
]losstooptimizethe
fully-connectednetworkofmappingfunctionsinstage-II.
Weuse

and
D
2
todenotethemappingfunctions(incl.

fg
,

bg
and

pose
)andthecorrespondingdiscriminatorsin
stage-II.Theoveralllossesfor

and
D
2
areasfollows,
L
D
2
M
=
E
e
˘
p
emb
(
e
)

D
2
(
e
)


E
z
˘
p
z
(
z
)

D
2

z
))

;
(4)
L

M
=
E
z
˘
p
z
(
z
)

D
2

z
))

;
(5)
where
e
denotestheembeddingfeaturesextractedfromthe
reconstructionnetworkinstage-I,
z
denotestheGaussian
noise.Notethat,wealsotriedthevanillaGANlossbut
sufferedamodelcollapse.Foradversarialtraining,weop-
timizethediscriminatorandgeneratoralternatively.
4.Experiments
Theproposedpipelineenablesmanyapplications,incl.
imagemanipulation,pose-guidedpersonimagegeneration,
imageinterpolation,imagesamplingandpersonre-ID.
2
4.1.Datasetsandmetrics
Ourmainexperimentsusethechallengingre-IDdataset
Market-1501[
41
],containing32,668imagesof1,501per-
sonscapturedfromsixdisjointsurveillancecameras.All
imagesareresizedto128

64pixels.Weusethesame
train/testsplit(12,936/19,732)asin[
41
],butusealltheim-
agesinthetrainsetfortrainingwithoutanyidentitylabel.
2
Moregeneratedresults,parametersofournetworkarchitectureand
trainingdetailsaregiveninthesupplementarymaterial.
5
Forpose-guidedpersonimagegeneration,werandomlyse-
lect12,800pairsinthetestsetfortesting,following[
21
].
Forre-ID,wefollowthesametestingprotocolasin[
41
].
Wealsoexperimentwithahigh-resolutiondataset,
namelyDeepFashion(In-shopClothesRetrievalBench-
mark)[
20
],thatconsistsof52,712in-shopclothesimages
and200,000cross-pose/scalepairs.Following[
21
],we
usetheup-bodypersonimagesandoutfailurecases
inposeestimationforbothtrainingandtesting.Thus,we
have15,079trainingimagesand7,996testingimages.We
alsorandomlyselect12,800pairsfromthetestsetforpose-
guidedpersonimagegenerationtesting.
Implementationdetails.
ForMarket-1501,ourmethodis
appliedtodisentangletheimageintothreefactors:fore-
ground,backgroundandpose.Wesetthenumberofcon-
volutionalresidualblocks
N
=5
forforegroundandback-
groundencodersanddecoders.ForDeepFashion,sinceit
containsalmostnobackground,wedisentangletheimages
intoonlytwofactors:appearanceandpose.Wesetthenum-
berofconvolutionblocks
N
=7
fortheforegroundencoder
anddecoder.Onbothdatasets,wedoaleft-rightdata
augmentation.Fortheposekeypointsandmaskextraction,
weusethesameprocedureas[
21
].
4.2.Imagemanipulation
Asexplained,aperson'simagecanbedisentangledinto
threefactors:FG,BGandPose.Eachfactorcanthenbe
generatedeitherfromaGaussiansignal(sampling)orcon-
ditionedoninputdata,namelyimageandpose(condition-
ing).Theconditionalcasecontainsatleastoneotherfac-
torsampledfromGaussiansignals.InFig.
1
,theleft-top3
rowsshowexampleswithone-factorsamplingandtwo-
factorconditioningforFG,BGandPoseonMarket-1501,
respectively.Ourframeworksuccessfullymanipulateseach
intendedfactorwhilekeepingtheothersunchanged.Inthe
row,wesampleforegroundwith
z
fg
!
~
e
fg
andcon-
ditionbackgroundandposewith
x
!
e
,sothatdifferent
clothcolors,stylesandhairstylescanbegeneratedwhile
theposeandbackgroundstaymostlythesame.Similarly,
wecanmanipulatethebackgroundandposeindependently
asshownintheleft-second/thirdrow.Theleft-lastrow
showsasamplingexamplewithoutanyconditioning.In
thisway,wecansamplenovelpersonimagesfromnoise
andstillgeneraterealisticimagescomparedtovanillaVAE
andDCGANasshowninSec.
4.5
.Finally,ontherightrows
weshowthatourmethodcanalsosample256

256images
withrealisticclothandhairdetailsonDeepFashion.
4.3.Ppersonimagegeneration
WecompareourmethodwithPG
2
[
21
]onpose-
conditionalpersonimagegeneration.UnlikePG
2
,our
methoddoesnotneedpairedtrainingimages.Asshown
inFig.
5
,ourmethodcangeneratemorerealisticdetails
DeepFashionMarket-1501
ModelSSIMISSSIMISmask-SSIMmask-IS
PG
2
[
21
]0.7623.0900.2533.4600.7923.435
Ours0.6143.2280.0993.4830.6143.491
Table1:Quantitativeevaluation.Higherscoresarebetter.
andlessartifacts.Especially,thearmsandlegsarebetter
shapedonbothdatasets,andthehairdetailsaremoreclear
onDeepFashion.ThisisinagreementwiththeInception
Score(IS)andmaskInceptionScore(mask-IS)inTable
1
.
TheSSIMscoreofourmethodislowerthanPG
2
mainly
fortworeasons.1)Instage-I,therearenoskip-connections
betweenencoderanddecoder,andassuchourmethodhas
togenerateimagesfromcompressedembeddingfeaturesin-
steadofpixelleveltransformslikeinPG
2
,whichisaharder
task.2)Ourmethodgeneratessharperimageswhichmight
decreasetheSSIMscore,asalsoobservedin[
21
,
10
,
31
].
4.4.Imageinterpolation
Interpolationispossibleforsampledandrealimages.
Samplinginterpolation.
Forsamplinginterpolation,we
directlyinterpolateinGaussianspaceandgenerateimages
ina
z
!
~
e
!
~
x
manner.Weinterpolatelinearly
betweentwoGaussiancodes
z
1
and
z
2
toobtaininterme-
diatecodes
z
i
,whichinturnaremappedintoembedding
features
~
e
i
usingthelearnedmappingfunctions.Theper-
son'simageisthengeneratedfromtheembeddingfeatures
~
e
i
.AsFig.
6
(a)(b)(c)shows,ourmethodcansmoothlyin-
terpolateeachfactorinGaussianspaceseparately,hence:
1)ourmethodcanlearnforeground,backgroundandpose
encodersinadisentangledway;2)thesecanmapreal
high-dimensionaldatadistributionsintocontinuouslow-
dimensionalfeatureembeddingdistributions;3)themap-
pingstrainedadversariallycanmapGaussiantofeatureem-
beddingdistributions;4)thedecodercanmapfeatureem-
beddingdistributionsbacktorealdatadistributions.
Inverseinterpolation
Tointerpolatebetweenrealdata
(incl.imageandposekeypoints),weproceedin3steps.1)
x
!
e
:Usethelearnedencoderstoencoderealdata
x
into
embeddingfeatures
e
.2)
e
!
z
:Usegradient-basedmin-
imization[
19
]tothecorrespondingGaussiancodes
z
.
3)
z
!
~
e
!
~
x
:InterpolatelinearlybetweentwoGaussian
codes,thenmapintermediatecodesintoembeddingfea-
tures-usingthelearnedmappingfunctions-togeneratethe
personimages.AsshowninFig.
6
,ourmethodinterpolates
reasonableframesbetweentheinputpairshowingaperson
withdifferentposes.Theresultshowsrealisticintermediate
statesandcanbeusedtopredictpotentialbehaviors.
4.5.Samplingresultscomparison
Inthisexperiment,wecomparesamplingresultsfrom
ourmethodandbaselinemodels,
i.e
.VAE[
13
]andDC-
6
Figure5:ComparisontoPG
2
.Left:resultsonMarket-1501.Right:resultsonDeepFashion.Zoominfordetails.
(a)Foregroundinterpolation
(b)Backgroundinterpolation
(c)Poseinterpolation
(d)Inverseinterpolationbetweentwoimages.
Figure6:Factorinterpolation.(a)(b)(c)WerandomlyselecttwoGaussiancodes
z
1
and
z
2
andinterpolatecodesbetween
z
1
and
z
2
linearly;wethengeneratetheinterpolatedimagesaccordingly.(d)Weinvertanimagepairtoembeddingfeatures
e
1
and
e
2
,thentoGaussiancodes
z
1
and
z
2
.Wethenfollowthesameprocedureasin(a)(b)(c).
GAN[
25
].AsillustratedinFig.
7
,VAEgeneratesblurry
imagesandDCGANsharpbutunrealisticpersonimages.
Incontrast,ourmodelgeneratesmorerealisticimages(see
Fig.
7
(c)(d)(e)).Bycomparing(d)and(c),weobservethat
ourmodelusingbodyROIgeneratesmoresharpandreal-
isticimageswhosecolorsoneachbodypartaremorenatu-
ral.Asimilartendencycanbeobservedforre-ID.Bycom-
paring(e)and(d),weseethatwhensamplingforeground
andbackgroundbutusingtherealposekeypointsrandomly
selectedfromthetrainingdata,wegeneratebetterresults.
Therefore,weusethissettingin(e)tosamplevirtualdata
forthefollowingre-IDexperiment.
4.6.Personr
Personre-IDassociatesimagesofthesameperson
acrossviewsortime.Giventhequerypersonimage,re-ID
isexpectedtoprovidematchingimagesofthesameiden-
tity.Weproposetousethere-IDperformanceasaquan-
titativemetricforourgenerationapproach.Weadoptthe
re-IDmodelin[
6
]anduserank-1matchingrateandmean
AveragePrecision(mAP)following[
41
].Weshowthat
ourapproachcanbeevaluatedintwoways:(1)useFG
featuresextractedinstage-Iforre-ID;(2)generatevirtual
imagepairstotrainre-IDmodel.Thevirtualmarketdata
isdenotedasﬁVMﬂgeneratedwithourBodyROI7model.
Notethat,CUHK03[
17
]andDuke[
42
]datasetsareused
withidentitylabels,whileMarket-1501andVMdatasets
areusedwithnolabels.
Usingembeddingfeatures.
WeusetheFGencodertoex-
tractthefeaturesforre-IDandusethere-IDperformanceto
evaluatethereconstructionnetworkinstage-I.Intuitively,
there-IDperformancewillbehigheriftheencodedfeatures
aremorerepresentative.Euclideandistanceisusedtocalcu-
latetheextractedfeaturesafter
l
2
-normnormalization[
6
].
AsshowninthetoprowsofTable
2
,ourBodyROI7model
7
(a)VAE[
13
]
(b)DCGAN[
25
]
(c)Ours-WholeBody
(d)Ours-BodyROI7
(e)Ours-BodyROI7withrealposefromtrainingset
(f)Realdata
Figure7:Samplingresultscomparison.Fromlefttorightandfromtoptobottom:(a)VAE[
13
](b)DCGAN[
25
](c)Ours-
WholeBody(d)Ours-BodyROI7(e)Ours-BodyROI7withrealposefromtrainingset(f)Realdata.
Figure8:Virtualidentitiesforre-IDmodeltraining.Each
columncontainsapairofimagesofoneidentity(oneFG).
BGandPosearerandomlyselectedfromtrainingdata.
ModelTrainingdataRank-1mAP
Bow[
41
]Market0.3440.141
Bow*[
41
]Market0.3580.148
LOMO*[
18
]/0.2720.08
WholeBodyfeature(Ours)Market0.3070.100
BodyROI7feature(Ours)Market0.3380.107
BodyROI7featurePCA(Ours)Market0.3550.114
Res50*[
6
]CUHK03(labeled)0.3000.115
Res50*[
6
]Duke(labeled)0.3610.142
Res50VM0.3380.134
Res50+PULVM+Market0.3690.156
Res50+PUL+KISSMEVM+Market0.3750.154
Table2:Re-IDresultsonMarket-1501.Top:usingembed-
dingfeatures.Bottom:usingVMandMarket-1501dataset
withoutlabels.Higherscoresarebetter.*Resultsarere-
portedin[
6
]./meansthathand-craftedfeatureextractor
LOMOdoesnotrequiretrainingdata.
achieves0.338and0.355(withPCA)rank-1performance,
higherthanourWholeBodymodel,whichisinaccordance
withthesamplingresultsinSec.
4.5
.Besides,ourmethod
canachievecomparableperformancewiththeunsupervised
baselinemethods,whichindicatesthatourencodercanex-
tractnotonlygenerativebutalsodiscriminativefeatures.
Usinggeneratedvirtualimagepairs.
Weusethegener-
atedimagepairstotrainthere-IDmodelandusethere-
IDperformancetoevaluateourgenerationframeworkinan
indirectmanner.WegeneratetheVMre-IDdataset
consistingof500identitieswith24imagesforeachIDas
illustratedinFig.
8
.Foreachidentity,werandomlysample
oneforegroundfeatureand24backgroundfeaturesandran-
domlyselect24posekeypointheatmapsfromtheMarket-
1501trainingdata.Then,weusethesamere-IDmodeland
trainingprocedureasin[
6
],butwithdifferenttrainingdata.
AsshowninthebottomrowsofTable
2
,usingourVMdata
themodelcanachievetherank-1performance0.338which
iscomparabletothemodeltrainedusinganotherDukere-
IDdataset.Whenusingthepost-processingprogressiveun-
supervisedlearning(PUL)proposedin[
6
],therank-1per-
formanceisimprovedto0.369.Additionally,usingourVM
data,wecantrainametricmodel,
e.g
.KISSME[
14
],and
furtherimprovetherank-1performanceto0.375.Com-
paredtothemodeltrainedusingCUHK03(rank-10.300)
orDuke(rank-10.361)re-IDdatasetwithexpensivehuman
annotations,ourmethodachievesbetterperformanceusing
onlyMarketdatasetwithoutidentitylabels.Theseresults
showthatourdisentangledgeneratedimagesaresimilarto
therealdataandcanbefurthertore-IDtasks.
5.Conclusion
Weproposeanoveltwo-stagepipelineforaddressingthe
personimagegenerationtask.Stage-Idisentanglesanden-
codesthreemodesofvariationintheinputimage,namely
foreground,backgroundandpose,intoembeddingfeatures
thendecodesthembacktoanimageusingamulti-branched
reconstructionnetwork.Stage-IIlearnsmappingfunctions
inanadversarialmannerformappingnoisedistributions
tofeatureembeddingdistributionsguidedbythedecoders
learnedinstage-I.Experimentsshowthatourmethodcan
manipulatetheinputforeground,backgroundandpose,and
samplenewembeddingfeaturestogenerateintendedma-
nipulationsofthesefactors,thusprovidingmorecontrol.In
thefuture,weplantoapplyourmethodtofacesandrigid
objectimageswithdifferenttypesofstructure.
Acknowledgments
Thisresearchwassup-
portedinpartbyToyotaMotorsEurope,
GermanResearchFoundation(DFGCRC
1223).
8
References
[1]
M.Arjovsky,S.Chintala,andL.Bottou.Wassersteingan.
In
ICLR
,2017.
1
,
5
[2]
X.Chen,Y.Duan,R.Houthooft,J.Schulman,I.Sutskever,
andP.Abbeel.Infogan:Interpretablerepresentationlearning
byinformationmaximizinggenerativeadversarialnets.In
NIPS
,2016.
1
,
3
[3]
B.Cheung,J.A.Livezey,A.K.Bansal,andB.A.Olshausen.
Discoveringhiddenfactorsofvariationindeepnetworks.In
ICLRworkshop
,2015.
3
[4]
C.Doersch,A.Gupta,andA.A.Efros.Unsupervisedvi-
sualrepresentationlearningbycontextprediction.In
ICCV
,
2015.
3
[5]
P.Doll
´
ar,V.Rabaud,andS.J.Belongie.Learningtotraverse
imagemanifolds.In
NIPS
,2007.
4
[6]
H.Fan,L.Zheng,andY.Yang.Unsupervisedpersonre-
Clusteringand
arXivpreprint
arXiv:1705.10444
,2017.
7
,
8
[7]
I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,
D.Warde-Farley,S.Ozair,A.Courville,andY.Bengio.Gen-
erativeadversarialnets.In
NIPS
,2014.
1
,
2
[8]
S.Gupta,J.Hoffman,andJ.Malik.Crossmodaldistillation
forsupervisiontransfer.In
CVPR
,2016.
4
[9]
G.Huang,Z.Liu,K.Q.Weinberger,andL.vanderMaaten.
Denselyconnectedconvolutionalnetworks.In
CVPR
,2017.
5
[10]
J.Johnson,A.Alahi,andL.Fei-Fei.Perceptuallossesfor
real-timestyletransferandsuper-resolution.In
ECCV
,2016.
6
[11]
Y.Kim,K.Zhang,A.M.Rush,Y.LeCun,etal.Adversar-
iallyregularizedautoencodersforgeneratingdiscretestruc-
tures.
arXivpreprintarXiv:1706.04223
,2017.
2
[12]
D.P.KingmaandJ.Ba.Adam:Amethodforstochastic
optimization.
arXivpreprintarXiv:1412.6980
,2014.
11
[13]
D.P.KingmaandM.Welling.Auto-encodingvariational
bayes.
arXivpreprintarXiv:1312.6114
,2013.
1
,
2
,
7
,
8
[14]
M.K
¨
ostinger,M.Hirzer,P.Wohlhart,P.M.Roth,and
H.Bischof.LargeScaleMetricLearningfromEquivalence
Constraints.In
CVPR
,2012.
8
[15]
C.Lassner,G.Pons-Moll,andP.V.Gehler.Agenerative
modelofpeopleinclothing.In
ICCV
,2017.
3
[16]
D.Lee,S.Yun,S.Choi,H.Yoo,M.-H.Yang,andS.Oh.Un-
supervisedholisticimagegenerationfromkeylocalpatches.
arXivpreprintarXiv:1703.10730
,2017.
3
[17]
W.Li,R.Zhao,T.Xiao,andX.Wang.Deepreid:Deep
pairingneuralnetworkforpersonIn
CVPR
,
2014.
7
[18]
S.Liao,Y.Hu,X.Zhu,andS.Z.Li.Person
bylocalmaximaloccurrencerepresentationandmetric
learning.In
CVPR
,2015.
8
[19]
Z.C.LiptonandS.Tripathi.Preciserecoveryoflatentvec-
torsfromgenerativeadversarialnetworks.In
ICLRwork-
shop
,2017.
6
[20]
Z.Liu,P.Luo,S.Qiu,X.Wang,andX.Tang.Deepfashion:
Poweringrobustclothesrecognitionandretrievalwithrich
annotations.In
CVPR
,2016.
6
[21]
L.Ma,J.Xu,Q.Sun,B.Schiele,T.Tuytelaars,and
L.VanGool.Poseguidedpersonimagegeneration.In
NIPS
,
2017.
1
,
3
,
4
,
5
,
6
[22]
A.Makhzani,J.Shlens,N.Jaitly,I.Goodfellow,andB.Frey.
Adversarialautoencoders.
arXivpreprintarXiv:1511.05644
,
2015.
2
[23]
T.B.Moeslund,A.Hilton,andV.Kr
¨
uger.Asurveyofad-
vancesinvision-basedhumanmotioncaptureandanalysis.
Computervisionandimageunderstanding
,104(2):90Œ126,
2006.
2
[24]
M.NorooziandP.Favaro.Unsupervisedlearningofvisual
representationsbysolvingjigsawpuzzles.In
ECCV
,2016.
3
[25]
A.Radford,L.Metz,andS.Chintala.Unsupervisedrepre-
sentationlearningwithdeepconvolutionalgenerativeadver-
sarialnetworks.In
ICLR
,2016.
1
,
2
,
7
,
8
[26]
S.Reed,Z.Akata,S.Mohan,S.Tenka,B.Schiele,and
H.Lee.Learningwhatandwheretodraw.In
NIPS
,2016.
2
[27]
S.Reed,A.vandenOord,N.Kalchbrenner,V.Bapst,
M.Botvinick,andN.deFreitas.Generatinginterpretable
imageswithcontrollablestructure.Technicalreport,2016.
2
[28]
A.Romero,N.Ballas,S.E.Kahou,A.Chassang,C.Gatta,
andY.Bengio.Fitnets:Hintsforthindeepnets.In
ICLR
,
2015.
4
[29]
O.Ronneberger,P.Fischer,andT.Brox.U-net:Convolu-
tionalnetworksforbiomedicalimagesegmentation.In
MIC-
CAI
,2015.
4
,
5
,
12
[30]
L.K.SaulandS.T.Roweis.Thinkglobally,locally:un-
supervisedlearningoflowdimensionalmanifolds.
Journal
ofmachinelearningresearch
,4(Jun):119Œ155,2003.
4
[31]
W.Shi,J.Caballero,F.Husz
´
ar,J.Totz,A.P.Aitken,
R.Bishop,D.Rueckert,andZ.Wang.Real-timesingleim-
ageandvideosuper-resolutionusinganefsub-pixel
convolutionalneuralnetwork.In
CVPR
,2016.
6
[32]
Q.Sun,L.Ma,S.J.Oh,L.V.Gool,B.Schiele,andM.Fritz.
Naturalandeffectiveobfuscationbyheadinpainting.In
CVPR
,2018.
3
[33]
Q.Sun,B.Schiele,andM.Fritz.Adomainbasedapproach
tosocialrelationrecognition.In
CVPR
,2017.
3
[34]
L.Tran,X.Yin,andX.Liu.Disentangledrepresentation
learningganforpose-invariantfacerecognition.In
CVPR
,
2017.
3
[35]
A.VanDenOord,N.Kalchbrenner,andK.Kavukcuoglu.
Pixelrecurrentneuralnetworks.In
ICML
,2016.
1
,
2
[36]
K.Q.WeinbergerandL.K.Saul.Unsupervisedlearning
ofimagemanifoldsbyprogramming.
Interna-
tionaljournalofcomputervision
,70(1):77Œ90,2006.
4
[37]
K.Yu,T.Zhang,andY.Gong.Nonlinearlearningusinglocal
coordinatecoding.In
NIPS
,2009.
4
[38]
H.Zhang,T.Xu,H.Li,S.Zhang,X.Huang,X.Wang,and
D.Metaxas.Stackgan:Texttophoto-realisticimagesynthe-
siswithstackedgenerativeadversarialnetworks.In
ICCV
,
2017.
4
[39]
B.Zhao,X.Wu,Z.Cheng,H.Liu,andJ.Feng.Multi-view
imagegenerationfromasingle-view.
arXiv
,1704.04886,
2017.
3
9
[40]
H.Zhao,M.Tian,S.Sun,J.Shao,J.Yan,S.Yi,X.Wang,
andX.Tang.Spindlenet:Personwithhu-
manbodyregionguidedfeaturedecompositionandfusion.
In
CVPR
,2017.
4
[41]
L.Zheng,L.Shen,L.Tian,S.Wang,J.Wang,andQ.Tian.
ScalablepersonAbenchmark.In
ICCV
,
2015.
5
,
6
,
7
,
8
[42]
Z.Zheng,L.Zheng,andY.Yang.Unlabeledsamplesgener-
atedbyganimprovethepersonbaselinein
vitro.In
ICCV
,2017.
2
,
7
[43]
J.-Y.Zhu,T.Park,P.Isola,andA.A.Efros.Unpairedimage-
to-imagetranslationusingcycle-consistentadversarialnet-
works.In
ICCV
,2017.
3
10
Supplementarymaterials
Thissupplementarymaterialincludesadditionaldetails
regardingthenetworkarchitecture(
x
A
)andtraining(
x
B
),
aswellasextendedresultsforimagemanipulation(
x
C
),
pose-guidedpersonimagegeneration(
x
D
),inverseinterpo-
lation(
x
E
)andimagesampling(
x
F
),respectively.
A.Networkarchitecture
Inthissection,weprovidedetailsregardingthenet-
workarchitecturesinourtwo-stageframeworkusedonthe
Market-1501dataset.Fig.
10
shows4networkarchitec-
turesusedatstage-I:1)FGencoderconsistsof5convo-
lutionalresidualblocks;2)BGencoderconsistsof5con-
volutionalresidualblocks;3)FG&BGdecoderfollowsa
ﬁU-netﬂ-basedarchitecture;4)Poseauto-encoderfollows
afully-connectedauto-encoderarchitecture.Fig.
9
shows
thenetworkarchitectureofthemappingfunctions

used
atstage-II.Itcontains4fully-connectedresidualmodules.
B.Trainingdetails
OnMarket-1501,ourmethodisappliedtodisentangle
theimageintothreefactors:foreground,backgroundand
pose.Wetraintheforegroundandbackgroundmodelswith
amini-batchofsize
16
for
˘
70
k
iterationsatstage-Iand
withamini-batchofsize
32
for
˘
30
k
iterationsatstage-II.
Theposemodelsaretrainedwithamini-batchofsize
64
for
˘
30
k
iterationsatstage-Iandwithamini-batchofsize
32
for
˘
60
k
iterationsatstage-II.
DeepFashiondatacontaincleanbackground,therefore,
ourmethodisappliedtodisentangletheimageintoonlytwo
factors:appearance(
i.e
.foreground)andpose.Wetrainthe
appearancemodelwithaminibatchofsize
6
for
˘
100
k
it-
erationsatstage-Iandwithaminibatchofsize
16
for
˘
60
k
iterationsatstage-II.Theposemodelsaretrainedwitha
minibatchofsize
32
for
˘
30
k
iterationsatstage-Iandwith
aminibatchofsize
32
for
˘
60
k
iterationsatstage-II.
Onbothdatasets,weusetheAdamoptimizer[
12
]with
weights

1
=0
:
5
and

2
=0
:
999
.Theinitiallearning
rateissetto
2
e
-
5
.Foradversarialtraining,weoptimizethe
discriminatorandgeneratoralternatively.
C.Imagemanipulationresults
InFig.
11
andFig.
12
,weprovideresultsonappearance
samplingandposesamplingfortheDeepFashiondatasetas
anextensionofFig.1inthemainpaper.Foreachfactor,we
sampletheembeddingfeaturefromGaussiannoiseand
theotherfactorsbyusingtheembeddingfeatureextracted
fromtherealdataasexplainedinSec.4.2inthemainpaper.
D.Pose-guidedpersonimagegenerationre-
sults
Forpose-guidedpersonimagegeneration,weprovide
moregeneratedresults.AsanextensionofFig.5inthe
mainpaper,Fig.
13
showsthegeneratedimagesofoneap-
pearancewithvariousrealposesselectedrandomlyfrom
DeepFashion.
E.Inverseinterpolationresults
Inthissection,weprovidemoreinverseinterpolationre-
sultsinFig.
14
asanextensionofFig.6inthemainpaper.
Fortwoimages
x
1
and
x
2
,wethecorrespondingGaus-
siancodes
z
1
and
z
2
asexplainedintheSec.4.4ofthemain
paper.AsshowninFig.
14
(a)(b),ourmethodsuccessfully
generatestheintermediatestatesbetweentwoimagesofthe
sameperson.Notethat,theinverseinterpolationbetween
twoimagesofdifferentpersonsismorechallenging(see
Fig.
14
(c))sinceweneedtointerpolateboththeappearance
andpose.
F.Imagesamplingresults
Wealsogivemoresamplingresultsasextensionsof
Fig.7inthemainpaper.Fig.
15
showsthesamplingresults
(a-e)andrealimages(f)onMarket-1501dataset.VAEgen-
eratesblurryimagesandDCGANsharpbutunrealisticper-
sonimages.Incontrast,ourmodelgeneratesmorerealistic
images(c)(d)(e).Bycomparing(d)and(c),weobservethat
ourmodelusingbodyROIgeneratesmoresharpandrealis-
ticimageswhosecolorsoneachbodypartaremorenatural.
Bycomparing(e)and(d),weseethatwhensamplingfore-
groundandbackgroundbutusingtherealposekeypoints
randomlyselectedfromthetrainingdata,wegeneratebet-
terresults.
Figure9:Networkarchitectureofthemappingfunctionsfor
FG,BGandPoseinstage-II.
11
(a)
(b)
(c)
(d)
Figure10:Networkarchitecturesofstage-I.(a)FGencoder,fedwiththeextracted7FGbodyROIfeaturemapsandout-
putting7FGembeddingfeaturesof32-dimafter5convolutionalresidualblocks.(b)BGencoder,fedwiththeBGfeature
mapsandoutputtingaBGembeddingfeatureof128-dimafter5convolutionalresidualblocks.(c)FGandBGdecoder,fed
withtheconcatenatedappearanceandposefeaturemapsandoutputtingthegeneratedimageaftertheﬁU-netﬂ-based[
29
]
architecture.(d)Poseauto-encoder,fedwiththeconcatenatedkeypointcoordinatesandvisibilityvectorandoutputtingthe
reconstructedvectoraftertheauto-encoder.
12
Figure11:AppearancesamplingedPose)resultsontheDeepFashiondataset.Ineachrow,6differentappearancefactors
aresampledfromGaussiannoiseandtheposefactorisedtoarealone.
13
Figure12:PosesamplingedAppearance)resultsontheDeepFashiondataset.Ineachrow,6differentposefactorsare
sampledfromGaussiannoisesandtheappearancefactorisedtoarealone.
14
Figure13:GeneratedresultsforoneappearancewithvariousposesontheDeepFashiondataset.
15
(a)
(b)
(c)
Figure14:InverseinterpolationresultsonMarket-1501.(a)Interpolationbetweentwoimagesofthesameperson.(b)
Interpolationbetweenthreeimagesofthesameperson.(c)Interpolationbetweentwoimagesofdifferentpersons.
16
(a)VanillaVAE
(b)VanillaDCGAN
(c)Ours-WholeBody
(d)Ours-BodyROI7
(e)Ours-BodyROI7withrealposefromtrainingset
(f)Realdata
Figure15:Samplingresults.(a)VanillaVAE;(b)VanillaDCGAN;(c)Ours-WholeBody;(d)Ours-BodyROI7;(e)Ours-
BodyROI7posewithrealposefromtrainingset;(f)Realdata.
17
"
73,Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D Representations,http://arxiv.org/pdf/1708.07303v4.pdf,https://github.com/koryako/selfcarRaspberryPi,"Learning6-DOFGraspingInteractionviaDeepGeometry-aware3DRepresentations
XinchenYan

JasmineHsu
z
MohammadKhansari
y
YunfeiBai
y
ArkanathPathak
z
AbhinavGupta
z
JamesDavidson
z
HonglakLee
z
Abstract
ŠThispaperfocusesontheproblemoflearning6-
DOFgraspingwithaparalleljawgripperinsimulation.Our
keyideaisconstrainingandregularizinggraspinginteraction
learningthrough3Dgeometryprediction.Weintroduceadeep
geometry-awaregraspingnetwork(DGGN)thatdecomposes
thelearningintotwosteps.First,welearntobuildmental
geometry-awarerepresentationbyreconstructingthescene
(i.e.,3Doccupancygrid)fromRGBDinputviagenerative3D
shapemodeling.Second,welearntopredictgraspingoutcome
withitsinternalgeometry-awarerepresentation.Thelearned
outcomepredictionmodelisusedtosequentiallypropose
graspingsolutionsviaanalysis-by-synthesisoptimization.Our
contributionsarefourfold:(1)Tobestofourknowledge,we
arepresentingforthetimeamethodtolearna6-DOF
graspingnetfromRGBDinput;(2)Webuildagraspingdataset
fromdemonstrationsinvirtualrealitywithrichsensoryand
interactionannotations.Thisdatasetincludes101everyday
objectsspreadacross7categories,additionally,wepropose
adataaugmentationstrategyforeffectivelearning;(3)We
demonstratethatthelearnedgeometry-awarerepresentation
leadstoabout10
%
relativeperformanceimprovementover
thebaselineCNNongraspingobjectsfromourdataset.(4)
Wefurtherdemonstratethatthemodelgeneralizestonovel
viewpointsandobjectinstances.
I.I
NTRODUCTION
Learningtointeractwithandgraspobjectsisafundamental
andchallengingprobleminrobotlearningthatcombines
perception,motionplanning,andcontrol.Theproblem
ischallengingbecauseitnotonlyrequiresunderstanding
geometry(theglobalshapeofanobject,thelocalsurface
aroundtheinteractionspace)butitalsorequiresestimating
physicalproperties,suchasweight,density,andfriction.
Furthermore,itrequiresinvariancetoillumination,object
location,andviewpoint.Tohandlethis,currentdata-driven
approaches[
17
],[
29
],[
19
],[
22
],[
21
]usehundredsof
thousandsofexamplestolearnasolution.
Whilefurtherscalingmayhelpimproveperformanceof
thesemethods,wepostulateshapeiscoretointeractionand
thatadditionalshapesignalstofocuslearningwillboost
performance.Thenotionofusingshapeandgeometryhas
beenpioneeredingraspingresearch[
11
],[
18
],[
1
],[
20
],[
36
].
Inspiredbytheseapproaches,weproposetheconceptof
adeep
geometry-aware
representation(e.g.,[
40
],[
9
],[
2
],
[
39
],[
23
],[
30
],[
41
],[
34
],[
10
],[
8
])forgrasping.Keyto
ourapproachisthatwebuildamentalrepresentationby
recognizing
and
reconstructing
the3Dgeometryofthescene
fromRGBDinput,asdemonstratedinFigure1.Withthe

UniversityofMichigan,duringinternshipwithGoogleBrain.
z
Google,
y
XInc
Fig.1.Learninggraspinginteractionsfromdemonstrationswithdeep
geometry-awarerepresentations.First,welearntobuildmentalgeometry-
awarerepresentationbyreconstructingthe3Dscenewith2.5Dtrainingdata.
Second,welearntopredictgraspingoutcomewithitsinternalgeometry-
awarerepresentation.
built-in3Dgeometry-awarerepresentation,wecanhallucinate
alocalviewoftheobject'sgeometricsurfacefromthe
gripperperspectivethatwillbedirectlyusefulforgrasping
interaction.Incontrastwithblack-boxmodelsthatdonot
haveexplicitnotionof3Dgeometryandpriorshape-based
graspingapproaches,ourapproachhasthefollowingfeatures:
(1)itperforms3Dshapereconstructionasanauxiliary
task;(2)ithallucinatesthelocalviewusingalearning-free
physicalprojectionoperator;and(3)itexplicitlyreusesthe
learnedgeometry-awarerepresentationforgraspingoutcome
prediction.
Inthiswork,wedesignanend-to-enddeepgeometry-
awaregraspingnetworkforlearningthisrepresentation.Our
geometry-awarenetworkhastwocomponents:ashapegenera-
tionnetworkandagraspingoutcomepredictionnetwork.The
shapegenerationnetworklearnstorecognizeandreconstruct
the3Dgeometryofthescenewithanimageencoderand
voxeldecoder.TheimageencodertransformstheRGBDinput
intoahigh-levelgeometryrepresentationthatinvolvesshape,
location,andorientationoftheobject.Thevoxeldecoder
networktakesinthegeometryrepresentationandoutputsthe
occupancygridoftheobject.Tofurtherhallucinatethelocal
viewfromgripperperspective,weproposeanovellearning-
freeimageprojectionlayersimilarto[
41
],[
30
].Building
upontheshapegenerationnetwork,ourgraspingoutcome
predictionnetworklearnstoproduceagraspingoutcome
(e.g.,successorfailure)basedontheaction(i.e.gripper
pose),thecurrentvisualstate(e.g.,objectandgripper),and
thelearnedgeometry-aware3Drepresentation.Unlikeour
end-to-endmulti-objectivelearningframework,existingdata-
drivengraspingpipelines[
29
],[
22
],[
21
]canbeviewedas
modelswithoutashapegenerationcomponent.Theyrequire
eitheranadditionalcameratocapturetheglobalobject
shapeorextraprocessingsteps,suchasobjectdetection
andpatchalignment.Furthermore,thesemethodslearnover
aconstrainedgraspspace,typicallyeither3-DOFor4-DOF.
arXiv:1708.07303v4  [cs.RO]  15 Jun 2018Werelaxthisconstrainttolearnfullygeneralized6-DOF
graspposes.
Wehavebuiltalargedatabaseconsistingof101everyday
objectswitharound150KgraspingdemonstrationsinVirtual
Realitywithbothhumanandaugmentedsyntheticinteractions.
Foreachobject,wecollect10-20graspingattemptswith
aparalleljawgripperfromright-handedusers.Foreach
attempt,werecordapre-graspingstatuswhichincludes
thelocationandorientationoftheobjectandgripper,as
wellasthegraspingoutcome(e.g.,successorfailuregiven
iftheobjectisbetweenthegripperafterclosing
andlifting).Toacquiresufdataforlearning,we
generateadditionalsyntheticdatabyperturbingthegripper
locationandorientationfromhumandemonstrationsusing
PyBullet[
3
].Moreinformationaboutourgeometry-aware
graspingprojectcanbefoundat
https://goo.gl/gPzPhm
.
Ourmaincontributionsaresummarizedbelow:

Tobestofourknowledge,wearepresentingforthe
timeamethodtolearna6-DOFdeepgraspingneural
networkfromRGBDinput.

Webuildadatabasewithrichvisualsensorydataand
graspingannotationswithavirtualrealitysystemand
proposeadataaugmentationstrategyforeffectivelearn-
ingwithonlymodestamountofhumandemonstrations.

Wedemonstratethattheproposedgeometry-aware
graspingnetworkisabletolearntheshapeaswell
asgraspingoutcomebetterthanmodels
withoutnotionofgeometry.

Wedemonstratethattheproposedmodelhasadvantages
inguidinggraspingexplorationandachievesbetter
generalizationtonovelviewpointsandnovelobject
instances.
II.R
ELATED
W
ORK
Acommonapproachforroboticgraspingistodetectthe
optimalgraspinglocationfrom2Dor2.5Dvisualinputs(RGB
orRGBDimages,respectively)[
32
],[
24
],[
17
],[
29
],[
12
],
[
16
],[
27
].Earlierwork[
32
],[
24
]studiedtheplanargrasping
problemusingvisualfeaturesextractedfrom2Dsensoryinput
andadoptedlogisticregressionforoptimalgrasping
locationwithvisualfeatures.Lenzetal.[
17
]proposedatwo-
stepdetectionpipeline(objectdetectionandgraspingpart
detection)withdeepneuralnetworks.PintoandGupta[
29
]
builtaroboticsystemforlearninggraspingfromlarge-scale
real-worldtrial-and-errorexperiments.Inthiswork,adeep
convolutionalneuralnetworkwastrainedon700hoursof
roboticgraspingdatacollectedfromthesystem.
Fine-grainedgraspingplanningandcontrolofteninvolves
3Dmodelingofobjectshape,modelingdynamicsofrobot
hands,andlocalsurfacemodeling[
11
],[
18
],[
14
],[
37
],[
20
],
[
36
],[
22
],[
21
].Someworkfocusedonanalyticmodelingof
roboticgraspswithknownobjectshapeinformation[
11
],[
18
].
Varleyetal.[
37
]proposedashapecompletionmodelthat
reconstructsthe3Doccupancygridforroboticgraspingfrom
partialobservations,whereground-truth3Doccupancygrid
isusedduringmodeltraining.Incomparison,ourapproach
doesnotrequirefull3Dvolumesupervisionfortraining(e.g.,
occupancygrid).Similartoourwork,[
1
]usealearnedshape-
contexttohelppredictgrasps.Unliketheirwork,weusethe
shapetobuildavirtualglobalgeometricrepresentationalong
withalocalgrippercentricmodeltosequentiallyproposeand
evaluategraspproposals.Lietal.[
20
]investigatedthehand
poseestimationinroboticgraspingbydecouplingcontact
pointsandhandwithparametrizedobjectshape.
Buildinguponthecompositionalaspectofeverydayobjects,
Vahrenkampetal.[
36
]proposedapart-basedmodelfor
roboticgraspingthathasbettergeneralizationtonovelobject.
Veryrecently,effortwasalsomadeinbuildingDexNet[
22
],
[
21
],alarge-scalepointclouddatabaseforplanargrasping
(fromtop-down).Inadditiontogeneralroboticgrasping,
severalrecentworkinvestigatedthesemanticor
grasping[4],[15],[25].
Incontrasttoexistinglearningframeworksappliedto
roboticgrasping(eithertop-downgraspingorside-grasping),
ourapproachfeatures(1)providingamethodtolearna6D
graspingnetworkfromRGBDinput(2)anend-to-enddeep
learningframeworkforgenerative3Dshapemodelingand
leveragingitforpredictive6Dgraspinginteraction,and(3)
learning-freeprojectionlayerthatlinksthe2Dobservations
with3Dobjectshapewhichallowsforlearningtheshape
representationwithoutexplicit3Dvolumesupervision.
III.M
ULTI
-
OBJECTIVEFRAMEWORKWITH
GEOMETRY
-
AWAREREPRESENTATION
Inthissection,wedevelopamulti-objectivelearning
frameworkthatperforms3Dshapegenerationandgrasping
outcomeprediction.
A.Learninggenerativegeometry-awarerepresentationfrom
RGBDinput
Beingableto
recognize
and
reconstruct
the3Dgeometry
givenRGBDinputisaveryimportantstepduringgrasping
planning.Inourformulation,weproposeareconstruction
ofa3Doccupancygrid[
40
],[
9
],[
2
],[
39
],[
30
],[
41
],[
34
],
[
10
],[
8
]thatencodestheshape,location,andorientationof
theobjectasourgeometry-awarerepresentation.Previous
workgeneratenormalized3Doccupancygridscenteredatthe
origin.Ourformulatedgeometry-awarerepresentationdiffers
inthat(1)ittakeslocationandorientationintoconsideration
(theorientationofanovelobjectisusually(2)it
isinvarianttocameraviewpointanddistance(weobtainthe
samerepresentationfromarbitrarycamerasetting).
GivenanRGBDinput
I
andacorresponding3Doccupancy
grid
V
,thetaskistolearnafunctionalmapping
f
V
:
I!
V
.
Simplyfollowingthisformulation,previouswork[
40
],[
9
],
[
2
],[
39
],[
23
]thatuse3Dsupervisionobtainedreasonable
qualityingeneratingnormalized3Dvolumesbyusing
thousandsofshapeinstances.However,inourproblemsetting,
thesemethodswouldrequireevenmoredataconsideringthe
entangledfactorsfromshape,location,andorientation.
B.Depthsupervisionwithin-networkprojectionlayer
Recentbreakthroughsinreconstructing3Dgeometrywith
2Dsupervision[
30
],[
41
],[
34
],[
43
],[
10
],[
8
],[
6
],[
35
]
Fig.2.IllustrationofDGGN(deepgeometry-awaregraspingnetwork).OurDGGNhasashapegenerationnetworkandanoutcomepredictionnetwork.
Theshapegenerationnetworkhasa2DCNNencoder,3DCNNdecoder,andaglobalsamplinglayer(detailedinSec.III-B).Ouroutcomeprediction
networkhasa2DCNNencoder,alocalsamplinglayer(detailedinSec.III-D),andafully-connectedpredictionnetwork.
suggestthat(1)thequalityofreconstructed3Dgeometry
isasgoodaspreviousworkwith3Dsupervision;(2)the
learnedrepresentationgeneralizesbettertonovelsettings
thanpreviousworkwith3Dsupervision;and(3)learning
becomesmoreefwith2Dsupervision.Inspiredby
thesewetacklethe3Dreconstructioninaweakly
supervisedmannerwithoutexplicit3Dshapesupervision.
In[
41
],anin-networkprojectionlayerisintroducedfor
3Dshapelearningfrom2Dmasks(e.g.2Dsilhouetteof
object).Unfortunately,2Dsilhouetteisusuallyinsufcient
supervisionsignaltoreconstructobjectswithconcave3D
parts(e.g.,containers).Forthesereasons,wechosetousea
depthsignalinourshapereconstruction.Additionally,RGBD
sensorsarecommonlyavailableinmostrobotplatforms.
Toenabledepthsupervisioninourshapegenerationcom-
ponent,weproposeanovelin-networkOpenGLprojection
operatorthatutilizesa2Ddepthmap
D
assupervisionsignal
forlearningtoreconstructthe3Dgeometry.Weformulatethe
projectionoperationby
f
D
:
V

P
!D
thattransformsa
3Dshapeintoa2Ddepthmapwiththecameratransformation
matrix
P
.Here,thecameratransformationmatrixdecomposes
as
P
=
K
[
R
;
t
]
,where
K
isthecameraintrinsicmatrix,
R
isthecamerarotationmatrix,and
t
isthecameratranslation
vector.Inourimplementation,wealsousea2Dsilhouetteas
anobjectmask
M
forlearning.Empirically,thisadditional
objectivemakesthelearningstableandef
FollowingtheOpenGLcameratransformationstandard,
foreachpoint
p
s
=(
x
s
;y
s
;z
s
;
1)
in3Dworldframe,we
computethecorrespondingpoint
p
n
=(
x
n
;y
n
;z
n
;
1)
inthe
normalizeddevicecoordinatesystem(

1

x
n
;y
n
;z
n

1
)
usingthetransformation:
p
n
˘
Pp
s
.Here,theconversion
fromdepthbuffer
z
n
torealdepth
z
e
isgivenby
z
e
=
f
e
(
z
n
)=

1
=
(


z
n
+

)
where

=
Z
near

Z
far
2
Z
near
Z
far
and

=
Z
near
+
Z
far
2
Z
near
Z
far
.Here,
Z
far
and
Z
near
representsthefar
andnearclippingplanesofthecamera.
Similartotheﬁtransformernetworksﬂproposedin[
41
],
[
13
],ourdepthprojectioncanbeseenas:(1)performingdense
samplingfrominputvolume(inthe3Dworldframe)tooutput
volume(innormalizeddevicecoordinates);and(2)
the3Dspatialoutputacrossonedimension.Again,
j
-thpoint
(
x
n
j
;y
n
j
;z
n
j
)
inoutputvolume
U
2
R
H
0

W
0

D
0
(
j
-thpointis
indexedby
[
n
0
;m
0
;l
0
]
inthevolumespace)andcorresponding
point
(
x
s
j
;y
s
j
;z
s
j
)
ininputvolume
V
2
R
H

W

D
are
relatedbythetransformationmatrix
P
.Here,
(
W;H;D
)
and
(
W
0
;H
0
;D
0
)
arethewidth,height,anddepthofthe
inputandoutputvolume,respectively.Wethedense
samplingstepandchannel-wisestepasfollows:
U
[
n
0
;m
0
;l
0
]=
H
X
n
=1
W
X
m
=1
D
X
l
=1
V
[
n;m;l
]max(0
;
1
j
x
s
j

m
j
)
max(0
;
1
j
y
s
j

n
j
)max(0
;
1
j
z
s
j

l
j
)
^
M
[
n
0
;m
0
]=max
l
0
U
[
n
0
;m
0
;l
0
]
^
D
[
n
0
;m
0
]=
8
>
>
>
<
>
>
>
:
Z
far
;
if
^
M
[
n
0
;m
0
]=0
f
e
(
2
l
0
D
0

1)
;
where
l
0
=argmin
l
0
(
U
[
n
0
;m
0
;l
0
]
>
0
:
5)
Z
near
;
otherwise
(1)
Inourimplementation,wepre-computedtheactualdepth
f
e
(
2
l
0
D
0

1)
giventhedifthat
argmin
isnotback-
propagatable.Aswewillseeinthefollowingsection,the
networkwillbetrainedtomatchthesepredictions
^
M
and
^
D
totheground-truth
M
and
D
.Pleasenotethatourin-
networkprojectionlayeris
learning-free
asitimplements
theexactray-tracingalgorithmwithoutextrafreeparameters
involved.Wenotethattheconceptofdepthprojectionisalso
exploredinsomeveryrecentwork[
38
],[
33
],[
43
],buttheir
implementationsarenotexactlythesameasourOpenGL
projectionlayerinEq.1.
C.Viewpoint-invariantgeometry-awarerepresentationwith
multi-viewsupervision
Learningtoreconstruct3Dgeometryfromsingle-view
RGBDsensoryinputisachallengingtaskincomputervision
duetoshapeambiguity.Weadopttheshapeconsistency
learningthatenforcesviewpoint-invarianceacrossmulti-view
observations[
2
],[
41
],[
34
].More,we(1)usethe
averagedidentityunitsfrommultipleviewpointsasinputto
shapedecodernetworkand(2)providemultipleprojections
forsupervisingthe3Dshapereconstructionduringtraining.
Suchshapeconsistencylearningencouragesanimagetaken
fromoneviewpointsharingthesamerepresentationwith
theimagetakenfromanotherviewpoint.Attestingtime,
weonlyprovideRGBDinputfromsingleviewpoint.Given
aseriesof
n
observations
I
1
;
I
2
;

;
I
n
ofthescene,the
3Dreconstructioncanbeformulatedas
f
V
:
fI
i
g
n
i
=1
!
V
.Similarly,theprojectionoperatorfrom
i
-thviewpointis
f
D
:
V

P
i
!D
i
,where
D
i
and
P
i
arethedepthand
cameratransformationmatrixfromcorrespondingviewpoint,
respectively.Finally,wetheshapereconstructionloss
L
shape
inEq.2.
L
shape

=

D
n
X
i
=1
L
depth

(
^
D
i
;
D
i
)+

M
n
X
i
=1
L
mask

(
^
M
i
;
M
i
)
(2)
Here,

D
and

M
aretheconstantcoefcientsforthedepth
andmaskpredictionterms,respectively.
D.Learningpredictivegraspinginteractionwithgeometry-
awarerepresentation.
Asdemonstratedinpreviouswork[
26
],[
7
],[
5
],[
42
],[
28
]
thatlearninteractionsfromdemonstrations,
prediction
ofthe
futurestatecanbeametricforunderstandingthephysical
interaction.Inourgraspingsetting,wetheRGBD
input
I
ascurrentstate,the6Dpre-graspingparameters
a
(positionandorientationoftheparalleljawgripper)
asaction,andthegraspingoutcome
l
(e.g.,binarylabel
representingasuccessfulgraspornot)asfuturestate.The
futurepredictiontaskcanbesolvedbylearningafunctional
mapping
f
l
baseline
:
I
a
!
l
.Werefertothismethod
asabaselinegraspinginteractionpredictionmodel,which
hasbeenabasisofseveralrecentstate-of-the-artgrasping
methodsusingdeeplearning(e.g.,[
17
],[
19
],[
21
]).These
workmanagedtolearnsuchmappingwitheither(a)millions
ofrandomlygeneratedgrasps,(b)additionalviewfrom
eye/handperspective,or(c)additionalprocessingstepssuch
asobjectdetectionandimagealignment.
Incomparison,ourgeometry-awaremodelisanend-
to-endarchitecturewhichconstrainsitspredictionwith
geometryinformation.Aswelearntoreconstructthe3D
geometry,wearguethatthe
localsurfaceview
(typically
fromawristcameraperspective)canbe
directlyinferred
fromourviewpoint-invariantgeometry-awarerepresentation
^
D
local
=
f
D
(
^
V
;
P
(
a
))
,where
^
V
=
f
V
(
I
)
.Here,wetreat
thegripperasavirtualcamerawiththetransformationmatrix
P
(
a
)
withitsworld-spacecoordinatesgivenbythe6Dpre-
graspingparameters
a
.Inadditiontothe
localview
,our
geometry-awarerepresentationprovidesa
globalview
ofthe
scene
V
thattakesashapeprior,location,andorientationof
objectintoconsideration.Finally,givenacurrentobservation
I
,proposedaction
a
,andinferred3Dshaperepresentation
V
,
weafunctionalmapping
f
l
geometry

aware
:
I
a

V
!
l
,
where
l
isthebinaryoutcome.
E.DGGN:Deepgeometry-awaregraspingnetwork.
Toimplementthetwocomponentsproposedintheprevi-
oussections,weintroduce
DGGN(deepgeometry-aware
graspingnetwork)
(seeFigure2),composedofashape
generationnetworkandanoutcomepredictionnetwork.The
shapegenerationnetworkhasa2Dconvolutionalshape
encoderanda3Ddeconvolutionalshapedecoderfollowedby
aglobalprojectionlayer.Ourshapeencodernetworktakes
RGBDimagesofresolution128

128andcorresponding
4-by-4cameraviewmatricesasinput;thenetworkoutputs
identityunitsasanintermediaterepresentation.Ourshape
decoderisa3Ddeconvolutionalneuralnetworkthatoutputs
voxelsataresolutionof32

32

32.Weimplemented
theprojectionlayer(givencameraviewandprojection
matrices)thattransformsthevoxelsbackintoforeground
objectsilhouettesanddepthmapsataninputresolution
(128

128).Here,thepurposeofgenerativepre-training
istolearnviewpointinvariantunits(e.g.,objectidentity
units)throughobjectsegmentationanddepthprediction.The
outcomepredictionnetworkhasa2Dconvolutionalstate
encoderandafullyconnectedoutcomepredictorwithan
additionallocalshapeprojectionlayer.Ourstateencoder
takesRGBDinput(thepre-graspscene)ofresolution128

128andcorrespondingactions(positionandorientationofthe
gripperend-effector)andoutputsstateunitsasintermediate
representation.Ouroutcomepredictortakesbothcurrentstate
(e.g.,thepre-graspsceneandgripperaction)andgeometry
features(e.g.,viewpoint-invariantglobalandlocalgeometry
fromthelocalprojectionlayer)intoconsideration.Notethat
thelocaldense-samplingtransformsthesurfaceareaaround
thegripperintoaforegroundsilhouetteandadepth
mapatresolution48

48.
IV.E
XPERIMENTS
Thissectiondescribesourdatacollectionandaugmentation
process,aswellasexperimentalevaluationongrasping
outcomepredictionandgraspingtrials.
A.Datasetcollection
a)HumandemonstrationsinVR:
Wecollectedgrasping
demonstrationsonsevencategoriesofobjects,whichinclude
atotalof101everydayobjects.Tocollectgraspingdemon-
strations,wesetuptheHTCVivesysteminVirtualReality
(VR)andassigntargetobjectsrandomlytoveright-handed
users(threemalesandtwofemales).Intotal,1597human
graspsaredemonstrated,withanaverageof15graspsper
object(withlowestandhighestnumberofgraspsat7and
39foraplateandawineglass,respectively).Werandomly
split101objectsintothreesets(e.g.,training,validationand
testing)andmakesureeachsetcoversthesevencategories
(70%fortraining,10%forvalidationand20%fortesting).
Fig.3.IllustrationsofourVR-Grasping-101dataset.
b)Dataaugmentation:
Inordertocollectsufcient
graspingdemonstrationsformodeltrainingandevaluation,
wegeneratesyntheticgraspsbyperturbingthehuman
demonstrationsusingPyBullet[
3
].Thishelps
inincreasingthenumberofgraspsbyaddingperturbations
tothedemonstrations.Intotal,wecollected150Kgrasping
demonstrationscovering101objects.Figure3illustrates
examplesofobjectsinthedataset,successfulandunsuccessful
graspingtrialsfromhumandemonstrations,andsynthetic
grasps(visualizedbygripperpositions)forsuccessfuland
unsuccessfultrialsthatweregeneratedbythisaugmentation
process.MoredetailsaredescribedintheAppendix.
Foreachdemonstration,wetakeasnapshotofthepre-
graspingscene(e.g.,beforeclosingthetwogripper
byrandomlysettingthecameraatadistance(rangingbetween
35centimetresand45centimetres).Wedrawacameratarget
positionfromanormaldistributionwithitsmeanastheobject
centerandadesiredvariance(inourexperiment,weuse3
centimetresasstandarddeviation).Furthermore,wesetupthe
cameraaroundthetargetpositionfrom8differentazimuth
angles(withstepsof45degrees)andadjusttheelevation
from4differentangles(e.g.,15,30,45,and60degrees).
Finally,wesaveastateofthescenewithoutagripper,which
isusedforshapepre-training;thiswillbereferredtoas
thestaticscenethroughoutthepaper.Weincludeonlytwo
elevationangles(e.g.,15and45degrees)inthetrainingset
whileleavingtherestforevaluation.
B.Implementationdetails
a)DeepCNNbaseline.:
Weadoptthecurrentdata-
drivenframeworkasourgraspingbaselinebyremovingthe
shapeencoderandshapedecoderfromourdeepgeometry-
awaregraspingmodel.Thisbaselinecanbeinterpretedas
thegraspingqualityCNN[
21
]withoutanadditionalview
fromatop-downcamera.Wetrainedthemodelusingthe
ADAMoptimizerwithalearningrateof
10

5
for200K
iterationsandamini-batchofsizeof4.Asanablationstudy,
weaddedviewandstaticsceneasanadditionalinputchannel
ontopofthebaselinemodelbutdidn'tobserve
improvements.
b)TrainingDGGN.:
Weadoptedatwo-stagetraining
procedure:First,wepre-trainedtheshapegenerationmodel
(shapeencoderandshapedecoder)usingtheADAMoptimizer
withalearningrateof
10

5
for400Kiterationsandamini-
batchofsizeof4.Ineachbatch,wesample4random
viewpointsforthepurposeofmulti-viewsupervisioninthe
trainingtime.Weobservedthatthissettingledtoamore
stableshapegenerationperformancecomparedtosingle-
viewtraining.Inaddition,weused
L
1
lossforforeground
depthpredictionand
L
2
lossforsilhouettepredictionwith
coef

D
=0
:
5
and

M
=10
:
0
.Inthesecondstage,
wethestateencoderandoutcomepredictorusing
theADAMoptimizerwithalearningrateof
3

10

6
for
200Kiterationsandamini-batchofsizeof4.Weusedcross-
entropyasourobjectivefunctionsincethegraspingprediction
isformulatedasabinarytask.
Inourexperiments,allthemodelsaretrainedusing20
GPUworkersand32parameterserverswithasynchronized
updates.Bothbaselineandourgeometry-awaremodeladopt
convolutionalencoder-decoderarchitecturewithresidual
connections.Thebottlenecklayer(e.g.,theidentityunitin
thegeometry-awaremodel)isa
768
dimensionalvector.
C.Visualization:3Dshapegeneration
Weevaluatethequalityoftheshapegenerationmodelby
visualizingthegeometryrepresentationsthroughtheshape
encoderanddecodernetwork.Inourevaluations,weused
single-viewRGBDinputandcorrespondingcameraview
matrixasinputtothenetwork.AsshowninFigure4(a),
ourshapegenerationmodelisabletogenerateadetailed
3Doccupancygridfromsingle-viewinputwithout3D
supervisionduringtraining.AsshowninFigure4(b),our
modeldemonstratesreasonablegeneralizationqualityeven
onnovelobjectinstances.
a)Analysis:localgeometryinferenceviaprojection.:
Oneadvantageofourshapegenerationcomponentisthat
wecanobtainadditionallocalgeometryinformation(see
thered-dashedboxinFigure2(c))fromourgeometry-aware
representation.Thisisthekeydifferencebetweenourwork
andtherelatedworkthatrequireadditionalcamerafrom
thegripper.With3Dgeometryaspartoftheintermediate
representation,wehallucinatethelocalgeometrybyrunninga
projectionfromthegripper'sperspective(i.e.,simplytreatthe
gripperasanothervirtualcamera).Tofurtherunderstandthe
advantagesofourshapegenerationcomponent,wevisualized
theintermediatelocalgeometryprojectedfromgenerated3D
occupancygrid.AsshowninFigure4(c),ourshapegeneration
componentprovidesaccuratelocalgeometryestimationthat
isusefulforgraspingoutcomeprediction.
D.Modelevaluation:Graspingoutcomeprediction
Toevaluatetheactualadvantagesingraspingoutcome
predictionfromourmodeling,wecomputedtheaverage
accuracyover30Kdemonstrationsfromnovel
objectinstances(fromtestingset)withdiverseobservation
viewpoints.Foreachhumandemonstration,wegenerated
100syntheticgraspsthroughperturbation(amongwhich
Fig.4.Visualization:3Dshapegenerationfromsingle-viewRGBD.(a)Theperformanceontraining(seen)objects.(b)Theperformanceontesting
(novel)objects.(c)Localgeometryinferencefromgeneratedoccupancygrid.
Method
=
Category
bottle
bowl
cup
plate
mug
sugarbowl
teapot
all
baselineCNN(15)
72.81
73.36
73.26
66.92
72.23
70.45
66.13
71.42
ourDGGN(15)
78.83
79.32
77.60
68.88
78.25
76.09
73.69
76.55
baselineCNN(45)
71.02
74.16
73.50
63.31
74.23
72.70
64.19
71.32
ourDGGN(45)
78.77
80.63
78.06
70.13
79.29
77.52
72.88
77.25
TABLEI
G
RASPING
O
UTCOMEPREDICTIONACCURACYFROMSEENELEVATIONANGLES
.
Method
=
Category
bottle
bowl
cup
plate
mug
sugarbowl
teapot
all
baselineCNN(30)
71.15
72.98
71.65
61.90
71.01
70.06
61.88
69.50
DGGN(30)
79.17
77.71
77.23
67.00
75.95
75.06
70.66
75.27
baselineCNN(60)
68.45
73.05
72.50
61.27
74.40
71.30
63.25
70.18
DGGN(60)
77.40
78.52
76.24
68.13
79.39
76.15
70.34
75.76
TABLEII
G
RASPING
O
UTCOMEPREDICTIONACCURACYFROMNOVELELEVATIONANGLES
.
50%ofthemaresuccessgrasps)andcomputedtheaverage
accuracyon100grasps(i.e.,randomguessachieves50%
accuracy).Toinvestigatethemodelperformancedueto
viewpointchanges,werepeattheevaluationexperimentfor
fourdifferentelevationangles(e.g,15,30,45,and60degrees).
Weuseparallelcomputingresources(
500
machines)during
evaluationandtheentireevaluationtookabout
1
day.The
resultsaresummarizedinTableIandTableII.Overall,the
deepgeometry-awaremodelconsistentlyoutperformsthe
deepCNNbaselineingraspingoutcomeAs
wecansee,ﬁteapotﬂandﬁplateﬂarecomparativelymore
challengingcategoriesforoutcomeprediction,sinceﬁteapotﬂ
hasirregularshapeparts(e.g.,tipandhandle)andﬁplateﬂhas
afairlyshape.Whenitcomestonovelelevationangles
(e.g.,compareTableIandTableII),ourdeepgeometry-aware
modelislessaffected,especiallyincategoriessuchasﬁteapotﬂ
andﬁplateﬂwhereviewpoint-invariantshapeunderstanding
iscrucial.
E.Application:Analysis-by-synthesisgraspingplanning.
Asweimprovetheaccuracyoverthegrasping
outcome,anaturalquestioniswhetherthisimprovementcan
beusedtoguidebettergraspingplanning.Givenagrasping
proposalastargetgripperpose)seed,weconducted
graspingplanningbysequentiallyadjustingthegraspingpose
guidedbyourdeepgraspingnetworkuntilagraspsuccess.In
eachoptimizationstep,weperformedcross-entropymethod
(CEM)[
31
],[
19
]asfollows.(1)Weinitializedwithafailure
graspinordertoforcethemodeltobettergrasping
pose.(2)Toobtainthegradientdirectioninthe6Dspace,
wesample10randomdirectionsandselectedthetopone
basedonthescorereturnedbytheneuralnetwork(output
ofoutcomepredictor).Werepeattheiterationsuntilsuccess
(wesetanupperboundof20steps).Weconductedthesame
graspingexploreevaluationforboththebaselineCNNand
ourdeepgeometry-awaremodel.Toaccountforthevariations
inobservationviewpointsandinitialseeds,werepeatthe
evaluationforeighttimespertestingdemonstrationinour
Method
=
Category
bottle
bowl
cup
plate
mug
sugarbowl
teapot
all
baselineCNN+CEM
48.60
64.28
55.44
45.99
61.00
53.97
63.08
55.85
ourDGGN+CEM
56.73
68.84
60.31
50.09
67.21
59.87
69.22
61.46
rel.improvement(%)
16.72
7.09
8.77
8.92
10.18
10.92
9.73
10.03
TABLEIII
G
RASPINGPLANNINGONNOVELOBJECTS
:
SUCCESSRATEBYOPTIMIZINGFORUPTO
20
STEPS
.
Fig.5.Visualization:graspingoptimizationwithCEMbasedonthe
graspingpredictionoutput.Ineachrow,weselectedthreerepresentative
stepsingraspingoptimization(insequentialorderfromlefttoright).Red
boxrepresentsafailuregraspwhilegreenboxrepresentsasuccessfulgrasp.
datasetandreportedtheaveragesuccessrateafter20iterations
(markedasfailureonlyifthereisnosuccessin20steps).As
showninTableIII,CEMguidedourgeometry-awaremodel
performanceconsistentlybetterthanthebaselineCNNmodel.
Webelievetheimprovedperformancecomesfromtheexplicit
modelingofthe3Dgeometryasintermediaterepresentation
inourdeepgeometry-awaremodel.Ourmodelachieved
themostimprovementintheﬁbottleﬂcategory,
sinceabottleshapeisrelativelyeasytoreconstruct.Our
improvementintheﬁbowlﬂcategoryislesspartly
duetothedifcultyofpredictingitsconcaveshapeinnovel
objectinstances.Figure5demonstratesexamplegrasping
planningtrajectoriesondifferentobjects.ThebaselineCNN
islessrobustcomparedtoourdeepgeometry-awaremodel,
whichismorelikelytotransitfromonesideoftheobjectto
theothersidewithaclearnotionof3Dgeometry.
V.C
ONCLUSIONSAND
F
UTURE
W
ORK
Inthiswork,westudiedtheproblemoflearningthe
graspinginteractionwithdeepgeometry-awarerepresentation.
Weproposedadeepgeometry-awarenetworkthatperforms
shapegenerationaswellasgraspingoutcomepredictionwith
alearning-freephysicalprojectionlayer.Comparedtothe
CNNbaseline,experimentalresultsdemonstratedimproved
performanceinoutcomepredictionthankstogenerativeshape
modeling.Guidedbythegeometry-awarerepresentation,we
obtainedbetterplanningviaanalysis-by-synthesisgrasping
optimization.
Webelievetheproposeddeepgeometry-awaregrasping
frameworkhasmanypotentialsinadvancingrobotlearningin
general.Oneinterestingfuturedirectionistoapplythelearned
geometry-awarerepresentationtoperformtasksusingother
typesofhands(e.g.,handswithverydifferentkinematics).
Inaddition,wewouldliketoexploresomealternativemodel
designs(e.g.,learntograspwithouttheauxiliarystate
encoder)suchthatthelearnedgeometry-awarerepresentation
mightbeeasilyadaptedtootherdomains(e.g.,realrobot
setup).
A
PPENDIX
:D
ETAILSOF
S
YNTHETICDATAGENERATION
WetakeadvantageofthePybulletsimulator[
3
]by
switchingbetweentwomodes:simulationandlogplayback.
Inessence,weusethefollowingprotocolstogenerate
additionalgrasps:

StartthedemonstrationlogplaybackintheBullet
physicsengine.

Pausethelogplaybackonceagraspisdetected(pre-
graspstate).

Storethecurrentscenestate(positionandorientation
ofallobjects).

Repeatrandomgraspingexploration100times.(1)Draw
anewgraspposefromanormaldistributionwithits
meanbeingthevalueofthedemonstratedgrasppose
andadesiredvariance(inourexperimentweuse5
centimetresasstandarddeviationforpositionand20
Eulerdegreesasstandarddeviationfororientation).(2)
Switchtosimulationmode,openthegripper,place
itatthenewdrawnrandompose,closethegripper,and
liftit.(3)Checkwhethertheobjectisstillbetween
gripperBasedontheoutcome,weaddanew
posetothelistofsuccessfulorfailedgrasps(see
Figure3(b)(c)).(4)Resetthesimulationenvironmentto
thepreviouslystoredstate.

Resumelogplaybackuntilnextgraspisdetected.
Withthisprotocol,wecollectedatotalof150Kgrasping
syntheticgraspsbasedonhumandemonstrations.Asshown
inFigure3(d),wevisualizethegripperpositionsusing
coloreddots(weomitthegripperorientations):greenones
representingsuccessfulgraspsandredonesrepresenting
failuregrasps.
R
EFERENCES
[1]
J.BohgandD.Kragic.Learninggraspingpointswithshapecontext.
RoboticsandAutonomousSystems
,58(4):362Œ377,2010.
[2]
C.B.Choy,D.Xu,J.Gwak,K.Chen,andS.Savarese.3d-r2n2:A
approachforsingleandmulti-view3dobjectreconstruction.In
EuropeanConferenceonComputerVision
,pages628Œ644.Springer,
2016.
[3]
E.Coumans,Y.Bai,andJ.Hsu.Pybulletphysicsengine.
http:
//pybullet.org
.
[4]
H.DangandP.K.Allen.Semanticgrasping:planningtask-speci
stableroboticgrasps.
AutonomousRobots
,37(3):301Œ316,2014.
[5]
A.DosovitskiyandV.Koltun.Learningtoactbypredictingthefuture.
arxivpreprint:1611.01779
,2016.
[6]
H.Fan,H.Su,andL.Guibas.Apointsetgenerationnetworkfor3d
objectreconstructionfromasingleimage.In
CVPR
,2017.
[7]
C.Finn,I.Goodfellow,andS.Levine.Unsupervisedlearningfor
physicalinteractionthroughvideoprediction.In
AdvancesinNeural
InformationProcessingSystems
,pages64Œ72,2016.
[8]
M.Gadelha,S.Maji,andR.Wang.3dshapeinductionfrom2dviews
ofmultipleobjects.
arXivpreprintarXiv:1612.05872
,2016.
[9]
R.Girdhar,D.F.Fouhey,M.Rodriguez,andA.Gupta.Learning
apredictableandgenerativevectorrepresentationforobjects.In
EuropeanConferenceonComputerVision
,pages484Œ499.Springer,
2016.
[10]
C.Godard,O.MacAodha,andG.J.Brostow.Unsupervisedmonocular
depthestimationwithleft-rightconsistency.In
CVPR
,2016.
[11]
C.Goldfeder,M.Ciocarlie,H.Dang,andP.K.Allen.Thecolumbia
graspdatabase.In
RoboticsandAutomation,2009.ICRA'09.IEEE
InternationalConferenceon
,pages1710Œ1716.IEEE,2009.
[12]
M.Gualtieri,A.tenPas,K.Saenko,andR.Platt.Highprecision
graspposedetectionindenseclutter.In
IntelligentRobotsandSystems
(IROS),2016IEEE/RSJInternationalConferenceon
,pages598Œ605.
IEEE,2016.
[13]
M.Jaderberg,K.Simonyan,A.Zisserman,etal.Spatialtransformer
networks.In
AdvancesinNeuralInformationProcessingSystems
,
pages2017Œ2025,2015.
[14]
E.Johns,S.Leutenegger,andA.J.Davison.Deeplearningagrasp
functionforgraspingundergripperposeuncertainty.In
Intelligent
RobotsandSystems(IROS),2016IEEE/RSJInternationalConference
on
,pages4461Œ4468.IEEE,2016.
[15]
D.Katz,A.Venkatraman,M.Kazemi,J.A.Bagnell,andA.Stentz.
Perceiving,learning,andexploitingobjectaffordancesforautonomous
pilemanipulation.
AutonomousRobots
,37(4):369Œ382,2014.
[16]
M.Kopicki,R.Detry,M.Adjigble,R.Stolkin,A.Leonardis,and
J.L.Wyatt.One-shotlearningandgenerationofdexterousgrasps
fornovelobjects.
TheInternationalJournalofRoboticsResearch
,
35(8):959Œ976,2016.
[17]
I.Lenz,H.Lee,andA.Saxena.Deeplearningfordetectingrobotic
grasps.
TheInternationalJournalofRoboticsResearch
,34(4-5):705Œ
724,2015.
[18]
B.Le
´
on,S.Ulbrich,R.Diankov,G.Puche,M.Przybylski,A.Morales,
T.Asfour,S.Moisio,J.Bohg,J.Kuffner,etal.Opengrasp:Atoolkit
forrobotgraspingsimulation.
[19]
S.Levine,P.Pastor,A.Krizhevsky,J.Ibarz,andD.Quillen.Learning
hand-eyecoordinationforroboticgraspingwithdeeplearningand
large-scaledatacollection.
TheInternationalJournalofRobotics
Research
,page0278364917710318.
[20]
M.Li,K.Hang,D.Kragic,andA.Billard.Dexterousgraspingunder
shapeuncertainty.
RoboticsandAutonomousSystems
,75:352Œ364,
2016.
[21]
J.Mahler,J.Liang,S.Niyaz,M.Laskey,R.Doan,X.Liu,J.A.Ojea,
andK.Goldberg.Dex-net2.0:Deeplearningtoplanrobustgrasps
withsyntheticpointcloudsandanalyticgraspmetrics.
arxivpreprint:
1703.09312
,2017.
[22]
J.Mahler,F.T.Pokorny,B.Hou,M.Roderick,M.Laskey,M.Aubry,
K.Kohlhoff,T.Kr
¨
oger,J.Kuffner,andK.Goldberg.Dex-net
1.0:Acloud-basednetworkof3dobjectsforrobustgraspplanning
usingamulti-armedbanditmodelwithcorrelatedrewards.In
IEEE
InternationalConferenceonRoboticsandAutomation(ICRA)
,pages
1957Œ1964.IEEE,2016.
[23]
D.MaturanaandS.Scherer.Voxnet:A3dconvolutionalneuralnetwork
forreal-timeobjectrecognition.In
IntelligentRobotsandSystems
(IROS),2015IEEE/RSJInternationalConferenceon
,pages922Œ928.
IEEE,2015.
[24]
L.MontesanoandM.Lopes.Activelearningofvisualdescriptorsfor
graspingusingnon-parametricsmoothedbetadistributions.
Robotics
andAutonomousSystems
,60(3):452Œ462,2012.
[25]
E.NikandrovaandV.Kyrki.Category-basedtaskgrasping.
RoboticsandAutonomousSystems
,70:25Œ35,2015.
[26]
J.Oh,X.Guo,H.Lee,R.L.Lewis,andS.Singh.Action-conditional
videopredictionusingdeepnetworksinatarigames.In
Advancesin
NeuralInformationProcessingSystems
,pages2863Œ2871,2015.
[27]
T.Osa,J.Peters,andG.Neumann.Experimentswithhierarchical
reinforcementlearningofmultiplegraspingpolicies.In
International
SymposiumonExperimentalRobotics
,pages160Œ172.Springer,2016.
[28]
L.Pinto,D.Gandhi,Y.Han,Y.-L.Park,andA.Gupta.Thecurious
robot:Learningvisualrepresentationsviaphysicalinteractions.In
EuropeanConferenceonComputerVision
,pages3Œ18.Springer,2016.
[29]
L.PintoandA.Gupta.Supersizingself-supervision:Learningtograsp
from50ktriesand700robothours.In
RoboticsandAutomation
(ICRA),2016IEEEInternationalConferenceon
,pages3406Œ3413.
IEEE,2016.
[30]
D.J.Rezende,S.A.Eslami,S.Mohamed,P.Battaglia,M.Jaderberg,
andN.Heess.Unsupervisedlearningof3dstructurefromimages.In
AdvancesInNeuralInformationProcessingSystems
,pages4997Œ5005,
2016.
[31]
R.RubinsteinandD.Kroese.Thecross-entropymethod:A
approachtocombinatorialoptimization,monte-carlosimulation,and
machinelearning.2004.
[32]
A.Saxena,J.Driemeyer,andA.Y.Ng.Roboticgraspingofnovel
objectsusingvision.
TheInternationalJournalofRoboticsResearch
,
27(2):157Œ173,2008.
[33]
A.Tewari,M.Zollh
¨
ofer,H.Kim,P.Garrido,F.Bernard,P.Perez,and
C.Theobalt.Mofa:Model-baseddeepconvolutionalfaceautoencoder
forunsupervisedmonocularreconstruction.In
TheIEEEInternational
ConferenceonComputerVision(ICCV)
,volume2,2017.
[34]
S.Tulsiani,T.Zhou,A.A.Efros,andJ.Malik.Multi-viewsupervision
forsingle-viewreconstructionviadifferentiablerayconsistency.In
CVPR
,2017.
[35]
H.-Y.Tung,H.-W.Tung,E.Yumer,andK.Fragkiadaki.Self-supervised
learningofmotioncapture.In
AdvancesinNeuralInformation
ProcessingSystems
,pages5242Œ5252,2017.
[36]
N.Vahrenkamp,L.Westkamp,N.Yamanobe,E.E.Aksoy,and
T.Asfour.Part-basedgraspplanningforfamiliarobjects.In
Humanoid
Robots(Humanoids),2016IEEE-RAS16thInternationalConference
on
,pages919Œ925.IEEE,2016.
[37]
J.Varley,C.DeChant,A.Richardson,A.Nair,J.Ruales,andP.Allen.
Shapecompletionenabledroboticgrasping.
arxivpreprint:1609.08546
,
2016.
[38]
J.Wu,Y.Wang,T.Xue,X.Sun,B.Freeman,andJ.Tenenbaum.
Marrnet:3dshapereconstructionvia2.5dsketches.In
AdvancesIn
NeuralInformationProcessingSystems
,pages540Œ550,2017.
[39]
J.Wu,C.Zhang,T.Xue,B.Freeman,andJ.Tenenbaum.Learninga
probabilisticlatentspaceofobjectshapesvia3dgenerative-adversarial
modeling.In
AdvancesinNeuralInformationProcessingSystems
,
pages82Œ90,2016.
[40]
Z.Wu,S.Song,A.Khosla,F.Yu,L.Zhang,X.Tang,andJ.Xiao.3d
shapenets:Adeeprepresentationforvolumetricshapes.In
Proceedings
oftheIEEEConferenceonComputerVisionandPatternRecognition
,
pages1912Œ1920,2015.
[41]
X.Yan,J.Yang,E.Yumer,Y.Guo,andH.Lee.Perspective
transformernets:Learningsingle-view3dobjectreconstructionwithout
3dsupervision.In
AdvancesinNeuralInformationProcessingSystems
,
pages1696Œ1704,2016.
[42]
J.Yang,S.E.Reed,M.-H.Yang,andH.Lee.Weakly-supervised
disentanglingwithrecurrenttransformationsfor3dviewsynthesis.In
AdvancesinNeuralInformationProcessingSystems
,pages1099Œ1107,
2015.
[43]
T.Zhou,M.Brown,N.Snavely,andD.G.Lowe.Unsupervised
learningofdepthandego-motionfromvideo.In
CVPR
,2017.
"
74,Image classification and retrieval with random depthwise signed convolutional neural networks,http://arxiv.org/pdf/1806.05789v3.pdf,https://github.com/xyzacademic/RandomDepthwiseCNN,"Imageandretrievalwithrandomdepthwisesigned
convolutionalneuralnetworks
YunzheXueandUsmanRoshan
DepartmentofComputerScience,NewJerseyInstituteofTechnology,NewarkNJ07090,USA,
yx277@njit.edu,usman@njit.edu
Abstract.
Weproposearandomconvolutionalneuralnetworktogenerateafeaturespaceinwhich
westudyimagenandretrievalperformance.Putweapplyrandomconvolutional
blocksfollowedbyglobalaveragepoolingtogenerateanewfeature,andwerepeatthis
k
times
toproducea
k
-dimensionalfeaturespace.Thiscanbeinterpretedaspartitioningthespaceof
imagepatcheswithrandomhyperplaneswhichweformalizeasarandomdepthwiseconvolutional
neuralnetwork.Inthenetwork'slayerweperformimageandretrievalwith
thelinearsupportvectormachineand
k
-nearestneighborandstudyotherempirical
properties.Weshowthattheratioofimagepixeldistributionsimilarityacrossclassestowithin
classesishigherinournetwork'slayercomparedtotheinputspace.Whenweapplythelinear
supportvectormachineforimageweseethattheaccuracyishigherthanifwewere
totrainjustthelayerofVGG16,ResNet18,andDenseNet40withrandomweights.Inthe
samesettingwecompareittoarecentunsupervisedfeaturelearningmethodandouraccuracy
tobecomparableonCIFAR10buthigheronCIFAR100andSTL10.Weseethattheaccuracyis
notfarbehindthatoftrainednetworks,particularlyinthetop-
k
setting.Forexamplethetop-2
accuracyofournetworkisnear90%onbothCIFAR10anda10-classminiImageNet,and85%
onSTL10.Wethat
k
-nearestneighborgivesacomparableprecisionontheCorelPrinceton
ImageSimilarityBenchmarkthanifweweretousethelayeroftrainednetworks.Aswith
othernetworkswethatournetworkfailstoablackboxattackeventhoughwelackagradient
andusethesignactivation.Wehighlightsensitivityofournetworktobackgroundasapotential
pitfallandanadvantage.Overallourworkpushestheboundaryofwhatcanbeachievedwith
randomweights.
1Introduction
Convolutionalneuralnetworks(CNNs)arethestateoftheartinimagerecognitionbenchmarkstoday
[1].Optimizationmethodssuchasstochasticgradientdescent[2]combinedwithdataaugmentation[3],
regularization[4],dropoutandcutout[5,6]havemadeCNNsthede-factoapproachforaccurateimage
recognition[7].Interestingly,severalofthesemethodsinvolverandomness.
Forexamplethedropoutmethod[5]ignoresarandomsetofnodesduringtraining.Thecutoutmethod
[6]masksrandomsquarepatchesintheinputtrainingimages.Stochasticgradientdescentrandomlyuses
asingletrainingexampleatatime(orminibatches)toobtainthegradientasopposedtocomputing
itfromtheentiredataset.Thismethodhasbeenshowntoconvergetotheglobaloptimumforconvex
functionsasweincreasethenumberofiterations[8].Allofthesemethodsuserandomnesstoavoid
ovduringtrainingandthusgivebettermodelgeneralization.
Randomweightshavebeenexploredpreviouslyinseveralstudies[9,10,11,12]includinggenerating
imageswithrandomnets[13].Theyshowtheimportanceofanetworkarchitectureinachievinghigh
accuraciesandconnectunsupervisedpre-traininganddiscriminativetuningtoarchitecture.We
explorerandomnessfromtheperspectiveofrandomdepthwiseconvolutionalblocks.
Consideraseriesofconvolutionalblocksappliedrepeatedlytoanimage.Theimagerepresentation
inthelayeristhengloballyaveragepooledtoobtainasinglevalue.Ifwerepeatthis
k
timeswe
obtaina
k
dimensionalspace.Thiscanalsobeconsideredasamethodforunsupervisedfeaturelearning
sincewemakenouseoflabelsingeneratingthenewspace.Weshowbelowthatourmethodcanbe
interpretedasapplyingrandomhyperplanestoallpatchesofallinputimagesandaveragepoolingthe
imagerepresentations.Weformalizethisasarandomdepthwiseconvolutionalneuralnetworkand
studyvariousaspectsofimageandretrievalinournetwork'slayer.
Wepresentseveralexperimentalresultsonimageandretrievalinournetwork's
layer.Westartbyshowingthatimagesacrossandwithinclassesarebetterrepresentedthantheinput
arXiv:1806.05789v3  [cs.CV]  15 Mar 2019featurespace.Wethencomparetothetrainedlayerofotherrandomnetworksandanunsupervised
featurelearningmethod.Inbothcasesournetworkattainscomparableorhigheraccuracies.Compared
totrainednetworksourrandomweightsarenomatchbutarenotfarbehindinaccuracy,especiallyas
wegointotop-2andtop-3accuracy.
Interestinglyournetworkperformscompetitivelytotrainednetworkswhenitcomestotheproblem
ofimageretrieval,particularlyimageretrievalbysimilaritysuchastheCorel-PrincetonImageSimilarity
benchmark[14].Finallywehighlightsomelimitationssensitivityofournetwork'slayertobackground
colorswhichcanbeadisadvantagebutpossiblyalsoanadvantageinsomecases.Overallwepushthe
envelopeofrandomnetworksandshowthataccuraciesbetterthanexpectedcanbeachieved.
2Methods
2.1Motivationandintuitionbehinddepthwiserandomconvolutions
WeprovidemotivationandintuitionwithasimpletoyexampleinFigure1.Thereweseefourimages
I
0
;I
1
;I
2
,and
I
3
containingvariousobjects.Clearlyimages
I
0
and
I
1
areverysimilartoeachother.
Image
I
2
isalsosimilarsinceitcontainscommonobjectsas
I
0
and
I
1
buttheyareindrentpositions.
Image
I
3
sharesonlyonecommonobjecttotheotherimagesandthusisthemostdissimilar.Weseeka
representationthatwouldcapturethesesimilarities.
Supposewedivideeachimageintofourequalquadrants(orpatches)andconsiderfourrandom
hyperplanes
H
0
;H
1
;H
2
,and
H
3
inthespaceofallpatchesasshowninFigure1.Weplacepatches
containingthesameobjectnearbyinthesincetheyarelikelytobesimilarinpixelvalues.For
examplethecarinpartitions
I
02
;I
12
;I
23
,and
I
32
areclusteredinthelowerleftinthemiddlenear
theorigin.
(a)(b)(c)
Fig.1.
Shownin(a)arefourimageseachcontainingobjectsintpartsoftheimageanddividedintofour
partitions.In(b)weshowfourrandomhyperplanes(inred)ontheinputspaceoffeaturesfromallpatchesof
theimages.Theoutputsofthefourrandomhyperplanescorrespondtoaconvolutionalkernelthatconsidersjust
fourpartitions(part(c)).Weaveragepooltoobtainafeaturevalue.
Wedeterminethesignofeachpatchaccordingtoeachhyperplaneandobtaina2

2matrixforeach
image.Thisisexactlytheoutputofaconvolutionkernelthatconsidersjustfourpartitionsofanimage.
Wethenaveragepoolthematrixtoobtainasinglefeaturevaluefortheimagegivenbythehyperplane.
Byrepeatingthisformanyhyperplanesweobtainmorefeaturesperimage.Weseeinourtoyexample
thatthefeaturerepresentationforimages
I
0
;I
1
,and
I
2
aremoresimilarthanimage
I
3
.
Wedon'thaveatheoreticalguaranteethatapplyingrandomhyperplanestoimagepatchesrepeatedly
(aswedoinournetwork)wouldyieldalinearlyseparablespace.However,onecandrawintuitionfrom
ourtoyexampleinFigure1.Thereweseethatthefourrandomhyperplanespartitionthespaceand
thatpatchesinthesamespacewillhaveexactlythesameoutputsforallfourhyperplanes.Patchesin
adjacentpartitionsarelikelytobelesssimilarandwillhaveexactlyoneofthefourhyperplaneoutputs
tobet.
2
2.2Randomdepthwiseconvolutionalneuralnetworks
Beforeformalizingournotionintorandomdepthwisenetworkswereviewconvolutionalneural
networks.Convolutionalneuralnetworksaretypicallycomposedofalternatingconvolutionandpooling
layersfollowedbyalayer.Aconvolutionlayerisspbyakernelsizeandthenumber
ofkernelsinthelayer.,theconvolutionlayerperformsaamovingnon-linearizeddotproduct
againstpixelsgivenbyakernelsize
k

k
(usually3

3or5

5).Thedotproductisusually
non-linearizedwiththesigmoidorhinge(relu)functionsincebotharetiableandintothe
gradientdescentframework.Theoutputofapplyinga
k

k
convolutionagainsta
p

p
imageisan
imageofsize(
p

k
+1)

(
p

k
+1).
Considerapplyingrandomconvolutionalblocksrepeatedlyandthenaveragingallthevaluesinthe
representationoftheimage.Ifwerepeatthis
k
timesitgivesus
k
newfeatures.Thiscanbe
describedasarandomdepthwiseconvolutionalneuralnetwork(RDCNN).Eachconvolutionalblockin
ournetworkisaconvolutionalkernelfollowedby2

2averagepoolingwithstride2.
Ournetworkisparameterizedbythenumberofconvolutionalblocks
b
,thesizeofeachkernel
k

k
andthenumberofkernels
m
ineachlayer(thisisthesameineachlayer).InFigure2weshowanexample
ofournetworkwithtwolayers(
l
=2)ande3

3convolutionblocksineachlayer(
m
=5
;k
=3).
WesetthevaluesineachconvolutionalkernelrandomlyfromtheNormaldistributionwithmean0and
variance1.
Wenon-linearizetheoutputofeachconvolutionwiththesignfunctionandourconvolutionis
depth-
wise
.Thismeansthe
i
th
convolutionisappliedonthe
i
th
kernelonlyofthepreviouslayer.Intheinput
layer,however,theconvolutionisappliedintheconventionalwaytoaccountforRGBimagesthathave
threelayers.Afterwearedonewithconvolutionswegloballyaveragepoolthelayerwhichgivesus
afeaturespace.Wethenapplyalinearsupportvectormachineorstochasticgradientdescent
onthefeaturespace.
Fig.2.
Arandomdepthwiseconvolutionalneuralnetworkwithtwoconvolutionalblocks,kernelsizeof
k
,and
m
=5kernelsineachlayer
2.3Experimentalperformancestudy
Inordertoevaluatetheempiricalperformanceofourrandomnetworkwecompareittothreepopular
networkswithrandomandtrainedweightsandanunsupervisedfeaturelearningmethodonseveral
imagebenchmarks.
Deepnetworkscomparedinourstudy
Wecompareourmethodtomodernnetworksusedinimage
recognitiontoday.Theseareallconvolutionalneuralnetworksdesignedtoenabledeeperarchitectures
andaretrainedwithstochasticgradientdescent.WeimplementthesenetworksourselveswithPyTorch
andTensorFlowandmakeourimplementationsfreelyavailablefromthestudywebsite
https://github.
com/xyzacademic/RandomDepthwiseCNN
.
{
ResNet18[15]:Residualconvolutionalnetworkscontainconnectionsfrompreviouslayersandnot
justthelastone.
{
DensenNet40[16]:Convolutionalnetworkscontaindenselayersinbetweenconvolutions.
{
VGG16[17]:Deepconvolutionalneuralnetworkwithlayersofconvolutionandpooling.
3
Datasets
Wecollectseveralimagebenchmarksonwhichweevaluateourmethod.
{
MNIST[18]:Handwrittendigitrecognitionfrom10classesin32

32images,trainingsizeof60,000
andtestsizeof10,000
{
CIFAR10[19]:Objectrecognitionfrom10classesin32

32colorimages,trainingsizeof50,000,
testsizeof10,000
{
CIFAR100[19]:AsCIFAR10exceptfrom100classes
{
STL10[20]:Objectrecognitionfrom10classesin96

96colorimages,trainingsizeof5000,andtest
sizeof8000
{
Mini-ImageNet[21]:Werandomlyselect10classesfromthebenchmarkgivingatotalof12,730
trainingand500testcolorimageseachofsize256

256.Weprovidethesesetofimagesonthe
study'swebsiteat
https://github.com/xyzacademic/RandomDepthwiseCNN
.
{
CorelPrincetonImageSimilarity[14]:Eightqueryimagesandtheirsimilarimages(totaling10,000)
rankedbyhumans.Weprovidetheimagebenchmarkonourstudy'swebsite
https://github.com/
xyzacademic/RandomDepthwiseCNN
.
Experimentalplatformandsourcecode
Weconductallexperimentsoncomputingnodesequipped
withIntelXeonE5-2630-v4CPUsandNVIDIATeslaP10016GBPascalGPUs.Weimplementour
methodtoproducethelayerwithPyTorchandTensorFlowandmakeitavailableonthis
study'swebsite
https://github.com/xyzacademic/RandomDepthwiseCNN
.Wealsoprovidethereour
implementationsofothernetworksthatwestudyinthispaperincludingcodetogenerateadversarial
examples.
Programparametersandtraining
Weusethelibinearprogram[22]version2.20fordetermining
alinearsupportvectormachineonthenallayerobtainedbyourmodel.Ourliblinearparametersare
-s2-B1whichturnsonprimaloptimizationandathresholdvalueofnonzero.Our
C
(regularization)
parametervaluesare0.5forCIFAR10,CIFAR100,andSTL10,and0.01forMNISTandMini-ImageNet.
WeoptimizeResNet18,DenseNet40,andVGG16networkswithstochasticgradientdescent.ForCI-
FAR10andCIFAR100weuseabatchsizeof256whereasforSTL10andMini-ImageNetweuse32and
16respectively.OnMini-ImageNetweuse90epochsandontheotherbenchmarksweuse300.Wevary
thelearningrateacrossthenumberofepochsbystartingwithalargevalueof0.1andprogressively
reducingto0.01and0.001asthenumberofepochsincreases.
3Results
3.1ofnumberofblocksandkernel;size
Westartbyexploringtheectofnumberofblocksandkernelsizeonourmodeltestaccuracy.In
Table1weshowtheaccuracyofourmodelwithanumberoflayers,kernelsize,and100,000
kernels.Weseethatthenumberoflayersandkernelsizehasaconsiderableontestaccuracy
Dataset
k
=3
;b
=1
k
=3
;b
=2
k
=5
;b
=1
k
=5
;b
=2
CIFAR1075.874.876.470.1
CIFAR10051.851.953.347.5
Table1.
AccuracyofournetworkRDCNNwithtnumberofconvolutionalblocksandkernelsize.
InTable2weshowtheparametersusedinRDCNNforeachimagebenchmark.Theseparameters
giveusthebesttestaccuracyovercombinationsof
k
=3
;
5
;
7and
b
=1
;
2
;
3.
4
DatasetMNISTCIFAR10CIFAR100STL10MiniImageNet
Parameters
k
=7
;b
=1
k
=5
;b
=1
k
=5
;b
=1
k
=3
;b
=3,
k
=5
;b
=2
Table2.
ParametersusedinRDCNNforeachimagebenchmark
3.2ofnumberofrandomkernelsontestaccuracy
Havingshowntheofkernelsizeandnumberofconvolutionalblocksweproceedtodeterminethe
ofnumberofkernelsonthetestaccuracy.Eachkernelgivesrisetoanewfeatureinthe
layer.Weexpectanimprovementintestaccuracyasweincreasethenumberofkernelsand
indeedweseethisisthecaseinFigure3.Thereweseethatincreasingthenumberofkernelsimproves
theaccuracyontheSTL10andCIFAR10benchmarks.Wealsoseethatthetrainaccuracyreaches100%
muchfasterandstaystherewhiletestaccuracycontinuestoimprove.
Fig.3.
ofnumberofkernelsfeatures)onthetestaccuracy
3.3Imagepixeldistributionsimilarityacrossandwithinclasses
Weselect10randomimagesfromSTL10(6fromclass0and4fromclass1)andshowthedistributionof
theimagepixelvaluesbeforeandafterournetwork.InFigure4(a)weseethedistributionofimagepixel
valuesintheoriginaldataandin(b)weshowthedistributioninourlayerbeforetheSVM.We
seethattherandomkernelssmoothentheimagedistributionsandappeartoshowabetterseparation
ofimagedistributionsbetweenthetwoclasses.
InordertoobtainaquantitativemeasurewereporttheaverageJensenShannon(avgJS)divergence
[23]betweenimagepixeldistributionsacrossclassesandwithinclasses.Toobtainthiswesimplyaverage
thedivergenceacrossallpairsofimagedistributionsacrosstwoclasses(andwithinclasses).Wethen
measuretheratioof
avgJS
(
class
0
;class
1)
avgJS
(
class
0
;class
0)+
avgJS
(
class
1
;class
1)
Forimagesintheoriginalfeaturerepresentationwearatioof
0
:
25
0
:
3+0
:
18
=0
:
52andinourRDCNN
layertheratioincreasesto
0
:
02
0
:
018+0
:
016
=0
:
59.Thisratiovariesacrossclassesandtheabovevalues
areforclass0and1thatwerandomlychose.Ifwemeasurethisratiobetweenclasses0and6wesee
alargerintheratioacrossthetwofeaturerepresentations.Intheoriginalrepresentationthis
valueis
0
:
24
0
:
3+0
:
13
=0
:
56andinourRDCNNlayeritincreasesto
0
:
025
0
:
018+0
:
015
=0
:
76.Thuswesee
thatournewfeaturerepresentationgivesabetter
signaltonoise
ratio.
3.4Comparisontorandomweightsinothernetworksandunsupervisedfeaturelearning
HereweconsiderrandomweightsinResNet18andDenseNet40andexceptforthelayerthatwe
train.Wealsoconsiderarecentunsupervisedfeaturelearningmethod(DiscriminativeUFL)[24]that
5
(a)
(b)
Fig.4.
Pixeldistributionofimagesfromtwoclassesintheoriginalandnewrepresentations
Fig.5.
Testaccuracyof(1)deepnetworksDenseNet40andResNet18withrandomweightsexceptfortraining
inlayer,(b)unsupervisedfeaturelearningwithconvolutionalnetworksdenotedasDiscriminativeUFL,and
(3)ournetworkRDCNNonthreeimagebenchmarks
targetsinstanceleveldiscriminationwithconvolutionalneuralnetworks.InFigure5weseethatall
therandomnetworkshaveloweraccuracythanours(denotedasRDCNN)evenaftertrainingthe
layer.WeobtainthesameaccuracywithDiscriminativeUFLonCIFAR10asintheirpublishedpaper.
However,whenwerantheircodeontheotherbenchmarkstheaccuraciesweremuchlower.
TheaccuraciesofrandomnetworksonCIFAR10thatwereportherearelowerthanthoseofpreviously
reportedbySaxeset.al.[9](53.2%)andGilbertel.al.[12](74.8%).It'slikelytheymayhavetrained
additionalparametersbesidesjustthelayerthatwedo.Jarrettet.al.[10]report99.46%onMNIST
withrandomweightsandweperformcomparablybyreaching99.4%.
6
3.5Comparisontotrainedconvolutionalnetworks
InFigure6weshowthetestaccuraciesofthetop-1,top-2,andtop-3outputsfromallnetworksonfour
benchmarks.Thetop-koutputsareobtainedbyconsideringimageswiththetop
k
highestoutputsin
thelayer.InourcaseweusetheSVMdiscriminanttoranktheoutputs.
InSTL10ourrandomnetwork(RDCNN)performscomparablytotrainednetworkswithoutdata
augmentation.InfactitperformsbetterthanwhenDenseNet40istrainedwithoutdataaugmentation.
Ontheotherbenchmarksournetworkistrailinginaccuracybutthatbecomessmalleraswe
considertop-2andtop-3outputsofthe
InbothCIFAR10andMiniImageNetweseethattherandomnetworkhasthesteepestincreasein
accuracyfromtop-1totop-2.Infactintop-2ournetworkisabout90%accurateonCIFAR10andMini
ImageNetand85%onSTL10.InCIFAR100thathasa100classesournetworkcatchesuptotrained
networksatamuchslowerpace.
Fig.6.
Top-kaccuracyoftnetworksonbenchmarks.Methodswithasterixdenotedataaugmentation
wasenabled.
Fig.7.
CosinesimilaritydistributionacrossallSTL10images.Eachcosinevalueisbetweenanimageandits
augmentationinournetwork'slayerfeaturespace.
7
Dataaugmentation
Forourmethodweseparatelygeneratedaugmentedimageswithand
randomrotationsfromSTL10(10augmentsperinputimage).Wethencombinedtheseintotheoriginal
trainingsetandusedthemasinputtoournetwork.Ourtestaccuracy,however,wasnobetterthan
trainingonjusttheoriginaldata.Tounderstandthiswecomparethecosinesimilarityofeachaugmented
imagetoitsoriginalandplotthecosinesimilarityvaluedistributionacrossallimages.Weperformthe
cosinesimilarityinthefeaturespacegivenbythelayerofournetwork.InFigure7weseethatthe
androtationsproduceimagesthatarehighlysimilartotheoriginalinourfeaturerepresentation.
However,ifweperformthecutoutaugmentation[6]theimagesarerelativelymoret.Perhaps
thisaugmentationorsomethingevenstrongermayboosttheaccuracyofournetwork.
3.6Similarimageretrieval
WetraintheVGG16andResNet18networksonall10,000imagesintheCorelPrincetonImageSimilarity
dataset[14]exceptfortheeightqueries.Inthisbenchmarkthemostsimilarimagesarerankedandgiven
ascorebyhumanobservers.Weusethelasthiddenlayerforrepresentingallimages.Ourchoiceforthis
comesfrompreviousstudiesthathaveshownthehiddenlayer(thatisusuallyadenselayer)works
bestforimageretrievalwithdeepconvolutionalneuralnetworks[25,26,27].
Hereweuseournetwork'slayertorepresentimages.Wealsostudyaversionofournetwork
whereweperformtrainingonthelayer.Weuseourlayerasinputintoasinglelayermultilayer
perceptronwithsigmoidactivationand1000featuresinthehiddenlayer(asimplementedinPython
scikit-learn[28]).Hereweexcludetheeightqueriesfromthetrainingprocedure.
(a)(b)
Fig.8.
Shownherein(a)istheintersectionratio(alsoknownasprecision)oftop-kimagesreturnsinthefeature
spaceofeachtrainedmodel.In(b)arethesumofscoresofimagesintheintersection.WedenoteRDCNNas
rh
originandRDCNNtrainedwithasinglelayerneuralnetwork(1000hiddennodes)asrh
trained.
Weobtainthetop-knearestneighborsforeachoftheeightqueriesinthefourtfeaturespaces:
fullytrainedVGG16andResNet18,RDCNN,andRDCNNlayertrainedwithasinglelayerneural
network.Foreachvalueof
k
(in
k
-nearestneighbor)wedeterminetheintersectionratio(alsoknownas
theprecision)asthenumberofcommonimagesinthetruetop
k
imagesforthegivenquerydividedby
k
.InFigure8(a)weseetheintersectionratiosforVGG16,Renset18,RDCNN,andRDCNNfollowed
byatrainedsinglelayerneuralnetwork.
OntheaverageacrosstheeightqueriesourtrainedRDCNNhasthehighestprecisionvaluesaswe
crossvaluesof
k
above10.Ifwesumthescoreofallimagesintheintersectiontheretooweseeour
trainednetworkinthelead(Figure8(b)).Ourrandomone(withouttraining)isbehindVGGbutbetter
thanResNet.
Whileourmethodperformswellforimagesimilarityonthisbenchmarkweseethatthefullytrained
networksproduceabetteroftheeightqueries.Weconsiderthecorrectofthe
8
querytobethecategoryitbelongstointhedatabaseimages.BothVGG16andResNet18classify7
outof8correctlywhereasourRDCNNtrainedlayer)does6outof8correctly.Onthetraining
VGGandResNetachieve98%accuracywhereasourtrainedRDCNNgetsto76%withthemultilayer
perceptron.ThusitappearsthatRDCNNcapturesimagesimilaritybetterthanimagecontent.
3.7Sensitivitytoadversarialattacks
Weexplorewhetherthesignactivationandlackofgradientinourmodeladefensetoadversarial
attacks.Weperformapracticalblackboxattack[29]onourmodelandtheothertrainedones.Foreach
modelincludingoursweinputandoutputexamplesfromtheSTL10dataset.Wethenusethese
examplestolearnatwolayermulti-layerperceptronfromwhichweproduce5,000adversarialexamples.
InTable3weseethatallmodelsincludingoursfailontheadversarialexamples.Thisdespitethesign
activationfunctionandlackofgradientourmodelcanbeattacked.
RDCNNVGG16DenseNet40ResNet18
STL1051%58%46%56%
Table3.
AccuracyofourmodelandothersonSTL10adversarialexamplesgeneratedwithablackboxattack
4Discussion
Wefoundthatournetworkissomewhatmoresensitivetobackgroundthantrainednetworks.From
STL10wepickabirdimageandshowthetopsimilarimagestoitasgivenbyVGG16andournetwork.
HerewemeasurecosinesimilarityinlayerofVGGandournetwork.InFigure9(a)weseethat
VGG16reportsalmostallbirdsasthemostsimilarimagetotheoriginalbird.Inournetworkshown
inFigure9(b)weseethatimagessimilartothebirdarealsosimilarincolorandbackgroundandare
mostlynotbirds.
Thusourrandomnetworkissensitivetobackgroundandcolorwhichpossiblyaccountsforitstrailing
accuracyinctasks.ThiscanalsobeseenintheimageretrievalexperimentswhereRDCNN
performsbetterinretrievalbutworsewhenweclassifythequeries.Whilethisobviouslyisapitfallit
mayalsobeadvantageousinproblemswheresimilarityplaysanimportantrolesuchasmedicalimaging
[30,31].Ifwetrainournetwork'slayerwebegintoseebirdsintopsimilarimagesasshownin
Figure9(c).EvenintheimageretrievalbysimilaritytaskRDCNNperformsbetterwithsometraining.
TodeterminethesensitivityofourmethodtotrainingsetsizewereversedourMiniImageNettraining
andtest:weusethemuchsmallertestsetof500imagesastrainingand12,370fortest.Inthisexperiment
wefoundourmethodtogive58%accuracywhileatrainedResNet18gives48%and63.7%withoutand
withdataaugmentationrespectively.
OnechallengeinRDCNNisthehighdimensionalfeaturespace.Thiscanbeachallengeforlarge
datasetslikeImageNet[21].Weproposetosolvethisbycompressingthelayerwithasimplebit-wise
encodingmethod.Weplantostudythisandapplicationsonmedicalimagesaspartoffuturework.
5Conclusion
Weproposearandomdepthwiseconvolutionalneuralnetworkwiththousandsofconvolutionalkernels
perlayer.Weshowthatournetworkcanachievebetteraccuraciesthanotherrandomnetworksand
unsupervisedfeaturelearningmethods,andcompetitiveaccuraciesinimageandretrieval
comparedtotrainednetworks.Wehighlightsensitivitytobackgroundwhichisapitfallbutalsoa
potentialadvantagethatremainstobeexplored.
9
(a)(b)
(c)
Fig.9.
Shownherearecosinesimilarityvaluesofthetop16imagessimilartothebirdintheupperleft.In(a)
areimagesfromatrainedVGGandin(b)areimagesfromRDCNN'suntrainedlayer.In(c)areimages
similartothebirdaftertrainingthelayerofRDCNN.
References
1.AlexKrizhevsky,IlyaSutskever,andEHinton.Imagenetwithdeepconvolutional
neuralnetworks.InF.Pereira,C.J.C.Burges,L.Bottou,andK.Q.Weinberger,editors,
Advancesin
NeuralInformationProcessingSystems25
,pages1097{1105.CurranAssociates,Inc.,2012.
2.LeonBottou.Large-scalemachinelearningwithstochasticgradientdescent.In
ProceedingsofCOMP-
STAT'2010
,pages177{186.Springer,2010.
3.JustinSalamonandJuanPabloBello.Deepconvolutionalneuralnetworksanddataaugmentationfor
environmentalsound
IEEESignalProcessingLetters
,24(3):279{283,2017.
4.QuocVLe.Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning.In
Acoustics,Speechand
SignalProcessing(ICASSP),2013IEEEInternationalConferenceon
,pages8595{8598.IEEE,2013.
5.NitishSrivastava,Hinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdinov.Dropout:
Asimplewaytopreventneuralnetworksfromov
TheJournalofMachineLearningResearch
,
15(1):1929{1958,2014.
6.TerranceDeVriesandGrahamWTaylor.Improvedregularizationofconvolutionalneuralnetworkswith
cutout.
arXivpreprintarXiv:1708.04552
,2017.
7.AlexKrizhevsky,IlyaSutskever,andEHinton.Imagenetwithdeepconvolutional
neuralnetworks.In
Advancesinneuralinformationprocessingsystems
,pages1097{1105,2012.
8.NoboruMurata.Astatisticalstudyofon-linelearning.
OnlineLearningandNeuralNetworks.Cambridge
UniversityPress,Cambridge,UK
,pages63{92,1998.
9.AndrewMSaxe,PangWeiKoh,ZhenghaoChen,ManeeshBhand,BipinSuresh,andAndrewYNg.On
randomweightsandunsupervisedfeaturelearning.In
ICML
,pages1089{1096,2011.
10
10.KevinJarrett,KorayKavukcuoglu,YannLeCun,etal.Whatisthebestmulti-stagearchitectureforobject
recognition?In
ComputerVision,2009IEEE12thInternationalConferenceon
,pages2146{2153.IEEE,
2009.
11.NicolasPinto,DavidDoukhan,JamesJDiCarlo,andDavidDCox.Ahigh-throughputscreeningap-
proachtodiscoveringgoodformsofbiologicallyinspiredvisualrepresentation.
PLoScomputationalbiology
,
5(11):e1000579,2009.
12.AnnaCGilbert,YiZhang,KibokLee,YutingZhang,andHonglakLee.Towardsunderstandingtheinvert-
ibilityofconvolutionalneuralnetworks.
arXivpreprintarXiv:1705.08664
,2017.
13.KunHe,YanWang,andJohnHopcroft.Apowerfulgenerativemodelusingrandomweightsforthedeep
imagerepresentation.In
AdvancesinNeuralInformationProcessingSystems
,pages631{639,2016.
14.Corel-princetonimagesimilaritybenchmark.
http://www.cs.princeton.edu/cass/benchmark/
.
15.KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningforimagerecognition.In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition
,pages770{778,2016.
16.GaoHuang,ZhuangLiu,KilianQWeinberger,andLaurensvanderMaaten.Denselyconnectedconvolutional
networks.In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition
,volume1,page3,
2017.
17.KarenSimonyanandAndrewZisserman.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.
arXivpreprintarXiv:1409.1556
,2014.
18.YannLeCun,LeonBottou,YoshuaBengio,andPatrickGradient-basedlearningappliedtodocument
recognition.
ProceedingsoftheIEEE
,86(11):2278{2324,1998.
19.AlexKrizhevsky.Learningmultiplelayersoffeaturesfromtinyimages.2009.
20.AdamCoates,AndrewNg,andHonglakLee.Ananalysisofsingle-layernetworksinunsupervisedfeature
learning.In
Proceedingsofthefourteenthinternationalconferenceonintelligenceandstatistics
,
pages215{223,2011.
21.OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,Andrej
Karpathy,AdityaKhosla,MichaelBernstein,AlexanderC.Berg,andLiFei-Fei.ImageNetLargeScaleVisual
RecognitionChallenge.
InternationalJournalofComputerVision(IJCV)
,115(3):211{252,2015.
22.Rong-EnFan,Kai-WeiChang,Cho-JuiHsieh,Xiang-RuiWang,andChih-JenLin.Liblinear:Alibraryfor
largelinear
TheJournalofMachineLearningResearch
,9:1871{1874,2008.
23.BentFugledeandFlemmingTopsoe.Jensen-shannondivergenceandhilbertspaceembedding.In
Information
Theory,2004.ISIT2004.Proceedings.InternationalSymposiumon
,page31.IEEE,2004.
24.ZhirongWu,YuanjunXiong,StellaXYu,andDahuaLin.Unsupervisedfeaturelearningvianon-parametric
instancediscrimination.In
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
,
pages3733{3742,2018.
25.HuafengWang,YeheCai,YanxiangZhang,HaixiaPan,WeifengLv,andHaoHan.Deeplearningforimage
retrieval:Whatworksandwhatdoesn't.In
DataMiningWorkshop(ICDMW),2015IEEEInternational
Conferenceon
,pages1576{1583.IEEE,2015.
26.AliSharifRazavian,HosseinAzizpour,JosephineSullivan,andStefanCarlsson.Cnnfeatures
anastoundingbaselineforrecognition.In
ProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognitionworkshops
,pages806{813,2014.
27.JiWan,DayongWang,StevenChuHongHoi,PengchengWu,JiankeZhu,YongdongZhang,andJintaoLi.
Deeplearningforcontent-basedimageretrieval:Acomprehensivestudy.In
Proceedingsofthe22ndACM
internationalconferenceonMultimedia
,pages157{166.ACM,2014.
28.F.Pedregosa,G.Varoquaux,A.Gramfort,V.Michel,B.Thirion,O.Grisel,M.Blondel,P.Prettenhofer,
R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,andE.Duchesnay.
Scikit-learn:MachinelearninginPython.
JournalofMachineLearningResearch
,12:2825{2830,2011.
29.NicolasPapernot,PatrickMcDaniel,IanGoodfellow,SomeshJha,ZBerkayCelik,andAnanthramSwami.
Practicalblack-boxattacksagainstmachinelearning.In
Proceedingsofthe2017ACMonAsiaConference
onComputerandCommunicationsSecurity
,pages506{519.ACM,2017.
30.LindaGShapiro,IndriyatiAtmosukarto,HansangCho,HJillLin,SalvadorRuiz-Correa,andJennyYuen.
Similarity-basedretrievalforbiomedicalapplications.In
Case-BasedReasoningonImagesandSignals
,pages
355{387.Springer,2008.
31.ChristopherTown.Content-basedandsimilarity-basedqueryingforbroad-usagemedicalimageretrieval.In
AdvancesinBiomedicalInfrastructure2013
,pages63{76.Springer,2013.
11
"
75,On the Power of Over-parametrization in Neural Networks with Quadratic Activation,http://proceedings.mlr.press/v80/du18a/du18a.pdf,https://github.com/Clumsyndicate/One_layer_analysis_network,"OnthePowerofOver-parametrizationin
NeuralNetworkswithQuadraticActivation
SimonS.Du
1
JasonD.Lee
2
Abstract
Weprovidenewtheoreticalinsightsonwhyover-
parametrizationiseffectiveinlearningneuralnet-
works.Fora
k
hiddennodeshallownetworkwith
quadraticactivationand
n
trainingdatapoints,we
showaslongas
k

p
2
n
,over-parametrization
enableslocalsearchalgorithmstoa
globally
optimalsolutionforgeneralsmoothandconvex
lossfunctions.Further,despitethatthenumber
ofparametersmayexceedthesamplesize,using
theoryofRademachercomplexity,weshowwith
weightdecay,thesolutionalsogeneralizeswell
ifthedataissampledfromaregulardistribution
suchasGaussian.Toprovewhen
k

p
2
n
,the
lossfunctionhasbenignlandscapeproperties,we
adoptanideafromsmoothedanalysis,whichmay
haveotherapplicationsinstudyinglosssurfaces
ofneuralnetworks.
1.Introduction
Neuralnetworkshaveachievedaremarkableimpacton
manyapplicationssuchcomputervision,reinforcement
learningandnaturallanguageprocessing.Thoughneural
networksaresuccessfulinpractice,theirtheoreticalprop-
ertiesarenotyetwellunderstood.,thereare
twointriguingempiricalobservationsthatexistingtheories
cannotexplain.

Optimization
:Despitethehighlynon-convexnature
oftheobjectivefunction,simplealgorithms
likestochasticgradientdescentareabletominimize
thetraininglossofneuralnetworks.Researchershave
conjecturedthattheuseofover-parametrization(
Livni
etal.
,
2014
;
SafranandShamir
,
2017
)istheprimary
1
MachineLearningDepartment,CarnegieMellonUniver-
sity
2
DepartmentofDataSciencesandOperations,University
ofSouthernCalifornia.Correspondenceto:SimonS.Du
<
ssdu@cs.cmu.edu
>
.
Proceedingsofthe
35
th
InternationalConferenceonMachine
Learning
,Stockholm,Sweden,PMLR80,2018.Copyright2018
bytheauthor(s).
reasonwhylocalsearchalgorithmscanachievelow
trainingerror.Theintuitionisover-parametrization
altersthelossfunctiontohavealargemanifoldof
globallyoptimalsolutions,whichinturnallowslocal
searchalgorithmstomoreeasilyaglobaloptimal
solution.

Generalization
:Fromthestatisticalpointofview,
over-parametrizationmayhindereffectivegeneraliza-
tion,sinceitgreatlyincreasesthenumberofparameters
tothepointofhavingnumberofparametersexceed
thesamplesize.Toaddressthis,practitionersoftenuse
explicitformsofregularizationsuchasweightdecay,
dropout,orearlystoppingtoimprovegeneralization.
Howeverinthenon-convexsetting,theoretically,we
donothaveagoodquantitativeunderstandingonhow
theseregularizationshelpgeneralizationforneuralnet-
workmodels.
Inthispaper,weprovidenewtheoreticalinsightsintothe
optimizationlandscapeandgeneralizationabilityofover-
parametrizedneuralnetworks.weconsiderthe
neuralnetworkofthefollowingform:
f
(
x
;
W
)=
k
X
j
=1
a
i
˙
(
h
w
j
;
x
i
)
:
(1)
Intheabove
x
2
R
d
istheinputvector,
W
2
R
d

k
with
w
j
2
R
d
denotesthe
j
-throwof
W
and
a
i
'saretheweights
inthesecondlayer.Finally
˙
(

):
R
!
R
denotesthe
activationfunctionappliedtoeachhiddennode.Whenthe
neuralnetworkisover-parameterized,thenumberofhidden
notes
k
canbeverylargecomparedwithinputdimension
d
orthenumberoftrainingsamples.
Inoursetting,wethesecondlayertobe
a
=(1
;:::;
1)
.
Althoughitissimplerthanthecasewherethesecondlayeris
noted,theeffectofover-parameterizationcanbestudied
inthissettingaswellbecausewedonothaveanyrestriction
onthenumberofhiddennodes.
Wefocusonquadraticactivationfunction
˙
(
z
)=
z
2
.
Thoughquadraticactivationsarerarelyusedinpractice,
stackingmultiplesuchtwo-layerblockscanbeusedtosimu-
latehigher-orderpolynomialneuralnetworksandsigmodial
OverparametrizationHelpsOptimizationandGeneralizesWell
activatedneuralnetworks(
Livnietal.
,
2014
;
Soltaniand
Hegde
,
2017
).
Inpractice,wehave
n
trainingsamples
f
x
i
;y
i
g
n
i
=1
and
solvethefollowingoptimizationproblemtolearnaneural
network
min
W
1
n
n
X
i
=1
`
(
f
(
x
i
;
W
)
;y
i
)
where
`
(

;

)
issomelossfunctionsuchas
`
2
orlogisticloss.
Forgradientdescentweusethefollowingupdate
W
 
W


n
X
i
=1
r
W
`
(
f
(
x
i
)
;y
i
)
where

isthestepsize.
Toimprovethegeneralizationability,weoftenaddexplicit
regularization.Inthispaper,wefocusonaparticularreg-
ularizationtechnique,weightdecayforwhichweslightly
changethegradientdescentalgorithmto
W
 
W


n
X
i
=1
r
W
`
(
f
(
x
i
;
W
)
;y
i
)


W
:
where

isthedecayrate.Notethisalgorithmisequivalent
toapplyingthegradientdescentalgorithmontheregularized
loss
min
W
L
(
W
)=
1
n
n
X
i
=1
`
(
f
(
x
i
)
;y
i
)+

2
k
W
k
2
F
:
(2)
Inthissetup,wemakethefollowingtheoreticalcontribu-
tionstoexplainwhyover-parametrizationhelpsoptimiza-
tionandstillallowsforgeneralization.
1.1.MainContributions
Over-parametrizationHelpsOptimization.
Weana-
lyzetwokindsofover-parameterization.Firstweshow
thatfor
k

d;
thenalllocalminimainProblem
(
2
)
isglobalandallsad-
dlepointsarestrict.Thispropertiestogetherwithrecent
algorithmicadvancesinnon-convexoptimization(
Leeetal.
,
2016
)implygradientdescentcanagloballyoptimal
solutionwithrandominitialization.Thisisaminorgeneral-
izationofresultsin(
Soltanolkotabietal.
,
2017
)whichonly
includes
`
2
loss,and(
HaeffeleandVidal
,
2015
;
Haeffele
etal.
,
2014
)whichonlyinclude
k

d
+1
.
Second,weconsideranotherformofover-parametrization,
k
(
k
+1)
2
>n:
Thisconditionontheamountofover-parameterizationis
muchmilderthan
k

n
,aconditionusedinmanyprevious
papers(
NguyenandHein
,
2017a
;
b
).Furtherinpractice,
k
(
k
+1)
=
2
>n
isamuchmilderrequirementthan
k

d
,
sinceif
k
ˇ
p
2
n
and
n<<d
2
then
k<<d
.Inthissetting,
weconsiderthe
perturbed
versionoftheProblem(
2
):
min
W
L
C
(
W
)=
1
n
n
X
i
=1
`
(
f
(
x
i
)
;y
i
)+

2
k
W
k
2
F
+
h
C
;
W
>
W
i
(3)
where
C
isarandompositivematrixwith
arbi-
trarily
smallFrobeniusnorm.Weshowthatif
k
(
k
+1)
2
>n
,
Problem
(
3
)
alsohasthedesiredpropertiesthatalllocal
minimaareglobalandallsaddlepointsarestrictwithprob-
ability
1
.Since
C
hassmallFrobeniusnorm,theoptimal
valueofProblem
(
3
)
isveryclosetothatofProblem
(
2
)
.
SeeSection
3
fortheprecisestatement.
Toprovethissurprisingfact,webringforwardideasfrom
smoothedanalysisinconstructingtheperturbedlossfunc-
tion
(
3
)
,whichwebelieveisusefulforanalyzingtheland-
scapeofnon-convexlosses.
Weight-decayHelpsGeneralization.
Weshowbecause
ofweight-decay,theoptimalsolutionofProblem
(
2
)
also
generalizeswell.Themajorobservationisweight-decay
ensuresthesolutionofProblem
(
2
)
haslow
Frobenius
norm,
whichisequivalenttomatrix
W
>
W
havinglow
nuclear
norm(
Srebroetal.
,
2005
).Thisobservationallowsusto
usetheoryofRademachercomplexitytodirectlyobtain
quantitativegeneralizationbounds.Ourtheoryappliesto
awiderangeofdatadistributionandinparticular,does
notneedtoassumethemodelisrealizable.Further,the
generalizationbounddoesnotdependonthenumberof
epochsSGDrunsorthenumberofhiddennodes.
Tosumup,inthispaperwejustifythefollowingfolklore.
Over-parametrizationallowsustoglobaloptimaand
withweightdecay,thesolutionalsogeneralizeswell.
1.2.Organization
Thispaperisorganizedasfollows.InSection
2
weintro-
ducenecessarybackgroundandInSection
3
wepresentourmaintheoremsonwhyover-parametrization
helpsoptimizationwhen
k

d
or
k
(
k
+1)
2
>n
.InSec-
tion
4
,wegivequantitativegeneralizationboundstoexplain
whyweightdecayhelpsgeneralizationinthepresenceof
over-parametrization.InSection
5
,weproveourmaintheo-
rems.WeconcludeandlistfutureworksinSection
6
.
OverparametrizationHelpsOptimizationandGeneralizesWell
1.3.RelatedWorks
Neuralnetworkshaveenjoyedgreatsuccessinmanyprac-
ticalapplications(
Krizhevskyetal.
,
2012
;
Dauphinetal.
,
2016
;
Silveretal.
,
2016
).Toexplainthissuccess,many
workshavestudiedtheexpressivenessofneuralnetworks.
Theexpressiveabilityofshallowneuralnetworkdatesback
to90s(
Barron
,
1994
).Recentresultsgivemore
analysisondeepermodels(
B
¨
olcskeietal.
,
2017
;
Telgarsky
,
2016
;
Wiatowskietal.
,
2017
).
However,fromthepointofviewoflearningtheory,itis
wellknownthattraininganeuralnetworkishardinthe
worstcase(
BlumandRivest
,
1989
).Despitetheworst-case
pessimism,localsearchalgorithmssuchasgradientde-
scentareverysuccessfulinpractice.Withsomeadditional
assumptions,manyworkstriedtodesignalgorithmsthat
provablylearnaneuralnetwork(
Goeletal.
,
2016
;
Sedghi
andAnandkumar
,
2014
;
Janzaminetal.
,
2015
).However
thesealgorithmsarenotgradient-basedanddonotprovide
insightonwhylocalsearchalgorithmworkswell.
Focusingongradient-basedalgorithms,alineofre-
search(
Tian
,
2017
;
BrutzkusandGloberson
,
2017
;
Zhong
etal.
,
2017a
;
b
;
LiandYuan
,
2017
;
Duetal.
,
2017b
;
c
)an-
alyzedthebehaviorof(stochastic)gradientdescentwitha
structuralassumptionontheinputdistribution.Themajor
drawbackofthesepapersisthattheyallfocusontheregres-
sionsettingwithleast-squareslossandfurtherassumethe
modelisrealizablemeaningthelabelistheoutputofaneu-
ralnetworkplusazeromeannoise,whichisunrealistic.In
thecaseofmorethanonehiddenunit,thepapersof(
Liand
Yuan
,
2017
;
Zhongetal.
,
2017b
)furtherrequireastringent
initializationconditiontorecoverthetrueparameters.
Findingtheoptimalweightsofaneuralnetworkisnon-
convexproblem.Recently,researchersfoundthatifthe
objectivefunctionssatisfythefollowingtwokeyproperties:
(1)alllocalminimaareglobaland(2)allsaddlepointsand
localmaximaarestrict,thenordermethodlikegradient
descent(
Geetal.
,
2015
;
Jinetal.
,
2017
;
Levy
,
2016
;
Du
etal.
,
2017a
;
Leeetal.
,
2016
)canaglobalminimum.
Thismotivatestheresearchofstudyingthelandscapeofneu-
ralnetworks(
Kawaguchi
,
2016
;
Choromanskaetal.
,
2015
;
FreemanandBruna
,
2016
;
ZhouandFeng
,
2017
;
Nguyen
andHein
,
2017a
;
b
;
Geetal.
,
2017
;
SafranandShamir
,
2017
;
Soltanolkotabietal.
,
2017
;
Postonetal.
,
1991
;
Haef-
feleandVidal
,
2015
;
Haeffeleetal.
,
2014
;
Soudryand
Hoffer
,
2017
)Inparticular,
HaeffeleandVidal
(
2015
);
Postonetal.
(
1991
);
NguyenandHein
(
2017a
;
b
)studied
theeffectofover-parameterizationontrainingtheneural
networks.Theseresultsrequirealargeamountofover-
parameterizationthatthewidthofoneofthehiddenlayers
hastobegreaterthanthenumberoftrainingexamples,
whichisunrealisticincommonlyusedneuralnetworks.Re-
cently,
Soltanolkotabietal.
(
2017
)showedforshallowneu-
ralnetworks,thenumberofhiddennodesisonlyrequired
tobelargerorequaltotheinputdimensionfor
`
2
-loss.In
comparison,ourtheoremsworkforgenerallossfunctions
withregularizationunderthesameassumption.Furtherwe
alsoproposeanewformofover-parameterization,namely
aslongas
k

p
2
n
,thelossfunctionalsoadmitsabenign
landscape.
Wenowturnourattentiontogeneralizationabilityoflearned
neuralnetworks.Itiswellknownthattheclassicallearning
theorycannotexplainthegeneralizationabilitybecauseVC-
dimensionofneuralnetworksislarge(
Harveyetal.
,
2017
;
Zhangetal.
,
2016
).Alineofresearchtriestoexplainthis
phenomenonbystudyingtheimplicitregularizationfrom
stochasticgradientdescentalgorithm(
Hardtetal.
,
2015
;
Pensiaetal.
,
2018
;
Mouetal.
,
2017
;
Brutzkusetal.
,
2017
;
Lietal.
,
2017
).However,thegeneralizationboundsofthese
papersoftendependonthenumberofepochsSGDruns,
whichislargeinpractice.Anotherdirectionistostudythe
generalizationabilitybasedonthenormsofweightmatri-
cesinneuralnetworks(
Neyshaburetal.
,
2015
;
2017a
;
b
;
Bartlettetal.
,
2017
;
Liangetal.
,
2017
;
Golowichetal.
,
2017
;
DziugaiteandRoy
,
2017
;
Wuetal.
,
2017
).Ourtheo-
remongeneralizationabilityalsousesthisideabutismore
specializedtothenetworkarchitecture(
1
).
Aftertheinitialsubmissionofthismanuscript,webecame
awareofconcurrentworkof(
Bhojanapallietal.
,
2018
),
whichalsoconsideredthesmoothedanalysistechniqueto
solveprogramsinpenaltyform.Themath-
ematicaltechniquesinourworkand(
Bhojanapallietal.
,
2018
)aresimilar,butthefocusisontwodistinctproblems
ofsolvingprogramsandquadraticactivation
neuralnetworks.
2.Preliminaries
Weusebold-facedlettersforvectorsandmatrices.Fora
vector
v
,weuse
k
v
k
2
todenotetheEuclideannorm.Fora
matrix
M
,wedenote
k
M
k
2
thespectralnormand
k
M
k
F
theFrobeniusnorm.Welet
N
(
M
)
todenotetheleftnull-
spaceof
M
,i.e.
N
(
M
)=

v
:
v
>
M
=
0

:
Weuse
(
M
)
todenotethesetofmatriceswithFrobenius
normboundedby
M
and

1
(1)
todenotethesetofrank-
1
matriceswithspectralnormboundedby
1
.Wealsodenote
S
d
thesetof
d

d
symmetricpositivematrices.
Inthispaper,wecharacterizethelandscapeofover-
parameterizedneuralnetworks.Morewestudy
thepropertiesofcriticalpointsofempiricalloss.Here
foralossfunction
L
(
W
)
,acriticalpoint
W


r
L
(
W

)=0
.Acriticalpointcanbealocalminimumor
OverparametrizationHelpsOptimizationandGeneralizesWell
asaddlepoint.
1
If
W

isalocalminimum,thenthereisa
neighborhood
O
around
W

suchthat
L
(
W

)

L
(
W
)
forall
W
2
O
.If
W

isasaddlepoint,thenforall
neighborhood
O
around
W

,thereisa
W
2
O
suchthat
L
(
W
)
<L
(
W

)
.
Ideally,wewouldlikealossfunctionthatthefol-
lowingtwogeometricproperties.
Property2.1
(Alllocalminimaareglobal)
.
If
W

isa
localminimumof
L
(

)
itisalsotheglobalminimum,i.e.,
W

2
argmin
W
L
(
W
)
.
Property2.2
(Allsaddlesarestrict)
.
Atasaddlepoint
W
s
,
thereisadirection
U
2
R
k

d
suchthat
vect
(
U
)
>
r
2
L
(
W
s
)
vect
(
U
)
<
0
:
Ifalossfunction
L
(

)
Property
2.1
andProp-
erty
2.2
,recentalgorithmicadvancesinnon-convexop-
timizationshowrandomlyinitializedgradientdescental-
gorithmorperturbedgradientdescentcanaglobal
minimum(
Leeetal.
,
2016
;
Geetal.
,
2015
;
Jinetal.
,
2017
;
Duetal.
,
2017a
).
Lastly,standardapplicationsofRademachercomplexity
theorywillbeusedtoderivegeneralizationbounds.
2.1
ofRademacherComplexity)
.
Givenasample
S
=(
x
1
;:::;
x
n
)
,theempirical
Rademachercomplexityofafunctionclass
F
is
as
R
S
(
F
)=
1
n
E
˙
""
sup
f
2F
n
X
i
=1
˙
i
f
(
x
i
)
#
where
˙
=(
˙
1
;:::;˙
m
)
areindependentrandomvaraibles
drawnfromtheRademacherdistribution,i.e.,
P
(
˙
i
=1)=
P
(
˙
i
=

1)=1
=
2
for
i
=1
;:::;n
.
3.OverparametrizationHelpsOptimization
Inthissectionwepresentourmainresultsonexplaining
whyover-parametrizationhelpslocalsearchalgorithms
aglobaloptimalsolution.Weconsidertwokindsofover-
parameterization,
k

d
and
k
(
k
+1)
2
>n
.Webeginwith
thesimplercasewhen
k

d
.
Theorem3.1.
Assumewehaveanarbitrarydatasetof
input/labelpairs
x
i
2
R
d
and
y
i
2
R
for
i
=1
;:::;n
andaconvex
C
2
loss
`
(^
y;y
)
.Consideraneuralnetwork
oftheform
f
(
x
;
W
)=
P
k
j
=1
˙

w
>
j
x

with
˙
(
z
)=
z
2
and
W
2
R
k

d
denotingtheweightsconnectinginputto
hiddenlayers.Suppose
k

d
.Then,thetraininglossasa
1
Wedonotdifferentiatebetweensaddlepointsandlocalmax-
imainthispaper.
functionofweight
W
ofthehiddenlayers
L
(
W
)=
1
2
n
`
(
f
(
x
i
;
W
)
;y
i
)+

2
k
W
k
2
F
obeysProperty
2.1
andProperty
2.2
.
Theaboveresultstatesthatgivenan
arbitrary
dataset,the
optimizationlandscapehasbenignpropertiesthatfacilitate
globallyoptimalneuralnetworks.Inparticular,
bysettingthelastlayertobetheaveragepoolinglayer,
alllocalminimaareglobalminimaandallsaddleshavea
directionofnegativecurvature.Thisinturnimpliesthat
gradientdescentonthelayerweights,wheninitialized
atrandom,convergestoaglobaloptimum.Thesedesired
propertiesholdaslongasthehiddenlayeriswide(
k

d
).
AninterestingandperhapssurprisingaspectofTheorem
3.1
isitsgenerality.Itappliestoarbitrarydatasetofanysize
withanyconvexdifferentiablelossfunction.
Nowweconsiderthesecondcasewhen
k
(
k
+1)
2
>n
.As
mentionedearlier,inpracticethisisoftenamilderrequire-
mentthan
k

d
,andoneofthemainnoveltiesofthis
paper.
Theorem3.2.
Assumewehaveanarbitrarydatasetof
input/labelpairs
x
i
2
R
d
and
y
i
2
R
for
i
=1
;:::;n
,
andaconvex
C
2
loss
`
(^
y;y
)
.Consideraneuralnetwork
oftheform
f
(
x
)=
P
k
j
=1
˙

w
>
j
x

with
˙
(
z
)=
z
2
and
W
2
R
k

d
denotingtheweightsconnectinginputtohidden
layers.Suppose
k
(
k
+1)
2
>n;k<d
and
C
isarandompos-
itivetematrixwith
k
C
k
F


whosedistribution
isabsolutelycontinuouswithrespecttoLebesguemeasure.
Then,thetraininglossasafunctionofweight
W
ofthe
hiddenlayers
L
(
W
)=
1
2
n
`
(
f
(
x
i
)
;y
i
)+

2
k
W
k
2
F
+
h
C
;
W
>
W
i
(4)
obeysProperty
2.1
andProperty
2.2
.Further,anyglobal
optimalsolution
c
W
ofProblem
(
4
)

1
2
n
n
X
i
=1
`

f
(
x
i
;
c
W
)
;y
i

+

2



c
W



2
F

1
2
n
n
X
i
=1
`
(
f
(
x
i
;
W

)
;y
i
)+

2
k
W

k
F
2
+

k
W

k
2
F
where
W

2
argmin
W
n
X
i
=1
1
2
n
`
(
f
(
x
i
;
W
)
;y
i
)+

2
k
W

k
2
F
:
SimilartoTheorem
3.1
,Theorem
3.2
statesthatif
k
(
k
+1)
2
>
n
,thenforanarbitrarydataset,the
perturbed
objective
OverparametrizationHelpsOptimizationandGeneralizesWell
function
(
3
)
hasthedesiredpropertiesthatenablelocal
searchheuristicstogloballyoptimalsolutionfora
generalclassoflossfunctions.Further,wecanchoose
thisperturbationtobearbitrarilysmallsotheminimum
of(
3
)iscloseto(
2
).
Theproofoftheoremisinspiredbyalineofliteraturestarted
by
Pataki
(
1998
;
2000
);
BurerandMonteiro
(
2003
);
Boumal
etal.
(
2016
).Insummary,
Boumaletal.
(
2016
)showedthat
forﬁalmostallﬂprograms,everylocalminima
oftherank
r
non-convexformulationofanSDPisaglobal
minimumoftheoriginalSDP.However,thistheoremapplies
withtheimportantcaveatofonlyapplyingto
programsthatdonotfallintoameasurezeroset.Our
primarycontributionistodevelopaprocedurethatexploits
thisbya)constructingaperturbedobjectivetoavoidthe
measurezeroset,b)provingthattheperturbedobjective
hasProperty
2.1
and
2.2
,andc)showingtheoptimalvalue
oftheperturbedobjectiveisclosetotheoriginalobjective.
Furthernotethattheanalysisof(
Boumaletal.
,
2016
)does
notapplysinceourlossfunctions,suchasthelogisticloss,
arenotrepresentable.Wereferreadersto
Section
5.2
formoretechnicalinsights.
4.Weight-decayHelpsGeneralization
Inthissectionweswitchourfocustothegeneralization
abilityofthelearnedneuralnetwork.Sinceweuseweight-
decayorequivalently
`
2
regularizationin
(
2
)
,theFrobenius
normoflearnedweight
W
isbounded.Therefore,inthis
sectionwefocusweightmatrixinboundedFrobeniusnorm
space,i.e.,
k
W
k
F
2
(
M
)
.
Toderivethegeneralizationbound,werecalltheclassi-
calgeneralizationboundbasedonRademachercomplexity
bound(c.f.Theorem2of(
KoltchinskiiandPanchenko
,
2002
)).
Theorem4.1.
Assumeeachdatapointissampledi.i.dfrom
somedistribution
P
,i.e.,
(
x
i
;y
i
)
˘
P
for
i
=1
;:::;n:
Wedenote
S
=
f
x
i
;y
i
g
n
i
=1
and
L
tr
(
W
)=
1
n
P
n
i
=1
`
(
f
(
x
i
;
W
)
;y
i
)
and
L
te
(
W
)=
E
(
x
;y
)
˘P
[
`
(
f
(
x
i
;
W
)
;y
i
)]
.Supposelossfunction
`
(

;

)
is
L
-Lipschitzinthestargument,thenforall
W
2
(
M
)
,wehavewithhighprobability
L
te
(
W
)

L
tr
(
W
)

CL

R
S
(
M
))
where
C>
0
isanabsoluteconstantand
R
S
(
M
))
is
theRademachercomplexityof
(
M
)
.
WithTheorem
4.1
athand,weonlyneedtoboundthe
Rademachercomplexityof
(
M
)
.NotethatRademacher
complexityisadistributiondependentquantity.Ifthedata
isarbitrary,wecannothaveanyguarantee.Webeginwitha
theoremforboundedinputdomain.
Theorem4.2.
Supposeinputissampledfromabounded
ballin
R
d
,i.e.,
k
x
k
2

b
forsome
b>
0
and
E
h


xx
>


2
2
i

B
,thentheRademachercomplexitysat-

R
S
(
M
))

r
2
b
4
M
4
log
d
n
:
CombiningTheorem
4.1
andTheorem
4.2
wecanobtaina
generalizationbound.
Theorem4.3.
UnderthesameassumptionsofTheorem
4.1
andTheorem
4.2
,wehave
L
te
(
W
)

L
tr
(
W
)

CLM
2
b
2
r
log
d
n
:
forsomeabsoluteconstant
C>
0
.
WhileTheorem
4.2
isavalidbound,itisratherpessimistic
becauseweonlyassume
x
isbounded.Considerthefollow-
ingscenarioinwhicheachinputissampledfromastandard
Gaussiandistribution
x
i
˘
N
(0
;
I
)
.Thenignoringthe
logarithmicfactors,usingstandardGaussianconcentration
boundwecanshowwithhighprobability
k
x
i
k
2
=
e
O

p
d

.
2
Plugginginthisboundwehave
L
te
(
W
)

L
tr
(
W
)

CLM
2
r
d
2
log
d
n
(5)
Noteinthisbound,ithasaquadraticdependencyonthe
dimension,soweneedtohave
n
=

d
2

tohaveamean-
ingfulbound.
Infact,fordistributionslikeGaussianusingThe-
orem
5.2
,wecanoftenderiveastrongergeneralization
bound.
Corollary4.1.
Suppose
x
i
˘
N
(0
;
I
)
for
i
=1
;:::;n
.If
thenumberofsamples
n

d
log
d
,wehavewith
highprobabilitythatRademachercomplexity
R
S
(
M
))

C
r
M
4
d
n
:
forsomeabsoluteconstant
C>
0
.
Again,combiningTheorem
4.1
andCorollary
4.1
weob-
tainthefollowinggeneralizationboundforGaussianinput
distribution
Theorem4.4.
UnderthesameassumptionsofTheorem
4.1
andCorollary
4.1
,wehave
L
te
(
W
)

L
tr
(
W
)

CLM
2
r
d
n
:
forsomeabsoluteconstant
C>
0
.
2
e
O
(

)
hideslogarithmicfactors.
OverparametrizationHelpsOptimizationandGeneralizesWell
ComparingTheorem
4.4
withgeneralizationbound
(
5
)
,The-
orem
4.4
hasan
O
(
p
d
)
advantage.Theorem
4.4
hasthe
p
d=n
dependency,whichistheusualparametricrate.Fur-
therinpractice,numberoftrainingsamplesandinputdi-
mensionareoftenofthesameorderforcommondatasets
andarchitectures(
Zhangetal.
,
2016
).
Corollary
4.1
isaspecialcaseofthemoregeneralTheorem
5.2
whichonlyrequiresaboundonthefourthmoment,


P
n
i
=1
(
x
i
x
>
i
)
2


2

s
.Ingeneral,ourtheoremssuggestif
theFrobeniusnormofweightmatrix
W
issmallandthe
input
x
issampledfromabenigndistributionwithcontrolled
4thmoments,thenwehavegoodgeneralization.
Asaconcretescenario,considerafavorablesettingwhere
thetruedatacanbecorrectlybyasmallnetwork
usingonly
k
0
˝
k
hiddenunits.Theweights
W

arenon-
zeroonlyinthe
k
0
rowsand
max
j
2
[
k
0
]


e
>
j
W



2

R
.
FromTheorem
4.4
toreachgeneralizationgapof

,we
havesamplecomplexityof
n

1

2
C
2
L
2
R
4
dk
2
0
,which
onlydependsontheeffectivenumberofhiddenunits
k
0
˝
k
.Thesameresultcanbereachedformoregeneralinput
distributionsbyusingTheorem
5.2
inplaceofTheorem
4.4
.
5.Proofs
5.1.ProofofTheorem
3.1
andTheorem
3.2
Ourproofsofover-parametrizationhelpsoptimizationbuild
uponexistinggeometriccharacterizationonmatrixfactor-
ization.WeciteausefulTheoremby
Haeffeleetal.
(
2014
).
3
Theorem5.1
(Theorem2of(
Haeffeleetal.
,
2014
)adapted
tooursetting)
.
Let
`
(

;

)
beatwicedifferentiableconvex
functioninthestargument.Ifthefunction
L
(
W
)

in
(
2
)
atarmatrix
W

r
L
(
W
)=
0
and
r
2
L
(
W
)
<
0
;
then
W
isaglobalminimum.
ProofofTheorem
3.1
.
WeproveProperty
2.1
andProp-
erty
2.2
simultaneouslybyshowingifa
W
satisfy
r
L
(
W
)=
0
and
r
2
L
(
W
)
<
0
thenitisaglobalminimum.
If
rank(
W
)
<d
,wecandirectlyapplyTheorem
5.1
.Thus
itremainstoconsiderthecase
rank(
W
)=
d
.We
3
Theorem2of(
Haeffeleetal.
,
2014
)assumes
W
isalocal
minimum,butscrutinizingitsproof,wecanseethattheassumption
canberelaxedto
r
L
(
W
)=
0
and
r
2
L
(
W
)
<
0
.
noticethat
r
L
(
W
)=0
isequivalentto
W
 
1
n
n
X
i
=1
@`
(^
y
i
;y
i
)
@
^
y
i
x
i
x
>
i
+

I
!
=
0
(6)
where
^
y
i
=
f
(
x
i
;
W
)
.Since
rank(
W
)=
d
and
k

d
,
weknow
W
hasaleftpseudo-inverse,i.e.,thereexists
W
y
suchthat
W
y
W
=
I
.Multiplying
W
y
ontheleftin
Equation(
6
),wehave
1
n
n
X
i
=1
@`
(^
y
i
;y
i
)
@
^
y
i
x
i
x
>
i
+

I
=
0
:
(7)
ToproveTheorem
3.1
,thekeyideaistoconsiderthefollow
referenceoptimizationproblem.
min
M
:
M
2S
d
L
(
M
)=
1
n
n
X
i
=1
`

x
>
i
Mx
i
;y
i

+

k
M
k

:
(8)
Problem
(
8
)
isaconvexoptimizationproblemin
M
andhas
thesameglobalminimumastheoriginalproblem.Nowwe
denote
~
y
i
=
x
>
i
Mx
i
.Sincethisisaconvexfunction,the
optimalityconditionforglobaloptimalityis
0
2
1
n
n
X
i
=1
@`
(~
y
i
;y
i
)
@
~
y
i
x
i
x
>
i
+

k
M
k

;
M
isaglobalminimum.
UsingEquation
(
7
)
,weknow
M
=
W
>
W
achievesthe
globalminimuminProblem
(
8
)
.Theproofisthuscomplete.
5.2.ProofofTheorem
3.2
Proof.
Weprove
L
C
(
W
)
Property
2.1
and
Property
2.2
.SimilartotheproofofTheorem
3.1
,weprove
thesetwopropertiessimultaneouslybyshowingifa
W
satisfy
r
L
C
(
W
)=
0
and
r
2
L
C
(
W
)
<
0
(9)
thenitisaglobalminimum.BecauseofTheorem
5.1
,we
onlyneedtoshowthatif
W
satisfycondition
(
9
)
,itisrank
i.e.
rank(
W
)
<k
.
Forthegradientcondition,wehave
W
 
1
n
n
X
i
=1
@`
(^
y
i
;y
i
)
@
^
y
i
x
i
x
>
i
+

I
!
+
WC
=0
:
Forsimplicitywedenote
v
i
=
@`
(^
y
i
;y
i
)
@
^
y
i
where
^
y
i
=
x
>
i
W
>
Wx
i
and
S
(
v
)=
P
n
i
=1
v
i
x
i
x
>
i
.Usingthe
orderconditionweknow
W
isinthenullspaceof
(
S
(
v
)+

I
+
C
)
.Thus,wecanboundtherankof
W
by
rank(
W
)

dim
N
(
S
(
v
)+

I
+
C
)

max
v
dim
N
(
S
(
v
)+

I
+
C
)
:
OverparametrizationHelpsOptimizationandGeneralizesWell
Weprovebycontradiction.Assume
rank(
W
)

k
,we
musthave
k

max
v
N
(
S
(
v
)+

I
+
C
)
:
Now
M
=
S
(
v

)+

I
+
C
with
v

=argmaxdim
N
(
S
(
v
)+

I
+
C
)
:
Thuswehavefollowingconditions
C
=
M

S
(
v

)


I
;
dim
N
(
M
)

k:
Thekeyideaistousethesetwoconditionstoupperbound
thedimensionof
C
.Tothisend,wetheset
B
`
=

A
:
A
=
M

S
(
v
)


I
;
M
2S
d
;
v
2
R
n
;
dim
N
(
M


I
)=
`
g
:
Notethatthedimensionofthemanifold
B
`
is

d
(
d
+1)
2

`
(
`
+1)
2

+
n
wherethetermisthedimensionof
d

d
matrices,the
secondtermisthedimensionofthenullspaceandthelast
termisdimensionof
Range(
S
(
v
))
for
v
2
R
n
,whichis
upperboundedby
n
.
Nextnotethat
B
`
1
ˆB
`
2
for
`
1

`
2
.Therefore,wecan
computethedimensionoftheunion
dim

[
d
`
=
k
B
`

=

d
(
d
+1)
2

k
(
k
+1)
2

+
n:
Notebecauseweassume
k
(
k
+1)
2
>n
,wehave
dim

[
d
`
=
k
B
`

<
d
(
d
+1)
2
.However,recall
C
2[
d
`
=
k
B
`
by
sowehave
C
issampledfromalow-dimensional
manifoldwhichhasLebesugemeasure
0
.Sincewesample
C
fromadistributionthatisabsolutecontinuouswithre-
specttotheLebesguemeasure,theevent

C
2[
d
`
=
k
B
`

happenswithprobability
0
.Therefore,withprobability
1
,
rank(
W
)
<k
.TheproofofthepartofTheorem
3.2
iscomplete.
Forthesecondpart.Let
c
W
=argmin
W
L
C
(
W
)
and
W

=argmin
W
(
W
)
.Thereforewehave
L

c
W

+
h
C
;
c
W
>
c
W
i

L
(
W

)+
h
C
;
(
W

)
>
W

i

L
(
W

)+

k
W

k
2
F
:
Notebecause
C
and
c
W
>
c
W
arebothpositive
wehave
h
C
;
c
W
>
c
W
i
0
.Thus
L

c
W


L
(
W

)+

k
W

k
2
F
.
5.3.ProofofTheorem
4.2
andTheorem
4.4
Ourproofisinspiredby(
SrebroandShraibman
,
2005
)
whichexploitsthestructureofnuclearnormboundedspace.
WeproveageneralTheoremthatonlydependsonthe
propertyofthefourth-momentofinputrandomvariables.
Theorem5.2.
Supposetheinputrandomvariable



P
n
i
=1

x
i
x
>
i

2



2

s
.ThentheRademachercomplexity
of
(
M
)
isboundedby
R
S
(
M
))

p
2
M
4
s
log
d
n
:
Proof.
Foragivensetofinputs
S
=
f
x
i
g
n
i
=1
inourcontext,
wecanwriteRademachercomplexityas
R
S
(
M
))=
1
n
E
˙
""
sup
W
2

M
)
n
X
i
=1
˙
i
x
>
i
W
>
Wx
i
#
SinceRademachercomplexitydoesnotchangewhentak-
ingconvexcombinations,wecanboundRademacher
complexityoftheclassofrank-
1
matriceswithspectral
normboundedby
1
andthentakeconvexhullandscaleby
M
.Notefor
W
2

1
(1)
,wecanwrite
W
=
vw
>
with
k
w
k
2

1
and
k
v
k
2
=1
.Usingthisexpression,wecan
obtainanexplicitformulaofRademachercomplexity.
R
S

1
(1))
=
1
n
E
˙
""
sup
W
2

1
(1)
n
X
i
=1
˙
i
x
>
i
W
>
Wx
i
#
=
1
n
E
˙
""
sup
W
2

1
(1)
n
X
i
=1
˙
i
x
>
i

w
>
v

>

vw
>

x
i
#
=
1
n
E
˙
""
sup
w
:
k
w
k
2

1
n
X
i
=1
˙
i
x
>
i
ww
>
x
i
#
=
1
n
E
˙
""
sup
w
:
k
w
k
2

1
n
X
i
=1
˙
i
w
>
x
>
i
x
i
w
#
=
1
n
E
˙
""





n
X
i
=1
˙
i
x
i
x
>
i





2
#
:
Now,tobound
E
˙



P
n
i
=1
˙
i
x
i
x
>
i


2

,wecanusethere-
sultsfromrandommatrixtheoryonRademacherseries.
Recallthatweassume





n
X
i
=1

x
i
x
>
i

2





2

s
andnoticethat
E
˙
""
n
X
i
=1
˙
i
x
i
x
>
i
#
=
0
:
OverparametrizationHelpsOptimizationandGeneralizesWell
ApplyingRademachermatrixseriesexpectationbound(The-
orem4.6.1of(
Troppetal.
,
2015
)),wehave
E
˙
""





n
X
i
=1
˙
i
x
i
x
>
i





2
#

v
u
u
t
2





n
X
i
=1

x
i
x
>
i

2





2
log
d

p
2
s
log
d:
Nowtakingtheconvexhulland,scalingby
M
weobtain
thedesiredresult.
WithTheorem
5.2
athand,fordifferentdistributions,we
onlyneedtobound



P
n
i
=1

x
i
x
>
i

2



2
.
ProofofTheorem
4.3
.
Sinceweassume
k
x
i
k
2

b
,we
directlyhave





n
X
i
=1

x
i
x
>
i

2





2

n
X
i
=1




x
i
x
>
i

2



2
=
n
X
i
=1
k
x
i
k
2
2


x
i
x
>
i


2

nb
4
:
PluggingthisboundinTheorem
5.2
weobtainthedesired
inequality.
ProofofCorollary
4.1
andTheorem
4.4
.
ToproveCorol-
lary
4.1
,weuseTheorem
5.2
andLemma4.7in
(
Soltanolkotabietal.
,
2017
)toupperbound
s
=



P
n
i
=1
k
x
i
k
2
x
i
x
>
i



.Byletting
A
=
I
inLemma4.7,
wethat
s

Cnd;
withprobabilityatleast
1

C
d
.Thiscompletestheproofof
Corollary
4.1
.
UsingthisboundinTheorem
5.2
comletestheproofof
Theorem
4.4
.
6.ConclusionandFutureWorks
Inthispaperweprovidednewtheoreticalresultsonover-
parameterizedneuralnetworks.Usingsmoothedanalysis,
weshowedaslongasthenumberofhiddennodesisbigger
thantheinputdimension
or
squarerootofthenumberof
trainingdata,thelosssurfacehasbenignpropertiesthat
enablelocalsearchalgorithmstoglobalminima.We
furtherusethetheoryofRademachercomplexitytoshow
thelearnedneuralcangeneralizewell.
Ournextstepisconsiderneuralnetworkswithotheracti-
vationfunctionsandhowover-parametrizationallowsfor
eflocal-searchalgorithmstonearglobalminimz-
ers.Anotherinterestingdirectiontoextendourresultsto
deepermodel.
7.Acknowledgment
S.S.D.wassupportedbyNSFgrantIIS1563887,AFRL
grantFA8750-17-2-0212andDARPAD17AP00001.J.D.L.
acknowledgessupportoftheAROunderMURIAward
W911NF-11-1-0303.Thisispartofthecollaborationbe-
tweenUSDOD,UKMODandUKEngineeringandPhysi-
calResearchCouncil(EPSRC)undertheMultidisciplinary
UniversityResearchInitiative.
References
AndrewRBarron.Approximationandestimationbounds
forneuralnetworks.
Machinelearning
,14(1):
115Œ133,1994.
PeterLBartlett,DylanJFoster,andMatusJTelgarsky.
Spectrally-normalizedmarginboundsforneuralnet-
works.In
AdvancesinNeuralInformationProcessing
Systems
,pages6241Œ6250,2017.
SrinadhBhojanapalli,NicolasBoumal,PrateekJain,and
PraneethNetrapalli.Smoothedanalysisforlow-rank
solutionstoprogramsinquadraticpenalty
form.
arXivpreprintarXiv:1803.00186
,2018.
AvrimBlumandRonaldLRivest.Traininga3-nodeneural
networkisNP-complete.In
Advancesinneuralinforma-
tionprocessingsystems
,pages494Œ501,1989.
HelmutB
¨
olcskei,PhilippGrohs,GittaKutyniok,and
PhilippPetersen.Optimalapproximationwithsparsely
connecteddeepneuralnetworks.
arXivpreprint
arXiv:1705.01714
,2017.
NicolasBoumal,VladVoroninski,andAfonsoBandeira.
Thenon-convexburer-monteiroapproachworkson
smoothprograms.In
AdvancesinNeural
InformationProcessingSystems
,pages2757Œ2765,2016.
AlonBrutzkusandAmirGloberson.Globallyoptimalgra-
dientdescentforaconvnetwithgaussianinputs.
arXiv
preprintarXiv:1702.07966
,2017.
AlonBrutzkus,AmirGloberson,EranMalach,andShai
Shalev-Shwartz.Sgdlearnsover-parameterizednetworks
thatprovablygeneralizeonlinearlyseparabledata.
arXiv
preprintarXiv:1710.10174
,2017.
SamuelBurerandRenatoDCMonteiro.Anonlinearpro-
grammingalgorithmforsolvingprograms
vialow-rankfactorization.
MathematicalProgramming
,
95(2):329Œ357,2003.
AnnaChoromanska,MikaelHenaff,MichaelMathieu,
G
´
erardBenArous,andYannLeCun.Thelosssurfaces
ofmultilayernetworks.In
Intelligenceand
Statistics
,pages192Œ204,2015.
OverparametrizationHelpsOptimizationandGeneralizesWell
YannNDauphin,AngelaFan,MichaelAuli,andDavid
Grangier.Languagemodelingwithgatedconvolutional
networks.
arXivpreprintarXiv:1612.08083
,2016.
SimonSDu,ChiJin,JasonDLee,MichaelIJordan,Barn-
abasPoczos,andAartiSingh.Gradientdescentcantake
exponentialtimetoescapesaddlepoints.
arXivpreprint
arXiv:1705.10412
,2017a.
SimonSDu,JasonDLee,andYuandongTian.When
isaconvolutionaleasytolearn?
arXivpreprint
arXiv:1709.06129
,2017b.
SimonSDu,JasonDLee,YuandongTian,BarnabasPoczos,
andAartiSingh.Gradientdescentlearnsone-hidden-
layercnn:Don'tbeafraidofspuriouslocalminima.
arXiv
preprintarXiv:1712.00779
,2017c.
GintareKarolinaDziugaiteandDanielMRoy.Computing
nonvacuousgeneralizationboundsfordeep(stochastic)
neuralnetworkswithmanymoreparametersthantraining
data.
arXivpreprintarXiv:1703.11008
,2017.
CDanielFreemanandJoanBruna.Topologyandgeometry
ofnetworkoptimization.
arXivpreprint
arXiv:1611.01540
,2016.
RongGe,FurongHuang,ChiJin,andYangYuan.Escaping
fromsaddlepoints

onlinestochasticgradientfortensor
decomposition.In
ProceedingsofThe28thConference
onLearningTheory
,pages797Œ842,2015.
RongGe,JasonDLee,andTengyuMa.Learningone-
hidden-layerneuralnetworkswithlandscapedesign.
arXivpreprintarXiv:1711.00501
,2017.
SurbhiGoel,VarunKanade,AdamKlivans,andJustin
Thaler.ReliablylearningtheReLUinpolynomialtime.
arXivpreprintarXiv:1611.10258
,2016.
NoahGolowich,AlexanderRakhlin,andOhadShamir.Size-
independentsamplecomplexityofneuralnetworks.
arXiv
preprintarXiv:1712.06541
,2017.
BenjaminHaeffele,EricYoung,andReneVidal.Structured
low-rankmatrixfactorization:Optimality,algorithm,and
applicationstoimageprocessing.In
InternationalCon-
ferenceonMachineLearning
,pages2007Œ2015,2014.
BenjaminDHaeffeleandRen
´
eVidal.Globaloptimality
intensorfactorization,deeplearning,andbeyond.
arXiv
preprintarXiv:1506.07540
,2015.
MoritzHardt,BenjaminRecht,andYoramSinger.Train
faster,generalizebetter:Stabilityofstochasticgradient
descent.
arXivpreprintarXiv:1509.01240
,2015.
NickHarvey,ChrisLiaw,andAbbasMehrabian.Nearly-
tightvc-dimensionboundsforpiecewiselinearneural
networks.
arXivpreprintarXiv:1703.02930
,2017.
MajidJanzamin,HanieSedghi,andAnimaAnandkumar.
Beatingtheperilsofnon-convexity:Guaranteedtraining
ofneuralnetworksusingtensormethods.
arXivpreprint
arXiv:1506.08473
,2015.
ChiJin,RongGe,PraneethNetrapalli,ShamM.Kakade,
andMichaelI.Jordan.Howtoescapesaddlepoints
ef.In
Proceedingsofthe34thInternationalCon-
ferenceonMachineLearning
,pages1724Œ1732,2017.
KenjiKawaguchi.Deeplearningwithoutpoorlocalminima.
In
AdvancesInNeuralInformationProcessingSystems
,
pages586Œ594,2016.
VladimirKoltchinskiiandDmitryPanchenko.Empirical
margindistributionsandboundingthegeneralizationer-
rorofcombinedrs.
AnnalsofStatistics
,pages
1Œ50,2002.
AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.
Imagenetclasswithdeepconvolutionalneural
networks.In
Advancesinneuralinformationprocessing
systems
,pages1097Œ1105,2012.
JasonDLee,MaxSimchowitz,MichaelIJordan,andBen-
jaminRecht.Gradientdescentonlyconvergestominimiz-
ers.In
ConferenceonLearningTheory
,pages1246Œ1257,
2016.
YLevy.Thepowerofnormalization:Fasterevasionof
saddlepoints.
arXivpreprintarXiv:1611.04831
,2016.
YuanzhiLiandYangYuan.Convergenceanalysisoftwo-
layerneuralnetworkswithReLUactivation.
arXiv
preprintarXiv:1705.09886
,2017.
YuanzhiLi,TengyuMa,andHongyangZhang.Algorith-
micregularizationinover-parameterizedmatrixrecovery.
arXivpreprintarXiv:1712.09203
,2017.
TengyuanLiang,TomasoPoggio,AlexanderRakhlin,and
JamesStokes.Fisher-raometric,geometry,andcomplex-
ityofneuralnetworks.
arXivpreprintarXiv:1711.01530
,
2017.
RoiLivni,ShaiShalev-Shwartz,andOhadShamir.On
thecomputationalefyoftrainingneuralnetworks.
In
AdvancesinNeuralInformationProcessingSystems
,
pages855Œ863,2014.
WenlongMou,LiweiWang,XiyuZhai,andKaiZheng.
Generalizationboundsofsgldfornon-convexlearn-
ing:Twotheoreticalviewpoints.
arXivpreprint
arXiv:1707.05947
,2017.
OverparametrizationHelpsOptimizationandGeneralizesWell
BehnamNeyshabur,RyotaTomioka,andNathanSrebro.
Norm-basedcapacitycontrolinneuralnetworks.In
Con-
ferenceonLearningTheory
,pages1376Œ1401,2015.
BehnamNeyshabur,SrinadhBhojanapalli,David
McAllester,andNathanSrebro.Apac-bayesian
approachtospectrally-normalizedmarginboundsfor
neuralnetworks.
arXivpreprintarXiv:1707.09564
,
2017a.
BehnamNeyshabur,SrinadhBhojanapalli,David
McAllester,andNatiSrebro.Exploringgeneralization
indeeplearning.In
AdvancesinNeuralInformation
ProcessingSystems
,pages5949Œ5958,2017b.
QuynhNguyenandMatthiasHein.Thelosssurface
ofdeepandwideneuralnetworks.
arXivpreprint
arXiv:1704.08045
,2017a.
QuynhNguyenandMatthiasHein.Thelosssurfaceand
expressivityofdeepconvolutionalneuralnetworks.
arXiv
preprintarXiv:1710.10928
,2017b.
G
´
aborPataki.Ontherankofextremematricesinsemidef-
initeprogramsandthemultiplicityofoptimaleigenval-
ues.
Mathematicsofoperationsresearch
,23(2):339Œ358,
1998.
G
´
aborPataki.Thegeometryofprogramming.
In
Handbookofprogramming
,pages29Œ65.
Springer,2000.
AnkitPensia,VarunJog,andPo-LingLoh.Generaliza-
tionerrorboundsfornoisy,iterativealgorithms.
arXiv
preprintarXiv:1801.04295
,2018.
TimothyPoston,C-NLee,YChoie,andYonghoonKwon.
Localminimaandbackpropagation.In
NeuralNetworks,
1991.,IJCNN-91-SeattleInternationalJointConference
on
,volume2,pages173Œ176.IEEE,1991.
ItaySafranandOhadShamir.Spuriouslocalminimaare
commonintwo-layerreluneuralnetworks.
arXivpreprint
arXiv:1712.08968
,2017.
HanieSedghiandAnimaAnandkumar.Provablemeth-
odsfortrainingneuralnetworkswithsparseconnectivity.
arXivpreprintarXiv:1412.2693
,2014.
DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,
LaurentSifre,GeorgeVanDenDriessche,JulianSchrit-
twieser,IoannisAntonoglou,VedaPanneershelvam,
MarcLanctot,etal.Masteringthegameofgowith
deepneuralnetworksandtreesearch.
Nature
,529(7587):
484Œ489,2016.
MohammadrezaSoltaniandChinmayHegde.Towards
provablelearningofpolynomialneuralnetworksusing
low-rankmatrixestimation.2017.
MahdiSoltanolkotabi,AdelJavanmard,andJasonDLee.
Theoreticalinsightsintotheoptimizationlandscapeof
over-parameterizedshallowneuralnetworks.
arXiv
preprintarXiv:1707.04926
,2017.
DanielSoudryandEladHoffer.Exponentiallyvanishing
sub-optimallocalminimainmultilayerneuralnetworks.
arXivpreprintarXiv:1702.05777
,2017.
NathanSrebroandAdiShraibman.Rank,trace-normand
max-norm.In
InternationalConferenceonComputa-
tionalLearningTheory
,pages545Œ560.Springer,2005.
NathanSrebro,JasonRennie,andTommiSJaakkola.
Maximum-marginmatrixfactorization.In
Advancesin
neuralinformationprocessingsystems
,pages1329Œ1336,
2005.
MatusTelgarsky.ofdepthinneuralnetworks.
arXivpreprintarXiv:1602.04485
,2016.
YuandongTian.Ananalyticalformulaofpopulationgradi-
entfortwo-layeredReLUnetworkanditsapplicationsin
convergenceandcriticalpointanalysis.
arXivpreprint
arXiv:1703.00560
,2017.
JoelATroppetal.Anintroductiontomatrixconcentra-
tioninequalities.
FoundationsandTrends
R

inMachine
Learning
,8(1-2):1Œ230,2015.
ThomasWiatowski,PhilippGrohs,andHelmutB
¨
olcskei.
Energypropagationindeepconvolutionalneuralnet-
works.
IEEETransactionsonInformationTheory
,2017.
LeiWu,ZhanxingZhu,etal.Towardsunderstandinggener-
alizationofdeeplearning:Perspectiveoflosslandscapes.
arXivpreprintarXiv:1706.10239
,2017.
ChiyuanZhang,SamyBengio,MoritzHardt,Benjamin
Recht,andOriolVinyals.Understandingdeeplearn-
ingrequiresrethinkinggeneralization.
arXivpreprint
arXiv:1611.03530
,2016.
KaiZhong,ZhaoSong,andInderjitSDhillon.Learning
non-overlappingconvolutionalneuralnetworkswithmul-
tiplekernels.
arXivpreprintarXiv:1711.03440
,2017a.
KaiZhong,ZhaoSong,PrateekJain,PeterLBartlett,and
InderjitSDhillon.Recoveryguaranteesforone-hidden-
layerneuralnetworks.
arXivpreprintarXiv:1706.03175
,
2017b.
PanZhouandJiashiFeng.Thelandscapeofdeeplearning
algorithms.
arXivpreprintarXiv:1705.07038
,2017.
"
76,Motion Planning Networks,http://arxiv.org/pdf/1806.05767v2.pdf,https://github.com/ahq1993/MPNet,"MotionPlanningNetworks
AhmedH.Qureshi,AnthonySimeonov,MayurJ.BencyandMichaelC.Yip
Abstract
ŠFastandmotionplanningalgorithmsare
crucialformanystate-of-the-artroboticsapplicationssuchas
self-drivingcars.Existingmotionplanningmethodsbecome
ineffectiveastheircomputationalcomplexityincreasesexponen-
tiallywiththedimensionalityofthemotionplanningproblem.
Toaddressthisissue,wepresentMotionPlanningNetworks
(MPNet),aneuralnetwork-basednovelplanningalgorithm.
Theproposedmethodencodesthegivenworkspacesdirectly
fromapointcloudmeasurementandgeneratestheend-to-end
collision-freepathsforthegivenstartandgoal
WeevaluateMPNetonvarious2Dand3Denvironments
includingtheplanningofa7DOFBaxterrobotmanipulator.
TheresultsshowthatMPNetisnotonlyconsistentlycom-
putationallyinallenvironmentsbutalsogeneralizes
tocompletelyunseenenvironments.Theresultsalsoshowthat
thecomputationtimeofMPNetconsistentlyremainslessthan1
secondinallpresentedexperiments,whichislower
thanexistingstate-of-the-artmotionplanningalgorithms.
I.I
NTRODUCTION
Roboticmotionplanningaimstocomputeacollision-free
pathforthegivenstartandgoal[1].Asmotion
planningalgorithmsarenecessaryforsolvingavariety
ofcomplicated,high-dimensionalproblemsrangingfrom
autonomousdriving[2]tospaceexploration[3],therearises
acritical,unmetneedforcomputationallytractable,real-time
algorithms.Thequestfordevelopingcomputationallyef
cientmotionplanningmethodshasledtothedevelopmentof
varioussampling-basedmotionplanning(SMP)algorithms
suchasRapidly-exploringRandomTrees(RRT)[4],optimal
Rapidly-exploringRandomTrees(RRT*)[5],Potentially
guided-RRT*(P-RRT*)[6]andtheirbi-directionalvariants
[7],[8].Despitepreviouseffortstodesignfast,ef
planningalgorithms,thecurrentstate-of-the-artstrugglesto
offermethodswhichscaletothehigh-dimensionalsetting
thatiscommoninmanyreal-worldapplications.
Toaddresstheabove-mentionedchallenges,wepropose
aDeepNeuralNetwork(DNN)basediterativemotionplan-
ningalgorithm,calledMPNet(MotionPlanningNetworks)
thatefscalestohigh-dimensionalproblems.MPNet
consistsoftwocomponents:anencodernetworkanda
planningnetwork.Theencodernetworklearnstoencodea
pointcloudoftheobstaclesintoalatentspace.Theplanning
networklearnstopredicttherobotattime
step
t
+1
giventherobotattime
t
,goal
andthelatent-spaceencodingoftheobstacle
space.Oncetrained,MPNetcanbeusedinconjunction
withournovelbi-directionaliterativealgorithmtogenerate
A.H.Qureshi
1
,A.Simeonov
2
,M.J.Bency
1
,andM.C.Yip
1
;
2
arewith
(1)
DepartmentofElectricalandComputerEngineering;
(2)
Departmentof
MechanicalandAerospaceEngineering;UniversityofCaliforniaSanDiego,
LaJolla,CA92093USA.
f
a1qureshi,asimeono,mbency,
yip
g
@ucsd.edu
Fig.1:MPNetplannedmotionfora7DOFBaxterrobot
manipulator.Thepathfollowedbytherobotfrom
initialtotargetisshownthroughframes1-4.
Thestopwatchintheimagesshowtheexecutiontime.In
thisparticularcase,MPNettooklessthan1secondwhereas
BIT*[9]took3.1minutesonaveragetoafeasiblepath
solutionofcomparableeuclideancost.
feasibletrajectories.WeevaluateMPNetonalargetest
datasetincludingmultipleplanningproblemssuchasthe
planningofapoint-massrobot,rigid-body,and7DOFBax-
terrobotmanipulatorinvarious2Dand3Denvironments.
Asneuralnetworksdonotprovidetheoreticalguarantees
ontheirperformance,wealsoproposeahybridalgorithm
whichcombinesMPNetwithanyexistingclassicalplanning
algorithm,inourcaseRRT*.Thehybridplanningtechnique
demonstratesa100%successrateconsistentlyoveralltested
environmentswhileretainingthecomputationalgains.Our
resultsindicatethatMPNetgeneralizesverywell,notonly
tounseenstartandgoalwithinworkspaces
whichwereusedintraining,butalsotonewworkspaces
whichthealgorithmhasneverseen.
II.R
ELATED
W
ORK
Researchintodevelopingneuralnetwork-basedmotion
plannersgainedtractionintheearly1990sbutfaded
awayduetocomputationalcomplexityoftrainingdeep
neuralnetworks[10].However,recentdevelopmentsinDeep
Learning(DL)haveallowedresearcherstoapplyvariousDL
architecturestoroboticcontrolandplanning.
Anactiveareaofresearchwithinroboticcontrolandplan-
ningisDeepReinforcementLearning(DRL).Forinstance,
[11]showshowtotrainarobottolearnvisuomotorpolicies
toperformvarioustaskssuchasscrewingabottlecap,or
insertingapeg.AlthoughDRLisapromisingframework,it
arXiv:1806.05767v2  [cs.RO]  24 Feb 2019extensivelyreliesonexplorationthroughinteractionwiththe
environment,thusmakingitdiftotrainformanyreal-
worldroboticapplications.Arecentwork,ValueIteration
Networks(VIN)[12]emulatesvalueiterationbyleveraging
recurrentconvolutionalneuralnetworksandmax-pooling.
However,inadditiontolimitationsinheritedfromunderlying
DRLframework,VINhasonlybeenevaluatedonsimpletoy
problems,anditisnotclearhowVINcouldextendbeyond
suchenvironments.
Imitationlearningisanotheremerginginwhich
themodelsaretrainedfromexpertdemonstrations.Many
interestingproblemshavebeenaddressedthroughimitation
learning[13],[14],[15].Arecentmethod[16]usesdeep
neuralnetworkstrainedviaimitationtoadaptivelysamplethe
spaceforSMPmethods.Ourproposedmethod
alsolearnsthroughimitationbutunlike[16],itprovidesa
completefeasiblemotionplanforarobottofollow.
AnotherrecentandrelevantmethodistheLightning
Framework[17],whichiscomposedoftwomodules.The
moduleperformspathplanningusinganytraditional
motionplanner.Thesecondmodulemaintainsalookuptable
whichcachesoldpathsgeneratedbythemodule.For
newplanningproblems,theLightningFrameworkretrieves
theclosestpathfromalookuptableandrepairsitusingatra-
ditionalmotionplanner.Thisapproachdemonstratessuperior
performancecomparedtoconventionalplanningmethods.
However,notonlyarelookuptablesmemoryinefthey
alsoareincapableofgeneralizingtonewenvironments.
III.P
ROBLEM
D
EFINITION
Thissectiondescribesthenotationsusedinthispaperand
formallythemotionplanningproblemaddressedby
theproposedmethod.
Let
Q
beanorderedlistoflength
N
2
N
,thenasequence
f
q
i
=
Q
(
i
)
g
i
2
N
isamappingfrom
i
2
N
tothe
i
-thelement
of
Q
.Moreover,forthealgorithmsdescribedinthispaper,
Q
(end)
and
Q:
length()
givethelastelementandthenumber
ofelementsinaset
Q
,respectively.Let
X
ˆ
R
d
beagiven
statespace,where
d
2
N
isthedimensionalityofthestate
space.Theworkspacedimensionalityisindicatedby
d
w
2
N
.
Theobstacleandobstacle-freestatespacesareas
X
obs
ˆ
X
and
X
free
=
X
n
X
obs
,respectively.Lettheinitial
statebe
x
init
2
X
free
,andgoalregionbe
X
goal
ˆ
X
free
.
Letanorderedlist
˝
beapathhavingpositivescalarlength.
Asolutionpath
˝
tothemotionplanningproblemisfeasible
ifitconnects
x
init
and
x
2
X
goal
,i.e.
˝
(0)=
x
init
and
˝
(end)
2
X
goal
,andliesentirelyintheobstacle-freespace
X
free
.Theproposedworkaddressesthefeasibilityproblem
ofmotionplanning.
IV.MPN
ET
:AN
EURAL
M
OTION
P
LANNER
Thissectionintroducesourproposedmodel,MPNet
1
(see
Fig.2).MPNetisaneuralnetworkbasedmotionplanner
comprisedoftwophases:(A)oftrainingoftheneural
models,and(B)onlinepathgeneration.
1
Supplementarymaterialincludingimplementationparametersandproject
videosareavailableathttps://sites.google.com/view/mpnet/home.
A.OfTraining
Ourproposedmethodusestwoneuralmodelstosolvethe
motionplanningproblem.Themodelisanencodernet-
workwhichembedstheobstaclespointcloud,corresponding
toapointcloudrepresenting
X
obs
,intoalatentspace(see
Fig.2(a)).Thesecondmodelisaplanningnetwork(Pnet)
whichlearnstodomotionplanningforthegivenobstacle
embedding,andstartandgoaloftherobot
(seeFig.2(b)).
1)EncoderNetwork:
Theencodernetwork(Enet)embeds
theobstaclespointcloudintoafeaturespace
Z
2
R
m
with
dimensionality
m
2
N
.Enetcanbetrainedeitherusing
encoder-decoderarchitecturewithareconstructionlossor
inanend-to-endfashionwiththePnet(describedbelow).
Forencoder-decodertraining,wefoundthatthecontrative
autoencoders(CAE)[18]learnsrobustandinvariantfeature
spacerequiredforplanningandgenalizationtounseen
workspaces.ThereconstructionlossofCAEisas:
L
AE


e
;

d

=
1
N
obs
X
x
2
D
obs
jj
x

^
x
jj
2
+

X
ij
(

e
ij
)
2
(1)
where

e
aretheparametersofencoder,

d
aretheparam-
etersofdecoder,

isapenalizingcoef
D
obs
isa
datasetofpointclouds
x
2
X
obs
from
N
obs
2
N
different
workspaces,and
^
x
isthepointcloudreconstructedbythe
decoder.
2)PlanningNetwork:
Weuseafeed-forwarddeepneural
network,parameterizedby

,toperformplanning.Given
theobstaclesencoding
Z
,currentstate
x
t
andthegoal
state
x
T
,Pnetpredictsthenextstate
^
x
t
+1
2
X
free
which
wouldleadarobotclosertothegoalregion,i.e.,
^
x
t
+1
=
Pnet((
x
t
;
x
T
;
Z
);

)
TotrainPnet,anyplannerorhumanexpertcanprovide
feasible,near-optimalpathsasexpertdemonstrations.We
assumethepathsgivenbytheexpertdemonstratorarein
aformofatuple,
˝

=
f
x
0
;
x
1
;

;
x
T
g
,offeasiblestates
thatconnectthestartandgoalsothatthe
connectedpathliesentirelyin
X
free
.Thetrainingobjective
forthePnetistominimizethemean-squared-error(MSE)
lossbetweenthepredictedstates
^
x
t
+1
andtheactualstates
x
t
+1
givenbytheexpertdata,formalizedas:
L
Pnet
(

)=
1
N
p
^
N
X
j
T

1
X
i
=0
jj
^
x
j;i
+1

x
j;i
+1
jj
2
;
(2)
where
N
p
2
N
istheaveragingtermcorrespondingtothe
totalnumberofpaths,
^
N
2
N
,inthetrainingdatasettimes
thepathlengths.
B.OnlinePathPlanning
Theonlinephaseexploitstheneuralmodelsfromthe
ofphasetodomotionplanninginclutteredandcomplex
environments.TheoverallwofinformationbetweenEnet
andPnetisshowninFig.2(c).Togenerateend-to-endfea-
siblepathsconnectingthestartandgoalstates,wepropose
anovelincrementalbidirectionalpathgenerationheuristic.
Algorithm1presentstheoverallpathgenerationprocedure
anditsconstituentfunctionsaredescribedbelow.
(a)OfEncoderNetwork
(b)OfPlanningNetwork
(c)Online:NeuralPlanner
Fig.2:TheofandonlinephasesofMPNet.Thegreyshadedblocksindicatethetrainingobjectives.Theblueblocks
representsfrozenmodulesthatdonotundergoanytraining.
1)Enet:
Theencodernetwork
Enet(
x
obs
)
,trainedduring
theofphase,isusedtoencodetheobstaclespointcloud
x
obs
2
X
obs
intoalatentspace
Z
2
R
m
.
2)Pnet:
Pnetisafeed-forwardneuralnetworkfromthe
ofphasewhichtakes
Z
,currentstate
x
t
,goalstate
x
T
andpredictsthenextstateoftherobot
^
x
t
+1
.Toinculcate
stochasticityintothePnet,someofthehiddenunitsineach
ofitshiddenlayerweredroppedoutwithaprobability
p
:[0
;
1]
2
R
.Themeritofaddingthestochasticityduring
theonlinepathgenerationarepresentedinthediscussion
section.
3)LazyStatesContraction(LSC):
Givenapath
˝
=
f
x
0
;
x
1
;

;
x
T
g
,theLSCalgorithmconnectsthedirectly
connectablenon-consecutivestates,i.e.,
x
i
and
x
>i
+1
,and
removestheintermediate/lazystates.
4)Steering:
The
steerTo
functiontakestwostatesasan
inputandcheckseitherastraighttrajectoryconnectingthe
giventwostatesliesentirelyincollision-freespace
X
free
or
not.Thesteeringisdonefrom
x
1
to
x
2
insmall,discrete
stepsandcanbesummarizedas
˝
(

)=(1


)
x
1
+

x
2
;
8

2
[0
;
1]
.Thediscretestepsizecouldbeaednumberorcan
beadaptedfordifferentpartsofthealgorithm.
5)isFeasible:
Givenapath
˝
=
f
x
0
;
x
1
;

;
x
T
g
,this
procedurecheckseithertheend-to-endpath,formedby
connectingtheconsecutivestatesin
˝
,liesentirelyin
X
free
ornot.
Algorithm1:
MPNet(
x
init
;
x
goal
;
x
obs
)
1
Z
 
Enet(
x
obs
)
2
˝
 
NeuralPlanner(
x
init
;
x
goal
;
Z
);
3
if
˝
then
4
˝
 
LazyStatesContraction(
˝
)
5
if
IsFeasible(
˝
)
then
6
return
˝
7
else
8
˝
new
 
Replanning(
˝;
Z
)
9
˝
new
 
LazyStatesContraction(
˝
new
)
10
if
IsFeasible(
˝
new
)
then
11
return
˝
new
12
return
?
6)NeuralPlanner:
Thisisanincrementalbidirectional
pathgenerationheuristic(seeAlgorithm2fortheoutline).
Ittakestheobstacles'representation,
Z
,aswellasthestart
andgoalstatesasaninput,andoutputsapathconnecting
thetwogivenstates.Thesets
˝
a
and
˝
b
correspondtothe
pathsgeneratedfromthestartandgoalstates,respectively.
Thealgorithmstartswith
˝
a
,itgeneratesanewstate
x
new
,
usingPnet,fromstarttowardsthegoal(Line5),andchecks
ifapathfromstart
˝
a
isconnectabletothepathfromagoal
˝
b
(Line7).Ifpathsareconnectable,anend-to-endpath
˝
isreturnedbyconcatenating
˝
a
and
˝
b
.However,ifpaths
arenotconnectable,therolesof
˝
a
and
˝
b
areswapped
(Line11)andthewholeprocedureisrepeatedagain.The
swapfunctionenablesthebidirectionalgenerationofpaths,
i.e.,ifatanyiteration
i
,path
˝
a
isextendedtheninthe
nextiteration
i
+1
,path
˝
b
willbeextended.Thisway,two
trajectories
˝
a
and
˝
b
marchtowardseachotherwhichmakes
thispathgenerationheuristicgreedyandfast.
7)Replanning:
ThisprocedureisoutlinedintheAlgo-
rithm3.Ititeratesoveralltheconsecutivestates
x
i
and
x
i
+1
inagivenpath
˝
=
f
x
0
;
x
1
;

;
x
T
g
,andchecksif
theyareconnectableornot,where
i
=[0
;T

1]
ˆ
N
.If
anyconsecutivestatesarefoundnotconnectable,anewpath
isgeneratedbetweenthosestatesusingoneofthefollowing
replanningmethods(Line5).
a)NeuralReplanning:
Givenastartandgoalstates
Algorithm2:
NeuralPlanner(
x
start
;
x
goal
;
Z
)
1
˝
a
 f
x
start
g
;
˝
b
 f
x
goal
g
;
2
˝
 
?
;
3
Reached
 
False;
4
for
i
 
0
to
N
do
5
x
new
 
Pnet

Z
;˝
a
(end)
;˝
b
(end)

6
˝
a
 
˝
a
[f
x
new
g
7
Connect
 
steerTo

˝
a
(end)
;˝
b
(end)

8
if
Connect
then
9
˝
 
concatenate(
˝
a
;˝
b
)
10
return
˝
11
SWAP(
˝
a
;˝
b
)
12
return
?
Algorithm3:
Replanning(
˝;
Z
)
1
˝
new
 
?
;
2
for
i
 
0
to
˝:
length()
do
3
if
steerTo(
˝
i
;˝
i
+1
)
then
4
˝
new
 
˝
new
[f
˝
i
;˝
i
+1
g
5
else
6
˝
mini
 
Replanner(
˝
i
;˝
i
+1
;
Z
);
7
if
˝
mini
then
8
˝
new
 
˝
new
[
˝
mini
9
else
10
return
?
11
return
˝
new
togetherwithobstaclespaceencoding
Z
,thismethodre-
cursivelyanewpathbetweenthetwogivenstates.To
doso,itstartsbyndingacoarsepathbetweenthegiven
statesandthenifrequired,itreplansonalevelby
callingitselfoverthenon-connectableconsecutivestatesof
thenewpath.Thisrecursiveneuralreplanningisperformed
fortheednumberofstepstolimitthealgorithmwithin
thecomputationalbounds.
b)HybridReplanning:
Thisheuristiccombinestheneural
replanningwiththeclassicalmotionplanningmethods.It
performstheneuralreplanningfortheednumberofsteps.
Theresultingnewpathistestedforfeasibility.Ifapathis
notfeasible,thenon-connectablestatesinthenewpathare
thenconnectedusingaclassicalmotionplanner.
V.I
MPLEMENTATIONDETAILS
ThissectiongivestheimplementationdetailsofMPNet,
foradditionaldetailsrefertosupplementarymaterial.The
proposedneuralmodelswereimplementedinPyTorch[19].
ForenvironmentsotherthanBaxter,thebenchmarkmethods,
Informed-RRT*andBIT*,wereimplementedinPython,and
theirtimeswerecomparedagainsttheCPU-timeofMPNet.
TheBaxterenvironmentswereimplementedwithMoveIt!
[20]andROS.Intheseenvironments,weuseaC++OMPL
[21]implementationofBIT*tocompareagainstaC++
implementationofMPNet.Thesystemusedfortrainingand
testinghas3.40GHz

8IntelCorei7processorwith32GB
RAMandGeForceGTX1080GPU.Theremainingsection
explainsdifferentmodulesthatleadtoMPNet.
A.DataCollection
Wegenerate110differentworkspacesforeachpresented
case,i.e.,simple2D(s2D),rigid-body(rigid),complex
2D(c2D)and3D(c3D).Ineachoftheworkspaces,
5000collision-free,near-optimal,pathsweregeneratedusing
RRT*.Thetrainingdatasetcomprisedof100workspaces
with4000pathsineachworkspace.Fortesting,twotypes
oftestdatasetswerecreatedtoevaluatetheproposedand
benchmarkmethods.Thetestdataset,seen-
X
obs
,com-
prisedofalreadyseen100workspaceswith200unseenstart
andgoalineachworkspace.Thesecondtest
dataset,unseen-
X
obs
,comprisedofcompletelyunseen10
workspaceswhereeachcontained2000unseenstartand
goalIntheBaxterexperiments,wecreated
adatasetcomprisedoftenchallengingsimulatedenviron-
ments,andweshowtheexecutionontherealrobot.Foreach
environment,wecollected900pathsfortrainingand100
pathsfortesting.Theobstaclepointcloudswereobtained
usingaKinectdepthcamerawiththePCL[22]andpcl
ros
2
package.
B.ModelsArchitecture
1)EncoderNetwork:
ForallenvironmentsexceptBaxter,
weuseencoder-decodertraining,whereasforBaxter,we
traintheencoderandplanningnetworkend-to-end.Since
thedecoderisusuallytheinverseoftheencoder,weonly
describetheencoder'sstructure.Theencodingfunction
Enet(
x
obs
)
comprisedofthreelinearlayersandanoutput
layer,whereeachlinearlayerisfollowedbytheParametric
LinearUnit(PReLU)[23].Theinputtotheencoder
isavectorofpointcloudsofsize
N
pc

d
w
where
N
pc
is
thenumberofdatapoints,and
d
w
2
N
isthedimensionof
aworkspace.
2)PlanningNetwork:
PNetis9-layersand12-layers
DNNforBaxterandotherenvironments,respectively.We
useParametricLinearUnit(PReLU)[23]fornon-
linearity.Toaddstochasticity,weDropout(p)[24]inall
hiddenlayersexceptthelastone.Inpoint-massandrigid-
bodycases,wethepretrainedencoderparameters,since
theyweretrainedusingtheencoder-decodermethod,anduse
themtocomputeenvironmentencoding,whereas,forBaxter
environments,wetraintheEnetandPnetend-to-end.
VI.R
ESULTS
Inthissection,wecomparetheperformanceofMP-
NetwithNeural-Replanning(MPNet:NR)andHybrid-
Replanning(MPNet:HR)againststate-of-the-artmotion
planningmethods,i.e.,Informed-RRT*andBIT*,forthe
motionplanningofthe2D/3Dpoint-massrobots,rigid-
body,andBaxter7DOFmanipulatorinthe2Dand3D
environments.
Figs.3showdifferentexamplescenarioswhereMPNet
andexpertplanner,inthiscaseRRT*,providedsuccessful
paths.Theredandbluecoloredtrajectoriesindicatethepaths
generatedbyMPNetandRRT*,respectively.Thegoalregion
isindicatedasabrowncoloreddisk.Themeancomputational
timefortheMPNetandRRT*isdenotedas
t
MP
and
t
R
,
respectively.WeseethatMPNetisabletocomputenear-
optimalpathsforbothpoint-massandrigid-bodyrobotin
considerablylesstimethanRRT*.
TableIpresentstheCPU-timecomparisonofMPNet:
NPandMPNet:HRagainstInformed-RRT*[25]andBIT*
[9]overthetwotestdatasets,i.e.,seen-
X
obs
andunseen-
X
obs
.Wereportthemeantimeswithstandarddeviation
ofallalgorithmsfortheinitialpathsinagiven
problem.Forinitialpaths,itisobservedthatonaverage
thepathlengthsofbenchmarkmethodswerehigherthan
2
http://wiki.ros.org/pcl
ros
(a)
t
R
=6
:
9
s;t
MP
=0
:
50
s
(b)
t
R
=6
:
9
s;t
MP
=0
:
50
s
(c)
t
R
=6
:
9
s;t
MP
=0
:
50
s
(d)
t
R
=5
:
3
s;t
MP
=0
:
44
s
Fig.3:MPNet(Red)andRRT*(Blue)planningpathsincomplex3Denvironments(c3D).
Environment
Testcase
MPNet(NR)
MPNet(HR)
Informed-RRT*
BIT*
BIT:
t
mean
MPNet(NR):
t
mean
Simple2D
Seen
X
obs
0
:
11

0
:
037
0
:
19

0
:
14
5
:
36

0
:
34
2
:
71

1
:
72
24.64
Unseen
X
obs
0
:
11

0
:
038
0
:
34

0
:
21
5
:
39

0
:
18
2
:
63

0
:
75
23.91
Complex2D
Seen
X
obs
0
:
17

0
:
058
0
:
61

0
:
35
6
:
18

1
:
63
3
:
77

1
:
62
22.17
Unseen
X
obs
0
:
18

0
:
27
0
:
68

0
:
41
6
:
31

0
:
85
4
:
12

1
:
99
22.89
Complex3D
Seen
X
obs
0
:
48

0
:
10
0
:
34

0
:
14
14
:
92

5
:
39
8
:
57

4
:
65
17.85
Unseen
X
obs
0
:
44

0
:
107
0
:
55

0
:
22
15
:
54

2
:
25
8
:
86

3
:
83
20.14
Rigid
Seen
X
obs
0
:
32

0
:
28
1
:
92

1
:
30
30
:
25

27
:
59
11
:
10

5
:
59
34.69
Unseen
X
obs
0
:
33

0
:
13
1
:
98

1
:
85
30
:
38

12
:
34
11
:
91

5
:
34
36.09
TABLEI:TimecomparisonofMPNet(NR:NeuralReplanning;HR:HybridReplanning),Informed-RRT*andBIT*on
twotestdatasets.NoteintherightmostcolumnthatMPNetisatleast
20

fasterthanBIT*.
thepathlengthsofMPNet.Itcanbeseenthatinalltest
cases,themeancomputationtimeofMPNetwithneural
andhybridreplanningremainedaround1second.Themean
computationtimeofInformed-RRT*andBIT*increases
asthedimensionalityofplanningproblemis
increased.Notethat,onaverage,MPNetisabout40and20
timesfasterthanInformed-RRT*andBIT*,respectively,in
alltestcasesandconsistentlydemonstrateslowcomputa-
tionaltimeirrespectiveofthedimensionalityoftheplanning
problem.Intheseexperiments,themeanaccuracyofMPNet:
HRandMPNet:NPwas100%and97%withthestandard
deviationofabout0.4%overvedifferenttrials.
Fromexperimentspresentedsofar,itisevidentthatBIT*
outperformsInformed-RRT*,therefore,inthefollowing
experimentsonlyMPNetandBIT*arecompared.Fig.4
comparesthemeancomputationtimeofMPNet:NPand
BIT*inourtwotestdatasets.Itcanbeseenthatthemean
computationtimeofMPNetstaysaround1secondirrespec-
tiveoftheplanningproblemdimensionality.Furthermore,the
meancomputationaltimeofBIT*notonlybutalso
increasesintherigid-bodyplanningproblem.
Finally,Fig.1showsasingle7DOFarmoftheBaxterrobot
executingamotionplannedbyMPNetforagivenstartand
goalInFig.1,theroboticmanipulatorisat
thestartandtheshadowedregionindicatesthe
manipulatoratthetargetOntheBaxter'stest
dataset,MPNettookabout1secondonaveragewith85%
successrate.BIT*tookabout9secondsonaveragewitha
successrateof56%topathswithina40%rangeof
thepathlengthsfoundbyMPNet,andwasfoundtotake
uptoseveralminutestopathswithinthe10%rangeof
averageMPNetpathlengths.
VII.D
ISCUSSION
A.StochasticitythroughDropout
OurmethodusesDropout[24]duringbothonlineand
ofexecution.Dropoutisappliedlayer-wisetotheneural
networkanditdropseachunitinthehiddenlayerwitha
probability
p
2
[0
;
1]
,inourcase
p
=0
:
5
.Theresulting
neuralnetworkisathinnednetworkandisessentially
differentfromtheactualneuralmodel[24].
Notethat,intheneuralreplanningphase,MPNetiterates
overthenon-connectableconsecutivestatesofthecoarse
pathtodomotionplanningonalevelandthus,
producesanewpath.Thereplanningprocedureiscalled
recursivelyoneachofitsownnewlygeneratedpathsuntil
afeasiblesolutionisfoundoralooplimitisreached.
DropoutaddsstochasticitytothePnetwhichimpliesthat
oneachreplanningstep,thePnetwouldgeneratedifferent
pathsfrompreviousre-planningsteps.Thisphenomenonis
evidentfromFig.5wherethePnetgenerateddifferentpaths
foraedstartandgoalTheseperturbations
ingeneratedpathsforedstartandgoalhelpinrecovery
fromthefailure.Thus,addingDropoutincreasestheoverall
performanceofMPNet.Furthermore,duetostochasticityby
Dropout,ourmethodcanalsobeusedtogenerateadaptive
samplesforsamplingbasedmotionplanners[26].
B.Completeness
Intheproposedmethod,acoarsepathiscomputedbya
neuralnetwork.Ifacoarsepathisfoundtobenotfully
connectable,are-planningheuristicisexecutedtorepair
thenon-connectablepathsegmentstoprovideanend-to-
endcollision-freepath.Thecompletenessguaranteesfor
theproposedmethoddependsontheunderlinereplanning
(a)Test-case1:seen-
X
obs
(b)Test-case2:unseen-
X
obs
Fig.4:ComputationaltimecomparisonofMPNetandRRT*
ontestdatasets.TheplotsshowMPNetismoreconsistent
andfasterthanBIT*inalltestcases.
heuristic.Theclassicalmotionplannerbasedreplanning
methodsarepresentedtoguaranteethecompletenessofthe
proposedmethod.SinceweuseRRT*,ourproposedmethod
inheritstheprobabilisticcompletenessofRRTsandRRT*[5]
whileretainingthecomputationalgains.
C.ComputationalComplexity
Thissectionformallyhighlightsthecomputationalcom-
plexityoftheproposedmethod.Neuralnetworksareknown
tohaveonlineexecutioncomplexityof
O
(1)
.Therefore,the
executionoflines1-2ofAlgorithm1willhaveacomplexity
nogreaterthan
O
(1)
.Thelazystatecontraction(LSC)
heuristicisasimplepathsmoothingtechniquewhichcan
beexecutedinaednumberofiterationasafeasible
trajectorymusthavealength.Also,notethattheLSC
isnotanessentialcomponentoftheproposedmethod.Its
inclusionhelpstogeneratenear-optimalpaths.Thecompu-
tationalcomplexityofthereplanningheuristicdependson
themotionplannerusedforreplanning.Weproposedthe
Fig.5:MPNetgeneratesmultiplecollision-freepaths(red)
betweenedstart(green)andgoalpairs(blue)ina
timeduetoitsstochasticbehavior.
neuralreplanningandhybridreplanningmethods.Sincethe
neuralreplannerisexecutedforednumberofsteps,the
complexityis
O
(1)
.Fortheclassicalmotionplanner,we
useRRT*whichhas
O
(
nlogn
)
complexity,where
n
isthe
numberofsamplesinthetree[5].Hence,forhybridreplan-
ning,wecanconcludethattheproposedmethodhasaworst
casecomplexityof
O
(
nlogn
)
andabestcasecomplexityof
O
(1)
.Notethat,MPNet:NRisabletocomputecollision-
freepathsformorethan97%ofthecases,presentedinTable
I.Therefore,itcanbesaidthatMPNetwillbeoperatingwith
O
(1)
mostofthetimeexceptfornearly3%caseswherethe
RRT*needstobeexecutedforasmallsegmentofoverall
pathgivenbyMPNet:NR.ThisexecutionofRRT*onsmall
segmentsofaglobalpathreducesthecomplicatedproblemto
asimpleplanningproblemwhichmakestheRRT*execution
computationallyacceptableandpracticallymuchlessthan
O
(
nlogn
)
.
VIII.C
ONCLUSIONSAND
F
UTUREWORK
Inthispaper,wepresentafastandefNeural
MotionPlannercalledMPNet.MPNetconsistsofanen-
codernetworkthatencodesthepointcloudofarobot's
surroundingsintoalatentspace,,andaplanningnetwork
thattakestheenvironmentencoding,andstartandgoal
robotictooutputacollision-freefeasiblepath
connectingthegivenTheproposedmethod
(1)plansmotionsirrespectiveoftheobstaclesgeometry,
(2)demonstratesmeanexecutiontimeofabout1second
inallpresentedexperiments,(3)generalizestonewunseen
obstaclelocations,and(4)hascompletenessguarantees.
Inourfuturework,weplantoextendMPNettobuild
learning-basedactor-criticmotionplanningmethodsbycom-
biningitwithproxycollisioncheckerssuchastheFastron
algorithm[27],[28].Anotherinterestingextensionwouldbe
toaddressthechallengeofkinodynamicmotionplanningin
dynamicallychangingenvironments.
R
EFERENCES
[1]
S.M.LaValle,
Planningalgorithms
.Cambridgeuniversitypress,
2006.
[2]
T.Lozano-Perez,
Autonomousrobotvehicles
.SpringerScience&
BusinessMedia,2012.
[3]
R.Volpe,ﬁRoverfunctionalautonomydevelopmentforthemarsmo-
bilesciencelaboratory,ﬂin
Proceedingsofthe2003IEEEAerospace
Conference
,vol.2,2003,pp.643Œ652.
[4]
S.M.LaValle,ﬁRapidly-exploringrandomtrees:Anewtoolforpath
planning,ﬂ1998.
[5]
S.KaramanandE.Frazzoli,ﬁSampling-basedalgorithmsforoptimal
motionplanning,ﬂ
Theinternationaljournalofroboticsresearch
,
vol.30,no.7,pp.846Œ894,2011.
[6]
A.H.QureshiandY.Ayaz,ﬁPotentialfunctionsbasedsampling
heuristicforoptimalpathplanning,ﬂ
AutonomousRobots
,vol.40,
no.6,pp.1079Œ1093,2016.
[7]
ŠŠ,ﬁIntelligentbidirectionalrapidly-exploringrandomtreesfor
optimalmotionplanningincomplexclutteredenvironments,ﬂ
Robotics
andAutonomousSystems
,vol.68,pp.1Œ11,2015.
[8]
Z.Tahir,A.H.Qureshi,Y.Ayaz,andR.Nawaz,ﬁPotentiallyguided
bidirectionalizedrrt*forfastoptimalpathplanningincluttered
environments,ﬂ
RoboticsandAutonomousSystems
,vol.108,pp.13Œ
27,2018.
[9]
J.D.Gammell,S.S.Srinivasa,andT.D.Barfoot,ﬁBatchinformed
trees(bit*):Sampling-basedoptimalplanningviatheheuristically
guidedsearchofimplicitrandomgeometricgraphs,ﬂin
Roboticsand
Automation(ICRA),2015IEEEInternationalConferenceon
.IEEE,
2015,pp.3067Œ3074.
[10]
J.Schmidhuber,ﬁDeeplearninginneuralnetworks:Anoverview,ﬂ
Neuralnetworks
,vol.61,pp.85Œ117,2015.
[11]
S.Levine,C.Finn,T.Darrell,andP.Abbeel,ﬁEnd-to-endtraining
ofdeepvisuomotorpolicies,ﬂ
JournalofMachineLearningResearch
,
vol.17,no.39,pp.1Œ40,2016.
[12]
A.Tamar,Y.Wu,G.Thomas,S.Levine,andP.Abbeel,ﬁValue
iterationnetworks,ﬂin
AdvancesinNeuralInformationProcessing
Systems
,2016,pp.2154Œ2162.
[13]
M.Bojarski,D.DelTesta,D.Dworakowski,B.Firner,B.Flepp,
P.Goyal,L.D.Jackel,M.Monfort,U.Muller,J.Zhang
etal.
,ﬁEndto
endlearningforself-drivingcars,ﬂ
arXivpreprintarXiv:1604.07316
,
2016.
[14]
S.Calinon,F.D'halluin,E.L.Sauser,D.G.Caldwell,andA.G.
Billard,ﬁLearningandreproductionofgesturesbyimitation,ﬂ
IEEE
Robotics&AutomationMagazine
,vol.17,no.2,pp.44Œ54,2010.
[15]
R.Rahmatizadeh,P.Abolghasemi,A.Behal,andL.B
¨
ol
¨
oni,ﬁLearning
realmanipulationtasksfromvirtualdemonstrationsusinglstm,ﬂ
arXiv
preprint
,2016.
[16]
B.Ichter,J.Harrison,andM.Pavone,ﬁLearningsamplingdistributions
forrobotmotionplanning,ﬂin
2018IEEEInternationalConference
onRoboticsandAutomation(ICRA)
.IEEE,2018,pp.7087Œ7094.
[17]
D.Berenson,P.Abbeel,andK.Goldberg,ﬁArobotpathplanning
frameworkthatlearnsfromexperience,ﬂin
RoboticsandAutomation
(ICRA),2012IEEEInternationalConferenceon
.IEEE,2012,pp.
3671Œ3678.
[18]
S.Rifai,P.Vincent,X.Muller,X.Glorot,andY.Bengio,ﬁContrac-
tiveauto-encoders:Explicitinvarianceduringfeatureextraction,ﬂin
Proceedingsofthe28thInternationalConferenceonInternational
ConferenceonMachineLearning
.Omnipress,2011,pp.833Œ840.
[19]
A.Paszke,S.Gross,S.Chintala,G.Chanan,E.Yang,Z.DeVito,
Z.Lin,A.Desmaison,L.Antiga,andA.Lerer,ﬁAutomaticdifferen-
tiationinpytorch,ﬂ2017.
[20]
I.A.S¸ucanandS.Chitta,ﬁMoveit!,[online]available:
http://moveit.ros.org.ﬂ
[21]
I.A.S¸ucan,M.Moll,andL.E.Kavraki,ﬁTheOpenMotionPlanning
Library,ﬂ
IEEERobotics&AutomationMagazine
,vol.19,no.4,pp.
72Œ82,December2012,http://ompl.kavrakilab.org.
[22]
R.B.RusuandS.Cousins,ﬁ3Dishere:PointCloudLibrary(PCL),ﬂin
IEEEInternationalConferenceonRoboticsandAutomation(ICRA)
,
Shanghai,China,May9-132011.
[23]
L.Trottier,P.Gigu
˚
ere,andB.Chaib-draa,ﬁParametricexponential
linearunitfordeepconvolutionalneuralnetworks,ﬂ
arXivpreprint
arXiv:1605.09332
,2016.
[24]
N.Srivastava,G.E.Hinton,A.Krizhevsky,I.Sutskever,and
R.Salakhutdinov,ﬁDropout:asimplewaytopreventneuralnetworks
fromovng.ﬂ
Journalofmachinelearningresearch
,vol.15,no.1,
pp.1929Œ1958,2014.
[25]
J.D.Gammell,S.S.Srinivasa,andT.D.Barfoot,ﬁInformedrrt*:
Optimalsampling-basedpathplanningfocusedviadirectsamplingof
anadmissibleellipsoidalheuristic,ﬂin
IntelligentRobotsandSystems
(IROS2014),2014IEEE/RSJInternationalConferenceon
.IEEE,
2014,pp.2997Œ3004.
[26]
A.H.QureshiandM.C.Yip,ﬁDeeplyinformedneuralsamplingfor
robotmotionplanning,ﬂin
2018IEEE/RSJInternationalConference
onIntelligentRobotsandSystems(IROS)
.IEEE,2018,pp.6582Œ
6588.
[27]
N.DasandM.Yip,ﬁLearning-basedproxycollisiondetectionfor
robotmotionplanningapplications,ﬂ
arXivpreprintarXiv:1902.08164
,
2019.
[28]
N.Das,N.Gupta,andM.Yip,ﬁFastron:Anonlinelearning-based
modelandactivelearningstrategyforproxycollisiondetection,ﬂin
ConferenceonRobotLearning
,2017,pp.496Œ504.
[29]
J.Kirkpatrick,R.Pascanu,N.Rabinowitz,J.Veness,G.Desjardins,
A.A.Rusu,K.Milan,J.Quan,T.Ramalho,A.Grabska-Barwinska
etal.
,ﬁOvercomingcatastrophicforgettinginneuralnetworks,ﬂ
Pro-
ceedingsoftheNationalAcademyofSciences
,vol.114,no.13,pp.
3521Œ3526,2017.
[30]
J.Duchi,E.Hazan,andY.Singer,ﬁAdaptivesubgradientmethods
foronlinelearningandstochasticoptimization,ﬂ
JournalofMachine
LearningResearch
,vol.12,no.Jul,pp.2121Œ2159,2011.
"
77,PAC-Bayes Control: Learning Policies that Provably Generalize to Novel Environments,https://arxiv.org/pdf/1806.04225v5.pdf,https://github.com/irom-lab/PAC-Bayes-Control,"PAC-BayesControl:LearningPoliciesthat
ProvablyGeneralizetoNovelEnvironments
AnirudhaMajumdar
1
,AlecFarid
2
,andAnoopkumarSonar
3
1,2
DepartmentofMechanicalandAerospaceEngineering
3
DepartmentofComputerScience
PrincetonUniversity
Princeton,NJ,08544,USA
Emails:
f
ani.majumdar,afarid,asonar
g
@princeton.edu
August27,2020
Abstract
Ourgoalistolearncontrolpoliciesforrobotsthatprovablygeneralizewelltonovelenviron-
mentsgivenadatasetofexampleenvironments.Thekeytechnicalideabehindourapproachis
toleveragetoolsfrom
generalizationtheory
inmachinelearningbyexploitingapreciseanalogy
(whichwepresentintheformofareduction)betweengeneralizationofcontrolpoliciestonovel
environmentsandgeneralizationofhypothesesinthesupervisedlearningsetting.Inparticular,
weutilizethe
ProbablyApproximatelyCorrect(PAC)-Bayes
framework,whichallowsustoob-
tainupperboundsthatholdwithhighprobabilityontheexpectedcostof(stochastic)control
policiesacrossnovelenvironments.Weproposepolicylearningalgorithmsthatexplicitlyseek
tominimizethisupperbound.Thecorrespondingoptimizationproblemcanbesolvedusing
convex
optimization(
RelativeEntropyProgramming
inparticular)inthesettingwhereweare
optimizingoverapolicyspace.Inthemoregeneralsettingofcontinuouslyparameterized
policies(e.g.,neuralnetworkpolicies),weminimizethisupperboundusingstochasticgradient
descent.Wepresentsimulatedresultsofourapproachappliedtolearning(1)reactiveobstacle
avoidancepoliciesand(2)neuralnetwork-basedgraspingpolicies.Wealsopresenthardware
resultsfortheParrotSwingdronenavigatingthroughtobstacleenvironments.Our
examplesdemonstratethepotentialofourapproachtoprovidestronggeneralizationguaran-
teesforroboticsystemswithcontinuousstateandactionspaces,complicated(e.g.,nonlinear)
dynamics,richsensoryinputs(e.g.,depthimages),andneuralnetwork-basedpolicies.
1Introduction
Imagineanunmannedaerialvehiclethatsuccessfullynavigatesathousanddierentobstacleenvi-
ronmentsoraroboticmanipulatorthatsuccessfullygraspsamillionobjectsinourdataset.How
likelyarethesesystemstosucceedonanovel(i.e.,previouslyunseen)environmentorobject?How
canweexplicitlylearncontrolpoliciesthatprovablygeneralizewelltoenvironmentsorobjects
thatourrobothasnotpreviouslyencountered?Currentapproachesfordesigningcontrolpolicies
1
arXiv:1806.04225v5  [cs.RO]  25 Aug 2020(a)
(b)
Figure1
:
Wedemonstrateourapproachforlearning(i)reactiveobstacleavoidancepoliciesforatialdrive
groundvehiclemodelequippedwithadepthsensor,and(ii)neuralnetwork-basedgraspingpoliciesforamanipulator
modelequippedwithanRGB-Dsensor.Ourapproachprovidesstrongguaranteesontheperformanceofthelearned
policiesonnovelenvironmentsevenwitharelativelysmallnumberoftrainingenvironments(e.g.,aguaranteed
expectedcollision-freetraversalrateof87
:
9%using1000trainingenvironmentsfortheobstacleavoidanceexample
andaguaranteedexpectedsuccessrateof70
:
6%forthegraspingexampleusing2000trainingobjects).
(a)
(b)
Figure2
:
Picturedin(a)isaParrotSwingdrone|ahybridvehicle.Wedemonstrate
ourapproachforlearningreactiveobstacleavoidancepoliciesfortheSwinggivenasimulateddepthsensor.Our
approachprovidesaguaranteedexpectedcollision-freetraversalrateof88.6%onnovelenvironmentsusing1000
simulatedtrainingenvironments.Whentestingonunseenenvironmentswithinthenettedareapicturedin(b),the
Swingsucceedsin18/20trials.Videosofrepresentativetrialscanbefoundathttps://youtu.be/p5CjcSsojg8.
forroboticsystemseitherdonotprovidesuchguaranteesongeneralizationorprovideguaran-
teesonlyunderveryrestrictiveassumptions(e.g.,strongassumptionsonthegeometryofanovel
environment[69,29,2,51]).
Thegoalofthispaperistodevelopanapproachforlearningcontrolpoliciesforroboticsystems
thatprovablygeneralizewellwithhighprobabilitytonovelenvironmentsgivenadatasetofexample
environments.Thekeyconceptualideaforenablingthisistoestablishapreciseanalogybetween
generalizationofpoliciestonovelenvironmentsandgeneralizationinsupervisedlearning.This
analogyallowsustotranslatetechniquesforlearninghypotheseswithgeneralizationguarantees
2
inthesupervisedlearningsettingintotechniquesforlearningcontrolpoliciesforrobottaskswith
performanceguaranteesonnovelenvironments.
Inordertoobtainmoreinsightintothisanalogy,supposewehaveadatasetof
N
objects.A
simpleapproachtolearningagraspingpolicyistosynthesizeonethatachievesthebestpossible
performanceonthese
N
objects.However,suchastrategymightresultinanoverlycomplexpolicy
thatovstothespobjectsathand.Thisisaparticularlyimportantchallengeforrobotics
applicationssincedatasetsaregenerallyrelativelysmall(e.g.,ascomparedtotrainingsetsfor
imagetasks).Inordertolearnapolicythatgeneralizeswelltonovelenvironments,
wemayneedtoadda\regularizer""thatpenalizesthe\complexity""ofthepolicy.Thisraisesthe
followingquestions:(1)whatformshouldthisregularizertake?;and(2)canweprovideaformal
guaranteeontheperformanceoftheresultingpolicyonnovelenvironments?
Theanalogousquestionsfor
supervised
learningalgorithmshavebeenextensivelystudiedinthe
literatureon
generalizationtheory
inmachinelearning.HereweleveragePAC-Bayestheory(Prob-
ablyApproximatelyCorrectBayes)[54],whichprovidessomeofthetightestknowngeneralization
boundsforclassicalsupervisedlearningapproaches[45,71,32].Veryrecently,PAC-Bayesanalysis
hasalsobeenusedtotraindeepneuralnetworkswithguaranteesongeneralizationperformance
[22,60,61].Aswewillsee,wecanleveragePAC-Bayestheorytoprovidepreciseanswerstoboth
questionsposedabove;itwillallowustospecifyaregularizerfordesigning(stochastic)control
policiesthatprovablygeneralizewell(withhighprobability)tonovelenvironments.
1.1StatementofContributions
Theprimarycontributionofthispaperistointroduceaframeworkforproviding
generalization
guarantees
forlearning-basedcontrolofrobots.Whilegeneralizationboundshavebeenstudied
extensivelyintheliteratureonsupervisedlearning(asdiscussedabove),therehasbeenrelatively
littleworkonthistopicintheliteratureonrobotlearning(seeSection1.2forathoroughlitera-
turereview).Toourknowledge,theresultsinthispaperconstitutetheattempttoprovide
generalizationguaranteesonlearning-basedcontrolpoliciesforroboticsystemswithcontinuous
stateandactionspaces,complicated(e.g.,nonlinearorhybrid)dynamics,andrichsensoryinputs
(e.g.,RGB-Dimages).Tothisend,thispapermakesfourspcontributions.First,weprovide
a
reduction
thatallowsustotranslategeneralizationboundsforsupervisedlearningproblemsto
generalizationboundsforcontrolpolicies.WeapplythisreductiontotranslatePAC-Bayesbounds
tothecontrolsettingweconsiderhere(Section4).Second,weproposelearningalgorithmsthat
minimizetheregularizedcostfunctionsspbyPAC-Bayestheoryinordertosynthesizecon-
trolpolicieswithgeneralizationguarantees(Section5).Inthesettingwhereweareoptimizingover
apolicyspace(Section5.1),thecorrespondingoptimizationproblemcanbesolvedusing
convex
optimizationtechniques(
RelativeEntropyPrograms(REPs)
inparticular).Inthemore
generalsettingofcontinuously-parameterizedpolicies(Section5.2),werelyonstochasticgradient
descenttoperformtheoptimization.Third,inSection6.2wepresentanextensionofourbasicap-
proachthatallowsustolearnpoliciesthataredistributionallyrobust(i.e.,handlesettingswhere
testenvironmentsaredrawnfromatdistributionthantrainingenvironments).Fourth,
wedemonstrateourapproachinsimulationforlearning(i)depthsensor-basedreactiveobstacle
avoidancepoliciesforthegroundrobotmodelshowninFigure1(a)(Section7.1),and(ii)neural
network-basedgraspingpoliciesforthemanipulatormodelshowninFigure1(b)(Section7.2).
Finally,wealsopresenthardwareresultsforreactiveobstacleavoidancecontrolwiththeParrot
SwingdroneshowninFigure2(a)(Section8).Oursimulationandhardwareresultsdemonstrate
3
thatweareabletoobtainstronggeneralizationguaranteesevenwitharelativelysmallnumberof
trainingenvironments.WecomparetheboundsobtainedfromPAC-Bayestheorywithexhaustive
samplingtoillustratethetightnessofthebounds.
Apreliminaryversionofthiswork[50]waspresentedattheConferenceonRobotLearning
(CoRL)2018.Inthistlyrevisedandextendedversion,weadditionallypresent:(i)an
extensionofourbasicapproachforprovidinggeneralizationguaranteesinsettingswheretesten-
vironmentsaredrawnfromatdistributiontotrainingenvironments(Section6.2),(ii)an
applicationofourframeworkforlearningneural-networkbasedgraspingpolicies(Section7.2),(iii)
amethodforhandlingstochasticdynamics(Section6.1),(iv)hardwareimplementationofthedepth
sensor-basedreactiveobstacleavoidancepolicies(Section8),and(v)amorethoroughdiscussion
ofchallengesassociatedwithourapproachandpromisingfuturedirections(Section9).
1.2RelatedWork
Oneapproachforsynthesizingcontrolpolicieswithguaranteedperformanceistoleveragerobust
controltechniques(e.g.,ycontrol[30]orchance-constrainedprogramming[15,9,76,65]).
However,suchtechniquestypicallyrequireanexplicitdescriptionoftheuncertaintythe
system.Whileuncertaintymodelsfortherobot'sdynamicsormeasurementscanoftenbeobtained
viasystemiden,assuminganuncertaintymodelfortheenvironment(e.g.,adistribution
overallpossibleenvironmentgeometries)isunrealistic.Onewaytoaddressthisistoassume
thatanovelenvironmentconditionsthatallowareal-timeplannerto
always
succeed.
Forexample,inthecontextofnavigation,thisconstraintcouldbebyhand-codingemer-
gencymaneuvers(e.g.,stoppingmaneuversorloitercircles)thatarealwaysguaranteedtosucceed
[69,29,2].However,requiringtheexistenceofsuchemergencymaneuverscanleadtoextremely
conservativebehavior.Anotherapproachistoassumethattheenvironmentcertainge-
ometricconditions(e.g.,largeseparationbetweenobstacles)thatallowforsafenavigation[51].
However,suchconditionsarerarelybyreal-worldenvironments.Moreover,suchcondi-
tionsaredomainspitisnotclearhowonewouldspecifysuchconstraintsforproblemsother
thannavigation(e.g.,grasping).
Anotherconceptuallyappealingapproachforsynthesizingpolicieswithguaranteedperformance
onaprioriunknownenvironmentsistomodeltheproblemasaPartiallyObservableMarkovDeci-
sionProcess(POMDP)[38],wheretheenvironmentispartofthe(partiallyobserved)stateofthe
system[67].Computationalconsiderationsaside,suchanapproachismadeinfeasiblebytheneed
tospecifyadistributionoverenvironmentstherobotmightencounter.Unfortunately,specifying
suchadistributionoverreal-worldenvironmentsisanextremelychallengingendeavor.Thus,many
approaches(includingours)assumethatweonlyhave
indirect
accesstothetrueunderlyingdistri-
butionoverenvironmentsintheformofexamples.Forexample,[67,66]proposeanapproximation
tothePOMDPframeworkinthecontextofnavigationbylearningtopredictfuturecollisionproba-
bilitiesfrompastdata.Theworkondeep-learningbasedapproachesforcontrolrepresentsanother
prominentsetoftechniqueswhereinteractionswithexampleenvironmentsareusedtolearncontrol
policies(see,e.g.,[46,47,1,49,75,34,35,79,74]).Whiletheapproachesmentionedabovehave
ledtoimpressiveempiricaldemonstrations,itischallengingtoguaranteethatsuchmethodswill
performwellonenvironmentsthatarenotpartofthetrainingdata(especiallywhenalimited
numberoftrainingexamplesareavailable,asisoftenthecaseforroboticsapplications).Ourwork
seekstoaddressthischallengeusingideasfromgeneralizationtheory.
TheprimarytheoreticalframeworkweutilizeinthispaperisPAC-Bayesgeneralizationtheory
4
[54].PAC-Bayestheoryprovidessomeofthetightestknowngeneralizationboundsforclassical
supervisedlearningproblems[45,71,32]andhasrecentlybeenappliedtoexplainandpromote
generalizationindeeplearning[22,60,61].PAC-Bayestheoryhasalsobeenappliedtolearn
controlpoliciesforMarkovDecisionProcesses(MDPs)withprovablesamplecomplexitybounds
[25,26].Theseapproachesalsoexploittheintuition(seeSection1)that\regularizing""policiesinan
appropriatemannercanpreventovingandleadtosample(seealso[58,41,6,5,70]
forotherapproachesthatexploitthisintuitioninthereinforcementlearningcontext).However,
wenotethatthefocusofourworkisquitetfromtheworkonPAC-BayesMDPbounds
(andthemoregeneralframeworkofPACMDPbounds[40,12,31]),whichconsiderthestandard
reinforcementlearningsetupwhereacontrolpolicymustbelearnedthroughmultipleinteractions
withagivenMDP(withunknowntransitiondynamicsand/orrewards).Incontrast,herewefocus
on
zero-shot
generalizationtoanovelenvironment(e.g.,obstacleenvironmentsorobjects).In
otherwords,apolicylearnedfromexamplesoftenvironmentsmustimmediatelyperform
wellonanewone(i.e.,withoutfurtherexploratoryinteractionswiththenewenvironment).We
furthernotethat[25]considersstateandactionspacesalongwithpoliciesthatdependon
fullstatefeedbackwhile[26]relaxestheassumptiononstatespacesbutretainstheother
modelingassumptions.Incontrast,wetargetsystemswithcontinuousstateandactionspacesand
synthesizecontrolpoliciesthatrelyonrichsensoryinputs.
Onthealgorithmicfront,wemaketuseofRelativeEntropyPrograms(REPs)[13].
REPsconstitutearichclassof
convex
optimizationproblemsthatgeneralizemanyotherproblems
includinglinearprograms,geometricprograms,andsecond-orderconeprograms[11].REPsare
optimizationproblemsinwhichalinearfunctionalofthedecisionvariablesisminimizedsubject
tolinearconstraintsandconicconstraintsgivenbya
relativeentropycone
.REPsareamenable
totsolutiontechniques(e.g.,interiorpointmethods[57])andcanbesolvedusingexisting
softwarepackages(e.g.,Mosek[56],SCS[64,63],andECOS[20]).Wereferthereaderto[13]for
amorethoroughintroductiontoREPs.Importantlyforus,REPscanhandleconstraintsofthe
form
D
(
p
k
q
)

c
,where
p
and
q
aredecisionvariablescorrespondingtoprobabilityvectors,
D
(

)
representstheKullback-Leiblerdivergence,and
c
isascalardecisionvariable.Aswewillsee,this
allowsustouseREPstolearncontrolpoliciesusingthePAC-Bayesframeworkinthesettingwhere
weareoptimizingoverasetofpolicies.
1.3Notation
Weusethenotation
v
[
i
]torefertothei-thcomponentofavector
v
2
R
n
.Weuse
R
n
+
todenotethe
setofelementwisenonnegativevectorsin
R
n
,
Z
+
todenotenonnegativeintegers,and

todenote
element-wisemultiplication.
2ProblemFormulation
Weassumethattherobot'sdynamicsaredescribedbyadiscrete-timesystem:
x
(
t
+1)=
f
(
x
(
t
)
;u
(
t
);
E
)
;
(1)
where
t
2
Z
+
isthetimeindex,
x
(
t
)
2X
isthestateattime
t
,
u
(
t
)
2U
isthecontrolinputat
time
t
,and
E
istheenvironmentthattherobotoperatesin.Weusetheterm\environment""here
broadlytorefertoanyfactorsthatareexternaltotherobot.Forexample,
E
couldrefertoan
5
obstaclethatamobilerobotisattemptingtonavigatethrough,externaldisturbances(e.g.,
windgusts)thataUAVissubjectedto,oranobjectthatamanipulatorisattemptingtograsp.
Let
E
denotethespaceofallpossibleenvironments.Wethenmakethefollowingassumption.
Assumption1.
Thereisanunderlyingdistribution
D
over
E
fromwhichenvironmentsaredrawn.
Importantly,we
donot
assumethatwehaveexplicitdescriptionsof
E
or
D
.Instead,weonly
assumeindirectaccessto
D
intheformofadataset
S
=
f
E
1
;:::;E
N
g
of
N
trainingenvironments
drawni.i.d.from
D
.InSection6.2,wewillpresentanextensionofourbasicframeworkthatallows
ustorelaxthisassumptionandhandlesettingswheretrainingandtestenvironmentsaredrawn
fromtdistributions.
Let
g
:
XE!Y
denotetherobot'ssensormappingfromastate
x
andanenvironment
E
toanobservation
y
=
g
(
x
;
E
)
2Y
.Sinceweareinterestedinpartiallyobservablesettings,wedo
notmakeanyparticularassumptions(e.g.,injectivityorbijectivity)onthesensormapping
g
.Let
ˇ
:
Y!U
denoteacontrolpolicythatmapssensormeasurementstocontrolinputs.Notethat
thisisaverygeneralmodelandcancapturecontrolpoliciesthatdependon
histories
ofsensor
measurements(bysimplyaugmentingthestatetokeeptrackofhistoriesofstatesandletting
Y
denotethespaceofhistoriesofsensormeasurements).
Weassumethattherobot'sdesiredbehaviorisencodedthroughacostfunction.Inparticular,
let
r
ˇ
:
E!
(
XU
)
T
denotethefunctionthat\rollsout""thesystemwithcontrolpolicy
ˇ
,i.e.,
r
ˇ
mapsanenvironment
E
tothestate-controltrajectoryoneobtainsbyapplyingthecontrolpolicy
ˇ
(uptoatimehorizon
T
).Wewillassumethattheenvironmentcapturesallsourcesofstochasticity
(includingrandominitialconditions)andtherolloutfunctionfora
particular
environmentisthus
deterministic(wediscussthecaseofstochasticrolloutsinSection6.1).Wethenlet
C
(
r
ˇ
;
E
)denote
thecostincurredbycontrolpolicy
ˇ
whenoperatinginenvironment
E
overatimehorizon
T
.We
assumethatthecost
C
(
r
ˇ
;
E
)isboundedandwillassume(withoutfurtherlossofgenerality)that
C
(
r
ˇ
;
E
)
2
[0
;
1].Wemakethefollowingimportantassumptioninthiswork.
Assumption2.
Givenanycontrolpolicy
ˇ
,wecancomputethecost
C
(
r
ˇ
;
E
i
)
forthetraining
environments
E
1
;:::;E
N
.
Thisassumptionisifonecansimulatetherobot'soperationintheenvironments
E
1
;:::;E
N
.Wenotethatcomputationalconsiderationsaside,wedonotmakeanyrestrictionson
thedynamics
f
orthesensormapping
g
beyondtheabilitytosimulatethem.Themodelsthat
ourapproachcanhandlearethusextremelyrichinprinciple(e.g.,nonlinearorhybriddynamics,
sensormodelsinvolvingraycastingorsimulatedvision,etc.).
AnotherpossibilityforsatisfyingAssumption2istorunthepolicy
ˇ
onthehardwaresystem
itselfinthegivenenvironments.Thismaybeafeasibleoptionforproblemssuchasgrasping,
whicharenotsafety-criticalinnature.Insuchcases,ourapproachdoesnotrequiremodelsofthe
dynamics,sensormapping,ortherolloutfunction.
Goal:
Ourgoalistodesignacontrolpolicythatminimizestheexpectedvalueofthecost
C
acrossenvironments:
min
ˇ
2

C
D
(
ˇ
):=min
ˇ
2

E
E
˘D
[
C
(
r
ˇ
;
E
)]
:
(2)
Inthiswork,itwillbeusefultoconsideramoregeneralsettingwherewechoosea
distribution
P
overthecontrolpolicyspaceinsteadofmakingasingledeterministicchoice.Thisisbecause
6
thePAC-Bayesboundsweusewillassumethissetting.Ourgoalisthentosolvethefollowing
optimizationproblem,whichwerefertoas
OPT
:
C
?
:=min
P
2P
C
D
(
P
):=min
P
2P
E
E
˘D
E
ˇ
˘
P
[
C
(
r
ˇ
;
E
)]
;
(
OPT
)
where
P
denotesthespaceofprobabilitydistributionsoverNotethattheouterexpectation
hereistakenwithrespecttothe
unknown
distribution
D
.Thisconstitutestheprimarychallenge
intacklingthisproblem.
3Background
TheprimarytechnicalframeworkweleverageinthispaperisPAC-Bayestheory.InSection3.2,
weprovideabriefoverviewofthekeyresultsfromPAC-Bayestheoryinthecontextofsuper-
visedlearning.WeprovidesomebriefbackgroundonthepropertiesoftheKullback-Leibler
(KL)divergenceinSection3.1andshowhowwecancomputeitsinverseusingRelativeEntropy
Programming(REP)inSection3.1.1.
3.1KLdivergence
Giventwodiscreteprobabilitydistributions
P
and
Q
overacommonset,theKLdivergence
fromQtoPisedas
D
(
P
k
Q
):=
X
i
P
[
i
]log
 
P
[
i
]
Q
[
i
]
!
:
(3)
Forscalars
p;q
2
[0
;
1],we
D
(
p
k
q
):=
D
(
B
(
p
)
k
B
(
q
))=
p
log
p
q
+(1

p
)log
1

p
1

q
;
(4)
where
B
(
p
)denotesaBernoullidistributionon
f
0
;
1
g
withparameter(i.e.,mean)
p
.
FordistributionsPandQofacontinuousrandomvariable,theKLdivergenceistobe
D
(
P
k
Q
)=
Z
p
(
x
)log
p
(
x
)
q
(
x
)
dx;
(5)
where
p
and
q
denotethedensitiesof
P
and
Q
.Importantly,if
P
and
Q
correspondtonormal
distributions
N
p
=
N
(

p
;

p
)and
N
q
=
N
(

q
;

q
)over
R
d
,theKLdivergencecanbecomputedin
closedformas
D
(
N
p
k
N
q
)=
1
2
 
T

1
q

p
)+(

q


p
)
T


1
q
(

q


p
)+log

q
)

p
)

d
!
:
(6)
3.1.1ComputingKLinverseusingRelativeEntropyProgramming
PAC-Bayesbounds(Section3.2)aretypicallyexpressedasboundsonaquantity
q
?
2
[0
;
1]ofthe
form
D
(
p
k
q
?
)

c
(forsome
p
2
[0
;
1]and
c

0).Theseboundscanthenbeusedtoupperbound
q
?
bythe
KLinverse
asfollows:
q
?

D

1
(
p
k
c
):=sup
f
q
2
[0
;
1]
j
D
(
p
k
q
)

c
g
:
(7)
7
InpriorworkonPAC-Bayestheory,theKLinversewasnumericallyapproximatedusinglocal
rotechniquessuchasNewton'smethod[22,23],whichdonothaveaprioriguaranteeson
convergencetoaglobalsolution.HereweobservethattheKLinverseisreadilyexpressedasthe
optimalvalueofasimpleRelativeEntropyProgram(ref.Section1.2).Inparticular,theexpression
fortheKLinversein(7)correspondstoanoptimizationproblemwitha(scalar)decisionvariable
q
,alinearcostfunction(i.e.,

q
),linearinequalityconstraints(i.e.,0

q

1),andaconstraint
ontheKLdivergencebetweenthedecisionvariable
q
andtheconstant
p
.Wecanthuscompute
theKLinverseexactly(uptonumericaltolerances)usingconvexoptimization(e.g.,interiorpoint
methods[13]).
3.2PAC-BayesTheoryinSupervisedLearning
WenowprovideabriefoverviewofthekeyresultsfromPAC-Bayestheoryinthecontextof
supervisedlearning.Let
Z
beaninputspaceand
Z
0
beasetoflabels.Let
D
bethe(unknown)true
distributionon
Z
.Let
H
beahypothesisclassconsistingoffunctions
h
w
:
Z!Z
0
parameterized
by
w
2
R
d
(e.g.,neuralnetworksparameterizedbyweights
w
).Let
l
:
HZ!
R
bealoss
function
1
.Wewilldenoteby
P
thespaceofprobabilitydistributionsontheparameterspace
R
d
.
Informally,wewillrefertodistributionson
H
whenwemeandistributionsovertheunderlying
parameterspace.
PAC-Bayesanalysisthenappliestolearningalgorithmsthatoutputa
distribution
overhy-
potheses.Sp,thePAC-Bayesframeworkappliestolearningalgorithmswiththefollowing
structure:
1.
Choosea
\prior""distribution
P
0
2P
before
observinganydata.
2.
Observetrainingdatasamples
S
=
f
z
i
g
N
i
=1
andchoosea
posteriordistribution
P
2P
.This
posteriorcandependonthedataandtheprior.
Itisimportanttonotethattheposteriordistribution
P
neednot
betheBayesianposterior.PAC-
Bayestheoryappliesto
any
distribution
P
.
Letusdenotethetraininglossassociatedwiththeposteriordistribution
P
as:
l
S
(
P
):=
1
N
X
z
2
S
E
w
˘
P
[
l
(
h
w
;
z
)]
;
(8)
andthetrueexpectedlossas:
l
D
(
P
):=
E
z
˘D
E
w
˘
P
[
l
(
h
w
;
z
)]
:
(9)
ThefollowingtheoremistheprimaryresultfromPAC-Bayestheory
2
.
Theorem1
(PAC-BayesBoundforSupervisedLearning[54,52])
.
Forany

2
(0
;
1)
,withproba-
bilityatleast
1


oversamples
S
˘D
N
,thefollowinginequalityholds:
D
(
l
S
(
P
)
k
l
D
(
P
))

D
(
P
k
P
0
)+log(
2
p
N

)
N
:
(10)
1
Notethatweareconsideringaslightlyrestrictedformofthesupervisedlearningproblemwhereeachinput
z
2Z
hasonlyonecorrectlabel
z
0
2Z
0
.Thelossthusonlydependsontheinput
z
andthelabel
h
w
(
z
).ThePAC-Bayes
frameworkappliestothemoregeneralsettingwherethereisanunderlyingtruedistributionon
ZZ
0
andtheloss
thushastheform
l
:
HZZ
0
!
R
.However,themorerestrictedsettingistforourneedshere.
2
TheboundwestatehereisduetoMaurer[52]andimprovesslightlyupontheoriginalPAC-Bayesbounds[54].
Thestatedboundholdswhencostsareboundedintherange[0
;
1](asassumedhere)andwehave
N

8samples.
8
Here,
D
(
l
S
(
P
)
k
l
D
(
P
))isinterpretedasaKLdivergencebetweenBernoullidistributionsand
computedusing(4)(thisismeaningfulsince
l
S
(
P
)and
l
D
(
P
)arescalarsboundedwithin[0
;
1]).
Intuitively,Theorem1providesaboundonhow\close""thetrainingloss
l
S
(
P
)andthetrue
expectedloss
l
D
(
P
)are.However,inpractice,onewouldliketoan
upperbound
onthetrue
expectedloss
l
D
(
P
).SuchanupperboundcanbeobtainedbycomputingtheKLinverse(ref.
Section3.1.1):
l
D
(
P
)

D

1

l
S
(
P
)
k
D
(
P
k
P
0
)+log(
2
p
N

)
N

:
(11)
Anotherupperboundthatisusefulforthepurposeofoptimizationisprovidedbythefollowing
corollary,whichfollowsfromTheorem1byapplyingthewell-knownupperboundfortheKLinverse
oneobtainsbyapplyingPinsker'sinquality:
D

1
(
p
k
c
)

p
+
p
c=
2.
Corollary1
(PAC-BayesUpperBoundforSupervisedLearning[54,52])
.
Forany

2
(0
;
1)
,with
probabilityatleast
1


oversamples
S
˘D
N
,thefollowinginequalityholds:
l
D
(
P
)
|
{z
}
Trueexpectedloss

l
S
(
P
)
|
{z
}
Trainingloss
+
s
D
(
P
k
P
0
)+log(
2
p
N

)
2
N
|
{z
}
\Regularizer""
:
(12)
Corollary1providesastrategyforchoosingadistribution
P
overhypotheseswithaprovable
guaranteeongeneralization:minimizetherighthandside(RHS)ofinequality(12)consistingof
thetraininglossanda\regularization""term.
4PAC-BayesControl
WenowdescribeourapproachforadaptingthePAC-Bayesframeworkinordertotacklethe
policylearningproblem
OPT
andsynthesize(stochastic)controlpolicieswithguaranteedexpected
performanceacrossnovelenvironments.Ourkeyideafordoingthisistoexploitapreciseanalogy
betweenthesupervisedlearningsettingfromSection3.2andthepolicylearningsettingdescribed
inSection2.Table1presentsthisrelationship.
SupervisedLearningPolicyLearning
Inputdata
z
2Z
Environment
E
2E
Hypothesis
h
w
:
Z!Z
0
 
Rolloutfunction
r
ˇ
:
E!
(
XU
)
H
Loss
l
(
h
w
;
z
)Cost
C
(
r
ˇ
;
E
)
Table1
:
Areductionfromthecontrolpolicylearningproblemweconsiderheretothesupervised
learningsetting.
OnecanthinkoftherelationshipinTable1asprovidinga
reduction
fromthepolicylearning
problem
OPT
toasupervisedlearningproblem.Weareprovidedinputdataintheformofadata
setofexampleenvironments.Choosinga\hypothesis""correspondstochoosingacontrolpolicy
ˇ
(sincetherolloutfunction
r
ˇ
isdeterminedby
ˇ
).A\hypothesis""mapsanenvironment
E
toa
\label"",correspondingtothestate-controltrajectoryobtainedbyapplying
ˇ
on
E
.This\label""
incursaloss
C
(
r
ˇ
;
E
).
9
WecanusethisreductiontotranslatethePAC-Bayestheoremsforsupervisedlearning(Theo-
rem1andCorollary1)tothecontrolsetting.Similartothesupervisedlearningsetting,weassume
thatthespaceofcontrolpoliciesisparameterizedby
w
2
R
d
.Thisinturnproducesaparam-
eterizationofrolloutfunctions.Withaslightabuseofnotation,wewillrefertorolloutfunctions
r
w
insteadof
r
ˇ
(withtheunderstandingthat
w
istheparametervectorforthecontrolpolicy
ˇ
).
Let
P
0
bea\prior""distributionovertheparameterspace
R
d
chosenbeforeseeinganyexample
environments.Thepriorcanbeusedtoencodedomainknowledge,butneednotbe\true""inany
Bayesiansense(i.e.,boundswillholdforanyprior).Let
P
bea(possiblydata-dependent)\poste-
rior"".FollowingthenotationfromSection2,wedenotethetrueexpectedcostacrossenvironments
by
C
D
(
P
).Wewilldenotethecostonthetrainingenvironmentsas
C
S
(
P
):=
1
N
X
E
2
S
E
w
˘
P
[
C
(
r
w
;
E
)]
:
(13)
ThefollowingtheoremisthenanexactanalogyofCorollary1.
Theorem2
(PAC-BayesBoundforControlPolicies)
.
Forany

2
(0
;
1)
,withprobabilityatleast
1


oversampledenvironments
S
˘D
N
,thefollowinginequalityholds:
C
D
(
P
)
|
{z
}
Trueexpectedcost

C
PAC
(
P
):=
C
S
(
P
)
|
{z
}
Trainingcost
+
s
D
(
P
k
P
0
)+log(
2
p
N

)
2
N
|
{z
}
\Regularizer""
:
(14)
Proof.
TheprooffollowsimmediatelyfromCorollary1giventhereductioninTable1.
Thistheoremwillconstituteourprimarytoolforlearningpolicieswithguaranteesontheir
expectedperformanceacrossnovelenvironments.Inparticular,thelefthandsideofinequality(14)
isthecostfunction
C
D
(
P
)oftheoptimizationproblem
OPT
.Theorem2thusprovidesanupper
bound(thatholdswithprobability1


)onthetrueexpectedperformanceacrossenvironmentsof
anypolicydistribution
P
intermsofthelossonthesampledenvironmentsin
S
=
f
E
i
g
N
i
=1
anda
\regularizer"".Ourapproachforchoosing
P
istominimizethisupperbound.Algorithm1outlines
thestepsinvolvedinourapproach.
Wenotethatwhile
P
ischosenbyoptimizing
C
PAC
(
P
)(i.e.,theRHSofinequality(14)),the
upperbound
C
?
bound
on
C
D
(
P
)isnotcomputedas
C
PAC
(
P
?
PAC
).Whilethisisavalidupper
bound,atighterboundisprovidedbyinequality(11).TheobservationsmadeinSection3.1.1allow
ustocomputethisboundusingaREP.Thisistheboundwereportintheresultspresented
inSection7.
Algorithm1
PAC-BayesPolicyLearning
1:
Fixpriordistribution
P
0
2P
overpolicies
2:
Inputs:
S
=
f
E
1
;:::;E
N
g
:Trainingenvironments,

:Probabilitythreshold
3:
Outputs:
4:
P
?
PAC
=argmin
P
2
P
C
PAC
(
P
):=
1
N
P
E
2
S
E
w
˘
P
[
C
(
r
w
;
E
)]+
q
D
(
P
k
P
0
)+log(
2
p
N

)
2
N
5:
C
?
bound
:=
D

1

C
S
(
P
?
PAC
)
k
D
(
P
?
PAC
k
P
0
)+log(
2
p
N

)
N

10
5ComputingPAC-BayesControlPolicies
WenowdescribehowtotackletheoptimizationprobleminAlgorithm1forminimizingtheupper
boundonthetrueexpectedcost.Wewilldiscussthesettingwherethecontrolpolicyspaceis
(Section5.1).Forthissetting,theoptimizationproblemcanbesolvedtoglobaloptimalityvia
RelativeEntropyProgramming.Wethentacklethemoregeneralsettingwhereiscontinuously
parameterizedinSection5.2.
5.1FiniteControlPolicySpace
Letthespaceofpoliciesbe=
f
ˇ
1
;:::;ˇ
L
g
.Ourgoalisthentooptimizea
discrete
probability
distribution
P
(withcorrespondingprobabilityvector
p
)overthespaceThus,
p
[
j
]denotesthe
probabilityassignedtopolicy
ˇ
j
.amatrix
^
C
ofcosts,whereeachelement
^
C
[
i;j
]=
C
(
r
ˇ
j
;
E
i
)(15)
correspondstothecostincurredonenvironment
E
i
2
S
bypolicy
ˇ
j
2
(recallthatAssumption
2impliesthatwecancomputeeach
^
C
[
i;j
]).Thetrainingcostfrominequality(14)canthenbe
writtenas:
1
N
X
E
2
S
E
ˇ
˘
P
[
C
(
r
ˇ
;
E
)]=
1
N
N
X
i
=1
L
X
j
=1
^
C
[
i;j
]
p
[
j
]:=

Cp;
(16)
wherethematrix

C
isas:

C
:=
1
N
1
T
^
C:
(17)
Here,
1
istheall-onesvectorofsize
N

1.Wenotethatavector
p
thatminimizesthe
trainingcostcorrespondstosolvinga
linearprogram
.
MinimizingthePAC-Bayesupperbound
C
PAC
(
P
)correspondstosolvingthefollowingopti-
mizationproblem:
min
p
2
R
L

Cp
+
s
D
(
p
k
p
0
)+log(
2
p
N

)
2
N
(18)
s.t.0

p

1
;
X
j
p
[
j
]=1
:
Thisoptimizationproblemcanbe
equivalently
reformulatedviaan
epigraphconstraint
[11]as:
min
p
2
R
L
;˝
˝
s.t.
˝


Cp
+
s
D
(
p
k
p
0
)+log(
2
p
N

)
2
N
0

p

1
;
X
j
p
[
j
]=1
:
11
Wefurtherrewritetheproblemas:
min
p
2
R
L
;˝
˝
(19)
s.t.

2

D
(
p
k
p
0
)+log(
2
p
N

)
2
N

=
˝


Cp;

0
0

p

1
;
X
j
p
[
j
]=1
:
Ourkeyobservationhereisthatfora
d

=

0
,theaboveproblemisaRelativeEntropy
Program(REP)sinceitconsistsofminimizingalinearcostfunctionsubjecttolinearequalityand
inequalityconstraintsandanadditionalinequalityconstraintoftheform
D
(
p
k
p
0
)

constant.
Wenotethat

2
[0
;
1]since

=
˝


Cp
,where
˝
2
[0
;
1](because
˝
upperboundsthetrue
expectedcost)and

Cp
2
[0
;
1](recallthatweassumedthatcostsareboundedbetween0and1).In
ordertosolveproblem(19)toglobaloptimality,wecanthussimplysearchovertheone-dimensional
parameter

2
[0
;
1](e.g.,bysimplydiscretizingtheinterval[0
;
1],performingabisectionsearch,
etc.)andthesettingof

thatleadstothelowestoptimalvalueforthecorrespondingREP.
5.2Continuously-ParameterizedControlPolicySpace
Wenowconsiderpolicies
ˇ
w
parameterizedbythevector
w
2
R
d
(e.g.,neuralnetworksparame-
terizedbyweights).Wewillconsiderstochasticpoliciesbyprobabilitydistributionsover
theparameters
w
.Here,wechooseGaussiandistributions
w
˘N
(

withdiagonalcovariance
=diag(
s
)(with
s
2
R
d
+
)andusetheshorthand
N

:=
N
(

diag(
s
)).UsingGaussiansmakes
computationseasiersincewecanexpresstheKLdivergencebetweenGaussiansinclosedform(see
Section3.1).WecanthenapplyAlgorithm1andchoose
s
tominimizethePAC-Bayesupper
bound
C
PAC
(
N

).Inordertoturnthisintoapracticalalgorithm,therearetwoprimaryissues
weneedtoaddress.
First,inordertominimizethebound
C
PAC
(
N

),onewouldliketoapplygradient-based
methods(e.g.,stochasticgradientdescent).However,thecostfunctionmaynotbeatiable
functionoftheparameters
w
.Forexample,inthecaseofdesigningobstacleavoidancepolicies,a
natural(buttiable)costfunctionistheonethatassignsacostof1iftherobotcollides
(and0otherwise).Totacklethisissue,weemployatiablesurrogateforthecostfunction
duringoptimization(notethattheboundisstillevaluatedfortheoriginalcostfunction).This
surrogatewillnecessarilydependontheapplicationathand;wepresentexamplesinthecontexts
ofobstacleavoidanceandgraspinginSection7.
Thesecondchallengeisthefactthatcomputingthetrainingcost
C
S
(
N

)requirescomputing
thefollowingexpectationoverpolicies:
E
w
˘N

[
C
(
r
w
;
E
)]
:
(20)
Formostrealisticsettings,thisexpectationcannotbecomputedinclosedform.Weaddressthis
issueinamannersimilarto[22].Inparticular,inordertooptimize

and
s
usinggradientdescent,
wetakegradientstepswithrespecttothefollowingunbiasedestimatorof
C
S
(
N

):
1
N
X
E
2
S
C
(
r

+
p
s

˘
;
E
)
;˘
˘N
0
;I
d
:
(21)
12
Inotherwords,ineachgradientstepweuseani.i.d.sampleof
˘
andcomputethegradientof(21)
withrespectto

and
s
.
Attheendoftheoptimizationprocedure,wetheoptimal

?
and
s
?
andestimatethetraining
cost
C
S
(
P
)=
C
S
(
N

?
;s
?
)byproducingalargenumberofsamples
w
1
;:::;w
L
drawnfrom
N

?
;s
?
:
^
C
S
(
N

?
;s
?
):=
1
NL
X
E
2
S
L
X
i
=1
C
(
r
w
i
;
E
)
:
(22)
Wecanthenuseasampleconvergencebound(see[44])toboundtheerrorbetween
^
C
S
(
N

?
;s
?
)and
C
S
(
N

?
;s
?
).Inparticular,thefollowingboundisanapplicationoftherelativeentropyversionof
theboundforrandomvariables(i.e.,costs)boundedin[0
;
1]andholdswithprobability
1


0
:
C
S
(
N

?
;s
?
)


C
S
(
N

?
;s
?
;
L;
0
):=
D

1
(
^
C
S
(
N

?
;s
?
)
k
1
L
log(
2

0
))
:
(23)
Combininginequalities(10)and(23)usingtheunionbound,weseethatthefollowingboundholds
withprobabilityatleast1




0
:
C
D
(
N

?
;s
?
)

C
?
bound
:=
D

1
 

C
S
(
N

?
;s
?
;
L;
0
)
k
D
(
N

?
;s
?
k
P
0
)+log(
2
p
N

)
N
!
:
(24)
Thisistheversionofourboundontheexpectedperformanceofpolicies(drawnfrom
N

?
;s
?
).
Algorithm2summarizesourapproachfromthissection.Notethatinordertoensurepositivity
of
s
2
R
d
+
,weperformtheoptimizationwithrespectto

:=log(
s
).
Algorithm2
PAC-BayesPolicyLearningviaGradientDescent
1:
Inputs:
2:
S
=
f
E
1
;:::;E
N
g
:Trainingenvironments
3:
;
0
2
(0
;
1):Probabilitythresholds
4:
P
0
:Prioroverpolicies
5:
s
2
R
d
:Initializationsfor

and
s
6:

:stepsizeforgradientdescent
7:
Outputs:
8:

?
;s
?
:Optimal
s
9:
C
?
bound
:=
D

1


C
S
(
N

?
;s
?
;
L;
0
)
k
D
(
N

?
;s
?
k
P
0
)+log(
2
p
N

)
N

10:
Procedure:
11:
B
(
s;w
):=
1
N
P
E
2
S
C
(
r
w
;
E
)+
q
D
(
N

?
;s
?
k
P
0
)+log(
2
p
N

)
2
N
12:
while
:
converged
do
13:
Sample
˘
˘N
0
;I
d
andset
w
 

+
p
s

˘
14:

 



r

B
(

exp(

)
;w
)
15:

 



r

B
(

exp(

)
;w
)
16:
s
 
exp(

)
17:
endwhile
6Extensions
Inthissection,wepresenttwoextensionstothebasicframeworkpresentedsofar.InSection6.1,
wediscussextensionstosystemswithstochasticdynamicsorsensormeasurements.InSection6.2,
13
wepresentanapproachthatallowsustotacklesettingswheretrainingandtestenvironmentsare
drawnfromtdistributions.
6.1StochasticRolloutFunctions
InourproblemformulationinSection2,weassumedthattherolloutfunction
r
w
:
E!
(
XU
)
H
isdeterministic(i.e.,oncetheenvironmentistheresultingstate-actiontrajectoryobtained
byapplyingagivenpolicyiscompletelydetermined).Herewesketchanextensionofour
frameworktosettingswheretherolloutfunctionisstochastic(e.g.,duetostochasticityinthe
dynamicsofthesystemorinsensormeasurements).Thisismadepossiblebyareinterpretation
ofthevariable
w
.Previously,
w
correspondedtoparametersofthecontrolpolicy.Supposenow
thatwethinkof
w
asconsistingoftwocomponents
w
:
=[
w
int
;w
ext
];an\internal""component
w
int
correspondingtoparametersofthecontrolpolicy(justasbefore),andanadditional\external""
componentcorrespondingtouncertainparameters(e.g.,externaldisturbancesthattherobotmight
experience).Therolloutfunctionnowhasthefollowingstructure:
r
[
w
int
;w
ext
]
:
E!
(
XU
)
H
.The
stochasticityin
w
int
isdirectlysetbyus(i.e.,bychoosingaprior
P
0
andposterior
P
asbefore).
However,thestochasticityover
w
ext
isbeyondourcontrol.
Wenotethatthestructureoftheresultingproblemisidenticaltotheoriginalformulation
consideredinSection2.Theonlycomesfromthefactthataportionofthestochasticity
intherolloutsisbeyondourcontrol.WecanthusdirectlyapplyTheorem2inordertoobtainan
upperboundonthetrueexpectedcost.Inparticular,let
P
0
and
P
bethepriorandposteriorover
w
int
(asbefore)andsupposethatthedistributionover
w
ext
isgivenby
P
ext
.Further,assumethat
w
int
and
w
ext
areindependentrandomvariables.Wecanthen
P
0
0
and
P
0
tobethepriorand
posteriordistributionsover
w
:
=[
w
int
;w
ext
]andevaluatethe\regularizer""terminthePAC-Bayes
boundinTheorem2bynotingthat:
D
(
P
0
k
P
0
0
)=
E
P;P
ext
""
log

P
ext
P
P
ext
P
0

#
=
E
P;P
ext
""
log

P
P
0

#
(25)
=
E
P
""
log
P
P
0
#
E
P
ext
[1]
|
{z
}
=1
=
D
(
P
k
P
0
)
:
(26)
Theequalitybetweenthetwolinesfollowsfromthefactthat
w
int
and
w
ext
areindependent.
Inordertoevaluatethetrainingcost
C
S
(
P
0
)=
1
N
P
E
2
S
E
w
˘
P
0
[
C
(
r
w
;
E
)],wecanemploythe
samplingproceduredescribedinSection5.2(i.e.,samplingthedisturbances
w
ext
˘
P
ext
ina
manneranalogoustohow
w
wassampledinSection5.2).Thus,theframeworkforthedeterministic
rolloutsettingcanbeappliedwithalmostnomoinordertohandlethestochasticrollout
case(aslongasonecansampledisturbances
w
ext
andassumingthatthedisturbancesaredrawn
independentlyof
w
int
).
6.2Distributionally-RobustControlPolicies
Sofar,wehaveassumedthattherobotwillbetestedonenvironmentsthataredrawnfromthesame
distributionasthetrainingenvironments.Wewillnowaddressthesettingwherethisassumption
isnotvalidandlearn
distributionally-robustpolicies
(i.e.,policiesthatarerobusttochangesinthe
distributionfromwhichenvironmentsaredrawn).Wewillassumethatthedistribution
D
0
from
14
whichtestenvironmentsaredrawnisboundedintermsofan
f
-
divergence
(seebelow)fromthe
trainingdistribution
D
andformulatearobustversionofthePAC-Bayesboundalreadydescribed.
1
(
f
-divergencebetween
D
0
and
D
[62])
.
Foranyconvex
f
(
x
)
suchthat
f
(1)=0
,let
D
f
(
D
0
jjD
):=
E
E
˘D
""
f
 
D
0
D
!#
:
(27)
The
f
-divergencesencapsulateabroadclassofdivergencesbetweendistributionsandinclude
theKLdivergenceasaspecialcase(with
f
(
x
)=
x
log
x
).Wewillassumethatthetestdistribution
D
0
isboundedintermsofan
f
-divergence:
D
f
(
D
0
jjD
)
B
(butnofurtherassumptionon
D
0
will
bemade).Thecontrolpolicywelearnwillhaveanassociatedguaranteeon
any
testdistribution
thatthisassumption.
Theorem3
(
f
-divergencebetween
D
0
and
D
intermsof
f
and
f

[62])
.
Foragiven
f
(
x
)
andits
convexconjugate
f

(
y
)
:
=sup
x
2
R

xy

f
(
x
)

,wecanwritethe
f
-divergence
D
f
(
D
0
jjD
)
interms
ofonly
f
anditsconjugate:
D
f
(
D
0
jjD
)=sup
C

!
R

E
E
˘D
0
E
w
˘
P
[
C
(
r
w
;
E
)]

E
E
˘D
E
w
˘
P
[
f

(
C
(
r
w
;
E
))]

:
(28)
Thesupremumaboveistakenoverallfunctions
C
thatresultintheexpectationsintheRHS
beingnite.Thus,foranyparticular(cost)function
C
,weobtainalowerboundonthesupremum
term.Thisallowsustoobtainthefollowingusefulcorollary.
Corollary2
(
f
-Divergencevariationalinequality)
.
If
D
f
(
D
0
jjD
)
B
,then
C
D
0
(
P
):=
E
E
˘D
0
E
w
˘
P
[
C
(
r
w
;
E
)]
B
+
E
E
˘D
E
w
˘
P
[
f

(
C
(
r
w
;
E
))]
:
(29)
Notethatthiscorollaryisvalidforany
f
-divergence.Inparticular,itholdswhen
f
(
x
)=
x
log
x
,
making
f

(
y
)=
e
y

1
.Withthischoiceof
f
,weobtainanupperboundon
C
D
0
(
P
)intermsofthe
bound
B
ontheKLdivergence.
Corollary3
(KLdivergencevariationalinequality)
.
For
f
-divergencewith
f
(
x
)=
x
log
x
and
D
f
(
D
0
jjD
)
B
,wehave
D
f
(
D
0
jjD
)=
D
(
D
0
jjD
)
B
and
C
D
0
(
P
)
B
+
E
E
˘D
E
w
˘
P
[
e
C
(
r
w
;
E
)
]

1
:
(30)
WhileCorollary3providesavalidinequalityinthespecialcaseoftheKLdivergence,atighter
boundcanbeobtainedusingthe
Donsker-Varadhan(DV)
inequality.
Theorem4
(Donsker-Varadhanvariationalinequality[21];Theorem3.2in[33])
.
If
D
(
D
0
jjD
)
B
,
then
C
D
0
(
P
)
B
+log

E
E
˘D
E
w
˘
P
[
e
C
(
r
w
;
E
)
]

:
(31)
TheDVinequalityprovidesatighterboundthaninequality(30)since
x

1

log(
x
)
;
8
x>
0.
Fortherestofthissection,wewillspecializeourdiscussiontotheKLdivergenceandusethe
DVinequality.However,wenotethatourapproachgeneralizestoany
f
-Divergencebyleveraging
Corollary2.
15
Aswritten,inequality(31)cannotbeusedtodirectlyupperbound
C
D
0
(
P
)since
E
E
˘D
E
w
˘
P
[
e
C
(
r
w
;
E
)
]isnotanobservablequantity.However,wecanleveragetheinequality(14)
toobtainanupperboundon
C
D
0
(
P
)intermsofobservablequantities.SinceTheorem2holdsfor
anycostfunctionbetween0and1,wewillbeabletoapplyinequality(14)ifwereplacethecost
withanexponentiatedone,aslongaswerescaletostaybetween0and1.Thus,ifwemakethe
substitution
C
(
r
w
;
E
)
 
e
C
(
r
w
;
E
)

1
e

1
;
weobtainthefollowingboundusinginequality(14):
E
E
˘D
E
w
˘
P
[
e
C
(
r
w
;
E
)
]

1
N
X
E
2
S
E
w
˘
P
[
e
C
(
r
w
;
E
)
]+(
e

1)
s
D
(
P
k
P
0
)+log(
2
p
N

)
2
N
:
(32)
Thisinequalityholdsbecausethetransformationkeepsthecostbetween[0
;
1].Now,sincewe
assumedthat
D
(
D
0
jjD
)
B
,wecanapplyTheorem4tobound
C
D
0
(
P
).
Corollary4
(Distributionally-robustPAC-Bayesbound)
.
Forany
D
0
suchthat
D
(
D
0
jj
D
)
B
and
any

2
(0
;
1)
,withprobabilityatleast
1


oversampledenvironments
S
˘D
N
thefollowing
inequalityholds:
C
D
0
(
P
)
B
+log
 
1
N
X
E
2
S
E
w
˘
P
[
e
C
(
r
w
;
E
)
]+(
e

1)
s
D
(
P
k
P
0
)+log(
2
p
N

)
2
N
!
:
(33)
TheRHSofinequality(33)givesusanupperbound
C
PAC
0
on
C
D
0
.Wecanthusapplyan
analogousproceduretoAlgorithm1toobtain
P
?
PAC
0
(adistributionally-robuststochasticpolicy)
byminimizingthisupperboundand
C
?
bound
0
(thedistributionally-robustPAC-Bayesbound).
Intheitepolicyspacesetting,wecanapplyaproceduresimilartotheoneemployedin
Section5.1towriteanREPthatminimizesthebound
C
PAC
0
.
^
C
e
[
i;j
]
:
=
e
C
(
r
ˇ
j
;
E
i
)
:
(34)
Wethenhave
1
N
X
E
2
S
E
ˇ
˘
P
[
e
C
(
r
ˇ
;
E
)
]=
1
N
N
X
i
=1
L
X
j
=1
^
C
e
[
i;j
]
p
[
j
]:=

C
e
p:
(35)
Wecanthenminimizethebound
C
PAC
0
usinganREPanalogoustoProblem(19):
min
p
2
R
L
;˝
˝
(36)
s.t.

2

(
e

1)
2
D
(
p
k
p
0
)+log(
2
p
N

)
2
N

=
˝


C
e
p;

0
0

p

1
;
X
j
p
[
j
]=1
:
Here

C
e
p
2
[1
;e
],andsince
˝
upperboundsthetrueexpectedcost,weareonlyinterestedinvalues
of
˝
2
[1
;e
].Thusanoptimal

canbefoundbysearchingover

2
[0
;e

1],whichcanthenbe
16
usedtoobtain
P
?
PAC
0
and
C
PAC
0
.Additionally,inthecontinuously-parameterizedcontrolpolicy
spacecase,wecanmakemotoAlgorithm2andequations(20

24)todirectlyadaptthe
SGDapproachtominimize
C
PAC
0
.
Finally,asinSection5,thedistributionally-robustupperbound
C
?
bound
0
isnotcomputed
as
C
PAC
0
(
P
?
PAC
0
)butwithananaloguetotheKLinverseinequation(11):
max
c
D
0
;c
D
2
[0
;
1]
c
D
0
(37)
s.t.
D
(
C
S
(
P
)
jj
c
D
)

D
(
P
k
P
0
)+log(
2
p
N

)
N
D
(
c
D
0
jj
c
D
)
B
:
Theconstraintisthesameasinthenon-robustcase,andthesecondconstraintaccountsfor
theinthetrainingandtestdistributions.TogetherthesecreateanREPthatcanbe
solvedto
C
?
bound
0
.
7Examples
Inthissection,wedemonstrateourframeworkinsimulationontwodomains:obstacleavoidance
(Section7.1)andgrasping(Section7.2).Ourgoalistodemonstratetheabilityofourapproach
tolearncontrolpolicieswithstrongguaranteesongeneralizationtonovelenvironments.Wewill
considerahardwareexampleinSection8.
7.1ReactiveObstacleAvoidanceControl
Inthissection,weapplyourapproachontheproblemoflearningreactiveobstacleavoidancepolicies
foragroundvehiclemodelequippedwithadepthsensor.Weconsiderapolicyspace
andleveragetheREP-basedframeworkdescribedinSection5.1.Wethenconsidercontinuously
parameterizedpoliciesandapplytheapproachfromSection5.2.Finally,weapplytheapproach
fromSection6.2tolearndistributionally-robustcontrolpolicies.
Dynamics.
ApictorialdepictionofthegroundvehiclemodelisprovidedinFigure1(a).The
stateofthesystemisgivenby[
x;y; 
],where
x
and
y
arethexandypositionsofthevehicle
respectively,and
 
istheyawangle.Wemodelthesystemasatialdrivevehiclewiththe
followingnonlineardynamics:
2
4
_
x
_
y
_
 
3
5
=
2
4

r
2
(
u
l
+
u
r
)sin(
 
)
r
2
(
u
l
+
u
r
)cos(
 
)
r
L
(
u
r

u
l
)
3
5
;
(38)
where
u
l
and
u
r
arethecontrolinputs(correspondingtotheleftandrightwheelspeedsrespec-
tively),
r
=0
:
1mcorrespondstotheradiusofthewheels,and
L
=0
:
5mcorrespondstothewidth
ofthebaseofthevehicle.Weset:
u
l
=
u
0

u

;u
r
=
u
0
+
u

;
(39)
where
u
0
=
v
0
=r
with
v
0
=2
:
5m/s.Thisensuresthattherobothasaspeed
v
0
.Welimit
theturningratebyconstraining
u

2
[

u
0
=
2
;u
0
=
2].Thesystemissimulatedasadiscrete-time
systemwithtime-step
t
=0
:
05s.
17
Obstacleenvironments.
AtypicalobstacleenvironmentisshowninFigure1(a)andconsists
of
N
obs
cylindersofvaryingradiialongwiththreewallsthatboundtheenvironmentbetween
x
2
[

5
;
5]mand
y
2
[0
;
10]m.Environmentsaregeneratedbystsamplingtheinteger
N
obs
uniformlybetween20and40,andthenindependentlysamplingthex-ypositionsofthecylinders
fromauniformdistributionovertheranges
x
2
[

5
;
5]mand
y
2
[2
;
10]m.Theradiusofeach
obstacleissampledindependentlyfromauniformdistributionovertherange[0
:
05
;
0
:
2]m.The
robot'sstateisalwaysinitializedat[
x;y; 
]=[0
;
1
;
0].
ObstacleAvoidancePolicies.
Weassumethattherobotisequippedwithadepthsensorthat
providesdistances
y
[
i
]along20raysintherange

[
i
]
2
[

ˇ=
3
;ˇ=
3]radians(positiveisclockwise)
uptoasensinghorizonof5m(asshowninFigure1(a)).Agivensensormeasurement
y
thus
belongstothespace
Y
=
R
20
.Let^
y
=1
=y
2
R
20
betheinversedistancevectorcomputedby
takinganelement-wisereciprocalof
y
.Wethenchoose
u

asthefollowingdotproduct:
u

=
K

^
y:
(40)
Anexampleof
K
2
R
20
is:
K
[
i
]=
(
(
y
0
=x
0
)(
x
0


[
i
])if

[
i
]

0
;
(
y
0
=x
0
)(

x
0


[
i
])if

[
i
]
<
0
:
(41)
Sucha
K
isshowninFigure3.For

[
i
]
>
0,
K
[
i
]isalinearfunctionof

[
i
]withx-andy-intercepts
equalto
x
0
and
y
0
respectively.Thislinearfunctionisabouttheoriginfor

[
i
]
<
0.
Figure3
:
Exampleof
K
[
i
]asafunctionof

[
i
].
Intuitively,thiscorrespondstoasimplereactivepolicythatcomputesaweightedcombination
ofinversedistancesinordertoturnawayfromobstaclesthatareclose.Asasimpleexample,
considerthecasewherewehavetwoobstacles:onelocated4mawayalong

=

ˇ=
4(i.e.,to
therobot'sleft)andtheotherlocated1mawayalong

=
ˇ=
4(i.e.,totherobot'sright).The
computedcontrolinputwillthenbe
u

>
0(i.e.,robotturnsleft)sincetheinversedepthforthe
18
N(#oftrainingenvironments)
100
500
1000
10000
PAC-Bayesbound(
C
?
bound
)
0.178
0.135
0.121
0.096
Trueexpectedcost(estimate)
0.087
0.084
0.088
0.083
Table2
:
ComparisonofPAC-Bayesboundwiththetrueexpectedcost(estimatedbysampling10
5
obstacleenvi-
ronments).Usingonly100samples,withprobability0
:
99oversamples,thePAC-Bayespolicyisguaranteedtohave
anexpectedsuccessrateof82
:
2%.Thetrueexpectedsuccessrateisapproximately91
:
3%.
obstacletotherightislargerthanthatoftheobstacletotheleft.Simplereactivepoliciesofthis
kindhavebeenshowntobequiteeinpractice[3,7,68,16],butcanoftenbechallengingto
tunebyhandinordertoachievegoodexpectedperformanceacross
all
environments.Wetackle
thischallengebyapplyingthePAC-Bayescontrolframeworkproposedhere.
Resultspolicyspace).
Inordertoobtainapolicyspace,wechoose
L
=50
t
K
'softheform(41)bychoosingtxandyintercepts
x
0
and
y
0
.Inparticular,
(
x
0
;y
0
)ischosenbydiscretizingthespace[0
:
1
;
5
:
0]

[0
;
10
:
0]into5valuesfor
x
0
and10values
for
y
0
.Ourcontrolpolicyspaceisthus=
f
ˇ
1
;:::;ˇ
L
g
,whereeachpolicy
ˇ
i
correspondstoa
particularchoiceof
K
.
Weconsideratimehorizonof
T
=100andassignacostof1iftherobotcollideswithan
obstacleduringthisperiodandacostof0otherwise.Wechooseauniformprioroverthepolicy
spaceandapplytheREPframeworkfromSection5.1inordertooptimizeadistributionover
policies.ThePyBulletpackage[17]isusedtosimulatethedynamicsanddepthsensor;weusethese
simulationstocomputetheelementsofthecostmatrix

C
(ref.Section5.1).Eachsimulationtakes
˘
0
:
01stoexecuteinourimplementation(notethatthecomputationofthetelementsof

C
canbeentirelyparallelized).Giventhematrix

C
with100sampledenvironments,eachREP
(correspondingtoavalueof

inProblem(19))takes
˘
0
:
05stosolveusingtheCVXPY
package[19]andtheSCSsolver[64].Wediscretizetheinterval[0
;
1]into100valuestothe
optimal

.CompletecodeforthisimplementationisfreelyavailableonGitHub
3
.
Table2presentstheupperbound
C
?
bound
onthetrueexpectedcostofthePAC-Bayescontrol
policy
P
?
PAC
(ref.Algorithm1)fordierentsamplesizes
N
with

=0
:
01.Thetablealsopresents
anestimateofthetrueexpectedcost
C
D
(
P
?
PAC
)obtainedbysampling10
5
environments.As
thetableillustrates,thePAC-Bayesboundprovidesstrongguaranteesevenforrelativelysmall
samplesizes.Forexample,usingonly100samples,thePAC-Bayespolicyisguaranteed(with
probability1


=0
:
99)tohaveanexpectedsuccessrateof82
:
2%(i.e.,anexpectedcostof
0
:
178).ExhaustivesamplingindicatesthattheexpectedsuccessrateforthePAC-Bayespolicy
isapproximately91
:
3%forthiscase.Videosofrepresentativetrialsontestenvironmentscanbe
foundathttps://youtu.be/y4zTK79s1mI.
Results(continuouspolicyspace).
Next,weconsideracontinuouslyparameterizedpolicy
spaceandapplytheapproachdescribedinSection5.2.Inparticular,weparameterizeourpolicy
usingthematrix
K
2
R
20
inequation(40)whileensuringsymmetryofthecontrollaw,i.e.,we
constrain
K
[
i
]=

K
[
j
]for

[
i
]=


[
j
](notethat
K
isnolongerconstrainedtohavethelinear
formfromequation(41)).Thedimensionalityoftheparameterspaceisthus
d
=10.Weapply
Algorithm2tooptimizeadistribution
N

?
;s
?
overpolicies.Forthepurposeofoptimization,we
employacontinuoussurrogatecostfunctioninplaceofthediscontinuous0-1cost.Wechoosethis
tobethenegativeoftheminimumdistancetoanobstaclealongatrajectory(appropriatelyscaled
3
Code:https://github.com/irom-lab/PAC-Bayes-Control
19
Figure4
:
Optimized
K
correspondingto

?
.
toliewithin[0
;
1]).Notethatweemploythissurrogatecostonlyforoptimization;allresultsare
presentedforthe0-1cost.GradientsinAlgorithm2areestimatednumerically.Wechooseaprior
P
0
=
N

0
;s
0
with
s
0
=0
:
01;themean

0
isgivenbyavector
K
oftheform(41)withx-intercept
2
:
5andy-intercept10
:
0.
Weuse
N
=100trainingenvironmentsandchooseparameters

=0
:
009,

0
=0
:
001,
and
L
=30
;
000samplestoevaluatethesampleconvergenceboundinequation(23).Figure4shows
themean

?
oftheoptimizedpolicyobtainedusingAlgorithm2.ThecorrespondingPAC-Bayes
bound
C
?
bound
is0
:
224.Thus,withprobability0
:
99oversampledtrainingdata,theoptimized
PAC-Bayespolicyisguaranteedtohaveanexpectedsuccessrateof77
:
6%.Exhaustivesampling
with10
5
environmentsindicatesthattheexpectedsuccessrateisapproximately92
:
5%.Videosof
representativetrialsontestenvironmentscanbefoundathttps://youtu.be/y4zTK79s1mI.
Results(distributionally-robustpolicies).
WenowapplytheapproachpresentedinSec-
tion6.2tolearndistributionally-robustpolicies.Completecodefortheimplementationofthe
examplehereisfreelyavailableonGitHub
4
.Toprovideaconcretewayofbounding
D
(
D
0
jjD
),
thetrainingandtestdistributionsonlyinthewaythattheradiusofthecylindricalob-
staclesforthatenvironmentissampled.Forasingleenvironment,allobstacleswillhavethe
sameradius,butthebetadistributionfromwhichthisradiusissampledThismeansthat
D
(
D
0
jjD
)=
D
(
B
(

0
;
0
)
jj
B
(
;
))where
B
(
;
)isthebetadistribution,withparameters

and

,
usedtosampletheradius:
D
(
B
(

0
;
0
)
jj
B
(
;
))=log

B(
;
)
B(

0
;
0
)

+(

0


)
 
(

0
)+(

0


)
 
(

0
)+(



0
+



0
)
 
(

0
+

0
)
(42)
where

and

arethebetadistributionparametersfor
D
,

0
and

0
arethebetadistribution
parametersfor
D
0
,B(

;

)isthebetafunction(distinctfromthebetadistribution),and
 
(

)isthe
4
Code:https://github.com/irom-lab/PAC-Bayes-Control/tree/master/Extension-Domain
Shifts
20
digammafunction.Thisdivergencecanbecomputedanalyticallywithasymbolicintegratorsuch
asMathematica[77].SeeFigure5fortheprobabilitydensityfunctionsofthedistributionsusedto
determinetheradiiofobstaclesforthisexample,where
D
(
D
0
jjD
)
B
=0
:
0819.Notethatforany
testdistributionoverenvironmentsthattheinequality
D
(
D
0
jjD
)

0
:
0819,thecomputed
bound
C
?
bound
0
willbevalid.
Figure5
:
Probabilitydensityfunctions(PDFs)forthebetadistributionsusedtodetermineobstacleradiifor
thetrainingandtestenvironments.Here

=0
:
8,

=1
:
25,

0
=1,and

0
=1whichmakes
D
(
D
0
jjD
)=
D
(
B
(

0
;
0
)
jj
B
(
;
))
B
=0
:
0819.Therobot'sradiusis0
:
27m,depictednexttothe0
:
3mobstacle,andtheradiiof
theobstaclesareboundedbetween0
:
10mand1
:
10m.
Figure6
:
Comparisonofobstaclesgeneratedbythebetadistributionontheradiusforthetraining(topimage)and
test(bottomimage)environments.Whentheobstaclesaresorted,itiseasiertoseethatthetrainingenvironment's
obstaclesareskewedtowardsasmallerradius,makingthoseenvironmentseasiertonavigate.Thisisapparentinthe
PDFcomparisondisplayedinFigure5aswell.
InFigure6,trainingandtestobstaclesarecontrasted.Sincethetrainingenvironmentsare
generatedwithabetadistributionthatfavorssmallerradii,theyarelikelytobesmallerthanthose
generatedwiththetestbetadistribution(uniformdistributionovertheradiusrange).Additionally,
theaverageobstacleradius,whichcanbecalculatedwiththebetadistributionparametersforthe
trainingandtestdistributions(
r
avg
=
=
(

+

)

(
r
max

r
min
)+
r
min
),by0
:
11m.This
correspondsto41%oftherobot'sradius.Resultsonthisexamplefortheapproachpresentedin
Section6.2arepresentedinTable3.Thetabledemonstratesthatweareabletoobtainstrong
boundsongeneralizationeveninthisdistributionally-robustsetting(albeitwithalargernumber
21
oftrainingenvironments).Forexample,with5000trainingenvironments,weobtainaguaranteed
expectedsuccessrateof80
:
3%.Weemphasizethatthisboundholdsfor
any
D
0
thatthe
constraintontheKLdivergence(notjustthesptestdistributionchosenhere).Theestimated
truesuccessrateisapproximately91.8%for
N
=5000.Withatlylargenumberoftraining
environments(10
5
inthisexample),therobustPAC-Bayesbound
ˇ
trueexpectedcostontest+
B
+thescaledregularizerthatappearsinequation(33).
N(#oftrainingenvironments)
100
500
1000
5000
10000
RobustPAC-Bayesbound(
C
?
bound
0
)
0.453
0.276
0.238
0.197
0.185
True(estimated)coston
D
0
usingrobustpolicylearnedusing
D
0.079
0.081
0.080
0.082
0.080
Non-robustPAC-Bayesboundon
D
0.221
0.107
0.089
0.070
0.066
True(estimated)coston
D
usingpolicylearnedon
D
0.054
0.057
0.054
0.054
0.056
Non-robustPAC-Bayesboundon
D
0
0.262
0.170
0.138
0.110
0.096
True(estimated)coston
D
0
usingpolicylearnedon
D
0
0.081
0.080
0.081
0.079
0.083
Table3
:
Comparisonofdistributionally-robustPAC-Bayesboundswithtruecostsestimatedusing10
5
environments.
Weareabletoobtainstrongboundsongeneralization(albeitwithalargernumberoftrainingenvironmentsthanin
thenon-robustcase).Forexample,with5000trainingenvironments,weobtainaguaranteedexpectedsuccessrate
of80
:
3%.Theestimatedtruesuccessrateisapproximately91.8%.Wealsoprovideboundsandestimatedtruecosts
obtainedusingthestandard(non-robust)PAC-Bayesframeworkaspointsofcomparison.
7.2Grasping
Wenowconsidertheproblemoflearningneuralnetwork-basedgraspingpolicieswithguarantees
onperformanceacrossnovelobjects.
Dynamicsandsensors.
ThesystemweconsiderisshowninFigure1(b)andconsistsofa
KUKAiiwaarmgraspinganobjectplacedonatable.Therobotisequippedwithacamerathat
providesRGB-Dimages.Theentiresimulation(rigid-bodydynamicsandsensing)isperformed
usingthePyBulletsimulator[17].
Objects.
WeusetheShapeNetdatabase[14]togenerateobjectsforgrasping.ShapeNet
consistsofmorethan50
;
000objectsandthusprovidesarichandchallengingdataset.Wescale
theobjectssotheyina10cm
3
volume.Themassesoftheobjectsarerandomlychosen
uniformlyfromtherange[0
:
05
;
0
:
15]kgandtheinertiamatricesarerandomlychosendiagonal
matriceswithelementschosenuniformlyfromtherange[0
:
75
;
1
:
25].Objectsareinitializedin
theenvironmentbydroppingthemfromacertainheightabovethetableandallowedtosettle.
Theinitialorientationfromwhichtheyaredroppedisalsorandomized(yaw
˘N
(0
;
0
:
5
2
),roll
˘N
(0
;
0
:
5
2
),pitch
˘N
(0
;
0
:
01
2
)).Notethattherandomizationfortheinitialpitchangleis
smaller;thisensuresthatobjectsland\upright""onthetable.Werandomlyselect
N
=2000
objectsasourtrainingdata.Figure7showsrandomlychosenrepresentativeobjectsfromthe
ShapeNetdatabase.
Costfunction.
Wechooseacostfunctionthatassignsacostof0iftherobotsuccessfully
graspstheobjectandacostof1otherwise.Inparticular,a\successful""graspisonethatliftsthe
objecttoacertainheight(
>
2cm)abovethetable.
Neuralnetworkpolicy.
Ourcontrolpolicymapsadepthimageofanobject(andacor-
respondingmaskimage)toagrasplocation(
x
g
;y
g
)andwristangle

g
.Agraspisexecutedby
servoingtherobot'sgrippertothegrasplocation(
x
g
;y
g
),settingthewristangletothedesired
22
Figure7
:
RepresentativeexamplesofobjectsfromtheShapeNetdatabase.
Figure8
:
Theneuralnetwork-basedarchitectureforourgraspingpolicies.
angle

g
,andthenexecutinganopen-loopgraspingmaneuverthatclosesthegrippersandliftsthe
robotarmup.
Thearchitectureforthepipelinethatmapsdepthimages(andcorrespondingmasks)tograspsis
illustratedinFigure8.Themaskimageisusedtoestimatethecenterofmass(COM)(
x
com
;y
com
)
oftheobject(bysimplycomputingthecentroidoftheobject).Following[24],therawdepth
imageis
colorized
viaajetcolormap.Thistransformsthedepthimagefromasingle-channelimage
intoathree-channel(RGB)image,thusallowingustore-useneuralnetworkarchitecturespre-
trainedontheImageNetdataset[18].Inparticular,wepassthecolorizeddepthimagethrougha
VGG16network[72]pretrainedonImageNetandtruncatedtooutputafeaturerepresentationof
size7

7

512.Thisfeaturevectorispassedthroughthreefully-connectedlayerswithsigmoid
activation.The(distributionsover)weightsofthesefully-connectedlayers(representedinFigure
8usingdashedlines)arelearnedusingthetrainingproceduredescribedbelow.Theoutputofthis
pipelineisa1

3vector
x;

y;
),whichiscombinedwiththeestimatedcenterofmassinorder
toobtainthetargetgrasplocationandorientation(
x
g
;y
g
;
g
):=
x
+
x
com
;

y
+
y
com
;
).
Training.
WeapplytheproceduredescribedinSection5.2fortrainingourstochasticcontrol
23
policy.Inparticular,weGaussiandistributions
N

overtheweights(andbiases)ofthe
fully-connectedlayersandchooseaGaussiandistribution
N

0
;s
0
asourpriordistribution.Our
particularchoiceofpriorismotivatedbythefactthattheCOMisareasonablegraspposition(i.e.,

x;

y
)=(0
;
0)isagoodguessforthegrasppositionintheabsenceofanyfurtherknowledge).
Inparticular,wesetthepriormean

0
byrandomlysamplingfromthedistribution
N
0
;
0
:
001
2
.Note
thatwhilewecouldhavechosen

0
tobezero,arandomlychosen

0
helpsinbreakingsymmetries
inthenetwork(see[22,AppendixB]forathoroughdiscussionofthispoint).Thepriorvariance
s
0
issetto0
:
01.
Forthepurposeofoptimizationviastochasticgradientdescent,weemployatiable
surrogatecostfunctioninplaceofthediscontinuous0-1cost.Inparticular,foreachobject
O
i
in
ourtrainingdataset,weexhaustivelyattempt750graspsbydiscretizingthespace
x;

y;
)
2
[

0
:
05cm
;
0
:
05cm]

[

0
:
05cm
;
0
:
05cm]

[0rad
;ˇ
rad]into5

5

30points.Asbefore,
x;

y
)
denotesaperturbationfromtheestimatedcentroidoftheobject.Foreachofthe750grasps,we
recordwhetherthegraspsucceededorfailed.Wethenchoosethemost\robust""graspforthe
giventrainingobjectbyselectingthegrasp
x
?
;

y
?
;
?
)thatismosttoleranttoerrorsin

(i.e.,
thegraspforwhichonecanperturb

bythelargestmagnitudeandstillsuccessfullygraspthe
object).Thisheuristicmeasureofrobustnessismotivatedbyourempiricalobservationthat,in
oursetting,changesingrasporientationhaveaverylargeimpactonwhetheragraspsucceedsor
not,whilesuccessislesssensitivetochangesinthegraspposition.Oursurrogatecostfunctionis
thencomputedasthemagnitudeofthebetween
x
?
;

y
?
;
?
)andthegrasp
x;

y;
)
predictedbyourneuralnetworkpolicy(notethatthismusttakeintoaccountthefact
that

liesonacircleandmustalsobescaledtoliebetween[0
;
1]sincecostsinourframework
areassumedtotakevaluesinthisrange).Importantly,weemploythissurrogatecost
only
for
optimization;allboundsandresultsarepresentedforthe0-1cost.
Results.
Weuse
N
=2000objectsrandomlyselectedfromtheShapeNetdatabaseasour
trainingobjects.Wechooseparameters

=0
:
009
;
0
=0
:
001,anduse
L
=1000
samplestoevaluatethesampleconvergenceboundinequation(23).TheresultingPAC-Bayes
bound
C
?
bound
is0
:
294.Thus,withprobability0
:
99oversampledtrainingdata,theoptimized
PAC-Bayescontrolpolicyisguaranteedtohaveanexpectedsuccessrateof70
:
6%onnovelobjects
(assumingthattheyaredrawnfromthesameunderlyingdistributionasthetrainingexamples).
Wehypothesizethatthisboundcouldbefurtherimprovedbyusingalargernumberofsamples
L
in
ordertoevaluatethesampleconvergenceboundinequation(23)(thiswouldcomeatanincreased
computationalcost).
WeevaluatedourPAC-Bayespolicyon1000testobjects(unseeninthetrainingphase).The
policywassuccessfulon82
:
0%oftheseobjects.Videosfromrepresentativetrialsontestobjects
canbefoundathttps://youtu.be/NGI0
oXBdqw.
Wealsocomparedourlearnedpolicywitha(deterministic)neuralnetworkpolicytrainedby
minimizingthetrainingcost(i.e.,withouttheregularizationthatcomesfromPAC-Bayes).Weused
anarchitecturethatisidenticaltothePAC-Bayespolicyandinitializedweightsforthenetworkin
thesamemanneraswell(byusingthemeansofthedistributionusedtotheinitialization
ofthestochasticPAC-Bayespolicy).Thesuccessratefortheresultingpolicyontestobjectsis
approximately78
:
0%(ascomparedto82
:
0%forthePAC-Bayespolicy).Wethusseethatwithout
theregularizationtermfromPAC-Bayes,thelearnedpolicyovtoalargerdegree.Wealso
notethatinadditiontoalossintheempiricalperformance,simplyminimizingthetrainingcost
doesnotallowustoobtainguaranteesongeneralizationperformance.
24
8HardwareImplementation
Inthissection,wepresentresultsfromhardwareexperimentsaimedatvalidatingourapproach.
ThehardwareplatformweuseistheParrotSwingdrone(Figure2(a)).Thislightweight(75g)
hybridvehicleisanappealingplatformsinceitcombinesverticaltak
andlandingwithhorizontalt(thusmakingitmoretthanatraditionalquadrotor
WeimplementourapproachfromSection5.1policyspaces)toachieve
obstacleavoidanceontenvironments.
Experimentalsetup.
TheSwingtakesfromoneendofanettedarea(seeFigure2(b))and
travelsataspeedof2
:
0m/s.Toachieveacostof0,theSwingmustavoidlarge,cylindrical
obstaclesoveratimehorizonof5secondsandlandsafely;otherwise,theSwingwillincuracost
of1.Additionally,weconsiderthenetencompassingthearea(7m

18m)as\wall""obstacles.
WeuseaViconmotiontrackingsystemtotracktheobstacleandSwing'slocations.Sincethe
Swingdoesnotpossessanysensorsfordetectingobstacles,we
simulate
a40-raydepthsensoras
ifitweremountedontheSwing.Thisisdoneusingthelocationsofobstacles,walls,andSwing
reportedbythemotioncapturesystem.Thus,theSwing
only
usesreal-timeinformationfromthis
simulateddepthsensor;wedonotprovideanyadditionalinformation(e.g.,theSwingorobstacle
locations,etc.).Thesesensormeasurementsareprovidedtoa\ground""computerthatcalculates
thecontrolinputgivenapolicy.ThecontrolinputstotheSwingcorrespondtopercentagesof
maximumroll,pitch,andyawangles,aswellastheverticalpositionoftheSwing.Commandsare
senttotheSwingat10HzviabluetoothusingthePyParrotpythonlibrary[55].Weimplementa
reactiveobstacleavoidancepolicy(identicaltotheonedescribedinSection7.1)ontheSwingfor
thediscretepolicyspacesetting.
Dynamicsmodel.
WetrainourpoliciesbyminimizingthePAC-Bayesboundinsimulation;
thelearneddistributionoverpoliciesisthenimplementedontheSwinghardwareforvalidation(on
environmentsnotseenduringtraining).WeperformedsystemidenontheSwingin
ordertoobtainanaccuratedynamicsmodel.IfwekeeptheSwing'sspeedandverticalposition
constant,wecanuseasimplemodelforitsdynamicssimilartotheoneforthegroundvehiclewith
states[
x;y; 
](Section7.1).WecankeeptheSwing'sspeedconstantbyitspitchangle

.
Thuswextheverticalpositionto1m,and

=27

;theSwingwillthentravelatabout
u
0
=2
:
0
m/s.Considerthefollowingdynamics:
2
4
_
x
_
y
_
 
3
5
=
2
4

u
0
sin(
 
)
u
0
cos(
 
)
k
p
(
k
u
u
 

 
)
3
5
;
(43)
wheretheonlycontrolinput
u
 
isapercentageoftheSwing'smaximumyawangle
 
,and
k
p
and
k
u
aregains.Both
k
p
and
k
u
arewithempiricaldatatocreatearealisticsimulation.The
gain
k
u
isneededtoscale
u
 
suchthat
k
u
u
 
=
 
if
u
 
remainsconstantandasteadystateis
reached.Welimit
k
u
u
 
2
[

ˇ
4
;
ˇ
4
]torestricttheSwingtomaneuversthatdonottly
changetheSwing'sforwardvelocityorverticalposition.Wedetermine
k
u
bymeasuring(with
theViconmotiontrackingsystem)thesteadystateyawanglegivenacontrolinput.Wethen
modeltheproportionalgain
k
p
withvariedinputsignalssuchassinusoidalandchirpfunctionsof
varyingamplitude.Theresultingdynamics,givenby
k
u
=3
:
0and
k
p
=0
:
4,areimplementedina
simulatedPyBulletenvironmentanalogoustotheonedescribedforthegroundvehicle.
25
Results.
Wechooseatimehorizon
T
=50;theSwingthenfor5
s
.Wethenchoose
L
=100
t
K
'sintheformof(41)with(
x
0
;y
0
)chosenbydiscretizingthespace[0
:
1
;
5
:
0]

[0
;
60
:
0]
into5and20valuesfor
x
0
and
y
0
respectively.Aswiththemethodforthegroundvehiclein
Section7.1,weanupperbound
C

bound
onthetrueexpectedcostofthePAC-Bayescontrol
policy
P
?
PAC
usingAlgorithm1.With1000trainingenvironments,
P
?
PAC
isguaranteed(with99%
probability)tosucceedonnewenvironments88.6%ofthetime.Theempiricalsuccessrate,tested
onSwinghardwareinunseenreal-worldenvironments,isapproximately90%(18/20trials).Videos
ofrepresentativetrialscanbefoundathttps://youtu.be/p5CjcSsojg8.
9DiscussionandConclusions
Wehavepresentedanapproachforlearningcontrolpoliciesthatprovablygeneralizewelltonovel
environmentsgivenadatasetofexampleenvironments.OurapproachleveragesPAC-Bayestheory
toobtainupperboundsontheexpectedcostof(stochastic)policiesonnovelenvironmentsandcan
beappliedtoroboticsystemswithcontinuousstateandactionspaces,complicateddynamics,rich
sensoryinputs,andneuralnetwork-basedpolicies.Wesynthesizepoliciesbyexplicitlyminimizing
thisupperboundusingconvexoptimizationinthecaseofapolicyspace,andusingstochastic
gradientdescentinthemoregeneralcaseofcontinuouslyparameterizedpolicies.Wealsopresent
anextensionofourapproachforlearningdistributionally-robustpolicies,i.e.,settingswheretest
environmentsaredrawnfromatdistributionthantrainingenvironments.Wedemonstrated
ourframeworkbylearning(i)depthsensor-basedobstacleavoidancepolicieswithguaranteeson
collision-freenavigationinnovelenvironments,and(ii)neuralnetwork-basedgraspingpolicieswith
guaranteesongeneralizationtonewobjects.Oursimulationresultscomparedthegeneralization
guaranteesprovidedbyourtechniquewithexhaustivenumericalevaluationsinordertodemonstrate
thatourapproachisabletoprovidestrongboundsevenwithrelativelyfewtrainingenvironments.
Ourhardwareexperiments{whichtestedpolicieslearnedusingourframeworkonareal-world
obstacleavoidanceexample{suggestthatourtechniqueisefordevelopingpoliciesthat
generalizewellto(unseen)real-worldenvironments.Webelievethattakentogether,thesimulation
andhardwareresultsprovidetevidencefortheabilityofourapproachtoprovidestrong
generalizationguaranteesinrealisticrobotcontrolsettings.
9.1ChallengesandFutureWork
Thereareanumberofchallengesandexcitingopportunitiesforfutureworkonboththetheoretical
andpracticalfronts.Wehighlightafewsuchdirectionshere.
Deterministicpolicies.
Itmaybedesirableinmanycases(e.g.,safety-criticalsettings)
tolearndeterministicpoliciesinsteadofstochasticones.Techniquesforconvertingstochastichy-
pothesesintodeterministichypotheseshavebeendevelopedwithinthePAC-Bayesframework(e.g.,
usingmajorityvotinginthesetting[45,42]);aninterestingavenueforfuturework
istoextendsuchtechniquestothepolicylearningsettingweconsiderhere.Anotherpossibility
istousetframeworksforobtaininggeneralizationboundsthatarebettersuitedtodeter-
ministicpolicies(e.g.,boundsbasedonalgorithmicstability[10,39,36]andsamplecompression
[28,43]).Animportantfeatureofthereduction-basedperspectivewepresentedinSection4is
thatitimmediatelyallowsustoportoversuchboundsfromthesupervisedlearningsettingtoour
setting.
26
Choosingtheprior.
Whilewehavedemonstratedthatourframeworkallowsustoobtain
strongboundsongeneralizationperformance,animportantdirectionforfutureworkistoways
tofurtherimprovethesebounds.Webelievethataparticularlypromisingapproachfordoingthis
istosystematicallychoosetheprior
P
0
overthecontrolpolicyspace.Theabilitytospecifyastrong
priorisanimportantdistinctionbetweentherobotcontrolsettingsconsideredinthispaperand
standardsupervisedlearningproblems(e.g.,imagerecognition).Forstandardsupervisedlearning
problems,itisoftenchallengingtospecifyaprioroverthespaceofhypotheses.Whilethepriors
intheexamplesconsideredinthispaperwerechoseninafairlysimplisticmanner(e.g.,auniform
prioroverthepolicyspacefortheobstacleavoidanceexampleinSection7.1,orapriorthat
attemptstokeepthegrasppositionclosetothecenterofmassinthegraspingexampleinSection
7.2),webelievethatchoosingpriorsinamoresystematicmannercouldtlyimprovethe
generalizationbounds.Onepossibilityforchoosingapriormorecarefullyistoembeddomain
knowledgeintotheprior;forexample,onecouldchooseapriorthatincorporatesaphysicsmodel
ofthesystem,oronethatisderivedfromanexistingstate-of-the-artapproachfortheproblem
underconsideration(e.g.,choosingapriorthatencouragesforce-closuregrasps).Anotherpromising
possibilityistolearnthepriorfromahumanexpertusingimitationlearning.Byincorporatingsuch
priorsforrobotcontrolproblems,wemayneedcantlysmallerdatasetsthantheonescurrently
usedtotrainstate-of-the-artsupervisedlearningmodelswhilestillobtainingstronggeneralization
guarantees.
Incorporatingtregularizers.
Thealgorithmicapproachweemployinthiswork
(Section5)involvesminimizingacombinationofthetrainingcostandaregularizerspby
PAC-Bayestheory.ThisismotivatedbythedesiretooptimizethePAC-Bayesupperboundonthe
expectedcostonnovelenvironments.However,wenotethatthereareavarietyofregularization
techniquesthathavebeenempiricallydemonstratedtopromotegeneralization(e.g.,dropout[73]
andoverparameterization[59,78,4]),inadditiontoothertechniquessuchasdomainrandomization
[75]andbatchnormalization[37].Whilethesetechniquesdonotyethavestronggeneralization
boundsassociatedwiththem,thereisagrowingliteratureonthistopic[53,4,48,8].Incorpo-
ratingtregularizationschemesintoourframeworkwhilemaintainingstronggeneralization
guaranteesisapromisingdirectionforfuturework.
Extensionstometa-learning.
Anotherexcitingfuturedirectionistocombinethetechniques
presentedherewith
meta-learning
techniquesinordertoachieveprovablytcontrolon
noveltasks.Sp,wearecurrentlyinvestigatingusingaPAC-Bayesboundaspartofthe
objectiveofameta-learningalgorithmsuchasMAML[27]toachieveimprovedgeneralization
performanceandfew-shotlearning.
Webelievethattheapproachpresentedherealongwiththeindicatedfuturedirectionsrepresent
animportantsteptowardslearningcontrolpolicieswithprovableguaranteesforchallengingrobotic
platformswithrichsensoryinputsoperatinginnovelenvironments.
Acknowledgements
TheauthorsaregratefultoMaxGoldsteinforinitiatingthegraspingexampleinSection7.2and
contributionstotheconferenceversionofthispaperpresentedatCoRL2018.Wealsogratefully
acknowledgethesupportofNVIDIACorporationwiththedonationoftheTitanXpGPUusedfor
thisresearch.
27
Funding
TheauthorswerepartiallysupportedbytheofNavalResearch[AwardNumber:N00014-18-
1-2873],theNationalScienceFoundation[IIS-1755038],theGoogleFacultyResearchAward,and
theAmazonResearchAward.
References
[1]
P.Agrawal,A.V.Nair,P.Abbeel,J.Malik,andS.Levine.Learningtopokebypoking:Ex-
perientiallearningofintuitivephysics.In
AdvancesinNeuralInformationProcessingSystems
,
pages5074{5082,2016.
[2]
D.M.andS.Scherer.Onlinesafetyvoftrajectoriesforunmanned
twithcomputedrobustinvariantsets.In
IEEE/RSJInternationalConferenceon
IntelligentRobotsandSystems(IROS)
,pages3470{3477.IEEE,2015.
[3]
R.C.Arkin.
Behavior-BasedRobotics
.MITpress,1998.
[4]
S.Arora,N.Cohen,andE.Hazan.Ontheoptimizationofdeepnetworks:Implicitacceleration
byoverparameterization.
arXivpreprintarXiv:1802.06509
,2018.
[5]
J.A.Bagnell.
LearningDecisions:Robustness,Uncertainty,andApproximation
.PhDthesis,
CarnegieMellonUniversity,Pittsburgh,PA,August2004.
[6]
J.A.BagnellandJ.G.Schneider.Autonomoushelicoptercontrolusingreinforcementlearning
policysearchmethods.In
ProceedingsoftheIEEEInternationalConferenceonRoboticsand
Automation(ICRA)
,volume2,pages1615{1620.IEEE,2001.
[7]
A.Beyeler,J.-C.,andD.Floreano.Vision-basedcontrolofnear-obstaclet.
Au-
tonomousRobots
,27(3):201,2009.
[8]
N.Bjorck,C.P.Gomes,B.Selman,andK.Q.Weinberger.Understandingbatchnormalization.
In
AdvancesinNeuralInformationProcessingSystems
,pages7694{7705,2018.
[9]
L.Blackmore,H.Li,andB.Williams.Aprobabilisticapproachtooptimalrobustpath
planningwithobstacles.In
ProceedingsoftheIEEEAmericanControlConference(ACC)
.
IEEE,2006.
[10]
O.BousquetandA.Stabilityandgeneralization.
JournalofMachineLearning
Research
,2(Mar):499{526,2002.
[11]
S.BoydandL.Vandenberghe.
ConvexOptimization
.CambridgeUniversityPress,2004.
[12]
R.I.BrafmanandM.Tennenholtz.R-max{Ageneralpolynomialtimealgorithmfornear-
optimalreinforcementlearning.
JournalofMachineLearningResearch
,3(Oct):213{231,2002.
[13]
V.ChandrasekaranandP.Shah.Relativeentropyoptimizationanditsapplications.
Mathe-
maticalProgramming
,161(1-2):1{32,2017.
28
[14]
A.X.Chang,T.Funkhouser,L.Guibas,P.Hanrahan,Q.Huang,Z.Li,S.Savarese,M.Savva,
S.Song,H.Su,J.Xiao,L.Yi,andF.Yu.ShapeNet:AnInformation-Rich3DModelReposi-
tory.TechnicalReportarXiv:1512.03012[cs.GR],StanfordUniversity|PrincetonUniversity
|ToyotaTechnologicalInstituteatChicago,2015.
[15]
A.CharnesandW.W.Cooper.Chance-constrainedprogramming.
ManagementScience
,6
(1):73{79,1959.
[16]
J.Conroy,G.Gremillion,B.Ranganathan,andJ.S.Humbert.Implementationof
integrationofopticwforautonomousquadrotornavigation.
AutonomousRobots
,27(3):189,
2009.
[17]
E.CoumansandY.Bai.Pybullet,apythonmoduleforphysicssimulationforgames,robotics
andmachinelearning,2018.
[18]
J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei.Imagenet:Alarge-scalehierarchi-
calimagedatabase.In
ProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition
,pages248{255.IEEE,2009.
[19]
S.DiamondandS.Boyd.CVXPY:APython-embeddedmodelinglanguageforconvexopti-
mization.
JournalofMachineLearningResearch
,17(83):1{5,2016.
[20]
A.Domahidi,E.Chu,andS.Boyd.ECOS:AnSOCPsolverforembeddedsystems.In
EuropeanControlConference(ECC)
,pages3071{3076,2013.
[21]
M.D.DonskerandS.R.S.Varadhan.AsymptoticevaluationofcertainMarkovprocess
expectationsforlargetime.
CommunicationsonPureandAppliedMathematics
,28:1{47,
1975.
[22]
G.K.DziugaiteandD.M.Roy.Computingnonvacuousgeneralizationboundsfordeep
(stochastic)neuralnetworkswithmanymoreparametersthantrainingdata.
arXivpreprint
arXiv:1703.11008
,2017.
[23]
G.K.DziugaiteandD.M.Roy.Entropy-SGDoptimizesthepriorofaPAC-Bayesbound:
Data-dependentPAC-Bayespriorsviatialprivacy.
arXivpreprintarXiv:1712.09376
,
2017.
[24]
A.Eitel,J.T.Springenberg,L.Spinello,M.Riedmiller,andW.Burgard.Multimodaldeep
learningforrobustRGB-Dobjectrecognition.In
ProceedingsoftheIEEE/RSJInternational
ConferenceonIntelligentRobotsandSystems(IROS)
,pages681{687.IEEE,2015.
[25]
M.M.FardandJ.Pineau.PAC-Bayesianmodelselectionforreinforcementlearning.In
AdvancesinNeuralInformationProcessingSystems
,pages1624{1632,2010.
[26]
M.M.Fard,J.Pineau,andC.Szepari.PAC-Bayesianpolicyevaluationforreinforcement
learning.
arXivpreprintarXiv:1202.3717
,2012.
[27]
C.Finn,P.Abbeel,andS.Levine.Model-agnosticmeta-learningforfastadaptationofdeep
networks.
arXivpreprintarXiv:1703.03400
,2017.
29
[28]
S.FloydandM.Warmuth.Samplecompression,learnability,andtheVapnik-Chervonenkis
dimension.
MachineLearning
,21(3):269{304,1995.
[29]
T.Fraichard.Ashortpaperaboutmotionsafety.In
ProceedingsoftheIEEEInternational
ConferenceonRoboticsandAutomation(ICRA)
,pages1140{1145.IEEE,2007.
[30]
B.A.Francis.
ACourseinControlTheory
.Springer-Verlag,1987.
[31]
J.FuandU.Topcu.ProbablyapproximatelycorrectMDPlearningandcontrolwithtemporal
logicconstraints.
arXivpreprintarXiv:1404.7073
,2014.
[32]
P.Germain,A.Lacasse,F.Laviolette,andM.Marchand.PAC-Bayesianlearningoflinear
In
Proceedingsofthe26thAnnualInternationalConferenceonMachineLearning
,
pages353{360.ACM,2009.
[33]
R.M.Gray.
EntropyandInformationTheory
.SpringerScience&BusinessMedia,2ndedition,
2011.
[34]
S.Gupta,J.Davidson,S.Levine,R.Sukthankar,andJ.Malik.Cognitivemappingand
planningforvisualnavigation.In
ProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognition
,pages2616{2625,2017.
[35]
S.Gupta,D.Fouhey,S.Levine,andJ.Malik.Unifyingmapandlandmarkbasedrepresenta-
tionsforvisualnavigation.
arXivpreprintarXiv:1712.08125
,2017.
[36]
M.Hardt,B.Recht,andY.Singer.Trainfaster,generalizebetter:Stabilityofstochastic
gradientdescent.
arXivpreprintarXiv:1509.01240
,2015.
[37]
S.eandC.Szegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducing
internalcovariateshift.
arXivpreprintarXiv:1502.03167
,2015.
[38]
L.P.Kaelbling,M.L.Littman,andA.R.Cassandra.Planningandactinginpartially
observablestochasticdomains.
AIntelligence
,101(1-2):99{134,1998.
[39]
M.KearnsandD.Ron.Algorithmicstabilityandsanity-checkboundsforleave-one-outcross-
validation.
NeuralComputation
,11(6):1427{1453,1999.
[40]
M.KearnsandS.Singh.Near-optimalreinforcementlearninginpolynomialtime.
Machine
Learning
,49(2-3):209{232,2002.
[41]
M.J.Kearns,Y.Mansour,andA.Y.Ng.ApproximateplanninginlargePOMDPsviareusable
trajectories.In
AdvancesinNeuralInformationProcessingSystems
,pages1001{1007,2000.
[42]
A.Lacasse,F.Laviolette,M.Marchand,P.Germain,andN.Usunier.PAC-Bayesboundsfor
theriskofthemajorityvoteandthevarianceoftheGibbsIn
AdvancesinNeural
InformationProcessingSystems
,pages769{776,2007.
[43]
J.Langford.Tutorialonpracticalpredictiontheoryfor
JournalofMachine
LearningResearch
,6(Mar):273{306,2005.
[44]
J.LangfordandR.Caruana.(Not)boundingthetrueerror.In
AdvancesinNeuralInformation
ProcessingSystems
,pages809{816,2002.
30
[45]
J.LangfordandJ.Shawe-Taylor.PAC-Bayes&margins.In
AdvancesinNeuralInformation
ProcessingSystems
,pages439{446,2003.
[46]
I.Lenz,H.Lee,andA.Saxena.Deeplearningfordetectingroboticgrasps.
TheInternational
JournalofRoboticsResearch
,34(4-5):705{724,2015.
[47]
S.Levine,C.Finn,T.Darrell,andP.Abbeel.End-to-endtrainingofdeepvisuomotorpolicies.
TheJournalofMachineLearningResearch
,17(1):1334{1373,2016.
[48]
Y.LiandY.Liang.Learningoverparameterizedneuralnetworksviastochasticgradientdescent
onstructureddata.In
AdvancesinNeuralInformationProcessingSystems
,pages8157{8166,
2018.
[49]
J.Mahler,J.Liang,S.Niyaz,M.Laskey,R.Doan,X.Liu,J.A.Ojea,andK.Goldberg.
Dex-Net2.0:Deeplearningtoplanrobustgraspswithsyntheticpointcloudsandanalytic
graspmetrics.
arXivpreprintarXiv:1703.09312
,2017.
[50]
A.MajumdarandM.Goldstein.PAC-BayesControl:synthesizingcontrollersthatprovably
generalizetonovelenvironments.In
ProceedingsoftheConferenceonRobotLearning(CoRL)
,
2018.
[51]
A.MajumdarandR.Tedrake.Funnellibrariesforreal-timerobustfeedbackmotionplanning.
TheInternationalJournalofRoboticsResearch(IJRR)
,36(8):947{982,July2017.
[52]
A.Maurer.AnoteonthePACBayesiantheorem.
arXivpreprintcs/0411099
,2004.
[53]
D.McAllester.APAC-Bayesiantutorialwithadropoutbound.
arXivpreprint
arXiv:1307.2118
,2013.
[54]
D.A.McAllester.SomePAC-Bayesiantheorems.
MachineLearning
,37(3):355{363,1999.
[55]
A.McGovern.Pyparrot1.5.21,2019.URL
https://github.com/amymcgovern/pyparrot
.
[56]
MOSEKApS.Mosekfusionapiforpython9.0.84(beta),2019.URL
https://docs.mosek.
com/9.0/pythonfusion/index.html
.
[57]
Y.NesterovandA.Nemirovskii.
Interior-pointpolynomialalgorithmsinconvexprogramming
,
volume13.SIAM,1994.
[58]
G.Neu,A.Jonsson,andV.omez.Aviewofentropy-regularizedmarkovdecision
processes.
arXivpreprintarXiv:1705.07798
,2017.
[59]
B.Neyshabur,R.Tomioka,andN.Srebro.Insearchoftherealinductivebias:ontheroleof
implicitregularizationindeeplearning.
arXivpreprintarXiv:1412.6614
,2014.
[60]
B.Neyshabur,S.Bhojanapalli,D.McAllester,andN.Srebro.APAC-Bayesianapproachto
spectrally-normalizedmarginboundsforneuralnetworks.
preprintarXiv:1707.09564
,2017.
[61]
B.Neyshabur,S.Bhojanapalli,D.McAllester,andN.Srebro.Exploringgeneralizationindeep
learning.In
AdvancesinNeuralInformationProcessingSystems
,pages5949{5958,2017.
31
[62]
X.Nguyen,M.J.Wainwright,andM.I.Jordan.Estimatingdivergencefunctionalsandthe
likelihoodratiobyconvexriskminimization.
IEEETransactionsOnInformationTheory
,56
(11):5847{5861,2010.
[63]
B.O'Donoghue,E.Chu,N.Parikh,andS.Boyd.Conicoptimizationviaoperatorsplitting
andhomogeneousself-dualembedding.
JournalofOptimizationTheoryandApplications
,169
(3):1042{1068,June2016.URL
http://stanford.edu/
~
boyd/papers/scs.html
.
[64]
B.O'Donoghue,E.Chu,N.Parikh,andS.Boyd.SCS:Splittingconicsolver,version2.0.2.
https://github.com/cvxgrp/scs
,Nov.2017.
[65]
M.Ono,M.Pavone,Y.Kuwata,andJ.Balaram.Chance-constraineddynamicprogramming
withapplicationtorisk-awareroboticspaceexploration.
AutonomousRobots
,39(4):555{571,
2015.
[66]
C.RichterandN.Roy.Safevisualnavigationviadeeplearningandnoveltydetection.In
ProceedingsofRobotics:ScienceandSystems(RSS)
,2017.
[67]
C.Richter,W.Vega-Brown,andN.Roy.Bayesianlearningforsafehigh-speednavigationin
unknownenvironments.In
ProceedingsoftheInternationalSymposiumonRoboticsResearch
(ISRR)
,2015.
[68]
S.Ross,N.Melik-Barkhudarov,K.S.Shankar,A.Wendel,D.Dey,J.A.Bagnell,and
M.Hebert.Learningmonocularreactiveuavcontrolinclutterednaturalenvironments.In
ProceedingsoftheIEEEInternationalConferenceonRoboticsandAutomation(ICRA)
,pages
1765{1772.IEEE,2013.
[69]
T.Schouwenaars,J.How,andE.Feron.Recedinghorizonpathplanningwithimplicitsafety
guarantees.In
ProceedingsoftheIEEEAmericanControlConference(ACC)
,volume6,pages
5576{5581.IEEE,2004.
[70]
J.Schulman,S.Levine,P.Abbeel,M.Jordan,andP.Moritz.Trustregionpolicyoptimization.
In
ProceedingsoftheInternationalConferenceonMachineLearning
,pages1889{1897,2015.
[71]
M.Seeger.PAC-Bayesiangeneralisationerrorboundsforgaussianprocess
Jour-
nalofMachineLearningResearch
,3(Oct):233{269,2002.
[72]
K.SimonyanandA.Zisserman.Verydeepconvolutionalnetworksforlarge-scaleimagerecog-
nition.
CoRR
,abs/1409.1556,2014.
[73]
N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhutdinov.Dropout:a
simplewaytopreventneuralnetworksfromovg.
TheJournalofMachineLearning
Research
,15(1):1929{1958,2014.
[74]
N.Sunderhauf,O.Brock,W.Scheirer,R.Hadsell,D.Fox,J.Leitner,B.Upcroft,P.Abbeel,
W.Burgard,M.Milford,andP.Corke.Thelimitsandpotentialsofdeeplearningforrobotics.
TheInternationalJournalofRoboticsResearch(IJRR)
,37(4-5):405{420,2018.
[75]
J.Tobin,W.Zaremba,andP.Abbeel.Domainrandomizationandgenerativemodelsfor
roboticgrasping.
arXivpreprintarXiv:1710.06425
,2017.
32
[76]
M.P.VitusandC.J.Tomlin.Closed-loopbeliefspaceplanningforlinear,Gaussiansystems.
In
ProceedingsoftheIEEEInternationalConferenceonRoboticsandAutomation(ICRA)
,
pages2152{2159.IEEE,2011.
[77]
WolframResearch,Inc.Mathematica,Version12.0,2019.URL
https://www.wolfram.com/
mathematica/
.Champaign,IL.
[78]
C.Zhang,S.Bengio,M.Hardt,B.Recht,andO.Vinyals.Understandingdeeplearning
requiresrethinkinggeneralization.
arXivpreprintarXiv:1611.03530
,2016.
[79]
Y.Zhu,R.Mottaghi,E.Kolve,J.J.Lim,A.Gupta,L.Fei-Fei,andA.Farhadi.Target-
drivenvisualnavigationinindoorscenesusingdeepreinforcementlearning.In
Proceedings
oftheIEEEInternationalConferenceonRoboticsandAutomation(ICRA)
,pages3357{3364.
IEEE,2017.
33
"
78,Non-asymptotic Identification of LTI Systems from a Single Trajectory,http://arxiv.org/pdf/1806.05722v2.pdf,https://github.com/zhengy09/SysId,"Non-asymptoticIdenti˝cationofLTISystems
fromaSingleTrajectory
SametOymak
*
andNecmiyeOzay
—–
Abstract
Weconsidertheproblemoflearningarealizationforalineartime-invariant(LTI)dynamicalsystem
frominput/outputdata.Givenasingleinput/outputtrajectory,weprovide˝nitetimeanalysisfor
learningthesystem'sMarkovparameters,fromwhichabalancedrealizationisobtainedusingtheclassical
Ho-Kalmanalgorithm.ByprovingastabilityresultfortheHo-Kalmanalgorithmandcombiningitwith
thesamplecomplexityresultsforMarkovparameters,weshowhowmuchdataisneededtolearna
balancedrealizationofthesystemuptoadesiredaccuracywithhighprobability.
1
Introduction
Manymoderncontroldesigntechniquesrelyontheexistenceofafairlyaccuratestate-spacemodelofthe
planttobecontrolled.Althoughinsomecasesamodelcanbeobtainedfrom˝rstprinciples,thereare
manysituationsinwhichamodelshouldbelearnedfrominput/outputdata.Classicalresultsinsystem
identi˝cationprovideasymptoticconvergenceguaranteesforlearningmodelsfromdata[
19
,
29
].However,
˝nitesamplecomplexitypropertieshavebeenrarelydiscussedinsystemidenti˝cationliterature[
31
];and
earlierresultsareconservative[
25
].
Thereisrecentinterestfromthemachinelearningcommunityindata-drivencontrolandnon-asymptotic
analysis.Puttingasidethereinforcementlearningliteratureandrestrictingourattentiontolinearstate-space
models,theworkinthisareacanbedividedintotwocategories:(i)directlylearningthecontrolinputsto
optimizeacontrolobjectiveoranalyzingthepredictivepowerofthelearnedrepresentation[
6
,
8
,
14
],(ii)
learningtheparametersofthesystemmodelfromlimiteddata[
2

4
,
13
,
21
,
25
].Fortheformerproblem,the
focushasbeenonexploration/exploitationtypeformulationsandregretanalysis.Sincethegoalistolearn
howtocontrolthesystemtoachieveaspeci˝ctask,thesystemisnotnecessarilyfullylearned.Ontheother
hand,thelatterproblemaimstolearnageneralpurposemodelthatcanbeusedindi˙erentcontroltasks,
forinstance,bycombiningitwithrobustcontroltechniques[
3
,
4
,
27
].Thefocusforthelatterworkhasbeen
toanalyzedauracytrade-o˙s.
InthispaperwefocusonlearningarealizationforanLTIsystemfromasingle
input/output
trajectory.
Thissettingissigni˝cantlymorechallengingthanearlierstudiesthatassumethat(multipleindependent)
state
trajectoriesareavailable[
4
,
25
].Oneofourmaincontributionsistoderivesamplecomplexityresultsin
learningtheMarkovparameters,tobepreciselyde˝nedlater,ofthesystemusingaleastsquaresalgorithm[
10
].
Markovparametersplayacentralroleinsystemidenti˝cation[
19
]andtheycanalsobedirectlyusedin
controldesignwhenthesystemmodelitselfisnotavailable[
12
,
24
,
26
].InSection
4
,weshowthatusing
fewMarkovparameterestimatesandleveragingstabilityassumption,onecanapproximatesystem'sHankel
operatorwithnearoptimalsamplesize.Whenonlyinput/outputdataisavailable,itiswellknownthat
thesystemmatricescanbeidenti˝edonlyuptoasimilaritytransformationeveninthenoise-freecasebut
*
DepartmentofElectricalandComputerEngineering,UniversityofCalifornia,Riverside,CA.
—
ElectricalEngineeringandComputerScienceDepartment,UniversityofMichigan,AnnArbor,MI.
–
Version2hastwoimprovements:First,papernowusesspectralradiusratherthanlargestsingularvaluehenceappliestoa
largerclassofsystems.Secondly,newsamplecomplexityboundsareprovidedforapproximatingthesystem'sHankeloperator
viaestimatedMarkovparameters.Theseboundsleveragestabilityandtreatthesystemasifithasalogarithmicorder.
1
arXiv:1806.05722v2  [cs.LG]  3 Feb 2019Markovparametersareidenti˝able.Therefore,wefocusonobtainingarealization.Oneclassicaltechniqueto
derivearealizationfromtheMarkovparametersistheHo-Kalman(a.k.a.,eigensystemrealizationalgorithm
ERA)algorithm[
15
].TheHo-Kalmanalgorithmconstructsabalancedrealization
1
forthesystemfromthe
singularvaluedecompositionoftheHankelmatrixoftheMarkovparameters.Byprovingastabilityresult
fortheHo-Kalmanalgorithmandcombiningitwiththesamplecomplexityresults,weshowhowmuchdata
isneededtolearnabalancedrealizationofthesystemuptoadesiredaccuracywithhighprobability.
2
ProblemSetup
We˝rstintroducethebasicnotation.Spectralnorm
Y

Y
returnsthelargestsingularvalueofamatrix.
Multivariatenormaldistributionwithmean

andcovariancematrix

isdenotedby
N
‹

;

“
.
X
⁄
denotes
thetransposeofamatrix
X
.
X
—
returnstheMoenroseinverseofthematrix
X
.Covariancematrix
ofarandomvector
v
isdenotedby

‹
v
“
.
tr
‹

“
returnsthetraceofamatrix.
c;C;c
œ
;c
1
;c
2
;:::
standsfor
absoluteconstants.
Supposewehaveanobservableandcontrollablelinearsystemcharacterizedbythesystemmatrices
A
>
R
n

n
;
B
>
R
n

p
;
C
>
R
m

n
;
D
>
R
m

p
andthissystemevolvesaccordingto
x
t

1

Ax
t

Bu
t

w
t
;
(2.1)
y
t

Cx
t

Du
t

z
t
:
(2.2)
Ourgoalistolearnthecharacteristicsofthissystemandtoprovide˝nitesampleboundsontheestimation
accuracy.Givenahorizon
T
,wewilllearnthe˝rst
T
Markovparametersofthesystem.The˝rstMarkov
parameteristhematrix
D
,andtheremainingparametersarethesetofmatrices
Ÿ
CA
i
B
š
T

2
i

0
.Asitwillbe
discussedlateron,bylearningtheseparameters,
‹
wecanprovideboundsonhowwell
y
t
canbeestimatedforafuturetime
t
,
‹
wecanidentifythestate-spacematrices
A
;
B
;
C
;
D
(uptoasimilaritytransformation).
Problemsetup:
Weassumethat
Ÿ
u
t
;
w
t
;
z
t
š
ª
t

1
arevectorsthatareindependentofeachotherwith
distributions
u
t

N
‹
0
;˙
2
u
I
p
“
,
w
t

N
‹
0
;˙
2
w
I
n
“
,and
z
t

N
‹
0
;˙
2
z
I
m
“
2
.
u
t
istheinputvectorwhichisknown
tous.
w
t
and
z
t
aretheprocessandmeasurementnoisevectorsrespectively.Wealsoassumethattheinitial
conditionofthehiddenstateis
x
1

0
.ObservethatMarkovparameterscanbefoundifwehaveaccessto
crosscorrelations
E

y
t
u
⁄
t

k

.Inparticular,wehavetheidentities
E

y
t
u
⁄
t

k
˙
2
u


¢
¨
¨
¦
¨
¨
¤
D
if
k

0
;
CA
k

1
B
if
k
C
1
:
Hence,ifwehadaccesstoin˝nitelymanyindependent
‹
y
t
;
u
t

k
“
pairs,ourtaskcouldbeaccomplishedbya
simpleaveraging.Inthiswork,wewillshowthat,onecanrobustlylearnthesematricesfromasmallamount
ofdatageneratedfromasinglerealizationofthesystemtrajectory.Thechallengeise˚cientlyusing˝nite
anddependentdatapointstoperformreliableestimation.Observethat,ourproblemisidenticaltolearning
theconcatenatedmatrix
G
de˝nedas
G


D
;
CB
;
CAB
;:::;
CA
T

2
B

>
R
m

Tp
:
Nextsectiondescribesourinputandoutputdata.Basedonthis,weformulatealeast-squaresprocedurethat
estimates
G
.Theestimate
^
G
willplayacriticalroleintheidenti˝cationofthesystemmatrices.
1
Balancedrealizationsgivearepresentationofthesysteminabasisthatordersthestatesintermsoftheire˙ectonthe
input/outputbehavior.Thisisrelevantfordeterminingthesystemorderandformodelreduction[
23
].
2
Whileweassumediagonalcovariancethroughoutthepaper,webelieveourproofstrategycanbeadaptedtoarbitrarycovariance
matrices.
2
2.1
Least-SquaresProcedure
Todescribetheestimationprocedure,westartbyexplainingthedatacollectionprocess.Givenasingle
input/outputtrajectory
Ÿ
y
t
;
u
t
š

N
t

1
,wegenerate
N
subsequencesoflength
T
,where

N

T

N

1
and
N
C
1
.Toeaserepresentation,weorganizethedata
u
t
andthenoise
w
t
intolength
T
chunksdenotedbythe
followingvectors,

u
t


u
⁄
t
u
⁄
t

1
:::
u
⁄
t

T

1

⁄
>
R
Tp
;
(2.3)

w
t


w
⁄
t
w
⁄
t

1
:::
w
⁄
t

T

1

⁄
>
R
Tn
:
(2.4)
Inasimilarfashionto
G
de˝nethematrix,
F


0
CCA
:::
CA
T

2

>
R
m

Tn
:
ToestablishanexplicitconnectiontoMarkovparameters,
y
t
canbeexpandedrecursivelyuntil
t

T

1
to
relatetheoutputtotheinput

u
t
andMarkovparametermatrix
G
asfollows,
y
t

Cx
t

Du
t

z
t
;

C
‹
Ax
t

1

Bu
t

1

w
t

1
“

Du
t

z
t
;

CA
T

1
x
t

T

1

T

1
Q
i

1
CA
i

1
Bu
t

i

T

1
Q
i

1
CA
i

1
w
t

i

Du
t

z
t
;

G

u
t

F

w
t

z
t

e
t
;
(2.5)
where,
e
t

CA
T

1
x
t

T

1
correspondstotheerrorduetothee˙ectofthestateattime
t

T

1
.Withthis
relation,wewilluse
‹

u
t
;
y
t
“

N
t

T
asinputsandoutputsofourregressionproblem.Wetreat

w
t
,
z
t
,and
e
t
asadditivenoiseandattempttoestimate
G
fromcovariates

u
t
.Notethat,thenoisetermsarezero-mean
including
e
t
sinceweassumed
x
1

0
.Withtheseinmind,weformthefollowingleast-squaresproblem,
^
G

argmin
X
>
R
m

Tp

N
Q
t

T
Y
y
t

X

u
t
Y
2
`
2
:
De˝ningourlabelmatrix
Y
andinputdatamatrix
U
as,
Y


y
T
;
y
T

1
;:::;
y

N

⁄
>
R
N

m
and
U



u
T
;

u
T

1
;:::;

u

N

⁄
>
R
N

Tp
;
(2.6)
weobtaintheminimization
min
X
Y
Y

UX
⁄
Y
2
F
.Hence,theleast-squaressolution
^
G
isgivenby
^
G

‹
U
—
Y
“
⁄
;
(2.7)
where
U
—

‹
U
⁄
U
“

1
U
⁄
istheleftpseudo-inverseof
U
.Ideally,wewouldliketheestimationerror
Y
G

^
G
Y
2
F
tobesmall.Ourmainresultboundsthenormoftheerrorasafunctionofthesamplesize
N
andnoiselevels
˙
w
and
˙
z
.
3
ResultsonLearningMarkovParameters
Let
ˆ
‹

“
denotethespectralradiusofamatrixwhichisthelargestabsolutevalueofitseigenvalues.Ourresults
inthissectionapplytostablesystemswhere
ˆ
‹
A
“
@
1
.Additionallyweneedarelatedquantityinvolving
A
whichisthespectralnormtospectralradiusratioofitsexponentsde˝nedas

‹
A
“

sup
˝
C
0
Y
A
˝
Y
ˆ
‹
A
“
˝
.Wewill
assume

‹
A
“
@
ª
whichisamildcondition:Forinstance,if
A
isdiagonalizable,

‹
A
“
isafunctionofits
eigenvectormatrixandis˝nite.Anotherimportantparameteristhesteadystatecovariancematrixof
x
t
whichisgivenby

ª

ª
Q
i

0
˙
2
w
A
i
‹
A
⁄
“
i

˙
2
u
A
i
BB
⁄
‹
A
⁄
“
i
:
3
Itisrathertrivialtoshowthatforall
t
C
1
,

‹
x
t
“
j

ª
.Wewilluse

ª
toboundtheerror
e
t
duetothe
unknownstateattime
t

T

1
.Followingthede˝nitionof
e
t
,wehavethat
Y

‹
e
t
“Y
B
Y
CA
T

1
Y
2
Y

ª
Y
.We
characterizetheimpactof
e
t
byitstivestandard
˙
e
thatisobtainedbyscalingtheboundon
»
Y

‹
e
t
“Y
byanadditionalfactor

‹
A
“
»
T
~‹
1

ˆ
‹
A
“
2
T
“
whichyields,
˙
e


‹
A
“Y
CA
T

1
Y
¿
Á
Á
À
T
Y

ª
Y
1

ˆ
‹
A
“
2
T
:
(3.1)
Our˝rstresultisasimpli˝edversionofTheorem
3.2
andcapturestheproblemdependenciesintermsofthe
totalstandarddeviations
˙
z

˙
e

˙
w
Y
F
Y
andthe
totaldimensions
m

p

n
.
Theorem3.1
Suppose
ˆ
‹
A
“
T
B
0
:
99
and
N
C
N
0

cTq
log
2
‹
2
Tq
“
log
2
‹
2
Nq
“
where
q

p

n

m
.Given
observationsofasingletrajectoryuntiltime

N

N

T

1
,withhighprobability
3
,theleast-squareestimator
oftheMarkovparametermatrixobeys
Y
^
G

G
Y
B
˙
z

˙
e

˙
w
Y
F
Y
˙
u
¾
N
0
N
:
Remark:
Ourresultisstatedintermsofthespectralnormerror
Y
^
G

G
Y
.Onecandeducethefollowing
Frobeniusnormboundbynaivelybounding
˙
e
;˙
z
termsandswapping
Y
F
Y
termby
Y
F
Y
F
(following
(
A.2
)
,
(
A.3
)).Thisyields,
Y
^
G

G
Y
F
B
‹
˙
z

˙
e
“
º
m

˙
w
Y
F
Y
F
˙
u
¼
N
0
N
.
Ourboundindividuallyaccountsforthetheprocessnoisesequence
Ÿ
w
˝
š
t
˝

t

T

1
,measurementnoise
z
t
,
andthecontributionoftheunknownstate
x
t

T

1
.Setting
˙
w
and
˙
z
to
0
,weendupwiththeunknown
statecomponent
˙
e
.
˙
e
hasa
Y
CA
T

1
Y
multiplierinsidehencelarger
T
impliessmaller
˙
e
.Ontheother
hand,larger
T
increasesthesizeofthe
G
matrixasitsdimensionsare
m

Tp
.Thisdependenceiscontained
insidethe
N
0
termwhichgrowsproportionalto
Tp
(ignoring
log
terms).
Tp
correspondstotheminimum
observationperiodsincethereare
mTp
unknownsandwegettoobserve
m
measurementsateachtimestamp.
Hence,ignoringlogarithmicterms,ourresultrequires
N
à
Tp
andestimationerrordecaysas
»
Tp
~
N
.This
behaviorissimilartowhatwewouldgetfromsolvingalinearregressionproblemwithindependentnoiseand
independentcovariates[
11
].Thishighlightsthefactthatouranalysissuccessfullyovercomesthedependencies
ofcovariatesandnoiseterms.
OurmaintheoremisaslightlyimprovedversionofTheorem
3.1
andisstatedbelow.Theorem
3.1
is
operationalintheregime
N
à
T
‹
p

m

n
“
.Inpracticalapplications,hiddenstatedimension
n
canbe
muchlargerthannumberofsensors
m
andinputdimension
p
.Ontheotherhand,theinputdatamatrix
U
becomestallassoonas
N
C
Tp
henceideally
(
2.7
)
shouldworkassoonas
N
à
Tp
.Ourmainresultshows
thatreliableestimationisindeedpossibleinthismorechallengingregime.Italsocarefullyquanti˝esthe
contributionofeachtermtotheoverallestimationerror.
Theorem3.2
Supposesystemisstable(i.e.
ˆ
‹
A
“
@
1
)and
N
C
cTp
log
2
‹
2
Tp
“
log
2
‹
2
Np
“
.Weobservea
trajectoryuntiltime

N

N

T

1
.Then,withhighprobability,theleast-squareestimatoroftheMarkov
parametermatrixobeys
Y
^
G

G
Y
B
R
w

R
e

R
z
˙
u
º
N
;
(3.2)
where
R
w
;R
e
;R
z
aregivenby
R
z

8
˙
z
»
Tp

m;
R
w

˙
w
Y
F
Y
max
Ÿ
»
N
w
;N
w
~
º
N
š
;
R
e

C˙
e
¾
‹
1

mT
N
‹
1

ˆ
‹
A
“
T
“
“‹
Tp

m
“
:
Here
c;C
A
0
areabsoluteconstantsand
N
w

cTq
log
2
‹
2
Tq
“
log
2
‹
2
Nq
“
where
q

p

n
.
3
Precisestatementontheprobabilityofsuccessisprovidedintheproof
4
OnecanobtainTheorem
3.1
fromTheorem
3.2
asfollows.When
N
C
N
0
C
N
w
:
R
w
satis˝es
R
w
B
˙
w
Y
F
Y
º
N
w
B
˙
w
Y
F
Y
º
N
0
.Similarly,when
ˆ
‹
A
“
T
isboundedawayfrom
1
byaconstantand
N
C
N
0
C
O
‹
Tm
“
:
R
e
satis˝es
R
e
B
2
C˙
e
º
Tp

m
B
˙
e
º
N
0
.
OneadvantageofTheorem
3.2
isthatitworksintheregime
Tp
ß
N
ß
T
‹
p

n

m
“
.Additionally,
Theorem
3.2
providestighterindividualerrorboundsforthe
˙
z
;˙
w
;˙
e
termsandexplicitlycharacterizesthe
dependenceon
ˆ
‹
A
“
insidethe
R
e
term.
Theorem
3.2
canbeimprovedinafewdirections.Someofthelogfactorsthatappearinoursample
sizemightbespurious.ThesetermsarearisingfromatheoremborrowedfromKrahmeretal.[
18
];which
actuallyhasastrongerimplicationthanwhatweneedinthiswork.Wealsobelieve
(
3.1
)
isoverestimating
thecorrectdependencebyafactorof
º
T
.
3.1
EstimatingtheOutputviaMarkovParameters
ThefollowinglemmaillustrateshowlearningMarkovparametershelpsusboundthepredictionerror.
Lemma3.3(Estimating
y
T
)
Suppose
x
1

0
and
z
t

N
‹
0
;˙
2
z
I
“
,
u
t

N
‹
0
;˙
2
u
I
“
,
w
t

N
‹
0
;˙
2
w
I
“
for
t
C
0
asdescribedinSection
2
.Assume,wehaveanestimate
^
G
of
G
thatisindependentofthesevariables
andweemploythe
y
t
estimator
^
y
t

^
G

u
t
:
Then,
E

y
t

^
y
t
Y
2
`
2

B
˙
2
w
Y
F
Y
2
F

˙
2
u
Y
G

^
G
Y
2
F

m˙
2
z

Y
CA
T

1
Y
2
tr
‹

ª
“
:
Proof
Followingfromtheinput/outputidentity
(
2.5
)
,thekeyobservationisthatfora˝xed
t
,

u
t
;

w
t
;
z
t
;
e
t
areallindependentofeachotherandtheirpredictionerrorsareuncorrelated.Since

u
t

N
‹
0
;˙
2
u
I
“
,
E

G

^
G
“

u
Y
2
`
2


˙
2
u
Y
G

^
G
Y
2
F
.Sameargumentappliesto

w

N
‹
0
;˙
2
w
I
“
;
z
t

N
‹
0
;˙
2
z
I
“
and
e
t
whichobeys
E

e
t
Y
2
`
2


tr
‹

‹
e
t
““
.Observethat
i
thlargesteigenvalue

i
‹

‹
e
t
““
of

‹
e
t
“
isupper
boundedby
Y
CA
T

1
Y
2

i
‹

‹
x
t

T

1
““
viaMin-Maxprinciple[
17
]hence
E

e
t
Y
2
`
2

B
Y
CA
T

1
Y
2
tr
‹

‹
x
t

T

1
““
B
Y
CA
T

1
Y
2
tr
‹

ª
“
.
4
MarkovParameterstoHankelMatrix:
LowOrderApproximationofStableSystems
Sofarourattentionhasfocusedonestimatingtheimpulseresponse
G
foraparticularhorizon
T
.Clearly,we
arealsointerestedinunderstandinghowwellwelearntheoverallbehaviorofthesystembylearninga˝nite
impulseapproximation.Inthissection,wewillapplyourearlierresultstoapproximatetheoverallsystemby
usingasfewsamplesaspossible.Ausefulideatowardsthisgoalistakingadvantageofthestabilityofthe
system.TheMarkovparametersdecayexponentiallyfastifthesystemisstablei.e.
ˆ
‹
A
“
@
1
.Thismeans
that,mostoftheMarkovparameterswillbeverysmallafterawhileandnotlearningthemmightnotbea
biglossforlearningtheoverallbehavior.Inparticular,
˝
'thMarkovparameterobeys
Y
CA
˝
B
Y
B

‹
A
“
ˆ
‹
A
“
˝
Y
C
YY
B
Y
:
Thisimpliesthat,theimpactoftheimpulseresponsetermswedon'tlearncanbeupperbounded.For
instance,thetotalspectralnormofthetailtermsobey
ª
Q
˝

T

1
Y
CA
˝
B
Y
B
ª
Q
˝

T

1

‹
A
“
ˆ
‹
A
“
˝
Y
C
YY
B
Y
B

‹
A
“Y
C
YY
B
Y
ˆ
‹
A
“
T

1
1

ˆ
‹
A
“
:
(4.1)
Toproceed˝xa˝nitehorizon
K
thatwilllaterbeallowedtogoin˝nity.Representtheestimate
^
G
as

^
D
;
^
G
0
;:::
^
G
T

2

where
^
G
i
correspondstothenoisyestimateof
CA
i
B
.Now,letusconsidertheestimated
5
andtrueorder
K
Markovparameters
G
‹
K
“


^
D
;
^
G
0
;:::
^
G
T

2
0
:::
0

^
G
‹
K
“


D
;
CB
;
CAB
:::
CA
K

2
B

:
Similarlywede˝netheassociated
K

K
blockHankelmatricesofsize
mK

pK
asfollows
^
H
‹
K
“

<
@
@
@
@
@
@
@
@
@
@
@
@
@
@
>
^
D
^
G
0
:::
^
G
T

3
^
G
T

2
0
:::
0
^
G
0
^
G
1
:::
^
G
T

2
00
:::
0

^
G
T

3
^
G
T

2
:::
000
:::
0
^
G
T

2
0
:::
000
:::
0

0
:::
0
=
A
A
A
A
A
A
A
A
A
A
A
A
A
A
?
H
‹
K
“

<
@
@
@
@
@
@
@
>
DCB
:::
CA
K

2
B
CBCAB
:::
CA
K

1
B

CA
K

2
BCB
:::
CA
2
K

3
B
=
A
A
A
A
A
A
A
?
(4.2)
Thefollowingtheoremmergesresultsofthissectionwithaspeci˝cchoiceof
T
togiveapproximationbounds
forthein˝niteMarkovoperator
G
‹
ª
“
andHankeloperator
H
‹
ª
“
.Fornotationalsimplicity,weshallassume
thatthereisnoprocessnoise.
Theorem4.1
Supposethespectralradiusobeys
ˆ
‹
A
“
@
1
.Fixanumber
1
A
""
0
A
0
andsupposeprocessnoise
obeys
˙
w

0
.Assumesamplesize
N
andestimationhorizon
T
satis˝es
4
N
C
cTp
log
2
‹
2
Tp
“
log
2
N
T
C
c
0

log
‹
N
~
T

T
‹
1

m
~
p
““

log
""
0

log
ˆ
‹
A
“
:
(4.3)
Then,givenobservationsofasingletrajectoryuntiltime

N

N

T

1
andestimating˝rst
T
Markov
parametersvialeast-squaresestimator
(
2.7
)
,withhighprobability,thefollowingboundsholdonthein˝nite
impulseresponseandHankelmatrixofthesystem.
Y
G
‹
ª
“

^
G
‹
ª
“
Y
B
‹
8
˙
z
˙
u

""
0
“
¾
Tp

m
N
Y
H
‹
ª
“

^
H
‹
ª
“
Y
B
T
‹
8
˙
z
˙
u

""
0
“
¾
Tp

m
N
:
Inessence,theabovetheoremisacorollaryofTheorem
3.2
.However,itfurthersimpli˝esthebounds
andalsoprovidesapproximationtosystemsoverallbehavior(e.g.in˝niteHankelmatrix).Inparticular,
theseboundsexploitstabilityofthesystemandallowsustotreatthesystemasifithasalogarithmic
order.Observethat
(
4.3
)
onlylogarithmicallydependsonthecriticalproblemvariablessuchasprecision
""
0
andspectralradius.Inessence,thee˙ectivesystemorderisdictatedbytheeigen-decayandequalto
T

O
‹

1
log
‹
ˆ
‹
A
““
“
hencestabilityallowsustotreatthesystemasifithasalogarithmicallysmallorder.
Ignoringlogarithmictermsexcept
ˆ
‹
A
“
,using
""
0
;˙
z
~
˙
u

O
‹
1
“
andpicking
T

O
‹

1
log
‹
ˆ
‹
A
““
“
and
N

O
‹


2
‹
Tp

m
““
;
guarantees
Y
G
‹
ª
“

^
G
‹
ª
“
Y
B

and
Y
H
‹
ª
“

^
H
‹
ª
“
Y
B
O
‹


log
‹
ˆ
‹
A
““
“
:
Remarkably,samplesizeisindependentofthestatedimension
n
andonlylinearlygrowswith
p
.Indeed,one
needsatleast
O
‹
p
“
samplestoestimateasingleMarkovparameterandweneedonlylogarithmicallymore
thanthisminimum(i.e.
N


O
‹
p
“
log
‹
ˆ
‹
A
““
)toestimatethein˝niteHankelmatrix.
4
Exactformoftheboundsdependon
A
;
B
;
C
andisprovidedintheproof.
6
Algorithm1
Ho-KalmanAlgorithmto˝ndaState-SpaceRealization.
1:
procedure
Ho-KalmanMinimumRealization
2:
Inputs:
Length
T
,Markovparametermatrixestimate
^
G
,systemorder
n
,
Hankelshape
‹
T
1
;T
2

1
“
with
T
1

T
2

1

T
.
3:
Outputs:
State-spacerealization
^
A
;
^
B
;
^
C
.
4:
FormtheHankelmatrix
^
H
>
R
mT
1

p
‹
T
2

1
“
from
^
G
.
5:
^
H

>
R
mT
1

pT
2

˝rst-
pT
2
-columns-of
‹
^
H
“
.
6:
^
L
>
R
mT
1

pT
2

rank-
n
-approximation-of
‹
^
H

“
.
7:
U
;

;
V

SVD
‹
^
L
“
.
8:
^
O
>
R
mT
1

n

U

1
~
2
.
9:
^
Q
>
R
n

pT
2


1
~
2
V
⁄
.
10:
^
C

˝rst-
m
-rows-of
‹
^
O
“
.
11:
^
B

˝rst-
p
-columns-of
‹
^
Q
“
.
12:
^
H

>
R
mT
1

pT
2

last-
pT
2
-columns-of
‹
^
H
“
.
13:
^
A

^
O
—
^
H

^
Q
—
.
14:
return
^
A
>
R
n

n
;
^
B
>
R
n

p
;
^
C
>
R
m

n
.
15:
endprocedure
5
Non-AsymptoticSystemIdenti˝cationviaHo-Kalman
Inthissection,we˝rstdescribethe
Ho-Kalman
algorithm[
15
]thatgenerates
A
;
B
;
C
;
D
fromtheMarkov
parametermatrix
G
.Wealsoshowthatthealgorithmisstabletoperturbationsin
G
andtheoutput
of
Ho-Kalman
gracefully
degrades
asafunctionof
Y
G

^
G
Y
.CombiningthiswithTheorem
3.1
implies
guaranteed
non-asymptoticidenti˝cationofmulti-input-multi-outputsystemsfroma
singletrajectory
.We
remarkthatresultsofthissectiondonotassumestabilityandappliestoarbitrary,possiblyunstable,systems.
WewillusethefollowingHankelmatrixde˝nitiontointroducethealgorithms.
De˝nition5.1(ClippedHankelmatrix)
Givenablockmatrix
X


X
1
;
X
2
;:::
X
T

>
R
m

Tp
and
integers
T
1
;T
2
satisfying
T
1

T
2
B
T
,de˝netheassociated
‹
T
1
;T
2
“
Hankelmatrix
H

H
‹
X
“
>
R
T
1
m

T
2
p
to
bethe
T
1

T
2
blockmatrixwith
m

p
sizeblockswhere
‹
i;j
“
thblockisgivenby
H

i;j


X
i

j
:
Notethat,
H
doesnotcontain
X
1
,whichshallcorrespondtothe
D
(or
^
D
)matrixforourpurposes.Thisis
solelyfornotationalconvenienceasthe˝rstMarkovparameterin
G
is
D
;however
A
;
B
;
C
areidenti˝ed
fromtheremainingMarkovparametersoftype
CA
i
B
.
5.1
SystemIdenti˝cationAlgorithm
Givenanoisyestimate
^
G
of
G
,wewishtolearngoodsystemmatrices
^
A
;
^
B
;
^
C
;
^
D
from
^
G
uptotrivial
ambiguities.ThiswillbeachievedbyusingAlgorithm
1
whichadmitsthematrix
^
G
,systemorder
n
and
Hankeldimensions
T
1
;T
2
asinputs.
Throughoutthissection,wemakethefollowingtwoassumptionsto
ensurethatthesystemwewishtolearnisorder-
n
andoursystemidenti˝cationproblemiswell-conditioned.
‹
thesystemisobservableandcontrollable;hence
n
A
0
istheorderofthesystem.
‹
‹
T
1
;T
2
“
Hankelmatrix
H
‹
G
“
formedfrom
G
isrank-
n
.Thiscanbeensuredbychoosingsu˚ciently
large
T
1
;T
2
.Inparticular
T
1
C
n;T
2
C
n
isguaranteedtowork
bythe˝rstassumptionabove
.
Learningstate-spacerepresentationsisanon-trivial,inherentlynon-convexproblem.Observethatthereare
multiplestate-spacerealizationsthat
yieldsthesamesystemandMarkovmatrix
G
.Inparticular,forany
nonsingularmatrix
T
>
R
n

n
,
A
œ

T

1
AT
;
B
œ

T

1
B
;
C
œ

CT
;
7
isavalidrealization
andyieldsthesamesystem
.Hence,similaritytransformationsof
A
;
B
;
C
generatea
classofsolutions.Notethat
D
isalreadyestimatedaspartof
G
.Since
D
isasubmatrixof
G
,weclearly
have
Y
D

^
D
Y
B
Y
G

^
G
Y
:
Hence,wefocusourattentiononlearning
A
;
B
;
C
.SupposewehaveaccesstothetrueMarkovparameters
G
andthecorresponding
‹
T
1
;T
2

1
“
Hankelmatrix
H
‹
G
“
.Inthiscase,
H
isarank-
n
matrixand
‹
i;j
“
thblockof
H
isequalto
CA
i

j

2
B
.De˝ning(extended)controllabilityandobservabilitymatrices
Q


B
;
AB
;:::
A
T
2
B

and
O


C
⁄
;
‹
CA
“
⁄
;:::
‹
CA
T
1

1
“
⁄

⁄
,wehave
H

OQ
.However,itisnot
clearhowto˝nd
O
;
Q
.
The
Ho-Kalman
algorithmaccomplishesthistaskby˝ndingabalancedrealizationandreturning
some
^
A
;
^
B
;
^
C
matricesfrompossiblynoisyMarkovparametermatrix
^
G
.Lettheinputtothealgorithmbe
^
G


^
D
;
^
G
0
;:::
^
G
T

2

where
^
G
i
correspondstothenoisyestimateof
CA
i
B
.Weconstructthe
‹
T
1
;T
2

1
“
Hankelmatrix
^
H
asdescribedabovesothat
‹
i;j
“
thblockof
^
H
isequalto
^
G
i

j

2
.Let
^
H

>
R
mT
1

pT
2
bethe
submatrixof
^
H
afterdiscardingtherightmost
mT
1

p
blockand
^
L
bethebestrank-
n
approximationof
^
H

obtainedbysettingitsallbuttop
n
singularvaluestozero.Let
^
H

bethesubmatrixafterdiscardingthe
left-most
mT
1

p
block.Notethatboth
^
L
;
^
H

havesize
R
mT
1

pT
2
.Takethesingularvaluedecomposition
(SVD)oftherank-
n
matrix
^
L
as
^
L

U

V
⁄
(with

>
R
n

n
)andwrite
^
L

‹
U

1
~
2
“

1
~
2
V
⁄

^
O
^
Q
:
If
^
G
wasequaltothegroundtruth
G
,then
^
O
;
^
Q
wouldcorrespondtotheorder
T
1
observabilitymatrix

O

U

1
~
2
andtheorder
T
2
controllabilitymatrix

Q


1
~
2
V
⁄
oftheactualbalancedrealizationbasedon
noiseless
SVD.Here,

O
;

Q
matricesarenotnecessarilyequalto
O
;
Q
,
howevertheyyieldthesamesystem
.
Notethat,thecolumnsof
^
O
;
^
Q
arethescaledversionsoftheleftandrightsingularvectorsof
^
L
respectively.
TheHo-Kalmanalgorithm˝nds
^
A
;
^
B
;
^
C
asfollows.
‹
^
C
isthe˝rst
m

n
submatrixof
^
O
.
‹
^
B
isthe˝rst
n

p
submatrixof
^
Q
.
‹
^
A

^
O
—
^
H

^
Q
—
.
Thisprocedure(
Ho-Kalman
)returnsthetruebalancedrealizationofthesystemwhenMarkovparametersare
knowni.e.
^
G

G
.OurgoalistoshowthatevenwithnoisyMarkovparameters,thisprocedurereturnsgood
estimatesofthe
true
balancedrealization.
Weremarkthattherearevariationsofthisprocedure;however
thecoreideaisthesameandtheyareequivalentwhenthetrueMarkovparametersareusedasinput.For
instance,whenconstructing
^
H
,onecanattempttoimprovethenoiserobustnessofthealgorithmbypicking
balanceddimensions
mT
1

pT
2
.
5.2
RobustnessoftheHo-KalmanAlgorithm
Observethat
^
H
;
^
H

;
^
L
;
^
H

;
^
O
;
^
Q
ofAlgorithm
1
arefunctionsoftheinputmatrix
^
G
.Forthesubsequent
discussion,welet
‹
H
;
H

;
L
;
H

;
O
;
Q
bethematricescorrespondingtogroundtruth
G
.
‹
^
H
;
^
H

;
^
L
;
^
H

;
^
O
;
^
Q
bethematricescorrespondingtotheestimate
^
G
.
Furthermore,let

A
;

B
;

C
betheactualbalancedrealizationassociatedwith
G
andlet
^
A
;
^
B
;
^
C
bethe
Ho-Kalman
outputassociatedwith
^
G
.Notethat
L

H

since
H

isalreadyrank
n
.Wenowprovidea
lemmarelatingtheestimationerrorof
G
tothatof
L
and
H
.
Lemma5.2
H
;
^
H
and
L
;
^
L
satis˝esthefollowingperturbationbounds,
max
ŸY
H


^
H

Y
;
Y
H


^
H

Yš
B
Y
H

^
H
Y
B
»
min
Ÿ
T
1
;T
2

1
šY
G

^
G
Y
;
(5.1)
Y
L

^
L
Y
B
2
Y
H


^
H

Y
B
2
»
min
Ÿ
T
1
;T
2
šY
G

^
G
Y
:
(5.2)
8
Letusdenotethe
n
thlargestsingularvalueof
L
via
˙
min
‹
L
“
.Notethat
˙
min
‹
L
“
isthesmallestnonzero
singularvalueof
L
since
rank
‹
L
“

n
.
AusefulimplicationofTheorem
3.1
(inlightofLemma
5.2
)isthatif
˙
min
‹
L
“
islargeenough,thetruesystemorder
n
canbenon-asymptoticallyestimatedfromthenoisyMarkov
parameterestimatesviasingularvaluethresholding.
Ournextresultshowstherobustnessofthe
Ho-Kalman
algorithmtopossiblyadversarialperturbations
ontheMarkovparametermatrix
G
.
Theorem5.3
Suppose
H
and
^
H
betheHankelmatricesderivedfrom
G
and
^
G
respectivelyperDe˝nition
5.1
.Let

A
;

B
;

C
bethestate-spacerealizationcorrespondingtotheoutputof
Ho-Kalman
withinput
G
and
^
A
;
^
B
;
^
C
bethestate-spacerealizationcorrespondingtooutputof
Ho-Kalman
withinput
^
G
.Supposethesystem
A
;
B
;
C
;
D
is
observableandcontrollable
andlet
O
;
Q
and
^
O
;
^
Q
beorder-
n
controllability/observability
matricesassociatedwith
G
and
^
G
respectively.Suppose
˙
min
‹
L
“
A
0
andperturbationobeys
Y
L

^
L
Y
B
˙
min
‹
L
“~
2
:
(5.3)
Then,thereexistsaunitarymatrix
T
>
R
n

n
suchthat,
Y

C

^
CT
Y
F
B
Y
O

^
OT
Y
F
B
¼
5
n
Y
L

^
L
Y
;
(5.4)
Y

B

T
⁄
^
B
Y
F
B
Y
Q

T
⁄
^
Q
Y
F
B
¼
5
n
Y
L

^
L
Y
:
(5.5)
Furthermore,hiddenstatematrices
^
A
;

A
satisfy
Y

A

T
⁄
^
AT
Y
F
B
14
º
n
˙
min
‹
L
“
‹
¿
Á
Á
À
Y
L

^
L
Y
˙
min
‹
L
“
‹Y
H

Y

Y
H


^
H

Y“

Y
H


^
H

Y“
:
(5.6)
Above,
Y
H


^
H

Y
;
Y
L

^
L
Y
areperturbationtermsthatcanbeboundedintermsof
Y
H

^
H
Y
or
Y
G

^
G
Y
viaLemma
5.2
.Thisresultshowsthat
Ho-Kalman
solutionisrobusttonoiseuptotrivialambiguities.
Robustnessiscontrolledby
˙
min
‹
L
“
whichtypicallycorrespondstotheweakestmodeofthesystem.
We
remarkthatforreasonablylarge
T
2
choice,wehave
˙
min
‹
L
“

˙
min
‹
H
“
as
L

H

isobtainedbydiscarding
thelastblockcolumnof
H
whichisexponentiallysmallin
T
2
.
Sincethe
Ho-Kalman
algorithmisbasedonSVD,havingagoodcontroloversingularvectorsiscrucialfor
theproof.Wedothisbyutilizingtheperturbationresultsfromtherecentliterature[
28
].Whilewebelieve
ourresulthasthecorrectdependency,itisintermsofFrobeniusnormratherthanspectral.Havingabetter
spectralnormcontrolover

A
;

B
;

C
wouldbeanidealfutureimprovement.
Acorollarytothisresultcanbestatedintermsof
˙
min
‹
L
“
andHankelmatrices
H
;
^
H
.Theresultbelow
followsfromanapplicationofLemma
5.2
.
Corollary5.4
ConsiderthesetupofTheorem
5.3
andsuppose
˙
min
‹
L
“
A
0
and
Y
H

^
H
Y
B
˙
min
‹
L
“~
4
:
Then,thereexistsaunitarymatrix
T
>
R
n

n
suchthat,
max
ŸY

C

^
CT
Y
F
;
Y
O

^
OT
Y
F
;
Y

B

T
⁄
^
B
Y
F
;
Y
Q

T
⁄
^
Q
Y
F
š
B
5
¼
n
Y
H

^
H
Y
:
(5.7)
Furthermore,hiddenstatematrices
^
A
;

A
satisfy
Y

A

T
⁄
^
AT
Y
F
B
50
¼
n
Y
H

^
H
YY
H
Y
˙
3
~
2
min
‹
L
“
:
RecallfromLemma
5.2
that
Y
H

^
H
Y
B
»
min
Ÿ
T
1
;T
2

1
šY
G

^
G
Y
.Hence,combiningCorollary
5.4
and
Theorem
3.1
providesnon-asymptoticguaranteesforend-to-endsystemidenti˝cationprocedure.Theorem
3.1
˝ndsagoodMarkovparameterestimate
^
G
fromasmallamountofdataandCorollary
5.4
translatesthis
^
G
intoarobuststate-spacerealization
^
A
;
^
B
;
^
C
;
^
D
.
9
Figure1:
WeconsiderthematricesthatcandirectlybeinferredfromtheMarkovparametermatrix
G
.
Theseare
D
;
CB
whicharethe˝rsttwoblocksubmatricesof
G
,
G
itself,and
H
whichistheHankel
matrixthatisconstructedfromblocksof
G
.Theseresultsarefor
T

18
whichimplies
G
>
R
2

54
and
H
>
R
18

27
aswepicked
T
1

T
2

1

9
.
6
NumericalExperiments
WeconsideredaMIMO(multipleinput,multipleoutput)systemwith
m

2
sensors,
n

5
hiddenstatesand
inputdimension
p

3
.Toassessthetypicalperformanceoftheleast-squaresandthe
Ho-Kalman
algorithms,
weconsiderrandomstate-spacesasfollows.Wegenerate
C
;
D
withindependent
N
‹
0
;
1
~
m
“
entries.We
generate
B
withindependent
N
‹
0
;
1
~
n
“
entries.Thesevariancechoicesaretoensurethesematricesare
isometricinthesensethat
E

Mv
Y
2
`
2


Y
v
Y
2
`
2
foragivenvector
v
and
M
>
Ÿ
B
;
C
;
D
š
.Hence,theimpact
ofthestandarddeviations
˙
u
;˙
w
;˙
z
areproperlynormalized.Theinputvarianceis˝xedat
˙
u

1
however
noisevarianceswillbemodi˝edduringtheexperiments.
ThemostcriticalcomponentofanLTIsystemisthe
A
matrix.Wepicked
A
tobeadiagonalmatrixwith
its
n
eigenvalues(i.e.diagonalentries)aregeneratedtobeuniformrandomvariablesbetween

0
;
0
:
9

.The
upperbound
0
:
9
impliesthatweareworkingwithstablematricesandthee˙ectofunknownstatevanishes
forlarge
T
.
Finally,weconductexperimentsfordi˙erent
T
valuesof
T
>
Ÿ
6
;
12
;
18
š
.During
Ho-Kalman
procedure,we
createaHankelmatrix
^
H
ofsize
mT
~
2

pT
~
2
andapplyAlgorithm
1
.Duetorandomgenerationofproblem
data,evenfor
T

6
,thegroundtruthHankelmatrix
H

>
R
6

6
hasrank
n

5
sothat
Ho-Kalman
procedure
canindeedlearnagoodrealization.
Inourexperimentalsetup,wepickahyperparametercon˝gurationof
T;˙
w
;˙
z
andgenerateasingle
rolloutofthesystemuntilsometime
t
ª
.Foreach
T
B

N
B
t
ª
,wesolvethesystemvia
(
2.7
)
toobtainthe
estimateof
G
anduseAlgorithm
1
toobtainastate-spacerealization
^
A
;
^
B
;
^
C
;
^
D
.The
x
-axisdisplays
N
(whichistheamountofavailabledataattime
t


N
)andthe
y
-axisdisplaystheestimationerror.Eachcurve
inthe˝guresisgeneratedbyaveragingtheoutcomesof
20
independentrealizationsofsingletrajectories.
10
(a)
(b)
(c)
(d)
Figure2:
Relativeestimationerrorsforsystems
S
;
^
S
forvaryingnoiselevels.
^
S
isobtainedbythe
Ho-Kalman
procedureofAlgorithm
1
.BasedonTheorem
5.3
,weexpectimprovedestimationaccuracy
forlarger
N
andsmaller
˙
w
;˙
z
;sincetheerrorinestimatingthesystemmatricesisdirectlycontrolled
bytheerrorintheMarkovparametermatrix
G

^
G
.
InFigure
1
,weconsideredtheproblemofestimatingthematrices
D
;
CB
;
G
;
H
when
T

18
.
D
;
CB
are
the˝rsttwoimpulseresponses.Estimating
G
andtheassociatedHankelmatrix
H
helpsverifyour˝ndings
inTheorem
3.2
.Weplottedcurvesforvaryingnoiselevels
˙
w

˙
z
>
Ÿ
0
;
1
~
4
;
1
~
2
;
1
š
.Themainconclusionis
thatindeedestimationaccuracydrasticallyimprovesasweobservethesystemforalongerperiodoftime
andcollectmoredata.Notethatestimationerrorson
D
and
CB
areinthesameballpark.Theseare
submatricesof
G
hencetheirassociatedspectralnormerrorsarestrictlylowercomparedto
Y
G

^
G
Y
.
Per
De˝nition
5.1
,
H
isconstructedfromtheblocksof
G
anditsspectralnormerrorisinlineswith
G
.
The
otherobservationisthatestimationerrordecaysgracefullyasafunctionofthenoiselevelsforallmatricesof
interest.Sincewepickedalarge
T
,theerrorduetounknowninitialconditions(i.e.
e
t
)isfairlynegligible.
Hencewhen
˙
w

˙
z

0
,wequicklyachievenear
0
estimationerrorastheimpactofthe
e
t
termisalsosmall.
InFigure
2
westudythestabilityofthe
Ho-Kalman
procedurewhichreturnsarealizationuptoaunitary
transformationasdescribedinTheorem
5.3
.Hence,ratherthanfocusingonindividualoutputs
^
A
;
^
B
;
^
C
wedirectlystudytheLTIsystems
S

LTI-sys
‹
A
;
B
;
C
;
D
“
and
^
S

LTI-sys
‹
^
A
;
^
B
;
^
C
;
^
D
“
.Inparticular,we
focusonthe
H
ª
normoftheerror
S

^
S
.Duringthisprocess,weclippedthesingularvaluesof
^
A
at
0
:
99
i.e.
if
^
A
hasasingularvaluelargerthan
0
:
99
,wereplaceitby
0
:
99
intheSVDof
^
A
whichreturnsanew
^
A
whosesingularvectorsaresamebutsingularvaluesareclipped.Thisessentiallycorrespondstoprojecting
theestimatedsystemonthesetofstablesystems.Whileweveri˝edthat
Y
^
A
Y
A
0
:
99
rarelyhappensfor
large
N
,clippingensuresthat
H
ª
normisalwaysboundedandsmoothsouttheresults.Figure
2
illustrates
thenormalized
H
ª
error
Y
^
S

S
Y
H
ª
Y
S
Y
H
ª
forvarying
˙
w

˙
z
and
T
>
Ÿ
6
;
12
;
18
š
.Forzero-noiseregime,
T

18
11
outperformstherestdemonstratingthebene˝tofusingalarger
T
toovercomethecontributionofthe
˙
e
term.Intheotherregimes,all
T
choicesperformfairlysimilar;however
T

6
appearstosu˙erlessfrom
increasingnoiselevels
˙
w
;˙
z
.Anotherobservationisthatforverysmallsamplesize
N
,
T

6
converges
fasterthantheothers.ThisissupportedbyourTheorem
3.2
as
T

6
haslessunknownsandtheminimal
N
isintheorderof
Tp
,hencesmaller
T
meansfasterestimation.
WeremarkthatonemightbeinterestedinothermetricstoassesstheerrorsuchasFrobeniusnorm.While
notshowninthe˝gures,wealsoveri˝edthattheFrobeniusnorm
Y
G

^
G
Y
F
(andtheerrorsfor
CB
;
D
;
H
aswellas
Y
S

^
S
Y
H
2
)behavesinasimilarfashiontospectralnormand
H
ª
norm.
7
Conclusions
Inthispaper,weanalyzedthesamplecomplexityoflinearsystemidenti˝cationfrominput/outputdata.
Ouranalysisneitherrequiresmultipleindependenttrajectoriesnorreliesonsplittingthetrajectoryinto
non-overlappingintervals,thereforemakesverye˚cientuseoftheavailabledatafromasingletrajectory.
Morecrucially,itdoesnotrelyonstatemeasurementsandworkswithonlytheinputsandoutputs.Based
onthisanalysis,weshowedthatonecanapproximatesystem'sHankeloperatorusingnearoptimalamount
ofsamplesandshedlightonthestabilityof˝ndingabalancedrealization.
Therearemanydirectionsforfuturework.First,weareinterestedincombiningourresultswithcontrol
synthesistechniquesbasedonMarkovparameters.Second,itisshownempiricallythatminimizingthe
rankornuclearnormoftheestimatedHankelmatrixasadenoisingstep(seee.g.,[
9
])worksbetterthan
Ho-Kalman.Itisofinteresttoanalyzethestabilityofsuchoptimization-basedalgorithms.Finally,itwould
beinterestingtoseewhattypeofrecoveryguaranteescanbeobtainedifadditionalconstraints,suchas
subspaceconstraints,onthesystemmatricesareknown[
7
].
Acknowledgements
S.O.wouldliketothankMahdiSoltanolkotabiforpointingoutLemma
5
:
14
of[
28
]aswellasBabakHassibi
andJayFarrellforconstructivefeedback.AuthorswouldliketothankZheDuforacarefulreadingofthe
manuscriptandhelpfulsuggestions.N.O.wouldliketothankGlenChouforproofreadingadraftandDennis
Bernsteinforcommentsonthe
Ho-Kalman
algorithm.TheworkofN.O.issupportedinpartbyDARPA
grantN66001-14-1-4045andONRgrantN000141812501.
References
[1]
RadoslawAdamczaketal.AnoteontheHanson-Wrightinequalityforrandomvectorswithdependencies.
ElectronicCommunicationsinProbability
,20,2015.
[2]
SanjeevArora,EladHazan,HoldenLee,KaranSingh,CyrilZhang,andYiZhang.Towardsprovable
controlforunknownlineardynamicalsystems.
ICLRworkshop
,2018.
[3]
RossBoczar,NikolaiMatni,andBenjaminRecht.Finite-dataperformanceguaranteesfortheoutput-
feedbackcontrolofanunknownsystem.
arXivpreprintarXiv:1803.09186
,2018.
[4]
SarahDean,HoriaMania,NikolaiMatni,BenjaminRecht,andStephenTu.Onthesamplecomplexity
ofthelinearquadraticregulator.
arXivpreprintarXiv:1710.01688
,2017.
[5]
FroilánMDopico.Anoteonsin

theoremsforsingularsubspacevariations.
BITNumericalMathematics
,
2000.
[6]
MohamadKazemShiraniFaradonbeh,AmbujTewari,andGeorgeMichailidis.Finitetimeanalysisof
optimaladaptivepoliciesforlinear-quadraticsystems.
arXivpreprintarXiv:1711.07230
,2017.
12
[7]
SalarFattahiandSomayehSojoudi.Data-drivensparsesystemidenti˝cation.
arXivpreprint
arXiv:1803.07753
,2018.
[8]
MaryamFazel,RongGe,ShamMKakade,andMehranMesbahi.Globalconvergenceofpolicygradient
methodsforlinearizedcontrolproblems.
arXivpreprintarXiv:1801.05039
,2018.
[9]
MaryamFazel,TingKeiPong,DefengSun,andPaulTseng.Hankelmatrixrankminimizationwith
applicationstosystemidenti˝cationandrealization.
SIAMJournalonMatrixAnalysisandApplications
,
2013.
[10]
MatthewSFledderjohn,MatthewSHolzel,HarishJPalanthandalam-Madapusi,RobertJFuentes,and
DennisSBernstein.Acomparisonofleastsquaresalgorithmsforestimatingmarkovparameters.In
AmericanControlConference(ACC),2010
,pages373IEEE,2010.
[11]
JeromeFriedman,TrevorHastie,andRobertTibshirani.
Theelementsofstatisticallearning
,volume1.
SpringerseriesinstatisticsNewYork,2001.
[12]
KatsuhisaFurutaandManopWongsaisuwan.Discrete-timelqgdynamiccontrollerdesignusingplant
markovparameters.
Automatica
,1995.
[13]
MoritzHardt,TengyuMa,andBenjaminRecht.Gradientdescentlearnslineardynamicalsystems.
arXivpreprintarXiv:1609.05191
,2016.
[14]
EladHazan,KaranSingh,andCyrilZhang.Learninglineardynamicalsystemsviaspectral˝ltering.In
AdvancesinNeuralInformationProcessingSystems
,pages6702017.
[15]
BLHoandRudolfEKálmán.E˙ectiveconstructionoflinearstate-variablemodelsfrominput/output
functions.
at-Automatisierungstechnik
,1966.
[16]
LeslieHogben.
Handbookoflinearalgebra
.CRCPress,2006.
[17]
RogerAHorn,RogerAHorn,andCharlesRJohnson.
Matrixanalysis
.Cambridgeuniversitypress,
1990.
[18]
FelixKrahmer,ShaharMendelson,andHolgerRauhut.Supremaofchaosprocessesandtherestricted
isometryproperty.
CommunicationsonPureandAppliedMathematics
,2014.
[19]
LennartLjung.Systemidenti˝cation.In
Signalanalysisandprediction
,pagesSpringer,1998.
[20]
LingshengMengandBingZheng.Theoptimalperturbationboundsofthemoenroseinverseunder
thefrobeniusnorm.
LinearAlgebraanditsApplications
,2010.
[21]
JoséPereira,MortezaIbrahimi,andAndreaMontanari.Learningnetworksofstochasticdi˙erential
equations.In
AdvancesinNeuralInformationProcessingSystems
,pages0,2010.
[22]
MarkRudelson,RomanVershynin,etal.Hanson-wrightinequalityandsub-gaussianconcentration.
ElectronicCommunicationsinProbability
,18,2013.
[23]
RicardoSSanchez-PenaandMarioSznaier.
Robustsystemstheoryandapplications
.Wiley-Interscience,
1998.
[24]
MarioASantilloandDennisSBernstein.Adaptivecontrolbasedonretrospectivecostoptimization.
Journalofguidance,control,anddynamics
,2010.
[25]
MaxSimchowitz,HoriaMania,StephenTu,MichaelIJordan,andBenjaminRecht.Learningwithout
mixing:Towardsasharpanalysisoflinearsystemidenti˝cation.
arXivpreprintarXiv:1802.08334
,2018.
[26]
RobertESkeltonandGuojunShi.Thedata-basedlqgcontrolproblem.In
DecisionandControl,1994.,
Proceedingsofthe33rdIEEEConferenceon
,volume2,pagesIEEE,1994.
13
[27]
StephenTu,RossBoczar,AndrewPackard,andBenjaminRecht.Non-asymptoticanalysisofrobust
controlfromcoarse-grainedidenti˝cation.
arXivpreprintarXiv:1707.04791
,2017.
[28]
StephenTu,RossBoczar,MaxSimchowitz,MahdiSoltanolkotabi,andBenjaminRecht.Low-rank
solutionsoflinearmatrixequationsviaprocrustes˛ow.
arXivpreprintarXiv:1507.03566
,2015.
[29]
PeterVanOverscheeandBLDeMoor.
Subspaceidenti˝cationforlinearsystems:The
Applications
.SpringerScience&BusinessMedia,2012.
[30]
Per-ÅkeWedin.Perturbationtheoryforpseudo-inverses.
BITNumericalMathematics
,
1973.
[31]
ErikWeyer,RobertCWilliamson,andIvenMYMareels.Finitesamplepropertiesoflinearmodel
identi˝cation.
IEEETransactionsonAutomaticControl
,1999.
[32]
YiYu,TengyaoWang,andRichardJSamworth.Ausefulvariantofthedaahantheoremfor
statisticians.
Biometrika
,2014.
14
A
ProofoftheResultsonLearningMarkovParameters
We˝rstdescribethebasicproofidea.Followingequation
(
2.5
)
,tofurthersimplifythenotation,de˝nethe
matrices
W



w
T
;

w
T

1
;:::;

w

N

⁄
>
R
N

Tn
;
(A.1)
E


e
T
;
e
T

1
;:::;
e

N

⁄
>
R
N

n
;
Z


z
T
;
z
T

1
;:::;
z

N

⁄
>
R
N

m
:
Withthesevariables,wehavethesystemofequations
Y

UG
⁄

E

Z

WF
⁄
:
Following(
2.7
),estimationerrorisgivenby
‹
^
G

G
“
⁄

‹
U
⁄
U
“

1
U
⁄
‹
WF
⁄

Z

E
“
:
(A.2)
Thespectralnormoftheerrorcanbeboundedas
Y‹
^
G

G
“
⁄
Y
B
Y‹
U
⁄
U
“

1
Y‹Y
U
⁄
W
YY
F
⁄
Y

Y
U
⁄
Z
Y

Y
U
⁄
E
Y“
:
(A.3)
Eachofthesetermswillbeboundedindividually.Theboundson
Y‹
U
⁄
U
“

1
Y
and
Y
U
⁄
W
Y
willbeobtained
byusingthepropertiesofrandomcirculantmatricesinSection
C
.
Y
U
⁄
Z
Y
isarguablythesimplesttermdue
to
Z
beingani.i.d.Gaussianmatrix.ItisboundedviaLemma
A.1
.Finally,
Y
U
⁄
E
Y
termwillbeaddressed
byemployingamartingalebasedargumentinSection
D
.We˝rstproveTheorem
3.2
whichisourmain
theorem.ItwillbefollowedbytheproofofTheorem
3.1
.
A.1
ProofofTheorem
3.2
Proof
Theproofisobtainedbycombiningestimatesfromthesubsequentsections.Set


log
2
‹
2
Tp
“
log
2
‹
2
Np
“
tosimplifythenotation.Picking
c
C
‹
log
2
“

4
,ourassumptionof
N
C
cTp

implies
N
C
T
and
N
C
‹

N

1
“~
2
(using

N

N

T

1
).Consequently,
log
2
‹
2

Np
“
B
4
log
2
‹
2
Np
“
andwehave
N
C
‹
c
~
4
“
Tp
log
2
‹
2
Tp
“
log
2
‹
2

Np
“
.
ThisfactwillbeusefulwhenweneedtoutilizeresultsofSection
C
.We˝rstaddressthe
Z
componentofthe
errorwhichisrathertrivialtobound.
LemmaA.1
Let
M
>
R
m

n
beatallmatrix(
m
C
n
)with
Y
M
Y
B

.Let
G
>
R
m

k
beamatrixwith
independentstandardnormalentries.Then,withprobabilityatleast
1

2exp
‹

t
2
~
2
“
,
Y
M
⁄
G
Y
B

‹
»
2
‹
n

k
“

t
“
:
Inparticular,setting
t

»
2
‹
n

k
“
,we˝nd
Y
M
⁄
G
Y
B

»
8
‹
n

k
“
withprobabilityatleast
1

2
exp
‹

‹
n

k
““
.
Proof
Suppose
M
havesingularvaluedecomposition
M

V
1

V
⁄
2
where
V
1
>
R
m

n
.Observethat

G

V
⁄
1
G
>
R
n

k
havei.i.d.
N
‹
0
;
1
“
entries.Also
E


G

B
º
n

º
k
B
»
2
‹
n

k
“
.ApplyingLipschitz
Gaussianconcentrationonspectralnorm,withprobabilityatleast
1

2exp
‹

t
2
~
2
“
,
Y
M
⁄
G
Y

Y
V
2


G
Y

Y


G
Y
B

‹
»
2
‹
n

k
“

t
“
:
Thefollowingcorollarystatestheestimationerrorduetomeasurementnoise(
Z
term).
CorollaryA.2
Let
U
>
R
N

Tp
bethedatamatrixasin
(
2.6
)
andlet
Z
>
R
N

m
bethemeasurement
noisematrixfrom
(
A.1
)
.Suppose
N
C
cTp

forsomeabsoluteconstant
c
A
0
.Withprobabilityatleast
1

2exp
‹

‹
Tp

m
““

exp
‹


“
,
Y
U
⁄
Z
Y
B
4
˙
u
˙
z
»
N
‹
Tp

m
“
:
15
Proof
Set


º
2
N˙
u
.Using

N
C
N
,Lemma
C.2
yieldsthat
P
‹Y
U
Y
B

“
C
1

exp
‹


“
:
(A.4)
Hence,combiningLemmas
C.2
and
A.1
,usingthefactthat
Z
;
U
areindependent,andadjustingfor
Z
's
variance
˙
z
,we˝ndtheresult.
Next,weapplyLemmas
C.2
and
C.3
to˝ndthat,forsu˚cientlylarge
c
A
0
,whenever
N
C
cTp

,
Y‹
U
⁄
U
“

1
Y
B
2
˙

2
u
~
N;
Y
U
⁄
W
Y
B
1
2
˙
u
˙
w
max
Ÿ
»
N
w
N;N
w
š
;
(A.5)
where
N
w

cTq
log
2
‹
2
Tq
“
log
2
‹
2
Nq
“
and
q

p

n
withprobabilityatleast
1

2
exp
‹


“
.Finally,applying
Theorem
D.1
with


Y

ª
Y

‹
A
“
2
Y
CA
T

1
Y
2
1

ˆ
‹
A
“
2
T
,withprobabilityatleast
1

T
‹
exp
‹

100
Tp
“

2exp
‹

100
m
““
,
Y
U
⁄
E
Y
B
c
3
˙
u
¾
T
max
Ÿ
N;
mT
1

ˆ
‹
A
“
T
š
max
Ÿ
Tp;m
š
:
Combiningalloftheestimatesaboveviaunionboundandsubstituting

,withprobabilityatleast,
1

2exp
‹

‹
Tp

m
““

3
‹
2
Np
“

log
‹
2
Np
“
log
2
‹
2
Tp
“

T
‹
exp
‹

100
Tp
“

2exp
‹

100
m
““
;
theerrorterm
Y
G

^
G
Y
of(
A.3
)isupperboundedby
R
z

R
e

R
w
˙
u
º
N
where
R
z

8
˙
z
»
Tp

m;
(A.6)
R
w

˙
w
Y
F
Y
max
Ÿ
»
N
w
;N
w
~
º
N
š
;
(A.7)
R
e

2
C
¾
‹
1

mT
N
‹
1

ˆ
‹
A
“
T
“
“‹
Tp

m
“
T:
(A.8)
Absorbingthe

2
multiplierof
R
e
into
C
andobserving
T

˙
2
e
,weconcludewiththedesiredresult.
A.2
ProofofTheorem
3.1
TheproofusesthesamestrategyinSection
A.1
withslightmodi˝cations.Wewillrepeattheargumentfor
thesakeofcompleteness.Firstofall,weutilizethesameestimatesbasedonLemmas
C.2
and
C.3
,namely
(
A.5
)and(
A.4
)(
Y
U
Y
B
º
2
N˙
u
)whichholdwithprobabilityatleast
1

3
‹
2
Np
“

log
‹
2
Np
“
log
2
‹
2
Tp
“
.Observe
that
N
C
N
0
C
N
w

cT
‹
p

n
“
log
2
‹
2
T
‹
p

n
““
log
2
‹
2
N
‹
p

n
““
hence,wehavethat
Y
U
⁄
W
Y
B
1
2
˙
u
˙
w
»
N
w
N
B
1
2
˙
u
˙
w
»
N
0
N:
WeuseLemma
A.1
with
t

º
2
Tq
toobtain
P
‹Y
U
⁄
Z
Y
B
4
˙
u
˙
z
º
TqN
“
C
1

2exp
‹

Tq
“
.
Finally,toboundthecontributionof
E
weagainapplyTheorem
D.1
.Since
ˆ
‹
A
“
T
B
0
:
99
,picking
su˚cientlylarge
c
,weobservethat
max
Ÿ
N;
mT
1

ˆ
‹
A
“
T
š

N
Hence,using
˙
e

º
T
andapplyingTheorem
D.1
yieldsthatforsome
C
A
0
Y
U
⁄
E
Y
B
C˙
u
»
TN
‹
Tp

m
“

B
C˙
u
˙
e
»
NTq;
holdswithprobabilityatleast
1

T
‹
exp
‹

100
Tp
“

2
exp
‹

100
m
““
.Unionboundingoveralltheseevents
andfollowing(
A.3
),withprobabilityatleast,
1

2exp
‹

Tq
“

3
‹
2
Np
“

log
‹
2
Np
“
log
2
‹
2
Tp
“

T
‹
exp
‹

100
Tq
“

2exp
‹

100
m
““
;
16
we˝ndthespectralnormestimationerrorof
Y
^
G

G
Y
B
1
2
˙
u
˙
w
Y
F
Y
º
N
0
N

4
˙
u
˙
z
º
TqN

C˙
u
˙
e
º
TqN
‹
˙
2
u
N
“~
2
(A.9)
B
˙
w
Y
F
Y
º
N
0

8
˙
z
º
Tq

2
C˙
e
º
Tq
˙
u
º
N
;
(A.10)
whichisthedesiredboundafterensuring
max
Ÿ
8
;
2
C
š
2
Tq
B
N
0
bypickingtheconstant
c
(whichleads
N
0
)to
besu˚cientlylarge.
A.3
ProofofTheorem
4.1
Proof
Letusstartwith
G
‹
ª
“
estimate.Firstnotethat,thetailspectralnormisboundedvia
(
4.1
)
.Picking
theproposed
T
C
T
1

1

log
‹
2
""

1
0
‹
1

ˆ
‹
A
““

1

‹
A
“Y
C
YY
B
Y
¼
N
Tp

m
“
log
‹
ˆ
‹
A
““
impliesrighthandsideof
(
4.1
)
canbeupper
boundedas

‹
A
“Y
C
YY
B
Y
ˆ
‹
A
“
T

1
1

ˆ
‹
A
“
B
1
2
""
0
¾
Tp

m
N

ˆ
‹
A
“

‹
T

1
“
C

‹
A
“Y
C
YY
B
Y
¼
N
Tp

m
""
0
‹
1

ˆ
‹
A
““
(A.11)
Next,wewillboundthespectraldi˙erenceoforder
T
˝niteresponses
G
and
^
G
.Let
T
C

1
log
‹
ˆ
‹
A
““
toensure
ˆ
‹
A
“
T
B
1
~
2
.ApplyingTheorem
3.2
,wewillshowthatindividualerrorsummandsdueto
R
w
;R
e
;R
z
are
upperbounded.First,Theorem
3.2
isapplicableduetothechoiceof
N
.
R
w
summandiszeroas
˙
w

0
.
Second,observethat,forsome
C
A
0
R
e
˙
u
º
N
B
C
4
˙
e
¾
‹
1

mT
N
‹
1

ˆ
‹
A
“
T
“
“‹
Tp

m
“
B
˙
e
˙
u
C
4
¾
1

m
p
¾
Tp

m
N
:
whereweused
»
1

2
mT
~
N
B
¼
1

m
p
.Since
˙
w

0
,de˝ne,


ª


ª
˙
2
u

ª
Q
i

0
A
i
BB
⁄
‹
A
⁄
“
i
:
Notethat
˙
e
˙
u
B

‹
A
“Y
CA
T

1
Y
¼
T
Y


ª
Y
B

‹
A
“
2
Y
C
Y
ˆ
‹
A
“
T

1
¼
T
Y


ª
Y
(A.12)
B
2
¾
p
p

m
""
0
~
C;
(A.13)
whichisguaranteedby
T
C
T
2

1

log
‹
C""

1
0

‹
A
“
2
Y
C
Y
»
T
Y


ª
Y‹
1

m
~
p
““
log
‹
ˆ
‹
A
““
andensures
R
e
˙
u
º
N
B
""
0
2
¼
Tp

m
N
.Combining
with
R
z
boundofTheorem
3.2
andtailboundof(
A.11
),theseyield
Y
G
‹
ª
“

^
G
‹
ª
“
Y
B
‹
8
˙
z
˙
u

""
0
“
¾
Tp

m
N
;
whenever
N
isstatedasaboveand
T
obeys
T
C
max
‹

1
log
‹
ˆ
‹
A
““
;T
0
“
where
T
0


c
0

log
‹

‹
A
“
2
Y
C
Y
""

1
0
“

log
‹
max
Ÿ‹
1

ˆ
‹
A
““

1
Y
B
Y
¼
N
Tp

m
;
»
T
Y


ª
Y‹
1

m
~
p
“š“

log
‹
ˆ
‹
A
““
C
max
‹
T
1
;T
2
“
:
17
Treating
A
;
B
;
C
relatedvariablesinthenumeratorasconstantterms(whicharelessinsightfulthanthe
log
‹
ˆ
‹
A
““
termforourpurposes),we˝ndthecondition(
4.3
).
Toproceed,wewishtoshowtheresultonHankelmatrices
H
‹
ª
“
and
^
H
‹
ª
“
.Weshalldecomposethe
H
‹
ª
“
matrixas
H
‹
ª
“

H
main

H
tail
(samefor
^
H
).
H
main
;
^
H
main
arethe
m

p
blockscorrespondingtothe
˝rst
T
Markovparametersandtheirestimates.Observethat
H
main
livesontheupper-left
T

T
submatrix.
Furthermore,thesetofnon-zeroblocksineachofits˝rst
T
block-rowsofsize
m

Tp
isasubmatrixof
G
.
Forinstancein
(
4.2
)
,non-zerorowsof
^
H
areallsubmatricesof
^
G
.Consequently,addingspectralnormsof
nonzerorowsandusingtheaboveboundon
G
estimate,wehavethat
Y
^
H
main

H
main
Y
B
T
Y
G

^
G
Y
B
T
‹
8
˙
z
˙
u

""
0
2
“
¾
Tp

m
N
;
where
""
0
~
2
insteadof
""
0
isduetolackoftailterms.Whatremainsisthe
H
tail
term.Notethat
^
H
tail

0
.
H
tail
matrixiscomposedofanti-diagonalblocksthatstartfrom
T

1
tillin˝nity.Thenon-zeroblocksof
i
th
anti-diagonal(
i
C
T

1
)areallequalto
CA
i

2
B
duetoHankelstructure,henceitsspectralnormisequal
to
Y
CA
i

2
B
Y
.Consequently,thespectralnormof
H
tail
canbeobtainedbyaddingthespectralnormof
non-zeroanti-diagonalmatriceswhichisgivenby(
4.1
)andisupperboundedby
""
0
~
2
in(
A.11
).Hence,
Y
^
H
‹
ª
“

H
‹
ª
“
Y
B
Y
^
H
main

H
main
Y

Y
^
H
tail

H
tail
Y
B
T
‹
8
˙
z
˙
u

""
0
“
¾
Tp

m
N
;
concludingtheproof.
B
ProofoftheHo-KalmanStability
Inthissection,weprovideaproofforthestabilityofthe
Ho-Kalman
procedure.Sincesystemisassumed
tobeobservableandcontrollableand
T
1
;T
2
areassumedtobesu˚cientlylarge,
rank
‹
L
“

n
throughout
thissection.Recallthat,givenMarkovparametermatrices
G
;
^
G
,thematrices
H
;
H

;
L
;
H

(with
L

H

as
H

isrank
n
)correspondto
G
andthematrices
^
H
;
^
H

;
^
L
;
^
H

correspondto
^
G
.Wewillshowthat
Ho-Kalman
state-spacerealizationscorrespondingto
G
and
^
G
areclosetoeachotherasafunctionof
Y
G

^
G
Y
.We˝rstprovideaproofofLemma
5.2
.
B.1
ProofofLemma
5.2
Wewishtoshowthat
H

^
H
and
L

^
L
canbeupperboundedintermsof
G

^
G
via
(
5.1
)
.
H


^
H

isa
submatrixof
H

^
H
hencewehave
Y
H


^
H

Y
B
Y
H

^
H
Y
:
Denotethe
i
thblockrowof
H
by
H

i

.Since
H

i

(forall
i
)isasubmatrixoftheMarkovparameter
matrix
G
,wehavethat
Y
H

i


^
H

i

B
Y
G

^
G
Y
.Hence,theoverallmatrix
H
satis˝es
Y
H

^
H
Y

X
X
X
X
X
X
X
X
X
X
X
X
X
X
<
@
@
@
@
@
>
H

1


^
H

1


H

T
1


^
H

T
1

=
A
A
A
A
A
?
X
X
X
X
X
X
X
X
X
X
X
X
X
X
B
»
T
1
max
1
B
i
B
T
1
Y
H

i


^
H

i

B
»
T
1
Y
G

^
G
Y
:
Similarly,columnsof
H
arealsosubmatricesof
G
.Repeatingsameargumentforcolumns,yields
Y
H

^
H
Y
B
»
T
2

1
Y
G

^
G
Y
:
Combiningboth,we˝nd
(
5.1
)
.Thebound
(
5.2
)
isbasedonsingularvalueperturbation.First,noticingthat
rows/columnsof
H

areagaincopiedfrom
G
andcarryingoutthesameargument,wehavethat
Y
H


^
H

Y
B
»
min
Ÿ
T
1
;T
2
šY
G

^
G
Y
:
18
Recallthat
L

H

and
^
L
istherank-
n
approximationsof
^
H

.Denoting
i
thsingularvalueof
^
H

by
˙
i
‹
^
H

“
,standardsingularvalueperturbationboundyields
˙
n

1
‹
^
H

“

Y
^
H


^
L
Y
B
Y
^
H


H

Y
:
Consequently,using
L

H

,
Y
L

^
L
Y
B
Y
H


^
H

Y

Y
^
H


^
L
Y
B
2
Y
H


^
H

Y
B
2
»
min
Ÿ
T
1
;T
2
šY
G

^
G
Y
:
B.2
RobustnessofSingularValueDecomposition
Thenexttheoremshowsrobustnessofsingularvaluedecompositionsof
L
and
^
L
intermsof
Y
L

^
L
Y
.Itis
obtainedbyusingLemma
5
:
14
of[
28
]andprovidessimultaneouscontroloverleftandrightsingularvector
subspaces.ThisisessentiallysimilartoresultsofWedinandDavis-Kahan[
5
,
32
]withtheaddedadvantage
ofsimultaneouscontrolwhichwecruciallyneedforourresult.
LemmaB.1
Suppose
˙
min
‹
L
“
C
2
Y
L

^
L
Y
where
˙
min
‹
L
“
isthesmallestnonzerosingularvalue(i.e.
n
th
largestsingularvalue)of
L
.Letrank
n
matrices
L
,
^
L
havesingularvaluedecompositions
U

V
⁄
and
^
U
^

^
V
⁄
.
Thereexistsan
n

n
unitarymatrix
T
sothat
Y
U

1
~
2

^
U
^

1
~
2
T
Y
2
F

Y
V

1
~
2

^
V
^

1
~
2
T
Y
2
F
B
5
n
Y
L

^
L
Y
:
Proof
DirectapplicationofTheorem
5
:
14
of[
28
]guaranteestheexistenceofaunitary
T
suchthat
LHS

Y
U

1
~
2

^
U
^

1
~
2
T
Y
2
F

Y
V

1
~
2

^
V
^

1
~
2
T
Y
2
F
B
2
º
2

1
Y
L

^
L
Y
2
F
˙
min
‹
L
“
:
Toproceed,usingrank
‹
L

^
L
“
B
2
n
and
˙
min
‹
L
“
C
2
Y
L

^
L
Y
C
»
2
~
n
Y
L

^
L
Y
F
,we˝nd
LHS
B
º
2
n
º
2

1
Y
L

^
L
Y
F
B
2
n
º
2

1
Y
L

^
L
Y
B
5
n
Y
L

^
L
Y
:
Observethatourcontroloverthesubspacedeviationimprovesastheperturbation
Y
L

^
L
Y
getssmaller.The
nextlemmaisastandardresultonsingularvaluedeviation.
LemmaB.2
Suppose
˙
min
‹
L
“
C
2
Y
L

^
L
Y
.Then,
Y
^
L
Y
B
2
Y
L
Y
and
˙
min
‹
^
L
“
C
˙
min
‹
L
“~
2
.
Usingthese,wewillprovetherobustnessof
Ho-Kalman
.Therobustnesswillbeuptoaunitary
transformationsimilartoLemma
B.1
.
B.3
ProofofTheorem
5.3
Proof
ConsidertheSVDof
L
givenby
U

V
andSVDof
^
L
givenby
^
U
^

^
V
where

;
^

>
R
n

n
(recallthat
rank
‹
L
“

n
sinceweassumedsystemisobservableandcontrollable).De˝netheobservability/controllability
matrices(
O

U

1
~
2
;
Q


1
~
2
V
)associatedto
H
and(
^
O

^
U
^

1
~
2
;
^
Q

^

1
~
2
^
V
)associatedto
^
H
.Lemma
B.1
automaticallygivescontrolovertheseasitstatestheexistenceofaunitarymatrix
T
suchthat
Y
O

^
OT
Y
2
F

Y
Q

T
⁄
^
Q
Y
2
F
B
5
n
Y
L

^
L
Y
:
Since

C
isasubmatrixof
O
and

B
isasubmatrixof
Q
,weimmediatelyhavethesameupperboundon
‹

C
;
^
C
“
and
‹

B
;
^
B
“
pairs.
19
Theremainingtaskistoshowthat
^
A
and

A
areclose.Let
X

^
OT
,
Y

T
⁄
^
Q
.Now,notethat
Y

A

T
⁄
^
AT
Y
F

Y
O
—
H

Q
—

T
⁄
^
O
—
^
H

^
Q
—
T
Y
F

Y
O
—
H

Q
—

X
—
^
H

Y
—
Y
F
:
(B.1)
Consequently,wecandecomposetherighthandsideas
Y
O
—
H

Q
—

X
—
^
H

Y
—
Y
F
B
Y‹
O
—

X
—
“
H

Q
—
Y
F

Y
X
—
‹
H


^
H

“
Q
—
Y
F
(B.2)

Y
X
—
^
H

‹
Q
—

Y
—
“Y
F
:
Wetreatthetermsontherighthandsideindividually.First,pseudo-inversesatis˝estheperturbation
bound[
20
,
30
]
Y
O
—

X
—
Y
F
B
Y
O

X
Y
F
max
ŸY
X
—
Y
2
;
Y
O
—
Y
2
š
B
¼
5
n
Y
L

^
L
Y
max
ŸY
X
—
Y
2
;
Y
O
—
Y
2
š
:
Weneedtoboundtherighthandside.Luckily,Lemma
B.2
triviallyyieldsthecontroloverthetopsingular
valuesofpseudo-inversesnamely
max
ŸY
X
—
Y
2
;
Y
O
—
Y
2
š

max
Ÿ
1
˙
min
‹
L
“
;
1
˙
min
‹
^
L
“
š
B
2
˙
min
‹
L
“
:
Combiningthelasttwobounds,we˝nd
Y
O
—

X
—
Y
F
B
2
¼
5
n
Y
L

^
L
Y
˙
min
‹
L
“
Theidenticalboundsholdfor
Q
;
Y
.Forthesecondtermontherighthandsideof
(
B.2
)
,weshallusethe
estimate
Y
X
—
‹
H


^
H

“
Q
—
Y
F
B
º
n
Y
X
—
‹
H


^
H

“
Q
—
Y
B
2
º
n
˙
min
‹
L
“
Y
H


^
H

Y
:
Finally,wewillusethestandardtriangleinequalitytoaddressthe
^
H

term:
Y
^
H

Y
B
Y
H

Y

Y
H


^
H

Y
.
Combiningallofthese,weobtainthefollowingbounds
Y‹
O
—

X
—
“
H

Q
—
Y
F
B
Y
O
—

X
—
Y
F
Y
H

YY
Q
—
Y
(B.3)
B
¼
20
n
Y
L

^
L
Y
˙
min
‹
L
“
¾
2
˙
min
‹
L
“
Y
H

Y
(B.4)
B
7
¼
n
Y
L

^
L
Y
˙
min
‹
L
“
3
~
2
Y
H

Y
(B.5)
Y
X
—
^
H

‹
Q
—

Y
—
“Y
F
B
Y
X
—
YY
^
H

YY
Q
—

Y
—
Y
F
(B.6)
B
7
¼
n
Y
L

^
L
Y
˙
min
‹
L
“
3
~
2
‹Y
H

Y

Y
H


^
H

Y“
(B.7)
Y
X
—
‹
H


^
H

“
Q
—
Y
F
B
2
º
n
Y
H


^
H

Y
˙
min
‹
L
“
:
Combiningthesethreeindividualboundsandsubstitutingin(
B.2
),we˝ndtheoverallbound
Y

A

T
⁄
^
AT
Y
F
B
14
º
n
˙
min
‹
L
“
‹
¿
Á
Á
À
Y
L

^
L
Y
˙
min
‹
L
“
‹Y
H

Y

Y
H


^
H

Y“

Y
H


^
H

Y“
:
20
B.4
ProofofCorollary
5.4
UsingLemma
5.3
,thecondition
Y
H

^
H
Y
B
˙
min
‹
L
“~
4
impliesthecondition
(
5.3
)
.Consequently,inequalities
(
5.4
)
and
(
5.6
)
ofTheorem
5.3
holds.
(
5.7
)
followsbyusing
Y
L

^
L
Y
B
2
Y
H

^
H
Y
.Theresulton

A
isslightly
moreintricate.First,since
H

isasubmatrixof
H
Y
H


^
H

Y
B
Y
H

^
H
Y
;
Y
H

Y
B
Y
H
Y
Combiningthiswith(
5.2
),therighthandsideof(
5.6
)canbeupperboundedby
RHS

14
º
2
º
n
˙
min
‹
L
“
‹
¿
Á
Á
À
Y
H

^
H
Y
˙
min
‹
L
“
‹Y
H
Y

Y
H

^
H
Y“

Y
H

^
H
Y“
:
Next,
L

H

hence
4
Y
H

^
H
Y
B
˙
min
‹
L
“
B
Y
L
Y
B
Y
H
Y
.Hence
Y
H
Y

Y
H

^
H
Y
B
‹
5
~
4
“Y
H
Y
.Finally,
Y
H
Y
»
˙
min
‹
L
“
C
»
Y
H
Y
C
2
¼
Y
H

^
H
Y
:
Combiningthelasttwoobservations,RHScanbeupperboundedas
RHS

14
º
2
¼
n
Y
H

^
H
Y
˙
min
‹
L
“
‹
Y
H
Y

Y
H

^
H
Y
»
˙
min
‹
L
“

¼
Y
H

^
H
Y“
(B.8)
B
14
º
2
¼
n
Y
H

^
H
Y
˙
min
‹
L
“
‹
5
4
Y
H
Y
»
˙
min
‹
L
“

¼
Y
H

^
H
Y“
(B.9)
B
14
º
2
¼
n
Y
H

^
H
Y
˙
min
‹
L
“
‹
5
4
Y
H
Y
»
˙
min
‹
L
“

1
2
Y
H
Y
»
˙
min
‹
L
“
“
(B.10)
B
‹
7
~
4
“
14
º
2
¼
n
Y
H

^
H
Y
˙
min
‹
L
“
Y
H
Y
»
˙
min
‹
L
“
;
(B.11)
whichistheadvertisedboundafternoticing
‹
7
~
4
“
14
º
2
B
50
.
C
RestrictedIsometryofPartialCirculantMatrices
Toproceed,letusdescribethegoalofthissection.First,wewouldliketoshowthat
U
>
R
N

Tp
iswell
conditionedwhen
N
à
O
‹
Tp
“
toensureleast-squaresisrobust.Next,wewouldliketohaveanaccurateupper
boundonthespectralnormof
U
⁄
W
tocontroltheimpactofnoise
w
t
.Inparticular,wewillshowthat
Y
U
⁄
W
Y
ß
˙
u
˙
w
»
NT
‹
p

n
“
:
Bothofthesegoalswillbeachievedbyembedding
U
and
W
intopropercirculantmatrices.Thesame
argumentwillapplytobothscenarios.ThekeytechnicaltoolinouranalysiswillbetheresultsofKrahmer
etal.[
18
]onrestrictedisometriesofrandomcirculantmatrices.
ThefollowingtheoremisarestatementofTheorem
4
:
1
ofKrahmeretal[
18
].Weaddedaminor
modi˝cationtoaccountfortheregime
restrictedisometryconstant
isgreaterthan
1
.Thisresultisprovenin
Section
E
.Thistheoremshowsthatarbitrarysubmatricesofrandomcirculantmatricesarewellconditioned.
Itwillplayacrucialroleinestablishingthejointrelationofthedatamatrix
U
andnoisematrix
W
.Main
resultof[
18
]characterizesauniformboundonallsubmatrices;howeverweonlyneedasinglesubmatrixfor
ourresults.Hence,someofthelogarithmicfactorsbelowmightactuallyberedundantfortheboundweare
seeking.
21
TheoremC.1
Let
C
>
R
d

d
beacirculantmatrixwherethe˝rstrowisdistributedas
N
‹
0
;
I
d
“
.Given
s
C
1
,
set
m
0

c
0
s
log
2
‹
2
s
“
log
2
‹
2
d
“
forsomeabsoluteconstant
c
0
A
0
.Pickan
m

s
submatrix
S
of
C
.With
probabilityatleast
1

‹
2
d
“

log
‹
2
d
“
log
2
‹
2
s
“
,
S
satis˝es
Y
1
m
S
⁄
S

I
Y
B
max
Ÿ
½
m
0
m
;
m
0
m
š
:
Thenexttwosectionsaddresstheminimumsingularvalueofthe
U
matrixandupperboundingthemaximum
singularvalueofthe
U
⁄
W
matrixbyutilizingTheorem
C.1
.
C.1
ConditioningoftheDataMatrix
LemmaC.2
Let
U
>
R
N

Tp
betheinputdatamatrixasdescribedinSection
2.1
.Supposethesample
sizeobeys
N
C
cTp
log
2
‹
2
Tp
“
log
2
‹
2

Np
“
forsu˚cientlylargeconstant
c
A
0
.Then,withprobabilityatleast
1

‹
2

Np
“

log
2
‹
2
Tp
“
log
‹
2

Np
“
,
2
N˙
2
u
k
U
⁄
U
k
N˙
2
u
~
2
:
Proof
Theproofwillbeaccomplishedbyembedding
U
insideapropercirculantmatrix.Let
r
‹
v
“

R
d

R
d
bethecirculantshiftoperatorwhichmapsavector
v
>
R
d
toitssingleentrycircularrotationtotherighti.e.
r
‹
v
“


v
d
v
1
:::
v
d

1

>
R
d
.Let
C
>
R

Np


Np
beacirculantmatrixwherethe˝rstrow(transposed)isgiven
by
c
1


u
⁄

Np
u
⁄

Np

1
:::
u
⁄
2
u
⁄
1

⁄
:
The
i
throwof
C
is
c
i

r
i

1
‹
c
1
“
for
1
B
i
B

Np
.Observethat
C
isacirculantmatrixbyconstruction.For
instanceallofitsdiagonalentriesareequalto
u

Np;
1
.Additionally,notethatsecondrowof
C
startswiththe
lastentryof
u
1
henceentriesof
u
i
donotnecessarilylienexttoeachother.Focusingontherightmost
Tp
columns,let
R
Tp
betheoperatorthatreturnsrightmost
Tp
entriesofavector.Our˝rstobservationisthat
R
Tp
‹
c
1
“


u
T


u
⁄
T
u
⁄
T

1
:::
u
⁄
2
u
⁄
1

⁄
:
Secondly,observethatforeach
0
B
i
B
N

1
R
Tp
‹
c
1

ip
“

R
Tp
‹
r
ip
‹
c
1
““


u
⁄
T

i
u
⁄
T

1

i
:::
u
⁄
2

i
u
⁄
1

i

⁄


u
T

i
:
Thisimpliesthat

u
T

i
isembeddedinsideright-most
Tp
columnsand
1

ip
'throwof
C
.Similarly,theinput
datamatrix
U
>
R
N

Tp
isasubmatrixof
C
withcolumnindices
‹

N

T
“
p

1
to

Np
androwindices
1

ip
for
0
B
i
B
N

1
.ApplyingTheorem
C.1
,setting
N
0

cTp
log
2
‹
2
Tp
“
log
2
‹
2

Np
“
,andadjustingforvariance
˙
2
u
,withprobabilityatleast
1

‹
2

Np
“

log
2
‹
2
Tp
“
log
‹
2

Np
“
,wehave
2
˙
2
u
I
k
N

1
U
⁄
U
k
˙
2
u
2
I
Ô
2
N˙
2
u
k
U
⁄
U
k
N˙
2
u
~
2
;
whenever
N
C
N
0
.
C.2
UpperBoundingtheContributionoftheProcessNoise
LemmaC.3
Recall
U
;
W
from
(
2.6
)
and
(
A.1
)
respectively.Let
q

p

n
and
N
0

cTq
log
2
‹
2
Tq
“
log
2
‹
2

Nq
“
where
c
A
0
isanabsoluteconstant.Withprobabilityatleast
1

‹
2

Nq
“

log
2
‹
2
Tq
“
log
‹
2

Nq
“
,
Y
U
⁄
W
Y
B
˙
w
˙
u
max
Ÿ
»
N
0
N;N
0
š
:
22
Proof
TheproofisidenticaltothatofLemma
C.2
.Set
q

p

n
.First,wede˝ne
m
t


˙

1
u
u
⁄
t
˙

1
w
w
⁄
t

⁄
>
R
q
and

m
i


m
⁄
i
;
m
⁄
i

1
;:::
m
⁄
i

T

1

⁄
>
R
Tq
.Wealsode˝nethematrix
M



m
T
:::

m
T

N

1

⁄
>
R
N

Tq
.
Observethatbyconstruction,
˙

1
u
U
;˙

1
w
W
aresubmatricesof
M
.Inparticular,
‹
˙
u
˙
w
“

1
U
⁄
W
isan
o˙-diagonalsubmatrixof
M
⁄
M
ofsize
Tp

Tn
.Thisisduetothefactsthati)
˙

1
u
U
isasubmatrixof
M
characterizedbythecolumnindices
Ÿ‹
i

1
“
q

j
T
1
B
i
B
T;
1
B
j
B
p
š
;
andii)
˙

1
w
W
liesatthecomplementarycolumns.Observethatthespectralnormof
‹
˙
u
˙
w
“

1
U
⁄
W
canbe
upperboundedas
‹
˙
u
˙
w
“

1
Y
U
⁄
W
Y
B
Y
M
⁄
M

N
I
Y
:
(C.1)
Proof
Since
‹
˙
u
˙
w
“

1
U
⁄
W
isano˙-diagonalsubmatrixof
M
⁄
M
,itisalsoasubmatrixof
M
⁄
M

I
.
Spectralnormofasubmatrixisupperboundedbythenormoftheoriginalmatrixhencetheclaimfollows.
InasimilarfashiontoLemma
C.2
,wecomplete
M
tobeafullcirculantmatrixasfollows.Let
r
‹
v
“

R
d

R
d
bethecirculantshiftoperatoraspreviously.Let
C
>
R

Nq


Nq
beacirculantmatrixwith˝rst
rowgivenby
c
1


m
⁄

Nq
m
⁄

Nq

1
:::
m
⁄
2
m
⁄
1

⁄
:
The
i
throwof
C
is
c
i

r
i

1
‹
c
1
“
for
1
B
i
B

Nq
.Let
R
Tq
betheoperatorthatreturnsrightmost
Tq
entries
ofavector.Our˝rstobservationisthat
R
Tq
‹
c
1
“


m
T


m
⁄
T
m
⁄
T

1
:::
m
⁄
2
m
⁄
1

⁄
:
Secondly,observethatforeach
0
B
i
B
N

1
R
Tq
‹
c
1

iq
“

R
Tq
‹
r
iq
‹
c
1
““


m
⁄
T

i
m
⁄
T

1

i
:::
m
⁄
2

i
m
⁄
1

i

⁄


m
T

i
:
Thisimpliesthat

m
i
'sareembeddedinsidetherowsof
R
Tq
‹
C
“
inanequallyspacedmannerwithspacing
q
for
T
B
i
B
T

N

1


N
.Hence,
M
isa
N

Tq
submatrixof
C
wherethecolumnindicesarethelast
Tq
columnsandtherowindicesare
1
;
1

q;:::;
1

‹
N

1
“
q
.
Withthisobservation,wearereadytoapplyTheorem
C.1
.Theorem
C.1
statesthatfor
N
0

cTq
log
2
‹
2
Tq
“
log
2
‹
2

Nq
“
;
withprobabilityatleast
1

‹
2

Nq
“

log
2
‹
2
Tq
“
log
‹
2

Nq
“
,
Y
1
N
M
⁄
M

I
Y
B
max
Ÿ
¾
N
0
N
;
N
0
N
š
;
whichinturnimplies
Y
U
⁄
W
Y
B
˙
w
˙
u
max
Ÿ
º
N
0
N;N
0
š
viainequality(
C.1
).
D
BoundingtheErrorduetotheUnknownState
Thegoalofthissectionisboundingtheestimationerrorduetothe
e
t

CA
T

1
x
t

T

1
term.Asdescribed
inSection
2.1
and
(
A.1
)
,weformthematrices
E


e
T
:::
e

N

⁄
and
U



u
T
:::

u

N

⁄
.Ourinterestin
thissectionisbounding
Y
U
⁄
E
Y
.Thistermcapturestheimpactofapproximatingthesystemwitha˝nite
impulseresponseoflength
T
.Wewillshowthat
Y
U
⁄
E
Y
ß
˙
u
»
‹
Tp

m
“
NT
Y

ª
YY
CA
T

1
Y
2
:
Themainchallengeinanalyzing
U
⁄
E
isthefactthat
Ÿ
e
t
š

N
t

T
termsand
Ÿ

u
t
š

N
t

T
termsaredependent.
Infact
e
t
containsa
u
˝
componentinsideforany
˝
B
t

T
.Thefollowingtheoremisourmainresulton
boundingthistermwhichcarefullyaddressesthesedependencies.
23
TheoremD.1
Supposewearegiven
U
;
E
,asdescribedinSection
2.1
and
(
A.1
)
.De˝ne


Y

ª
Y

‹
A
“
2
Y
CA
T

1
Y
2
1

ˆ
‹
A
“
2
T
andsuppose
N
C
T
.Then,withprobabilityatleast
1

T
‹
exp
‹

100
Tp
“

2exp
‹

100
m
““
,
Y
U
⁄
E
Y
B
c˙
u
¾
T
max
Ÿ
N;
mT
1

ˆ
‹
A
“
T
š
max
Ÿ
Tp;m
š
:
Proof
We˝rstdecompose
U
⁄
E

P

N
t

T

u
t
e
⁄
t
intosumof
T
smallerproducts.Given
0
B
t
@
T
,create
sequences
S
t

Ÿ
t

T;t

2
T;:::;t

N
t
T
š
where
N
t
isthelargestintegersatisfying
t

N
t
T
B

N
.Eachsequence
haslength
N
t
whichisatleast

N
~
T

andatmost

N
~
T


1
.Withthis,weformthematrices
U
t



u
t

T
;

u
t

2
T
;:::;

u
t

N
t
T

⁄
;
E
t


e
t

T
;
e
t

2
T
;:::;
e
t

N
t
T

⁄
:
(D.1)
Then,
U
⁄
E
canbedecomposedas
U
⁄
E

T

1
Q
t

0
U
⁄
t
E
t
Ô
Y
U
⁄
E
Y
B
T

1
Q
t

0
Y
U
⁄
t
E
t
Y
:
(D.2)
Corollary
D.3
providesaprobabilisticspectralnormboundoneachtermofthisdecompositionontheright
handside.Inparticular,applyingCorollary
D.3
,substituting
˛
de˝nition,andunionboundingover
T
terms,
forall
t
,weobtain
Y
U
⁄
t
E
t
Y
B
c˙
u
¾
max
Ÿ
N;
mT
1

ˆ
‹
A
“
T
š
max
Ÿ
p;m
~
T
š
;
withprobabilityatleast
1

T
‹
exp
‹

Tq
“

2
exp
‹

100
m
““
.Thisgivestheadvertisedboundon
U
⁄
E
via
(
D.2
).
D.1
UpperBoundingtheComponentsoftheUnknownStateDecomposition
Ourgoalinthissectionisprovidinganupperboundonthespectralnormof
U
⁄
t
E
t
whichisdescribedin
(
D.1
)
.Thefollowinglemmaprovidesaboundthatdecayswith
1
~
º
N
t
.Themaintoolsinouranalysisarethe
probabilisticupperboundonthe
E
t
matrixdevelopedinSection
D.2
andmartingaleconcentrationbound
thatwasdevelopedandutilizedbytherecentworkofSimchowitzetal[
25
].Belowwestateourboundinthe
morepracticalsetup
m
B
n
toavoidredundantnotation.Ingeneral,ourboundscaleswith
min
Ÿ
m;n
š
.
TheoremD.2
De˝ne


Y

ª
Y

‹
A
“
2
Y
CA
T

1
Y
2
1

ˆ
‹
A
“
2
T
.
U
⁄
t
E
t
obeys
Y
U
⁄
t
E
t
Y
B
c
0
˙
u
»
˝
max
Ÿ
Tp;m
š
N
t
;
withprobabilityatleast
1

exp
‹

100max
Ÿ
Tp;m
š“

2exp
‹

c˝N
t
‹
1

ˆ
‹
A
“
T
“

3
m
“
for
˝
C
1
.
Proof
Givenmatrices
U
t
;
E
t
,de˝nethe˝ltrations
F
i

˙
‹Ÿ
u
j
;
w
j
š
t

iT
j

1
“
for
1
B
i
B
N
t
.Accordingtothis
de˝nition

u
t

iT
isindependentof
F
i

1
and

u
t

iT
>
F
i
.Thereasonisearliestinputvectorcontainedby

u
t

iT
hasindex
t

1

‹
i

1
“
T
whichislargerthan
t

‹
i

1
“
T
.Additionally,observethat
e
t

iT
>
F
i

1
as
e
t

iT
isa
deterministicfunctionof
x
t

1

‹
i

1
“
T
whichisafunctionof
Ÿ
u
j
;
w
j
š
t

‹
i

1
“
T
j

1
.
Wewouldliketousethefactthat,foreach
i
,
e
t

iT
and

u
t

iT
areindependent.Let
X
t


x
t

1
:::
x
t

1

‹
N
t

1
“
T

⁄
sothat
E
t

X
t
‹
CA
T

1
“
⁄
.InlightofLemma
D.5
,wewilluseacoveringboundonthematrix
U
⁄
t
E
t

U
⁄
t
X
t
‹
CA
T

1
“
⁄
:
Let
C
1
bea
1
~
4
`
2
-coveroftheunitsphere
S
Tp

1
and
C
2
bea
1
~
4
`
2
-coveroftheunitsphereintherowspace
of
C
.Thereexistssuchcoverssatisfying
log
S
C
1
S
B
3
Tp
and
log
S
C
2
S
B
3
min
Ÿ
m;n
š
B
3
m
.Pickvectors
a
;
b
from
C
1
;
C
2
respectively.Let
W
i

a
⁄

u
t

iT
and
Z
i

b
⁄
e
t

iT
.Observethat
N
t
Q
i

1
W
i
Z
i

a
⁄
‹
U
⁄
t
E
t
“
b
:
24
Wenextshowthat
P
N
t
i

1
W
i
Z
i
issmallwithhighprobability.ApplyingLemma
D.6
,we˝ndthat,for
˝
C
2
,
withprobabilityatleast
1

2exp
‹

c˝N
t
‹
1

ˆ
‹
A
“
T
““
,
Y
E
t
b
Y
2
`
2

N
t
Q
i

1
Z
2
i
B
˝N
t
;
(D.3)
whereourde˝nitionof

accountsforthe
Y

ª
Y
factor.WewillusethisboundtoensureLemma
D.4
is
applicablewithhighprobability.Since

u
t

iT
has
N
‹
0
;˙
2
u
“
entries,applyingLemma
D.4
,weobtain
P
‹Ÿ
N
t
Q
i

1
W
i
Z
i
C
t
š
˛
Ÿ
N
t
Q
i

1
Z
2
i
B
˝N
t

š“
B
exp
‹

t
2
c˝˙
2
u
N
t

“
:
forsomeabsoluteconstant
c
A
0
.Picking
t

11
˙
u
»
c˝
max
Ÿ
Tp;m
š
N
t

,we˝nd
P
‹Ÿ
N
t
Q
i

1
W
i
Z
i
C
t
š
˛
Ÿ
N
t
Q
i

1
Z
2
i
B
cN
t

š“
B
exp
‹

120max
Ÿ
Tp;m
š“
:
De˝ningvariables
W
i
‹
a
“
foreach
a
>
C
1
,andevents
E
‹
a
“

Ÿ
P
N
t
i

1
W
i
‹
a
“
Z
i
C
t
š
,applyingaunionbound,we
obtain,
P
‹Ÿ
˜
a
>
C
1
E
‹
a
“š
˛
Ÿ
N
t
Q
i

1
Z
2
i
B
cN
t

š“
B
exp
‹

110max
Ÿ
Tp;m
š“
:
Combiningthisboundwith
(
D.3
)
,we˝ndthat,fora
˝xed
b
and
forall
a
,withprobabilityatleast
1

exp
‹

110max
Ÿ
Tp;m
š“

2exp
‹

c˝N
t
‹
1

ˆ
‹
A
“
T
““
,wehave
a
⁄
U
⁄
t
E
t
b

N
t
Q
i

1
W
i
Z
i
B
c
0
˙
u
»
˝
max
Ÿ
Tp;m
š
N
t
;
(D.4)
forsome
c
0
A
0
.Applyingaunionboundoverall
b
>
C
2
,withprobabilityatleast
1

exp
‹

100
max
Ÿ
Tp;m
š“

2
exp
‹

c˝N
t
‹
1

ˆ
‹
A
“
T
“

3
m
“
,we˝ndthat
(
D.4
)
holdsforall
a
;
b
.Overall,wefoundthatforall
a
;
b
pairsin
the
1
~
4
covers,
a
⁄
‹
U
⁄
t
E
t
“
b
B


c
0
˙
u
»
˝
max
Ÿ
Tp;m
š
N
t

.ApplyingLemma
D.5
,thisimplies
Y
U
⁄
t
E
t
Y
B
2

.
Thefollowingcorollarysimpli˝estheresultwhen
N
C
T
whichistheinterestingregimeforourpurposes.
CorollaryD.3
Assume
N
C
T
.Withprobabilityatleast
1

exp
‹

100
Tp
“

2
exp
‹

100
m
“
,wehave
Y
U
⁄
t
E
t
Y
B
c
œ
˙
u
¼
max
Ÿ
N;
mT
1

ˆ
‹
A
“
T
š
max
Ÿ
p;m
~
T
š

forsomeconstant
c
œ
A
0
.
Proof
N
C
T
implies
N
t
C

N
~
T

C
N
~‹
2
T
“
.InTheorem
D.2
,pick
˝

max
Ÿ
1
;c
1
mT
N
‹
1

ˆ
‹
A
“
T
“
š
for
c
1

206
~
c
.
Thechoiceof
˝
guaranteestheprobabilityexponent
c˝N
t
‹
1

ˆ
‹
A
“
T
“

3
m
C
100
m
.Toconclude,observethat
c
0
˙
u
»
˝
max
Ÿ
Tp;m
š
N
t

B
c
œ
˙
u
¼
max
Ÿ
1
;
mT
N
‹
1

ˆ
‹
A
“
T
“
š
max
Ÿ
p;m
~
T
š
N
foranabsoluteconstant
c
œ
A
0
.
Forcompleteness,werestatethesubgaussianMartingaleconcentrationlemmaofSimchowitzetal.whichis
Lemma
4
:
2
of[
25
].
LemmaD.4
Let
Ÿ
F
t
š
t
C
1
bea˝ltration,
Ÿ
Z
t
;W
t
š
t
C
1
berealvaluedprocessesadaptedto
F
t
;
F
t

1
respectively
(i.e.
Z
t
>
F
t
;W
t
>
F
t

1
).Suppose
W
t
T
F
t
isa
˙
2
-sub-gaussianrandomvariablewithmeanzero.Then
P
‹Ÿ
T
Q
t

1
Z
t
W
t
C

š
˛
Ÿ
T
Q
t

1
Z
2
t
B

š“
B
exp
‹


2
2
˙
2

“
Thislemmaimpliesthat
P
T
t

1
Z
t
W
t
canessentiallybetreatedasaninnerproductbetweenadeterministic
sequence
Z
t
andani.i.d.subgaussiansequence
W
t
.
Thefollowinglemmaisaslightmodi˝cationofthestandardcoveringarguments.
25
LemmaD.5(Coveringbound)
Givenmatrices
A
>
R
n
1

N
;
B
>
R
N

n
2
,let
M

AB
.Let
C
1
bea
1
~
4
-
coveroftheunitsphere
S
n
1

1
and
C
2
bea
1
~
4
-coveroftheunitsphereintherowspaceof
B
(whichisat
most
min
Ÿ
N;n
2
š
dimensional).Supposeforall
a
>
C
1
;
b
>
C
2
,wehavethat
a
⁄
Mb
B

.Then,
Y
M
Y
B
2

.
Proof
Pickunitlengthvectors
x
;
y
achieving
x
⁄
My

Y
M
Y
.Let
S
betherowspaceof
B
.Observethat
y
>
S
.Otherwise,itsnormalizedprojectionon
S
,
P
S
‹
y
“~Y
P
S
‹
y
“Y
`
2
achievesastrictlybetterinnerproduct
with
x
⁄
M
.Pick
1
~
4
closeneighbors
a
;
b
of
x
;
y
fromthecovers
C
1
;
C
2
.Then,
x
⁄
My

a
⁄
Mb

‹
x

a
“
⁄
Mb

x
⁄
M
‹
y

b
“
B


x
⁄
My
~
2
;
whereweusedthemaximalityof
x
;
y
.Thisyields
x
⁄
My
B
2

.
D.2
BoundingInnerProductswiththeUnknownState
Inthissection,wedevelopprobabilisticupperboundsfortherandomvariable
E
t
a
where
a
isa˝xedvector
and
E
t
isasde˝nedin(
D.1
).
LemmaD.6
Let
E
t
>
R
N
t

m
bethematrixcomposedoftherows
e
t

iT

CA
T

1
x
t

1

iT
.De˝ne



‹
A
“
2
Y
CA
T

1
Y
2
1

ˆ
‹
A
“
2
T
:
Givenaunitlengthvector
a
>
R
m
,forall
˝
C
2
andforsomeabsoluteconstant
c
A
0
,wehavethat
P
‹Y
E
t
a
Y
2
`
2
C
˝N
t
Y

ª
Y

“
B
2exp
‹

c˝N
t
‹
1

ˆ
‹
A
“
T
““
:
Proof
Let
d
t

x
t

A
T
x
t

T
.Byconstruction(i.e.duetothestate-spacerecursion
(
2.2
)
),
d
t
isindependent
of
x
t

T
.Wecanwrite
x
t

iT
as
x
t

iT

i
Q
j

1
A
‹
i

j
“
T
d
t

jT

A
iT
x
t
:
(D.5)
Wewishtounderstandthepropertiesoftherandomvariable
Y
E
t
a
Y
2
`
2
whichissameas,
s
a

N
t
Q
i

1
‹
a
⁄
e
t

iT
“
2

N
t

1
Q
i

0
‹‹
a
⁄
CA
T

1
“
x
t

1

iT
“
2
:
Denote

a

‹
CA
T

1
“
⁄
a
,
a
j

‹
A
jT
“
⁄

a
,
g
0

x
t

1
,and
g
i

d
t

1

iT
for
N
t

1
C
i
C
1
,allofwhichare
n
dimensionalvectors.Usingthesechangeofvariablesandapplyingtheexpansion
(
D.5
)
,the
i
thcomponentof
thesum
s
a
isgivenby
s
a
;i

‹

a
⁄
x
t

1

iT
“
2

‹

a
⁄
i
Q
j

0
A
‹
i

j
“
T
g
j
“
2

‹
i
Q
j

0
a
⁄
i

j
g
j
“
2

Q
0
B
j;k
B
i
a
⁄
i

j
g
j
a
⁄
i

k
g
k
:
(D.6)
Observethat,summingoverall
s
a
;i
for
0
B
i
B
N
t

1
,themultiplicativecoe˚cientofthe
g
j
g
⁄
k
pairisgiven
bythematrix,
M
j;k

¢
¨
¨
¨
¨
¦
¨
¨
¨
¨
¤
P
N
t
A
i
C
max
Ÿ
j;k
š
a
i

j
a
⁄
i

k
if
j
x
k;
P
N
t
A
i
C
j
a
i

j
a
⁄
i

j

P
N
t

1

j
i

0
a
i
a
⁄
i
if
j

k
(D.7)
26
Next,weshowthatthese
M
j;k
submatriceshaveboundedspectral,Frobeniusandnuclearnorms(nuclear
normisthesumofthesingularvaluesofamatrix).Thisfollowsbywritingeachsubmatrixasasumofrank
1
matricesandusingthefactthatspectralradiusof
A
isstrictlyboundedfromaboveby
1
.
Y
M
j;k
Y
B
Y
M
j;k
Y
F
B
Y
M
j;k
Y
ƒ
B
Q
i
C
max
Ÿ
j;k
š
Y
a
i

j
a
⁄
i

k
Y
ƒ

Q
i
C
max
Ÿ
j;k
š
Y
a
i

j
a
⁄
i

k
Y
B
Q
i
C
max
Ÿ
j;k
š
Y‹
A
‹
i

j
“
T
“
⁄

a

a
⁄
A
‹
i

k
“
T
Y
B
Q
i
C
max
Ÿ
j;k
š
Y

a
Y
2
`
2
Y
A
‹
i

j
“
T
YY
A
‹
i

k
“
T
Y
B
ª
Q
i

0
Y

a
Y
2
`
2
ˆ
‹
A
“
S
j

k
S
T
ˆ
‹
A
“
2
iT

‹
A
“
2
B

‹
A
“
2
Y

a
Y
2
`
2
1

ˆ
‹
A
“
2
T
ˆ
‹
A
“
S
j

k
S
T
:
Tofurthersimplify,observethat
Y

a
Y
2
`
2
B
Y
CA
T

1
Y
2
as
Y
a
Y
`
2

1
.Setting



‹
A
“
2
Y
CA
T

1
Y
2
1

ˆ
‹
A
“
2
T
;
wehave
Y
M
j;k
Y
;
Y
M
j;k
Y
F
;
Y
M
j;k
Y
ƒ
B
ˆ
‹
A
“
S
j

k
S
T
:
(D.8)
Basedonthesubmatrices
M
j;k
,createthe
N
t
n

N
t
n
matrix
M
.Nowwede˝nethevector

g


g
⁄
0
g
⁄
1
:::
g
⁄
N
t

1

⁄
.
Observethat,following(
D.6
)and(
D.7
),byconstruction,
s
a


g
⁄
M

g

Q
0
B
j;k
@
N
t
g
⁄
j
M
j;k
g
k
:
(D.9)
Thisputs
s
a
inaformforwhichHanson-WrightTheoremisapplicable[
1
,
22
].ToapplyHanson-Wright
Theorem,letus˝rstboundtheexpectationof
s
a
.Since
Ÿ
g
i
š
N
t

1
i

0
'saretruncationsofthestatevector,we
havethat

‹
g
i
“
j

‹
x
t

1

iT
“
j

ª
.Write
g
i


‹
g
i
“
1
~
2
h
i
forsome
h
i

N
‹
0
;
I
n
“
.Usingindependenceof
h
i
;
h
j
for
i
x
j
and

‹
g
i
“
j

ª
,wehavethat
E

s
a


N
t

1
Q
i

0
E

g
⁄
i
M
i;i
g
i


N
t

1
Q
i

0
E

h
⁄
i

‹
g
i
“
1
~
2
M
i;i

‹
g
i
“
1
~
2
h
i

(D.10)

N
t

1
Q
i

0
tr
‹

‹
g
i
“
1
~
2
M
i;i

‹
g
i
“
1
~
2
“
(D.11)
B
N
t

1
Q
i

0
Y

‹
g
i
“Y
tr
‹
M
i;i
“
B
N
t

1
Q
i

0
Y

ª
Y
tr
‹
M
i;i
“
(D.12)
B
N
t
Y

ª
Y
:
(D.13)
In
(
D.11
)
,weutilizedthefactthatforpositivesemide˝nitematricestraceisequaltothenuclearnormand
thenweusedthefactthatnuclearnormoftheproductobeys
Y
XY
Y
ƒ
B
Y
X
Y
ƒ
Y
Y
Y
[
16
].Finally,weupper
bounded
Y

‹
g
i
“Y
byusingtherelation

‹
g
i
“
j

ª
.Bounded
Y

‹
g
i
“Y
alsoimpliesthattheGaussianvector
g
i
obeystheentrationpropert(De˝nition
2
:
1
of[
1
])with
K

O
‹
»
Y

ª
Y“
asLipschitzfunctionsof
Gaussiansconcentrate.Recalling(
D.9
),theHanson-WrightTheoremof[
1
]statesthat
P
‹
s
a
C
E

s
a


t
“
B
2exp
‹

c
min
Ÿ
t
2
Y

ª
Y
2
Y
M
Y
2
F
;
t
Y

ª
YY
M
Y
š“
:
27
Toproceed,weupperbound
Y
M
Y
F
and
Y
M
Y
.First,recallagainthat
Y
M
i;j
Y
F
B
ˆ
‹
A
“
S
i

j
S
T
.Addingthese
overall
i;j
pairs,using
(
D.8
)
andthefactthatthereareatmost
2
N
t
pairswith˝xeddi˙erence
S
i

j
S

˝
,we
obtain
Y
M
Y
2
F

Q
i;j
Y
M
i;j
Y
2
F
B
Q
0
B
i;j
B
N
t

1

2
ˆ
‹
A
“
2
S
i

j
S
T
B
2
N
t

2
N
t

1
Q
˝

0
ˆ
‹
A
“
2
˝T
B
2

2
N
t
1

ˆ
‹
A
“
2
T
:
Toassessthespectralnorm,wedecompose
M
into
2
N
t

1
blockpermutationmatrices
Ÿ
M
‹
i
“
š
N
t

1
i


N
t

1
.
M
‹
0
“
isthemaindiagonalof
M
,and
M
‹
i
“
isthe
i
tho˙-diagonalthatcontainsonlythesubmatrices
M
j;k
with
˝xeddi˙erence
j

k

i
.Byconstruction
Y
M
‹
i
“
Y
B
ˆ
‹
A
“
S
i
S
T
aseachnonzerosubmatrixsatis˝esthesame
spectralnormbound.Henceusing(
D.8
),
Y
M
Y
B
N
t

1
Q
i


N
t

1
Y
M
‹
i
“
Y
B

‹
2
1

ˆ
‹
A
“
T

1
“
B
2

1

ˆ
‹
A
“
T
:
Withthese,setting
t

˝N
t
Y

ª
Y

andusing
(
D.13
)
andboundson
Y
M
Y
F
;
Y
M
Y
,for
˝
C
1
andusing
K

O
‹
»
Y

ª
Y“
,andapplyingTheorem
2
:
3
of[
1
],we˝ndtheconcentrationbound
P
‹
s
a
C
‹
˝

1
“
N
t
Y

ª
Y

“
B
2exp
‹

2
c˝
min
Ÿ
‹
N
t
Y

ª
Y

“
2
Y

ª
Y
2
2

2
N
t
1

ˆ
‹
A
“
2
T
;
N
t
Y

ª
Y

Y

ª
Y
2

1

ˆ
‹
A
“
T
š“
(D.14)
B
2exp
‹

c˝
min
Ÿ
N
t
‹
1

ˆ
‹
A
“
2
T
“
;N
t
‹
1

ˆ
‹
A
“
T
“š“
(D.15)

2exp
‹

c˝N
t
‹
1

ˆ
‹
A
“
T
““
;
(D.16)
whichisthedesiredresultafter
1

˝

˝
substitutionandusingtheinitialassumptionof
˝
C
2
.
E
ProofofTheorem
C.1
Thisproofisaslightmodi˝cationoftheproofofTheorem
4
:
1
ofKrahmeretal.[
18
]andwewilldirectly
borrowtheirnotationandestimates.First,werestatetheirTheorem
3
:
1
.
TheoremE.1
Let
A
beasetofmatricesandlet
˘
bearandomvectorwhoseentries
˘
j
arestandardnormal.
Let
d
F
;d
2

2
betheFrobeniusandspectralnormdistancemetricsrespectively.Set
E


2
‹
A
;
Y

Y“‹

2
‹
A
;
Y

Y“

d
F
‹
A
““

d
F
‹
A
“
d
2

2
‹
A
“
;
(E.1)
V

d
2

2
‹
A
“‹

2
‹
A
;
Y

Y“

d
F
‹
A
““
;
and
U

d
2
2

2
‹
A
“
:
(E.2)
Then,forsomeabsoluteconstants
c
1
;c
2
A
0
andforall
t
A
0
,
P
‹
sup
A
>
A
SY
A
˘
Y
2
`
2

E
Y
A
˘
Y
2
`
2
S
C
c
1
E

t
“
B
exp
‹

c
2
min
Ÿ
t
2
V
2
;
t
U
š“
:
Theorem
E
isavariationofTheorem
4
:
1
of[
18
].InlightofTheorem
E.1
,wesimplyneedtoadaptthe
estimatesdevelopedduringtheproofofTheorem
4
:
1
of[
18
]forourpurposes.Weareinterestedina˝xed
submatrixofsize
m

s
comparedtoall
s
-columnsubmatricesfor˝xed
m
-rows.Thismakesourset
A
a
subsetoftheirsetandalsomakestheirestimatesanupperboundonourestimates.Followingarguments
of[
18
],forsomeconstant
c
3
A
0
,wehave
d
F
‹
A
“

1
;d
2

2
‹
A
“
B
»
s
~
m;
2
‹
A
;
Y

Y“
B
c
3
»
s
~
m
log
‹
2
s
“
log
‹
2
d
“
:
Toproceed,wewillapplyTheorem
E.1
.Thiswillbedoneintwoscenariosdependingonwhetherisometry
constantobeys

B
1
ornot.Recallthat
m
0

c
0
s
log
2
‹
2
s
“
log
2
‹
2
d
“
.Below,wepick
c
0
su˚cientlylargeto
compensatefor
c
1
;c
2
;c
3
.
28
m
C
m
0
case:
Wehavethat

2
‹
A
;
Y

Y“
B
c
3
»
s
~
m
log
‹
2
s
“
log
‹
2
d
“
B
1
sothat
E
B
»
s
~
m

2

2
‹
A
;
Y

Y“
B
3
c
3
»
s
~
m
log
‹
2
s
“
log
‹
2
d
“
.Similarly,
V
B
2
»
s
~
m
and
U
B
s
~
m
.Inthiscase,pickinglarge
c
0
,observethat
c
1
E
B
»
m
0
~‹
4
m
“
.Withthis,wecanpick
t

»
m
0
~‹
4
m
“
toguarantee
c
1
E

t
B
»
m
0
~
m
.Wehavethat
t
2
~
V
2
C
m
0
~‹
16
s
“
;t
~
U
C
t
2
~
U
C
m
0
~‹
4
s
“
.Picking
c
0
C
16
~
c
2
,weconcludewiththedesiredprobability
exp
‹

log
2
‹
2
d
“
log
2
‹
2
s
““
.
m
@
m
0
case:
Inthiscase,wehave

2
‹
A
;
Y

Y“

d
F
‹
A
“
B
c
œ
»
s
~
m
log
‹
2
s
“
log
‹
2
d
“
;
where
c
œ

c
3

º
c
0
.Hence,we˝nd
E
B
c
œ
c
3
‹
s
~
m
“
log
‹
2
s
“
2
log
‹
2
d
“
2

»
s
~
m:
Observethat,wecanensurei)
c
1
»
s
~
m
B
»
m
0
~
m
~
4
B
m
0
~‹
4
m
“
andii)
c
1
c
œ
c
3
‹
s
~
m
“
log
‹
2
s
“
2
log
‹
2
d
“
2
B
m
0
~‹
4
m
“
forsu˚cientlylargeconstant
c
0
.Thelatteronefollowsfromthefactthat
c
œ
growsproportionalto
º
c
0
whereas
m
0
growsproportionalto
c
0
.Withthis,wecanpick
t

m
0
~‹
2
m
“
whichguarantees
c
1
E

t
B
m
0
m
.
To˝ndtheprobability,weagainpick
c
0
tobesu˚cientlylargetoguaranteethati)
c
2
t
~
U
C
c
2
‹
m
0
~‹
2
m
““~‹
s
~
m
“
C
log
2
‹
2
s
“
log
2
‹
2
d
“
andii)
t
2
~
V
2
C
‹
m
0
~
m
“
2
4
‹
s
~
m
“‹
c
œ
»
s
~
m
log
‹
2
s
“
log
‹
2
d
““
2

c
2
0
s
2
log
4
‹
2
s
“
log
4
‹
2
d
“
4
s
2
‹
c
œ
“
2
log
2
‹
2
s
“
log
2
‹
2
d
“

c
2
0
log
2
‹
2
s
“
log
2
‹
2
d
“
4
‹
c
3

º
c
0
“
2
C
log
2
‹
2
s
“
log
2
‹
2
d
“~
c
2
;
whichconcludestheproofbyyielding
exp
‹

log
2
‹
2
d
“
log
2
‹
2
s
““
probabilityofsuccess.
29
"
79,Multilevel Artificial Neural Network Training for Spatially Correlated Learning,https://arxiv.org/pdf/1806.05703v3.pdf,https://github.com/scottcb/MsANN,
80,IVUS-Net: An Intravascular Ultrasound Segmentation Network,http://arxiv.org/pdf/1806.03583v2.pdf,https://github.com/Kulbear/ivus-segmentation-icsm2018,"IVUS-Net:AnIntravascularUltrasoundSegmentationNetwork
JiYang,LinTong,MehdiFaraji

,AnupBasu
DepartmentofComputingScience,
UniversityofAlberta,Canada
f
jyang7,ltong2,faraji,basu
g
@ualberta.ca
Abstract
I
ntra
V
ascular
U
ltra
S
ound(IVUS)isoneofthemostef-
fectiveimagingmodalitiesthatprovidesassistancetoex-
pertsinordertodiagnoseandtreatcardiovasculardis-
eases.WeaddressacentralprobleminIVUSimageanal-
ysiswithFullyConvolutionalNetwork(FCN):automati-
callydelineatethelumenandmedia-adventitiabordersin
IVUSimages,whichiscrucialtoshortenthediagnosispro-
cessorafasterandmoreaccurate3Dreconstruc-
tionoftheartery.Particularly,weproposeanFCNar-
chitecture,calledIVUS-Net,followedbyapost-processing
contourextractionstep,inordertoautomaticallysegments
theinterior(lumen)andexterior(media-adventitia)regions
ofthehumanarteries.WeevaluatedourIVUS-Neton
thetestsetofastandardpubliclyavailabledatasetcon-
taining326IVUSB-modeimageswithtwomeasurements,
namelyJaccardMeasure(JM)andHausdorffDistances
(HD).TheevaluationresultshowsthatIVUS-Netoutper-
formsthestate-of-the-artlumenandmediasegmentation
methodsby4%to20%intermsofHDdistance.IVUS-
Netperformswellonimagesinthetestsetthatcontaina
amountofmajorartifactssuchasbifurcations,
shadows,andsidebranchesthatarenotcommoninthe
trainingset.Furthermore,usingamodernGPU,IVUS-Net
segmentseachIVUSframeonlyin0.15seconds.Thepro-
posedwork,tothebestofourknowledge,isthestdeep
learningbasedmethodforsegmentationofboththelumen
andthemediavesselwallsin20MHzIVUSB-modeim-
agesthatachievesthebestresultswithoutanymanualinter-
vention.Codeisavailableat
https://github.com/
Kulbear/ivus-segmentation-icsm2018
.
Keywords
IntravascularSegmentationUltrasoundIVUS
DeepLearning

Correspondingauthor.
1.Introduction
ConvolutionalNeuralNetworks(CNNs)playanimpor-
tantroleinvisualimagerecognition.Inthepastfewyears,
CNNshaveachievedpromisingresultsinimageclassi-
[15,22,23,12,13]andsemanticsegmentation
[5,16,4,20,19].FullyConvolutionalNetworks(FCNs)
[16]havebecomepopularandusedtosolvetheproblemof
makingdensepredictionsatapixellevel.Therearetwoma-
jordifferencesbetweenFCNandthetypeofCNNswhich
areprimarilydesignedfor[15,22,24].First,
FCNdoesnothavefully-connectedlayersthereforecanac-
ceptanyarbitrarysizeofinputs.Secondly,FCNconsistsof
anencodernetworkthatproducesembeddedfeaturemaps
thatarefollowedbyadecodernetworktoexpandand
thefeaturemapsoutputtedbytheencoder.Skipconnections
arealsocommoninthearchitecturetoconnectcorrespond-
ingblocksintheencoderanddecoder[2,7,4].
SegmentationoftheacquiredIVUSimagesisachalleng-
ingtasksinceIVUSimagesusuallycomeswithartifacts.
Particularly,asuccessfulseparationoftheinterior(lumen)
andexterior(media)vesselwallsinIVUSimagesplaysa
criticalroletodiagnosecardiovasculardiseases.Italso
helpsbuildingthe3Dreconstructionofthearterywhere
theinformationofthecathetermovementshasbeenpro-
videdusinganotherimagingmodalitysuchasX-Ray.The
segmentationofIVUSimageshasbeenawell-investigated
problemfromaconventionalperspectivewherenumerous
ideasandapproachesofcomputervisionandimagepro-
cessingsuchasin[17,18,25,30,26]havebeenemployed.
Oneofthebestsegmentationresultshavebeenachieved
inaveryrecentwork[8]whereauthorsproposedatwo-
foldIVUSsegmentationpipelinebasedontraditionalcom-
putervisionmethods[10,9].Althoughnolearningmethod
wasused,itoutperformsexistingmethodsfromboththe
accuracyandefyperspective.Althoughthereported
performanceof[8]isveryclosetothegroundtruthlabel
(0.30mmerrorofthesegmentedlumenand0.22mmerror
ofthesegmentedmediafromthegoldstandard),webe-
lievethatthedeeplearningtechniquehasthepotentialto
performbetter.Inthispaper,weproposeanFCN-based
1
arXiv:1806.03583v2  [stat.ML]  14 Jun 2018Figure1:TheIVUS-Netarchitecture.Everyconvolutionallayerinthesameblockhasthesameoutputdepthaslabeledon
thetopoftheblock.Theshouldbereadfromlefttorightasweomittedarrowheadstosavethespace.
pipelinethatautomaticallydelineatestheboundaryofthe
lumenandthemediavesselwalls.Thepipelinecontains
twomajorcomponents:acarefullydesignedFCNforpre-
dictingapixel-wisemaskwhichiscalledIVUS-Net,fol-
lowedbyacontourextractionpost-processingstep.Inad-
dition,theFCNistrainedfromscratchwithoutrelyingon
anypre-trainedweights.WeevaluatedtheproposedIVUS-
NetonthetestsetofapubliclyavailableIVUSB-mode
benchmarkdataset[3]whichcontains32620MHzIVUS
imagesconsistofvariousartifactssuchasmotionofthe
catheterafteraheartcontraction,guidewireeffects,bifur-
cationandside-branches.Twostandardmetrics,namely,
JaccardMeasure(JM),alternativelycalledIntersectionover
Union(IoU),andHausdorffDistance(HD)wereusedfor
evaluation.
Thecontributionsoftheproposedworkcanbesumma-
rizedasfollows:

WeproposeapipelinebasedonaFCNfollowedby
apost-processingcontourextractiontoautomatically
delineatethelumenandmediavesselwalls.

Weshowthattheproposedworkoutperformsthecur-
rentstate-of-the-artstudiesoverapubliclyavailable
IVUSbenchmarkdataset[3]whichcontainsIVUSim-
ageswithaamountofartifacts.Thisshows
thattheproposedworkhasthepotentialtobegeneral-
izedtootherIVUSbenchmarksaswell.

Tothebestofourknowledge,thereisnopreviouswork
basedondeeparchitecturethatcanproducesegmenta-
tionforboththelumenandmediavesselwallsinB-
modeIVUSimages.
Therestofthepaperisorganizedasthefollows:Sec-
tion2containsadetaileddescriptionofourproposedwork.
InSection3,wedemonstratemultipleexperimentsthatre-
inforceourcontribution.Finally,weconcludethework
inSection4.
2.ProposedMethod
Inthissection,weintroducethedatasetweusedto
trainthedeepmodel.Then,wepresentthearchitecture,
IVUS-Net,thatproducesbinarypredictionmaskforeither
thelumenormediaarea,followedbyacontourextraction
steptodelineatethevesselwall.
2.1.Dataset
WeusedapubliclyavailableIVUSdataset[3]thatcon-
tainstwosets(trainandtest)ofIVUSgatedframesusing
afullpullbackattheend-diastoliccardiacphasefrom10
patients.Eachframehasbeenmanuallyannotatedbyfour
clinicalexperts.Thetrainandtestsetsconsistof109and
326IVUSframes,respectively.Also,testsetcontainsa
largenumberofIVUSartifactsincludingbifurcation(44
frames),sidevessel(93frames),andshadow(96frame)
artifacts.Theremaining143framesdonotcontainanyar-
tifactsexceptforplaque.
2.2.
IVUS-Netisdesignedbasedonfullyconvolutionalnet-
work(FCN)[16],withinspirationsfromaggregated,multi-
brancharchitecturessuchasResNeXT[27]andtheIncep-
tionmodel[24].BothSegNet[2]andU-Net[21]canbe
consideredasabaseversionofourproposedworkaccord-
ingtothenetworkarchitecturedesign.Ithastwomajor
components:
1.
Anencodernetworkthatcandownsampleandprocess
theinputtoproducealow-resolutiondeepfeaturemap.
2
(a)(b)
Figure2:Adetailedillustrationoftheencodingblockandthedecodingblock.Notetheencodingblockdoesnothave
thedownsamplingbranch,thereforethemainbranchandbranchwilldirectlyaccepttherawimageastheinput.(a)
Anencodingblockwithdownsamplingbranch,followedbythemainbranchandthebranch.(b)Atypicaldecoding
blockthatacceptfeaturemapfromboththepreviousblockandtheskip-connection.
2.
Adecodernetworkthatcanrestoretheresolutionof
thedeepfeaturemapoutputtedbytheencodernetwork
towardsoriginalsize.
Theoutputfeaturemapissenttoonemoreconvolutional
layerfollowedbyasigmoidactivationtoproducethe
result.
Theencodernetworkcontains4encodingblockswhere
thedecodernetworkcontains3decodingblocks.Eachde-
codingblockreceivesfeaturemapfromitspreviousblock
andextrainformationfromtheencodernetworkbyskip-
connections.Theentirearchitectureisthereforesymmetric
asshowninFig.1.Thereareminordifferencesamongthe
blocksinthearchitecture.Wegiveabriefillustrationfor
thedesignandalsoshowtheintuitionsbehind.
Exceptfortheencodingblock,eachencodingblock
containsadownsamplingbranchthatdownsamplesthein-
putfeaturemap,thenfollowedbyatwo-branchconvolution
path,asshowninFig.2(a).Webuildandexpanddownsam-
plingbranchesinordertoavoidlosinginformationdueto
usingthepooling.Infact,thedownsamplingbranchfacili-
tatesreducingthespatialresolutionoftheinput.Itemploys
a2-by-2averagepoolinglayeranda2-by-2convolutional
layerwithastrideof2atthesametime,andcon-
catenatethetwooutputstogether,thisaggregationideais
similarto[24,27].
Afterthedownsampled,aggregatedfeaturemapoutputs
bythedownsamplingbranchispassedtotwosubsequent
branches,namelythebranchandthemainbranch.
First,wefollowthedesignin[2,21]toincludeabranch
withconsecutiveconvolutionallayersfollowedbyactiva-
tionandbatchnormalization,herewecallitﬁmainbranchﬂ.
Arecenttrendistousesmallkernelsizeforthefeature
map[4,19].Soweintentionallydesigna
ingbranchﬂthathasoneconvolutionallayerwitha3-by-3
kernelsizefollowedbyaconvolutionallayerwitha1-by-1
kernelsizeproducessimilarbutfeaturemap.The
outputsfromthemainbranchandwillbesummed
upandpasstothenextblockanditscorrespondingdecod-
ingblock.
Decodingblocksneedaslightlydifferent
asshowninFig.2(b).Everydecodingblockreceivesthe
featuremapfrombothitspreviousblockanditscorrespond-
ingencodingblock.Onlythefeaturemapreceivedfromthe
previousblockisupsampledbya2-by-2deconvolutionand
thenconcatenatedwiththefeaturemapfromitscorrespond-
ingencodingblock.Notethatthisconcatenatedfeaturemap
willonlybepassedtothemainbranch,wherethe
branchhandlestheupsampledfeaturemaponly.
TheactivationusedintheIVUS-Netisthe
P
arametric
Re

L
inear
U
nit(PReLU)[11].
PReLU
(
x
)=max(0
;x
)


max(0
;

x
)
(1)
ComparedwiththeordinaryReLUactivation,PReLUal-
lowsapartofthegradientswthroughwhentheneuron
isnotactivated,whereReLUonlypassesgradientswhen
theneuronisactive.Assuggestedin[11,28],PReLUout-
performsReLUinmanybenchmarksandalsohasamore
stableperformance.
3
Finally,theoutputfeaturemapfromthelastdecoding
blockisbya5-by-5convolutionallayer,whichis
experimentallyprovedtobehelpfulonimprovingperfor-
mance.AswewantIVUS-Nettoproducebinarymasks,
thelastactivationisasigmoidfunction.
2.3.Pocessing
Generally,asithasbeenproposedin[8],sincetheshape
ofthelumenandmediaregionsofthevesselareverysim-
ilartoconicsections,representingthepredictedmasksby
anellipseonthemaskscanincreasetheaccuracyof
thesegmentations.Therefore,wefollowthesameprocess
explainedin[8]topost-processthepredictedmasksinorder
toextractthecontours.
3.Experiments
TheevaluationisbasedonapubliclyavailableIVUSB-
modedataset[3],whichhasbeenwidelyusedintheIVUS
segmentationliterature[8,6,17,30,26].Thereare109im-
agesinthetrainingset,326imagesinthetestsetandno
ofvalidationsetisprovided.Modelsaretrainedend-
to-end,basedononlythegivendatasetwithoutinvolving
anyotherexternalresourcessuchasextratrainingimages
andpre-trainedmodelweights.Twometricsareusedforthe
evaluation,namelyJaccardMeasure(JM)andHausdorff
Distance(HD).TheJaccardMeasure,sometimescalledIn-
tersectionoverUnion,iscalculatedbasedonthecompari-
sonoftheautomaticsegmentationfromthepipeline(
R
pred
)
andthemanualsegmentationdelineatedbyexperts(
R
true
).
JM
=
R
pred
\
R
true
R
pred
[
R
true
(2)
TheHausdorffDistancebetweentheautomatic(
C
pred
)and
manual(
C
true
)curvesistheﬁgreatestdistanceofallpoints
belongingto
C
pred
totheclosestpointﬂ[8]in
C
true
andis
asfollows:
HD
=max
f
d
(
C
pred
;C
true
)
;d
(
C
true
;C
pred
)
g
(3)
3.1.DataAugmentation
Thetrainingsetcontainsonly109images,whichiscon-
sideredasarelativelysmalltrainingsetfortrainingadeep
modelfromscratch.Wethenemploydataaugmentation
onalltheavailabletrainingimages.Theaugmentationis
twofold.First,everyoriginalIVUSimageanditscorre-
spondinggroundtruthmasksare(1)lefttoright,(2)
uptodown,and(3)lefttorightthenuptodown,togen-
eratethreenewimage-maskpairs.Secondly,weaddheavy
noisestoinputimages.Themethodsweusetoaddnoises
totheinputimageincludegivingadditiveGaussiannoise,
Table1:DataAugmentationandBranchValida-
tion.
(a)Dataaugmentationevaluationresult
LumenMedia
Jacc.Acc.
Jacc.Acc.
Aug.Data
0.8698.6
0.8496.8
Orig.Data
0.8397.6
0.7996.1
(b)branchevaluationresult
LumenMedia
Jacc.Acc.
Jacc.Acc.
W/RefPath
0.8698.6
0.8496.8
W/ORefPath
0.8497.9
0.8096.0
assuggestedby[29],orconvertingtheinputimagetoen-
tirelyblack.Noisdoneonthegroundtruth
masks.Theeffectivenessofdataaugmentationisdiscussed
inSection3.3.
3.2.TrainingtheModel
Allthemodelsaretrainedandevaluatedonacomputer
withaCorei7-8700Kprocessor,16GBofRAM,andaGTX
10808GBgraphicscard.Trainingamodelfromscratch
generallytakeslessthan2hourstocomplete.Tomake
thetrainingfasterandusearelativelylargebatchsize,we
downsizedeveryframeofthedatasetbyafactorof0.5.
WeimplementIVUS-NetwithTensorFlow[1].The
weightsinthemodelareallinitializedrandomly.Thenwe
trainthemodelwithAdamoptimizer[14].Thelearningrate
issettobe0.0001withnodecayscheme.Theaugmented
trainingsetisusedtotraineachmodelfor96epochs,with
abatchsizeof6and144iterationsintotalforeachepoch.
Notethatweneedtwogroupsofmodelstopredictthelu-
menareaandthemediaareasincetheoutputactivationisa
sigmoidfunction:
˙
(
x
)=
1
1+
e

x
(4)
Fortrainingeachmodel,werandomlyselect10original
IVUSimagesasthevalidationsettomonitortheaverage
JaccardMeasurewithoutextractingcontours.Thegiven
predictionbyasinglemodelisaprobabilitymapthathas
equaldimensionstotheinputimagesize.Wefollowthe
ensemblepracticein[5]toproducetheresult.
3.3.TheEffectivenessofDataAugmentation
Wevalidatetheeffectivenessofdataaugmentationwith
asmallexperiment.Ineachcase,5modelswithidentical
4
Table2:PerformanceoftheproposedIVUS-Netwithcontourextraction.Measuresrepresentthemeanandstandarddeviation
evaluatedon326framesofthedataset[3]andcategorizedbasedonthepresenceofaartifactineachframe.The
evaluationmeasuresareJaccardMeasure(JM)andHausdorffDistance(HD).
LumenMedia
JMHD
JMHD
AllProposed
0.90(0.06)0.26(0.25)
0.86(0.11)0.48(0.44)
Farajietal.[8]
0.87(0.06)0.30(0.20)
0.77(0.17)0.67(0.54)
Downeetal.[6]
0.77(0.09)0.47(0.22)
0.74(0.17)0.76(0.48)
Exarchosetal.[3]
0.81(0.09)0.42(0.22)
0.79(0.11)0.60(0.28)
NoArtifactProposed
0.91(0.03)0.21(0.09)
0.92(0.05)0.27(0.23)
Farajietal.[8]
0.88(0.05)0.29(0.17)
0.89(0.07)0.31(0.23)
BifurcationProposed
0.82(0.11)
0.50(0.58)
0.78(0.11)
0.82(0.60)
Farajietal.[8]
0.79(0.10)0.53(0.34)
0.57(0.13)1.22(0.45)
Downeetal.[6]
0.70(0.11)0.64(0.27)
0.71(0.19)0.79(0.53)
Exarchosetal.[3]
0.80(0.09)
0.47(0.23)
0.78(0.11)
0.63(0.25)
SideVesselsProposed
0.90(0.04)0.23(0.12)
0.83(0.14)0.59(0.49)
Farajietal.[8]
0.87(0.05)0.24(0.11)
0.73(0.60)0.74(0.18)
Downeetal.[6]
0.77(0.08)0.46(0.19)
0.74(0.16)0.76(0.47)
Exarchosetal.[3]
0.77(0.09)0.53(0.24)
0.78(0.12)0.63(0.31)
ShadowProposed
0.87(0.06)0.27(0.25)
0.76(0.12)0.80(0.45)
Farajietal.[8]
0.86(0.07)0.29(0.20)
0.58(0.13)1.24(0.39)
Downeetal.[6]
0.76(0.11)0.55(0.26)
0.74(0.16)0.77(0.48)
Exarchosetal.[3]
0.80(0.10)0.46(0.19)
0.82(0.11)0.57(0.28)
aretrainedandweusetheensemblestrat-
egyillustratedin[5]toproducetheprediction.The
resultisshowninTable1a.Notethatthisresultisbasedon
thepredictionsproduceddirectlybytheensemblingwith-
outcontourextraction.Nomatterwhichtypeofvesselseg-
mentationthemodelpredicts,wecansafelyconcludethat
theaugmentationhelpstoimprovethesegmentationperfor-
mance.
3.4.OnEvaluatingtheBranch
Doesbranchreallyhelp?Weusetheexactsame
totraintwogroupsof5models.Onegroup
includestheproposedmodel,anothergroupistheproposed
modelwithoutthebranch.Theevaluationproce-
duresandmetricsareassameaswedidforthedataaug-
mentationevaluation,theresultisshowninTable1b.There
are,indeed,improvementsmadebythebranch.
3.5.SegmentationResults
Inthissection,wepresentanddiscussexperimentalre-
sultsontheIVUSdataset[3].Wetrain10modelswiththe
mentionedinSection3.2andensemblethe
predictionsfollowedbycontourextractiontoproducethe
predictionmask.
ThequantitativeresultisshowninTable2.Aswecan
see,IVUS-Netoutperformsexistingmethodsbya
cantmargin.AccordingtotheJaccardMeasure,weachieve
4%and8%improvementforthelumenandthemedia,re-
spectively.IfwelookattheHausdorffdistance,IVUS-Net
obtains8%and20%improvementforthelumenandthe
media,respectively.
IVUS-Netperformsparticularlywellonimageswithno
artifact.Furthermore,itimprovestheperformancebya
largemarginforsegmentingboththelumenandtheme-
diaaccordingtotheHausdorffdistance.Thereasonwhy
IVUS-Netdoesnotexceedallthemethodsineverysingle
categoriesof[3]canbeaddressedfromtwoperspectives.
First,thetrainingsetistoosmalltocaptureallthecommon
artifactsintherealworldandeventhetestset.Butthearchi-
tectureisstillconsiderablyeffectiveasthetrainingsetcon-
tainsonly1imagewithsidevesselsartifactwhilethetest
setcontains93frameswithsidevesselartifacts.Secondly,
theshadowartifactsaregenerallyoverlappedwithpartsof
themediaareathatmakesthesegmentationbecomesmuch
morechallengingsincethemediaregionsleaktotheback-
ground.SomepredictionsareillustratedinFig.3.
4.Conclusion
Inthispaper,weproposedIVUS-Netforthesegmen-
tationofarterialwallsinIVUSimagesaswellasacon-
5
Figure3:Lumenandmediasegmentationresults.Segmentedlumenandmediahavebeenhighlightedbycyanandredcolors,
respectively.Theyellowdashedlinesillustratethegoldstandardthathavebeendelineatedbyfourclinicalexperts[3].
tourextractionpost-processingstepthatfor
theIVUSsegmentationtask.WeshowedthatIVUS-Net
canoutperformtheexistingconventionalmethodsondelin-
eatingthelumenandmediavesselwalls.Thisisalsothe
deeparchitecture-basedworkthatachievessegmenta-
tionresultsthatareveryclosetothegoldstandard.Weeval-
uatedIVUS-Netonapubliclyavailabledatasetcontaining
326IVUSframes.Theresultsofourevaluationshowedthe
superiorityofIVUS-Netoutputsegmentationsoverthecur-
rentstate-of-the-arts.Also,IVUS-Netcanbeemployedin
real-worldapplicationssinceitonlyneeds0.15secondto
segmentanyIVUSframe.
Acknowledgment
TheauthorswouldliketothankthePhDstudentsinthe
MultimediaResearchCentreatUniversityofAlberta.Spe-
cialthankstoXinyaoSunforthediscussionsontherelated
workandthenetworkarchitecturedesign.
References
[1]
M.Abadi,A.Agarwal,P.Barham,E.Brevdo,Z.Chen,
C.Citro,G.S.Corrado,A.Davis,J.Dean,M.Devin,S.Ghe-
mawat,I.Goodfellow,A.Harp,G.Irving,M.Isard,Y.Jia,
R.Jozefowicz,L.Kaiser,M.Kudlur,J.Levenberg,D.Man
´
e,
R.Monga,S.Moore,D.Murray,C.Olah,M.Schuster,
J.Shlens,B.Steiner,I.Sutskever,K.Talwar,P.Tucker,
V.Vanhoucke,V.Vasudevan,F.Vi
´
egas,O.Vinyals,P.War-
den,M.Wattenberg,M.Wicke,Y.Yu,andX.Zheng.Tensor-
Flow:Large-scalemachinelearningonheterogeneoussys-
tems,2015.Softwareavailablefromw.org.
[2]
V.Badrinarayanan,A.Kendall,andR.Cipolla.Segnet:A
deepconvolutionalencoder-decoderarchitectureforimage
segmentation.
IEEEtransactionsonpatternanalysisand
machineintelligence
,39(12):2481Œ2495,2017.
[3]
S.Balocco,C.Gatta,F.Ciompi,A.Wahle,P.Radeva,S.Car-
lier,G.Unal,E.Sanidas,J.Mauri,X.Carillo,etal.Stan-
dardizedevaluationmethodologyandreferencedatabasefor
evaluatingivusimagesegmentation.
Computerizedmedical
imagingandgraphics
,38(2):70Œ90,2014.
[4]
L.-C.Chen,G.Papandreou,I.Kokkinos,K.Murphy,and
A.L.Yuille.Deeplab:Semanticimagesegmentationwith
deepconvolutionalnets,atrousconvolution,andfullycon-
nectedcrfs.
arXivpreprintarXiv:1606.00915
,2016.
[5]
D.Ciresan,A.Giusti,L.M.Gambardella,andJ.Schmidhu-
ber.Deepneuralnetworkssegmentneuronalmembranesin
electronmicroscopyimages.In
Advancesinneuralinforma-
tionprocessingsystems
,pages2843Œ2851,2012.
[6]
R.Downe,A.Wahle,T.Kovarnik,H.Skalicka,J.Lopez,
J.Horak,andM.Sonka.Segmentationofintravascularultra-
soundimagesusinggraphsearchandanovelcostfunction.
In
Proc.2ndMICCAIworkshoponcomputervisionforin-
travascularandintracardiacimaging
,pages71Œ9.Citeseer,
2008.
[7]
M.Drozdzal,E.Vorontsov,G.Chartrand,S.Kadoury,and
C.Pal.Theimportanceofskipconnectionsinbiomedical
imagesegmentation.In
DeepLearningandDataLabeling
forMedicalApplications
,pages179Œ187.Springer,2016.
[8]
M.Faraji,I.Cheng,I.Naudin,andA.Basu.Segmentationof
arterialwallsinintravascularultrasoundcross-sectionalim-
agesusingextremalregionselection.
Ultrasonics
,84:356Œ
365,2018.
[9]
M.Faraji,J.Shanbehzadeh,K.Nasrollahi,andT.B.Moes-
lund.Erel:extremalregionsofextremumlevels.In
Image
6
Processing(ICIP),2015IEEEInternationalConferenceon
,
pages681Œ685.IEEE,2015.
[10]
M.Faraji,J.Shanbehzadeh,K.Nasrollahi,andT.B.Moes-
lund.Extremalregionsdetectionguidedbymaximaofgra-
dientmagnitude.
IEEETransactionsonImageProcessing
,
24(12):5401Œ5415,2015.
[11]
K.He,X.Zhang,S.Ren,andJ.Sun.Delvingdeepinto
Surpassinghuman-levelperformanceonimagenet
In
ProceedingsoftheIEEEInternationalCon-
ferenceonComputerVision
,pages1026Œ1034,2015.
[12]
K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearn-
ingforimagerecognition.In
ProceedingsoftheIEEECon-
ferenceonComputerVisionandPatternRecognition
,pages
770Œ778.IEEEComputerSociety,2016.
[13]
G.Huang,Z.Liu,L.vanderMaaten,andK.Q.Weinberger.
Denselyconnectedconvolutionalnetworks.In
Proceedings
oftheIEEEConferenceonComputerVisionandPattern
Recognition
,2017.
[14]
D.P.KingmaandJ.Ba.Adam:Amethodforstochastic
optimization.
arXivpreprintarXiv:1412.6980
,2014.
[15]
A.Krizhevsky,I.Sutskever,andG.E.Hinton.Imagenet
withdeepconvolutionalneuralnetworks.In
Advancesinneuralinformationprocessingsystems
,pages
1097Œ1105,2012.
[16]
J.Long,E.Shelhamer,andT.Darrell.Fullyconvolutional
networksforsemanticsegmentation.In
Proceedingsofthe
IEEEconferenceoncomputervisionandpatternrecogni-
tion
,pages3431Œ3440,2015.
[17]
E.G.Mendizabal-Ruiz,M.Rivera,andI.A.Kakadiaris.
Segmentationoftheluminalborderinintravascularultra-
soundb-modeimagesusingaprobabilisticapproach.
Medi-
calimageanalysis
,17(6):649Œ670,2013.
[18]
G.Mendizabal-RuizandI.A.Kakadiaris.Aphysics-based
intravascularultrasoundimagereconstructionmethodforlu-
mensegmentation.
Computersinbiologyandmedicine
,
75:19Œ29,2016.
[19]
C.Peng,X.Zhang,G.Yu,G.Luo,andJ.Sun.Largekernel
mattersŒimprovesemanticsegmentationbyglobalconvolu-
tionalnetwork.
arXivpreprintarXiv:1703.02719
,2017.
[20]
P.Rajpurkar,J.Irvin,K.Zhu,B.Yang,H.Mehta,T.Duan,
D.Ding,A.Bagul,C.Langlotz,K.Shpanskaya,etal.
Chexnet:Radiologist-levelpneumoniadetectiononchestx-
rayswithdeeplearning.
arXivpreprintarXiv:1711.05225
,
2017.
[21]
O.Ronneberger,P.Fischer,andT.Brox.U-net:Convo-
lutionalnetworksforbiomedicalimagesegmentation.In
InternationalConferenceonMedicalimagecomputingand
computer-assistedintervention
,pages234Œ241.Springer,
2015.
[22]
K.SimonyanandA.Zisserman.Verydeepconvolutional
networksforlarge-scaleimagerecognition.
International
ConferenceonLearningRepresentations(ICRL)
,pages1Œ
14,2015.
[23]
R.K.Srivastava,K.Greff,andJ.Schmidhuber.Highway
networks.
arXivpreprintarXiv:1505.00387
,2015.
[24]
C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,
D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich.
Goingdeeperwithconvolutions.In
ComputerVisionand
PatternRecognition(CVPR)
,2015.
[25]
A.Taki,Z.A.Roodaki,S.K.Setarehdan,R.A.
A.Konig,andN.Navab.Automaticsegmentationof
plaquesandvesselbordersinivusimages.
Interna-
tionalJournalofComputerAssistedRadiologyandSurgery
,
3(3-4):347Œ354,2008.
[26]
G.Unal,S.Bucher,S.Carlier,G.Slabaugh,T.Fang,and
K.Tanaka.Shape-drivensegmentationofthearterialwallin
intravascularultrasoundimages.
IEEETransactionsonIn-
formationTechnologyinBiomedicine
,12(3):335Œ347,2008.
[27]
S.Xie,R.Girshick,P.Doll
´
ar,Z.Tu,andK.He.Aggregated
residualtransformationsfordeepneuralnetworks.In
Com-
puterVisionandPatternRecognition(CVPR),2017IEEE
Conferenceon
,pages5987Œ5995.IEEE,2017.
[28]
B.Xu,N.Wang,T.Chen,andM.Li.Empiricalevaluationof
activationsinconvolutionalnetwork.
arXivpreprint
arXiv:1505.00853
,2015.
[29]
C.Zhang,S.Bengio,M.Hardt,B.Recht,andO.Vinyals.
Understandingdeeplearningrequiresrethinkinggeneraliza-
tion.In
InternationalConferenceonLearningRepresenta-
tions(ICLR)
,2017.
[30]
X.Zhu,P.Zhang,J.Shao,Y.Cheng,Y.Zhang,andJ.Bai.
Asnake-basedmethodforsegmentationofintravascularul-
trasoundimagesanditsinvivovalidation.
Ultrasonics
,
51(2):181Œ189,2011.
7
"
81,Learning Human Optical Flow,http://arxiv.org/pdf/1806.05666v2.pdf,https://github.com/anuragranj/humanflow,"RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
1
LearningHumanOpticalFlow
AnuragRanjan
1
aranjan@tuebingen.mpg.de
JavierRomero

;
2
javier@amazon.com
MichaelJ.Black
1
black@tuebingen.mpg.de
1
MPIforIntelligentSystems
Tübingen,Germany
2
AmazonInc.
Abstract
Theopticalwofhumansiswellknowntobeusefulfortheanalysisofhuman
action.Giventhis,wedeviseanopticalwalgorithmforhumanmotion
andshowthatitissuperiortogenericwmethods.Designingamethodbyhandisim-
practical,sowedevelopanewtrainingdatabaseofimagesequenceswithgroundtruth
opticalw.Forthisweusea3Dmodelofthehumanbodyandmotioncapturedata
tosynthesizerealisticwWethentrainaconvolutionalneuralnetworktoesti-
matehumanwfrompairsofimages.Sincemanyapplicationsinhumanmotion
analysisdependonspeed,andweanticipatemobileapplications,webaseourmethodon
SpyNetwithseveralWedemonstratethatourtrainednetworkismoreac-
curatethanawiderangeoftopmethodsonheld-outtestdataandthatitgeneralizeswell
torealimagesequences.Whencombinedwithapersondetector/tracker,theapproach
providesafullsolutiontotheproblemof2Dhumanwestimation.Boththecodeand
thedatasetareavailableforresearch.
1Introduction
AfractionofvideosontheInternetcontainpeoplemoving[
18
]andtheliterature
suggeststhatopticalwplaysanimportantroleinunderstandinghumanaction[
25
,
39
].
Severalactionrecognitiondatasets[
28
,
39
]containhumanmotionasamajorcomponent.
The2Dmotionofhumansinvideo,or
human
,isanimportantfeaturethatprovidesa
buildingblockforsystemsthatcanunderstandandinteractwithhumans.Humanwis
usefulforvariousapplicationsincludinganalyzingpedestriansinroadsequences,motion-
controlledgaming,activityrecognition,humanposeestimationsystem,etc.
Despitethis,opticalwhaspreviouslybeentreatedasageneric,low-level,visionprob-
lem.Giventheimportanceofpeople,andthevalueofopticalwinunderstandingthem,
wedevelopawalgorithmthatistailoredtohumansandtheirmotion.Such
motionsarenon-trivialsincehumansarecomplex,articulated,objectsthatvaryinshape,
sizeandappearance.Theymovequickly,selfocclude,andadoptawiderangeofposes.
Ourgoalistoobtainmoreaccurate2Dmotionestimatesforhumanbodiesbytraining
awalgorithmforhumanmovement.Todoso,wecreatealargeandrealistic
datasetofhumansmovinginvirtualworldswithgroundtruthopticalw(Fig.
1
(a)).We
*ThisworkwasdonebyJRwhileatMPI.
c

2018.Thecopyrightofthisdocumentresideswithitsauthors.
Itmaybedistributedunchangedfreelyinprintorelectronicforms.
arXiv:1806.05666v2  [cs.CV]  22 Jul 20182
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
(a)Ourdataset(b)Resultsonsyntheticscenes(c)Resultsonrealworldscenes
Figure1:(a)Wesimulatehumanmotioninvirtualworldcreatinganextensivedatasetwith
images(toprow)andw(bottomrow);colorcodingfrom[
2
].(b)Wetrainanexisting
deepnetworkforhumanmotionestimationandshowthatitperformsbetterwhentrainedon
ourdatasetand(c)generalizestohumanmotionsinrealworldscenes.
trainaneuralnetworkbasedonSPyNet[
35
]usingthisdatasetandshowthatitoutperforms
stateoftheartopticalwonthetestsequencesofthisdataset(Fig.
1
(b)).Furthermorewe
showthatitgeneralizestorealvideosequences(Fig.
1
(c)).HerewealsoextendSPyNet,
makingitend-to-endtrainable.
Severaldatasetsandbenchmarks[
2
,
5
,
17
]havebeenestablishedtodrivetheprogress
inopticalw.Wearguethatthesedatasetsareinsufforthetaskofhumanmotion
estimationand,despiteitsimportancenoattentionhasbeenpaidtodatasetsandalgorithms
forhumanw.Oneofthemainreasonsisthatdensehumanmotionisextremelydif
tocaptureaccuratelyinrealscenes.Withoutgroundtruth,therehasbeenlittleworkfocused
onestimatinghumanopticalw.Toadvanceresearchonthisproblem,the
communityneedsadatasettailoredtohumanw.
Akeyobservationisthatrecentworkhasshownthatopticalwmethodstrainedon
syntheticdata[
9
,
22
,
35
]generalizerelativelywelltorealdata.Additionally,thesemethods
obtainstateoftheartresultswithincreasedrealismofthetrainingdata[
15
,
33
].This
motivatesourefforttocreateadatasetdesignedforhumanmotion.
Tothatend,weusetheSMPLbodymodel[
30
]togenerateaboutahundredthousand
differenthumanshapes.Wethenplacethemonrandomindoorbackgroundsandsimulate
humanactivitieslikerunning,walking,dancingetc.usingmotioncapturedata[
29
].Thus,
wecreatealargevirtualdatasetthatcapturesthestatisticsofnaturalhumanmotion.We
thentrainadeepneuralnetworkbasedonspatialpyramids[
35
]andevaluateitsperformance
forestimatinghumanmotion.Whilethedatasetcanbeusedtotrainanywmethod,we
chooseSpyNetbecauseitiscompactandcomputationallyef
Insummary,ourmajorcontributionsare:1)weprovidetheﬁHumanFlowdatasetﬂwith
146,020framepairsofhumanbodiesinmotionwithrealistictexturesandbackgrounds;2)
weshowthatournetworkoutperformspreviousopticalwmethodsby30%ontheHuman
Flowdataset,anditgeneralizestorealworldscenes;3)weextendSPyNettobefullyend-
to-endtrainable;4)ourneuralnetworkisverysmall(7.8MBforthenetworkparameters)
andrunsinrealtime(32fps),henceitcanbepotentiallyusedforembeddedapplications;5)
weprovidedata,code,andthetrainedmodel
1
forresearchpurposes.
1
http://github.com/anuragranj/humanflow
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
3
2RelatedWork
HumanMotion.
Humanmotioncanbeunderstoodfrom2Dmotion.Earlyworkfocusedon
themovementof2Djointlocations[
26
]orsimplemotionhistoryimages[
8
].Opticalw
isalsoausefulcue.Blacketal.[
3
]useprincipalcomponentanalysis(PCA)toparametrize
humanmotionbutusenoisywcomputedfromimagesequencesfortrainingdata.More
similartous,FabletandBlack[
10
]usea3Darticulatedbodymodelandmotioncapturedata
toproject3Dbodymotioninto2Dopticalw.Theythenlearnaview-basedPCAmodelof
thewWeuseamorerealisticbodymodeltogeneratealargedatasetandusethis
totrainaCNNtodirectlyestimatedensehumanwfromimages.
Onlyafewworksinposeestimationhaveexploitedhumanmotionand,inparticular
severalmethods[
13
,
44
]useopticalwconstraintstoimprove2Dhumanposeestimation
invideos.Similarwork[
6
,
34
]propagatesposeresultstemporallyusingopticalwto
encouragetimeconsistencyoftheestimatedbodies.Apartfromitsapplicationinwarping
betweenframes,thestructuralinformationexistinginopticalwhasbeenusedforpose
estimationalone[
38
]orinconjunctionwithanimagestream[
11
].
LearningOpticalFlow.
Thereisalonghistoryofopticalwestimation,whichwedo
notreviewhere.Instead,wefocusontherelativelyrecentliteratureonlearningw.Early
worklookedatlearningwusingMarkovRandomFields[
14
],PCA[
42
],orshallow
convolutionalmodels[
7
].Othermethodsalsocombinelearningwithtraditionalapproaches,
formulatingwasadiscrete[
20
]orcontinuous[
36
]optimizationproblem.
Themostrecentmethodsemploylargedatasetstoestimateopticalwusingdeepneural
networks.Voxel2Voxel[
40
]isbasedonvolumetricconvolutionstopredictopticalw
using16framessimultaneouslybutdoesnotpeformwellonbenchmarks.Othermethods
[
9
,
22
,
35
]computetwoframeopticalwusinganend-to-enddeeplearningapproach.
FlowNet[
9
]usestheFlyingChairsdataset[
9
]tocomputeopticalwinanendtoenddeep
network.FlowNet2.0[
22
]usesstacksofnetworksfromFlowNetandperforms
better,particularlyforsmallmotions.RanjanandBlack[
35
]proposeaSpatialPyramid
Networkthatemploysasmallneuralnetworkoneachlevelofanimagepyramidtocompute
opticalw.Theirmethodusesamuchsmallernumberofparametersandachievessimilar
performanceasFlowNet[
9
]usingthesametrainingdata.Sincetheabovemethodsarenot
trainedwithhumanmotions,theydonotperformwellonourHumanFlowdataset.
OpticalFlowDatasets.
Severaldatasetshavebeendevelopedtofacilitatetrainingand
benchmarkingofopticalwmethods.Middleburyislimitedtosmallmotions[
2
],KITTIis
focusedonrigidscenesandautomotivemotions[
17
],whileSintelhasalimitednumberof
syntheticscenes[
5
].Thesedatasetsaremainlyusedforevaluationofopticalwmethods
andaregenerallytoosmalltosupporttrainingneuralnetworks.
Tolearnopticalwusingneuralnetworks,moredatasetshaveemergedthatcontain
examplesontheorderoftensofthousandsofframes.TheFlyingChairs[
9
]datasetcontains
about22,000samplesofchairsmovingagainstrandombackgrounds.Althoughitisnot
veryrealisticordiverse,itprovidestrainingdataforneuralnetworks[
9
,
35
]thatachieve
reasonableresultsonopticalwbenchmarks.Evenmorerecentdatasets[
15
,
33
]foroptical
wareespeciallydesignedfortrainingdeepneuralnetworks.FlyingThings[
33
]contains
tensofthousandsofsamplesofrandom3Dobjectsinmotion.TheMonkaaandDriving
scenedatasets[
33
]containframesfromanimatedscenesandvirtualdrivingrespectively.
VirtualKITTI[
15
]usesgraphicstogeneratesceneslikethoseinKITTIandistwoordersof
magnitudelarger.Recentsyntheticdatasets[
16
]showthatsyntheticdatacantrainnetworks
thatgeneralizetorealscenes.
4
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
Figure2:PipelineforgeneratingtheRGBframesandgroundtruthopticalwforthe
HumanFlowdataset.
Forhumanbodies,theSURREALdataset[
41
]uses3Dhumanmeshesrenderedontop
ofimagestotrainnetworksfor2Dposeestimation,depthestimation,andbodypartsegmen-
tation.Whilenotfullyrealistic,theyshowthatthisdataissuftotrainmethodsthat
generalizetorealdata.Wegobeyondtheirworktoaddresstheproblemofopticalw.
3TheHumanFlowDataset
Ourapproachgeneratesarealisticdatasetofsynthetichumanmotionsbysimulatingthem
againstdifferentrealisticbackgrounds.AsshowninFigure
2
,weusetheSMPLmodel[
30
]
togenerateawidevarietyofdifferenthumanshapesandappearances.WeuseBlender
2
as
arenderingenginetogeneratesyntheticimageframesandopticalw.Intherestofthe
section,wedescribeeachcomponentofourpipelineshowninFigure
2
.
BodymodelandRenderingEngine.
Themainchallengeinthegenerationofrealistic
humanopticalwismodelingrealisticarticulatedmotions.AsshowninFigure
2
,we
usetheSMPLmodel[
30
],parameterizedbyposeandshapeparameterstochangethebody
postureandidentity.ThemodelalsocontainsaUVappearancemapthatallowsustochange
theskintone,facefeaturesandclothingtextureofthemodel.AkeycomponentofBlender
inthisprojectisits
Vectorpass
.Thisrenderpassistypicallyusedforproducingmotion
blur,anditproducesthemotioninimagespaceofeverypixel;i.e.groundtruthopticalw.
Wearemainlyinterestedintheresultofthispass,togetherwiththecolorrenderingofthe
texturedbodies.
BodyPoses.
Inordertoobtainavariedsetofposes,weusemotionsfromtheHu-
man3.6Mdataset[
23
].Human3.6Mcontainsvesubjectsfortraining(S1,S5,S6,S7,S8)
andtwofortesting(S9,S11).Eachsubjectperforms15actionstwice,resultingin1,559,985
framesfortrainingand550,727fortesting.Thesesequencesaresubsampledatarateof
16

,resultingin97,499trainingand34,420testingposesfromHuman3.6M.Theposedata
isthenconvertedintoSMPLbodymodelsusingMoSh[
29
].Welimiteachofourpose
sequencesto20frames.
Bodyshapes.
Tomaximizethevarietyofthedata,eachsequenceof20framesusesa
randombodyshapedrawnfromauniformdistributionofSMPLshapeparametersbounded
by
[

3
;
3
]
standarddeviationsforeachshapecoefaccordingtotheshapedistribution
inCAESAR[
37
].Usingaparametricdistributionensuresthateachsub-sequenceofframes
hasauniquebodyshape.AuniformdistributionhasmoreextremeshapesthantheGaussian
2
https://www.blender.org/
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
5
distributioninferredoriginallyfromCAESAR,whileavoidingunlikelyshapesbystrictly
boundingthecoef
SceneIllumination.
Opticalwestimationshouldberobusttodifferentsceneillumi-
nation.Inordertoachievethisinvariance,weilluminatethebodieswithSphericalHarmon-
icslighting[
19
].SphericalHarmonicsbasisvectorsforlightdirectionsthatarescaled
andlinearlycombined.Thiscompactparameterizationisparticularlyusefulforrandomizing
thescenelight.Thelinearcoefarerandomlysampledwithaslightbiastowardsnat-
uralillumination.Thecoefareuniformlysampledbetween

0
:
7and0
:
7,apartfrom
theambientillumination(whichisstrictlypositiveandaminimumof0
:
3)andthevertical
illumination(whichisstrictlynegativetopreventilluminationfrombelow).
Bodytexture.
Toprovideavariedsetofappearancestothebodiesinthescene,weuse
texturesfromtwodifferentsources.Awidevarietyofhumanskintonesisextractedfrom
theCAESARdataset[
37
].GivenSMPLregistrationstoCAESARscans,theoriginalper-
vertexcolorintheCAESARdatasetistransferredintotheSMPLtexturemap.Since
markerswereplacedonthebodiesofCAESARsubjects,weremovethemfromthetextures
andinpaintthemtoproduceanaturaltexture.ThemaindrawbackofCAESARscansistheir
homogeneityintermsofsinceallofthesubjectsworegreyshortsandthewomenwore
sportsbras.Inordertoincreasetheclothingvariety,wealsousetexturesextractedfrom3D
scans.Atotalof772texturesfrom7differentsubjectswithdifferentclotheswerecaptured.
AlltextureswereanonymizedbyreplacingthefacebytheaveragefaceinCAESAR,after
correctingittomatchtheskintoneofthetexture.Thedatasetswerepartitioned70%
j
30%
intotrainingandtesting,andeachtexturedatasetwassampledwitha50%chance.
Backgroundtexture.
Theothercrucialcomponentofimageappearanceistheback-
ground.Sincehumanmotionrarelyhappensinfrontofcleanandeasilysegmentablescenes,
realisticbackgroundsshouldbeincludedinthesyntheticscenes.Wefoundthatusingran-
domindoorimagesfromtheLSUNdataset[
43
]asbackgroundprovidedagoodcompromise
betweensimplicityandthecomplextaskofgeneratingvariedfull3Denvironments.Weused
417,597imagesfromLSUNcategorieskitchen,livingroom,bedroomanddiningroom.The
backgroundimageswereplacedasbillboards9metersfromthecamera,andwerenotaf-
fectedbythesphericalharmonicslighting.
Increasingimagerealism.
Oneofthemaindifferencesbetweensyntheticandrealim-
agesaretheimperfectionsexistinginthelatter.Fullysharpimagesrenderedwithperfectly
staticvirtualcamerasdonotrepresentwelltheimagescapturedinrealsituations.Inorder
toincreaserealism,weintroducedthreetypesofimagesimperfections.First,in30%ofthe
generatedimagesweintroducedcameramotionbetweenframes.Thismotionperturbsthe
locationofthecamerawithGaussiannoiseof1centimeterstandarddeviation,androtation
noiseof0
:
2degreesstandarddeviationperdimensioninanEuleranglerepresentation.Sec-
ond,motionblurwasaddedtothescene.Themotionblurwasimplementedwiththe
Vector
BlurNode
inBlender,andintegratedover2framessampledwith64stepsbetweenthebe-
ginningandendpointofthemotion.Finally,generalimageblurwasaddedto30%ofthe
images,asGaussianblurwithastandarddeviationof1pixel.
DatasetDetails.
Incomparisonwithotheropticalwdatasets,ourdatasetislarger
byanorderofmagnitude,containing135,153trainingframesand10,867testframeswith
opticalwgroundtruth.Wekeeptheresolutionsmallat256

256tofacilitateeasyde-
ploymentfortrainingneuralnetworks.WeshowthecomparisonsinTable
1
(a).Thisalso
speedsuptherenderingprocessinBlenderforgeneratinglargeamountsofdata.Ourdatais
extensive,containingawidevarietyofhumanshapes,poses,actionsandvirtualbackgrounds
tosupportdeeplearningsystems.
6
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
Figure3:ASpatialPyramidNetwork[
35
]forOpticalFlow.Ateachpyramidlevel,network
G
k
predictswresiduals
v
k
thatgetaddeduptoproducefullw
V
2
.
w
iswarpingoperator.
u
;
d
arelearnedconvolutionallayersthatupsamplewsanddownsampleimages.The
shows3pyramidlevelsforsimplicity.Ourimplementationuses4levels.
4Learning
OurneuralnetworkderivesfromSPyNet[
35
],whichemploysdifferentconvnetsatdifferent
levelsofanimagepyramid.InSPyNet,theseconvnetsaretrainedindependentlyandse-
quentiallytoestimateopticalw.Incontrast,weintroducelearnableconvolutionallayers
u
;
d
,forupsamplinganddownsamplingbetweenpyramidlevels.InSPyNet,theseareed
bilinearoperators.Adifferentialwarpingoperator,
w
[
24
]facilitatesourmodeltobefully
differentiableandend-to-endtrainable.Thus,weperformjointtrainingofalltheconvnets
atdifferentpyramidlevelsratherthantrainingthemsequentiallylikeSPyNet.Wede-
scribethespatialpyramidstructureandintroduceournetworkandlearningprocessbelow.
Ourarchitectureconsistsof4pyramidlevels.Forsimplicity,weshow3ofthe4pyramid
levelsinFigure
3
.Eachlevelworksonaparticularresolutionoftheimage.Thetoplevel
worksonthefullresolutionandtheimagesaredownsampledaswemovetothebottomof
thepyramid.Eachlevellearnsaconvolutionallayer
d
,toperformdownsamplingofimages.
Similarly,aconvolutionlayer
u
,islearnedforupsamplingopticalw.Ateachlevel,we
alsolearnaconvnet
G
k
topredictopticalwresiduals
v
k
atthatlevel.Thesewresiduals
getaddedateachleveltoproducethefullw,
V
K
atthelevelofthepyramid.
Eachconvnet
G
k
takesapairofimagesasinputsalongwithw
V
k

1
obtainedby
upsamplingtheoutputofthepreviouslevel.Thesecondframeishoweverwarpedusing
V
k

1
andthetriplet
f
I
1
k
;
w
(
I
2
k
;
V
k

1
)
;
V
k

1
g
isfedasinputtotheconvnet
G
k
.Thestructureofthe
convnets
G
k
issameasSPyNet.Eachoftheconvnets
G
k
isavelayernetworkcontaining
{32,64,32,16,2}featuremapswith7x7kernels.Ateachlevel,thedownsamplinglayers
d
learn3x3convolutionalkernelswith6featuremapstooperateon6channelsofimage
pairs.Similarly,theupsamplinglayers
u
learn4x4convolutionalkernelswith2feature
mapstooperateon2-channelws.Weuse
w
torefertoabilinearwarpingoperatorwhich
isnon-learnable.Thegeneralstructureofspatialpyramidscanbeseenin[
35
].Weimport
theweightsofourconvnets
G
i
fromthefourconvnets
f
G
0
;
G
1
;
G
2
;
G
3
g
ofSPyNetpre-
trainedontheFlyingChairsdataset[
9
].Weuseadifferentiablewarpingoperator,
w
[
24
].
Wenowconstructourfullydifferentiablespatialpyramidarchitectureandtrainitend-to-end
minimizinganEndPointError(EPE).
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
7
Hyperparameters.
WeuseAdam[
27
]tooptimizeourlossataconstantlearningrate
of10

6
,
b
1
=
0
:
9and
b
2
=
0
:
999.Weuseabatchsizeof8andrun4000iterationsper
epoch.Wetrainourmodelfor100epochsontheHumanFlowdataset.WeusetheTorch7
3
frameworkforourimplementationandusefourNvidiaK80GPUstotraininparallel.It
takes1dayforourmodeltotrain.
DataAugmentations.
Wealsoaugmentourdatabyapplyingseveraltransformations
andaddingnoise.Althoughourdatasetisquitelarge,augmentationimprovesthequalityof
resultsonrealscenes.Inparticular,weapplyscalingintherangeof
[
0
:
3
;
3
]
,androtations
in
[

17

;
17

]
.Werandomlycroptheimagesandwsto256

256atthelevel.We
sampleuniformGaussiannoise
N
(
0
;
1
)
andaddittotheimageswithaweightfactorof
1:10.Wealsoapplycolorjitterwithadditivebrightness,contrast,andsaturationchanges.
Thedatasetisnormalizedtohavezeromeanandunitstandarddeviationusing[
21
].
5Experiments
WecomparetheaverageEndPointErrors(EPEs)ofcompetingmethodsinTable
1
(b)along
withthetimeforevaluation.Humanmotioniscomplexandgeneralopticalwmethods
failtocaptureit.Ourtrainednetworkoutperformspreviousmethods,andSPyNet[
35
]
inparticular,intermsofaverageEPEontheHumanFlowDataset.Thisindicatesthat
otheropticalwnetworkscanalsoimprovetheirperformanceonhumanmotionusingour
dataset.WeshowvisualcomparisonsinFigure
4
.
OpticalwmethodsthatemploylearningusinglargedatasetssuchasFlowNetS[
9
]
showpoorgeneralizationonourdataset.SincetheresultsofFlowNetSareveryclosetothe
zerowbaseline(Table
1
),wecross-verifybyevaluatingFlowNetSonamixtureofFlying
Chairs[
9
]andHumanFlowdatasetandobservethatthewoutputsonhumanwdataset
isquiterandom(seeFigure
4
).Themainreasonisthatourdatasetcontainsa
amountofsmallmotionsanditisknownthatFlowNetSdoesnotperformverywellonsmall
motions.SPyNet[
35
]howeverperformsquitewellandisabletogeneralizetobodymotions.
Theresultshoweverlooknoisyinmanycases.
Ourdatasetemploysalayeredstructurewhereahumanisplacedagainstabackground.
AssuchlayeredmethodslikePCA-layers[
42
]performverywellonafewimages(row4in
Figure
4
)wheretheyareabletosegmentapersonfromthebackground.However,inmost
cases,theydonotobtaingoodsegmentationintolayers.
Previousstate-of-the-artmethodslikeLDOF[
4
],EpicFlow[
36
]andFlowFields[
1
]per-
formmuchbetterthanothers.Theygetagoodoverallshape,andsmoothbackgrounds.
However,theirestimationisquiteblurred.Theytendtomissthesharpedgesthataretypical
ofhumanhandsandlegs.Theyarealsoslower.
Incontrast,ournetworkoutperformsthestateoftheartopticalwmethodsby30%
ontheHumanFlowdataset.Ourqualitativeresultsshowthatourmethodcancapturesharp
detailslikehandsandlegsoftheperson.Since,ourtestdataiscomparativelylarge,we
evaluateonlythefastestopticalmethodsonourdataset.
RealScenes.
Weshowavisualcomparisonofresultsonreal-worldscenesofpeople
inmotion.Wecollectthesescenesbycroppingpeoplefromrealworldvideosasshownin
Figure
5(a)
.WeuseDPM[
12
]fordetectingpeopleandcomputeboundingboxregionsin
twoframesusingthegroundtruthoftheMOT16dataset[
32
].
3
http://torch.ch
8
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
Dataset
#Train
Frames
#Test
Frames
Resolution
MPISintel[
5
]1
;
0645641024

436
KITTI2012[
17
]1941951226

370
KITTI2015[
31
]2002001242

375
VirtualKitti[
15
]21
;
260

1242

375
FlyingChairs[
9
]22
;
232640512

384
FlyingThings[
33
]21
;
8184
;
248960

540
Monkaa[
33
]8
;
591

960

540
Driving[
33
]4
;
392

960

540
HumanFlow135
;
15310
;
867256

256
(a)
Method
AEPETime(s)
Zero0.6611
-
FlowNet[
9
]0.58460.080
PCAFlow[
42
]0.365210.357
SPyNet[
35
]0.20660.038
EpicFlow[
36
]0.19401.863
LDOF[
4
]0.18818.620
FlowFields[
1
]0.17094.204
HumanFlow
0.11640.031
(b)
Table1:(a)ComparisonofHumanFlowdatasetwithpreviousopticalwdatasets.(b)
EPEcomparisonsandevaluationtimesofdifferentopticalwmethodsontheHumanFlow
dataset.ZeroreferstotheEPEwhenzerowisalwaysusedforevaluation.
Wevisuallycompareourresultswithotherpopularopticalwmethodsonrealworld
scenesinFigure
5(b)
.TheperformanceofPCA-Layers[
42
]ishighlydependentonits
abilitytosegment.Hence,weseeonlyafewcaseswhereitlooksvisuallycorrect.SPyNet
[
35
]getstheoverallshapebuttheresultslooknoisyincertainimageparts.WhileLDOF
[
4
],EpicFlow[
36
]andFlowFields[
1
]generallyperformwell,theyoftenitdifto
resolvethelegs,handsandheadoftheperson.TheresultsfromHumanFlowlookappealing
especiallywhileresolvingtheoverallhumanshape,andvariouspartslikelegs,handsand
thehumanhead.HumanFlowalsoperformsverywellunderocclusionswhenthefullbody
isnotvisible.
TimingEvaluation.
Althoughsomeofthemethodsdoquitewellonourbenchmark,
theytendtobeslowduetotheircomplexnature.Assuch,theyarenotlikelytobeused
inrealtimeorembeddedapplications.Weshowtimingcomparisonsonapairofframes
inTable
1
.Weshowthatlearningwithhumanwdatacanmakeourmodelalotsimpler
whilesimultaneouslymakingitfastandaccurateforcapturinghumanmotion.Ourmodel
takes31msforinferenceonNVIDIATitanX.Assuchitcanruninrealtimeat32fps.
NetworkSize.
Employingspatialpyramidstructures[
35
]toneuralnetworkarchitec-
turesleadstoreductionofsize.Assuchournetworkisquitesmallandcanbe
storedin7.8MBofmemory.Ournetworkhasatotalof4.2millionlearnableparameters
makingitfastandeasytotrain.
6ConclusionandFutureWork
Insummary,wecreatedanextensivedatasetcontainingimagesofrealistichumanshapes
inmotiontogetherwithgroundtruthopticalw.Therealismandextentofthisdataset,to-
getherwithanend-to-endtrainedsystem,allowsournewHumanFlowmethodtooutperform
previousstate-of-the-artopticalwmethodsinournewdataset.Further-
more,weshowthatourmethodgeneralizestohumanmotioninrealworldscenesforoptical
wcomputation.Ourmethodiscompactandrunsinrealtimemakingithighlysuitablefor
phonesandembeddeddevices.
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
9
Frame1Frame2GroundTruthFlowNetSPCA-LayersSPyNetEpicFlowLDOFFlowFieldsHumanFlow
Figure4:VisualcomparisonofopticalwestimatesusingdifferentmethodsontheHuman
Flowtestset.Fromlefttoright,weshowFrame1,Frame2,GroundTruthw,results
onFlowNetS[
9
],PCA-Layers[
42
],SPyNet[
35
],EpicFlow[
36
],LDOF[
4
],FlowFields[
1
]
andHumanFlow(ours).
Infuturework,weplantomodelmoresubtlehumanmotionssuchasthoseoffacesand
hands.Wealsoplantogeneratetrainingsequencescontiningmultiple,interactingpeople,
andmorecomplex3Dscenemotions.Additionally,weplantoadd3Dclothingandacces-
sories.Thedatasetandourfocusonhumanwopensupanumberofresearchdirections
inhumanmotionunderstandingandopticalwcomputation.Wewouldliketoextendour
datasetbymodelingmorediverseclothingandoutdoorscenarios.Adirectionofpotentially
highimpactforthisworkistointegrateitinend-to-endsystemsforactionrecognition,which
typicallytakepre-computedopticalwasinput.Thereal-timenatureofthemethodcould
supportmotion-basedinterfaces,potentiallyevenondeviceslikecellphoneswithlimited
computingpower.Thedata,model,andtrainingcodeisavailable,enablingresearchersto
applythistoproblemsinvolvinghumanmotion.
(a)
(b)
Figure5:(a)Weuseapersondetectortocropoutpeoplefromreal-worldscenes(left)and
useourmodeltocomputeopticalwonthecroppedsection(right).(b)Visualcomparison
onrealscenes.Fromlefttoright,weshowFrame1,Frame2,resultsonPCA-Layers[
42
],
andSPyNet[
35
],EpicFlow[
36
],LDOF[
4
],FlowFields[
1
]andHumanFlow(ours).Our
methodisgoodatpickinguplimbandheadmotion.
10
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
Acknowledgements
WethankSiyuTangforcompilingthepersondetectionsandYiyiLiaoforhelpinguswith
opticalwevaluation.WethankCristianSminchisescufortheHuman3.6Mmocapmarker
data.
References
[1]
ChristianBailer,BertramTaetz,andDidierStricker.FlowDensecorrespon-
denceforhighlyaccuratelargedisplacementopticalwestimation.In
Pro-
ceedingsoftheIEEEInternationalConferenceonComputerVision
,pages4015Œ4023,
2015.
[2]
SimonBaker,DanielScharstein,JPLewis,StefanRoth,MichaelJBlack,andRichard
Szeliski.Adatabaseandevaluationmethodologyforopticalw.
InternationalJour-
nalofComputerVision
,92(1):1Œ31,2011.
[3]
M.J.Black,Y.Yacoob,A.D.Jepson,andD.J.Fleet.Learningparameterizedmodels
ofimagemotion.In
IEEEConf.onComputerVisionandPatternRecognition,CVPR-
97
,pages561Œ567,PuertoRico,June1997.
[4]
ThomasBrox,ChristophBregler,andJitendraMalik.Largedisplacementopticalw.
In
ComputerVisionandPatternRecognition,2009.CVPR2009.IEEEConferenceon
,
pages41Œ48.IEEE,2009.
[5]
D.J.Butler,J.Wulff,G.B.Stanley,andM.J.Black.Anaturalisticopensource
movieforopticalwevaluation.InA.Fitzgibbonetal.(Eds.),editor,
EuropeanConf.
onComputerVision(ECCV)
,PartIV,LNCS7577,pages611Œ625.Springer-Verlag,
October2012.
[6]
JamesCharles,Tomas,DerekR.Magee,DavidC.Hogg,andAndrewZisser-
man.Personalizinghumanvideoposeestimation.In
CVPR
,pages3063Œ3072.IEEE
ComputerSociety,2016.ISBN978-1-4673-8851-1.URL
http://ieeexplore.
ieee.org/xpl/mostRecentIssue.jsp?punumber=7776647
.
[7]
SunD.,SRoth,JPLewis,andMJBlack.Learningopticalw.In
ECCV
,pages
83Œ97,2008.
[8]
J.W.Davis.Hierarchicalmotionhistoryimagesforrecognizinghumanmotion.In
DetectionandRecognitionofEventsinVideo
,pages39Œ46,2001.URL
http://
dx.doi.org/10.1109/EVENT.2001.938864
.
[9]
AlexeyDosovitskiy,PhilippFischery,EddyIlg,CanerHazirbas,VladimirGolkov,
PatrickvanderSmagt,DanielCremers,ThomasBrox,etal.Flownet:Learningop-
ticalwwithconvolutionalnetworks.In
2015IEEEInternationalConferenceon
ComputerVision(ICCV)
,pages2758Œ2766.IEEE,2015.
[10]
R.FabletandM.J.Black.Automaticdetectionandtrackingofhumanmotionwith
aview-basedrepresentation.In
EuropeanConf.onComputerVision,ECCV2002
,
volume1of
LNCS2353
,pages476Œ491.Springer-Verlag,2002.
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
11
[11]
ChristophFeichtenhofer,AxelPinz,andAndrewZisserman.Convolutionaltwo-stream
networkfusionforvideoactionrecognition.In
CVPR
,pages1933Œ1941.IEEECom-
puterSociety,2016.ISBN978-1-4673-8851-1.URL
http://ieeexplore.
ieee.org/xpl/mostRecentIssue.jsp?punumber=7776647
.
[12]
P.F.Felzenszwalb,R.B.Girshick,D.McAllester,andD.Ramanan.Objectdetection
withdiscriminativelytrainedpart-basedmodels.
TPAMI
,32(9):1627Œ1645,2010.
[13]
K.Fragkiadaki,H.Hu,andJ.Shi.Posefromwandwfrompose.In
2013
IEEEConferenceonComputerVisionandPatternRecognition
,pages2059Œ2066,June
2013.doi:10.1109/CVPR.2013.268.
[14]
WilliamT.Freeman,EgonC.Pasztor,andOwenT.Carmichael.Learninglow-level
vision.
InternationalJournalofComputerVision
,40(1):25Œ47,2000.ISSN1573-
1405.doi:10.1023/A:1026501619075.URL
http://dx.doi.org/10.1023/
A:1026501619075
.
[15]
AGaidon,QWang,YCabon,andEVig.Virtualworldsasproxyformulti-object
trackinganalysis.In
CVPR
,2016.
[16]
AdrienGaidon,ZaidHarchaoui,andCordeliaSchmid.Activityrepresentationwith
motionhierarchies.
InternationalJournalofComputerVision
,107(3):219Œ238,2014.
ISSN1573-1405.doi:10.1007/s11263-013-0677-1.
[17]
AndreasGeiger,PhilipLenz,andRaquelUrtasun.Arewereadyforautonomousdriv-
ing?theKITTIvisionbenchmarksuite.In
ConferenceonComputerVisionandPattern
Recognition(CVPR)
,2012.
[18]
DonaldGemanandStuartGeman.Opinion:Scienceintheageof
Proceedings
oftheNationalAcademyofSciences
,113(34):9384Œ9387,2016.
[19]
RobinGreen.SphericalHarmonicLighting:TheGrittyDetails.
ArchivesoftheGame
DevelopersConference
,March2003.URL
http://www.research.scea.
com/gdc2003/spherical-harmonic-lighting.pdf
.
[20]
FatmaGüneyandAndreasGeiger.Deepdiscretew.In
AsianConferenceonCom-
puterVision(ACCV)
,2016.
[21]
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningfor
imagerecognition.
arXivpreprintarXiv:1512.03385
,2015.
[22]
EddyIlg,NikolausMayer,TonmoySaikia,MargretKeuper,AlexeyDosovitskiy,and
ThomasBrox.Flownet2.0:Evolutionofopticalwestimationwithdeepnetworks.
arXivpreprintarXiv:1612.01925
,2016.
[23]
CatalinIonescu,DragosPapava,VladOlaru,andCristianSminchisescu.Human3.6m:
Largescaledatasetsandpredictivemethodsfor3dhumansensinginnaturalenviron-
ments.
IEEETransactionsonPatternAnalysisandMachineIntelligence
,36(7):1325Œ
1339,jul2014.
[24]
MaxJaderberg,KarenSimonyan,AndrewZisserman,etal.Spatialtransformernet-
works.In
AdvancesinNeuralInformationProcessingSystems
,pages2017Œ2025,
2015.
12
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
[25]
HueihanJhuang,JuergenGall,SilviaZufCordeliaSchmid,andMichaelJ.Black.
Towardsunderstandingactionrecognition.In
IEEEInternationalConferenceonCom-
puterVision(ICCV)
,pages3192Œ3199,Sydney,Australia,December2013.IEEE.
[26]
GunnarJohansson.Visualperceptionofbiologicalmotionandamodelforitsanalysis.
Perception&Psychophysics
,14(2):201Œ211,1973.ISSN1532-5962.doi:10.3758/
BF03212378.
[27]
DiederikKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.
arXiv
preprintarXiv:1412.6980
,2014.
[28]
HildegardKuehne,HueihanJhuang,EstíbalizGarrote,TomasoPoggio,andThomas
Serre.Hmdb:alargevideodatabaseforhumanmotionrecognition.In
ComputerVision
(ICCV),2011IEEEInternationalConferenceon
,pages2556Œ2563.IEEE,2011.
[29]
MatthewLoper,NaureenMahmood,andMichaelJBlack.Mosh:Motionandshape
capturefromsparsemarkers.
ACMTransactionsonGraphics(TOG)
,33(6):220,2014.
[30]
MatthewLoper,NaureenMahmood,JavierRomero,GerardPons-Moll,andMichaelJ.
Black.SMPL:Askinnedmulti-personlinearmodel.
ACMTrans.Graphics(Proc.
SIGGRAPHAsia)
,34(6):248:1Œ248:16,October2015.
[31]
MoritzMenzeandAndreasGeiger.Objectscenewforautonomousvehicles.In
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
,
pages3061Œ3070,2015.
[32]
AntonMilan,LauraLeal-Taixé,IanD.Reid,StefanRoth,andKonradSchindler.
MOT16:Abenchmarkformulti-objecttracking.
arXiv:1603.00831
,2016.
[33]
N.Mayer,E.Ilg,P.Häusser,P.Fischer,D.Cremers,A.Dosovitskiy,andT.Brox.Alarge
datasettotrainconvolutionalnetworksfordisparity,opticalw,andscenewes-
timation.In
IEEEInternationalConferenceonComputerVisionandPatternRecog-
nition(CVPR)
,2016.URL
http://lmb.informatik.uni-freiburg.de/
Publications/2016/MIFDB16
.arXiv:1512.02134.
[34]
Tomas,JamesCharles,andAndrewZisserman.Flowingconvnetsforhu-
manposeestimationinvideos.In
ICCV
,pages1913Œ1921.IEEEComputerSociety,
2015.ISBN978-1-4673-8391-2.URL
http://ieeexplore.ieee.org/xpl/
mostRecentIssue.jsp?punumber=7407725
.
[35]
AnuragRanjanandMichaelJBlack.Opticalwestimationusingaspatialpyramid
network.
arXivpreprintarXiv:1611.00850
,2016.
[36]
JeromeRevaud,PhilippeWeinzaepfel,ZaidHarchaoui,andCordeliaSchmid.
EpicFlow:Edge-PreservingInterpolationofCorrespondencesforOpticalFlow.In
ComputerVisionandPatternRecognition
,2015.
[37]
KathleenMRobinette,SherriBlackwell,HeinDaanen,MarkBoehmer,andScott
Fleming.Civilianamericanandeuropeansurfaceanthropometryresource(caesar),
report.volume1.summary.Technicalreport,DTICDocument,2002.
RANJAN,ROMERO,BLACK:LEARNINGHUMANOPTICALFLOW
13
[38]
JavierRomero,MatthewLoper,andMichaelJ.Black.FlowCap:2Dhumanpose
fromopticalw.In
PatternRecognition,Proc.37thGermanConferenceonPattern
Recognition(GCPR)
,volumeLNCS9358,pages412Œ423.Springer,2015.
[39]
KhurramSoomro,AmirRoshanZamir,andMubarakShah.Ucf101:Adatasetof101
humanactionsclassesfromvideosinthewild.
arXivpreprintarXiv:1212.0402
,2012.
[40]
DuTran,LubomirBourdev,RobFergus,LorenzoTorresani,andManoharPaluri.Deep
End2EndVoxel2Voxelprediction.In
The3rdWorkshoponDeepLearninginComputer
Vision
,2016.
[41]
GülVarol,JavierRomero,XavierMartin,NaureenMahmood,MichaelBlack,Ivan
Laptev,andCordeliaSchmid.Learningfromsynthetichumans.In
CVPR
.
[42]
JonasWulffandMichaelJBlack.Efsparse-to-denseopticalwestimation
usingalearnedbasisandlayers.In
2015IEEEConferenceonComputerVisionand
PatternRecognition(CVPR)
,pages120Œ130.IEEE,2015.
[43]
FisherYu,YindaZhang,ShuranSong,AriSeff,andJianxiongXiao.Lsun:Con-
structionofalarge-scaleimagedatasetusingdeeplearningwithhumansintheloop.
arXiv:1506.03365
,2015.
[44]
SilviaZufJavierRomero,CordeliaSchmid,andMichaelJBlack.Estimatinghuman
posewithwingpuppets.In
IEEEInternationalConferenceonComputerVision
(ICCV)
,pages3312Œ3319,2013.
"
82,Interactive Classification for Deep Learning Interpretation,http://arxiv.org/pdf/1806.05660v2.pdf,https://github.com/poloclub/interactive-classification,"InteractiveClasfor
DeepLearningInterpretation

AngelCabrera,FredHohman,JasonLin,andDuenHorngChau
GeorgiaInstituteofTechnology,AtlantaGA,USA
f
alex.cabrera,fredhohman,jlin401,polo
g
@gatech.edu
Abstract.
Wepresentaninteractivesystemenablinguserstomanip-
ulateimagestoexploretherobustnessandsensitivityofdeeplearning
imageUsingmodernwebtechnologiestorunin-browserin-
ference,userscanremoveimagefeaturesusinginpaintingalgorithms
andobtainnewinrealtime,whichallowsthemtoask
avarietyof\whatif""questionsbyexperimentallymodifyingimages
andseeinghowthemodelreacts.Oursystemallowsuserstocompare
andcontrastwhatimageregionshumansandmachinelearningmod-
elsuseforrevealingawiderangeofsurprisingresults
rangingfromspectacularfailures(e.g.,a
waterbottle
imagebecomes
a
concert
whenremovingaperson)toimpressiveresilience(e.g.,a
base-
ballplayer
imageremainscorrectlyevenwithoutagloveor
base).WedemonstrateoursystematThe2018ConferenceonCom-
puterVisionandPatternRecognition(CVPR)fortheaudiencetotry
itlive.Oursystemisopen-sourcedat
https://github.com/poloclub/
interactive-classification
.Avideodemoisavailableat
https://
youtu.be/llub5GcOF6w
.
Keywords:
Interactive

inpainting

interpretabledeep
learning

imageclassi
1Introduction
Publictrustinintelligenceandmachinelearningisessentialtoitspreva-
lenceandwidespreadacceptance.Tocreatetrust,bothresearchersandthegen-
eralpublichavetounderstandwhymodelsbehavethewaytheydo.Existing
researchhasusedinteractivedatavisualizationasamechanismforhumansto
interfacewithblack-boxmachinelearningmodels[4],revealinghowmodelslearn
andbehaveusinganinteractivedialogue[9]asopposedtostaticexplanations.
1.1InteractiveforInterpretation
Wehavedesignedanddevelopedaninteractivesystemthatallowsuserstoex-
perimentwithdeeplearningimageandexploretheirrobustnessand
sensitivity.Usersareabletoremoveselectedareasofanimageinrealtimewith
classicalcomputervisioninpaintingalgorithms,Telea[8]andPatchMatch[2],
arXiv:1806.05660v2  [cs.CV]  10 Apr 20192Cabreraetal.
Fig.1.
Themoimage(left),originallyas
dock
ismisclaas
ocean
liner
whenthemastsofacoupleboatsareremovedfromtheoriginalimage(right).
Thetopescoresaretabulatedunderneatheachimage.
whichallowsthemtoaskavarietyof\whatif""questionsbyexperimentally
modifyingimagesandseeinghowthemodelreacts[3].Allowingahumanuser
toselecttheregionstobeinpaintedenablesmoresemanticallymeaningfulre-
gionstobeconsideredforstudycomparedtoothertechniqueswhereshapes(e.g.,
graysquares)areplacedoverrandompartsofanimagetosubduesignaltothe
imageclassi[10].Someexistingworkusesautomatedimagesegmentation
techniques,e.g.,superpixels[1],andacombinatorialsearchalgorithmto
themostimportantsuperpixels;however,thereisnoguaranteethesuperpixels
aresemanticallymeaningfulfeaturestoconsider[7].
1.2ofOurApproach
Throughinteractiveinpainting,oursystemhelpsrevealawiderangeofsur-
prisingresultsrangingfromspectacularfailures(e.g.,a
waterbottle
imagebe-
comesa
concert
whenremovingaperson)toimpressiveresilience(e.g.,a
base-
ballplayer
imageremainscorrectlyevenwithoutagloveorbase).
Thesystemalsocomputesclassactivationmapsondemand,whichhighlight
theimportantsemanticregionsofanimageamodelusesfor[11].
Combiningthesetools,userscandevelopqualitativeinsightintowhatamodel
seesandwhichfeaturesimpactanimage'sOursystemisopen-
sourcedat
https://github.com/poloclub/interactive-classification
.A
videodemoisavailableat
https://youtu.be/llub5GcOF6w
.
InteractiveforDeepLearningInterpretation3
2UsageExamples
2.1
InFigure1wepresentanexampleofourinteractivesystemwith
adeeplearningimagethatexhibitsfailureswhengivensemanticedits.
Consideranimageofacouplecarsdrivingnexttoboatsmooredatadock.
Theoriginaltop5resultsforthisimageareshownontherightin
Figure1.Theoriginallycorrectlyproducesatopclassof
dock
.Using
oursystem,theuserhighlightsthemastsoftheboats,inpaintingthemand
replacingthemwithsky.Theeditsdonotalterthemeaningofthepicture,but
unfortunatelythechangestoaninaccurateclass.Withthemasts
removed,themoimage(Figure1left)isnowmisasan
oceanliner
insteadofthecorrectofa
dock
(Figure1right).
Fig.2.
Themoimage(left),originallyedas
baseball
and
ballplayer
re-
mainscorrectlywhentheball,glove,andbaseareremovedfromtheoriginal
image(right).Thetopescoresaretabulatedunderneatheachimage
withdecreasing
2.2ResilientCla
InFigure2wepresentanexampleofourinteractivesystemwitha
deeplearningimagethatexhibitsresiliencetosemanticedits.Consider
animageinstanceofabaseballplayerwithaglovethrowingaballatbase.
Theoriginaltop5resultsforthisimageareshownontherightin
Figure2.Thetop2classare
baseball
and
ballplayer
.Usingoursystem,theuser
4Cabreraetal.
highlightsthebaseball,glove,andbase,inpaintingandreplacingthemwiththe
browndirt.Surprisingly,themodimageremainsas
baseballplayer
whentheseimportantfeaturesareremovedfromtheoriginalimage(Figure2
right),albeitwithadjustedprobabilities(Figure2left).
3SystemDesign
Toprovideaneasilyaccessibleexperience,wecreatedanin-browsersystemby
combiningmultiplenovelwebtechnologies.React.jsformsthebaseofthesys-
tem,providinganinteractiveandapproachableinterfacethatencouragesimage
manipulation.TheothercoretechnologyisTensorw.js,whichrunsdeeplearn-
ingmodelsinthebrowserandcanclassifyimagesinnearreal-time.Weused
theSqueezeNet[6]andMobileNet[5]deeplearningmodelsforTw.js
sincetheyaredesignedtobeportableandfast.Lastly,wemaketwot
inpaintingalgorithmsavailabletotheuser:Telea[8]andPatchMatch[2].The
Teleaalgorithmusesafast,heuristic-basedmethodandisimplementedfully
inJavaScript,allowingthesystemtoberuncompletelyin-browser.Forbetter,
moreadvancedinpaintingweutilizetheGIMPResynthesizerplugin,whichisan
open-sourceimplementationofthePatchMatchalgorithm.Thisisimplemented
inCandexposedtothesystemthroughaPythonserver.
4Conclusion
Wepresentaninteractivesystemthatenablesuserstoexploretherobustness
andsensitivityofdeeplearningimageclassers.Usingmodernwebtechnologies
torunin-browserinferenceandcomputeclassactivationmaps,userscaninpaint
imagesinrealtime,toaskavarietyof\whatif""questionsbyexperimentally
modifyingimagesandseeinghowthemodelreacts.Oursystemhelpsreveala
widerangeofsurprisingresultsrangingfromspectacularfailuresto
impressiveresilience.Oursystemisopen-sourced.Webelieveourinvestigation
willhelppeopleexploretheextenttowhichhumansandmachinesthinkalike,
andshedlightontheadvantagesandpotentialpitfallsofdeeplearningimage

Acknowledgments
ThisworkwassupportedbyNSFgrantsIIS-1563816,CNS-1704701,andTWC-
1526254;NASASpaceTechnologyResearchFellowship;andgiftsfromGoogle,
Symantec,Yahoo,Intel,Microsoft,eBay,Amazon.
References
1.
Achanta,R.,Shaji,A.,Smith,K.,Lucchi,A.,Fua,P.,Susstrunk,S.:Slicsuperpix-
elscomparedtostate-of-the-artsuperpixelmethods.IEEEtransactionsonpattern
analysisandmachineintelligence
34
(11),2274{2282(2012)
InteractiveforDeepLearningInterpretation5
2.
Barnes,C.,Shechtman,E.,Finkelstein,A.,Goldman,D.B.:Patchmatch:Aran-
domizedcorrespondencealgorithmforstructuralimageediting.ACMTransactions
onGraphics-TOG(2009)
3.
Hohman,F.,Hodas,N.,Chau,D.H.:ShapeShop:TowardsUnderstandingDeep
LearningRepresentationsviaInteractiveExperimentation.In:Proceedingsofthe
2017CHIConferenceExtendedAbstractsonHumanFactorsinComputingSys-
tems.ACM(2017)
4.
Hohman,F.,Kahng,M.,Pienta,R.,Chau,D.H.:VisualAnalyticsinDeepLearn-
ing:AnInterrogativeSurveyfortheNextFrontiers.IEEETransactionsonVisu-
alizationandComputerGraphics(TVCG)(2018)
5.
Howard,A.G.,Zhu,M.,Chen,B.,Kalenichenko,D.,Wang,W.,Weyand,T.,An-
dreetto,M.,Adam,H.:Mobilenets:tconvolutionalneuralnetworksfor
mobilevisionapplications.arXiv:1704.04861(2017)
6.
Iandola,F.N.,Han,S.,Moskewicz,M.W.,Ashraf,K.,Dally,W.J.,Keutzer,K.:
SqueezeNet:Alexnet-levelaccuracywith50xfewerparametersand
<
0.5mbmodel
size.arXiv:1602.07360(2016)
7.
Ribeiro,M.T.,Singh,S.,Guestrin,C.:Whyshoulditrustyou?:Explainingthe
predictionsofanyIn:Proceedingsofthe22ndACMSIGKDDInterna-
tionalConferenceonKnowledgeDiscoveryandDataMining.pp.1135{1144.ACM
(2016)
8.
Telea,A.:Animageinpaintingtechniquebasedonthefastmarchingmethod.
Journalofgraphicstools(2004)
9.
Weld,D.S.,Bansal,G.:IntelligibleIntelligence.arXiv:1803.04263(2018)
10.
Zeiler,M.D.,Fergus,R.:Visualizingandunderstandingconvolutionalnetworks.
In:Europeanconferenceoncomputervision.pp.818{833.Springer(2014)
11.
Zhou,B.,Khosla,A.,A.,L.,Oliva,A.,Torralba,A.:LearningDeepFeaturesfor
DiscriminativeLocalization.ConferenceonComputerVisionandPatternRecog-
nition(CVPR)(2016)
"
83,Structure-Infused Copy Mechanisms for Abstractive Summarization,http://arxiv.org/pdf/1806.05658v2.pdf,https://github.com/KaiQiangSong/struct_infused_summ,"Structure-InfusedCopyMechanismsforAbstractiveSummarization
KaiqiangSong
ComputerScienceDept.
UniversityofCentralFlorida
Orlando,FL32816,USA
kqsong@knights.ucf.edu
LinZhao
ResearchandTech.Center
RobertBoschLLC
Sunnyvale,CA94085,USA
lin.zhao@us.bosch.com
FeiLiu
ComputerScienceDept.
UniversityofCentralFlorida
Orlando,FL32816,USA
feiliu@cs.ucf.edu
Abstract
Seq2seqlearninghasproducedpromisingresultsonsummarization.However,inmanycases,
systemsummariesstillstruggletokeepthemeaningoftheoriginalintact.Theymaymissoutim-
portantwordsorrelationsthatplaycriticalrolesinthesyntacticstructureofsourcesentences.In
thispaper,wepresentstructure-infusedcopymechanismstofacilitatecopyingimportantwords
andrelationsfromthesourcesentencetosummarysentence.Theapproachnaturallycombines
sourcedependencystructurewiththecopymechanismofanabstractivesentencesummarizer.
Experimentalresultsdemonstratetheeffectivenessofincorporatingsource-sidesyntacticinfor-
mationinthesystem,andourproposedapproachcomparesfavorablytostate-of-the-artmethods.
1Introduction
Recentyearshavewitnessedincreasinginterestinabstractivesummarization.Thesystemsseektocon-
densesourcetextstosummariesthatareconcise,grammatical,andpreservetheimportantmeaningof
theoriginaltexts(NenkovaandMcKeown,2011).Thetaskencompassesanumberofhigh-leveltext
operations,e.g.,paraphrasing,generalization,textreductionandreordering(JingandMcKeown,1999),
posingaconsiderablechallengetonaturallanguageunderstanding.
Src
AMozambicanmansuspectofmurderingJorgeMicrosse,directorofMaputocentralprison,
has
escaped
fromthecity'spoliceheadquarters,localmediareportedonTuesday.
Ref
MozambicansuspectedofkillingMaputoprisondirector
escapes
Sys
mozambicanman
arrested
formurder
Src
AnAlaskafatherwhowastoodrunktodrive
had
his11-year-oldsontakethewheel,authoritiessaid.
Ref
DrunkAlaskadad
has
11yearolddrivehome
Sys
alaskafatherwhowastoodrunktodrive
Table1:Examplesourcesentences,referenceandsystemsummariesproducedbyaneuralattentiveseq-to-seqmodel.System
summariesfailtopreservesummary-worthycontentofthesource(e.g.,mainverbs)despitetheirsyntacticimportance.
Thesequence-to-sequencelearningparadigmhasachievedremarkablesuccessonabstractivesum-
marization(Rushetal.,2015;Nallapatietal.,2016;Seeetal.,2017;Paulusetal.,2017).Whilethe
resultsareimpressive,individualsystemsummariescanappearunreliableandfailtopreservethemean-
ingofthesourcetexts.Table1presentstwoexamples.Inthesecases,thesyntacticstructureofsource
sentencesisrelatively
rare
butperfectlynormal.Thesentencecontainstwoappositionalphrases
(ﬁsuspectofmurderingJorgeMicrosse,ﬂﬁdirectorofMaputocentralprisonﬂ)andthesecondsentence
hasarelativeclause(ﬁwhowastoodrunktodriveﬂ),bothlocatedbetweenthesubjectandthemainverb.
Thesystem,however,failstoidentifythemainverbinbothcases;itinsteadchoosestofocusonthe
fewwordsofthesourcesentences.Weobservethat
raresyntacticconstructions
ofthesourcecan
poseproblemsforneuralsummarizationsystems,possiblyfortworeasons.First,similarto
rarewords
,
certainsyntacticconstructionsdonotoccurfrequentlyenoughinthetrainingdatatoallowthesystem
tolearnthepatterns.Second,neuralsummarizationsystemsarenotexplicitlyinformedofthesyntactic
structureofthesourcesentencesandtheytendtobiastowardssequentialrecency.
ThisworkislicensedunderaCreativeCommonsAttribution4.0InternationalLicense.Licensedetails:
http://
creativecommons.org/licenses/by/4.0/
.
arXiv:1806.05658v2  [cs.CL]  24 Jun 2018Figure1:AnexampledependencyparsetreecreatedforthesourcesentenceinTable1.Ifimportantdependencyedgessuchas
ﬁfather
 
hadﬂcanbepreservedinthesummary,thesystemsummaryislikelytopreservethemeaningoftheoriginal.
Inthispaperweseektoaddressthisproblembyincorporatingsourcesyntacticstructureinneural
sentencesummarizationtohelpthesystemidentifysummary-worthycontentandcomposesummaries
thatpreservetheimportantmeaningofthesourcetexts.Wepresentstructure-infusedcopymechanisms
tofacilitatecopyingsourcewordsandrelationstothesummarybasedontheirsemanticandstructural
importanceinthesourcesentences.Forexample,ifimportantpartsofthesourcesyntacticstructure,
suchasadependencyedgefromthemainverbtothesubject(ﬁfatherﬂ
 
ﬁhad,ﬂshowninFigure1),
canbepreservedinthesummary,theﬁmissingverbﬂissueinTable1canbeeffectivelyalleviated.Our
modelthereforelearnstorecognizeimportantsourcewordsandsourcedependencyrelationsandstrives
topreservetheminthesummaries.Ourresearchcontributionsincludethefollowing:

weintroducenovelneuralarchitecturesthatencouragesalientsourcewords/relationstobepreserved
insummaries.Theframeworknaturallycombinesthedependencyparsetreestructurewiththecopy
mechanismofanabstractivesummarizationsystem.Tothebestofourknowledge,thisisthe
attemptatcomparingvariousneuralarchitecturesforthispurpose;

westudytheeffectivenessofseveralimportantcomponents,includingthevocabularysize,acoverage-
basedregularizer(Seeetal.,2017),andabeamsearchwithreferencemechanism(Tanetal.,2017);

throughextensiveexperimentswedemonstratethatincorporatingsyntacticinformationinneuralsen-
tencesummarizationiseffective.Ourapproachsurpassesstate-of-the-artpublishedsystemsonthe
benchmarkdataset.
1
2RelatedWork
Priortothedeeplearningera,sentencesyntacticstructurehasbeenutilizedtogeneratesummarieswith
anﬁ
extract-and-compress
ﬂframework.Compressedsummariesaregeneratedusingajointmodelto
extractsentencesanddropnon-importantsyntacticconstituents(DaumeIIIandMarcu,2002;Berg-
Kirkpatricketal.,2011;ThadaniandMcKeown,2013;Durrettetal.,2016),orapipelineapproachthat
combinesgenericsentencecompression(McDonald,2006;ClarkeandLapata,2008;Filippovaetal.,
2015)withasentencepre-selectionorpost-selectionprocess(Zajicetal.,2007;GalanisandAndrout-
sopoulos,2010;Wangetal.,2013;Lietal.,2013;Lietal.,2014).Althoughsyntacticinformationis
helpfulforsummarization,therehasbeenlittlepriorworkinvestigatinghowbesttocombinesentence
syntacticstructurewiththeneuralabstractivesummarizationsystems.
Existingneuralsummarizationsystemshandlesyntacticstructureonlyimplicitly(Kikuchietal.,2016;
Chenetal.,2016;Zhouetal.,2017;Tanetal.,2017;Paulusetal.,2017).Mostsystemsadoptaﬁ
cut-and-
stitch
ﬂschemethatpickswordseitherfromthevocabularyorthesourcetextandstitchthemtogether
usingarecurrentlanguagemodel.However,therelacksamechanismtoensurestructurallysalientwords
andrelationsinsourcesentencesarepreservedinthesummaries.Theresultingsummarysentencescan
containmisleadinginformation(e.g.,ﬁmozambicanmanarrestedformurderﬂthemeaningofthe
original)orgrammaticalerrors(e.g.,verbless,asinﬁalaskafatherwhowastoodrunktodriveﬂ).
Naturallanguagegeneration(NLG)-basedabstractivesummarization(CareniniandCheung,2008;
Geranietal.,2014;Fabbrizioetal.,2014;Liuetal.,2015;Takaseetal.,2016)alsomakesextensive
useofstructuralinformation,includingsyntactic/semanticparsetrees,discoursestructures,anddomain-
templatesbuiltusingatextplanneroranOpenIEsystem(Pighinetal.,2014).Inparticular,Cao
etal.(2018)leverageOpenIEanddependencyparsingtoextractfacttuplesfromthesourcetextanduse
thosetoimprovethefaithfulnessofsummaries.
1
Wemadeoursystempubliclyavailableat:
https://github.com/KaiQiangSong/struct_infused_summ
Differentfromtheaboveapproaches,thispaperseekstodirectlyincorporatesource-sidesyntactic
structureinthecopymechanismofanabstractivesentencesummarizationsystem.Itlearnstorecognize
importantsourcewordsandrelationsduringtraining,whilestrivingtopreservetheminthesummariesat
testtimetoaidreproductionoffactualdetails.Ourintentofincorporatingsourcesyntaxinsummariza-
tionisdifferentfromthatofneuralmachinetranslation(NMT)(Lietal.,2017a;Chenetal.,2017),in
partbecauseNMTdoesnothandletheinformationlossfromsourcetotarget.Incontrast,asummariza-
tionsystemmustselectivelypreservesourcecontenttorenderconciseandgrammaticalsummaries.We
focusonsentencesummarization,wherethegoalistoreducethesentenceofanarticle
toatitle-likesummary.Webelieveevenforthisreasonablysimpletaskthereremainsissuesunsolved.
3OurApproach
Weseektotransformasourcesentence
x
toasummarysentence
y
thatisconcise,grammatical,and
preservesthemeaningofthesourcesentence.AsourcewordisreplacedbyitsGloveembedding(Pen-
ningtonetal.,2014)beforeitisfedtothesystem;thevectorisdenotedby
x
i
(
i
2
[
S
]
;`S'forsource).
Similarly,asummarywordisdenotedby
y
t
(
t
2
[
T
]
;`T'fortarget).Ifaworddoesnotappearinthe
inputvocabulary,itisreplacedbyaspecial`
h
unk
i
'token.Webeginthissectionbydescribingthebasic
summarizationframework,followedbyournewcopymechanismsusedtoencouragesourcewordsand
dependencyrelationstobepreservedinthesummary.
3.1TheBasicFramework
Webuildanencoder-decoderarchitectureforthiswork.Anencodercondensestheentiresourcetextto
acontinuousvector;italsolearnsavectorrepresentationforeachunitofthesourcetext(e.g.,wordsas
units).Inthisworkweuseatwo-layerstackedbi-directionalLongShort-TermMemory(Hochreiterand
Schmidhuber,1997)networksastheencoder,wheretheinputtothesecondlayeristheconcatenationof
hiddenstatesfromtheforwardandbackwardpassesofthelayer.Weobtainthehiddenstatesofthe
secondlayer;theyaredenotedby
h
e
i
.Thesourcetextvectorisconstructedbyaveragingoverall
h
e
i
and
passingthevectorthroughafeedforwardlayerwith
tanh
activationtoconvertfromtheencoderhidden
statestoaninitialdecoderhiddenstate(
h
d
0
).ThisprocessisillustratedinEq.(2).
h
e
i
=
f
e
(
h
e
i

1
;
x
i
)
h
d
t
=
f
d
(
h
d
t

1
;
y
t

1
)
(1)
h
d
0
=tanh(
W
h
0
1
S
S
P
i
=1
h
e
i
+
b
h
0
)
(2)
Adecoderunrollsthesummarybypredictingonewordatatime.Duringtraining,thedecodertakes
asinputtheembeddingsofgroundtruthsummarywords,denotedby
y
t
,whileattesttime
y
t
areembed-
dingsofsystempredictedsummarywords(i.e.,teacherforcing).WeimplementanLSTMdecoderwith
theattentionmechanism.Acontextvector
c
t
isusedtoencodethesourcewordsthatthesystemattends
toforgeneratingthenextsummaryword.ItisinEqs(3-5),where
[

]
denotestheconcatena-
tionoftwovectors.The

matrixmeasuresthestrengthofinteractionbetweenthedecoderhiddenstates
f
h
d
t
g
andencoderhiddenstates
f
h
e
i
g
.Topredictthenextword,thecontextvector
c
t
and
h
d
t
arecon-
catenatedandusedasinputtobuildanewvector
e
h
d
t
(Eq.(6)).
e
h
d
t
isasurrogateforsemanticmeanings
carriedattimestep
t
ofthedecoder.Itissubsequentlyusedtocomputeaprobabilitydistributionover
theoutputvocabulary(Eq.(7)).
e
t;i
=
v
>
tanh(
W
e
[
h
d
t
jj
h
e
i
]+
b
e
)
(3)

t;i
=
exp(
e
t;i
)
P
S
i
0
=1
exp(
e
t;i
0
)
(4)
c
t
=
P
S
i
=1

t;i
h
e
i
(5)
e
h
d
t
=tanh(
W
h
[
h
d
t
jj
c
t
]+
b
h
)
(6)
P
vocab
(
w
)=
softmax
(
W
y
e
h
d
t
+
b
y
)
(7)
Figure2:Systemarchitecturesfor`Struct+Input'(left)and`Struct+Hidden'(right).Acriticalquestionweseektoansweris
whetherthestructuralembeddings(
s
e
i
)shouldbesuppliedasinputtotheencoder(left)orbeexemptedfromencodingand
directlyconcatenatedwiththeencoderhiddenstates(right).
Thecopymechanism(Gulcehreetal.,2016;Seeetal.,2017)allowswordsinthesourcesequenceto
beselectivelycopiedtothetargetsequence.Itexpandsthesearchspaceforsummarywordstoinclude
boththeoutputvocabularyandthesourcetext.Thecopymechanismcaneffectivelyreduceout-of-
vocabularytokensinthegeneratedtext,potentiallyaidinganumberofapplicationssuchasMT(Luong
etal.,2015b)andtextsummarization(Guetal.,2016;ChengandLapata,2016;Zengetal.,2017).
Ourcopymechanismemploysa`switch'toestimatethelikelihoodofgeneratingawordfromthe
vocabulary(
p
gen
)vs.copyingitfromthesourcetext(
1

p
gen
).Thebasicmodelissimilartothatofthe
pointer-generatornetworks(Seeetal.,2017).Theswitchisafeedforwardlayerwithsigmoidactivation
(Eq.(8)).Attimestep
t
,itsinputisaconcatenationofthedecoderhiddenstate
h
d
t
,contextvector
c
t
,
andtheembeddingofthepreviouslygeneratedword
y
t

1
.Forpredictingthenextword,wecombine
thegenerationandcopyprobabilities,showninEq.(9).Ifaword
w
appearsonceormoreintheinput
text,itscopyprobability(
P
i
:
w
i
=
w

t;i
)isthesumoftheattentionweightsoverallitsoccurrences.If
w
appearsinboththevocabularyandsourcetext,
P
(
w
)
isaweightedsumofthetwoprobabilities.
p
gen
=
˙
(
W
z
[
h
d
t
jj
c
t
jj
y
t

1
])+
b
z
)
(8)
P
(
w
)=
p
gen
P
vocab
(
w
)+(1

p
gen
)
X
i
:
w
i
=
w

t;i
(9)
3.2Structure-InfusedCopyMechanisms
Theaforementionedcopymechanismattendstosourcewordsbasedontheirﬁsemanticﬂimportanceen-
codedin
f

t;i
g
,whichmeasuresthesemanticrelatednessoftheencoderhiddenstate
h
e
i
andthedecoder
hiddenstate
h
d
t
(Eq.(4)).However,thesourcesyntacticstructureisignored.Thisisproblematic,because
ithurtsthesystem'sabilitytoeffectivelyidentifysummary-worthysourcewordsthataresyntactically
important.Wenextproposethreestrategiestoinjectsourcesyntacticstructuretothecopymechanism.
3.2.1ShallowCombination
Structuralinfo
Example
(1)depthinthedependencyparsetree
0
(2)labeloftheincomingedge
`root'
(3)numberofoutgoingedges
3
(4)part-of-speechtag
`VBD'
(5)absolutionpositioninthesourcetext
9
(6)relativepositioninthesourcetext
(0.5,0.6]
Table2:Sixcategoriesofstructurallabels.Examplelabelsare
generatedforword`had'inFigure1.Relativewordpositions
arediscretizedintotenbuckets.
Inspiredbycompressivesummarizationvia
structuredprediction(Berg-Kirkpatricketal.,
2011;AlmeidaandMartins,2013),wehypoth-
esizethatstructurallabels,suchastheincoming
dependencyarcandthedepthinadependency
parsetree,canbehelpfultopredictwordimpor-
tance.Weconsidersixcategoriesofstructural
labelsinthiswork;theyarepresentedinTable2.
Eachstructurallabelismappedtoaed-length,
trainablestructuralembedding.However,acrit-
icalquestionremainsastowherethestructural
embeddingsshouldbeinjectedintheexistingneuralarchitecture.Thisproblemhasnotyetbeensys-
tematicallyinvestigated.Inthiswork,wecomparetwosettings:

Struct+Input
concatenatesstructuralembeddingsofposition
i
intoonevector
s
e
i
)with
thesourcewordembedding
x
i
andusesthemasanewformofinputtotheencoder:
x
i
)
[
x
i
jj
s
e
i
]
;
Figure3:Systemarchitecturesfor`Struct+2Way+Word'(left)and`Struct+2Way+Relation'(right).

t;i
(left)measuresthe
structuralimportanceofthe
i
-thsourceword;

t;i
(right)measuresthesaliencyofthedependencyedgepointingtothe
i
-th
sourceword.
g
e
p;i
isthestructuralembeddingoftheparent.Inbothcases

t;i
replaces

t;i
tobecomethenewattentionvalue
usedtoestimatethecontextvector
c
t
.

Struct+Hidden
concatenatesstructuralembeddingsofposition
i
withtheencoderhid-
denstate
h
e
i
andusesthemasanewformofhiddenstates:
h
e
i
)
[
h
e
i
jj
s
e
i
]
.
ThearchitecturaldifferenceisillustratedinFigure2.Structuralembeddingsareimportantcomple-
mentstoexistingneuralarchitectures.However,itisunclearwhethertheyshouldbesuppliedasinput
totheencoderorbeleftoutoftheencodingprocessanddirectlyconcatenatedwiththeencoderhidden
states.Thisisacriticalquestionweseektoanswerbycomparingthetwosettings.Notethatanalter-
nativesettingistoseparatelyencodewordsandstructurallabelsusingtwoRNNencoders,weconsider
thisasasubproblemoftheﬁStruct+Inputﬂcase.
Theabovemodelscomplementstate-of-the-artbycombiningsemanticandstructuralsignalstodeter-
minesummary-worthycontent.Intuitively,asourcewordiscopiedtothesummaryfortworeasons:it
containssalientsemanticcontent,oritservesacriticalsyntacticroleinthesourcesentence.Without
explicitlymodelingthetwofactors,`semantics'canoutweigh`structure,'resultinginsummariesthat
failtokeeptheoriginalmeaningintact.Inthefollowingweproposeatwo-waymechanismtoseparately
modeltheﬁsemanticﬂandﬁstructuralﬂimportanceofsourcewords.
3.2.22-WayCombination(+Word)
Ournewarchitectureinvolvestwoattentionmatricesthatareparalleltoeachother,denotedby

and

.

t;i
isaspreviouslyinEq.(3-4).Itrepresentstheﬁsemanticﬂaspect,calculatedasthestrength
ofinteractionbetweentheencoderhiddenstate
h
e
i
andthedecoderhiddenstate
h
d
t
.Incontrast,

t;i
measurestheﬁstructuralﬂimportanceofthe
i
-thinputwordtogeneratingthe
t
-thoutputword,calculated
bycomparingthestructure-enhancedembedding
g
e
i
withthedecoderhiddenstate
h
d
t
(Eq.(10-11)).We
use
g
e
i
=[
s
e
i
jj
x
i
]
asaprimitive(unencoded)representationofthe
i
-thsourceword.
We

t;i
/

t;i
+

t;i
asaweightedsumof

t;i
and

t;i
,whereatrainablecoef

is
introducedtobalancethecontributionfrombothsides(Eq.(12)).Mergingsemanticandstructural
salienceatthisstageallowsustoacquireanaccurateestimateofhowimportantthe
i
-thsourceword
istopredictingthe
t
-thoutputword.

t;i
replaces

t;i
tobecomethenewattentionvalue.Itisusedto
calculatethecontextvector
c
t
(Eq.(13)).Areliableestimateof
c
t
iscrucialasitisusedtoestimatethe
generationprobabilityoverthevocabulary(
P
vocab
(
w
)
,Eq.(6-7)),theswitchvalue(
p
gen
,Eq.(8)),and
ultimatelyusedtopredictthenextword(
P
(
w
)
,Eq.(9)).
f
t;i
=
u
>
tanh(
W
f
[
g
e
i
jj
h
d
t
]+
b
f
)
(10)

t;i
=
exp(
f
t;i
)
P
S
i
0
=1
exp(
f
t;i
0
)
(11)

t;i
=

t;i
+

t;i
P
S
i
0
=1
(

t;i
0
+

t;i
0
)
(12)
c
t
=
S
X
i
=1

t;i
h
e
i
(13)
3.2.32-WayCombination(+Relation)
Weobservethatsalientsourcerelationsalsoplayacriticalroleinpredictingthenextword.Forexample,
ifadependencyedge(ﬁfatherﬂ
nsubj
 
ﬁhadﬂ)issalientandﬁfatherﬂisselectedtobeincludedinthe
summary,itislikelythatﬁhadﬂwillbeselectednextsuchthatasalientsourcerelation(ﬁnsubjﬂ)is
preservedinthesummary.Becausesummarywordstendtofollowthewordorderoftheoriginal,we
assumeselectingasourcewordandincludingitinthesummaryhasanimpactonitssubsequentsource
words,butnotthereverse.
Inthisformulationweuse

t;i
tocapturethesaliencyofthedependencyedgepointingtothe
i
-th
sourceword.Thus,anedge
w
j
 
w
i
hasitssaliencescoresavedin

t;j
;andconversely,anedge
w
j
!
w
i
hasitssaliencescorein

t;i
.

iscalculatedinthesamewayasdescribedinEq.(10-11).However,
wereplace
g
e
i
with
[
g
e
i
jj
g
e
p;i
]
sothatadependencyedgeischaracterizedbytheembeddingsofitstwo
endpoints(
g
e
p;i
istheparentembedding).ThearchitecturaldifferencebetweenﬁStruct+2Way+Wordﬂ
andﬁStruct+2Way+RelationﬂisillustratedinFigure3.
Toobtainthelikelihoodof
w
j
beingselectedtothesummarypriortotimestep
t
,we
e

t;j
=
P
t

1
t
0
=0

t
0
;j
thatsumsuptheindividualprobabilitiesuptotimestep
t
-1.Assumethereisadependency
edge
w
j
!
w
i
(j
<
i)whosesaliencescoreisdenotedby

t;i
.Attimestep
t
,wecalculate
e

t;j

t;i
(or
e

t;j

t;j
foredge
w
j
 
w
i
)astheprobabilityof
w
i
beingselectedtothesummary,giventhat
one
ofits
priorwords
w
j
(j
<
i)isincludedinthesummaryandthereisadependencyedgeconnectingthetwo.By
summingtheimpactover
all
itspreviouswords,weobtainthelikelihoodofthe
i
-thsourcewordbeing
includedtothesummaryattimestep
t
inordertopreservesalientsourcerelations;thisisdenotedby

t;i
(Eq.(15)).Next,we

t;i
/

t;i
+

t;i
asaweightedcombinationofsemanticandstructural
salience(Eq.(16)).

t;i
replace

t;i
tobecomethenewattentionvaluesusedtoestimatethecontext
vector
c
t
(Eq.(13)).Finally,thecalculationofgenerationprobabilities
P
vocab
(
w
)
,switchvalue
p
gen
,
andprobabilitiesforpredictingthenextword
P
(
w
)
remainsthesameaspreviously(Eq.(6-9)).
e

t;j
=
P
t

1
t
0
=0

t
0
;j
(14)

t;i
=
X
j
:
j<i
(
e

t;j

t;i
if
w
j
!
w
i
e

t;j

t;j
if
w
j
 
w
i
(15)

t;i
=

t;i
+

t;i
P
S
i
0
=1
(

t;i
0
+

t;i
0
)
(16)
3.3LearningObjectiveandBeamSearch
Wenextdescribeourlearningobjective,includingacoverage-basedregularizer(Seeetal.,2017),and
abeamsearchwithreferencemechanism(Tanetal.,2017).Wewanttoinvestigatetheeffectivenessof
thesetechniquesonsentencesummarization,whichhasnotbeenexploredinpreviouswork.
Learningobjective.
Ourtrainingproceedsbyminimizingaper-target-wordcross-entropylossfunction.
Aregularizationtermisappliedtothe

matrix.Recallthat

t;i
2
[0
;
1]
measurestheinteractionstrength
betweenthe
t
-thoutputwordandthe
i
-thinputword.Naturally,weexpecta1-to-1mappingbetweenthe
twowords.Thecoverage-basedregularizer,proposedbySeeetal.,(2017),encouragesthisbehaviorby
trackingthehistoricalattentionvaluesattributedtothe
i
-thinputword(uptotimestep
t
-
1
),denotedby
e

t;i
=
P
t

1
t
0
=0

t
0
;i
.Theapproachthentakestheminimumbetween
e

t;i
and

t;i
,whichhasthepractical
effectofforcing

t;i
(
8
t
)tobeclosetoeither0or1,otherwiseapenaltywillbeapplied.Theregularizer

isinEq.(17),where
M
isthesizeofthemini-batch,
S
and
T
arethelengthsofthesource
andtargetsequences.Fortwo-waycopymechanisms,

replaces

tobecomethenewattentionvalues,
wethereforeapplyregularizationto

insteadof

.Whentheregularizerapplies,theobjectivebecomes
minimizing(
L
+
).
=

M
X
m
=1
1
T
(
m
)
S
(
m
)
T
(
m
)
X
t
=1
S
(
m
)
X
i
=1

min(
e

t;i
;
t;i
)

(17)
Beamsearchwithreference.
Duringtesting,weemploygreedysearchtogeneratesystemsummary
sequences.Forthetaskofsummarization,thegroundtruthsummarysequencesareusuallyclosetothe
sourcetexts.Thispropertycanbeleveragedinbeamsearch.Tanetal.,(2017)describeabeamsearch
withreferencemechanismthatrewardssystemsummariesthathaveahighdegreeofbigramoverlapwith
thesourcetexts.WedescribeitinEq.(18),wherewhere
S
(
w
)
denotesthescoreofword
w
.
B
(
y
<t
;
x
)
measuresthenumberofbigramssharedbythesystemsummary(uptotimestep
t
-1)andthesourcetext;
f
y
<t
;w
g
addsaword
w
totheendofthesystemsummary.Theshorterthesourcetext(measuredby
length
S
),themoreweightasharedbigramwilladdtothescoreofthecurrentword
w
.Ahyperparameter

controlsthedegreeofclosenessbetweenthesystemsummaryandthesourcetext.
S
(
w
)=log
P
(
w
)+

B
(
f
y
<t
;w
g
;
x
)
B
(
y
<t
;
x
)
S
(18)
4Experiments
Weevaluatetheproposedstructure-infusedcopymechanismsforsummarizationinthissection.We
describethedataset,experimentalsettings,baselines,and,evaluationresultsandanalysis.
4.1DataSets
WeevaluateourproposedmodelsontheGigawordsummarizationdataset(Parker,2011;Rushetal.,
2015).Thetaskistoreducethesentenceofanarticletoatitle-likesummary.Weobtaindependency
parsetreesforsourcesentencesusingtheStanfordneuralnetworkparser(ChenandManning,2014).We
alsousethestandardtrain/valid/testdatasplits.Following(Rushetal.,2015),thetrainandvalidsplits
arepruned
2
toimprovethedataquality.Spuriouspairsthatarerepetitive,overlylong/short,andpairs
whosesourceandsummarysequenceshavelittlewordoverlapareremoved.Nopruningisperformed
forinstancesinthetestset.Theprocessedcorpuscontains4,018Ktraininginstances.Weconstruct
two(non-overlapped)validationsets:ﬁvalid-4096ﬂcontains4,096randomlysampledinstancesfromthe
validsplit;itisusedforhyperparametertuningandearlystopping.ﬁvalid-2000ﬂisusedforevaluation;
itallowsthemodelstobetrainedandevaluatedonprunedinstances.Finally,wereportresultsonthe
standardGigawordtestset(Rushetal.,2015)containing1,951instances(ﬁtest-1951ﬂ).
4.2ExperimentalSetup
WeusetheXavierscheme(GlorotandBengio,2010)forparameterinitialization,whereweightsare
initializedusingaGaussiandistribution
W
i;j
˘N
(0
;˙
)
,
˙
=
q
2
n
in
+
n
out
;
n
in
and
n
out
arenumbersof
theinputandoutputunitsofthenetwork;biasesaresettobe0.Wefurtherimplementtwotechniquesto
acceleratemini-batchtraining.First,alltraininginstancesaresortedbythesourcesequencelengthand
partitionedintomini-batches.Theshortersequencesarepaddedtohavethesamelengthasthelongest
sequenceinthebatch.Allbatchesareshufatthebeginningofeachepoch.Second,weintroducea
variable-lengthbatchvocabularycontainingonlysourcewordsofthecurrentmini-batchandwordsof
theoutputvocabulary.
P
(
w
)
inEq.(9)onlyneedstobecalculatedforwordsinthebatchvocabulary.
Itismagnitudessmallerthanadirectcombinationoftheinputandoutputvocabularies.Finally,our
inputvocabularycontainsthemostfrequent70Kwordsinthesourcetextsandsummaries.Theoutput
vocabularycontains5Kwordsbydefault.MorenetworkparametersarepresentedinTable3.
2
https://github.com/facebookarchive/NAMAS/blob/master/dataset/filter.py
Inputvocabularysize
70K
Outputvocabularysize
5K(default)
Dim.ofwordembeddings
100
Dim.ofstructuralembeddings
16
Num.ofencoder/decoderhiddenunits
256
Adamoptimizer
(KingmaandBa,2015)
lr
=1e-4
Coeff.forcoverage-basedregularizer

=1
Coeff.forbeamsearchwithreference

ˇ
13.5
Beamsize
K
=5
Minibatchsize
M
=64
Earlystoppingcriterion(max20epochs)
valid.loss
Gradientclipping
(Pascanuetal.,2013)
g
2
[-5,5]
Table3:Parametersettingsofoursummarizationsystem.
GigawordValid-2000
System
R-1R-2R-L
Baseline
42.4821.3440.18
Struct+Input
42.4421.7540.46
Struct+Hidden
42.8821.8140.63
Struct+2Way+Word
43.21
21.84
40.86
Struct+2Way+Relation
42.83
21.85
40.60
Table4:ResultsontheGigawordvalid-2000set(full-length
F1).Modelsimplementingthestructure-infusedcopymech-
anisms(ﬁStruct+*ﬂ)outperformthebaseline.
S:thegovernmentanotherroundofcriminalchargesina
wideningstockoptionsscandal
T:optionsscandalwidens
B:governmentmorechargesinstockoptionsscandal
I:anotherroundofcriminalchargesinstockoptionsscandal
H:chargesinstockoptionsscandal
W:anotherroundofcriminalchargesinstockoptionsscandal
R:governmentanotherroundofcriminalchargesinoptions
scandal
Table5:Examplesystemsummaries.`S:'source;`T:'target;
`B:'baseline;`I:'Struct+Input;`H:'Struct+Hidden;`W:'
2Way+Word;ﬁR:ﬂ2Way+Relation.ﬁ2Way+Relationﬂisable
topreserveimportantsourcerelationsinthesummary,e.g.,
ﬁgovernment
nsubj
 
ﬂ
dobj
!
round,ﬂandﬁround
nmod
!
charges.ﬂ
S:redcrossnegotiatorsfromrivalsnorthkoreaandsouthkoreaheld
talkswednesdayonemergencyfoodshipmentstostarvingnorth
koreansandagreedtomeetagainthursday
T:koreasmeetinbeijingtodiscussfoodaidfromsoutheds
B:northkorea,southkoreaagreetomeetagain
I:northkorea,southkoreameetagain
H:northkorea,southkoreameetonemergencyfoodshipments
W:northkorea,southkoreaholdtalksonfoodshipments
R:northkorea,southkoreaholdtalksonemergencyfoodshipments
Table6:Examplesystemsummaries.ﬁStruct+Hiddenﬂand
ﬁ2Way+Relationﬂsuccessfullypreservesalientsourcewords
(ﬁemergencyfoodshipmentsﬂ),whicharemissedoutby
othersystems.Weobservethatcopyingﬁholdtalksﬂfromthe
sourcealsomakestheresultingsummariesmoreinformative
thanusingthewordﬁmeet.ﬂ
4.3Results
ROUGEresultsonvalidset.
WereportresultsontheGigawordvalid-2000datasetinTable4.We
presentR-1,R-2,andR-Lscores(Lin,2004)thatrespectivelymeasurestheoverlappedunigrams,bi-
grams,andlongestcommonsubsequencesbetweenthesystemandreferencesummaries
3
.Ourbaseline
system(ﬁBaselineﬂ)implementstheseq2seqarchitecturewiththebasiccopymechanism(Eq.(1-9)).Itis
astrongbaselinethatresemblesthepointer-generatornetworksdescribedin(Seeetal.,2017).Thestruc-
turalmodels(ﬁStruct+*ﬂ)differfromthebaselineonlyonthestructure-infusedcopymechanisms.All
modelsareevaluatedwithoutthecoverageregularizerorbeamsearch(
x
3.3)toensurefaircomparison.
Overall,weobservethatmodelsequippedwiththestructure-infusedcopymechanismsaresuperiorto
thebaseline,suggestingthatcombiningsourcesyntacticstructurewiththecopymechanismiseffective.
WefoundthattheﬁStruct+Hiddenﬂarchitecture,whichdirectlyconcatenatesstructuralembeddingswith
theencoderhiddenstates,outperformsﬁStruct+Inputﬂdespitethatthelatterrequiresmoreparameters.
ﬁStruct+2Way+Wordﬂalsodemonstratesstrongperformance,achieving43.21%,21.84%,and40.86%
F
1
scores,forR-1,R-2,andR-Lrespectively.
ROUGEresultsontestset.
Wecompareourproposedapproachwitharangeofstate-of-the-artneural
summarizationsystems.ResultsonthestandardGigawordtestset(ﬁtest-1951ﬂ)arepresentedinTable7.
DetailsaboutthesesystemsareprovidedinTable8.Overall,ourproposedapproachwithstructure-
infusedpointernetworksperformstrongly,yieldingROUGEscoresthatareon-parwithorsurpassing
state-of-the-artpublishedsystems.Noticethatthescoresonthevalid-2000datasetaregenerallyhigher
thanthoseoftest-1951.Thisisbecausethe(source,summary)pairsintheGigawordtestsetarenot
pruned(see
x
4.1).Insomecases,none(orveryfew)ofthesummarywordsappearinthesource.This
maycausediftothesystemsequippedwiththecopymechanism.TheﬁStruct+2Way+Wordﬂ
architecturethatrespectivelymodelsthesemanticandsyntacticimportanceofsourcewordsachievesthe
highestscores.ItoutperformsitscounterpartofﬁStruct+2Way+Relation,ﬂwhichseekstopreservesource
dependencyrelationsinsummaries.Weconjecturethattheimperfectdependencyparsetreesgenerated
3
w/ROUGEoptions:
-n2-m-w1.2-c95-r1000
GigawordTest-1951
System
R-1R-2R-L
ABS
(Rushetal.,2015)
29.5511.3226.42
ABS+
(Rushetal.,2015)
29.7611.8826.96
Luong-NMT
(Chopraetal.,2016)
33.1014.4530.71
RAS-LSTM
(Chopraetal.,2016)
32.5514.7030.03
RAS-Elman
(Chopraetal.,2016)
33.7815.9731.15
ASC+FSC1
(MiaoandBlunsom,2016)
34.1715.9431.92
lvt2k-1sent
(Nallapatietal.,2016)
32.6715.5930.64
lvt5k-1sent
(Nallapatietal.,2016)
35.3016.6432.62
Multi-Task
(Pasunuruetal.,2017)
32.7515.3530.82
DRGD
(Lietal.,2017b)
36.2717.5733.62
Baseline(thispaper)
35.4317.4933.39
Struct+Input(thispaper)
35.3217.5033.25
Struct+2Way+Relation(thispaper)
35.4617.5133.28
Struct+Hidden(thispaper)
35.49
17.6133.33
Struct+2Way+Word(thispaper)
35.47
17.6633.52
Table7:ResultsontheGigawordtest-1951set(full-
lengthF1).Modelswithstructure-infusedcopymechanisms
(ﬁStruct+*ﬂ)performwell.TheirR-2F-scoresareon-par
withoroutperformstate-of-the-artpublishedsystems.
ABS
and
ABS+
(Rushetal.,2015)
aretheworkintroduc-
inganencoder-decoderarchitectureforsummarization.
Luong-NMT
(Chopraetal.,2016)
isare-implementationof
theattentivestackedLSTMencoder-decoderofLuong
etal.(2015a).
RAS-LSTM
and
RAS-Elman
(Chopraetal.,2016)
describea
convolutionalattentiveencoderthatensuresthedecoder
focusesonappropriatewordsateachstepofgeneration.
ASC+FSC1
(MiaoandBlunsom,2016)
presentsagenera-
tiveauto-encodingsentencecompressionmodeljointly
trainedonlabelled/unlabelleddata.
lvt2k-1sent
and
lvt5k-1sent
(Nallapatietal.,2016)
address
issuesintheattentiveencoder-decoderframework,
includingmodelingkeywords,capturingsentence-to-
wordstructure,andhandlingrarewords.
Multi-Taskw/Entailment
(Pasunuruetal.,2017)
combines
entailmentwithsummarizationinamulti-tasksetting.
DRGD
(Lietal.,2017b)
describesadeeprecurrentgenerative
decoderlearninglatentstructureofsummarysequences
viavariationalinference.
Table8:Existingsummarizationmethods.
bytheparsermayaffecttheﬁStruct+2Way+Relationﬂresults.However,becausetheGigaworddataset
doesnotprovidegold-standardannotationsforparsetrees,wecouldnoteasilyverifythisandwillleave
itforfuturework.InTable5and6,wepresentsystemsummariesproducedbyvariousmodels.
System
Info.FluencyFaithful.
Struct+Input
2.93.33.0
Struct+2Way+Relation
3.03.43.1
Ground-truthSumm.
3.23.53.1
Table9:Informativeness,y,andfaithfulnessscores
ofsummaries.TheyareratedbyAmazonturkersona
Likertscaleof1(worst)to5(best).Wechoosetoeval-
uateStruct+2Way+Relation(asopposeto2Way+Word)
becauseitfocusesonpreservingsourcerelationsinthe
summaries.
Linguisticquality.
Tofurthergaugethesummary
quality,wehirehumanworkersfromtheAmazon
MechanicalTurkplatformtoratesummariesonaLik-
ertscaleof1to5accordingtothreecriteria(Zhang
andLapata,2017):

(isthesummarygrammat-
icalandwell-formed?),
informativeness
(towhatex-
tentisthemeaningoftheoriginalsentencepreserved
inthesummary?),and
faithfulness
(isthesummary
accurateandfaithfultotheoriginal?).Wesample100
instancesfromthetestsetandemploy5turkerstorate
eachsummary;theiraveragedscoresarepresentedinTable9.WefoundthatﬁStruct+2Way+Relationﬂ
outperformsﬁStruct+Inputﬂonallthreecriteria.Italsocomparesfavorablytoground-truthsummaries
onyﬂandﬁfaithfulness.ﬂOntheotherhand,theground-truthsummaries,correspondingtoarticle
titles,arejudgedaslesssatisfyingaccordingtohumanraters.
Dependencyrelations.
Weinvestigatethesourcedependencyrelationspreservedinthesummariesin
Table10.Asourcerelationisconsideredpreservedifbothitswordsappearinthesummary.Weobserve
thatthemodelsimplementingstructure-infusedcopymechanisms(e.g.,ﬁStruct+2Way+Wordﬂ)aremore
likelytopreserveimportantdependencyrelationsinthesummaries,including
nsubj
,
dobj
,
amod
,
nmod
,
and
nmod:poss
.Dependencyrelationsthatarelessimportant(
mark
,
case
,
conj
,
cc
,
det
)arelesslikelyto
bepreserved.Theseresultsshowthatourstructure-infusedcopymechanismscanlearntorecognizethe
importanceofdependencyrelationsandselectivelypreservetheminthesummaries.
Coverageandreferencebeam.
InFigure11,weinvestigatetheeffectofapplyingthecoverageregu-
larizer(ﬁcoverageﬂ)andreference-basedbeamsearch(ﬁref
beamﬂ)(
x
3.3)toourmodels.Thecoverage
regularizerisappliedinasecondtrainingstage,wherethesystemistrainedforanextra5epochswith
coverageandthemodelyieldingthelowestvalidationlossisselected.Bothcoverageandref
beam
canimprovethesystemperformance.Ourobservationsuggeststhatref
beamisaneffectiveadditionto
shortenthegapbetweendifferentsystems.
System
nsubjdobjamodnmodnmod:poss
markcaseconjccdet
Baseline
7.2312.0720.458.7312.46
15.8314.849.725.032.22
Struct+Input
7.0311.7219.72
9.17
""
12.46
15.3514.699.554.671.97
Struct+Hidden
7.78
""
12.34
""
21.11
""
9.18
""
14.86
""
14.93
15.84
""
9.473.93
2.65
""
Struct+2Way+Word
7.46
""
12.69
""
20.59
""
9.03
""
13.00
""
15.8314.438.863.481.91
Struct+2Way+Relation
7.35
""
12.07
""
20.59
""
8.68
13.47
""
15.4114.399.124.301.89
Table10:Percentagesofsourcedependencyrelations(ofvarioustypes)preservedinthesystemsummaries.
Table11:Effectsofapplyingthecoverageregularizerand
thereferencebeamsearchtostructuralmodels,evaluatedon
test-1951.Combiningbothyieldsthehighestscores.
j
V
j
R-2
TrainSpeed
InVcb
InVcb+Src
1K
13.99
2.5h/epoch
60.57
76.04
2K
15.35
2.7h/epoch
69.71
80.72
5K
17.25
3.2h/epoch
79.98
86.51
10K
17.62
3.8h/epoch
88.26
92.18
Table12:ResultsoftheﬁStruct+2Way+Relationﬂsystem
trainedusingoutputvocabulariesofvarioussizes(
j
V
j
),eval-
uatedontest-1951w/ocoverageorref
beam.Thetraining
speediscalculatedastheelapsedtime(hours)perepoch,
testedonaGTX1080TiGPUcard.
Outputvocabularysize.
Finally,weinvestigatetheimpactoftheoutputvocabularysizeonthesum-
marizationperformanceinTable12.Allourmodelsbydefaultuseanoutputvocabularyof5Kwordsin
ordertomaketheresultscomparabletostate-of-the-art-systems.However,weobservethatthereisapo-
tentialtofurtherboostthesystemperformance(17.25
!
17.62R-2F
1
-score,w/ocoverageorref
beam)
ifwehadchosentousealargervocabulary(10K)andcanendureaslightlylongertrainingtime(1.2x).
InTable12,wefurtherreportthepercentagesofreferencesummarywordscoveredbytheoutputvocab-
ulary(ﬁInVcbﬂ)andcoveredbyeithertheoutputvocabularyorthesourcetext(ﬁInVcb+Srcﬂ).Thegap
betweenthetwoconditionsshortensasthesizeoftheoutputvocabularyisincreased.
5Conclusion
Inthispaper,weinvestigatedstructure-infusedcopymechanismsthatcombinesourcesyntacticstruc-
turewiththecopymechanismofanabstractivesummarizationsystem.Wecomparedvarioussystem
architecturesandshowedthatourmodelscaneffectivelypreservesalientsourcerelationsinsummaries.
Resultsonbenchmarkdatasetsshowedthatthestructuralmodelsareon-parwithorsurpassstate-of-the-
artpublishedsystems.
References
MiguelB.AlmeidaandAndreF.T.Martins.2013.Fastandrobustcompressivesummarizationwithdualdecom-
positionandmulti-tasklearning.In
ProceedingsofACL
.
TaylorBerg-Kirkpatrick,DanGillick,andDanKlein.2011.Jointlylearningtoextractandcompress.In
Proceed-
ingsoftheAnnualMeetingoftheAssociationforComputationalLinguistics(ACL)
.
ZiqiangCao,FuruWei,WenjieLi,andSujianLi.2018.Faithfultotheoriginal:Factawareneuralabstractive
summarization.In
ProceedingsoftheAAAIConferenceonIntelligence(AAAI)
.
GiuseppeCareniniandJackieChiKitCheung.2008.Extractivevs.NLG-basedabstractivesummarizationofeval-
uativetext:Theeffectofcorpuscontroversiality.In
ProceedingsoftheFifthInternationalNaturalLanguage
GenerationConference(INLG)
.
DanqiChenandChristopherD.Manning.2014.Afastandaccuratedependencyparserusingneuralnetworks.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
.
QianChen,XiaodanZhu,Zhen-HuaLing,SiWei,andHuiJiang.2016.Distraction-basedneuralnetworks
fordocumentsummarization.In
ProceedingsoftheTwenty-FifthInternationalJointConferenceon
Intelligence(IJCAI)
.
HuadongChen,ShujianHuang,DavidChiang,andJiajunChen.2017.Improvedneuralmachinetranslationwitha
syntax-awareencoderanddecoder.In
ProceedingsoftheAnnualMeetingoftheAssociationforComputational
Linguistics(ACL)
.
JianpengChengandMirellaLapata.2016.Neuralsummarizationbyextractingsentencesandwords.In
Proceed-
ingsofACL
.
SumitChopra,MichaelAuli,andAlexanderM.Rush.2016.Abstractivesentencesummarizationwithattentive
recurrentneuralnetworks.In
ProceedingsofNAACL
.
JamesClarkeandMirellaLapata.2008.Globalinferenceforsentencecompression:Anintegerlinearprogram-
mingapproach.
JournalofIntelligenceResearch
.
HalDaumeIIIandDanielMarcu.2002.Anoisy-channelmodelfordocumentcompression.In
Proceedingsof
theAnnualMeetingoftheAssociationforComputationalLinguistics(ACL)
.
GregDurrett,TaylorBerg-Kirkpatrick,andDanKlein.2016.Learning-basedsingle-documentsummarization
withcompressionandanaphoricityconstraints.In
ProceedingsoftheAssociationforComputationalLinguistics
(ACL)
.
GiuseppeDiFabbrizio,AmandaJ.Stent,andRobertGaizauskas.2014.Ahybridapproachtomulti-document
summarizationofopinionsinreviews.
Proceedingsofthe8thInternationalNaturalLanguageGeneration
Conference(INLG)
.
KatjaFilippova,EnriqueAlfonseca,CarlosColmenares,LukaszKaiser,andOriolVinyals.2015.Sentencecom-
pressionbydeletionwithlstms.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP)
.
DimitriosGalanisandIonAndroutsopoulos.2010.Anextractivesupervisedtwo-stagemethodforsentence
compression.In
ProceedingsofNAACL-HLT
.
ShimaGerani,YasharMehdad,GiuseppeCarenini,RaymondT.Ng,andBitaNejat.2014.Abstractivesumma-
rizationofproductreviewsusingdiscoursestructure.In
ProceedingsoftheConferenceonEmpiricalMethods
inNaturalLanguageProcessing(EMNLP)
.
XavierGlorotandYoshuaBengio.2010.Understandingthedifoftrainingdeepfeedforwardneuralnet-
works.In
Proceedingsofthe13thInternationalConferenceonIntelligenceandStatistics(AISTATS)
.
JiataoGu,ZhengdongLu,HangLi,andVictorO.K.Li.2016.Incorporatingcopyingmechanisminsequence-to-
sequencelearning.In
ProceedingsofACL
.
CaglarGulcehre,SungjinAhn,RameshNallapati,BowenZhou,andYoshuaBengio.2016.Pointingtheunknown
words.In
Proceedingsofthe54thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL)
.
SeppHochreiterandJurgenSchmidhuber.1997.Longshort-termmemory.
NeuralComputation
,9(8):1735Œ
1780.
HongyanJingandKathleenMcKeown.1999.Thedecompositionofhuman-writtensummarysentences.In
Pro-
ceedingsoftheInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval
(SIGIR)
.
YutaKikuchi,GrahamNeubig,RyoheiSasano,HiroyaTakamura,andManabuOkumura.2016.Controlling
outputlengthinneuralencoder-decoders.In
ProceedingsofEMNLP
.
DiederikP.KingmaandJimmyBa.2015.Adam:Amethodforstochasticoptimization.In
Proceedingsofthe
InternationalConferenceonLearningRepresentations(ICLR)
.
ChenLi,FeiLiu,FuliangWeng,andYangLiu.2013.Documentsummarizationviaguidedsentencecompression.
In
ProceedingsofEMNLP
.
ChenLi,YangLiu,FeiLiu,LinZhao,andFuliangWeng.2014.Improvingmulti-documentssummarizationby
sentencecompressionbasedonexpandedconstituentparsetree.In
ProceedingsofEMNLP
.
JunhuiLi,DeyiXiong,ZhaopengTu,MuhuaZhu,MinZhang,andGuodongZhou.2017a.Modelingsourcesyn-
taxforneuralmachinetranslation.In
ProceedingsoftheAnnualMeetingoftheAssociationforComputational
Linguistics(ACL)
.
PijiLi,WaiLam,LidongBing,andZihaoWang.2017b.Deeprecurrentgenerativedecoderforabstractive
textsummarization.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing
(EMNLP)
.
Chin-YewLin.2004.ROUGE:apackageforautomaticevaluationofsummaries.In
ProceedingsofACLWork-
shoponTextSummarizationBranchesOut
.
FeiLiu,JeffreyFlanigan,SamThomson,NormanSadeh,andNoahA.Smith.2015.Towardabstractivesumma-
rizationusingsemanticrepresentations.In
ProceedingsoftheNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics:HumanLanguageTechnologies(NAACL)
.
Minh-ThangLuong,HieuPham,andChristopherD.Manning.2015a.Effectiveapproachestoattention-based
neuralmachinetranslation.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP)
.
Minh-ThangLuong,IlyaSutskever,QuocV.Le,OriolVinyals,andWojciechZaremba.2015b.Addressingthe
rarewordprobleminneuralmachinetranslation.In
ProceedingsoftheAnnualMeetingoftheAssociationfor
ComputationalLinguistics(ACL)
.
RyanMcDonald.2006.Discriminativesentencecompressionwithsoftsyntacticevidence.In
Proceedingsof
EACL
.
YishuMiaoandPhilBlunsom.2016.Languageasalatentvariable:Discretegenerativemodelsforsentencecom-
pression.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
.
RameshNallapati,BowenZhou,CicerodosSantos,CaglarGulcehre,andBingXiang.2016.Abstractivetext
summarizationusingsequence-to-sequenceRNNsandbeyond.In
Proceedingsofthe20thSIGNLLConference
onComputationalNaturalLanguageLearning(CoNLL)
.
AniNenkovaandKathleenMcKeown.2011.Automaticsummarization.
FoundationsandTrendsinInformation
Retrieval
.
RobertParker.2011.EnglishGigawordeditionLDC2011T07.
Philadelphia:LinguisticDataConsortium
.
RazvanPascanu,TomasMikolov,andYoshuaBengio.2013.Onthedifoftrainingrecurrentneuralnet-
works.In
ProceedingsoftheInternationalConferenceonMachineLearning(ICML)
.
RamakanthPasunuru,HanGuo,andMohitBansal.2017.Towardsimprovingabstractivesummarizationvia
entailmentgeneration.In
ProceedingsoftheWorkshoponNewFrontiersinSummarization
.
RomainPaulus,CaimingXiong,andRichardSocher.2017.Adeepreinforcedmodelforabstractivesummariza-
tion.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
.
JeffreyPennington,RichardSocher,andChristopherD.Manning.2014.GloVe:Globalvectorsforwordrepre-
sentation.In
ProceedingsoftheConferenceEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
.
DanielePighin,MarcoCornolti,EnriqueAlfonseca,andKatjaFilippova.2014.Modellingeventsthrough
memory-based,open-iepatternsforabstractivesummarization.In
ProceedingsoftheAnnualMeetingofthe
AssociationforComputationalLinguistics(ACL)
.
AlexanderM.Rush,SumitChopra,andJasonWeston.2015.Aneuralattentionmodelforsentencesummariza-
tion.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
.
AbigailSee,PeterJ.Liu,andChristopherD.Manning.2017.Gettothepoint:Summarizationwithpointer-
generatornetworks.In
ProceedingsoftheAnnualMeetingoftheAssociationforComputationalLinguistics
(ACL)
.
ShoTakase,JunSuzuki,NaoakiOkazaki,TsutomuHirao,andMasaakiNagata.2016.Neuralheadlinegenera-
tiononabstractmeaningrepresentation.In
ProceedingsoftheConferenceonEmpiricalMethodsinNatural
LanguageProcessing(EMNLP)
.
JiweiTan,XiaojunWan,andJianguoXiao.2017.Abstractivedocumentsummarizationwithagraph-basedat-
tentionalneuralmodel.In
ProceedingsoftheAnnualMeetingoftheAssociationforComputationalLinguistics
(ACL)
.
KapilThadaniandKathleenMcKeown.2013.Sentencecompressionwithjointstructuralinference.In
Proceed-
ingsofCoNLL
.
LuWang,HemaRaghavan,VittorioCastelli,RaduFlorian,andClaireCardie.2013.Asentencecompression
basedframeworktoquery-focusedmulti-documentsummarization.In
ProceedingsofACL
.
DavidZajic,BonnieJ.Dorr,JimmyLin,andRichardSchwartz.2007.Multi-candidatereduction:Sentence
compressionasatoolfordocumentsummarizationtasks.
InformationProcessingandManagement
.
WenyuanZeng,WenjieLuo,SanjaFidler,andRaquelUrtasun.2017.Efsummarizationwithread-again
andcopymechanism.In
ProceedingsoftheInternationalConferenceonLearningRepresentations(ICLR)
.
XingxingZhangandMirellaLapata.2017.Sentencewithdeepreinforcementlearning.In
Pro-
ceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
.
QingyuZhou,NanYang,FuruWei,andMingZhou.2017.Selectiveencodingforabstractivesentencesumma-
rization.In
ProceedingsoftheAnnualMeetingoftheAssociationforComputationalLinguistics(ACL)
.
"
84,Grounded Textual Entailment,http://arxiv.org/pdf/1806.05645v1.pdf,https://github.com/claudiogreco/coling18-gte,"GroundedTextualEntailment
HoaTrongVu

+
,ClaudioGreco
y
,AliiaErofeeva
y
+
,SomayehJafaritazehjan

+
GuidoLinders
y
+
,MarcTanti

,AlbertoTestoni
y
,RaffaellaBernardi
y
,AlbertGatt

+
ErasmusMundusEuropeanPrograminLanguageandCommunicationTechnology

UniversityofMalta,
y
UniversityofTrento

name.surname@um.edu.mt
,
y
name.surname@unitn.it
Abstract
Capturingsemanticrelationsbetweensentences,suchasentailment,isalong-standingchallenge
forcomputationalsemantics.Logic-basedmodelsanalyseentailmentintermsofpossibleworlds
(interpretations,orsituations)whereapremisePentailsahypothesisHiffinallworldswhere
Pistrue,Hisalsotrue.Statisticalmodelsviewthisrelationshipprobabilistically,addressingit
intermsofwhetherahumanwouldlikelyinferHfromP.Inthispaper,wewishtobridgethese
twoperspectives,byarguingforavisually-groundedversionoftheTextualEntailmenttask.
,weaskwhethermodelscanperformbetterif,inadditiontoPandH,thereisalsoan
image(correspondingtotherelevantﬁworldﬂorﬁsituationﬂ).Weuseamultimodalversionofthe
SNLIdataset(Bowmanetal.,2015)andwecompareﬁblindﬂandvisually-augmentedmodelsof
textualentailment.Weshowthatvisualinformationisbutwealsoconductanin-depth
erroranalysisthatrevealsthatcurrentmultimodalmodelsarenotperformingﬁgroundingﬂinan
optimalfashion.
1Introduction
Evaluatingtheabilitytoinferinformationfromatextisacrucialtestofthecapabilityofmodelstograsp
meaning.Asaresult,thecomputationallinguisticscommunityhasinvestedhugeeffortsintodeveloping
textualentailment(TE)datasets.
AfterformalsemanticistsdevelopedFraCasinthemid'90(Cooperetal.,1996),anincreaseinstatisti-
calapproachestocomputationalsemanticsgaverisetotheneedforsuitableevaluationdatasets.Hence,
RecognizingTextualEntailment(RTE)sharedtaskshavebeenorganizedregularly(Sammonsetal.,
2012).RecentworkoncompositionaldistributionalmodelshasmotivatedthedevelopmentoftheSICK
datasetofsentencepairsinentailmentrelationsforevaluatingsuchmodels(Marellietal.,2014).Fur-
theradvanceswithNeuralNetworks(NNs)haveoncemoremotivatedeffortstodevelopalargenatural
languageinferencedataset,SNLI(Bowmanetal.,2015),sinceNNsneedtobetrainedonbigdata.
However,meaningisnotsomethingweobtainjustfromtextandtheabilitytoreasonisnotunimodal
either.Theimportanceofenrichingmeaningrepresentationswithothermodalitieshasbeenadvocated
bycognitivescientists,(e.g.,(Andrewsetal.,2009;Barsalou,2010))andcomputationallinguists(e.g.,
(Glava

setal.,2017)).Whileeffortshavebeenputintodevelopingmultimodaldatasetsforthetaskof
checkingSemanticTextSimilarityText(Agirreetal.,2017),wearenotawareofanyavailabledatasets
totackletheproblemofGroundedTextualEntailment(GTE).Ourpaperisaeffortinthisdirection.
TextualEntailmentisintermsofthelikelihoodoftwosentences(apremisePandanhypoth-
esisH)tobeinacertainrelation:Pentails,contradictsorisunrelatedtoH.Forinstance,thepremise
ﬁPeopletryingtogetwarminfrontofachimneyﬂandthehypothesisﬁPeopletryingtogetwarmat
homeﬂarehighlylikelytobeinanentailmentrelation.Ourquestioniswhetherhavinganimagethat
illustratestheevent(e.g.,Figure1a)canhelpamodeltocapturetherelation.Inordertoanswerthis
CorrespondenceshouldbeaddressedtoRaffaellaBernardi(
raffaella.bernardi@unitn.it
)andAlbertGatt
(
albert.gatt@um.edu.mt
).
ThisworkislicensedunderaCreativeCommonsAttribution4.0InternationalLicense.Licensedetails:
http://creativecommons.org/licenses/by/4.0/
.Thedataset,annotationandcodeisavailablefrom
https://github.com/claudiogreco/coling18-gte
.
arXiv:1806.05645v1  [cs.CL]  14 Jun 2018(a)P:
Afamilyinfrontofachimney
andH:
Afamilytrying
togetwarm
(b)P:
Peopletryingtogetwarm
andH:
Peopleareout-
sideonachillyday
.TE:unrelatedvs.GTE:related
Figure1:Premise,HypothesisandImageexamples
question,weaugmentthelargestavailableTEdatasetwithimages,weenhanceastateoftheartmodel
oftextualentailmenttotakeimagesintoaccountandweevaluateitagainsttheGTEtask.
Theinclusionofimagescanalsoalterrelationswhich,basedontextalone,wouldseemlikely.For
example,toaﬁblindﬂmodelthesentencesofthesentencepairinFigure1bwouldseemtobeunrelated,
butwhenthetwosentencesareviewedinthecontextoftheimage,theydobecomerelated.
AsuitableGTEmodelthereforehastoperformtwosub-tasks:(a)itneedstogrounditslinguistic
representationsofP,Horbothinnon-linguistic(visual)data;(b)itneedstoreasonaboutthepossible
relationshipbetweenPandH(modulothevisualinformation).
2RelatedWork
Groundinglanguagethroughvisionhasrecentlybecomethefocusofseveraltasks,includingImage
Captioning(IC,e.g.(Hodoshetal.,2013;Xuetal.,2015))andVisualQuestionAnswering(VQA,
eg.(MalinowskiandFritz,2014;Antoletal.,2015)),andevenmorerecently,VisualReasoning(Johnson
etal.,2017;Suhretal.,2017)andVisualDialog(Dasetal.,2017).OurfocusisonGroundedTextual
Entailment(GTE).WhiletheliteratureonTEisrathervast,GTEisstillratherunexploredterritory.
TextualEntailment
ThroughoutthehistoryofComputationalLinguisticsvariousdatasetshavebeen
builttoevaluateComputationalSemanticsmodelsontheTEtask.Usuallytheycontaindatadividedinto
entailment,contradictionorunknownclasses.Theﬁunknownﬂlabelhassometimesbeenreplacedwith
theﬁunrelatedﬂorﬁneutralﬂlabel,capturingslightlydifferenttypesofphenomena.Interestingly,the
ﬁentailmentﬂandﬁcontradictionﬂclassesalsodifferacrossdatasets.Inthemid-'90sagroupofformal
semanticistsdevelopedFraCaS(FrameworkforComputationalSemantics).(Cooperetal.,1996)
1
The
datasetcontainslogicalentailmentproblemsinwhichaconclusionhastobederivedfromoneormore
premises(butnotnecessarilyallpremisesareneededtoverifytheentailment).Theentailmentsaredriven
bylogicalpropertiesoflinguisticexpressions,likethemonotonicityofortheirconservativity
propertyetc.Hence,thesetofpremisesentailstheconclusioniffinalltheinterpretations(worlds)in
whichthepremisesaretruetheconclusionisalsotrue;otherwisetheconclusioncontradictsthepremises.
In2005,thePASCALRTE(RecognizingTextualEntailment)challengewaslaunched,tobecomeatask
organizedannually.In2008,theRTE-4committeemadethetaskmorebyrequiringthe
ofthepairsasﬁentailmentﬂ,ﬁcontradictionﬂandﬁunknownﬂ(Giampiccoloetal.,2008).
TheRTEdatasets,unlikeFraCaS,containreal-lifenaturallanguagesentencesandthesortofentailment
problemswhichoccurincorporacollectedfromtheweb.Importantly,thesentencepairrelationsare
annotatedasentailment,contradictionorneutralbasedonalikelihoodcondition:ifahumanreadingthe
premisewouldtypicallyinferthattheconclusion(calledthehypothesis)ismostlikelytrue(entailment),
itsnegationismostlikelytrue(contradiction)ortheconclusioncanbeeithertrueorfalse(neutral).
AtSemEval2014,inordertoevaluateCompositionalDistributionalSemanticsModelsfocusingonthe
compositionalityabilityofthosemodels,theSICKdataset(SentencesInvolvingCompositionalKnowl-
edge)wasusedinasharedentailmenttask(Marellietal.,2014).Sentencepairswereobtainedthrough
1
Acleanlyprocessableversionofithasbeenmadeavailableonlyrecently:
http://www-nlp.stanford.edu/
Ÿ
wcmac/downloads/fracas.xml
re-writingrulesandannotatedwiththethreeRTElabelsviaacrowdsourcingplatform.BothinRTEand
SICKthelabelassignedtothesentencepairscapturestherelationholdingbetweenthetwosentences.
AdifferentapproachhasbeenusedtobuildthemuchlargerSNLI(StanfordNaturalLanguageIn-
ference)dataset(Bowmanetal.,2015):Premisesaretakenfromadatasetofimagesannotatedwith
descriptivecaptions;thecorrespondinghypotheseswereproducedthroughcrowdsourcing,wherefora
givenpremise,annotatorsprovidedasentencewhichistrueornottruewithrespecttoapossibleimage
whichthepremisecoulddescribe.Aconsequenceofthischoiceisthatthecontradictionrelationcanbe
assignedtopairswhichareratherunrelated(ﬁApersoninablackwetsuitisasmallwaveﬂand
ﬁAwomanistryingtosleeponherbedﬂ),differentlyfromwhathappensinRTEandSICK.
SincetheinceptionofRTEsharedtasks,therehasbeenanincreasingemphasisondata-drivenap-
proacheswhich,giventhehypothesisHandpremiseP,seektoclassifythesemanticrelation(see(Sam-
monsetal.,2012)forareview).Morerecently,neuralapproacheshavecometodominatethescene,
asshownbytherecentRepEval2017task(Nangiaetal.,2017),whereallsubmissionsreliedonbidi-
rectionalLSTMmodels,withorwithoutpretrainedembeddings.RTEalsointersectswithanumberof
relatedinferenceproblems,includingsemantictextsimilarityandQuestionAnswering,andsomemod-
elshavebeenproposedtoaddressseveralsuchproblems.Inonepopularapproach,bothPandHare
encodedwithinthesameembeddingspace,usingasingleRNN,withadecisionmadebasedontheen-
codingsofthetwosentences.ThisistheapproachweadoptforourbaselineLSTMinSection4,based
onthemodelproposedbyBowmanetal.(2015),albeitwithsome(seealso(Tanetal.,
2016)).Asecondpromisingapproach,basedonwhichweadaptourstateoftheartmodel,relieson
matchingandaggregation(Wangetal.,2017).Here,thedecisionconcerningtherelationshipbetweenP
andHisbasedonanaggregaterepresentationachievedafterthetwosentencesarematched.Yetanother
areawhereneuralapproachesarebeingappliedtosentencepairsinanentailmentrelationshipisgenera-
tion,whereanRNNgeneratesanentailedhypothesis(orachainofsuchhypotheses)givenanencoding
ofthepremise(Kolesnyketal.,2016;StarcandMladeni
´
c,2017).
VisionandTextualEntailment
Inrecentyears,severalmodelshavebeenproposedtointegratethe
languageandvisionmodalities;usuallytheintegrationisoperationalizedbyelement-wisemultiplication
betweenlinguisticandvisualvectors.Thoughtheinterestinthesemodalitieshasspreadinanastonish-
ingwaythankstovariousmultimodaltasksproposed,includingtheIC,VQA,VisualReasoningand
VisualDialoguetasksmentionedabove,verylittleworkhasbeendoneongroundingentailment.In-
terestingly,Youngetal.(2014)hasproposedtheideaofconsideringimagesastheﬁpossibleworldsﬂ
onwhichsentencestheirdenotation.Hence,theyreleasedaﬁvisualdenotationgraphﬂwhichasso-
ciatessentenceswiththeirdenotation(setsofimages).TheideahasbeenfurtherexploitedbyLaiand
Hockenmaier(2017)andHanetal.(2017).Vendrovetal.(2016)lookathypernymy,textualentailment
andimagecaptioningasspecialcasesofasinglevisual-semantichierarchyoverwords,sentencesand
images,andtheyclaimthatmodellingthepartialorderstructureofthishierarchyinvisualandlinguistic
semanticspacesimprovesmodelperformanceonthosethreetasks.
Wesharewiththisworktheideathattheimagecanbetakenasapossibleworld.However,we
don'tusesetsofimagestoobtainthevisualdenotationoftextinordertocheckwhetherentailmentis
logicallyvalid/highlylikely.Rather,wetaketheimagetobetheworld/situationinwhichthetext
itsinterpretation.Theonlyworkthatisclosetooursisanunpublishedstudentreport(Sitzmannetal.,
2016),whichhoweverlacksthein-depthanalysispresentedhere.
3Annotateddatasetofimagesandsentencepairs
WetookasourstartingpointtheStanfordNaturalLanguageInference(SNLI)dataset(Bowmanetal.,
2015),thelargestnaturallanguageinferencedatasetavailablewithsentencepairslabelledwithentail-
ment,contradictionandneutralrelations.Weaugmentedthisdatasetwithimages.Ithasbeenshown
veryrecentlythatSNLIcontainslanguagebias,suchthatasimplecanachievehighaccuracy
inpredictingthethreeclassesjustbyhavingasinputthehypothesissentence.AsubsetoftheSNLItest
setwith`hard'cases,wheresuchasimplisticfails(hereafterSNLI
hard
)hasbeenreleased(Gu-
ruranganetal.,2018).Hence,inthispaperwewillreportourresultsonboththefulldatasetandthehard
testset,butthenzoominonSNLI
hard
tounderstandthemodels'behaviour.WeintroduceSNLI
andthenewtestsetandcomparethemthroughourannotationoflinguisticphenomena.
3.1Datasetconstruction
SNLIandSNLI
hard
testset
TheSNLIdataset(Bowmanetal.,2015)wasbuiltthroughAmazon
MechanicalTurk.Workerswereshowncaptionsofphotographswithoutthephotoandwereaskedto
writeanewcaptionthat(a)isatruedescriptionofthephoto(entailment);(b)mightbea
truedescriptionofthephoto(neutral);(c)isafalsedescriptionofthephoto(contradiction).
Exampleswereprovidedforeachofthethreecases.Thepremisesarecaptionswhichcomemostly
fromFlickr30K(Youngetal.,2014);only4KcaptionsarefromVisualGenome(Krishnaetal.,2017).
Intotal,thedatasetcontains570,152sentencepairs,balancedwithrespecttothethreelabels.Around
10%ofthesedatahavebeenvalidated(4annotatorsforeachexampleplusthelabelassignedthrough
thepreviousdatacollectionphase).Thedevelopmentandtestdatasetscontain10Kexampleseach.
Moreover,eachImage/Flickrcaptionoccursinonlyoneofthethreesets,andalltheexamplesinthe
developmentandtestsetshavebeenvalidated.
V-SNLIandV-SNLI
hard
testset
OurgroundedversionofSNLI,V-SNLI,hasbeenbuiltbymatching
eachsentencepairinSNLIwiththecorrespondingimagecomingfromtheFlickr30Kdataset;thusthe
V-SNLIdatasetisslightlysmallerthantheoriginal,whichalsocontainscaptionsfromVisualGenome.
V-SNLIconsistsof565,286pairs(187,969neutral,188,453contradiction,and188,864entailment).
Training,test,anddevelopmentsplitshavebeenbuiltaccordingtothesplitsinSNLI.Themainstatistics
ofthesplitsofthedatasetarereportedinTable1togetherwithstatisticsforthevisualcounterpartof
HardSNLI,namelyV-SNLI
hard
.Byconstruction,V-SNLIcontainsdatapointssuchthatthepremiseis
alwaystruewithrespecttotheimage,whereasthehypothesiscanbeeithertrue(entailmentorneutral
cases)orfalse(contradictionorneutralcases.)
Split
#entailment
#contradiction
#neutral
#total
V-SNLItrain
182,167
181,938
181,515
545,620
V-SNLIdev
3,329
3,278
3,235
9,842
V-SNLItest
3,368
3,237
3,219
9,824
V-SNLI
hard
test
1,058
1,135
1,068
3,261
Table1:StatisticsoftheV-SNLIdataset.
3.2Datasetannotation
FordeeperanalysisandcomparisonofthecontentsofSNLIandSNLI
hard
,wehaveannotatedtheSNLI
datasetbybothautomaticallydetectingsomesurfacelinguisticcuesandmanuallylabellinglesstrivial
phenomena.Usinganin-houseannotationinterface,wecollectedhumanjudgmentsaimingto(a)
outthosecasesforwhichthegold-standardannotationwasconsideredtobewrong
2
;(b)connectthethree
ungroundedrelationstovariouslinguisticphenomena.Toachievethis,weannotatedarandomsample
oftheSNLItestsetcontaining527sentencepairs(185entailment,171contradiction,171neutral),out
ofwhich176werefromthehardtestset(56entailment,62contradiction,58neutral).
Allthepairswereannotatedbyatleasttwoannotators,asfollows:(a)Weoutallthepairs
whichhadawronggoldlabel(seeTable2fordetails).Whenourannotatorsdidnotagreewhethera
givenrelationholdsforapair,weappealedtothecorrespondingvejudgmentscomingfrom
thevalidationstageoftheSNLIdatasettoreachaconsensusbasedonthemajorityoflabels.(b)We
consideredasvalidanylinguistictagassignedbyatleastoneannotator.Sincetheannotationfor(a)
isbinarywhereasfor(b)itismulti-class,weusedCohen's

fortheformerandalsoScott's
ˇ
and
Krippendorff's

forthelatterassuggestedbyPassonneau(2006).Theinter-annotatoragreementfor
therelationtype(a)was

=0
:
93
;for(b)linguistictagsitwas
ˇ
=0
:
63
,

=0
:
61
,and

=0
:
64
3
.
2
Anexampleofawrongannotationisthepair
Awhitegreyhounddogwearingamuzzlerunsaroundatrack
and
Thedogis
racingotherdogs
,labelledasentailmentintheSNLItestset.
3
Inter-rateragreementwascalculatedusingtheNLTKimplementation,
http://www.nltk.org
Ungrounded
Grounded
entailmentcontradictionneutral
entailmentcontradictionneutral
SNLI
7%16%2%
1%1%31%
SNLI
hard
6%10%1%
<
1%1%20%
Table2:Wronggold-standardlabels:Dataforwhichthegoldstandardlabelwasconsideredtobewrong
(a)inthe
ungrounded
settingor(b)correctintheungroundedsettingbutnotinthe
grounded
one.We
outthedatain(a)andkeepthosein(b).
TagDescriptionExample
ParaphraseTwo-wayentailment,i.e.,HentailsPandvice
versa.
P:
Amiddleeasternmarketplace
,H:
Amiddleeast-
ernstore
GeneralisationOne-wayentailment,i.e.,HentailsPbutnotnec-
essarilyviceversa.
P:
Agroupofpeopleonthebeachwithcameras
,
H:
Peopleareonabeach
.
EntityPandHdescribedifferententities(e.g.,subject,
object,location)orincompatiblepropertiesofen-
tities(e.g.,color).
P:
Adogrunsalongtheoceansurf
,H:
Acatis
runninginthewaves
.
VerbThesentencesdescribedifferent,incompatibleac-
tions.
P:
Militarypersonnelareshopping
,H:
Peoplein
themilitaryaretraining
.
InsertionHcontainsdetailsandfactsnotpresentinP(e.g.,
subjectivejudgmentsandemotions.)
P:
Womanreadingabookinalaundryroom
,H:
Thebookisold
.
UnrelatedThesentencesarecompletelyunrelated.P:
Awomanisapplyinglipmakeuptoanother
woman
,H:
Themanisreadyto
.
Thesentencescontainnumbersor(e.g.,
all,no,some,both,group
).
P:
Agroupofpeoplearetakingafuntrainride
,H:
Peopleridethetrain
.
WorldknowledgeCommonsenseassumptionsareneededtounder-
standtherelationbetweensentences(e.g.,ifthere
arenamedentities).
P:
AcrowdgatheredoneithersideofaSoapBox
Derby
,H:
Thepeopleareattherace
.
VoiceThepremiseisanactive/passivetransformationof
thehypothesis.
P:
Kidsbeingwalkedbyanadult
,H:
Anadultis
escortingsomechildren
.
SwapThesentences'subjectandobjectareswapped
fromPtoH.
P:
Awomanwalksinfrontofagiantclock
,H:
The
clockwalksinfrontofthewoman
.
Table3:TagsusedinmanualannotationofasubsetoftheSNLItestset.
Linguisticphenomena
Followingtheerroranalysisapproachdescribedinrecentwork(Nangiaet
al.,2017;Williamsetal.,2018),wecompiledanewlistoflinguisticfeaturesthatcanbeofinterest
whencontrastingSNLIandSNLI
hard
,aswellasforevaluatingRTEmodels.Someofthesewere
detectedautomatically,whileotherswereassignedmanually.AutomatictagsincludedS
YNONYM
and
A
NTONYM
,whichweredetectedusingWordNet(Miller,1995).Q
UANTIFIER
,P
RONOUN
,D
IFF
T
ENSE
,
S
UPERLATIVE
andB
ARE
NPwereusingPenntreebanklabels(Marcusetal.,1993),while
labelssuchasN
EGATION
werefoundwithastraightforwardkeywordsearch.ThetagL
ONG
hasbeen
assignedtosentencepairswithapremisecontainingmorethan30tokens,orahypothesiswithmore
than16tokens.DetailsaboutthetagsusedinthemanualannotationarepresentedinTable3.
WeexaminedthedifferencesinthetagsdistributionsbetweentheSNLIandSNLI
hard
testsets(Ta-
ble4).Interestingly,thehardsentencepairsfromourrandomsampleincludeproportionatelymore
antonymsbutfewerpronouns,aswellasexampleswithdifferentverbtensesinthepremiseandhypoth-
esis,comparedtothefulltestset.Furthermore,SNLI
hard
containsalargerproportionof
gold-standardlabelswhichbecomewrongwhentheimageisfactoredin(
˜
2
-testwith

=0
:
05
).
4Models
Inthissection,wedescribeavarietyofmodelsthatwerecomparedonbothV-SNLIandV-SNLI
hard
,
rangingfrombaselinemodelsbasedonBowmanetal.(2015)toastateoftheartmodelbyWangetal.
(2017).Wecomparetheoriginal`blind'versionofamodelwithavisually-augmentedcounterpart.In
whatfollows,weuse
P
and
H
torefertoapremiseandhypothesis,respectively.
LSTMbaseline(Blind)
ThismodelexploitsaRecurrentNeuralNetworkwithLongShort-TermMem-
oryunits(HochreiterandSchmidhuber,1997)toencodebothPandHin512Dvectors.Thetwovectors
SNLISNLI
hard
SNLISNLI
hard
ManualtagsFreq%Freq%
AutomatictagsFreq%Freq%
Insertion167325732
D
IFF
T
ENSE
7431762384
#
74
Generalisation163314626
Q
UANTIFIER
377939124438
Entity107203721
P
RONOUN
320333979
#
30
Verb101193118
S
YNONYM
17981860519
Worldknowledge93183419
A
NTONYM
8829327
""
10
91172313
S
UPERLATIVE
30431063
Paraphrase7142
L
ONG
30331093
Unrelated6121
B
ARE
NP28131073
Voice3100
N
EGATION
1852552
Swap1
<
111
Table4:DistributionoftheautomaticandmanuallyassignedtagsintheSNLIandSNLI
hard
testsets.
Automatictagsaredetectedinthewholetestset,manualonesareassignedtoitsrandomsubset.Arrows
""#
signifyastatisticallydifferenceintagproportionsbetweenthedatasets(Pearson's
˜
2
-test).
arethenconcatenatedinastackofthree512DlayershavingaReLUactivationfunction(NairandHinton,
2010),withasoftmaxlayertoclassifytherelationbetweenthetwosentencesasentailment,contra-
dictionorneutral.ThemodelisinspiredbytheLSTMbaselineproposedbyBowmanetal.(2015)
4
.The
modelexploitsthe300,000mostfrequentpretrainedGloVeembeddings(Penningtonetal.,2014)and
improvesthemduringthetrainingprocess.Toregularizethemodel,Dropout(Srivastavaetal.,2014)is
appliedtotheinputsandoutputsoftherecurrentlayersandtotheReLUfullyconnectedlayerswitha
keepingprobabilityof0.5.ThemodelistrainedusingtheAdamoptimizer(KingmaandBa,2014)with
alearningrateof0.001untilitsaccuracyonthedevelopmentsetdropsforthreesuccessiveiterations.
V-LSTMbaseline
TheLSTMmodeldescribedaboveisaugmentedwithavisualcomponentfollowing
astandardVisualQuestionAnsweringbaselinemodel(Antoletal.,2015).Followinginitialrepresenta-
tionofPandHin512DvectorsthroughanLSTM,afully-connectedlayerprojectstheL2-normalized
4096DimagevectorcomingfromthepenultimatelayerofaVGGnet16ConvolutionalNeuralNet-
work(SimonyanandZisserman,2014)toareduced512Dvector.Afully-connectedlayerwithaReLU
activationfunctionisalsoappliedtoPandHtoobtaintwo512Dvectors.Themultimodalfusionbetween
thetextandtheimageisobtainedbyperforminganelement-wisemultiplicationbetweenthevectorof
thetextrepresentationandthereducedvectoroftheimage.Themultimodalfusionisperformedbetween
theimageandboththepremiseandthehypothesis,resultingintwomultimodalrepresentations.There-
lationbetweenthemiscapturedasinthemodeldescribedabove.ThismodelusesGloVeembeddings
andthesameoptimizationandproceduredescribedabove.
Wehavealsoadaptedastate-of-the-artattention-basedmodelforICandVQA(Andersonetal.,2017;
Teneyetal.,2017)totheGTEtask.ItobtainsresultscomparabletotheV-LSTM.Thislackofimprove-
mentmightbeduetotheneedoffurtherparametertuning.Wereportthedetailsofourimplementation
anditsresultsintheSupplementaryMaterial.
BiMPM
TheBilateralMulti-PerspectiveMatching(BiMPM)model(Wangetal.,2017)obtainsstate-
of-the-artperformanceontheSNLIdataset,achievingamaximumaccuracyof86.9%,andgoingupto
88.8%inanensemblesetup.AninitialembeddinglayervectoriseswordsinPandHusingpretrained
GLoVeembeddings(Penningtonetal.,2014),andpassingthemtoacontextrepresentationlayer,which
usesbidirectionalLSTMs(BiLSTMs)toencodecontextvectorsforeachtime-step.Thecorepartofthe
modelisthesubsequentmatchinglayer,whereeachcontextualembeddingortime-stepofPismatched
againstalltheembeddingsofH,andviceversa.Theoutputofthislayeriscomposedoftwosequences
ofmatchingvectors,whichconstitutetheinputtoanotherBiLSTMattheaggregationlayer.Thevectors
fromthelasttime-stepoftheBiLSTMareconcatenatedintoaed-lengthvector,whichispassedto
thepredictiontier,atwo-layerfeed-forwardnetworkwhichtherelationbetweenPandH
4
TherearesomedifferencesbetweenourbaselineandtheLSTMbaselineusedin(Bowmanetal.,2015).Inparticular,
weusedtheAdamoptimizerinsteadoftheAdaDeltaoptimizer,theReLUactivationfunctioninsteadofthetanhactivation
function,512Dinsteadof100DfortheoutputdimensionofLSTMunits,anddropoutinallfully-connectedlayersinsteadof
L2regularization.Oursettingsoutperformedtheoriginalonesinourexperiments.
LSTM[H]
LSTMV-LSTMBiMPMV-BiMPM
Entailment
72.65
87.7187.1490.0390.38
Contradiction
66.29
79.771.3986.2587.53
Neutral
66.36
76.7968.0682.7982.91
Overall
68.49
81.4975.7086.41
86.99
Table5:Accuracies(%)forV-SNLI.[H]indicatesabaselinemodelencodingonlythehypothesis.
LSTM[H]
LSTMV-LSTMBiMPMV-BiMPM
Entailment
31.28
72.1269.0980.4381.38
Contradiction
25.29
60.7946.3477.6276.12
Neutral
20.22
50.1932.0259.3663.67
Overall
25.57
60.9949.0372.55
73.75
Table6:Accuracies(%)forV-SNLI
hard
.[H]indicatesabaselinemodelencodingonlythehypothesis.
viasoftmax.Matchingisperformedviaacosineoperation,whichyieldsan
l
-dimensionalvector,where
l
isthenumberofperspectives.Wangetal.(2017)experimentwithfourdifferentmatchingstrategies.
Intheirresults,thebest-performingversionoftheBiMPMmodelusedallfourmatchingstrategies.We
adoptthisversionofthemodelinwhatfollows.
V-BiMPMmodel
WeenhancedBiMPMtoaccountfortheimage,too.Ourversionofthismodelis
referredtoasthe
V-
BiMPM.Here,thefeaturevectorforanimageisobtainedfromthelayerbeforethe
fully-connectedlayerofaVGGnet-16.Thisresultsina
7

7

512
tensor,whichweconsideras49
512-dimensionalvectors.Thesamematchingoperationsareperformed,exceptthatmatchingoccurs
betweenP,H,andtheimage.Sincethetextualandvisualvectorshavedifferentdimensionalityand
belongtodifferentspaces,wemapthemtoamutualspaceusinganaftransformation.Wematch
textualandimagevectorsusingacosineoperation,asbefore.Fulldetailsofthemodelarereportedin
theSupplementaryMaterialsforthispaper.
5ExperimentsandResults
Themodelsdescribedintheprevioussectionswereevaluatedonboth(V-)SNLIand(V-)SNLI
hard
.
Forthevisually-augmentedmodels,weexperimentedwithwhereimagevectorswere
combinedwithbothPandH(namelyP+IandH+I),oronlywithH(PandH+I).Thebestsetting
wasinvariablytheonewhereonlyHwasgrounded;hence,wefocusontheseresultsinwhatfollows,
comparingthemtoﬁblindﬂmodels.InviewofrecentresultssuggestingthatbiasesinSNLIaffordahigh
accuracyinthepredictiontaskwithonlythehypothesissentenceasinput(Gururanganetal.,2018),we
alsoincluderesultsfortheblindmodelswithoutthepremise(denotedwith[H]inwhatfollows).
Table5showstheresultsofthevariousmodelsonthefullV-SNLIdataset.Thesamemodelsare
comparedinTable6onV-SNLI
hard
.First,notethattheLSTM[H]modelevincesadropinperformance
comparedtoLSTM(from81.49%to68.49%),thoughthedropismuchgreaterontheunbiasedSNLI
hard
subset(from60.99to25.57%).ThistheresultsreportedbyGururanganetal.(2018)and
ouradditionalfocusonthissubsetofthedata.
Theeffectofgroundinginthesemodelsislessclear.TheLSTMbaselineperformsworsewhenit
isvisuallyaugmented;thisisthecaseofV-SNLIand,evenmoredrastically,V-SNLI
hard
.Itisalso
trueirrespectiveoftherelationshiptype.Ontheotherhand,theV-BiMPMmodelimprovesmarginally
acrosstheboard,comparedtoBiMPM,ontheV-SNLIdata.Onthehardsubset,theimagesappearto
hurtperformancesomewhatinthecaseofcontradiction(from77.62%to76.12%),butimproveitby
asubstantialmarginonneutralcases(from59.36%to63.67%).Theneutralcaseisthehardestforall
models,withthepossibleexceptionofLSTM[H]onthefulldataset.
Overall,theresultssuggestthatfactoringinimageseitherhindersperformance(asinthecaseofthe
V-LSTMbaseline),orhelpsonlymarginally(asinthecaseofV-BiMPM).Inthelattercase,wealso
observeinstanceswherefactoringinimageshurtsperformance.Inanefforttounderstandtheresults,we
turntoamoredetailederroranalysisoftheV-BiMPMmodel,inrelationtothedatasetannotations,
andthenbyzoominginsomewhatcloseronV-SNLI
hard
.
5.1Erroranalysisbylinguisticannotationlabel
ManualtagsBiMPMV-BiMPM
AutomatictagsBiMPMV-BiMPM
Insertion5863
A
NTONYM
8484
Generalisation9389
B
ARE
NP7975
Entity95
#
78
Q
UANTIFIER
7373
Verb7768
D
IFF
T
ENSE
7273
Worldknowledge7971
P
RONOUN
6970
7870
S
YNONYM
6971
Paraphrase7575
L
ONG
6773
Unrelated5050
S
UPERLATIVE
6463
Swap00
N
EGATION
5156
Table7:AccuraciesobtainedbyBiMPMandV-BiMPMmodelsonSNLI
hard
,byannotationtags.Ar-
rows
""#
signifyastatisticallydifferenceintagproportionsbetweenthedatasets(Pearson's
˜
2
-test).
InTable7,accuraciesfortheblindandgroundedversionofBiMPMarebrokendownbythelabels
giventothesentencepairsintheannotatedsubsetofSNLIdescribedinSection3.Weonlyobservea
differenceinthe
Entity
case,thatis,wherethereferentsinPandHareinconsistent.Here,the
blindmodeloutperformsthegroundedone,anunexpectedresult,sinceonewouldassumeagrounded
modeltobebetterequippedtoidentifymismatchedreferents.Hence,inthefollowingweaimtounder-
standwhetherthemodelsproperlydealwiththegroundingsub-task.
5.2ErroranalysisongroundingintheSNLI
hard
Wenextturntotheﬁhardﬂsubsetofthedata,whereV-BiMPMshowedsomeimprovementoverthe
blindcase,butsufferedoncontradictioncases(Table6).Weanalysedthe207casesinSNLI
hard
where
theV-BiMPMmadeincorrectpredictionscomparedtotheblindmodel,thatis,wheretheimagehurt
performance.Thesewereannotatedindependentlybytwooftheauthors(rawinter-annotatoragreement:
96%)who(a)readthetwosentences,PandH;(b)checkedwhethertherelationannotatedinthedataset
actuallyheldorwhetheritwasanannotationerror;(c)inthosecaseswhereitheld,checkedwhether
includingtheimageactuallyresultedinachangeintherelation.
Table8displaystheproportionsofimagemismatchandincorrectannotations.Asthetablesuggests,
inthecaseswhereimageshinderperformanceintheV-BiMPM,itisusuallybecausetheimagechanges
therelation(thus,thesearecasesofimagemismatch;seeSection1foranexample);thisoccursina
largeproportionofcaseslabelledasneutralinthedataset.
Inspiredbytheworkin(Mironencoetal.,2017),wefurtherexploredtheimpactofvisualgrounding
inboththeV-LSTMandV-BiMPMbycomparingtheirperformanceonSNLI
hard
,withthesamesubset
incorporatingimageﬁfoilsﬂ.VectorsfortheimagesintheV-SNLItestsetwerecomparedpairwiseusing
cosine,andforeachtestcaseinV-SNLI
hard
,theactualimagewasreplacedwiththemostdissimilar
imageinthefulltestset.Therationaleisthat,ifvisualgroundingisreallyhelpfulinrecognisingthe
semanticrelationshipbetweenPandH,weshouldobserveadropinperformancewhentheimagesare
Relation
Imagemismatch
Incorrectannotation
Entailment
6.82
15.91
Neutral
44.58
1.20
Contradiction
3.80
22.78
Overall
24.76
12.62
Table8:CaseswhereimageshurttheV-BiMPM'sperformance:%ofcasesinwhichincludingthe
imagetheoriginalSNLIrelation(Imagemismatch),and%ofcasesinwhichtheoriginalSNLI
relationisincorrectlyannotated(Incorrectannotation).
V-LSTM
V-BiMPM
OriginalFoil
OriginalFoil
Entailment
69.0965.03
81.3880.81
Contradiction
46.3430.92
76.1274.98
Neutral
32.0231.46
63.6763.39
Overal
49.0346.92(-2.11)
73.7573.08(-0.67)
Table9:Accuraciesofthevisually-augmentedmodelsonV-SNLI
hard
withoriginalorfoilimage.
V-LSTMV-BiMPM
Entailment
40.7451.89
Contradiction
30.2240.7
Neutral
22.4732.02
Overall
31.0941.49
GTclassPrediction
V-LSTMV-BiMPM
ContradictionContradiction
343462
Contradiction*Entailment
442327
ContradictionNeutral
350346
EntailmentEntailment
431549
Entailment*Contradiction
254166
EntailmentNeutral
373343
NeutralNeutral
240342
NeutralContradiction
377263
NeutralEntailment
451463
Table10:Confusionmatricesfor[H+I].(*)marksimplausibleerrors.
unrelatedtothescenariodescribedbythesentences.TheresultsaredisplayedinTable9,whichalso
reproducestheoriginalresultsonV-SNLI
hard
fromTable6foreaseofreference.
Astheresultsshow,modelsarenothurtbythefoilimage,contrarytoourexpectations.V-BiMPM
overalldropsjustby0.67%whereasV-LSTMdropissomewhathigher(-2.11%)showingitmightbe
doingabetterjobonthegroundingsub-task.
Asacheck,wesoughttoisolatethegroundingfromthereasoningsub-task,focusingonlyon
theformer.Wecomparedthemodelswhengroundingonlythehypothesis[H+I],whileleavingoutthe
premise.Notethatthistestisdifferentfromtheevaluationofthemodelusingonlythehypothesis[H]:
Whereasinthatcasetheinputisnotexpectedtoprovideanyusefulinformationtoperformthetask,here
itis.AswenotedinSection3,byconstructionthepremiseisalwaystruewithrespecttotheimage
whilethehypothesiscanbeeithertrue(entailmentorneutralcases)orfalse(contradictionorneutral
cases).Amodelthatisgroundingthetextadequatelywouldbeexpectedtoconfusebothentailment
andcontradictioncaseswithneutralones;ontheotherhand,neutralcasesshouldbeconfusedwith
entailmentsorcontradictions.Confusingcontradictionswithentailmentswouldbeasignthatamodelis
groundinginadequately,sinceitisnotrecognisingthatHisfalsewithrespecttotheimage.
AstheleftpanelofTable10shows,V-BiMPMoutperformsV-LSTMbyasubstantialmargin,though
theperformanceofbothmodelsdropssubstantiallywiththissetup.Therightpanelinthetableshowsthat
neithermodelisfreeofimplausibleerrors(confusingentailmentsandcontradictions),thoughV-BiMPM
makessubstantiallyfewerofthese.
6Conclusion
Thispaperhasinvestigatedthepotentialofgroundingthetextualentailmenttaskinvisualdata.We
arguedthataGroundedTextualEntailmentmodelneedstoperformtwotasks:(a)thegroundingitself,
and(b)reasoningabouttherelationbetweenthesentences,againstthevisualinformation.Ourresults
suggestthatamodelbasedonmatchingandaggregationliketheBiMPMmodel(Wangetal.,2017)can
performverywellatthereasoningtask,classifyingentailmentrelationscorrectlymuchmorefrequently
thanabaselineV-LSTM.Ontheotherhand,itisnotclearthatgroundingisbeingperformedadequately
inthismodel.Itisprimarilyinthecaseofcontradictionsthattheimageseemstoplayadirectrolein
biasingthetowardstherightorwrongclass,dependingonwhethertheimageiscorrect.
Insummary,twoconclusionscanbedrawnfromtheseresults.First,inthosecaseswheretheinclusion
ofvisualinformationresultsinalossofaccuracy,thisisoftenduetotheimageresultinginachangein
theoriginalrelationannotatedinthedataset.Arelatedobservationisthatusingfoilimagesresultsina
greaterdropinperformanceoncontradictioncases,possiblybecauseinsuchcases,groundingservesto
identifyamismatchbetweenthehypothesisandthescenedescribedbythepremise,asituationwhichis
renderedopaquebytheintroductionoffoils.Second,inthosecaseswhereimprovementsareobserved
inthestateoftheartV-BiMPM,thepreciseroleplayedbytheimageisnotstraightforward.Indeed,we
thatthismodelstillmarginallyoutperformsthe`blind',text-onlymodeloverall,whentheimages
involvedarefoilsratherthanactualimages.
WebelievethatfurtherresearchongroundedTEisworthyoftheNLPcommunity'sattention.While
linkinglanguagewithperceptioniscurrentlyatopicalissue,therehasbeenrelativelylittleworkon
linkinggroundingdirectlywithinference.Bydrawingclosertoajointsolutiontothegroundingand
inferencetasks,modelswillalsobebetterabletoaddresslanguageunderstandingintherealworld.
ThepresentpaperpresentedastepinthisdirectionusingaversionofanexistingTEdatasetwhich
wasaugmentedwithimagesthatcouldbepaireddirectlywiththepremises,sincethesewereoriginally
captionsforthoseimages.However,itisimportanttonotethatinthisdatasetpremise-hypothesespairs
werenotgenerateddirectlywithreferencetotheimagesthemselves.Animportantissuetoconsiderin
futureworkonGTE,besidesthedevelopmentofbettermodels,isthedevelopmentofdatasetsinwhich
theroleofperceptualinformationiscontrolled,ensuringthatthedataonwhichmodelsaretrained
representstrulygroundedinferences.
Acknowledgements
WekindlyacknowledgetheEuropeanNetworkonIntegratingVisionandLanguage(iV&LNet)ICT
COSTActionIC1307.Moreover,wethanktheErasmusMundusEuropeanPrograminLanguage
andCommunicationTechnology.MarcTanti'sworkispartiallyfundedbytheEndeavourScholarship
Scheme(Malta),bytheEuropeanUnion'sEuropeanSocialFund(ESF).Finally,wegrate-
fullyacknowledgethesupportofNVIDIACorporationwiththedonationstotheUniversityofTrentoof
theGPUsusedinourresearch.
AppendixA:Bottom-uptop-downattention(VQA)
WeadaptedtheVisualQuestionAnsweringmodelproposedin(Andersonetal.,2017;Teneyetal.,2017)
totheGroundedTextualEntailmenttask.Themodelpresentsamoreattentionmechanism
whichallowstoidentifythemostimportantregionsdiscoveredintheimageandtoperformattention
overeachofthem.
ThemodelusesaaRecurrentNeuralNetworkwithLongShort-TermMemoryunitstoencodethe
premisePandhypothesisHin512Dvectors.Abottom-upattentionmechanismexploitsaFastR-CNN
(Girshick,2015)basedonaResNet-101convolutionalneuralnetwork(Heetal.,2016)toobtainregion
proposalscorrespondingtothe36mostinformativeregionsoftheimage.Atop-downattentionmecha-
nismisusedbetweenthepremise(resp.hypothesis)andeachoftheL2-normalized2048Dimagevectors
correspondingtotheregionproposalstoobtainanattentionscoreforeachofthem.Then,a2048Dimage
vectorencodingthemostinterestingvisualfeaturesforthepremise(hypothesis)isobtainedasasumof
the36imagevectorsweightedbythecorrespondingattentionscoresforthepremise(hypothesis).A
fully-connectedlayerwithagatedtanhactivationfunctionisappliedtotheimagevectorofthemost
interestingvisualfeaturesforthepremiseandforthehypothesistoobtainareduced512Dvectorfor
eachofthem.Afully-connectedlayerwithagatedtanhactivationfunctionisalsoappliedtothepremise
andtothehypothesisinordertoobtainareduced512Dvectorforeachofthem.
Themultimodalfusionbetweenthepremise(hypothesis)andtheimagevectorofthemostinteresting
visualfeaturesforthepremise(hypothesis)isobtainedbyperforminganelement-wisemultiplication
betweenthereducedvectorofthepremise(hypothesis)andthereducedvectorofthemostinteresting
visualfeaturesforthepremise(hypothesis).Afterthat,themodelfeedstheconcatenationofthetwore-
sultingmultimodalrepresentationstoastackofthree512Dlayershavingagatedtanhactivationfunction,
withasoftmaxlayertoclassifytherelationbetweenthetwosentencesasentailment,contradiction
orneutral.ThismodelusesGloVeembeddingsandthesameoptimizationtricksandprocedureofthe
LSTMandV-LSTMmodels.
WereporttheaccuraciesoftheVQAmodelsagainstthevarioustestsreportedinthepaper.Forease
ofcomparisonwereproducethefulltablefromthemainpaper,withtheadditionoftheVQAresults.
LSTM[H]
LSTMV-LSTMVQABiMPMV-BiMPM
Entailment
72.65
87.7187.1486.190.0390.38
Contradiction
66.29
79.771.3978.9986.2587.53
Neutral
66.36
76.7968.0673.5682.7982.91
Overall
68.49
81.4975.7079.6586.41
86.99
Table11:Accuracies(%)forV-SNLI.[H]indicatesabaselinemodelencodingonlythehypothesis
(Table5inthepaper).
LSTM[H]
LSTMV-LSTMVQABiMPMV-BiMPM
Entailment
31.28
72.1269.0967.3980.4381.38
Contradiction
25.29
60.7946.3459.0377.6276.12
Neutral
20.22
50.1932.0242.13
59.3663.67
Overall
25.57
60.9949.0356.2172.5573.75
Table12:Accuracies(%)forV-SNLI
hard
.[H]indicatesabaselinemodelencodingonlythehypothesis
(Table6inthepaper).
V-LSTM
V-BiMPM
VQA
OriginalFoil
OriginalFoil
OriginalFoil
Entailment
69.0965.03
81.3880.81
67.3960.4
Contradiction
46.3430.92
76.1274.98
59.0360.97
Neutral
32.0231.46
63.6763.39
42.1342.79
Overal
49.0346.92(-2.11)
73.7573.08(-0.03)
56.2154.83(-1.38)
Table13:AccuraciesofthevisuallyaugmentedmodelsonV-SNLI
hard
containingtheoriginalorfoil
image(Table9inthepaper).
V-LSTMV-BiMPMVQA
Entailment
40.7451.8948.11
Contradiction
30.2240.747.05
Neutral
22.4732.0231.37
Overall
31.0941.4942.26
GTclassPrediction
V-LSTMV-BiMPMVQA
ContradictionContradiction
343462534
Contradiction*Entailment
442327286
ContradictionNeutral
350346315
EntailmentEntailment
431549509
Entailment*Contradiction
254166188
EntailmentNeutral
373343361
NeutralNeutral
240342335
NeutralContradiction
377263272
NeutralEntailment
451463461
Table14:Confusionmatricesfor[H+I].(*)marksimplausibleerrors(Table10inthepaper).
AppendixB:V-biMPMModeldetails
Premise
Hypothesis
p
1
p
2
...
...
p
i
...
...
p
M
h
1
h
2
...
...
h
i
...
...
h
N
...
...
...
...
...
...
...
...
f
1
f
2
...
...
f
i
...
...
f
L
VGGnet
...
...
PvsH
...
...
HvsP
...
...
HvsImage
...
...
ImagevsH
















...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
softmax
P
(
y
j
premise;image;hypothesis
)
Embeddinglayer
Contextlayer
Matchinglayer
Aggregationlayer
Predictionlayer
Figure2:Vision-enhancedBilateralMulti-PerspectiveMatchingmodel(v-BiMPM).
Here,wereportsomefurtherdetailsofourimplementationoftheV-BiMPMmodeldescribedin
Section4ofthemainpaper,basedontheworkofWangetal.(2017).OurmodelisdisplayedinFigure2.
ThecorepartoftheoriginalBiMPMisthematchinglayer.Giventwo
d
-dimensionalvectors
v
P
and
v
H
,eachreplicated
l
times(
l
isthenumberof`perspectives')andatrainable
l

d
weightmatrix
W
,matchinginvolvesacosinesimilaritycomputationthatyieldsan
l
-dimensionalmatchingvector
m
,
whoseelementsareasfollows:
m
k
=
cosine
(
W
k

v
P
;W
k

v
H
)
(1)
Thematchingoperationsincludedarethefollowing:
1.
full-matching
,whereeachforwardorbackwardcontextualembeddingofthepremiseP(resp.the
hypothesisH)ismatchedtothelasttime-stepofH(resp.P);
2.
max-pooling
,whereeachforward/backwardcontextualembeddingofonesentenceiscomparedto
theembeddingsoftheother,retainingthemaximumvalueforeachdimension;
3.
attentivematching
,wherethepairwisecosinesimilaritybetweenforward/backwardembed-
dingsofPandHisestimated,beforecalculatinganattentivevectorovertheweightedsumof
contextualembeddingsforHandmatchingeachforward/backwardembeddingofPagainstthe
attentivevector;
4.
max-attentivematching
,aversionofattentivematchingwherethecontextualembeddingwiththe
highestcosineisusedastheattentivevector,insteadoftheweightedsum.
Thevisually-augmentedversionoftheoriginalmodel,V-BiMPM,isdisplayedinFigure2.Toperform
multimodalmatching,thevisualandtextualvectorsaremappedtoamutualspaceusingthefollowing
aftransformation:
v
i
=
W
t
f
i
+
b
t
;
f
i
2
R
e
;
W
t
2
R
e

d
;
b
t
;v
i
2
R
d
(2)
where
W
t
,
b
t
,
f
i
,and
v
i
aretheweightmatrix,thebias,theinputfeaturesandoutputfeatures,respec-
tively,and
t
isanytext(PorH).Givenweightmatrices
W
2
R
l

d
fortextand
U
l

d
forimages,we
computethematchingvector
m
betweenatextualvector
v
t
andimagevector
v
i
as:
m
k
=
cosine
(
W
k

v
t
;U
k

v
i
)
(3)
References
[Agirreetal.2017]
EnekoAgirre,OierLopezdeLacalle,andAitorSoroa.2017.EvaluatingMultimodalRepre-
sentationsonSentenceSimilarity:vSTS,VisualSemanticTextualSimilarityDataset.In
Proceedingsofthe
SecondWorkshoponClosingtheLoopBetweenVisionandLanguage(ICCV'17)
.
[Andersonetal.2017]
PeterAnderson,XiaodongHe,ChrisBuehler,DamienTeney,MarkJohnson,StephenGould,
andLeiZhang.2017.Bottom-upandtop-downattentionforimagecaptioningandvisualquestionanswering.
arXivpreprintarXiv:1707.07998
.
[Andrewsetal.2009]
MarkAndrews,GabriellaVigliocco,andDavidVinson.2009.Integratingexperientialand
distributionaldatatolearnsemanticrepresentations.
PsychologicalReview
,116(3):463Œ498.
[Antoletal.2015]
StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,C.Lawrence
Zitnick,andDeviParikh.2015.VQA:Visualquestionanswering.In
InternationalConferenceonComputer
Vision(ICCV)
.
[Barsalou2010]
LawrenceW.Barsalou.2010.GroundedCognition:Past,Present,andFuture.
TopicsinCognitive
Science
,2(4):716Œ724.
[Bowmanetal.2015]
SamuelR.Bowman,GaborAngeli,ChristopherPotts,andChristopherD.Manning.2015.
Alargeannotatedcorpusforlearningnaturallanguageinference.In
Proceedingsofthe2015Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
.AssociationforComputationalLinguistics.
[Cooperetal.1996]
RobinCooper,DickCrouch,JanVanEijck,ChrisFox,JohanVanGenabith,JanJaspars,Hans
Kamp,DavidMilward,ManfredPinkal,MassimoPoesio,andStevePulman.1996.Usingtheframework.
TechnicalReportTechnicalReportLRE62-051D-16,TheFraCaSConsortium.
[Dasetal.2017]
AbhishekDas,SatwikKottur,KhushiGupta,AviSingh,DeshrajYadav,Jos
´
eM.F.Moura,Devi
Parikh,andDhruvBatra.2017.VisualDialog.In
ProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognition(CVPR)
.
[Giampiccoloetal.2008]
D.Giampiccolo,H.T.Dang,M.Bernardo,I.Dagan,andE.Cabrio.2008.Thefourth
pascalrecognisingtextualentailmentchallenge.In
ProceedingsoftheTAC2008WorkshoponTextualEntail-
ment
.
[Girshick2015]
RossGirshick.2015.Fastr-cnn.
arXivpreprintarXiv:1504.08083
.
[Glava

setal.2017]
GoranGlava

s,IvanVuli,andSimonePaoloPonzetto.2017.IfSentencesCouldSee:Investi-
gatingVisualInformationforSemanticTextualSimilarity.In
Proceedingsofthe12thInternationalConference
onComputationalSemantics(IWCS'17)
,pages1Œ15.
[Gururanganetal.2018]
SuchinGururangan,SwabhaSwayamdipta,OmerLevy,RoySchwartz,SamuelR.Bow-
man,andNoahA.Smith.2018.AnnotationArtifactsinNaturalLanguageInferenceData.
arXivpreprint
arXiv:1803.02324
.
[Hanetal.2017]
DanHan,PascualMartinezGomez,andKojiMineshima.2017.Visualdenotationsforrecogniz-
ingtextualentailment.In
EMNLP
.
[Heetal.2016]
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Deepresiduallearningfor
imagerecognition.In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition
,pages
770Œ778.
[HochreiterandSchmidhuber1997]
SeppHochreiterandJ
¨
urgenSchmidhuber.1997.Longshort-termmemory.
Neuralcomputation
,9(8):1735Œ1780.
[Hodoshetal.2013]
MicahHodosh,PeterYoung,andJuliaHockenmaier.2013.Framingimagedescriptionasa
rankingtask:Data,modelsandevaluationmetrics.
JournalofIntelligenceResearch
,47:853Œ899.
[Johnsonetal.2017]
JustinJohnson,BharathHariharan,LaurensvanderMaaten,LiFei-Fei,C.LawrenceZitnick,
andRossGirshick.2017.Clevr:Adiagnosticdatasetforcompositionallanguageandelementaryvisual
reasoning.In
ProceedingsofCVPR2017
.
[KingmaandBa2014]
DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimization.
arXivpreprintarXiv:1412.6980
.
[Kolesnyketal.2016]
VladyslavKolesnyk,TimRockt
¨
aschel,andSebastianRiedel.2016.GeneratingNatural
LanguageInferenceChains.
arXivpreprintarXiv:1606.01404
.
[Krishnaetal.2017]
RanjayKrishna,YukeZhu,OliverGrothJustin,JohnsonKenjiHata,JoshuaKravitz,Stephanie
Chen,YannisKalantidis,Li-JiaLi,DavidA.Shamma,MichaelS.Bernstein,andLiFei-Fei.2017.Visual
genome:Connectinglanguageandvisionusingcrowdsourceddenseimageannotations.
InternationalJournal
ofComputerVision
,123(1):32Œ73.
[LaiandHockenmaier2017]
AliceLaiandJuliaHockenmaier.2017.Learningtopredictdenotationalprobabilities
formodelingentailment.In
Proceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociation
forComputationalLinguistics:Volume1,LongPapers
,pages721Œ730,Valencia,Spain,April.Associationfor
ComputationalLinguistics.
[MalinowskiandFritz2014]
M.MalinowskiandM.Fritz.2014.Amulti-worldapproachtoquestionanswering
aboutreal-worldscenesbasedonuncertaininput.In
AdvancesinNeuralInformationProcessingSystems
.
[Marcusetal.1993]
MitchellPMarcus,BeatriceSantorini,andMaryAnnMarcinkiewicz.1993.Buildingalarge
annotatedcorpusofEnglish:ThePennTreebank.
ComputationalLinguistics
,19(2):313Œ330.
[Marellietal.2014]
MarcoMarelli,StefanoMenini,MarcoBaroni,LuisaBentivogli,RaffaellaBernardi,and
RobertoZamparelli.2014.Asickcurefortheevaluationofcompositionaldistributionalsemanticmodels.
In
ProceedingsofLREC2014
,pages216Œ223.ELRA.
[Miller1995]
GeorgeA.Miller.1995.WordNet:alexicaldatabaseforEnglish.
CommunicationsoftheACM
,
38(11):39Œ41.
[Mironencoetal.2017]
M.Mironenco,D.Kianfar,K.Tran,E.Kanoulas,andE.Gavves.2017.Examiningcoop-
erationinvisualdialogmodels.In
NeuralInformationsProcessingSystems,WorkshoponVisually-Grounded
InteractionandLanguage
.
[NairandHinton2010]
VinodNairandGeoffreyEHinton.2010.linearunitsimproverestrictedboltz-
mannmachines.In
Proceedingsofthe27thinternationalconferenceonmachinelearning(ICML-10)
,pages
807Œ814.
[Nangiaetal.2017]
NikitaNangia,AdinaWilliams,AngelikiLazaridou,andSamuelRBowman.2017.The
RepEval2017SharedTask:Multi-GenreNaturalLanguageInferencewithSentenceRepresentations.
arXiv
preprintarXiv:1707.08172
.
[Passonneauetal.2006]
R.Passonneau,N.Habash,andO.Rambow.2006.Inter-annotatoragreementonamulti-
lingualsemanticannotationtask.In
ProceedingsoftheInternationalConferenceonLanguageResourcesand
Evaluation(LREC)
,pages1951Œ1956.
[Penningtonetal.2014]
JeffreyPennington,RichardSocher,andChristopherDManning.2014.GloVe:Global
VectorsforWordRepresentation.In
Proceedingsofthe2014ConferenceonEmpiricalMethodsinNatural
LanguageProcessing(EMNLP'14)
,pages1532Œ1543,Doha,Qatar.AssociationforComputationalLinguistics.
[Sammonsetal.2012]
MarkSammons,VinodVydiswaran,andDanRoth.2012.RecognisingTextualEntailment.
InDanielM.BikelZitouniandImed,editors,
MultilingualNaturalLanguageProcessing
,pages209Œ281.IBM
Press,Westford,MA.
[SimonyanandZisserman2014]
KarenSimonyanandAndrewZisserman.2014.Verydeepconvolutionalnet-
worksforlarge-scaleimagerecognition.
arXivpreprintarXiv:1409.1556
.
[Sitzmannetal.2016]
VincentSitzmann,MartinaMarek,andLeonidKeselman.2016.Multimodalnaturallan-
guageinference.Technicalreport,Standford.
[Srivastavaetal.2014]
NitishSrivastava,GeoffreyHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhut-
dinov.2014.Dropout:Asimplewaytopreventneuralnetworksfromov
TheJournalofMachine
LearningResearch
,15(1):1929Œ1958.
[StarcandMladeni
´
c2017]
JanezStarcandDunjaMladeni
´
c.2017.ConstructingaNaturalLanguageInference
datasetusinggenerativeneuralnetworks.
ComputerSpeechandLanguage
,46:94Œ112.
[Suhretal.2017]
AlaneSuhr,MikeLewis,JamesYeh,andYoavArtzi.2017.Acorpusofnaturallanguagefor
visualreasoning.In
Proceedingsofthe55thAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume2:ShortPapers)
,pages217Œ223,Vancouver,Canada,July.AssociationforComputationalLinguistics.
[Tanetal.2016]
MingTan,CicerodosSantos,BingXiang,andBowenZhou.2016.LSTM-basedDeepLearning
ModelsforNon-factoidAnswerSelection.
arXivpreprintarXiv:1511.04108
,pages1Œ11.
[Teneyetal.2017]
DamienTeney,PeterAnderson,XiaodongHe,andAntonvandenHengel.2017.Tipsandtricks
forvisualquestionanswering:Learningsfromthe2017challenge.
arXivpreprintarXiv:1708.02711
.
[Vendrovetal.2016]
IvanVendrov,RyanKiors,SanjaFidler,andRaquelUrtasun.2016.Order-embeddingsof
imagesandlanguage.In
ProceedingsoftheInternationalConferenceofLearningRepresentations(ICLR)
.
[Wangetal.2017]
ZhiguoWang,WaelHamza,andRaduFlorian.2017.BilateralMulti-PerspectiveMatchingfor
NaturalLanguageSentences.In
ProceedingsoftheTwenty-SixthInternationalJointConferenceon
Intelligence(IJCAI'17)
,pages4144Œ4150,Melbourne.
[Williamsetal.2018]
AdinaWilliams,NikitaNangia,andSamuelR.Bowman.2018.ABroad-CoverageChal-
lengeCorpusforSentenceUnderstandingthroughInference.In
ProceedingsofNAACL
.
[Xuetal.2015]
K.Xu,J.L.Ba,R.Kiros,K.Cho,A.Courville,R.Salakhutdinov,R.Zemel,andY.Bengio.2015.
Show,attendandtell:Neuralimagecaptiongenerationwithvisualattention.In
ProceedingsoftheInternational
ConferenceonMachineLearning(ICML)
.
[Youngetal.2014]
PeterYoung,AliceLai,MicahHodosh,andJuliaHockenmaier.2014.Fromimagedescriptions
tovisualdenotations:Newsimilaritymetricsforsemanticinferenceovereventdescriptions.
Transactionsof
theAssociationforComputationalLinguistics
,2:67Œ78.
"
85,Evaluation of Unsupervised Compositional Representations,http://arxiv.org/pdf/1806.04713v2.pdf,https://github.com/h-aldarmaki/sentence_eval,"EvaluationofUnsupervisedCompositionalRepresentations
HananAldarmaki
TheGeorgeWashingtonUniversity
aldarmaki@gwu.edu
MonaDiab
TheGeorgeWashingtonUniversity
mtdiab@gwu.edu
Abstract
Weevaluatedvariouscompositionalmodels,frombag-of-wordsrepresentationstocompositional
RNN-basedmodels,onseveralextrinsicsupervisedandunsupervisedevaluationbenchmarks.
Ourresultsthatweightedvectoraveragingcanoutperformcontext-sensitivemodelsin
mostbenchmarks,butstructuralfeaturesencodedinRNNmodelscanalsobeusefulincertain
tasks.Weanalyzedsomeoftheevaluationdatasetstoidentifytheaspectsofmean-
ingtheymeasureandthecharacteristicsofthevariousmodelsthatexplaintheirperformance
variance.
1Introduction
Distributedsemanticmodelsforwordsencodelatentfeaturesthatsemanticaspectsandcorre-
lationsamongwords.Thegoalofcompositionalsemanticmodelsistoinducelatentsemanticrepre-
sentationsthatencodethemeaningofphrases,sentences,andparagraphsofvariablelengths.Some
neuralarchitecturessuchasconvolutional(Kim,2014)andrecursivenetworks(Socheretal.,2013)han-
dlevariable-lengthinputbyidentifyingshift-invariantfeaturessuitablefortheproblemat
hand,whichmakesitpossibletoskipcompositionandworkdirectlywiththeentirespaceofindividual
wordembeddings.Whilesuchmodelscanachieveexcellentperformanceinsupervised
taskssuchassentimentanalysis,weareinterestedingenericunsuperviseded-lengthrepresentations
forvariable-lengthtextsequencessoastoefpreserveessentialsemanticcontentforlaterusein
varioussupervisedandunsupervisedsettings.
Binarybag-of-wordsaresimpleandeffectiverepresentationsthatserveasastrongbaselineinseveral
benchmarks(WangandManning,2012).However,theydonotexploitthedistributionalre-
lationshipsamongdifferentwords,whichlimitstheirapplicabilityandgeneralizationwhentrainingdata
arescarce.Additivecompositionalfunctions,suchaswordvectorsumoraverage,aremoreeffective
insemanticsimilaritytasksevenwhencomparedwithtensor-basedcompositionalfunctions(Milajevs
etal.,2014)andcanoutperformmorecomplexandbettertunedmodelsbasedonrecurrentneuralar-
chitecturesonout-of-domaindata(Wietingetal.,2015a).Yet,averagingalsohasseveraldrawbacks:
unlikebinaryrepresentations,theindividualwordidentitiesarelost,andsomewordsthatdonotcarry
semanticmayendupbeingmoreprominentlyrepresentedthanessentialwords.Further-
more,additivecompositionalmodelsdisregardsentencestructureandwordorder,whichcanleadto
lossofsemanticnuance.Toalleviatetheissue,theweightsofvariouswordscanbeadjustedusing
wordfrequencystatistics(Riedeletal.,2017)orbyinducingcontext-sensitiveweightsusingrecurrent
neuralnetworks(WietingandGimpel,2017),bothofwhichhavebeenshowntooutperformvectoraver-
aging.Context-sensitivefeed-forwardneuralmodelsliketheparagraphvector(LeandMikolov,2014)
potentiallyincorporatewordorder,yetthetrainingobjectivemaynotbesuftomodeldeeperstruc-
ture.Sequenceencoder-decodermodels,ontheotherhand,canbetrainedwithvarioussentence-level
objectives,suchasneuralmachinetranslation(NMT)(Sutskeveretal.,2014),predictingsurrounding
ThisworkislicensedunderaCreativeCommonsAttribution4.0InternationalLicense.Licensedetails:
http://
creativecommons.org/licenses/by/4.0/
arXiv:1806.04713v2  [cs.CL]  14 Jun 2018sentences(i.e.skip-thought)(Kirosetal.,2015),orreconstructionoftheinputusingdenoisingauto-
encoders(Hilletal.,2016).Thesesequentialmodelshavebeenevaluatedandcomparedagainstother
modelsandbaselinesonseveralsupervisedandunsupervisedtasksinHilletal.(2016).Thedenoising
autoencodermodelandskip-thoughtbothperformedwellinsupervisedtasks,whiletheNMTmodel
performedworsethanthebaselines.Allthreeperformedpoorlyinunsupervisedsettings.
Tobridgesomeofthegapsinevaluation,weevaluatedasubsetofmodelswithincreasingcomplexity,
frombinarybag-of-wordstoRNNs,onvarioussupervisedandunupervisedsettings.Ourobjectiveis
toevaluatecompositionalmodelsagainststrongbaselinesandidentifytheelementsthatleadtoperfor-
mancegains.Weevaluatedbinaryvs.distributedfeatures,weightedvs.unweightedaveraging,three
differentwordembeddingmodels,andfourcontext-sensitivemodelsthatoptimizedifferentobjectives:
theparagraphvector,thegatedrecurrentaveragingnetwork(WietingandGimpel,2017),skip-thought,
andanLSTMencodertrainedonlabelednaturallanguageinferencedata(inferSent)(Conneauetal.,
2017).Wealsoanalyzedtheintrinsicstructuresofthevariousmodelsbyvisualinspectionandk-means
clusteringtogaininsightsintostructuraldifferencesthatmayexplainthevarianceinperformance.
2Background:UnsupervisedCompositionalModels
2.1Baselines
Thesimplestwayofrepresentingasentenceisabinarybag-of-wordsrepresentation,whereeachword
isafeatureinthevectorspace.Thisresultsinlargeandsparserepresentationsthatonlyaccountfor
theexistenceofindividualwordswithinasentence,yettheyhavebeenshowntobeeffectiveinvarious
supervisedtasks,especiallyincombinationwith
n
-gramsandNaiveBayes(NB)features
(WangandManning,2012).Let
~x
i
bethebinaryrepresentationofsentence
i
,and
y
i
2f
0
;
1
g
itslabel.
Thelog-countratio
~r
iscalculatedas
~r
=log
~p=
k
~p
k
~q=
k
~q
k
(1)
Where
~p
=1+
P
i
:
y
i
=1
~x
i
and
~q
=1+
P
i
:
y
i
=0
~x
i
arethesmoothedcountvectorsforeachclass
(i.e.thenumberofsamplesintheclassthatincludeeachfeature).Thefeaturevectorsarethen
usingtheelement-wiseproduct
~x
i

~r
.NBfeaturesidentifythemostdiscriminativewordsforeachtask,
sousingthemresultsinratherthangeneralrepresentations.However,giventherelative
efyofthismodel,weincludeitasabaselineforcomparison.
2.2WordEmbeddingsandCompositionFunctions
Representationsofvariable-lengthsentencesandparagraphscanbeconstructedbyaveragingtheem-
beddingsofallwordswithinasentence.However,simpleaveragingmaynotbethebestapproachsince
notallwordswithinasentencearesemanticallyrelevant.Thefollowingmethodscanbeusedtoadjust
theweightsofwordsaccordingtotheirfrequency,assumingthatfrequentwordshavelowersemantic
content:
tf-idf
-weightedAverage
The
termfrequency-inversedocumentfrequency
statisticmeasuresthe
importanceofawordtoadocument.Wetreateachsentenceasadocumentandcalculatethe
idf
weightforterm
t
asfollows:
idf
t
=
log
N
1+
n
t
(2)
whereNisthetotalnumberofsentencesand
n
t
thenumberofsentencesinwhichthetermappears.
Termsthatappearinmoredocumentshavelower
idf
weights.
sif
-weightedAverage
The
smoothinversefrequency
(Riedeletal.,2017)isanalternativemeasure
fordiscountingtheweightsoffrequentwordsasfollows:
sif
t
=
a
a
+
p
(
t
)
(3)
where
a
isasmoothingparameterand
p
(
t
)
istherelativefrequencyoftheterminthetrainingcorpus.
Inaddition,asproposedin(Riedeletal.,2017),wesubtracttheprojectionofthevectorsonthe
principalcomponentwhichcorrespondstosyntacticfeaturesassociatedwithcommonwords.
2.2.1WordEmbeddings
Randomwordprojections:
wegeneratedarandomvectordrawnfromthestandardnormaldistri-
butionforeachwordinthevocabulary.Thevectorsumofrandomwordvectorsisalow-dimensional
projectionofbinarybag-of-wordsvectors..
ContinuousBagofWords:
CBOW
isaneflog-linearmodelforlearningwordembeddingsusing
afeed-forwardneuralnetworkthatpredictsawordgiventhesurroundingwordswithinaed
contextwindow(Mikolovetal.,2013a).InSchnabeletal.(2015)wordembeddingevaluation,
CBOW
outperformedotherwordembeddingsinwordrelatednessandanalogytasks.
GlobalVectors:
GloVe
isagloballog-bilinearregressionmodel(Penningtonetal.,2014)thatpro-
duceswordembeddingsusingweightedmatrixfactorizationofwordco-occurrenceprobabilities.
SubwordInformationSkip-gram
si-skip
learnsrepresentationsfor
n
-gramsofvariouslengths,
andwordsarerepresentedassumsof
n
-gramrepresentations(Bojanowskietal.,2017).Thelearning
architectureisbasedonthecontinuousskip-grammodel(Mikolovetal.,2013b),whichistrainedby
maximizingtheconditionalprobabilityofcontextwordswithinaedwindowwithnegativesampling.
Themodelexploitsthemorphologicalvariationswithinalanguagetolearnmorereliablerepresentations,
particularlyforraremorphologicalvariants.
2.3NeuralCompositionalModels
Severalmodelshavebeenproposedtoovercomesomeoftheweaknessesofbag-of-wordsandadditive
representations,suchaslackofstructure.Weevaluatedthefollowingcontext-sensitivemodels:
TheParagraphVector:
doc2vec
distributedmemorymodel(LeandMikolov,2014)constructsrep-
resentationsforsentencesandparagraphsusinganeuralfeedforwardnetworkthatmaximizesthecon-
ditionalprobabilityofwordswithinaparagraphgivenacontextwindowandtheparagraphembedding,
whichissharedforallcontextsgeneratedfromthesameparagraph.Afterlearningwordandparagraph
embeddingsforthetrainingcorpus,themodellearnsrepresentationsfornewparagraphsbythe
modelparametersandupdatingtheparagraphembeddingsusingbackpropagation.Thisadditionaltrain-
ingatinferencetimeconsiderablyincreasesthetimecomplexityofthemodelcomparedtoallothersin
thisstudy.
GatedRecurrentAveragingNetwork:
GRAN
hasbeenrecentlyintroducedtocombinethe
ofLSTMnetworksandaveraging,wheretheweightsarecomputedalongwiththewordandsentence
representations(WietingandGimpel,2017).Themodelistrainedusingalignedsentencesthatareas-
sumedtobeparaphrasestomaximizethesimilarityoftheirrepresentationsagainstnegativeexamples.
Theintuitionistomaketheaveragingoperationcontext-sensitive,resultinginamorepowerfulconstruc-
tionthansimpleaveragingwhereallwordsareequallyimportant.Themodelwasshowntooutperform
averagingandLSTMmodelsinsemanticrelatednesstasks.
Skip-Thought:
The
skip-th
modelisasequenceencoder-decodertrainedbyprojectingsentences
intoed-lengthvectors,whichinturnareusedasinputtoadecoderthatistrainedtoreconstructsur-
roundingsentences(Kirosetal.,2015),wheretheencoderanddecoderareRNNswithGRUactivations
(Chungetal.,2014).Themodelistrainedwithcontiguoussentencesextractedfromacollectionofnov-
els.Aftertraining,themodel'svocabularyisexpandedbylearningalinearmappingfrompre-trained
CBOW
wordembeddingstothevectorspaceofthe
skip-th
wordembeddings.
NaturalLanguageInferenceEncoder:
In
inferSent
(Conneauetal.,2017),abidirectionalLSTM
encoderwithmax-poolingistrainedjointlywithaninferencetrainedontheStanfordNatural
LanguageInference(SNLI)dataset(Bowmanetal.,2015),whichisalargemanually-annotateddataset
ofEnglishsentencepairsandtheirinferencelabels:
f
entailment,contradition,neutral
g
.
Dataset
Pair-wiseSimilarity
SentimentAnalysis
Newsgroup
TREC
STS
SICK
MSRP
CR
MPQA
RT-s
Subj
IMDB
REL
SPO
COM
POL
Train
5,749
4,934
4,076
3,775
10,606
10,662
10,000
25k
1,078
1,604
1,694
1,310
5,452
Test
1,379
4,906
1,725
Œ
Œ
Œ
Œ
25k
Œ
Œ
Œ
Œ
500
posratio
Œ
Œ
0.66
0.64
0.31
0.50
0.50
0.50
0.58
0.51
0.51
0.52
Œ
l
12
10
23
21
3
21
24
262
82
82
82
92
10
Table1:Datasetstatistics.
Train
:numberofsamplesinthetrainingset.
Test
:numberofsamplesin
thetestset,ifapplicable(CVisappliedotherwise).
posratio
:ratioofpositivesamplesinthetestset
(ortotalfordatasetswithnosplits).
l
:averagelengthofallsamples.
3EvaluationDatasets
Toevaluatethetextrepresentations,weusedthemasfeaturesinextrinsicsupervisedandunsupervised
tasksthatvarioussemanticaspects,whichcanbegroupedinthreecategories:pairwise-similarity,
sentimentanalysis,andcategorization.AsummaryofthedatasetstatisticsisinTable1.
1
3.1PairwiseSimilarity
SemanticTextualSimilarity:
theSTSbenchmarkdataset(Ceretal.,2017)includesacollectionof
Englishsentencepairsandhuman-annotatedsimilarityscoresthatrangefrom0(unrelatedsentences)to
5(paraphrases).Thedatasetincludestraining,development,andtestsets.Thistaskcanbeperformed
withoutsupervisionbycalculatingthecosinesimilaritybetweentwosentencevectors.Wealsoevaluated
themodelsinasupervisedsettingsusinglinearregression,wheretheinputvectorisaconcatenationof
theelement-wiseproduce
u:v
andabsolutedifference
j
u

v
j
ofeachpair
h
u;v
i
.
SentencesInvolvingCompositionalKnowledge:
SICKdatasetisabenchmarkforevaluatingcom-
positionalmodels(Marellietal.,2014).Weevaluatedthemodelsontherelatednesssubtask,whichis
constructedinasimilarmannerasSTSbenchmark.
ParaphraseDetection:
Thisisabinarytaskthatinvolvestheofpara-
phrasesinsimilarsentencepairsusingtheMicrosoftResearchParaphraseCorpus,MSRP(Dolanetal.,
2004).Weevaluatedthemodelsintwoways:calculatingthecosinesimilaritybetweenthesentencepairs
andclassifyingthemasparaphrasesifthesimilarityislargerthanathresholdtunedfromthetrainingset.
Thesecondapproachistolearnalogisticregressionusingaconcatenationof
u:v
and
j
u

v
j
.
3.2SentimentAnalysisandTextCategorization
SentimentAnalysis:
Weusedthefollowingbinaryclasstasks:
CR
customerproductreviews
(HuandLiu,2004),
MPQA
opinionpolaritysubtask(Wiebeetal.,2005),
RT-s
shortmoviereviews
(PangandLee,2005),
Subj
subjectivity/objectivitytask(PangandLee,2004),and
IMDB
full-lengthmoviereviewdataset(Maasetal.,2011).
Newsgroups:
Followingthesetupin(WangandManning,2012)weusedthe20-Newsgroupdataset
2
toextractseveralbinarytopiccategorizationtasks.Weprocessedthedatasetstoremoveheaders,
forwardedtext,andsignatures,whichresultsinsmallersentencesandparagraphs.Weusedthefollowing
newsgroupsforbinary
religion
(atheismvs.religion),
sports
(baseballvs.hockey),
computer
(windowsvs.graphics),and
politics
(middleeastvs.guns).Wealsotrainedmulti-class
onthe8newsgroups.
Question
WeusedtheTREC10coarsequestioncategorizationtask
3
whichcatego-
rizesquestionsinto6classes:human(HUM),entity(ENTY),location(LOC),number(NUM),descrip-
tion(DESC),andabbreviation(ABBR).
1
Evaluationscriptsanddatacanbedownloadedfrom:https://github.com/h-aldarmaki/sentence
eval
2
http://qwone.com/
˘
jason/20Newsgroups/
3
http://cogcomp.org/Data/QA/QC/
4ExperimentalSetup
4.1TrainingData
Wetrainedtheunsupervisedwordembeddingmodels
CBOW
,
GloVe
,and
si-skip
onasetof
˘
7
millionsentencesextractedfromtheEnglishWikipediaandAmazonmovieandproductreviews(Heand
McAuley,2016).WealsotrainedtheParagraphVector(
doc2vec
)modelonthisdataset,andinitialized
thewordembeddingsusingthe
si-skip
pre-trainedwordembeddingsabove.Whilebetterresults
overallcouldbeobtainedusingpre-trainedwordembeddingstrainedwithmuchlargertextcorpora,we
usedthismedium-sizecorpustoevaluatethevariousmodelsconsistentlyandreducemodelvariability
duetodataandvocabularycoverage.
Weusedthepubliclyavailablepre-trained
GRAN
4
and
skip-th
5
models,whichrequiretrainingwith
specialtypesofdatasets:paraphrasecollections,andcontiguoustextfrombooks,respectively.Toensure
afairevaluation,weonlycomparedthesemodelsagainstbinarybag-of-wordsandequivalentword
embeddings.Thewordembeddingswithinthe
GRAN
modelwereinitializedwith
PARAGRAM-SL999
wordvectors(Wietingetal.,2015b),soweusedthemasanevaluationbaselinefor
GRAN
.Wecompared
skip-th
againstthe
CBOW
embeddingsthatwereusedtoexpandthevocabulary,whichaccountfor
mostwordsinthemodel'svocabulary.Weusedthepre-trained
inferSent
model
6
whichuses
pre-trainedGloVewordembeddings
7
.Wealsoexperimentedwiththepost-trainedwordembeddings
foreachmodelwithsimilarresults,soweomittedthemforbrevity.
4.2TrainingSettings
Wetrainedtheunsupervisedwordembeddingmodelsusingtheoptimalparametersrecommendedfor
eachmodel.Thehyper-parametersin
doc2vec
weresetaccordingtotherecommendationsin(Lauand
Baldwin,2016).Forthesupervisedsentimentandtextcategorizationtasks,wetrainedand
tunedlinearSVMmodelsusinggridsearchfordatasetsthatincludetrain/dev/testsplits,andnestedcross-
validationotherwise.WealsoexperimentedwithkernelSVMsbutdidn'tobservenotabledifferencesin
theresults.
5EvaluationResults
5.1PairwiseSimilarityEvaluation
0
:
5
0
:
65
0
:
5
0
:
73
0
:
74
0
:
6
a:
wordoverlap
0
:
63
b:
doc2vec
0
:
2
c:
skip-th
0
:
68
d:
sif
y
0
:
69
e:
inferSent
Figure1:Scatterplotsofnormalizedgoldscoresinthexaxisvs.(a)wordoverlap(%)and(b-e)
cosinesimilarityusingvariousmodels.Top:SICK.Bottom:STSBenchmark.Pearson
ˆ
plottedinred
for
score

2
,
score
2
(2
;
4)
,and
score

4
.OverallPearson
ˆ
shownatthetop.
y
sif-weightedaverage
ofpre-trainedGlovevectorsusedininferSent.
4
https://github.com/jwieting/acl2017
5
https://github.com/ryankiros/skip-ths
6
https://github.com/facebookresearch/InferSent
7
https://nlp.stanford.edu/projects/glove
STSBenchmark
ˆ
SICK
ˆ
MSRPaccuracy/F1
cosine
linearreg.
cosine
linearreg.
cosine
logisticreg.
BinaryBOW
0.536
0.606
0.611
0.761
66.9/0.765
72.2/0.811
ParagraphVector(doc2vec)
0.628
0.673
0.654
0.655
68.6/0.797
70.4/0.803
Random
avg
0.558
0.616
0.602
0.669
70.6/0.780
70.8/0.797
idf
0.668
0.665
0.617
0.659
70.0/0.790
69.1/0.791
sif
0.666
0.665
0.628
0.655
70.1/0.786
69.9/0.699
CBOW
avg
0.630
0.672
0.679
0.728
71.9/0.815
72.0/0.807
idf
0.697
0.695
0.678
0.712
71.8/0.815
72.3/0.809
sif
0.683
0.686
0.690
0.715
72.2/0.814
71.4/0.804
GloVe
avg
0.336
0.574
0.602
0.694
68.9/0.807
71.0/0.810
idf
0.540
0.656
0.624
0.685
71.3/0.818
73.4/0.820
sif
0.685
0.665
0.701
0.695
71.8/0.809
72.1/0.811
si-skip
avg
0.608
0.690
0.684
0.730
72.1/0.817
71.9/0.895
idf
0.683
0.714
0.702
0.715
69.3/0.809
70.3/0.815
sif
0.694
0.721
0.716
0.721
70.6/0.804
70.6/0.802
GRAN
0.747
0.747
0.715
0.756
71.3/0.817
72.3/0.812
Pre-trained
PARAGRAM-SL999
y
avg
0.564
0.690
0.694
0.746
71.8/0.818
73.2/0.816
idf
0.711
0.733
0.723
0.756
72.1/0.817
73.3/0.817
sif
0.716
0.722
0.733
0.765
73.4/0.822
72.0/0.809
skip-th
0.213
0.729
0.498
0.811
62.3/0.761
73.0/0.812
Pre-trained
CBOW
y
avg
0.631
0.695
0.727
0.758
70.3/0.813
73.2/0.813
idf
0.674
0.708
0.710
0.731
69.6/0.809
71.3/0.807
sif
0.686
0.707
0.727
0.737
69.8/0.809
70.9/0.803
inferSent
0.692
0.773
0.744
0.865
0.697/0.806
0.746/0.827
Pre-trained
GloVe
y
avg
0.497
0.655
0.687
0.753
0.711/0.818
0.732/0.817
idf
0.606
0.688
0.696
0.736
0.688/0.809
0.711/0.804
sif
0.679
0.699
0.729
0.749
0.709/0.816
0.708/0.804
Table2:Pearson
ˆ
forSTSBenchmarkandSICKrelatedness,andAccuracy%/F1forMSRParaphrase
detection.ResultsareshadedaccordingtotheirstatisticalusingtheWilliamstest(Graham
andBaldwin,2014)with

=0
:
05
.
y
pre-trainedvectorsusedinthemodelabove.
Table2showstheperformanceofthevariousmodelsinthepair-wisesimilaritytasks.Wehighlightthe
bestperformanceineachblock;differenceswithinthesameshadearenotstatisticallyAmong
thewordembeddingmodels,thesubwordskipgram
si-skip
achievedthebestoverallperformance.
Forallmodelsexcept
si-skip
,simpleaveragingperformedpoorlyinsemanticrelatednesstasks,es-
peciallyintheunsupervisedsetting,while
sif
-weightinggenerallyoutperformed
idf
-weighting(the
improvementismostevidentfor
GloVe
).Asimilartrendisobservedwithrandomwordvectors,which
performedonparwith
doc2vec
.
Thebinarybag-of-wordsmodelperformedparticularlywellinthesupervisedSICKtask.Theper-
formanceofbinaryandrandomvectorscanbeexplainedbythehighcorrelationbetweentheper-
centageofoverlappingwordsandthesimilarityscoresasseeninFigure1.Wealsohighlightedthe
Pearsoncorrelationcoefforthefollowingsubsetsofrelatednessscores:
A
=
f
score

2
g
,
B
=
f
score
2
(2
;
4)
g
,and
C
=
f
score

4
g
.TheoverallPearsoncorrelationmostlythe
performanceonthemostandleastsimilarpairs,whichtendtobethepairswiththehighestandlowest
wordoverlap,respectively;withintheregionswehighlighted,allcorrelationswererelativelylow.How-
ever,distributedmodelslike
sif
and
inferSent
improvedthecorrelationof
A
forSICK,andboth
A
and
C
forSTSBenchmark.Table3showssomeexamplesofsentencepairsfrom
A
and
C
;whileboth
sif
and
doc2vec
vectorsconsistentlysimilarconcepts(namelyfoodrelatedandcompeti-
tionconcepts)regardlessofsurfacesimilarity,binaryscoresonlythelexicalsimilarity,which
resultedininconsistentscores.
Sentence1Sentence2ScoreBinarysi-sif
y
doc2vecskip-thinfergl-sif
y
A
person
is
frying
somefood
.There
is
no
person
peeling
apotato
.0.250.410.490.410.510.670.71
Awoman
is
cuttinga
.Themain
is
slicingpotatos
.0.250.130.460.420.530.650.57
Two
groupsofpeople
are
playing
footballTwo
team
are
competing
ina
football
match.0.930.520.740.720.600.790.71
Different
teamsareplaying
football
onthe

Two
teamsareplaying
soccer
0.700.560.930.890.510.820.91
Table3:ExamplesofsentencepairsinSICKandtheirrelatednessscoresvs.cosinesimilarityscores.
Allscoreswerenormalizedtobein[0,1].Sharedwordsareshowninboldandrelatedwordsunderlined.
y
sif-weightedaverageof
si-skipgram
andpre-trained
GloVe
Sentence1Sentence2Label
Bashirfelthewasbeingtriedbyopin-
ion
notonthe
facts
,Mahendradattatold
Reuters.
Bashir
also
felthewasbeingtriedbyopin-
ion
ratherthan
facts
oflaw,headded.
1
WestNileVirus
-which
isspread
through
infected
mosquitoes
-ispotentiallyfatal.
WestNile
isabird
virus
thatis
spread
topeo-
pleby
mosquitoes
.
0
SCOsays
the
pricing
termsforalicense
will
notbeannounced
forweek
Detailson
pricingwill
beannounced
withina
fewweeks
,McBridesaid
1
RussBritt
is
theLosAngeles
BureauChief
for
CBS.MarketWatch.com.
EmilyChurch
is
London
bureauchief
of
CBS.MarketWatch.com.
0
Table4:ExamplesofsentencepairsinMSRPandtheirlabels.
Sharedwordsareshowninboldandrelatedwordsunderlined.
Figure2:Ratioofpara-
phraseswithincreasingword
overlapinMSRP.
GRAN
outperformedsimpleaveraginginboththeSTSandSICKtasks,whichtheresults
in(WietingandGimpel,2017),butcomparedwith
idf
and
sif
averaging,thereisnoapparent
improvement;itonlyoutperformedweightedaveragingintheunsupervisedSTSbenchmark.
skip-th
vectorsperformedpoorlyintheunsupervisedsimilaritytasks,butoutperformedthepre-trainedvectors
inthesupervisedsimilaritytasks,particularlyinSICK.
Thelowvarianceoftheperformanceintheparaphrasedetectiontaskalsostheoverallcorre-
lationbetweenwordoverlapandthelikelihoodofbeingaparaphraseasseeninFigure2;fordif
cases,asintheexamplesinTable4,theoverallsimilarityisnotagoodindicationofbeingaparaphrase.
improvementsinthistaskmayrequiremorenuancedfeaturesasin(JiandEisenstein,2013).
5.2EvaluationonSentimentAnalysisandCategorization
SentimentAnalysis
TextCategorization
CR
mpqa
RT-s
subj
imdb
rel
spo
com
pol
mult
TREC
BinaryBOW
77.0/0.821
85.9/0.756
74.8
89.5
84.1
66.2/0.710
85.7
78.2
81.6
72.2
89.8
UnigramNBSVM
80.5
85.3
78.1
92.4
88.3
73.2
93.5
86.7
91.9
93.4
89.8
ParagraphVector(doc2vec)
76.6/0.831
82.4/0.688
78.6
89.9
87.8
68.9/0.751
89.6
82.3
90.6
76.1
59.6
si-skip
avg
81.3/0.857
87.2/0.778
78.8
91.6
88.0
67.1/0.759
87.8
80.3
87.2
74.4
79.6
idf
80.2/0.849
86.2/0.765
78.5
91.0
87.0
69.1/0.755
88.3
81.9
89.7
75.0
70.4
sif
80.6/0.852
86.7/0.774
78.6
91.0
88.2
69.1/0.765
88.8
81.0
89.4
74.7
71.8
CBOW
avg
81.6/0.859
86.1/0.759
78.1
91.0
87.2
63.6/0.745
74.9
78.2
84.7
66.6
82.8
idf
81.2/0.856
86.0/0.761
77.7
90.5
87.3
65.8/0.745
78.6
79.8
85.6
68.2
77.4
sif
80.8/0.852
85.7/0.756
77.8
90.2
87.4
65.6/0.742
80.0
79.5
85.
68.3
76.2
GloVe
avg
80.7/0.851
85.4/0.745
77.6
91.0
87.3
67.1/0.748
82.1
78.8
86.5
69.3
79.8
idf
80.7/0.851
85.8/0.756
77.8
90.8
87.5
65.5/0.725
85.2
77.7
87.6
70.9
72.4
sif
80.6/0.850
85.4/0.751
77.6
90.7
87.4
66.8/0.736
85.8
78.5
87.5
70.9
72.0
Random
avg
70.7/0.779
74.2/0.413
61.9
77.3
74.0
55.8/0.634
71.4
72.8
69.4
47.
70.2
idf
68.6/0.763
74.1/0.423
61.1
72.7
74.2
58.9/0.654
74.4
72.7
72.9
47.2
57.0
sif
69.2/0.770
72.7/0.369
61.5
69.6
74.5
59.4/0.655
73.0
72.1
72.6
47.4
54.8
GRAN
78.4/0.838
86.6/0.769
75.1
88.5
83.1
66.0/0.753
90.6
80.8
88.5
73.2
60.4
pre-trained
PARAGRAM-
SL999
y
avg
79.8/0.845
87.8/0.794
75.9
89.6
84.5
65.3/0.721
89.8
78.9
87.5
72.5
83.2
idf
79.1/0.840
87.4/0.791
75.8
89.3
84.1
68.2/0.737
90.3
79.8
89.0
74.1
74.6
sif
79.2/0.840
87.5/0.789
75.8
88.7
84.6
69.0/0.747
89.7
79.9
88.9
73.1
75.0
skip-th
80.4/0.851
87.0/0.78
76.4
93.4
81.8
65.5/0.736
70.4
69.4
81.5
60.1
88.2
Pre-trained
CBOW
y
avg
79.9/0.847
88.2/0.800
77.5
90.5
85.6
64.8/0.751
86.6
79.5
85.9
70.7
80.0
idf
79.6/0.844
87.9/0.797
77.2
90.0
85.6
68.4/0.766
87.5
80.5
85.8
72.6
72.2
sif
79.2/0.842
87.7/0.794
77.0
89.7
85.8
67.7/0.761
87.8
81.3
86.9
72.9
74.2
inferSent
83.0/0.867
88.5/0.811
77.1
91.0
86.4
68.5/0.731
88.6
80.9
85.2
74.4
88.6
Pre-trained
GloVe
y
avg
80.6/0.851
87.9/0.793
77.1
90.9
85.7
69.5/0.759
90.7
83.8
88.3
75.8
82.2
idf
79.8/0.844
87.4/0.788
77.2
90.1
85.5
69.7/0.757
89.8
83.0
88.8
76.9
75.4
sif
79.6/0.842
87.2/0.785
77.5
90.0
85.5
67.8/0.742
89.7
83.3
88.7
76.7
77.0
Table5:Accuracy%oracuracy/F1(forunbalanceddatasets)onsentimentandtopiccategorization
tasks.Resultsareshadedaccordingtotheirstatisticalusingatwo-tailedtest
with

=0
:
05
.
y
pre-trainedwordembeddingsusedinthemodelabove
a:Random
b:Binary
c:avg
y
d:sif
y
e:doc2vec
f:GRAN
g:skip-th
h:inferSent
Figure3:t-SNEvisualizationsofvectorsonthe20-Newsgroupdatasets.
y
avg
and
sif
using
si-skip
.
Table5showstheperformanceinsentimentanalysisandcategorizationtasks.Unlikeinpair-wise
similarity,randomvectorsunderperformedtheNBSVManddistributedmodelsbyalargemargin.This
underscorestheimportanceofglobalanddistributedfeaturesinthesetasks.
si-skip
outperformed
otherwordembeddingmodels,butweobservenoadvantageforweightedvs.unweightedaveraging.
InTRECquestionandthesubjectivitybenchmarks,
avg
performedbetter
thanboth
idf
and
sif
weightedaveraging.
skip-th
vectorsalsooutperformedthe
pre-trainedvectorsinthesetwotasks,andunderperformedinallothers.Wesurmisethatthesyntactic
featuresconveyedinfrequentfunctionwordsandtheoverallstructureencodedbytheLSTMnetwork
in
skip-th
mayprovideusefulcluesforthesetwoclassitasks.
inferSent
achievedthe
highestaccuracyinCRsentimenttask,andonparwith
skip-th
andNBSVMinTREC,anditslightly
underperformedtheaveragingmodelsinNewsgroupcategorization.Inthenextsection,weanalyzethe
NewsgroupandTRECdatasetstoshedlightonintrinsiccharacteristicsthatmayexplainsomeofthe
performancevariance.
6QualitativeAnalysis
Figure3showst-SNEvisualizationsoftheNewsgroupdatasetsusingthevariouscompositionalmodels,
includingrandomandbinaryvectors.Whilerandomandbinaryvectorscouldidentifyshallowsimilari-
tiesbetweensentencesasinSTStasks,theyfailedtodosoinagloballycohesivemanner.Therandom
vectorsalsointroducednoiseintherepresentations,whichresultedinaratheruniformvectorspace.All
othermodels,except
skip-th
,clearlyseparatedatleastthreeregionsthatcorrespondtothecategories
sport
,
computer
,and
religion/politics
.Smallerclusterswithconsistentlabelingcanalsobe
withminimalseparationbetweentheclusters.
Table6showsexamplesofnearestneighborsusingsomeofthemodels.
skip-th
vectorsseemtobe
clusteredmorebystructurethansemanticcontent,unlikethe
doc2vec
and
sif
models.Toquantify
thesedifferences,weappliedk-meansclusteringusing
k
=3
and
k
=8
,andcalculatedtheclustering
purityforeachmodelasfollows:
P
(
C;L
)=
1
N
X
k
max

j
c
k
\
`
j
j
(4)
where
C
=
f
c
1
;:::;c
K
g
isthesetofclusters,
L
=
`
1
;:::`
K
isthesetoflabels,and
N
thetotalnumber
ofsamples.AsshowninTable7,using
doc2vec
andtheaveragingmodels,including
GRAN
,k-means
successfullyseparatedthe3categories,with
doc2vec
and
sif
outperforminginboththe
andcoarseclustering.
skip-th
clusters,ontheotherhand,didnotcorrespondwiththecorrectlabels,
underperformingbinaryandrandomfeatures,whichexplainsitsrelativelylowperformanceintextcat-
egorizationtasks.
inferSent
achievedhigherpuritythanbinary,randomand
skip-th
vectorsbut
lowerthantheothermodels,particularlywith
k
=3
.
sif
y
Andtheyworkespeciallywellwhenthe
Feds
have
cutoffyourutilities.
TheDividiansdidnothavethatoptionafterthe
FBIcutofftheirelectric-
ity
.
Notwhenthe
powerhasbeencutoff
forweeksonend.
What
doesthis
bill
do?
And
Bill
Jamesisnot?DoyouownﬂtheBillJamesplayers
ratingbookﬂ?
Who
hastoconsiderit?Thebeingthatdoestheaction?Iam
stillnotsureIknowwhatyouaretryingtosay.
doc2vec
Andtheyworkespeciallywellwhenthe
Feds
have
cutoffyourutilities
.
TheDividiansdidnothavethatoptionafterthe
FBIcutofftheirelectric-
ity
.
Canthe
Feds
gethimontaxevasion?Idonotrememberhearingabouthim
runningtothePostOflastnight.
Ididnot
claim
thatoursystemwas
objective
.
DidI
claim
thattherewasanabsolutemorality,orjustan
ob-
jective
one?
Ihavejustspenttwosolidmonths
arguing
thatnosuchthing
asan
objective
moralsystemexists.
skip-th
What
doesthisbilldo?
Where
doIgetholdofthesewidgets?
What
givestheUnitedStatestherighttokeepWashingtonD.C.?
What
makesyouthinkBuckwillstillbeinNewYorkatyear'sendwith
Georgeback?
Ihavejustspent
twosolidmonthsarguingthatnosuchthing
asanobjectivemoralsystemexists.
Theamountofenergybeingspentononelousysyllogismsays
volumesforthetruepositionofreasoninthisgroup.
Ijustheard
thisweekthathehasstartedoncompuserve
modelsforumnow.
Table6:Examplesofnearestneighborsinthe20-Newsgroupdataset.
y
sif
using
si-skip
.
Model
Newsgroup
TREC
k=3,C=3
k=8,C=8
k=6,C=6
Random
0.5563
0.2431
0.4424
Binary
0.6236
0.2963
0.444
avg
y
0.8465
0.3969
0.4481
sif
y
0.8776
0.4523
0.4037
doc2vec
0.8625
0.4967
0.3855
skip-th
0.4471
0.1854
0.3896
GRAN
0.8227
0.3553
0.3514
inferSent
0.6562
0.3801
0.4424
Table7:Clusteringpuritymeasurewithcoarsecategories(sports,computers,religion/politics)andthe
original8categoriesfortheNewsgroupdataset,and6categoriesforTREC.
y
avg
and
sif
using
si-skip
.
Figure4showst-SNEvisualizationsofthequestionsinTRECtrainingset.Whileallmodels
someofthecategories,like
HUM
and
LOC
,the
skip-th
andbinaryvectorsappearstobemore
cohesivelyclusteredbytypethantheothermodels.Thequestiontypesarescatteredinmultiplesmaller
clusters,however,whichexplainswhyk-meansclusteringresultedinlowerpurityscoresthan
doc2vec
and
averaging
with
k
=6
.InFigure5,purityresultswithvarious
k
areplotted.Whilepurity
isexpectedtoincreasewithlarger
k
,therateofincreaseismuchhigherfor
skip-th
thanallother
models,includingbinaryfeatures.Thisisconsistentwiththet-SNEvisualizationwhichshowsseveral
consistentclusterswith
skip-th
thatarelargerthanthebinaryclusters.
inferSent
'sperformance
wasonparwith
avg
,whichisslightlylowerthanbinaryand
skip-th
,althoughtheperformancein
thesupervisedsettingwasequivalent.
Table8showsnearestneighborstothequestionﬁWhatcountrydotheGalapagosIslandsbelongto?ﬂ
usingthevariousmodels.Theaveragingmodelclusteredquestionsaboutislands;weobservedsimilar
behaviorusingweightedaveraging,
doc2vec
and
GRAN
.Ontheotherhand,
skip-th
clusteredques-
tionsthatstartwithﬁwhatcountryﬂ,whichhappenstobemoresuitableforidentifyingthe
LOC
question
type.Usingbinaryvectors,questionsthatincludethewordsﬁWhatﬂandﬁcountryﬂwereclusteredto-
gether,whichdonotnecessarilycorrespondtothesamequestiontype.
inferSent
vectorsseemtobe
clusteredbyacombinationofsemanticandsyntacticfeatures.
6.1DiscussionandConclusions
Inthisstudy,weattemptedtoidentifyqualitativedifferencesamongthecompositionalmodelsandgen-
eralcharacteristicsoftheirvectorspacesthatexplaintheirperformanceindownstreamtasks.Identifying
thefeaturesthataremostusefulforeachtaskmayshedlightonthetypeofinformationthey
encodeandhelpoptimizetherepresentationsforourneeds.Wordvectoraveragingperformedreason-
ablywellinmostsupervisedbenchmarks,andweightedaveragingresultedinbetterperformancein
unsupervisedsimilaritytasksoutperformingallothermodels.Usingthesubwordskipgrammodelfor
wordembeddingsresultedinbetterrepresentationsoverall,particularlywith
sif
weighting.Theonly
modelthatperformedonparwithorslightlybetterthanweightedaveraginginunsupervisedSTSwas
a:Binary
b:avg
y
c:sif
y
d:GRAN
e:skip-th
f:doc2vec
g:inferSent
Figure4:t-SNEvisualizationsofvectorsontheTRECdataset.
y
avg
and
sif
using
si-skip
.
avg
y
WhatcurrentsaffecttheareaoftheShetlandIslandsand
OrkneyIslandsintheNorthSea?
WhattwoCaribbeancountriessharetheislandofHispan-
iola?
Binary
WhatisaFirstWorldcountry?
Whatisthebestcollegeinthecountry?
skip-th
Whatcountryistheworldsleadingsupplierofcannabis?
WhatcountrydidtheNileRiveroriginatein?
Whatcountryboaststhemostdams?
inferSent
WhatcountrydidtheancientRomansrefertoasHibernia?
HowmanyislandsdoesFijihave?
WhatcountrydoesIleanaCotrubascomefrom?
Table8:NearestneighborstoﬁWhatcountrydotheGala-
pagosIslandsbelongto?ﬂinTREC.
y
avg
using
si-skip
Figure5:Clusteringpuritywithincreas-
ingkonTREC.
[
avg
and
sif
for
si-skip
]
inferSent
.AllmodelsachievedhighercorrelationscoresinthesupervisedSTSevaluation,including
skip-th
,whichperformedpoorlyintheunsupervisedsetting.Thissuggeststhatatleastsomeofthe
featuresinthevectorspaceencodethesemanticcontentandtheremainingfeaturesareor
encodestructuralinformation.
doc2vec
and
GRAN
representationswerequalitativelysimilarto
idf
and
sif
vectorswheresen-
tences/paragraphswereclusteredbytopicandsemanticsimilarity.
skip-th
vectors,ontheotherhand,
seemedtoprominentlyrepresentstructuralratherthansemanticfeatures,whichmakesthemmoresuit-
ableforsupervisedtasksthatrelyonsentencestructureratherthanunsupervisedsimilarityortopic
categorization.
inferSent
vectorsperformedconsistentlywellinallevaluationbenchmarks,anda
qualitativeanalysisofthevectorspacesuggeststhatthevectorsencodeabalanceofsemanticandsyn-
tacticfeatures.Thismakes
inferSent
suitableasageneral-purposemodelforsentencerepresentation,
particularlyinsupervisedFortopiccategorization,noneofthecompositionalmodelsout-
performedtheNBSVMbaseline,whichachievedhigheraccuraciesinallsupervisedtopic
categorizationtasks.However,thedistributionalmodels,particularlyweightedaveraging,aremoresuit-
ableinunsupervisedorlow-resourcesettingssincesentencestendtobeclusteredcohesivelybytopic
similarityandsemanticrelatedness.
References
PiotrBojanowski,EdouardGrave,ArmandJoulin,andTomasMikolov.2017.Enrichingwordvectorswith
subwordinformation.
TransactionsoftheAssociationofComputationalLinguistics
.
SamuelRBowman,GaborAngeli,ChristopherPotts,andChristopherDManning.2015.Alargeannotated
corpusforlearningnaturallanguageinference.
Proceedingsofthe2015ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing
.
DanielCer,MonaDiab,EnekoAgirre,InigoLopez-Gazpio,andLuciaSpecia.2017.Semeval-2017task1:
Semantictextualsimilarity-multilingualandcross-lingualfocusedevaluation.
Proceedingsofthe10thInterna-
tionalWorkshoponSemanticEvaluation(SemEval2017)
.
JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio.2014.Empiricalevaluationofgated
recurrentneuralnetworksonsequencemodeling.
arXivpreprintarXiv:1412.3555
.
AlexisConneau,DouweKiela,HolgerSchwenk,Lo
¨
Barrault,andAntoineBordes.2017.Supervisedlearningof
universalsentencerepresentationsfromnaturallanguageinferencedata.In
Proceedingsofthe2017Conference
onEmpiricalMethodsinNaturalLanguageProcessing
.
BillDolan,ChrisQuirk,andChrisBrockett.2004.Unsupervisedconstructionoflargeparaphrasecorpora:Ex-
ploitingmassivelyparallelnewssources.In
Proceedingsofthe20thinternationalconferenceonComputational
Linguistics
.
YvetteGrahamandTimothyBaldwin.2014.Testingforofincreasedcorrelationwithhumanjudg-
ment.In
Proceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
,
pages172Œ176.
RuiningHeandJulianMcAuley.2016.Upsanddowns:Modelingthevisualevolutionoffashiontrendswith
one-classcollaborativeIn
proceedingsofthe25thinternationalconferenceonWorldWideWeb
.
FelixHill,KyunghyunCho,andAnnaKorhonen.2016.Learningdistributedrepresentationsofsentencesfrom
unlabelleddata.pages1367Œ1377.
MinqingHuandBingLiu.2004.Miningandsummarizingcustomerreviews.In
ProceedingsofthetenthACM
SIGKDDinternationalconferenceonKnowledgediscoveryanddatamining
.
YangfengJiandJacobEisenstein.2013.Discriminativeimprovementstodistributionalsentencesimilarity.In
Proceedingsofthe2013ConferenceonEmpiricalMethodsinNaturalLanguageProcessing
,pages891Œ896.
YoonKim.2014.Convolutionalneuralnetworksforsentence
Proceedingsofthe2014Conference
onEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
.
RyanKiros,YukunZhu,RuslanRSalakhutdinov,RichardZemel,RaquelUrtasun,AntonioTorralba,andSanja
Fidler.2015.Skip-thoughtvectors.In
Advancesinneuralinformationprocessingsystems
,pages3294Œ3302.
JeyHanLauandTimothyBaldwin.2016.Anempiricalevaluationofdoc2vecwithpracticalinsightsintodocu-
mentembeddinggeneration.
arXivpreprintarXiv:1607.05368
.
QuocLeandTomasMikolov.2014.Distributedrepresentationsofsentencesanddocuments.In
International
ConferenceonMachineLearning
,pages1188Œ1196.
AndrewLMaas,RaymondEDaly,PeterTPham,DanHuang,AndrewYNg,andChristopherPotts.2011.
Learningwordvectorsforsentimentanalysis.In
Proceedingsofthe49thannualmeetingoftheassociationfor
computationallinguistics:Humanlanguagetechnologies
.
MarcoMarelli,StefanoMenini,MarcoBaroni,LuisaBentivogli,RaffaellaBernardi,RobertoZamparelli,etal.
2014.Asickcurefortheevaluationofcompositionaldistributionalsemanticmodels.In
LREC
,pages216Œ
223.
TomasMikolov,KaiChen,GregCorrado,andJeffreyDean.2013a.Efestimationofwordrepresentations
invectorspace.
ICLR
.
TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean.2013b.Distributedrepresentations
ofwordsandphrasesandtheircompositionality.In
Advancesinneuralinformationprocessingsystems
,pages
3111Œ3119.
DmitrijsMilajevs,DimitriKartsaklis,MehrnooshSadrzadeh,andMatthewPurver.2014.Evaluatingneuralword
representationsintensor-basedcompositionalsettings.
ProceedingsoftheConferenceonEmpiricalMethods
inNaturalLanguageProcessing(EMNLP)
.
BoPangandLillianLee.2004.Asentimentaleducation:Sentimentanalysisusingsubjectivitysummarization
basedonminimumcuts.In
Proceedingsofthe42ndannualmeetingonAssociationforComputationalLinguis-
tics
,page271.
BoPangandLillianLee.2005.Seeingstars:Exploitingclassrelationshipsforsentimentcategorizationwith
respecttoratingscales.In
Proceedingsofthe43rdannualmeetingonassociationforcomputationallinguistics
,
pages115Œ124.
JeffreyPennington,RichardSocher,andChristopherManning.2014.Glove:Globalvectorsforwordrepresen-
tation.In
Proceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP)
,
pages1532Œ1543.
BenjaminRiedel,IsabelleAugenstein,GeorgiosPSpithourakis,andSebastianRiedel.2017.Asimplebuttough-
to-beatbaselineforthefakenewschallengestancedetectiontask.
arXivpreprintarXiv:1707.03264
.
TobiasSchnabel,IgorLabutov,DavidMimno,andThorstenJoachims.2015.Evaluationmethodsforunsuper-
visedwordembeddings.In
Proceedingsofthe2015ConferenceonEmpiricalMethodsinNaturalLanguage
Processing
,pages298Œ307.
RichardSocher,AlexPerelygin,JeanWu,JasonChuang,ChristopherDManning,AndrewNg,andChristopher
Potts.2013.Recursivedeepmodelsforsemanticcompositionalityoverasentimenttreebank.In
Proceedings
ofthe2013conferenceonempiricalmethodsinnaturallanguageprocessing
.
IlyaSutskever,OriolVinyals,andQuocVLe.2014.Sequencetosequencelearningwithneuralnetworks.In
Advancesinneuralinformationprocessingsystems
,pages3104Œ3112.
SidaWangandChristopherDManning.2012.Baselinesandbigrams:Simple,goodsentimentandtopic
cation.In
Proceedingsofthe50thAnnualMeetingoftheAssociationforComputationalLinguistics
.
JanyceWiebe,TheresaWilson,andClaireCardie.2005.Annotatingexpressionsofopinionsandemotionsin
language.
Languageresourcesandevaluation
,39(2-3):165Œ210.
JohnWietingandKevinGimpel.2017.Revisitingrecurrentnetworksforparaphrasticsentenceembeddings.
JohnWieting,MohitBansal,KevinGimpel,andKarenLivescu.2015a.Towardsuniversalparaphrasticsentence
embeddings.
ICLR
.
JohnWieting,MohitBansal,KevinGimpel,KarenLivescu,andDanRoth.2015b.Fromparaphrasedatabaseto
compositionalparaphrasemodelandback.
TransactionsoftheAssociationforComputationalLinguistics
.
"
86,Stochastic Variance-Reduced Policy Gradient,http://arxiv.org/pdf/1806.05618v1.pdf,https://github.com/Dam930/rllab,"StochasticVariance-ReducedPolicyGradient
MatteoPapini
*1
DamianoBinaghi
*1
GiuseppeCanonaco
*1
MatteoPirotta
2
MarcelloRestelli
1
Abstract
Inthispaper,weproposeanovelreinforcement-
learningalgorithmconsistinginastochastic
variance-reducedversionofpolicygradientfor
solvingMarkovDecisionProcesses(MDPs).
Stochasticvariance-reducedgradient(SVRG)
methodshaveproventobeverysuccessfulin
supervisedlearning.However,theiradaptationto
policygradientisnotstraightforwardandneeds
toaccountforI)anon-concaveobjectivefunc-
tion;II)approximationsinthefullgradientcom-
putation;andIII)anon-stationarysamplingpro-
cess.TheresultisSVRPG,astochasticvariance-
reducedpolicygradientalgorithmthatleverages
onimportanceweightstopreservetheunbiased-
nessofthegradientestimate.Understandardas-
sumptionsontheMDP,weprovideconvergence
guaranteesforSVRPGwithaconvergencerate
thatislinearunderincreasingbatchsizes.Finally,
wesuggestpracticalvariantsofSVRPG,andwe
empiricallyevaluatethemoncontinuousMDPs.
1.Introduction
Onaverygenerallevel,intelligenceaddresses
theproblemofanagentthatmustselecttherightactions
tosolveatask.TheapproachofReinforcementLearning
(RL)(
Sutton&Barto
,
1998
)istolearnthebestactions
bydirectinteractionwiththeenvironmentandevaluation
oftheperformanceintheformofarewardsignal.This
makesRLfundamentallydifferentfromSupervisedLearn-
ing(SL),wherecorrectactionsareexplicitlyprescribed
byahumanteacher(e.g.,forintheformof
classlabels).However,thetwoapproachessharemanychal-
lengesandtools.Theproblemofestimatingamodelfrom
samples,whichisatthecoreofSL,isequallyfundamen-
talinRL,whetherwechoosetomodeltheenvironment,
*
Equalcontribution
1
PolitecnicodiMilano,Milano,Italy
2
Inria,Lille,France.Correspondenceto:MatteoPapini
<
mat-
teo.papini@polimi.it
>
.
Proceedingsofthe
35
th
InternationalConferenceonMachine
Learning
,Stockholm,Sweden,PMLR80,2018.Copyright2018
bytheauthor(s).
avaluefunction,ordirectlyapolicytheagent's
behaviour.Furthermore,whenthetasksarecharacterized
bylargeorcontinuousstate-actionspaces,RLneedsthe
powerfulfunctionapproximators(e.g.,neuralnetworks)
thatarethemainsubjectofstudyofSL.InatypicalSL
setting,aperformancefunction
J
(

)
hastobeoptimized
w.r.t.tomodelparameters

.Thesetofdatathatareavail-
ablefortrainingisoftenasubsetofallthecasesofinterest,
whichmayevenbeleadingtooptimizationof
nitesumsthatapproximatetheexpectedperformanceover
anunknowndatadistribution.Whengeneralizationtothe
completedatasetisnottakenintoconsideration,wetalk
aboutEmpiricalRiskMinimization(ERM).Eveninthis
case,stochasticoptimizationisoftenusedforreasonsof
efy.Theideaofstochasticgradient(SG)ascent(
Nes-
terov
,
2013
)istoiterativelyfocusonarandomsubsetof
theavailabledatatoobtainanapproximateimprovement
direction.Atthelevelofthesingleiteration,thiscanbe
muchlessexpensivethantakingintoaccountallthedata.
However,thesub-samplingofdataisasourceofvariance
thatcanpotentiallycompromiseconvergence,sothatper-
iterationefyandconvergenceratemustbetradedoff
withproperhandlingofmeta-parameters.Variance-reduced
gradientalgorithmssuchasSAG(
Rouxetal.
,
2012
),SVRG
(
Johnson&Zhang
,
2013
)andSAGA(
Defazioetal.
,
2014a
)
offerbetterwaysofsolvingthistrade-off,with
resultsbothintheoryandpractice.Althoughdesignedex-
plicitlyforERM,thesealgorithmsaddressaproblemthat
affectsmoregeneralmachinelearningproblems.
InRL,stochasticoptimizationisrarelyamatterofchoice,
sincedatamustbeactivelysampledbyinteractingwithan
initiallyunknownenvironment.Inthisscenario,limiting
thevarianceoftheestimatesisanecessitythatcannotbe
avoided,whichmakesvariance-reducedalgorithmsvery
interesting.AmongRLapproaches,policygradient(
Sutton
etal.
,
2000
)istheonethatbearstheclosestsimilaritytoSL
solutions.Thefundamentalprincipleofthesemethodsis
tooptimizeaparametricpolicythroughstochasticgradient
ascent.ComparedtootherapplicationsofSG,thecostof
collectingsamplescanbeveryhighsinceitrequirestointer-
actwiththeenvironment.ThismakesSVRG-likemethods
potentiallymuchmoreefthan,e.g.,batchlearning.
Unfortunately,RLhasaseriesofdifthatarenot
presentinERM.First,inSLtheobjectivecanoftenbede-
arXiv:1806.05618v1  [cs.LG]  14 Jun 2018StochasticVariance-ReducedPolicyGradient
signedtobestronglyconcave(weaimtomaximize).This
isnotthecaseforRL,sowehavetodealwithnon-concave
objectivefunctions.Then,asmentionedbefore,thedataset
isnotinitiallyavailableandmayevenbewhich
makesapproximationsunavoidable.ThisrulesoutSAGand
SAGAbecauseoftheirstoragerequirements,whichleaves
SVRGasthemostpromisingchoice.Finally,thedistribu-
tionusedtosampledataisnotunderdirectcontrolofthe
algorithmdesigner,butitisafunctionofpolicyparameters
thatchangeovertimeasthepolicyisoptimized,whichisa
formofnon-stationarity.SVRGhasbeenusedinRLasan
eftechniqueforoptimizingtheper-iterationproblem
inTrust-RegionPolicyOptimization(
Xuetal.
,
2017
)orfor
policyevaluation(
Duetal.
,
2017
).Inboththecases,the
optimizationproblemsfacedresembletheSLscenarioand
arenotaffectedbyallthepreviouslymentionedissues.
AfterprovidingbackgroundonpolicygradientandSVRG
inSection
2
,weproposeSVRPG,avariantofSVRGfor
thepolicygradientframework,addressingallthedif
mentionedabove(seeSection
3
).InSection
4
weprovide
convergenceguaranteesforouralgorithm,andweshow
aconvergenceratethathasan
O
(
1
=
T
)
dependenceonthe
number
T
ofiterations.InSection
5.2
wesuggesthowto
setthemeta-parametersofSVRPG,whileinSection
5.3
we
discusssomepracticalvariantsofthealgorithm.Finally,in
Section
7
weempiricallyevaluatetheperformanceofour
methodonpopularcontinuousRLtasks.
2.Preliminaries
Inthissection,weprovidetheessentialbackgroundon
policygradientmethodsandstochasticvariance-reduced
gradientmethodsforoptimization.
2.1.PolicyGradient
AReinforcementLearningtask(
Sutton&Barto
,
1998
)can
bemodelledwithadiscrete-timecontinuousMarkovDeci-
sionProcess(MDP)
M
=
fS
;
A
;
P
;
R
;;ˆ
g
,where
S
is
acontinuousstatespace;
A
isacontinuousactionspace;
P
isaMarkoviantransitionmodel,where
P
(
s
0
j
s;a
)
de-
thetransitiondensityfromstate
s
to
s
0
underaction
a
;
R
istherewardfunction,where
R
(
s;a
)
2
[

R;R
]
is
theexpectedrewardforstate-actionpair
(
s;a
)
;

2
[0
;
1)
isthediscountfactor;and
ˆ
istheinitialstatedistribution.
Theagent'sbehaviourismodelledasapolicy
ˇ
,where
ˇ
(

s
)
isthedensitydistributionover
A
instate
s
.We
considerepisodicMDPswitheffectivehorizon
H
.
1
In
thissetting,wecanlimitourattentiontotrajectoriesof
length
H
.Atrajectory
˝
isasequenceofstatesandac-
tions
(
s
0
;a
0
;s
1
;a
1
;:::;s
H

1
;a
H

1
)
observedbyfollow-
1
Theepisodedurationisarandomvariable,buttheoptimal
policycanreachthetargetstate(i.e.,absorbingstate)inlessthan
H
steps.Thishasnottobeconfusedwithahorizonproblem
wheretheoptimalpolicyisnon-stationary.
ingastationarypolicy,where
s
0
˘
ˆ
.Wedenotewith
p
(
˝
j
ˇ
)
thedensitydistributioninducedbypolicy
ˇ
onthe
set
T
ofallpossibletrajectories(seeAppendix
A
forthe
andwith
R
(
˝
)
thetotaldiscountedrewardpro-
videdbytrajectory
˝
:
R
(
˝
)=
P
H

1
t
=0

t
R
(
s
t
;a
t
)
:
Poli-
ciescanberankedbasedontheirexpectedtotalreward:
J
(
ˇ
)=
E
˝
˘
p
(

ˇ
)
[
R
(
˝
)
j
M
]
.SolvinganMDP
M
means

ˇ

2
argmax
ˇ
f
J
(
ˇ
)
g
.
Policygradientmethodsrestrictthesearchforthebest
performingpolicyoveraclassofparametrizedpolicies


=
f
ˇ

:

2
R
d
g
,withtheonlyconstraintthat
ˇ

is
differentiablew.r.t.

.Forsakeofbrevity,wewilldenote
theperformanceofaparametricpolicywith
J
(

)
andthe
probabilityofatrajectory
˝
with
p
(
˝
j

)
(insomeoccasions,
p
(
˝
j

)
willbereplacedby
p

(
˝
)
forthesakeofreadability).
Thesearchforalocallyoptimalpolicyisperformedthrough
gradientascent,wherethepolicygradientis(
Suttonetal.
,
2000
;
Peters&Schaal
,
2008a
):
r
J
(

)=
E
˝
˘
p
(


)
[
r
log
p

(
˝
)
R
(
˝
)]
:
(1)
Noticethatthedistributionthegradientisinduced
bythecurrentpolicy.Thisaspectintroducesanonstation-
arityinthesamplingprocess.Sincetheunderlyingdis-
tributionchangesovertime,itisnecessarytoresample
ateachupdateoruseweightingtechniquessuchasim-
portancesampling.Here,weconsiderthe
onlinelearn-
ingscenario
,wheretrajectoriesaresampledbyinteracting
withtheenvironmentateachpolicychange.Inthisset-
ting,stochasticgradientascentistypicallyemployed.At
eachiteration
k>
0
,abatch
D
k
N
=
f
˝
i
g
N
i
=0
of
N>
0
trajectoriesiscollectedusingpolicy
ˇ

k
.Thepolicyis
thenupdatedas

k
+1
=

k
+

b
r
N
J
(

k
)
,where

is
astepsizeand
b
r
N
J
(

)
isanestimateofEq.
(
1
)
using
D
k
N
.Themostcommonpolicygradientestimators(e.g.,
REINFORCE(
Williams
,
1992
)andG(PO)MDP(
Baxter&
Bartlett
,
2001
))canbeexpressedasfollows
b
r
N
J
(

)=
1
N
N
X
n
=1
g
(
˝
i
j

)
;˝
i
2D
k
N
;
(2)
where
g
(
˝
i
j

)
isanestimateof
r
log
p

(
˝
i
)
R
(
˝
i
)
.Al-
thoughtheREINFORCEissimplerthanthe
G(PO)MDPone,thelatterisusuallypreferredduetoits
lowervariance.WereferthereadertoAppendix
A
for
detailsandaformalof
g
.
Themainlimitationofplainpolicygradientisthehighvari-
anceoftheseestimators.Thena
¨
eapproachofincreasing
thebatchsizeisnotanoptioninRLduetothehighcostof
collectingsamples,i.e.,byinteractingwiththeenvironment.
Forthisreason,literaturehasfocusedontheintroduction
ofbaselines(i.e.,functions
b
:
SA!
R
)aimingto
reducethevariance(e.g.,
Williams
,
1992
;
Peters&Schaal
,
StochasticVariance-ReducedPolicyGradient
2008a
;
Thomas&Brunskill
,
2017
;
Wuetal.
,
2018
),see
Appendix
A
foraformalof
b
.Thesebaselines
areusuallydesignedtominimizethevarianceofthegra-
dientestimate,buteventhemneedtobeestimatedfrom
data,partiallyreducingtheireffectiveness.Ontheother
hand,therehasbeenasurgeofrecentinterestinvariance
reductiontechniquesforgradientoptimizationinsupervised
learning(SL).Althoughthesetechniqueshavebeenmainly
derivedforproblems,wewillshowinSection
3
howtheycanbeusedinRL.Inparticular,wewillshow
thattheproposedSVRPGalgorithmcantakethebestof
bothworlds(i.e.,SLandRL)sinceitcanbepluggedinto
apolicygradientestimateusingbaselines.Thenextsec-
tionhastheaimtodescribevariancereductiontechniques
forproblems.Inparticular,wewillpresentthe
SVRGalgorithmthatisatthecoreofthiswork.
2.2.StochasticVariance-ReducedGradient
Finite-sumoptimizationistheproblemofmaximizingan
objectivefunction
f
(

)
whichcanbedecomposedintothe
sumoraverageofanumberoffunctions
z
i
(

)
:
max

(
f
(

)=
1
N
N
X
i
=1
z
i
(

)
)
:
Thiskindofoptimizationisverycommoninmachinelearn-
ing,whereeach
z
i
maycorrespondtoadatasample
x
i
from
adataset
D
N
ofsize
N
(i.e.,
z
i
(

)=
z
(
x
i
j

)
).Acommon
requirementisthat
z
mustbesmoothandconcavein

.
2
Underthishypothesis,fullgradient(FG)ascent(
Cauchy
,
1847
)withaconstantstepsizeachievesalinearconver-
gencerateinthenumber
T
ofiterations(i.e.,parameter
updates)(
Nesterov
,
2013
).However,eachiterationrequires
N
gradientcomputations,whichcanbetooexpensivefor
largevaluesof
N
.StochasticGradient(SG)ascent(e.g.,
Robbins&Monro
,
1951
;
Bottou&LeCun
,
2004
)over-
comesthisproblembysamplingasinglesample
x
i
per
iteration,butavanishingstepsizeisrequiredtocontrolthe
varianceintroducedbysampling.Asaconsequence,the
lowerper-iterationcostispaidwithaworse,sub-linearcon-
vergencerate(
Nemirovskiietal.
,
1983
).StartingfromSAG,
aseriesofvariationstoSGhavebeenproposedtoachieve
abettertrade-offbetweenconvergencespeedandcostper
iteration:e.g.,SAG(
Rouxetal.
,
2012
),SVRG(
Johnson
&Zhang
,
2013
),SAGA(
Defazioetal.
,
2014a
),Finito(
De-
fazioetal.
,
2014b
),andMISO(
Mairal
,
2015
).Thecommon
ideaistoreusepastgradientcomputationstoreducethe
varianceofthecurrentestimate.Inparticular,Stochastic
Variance-ReducedGradient(SVRG)isoftenpreferredto
othersimilarmethodsforitslimitedstoragerequirements,
whichisaadvantagewhendeepand/orwide
neuralnetworksareemployed.
2
Notethatweareconsideringamaximizationprobleminstead
oftheclassicalminimizationone.
Algorithm1
SVRG
Input:
adataset
D
N
,numberofepochs
S
,epochsize
m
,
stepsize

,initialparameter

0
m
:=
e

0
for
s
=0
to
S

1
do

s
+1
0
:=
e

s
=

s
m
e

=
r
f
(
e

s
)
for
t
=0
to
m

1
do
x
˘U
(
D
N
)
v
s
+1
t
=
e

+
r
z
(
x
j

s
+1
t
)
r
z
(
x
j
e

s
)

s
+1
t
+1
=

s
+1
t
+
v
s
+1
t
endfor
endfor
Concavecase:
return

S
m
Non-Concavecase:
return

s
+1
t
with
(
s;t
)
pickeduni-
formlyatrandomfrom
f
[0
;S

1]

[0
;m

1]
g
TheideaofSVRG(Algorithm
1
)istoalternatefulland
stochasticgradientupdates.Each
m
=
O
(
N
)
iterations,
asnapshot
e

ofthecurrentparameterissavedtogether
withitsfullgradient
r
f
(
e

)=
1
N
P
i
r
z
(
x
i
j
e

)
.Between
snapshots,theparameterisupdatedwith
H
f
(

)
,agradient
estimatecorrectedusingstochasticgradient.Forany
t
2
f
0
;:::;m

1
g
:
H
f
(

t
):=
v
t
=
r
f
(
e

)+
r
z
(
x
j

t
)
r
z
(
x
j
e

)
;
(3)
where
x
issampleduniformlyatrandomfrom
D
N
(i.e.,
x
˘U
(
D
N
)
).Notethat
t
=0
correspondstoaFGstep
(i.e.,
H
f
(

0
)=
r
f
(
e

)
)since

0
:=
e

.Thecorrected
gradient
H
f
(

)
isanunbiasedestimateof
r
f
(

)
,anditis
abletocontrolthevarianceintroducedbysamplingeven
withaedstepsize,achievingalinearconvergencerate
withoutresortingtoaplainfullgradient.
Morerecently,someextensionsofvariancereductional-
gorithmstothenon-concaveobjectiveshavebeenpro-
posed(e.g.,
Allen-Zhu&Hazan
,
2016
;
Reddietal.
,
2016a
;
b
).Inthisscenario,
f
istypicallyrequiredtobe
L
-smooth,i.e.,


r
f
(

0
)
r
f
(

)


2

L



0




2
for
each

;

0
2
R
n
andforsomeLipschitzconstant
L
.Un-
derthishypothesis,theconvergencerateofSGis
O
(
1
=
p
T
)
(
Ghadimi&Lan
,
2013
),i.e.,
T
=
O
(
1
=

2
)
iterationsare
requiredtoget
kr
f
(

)
k
2
2


.Again,SVRGachievesthe
samerateasFG(
Reddietal.
,
2016a
),whichis
O
(
1
T
)
inthis
case(
Nesterov
,
2013
).Theonlyadditionalrequirementis
toselect


uniformlyatrandomamongallthe

k
instead
ofsimplysettingittothevalue(
k
beingtheiterations).
3.SVRGinReinforcementLearning
InonlineRLproblems,theusualapproachistotunethe
batchsizeofSGtotheoptimaltrade-offbetweenvari-
anceandspeed.Recallthat,comparedtoSL,thesamples
StochasticVariance-ReducedPolicyGradient
arenotedinadvancebutweneedtocollectthemateach
policychange.Sincethisoperationmaybecostly,wewould
liketominimizethenumberofinteractionswiththeenvi-
ronment.Forthesereasons,wewouldliketoapplySVRG
toRLproblemsinordertolimitthevarianceintroducedby
samplingtrajectories,whichwouldultimatelyleadtofaster
convergence.However,adirectapplicationofSVRGtoRL
isnotpossibleduetothefollowingissues:
Non-concavity:
theobjectivefunction
J
(

)
istypically
non-concave.
dataset:
theRLoptimizationcannotbeexpressed
asaproblem.Theobjectivefunctionisan
expectedvalueoverthetrajectorydensity
p

(
˝
)
ofthe
totaldiscountedreward,forwhichwewouldneedan
dataset.
Non-stationarity:
thedistributionofthesampleschanges
overtime.Inparticular,thevalueofthepolicyparam-
eter

thesamplingprocess.
Todealwithnon-concavity,werequire
J
(

)
tobe
L
-smooth,
whichisareasonableassumptionforcommonpolicyclasses
suchasGaussian
3
andsoftmax(e.g.,
Furmston&Barber
,
2012
;
Pirottaetal.
,
2015
).Becauseofthedataset,we
canonlyrelyonanestimateofthefullgradient.
Harikandeh
etal.
(
2015
)analysedthisscenarioundertheassumptions
of
z
beingconcave,showingthatSVRGisrobusttoan
inexactcomputationofthefullgradient.Inparticular,it
isstillpossibletorecovertheoriginalconvergencerateif
theerrordecreasesatanappropriaterate.
Bietti&Mairal
(
2017
)performedasimilaranalysisonMISO.InSection
4
wewillshowhowtheestimationaccuracyimpactsonthe
convergenceresultswithanon-concaveobjective.Finally,
thenon-stationarityoftheoptimizationproblemintroduces
abiasintotheSVRGestimatorinEq.
(
3
)
.Toovercomethis
limitationweemployimportanceweighting(e.g.,
Rubin-
stein
,
1981
;
Precup
,
2000
)tocorrectthedistributionshift.
WecannowintroduceStochasticVariance-ReducedPolicy
Gradient(SVRPG)foragenericpolicygradientestimator
g
.
Pseudo-codeisprovidedinAlgorithm
2
.Theoverallstruc-
tureisthesameasAlgorithm
1
,butthesnapshotgradientis
notexactandthegradientestimateusedbetweensnapshots
iscorrectedusingimportanceweighting:
4
H
J
(

t
)=
b
r
N
J
(
e

)+
g
(
˝
j

t
)

!
(
˝
j

t
;
e

)
g
(
˝
j
e

)
forany
t
2f
0
;:::;m

1
g
,where
b
r
N
J
(
e

)
isasinEq.
(
2
)
where
D
N
issampledusingthesnapshotpolicy
ˇ
e

,
˝
is
3
SeeAppendix
C
formoredetailsontheGaussianpolicycase.
4
Notethat
g
canbeanyunbiasedestimator,withorwithout
baseline.Theunbiasednessisrequiredfortheoreticalresults(e.g.,
Appendix
A
).
Algorithm2
SVRPG
Input:
numberofepochs
S
,epochsize
m
,stepsize

,
batchsize
N
,mini-batchsize
B
,gradientestimator
g
,
initialparameter

0
m
:
=
e

0
:
=

0
for
s
=0
to
S

1
do

s
+1
0
:=
e

s
=

s
m
Sample
N
trajectories
f
˝
j
g
from
p
(

e

s
)
e

=
b
r
N
J
(
e

s
)
(seeEq.(
2
))
for
t
=0
to
m

1
do
Sample
B
trajectories
f
˝
i
g
from
p
(


s
+1
t
)
c
s
+1
t
=
1
B
B

1
P
i
=0

g
(
˝
i
j

s
+1
t
)

!
(
˝
i
j

s
+1
t
;
e

s
)
g
(
˝
i
j
e

s
)

v
s
+1
t
=
e

+
c
s
+1
t

s
+1
t
+1
=

s
+1
t
+
v
s
+1
t
endfor
endfor
return

A
:
=

s
+1
t
with
(
s;t
)
pickeduniformlyatran-
domfrom
f
[0
;S

1]

[0
;m

1]
g
sampledfromthecurrentpolicy
ˇ

t
,and
!
(
˝
j

t
;
e

)=
p
(
˝
j
e

)
p
(
˝
j

t
)
isanimportanceweightfrom
ˇ

t
tothesnapshot
policy
ˇ
e

.SimilarlytoSVRG,wehavethat

0
:=
e

,and
theupdateisaFGstep.Ourupdateisstillfundamentally
on-policysincetheweightingconcernsonlythecorrection
term.However,thispartialﬁoff-policynessﬂrepresentsan
additionalsourceofvariance.Thisisawell-knownissueof
importancesampling(e.g.,
Thomasetal.
,
2015
).Tomitigate
it,weusemini-batchesoftrajectoriesofsize
B
˝
N
to
averagethecorrection,i.e.,
H
J
(

t
):=
v
t
=
b
r
N
J
(
e

)
(4)
+
1
B
B

1
X
i
=0
h
g
(
˝
i
j

t
)

!
(
˝
i
j

t
;
e

)
g
(
˝
i
j
e

)
i
c
t
:
Itisworthnotingthatthefullgradientandthe
correctiontermhavethesameexpectedvalue:
E
˝
i
˘
p
(


t
)
h
1
B
P
B

1
i
=0
!
(
˝
i
j

t
;
e

)
g
(
˝
i
j
e

)
i
=
r
J
(
e

)
.
5
ThispropertywillbeusedtoproveLemma
3.1
.Theuseof
mini-batchesisalsocommonpracticeinSVRGsinceitcan
yieldaperformanceimprovementeveninthesupervised
case(
Harikandehetal.
,
2015
;
Kone

cn
˚
yetal.
,
2016
).Itis
easytoshowthattheSVRPGestimatorhasthefollowing,
desirableproperties:
Lemma3.1.
Let
b
r
N
J
(

)
beanunbiasedestimatorof
(
1
)
andlet


2
argmin

f
J
(

)
g
.Then,theSVRGestimate
5
ThereadercanrefertoAppendix
A
foroff-policygradients
andvariantsofREINFORCEandG(PO)MDP.
StochasticVariance-ReducedPolicyGradient
in
(
4
)
is
unbiased
E
[
H
J
(

)]=
r
J
(

)
:
(5)
andregardlessofthemini-batchsize
B
:
6
V
ar[
H
J
(


)]=
V
ar
h
b
r
N
J
(


)
i
:
(6)
PreviousresultsholdforbothREINFORCEandG(PO)MDP.
Inparticular,thelatterresultsuggeststhatanSVRG-like
algorithmusing
H
J
(

)
canachievefasterconvergence,by
performingmuchmoreparameterupdateswiththesame
datawithoutintroducingadditionalvariance(atleastasymp-
totically).NotethattherandomizedreturnvalueofAlgo-
rithm
2
doesnotaffectonlinelearningatall,butwillbe
usedasatheoreticaltoolinthenextsection.
4.ConvergenceGuaranteesofSVRPG
Inthissection,westatetheconvergenceguaranteesfor
SVRPGwithREINFORCEorG(PO)MDPgradientesti-
mator.Wemainlyleverageontherecentanalysisofnon-
concaveSVRG(
Reddietal.
,
2016a
;
Allen-Zhu&Hazan
,
2016
).Eachofthethreechallengespresentedatthebegin-
ningofSection
3
canpotentiallypreventconvergence,so
weneedadditionalassumptions.InAppendix
C
weshow
howGaussianpoliciessatisfytheseassumptions.
1)Non-concavity.
Acommonassumption,inthiscase,isto
assumetheobjectivefunctiontobe
L
-smooth.However,in
RLwecanconsiderthefollowingassumptionwhichissuf-
cientforthe
L
-smoothnessoftheobjective(seeLemma
B.2
).
Assumption4.1
(Onpolicyderivatives)
.
Foreachstate-
actionpair
(
s;a
)
,anyvalueof

,andallparametercompo-
nents
i;j
thereexistconstants
0

G;F<
1
suchthat:
jr

i
log
ˇ

(
a
j
s
)
j
G;




@
2
@
i
@
j
log
ˇ

(
a
j
s
)





F:
2)FGApproximation.
Sincewecannotcomputeanexact
fullgradient,werequirethevarianceoftheestimatorto
bebounded.Thisassumptionissimilarinspirittotheone
in(
Harikandehetal.
,
2015
).
Assumption4.2
(Onthevarianceofthegradientestimator)
.
Thereisaconstant
V<
1
suchthat,foranypolicy
ˇ

:
V
ar[
g
(


)]

V:
3)Non-stationarity.
SimilarlytowhatisdoneinSL(
Cortes
etal.
,
2010
),werequirethevarianceoftheimportance
weighttobebounded.
6
Foranyvector
x
,weuse
V
ar[
x
]
todenotethetraceofthe
covariancematrix,i.e.,
Tr(
E

(
x

E
[
x
])(
x

E
[
x
])
T

)
.
Assumption4.3
(Onthevarianceofimportanceweights)
.
Thereisaconstant
W<
1
suchthat,foreachpairof
policiesencounteredinAlgorithm
2
andforeachtrajectory,
V
ar
[
!
(
˝
j

1
;

2
)]

W;
8

1
;

2
2
R
d
;˝
˘
p
(


1
)
:
DifferentlyfromAssumptions
4.1
and
4.2
,Assumption
4.3
mustbeenforcedbyaproperhandlingoftheepochsize
m
.
WecannowstatetheconvergenceguaranteesforSVRPG.
Theorem4.4
(ConvergenceoftheSVRPGalgorithm)
.
As-
sumetheREINFORCEortheG(PO)MDPgradientestima-
torisusedinSVRPG(seeEquation
(
4
)
).UnderAssump-
tions
4.1
,
4.2
and
4.3
,theparametervector

A
returned
byAlgorithm
2
after
T
=
m

S
iterationshas,forsome
positiveconstants
 ;;˘
andforproperchoiceofthestep
size

andtheepochsize
m
,thefollowingproperty:
E
h
kr
J
(

A
)
k
2
2
i

J
(


)

J
(

0
)
 T
+

N
+
˘
B
;
where


isaglobaloptimumand
 ;;˘
dependonlyon
G;F;V;W;
and
m
.
RefertoAppendix
B
foradetailedproofinvolvingthe
nitionoftheconstantsandthemeta-parameterconstraints.
Byanalysingtheupper-boundinTheorem
4.4
weobserve
that:I)the
O
(
1
=
T
)
termiscoherentwithresultsonnon-
concaveSVRG(e.g.,
Reddietal.
,
2016a
);II)the
O
(
1
=
N
)
termisduetotheFGapproximationandisanalogousto
theonein(
Harikandehetal.
,
2015
);III)the
O
(
1
=
B
)
termis
duetoimportanceweighting.Toachieveasymptoticconver-
gence,thebatchsize
N
andthemini-batchsize
B
should
increaseovertime.Inpractice,itisenoughtochoose
N
and
B
largeenoughtomakethesecondandthethirdterm
negligible,i.e.,tomitigatethevarianceintroducedbyFG
approximationandimportancesampling,respectively.Once
thelasttwotermscanbeneglected,thenumberoftrajec-
toriesneededtoachieve
kr
J
(

)
k
2
2


is
O
(
B
+
N
=
m

)
.In
thissense,anadvantageoverbatchgradientascentcanbe
achievedwithproperlyselectedmeta-parameters.InSec-
tion
5.2
weproposeajointselectionofstepsize

andepoch
size
m
.Finally,fromthereturnstatementofAlgorithm
2
,
itisworthnotingthat
J
(

A
)
canbeseenastheaverage
performanceofallthepoliciestriedbythealgorithm.This
isparticularlymeaningfulinthecontextofonlinelearning
thatweareconsideringinthispaper.
5.RemarksonSVRPG
Theconvergenceguaranteespresentedintheprevioussec-
tioncomewithrequirementsonthemeta-parameters(i.e.,

and
m
)thatmaybetooconservativeforpracticalappli-
cations.Hereweprovideapracticalandautomaticway
tochoosethestepsize

andthenumberofsub-iterations
m
performedbetweensnapshots.Additionally,weprovide
StochasticVariance-ReducedPolicyGradient
avariantofSVRPGexploitingavariance-reductiontech-
niqueforimportanceweights.Despitelackingtheoretical
guarantees,wewillshowinSection
7
thatthismethodcan
outperformthebaselineSVRPG(Algorithm
2
).
5.1.FullGradientUpdate
AsnotedinSection
3
,theupdateperformedatthebe-
ginningofeachepochisequivalenttoafull-gradientup-
date.Inoursetting,wherecollectingsamplesisparticu-
larlyexpensive,the
B
trajectoriescollectedusingthesnap-
shottrajectory
ˇ
e

s
feelslikeawasteofdata(theterm
P
i
g
(
˝
i
)

!
(
˝
i
)
g
(
˝
i
)=0
since

0
=
e

).Inpractice,
wejustperformanapproximatefullgradientupdateusing
the
N
trajectoriessampledtocompute
b
r
N
J
(
e

s
)
,i.e.,

s
+1
1
=
e

s
+

b
r
N
J
(
e

s
)

s
+1
t
+1
=

s
+1
t
+

H
J
(

s
+1
t
)
for
t
=1
;:::;m

1
:
Inthefollowing,wewillalwaysusethispracticalvariant.
5.2.Meta-ParameterSelection
Thestepsize

iscrucialtobalancevariancereductionand
efy,whiletheepochlength
m
thevariance
introducedbytheimportanceweights.Lowvaluesof
m
are
associatedwithsmallvariancebutincreasethefrequency
ofsnapshotpoints(whichmeansmanyFGcomputations).
Highvaluesof
m
maymovepolicy
ˇ

t
farawayfromthe
snapshotpolicy
ˇ
e

,causinglargevarianceintheimportance
weights.Wewilljointlysetthetwometa-parameters.
Adaptivestepsize.
Astandardwaytodealwithnoisy
gradientsistouseadaptivestrategiestocomputethestep
size.ADAptiveMomentestimation(ADAM)(
Kingma&
Ba
,
2014
)stabilizestheparameterupdatebycomputing
learningratesforeachparameterbasedonanincremental
estimateofthegradientvariance.Duetothisfeature,we
wouldliketoincorporateADAMinthestructureofthe
SVRPGupdate.RecallthatSVRPGperformstwodifferent
updatesoftheparameters

:I)FGupdateinthesnapshot;
II)correctedgradientupdateinthesub-iterations.Giventhis
structure,wesuggestusingtwoseparateADAMestimators:

s
+1
1
=
e

s
+

FG
s

b
r
N
J
(
e

s
)


s
+1
t
+1
=

s
+1
t
+

SI
s
+1
;t

H
J
(

s
+1
t
)

for
t
=1
;:::;m

1
;
where

FG
s
isassociatedwiththesnapshotand

SI
s
+1
;t
with
thesub-iterations(seeAppendix
D
fordetails).Bydoing
so,wedecouplethecontributionofthevariancedueto
theapproximateFGfromtheoneintroducedbythesub-
iterations.Notethatthesetwotermshavedifferentorders
ofmagnitudesinceareestimatedwithadifferentnumber
oftrajectories(
B
˝
N
)andtheestimatorinthesnapshot
doesnotrequireimportanceweights.TheuseoftwoADAM
estimatorsallowstocaptureandexploitthisproperty.
Adaptiveepochlength.
Itiseasytoimaginethataprede-
schedule(e.g.,
m
edinadvanceorchangedwitha
policy-independentprocess)maypoorlyperformduetothe
highvariabilityoftheupdates.Inparticular,givenaed
numberofsub-iterations
m
,thevarianceoftheupdatesin
thesub-iterationsdependsonthesnapshotpolicyandthe
sampledtrajectories.SincetheADAMestimatepartlycap-
turessuchvariability,weproposetotakeanewsnapshot
(i.e.,interruptthesub-iterations)wheneverthestepsize

SI
proposedbyADAMforthesub-iterationsissmallerthanthe
onefortheFG(i.e.,

FG
).Ifthelatterconditionisvd,
itamountstosaythatthenoiseinthecorrectedgradienthas
overcometheinformationoftheFG.Formally,thestopping
conditionisasfollows
If

FG
N
>

SI
B
then
takesnapshot,
wherewehaveintroduced
N
and
B
totakeintoaccountthe
trajectoryefy(i.e.,weightedadvantage).Thelessthe
numberoftrajectoriesusedtoupdatethepolicy,thebetter.
Includingthebatchsizesinthestoppingconditionallowsus
tooptimizethetrade-offbetweenthequalityoftheupdates
andthecostofperformingthem.
5.3.NormalizedImportanceSampling
AsmentionedinSection
5.2
,importanceweightsarean
additionalsourceofvariance.Astandardwaytocopewith
thisissueisself-normalization(e.g.,
Precup
,
2000
;
Owen
,
2013
).Thistechniquecanreducethevarianceoftheimpor-
tanceweightsatthecostofintroducingsomebias(
Owen
,
2013
,Chapter9).Whetherthetrade-offisadvantageous
dependsonthetask.Introducingself-normalization
inthecontextofouralgorithm,weswitchfromEq.(
4
)to:
H
J
(

t
)=
b
r
N
J
(
e

)+
1
B
B

1
X
i
=0
[
g
(
˝
i
j

t
)]

1

B

1
X
i
=0
h
!
(
˝
i
j

t
;
e

)
g
(
˝
i
j
e

)
i
:
where
=
P
B

1
i
=0
!
(
˝
i
j

t
;
e

)
.InSection
7
weshowthat
self-normalizationcanprovideaperformanceimprovement.
6.RelatedWork
DespitetheconsiderableinterestreceivedinSL,variance-
reducedgradientapproacheshavenotattractedtheRLcom-
munity.Asfarasweknow,therearejusttwoapplications
ofSVRGinRL.Theapproach(
Duetal.
,
2017
)aimsto
exploitSVRGforpolicyevaluation.Thepolicyevaluation
problemismorestraightforwardthantheonefacedinthis
paper(controlproblem).Inparticular,sincethegoalisto
evaluatejusttheperformanceofapolicy,theop-
timizationproblemisstationary.Thesettingconsideredin
thepaperistheoneofpolicyevaluationbyminimizingthe
StochasticVariance-ReducedPolicyGradient
empiricalmeansquaredprojectedBellmanerror(MSPBE)
withalinearapproximationofthevaluefunction.
Duetal.
(
2017
)shownthatthisproblemcanbeequivalentlyrefor-
mulatedasaconvex-concavesaddle-pointproblemthatis
characterizedbyastructure.Thisproblemcanbe
solvedusingavariantofSVRG(
Palaniappan&Bach
,
2016
)
forwhichconvergenceguaranteeshavebeenprovided.The
secondapproach(
Xuetal.
,
2017
)usesSVRGasapractical
methodtosolvetheoptimizationproblemfacedbyTrust
RegionPolicyOptimization(TRPO)ateachiteration.This
isjustadirectapplicationofSVRGtoaproblemhaving
structuresincenostructureoftheRL
problemisexploited.Itisworthtomentionthat,forpracti-
calreasons,theauthorsproposedtouseaNewtonconjugate
gradientmethodwithSVRG.
Intherecentpast,therehasbeenasurgeofstudiesinves-
tigatingvariancereductiontechniquesforpolicygradient
methods.Thestructureofthepolicygradientallows
incorporatingabaseline(i.e.,afunction
b
:
SA!
R
)
withoutaffectingtheunbiasednessofthegradient(e.g.,
Williams
,
1992
;
Weaver&Tao
,
2001
;
Peters&Schaal
,
2008b
;
Thomas&Brunskill
,
2017
;
Wuetal.
,
2018
).Al-
thoughthebaselinecanbearbitrarilyselected,literature
oftenreferstotheoptimalbaselineastheoneminimizing
thevarianceoftheestimate.Nevertheless,eventhebaseline
needstobeestimatedfromdata.Thisfactmaypartiallyre-
duceitseffectivenessbyintroducingvariance.Evenifthese
approachessharethesamegoalasSVRG,theyaresubstan-
tiallydifferent.Inparticular,theproposedSVRPGdoes
notmakeexplicituseofthestructureofthepolicygradient
framework,anditisindependentoftheunderlyinggradient
estimate(i.e.,withorwithoutbaseline).Thissuggeststhat
wouldbepossibletointegrateanad-hocSVRPGbaselineto
furtherreducethevarianceoftheestimate.Sincethispaper
isabouttheapplicabilityofSVRGtechniquetoRL,we
considerthistopicasfuturework.Additionally,theexperi-
mentsshowthatSVRPGhasanadvantageoverG(PO)MPD
evenwhenthebaselineisused(seethehalf-cheetahdomain
inSection
7
).
Concerningimportanceweightingtechniques,RLhasmade
extensiveuseofthemforoff-policyproblems(e.g.,
Precup
,
2000
;
Thomasetal.
,
2015
).However,asmentionedbe-
fore,SVRPGcannotbecomparedtosuchmethodssince
itisinallrespectsanon-policyalgorithm.Here,impor-
tanceweightingisjustastatisticaltoolusedtopreservethe
unbiasednessofthecorrectedgradient.
7.Experiments
Inthissection,weevaluatetheperformanceofSVRPGand
compareitwithpolicygradient(PG)onwellknowncontinu-
ousRLtasks:Cart-polebalancingandSwimmer(e.g.,
Duan
etal.
,
2016
).WeconsiderG(PO)MDPsinceithasasmaller
variancethanREINFORCE.Forouralgorithm,weusea
batchsize
N
=100
,amini-batchsize
B
=10
,andthe
jointlyadaptivestepsize

andepochlength
m
proposedin
Section
5.2
.Sincetheaimofthiscomparisonistoshowthe
improvementthatvoredvariancereductionbrings
toSGinthepolicygradientframework,wesetthebatchsize
ofthebaselinepolicygradientalgorithmto
B
.Inthissense,
wemeasuretheimprovementyieldedbycomputingsnap-
shotgradientsandusingthemtoadjustparameterupdates.
Sinceweevaluateon-lineperformanceoverthenumberof
sampledtrajectories,thecostofcomputingsuchsnapshot
gradientsisautomaticallytakenintoconsideration.Tomake
thecomparisonfair,wealsouseAdaminthebaselinePG
algorithm,whichwewilldenotesimplyasG(PO)MDPin
thefollowing.Inalltheexperiments,weusedeepGaus-
sianpolicieswithadaptivestandarddeviation(detailson
networkarchitectureinAppendix
E
).Eachexperimentis
run
10
timeswitharandompolicyinitializationandseed,
butthisinitializationissharedamongthealgorithmsunder
comparison.Thelengthoftheexperiment,i.e.,thetotal
numberoftrajectories,isedforeachtask.Performance
isevaluatedbyusingtest-trajectoriesonasubsetofthepoli-
ciesconsideredduringthelearningprocess.Weprovide
averageperformancewith90%bootstrapinter-
vals.Taskimplementationsarefromthe
rllab
library(
Duan
etal.
,
2016
),onwhichouragentsarealsobased.
7
More
detailsonmeta-parametersandexhaustivetaskdescriptions
areprovidedinAppendix
E
.
Figure
1a
comparesSVRPGwithG(PO)MDPonacon-
tinuousvariantoftheclassicalCart-poletask,whichisa
2Dbalancingtask.Despiteusingmoretrajectoriesonaver-
ageforeachparameterupdate,ouralgorithmshowsfaster
convergence,whichcanbeascribedtothebetterqualityof
updatesduetovariancereduction.
TheSwimmertaskisa3Dcontinuous-controllocomotion
task.Thistaskismoredifcultthancart-pole.Inpartic-
ular,thelongerhorizonandthemorecomplexdynamics
canhaveadangerousimpactonthevarianceofimportance
weights.Inthiscase,theself-normalizationtechniquepro-
posedinSection
5.3
bringsanimprovement(evenifnot
statisticallyasshowninFigure
1b
.Figure
1c
showsself-normalizedSVRPGagainstG(PO)MDP.Our
algorithmoutperformsG(PO)MDPforalmosttheentire
learningprocess.Alsohere,wenoteanincreaseofspeedin
earlyiterations,and,towardtheendofthelearningprocess,
theimprovementbecomesstatistically
Preliminaryresultsonactor-critic.
Anothervariance-
reductiontechniqueinpolicygradientconsistsofusing
baselinesor
critics
.Thistoolisorthogonaltothemeth-
odsdescribedinthispaper,andthetheoreticalresultsof
Section
4
aregeneralinthissense.Intheexperimentsde-
7
Codeavailableat
github.com/Dam930/rllab
.
StochasticVariance-ReducedPolicyGradient
(a)SVRPGvsG(PO)MDPonCart-pole.
(b)Self-NormalizedSVRPGvsSVRPGonSwimmer.
(c)Self-NormalizedSVRPGvsG(PO)MDPonSwimmer.
(d)Self-NormalizedSVRPGvsG(PO)MDPonHalf-Cheetah.
Figure1:Comparisonofon-lineperformanceoversampledtrajectories,with90%intervals.
scribedsofar,wecomparedagainsttheso-called
actor-only
G(PO)MDP,i.e.,withoutthebaseline.Tomovetowardsa
moregeneralunderstandingofthevarianceissueinpolicy
gradient,wealsotestSVRPGinan
actor-critic
scenario.To
doso,weconsiderthemorechallengingMuJoCo(
Todorov
etal.
,
2012
)Half-cheetahtask,a3Dlocomotiontaskthat
hasalargerstate-actionspacethanSwimmer.Figure
1d
comparesself-normalizedSVRPGandG(PO)MDPonHalf-
cheetah,usingthecriticsuggestedin(
Duanetal.
,
2016
)
forbothalgorithms.Resultsarepromising,showingthata
combinationofthebaselineusageandSVRG-likevariance
reductioncanyieldanimprovementthatthetwotechniques
alonearenotabletoachieve.Moreover,SVRPGpresents
anoticeablylowervariance.Theperformanceofactor-
criticG(PO)MDP
8
onHalf-Cheetahiscoherentwiththe
onereportedin(
Duanetal.
,
2016
).Otherresultsarenot
comparablesincewedidnotusethecritic.
8.Conclusion
Inthispaper,weintroducedSVRPG,avariantofSVRG
designedexplicitlyforRLproblems.Thecontrolprob-
8
Duanetal.
(
2016
)reportresultsonREINFORCE.However,
inspectionon
rllab
codeanddocumentationrevealsthatitisactu-
allyPGT(
Suttonetal.
,
2000
),whichisequivalenttoG(PO)MDP
(shownby
Peters&Schaal
,
2008b
).UsingthenameREINFORCE
inageneralwayisinaccurate,butwidespread.
lemconsideredinthepaperhasaseriesofdifthat
arenotcommoninSL.Amongthem,non-concavityand
approximateestimatesoftheFGhavebeenanalysedinde-
pendentlyinSL(e.g.,
Allen-Zhu&Hazan
,
2016
;
Reddi
etal.
,
2016a
;
Harikandehetal.
,
2015
)butnevercombined.
Nevertheless,themainissueinRListhenon-stationarityof
thesamplingprocesssincethedistributionunderlyingthe
objectivefunctionispolicy-dependent.Wehaveshownthat
byexploitingimportanceweightingtechniques,itispossi-
bletoovercomethisissueandpreservetheunbiasedness
ofthecorrectedgradient.Wehaveadditionallyshownthat,
undermildassumptionsthatareoftenvinRLappli-
cations,itispossibletoderiveconvergenceguaranteesfor
SVRPG.Finally,wehaveempiricallyshownthatpractical
variantsofthetheoreticalSVRPGversioncanoutperform
classicalactor-onlyapproachesonbenchmarktasks.Prelim-
inaryresultssupporttheeffectivenessofSVRPGalsowith
acommonlyusedbaselineforthepolicygradient.Despite
that,webelievethatitwillbepossibletoderiveabaseline
designedexplicitlyforSVRPGtoexploittheRLstructure
andtheSVRGideajointly.Anotherpossibleimprovement
wouldbetoemploythenaturalgradient(
Kakade
,
2002
)
tobettercontroltheeffectsofparameterupdatesonthe
varianceofimportanceweights.Futureworkshouldalso
focusonmakingbatchsizes
N
and
B
adaptive,assuggested
in(
Papinietal.
,
2017
).
StochasticVariance-ReducedPolicyGradient
Acknowledgments
ThisresearchwassupportedinpartbyFrenchMinistry
ofHigherEducationandResearch,Nord-Pas-de-Calais
RegionalCouncilandFrenchNationalResearchAgency
(ANR)underprojectExTra-Learn(n.ANR-14-CE24-0010-
01).
References
Allen-Zhu,ZeyuanandHazan,Elad.Variancereductionfor
fasternon-convexoptimization.In
InternationalConfer-
enceonMachineLearning
,pp.699Œ707,2016.
Baxter,JonathanandBartlett,PeterL.
policy-gradientestimation.
JournalofIntelli-
genceResearch
,15:319Œ350,2001.
Bietti,AlbertoandMairal,Julien.Stochasticoptimization
withvariancereductionforidatasetswith
sumstructure.In
AdvancesinNeuralInformationPro-
cessingSystems
,pp.1622Œ1632,2017.
Bottou,L
´
eonandLeCun,Yann.Largescaleonlinelearning.
In
Advancesinneuralinformationprocessingsystems
,
pp.217Œ224,2004.
Cauchy,Augustin.M
´
ethodeg
´
en
´
eralepourlar
´
esolution
dessystemesd'
´
equationssimultan
´
ees.
Comp.Rend.Sci.
Paris
,25(1847):536Œ538,1847.
Cortes,Corinna,Mansour,Yishay,andMohri,Mehryar.
Learningboundsforimportanceweighting.In
Advances
inneuralinformationprocessingsystems
,pp.442Œ450,
2010.
Defazio,Aaron,Bach,Francis,andLacoste-Julien,Simon.
Saga:Afastincrementalgradientmethodwithsupport
fornon-stronglyconvexcompositeobjectives.In
Ad-
vancesinNeuralInformationProcessingSystems
,pp.
1646Œ1654,2014a.
Defazio,Aaron,Domke,Justin,etal.Finito:Afaster,per-
mutableincrementalgradientmethodforbigdataprob-
lems.In
InternationalConferenceonMachineLearning
,
pp.1125Œ1133,2014b.
Du,SimonS.,Chen,Jianshu,Li,Lihong,Xiao,Lin,and
Zhou,Dengyong.Stochasticvariancereductionmethods
forpolicyevaluation.In
ICML
,volume70of
Proceedings
ofMachineLearningResearch
,pp.1049Œ1058.PMLR,
2017.
Duan,Yan,Chen,Xi,Houthooft,Rein,Schulman,John,and
Abbeel,Pieter.Benchmarkingdeepreinforcementlearn-
ingforcontinuouscontrol.In
InternationalConference
onMachineLearning
,pp.1329Œ1338,2016.
Furmston,ThomasandBarber,David.Aunifyingper-
spectiveofparametricpolicysearchmethodsformarkov
decisionprocesses.In
NIPS
,pp.2726Œ2734,2012.
Ghadimi,SaeedandLan,Guanghui.Stochastic
zeroth-ordermethodsfornonconvexstochasticprogram-
ming.
SIAMJournalonOptimization
,23(4):2341Œ2368,
2013.
Harikandeh,Reza,Ahmed,MohamedOsama,Virani,Alim,
Schmidt,Mark,Kone

cn
˚
y,Jakub,andSallinen,Scott.
Stopwastingmygradients:Practicalsvrg.In
Advancesin
NeuralInformationProcessingSystems
,pp.2251Œ2259,
2015.
Johnson,RieandZhang,Tong.Acceleratingstochastic
gradientdescentusingpredictivevariancereduction.In
Advancesinneuralinformationprocessingsystems
,pp.
315Œ323,2013.
Jur

c
´


cek,Filip.Reinforcementlearningforspokendialogue
systemsusingoff-policynaturalgradientmethod.In
Spo-
kenLanguageTechnologyWorkshop(SLT),2012IEEE
,
pp.7Œ12.IEEE,2012.
Kakade,ShamM.Anaturalpolicygradient.In
Advancesin
neuralinformationprocessingsystems
,pp.1531Œ1538,
2002.
Kingma,DiederikPandBa,Jimmy.Adam:Amethodfor
stochasticoptimization.
arXivpreprintarXiv:1412.6980
,
2014.
Kone

cn
˚
y,Jakub,Liu,Jie,Richt
´
arik,Peter,andTak
´
a

c,Mar-
tin.Mini-batchsemi-stochasticgradientdescentinthe
proximalsetting.
IEEEJournalofSelectedTopicsin
SignalProcessing
,10(2):242Œ255,2016.
Mairal,Julien.Incrementalmajorization-minimizationopti-
mizationwithapplicationtolarge-scalemachinelearning.
SIAMJournalonOptimization
,25(2):829Œ855,2015.
Nemirovskii,Arkadii,Yudin,DavidBorisovich,andDaw-
son,EdgarRonald.Problemcomplexityandmethod
efyinoptimization.1983.
Nesterov,Yurii.
Introductorylecturesonconvexoptimiza-
tion:Abasiccourse
,volume87.SpringerScience&
BusinessMedia,2013.
Owen,ArtB.
MonteCarlotheory,methodsandexamples
.
2013.
Palaniappan,BalamuruganandBach,Francis.Stochastic
variancereductionmethodsforsaddle-pointproblems.In
NIPS
,pp.1408Œ1416,2016.
StochasticVariance-ReducedPolicyGradient
Papini,Matteo,Pirotta,Matteo,andRestelli,Marcello.
Adaptivebatchsizeforsafepolicygradients.In
Ad-
vancesinNeuralInformationProcessingSystems
,pp.
3594Œ3603,2017.
Peters,JanandSchaal,Stefan.Reinforcementlearningof
motorskillswithpolicygradients.
NeuralNetworks
,21
(4):682Œ697,2008a.
Peters,JanandSchaal,Stefan.Reinforcementlearningof
motorskillswithpolicygradients.
Neuralnetworks
,21
(4):682Œ697,2008b.
Pirotta,Matteo,Restelli,Marcello,andBascetta,Luca.
Adaptivestep-sizeforpolicygradientmethods.In
Ad-
vancesinNeuralInformationProcessingSystems
,pp.
1394Œ1402,2013.
Pirotta,Matteo,Restelli,Marcello,andBascetta,Luca.Pol-
icygradientinlipschitzmarkovdecisionprocesses.
Ma-
chineLearning
,100(2):255Œ283,2015.ISSN1573-0565.
doi:10.1007/s10994-015-5484-1.
Precup,Doina.Eligibilitytracesforoff-policypolicyevalu-
ation.
ComputerScienceDepartmentFacultyPublication
Series
,pp.80,2000.
Reddi,SashankJ,Hefny,Ahmed,Sra,Suvrit,Poczos,Barn-
abas,andSmola,Alex.Stochasticvariancereductionfor
nonconvexoptimization.In
Internationalconferenceon
machinelearning
,pp.314Œ323,2016a.
Reddi,SashankJ,Sra,Suvrit,P
´
oczos,Barnab
´
as,andSmola,
Alex.Fastincrementalmethodfornonconvexoptimiza-
tion.
arXivpreprintarXiv:1603.06159
,2016b.
Robbins,HerbertandMonro,Sutton.Astochasticapprox-
imationmethod.
Theannalsofmathematicalstatistics
,
pp.400Œ407,1951.
Roux,NicolasL,Schmidt,Mark,andBach,FrancisR.A
stochasticgradientmethodwithanexponentialconver-
gence
ratefortrainingsets.In
AdvancesinNeural
InformationProcessingSystems
,pp.2663Œ2671,2012.
Rubinstein,ReuvenYReuvenY.Simulationandthemonte
carlomethod.Technicalreport,1981.
Sutton,RichardSandBarto,AndrewG.
Reinforcement
learning:Anintroduction
.MITpressCambridge,1998.
Sutton,RichardS,McAllester,DavidA,Singh,SatinderP,
andMansour,Yishay.Policygradientmethodsforre-
inforcementlearningwithfunctionapproximation.In
Advancesinneuralinformationprocessingsystems
,pp.
1057Œ1063,2000.
Thomas,PhilipS.andBrunskill,Emma.Policygradi-
entmethodsforreinforcementlearningwithfunction
approximationandaction-dependentbaselines.
CoRR
,
abs/1706.06643,2017.
Thomas,PhilipS,Theocharous,Georgios,and
Ghavamzadeh,Mohammad.off-
policyevaluation.In
AAAI
,pp.3000Œ3006,2015.
Todorov,Emanuel,Erez,Tom,andTassa,Yuval.Mujoco:
Aphysicsengineformodel-basedcontrol.In
Intelligent
RobotsandSystems(IROS),2012IEEE/RSJInternational
Conferenceon
,pp.5026Œ5033.IEEE,2012.
Weaver,LexandTao,Nigel.Theoptimalrewardbaseline
forgradient-basedreinforcementlearning.In
Proceed-
ingsoftheSeventeenthconferenceonUncertaintyin
intelligence
,pp.538Œ545.MorganKaufmann
PublishersInc.,2001.
Williams,RonaldJ.Simplestatisticalgradient-following
algorithmsforconnectionistreinforcementlearning.
Ma-
chinelearning
,8(3-4):229Œ256,1992.
Wu,Cathy,Rajeswaran,Aravind,Duan,Yan,Kumar,
Vikash,Bayen,AlexandreM,Kakade,Sham,Mordatch,
Igor,andAbbeel,Pieter.Variancereductionforpol-
icygradientwithaction-dependentfactorizedbaselines.
InternationalConferenceonLearningRepresentations
,
2018.acceptedasoralpresentation.
Xu,Tianbing,Liu,Qiang,andPeng,Jian.Stochasticvari-
ancereductionforpolicygradientestimation.
CoRR
,
abs/1710.06034,2017.
Zhao,Tingting,Hachiya,Hirotaka,Niu,Gang,and
Sugiyama,Masashi.Analysisandimprovementofpolicy
gradientestimation.In
AdvancesinNeuralInformation
ProcessingSystems
,pp.262Œ270,2011.
StochasticVariance-ReducedPolicyGradient
A.PolicyGradientEstimators
Wepresentabriefoverviewofthetwomostwidespreadgradientestimators(REINFORCE(
Williams
,
1992
)and
G(PO)MDP(
Baxter&Bartlett
,
2001
))bothinon-policyandoff-policysettings.Let
˝
=
fh
s
t
;a
t
ig
H
t
=0
=
f
z
t
g
H
t
=0
=
z
0:
H
isa
(
H
+1)
-stepstrajectory.Giventhat
˝
dependsontheMDP
M
=
fS
;
A
;
P
;
R
;;ˆ
g
andtheactualpolicy
ˇ
,the
trajectoryissaiddrawnfromdensitydistribution
p
(
˝
j
ˇ;M
)
as:
p
(
˝
j
ˇ;M
)=
ˆ
(
s
0
)
ˇ
(
z
0
)
H
Y
k
=1
P
(
s
k
j
z
k

1
)
ˇ
(
z
k
)
:
Wecannowrecalltheofpolicyperformance
J
(
ˇ

)=
E
˝
˘
p
(

ˇ

)
[
R
(
˝
)
j
M
]
;
where
R
(
˝
)=
P
H
t
=0

t
R
(
z
t
)
.Thepolicygradient
r
J
(

)
is
r
J
(

)=
E
˝
˘
p
(

ˇ

)
[
r
log
p
(
˝
j

)
R
(
˝
)]=
E
˝
˘
p
(

ˇ

)
2
4
H
X
j
=0

j
R
(
z
j
)
H
X
i
=0
r
log
ˇ
(
z
i
)
3
5
:
(7)
On-policysetting.
Considerapolicy
ˇ

andlet
D
N
=
f
˝
i
g
N
i
=1
beadatasetcollectedusingpolicy
ˇ

.TheREINFORCE
gradientestimator(
Williams
,
1992
)providesasimple,unbiasedwayofestimatingthegradient:
b
r
N
J
(

)=
1
N
N
X
n
=1
 
H
X
h
=0
r
log
ˇ

(
z
n
h
)
! 
H
X
h
=0

h
r
n
h

b
(
z
n
h
)
!
g
(
˝
n
j

):=
r
log
p
(
˝
n
j

)
R
(
˝
n
)
;
wheresubscriptsdenotethetimestep,superscriptsdenotethetrajectory,
r
n
h
istherewardactuallycollectedattime
h
fromtrajectory
˝
n
and
b
:
SA!
R
(e.g.,
Thomas&Brunskill
,
2017
).TheG(PO)MDPgradientestimator(
Baxter&
Bartlett
,
2001
)isaofREINFORCEwhichissubjecttolessvariance(
Zhaoetal.
,
2011
)whilepreservingthe
unbiasedness:
b
r
N
J
(

)=
1
N
N
X
n
=1
H
X
h
=0
 
h
X
k
=0
r
log
ˇ

(
z
n
h
)
!


h
r
n
h

b
(
z
n
h
)

g
(
˝
n
j

)
:
G(PO)MDPcanbeseenasamoreefcientimplementationoftheREINFORCEalgorithm.Infact,thelatterdoesnot
performanoptimalcreditassignmentsinceitignoresthattherewardattime
t
doesnotdependontheactionperformed
aftertime
t
.G(PO)MDPovercomesthisissuetakingintoaccountthecausalityofrewardsintheREINFORCEof
policygradient.
Off-policysetting.
Inoff-policysettingtwopolicies,calledbehavioural
ˇ
B
andtarget
ˇ
T
,areinvolved.Theisused
toselectactionsfortheinteractionwiththesystem,whilethesecondisusedtoevaluatetheagentperformanceanditis
improvedineachupdate.Supposenowthatweaimtoestimatetheperformanceofthetargetpolicy
ˇ
T
butwehavesamples
collectedusingpolicy
ˇ
B
.Wecanuseimportanceweightcorrectiontocorrecttheshiftinthedistributionandobtainan
unbiasedestimateof
J
(
ˇ
T
)
:
J
(
ˇ
T
)=
E
˝
˘
p
(

ˇ
T
)
[
R
(
˝
)]=
E
˝
˘
p
(

ˇ
B
)
[
!
(
˝
)
R
(
˝
)]
where
!
(
˝
j
ˇ
B
;ˇ
T
)=
p
(
˝;ˇ
T
)
p
(
˝
j
ˇ
B
)
=
!
(
z
0:
H
j
ˇ
B
;ˇ
T
)=
Q
H
w
=0
!
(
z
w
j
ˇ
B
;ˇ
T
)
and
!
(
z
w
j
ˇ
B
;ˇ
T
)=
ˇ
T
(
a
w
j
s
w
)
ˇ
B
(
a
w
j
s
w
)
Theoftheoff-policyversionof(
7
)is(e.g.,
Jur

c
´


cek
,
2012
)
r
J
(
ˇ
T
)=
E
˝
˘
p
(

ˇ
B
)

!
(
˝
j
ˇ
B
;ˇ
T
)
r
log
p
(
˝
j
ˇ
T
)
R
(
˝
)

=
E
˝
˘
p
(

ˇ
B
)

!
(
˝
j
ˇ
B
;ˇ
T
)
g
(
˝
j
ˇ
T
)

:
(8)
StochasticVariance-ReducedPolicyGradient
For
!
(
˝
j
ˇ
B
;ˇ
T
)
beingwellthebehaviouralpolicyshouldhavenon-zeroprobabilityofselectinganyactioninevery
statei.e.,
ˇ
B
(
a
j
s
)
>
0
forany
(
s;a
)
2SA
.Equation
8
isimportantforprovingTheorem
4.4
sinceitprovidesacommon
representationofREINFORCEandG(PO)MDP.
Theoff-policyversionofREINFORCEiseasilyobtainedbytakingtheempiricalaverageof(
8
):
r
J
(
ˇ
T
)=
1
N
N
X
n
=1
!
(
˝
n
j
ˇ
B
;ˇ
T
)
 
H
X
h
=0
r
log
ˇ
T
(
z
n
h
)
! 
H
X
h
=0

h
R
(
z
n
h
)
!
g
(
˝
n
j
ˇ
T
)
:
TheG(PO)MDPoff-policyestimatorisasfollows
r
J
(
ˇ
T
)=
1
N
N
X
n
=1
H
X
h
=0
 
h
X
k
=0
r
log
ˇ
T
(
z
n
k
)
!

h
R
(
z
n
h
)
!
(
z
0:
h
j
ˇ
B
;ˇ
T
)
!
(
˝
n
j
ˇ
B
;ˇ
T
)
g
(
˝
n
j
ˇ
T
)
:
B.Proofs
Inthissection,weprovealltheclaimsmadeinthepaper,withtheprimaryobjectiveofprovingTheorem
4.4
.Ourproofis
adaptedfromtheoneofTheorem2from(
Reddietal.
,
2016a
)andhasaverysimilarstructure,butwithalltheadditional
challengesandassumptionsdescribedinSection
4
.
Notethatinthefollowingwewillmakewideuseofthefollowingproperties.
AssumptionB.1.
Weconsideranestimate
b
r
N
J
(

)
asinEq.
2
suchthat
1.
On-policyUnbiasedEstimator.
E
˝
i
˘
ˇ
(


)
h
b
r
N
J
(

)
i
=
E
˝
i
˘
ˇ
(


)
""
1
N
N
X
i
=0
g
(
˝
i
j

)
#
=
r
J
(

)
2.
Off-policyUnbiasedEstimator.
E
˝
i
˘


ˇ
B
)
""
1
N
N
X
i
=0
!
(
˝
i
j
ˇ
B
;

)
g
(
˝
i
j

)
#
=
r
J
(

)
NotethattheseassumptionsarevbyREINFORCEandG(PO)MDP.

Wegivesomeadditionalwhichwillbeusefulintheproofs.
B.1.
ForarandomvariableX:
E
s
[
X
]=
E
˝
j
˘
ˇ
(

e

s
)
8
j
2
N
h
X
j
e

s
i
;
where
e

s
isinAlgorithm
2
and
N
=[0
;:::;N
)
.
Weintroducethenotation
˝
i;h
todenotethe
h
-thtrajectorycollectedusingpolicy

s
+1
i
where
s
willbeclearfromthe
context.
StochasticVariance-ReducedPolicyGradient
B.2.
Forarandomvariable
X
:
E
t
j
s
[
X
]
:
=
E
˝
j
˘
ˇ
(

e

s
)
8
j
2
N
˝
i;h
˘
ˇ
(


s
+1
i
)
8
h
2
B
;
for
i
=0
;:::;t
h
X
j
f

s
i
:
=
E
˝
j
˘
ˇ
(

e

s
)
8
j
2
N
""
E
˝
0
;h
˘
ˇ
(


s
+1
0
)
8
h
2
B
""
:::
E
˝
t;h
˘
ˇ
(


s
+1
t
)
8
h
2
B

X
j

s
+1
t

:::
j

s
+1
0
#
j
e

s
#
=
E
t

1
j
s
""
E
˝
t;h
˘
ˇ
(


s
+1
t
)

X
j

s
+1
t

#
wherethesequence
e

s
;

s
+1
0
;:::;

s
+1
t
isinAlgorithm
2
,
N
=[0
;:::;N
)
,and
B
=[0
;:::;B
)
.Toavoid
inconsistencies,wealso
E
(

1)
j
s
[
X
]
:
=
E
s
[
X
]
.
Intuitively,the
E
t
j
s
[

]
operatorcomputestheexpectedvaluewithrespecttothesamplingoftrajectoriesfromthesnapshot
e

s
uptothe
t
-thiterationincluded.Notethattheorderinwhichexpectedvaluesaretakenisimportantsinceeach

s
+1
t
is
functionofpreviouslysampledtrajectoriesandisusedtosamplenewones.
B.3.
ForrandomvectorsX,Y:
C
ov
s
(
X;Y
)
:
=Tr

E
t
j
s

(
X

E
t
j
s
[
X
])(
Y

E
t
j
s
[
Y
])
T

;
V
ar
s
[
X
]
:
=
C
ov
s
(
X;Y
)
;
where
Tr(

)
denotesthetraceofamatrix.Fromthelinearityofexpectedvaluewehavethefollowing:
V
ar
s
[
X
]=
E
s
h
k
X

E
s
[
X
]
k
2
i
(9)
C
ov
t
j
s
(
X;Y
)
and
V
ar
t
j
s
[
X
]
areinthesamewayfrom
E
t
j
s
[
X
]
.
B.4.
Thefullgradientestimationerroris:
e
s
:
=
b
r
N
J
(
e

s
)
r
J
(
e

s
)
B.5.
TheidealSVRPGgradientestimateis:
rr
J
(

s
+1
t
)
:
=
r
J
(
e

s
)+
g
(
˝
i
j

s
+1
t
)

!
(
˝
i
j

s
+1
t
;
e

s
)
g
(
˝
i
j
e

s
)
=
H
J
(

s
+1
t
)

b
r
N
J
(
e

s
)+
r
J
(
e

s
)
=
H
J
(

s
+1
t
)

e
s
BasicLemmas
WeprovetwobasicpropertiesoftheSVRPGupdate.
Lemma3.1.
Let
b
r
N
J
(

)
beanunbiasedestimatorof
(
1
)
andlet


2
argmin

f
J
(

)
g
.Then,theSVRGestimatein
(
4
)
is
unbiased
E
[
H
J
(

)]=
r
J
(

)
:
(5)
andregardlessofthemini-batchsize
B
:
9
V
ar[
H
J
(


)]=
V
ar
h
b
r
N
J
(


)
i
:
(6)
Proof.
E
[
H
J
(

)]=
E
h
b
r
N
J
(
e

)
i
+
E
h
b
r
B
J
(

)
i

E
""
1
B
B

1
X
i
=0
!
(
˝
i
j

;
e

)
g
(
˝
i
j
e

)
#
=
r
J
(
e

)+
r
J
(

)
r
J
(
e

)=
r
J
(

)
:
9
Foranyvector
x
,weuse
V
ar[
x
]
todenotethetraceofthecovariancematrix,i.e.,
Tr(
E

(
x

E
[
x
])(
x

E
[
x
])
T

)
.
StochasticVariance-ReducedPolicyGradient
Notethattheimportanceweightisnecessarytoguaranteeunbiasedness,sincethe
˝
i
aresampledfrom
ˇ

.As

!


,also
e

!


.Hence,bycontinuityof
J
(

)
:
V
ar[
H
J
(

)]
!
V
ar
h
b
r
N
J
(


)
i
+
1
B
V
ar

g
(
˝
j


)

˘
˘
˘
˘
˘
˘
!
(
˝
j


;


)
g
(
˝
j


)

=
V
ar
h
b
r
N
J
(


)
i
:
Notethatitisimportantthatthetrajectoriesusedinthesecondandthethirdtermarethesameforthevariancetovanish.
AncillaryLemmas
Beforeaddressingthemainconvergencetheorem,weprovesomeusefullemmas.
LemmaB.2.
UnderAssumption
4.1
,
J
(

)
isL-smoothforsomepositiveLipschitzconstant
L
J
.
Proof.
Byof
J
(

)
:
@
2
J
(

)
@
i
@
j
=
Z
T
@
2
@
i
@
j
p
(
˝
j

)
R
(
˝
)d
˝
=
Z
T
p
(
˝
j

)
r
log
p

(
˝
)
r
log
p

(
˝
)
T
R
(
˝
)d
˝
+
Z
T
ˇ

(
˝
)
@
2
@
i
@
j
log
p
(
˝
j

)
R
(
˝
)d
˝

sup
˝
2T
fjR
(
˝
)
jg

H
2
G
2
+
HF

(10)
=
1


H
1


RH

HG
2
+
F

;
where
10
isfromAssumption
4.1
.SincetheHessianisbounded,
J
(

)
isLipschitz-smooth.
LemmaB.3.
UnderAssumption
4.1
,whetherweusetheREINFORCEortheG(PO)MDPgradientestimator,
g
(
˝
j

)
is
LipschitzcontinuouswithLipschitzconstant
L
g
,i.e.,foranytrajectory
˝
2T
:


g
(
˝
j

)

g
(
˝
j

0
)


2
2

L
g





0


2
2
:
Proof.
ForbothREINFORCEandG(PO)MDP,
g
(
˝
j

)
isalinearcombinationoftermsofthekind
r
log
ˇ

(
a
t
j
s
t
)

t
r
t
(
Peters&Schaal
,
2008b
).ThesetermshaveboundedgradientfromthesecondinequalityofAssumption
4.1
andthefactthat
j
r
t
j
R
.IfabaselineisusedinREINFORCEorG(PO)MDP,weonlyneedtheadditionalassumptionthatsaidbaselineis
bounded.BoundedgradientimpliesLipschitzcontinuity.Finally,thelinearcombinationofLipschitzcontinuousfunctions
isLipschitzcontinuous.
LemmaB.4.
UnderAssumption
4.1
,whetherweusetheREINFORCEortheG(PO)MDPgradientestimator,forevery
˝
2T
and

2

,thereisapositiveconstant

<
1
suchthat:
k
g
(
˝
j

)
k
2
2


:
Proof.
ForREINFORCEwehave,fromAssumption
4.1
:
k
g
(
˝
j

)
k
2
2
=
kr
log
p

(
˝
)
R
(
˝
)
k
2
2
=





 
H

1
X
t
=0
r
log
p

(
a
t
j
s
t
)
! 
H

1
X
t
=0

t
r
t
!





2
2

H
2
G
2
(1


H
)
2
(1


)
2
R
2
dim(

)
:
=
ForG(PO)MDP,wedonothaveacompactexpressionfor
g
,butsinceitisderivedfromREINFORCEbyneglectingsome
termsofthekind
r
log
ˇ

(
a
t
j
s
t
)

t
r
t
(
Baxter&Bartlett
,
2001
;
Peters&Schaal
,
2008b
),theaboveboundstillholds.Ifa
baselineisusedinREINFORCEorG(PO)MDP,weonlyneedtheadditionalassumptionthatsaidbaselineisbounded.
StochasticVariance-ReducedPolicyGradient
LemmaB.5.
ForanyrandomvectorX,thevariance(asin
B.3
),canbeboundedasfollows:
V
ar
s
[
X
]

E
s
h
k
X
k
2
i
:
Proof.
Byusingbasicpropertiesofexpectedvalueandscalarvariance:
V
ar
s
[
X
]=
E
s
h
k
X

E
s
[
X
]
k
2
i
=
E
s
2
4
dim(
X
)
X
i
=1
(
X
i

E
s
[
X
i
])
2
3
5
=
dim(
X
)
X
i
=1
E
s
h
(
X
i

E
s
[
X
i
])
2
i

dim(
X
)
X
i
=1
E
s

X
2
i

=
E
s
2
4
dim(
X
)
X
i
=1
X
2
i
3
5
=
E
s
h
k
X
k
2
i
:
LemmaB.6.
UnderAssumption
4.1
,theexpectedsquarednormoftheSVRPGgradientcanbeboundedasfollows:
E
t
j
s
h


H
J
(

s
+1
t
)


2
2
i

E
t

1
j
s
h


r
J
(

s
+1
t
)


2
2
i
+
L
2
g
B
E
t

1
j
s





s
+1
t

e

s



2
2

+
1
N
V
ar
s
h
g
(

e

s
)
i
+

W
B
Proof.
Foreaseofnotationdenote
!
(
˝
i
):=
!
(
˝
i
j

s
+1
t
;
e

s
)
.Then,
E
t
j
s



H
J
(

s
+1
t
)


2
2

=
E
t
j
s
2
4





b
r
N
J
(
e

s
)+
1
B
B

1
X
i
=0
g
(
˝
i
j

s
+1
t
)

1
B
B

1
X
i
=0
!
(
˝
i
)
g
(
˝
i
j
e

s
)





2
3
5
=
E
t
j
s
2
4





b
r
N
J
(
e

s
)+
1
B
B

1
X
i
=0

g
(
˝
i
j

s
+1
t
)

!
(
˝
i
)
g
(
˝
i
j
e

s
)

r
J
(

s
+1
t
)
r
J
(
e

s
)





2
3
5

E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i
+
E
s




b
r
N
J
(
e

s
)

E
s
h
b
r
N
J
(
e

s
)
i



2

+
E
t
j
s
2
4





1
B
B

1
X
i
=0

g
(
˝
i
j

s
+1
t
)

!
(
˝
i
)
g
(
˝
i
j
e

s
)


E
t
j
s
""
1
B
B

1
X
i
=0

g
(
˝
i
j

s
+1
t
)

!
(
˝
i
)
g
(
˝
i
j
e

s
)

#





2
3
5
=
E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i
+
V
ar
s
h
b
r
N
J
(
e

s
)
i
+
E
t
j
s
2
4





1
B
B

1
X
i
=0

g
(
˝
i
j

s
+1
t
)

!
(
˝
i
)
g
(
˝
i
j
e

s
)


E
t
j
s
""
1
B
B

1
X
i
=0

g
(
˝
i
j

s
+1
t
)

!
(
˝
i
)
g
(
˝
i
j
e

s
)

#





2
3
5
(11)
=
E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i
+
1
N
V
ar
s
h
g
(

e

s
)
i
+
E
t
j
s
2
4





1
B
B

1
X
i
=0

g
(
˝
i
j

s
+1
t
)

!
(
˝
i
)
g
(
˝
i
j
e

s
)


E
t
j
s
""
1
B
B

1
X
i
=0

g
(
˝
i
j

s
+1
t
)

!
(
˝
i
)
g
(
˝
i
j
e

s
)

#





2
3
5
(12)

E
t

1
j
s
h


r
J
(

s
+1
t
)


2
2
i
+
1
N
V
ar
s
h
g
(

e

s
)
i
+
E
t
j
s
2
4





1
B
B

1
X
i
=0

g
(
˝
i
j

s
+1
t
)

!
(
˝
i
)
g
(
˝
i
j
e

s
)






2
2
3
5
(13)

E
t

1
j
s
h


r
J
(

s
+1
t
)


2
2
i
+
1
N
V
ar
s
h
g
(

e

s
)
i
+
1
B
2
B

1
X
i
=0
E
t
j
s




g
(
˝
i
j

s
+1
t
)

!
(
˝
i
)
g
(
˝
i
j
e

s
)

g
(
˝
i
j
e

s
)



2
2


E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i
+
1
N
V
ar
s
h
g
(

e

s
)
i
StochasticVariance-ReducedPolicyGradient
+
1
B
2
B

1
X
i
=0
E
t
j
s




g
(
˝
i
j

s
+1
t
)

g
(
˝
i
j
e

s
)



2

+
1
B
2
B

1
X
i
=0
E
t
j
s




g
(
˝
i
j
e

s
)

!
(
˝
i
)
g
(
˝
i
j
e

s
)



2


E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i
+
1
N
V
ar
s
h
g
(

e

s
)
i
+
L
2
g
B
E
t

1
j
s





s
+1
t

e

s



2

+
1
B
2
B

1
X
i
=0
E
t
j
s




(1

!
(
˝
i
))
g
(
˝
i
j
e

s
)



2

(14)

E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i
+
1
N
V
ar
s
h
g
(

e

s
)
i
+
L
2
g
B
E
t

1
j
s





s
+1
t

e

s



2

+
1
B
2
B

1
X
i
=0
E
t
j
s

(
!
(
˝
i
)

1)
2

(15)
=
E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i
+
1
N
V
ar
s
h
g
(

e

s
)
i
+
L
2
g
B
E
t

1
j
s





s
+1
t

e

s



2

+

B
2
B

1
X
i
=0
V
ar
t
j
s
[
!
(
˝
i
)]

E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i
+
1
N
V
ar
s
h
g
(

e

s
)
i
+
L
2
g
B
E
t

1
j
s





s
+1
t

e

s



2

+

W
B
;
(16)
where(
11
)isfrom(
9
),(
12
)isfromtheof
b
r
N
J
(

)
,(
13
)isfromLemma
B.5
,(
14
)isfromLemma
B.3
,(
15
)is
fromLemma
B.4
,and(
16
)isfromAssumption
4.3
.
LemmaB.7.
UnderAssumption
4.1
,foranyfunction
'
(

s
+1
t
)
whichisdeterministicfora

s
+1
t
:


E
t
j
s

H
J
(

s
+1
t
)
;'
(

s
+1
t
)


E
t
j
s

r
J
(

s
+1
t
)
;'
(

s
+1
t
)




1
2
N
V
ar
s
h
g
(

e

s
)
i
+
1
2
E
t

1
j
s
h


'
(

s
+1
t
)


2
i
Proof.
E
t
j
s

H
J
(

s
+1
t
)
;'
(

s
+1
t
)

=
E
t
j
s

rr
J
(

s
+1
t
)
;'
(

s
+1
t
)

+
E
t

1
j
s

e
s
;'
(

s
+1
t
)

(17)
=
E
t
j
s

r
J
(

s
+1
t
)
;'
(

s
+1
t
)

+
E
t

1
j
s

e
s
;'
(

s
+1
t
)

(18)
=
E
t
j
s

r
J
(

s
+1
t
)
;'
(

s
+1
t
)

+

E
t
j
s
[
e
s
]
;
E
t
j
s

'
(

s
+1
t
)

+
C
ov
t

1
j
s

b
r
N
J
(
e

s
)
;'
(

s
+1
t
)

(19)
=
E
t
j
s

r
J
(

s
+1
t
)
;'
(

s
+1
t
)

+
C
ov
t

1
j
s

b
r
N
J
(
e

s
)
;'
(

s
+1
t
)

(20)
where
(
17
)
isfrom
B.5
;
(
18
)
isfromthefactthat
rr
J
(

s
+1
t
)
isbothunbiasedandindependentfrom
'
(

s
+1
t
)
w.r.t.thesamplingattime
t
alone,whichisnottruefor
H
J
(

s
+1
t
)
;
(
19
)
isfromthefactthat
r
J
(
e

s
)
isconstantw.r.t.
V
ar
s
[

]
;(
20
)isfrom
E
t
j
s
[
e
s
]=0
.Hence:


E
t
j
s

H
J
(

s
+1
t
)
;'
(

s
+1
t
)


E
t
j
s

r
J
(

s
+1
t
)
;'
(

s
+1
t
)



=



C
ov
t

1
j
s

b
r
N
J
(
e

s
)
;'
(

s
+1
t
)





r
V
ar
s
h
b
r
N
J
(
e

s
)
i

q
V
ar
t

1
j
s

'
(

s
+1
t
)

(21)

1
2
V
ar
s
h
b
r
N
J
(
e

s
)
i
+
1
2
V
ar
t

1
j
s

'
(

s
+1
t
)

(22)
=
1
2
N
V
ar
s
h
g
(

e

s
)
i
+
1
2
V
ar
t

1
j
s

'
(

s
+1
t
)

(23)

1
2
N
V
ar
s
h
g
(

e

s
)
i
+
1
2
E
t

1
j
s
h


'
(

s
+1
t
)


2
i
;
(24)
where(
21
)comesfromCauchy-Schwarzinequality
j
C
ov(
X;Y
)
j
=
j
E

(
X


X
)(
Y


Y
)
T

j
E

(
X


X
)
2

1
=
2
E

(
Y


Y
)
2

1
=
2
=
p
V
ar(
X
)
V
ar(
Y
)
;
StochasticVariance-ReducedPolicyGradient
(
22
)isfromYoung'sinequality,(
23
)isfromtheof
b
r
N
J
(

)
,and(
24
)isfromLemma
B.5
.
LemmaB.8.
UnderAssumptions
4.1
ans
4.3
,theexpectedsquarednormofthetruegradient
r
J
(

s
+1
t
)
,forappropriate
choicesof

t

0
and

t
>
0
,canbeboundedasfollows:
E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i

R
s
+1
t
+1

R
s
+1
t

t
+
d
t
V
N

t
+
f
t
W
B

t
;
where
R
s
+1
t
:
=
E
t

1
j
s

J
(

s
+1
t
)

c
t




s
+1
t

e

s



2

;
c
m
=0
;
c
t
=
c
t
+1

1+

t

t
+

t
+

2
t
L
2
B

+

2
t
L
3
2
B
;

t
=

t

1
2

c
t
+1

t


t
L
2


t
c
t
+1

;
d
t
=

t
2
(1+2
c
t
+1
+

t
L
+2

t
c
t
+1
)
;
f
t
=

2
t

L
+2
c
t
+1
)
2
;
where
L
=max
f
L
J
;L
g
g
,i.e.,thegreateroftheLipschitzconstantsfromLemmas
B.2
and
B.3
.
Inparticular,thefollowingconstraintson

t
and

t
aresuf
0


t
<
1

2
c
t
+1
=

t
L
+2
c
t
+1

t
>
2
c
t
+1
:
Proof.
Wehave:
E
t
j
s

J
(

s
+1
t
+1
)


E
t
j
s

J
(

s
+1
t
)+

r
J
(

s
+1
t
)
;

s
+1
t
+1


s
+1
t


L
2



s
+1
t
+1


s
+1
t


2

(25)
=
E
t
j
s

J
(

s
+1
t
)+

t

r
J
(

s
+1
t
)
;
H
J
(

s
+1
t
)



2
t
L
2


H
J
(

s
+1
t
)


2

(26)

E
t
j
s

J
(

s
+1
t
)+

t


r
J
(

s
+1
t
)


2


2
t
L
2


H
J
(

s
+1
t
)


2



t
2
N
V
ar
s
h
g
(

e

s
)
i


t
2
E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i
;
(27)
where
(
25
)
isfromtheL-smoothnessof
J
(

)
(
Nesterov
,
2013
)and
(
26
)
isfromtheSVRPGupdate.Inequality
27
follows
fromLemma
B.7
bynoticingthat
r
J
(

s
+1
t
)
isadeterministicfunctiongiven

s
+1
t
.Asaconsequence,wecandirectly
applyLemma
B.7
with
˚
(

s
+1
t
)
:
=
r
J
(

s
+1
t
)
.
Next,wehave:
E
t
j
s





s
+1
t
+1

e

s



2

=
E
t
j
s





s
+1
t
+1


s
+1
t
+

s
+1
t

e

s



2

=
E
t
j
s




s
+1
t
+1


s
+1
t


2
+




s
+1
t

e

s



2
+2
D

s
+1
t
+1


s
+1
t
;

s
+1
t

e

s
E

(28)
=
E
t
j
s


2
t


H
J
(

s
+1
t
)


2
+




s
+1
t

e

s



2
+2

t
D
H
J
(

s
+1
t
)
;

s
+1
t

e

s
E

(29)

E
t
j
s


2
t


H
J
(

s
+1
t
)


2
+




s
+1
t

e

s



2
+2

t
D
r
J
(

s
+1
t
)
;

s
+1
t

e

s
E

StochasticVariance-ReducedPolicyGradient
+

t
N
V
ar
s
h
g
(

e

s
)
i
+

t
E
t

1
j
s





s
+1
t

e

s



2

(30)

E
t
j
s


2
t


H
J
(

s
+1
t
)


2
+




s
+1
t

e

s



2

+2

t
E
t

1
j
s
h



D
r
J
(

s
+1
t
)
;

s
+1
t

e

s
E



i
+

t
N
V
ar
s
h
g
(

e

s
)
i
+

t
E
t

1
j
s





s
+1
t

e

s



2


E
t
j
s


2
t


H
J
(

s
+1
t
)


2
+




s
+1
t

e

s



2

+2

t
E
t

1
j
s
h


r
J
(

s
+1
t
)






s
+1
t

e

s



i
+

t
N
V
ar
s
h
g
(

e

s
)
i
+

t
E
t

1
j
s





s
+1
t

e

s



2


E
t
j
s


2
t


H
J
(

s
+1
t
)


2
+




s
+1
t

e

s



2

+2

t
E
t

1
j
s

1
2

t


r
J
(

s
+1
t
)


2
+

t
2




s
+1
t

e

s



2

(31)
+

t
N
V
ar
s
h
g
(

e

s
)
i
+

t
E
t

1
j
s





s
+1
t

e

s



2

;
(32)
where
(
28
)
isobtainedusingthetriangularinequality,
(
29
)
isfromtheSVRPGupdate,
(
30
)
isfromLemma
B.7
with
˚
(

s
+1
t
)
:
=

s
+1
t

~

s
,
(
31
)
isfromCauchy-Schwarzinequality,and
(
32
)
isfromYoung'sinequalityinthe`Peter-Paul'
variant.Letusconsiderthefollowingfunction:
R
s
+1
t
+1
:
=
E
t
j
s

J
(

s
+1
t
+1
)

c
t
+1




s
+1
t
+1

~

s



2

:
(33)
Theobjectiveisnowtoprovidealowerboundtoit.
R
s
+1
t
+1

E
t
j
s

J
(

s
+1
t
)


2
t
L
2


H
J
(

s
+1
t
)


2

+
E
t

1
j
s
h

t
2


r
J
(

s
+1
t
)


2
i


t
2
N
V
ar
s
h
g
(

~

s
)
i

c
t
+1
E
t
j
s





s
+1
t
+1

~

s



2

(34)

E
t
j
s

J
(

s
+1
t
)


2
t
L
2


H
J
(

s
+1
t
)


2

+
E
t

1
j
s
h

t
2


r
J
(

s
+1
t
)


2
i


t
2
N
V
ar
s
h
g
(

~

s
)
i

c
t
+1
E
t
j
s


2
t


H
J
(

s
+1
t
)


2
+




s
+1
t

~

s



2


2
c
t
+1

t
E
t

1
j
s

1
2

t


r
J
(

s
+1
t
)


2
+

t
2




s
+1
t

~

s



2


c
t
+1

t
N
V
ar
s
h
g
(

~

s
)
i

c
t
+1

t
E
t

1
j
s





s
+1
t

~

s



2

(35)
=
E
t

1
j
s

J
(

s
+1
t
)


c
t
+1
(1+

t

t
+

t
)
E
t

1
j
s





s
+1
t

~




2



2
t

L
2
+
c
t
+1

E
t
j
s
h


H
J
(

s
+1
t
)


2
i
+

t
2

1

2
c
t
+1

t

E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i


t
2
N
(1+2
c
t
+1
)
V
ar
s
h
g
(

~

s
)
i

E
t

1
j
s

J
(

s
+1
t
)


c
t
+1
(1+

t

t
+

t
)
E
t

1
j
s





s
+1
t

~




2



2
t

L
2
+
c
t
+1

E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i
+
1
N
V
ar
s
h
g
(

~

s
)
i
+
L
2
B
E
t

1
j
s





s
+1
t

~

s



2

+

W
B

+

t
2

1

2
c
t
+1

t

E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i


t
2
N
(1+2
c
t
+1
)
V
ar
s
h
g
(

~

s
)
i
(36)
StochasticVariance-ReducedPolicyGradient
=
E
t

1
j
s

J
(

s
+1
t
)


c
t
+1

1+

t

t
+

t
+

2
t
L
2
B

+

2
t
L
3
2
B





s
+1
t

~




2

+

t

1
2

c
t
+1

t


t
L
2


t
c
t
+1

E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i


t
2
N
(1+2
c
t
+1
+

t
L
+2

t
c
t
+1
)
V
ar
s
h
g
(

~

s
)
i


2
t
(
L
+2
c
t
+1

W
2
B
=
R
s
+1
t
+
t
E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i

d
t
N
V
ar
s
h
g
(

~

s
)
i

f
t
B
W;

R
s
+1
t
+
t
E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i

d
t
N
V

f
t
B
W;
(37)
where
(
34
)
isfrom
(
27
)
noticingthat
E
t
j
s
h


r
J
(

s
+1
t
)


2
i
=
E
t

1
j
s
h


r
J
(

s
+1
t
)


2
i
,
(
35
)
isfrom
(
32
)
,
(
36
)
isfrom
Lemma
B.6
,and(
37
)isfromAssumption
4.2
.Tocompletetheproof,besidesrearrangingterms,wehavetoensurethat

t
>
0
foreach
t
.Thisgivestheconstraintson

t
and

t
.
Maintheorem
Weprovidetheproofoftheconvergencetheorem:
Theorem4.4
(ConvergenceoftheSVRPGalgorithm)
.
AssumetheREINFORCEortheG(PO)MDPgradientestimatoris
usedinSVRPG(seeEquation
(
4
)
).UnderAssumptions
4.1
,
4.2
and
4.3
,theparametervector

A
returnedbyAlgorithm
2
after
T
=
m

S
iterationshas,forsomepositiveconstants
 ;;˘
andforproperchoiceofthestepsize

andtheepoch
size
m
,thefollowingproperty:
E
h
kr
J
(

A
)
k
2
2
i

J
(


)

J
(

0
)
 T
+

N
+
˘
B
;
where


isaglobaloptimumand
 ;;˘
dependonlyon
G;F;V;W;
and
m
.
Proof.
Weprovethetheoremforthefollowingvaluesoftheconstants:
 
:
=min
t
f

t
g
;
:
=
max
t
f
d
t
g
V
 
;˘
:
=
max
t
f
f
t
g
W
 
;
where

,
d
t
and
f
t
areinLemma
B.8
.StartingfromLemma
B.8
,summingoveriterationsofanepoch
s
andusing
telescopicsumweobtain
m

1
X
t
=0
E
t
j
s



r
J
((

s
+1
t
))
2




P
m

1
t
=0

R
s
+1
t
+1

R
s
+1
t

 
+

N
+
m˘
B
=
R
s
+1
m

R
s
+1
0
 
+

N
+
m˘
B
Byusingtheof
R
s
t
in(
33
),thefactthat
c
m
=0
and

s
+1
0
=
e

s
=

s
m
,wecanstatethat:
R
s
+1
m

R
s
+1
0
=
E
m
j
s

J
(

s
+1
m
)

c
m




s
+1
m

e

s



2


E
0
j
s

J
(

s
+1
0
)

c
0




s
+1
0

e

s



2

=
E
m
j
s

J
(

s
+1
m
)


E
0
j
s
h
J
(
e

s
)
i
=
E
m
j
s
h
J
(
e

s
+1
)

J
(
e

s
)
i
StochasticVariance-ReducedPolicyGradient
Next,summingoverepochs:
S

1
X
s
=0
m

1
X
t
=0
E
t
j
s



r
J
((

s
+1
t
))
2




P
S

1
s
=0
E
m
j
s
h
J
(
~

s
+1
)

J
(
~

s
)
i
 
+
T
N
+
T˘
B

E
h
J
(
~

S
)

J
(
~

0
)
i
 
+
T
N
+
T˘
B
(38)

J
(


)

J
(

0
)
 
+
T
N
+
T˘
B
;
(39)
wheretheexpectationin
(
38
)
isw.r.t.allthetrajectoriessampledinarunofAlgorithm
2
and
(
39
)
isfromtheof


(i.e.,thepolicyperformancemaximizer).Finally,weconsidertheexpectationw.r.t.allsourcesofrandomness,including
theuniformsamplingoftheoutputparameter:
E
h


r
J
((

s
+1
t
))


2
i
=
1
T
S

1
X
s
=0
m

1
X
t
=0
E
t
j
s
h


r
J
((

s
+1
t
))


2
i

J
(


)

J
(

0
)
 T
+

N
+
˘
B
:
C.ApplicabilitytoGaussianPolicies
WeprovidemoredetailsontheapplicabilityofTheorem
4.4
onthecaseofGaussianpolicies.Westartfromthecaseof
one-dimensionalboundedactionspace
Aˆ
R
,linearmean

(
s
)=

T
˚
(
s
)
andedstandarddeviation
˙
:
ˇ

(
a
j
s
)=
1
p
2
ˇ˙
exp
(

(

T
˚
(
s
)

a
)
2
2
˙
2
)
;
where
˚
(
s
)

M
˚
isaboundedfeaturevector,andweseeunderwhichconditionsthethreeassumptionsofSection
4
hold.
Assumption4.1
(Onpolicyderivatives)
.
Foreachstate-actionpair
(
s;a
)
,anyvalueof

,andallparametercomponents
i;j
thereexistconstants
0

G;F<
1
suchthat:
jr

i
log
ˇ

(
a
j
s
)
j
G;




@
2
@
i
@
j
log
ˇ

(
a
j
s
)





F:
FortheGaussianpolicyabove,it'seasytoshowthat:
r

i
log
ˇ

(
˝
)=
˚
i
(
s
)
a


T
˚
(
s
)
˙
2
;
@
2
@
i
@
j
log
ˇ

(
˝
)=
˚
i
(
s
)
˚
j
(
s
)
˙
2
:
Hence,Assumption
4.1
isautomatically
10
bytaking
G
=
M
˚
jAj
˙
2
and
F
=
M
2
˚
˙
2
.
Assumption4.2
(Onthevarianceofthegradientestimator)
.
Thereisaconstant
V<
1
suchthat,foranypolicy
ˇ

:
V
ar[
g
(


)]

V:
Asmentioned,(
Pirottaetal.
,
2013
)providesaboundonthevarianceoftheREINFORCEestimator,adaptedfrom(
Zhao
etal.
,
2011
),whichdoesnotdependon

:
V
ar
h
b
r
N
J
(

i
)
i

R
2
M
2
˚
H
(1


H
)
2
N˙
2
(1


)
2
:
ThesameauthorsprovideasimilarboundforG(PO)MDP.
10
Thisreliesonthefactthat

T
˚
(
s
)
liesinbounded
A
.Inpractice,thisisusuallyenforcedbyclippingtheactionselectedby
ˇ

.A
morerigorouswaywouldbetoemploythetruncatedGaussiandistribution.
StochasticVariance-ReducedPolicyGradient
Assumption4.3
(Onthevarianceofimportanceweights)
.
Thereisaconstant
W<
1
suchthat,foreachpairofpolicies
encounteredinAlgorithm
2
andforeachtrajectory,
V
ar
[
!
(
˝
j

1
;

2
)]

W;
8

1
;

2
2
R
d
;˝
˘
p
(


1
)
:
Itisnotedin(
Cortesetal.
,
2010
)that,foranytwoGaussiandistributions
N
(

1
;˙
1
)
and
N
(

2
;˙
2
)
,thevarianceofthe
importanceweightsfromthelattertotheformerisboundedwhenever
˙
2
>
p
2
2
˙
1
.Thisisautomaticallybyour
ed-varianceGaussianpolicies,since
˙
2
=
˙
1
=
˙
.
WenowexaminesomegeneralizationsofthesimpleGaussianpolicyabovethatcanbefoundinapplications:
Multi-dimensionalactions.
Whenactionsaremulti-dimensional,factoredGaussianpoliciesaretypicallyemployed,so
theresultsextendtriviallyfromtheone-dimensionalcase.Actualmulti-variateGaussiandistributionswouldrequiremore
calculations,butwedonotexpectsubstantiallydifferentresults.
Non-linearmean.
Incomplexcontinuoustasks,

(
s
)
oftenrepresentsadeepneuralnetwork,ormulti-layerperceptron,
where

aretheweightsofthenetwork.Theanalysisofandsecondorderlog-derivativesinsuchascenarioisbeyond
thescopeofthispaper.
Adaptivevariance.
Itisacommonpracticetolearnalsothevarianceofthepolicyinordertoadaptthedegreeof
exploration.Thevariance(ordiagonalcovariancematrixinthemulti-dimensionalcase)canbelearnedasaseparate
parameterorbestate-dependentlikethemean.Inanycase,adaptivevariancemustbecarefullyemployedsinceitcanclearly
undermineallthethreeassumptionsofTheorem
4.4
.
D.PracticalSVRPGVersions
WeprovidemoredetailsonthepracticalvariantsofSVRPG.
D.1.AdpativeStepSize
Algorithm3
Adam
Input:
Agradientestimate
g
t
andparameters

1
,

2
,

and

.

t
=

1

t

1
+(1


1
)
g
t

t
=

2

t

1
+(1


2
)
g
t

g
t
(

istheHadamard(component-wise)product)
^

t
=

t
1


t
1
^

t
=

t
1


t
2

g
t
)=

p
^

t
+

^

t
Return:
Theincrement

g
t
)
oftheparameters.
Letusgiveadeeperinsightonthetwodifferentlearningrateschedulesusedbyouralgorithm.Wereportpseudo-code
oftheoriginalADAM(
Kingma&Ba
,
2014
)inAlgorithm
3
.Asmentioned,weusetwodistinctinstancesofADAMto
managedifferentsourcesofvariance:onerelatedtothesnapshots,andonetothesub-iterations.InthiswaytheADAM
associatedtothesnapshotstakesintoaccountonlythehistoryofgradientmomentsatthesnapshots.ByusingAlgorithm
3
asasubroutine
ADAM
(
g;;
)
,wecanexplicitlyourgradientupdates:

s
+1
1
=
e

s
+
ADAM

b
r
N
J
(
e

s
)
;
1
;
2
;

;

s
+1
t
+1
=
ADAM

H
J
(

s
+1
t
)
;
1
;
2
;

2

for
t
=1
;:::;m

1
;
whereseparatehistoriesarekeptforestimatedmoments

FG
;
IS
andestimatedsecondmoments

FG
;
IS
.The
meta-parameters
;
1
;
2
areconstantandsettodefaultvaluesorwithminormanualtuning(seetable
1
).Notethatwe
StochasticVariance-ReducedPolicyGradient
doublethesub-iterations'learningrateforthesnapshotADAMsincewecanrelyonalargernumberoftrajectories(
N
insteadof
B
)tocontrolthevariance.
D.2.Baseline
ThebaselineusedintheHalfCheetahexperimentistheoneusedin(
Duanetal.
,
2016
).Itisalinearstate-valuefunction
estimator,orcritic.The(time-varying)featureencodingforthelinearbaselineis:
˚
(
s;t
)=[
s;s

s;
0
:
01
t;
(0
:
01
t
)
2
;
(0
:
01
t
)
3
;
1]
;
where
s
2
R
d
isthestatevectorand

istheelement-wiseproduct.Thebaselineisthen:
b
(
s
t
;a
t
)=

T
˚
(
s
t
;t
)
:
Thebaselineisfromscratchateachpolicygradientiteration,withleastsquares,tomatchstate-valuefunction
V
ˇ
(
s
)
.
WhenusedwithSVRPG,thecriticparameter

isupdatedonlyatthesnapshot.
E.ExperimentalDetails
WedescribetheRLtasksofSection
7
inmoredetail:
1.
Cart-PoleBalancing
:aninvertedpendulummountedonacartmustbekeptstandingbymovingthecartbackwardor
forward;4-dimensionalstatespace:cartpositionx,poleangle

,cartvelocity
_
x
andpolevelocity
_

;1-dimensional
actionspace:thehorizontalforceappliedtothecartbody.Rewardfunctionisas
r
(
s;a
):=10

(1

cos
(

))

10

5
k
a
k
2
.Theepisodesterminatewhen
j
x
j
>
2
:
4
or
j

j
>
0
:
2
orthenumberoftimestepsTisgreaterthan100.
2.
MujocoSwimmer
:asnake-likerobotimmersedinamustmoveforward;13-dimensionalstatespace:3links
velocities(
v
x
and
v
y
ofcenterofmasses)and2actuatedjointsangles.2-dimensionalactionspace:thetwomomentums
appliedonactuatedjoints.Therewardfunctionisas
r
(
s;a
):=
v
x

10

4
k
a
k
2
2
.Theepisodesterminatewhen
thenumberoftimestepsTisgreaterthan500.
3.
MujocoHalfCheetah
:aplanarbipedrobotmustmoveforward;20-dimensionalstatespace:9linksand6actuated
jointsangles.6-dimensionalactionspace:the6momentumsappliedonactuatedjoints.Therewardfunctionis
as
r
(
s;a
):=
v
x

0
:
05
k
a
k
2
2
.TheepisodesterminatewhenthenumberoftimestepsTisgreaterthan500.
Alltheparametersusedintheexperiments,includingneuralnetworkarchitectures,arereportedinthefollowingtable:
Table1:ParametersusedintheexperimentsofSection
7
.
Cart-PoleSwimmerHalfCheetah
NNhiddenweights
832x32100x50x25
NNactivation
tanhtanhtanh
Adam

(SVRPG)
5

10

2
10

3
10

3
Adam

(GPOMDP)
10

2
10

3
10

2
Adam

1
0.90.90.9
Adam

2
0.990.990.99
Snapshotbatchsize
N
(SVRPG)
100100100
Mini-batchsize
B
(SVRPG)
101010
Batchsize(GPOMDP)
1010100
Maxnumberofsub-iterations
502020
Taskhorizon
100500500
Baseline
NoNoYes
Discountfactor

0.990.9950.99
Totalnumberoftrajectories
100002000050000
Wherenotd,meta-parametersaresharedamongG(PO)MDPandSVRPG.Referto(
Duanetal.
,
2016
)formore
detailsaboutG(PO)MDP(REINFORCEinthepaper)ontheHalfCheetahtask.
StochasticVariance-ReducedPolicyGradient
Videos:
ThesupplementarymaterialsincludevideosshowingthebehavioroftheSVRPGagentsonthethree
consideredtasks.
"
87,There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average,http://arxiv.org/pdf/1806.05594v3.pdf,https://github.com/benathi/fastswa-semi-sup,"PublishedasaconferencepaperatICLR2019
T
HEREARE
M
ANY
C
ONSISTENT
E
XPLANATIONSOF
U
NLABELED
D
ATA
:W
HY
Y
OU
S
HOULD
A
VERAGE
BenAthiwaratkun,MarcFinzi,PavelIzmailov,AndrewGordonWilson
{pa338,maf388,pi49,andrew}@cornell.edu
CornellUniversity
Abstract
Presentlythemostsuccessfulapproachestosemi-supervisedlearningarebased
on
consistencyregularization
,wherebyamodelistrainedtoberobusttosmall
perturbationsofitsinputsandparameters.Tounderstandconsistencyregularization,
weconceptuallyexplorehowlossgeometryinteractswithtrainingprocedures.The
consistencylossdramaticallyimprovesgeneralizationperformanceoversupervised-
onlytraining;however,weshowthatSGDstrugglestoconvergeontheconsistency
lossandcontinuestomakelargestepsthatleadtochangesinpredictionson
thetestdata.Motivatedbytheseobservations,weproposetotrainconsistency-
basedmethodswithStochasticWeightAveraging(SWA),arecentapproachwhich
averagesweightsalongthetrajectoryofSGDwithalearningrateschedule.
Wealsopropose
fast-SWA
,whichfurtheracceleratesconvergencebyaveraging
multiplepointswithineachcycleofacyclicallearningrateschedule.Withweight
averaging,weachievethebestknownsemi-supervisedresultsonCIFAR-10and
CIFAR-100,overmanydifferentquantitiesoflabeledtrainingdata.Forexample,
weachieve5.0%erroronCIFAR-10withonly4000labels,comparedtothe
previousbestresultintheliteratureof6.3%.
1I
NTRODUCTION
Recentadvancesindeepunsupervisedlearning,suchasgenerativeadversarialnetworks(GANs)
(
Goodfellowetal.
,
2014
),haveledtoanexplosionofinterestinsemi-supervisedlearning.Semi-
supervisedmethodsmakeuseofbothunlabeledandlabeledtrainingdatatoimproveperformance
overpurelysupervisedmethods.Semi-supervisedlearningisparticularlyvaluableinapplications
suchasmedicalimaging,wherelabeleddatamaybescarceandexpensive(
Oliveretal.
,
2018
).
Currentlythebestsemi-supervisedresultsareobtainedby
consistency-enforcing
approaches(
Bach-
manetal.
,
2014
;
LaineandAila
,
2017
;
TarvainenandValpola
,
2017
;
Miyatoetal.
,
2017
;
Park
etal.
,
2017
).Thesemethodsuseunlabeleddatatostabilizetheirpredictionsunderinputorweight
perturbations.Consistency-enforcingmethodscanbeusedatscalewithstate-of-the-artarchitectures.
Forexample,therecentMeanTeacher(
TarvainenandValpola
,
2017
)modelhasbeenusedwiththe
Shake-Shake(
Gastaldi
,
2017
)architectureandhasachievedthebestsemi-supervisedperformanceon
theconsequentialCIFARbenchmarks.
Thispaperisaboutconceptuallyunderstandingandimprovingconsistency-basedsemi-supervised
learningmethods.Ourapproachcanbeusedasaguideforexploringhowlossgeometryinteractswith
trainingproceduresingeneral.Weprovideseveralnovelobservationsaboutthetrainingobjective
andoptimizationtrajectoriesofthepopular

(
LaineandAila
,
2017
)andMeanTeacher(
Tarvainen
andValpola
,
2017
)consistency-basedmodels.Inspiredbytheseweproposetoimprove
SGDsolutionsviastochasticweightaveraging(SWA)(
Izmailovetal.
,
2018
),arecentmethodthat
averagesweightsofthenetworkscorrespondingtodifferenttrainingepochstoobtainasinglemodel
withimprovedgeneralization.Onathoroughempiricalstudyweshowthatthisprocedureachieves
thebestknownsemi-supervisedresultsonconsequentialbenchmarks.Inparticular:

WeshowinSection
3.1
thata

modelimplicitlyregularizesthenormoftheJacobian
ofthenetworkoutputswithrespecttobothitsinputsanditsweights,whichinturnencourages
solutions.BoththereducedJacobiannormandofsolutionshavebeenrelated
togeneralizationintheliterature(
Sokoli
´
cetal.
,
2017
;
Novaketal.
,
2018
;
Chaudharietal.
,
1
arXiv:1806.05594v3  [cs.LG]  21 Feb 2019PublishedasaconferencepaperatICLR2019
2016
;
SchmidhuberandHochreiter
,
1997
;
Keskaretal.
,
2017
;
Izmailovetal.
,
2018
).Interpolating
betweentheweightscorrespondingtodifferentepochsoftrainingwedemonstratethatthesolutions
of

andMeanTeachermodelsareindeedalongthesedirections(Figure
1b
).

InSection
3.2
,wecomparethetrainingtrajectoriesofthe

,MeanTeacher,andsupervisedmodels
andthatthedistancesbetweentheweightscorrespondingtodifferentepochsaremuchlarger
fortheconsistencybasedmodels.Theerrorcurvesofconsistencymodelsarealsowider(Figure
1b
),whichcanbeexplainedbytheofthesolutionsdiscussedinsection
3.1
.Furtherwe
observethatthepredictionsoftheSGDiteratescandifferbetweendifferentiterations
ofSGD.

Weobservethatforconsistency-basedmethods,SGDdoesnotconvergetoasinglepointbut
continuestoexploremanysolutionswithhighdistancesapart.Inspiredbythisobservation,we
proposetoaveragetheweightscorrespondingtoSGDiterates,orensemblethepredictionsofthe
modelscorrespondingtotheseweights.AveragingweightsofSGDiteratescompensatesforlarger
steps,stabilizesSGDtrajectoriesandobtainsasolutionthatiscenteredinaregionoftheloss
(asafunctionofweights).Further,weshowthattheSGDiteratescorrespondtomodelswith
diversepredictionsŒusingweightaveragingorensemblingallowsustomakeuseoftheimproved
diversityandobtainabettersolutioncomparedtotheSGDiterates.InSection
3.3
wedemonstrate
thatbothensemblingpredictionsandaveragingweightsofthenetworkscorrespondingtodifferent
trainingepochsimprovegeneralizationperformanceandthattheimprovementis
muchlargerforthe

andMeanTeachermodelscomparedtosupervisedtraining.Wethat
averagingweightsprovidessimilarorimprovedaccuracycomparedtoensembling,whileoffering
thecomputationalandconvenienceofworkingwithasinglemodel.Thus,wefocuson
weightaveragingfortheremainderofthepaper.

MotivatedbyourobservationsinSection
3
weproposetoapplyStochasticWeightAveraging
(SWA)(
Izmailovetal.
,
2018
)tothe

andMeanTeachermodels.BasedonourresultsinSection
3.3
weproposeseveraltoSWAinSection
4
.Inparticular,weproposefast-SWA,
which(1)usesalearningrateschedulewithlongercyclestoincreasethedistancebetweenthe
weightsthatareaveragedandthediversityofthecorrespondingpredictions;and(2)averages
weightsofmultiplenetworkswithineachcycle(whileSWAonlyaveragesweightscorresponding
tothelowestvaluesofthelearningratewithineachcycle).InSection
5
,weshowthatfast-SWA
convergestoagoodsolutionmuchfasterthanSWA.

Applyingweightaveragingtothe

andMeanTeachermodelsweimprovethebestreported
resultsonCIFAR-10for
1
k
,
2
k
,
4
k
and
10
k
labeledexamples,aswellasonCIFAR-100with
10
k
labeledexamples.Forexample,weobtain
5
:
0%
erroronCIFAR-10withonly
4
k
labels,improving
thebestresultreportedintheliterature(
TarvainenandValpola
,
2017
)by
1
:
3%
.Wealsoapply
weightaveragingtoastate-of-the-artdomainadaptationtechnique(
Frenchetal.
,
2018
)closely
relatedtotheMeanTeachermodelandimprovethebestreportedresultsondomainadaptation
fromCIFAR-10toSTLfrom
19
:
9%
to
16
:
8%
error.

Wereleaseourcodeat
https://github.com/benathi/fastswa-semi-sup
2B
ACKGROUND
2.1C
ONSISTENCY
B
ASED
M
ODELS
Wereviewsemi-supervisedlearningwithconsistency-basedmodels.Thisclassofmodels
encouragespredictionstostaysimilarundersmallperturbationsofinputsornetworkparameters.For
instance,twodifferenttranslationsofthesameimageshouldresultinsimilarpredictedprobabilities.
Theconsistencyofamodel(student)canbemeasuredagainstitsownpredictions(e.g.

model)or
predictionsofadifferentteachernetwork(e.g.MeanTeachermodel).Inbothcaseswewillsaya
student
networkmeasuresconsistencyagainsta
teacher
network.
ConsistencyLoss
Inthesemi-supervisedsetting,wehaveaccesstolabeleddata
D
L
=
f
(
x
L
i
;y
L
i
)
g
N
L
i
=1
,andunlabeleddata
D
U
=
f
x
U
i
g
N
U
i
=1
.
Giventwoperturbedinputs
x
0
;x
00
of
x
andtheperturbedweights
w
0
f
and
w
0
g
,theconsistencyloss
penalizesthedifferencebetweenthe
student
'spredictedprobablities
f
(
x
0
;
w
0
f
)
andthe
teacher
's
2
PublishedasaconferencepaperatICLR2019
g
(
x
00
;
w
0
g
)
.ThislossistypicallytheMeanSquaredErrororKLdivergence:
`
MSE
cons
(
w
f
;x
)=
k
f
(
x
0
;
w
0
f
)

g
(
x
00
;w
0
g
)
k
2
or
`
KL
cons
(
w
f
;x
)=
KL
(
f
(
x
0
;
w
0
f
)
jj
g
(
x
00
;w
0
g
))
:
(1)
Thetotallossusedtotrainthemodelcanbewrittenas
L
(
w
f
)=
X
(
x;y
)
2D
L
`
CE
(
w
f
;x;y
)
|
{z
}
L
CE
+

X
x
2D
L
[D
U
`
cons
(
w
f
;x
)
|
{z
}
L
cons
;
(2)
wherefor
L
CE
isthecrossentropybetweenthemodelpredictionsandsupervised
traininglabels.Theparameter
>
0
controlstherelativeimportanceoftheconsistencyterminthe
overallloss.

Model
The

model,introducedin
LaineandAila
(
2017
)and
Sajjadietal.
(
2016
),usesthe
studentmodel
f
asitsownteacher.Thedata(input)perturbationsincluderandomtranslations,
crops,andadditiveGaussiannoise.Binarydropout(
Srivastavaetal.
,
2014
)isusedforweight
perturbation.
MeanTeacherModel
TheMeanTeachermodel(MT)proposedin
TarvainenandValpola
(
2017
)
usesthesamedataandweightperturbationsasthe

model;however,theteacherweights
w
g
are
theexponentialmovingaverage(EMA)ofthestudentweights
w
f
:
w
k
g
=


w
k

1
g
+(1


)

w
k
f
.
Thedecayrate

isusuallysetbetween
0
:
9
and
0
:
999
.TheMeanTeachermodelhasthebestknown
resultsontheCIFAR-10semi-supervisedlearningbenchmark(
TarvainenandValpola
,
2017
).
OtherConsistency-BasedModels
TemporalEnsembling(TE)(
LaineandAila
,
2017
)usesan
exponentialmovingaverageofthestudentoutputsastheteacheroutputsintheconsistencytermfor
training.Anotherapproach,VirtualAdversarialTraining(VAT)(
Miyatoetal.
,
2017
),enforcesthe
consistencybetweenpredictionsontheoriginaldatainputsandthedataperturbedinanadversarial
direction
x
0
=
x
+

adv
,where
r
adv
=argmax
r
:
k
r
k
=1
KL
[
f
(
x;w
)
k
f
(
x
+
˘r;w
)]
.
3U
NDERSTANDING
C
ONSISTENCY
-E
NFORCING
M
ODELS
InSection
3.1
,westudyaversionofthe

modeltheoreticallyandshowthatitpenalizes
thenormoftheJacobianoftheoutputswithrespecttoinputs,aswellastheeigenvaluesofthe
Hessian,bothofwhichhavebeenrelatedtogeneralization(
Sokoli
´
cetal.
,
2017
;
Novaketal.
,
2018
;
Dinhetal.
,
2017a
;
Chaudharietal.
,
2016
).InSection
3.2
weempiricallystudythetraining
trajectoriesofthe

andMTmodelsandcomparethemtothetrainingtrajectoriesinsupervised
learning.Weshowthatevenlateintrainingconsistency-basedmethodsmakelargetrainingsteps,
leadingtochangesinpredictionsontest.InSection
3.3
weshowthataveragingweights
orensemblingpredictionsofthemodelsproposedbySGDatdifferenttrainingepochscanleadto
substantialgainsinaccuracyandthatthesegainsaremuchlargerfor

andMTthanforsupervised
training.
3.1S
IMPLIFIED

M
ODEL
P
ENALIZES
L
OCAL
S
HARPNESS
Penalizationoftheinput-outputJacobiannorm.
Considerasimpleversionofthe

model,
whereweonlyapplysmalladditiveperturbationstothestudentinputs:
x
0
=
x
+
;z
˘N
(0
;I
)
with

˝
1
,andtheteacherinputisunchanged:
x
00
=
x:
1
Thentheconsistencyloss
`
cons
(Eq.
1
)becomes
`
cons
(
w;x;
)=
k
f
(
w;x
+

)

f
(
w;x
)
k
2
.Considertheestimator
^
Q
=
lim

!
0
1

2
1
m
P
m
i
=1
`
cons
(
w;x
i
;
)
.WeshowinSection
A.5
that
E
[
^
Q
]=
E
x
[
k
J
x
k
2
F
]
andVar
[
^
Q
]=
1
m

Var
[
k
J
x
k
2
F
]+2
E
[
k
J
T
x
J
x
k
2
F
]

;
1
Thisassumptioncanberelaxedto
x
00
=
x
+

2
z
2
˘N
(0
;I
)
withoutchangingtheresultsoftheanalysis,
since

(
z
1

z
2
)=2


z
with

z
˘N
(0
;I
)
.
3
PublishedasaconferencepaperatICLR2019
where
J
x
istheJacobianofthenetwork'soutputswithrespecttoitsinputsevaluatedat
x
,
kk
F
representsFrobeniusnorm,andtheexpectation
E
x
istakenoverthedistributionoflabeledand
unlabeleddata.Thatis,
^
Q
isanunbiasedestimatorof
E
x
[
k
J
x
k
2
F
]
withvariancecontrolledbythe
minibatchsize
m
.Therefore,theconsistencylossimplicitlypenalizes
E
x
[
k
J
x
k
2
F
]
.
Thequantity
jj
J
x
jj
F
hasbeenrelatedtogeneralizationboththeoretically(
Sokoli
´
cetal.
,
2017
)
andempirically(
Novaketal.
,
2018
).Forlinearmodels
f
(
x
)=
Wx
,penalizing
jj
J
x
jj
F
exactly
correspondstoweightdecay,alsoknownas
L
2
regularization,sinceforlinearmodels
J
x
=
W
,and
k
W
k
2
F
=
k
vec
(
W
)
k
2
2
.Penalizing
E
x
[
k
J
x
k
2
F
]
isalsocloselyrelatedtothegraphbased(manifold)
regularizationin
Zhuetal.
(
2003
)whichusesthegraphLaplaciantoapproximate
E
x
[
kr
M
f
k
2
]
for
nonlinearmodels,makinguseofthemanifoldstructureofunlabeleddata.
Isotropicperturbationsinvestigatedinthis

modelwillnotingeneralliealongthedata
manifold,anditwouldbemorepertinenttoenforceconsistencytoperturbationssampledfrom
thespaceofnaturalimages.Infact,wecaninterpretconsistencywithrespecttostandarddata
augmentations(whichareusedinpractice)aspenalizingthemanifoldJacobiannorminthesame
mannerasabove.SeeSection
A.5
formoredetails.
PenalizationoftheHessian'seigenvalues.
Now,insteadoftheinputperturbation,considerthe
weightperturbation
w
0
=
w
+

.Similarly,theconsistencylossisanunbiasedestimatorfor
E
x
[
k
J
w
k
2
F
]
,where
J
w
istheJacobianofthenetworkoutputswithrespecttotheweights
w
.In
Section
A.6
weshowthatforthe
MSE
loss,theexpectedtraceoftheHessianoftheloss
E
x
[
tr
(
H
)]
canbedecomposedintotwoterms,oneofwhichis
E
x
[
k
J
w
k
2
F
]
.Asminimizingtheconsistencyloss
ofa

modelpenalizes
E
x
[
k
J
w
k
2
F
]
,italsopenalizes
E
x
[
tr
(
H
)]
.Aspointedoutin
Dinh
etal.
(
2017a
)and
Chaudharietal.
(
2016
),theeigenvaluesof
H
encodethelocalinformationabout
sharpnessofthelossforagivensolution
w
.Consequently,thequantity
tr
(
H
)
whichisthesumof
theHessianeigenvaluesisrelatedtothenotionofsharpandoptima,whichhasrecentlygained
attentionasaproxyforgeneralizationperformance(seee.g.
SchmidhuberandHochreiter
,
1997
;
Keskaretal.
,
2017
;
Izmailovetal.
,
2018
).Thus,basedonouranalysis,theconsistencylossinthe


modelencouragessolutions.
3.2A
NALYSISOF
S
OLUTIONSALONG
SGDT
RAJECTORIES
Intheprevioussectionwehaveseenthatina

model,theconsistencylossencourages
lowerinput-outputJacobiannormandHessian'seigenvalues,whicharerelatedtobettergeneralization.
Inthissectionweanalyzethepropertiesofminimizingtheconsistencylossinapracticalsetting.
,weexplorethetrajectoriesfollowedbySGDfortheconsistency-basedmodelsand
comparethemtothetrajectoriesinsupervisedtraining.
(a)GradientNorm
(b)SGD-SGDRays
(c)

model
(d)Supervisedmodel
Figure1:
(a)
:Theevolutionofthegradientnormfortheconsistencyregularizationterm(Cons)and
thecross-entropyterm(CE)inthe

,MT,andstandardsupervised(CEonly)modelsduringtraining.
(b)
:TrainandtesterrorsalongraysconnectingtwoSGDsolutionsforeachrespectivemodel.
(c)
and
(d)
:ComparisonoferrorsalongraysconnectingtwoSGDsolutions,randomrays,andadversarial
raysforthe

andsupervisedmodels.SeeSection
A.1
fortheanalogousMeanTeachermodel'splot.
WetrainourmodelsonCIFAR-10using
4
k
labeleddatafor
180
epochs.The

andMeanTeacher
modelsuse
46
k
datapointsasunlabeleddata(seeSections
A.8
and
A.9
fordetails).First,inFigure
1a
wevisualizetheevolutionofnormsofthegradientsofthecross-entropyterm
kr
L
CE
k
and
4
PublishedasaconferencepaperatICLR2019
consistencyterm
kr
L
cons
k
alongthetrajectoriesofthe

,MT,andstandardsupervisedmodels
(usingCElossonly).Weobservethat
kr
L
Cons
k
remainshighuntiltheendoftraininganddominates
thegradient
kr
L
CE
k
ofthecross-entropytermforthe

andMTmodels.Further,forboththe

andMTmodels,
kr
L
Cons
k
ismuchlargerthaninsupervisedtrainingimplyingthatthe

andMT
modelsaremakingsubstantiallylargerstepsuntiltheendoftraining.Theselargerstepssuggest
thatratherthanconvergingtoasingleminimizer,SGDcontinuestoactivelyexplorealargesetof
solutionswhenappliedtoconsistency-basedmethods.
Forfurtherunderstandthisobservation,weanalyzethebehavioroftrainandtesterrorsintheregion
ofweightspacearoundthesolutionsofthe

andMeanTeachermodels.First,weconsidertheone-
dimensionalrays
˚
(
t
)=
t

w
180
+(1

t
)
w
170
;t

0
;
connectingtheweightvectors
w
170
and
w
180
correspondingtoepochs
170
and
180
oftraining.Wevisualizethetrainandtesterrors(measured
onthelabeleddata)asfunctionsofthedistancefromtheweights
w
170
inFigure
1b
.Weobserve
thatthedistancebetweentheweightvectors
w
170
and
w
180
ismuchlargerforthesemi-supervised
methodscomparedtosupervisedtraining,whichisconsistentwithourobservationthatthegradient
normsarelargerwhichimplieslargerstepsduringoptimizationinthe

andMTmodels.Further,we
observethatthetrainandtesterrorsurfacesaremuchwideralongthedirectionsconnecting
w
170
and
w
180
fortheconsistency-basedmethodscomparedtosupervisedtraining.Onepossibleexplanation
fortheincreasedwidthistheeffectoftheconsistencylossontheJacobianofthenetworkandthe
eigenvaluesoftheHessianofthelossdiscussedinSection
3.1
.Wealsoobservethatthetesterrorsof
interpolatedweightscanbelowerthanerrorsofthetwoSGDsolutionsbetweenwhichweinterpolate.
Thiserrorreductionislargerintheconsistencymodels(Figure
1b
).
Wealsoanalyzetheerrorsurfacesalong
random
and
adversarial
raysstartingattheSGDsolution
w
180
foreachmodel.Fortherandomrayswesample
5
randomvectors
d
fromtheunitsphereand
calculatetheaveragetrainandtesterrorsofthenetworkwithweights
w
t
1
+
sd
for
s
2
[0
;
30]
.With
adversarialraysweevaluatetheerroralongthedirectionsofthefastestascentoftestortrainloss
d
adv
=
r
L
CE
jjr
L
CE
jj
.Weobservethatwhilethesolutionsofthe

andMTmodelsaremuchwiderthan
supervisedtrainingsolutionsalongtheSGD-SGDdirections(Figure
1b
),theirwidthsalongrandom
andadversarialraysarecomparable(Figure
1c
,
1d
)
WeanalyzetheerroralongSGD-SGDraysfortworeasons.Firstly,infast-SWAweareaveraging
solutionstraversedbySGD,sotheraysconnectingSGDiteratesserveasaproxyforthespacewe
averageover.Secondly,weareinterestedinevaluatingthewidthofthesolutionsthatweexplore
duringtrainingwhichweexpectwillbeimprovedbytheconsistencytraining,asdiscussedinSection
3.1
and
A.6
.Weexpectwidthalongrandomraystobelessmeaningfulbecausetherearemany
directionsintheparameterspacethatdonotchangethenetworkoutputs(
Dinhetal.
,
2017b
;
Gur-
Arietal.
,
2018
;
Sagunetal.
,
2017
).However,byevaluatingSGD-SGDrays,wecanexpectthat
thesedirectionscorrespondstomeaningfulchangestoourmodelbecauseindividualSGDupdates
correspondtodirectionsthatchangethepredictionsonthetrainingset.Furthermore,weobservethat
differentSGDiteratesproducedifferentpredictionsonthetestdata.
Neuralnetworksingeneralareknowntoberesilienttonoise,explainingwhybothMT,

and
supervisedmodelsarealongrandomdirections(
Aroraetal.
,
2018
).Atthesametimeneural
networksaresusceptibletotargetedperturbations(suchasadversarialattacks).Wehypothesizethat
wedonotobserveimprovedforsemi-supervisedmethodsalongadversarialraysbecause
wedonotchooseourinputorweightperturbationsadversarially,butrathertheyaresampledfroma
setoftransformations.
Additionally,weanalyzewhetherthelargeroptimizationstepsforthe

andMTmodelstranslateinto
higher
diversity
inpredictions.Wediversityofapairofmodels
w
1
;w
2
as
Diversity
(
w
1
;w
2
)=
1
N
P
N
i
=1
1
[
y
(
w
1
)
i
6
=
y
(
w
2
)
i
]
,thefractionoftestsampleswherethepredictedlabelsbetweenthetwo
modelsdiffer.Wefoundthatforthe

andMTmodels,theDiversity(
w
170
,
w
180
)is
7
:
1%
and
6
:
1%
ofthetestdatapointsrespectively,whichismuchhigherthan
3
:
9%
insupervisedlearning.The
increaseddiversityinthepredictionsofthenetworkstraversedbySGDsupportsourconjecturethat
forthe

andMTmodelsSGDstrugglestoconvergetoasinglesolutionandcontinuestoactively
explorethesetofplausiblesolutionsuntiltheendoftraining.
5
PublishedasaconferencepaperatICLR2019
3.3E
NSEMBLINGAND
W
EIGHT
A
VERAGING
InSection
3.2
,weobservedthatthe

andMTmodelscontinuetakinglargestepsintheweightspace
attheendoftraining.Notonlyarethedistancesbetweenweightslarger,weobservethesemodels
tohavehigherdiversity.Inthissetting,usingthelastSGDiteratetoperformpredictionisnotideal
sincemanysolutionsexploredbySGDareequallyaccuratebutproducedifferentpredictions.
Ensembling.
InSection
3.2
weshowedthatthediversityinpredictionsislargerforthe

andMeanTeachermodelscomparedtopurelysupervisedlearning.Thediversityoftheseiterates
suggeststhatwecanachievegreaterfromensembling.WeusethesameCNNarchitecture
andhyper-parametersasinSection
3.2
butextendthetrainingtimebydoing
5
learningratecycles
of
30
epochsafterthenormaltrainingendsatepoch
180
(see
A.8
and
A.9
fordetails).Wesample
randompairsofweights
w
1
,
w
2
fromepochs
180
;
183
;:::;
330
andmeasuretheerrorreduction
fromensemblingthesepairsofmodels,
C
ens

1
2
Err
(
w
1
)+
1
2
Err
(
w
2
)

Err
(
Ensemble
(
w
1
;w
2
))
.In
Figure
2c
wevisualize
C
ens
,againstthediversityofthecorrespondingpairofmodels.Weobserve
astrongcorrelationbetweenthediversityinpredictionsoftheconstituentmodelsandensemble
performance,andtherefore
C
ens
issubstantiallylargerfor

andMeanTeachermodels.Asshownin
Izmailovetal.
(
2018
),ensemblingcanbewellapproximatedbyweightaveragingiftheweightsare
closeby.
WeightAveraging.
First,weexperimentonaveragingrandompairsofweightsattheendof
trainingandanalyzetheperformancewithrespecttotheweightdistances.Usingthethesamepairs
fromabove,weevaluatetheperformanceofthemodelformedbyaveragingthepairsofweights,
C
avg
(
w
1
;w
2
)

1
2
Err
(
w
1
)+
1
2
Err
(
w
2
)

Err

1
2
w
1
+
1
2
w
2

.Notethat
C
avg
isaproxyforconvexity:
if
C
avg
(
w
1
;w
2
)

0
foranypairofpoints
w
1
,
w
2
,thenbyJensen'sinequalitytheerrorfunctionis
convex(seetheleftpanelofFigure
2
).Whiletheerrorsurfacesforneuralnetworksareknownto
behighlynon-convex,theymaybeapproximatelyconvexintheregiontraversedbySGDlateinto
training(
Goodfellowetal.
,
2015
).Infact,inFigure
2b
,wethattheerrorsurfaceoftheSGD
trajectoryisapproximatelyconvexdueto
C
avg
(
w
1
;w
2
)
beingmostlypositive.Herewealsoobserve
thatthedistancesbetweenpairsofweightsaremuchlargerforthe

andMTmodelsthanforthe
supervisedtraining;andasaresult,weightaveragingachievesalargergainforthesemodels.
InSection
3.2
weobservedthatforthe

andMeanTeachermodelsSGDtraversesalargeregion
oftheweightspacelateintraining.Beingveryhigh-dimensional,thissethasmostofitsvolume
concentratednearitsboundary.Thus,weSGDiteratesattheperipheryofthisregion(see
Figure
2d
).Wecanalsoexplainthisbehaviorviatheargumentof(
Mandtetal.
,
2017
).Undercertain
assumptionsSGDiteratescanbethoughtofassamplesfromaGaussiandistributioncenteredatthe
minimumoftheloss,andsamplesfromhigh-dimensionalGaussiansareknowntobeconcentrated
onthesurfaceofanellipseandneverbeclosetothemean.AveragingtheSGDiterates(shown
inredinFigure
2d
)wecanmovetowardsthecenter(showninblue)oftheregion,stabilizing
theSGDtrajectoryandimprovingthewidthoftheresultingsolution,andconsequentlyimproving
generalization.
Weobservethattheimprovement
C
avg
fromweightaveraging(
1
:
2

0
:
2%
overMTand

pairs)is
onparorlargerthanthe
C
ens
ofpredictionensembling(
0
:
9

0
:
2%
)Thesmallergainfrom
ensemblingmightbeduetothedependencyoftheensembledsolutions,sincetheyarefromthesame
SGDrunasopposedtoindependentrestartsasintypicalensemblingsettings.Fortherestofthe
paper,wefocusattentiononweightaveragingbecauseofitslowercostsattesttimeandslightly
higherperformancecomparedtoensembling.
4SWA
ANDFAST
-SWA
InSection
3
weanalyzedthetrainingtrajectoriesofthe

,MT,andsupervisedmodels.Weobserved
thatthe

andMTmodelscontinuetoactivelyexplorethesetofplausiblesolutions,producing
diversepredictionsonthetestseteveninthelatestagesoftraining.Further,insection
3.3
wehave
seenthataveragingweightsleadstogainsinperformanceforthe

andMTmodels.In
particularthesegainsaremuchlargerthaninsupervisedsetting.
6
PublishedasaconferencepaperatICLR2019
(a)Convexvs.non-convex
(b)
C
avg
vs.
jj
w
1

w
2
jj
(c)
C
ens
vs.diversity
(d)Gainfrom
Figure2:
(a)
:Illustrationofaconvexandnon-convexfunctionandJensen'sinequality.
(b)
:Scatter
plotofthedecreaseinerror
C
avg
forweightaveragingversusdistance.
(c)
:Scatterplotofthedecrease
inerror
C
ens
forpredictionensemblingversusdiversity.
(d)
:Trainerrorsurface(orange)andTest
errorsurface(blue).TheSGDsolutions(reddots)aroundalocallyminimumarefarapartdue
totheofthetrainsurface(seeFigure
1b
)whichleadstolargeerrorreductionoftheSWA
solution(bluedot).
Figure3:
Left
:CyclicalcosinelearningratescheduleandSWAandfast-SWAaveragingstrategies.
Middle
:Illustrationofthesolutionsexploredbythecyclicalcosineannealingscheduleonanerror
surface.
Right
:IllustrationofSWAandfast-SWAaveragingstrategies.fast-SWAaveragesmore
pointsbuttheerrorsoftheaveragedpoints,asindicatedbytheheatcolor,arehigher.
StochasticWeightAveraging(SWA)(
Izmailovetal.
,
2018
)isarecentapproachthatisbasedon
averagingweightstraversedbySGDwithalearningrateschedule.InSection
3
we
analyzedaveragingpairsofweightscorrespondingtodifferentepochsoftrainingandshowedthatit
improvesthetestaccuracy.Averagingmultipleweightsreinforcesthiseffect,andSWAwasshown
toimprovegeneralizationperformanceinsupervisedlearning.Basedonourresultsin
section
3.3
,wecanexpectevenlargerimprovementsingeneralizationwhenapplyingSWAtothe

andMTmodels.
SWAtypicallystartsfromapre-trainedmodel,andthenaveragespointsinweightspacetraversedby
SGDwithaconstantorcyclicallearningrate.Weillustratethecyclicalcosinelearningrateschedule
inFigure
3
(left)andtheSGDsolutionsexploredinFigure
3
(middle).Forthe
`

`
0
epochsthe
networkispre-trainedusingthecosineannealingschedulewherethelearningrateatepoch
i
isset
equalto

(
i
)=0
:
5


0
(1+
cos
(
ˇ

i=`
0
))
.After
`
epochs,weuseacyclicalschedule,repeating
thelearningratesfromepochs
[
`

c;`
]
,where
c
isthecyclelength.SWAcollectsthenetworks
correspondingtotheminimumvaluesofthelearningrate(showningreeninFigure
3
,left)and
averagestheirweights.Themodelwiththeaveragedweights
w
SWA
isthenusedtomakepredictions.
WeproposetoapplySWAtothestudentnetworkbothforthe

andMeanTeachermodels.Notethat
theSWAweightsdonotinterferewithtraining.
Originally,
Izmailovetal.
(
2018
)proposedusingcyclicallearningrateswithsmallcyclelengthfor
SWA.However,aswehaveseeninSection
3.3
(Figure
2
,left)theofaveragingarethemost
prominentwhenthedistancebetweentheaveragedpointsislarge.Motivatedbythisobservation,
weinsteaduselongerlearningratecycles
c
.Moreover,SWAupdatestheaverageweightsonlyonce
percycle,whichmeansthatmanyadditionaltrainingepochsareneededinordertocollectenough
weightsforaveraging.Toovercomethislimitation,wepropose
fast-SWA
,aofSWA
thataveragesnetworkscorrespondingtoevery
k<c
epochsstartingfromepoch
`

c
.Wecanalso
averagemultipleweightswithinasingleepochsetting
k<
1
.
7
PublishedasaconferencepaperatICLR2019
Noticethatmostofthemodelsincludedinthefast-SWAaverage(showninredinFigure
3
,left)have
highererrorsthanthoseincludedintheSWAaverage(showningreeninFigure
3
,right)sincethey
areobtainedwhenthelearningrateishigh.Itisourcontentionthatincludingmoremodelsinthe
fast-SWAweightaveragecanmorethancompensateforthelargererrorsoftheindividualmodels.
Indeed,ourexperimentsinSection
5
showthatfast-SWAconvergessubstantiallyfasterthanSWA
andhaslowerperformancevariance.WeanalyzethisresulttheoreticallyinSection
A.7
).
5E
XPERIMENTS
Weevaluatethe

andMTmodels(Section
4
)onCIFAR-10andCIFAR-100withvaryingnumbers
oflabeledexamples.Weshowthatfast-SWAandSWAimprovetheperformanceofthe

andMT
models,asweexpectfromourobservationsinSection
3
.Infact,inmanycasesfast-SWAimproves
onthebestresultsreportedinthesemi-supervisedliterature.Wealsodemonstratethatthepreposed
fast-SWAobtainshighperformancemuchfasterthanSWA.WealsoevaluateSWAappliedtoa
consistency-baseddomainadaptationmodel(
Frenchetal.
,
2018
),closelyrelatedtotheMTmodel,
foradaptingCIFAR-10toSTL.Weimprovethebestreportedtesterrorrateforthistaskfrom
19
:
9%
to
16
:
8%
.
WediscusstheexperimentalsetupinSection
5.1
.WeprovidetheresultsforCIFAR-10andCIFAR-
100datasetsinSection
5.2
and
5.3
.Wesummarizeourresultsincomparisontothebestprevious
resultsinSection
5.4
.WeshowseveraladditionalresultsanddetailedcomparisonsinAppendix
A.2
.Weprovideanalysisoftrainandtesterrorsurfacesoffast-SWAsolutionsalongthedirections
connectingfast-SWAandSGDinSection
A.1
.
5.1S
ETUP
WeevaluatetheweightaveragingmethodsSWAandfast-SWAondifferentnetworkarchitecturesand
learningrateschedules.Weareabletoimproveonthebasemodelsinallsettings.Inparticular,we
considera
13
-layerCNNanda12-block(26-layer)ResidualNetwork(
Heetal.
,
2015
)withShake-
Shakeregularization(
Gastaldi
,
2017
),whichwerefertosimplyas
CNN
and
Shake-Shake
respectively
(seeSection
A.8
fordetailsonthearchitectures).Fortrainingallmethodsweusethestochastic
gradientdescent(SGD)optimizerwiththecosineannealinglearningratedescribedinSection
4
.We
usetwolearningrateschedules,the
shortschedule
with
`
=180
;`
0
=210
;c
=30
,similartothe
experimentsin
TarvainenandValpola
(
2017
),andthe
longschedule
with
`
=1500
;`
0
=1800
;c
=
200
,similartotheexperimentsin
Gastaldi
(
2017
).Wenotethatthelongscheduleimprovesthe
performanceofthebasemodelscomparedtotheshortschedule;however,SWAcanstillfurther
improvetheresults.SeeSection
A.9
oftheAppendixformoredetailsonotherhyperparameters.We
repeateachCNNexperiment
3
timeswithdifferentrandomseedstoestimatethestandarddeviations
fortheresultsintheAppendix.
5.2CIFAR-10
(a)
(b)
(c)
(d)
Figure4:Predictionerrorsof

andMTmodelswithandwithoutfast-SWA.
(a)
CIFAR-10with
CNN
(b)
CIFAR-100withCNN.
50
k
+
and
50
k
+

correspondto
50
k
+
500
k
and
50
k
+
237
k

settings
(c)
CIFAR-10withResNet+Shake-Shakeusingtheshortschedule
(d)
CIFAR-10withResNet+
Shake-Shakeusingthelongschedule.
Weevaluatetheproposedfast-SWAmethodusingthe

andMTmodelsontheCIFAR-10dataset
(
Krizhevsky
).Weuse
50
k
imagesfortrainingwith
1
k
,
2
k
,
4
k
,
10
k
and
50
k
labelsandreportthe
8
PublishedasaconferencepaperatICLR2019
top-1errorsonthetestset(
10
k
images).WevisualizetheresultsfortheCNNandShake-Shake
architecturesinFigures
4a
,
4c
,and
4d
.Forallquantitiesoflabeleddata,fast-SWAsubstantially
improvestestaccuracyinbotharchitectures.Additionally,inTables
2
,
4
oftheAppendixweprovide
athoroughcomparisonofdifferentaveragingstrategiesaswellasresultsforVAT(
Miyatoetal.
,
2017
),TE(
LaineandAila
,
2016
),andotherbaselines.
Notethatweappliedfast-SWAforVATaswellwhichisanotherpopularapproachforsemi-supervised
learning.WefoundthattheimprovementonVATisnotdrasticŒourbaseimplementationobtains
11
:
26%
errorwherefast-SWAreducesitto
10
:
97%
(seeTable
2
inSection
A.2
).Itispossiblethat
thesolutionsexploredbyVATarenotasdiverseasin

andMTmodelsduetoVATlossfunction.
Throughouttheexperiments,wefocusonthe

andMTmodelsastheyhavebeenshowntoscaleto
powerfulnetworkssuchasShake-Shakeandobtainedpreviousstate-of-the-artperformance.
InFigure
5
(left),wevisualizethetesterrorasafunctionofiterationusingtheCNN.Weobservethat
whenthecyclicallearningratestartsafterepoch
`
=180
,thebasemodelsdropinperformancedue
tothesuddenincreaseinlearningrate(seeFigure
3
left).However,fast-SWAcontinuestoimprove
whilecollectingtheweightscorrespondingtohighlearningratesforaveraging.Ingeneral,wealso
thatthecyclicallearningrateimprovesthebasemodelsbeyondtheusualcosineannealing
scheduleandincreasestheperformanceoffast-SWAastrainingprogresses.ComparedtoSWA,we
alsoobservethatfast-SWAconvergessubstantiallyfaster,forinstance,reducingtheerrorto
10
:
5%
at
epoch
200
whileSWAattainssimilarerroratepoch
350
forCIFAR-10
4
k
labels(Figure
5
left).We
provideadditionalplotsinSection
A.2
showingtheconvergenceofthe

andMTmodelsinalllabel
settings,whereweobservesimilartrendsthatfast-SWAresultsinfastererrorreduction.
Wealsothattheperformancegainsoffast-SWAoverbasemodelsarehigherforthe

model
comparedtotheMTmodel,whichisconsistentwiththeconvexityobservationinSection
3.3
and
Figure
2
.Inthepreviousevaluations(seee.g.
Oliveretal.
,
2018
;
TarvainenandValpola
,
2017
),the

modelwasshowntobeinferiortotheMTmodel.However,withweightaveraging,fast-SWAreduces
thegapbetween

andMTperformance.Surprisingly,wethatthe

modelcanoutperform
MTafterapplyingfast-SWAwithmoderatetolargenumbersoflabeledpoints.Inparticular,the

+fast-SWAmodeloutperformsMT+fast-SWAonCIFAR-10with
4
k
,
10
k
,and
50
k
labeleddata
pointsfortheShake-Shakearchitecture.
Figure5:Predictionerrorsofbasemodelsandtheirweightaverages(fast-SWAandSWA)forCNN
on
(left)
CIFAR-10with
4
k
labels,
(middle)
CIFAR-100with
10
k
labels,and
(right)
CIFAR-100
50
k
labelsandextra
500
k
unlabeleddatafromTinyImages(
Torralbaetal.
,
2008
).
5.3CIFAR-100
AND
E
XTRA
U
NLABELED
D
ATA
Weevaluatethe

andMTmodelswithfast-SWAonCIFAR-100.Wetrainourmodelsusing
50000
imageswith
10
k
and
50
k
labelsusingthe
13
-layerCNN.WealsoanalyzetheeffectofusingtheTiny
Imagesdataset(
Torralbaetal.
,
2008
)asanadditionalsourceofunlabeleddata.
TheTinyImagesdatasetconsistsof
80
millionimages,mostlyunlabeled,andcontainsCIFAR-100
asasubset.Following
LaineandAila
(
2016
),weusetwosettingsofunlabeleddata,
50
k
+
500
k
and
50
k
+
237
k

wherethe
50
k
imagescorrespondstoCIFAR-100imagesfromthetrainingsetand
the
+500
k
or
+237
k

imagescorrespondstoadditional
500
k
or
237
k
imagesfromtheTinyImages
dataset.Forthe
237
k

setting,weselectonlytheimagesthatbelongtotheclassesinCIFAR-100,
correspondingto
237203
images.Forthe
500
k
setting,weusearandomsetof
500
k
imageswhose
classescanbedifferentfromCIFAR-100.WevisualizetheresultsinFigure
4b
,whereweagain
observethatfast-SWAsubstantiallyimprovesperformanceforeveryofthenumberof
labeledandunlabeleddata.InFigure
5
(middle,right)weshowtheerrorsofMT,SWAandfast-SWA
9
PublishedasaconferencepaperatICLR2019
asafunctionofiterationonCIFAR-100forthe
10
k
and
50
k
+
500
k
labelsettings.Similartothe
CIFAR-10experiments,weobservethatfast-SWAreducestheerrorssubstantiallyfasterthanSWA.
WeprovidedetailedexperimentalresultsinTable
3
oftheAppendixandincludepreliminaryresults
usingtheShake-ShakearchitectureinTable
5
,Section
A.2
.
5.4A
DVANCING
S
TATE
-
OF
-
THE
-A
RT
Wehaveshownthatfast-SWAcanimprovetheperformanceofboththe

andMT
models.Weprovideasummarycomparingourresultswiththepreviousbestresultsintheliterature
inTable
1
,usingthe
13
-layerCNNandtheShake-Shakearchitecturethathadbeenappliedpreviously.
WealsoprovidedetailedresultstheAppendix
A.2
.
Table1:Testerrorsagainstcurrentstate-of-the-artsemi-supervisedresults.Thepreviousbestnumbers
areobtainedfrom(
TarvainenandValpola
,
2017
)
1
,(
Parketal.
,
2017
)
2
,(
LaineandAila
,
2016
)
3
and
(
Luoetal.
,
2018
)
4
.CNNdenotesperformanceonthebenchmark
13
-layerCNN(see
A.8
).Rows
marked
y
usetheShake-Shakearchitecture.Theresultmarked
z
arefrom

+fast-SWA,wherethe
restarebasedonMT+fast-SWA.Thesettings
50
k
+
500
k
and
50
k
+
237
k

useadditional
500
k
and
237
k
unlabeleddatafromtheTinyImagesdataset(
Torralbaetal.
,
2008
)where

denotesthatweuse
onlytheimagesthatcorrespondtoCIFAR-100classes.
DatasetCIFAR-10CIFAR-100
No.ofImages50k50k50k
50k50k+500k50k+237k

No.ofLabels1k2k4k
10k50k50k
PreviousBestCNN18.41
4
13.64
4
9.22
2
38.65
3
23.62
3
23.79
3
OursCNN15.5811.029.05
33.6221.0420.98
PreviousBest
y
6.28
1
Ours
y
6.65.75.0
z
28.019.317.7
5.5P
RELIMINARY
R
ESULTSON
D
OMAIN
A
DAPTATION
Domainadaptationproblemsinvolvelearningusingasourcedomain
X
s
equippedwithlabels
Y
s
andperformingonthetargetdomain
X
t
whilehavingnoaccesstothetargetlabelsat
trainingtime.Arecentmodelby
Frenchetal.
(
2018
)appliestheconsistencyenforcingprinciple
fordomainadaptationandachievesstate-of-the-artresultsonmanydatasets.Applyingfast-SWAto
thismodelondomainadaptationfromCIFAR-10toSTLwewereabletoimprovethebestresults
reportedintheliteraturefrom
19
:
9%
to
16
:
8%
.SeeSection
A.10
formoredetailsonthedomain
adaptationexperiments.
6D
ISCUSSION
Semi-supervisedlearningiscrucialforreducingthedependencyofdeeplearningonlargelabeled
datasets.Recently,therehavebeengreatadvancesinsemi-supervisedlearning,withconsistency
regularizationmodelsachievingthebestknownresults.Byanalyzingsolutionsalongthetraining
trajectoriesfortwoofthemostsuccessfulmodelsinthisclass,the

andMeanTeachermodels,
wehaveseenthatratherthanconvergingtoasinglesolutionSGDcontinuestoexploreadiverse
setofplausiblesolutionslateintotraining.Asaresult,wecanexpectthataveragingpredictionsor
weightswillleadtomuchlargergainsinperformancethanforsupervisedtraining.Indeed,applying
avariantoftherecentlyproposedstochasticweightaveraging(SWA)weadvancethebestknown
semi-supervisedresultsonbenchmarks.
Whilenotthefocusofourpaper,wehavealsoshownthatweightaveraginghasgreatpromisein
domainadaptation(
Frenchetal.
,
2018
).Webelievethatanalysisofthegeometric
propertiesofthetrainingobjectiveandoptimizationtrajectorieswillfurtherimproveresultsover
awiderangeofapplicationareas,includingreinforcementlearningwithsparserewards,
generativeadversarialnetworks(
Yetal.
,
2018
),orsemi-supervisednaturallanguageprocessing.
10
PublishedasaconferencepaperatICLR2019
R
EFERENCES
S.Arora,R.Ge,B.Neyshabur,andY.Zhang.Strongergeneralizationboundsfordeepnetsviaa
compressionapproach.In
ICML
,2018.
H.AvronandS.Toledo.Randomizedalgorithmsforestimatingthetraceofanimplicitsymmetric
positivematrix.
JournaloftheACM
,58(2):1Œ34,Apr.2011.ISSN00045411.doi:
10.1145/1944345.1944349.
P.Bachman,O.Alsharif,andD.Precup.Learningwithpseudo-ensembles.In
AdvancesinNeural
InformationProcessingSystems
,pages3365Œ3373,2014.
P.Chaudhari,A.Choromanska,S.Soatto,Y.LeCun,C.Baldassi,C.Borgs,J.Chayes,L.Sagun,and
R.Zecchina.Entropy-SGD:BiasingGradientDescentIntoWideValleys.
arXiv:1611.01838[cs,
stat]
,Nov.2016.arXiv:1611.01838.
L.Dinh,R.Pascanu,S.Bengio,andY.Bengio.SharpMinimaCanGeneralizeForDeepNets.
arXiv:1703.04933[cs]
,Mar.2017a.
L.Dinh,R.Pascanu,S.Bengio,andY.Bengio.Sharpminimacangeneralizefordeepnets.In
ICML
,
2017b.
G.French,M.Mackiewicz,andM.Fisher.Self-ensemblingforvisualdomainadaptation.In
InternationalConferenceonLearningRepresentations
,2018.
X.Gastaldi.Shake-shakeregularization.
CoRR
,abs/1705.07485,2017.
I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley,S.Ozair,A.Courville,and
Y.Bengio.Generativeadversarialnets.In
Advancesinneuralinformationprocessingsystems
,
pages2672Œ2680,2014.
I.Goodfellow,O.Vinyals,andA.Saxe.Qualitativelycharacterizingneuralnetworkoptimization
problems.
InternationalConferenceonLearningRepresentations
,2015.
G.Gur-Ari,D.A.Roberts,andE.Dyer.Gradientdescenthappensinatinysubspace.In
CoRR
,2018.
K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearningforimagerecognition.
CoRR
,
abs/1512.03385,2015.
P.Izmailov,D.Podoprikhin,T.Garipov,D.Vetrov,andA.G.Wilson.Averagingweightsleadsto
wideroptimaandbettergeneralization.
arXivpreprintarXiv:1803.05407
,2018.
N.S.Keskar,D.Mudigere,J.Nocedal,M.Smelyanskiy,andP.T.P.Tang.Onlarge-batchtraining
fordeeplearning:Generalizationgapandsharpminima.
ICLR
,2017.
D.P.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization.
ICLR
,2015.
A.Krizhevsky.LearningMultipleLayersofFeaturesfromTinyImages.page60.
S.LaineandT.Aila.TemporalEnsemblingforSemi-SupervisedLearning.
arXiv:1610.02242[cs]
,
Oct.2016.
S.LaineandT.Aila.Temporalensemblingforsemi-supervisedlearning.
InternationalConference
onLearningRepresentations
,2017.
I.LoshchilovandF.Hutter.SGDR:stochasticgradientdescentwithrestarts.
CoRR
,abs/1608.03983,
2016.
Y.Luo,J.Zhu,M.Li,Y.Ren,andB.Zhang.Smoothneighborsonteachergraphsforsemi-supervised
learning.In
CVPR
,2018.
S.Mandt,M.D.Hoffman,andD.M.Blei.Stochasticgradientdescentasapproximatebayesian
inference.
arXivpreprintarXiv:1704.04289
,2017.
T.Miyato,S.Maeda,M.Koyama,andS.Ishii.Virtualadversarialtraining:aregularizationmethod
forsupervisedandsemi-supervisedlearning.
CoRR
,abs/1704.03976,2017.
11
PublishedasaconferencepaperatICLR2019
R.Novak,Y.Bahri,D.A.J.Pennington,andJ.Sohl-Dickstein.Sensitivityandgeneraliza-
tioninneuralnetworks:anempiricalstudy.
ICLR
,2018.
A.Oliver,A.Odena,C.Raffel,E.D.Cubuk,andI.J.Goodfellow.Realisticevaluationofdeep
semi-supervisedlearningalgorithms.
ICLRWorkshop
,2018.
S.Park,J.-K.Park,S.-J.Shin,andI.-C.Moon.AdversarialDropoutforSupervisedandSemi-
supervisedLearning.
arXiv:1707.03631[cs]
,July2017.arXiv:1707.03631.
L.Sagun,U.Evci,V.U.Güney,Y.Dauphin,andL.Bottou.Empiricalanalysisofthehessianof
over-parametrizedneuralnetworks.
CoRR
,2017.
M.Sajjadi,M.Javanmardi,andT.Tasdizen.RegularizationWithStochasticTransformationsand
PerturbationsforDeepSemi-SupervisedLearning.
arXiv:1606.04586[cs]
,June2016.arXiv:
1606.04586.
J.SchmidhuberandS.Hochreiter.Flatminima.
NeuralComputation
,1997.
R.Shu,H.Bui,H.Narui,andS.Ermon.ADIRT-tapproachtounsuperviseddomainadaptation.In
InternationalConferenceonLearningRepresentations
,2018.
J.Sokoli
´
c,R.Giryes,G.Sapiro,andM.R.Rodrigues.Robustlargemargindeepneuralnetworks.
IEEETransactionsonSignalProcessing
,65(16):4265Œ4280,2017.
N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhutdinov.Dropout:Asimpleway
topreventneuralnetworksfromov
TheJournalofMachineLearningResearch
,15(1):
1929Œ1958,2014.
A.TarvainenandH.Valpola.Meanteachersarebetterrolemodels:Weight-averagedconsistency
targetsimprovesemi-superviseddeeplearningresults.In
NIPS
,2017.
A.Torralba,R.Fergus,andW.T.Freeman.80milliontinyimages:Alargedatasetfornonparametric
objectandscenerecognition.
IEEETrans.PatternAnal.Mach.Intell.
,30(11):1958Œ1970,2008.
Y.YC.-S.Foo,S.Winkler,K.-H.Yap,G.Piliouras,andV.Chandrasekhar.TheUnusual
EffectivenessofAveraginginGANTraining.
ArXiv
,2018.
X.Zhu,Z.Ghahramani,andJ.Lafferty.Semi-SupervisedLearningUsingGaussianFieldsand
HarmonicFunctions.
ICML
,2003.
12
PublishedasaconferencepaperatICLR2019
AA
PPENDIX
A.1A
DDITIONAL
P
LOTS
Figure6:Allplotsareaobtainedusingthe
13
-layerCNNonCIFAR-
10
with
4
k
labeledand
46
k
unlabeleddatapointsunlessotherwise.
Left
:Testerrorasafunctionofdistancealong
randomraysforthe

modelwith
0
,
4
k
,
10
k
,
20
k
or
46
k
unlabeleddatapoints,andstandardfully
supervisedtrainingwhichusesonlythecrossentropyloss.Allmethodsuse
4
k
labeledexamples.
Middle
:TrainandtesterrorsalongraysconnectingSGDsolutions(showedwithcircles)toSWA
solutions(showedwithsquares)foreachrespectivemodel.
Right
:Comparisonoftrainandtest
errorsalongraysconnectingtwoSGDsolutions,randomrays,andadversarialraysfortheMean
Teachermodel.
Inthissectionweprovideseveraladditionalplotsvisualizingthetrainandtesterroralongdifferent
typesofraysintheweightspace.TheleftpanelofFigure
6
showshowthebehavioroftesterror
changesasweaddmoreunlabeleddatapointsforthe

model.Weobservethatthetestaccuracy
improvesmonotonically,butalsothesolutionsbecomenarroweralongrandomrays.
ThemiddlepanelofFigure
6
visualizesthetrainandtesterrorbehavioralongthedirectionsconnecting
thefast-SWAsolution(shownwithsquares)tooneoftheSGDiteratesusedtocomputetheaverage
(shownwithcircles)for

,MTandsupervisedtraining.Similarlyto
Izmailovetal.
(
2018
)we
observethatforallthreemethodsfast-SWAacenteredsolution,whiletheSGDsolutionlies
neartheboundaryofawideregion.Agreeingwithourresultsinsection
3.2
weobservethatfor

andMeanTeachermodelsthetrainandtesterrorsurfacesaremuchwideralongthedirections
connectingthefast-SWAandSGDsolutionsthanforsupervisedtraining.
IntherightpanelofFigure
6
weshowthebehavioroftrainandtesterrorsurfacesalongrandomrays,
adversarialraysanddirectionsconnectingtheSGDsolutionsfromepochs
170
and
180
fortheMean
Teachermodel(seesection
3.2
).
Figure7:
(Left)
:Theevolutionofthegradientcovariancetraceinthe

,MT,andsupervisedmodels
duringtraining.
(Middle)
:Scatterplotofthedecreaseinerror
C
avg
forweightaveragingversus
diversity.
(Right)
:Scatterplotofthedistancebetweenpairsofweightsversusdiversityintheir
predictions.
IntheleftpanelofFigure
7
weshowtheevolutionofthetraceofthegradientofthecovarianceof
theloss
trcov
(
r
w
L
(
w
))=
E
kr
w
L
(
w
)

E
r
w
L
(
w
)
k
2
13
PublishedasaconferencepaperatICLR2019
forthe

,MTandsupevisedtraining.Weobservethatthevarianceofthegradientismuchlargerfor
the

andMeanTeachermodelscomparedtosupervisedtraining.
Inthemiddleandrightpanelsof
7
weprovidescatterplotsoftheimprovement
C
obtained
fromaveragingweightsagainstdiversityanddiversityagainstdistance.Weobservethatdiversityis
highlycorrelatedwiththeimprovement
C
comingfromweightaveraging.Thecorrelationbetween
distanceanddiversityislessprominent.
A.2D
ETAILED
R
ESULTS
Inthissectionwereportdetailedresultsforthe

andMeanTeachermodelsandvariousbaselineson
CIFAR-10andCIFAR-100usingthe
13
-layerCNNandShake-Shake.
Theresultsusingthe13-layerCNNaresummarizedinTables
2
and
3
forCIFAR-10andCIFAR-100
respectively.Tables
4
and
5
summarizetheresultsusingShake-ShakeonCIFAR-10andCIFAR-
100.Inthetables

EMAisthesamemethodas

,whereinsteadofSWAweapplyExponential
MovingAveraging(EMA)forthestudentweights.WeshowthatsimplyperformingEMAforthe
studentnetworkinthe

modelwithoutusingitasateacher(asinMT)typicallyresultsinasmall
improvementinthetesterror.
Figures
8
and
9
showtheperformanceofthe

andMeanTeachermodelsasafunctionofthetraining
epochforCIFAR-10andCIFAR-100respectivelyforSWAandfast-SWA.
Table2:CIFAR-10semi-supervisederrorsontestsetwitha13-layerCNN.Theepochnumbersare
reportedinparenthesis.Thepreviousresultsshowninthesectionofthetableareobtainedfrom
TarvainenandValpola
(
2017
)
1
,
Parketal.
(
2017
)
2
,
LaineandAila
(
2016
)
3
,
Miyatoetal.
(
2017
)
4
.
Numberoflabels1000200040001000050000
TE
3
--12.16

0.315.60

0.15
Supervised-only
1
46.43

1.2133.94

0.7320.66

0.575.82

0.15

1
27.36

1.2018.02

0.6013.20

0.276.06

0.15
MT
1
21.55

1.4815.73

0.3112.31

0.285.94

0.15
VAdD
3
9.22

0.10
4.40

0.12
VAT+EntMin
4
10.55
MT18.78

0.3114.43

0.2011.41

0.278.74

0.305.98

0.21
MT+fast-SWA(180)18.19

0.3813.46

0.3010.67

0.188.06

0.125.90

0.03
MT+fast-SWA(240)17.81

0.3713.00

0.3110.34

0.147.73

0.105.55

0.03
MT+SWA(240)18.38

0.2913.86

0.6410.95

0.218.36

0.505.75

0.29
MT+fast-SWA(480)16.84

0.6212.24

0.319.86

0.277.39

0.145.14

0.07
MT+SWA(480)17.48

0.1313.09

0.8010.30

0.217.78

0.495.31

0.43
MT+fast-SWA(1200)
15.58

0.1211.02

0.239.05

0.216.92

0.074.73

0.18
MT+SWA(1200)15.59

0.7711.42

0.339.38

0.287.04

0.115.11

0.35

21.85

0.6916.10

0.5112.64

0.119.11

0.216.79

0.22

EMA21.70

0.5715.83

0.5512.52

0.169.06

0.156.66

0.20

+fast-SWA(180)20.79

0.3815.12

0.4411.91

0.068.83

0.326.42

0.09

+fast-SWA(240)20.04

0.4114.77

0.1511.61

0.068.45

0.286.14

0.11

+SWA(240)21.37

0.6415.38

0.8512.05

0.408.58

0.416.36

0.55

+fast-SWA(480)19.11

0.2913.88

0.3010.91

0.157.91

0.215.53

0.07

+SWA(480)20.06

0.6414.53

0.8111.35

0.428.04

0.375.77

0.51

+fast-SWA(1200)
17.23

0.3412.61

0.1810.07

0.277.28

0.234.72

0.04

+SWA(1200)17.70

0.2512.59

0.2910.73

0.397.13

0.234.99

0.41
VAT11.99
VAT+SWA11.16
VAT+EntMin11.26
VAT+EntMin+SWA
10.97
14
PublishedasaconferencepaperatICLR2019
Table3:CIFAR-100semi-supervisederrorsontestset.Allmodelsaretrainedona13-layerCNN.
Theepochnumbersarereportedinparenthesis.Thepreviousresultsshowninthesectionofthe
tableareobtainedfrom(
LaineandAila
,
2016
)
3
.
Numberoflabels10k50k50k+500k50k+237k

Supervised-only
3
44.56

0.3026.42

0.17

model
3
39.19

0.5426.32

0.0425.79

0.1725.43

0.17
TemporalEnsembling
3
38.65

0.5126.30

0.1523.62

0.1723.79

0.17
MT(180)35.96

0.7723.37

0.1623.18

0.0623.18

0.24
MT+fast-SWA(180)34.54

0.4821.93

0.1621.04

0.1621.09

0.12
MT+SWA(240)35.59

1.4523.17

0.8622.00

0.2321.59

0.22
MT+fast-SWA(240)34.10

0.3121.84

0.1221.16

0.2121.07

0.21
MT+SWA(1200)34.90

1.5122.58

0.7921.47

0.2921.27

0.09
MT+fast-SWA(1200)
33.62

0.5421.52

0.1221.04

0.0420.98

0.36

(180)38.13

0.5224.13

0.2024.26

0.1524.10

0.07

+fast-SWA(180)35.59

0.6222.08

0.2121.40

0.1921.28

0.20

+SWA(240)36.89

1.5123.23

0.7022.17

0.1921.65

0.13

+fast-SWA(240)35.14

0.7122.00

0.2121.29

0.2721.22

0.04

+SWA(1200)35.35

1.1522.53

0.6421.53

0.1321.26

0.34

+fast-SWA(1200)
34.25

0.1621.78

0.0521.19

0.0520.97

0.08
Table4:CIFAR-10semi-supervisederrorsontestset.AllmodelsuseShake-ShakeRegularization
(
Gastaldi
,
2017
)+ResNet-26(
Heetal.
,
2015
).
Numberoflabels1000200040001000050000
ShortSchedule
(
`
=180
)
MT
y
(
TarvainenandValpola
,
2017
)6.28
MT(180)10.28.07.15.83.9
MT+SWA(240)9.77.76.24.93.4
MT+fast-SWA(240)9.67.46.24.93.2
MT+SWA(1200)7.66.45.84.6
3.1
MT+fast-SWA(1200)
7.56.3
5.84.5
3.1

(180)12.39.17.56.43.8

+SWA(240)11.08.36.75.53.3

+fast-SWA(240)11.28.26.75.53.3

+SWA(1200)8.26.75.74.2
3.1

+fast-SWA(1200)8.06.5
5.54.03.1
LongSchedule(
`
=1500
)
Supervised-only(
Gastaldi
,
2017
)
2.86
MT(1500)7.56.56.05.03.5
MT+fast-SWA(1700)6.45.85.23.83.4
MT+SWA(1700)6.95.95.54.23.2
MT+fast-SWA(3500)
6.65.7
5.13.93.1
MT+SWA(3500)6.75.85.23.93.1

(1500)8.57.06.35.03.4

+fast-SWA(1700)7.56.25.24.03.1

+SWA(1700)7.86.45.64.43.2

+fast-SWA(3500)7.46.0
5.03.8
3.0

+SWA(3500)7.96.25.14.03.0
15
PublishedasaconferencepaperatICLR2019
Table5:CIFAR-100semi-supervisederrorsontestset.OurmodelsuseShake-ShakeRegularization
(
Gastaldi
,
2017
)+ResNet-26(
Heetal.
,
2015
).
Numberoflabels10k50k50k+500k50k+237k

TE(CNN)(
LaineandAila
,
2016
)38.65

0.5126.30

0.1523.62

0.1723.79

0.17
ShortSchedule
(
`
=180
)
MT(180)29.419.521.919.0
MT+fast-SWA(180)28.919.319.718.3
MT+SWA(240)28.418.819.917.9
MT+fast-SWA(240)28.118.819.517.9
MT+SWA(300)28.118.5
18.917.5
MT+fast-SWA(300)
28.018.4
19.317.7
Figure8:Testerrorsasafunctionoftrainingepochforbaselinemodels,SWAandfast-SWAon
CIFAR-10trainedusing
1
k
,
2
k
,
4
k
,and
10
k
labelsfor
(top)
theMTmodel
(bottom)
the

model.
Allmodelsaretrainedusingthe13-layerCNN.
Figure9:Testerrorsversustrainingepochforbaselinemodels,SWAandfast-SWAonCIFAR-100
trainedusing
10
k
,
50
k
,
50
k
+
500
k
,and
50
k
+
237
k

labelsfor
(top)
theMTmodel
(bottom)
the

model.Allmodelsaretrainedusingthe13-layerCNN.
A.3E
FFECTOF
L
EARNING
R
ATE
S
CHEDULES
Theonlyhyperparameterforthefast-SWAsettingisthecyclelength
c
.WedemonstrateinFigure
10a
thatfast-SWA'sperformanceisnotsensitiveto
c
overawiderangeof
c
values.Wealsodemonstrate
theperformanceforconstantlearningschedule.fast-SWAwithcyclicallearningratesgenerally
convergesfasterduetohighervarietyinthecollectedweights.
16
PublishedasaconferencepaperatICLR2019
(a)
(b)
Figure10:TheplotsaregeneratedusingtheMTmodelwithCNNtrainedonCIFAR-10.We
randomlyselect
5
k
ofthe
50
k
trainimagesasavalidationset.Theremaining
45
k
imagesaresplitted
into
4
k
labeledand
41
k
unlabeleddatapoints.
(a)
Validationaccuracyasafunctionoftrainingepoch
fordifferentcyclelengths
c
(b)
fast-SWAwithconstantlearningrate.Theﬁlearningrateepochﬂ
correspondstotheepochinthecosineannealingschedule(Figure
3
,left)atwhichthe
learningrateisevaluated.Weusethisedlearningrateforallepochs
i

`
.
A.4EMA
VERSUS
SWA
ASA
T
EACHER
TheMTmodelusesanexponentialmovingaverage(EMA)ofthestudentweightsasateacherin
theconsistencyregularizationterm.WeconsidertwopotentialeffectsofusingEMAasateacher:
averagingweightsimprovesperformanceoftheteacherforthereasonsdiscussedinSections
3.2
,
3.3
;second,havingabetterteachermodelleadstobetterstudentperformancewhichinturn
furtherimprovestheteacher.Inthissectionwetrytoseparatethesetwoeffects.WeapplyEMA
tothe

modelinthesamewayinwhichweapplyfast-SWAinsteadofusingEMAasateacher
andcomparetheresultingperformancetotheMeanTeacher.Figure
11
showstheimprovementin
error-rateobtainedbyapplyingEMAtothe

modelindifferentlabelsettings.Aswecanseewhile
EMAimprovestheresultsoverthebaseline

model,theperformanceof

-EMAisstillinferior
tothatoftheMeanTeachermethod,especiallywhenthelabeleddataisscarce.Thisobservation
suggeststhattheimprovementoftheMeanTeacheroverthe

modelcannotbesimplyattributedto
EMAimprovingthestudentperformanceandweshouldtakethesecondeffectdiscussedaboveinto
account.
LikeSWA,EMAisawaytoaverageweightsofthenetworks,butitputsmoreemphasisonvery
recentmodelscomparedtoSWA.EarlyintrainingwhenthestudentmodelchangesrapidlyEMA
improvesperformanceandhelpsalotwhenusedasateacher.Howeveroncethestudent
modelconvergestothevicinityoftheoptimum,EMAofferslittlegain.InthisregimeSWAisa
muchbetterwaytoaverageweights.WeshowtheperformanceofSWAappliedto

modelinFigure
11
(left).
Figure11:
(left)
Comparisonofdifferentaveragingmethods.Theyaxiscorrespondstotheincreased
errorwithrespecttotheMTmodelwithfast-SWAsolution(whichhas
y
=0
).Allnumbersare
takenfromepoch
180
.
(right)
TheeffectsofusingSWAasateacher.
W
-
T
modelcorrespondstothe
performanceofamodelwithweight
W
usingamodelwithateacherbeing
T
.
17
PublishedasaconferencepaperatICLR2019
SinceSWAperformsbetterthanEMA,wealsoexperimentwithusingSWAasateacherinstead
ofEMA.WestartwiththeusualMTmodelpretraineduntilepoch
150
.Thenweswitchtousing
SWAasateacheratepoch
150
.InFigure
11
(right),ourresultssuggestthatusingSWAasateacher
performsonparwithusingEMAasateacher.Weconjecturethatonceweareataconvexregion
oftesterrorclosetotheoptimum(epoch
150
),havingabetterteacherdoesn'tleadtosubstantially
improvedperformance.ItispossibletostartusingSWAasateacherearlierintraining;however,
duringearlyepochswherethemodelundergoesrapidimprovementEMAismoresensiblethanSWA
aswediscussedabove.
A.5C
ONSISTENCY
L
OSS
A
PPROXIMATES
J
ACOBIAN
N
ORM
Estimatormeanandvariance:
Inthe

modelwithsmalladditivedataperturbations
thatarenormallydistributed,
z
˘N
(0
;I
)
,
^
Q
=lim

!
0
1

2
1
m
m
X
i
=1
`
cons
(
w;x
i
;
)=
1
m
m
X
i
=1
lim

!
0
1

2
k
f
(
w;x
i
+

i
)

f
(
w;x
i
)
k
2
Taylorexpanding
`
cons
in

,weobtain
`
cons
(
w;x;
)=

2
z
T
J
T
x
J
x
z
+
O
(

4
)
,where
J
x
isthe
Jacobianofthenetworkoutputs
f
withrespecttotheinputataparticularvalueof
x
.Therefore,
^
Q
i
=lim

!
0
1

2
`
cons
(
w;x
i
;
)=
z
T
i
J
(
x
i
)
T
J
(
x
i
)
z
i
:
Wecannowrecognizethistermasaonesamplestochastictraceestimatorfor
tr
(
J
(
x
i
)
T
J
(
x
i
))
witha
Gaussianprobevariable
z
i
;see
AvronandToledo
(
2011
)forderivationsandguaranteesonstochastic
traceestimators.
E
z
[
^
Q
i
]=
tr
(
J
(
x
i
)
T
J
(
x
i
)
E
[
z
i
z
T
i
])=
k
J
(
x
i
)
k
2
F
:
Takinganexpectationoverthe
m
samplesof
x
,weget
E
[
^
Q
]=
E
x
[
k
J
x
k
2
F
.
Ingeneralifwehave
m
samplesof
x
and
n
sampledperturbationsforeach
x
,thenforasymmetric
matrix
A
with
z
ik
iid
˘
N
(0
;I
)
andindependent
x
i
iid
˘
p
(
x
)
,
theestimator
^
Q
=
1
m
m
X
i
1
n
n
X
k
z
T
ik
A
(
x
i
)
z
ik
hasvariance
Var
[
^
Q
]=
1
m

Var
[
tr
(
A
)]+
2
n
E
[
tr
(
A
2
)]

:
Proof:Let
q
ik

z
T
ik
A
(
x
i
)
z
ik
.Itiseasytoshowthatfored
x
,
E
z
[
q
11
j
x
1
]=2
tr
(
A
(
x
1
)
2
)+
tr
(
A
(
x
1
))
2
,(seee.g.
AvronandToledo
,
2011
).Notethat
E
z
1
;z
2
[
q
i
1
q
i
2
j
x
i
]=
tr
(
A
(
x
i
))
2
.Since

1
n
P
n
k
q
ik

m
i
=1
arei.i.drandomvariables,
Var

1
m
m
X
i
1
n
n
X
k
q
ik

=
1
m
Var

1
n
n
X
k
q
1
k

;
whereasthisdoesnotholdfortheoppositeorderingofthesum.
E

1
n
n
X
k
q
1
k

2

=
E
x
1
E
f
z
g

1
n
2
n
X
l
n
X
k
q
il
q
ik


f
x
g

=
E
x
1
E
f
z
g

n
n
2
q
2
11
+
n
(
n

1)
n
2
q
11
q
12


f
x
g

=
E
x

1
n

2
tr
(
A
2
)+
tr
(
A
)
2

+

1

1
n

tr
(
A
)
2

=

E
x
[
tr
(
A
)
2
]+
2
n
E
x
[
tr
(
A
2
)]

Pluggingin
A
=
J
T
J
and
n
=1
,weget
Var
[
^
Q
]=
1
m

Var
[
k
J
x
k
2
F
]+2
E
[
k
J
T
x
J
x
k
2
F
]

:
18
PublishedasaconferencepaperatICLR2019
Non-isotropicperturbationsalongdatamanifold
Consistencyregularizationwithnaturalpertur-
bationssuchasimagetranslationcanalsobeunderstoodaspenalizingaJacobiannormasinSection
3.1
.Forexample,considerperturbationssampledfromanormaldistributiononthetangentspace,
z
˘
P
(
x
)
N
(0
;I
)
where
P
(
x
)=
P
(
x
)
2
istheorthogonalprojectionmatrixthatprojectsdownfrom
R
d
to
T
x
(
M
)
,thetangentspaceoftheimagemanifoldat
x
.Thentheconsistencyregularization
penalizestheLaplaciannormofthenetworkonthemanifold(withtheinheritedmetricfrom
R
d
).
E
[
z
]=0
and
E
[
zz
T
]=
PP
T
(=)
P
2
=
P
whichfollowsif
P
isanorthogonalprojectionmatrix.
Then,
E
[
z
T
J
T
Jz
]=
tr
(
J
T
JPP
T
)=
tr
(
P
T
J
T
JP
)=
tr
(
J
T
M
J
M
)=
k
J
M
k
2
F
:
Weviewthestandarddataaugmentationssuchasrandomtranslation(thatareappliedinthe

andMTmodels)asapproximatingsamplesofnearbyelementsofthedatamanifoldandtherefore
differences
x
0

x
approximateelementsofitstangentspace.
A.6R
ELATIONSHIP
B
ETWEEN
E
x
[
k
J
w
k
2
F
]
AND
R
ANDOM
R
AY
S
HARPNESS
Inthefollowinganalysiswereviewanargumentforwhysmaller
E
x
[
k
J
w
k
2
F
]
,impliesbroaderoptima.
Tokeepthingssimple,wefocusontheMSEloss,butinprincipleasimilarargumentshouldapply
fortheCrossEntropyandtheErrorrate.Forasingledatapoint
x
andonehotvector
y
with
k
classes,
thehessianof
`
MSE
(
w
)=
k
f
(
x;w
)

y
k
2
canbedecomposedintotwoterms,theGauss-Newton
matrix
G
=
J
T
w
J
w
andatermwhichdependsonthelabels.
H
(
w;x;y
)=
r
2
`
MSE
(
w
)=
J
T
w
J
w
+
k
X
i
=1
(
r
2
f
i
)(
f
i
(
x
)

y
i
)
;
tr
(
H
)=
k
J
w
k
2
F
+
k
X
i
=1
tr
(
r
2
f
i
)(
f
i
(
x
)

y
i
)
|
{z
}

(
x;y
)
Thus
tr
(
H
)
isalsothesumoftwoterms,
k
J
w
k
2
F
and

.Asthesolutionimproves,therelative
sizeof

goesdown.Intermsofrandomraysharpness,considertheexpected
MSE
loss,orrisk,
R
MSE
(
w
)=
E
(
x;y
)
k
f
(
x;w
)

y
k
2
alongrandomrays.Let
d
bearandomvectorsampledfromthe
unitsphereand
s
isthedistancealongtherandomray.Evaluatingtheriskonarandomray,and
Taylorexpandingin
s
wehave
R
MSE
(
w
+
sd
)=
R
MSE
(
w
)+
sd
T
E
(
x;y
)
[
J
T
w
(
f

y
)]+(1
=
2)
s
2
d
T
E
(
x;y
)
[
H
]
d
+
O
(
s
3
)
Since
d
isfromtheunitsphere,
E
[
d
]=0
and
E
[
dd
T
]=
I=p
where
p
isthedimension.Averaging
overtherays,
d
˘
Unif
(
S
p

1
)
,wehave
E
d
[
R
MSE
(
w
+
sd
)]

R
MSE
(
w
)=
s
2
2
p
E
x
[
tr
(
H
)]+
O
(
s
4
)=
s
2
2
p
E
x
[
k
J
w
k
2
F
]+
s
2
2
p
E
(
x;y
)
[

(
x;y
)]+
O
(
s
4
)
Alloftheoddtermsvanishbecauseofthesymmetryoftheunitsphere.Thismeansthat
locally,thesharpnessoftheoptima(asmeasuredbyrandomrays)canbeloweredbydecreasing
E
x
[
k
J
w
k
2
F
]
.
A.7I
NCLUDING
H
IGH
L
EARNING
R
ATE
I
TERATES
I
NTO
SWA
Asdiscussedin
Mandtetal.
(
2017
),undercertainassumptionsSGDsamplesfromaGaussian
distributioncenteredattheoptimumoftheloss
w
0
withcovarianceproportionaltothelearning
rate.Supposethenthatwehave
n
weightssampledatlearningrate

1
,
w
(1)
i
iid
˘N
(
w
0
;
1

and
m
weightssampledwiththehigherlearningrate

2
,
w
(2)
j
iid
˘N
(
w
0
;
2

.FortheSWA
estimator
^
w
SWA
=
1
n
P
i
w
(1)
i
,
E
[
k
^
w
SWA

w
0
k
2
]=
tr
(
Cov
(^
w
SWA
))=

1
n
tr

.Butifweinclude
thehighvariancepointsintheaverage,asinfast-SWA,
^
w
fSWA
=
1
n
+
m

P
i
w
(1)
i
+
P
j
w
(2)
j

,then
E
[
k
^
w
fSWA

w
0
k
2
]=

1
+

2
(
n
+
m
)
2
tr

.If

1
+

2
(
n
+
m
)
2
<

1
n
thenincludingthehighlearningratepoints
decreasestheMSEoftheestimatorfor
m>n


2

1

2

.Ifweincludeenoughpoints,wewillstill
improvetheestimate.
19
PublishedasaconferencepaperatICLR2019
A.8N
ETWORK
A
RCHITECTURES
IntheexperimentsweusetwoDNNarchitecturesŒ
13
layerCNNandShake-Shake.Thearchitecture
of
13
-layerCNNisdescribedinTable
6
.Itcloselyfollowsthearchitectureusedin(
Laineand
Aila
,
2017
;
Miyatoetal.
,
2017
;
TarvainenandValpola
,
2017
).Were-implementitinPyTorchand
removedtheGaussianinputnoise,sincewefoundhavingnosuchnoiseimprovesgeneralization.
ForShake-Shakeweuse26-2x96dShake-Shakeregularizedarchitectureof
Gastaldi
(
2017
)with
12
residualblocks.
Table6:A13-layerconvolutionalneuralnetworksforthe
CNNexperiments(CIFAR-10andCIFAR-100)inSection
5.2
and
5.3
.Notethatthedifferencefromthearchitecture
usedin
TarvainenandValpola
(
2017
)isthatweremoved
aGaussiannoiselayerafterthehorizontal
LayerHyperparameters
Input
32

32
RGBimage
TranslationRandomly{

x;

y
g˘
[

4
;
4]
HorizontalRandomly
p
=0
:
5
Convolutional
128

3

3
,
same
padding
Convolutional
128

3

3
,
same
padding
Convolutional
128

3

3
,
same
padding
PoolingMaxpool
2

2
Dropout
p
=0
:
5
Convolutional
256

3

3
,
same
padding
Convolutional
256

3

3
,
same
padding
Convolutional
256

3

3
,
same
padding
PoolingMaxpool
2

2
Dropout
p
=0
:
5
Convolutional
512

3

3
,
valid
padding
Convolutional
256

1

1
,
same
padding
Convolutional
128

1

1
,
same
padding
PoolingAveragepool(
6

6
!
1

1pixels)
SoftmaxFullyconnected
128
!
10
A.9H
YPERPARAMETERS
Weconsidertwodifferentschedules.Inthe
shortschedule
wesetthecosinehalf-period
`
0
=210
and
traininglength
`
=180
,followingthescheduleusedin
TarvainenandValpola
(
2017
)inShake-Shake
experiments.ForourShake-Shakeexperimentswealsoreportresultswith
longschedule
wherewe
set
`
=1800
;`
0
=1500
following
Gastaldi
(
2017
).Todeterminetheinitiallearningrate

0
andthe
cyclelength
c
weusedaseparatevalidationsetofsize
5000
takenfromtheunlabeleddata.After
determiningthesevalues,weaddedthevalidationsettotheunlabeleddataandtrainedagain.We
reusethesamevaluesof

0
and
c
forallexperimentswithdifferentnumbersoflabeleddataforboth

modelandMeanTeacherforaedarchitecture(
13
-layerCNNorShake-Shake).Fortheshort
scheduleweusecyclelength
c
=30
andaveragemodelsonceevery
k
=3
epochs.Forlongschedule
weuse
c
=200
,
k
=20
.
InallexperimentsweusestochasticgradientdescentoptimizerwithNesterovmomentum(
Loshchilov
andHutter
,
2016
).Infast-SWAweaverageeverytheweightsofthemodelscorrespondingtoevery
thirdepoch.Inthe

model,weback-propagatethegradientsthroughthestudentsideonly(as
opposedtobothsidesin(
LaineandAila
,
2016
)).ForMeanTeacherweuse

=0
:
97
decayratein
theExponentialMovingAverage(EMA)ofthestudent'sweights.Forallotherhyper-parameterswe
reusethevaluesfrom
TarvainenandValpola
(
2017
)unlessmentionedotherwise.
Likein
TarvainenandValpola
(
2017
),weuse
kk
2
fordivergenceintheconsistencyloss.Similarly,
werampuptheconsistencycost

overthe
5
epochsfrom
0
uptoit'smaximumvalueof
100
as
20
PublishedasaconferencepaperatICLR2019
donein
TarvainenandValpola
(
2017
).Weusecosineannealinglearningrateswithnolearningrate
rampup,unlikeintheoriginalMTimplementation(
TarvainenandValpola
,
2017
).Notethatthisis
similartothesamehyperparametersettingsasin
TarvainenandValpola
(
2017
)forResNet
2
.Wenote
thatweusetheexactsamehyperparametersforthe

andMTmodelsineachexperimentsetting.In
contrasttotheoriginalimplementationin
TarvainenandValpola
(
2017
)ofCNNexperiments,weuse
SGDinsteadofAdam.
UnderstandingExperimentsinSections
3.2
,
3.3
Weusethe
13
-layerCNNwiththeshortlearning
rateschedule.Weuseatotalbatchsizeof
100
forCNNexperimentswithalabeledbatchsizeof
50
forthe

andMeanTeachermodels.Weusethemaximumlearningrate

0
=0
:
1
.ForSection
3.2
werunSGDonlyfor
180
epochs,so
0
learningratecyclesaredone.ForSection
3.3
weadditionally
run
5
learningratecyclesandsamplepairsofSGDiteratesfromepochs
180
-
330
correspondingto
thesecycles.
CIFAR-10CNNExperiments
Weuseatotalbatchsizeof
100
forCNNexperimentswitha
labeledbatchsizeof
50
.Weusethemaximumlearningrate

0
=0
:
1
.
CIFAR-10ResNet+Shake-Shake
Weuseatotalbatchsizeof
128
forResNetexperimentswith
alabeledbatchsizeof
31
.Weusethemaximumlearningrate

0
=0
:
05
forCIFAR-10.Thisapplies
forboththeshortandlongschedules.
CIFAR-100CNNExperiments
Weuseatotalbatchsizeof
128
withalabeledbatchsizeof
31
for
10
k
and
50
k
labelsettings.Forthesettings
50
k
+
500
k
and
50
k
+
237
k

,weusealabeledbatch
sizeof
64
.Wealsolimitthenumberofunlabeledimagesusedineachepochto
100
k
images.Weuse
themaximumlearningrate

0
=0
:
1
.
CIFAR-100ResNet+Shake-Shake
Weuseatotalbatchsizeof
128
forResNetexperimentswith
alabeledbatchsizeof
31
inalllabelsettings.Forthesettings
50
k
+
500
k
and
50
k
+
237
k

,wealso
limitthenumberofunlabeledimagesusedineachepochto
100
k
images.Weusethemaximum
learningrate

0
=0
:
1
.Thisappliesforboththeshortandlongschedules.
A.10D
OMAIN
A
DAPTATION
Weapplyfast-SWAtothebestexperimentsetting
MT+CT+TFA
forCIFAR-10toSTLaccording
to
Frenchetal.
(
2018
).Thissettinginvolvesusingthresholding(CT)andalsoan
augmentationschemewithtranslation,andaftransformation(TFA).
WemodifytheoptimizertouseSGDinsteadofAdam(
KingmaandBa
,
2015
)andusecosine
annealingschedulewith
`
0
=600
;`
=550
;c
=50
.Weexperimentedwithtwofast-SWAmethods:
averagingweightsonceperepochandaveragingonceeveryiteration,whichismuchmorefrequent
thataveragingeveryepochasinthesemi-supervisedcase.Interestingly,wefoundthatforthistask
averagingtheweightsintheendofeveryiterationinfast-SWAconvergesfasterthan
averagingonceperepochandresultsinbetterperformance.WereporttheresultsinTable
7
.
Weobservethataveragingeveryiterationconvergesmuchfaster(
600
epochsinsteadof
3000
)and
resultsinbettertestaccuracy.Inourexperimentswithsemi-supervisedlearningaveragingmore
oftenthanonceperepochdidn'timproveconvergenceorresults.Wehypothesizethatthe
improvementfrommorefrequentaveragingisaresultofgeometryofthelosssurfacesand
trainingtrajectoriesindomainadaptation.Weleavefurtheranalysisofapplyingfast-SWAtodomain
adaptationforfuturework.
ImplementationDetails
Weusethepubliccode
3
of
Frenchetal.
(
2018
)totrainthemodeland
applyfast-SWA.WhiletheoriginalimplementationusesAdam(
KingmaandBa
,
2015
),weuse
stochasticgradientdescentwithNesterovmomentumandcosineannealinglearningratewith
`
0
=
600
;`
=550
;c
=100
and
k
=100
.Weusethemaximumlearningrate

0
=0
:
1
andmomentum
2
WeusethepublicPytorchcode
https://github.com/CuriousAI/mean-teacher
asourbase
modelfortheMTmodelanditforthe

model.
3
https://github.com/Britefury/self-ensemble-visual-domain-adapt.git
21
PublishedasaconferencepaperatICLR2019
Table7:DomainAdaptationfromCIFAR-10toSTL.VADAresultsarefrom(
Shuetal.
,
2018
)
andtheoriginalSE

isfrom
Frenchetal.
(
2018
).SEisthescorewithourimplementationwithout
fast-SWA.fast-SWA
1
performsaveragingeveryepochandtheresultisobtainedatepoch
3000
.
fast-SWA
2
performstheaveragingevery
iteration
andtheresultisobtainedatepoch
600
.
MethodVADASE

SESE+fast-SWA
1
SE+fast-SWA
2
TestError20.019.918.117.1
16.8
0
:
9
withweightdecayofscale
2

10

4
.WeusethedataaugmentationsettingMT+CF+TFAin
Table1of
Frenchetal.
(
2018
)andapplyfast-SWA.Theresultreportedisfromepoch
4000
.
22
"
88,LAP: a Linearize and Project Method for Solving Inverse Problems with Coupled Variables,http://arxiv.org/pdf/1705.09992v3.pdf,https://github.com/herrinj/LAP,"LAP:ALINEARIZEANDPROJECTMETHODFORSOLVING
INVERSEPROBLEMSWITHCOUPLEDVARIABLES
JAMESL.HERRING

,JAMESG.NAGY

,
AND
LARSRUTHOTTO

Abstract.
Manyinverseproblemsinvolvetwoormoresetsofvariablesthatrepresentt
physicalquantitiesbutaretightlycoupledwitheachother.Forexample,imagesuper-resolution
requiresjointestimationoftheimageandmotionparametersfromnoisymeasurements.Exploiting
thisstructureiskeyfortlysolvingtheselarge-scaleoptimizationproblems,whichareoften
ill-conditioned.
Inthispaper,wepresentanewmethodcalledLinearizeAndProject(LAP)thata
frameworkforsolvinginverseproblemswithcoupledvariables.LAPismostpromisingforcases
whenthesubproblemcorrespondingtooneofthevariablesisconsiderablyeasiertosolvethanthe
other.LAPisbasedonaGauss{Newtonmethod,andthusafterlinearizingtheresidual,iteliminates
oneblockofvariablesthroughprojection.Duetothelinearization,thisblockcanbechosenfreely.
Further,LAPsupportsdirect,iterative,andhybridregularizationaswellasconstraints.Therefore
LAPisattractive,e.g.,forill-posedimagingproblems.ThesetraitstiateLAPfromcommon
alternativesforthistypeofproblemsuchasvariableprojection(VarPro)andblockcoordinatedescent
(BCD).OurnumericalexperimentscomparetheperformanceofLAPtoBCDandVarProusingthree
coupledproblemswhoseforwardoperatorsarelinearwithrespecttooneblockandnonlinearforthe
othersetofvariables.
Keywords.
NonlinearLeast-Squares,Gauss{NewtonMethod,InverseProblems,Regulariza-
tion,ImageProcessing,VariableProjection
AMSsubject
65F10,65F22,65M32
1.Introduction.
WepresentantGauss{NewtonmethodcalledLin-
earizeAndProject(LAP)forsolvinglarge-scaleoptimizationproblemswhosevari-
ablesconsistoftwoormoreblocksrepresenting,e.g.,tphysics.Problems
withthesecharacteristicsarise,e.g.,whenjointlyreconstructingimageandmotion
parametersfromaseriesofnoisy,indirect,andtransformedmeasurements.LAPis
motivatedbyproblemsinwhichtheblocksofvariablesarenontriviallycoupledwith
eachother,butsomeoftheblocksleadtowell-conditionedandeasy-to-solvesub-
problems.Astwoexamplesofsuchproblemsarisinginimaging,weconsidersuper
resolution[
29
,
10
,
4
]andmotioncorrectedMagneticResonanceImaging(MRI)[
1
,
6
].
Ageneralapproachtosolvingcoupledoptimizationproblemsistousealternat-
ingminimizationstrategiessuchasBlockCoordinateDescent(BCD)[
23
,
26
].These
straightforwardapproachescanbeappliedtomostobjectivefunctionsandconstraints
andalsoprovideyforusingvariousregularizationstrategies.However,alter-
natingschemeshavebeenshowntoconvergeslowlyforproblemswithtightlycoupled
blocks[
26
,
4
].
Onespclassofcoupledoptimizationproblems,whichhasreceivedmuch
attention,isseparablenonlinearleast-squaresproblems;see,e.g.,[
18
,
17
,
27
].Here,
thevariablescanbepartitionedsuchthattheresidualfunctionislinearinoneblock
andnonlinearintheother.Forbrevity,wewillrefertothesetsofvariablesas
linear
and
nonlinearblockofvariables
,respectively.Onecommonmethodforsolvingsuch
problemsisVariableProjection(VarPro)[
18
,
17
,
27
].Theideaistoderiveanonlinear
least-squaresproblemofreducedsizebyeliminatingthethelinearblockofvariables
throughprojections.VarProismosttivewhentheprojection,whichentailssolv-
ingalinearleast-squaresproblem,canbecomputedcheaplyandaccurately.When

DepartmentofMathematicsandComputerScience,EmoryUniversity,Atlanta,Georgia,USA.
(
f
jlherri,nagy,lruthotto
g
@emory.edu
)
1
arXiv:1705.09992v3  [math.NA]  14 Jun 20182
J.L.HERRING,J.G.NAGY,ANDL.RUTHOTTO
thenumberoflinearvariablesintheproblemislarge,iterativemethodscanbeused
tocomputetheprojection[
11
],butiterativemethodscanbecometwhenthe
least-squaresproblemisill-posed.Further,standarditerativemethodsdonotprovide
boundsontheoptimality,whichareneededtoensurewgradients;seeour
discussioninSec.
2.2
.Wenotethatrecentlyproposedmethodssuchasin[
9
]can
providethatinformationandare,thus,attractiveoptionsinthesecases.Another
limitationofVarProisthatitisnotstraightforwardtoincorporateinequalitycon-
straintsonthelinearvariables.Someprogresshasbeenmadeforbox-constraintsusing
apseudo-derivativeapproach[
34
]leadingtoapproximategradientsforthenonlinear
problem.Finally,VarProlimitstheoptionsforadaptiveregularizationparameter
selectionstrategiesfortheleast-squaresproblem.
Asanalternativetothemethodsabove,weproposetheLAPmethodforsolving
coupledoptimizationproblems.Ourcontributionscanbesummarizedasfollows:

WeproposeantiterativemethodcalledLAPthatcomputestheGauss{
Newtonstepateachiterationbyeliminatingoneblockofvariablesthroughpro-
jectionandsolvingthereducedproblemiteratively.Sinceprojectionisperformed
afterlinearization,anyblockcanbeeliminated.HencetheLAPframeworkof-
ferssuperiorexibilitycomparedtoexistingprojection-basedapproaches,e.g.,
bysupportingvarioustypesofregularizationstrategiesandtheoptiontoimpose
constraintsforallvariables.

WedemonstrateLAP'silityfortregularizationstrategiesincluding
Tikhonovregularizationusingthediscretegradientoperatorwitharegular-
izationparameterandahybridregularizationapproach[
5
],whichsimultaneously
computesthesearchdirectionandselectsanappropriateregularizationparameter
ateachiteration.

WeuseprojectedGauss{Newtontoimplementelement-wiselowerandupper
boundconstraintswithLAPontheoptimizationvariables.Thisisadistinct
advantageoverVarPro,wherepreviouslyproposedmethodsforincorporatingin-
equalityconstraintsrequireusingapproximategradients.

Wepresentnumericalexperimentsforseveralseparablenonlinearleast-squares
problems.Theproblemsarecharacterizedbylinearimagingmodelsandnonlin-
earmotionmodelswithapplicationsincluding2Dand3Dsuper-resolutionand
motioncorrectionformagneticresonanceimaging.Wecomparetheperformance
ofLAPwithblockcoordinatedescent(BCD)andvariableprojection(VarPro)
byanalyzingconvergence,CPUtimings,numberofmatrix-vectormultiplications,
andthesolutionimagesandmotionparameters.

WeprovideourMATLABimplementationofLAPincludingtheexamplesusedin
thispaperfreelyat
https://github.com/herrinj/LAP
Ourpaperisorganizedasfollows:Sec.
2
introducesageneralformulationof
themotion-correctedimagingproblem,whichweusetomotivateandillustrateLAP,
andreviewsBCDandVarPro;Sec.
3
explainsourproposedscheme,LAP,for
thecoupledGauss{Newtoniterationalongwithadiscussionofregularizationoptions
andimplementationofboundconstraintsusingprojectedGauss{Newton;andSec.
4
providesexperimentalresultsforseveralexamplesusingLAPandcomparesitwith
BCDandVarPro.Weendwithabriefsummaryandsomeconcludingremarks.
2.Motion-CorrectedImagingProblem.
Inthissection,wegiveageneral
descriptionofcoupledoptimizationproblemsarisinginimagereconstructionfrom
LINEARIZEANDPROJECTFORINVERSEPROBLEMS
3
motionmeasurementsandreviewBlockCoordinateDescent(BCD)
andVariableProjection(VarPro).
Wefollowtheguidelinesin[
25
],andconsiderimagesascontinuouslytiable
andcompactlysupportedfunctionsonadomainofinterest,
ˆ
R
d
(typically,
d
=2
or3).Weassumethattheimageattainsvaluesina
F
where
F
=
R
corresponds
toreal-valuedand
F
=
C
tocomplex-valuedimages.Wedenoteby
x
2
F
n
adiscrete
imageobtainedbyevaluatingacontinuousimageatthecell-centersofarectangular
gridwith
n
cells.
Thediscretetransformation
y
2
R
d

n
isobtainedbyevaluatingafunction
y
:
!
R
d
atthecell-centersandcanbevisualizedasatransformedgrid.For
therigid,transformationsofprimaryinterestinthispaper,transformationsare
comprisedofshiftsandrotationsandcanbebyasmallsetofparameters
denotedbythevariable
w
,butingeneral,thenumberofparametersatrans-
formationmaybelarge.Weobservethatunderageneraltransformation
y
(
w
),the
cell-centersofatransformedgriddonotaligntothecell-centersoftheoriginalgrid,so
toevaluateadiscretizedimageunderatransformation
y
,wemustinterpolateusing
theknownimagecots
x
.Thisinterpolationcanberepresentedviaasparse
matrix
T
(
y
(
w
))
2
R
n

n
determinedbythetransformation.Fortheexamplesinthis
paper,weusebilinearortrilinearinterpolationcorrespondingtothedimensionofthe
problem,butotheralternativesarepossible;see,e.g.,[
25
]foralternatives.Thetrans-
formedversionofthediscreteimage
x
isthenexpressedasamatrix-vectorproduct
T
(
y
(
w
))
x
.
Usingtheaboveofdiscreteimagesandtheirtransformations,wethen
considerthediscrete,forwardproblemfor
N
distinctdataobservations,
d
k
=
K
k
T
(
y
(
w
k
))
x
+

k
;
forall
k
=1
;
2
;:::;N;
(2.1)
where
d
k
2
F
m
k
isthemeasureddata,
K
k
2
F
m
k

n
isamatrixcorrespondingto
theproblem-spimageoperator,and

k
isimagenoise,whichweassumetobe
independentlyandidenticallydistributedGaussiannoise.
Inthispaper,wefocusonthecaseinwhichthemotioncanbemodeledbyasmall
numberofparameters.Inourcase,
w
k
2
R
3
or
R
6
modelsrigidtransformationsin2D
and3D,respectively.Thetotaldimensionofthemotionparametersacrossalldata
measurementsisthengivenby
p
=3
N
or
p
=6
N
for
d
=2and
d
=3,respectively.
Intheapplicationathand,wenotethat
p
˝
n
.Tosimplifyournotation,weuse
thecolumnvectors
d
2
F
m
where
m
=
m
k

N
and
w
2
R
p
todenotethedataand
motionparametersforall
N
measurements,respectively.
Givenasetofmeasurements
f
d
1
;
d
2
;:::;
d
N
g
,themotion-correctedimaging
problemconsistsofjointlyestimatingtheunderlyingimageparametersandthemo-
tionparametersin(
2.1
).Weformulatethisasthecoupledoptimizationproblem
min
x
2C
x
;
w
2C
w

x
;
w
)=
1
2
k
KT
(
w
)
x

d
k
2
+

2
k
Lx
k
2
2
,
(2.2)
wherethematrices
K
and
T
havethefollowingblockstructure
K
=
2
6
6
6
4
K
1
K
2
.
.
.
K
N
3
7
7
7
5
and
T
(
w
)=
2
6
6
6
4
T
(
y
(
w
1
))
T
(
y
(
w
2
))
.
.
.
T
(
y
(
w
N
))
3
7
7
7
5
:
4
J.L.HERRING,J.G.NAGY,ANDL.RUTHOTTO
Wedenoteby
C
x
ˆ
F
n
and
C
w
ˆ
R
p
rectangularsetsusedtoimposeboundconstraints
ontheimageandmotionparameters.Lastly,weregularizetheproblembyadding
achosenregularizationoperator,
L
(e.g.,adiscreteimagegradientortheidentity
matrix)andaregularizationparameter,
>
0,thatbalancesminimizingthedata
andtheregularityofthereconstructedimage.Wenotethatagood
regularizationparameter

isaseparate,challengingproblemwhichhasbeenwidely
researched[
5
,
7
,
20
,
35
].OnestrengthofLAPisthat,for
L
=
I
,itallowsfor
regularizationmethodsthatautomaticallyselectthe

parameter[
5
,
12
,
14
,
15
].In
ournumericalexperiments,weinvestigateonesuchhybridmethodforautomatic
regularizationparameterselection[
5
]aswellasdirectregularizationusinga

value.
Problemsoftheform(
2.2
)areoftenreferredtoasseparablenonlinearleast-
squaresproblems.Theyareaspclassofcoupledoptimizationproblemschar-
acterizedbybeingnonlinearinoneblockofvariables,
w
,andlinearintheother,
x
.Severaloptimizationapproachesexistforsolvingsuchproblems.Oneoptionisa
fullycoupledGauss{Newtonapproachtooptimizeoverbothsetsofvariablessimul-
taneouslybysolvingasinglelinearsystem.However,thisapproachdoesnotexploit
theconvexityoftheproblemintheimagevariables,resultinginsmallGauss{Newton
stepsduetothenonlinearityofthemotionparameters[
4
].Twomethods,whichhave
beenshowntobepreferabletothefullycoupledoptimizationforsolvingseparable
nonlinearleast-squaresproblems,areBlockCoordinateDescent,whichrepresentsa
fullydecoupledoptimizationapproach,andVariableProjection,whichrepresentsa
partiallycoupled,optimizationapproach.Wenowprovideabriefreviewofthosetwo
methodsbeforeintroducingLAP.
2.1.BlockCoordinateDescent(BCD).
BCDrepresentsafullydecoupled
approachtosolvingcoupledoptimizationproblemssuchas(
2.2
);see,e.g.,[
26
].In
BCD,theoptimizationvariablesarepartitionedintoanumberofblocks.Themethod
thensequentiallyoptimizesoveroneblockofvariableswhileholdingalltheothers
Afteronecycleinwhichallsubsetsofvariableshavebeenoptimized,one
iterationiscompleted.Theprocessistheniterateduntilconvergence.Forthispaper,
weseparatethevariablesintothetwosubsetsofvariablessuggestedbythestructure
oftheproblem,onefortheimagevariablesandanotherforthemotionvariables.At
the
k
thiteration,we
w
k
andobtaintheupdatedimage
x
k
+1
bysolving
x
k
+1
=argmin
x
2C
x

x
;
w
k
)
:
(2.3)
Afterwards,weournewguessfor
x
k
+1
andoptimizeoverthemotion
w
,
w
k
+1
=argmin
w
2C
w

x
k
+1
;
w
)
:
(2.4)
Thesetwostepsconstituteasingleiterationofthemethod.WenotethatBCDis
decoupledinthesensethatwhileoptimizingoveronesetofvariables,weneglect
optimizationovertheother.Thisdegradesconvergencefortightlycoupledprob-
lems[
26
].However,BCDhasmanyadvantages.Itisapplicabletogeneralcoupled
problemsincludingonesthatarenonlinearinallblocksofvariables.Also,ital-
lowsforstraightforwardimplementationofboundconstraintsandsupportsvarious
typesofregularization.Forournumericalexperiments,wesolvetheBCDimaging
problem(
2.3
)inexactlyusingasinglestepofprojectedGauss{Newtonwithbound
constraintsandvariousregularizers,whichweintroduceinSection
3.3
.Theopti-
mizationprobleminthesecondstep(
2.4
)issmall-dimensionalandseparable,andwe
LINEARIZEANDPROJECTFORINVERSEPROBLEMS
5
performasinglestepofGauss{Newtonwithadirectsolvertocomputethesearch
direction.
2.2.VariableProjection(VarPro).
VarProisfrequentlyusedtosolvesep-
arablenonlinearleast-squaresproblemssuchastheonein(
2.2
);see,e.g.,[
17
,
27
].
ThekeyideainVarProistoeliminatethelinearvariables(here,theimage)bypro-
jectingtheproblemontoareducedsubspaceassociatedwiththenonlinearvariables
(here,themotion)andthensolvingtheresultingreduced,nonlinearoptimization
problem.Inourproblem(
2.2
),eliminatingtheimagevariablesrequiressolvinga
linearleast-squaresprobleminvolvingthematrix
T
(
w
)thatdependsonthecurrent
motionparameters.Weexpresstheprojectionby
x
(
w
)=argmin
x
2C
x

x
;
w
)
:
(2.5)
Substitutingthisexpressioninfor
x
,wethenobtainareduceddimensionalproblem
intermsofthenonlinearvariable
w
,
min
w
2C
w

x
(
w
)
;
w
)
:
(2.6)
Thereducedproblemissolvedtorecoverthemotionparameters,notingthatby
solving(
2.5
)ateachiteration,wesimultaneouslyrecoveriteratesfortheimage.As-
suming
C
w
=
R
p
(unconstrainedcase),weseethatthenecessaryoptimality
conditionin(
2.6
)is
0=
r
w

x
(
w
)
;
w
)+
r
w
x
(
w
)
r
x

x
(
w
)
;
w
)
:
(2.7)
Notethatintheabsenceofconstraintson
x
(i.e.,
C
x
=
F
n
)thesecondtermonthe
righthandsideof(
2.7
)vanishesduetotheoptimalityconditionof(
2.6
).
However,thisisnotnecessarilythecasewhen
C
x
6
=
F
n
orwhen(
2.5
)issolvedwith
lowaccuracy.Inthosecases
r
x

x
(
w
)
;
w
)doesnotequal0andcomputing
r
w
x
(
w
),
whichcanbeashardassolvingtheoriginaloptimizationproblem(
2.2
),isinevitable.
Insuchsituations,neglectingthesecondtermin(
2.7
)mayconsiderablydegradethe
performanceofVarPro.
3.LinearizeandProject(LAP).
WenowintroducetheLAPmethodfor
solvingthecoupledoptimizationproblem(
2.2
).Webeginbylinearizingtheresidual
in(
2.2
)followingastandardGauss{Newtonframework.Ineachiteration,computing
thesearchdirectionthenrequiressolvingalinearsystemthatcouplestheimageand
motionparameters.Wedothisbyprojectingthecoupledproblemontooneblock
ofvariables.Thisyintermsofregularizationandcanhandlebound
constraintsonboththemotionandimagevariables.Furthermore,italsoallowsthe
usertofreelychoosewhichblockofvariablesiseliminatedviaprojection.
WeintroduceourapproachtosolvingthelinearcoupledproblemfortheGauss{
Newtonstepbybreakingthediscussionintoseveralsubsections.Westartwitha
subsectionintroducingourstrategyofprojectionontotheimagespaceforanuncon-
strainedproblemwhere
C
x
=
F
n
and
C
w
=
R
p
.Thisisfollowedbyasubsectiononthe
variousoptionsforimageregularizationthatourprojectionapproachLastly,
weextendLAPtoaprojectedGauss{Newtonframeworktoallowforelement-wise
boundconstraintsonthesolution,i.e.,when
C
x
and
C
w
arepropersubsetsof
F
n
and
R
p
,respectively.
6
J.L.HERRING,J.G.NAGY,ANDL.RUTHOTTO
3.1.LinearizingtheProblem.
WebeginconsideringaGauss{Newtonframe-
worktosolvethecoupledproblem(
2.2
)intheunconstrainedcase,i.e.,
C
x
=
F
n
and
C
w
=
R
p
.TosolvefortheGauss{Newtonstepateachiteration,wereformulate
theproblembylinearizingtheresidual
r
(
x
;
w
)=
KT
(
w
)
x

d
aroundthecurrent
iterate,(
x
0
;
w
0
).Denotingthisresidualas
r
0
=
r
(
x
0
;
w
0
),wecanwriteits
Taylorapproximationas
r
(
x
0
+

x
;
w
0
+

w
)
ˇ
r
0
+

J
x
J
w



x

w

;
(3.1)
where
J
x
=
r
x
r
>
0
and
J
w
=
r
w
r
>
0
aretheJacobianoperatorswithrespecttothe
imageandmotionparameters,respectively.Theycanbeexpressedas
J
x
=
T
>
K
>
;
and
J
w
=diag(
r
w
1
(
T
(
y
(
w
1
))
x
)
;:::;
r
w
N
(
T
(
y
(
w
N
))
x
))
>
K
>
;
whereeachterm
r
w
k
(
T
(
y
(
w
k
))
x
)isthegradientofthetransformedimageatthe
transformation
y
(
w
k
);see[
4
]foradetailedderivation.Both
J
x
2
F
m

n
and
J
w
2
F
m

p
aresparseandwerecallthatintheapplicationathand
p
˝
n
.
Afterlinearizingtheresidualaroundthecurrentiterate,wesubstitutetheap-
proximation(
3.1
)fortheresidualtermin(
2.2
)toget
min

x

w
^


x
;
w
)=
1
2
k
J
x

x
+
J
w

w
+
r
0
k
2
+

2
k
L
(
x
0
+

x
)
k
2
(3.2)
Bysolvingthisproblemweobtaintheupdatesfortheimageandmotionparameters,
denotedby

x
and

w
,respectively.As(
3.2
)isbasedonalinearizationitiscommon
practicetosolveitonlytoalowaccuracy;see,e.g.,[
26
].Notethatsolving(
3.2
)di-
rectlyusinganiterativemethodequatestothefullycoupledGauss{Newtonapproach
mentionedinSec.
2
,whichhasbeenobservedtoconvergeslowlyduringoptimization.
ThismotivatesLAP'sprojection-basedstrategytosolvingthelinearizedproblemfor
theGauss{Newtonstep,whichwenowpresent.
3.2.Projectingtheproblemontotheimagespace.
RecallthatVarProis
restrictedtoprojectingontothenonlinearvariables(inourcase,thiswouldbethe
motionparameters,whichrequiresustosolveonelarge-scaleimagereconstruction
problemperiteration).Withourframework,thereisnosuchrestriction;thecoupled
linearproblem(
3.2
)canbeprojectedontoeithersetofvariables.Becausetherearea
smallnumberofnonlinearvariables,tosolvethecoupledlinearproblemin(
3.2
),we
proposeprojectingtheproblemontotheimagespaceandsolvingfor

x
.Toproject,
wenotethattheoptimalityconditionforthelinearizedproblemwith
respectto

w
is
0=
r

w

J
x

x
+
J
w

w
+
r
0

>

J
x

x
+
J
w

w
+
r
0

orequivalently,
0=2

J
>
w
J
w

w
+
J
>
w
J
x

x
+
J
>
w
r
0

:
Solvingthisconditionfor

w
,weget

w
=


J
>
w
J
w


1

J
>
w
J
x

x
+
J
>
w
r
0

:
(3.3)
LINEARIZEANDPROJECTFORINVERSEPROBLEMS
7
Wecanthensubstitutethisexpressionfor

w
into(
3.2
)andgroupterms.This
projectstheproblemontotheimagespaceandgivesanewproblemintermsof

x
,
theJacobians
J
x
and
J
w
,andtheresidual
r
0
givenby
min

x
1
2




I

J
w

J
>
w
J
w


1
J
>
w

J
x

x
+

I

J
w

J
>
w
J
w


1
J
>
w

r
0



2
+

2
k
L
(
x
0
+

x
)
k
2
ormoresuccinctly,
min

x
1
2


P
?
J
w

J
x

x
+
r
0



2
+

2
k
L
(
x
0
+

x
)
k
2
(3.4)
where
P
?
J
w
=
I

J
w
(
J
>
w
J
w
)

1
J
>
w
isaprojectionontotheorthogonalcomplementof
thecolumnspaceof
J
w
.Thisleast-squaresproblemcanbesolvedusinganiterative
methodwithanappropriaterightpreconditioner[
3
,
24
,
33
].Inparticular,weobserve
that
P
?
J
w
J
x
=
J
x

J
w

J
>
w
J
w


1
J
>
w
J
x
isalowrankperturbationoftheoperator
J
x
sincerank(
P
?
J
w
)

p
˝
n
.Hence,a
goodproblem-sppreconditionerfor
J
x
shouldbeasuitablepreconditionerfor
theprojectedoperator.
Itisimportanttoemphasizethatinourapproach,thematrix
J
>
w
J
w
2
R
p

p
ismoderatelysizedandsymmetricpositivif
J
w
isfullrank.Therefore,it
iscomputationallycienttocomputeitsCholeskyfactorsonceperouterGauss{
Newtoniterationandreusethemtoinvertthematrixwhenneeded.Furthermore,
onecanuseathinQRfactorizationof
J
w
tocomputetheCholeskyfactorsand
avoidforming
J
>
w
J
w
explicitly.Weusethisstrategytoincreasetheof
iterativelysolving(
3.4
)whenusingmatrix-freeimplementations,andinpracticehave
notseen.Aftersolvingthepreconditionedprojectedproblemfor

x
,
oneobtainstheaccompanyingmotionstep

w
via(
3.3
).
3.3.Regularization.
Afterlinearizingandprojectingontothespaceofthe
linear,imagevariables,wesolvetheregularizedleast-squaresproblem(
3.4
).For
ourapplications,thisproblemishigh-dimensional,andweuseaniterativemethod
toapproximatelysolveitwithlowaccuracy.Also,thisproblemisill-posedforour
numericaltestsandthusrequiresregularization.Herewediscussthetypesofdirect,
iterative,andhybridmethodsforregularizationthatcanbeusedinLAP.
Werstnotethat(
3.4
)isaTikhonovleast-squaresproblem.Problemsofthis
formarewell-studiedintheliterature;see,e.g.,[
21
,
8
,
35
,
22
].Thequalityofan
iterativesolutionforthistypeofproblemdependsontheselectionofanappropriate
regularizationparameter

andaregularizationoperator
L
.Commonchoicesfor
L
includethediscretizedgradientoperator
L
=
r
h
usingforwardsandthe
identity
L
=
I
.Additionally,problemsincludingnon-quadraticregularizers,e.g.,to-
talvariation[
32
]or
p
-normbasedregularizerscanbeaddressedbysolvingasequence
ofleast-squaresproblemsthatincludeweighted
`
2
regularizers[
30
,
31
,
36
],ormoreef-
tlyviaahybridKrylovsubspaceapproach[
12
].ientsolversforoptimization
problemswithquadraticregularizersarealsoakeyingredientofsplitting-basedmeth-
ods,e.g.,theSplitBregmanmethodfor
`
1
regularizedproblems[
16
].Thus,whilewe
restrictourselvestoquadraticregularizationtermsforthescopeofthispaper,LAP
issuitableforabroaderclassorregularizationoptions.Formoreinformationon
regularizationparameterselection,see[
21
,
35
].
8
J.L.HERRING,J.G.NAGY,ANDL.RUTHOTTO
Anotherapproachisiterativeregularization[
8
,
21
],whichaimstostopaniterative
methodattheiterationthatminimizesthereconstructionerror.Itexploitsthefact
thattheearlyiterationsofiterativemethodslikeLandweberandKrylovsubspace
methodscontainthemostimportantinformationaboutthesolutionandareless
bynoiseinthedata.Incontrast,lateriterationsofthesemethodscontainless
informationaboutthesolutionandaremoreThisresultsinareduction
ofthereconstructionerrorforthecomputedsolutionduringtheearlyiterationsof
theiterativemethodfollowedbyitsincreaseinlateriterations,aphenomenonknown
assemi-convergence.However,inpracticeiterativeregularizationcanbeas
thereconstructionerrorateachiterateisunknownandthequalityofthesolutionis
sensitivetoanaccuratechoiceofstoppingiterate.
Hybridregularizationmethodsrepresentfurtheralternativesthatseektocombine
theadvantagesofdirectanditerativeregularization,seee.g.,[
5
,
12
,
14
,
15
]andthe
referencestherein.HybridmethodsuseTikhonovregularizationwithanewregular-
izationparameter

k
ateachstepofaniterativeKrylovsubspacemethodsuchas
LSQR[
28
].DirectregularizationfortheearlyiteratesofaKrylovsubspacemethod
ispossibleduetothesmalldimensionalityoftheKrylovsubspace.ThismakesSVD-
basedparameterselectionmethodsforchoosing

k
computationallyfeasibleateach
iteration.Thevariabilityof

k
ateachiterationhelpstostabilizethesemi-convergence
behavioroftheiterativeregularizationfromtheKrylovsubspacemethod,makingit
lesssensitivetothechoiceofstoppingiteration.Thus,hybridmethodscombine
theadvantagesofbothdirectanditerativeregularizationwhileavoidingthecostof
SVD-basedparameterselectionforthefull-dimensionalproblemandalleviatingthe
sensitivitytostoppingcriteriawhichcomplicatesiterativeregularization.
Eachofthemethodsinthispapernecessitatessolvingaregularizedsysteminthe
imagevariablegivenby(
3.4
)forLAP,(
2.5
)forVarPro,and(
2.3
)forBCD.Forthese
problems,weusebothdirectandhybridregularizationwhenpossibletodemonstrate
theyoftheLAPapproach.WebeginbyrunningLAPwiththediscrete
gradientregularizer,
L
=
r
h
anda

.Thisapproachisalsofeasiblewithinthe
VarProframeworkandisstraightforwardtoimplementusingBCD.Totesthybrid
regularization,weusethe
HyBR
method[
5
](viatheinterfacegivenintheIRTools
package[
13
])forLAPandBCDtoautomaticallychoose

usingaweightedGCV
method.Wenotethatbecausesuchhybridregularizationmethodsaretailoredto
linearinverseproblems,theycannotbeusedtodirectlysolvethenonlinearVarPro
optimizationproblem(
2.5
).
3.4.Optimization.
InSec.
3.1
and
3.2
,weintroducedLAPfor(
2.2
)forthecase
ofanunconstrainedproblem.Inpracticeforimagingproblemssuchas(
2.2
),bounds
ontheimageand/ormotionvariablesareoftenknown,inwhichcaseimposingsuch
priorknowledgeintotheinverseproblemisdesirable.Thissectiondetailshowthe
LAPstrategycanbecoupledwithprojectedGauss{Newtontoimposeelement-wise
boundconstraintsonthesolutionsfor
x
and
w
.WeintroduceprojectedGauss{
Newtonfollowingthedescriptionin[
19
].Themethodrepresentsacompromisebe-
tweenafullGauss{Newton,whichconvergesquicklywhenappliedtothefullproblem
andprojectedgradientdescent,whichallowsforthestraightforwardimplementation
ofboundconstraints.ForprojectedGauss{Newton,weseparatethestepupdates

x
and

w
intotwosets:thesetofvariablesforwhichtheboundconstraintsareinactive
(theinactiveset)andthesetofvariablesforwhichtheboundconstraintsareactive
(theactiveset).Wedenotethesesubsetsfortheimageby

x
I
ˆ

x
and

x
A
ˆ

x
wherethesubscript
I
and
A
denotetheinactiveandactivesets,respectively.Identical
LINEARIZEANDPROJECTFORINVERSEPROBLEMS
9
notationisusedfortheinactiveandactivesetsforthemotion.
Ontheinactiveset,wetakethestandardGauss{Newtonstepateachiteration
usingtheLAPstrategydescribedinSec.
3.2
.Thisstepiscomputedbysolving(
3.2
)
restrictedtotheinactiveset.Thus,(
3.4
)becomes
min

x
1
2



^
P
?
J
w

^
J
x

x
I
+
r
0




2
+

2
k
^
L
(
x
0
;
I
+

x
I
)
k
2
;
(3.5)
where
^
J
x
,
^
J
w
,
^
P
?
J
w
,and
^
L
are
J
x
,
J
w
,
P
J
w
,and
L
restrictedtotheinactivesetvia
projection.Wethenobtainthecorrespondingmotionstepontheinactivestepby

w
I
=


^
J
>
w
^
J
w


1

^
J
>
w
^
J
x

x
I
+
^
J
>
w
r
0

:
(3.6)
Thisequationisanalogousto(
3.3
)fortheunconstrainedproblem.NotethatLAP's
projectionstrategyisonlyusedontheinactiveset.Thus,theconstraintsdonot
theoptimalityconditionfortheprojectiontoeliminatethe

w
I
blockofvariables.
Theprojectedleast-squaresproblem(
3.5
)isalsoAlso,forthespecialcase
whentheupperandlowerboundsonallvariablesare

and
1
andallvariables
belongtotheinactivesetateachiteration,themethodreducestostandardGauss{
NewtonaspresentedinSec.
3.2
.
Fortheactiveset,weperformascaled,projectedgradientdescentstepgivenby


x
A

w
A

=


~
J
>
x
r
0
~
J
>
w
r
0




~
L
>
~
L
(
x
0
;
A
+

x
A
)
0

:
(3.7)
whereagain
~
J
x
,
~
J
w
,and
~
L
representtheprojectionof
J
x
,
J
w
,and
L
ontotheactive
set.Wenotethattheregularizationparameter

shouldbeconsistentforboth(
3.5
)
and(
3.7
).Whenusingdirectregularizationwitha

,thisisobvious.However,
forthehybridregularizationapproachdiscussedinSec.
3.3
,achoiceisrequired.We
set

ontheactivesetateachiterationtobethesameasthe

adaptivelychosenby
thehybridregularizationontheinactivesetatthesameiterate.
ThefullstepforaprojectedGauss{Newtoniterationisthengivenbyascaled
combinationofstepsontheinactiveandactivesets


x

w

=


x
I

w
I

+



x
A

w
A

:
(3.8)
Here,theparameter
>
0isaweightingparametertoreconciletheinscales
betweentheGauss{Newtonandgradientdescentsteps.Toselectthisparameter,we
followtherecommendationof[
19
]anduse

=
max(
k

x
I
k
1
;
k

w
I
k
1
)
max(
k

x
A
k
1
;
k

w
A
k
1
)
:
(3.9)
Thischoiceof

ensuresthattheprojectedgradientdescentsteptakenontheactive
setisnolargerthantheGauss{Newtonsteptakenontheinactiveset,andwehave
useditwithnoillinpractice.
Aftercombiningthestepsforboththeinactiveandactivesetsusing(
3.8
),we
useaprojectedArmijolinesearchforthecombinedsteptoensurethenextiterate
obeystheproblem'sconstraints.AstandardArmijolinesearchchoosesastepsize
0
<

1bybacktrackingfromthefullGauss{Newtonstep(

=1)toensurea
10
J.L.HERRING,J.G.NAGY,ANDL.RUTHOTTO
reductionoftheobjectivefunction[
26
].TheprojectedArmijolinesearcha
moArmijoconditiongivenby


P
C
x
(
x
+

x
)
;
P
C
w
(
w
+

w
)



x
;
w
)+

Q

r

x
;
w
)

>


x

w

:
(3.10)
Here,
P
C
x
and
P
C
w
areprojectionsontothefeasiblesetfortheimageandmotion
variables,respectively,and
Q

r

x
;
w
)

istheprojectedgradient.Theconstant
c
2
(0
;
1)determinesthenecessaryreductionforthelinesearch;weset
c
=10

4
as
suggestedin[
26
].Undertheprojections
P
C
x
and
P
C
w
,variablesthatwouldleave
thefeasibleregionareprojectedontotheboundaryandjointheactivesetforthe
nextiteration.However,theprojectiondoesnotpreventvariablesfromleavingthe
activesetandjoiningtheinactiveset.Thisnecessitatesupdatingtheinactiveand
activesetsafterthelinesearchateachnextiteration.Alsonotethatforthespecial,
unconstrainedcase,thelinesearchrevertstothestandardArmijolinesearchwith
noneedforprojection.However,forthiscase,theproblemrevertstothestandard
Gauss{Newtonframeworkwhichshouldremovethenecessityforalinesearchinmost
cases.
Lastly,wediscussthechoiceofstoppingcriteriafortheprojectedGauss{Newton
method,whichagaindependsonthetypeofregularizationused.Foraregular-
izationparameter

,wemonitortherelativechangeoftheobjectivefunctionandthe
normoftheprojectedgradientincludingtheregularizerterm.Theprojectedgradient
istheoptimalityconditionoftheconstrainedproblemin(
2.2
)andcanbe
computedviaaprojection[
2
].Theprojectionisnecessarybecausetheboundcon-
straintspreventgradiententriescorrespondingtovariableswithminimaoutsidethe
feasibleregionfromconvergingtozero.Whenusinghybridregularization,wemust
considerthevariabilityof

k
ateachGauss{Newtoniteration.Selectingat

k
parameterateachiterationchangestheweightoftheregularizationterminthe
objectivefunction(
2.2
)anditsprojectedgradientateachiteration.Thismakesthe
fullobjectivefunctionvalueandprojectedgradientforthesemethodsunreliablestop-
pingcriteria.Instead,wemonitorboththenormofthebetweenthecurrent
iterateandthepreviousiterateandthederencebetweenthepreviousandcurrent
objectivefunctionvaluesforthedatatermoftheobjectivefunction.Westop
theGauss{Newtonmethodwheneitherofthosevaluesdropsbelowacertainthresh-
old,indicatingastagnationofthemethod.Thisallowsustomonitorthebehavior
oftheGauss{Newtoniterationwithoutbeingsubjecttotheassociated
withthevarying

k
.
3.5.SummaryoftheMethod.
WesummarizethediscussionofLAPbypre-
sentingtheentireprojectedGauss{NewtonalgorithmusingtheLAPframework.This
providesaframeworktotlysolvethecoupledimagingproblemsofinterestwhile
includingtheoptionsforregularizationandboundconstraintswhichhelped
motivateourapproach.ThecompletealgorithmisgiveninAlgorithm
1
.
4.NumericalExperiments.
WenowtestLAPforthreecoupledimagingprob-
lems.Theseareatwo-dimensionalsuper-resolutionproblem,athree-dimensional
super-resolutionproblem,andalinearMRImotioncorrectionproblem.Forallexam-
ples,wecomparetheresultsusingLAPtothoseofVarProandBCD.Welookatthe
qualityoftheresultantimage,theabilityofthemethodstocorrectlydeterminethe
motionparametersacrossalldataframes,thenumberofiterationsrequiredduring
LINEARIZEANDPROJECTFORINVERSEPROBLEMS
11
Algorithm1
LinearizeandProject(LAP)
1:
Given
x
0
and
w
0
2:
Computeactiveandinactivesets,
A
and
I
3:
Evaluate
x
0
;
w
0
),
r
0
,
J
(0)
x
and
J
(0)
w
4:
for
k
=1
;
2
;
3
;:::
do
5:
ComputethesteptheinactivesetwithLAPusing(
3.5
)and(
3.6
)
6:
Computethestepontheactivesetusingprojectedgradientdescent(
3.7
)
7:
Combinethestepsusing(
3.8
)and(
3.9
)
8:
PerformtheprojectedArmijolinesearchsatisfying(
3.10
),update
x
k
;
w
k
9:
Updateactiveandinactivesets,
A
and
I
10:
Evaluate
x
k
;
w
k
),
r
k
,and
J
(
k
)
x
and
J
(
k
)
w
11:
if
Stoppingcriteria
then
12:
returnx
k
;
w
k
13:
endif
14:
endfor
optimization,thecostofoptimizationintermsmatrix-vectormultiplications,andthe
CPUtimetoreachasolution.Tocomparethequalityoftheresultantimageand
motion,weuserelativeerrorswithrespecttotheknown,truesolutions.Thematrix-
vectormultiplicationsofinterestarethosebytheJacobianoperatorassociatedwith
thelinear,imagingvariable
J
x
.Matrix-vectormultiplicationsbythisoperatorare
requiredinthelinearleast-squaressystemsforallthreemethodsandarethemost
computationallyexpensiveoperationwithintheoptimization.
4.1.Two-DimensionalSuperResolution.
Next,werunnumericalexperi-
mentsusingarelativelysmalltwo-dimensionalsuperresolutionproblem.Toconstruct
asuperresolutionproblemwithknowngroundtruthimageandmotionparameters,
weusethe2DMRIdatasetprovidedinFAIR[
25
](originalresolution128

128)to
generate32framesoflow-resolutiontestdata(resolution32

32)afterapplying2D
rigidbodytransformationswithrandomlychosenparameters.Gaussianwhitenoise
isaddedusingtheformula
d
k
=

d
k
+

k
where

k
=

k

d
k
k
2
k
n
k
k
2
n
k
where

d
k
denotesanoisefreelow-resolutiondataframe,

isthepercentageofnoise
and
n
k
isavectorofnormallydistributedrandomvalueswithmean0andstandard
deviation1.Ourexperimentsshowresultswith

=1%,2%,and3%noiseadded
tothelow-resolutiondataframes.Theresultingsuper-resolutionproblemisanopti-
mizationproblemoftheform(
2.2
).Here,theimagingoperator
K
isablockdiagonal
matrixcomposedof32identicaldown-samplingmatrices
K
=
K
k
alongthediagonal,
whichrelatethehigh-resolutionimagetothelowerresolutiononeviablockaverag-
ing.Thetotalnumberofparametersintheoptimizationis16
;
528correspondingto
16
;
384fortheimageand96forthemotion.
Asmentionedin[
4
],thechoiceofinitialguessiscrucialinsuper-resolutionprob-
lems,soweperformrigidregistrationofthelow-resolutiondataontothestvolume
(resultingin31rigidregistrationproblems)toobtainastartingguessforthemotion
parameters.Theresultingrelativeerroroftheparametersisaround2%.Usingthese
parameters,wesolvethelinearimagereconstructionproblemtoobtainastarting
guessfortheimagewitharound5%relativeerror.
12
J.L.HERRING,J.G.NAGY,ANDL.RUTHOTTO
Wethensolvethesuper-resolutionproblemusingLAP,VarPro,andBCD.For
allthreeapproaches,wecomparetworegularizationstrategies.Allthreemethods
arerunusingthegradientoperator,
L
=
r
h
witharegularizationparameter

=0
:
01.LAPandBCDarethenrunwiththeGolub-Kahanhybridregularization
approachdetailedinSection
3.3
(denotedas
HyBR
intablesandAspreviously
mentioned,
HyBR
cannotbeapplieddirectlytotheVarProoptimizationproblem(
2.5
),
soforVarProweusea

=0
:
01andtheidentityasoursecondregularizer,
L
=
I
.
Toallowforcomparison,weusethesameregularizationparameterasin[
4
],which
maynotbeoptimalforanyofthemethodspresented.Formorerigorousselection
criteria,wereferthereadertothemethodsmentionedinSec.
3.3
.ForLAPand
BCD,weaddelement-wiseboundconstraintsontheimagespaceintherange[0
;
1]
forbothchoicesofregularizer,withbothboundsactiveinpractice.Thenumberof
activeboundsvariesfortnoiselevelsandrealizationsoftheproblembutcan
includeasmanyas30{35%oftheimagevariablesforbothLAPandBCD.VarPro
isrunwithoutboundconstraints.Neitherconstraintsnorregularizationareimposed
onthemotionparametersforthethreemethods.
Allthreemethodsrequiresolvingtwottypesoflinearsystems,oneasso-
ciatedwiththeimagevariablesandanotherwiththemotionvariables.LAPsolves
thesesystemstodeterminetheGauss{Newtonstep.WeuseLSQR[
28
]withastop-
pingtoleranceof
10

2
tosolve(
3.4
)forbothregularizationapproaches.Themotion
step(
3.3
)iscomputedusingtheCholeskyfactorsof
J
>
w
J
w
.VarProrequiressolving
thelinearsystem(
2.5
)withineachfunctioncall.Forbothchoicesofregularization,we
solvedthissystembyrunningLSQRforanumberof20iterations.Recallthat
thissystemmustbesolvedtoahigheraccuracythanthesimilarlysizedsystemsin
LAPandBCDtomaintaintherequiredaccuracyinthegradient;seeSec.
2.2
.Gauss{
NewtononthereduceddimensionalVarProfunction(
2.6
)thenrequiressolvingthe
reducedlinearsysteminthemotionparameters,whichwesolveusingCholeskyfac-
torizationonthenormalequations.ForBCD,coordinatedescentrequiresalternating
solutionsforthelinearsystemintheimage,(
2.3
),andanonlinearsysteminthemo-
tionparameters,(
2.4
).Forbothofthese,wetakeasingleprojectedGauss{Newton
step.Fortheimage,thisissolvedusingLSQRwithastoppingtoleranceof
10

2
using
MATLAB
's
lsqr
functionfordirectregularizationand
HyBR
'sLSQRforhybrid
regularization.LSQRwiththistoleranceisalsousedforthecorrespondingproblems
inthe3Dsuper-resolutionandMRImotioncorrectionexamplesinSec.
4.2
and
4.3
.
Forthemotion,weuseCholeskyfactorizationonthenormalequations.Duringthese
solves,wetrackcomputationalcostsforthenumberofmatrix-vectormultiplications
bytheJacobianoperatorassociatedwiththeimage,
J
x
.ForLAPandBCD,these
multiplicationsarerequiredwhensolvingthesystemfortheGauss{Newtonstepfor
theimagestep,whileforVarPro,theyarenecessaryfortheleast-squaressolvewithin
theobjectivefunction.
Wecompareresultsforthemethodsusingtherelativeerrorsoftheresultant
imageandmotionparameters.Weseparaterelativeerrorsfortheimageandmotion.
PlotsoftheseerrorsagainstiterationcanbeseeninFig.
4.2
fortheproblemwith2%
addednoise.ThecorrespondingresultingimageforLAPforthe2%errorcasecan
beseeninFig.
4.1
.Lastly,atableofrelevantvaluesincludingtheaveragenumberof
iterations,minimumerrorsforimageandmotion,matrix-vectormultiplications,and
CPUtimingsforthemethodstakenover10trealizationsoftheproblemfor
allthreenoiselevelsisinTable
4.1
.
Fordirectregularizationusingthediscretegradientoperator,thesolutionsfor
LINEARIZEANDPROJECTFORINVERSEPROBLEMS
13
Fig.4.1
.
Theimagesfromlefttorightshowamontageofdataframes,theinitialguess
x
initial
,
andthereconstructedimage
x
LAP
forLAPusingthediscretegradientregularizerforthe2Dsuper
resolutionproblemwith2%noise.
allthreemethodsarecomparableintermsoftherelativeerrorforthemotion,with
LAPandBCDslightlyoutperformingVarProfortherelativeerroroftherecovered
images.Thisisadirectresultoftheelement-wiseboundconstraintsontheresultant
imagesusingthesemethods.Furthermore,thesesolutionsaresuperiortothoseforall
threemethodsusinghybridregularizationortheidentityoperator,suggestingthat
thisisamoreappropriateregularizerforthisproblem.LAPwiththediscretegradient
operatorrecoversthemostaccuratereconstructedimageofthethreemethodsand
achievesbetterorcomparablerecoveryofthemotionparameters.Thisisobservable
intherelativeerrorplotsforthe2%addednoisecaseinFig.
4.2
,andfortheproblem
withallthreenoiselevelsinTable
4.1
.Wecanalsoseefromtherelativeerror
plotsthatLAPtendstorecoverthecorrectmotionparametersearlierintheGauss{
NewtoniterationsthaneitherBCDorVarPro.Intermsofmethodcost,boththe
LAPandBCDiterationscosttlylessintermsoftimeandmatrix-vector
multiplicationsthanthoseofVarPro,resultinginfasterCPUtimesandfewermatrix-
vectormultipliesfortheentireoptimization.However,whileBCDisalsorelatively
cheapintermsofmatrix-vectormultipliesandCPUtime,LAPoutperformsitinterms
ofsolutionquality.Overall,theLAPapproachcomparesfavorablytoVarProand
BCDforthisexampleintermsofboththeresultingsolutionsandcost,outperforming
bothmethods.
4.2.Three-DimensionalSuperResolution.
Thenextproblemisalarger
three-dimensionalsuper-resolutionproblem.Again,weusea3DMRIdatasetpro-
videdin
FAIR
[
25
]toconstructasuper-resolutionproblemwithaknowngroundtruth
imageandmotionparameters.Thegroundtruthimage(resolution160

96

144)
isusedtogenerate128framesoflow-resolutiontestdata(resolution40

24

32).
Eachframeofdataisshiftedandrotatedbyarandom3Drigidbodytransformation,
afterwhichitisdownsampledusingblockaveraging.Lastly,Gaussianwhitenoiseis
addedtoeachlow-resolutiondataframeinthesamewayasforthetwo-dimensional
super-resolutionproblem,andweruntheproblemfor

=1%,2%,and3%added
noiseperdataframe.Theresultingoptimizationproblemhas2
;
212
;
608unknowns,
2
;
211
;
840fortheimageand768forthemotionparameters.Thedatahasdimension
5
;
898
;
240.Theformulationoftheproblemisidenticaltothatofthetwo-dimensional
super-resolutionproblemwithappropriatecorrectionsforthechangeindimension.
14
J.L.HERRING,J.G.NAGY,ANDL.RUTHOTTO
Fig.4.2
.
Theseplotsshowtherelativeerrorsforboththereconstructedimageandthemotion
parametersforthe2Dsuperresolutionproblemwith
2
%addednoise.Wenotethatmethodsusing
thediscretegradientregularizerreachlowerminimafortheimageinthisproblem,andLAPwiththe
discretegradientregularizeroutperformsbothVarProandBCDinrecoveringthemotionparameters
intheearlyiterationsofthemethodforallnoiselevelstested.
1%Noise
Iter.
Rel.Err.
x
Rel.Err.
w
MatVecs.
Time(s)
LAP+
HyBR
11.1
5.34e-2
1.82e-2
93.4
14.3
LAP+
r
h
13.1
3.76e-2
1.78e-2
62.2
15.4
VarPro+
I
25.7
8.60e-2
1.84e-2
554.0
60.1
VarPro+
r
h
21.1
4.12e-2
1.81e-2
462.0
56.0
BCD+
HyBR
12.0
5.93e-2
2.03e-2
68.1
22.9
BCD+
r
h
29.7
3.95e-2
1.79e-2
89.6
55.4
2%Noise
Iter.
Rel.Err.
x
Rel.Err.
w
MatVecs.
Time(s)
LAP+
HyBR
12.0
6.38e-2
1.86e-2
77.0
12.5
LAP+
r
h
15.9
4.38e-2
1.81e-2
66.4
15.9
VarPro+
I
29.6
1.03e-1
1.86e-2
632.0
64.7
VarPro+
r
h
21.4
5.05e-2
1.85e-2
468.0
54.9
BCD+
HyBR
13.4
7.10e-2
2.15e-2
53.1
25.0
BCD+
r
h
29.9
4.55e-2
1.82e-2
91.7
57.0
3%Noise
Iter.
Rel.Err.
x
Rel.Err.
w
MatVecs.
Time(s)
LAP+
HyBR
12.3
7.54e-2
2.28e-2
68.6
16.3
LAP+
r
h
18.5
5.34e-2
2.22e-2
74.2
22.0
VarPro+
I
34.4
1.27e-1
2.32e-2
728.0
86.7
VarPro+
r
h
23.9
6.17e-2
2.13e-2
518.0
72.5
BCD+
HyBR
15.0
7.86e-2
2.32e-2
52.9
31.9
BCD+
r
h
29.7
5.42e-2
2.24e-2
83.2
63.9
Table4.1
Thistableshowsdataforthe2Dsuper-resolutionformultiplevaluesofaddedGaussiannoise.
Thecolumnsfromlefttorightgivethestoppingiteration,therelativeerrorofthesolutionimage,the
relativeerrorofthesolutionmotion,numberofmatrix-vectormultiplicationsduringoptimization,
andtimeinsecondsusing
tic
and
toc
in
MATLAB
.Allvaluesareaveragestakenfrom
10
instances
withentmotionparameters,initialguesses,andnoiserealizations.Thebestresultsforeach
columnisbold-faced.
Theimagingoperator
K
inthethree-dimensionalexampleisblockdiagonalwith128
identicaldown-samplingmatrices
K
whichrelatethehigh-resolutionimagetothe
low-resolutiondatabyblockaveraging.
Theinitialguessforthethree-dimensionalproblemisgeneratedusingthesame
strategyasinthetwo-dimensionalcase.Togenerateaguessforthemotionparam-
eters,weregisterallframesontotherstframe(thussolving127rigidregistration
problems.)Thisgivesaninitialguessforthemotionwithapproximately3%relative
error.Usingthisinitialguess,wethensolvealinearleast-squaresproblemtoobtain
LINEARIZEANDPROJECTFORINVERSEPROBLEMS
15
Fig.4.3
.
Thisgureshows3DvolumerenderingsofthereconstructedimagesusingLAPfor
the3Dsuperresolutionproblemwith2%noise.Thevolumeontheleftshowstheinitialguess,
x
initial
.OntherightisthereconstructedsolutionusingLAP,
x
LAP
withthediscretegradient
regularizer.
aninitialguessfortheimagewitharelativeerrorofaround13%.Wenotethatthis
isapoorerinitialguesswhencomparedtothetruesolution,thantheoneobtained
forthetwo-dimensionalsuper-resolutionexample.Thismayimpactthequalityof
thesolutionobtainableforexampleswithlargeamountsofnoise.
Forthisexample,wetestLAP,VarPro,andBCDusingonlythediscretegradi-
entregularizerduetoitsbetterperformanceinthe2Dexample.Weuseidentical
parameterstothe2Dproblemfortheregularizationparameter,boundconstraints
ontheimagevariables,andaccuracyoftheiterativeLSQR.Thelonein
theproblemsetupfromthe2Dcaseisinsolvingthelinearsystem(
2.5
)forVarPro.
Insteadofthe20LSQRiterationsfromthe2Dcase,werunanumber
of50iterationstoachievetherequiredaccuracyforthelargersystem.Forallthree
methods,thereducedsysteminthemotionissolvedusingCholeskyfactorizationon
thenormalequations.
TheresultingsolutionimageforLAPandtherelativeerrorplotsforallthree
methodsfortheimagesandmotionparameterscanbefoundinFigs.
4.3
and
4.4
,re-
spectively.Likethe2Dsuper-resolutionexample,LAPconvergesfastertothemotion
parametersintheearlyiterationsthanBCDandVarProandsucceedsinreaching
lowerrelativeerrorsforboththerecoveredimageandmotionparameters.Again,
VarPro'siterationsarefarmoreexpensiveintermsofmatrix-vectormultiplications
andCPUtimeasseeninTable
4.2
.BCDperformssimilarlywithLAPintermsofthe
reconstructedimage,butitdoeslesswellatrecoveringthemotionparametersand
isslightlymoreexpensiveintermsofCPUtimeduetoahighernumberoffunction
calls.
4.3.MRIMotionCorrection.
Thetestproblemisatwo-dimensional
MRImotioncorrectionproblem.ThegoalinthisMRIapplicationistoreconstructa
complex-valuedMRIimagefromitsFouriercotsthatareacquiredblock-wise
inasequenceofmeasurements.Sincethemeasurementprocesstypicallyrequiressev-
eralsecondsorminutes,insomecasestheobjectbeingimagedmovessubstantially.
MotionrenderstheFouriersamplesinconsistentand|withoutcorrection|results
16
J.L.HERRING,J.G.NAGY,ANDL.RUTHOTTO
Fig.4.4
.
Thisgureplotstherelativeerrorsforboththereconstructedimageandthemotion
parametersforthe3Dsuperresolutionfor
2
%addednoise.LAPsucceedsincapturingthecorrect
motionparametersinfeweriterationsthanVarProandBCDandinrecoveringimagesofcomparable
quality.
1%Noise
Iter.
Rel.Err.
x
Rel.Err.
w
MatVecs.
Time(s)
LAP+
r
h
14.1
6.23e-2
8.65e-4
22.1
2.30e3
VarPro+
r
h
16.4
7.21e-2
9.55e-4
1.79e3
1.23e4
BCD+
r
h
11.3
6.25e-2
1.40e-3
25.7
4.26e3
2%Noise
Iter.
Rel.Err.
x
Rel.Err.
w
MatVecs.
Time(s)
LAP+
r
h
15.3
6.35e-2
9.27e-4
21.0
2.15e3
VarPro+
r
h
18.0
7.57e-2
1.00e-3
1.95e3
1.22e4
BCD+
r
h
12.2
6.37e-2
1.59e-3
26.2
3.75e3
3%Noise
Iter.
Rel.Err.
x
Rel.Err.
w
MatVecs.
Time(s)
LAP+
r
h
16.2
6.48e-2
8.97e-4
20.3
2.94e3
VarPro+
r
h
17.2
8.08e-2
9.52e-4
1.88e3
1.07e4
BCD+
r
h
11.7
6.50e-2
1.53e-3
25.2
4.21e3
Table4.2
Thistablepresentsdataforthesolutionofthe3Dsuperresolutionformultiplevaluesofadded
Gaussiannoise.Thecolumnsfromlefttorightgivethestoppingiteration,relativeerrorofthe
solutionimage,relativeerrorofthesolutionmotion,numberofmatrix-vectormultipliesduring
optimization,andtimeinsecondsusing
tic
and
toc
in
MATLAB
.Allvaluesareaveragestakenfrom
10
separateproblemswithentmotionparameters,initialguesses,andnoiserealizations.
inartifactsandblurringinthereconstructedMRIimage.Tocorrectforthis,one
caninsteadviewthecollectedMRIdataasasetofdistinct,complex-valuedFourier
samplings,eachmeasuringsomeportionoftheFourierdomainandsubjecttosome
unknownmotionparameters.Theresultingproblemofrecoveringtheunknownmo-
tionparametersforeachFouriersamplingandcombiningthemtoobtainasingle
motion-correctedMRIimageintothecoupledimagingframeworkpresentedin
thispaper.
TheforwardmodelforthisproblemwaspresentedbyBatchelor
etal.
[
1
].Intheir
formulation,ourimagingoperator
K
isagainblockdiagonalwithdiagonalblocks
K
k
for
k
=1
;
2
;:::;N
givenby
K
k
=
A
k
F
S
:
Here,
S
isacomplex-valuedblockrectangularmatrixcontainingthegivencoilsensi-
tivitiesoftheMRImachine,
F
isblockdiagonalwitheachblockatwo-dimensional
Fouriertransform(2DFFT),and
A
k
isablockdiagonalmatrixwithrectangular
blockscontainingselectedrowsoftheidentitycorrespondingtotheFouriersampling
forthe
k
thdataobservation.Aswiththeotherexamples,theimagingoperator
K
LINEARIZEANDPROJECTFORINVERSEPROBLEMS
17
ismultipliedontherightbytheblockrectangularmatrix
T
with
T
(
y
(
w
k
))blocks
modelingthemotionparametersofeachFouriersampling.Wenotethatthecostof
matrix-vectormultiplicationsbythisimagingoperatorisdominatedbythe2DFFTs
inblock
F
,andwenotethatfor32receivercoils,thecostofasinglematrix-vector
multiplicationwillrequire32
N
2DFFTsofsize128

128where
N
isthenumberof
Fouriersamplingsinthedataset.Additionally,wenotethatthepresenceofthese
FFTmatricespreventsusfromexplicitlystoringthematrixandnecessitatespassing
itasafunctioncallforallofthemethods.ThisalsoappliestotheJacobianwith
respecttotheimage,
J
x
.However,becauseitisrelativelysmallinsize,
J
w
canstill
becomputedandstoredexplicitly.
Weusethedatasetprovidedinthe
alignedSENSE
package[
6
]tosetupanMRI
motioncorrectionwithaknowntrueimageandknownmotionparameters.Tothis
end,wegeneratenoisydatabyusingtheforwardproblem(
2.1
).Thegroundtruth
imagewithresolution128

128isrotatedandshiftedbyarandom2Drigidbody
transformation.Themotionimageisthenobservedon32sensorswithknown
sensitivities.Eachofthese32observationsisthensampledinFourierspace.Forour
problem,eachsamplingcorrespondsto1
=
16oftheFourierdomain,meaningthat
N
=16samplings(eachwithunknownmotionparameters)areneededtohaveafull
samplingofthewholespace.WesampleusingaCartesianparalleltwo-dimensional
samplingpattern[
6
].Noiseisaddedtothedatausingtheformula
d
=

d
+

where

=

k

d
k
1
k
n
k
2
n
:
Here,

d
isthenoisefreedata,

isthepercentageofnoiseadded,and
n
isacomplex-
valuedvectorwheretheentriesofRe(
n
)andIm(
n
)arenormally-distributedrandom
numberswithmean0andstandarddeviation1.Weruntheproblemfor

=5%,10%,
and15%addednoise.Theresultingdatahasdimension
128

128

32
16

16=524
;
288.
TheMRImotioncorrectionoptimizationproblemwithintheframeworkinEq.
2.2
andhas16
;
432unknownscorrespondingto16
;
384fortheimageand48forthe
motion.
Aswithpreviousexamples,wesolvetheMRImotioncorrectionproblemusing
LAP,VarPro,andBCD.Thesetupandparametersaresimilartothoseofthe2D
super-resolution.Weusethediscretegradientand
HyBR
asregularizationoptionsfor
LAPandBCD,andforVarProweregularizeusingthediscretegradientoperator
andtheidentityoperator.Forthenon-hybridregularizers,we

=0
:
01.For
theleast-squaresproblemsintheimagevariablesforLAPandBCD,wesolveusing
LSQRwithanidenticaltolerancetothesuper-resolutionexamples.ForVarPro,we
useLSQRwithatoleranceof10

8
oramaximumof100iterationstosolve(
2.5
)to
maintainaccuracyinthegradient.Choleskyfactorizationonthenormalequationsis
usedforthelower-dimensionalsolvesinthemotion.Noboundconstraintsareapplied
toanyofthemethodsforthisexamplebecauseelement-wiseboundconstraintson
therealorimaginarypartsofthecomplex-valuedimagevariableswilltheangle
(orphase)ofthesolution,whichisundesirable.
Foraninitialguessforthemotionparameters,westartwith
w
=0(corresponding
toarelativeerrorof100%).Usingthisinitialization,wesolvealinearleast-squares
problemtogetaninitialguessfortheimage.For10%addedGaussiannoiseinthe
data,theinitialguessfortheimagehasarelativeerrorofaround35%.Weshowthe
initialguessinFig.
4.5
.
LAP,VarPro,andBCDprovidefairlyaccuratereconstructionsofboththeim-
ageandmotionparametersforquitelargevaluesofnoiseusingeitherHyBRorthe
18
J.L.HERRING,J.G.NAGY,ANDL.RUTHOTTO
Fig.4.5
.
Thiseshowsasingleinverteddatasampling(left),theinitialguessfortheimage
(center),andreconstructedimage(right)fortheMRImotioncorrectionproblemwith10%noise.
ThesolutionimageshownisforLAPusingthe
HyBR
regularizer.NotethattheMRIimagesarethe
modulusofthecomplex-valuedimagesrecovered.
identityasaregularizer;seeFigs.
4.5
and
4.6
.Thisislikelyduetothefactthatthe
problemisnotseverelyill-posedandishighlyover-determined(32sensorreadingsfor
eachpointinFourierspace.)Forthisexample,thehybridregularizationapproach
forLAPandBCDproducesthebestresults,withLAPrequiringconsiderablyfewer
iterations.Weremarkthatthebestregularizationfromthisproblemfromthe
super-resolutionproblems,whichshowstheimportanceoftheythatLAP
forregularizingtheimage.ThecomparativespeedofLAPisobservableforthe
relativeerrorplotsfortheproblemwith10%noiseandfurtherevidencedinTable
4.3
forallnoiselevelsover10separaterealizationsoftheproblem.Forthegradient-based
regularizer,allthreemethodsdonotrecoverthemotionparametersaccurately.We
alsonotethatthenumberofiterationsandtheircostisanimportantconsideration
forthisproblem.Becauseofthedistanceoftheinitialguessfromthesolution,this
problemrequiresmoreiterationsthanthesuper-resolutionexamples.Additionally,
thehighnumberof2DFFTsrequiredforasinglematrix-vectormultiplicationmakes
multiplicationsbytheJacobian
J
x
and
J
w
expensive.Table
4.3
showsthatLAPout-
performsVarProandBCDforbothchoicesofregularizerbyrequiringfewer,cheaper
iterationsintermsofbothtimeandmatrix-vectormultiplications.Thedierencein
costismostdramaticwhencomparedwithVarProagainduetothelargenumberof
FFTsrequiredforasinglematrix-vectormultiplicationandthelargenumberofsuch
multiplicationsrequiredwithineachVarProfunctioncall.ForBCDandLAP,the
numberofmatrix-vectormultiplicationsissimilar,butBCDrequiresmoreiterations
forconvergence.Overall,weseethatLAPisabetterchoiceforthisproblemandthat
itprovidesbetterreconstructionsofboththeimageandmotioninfewer,cheaper
iterations.
5.SummaryandConclusion.
Weintroduceanewmethod,calledLinearize
AndProject(LAP),forsolvinglarge-scaleinverseproblemswithcoupledblocksof
variablesinaprojectedGauss{Newtonframework.Problemswiththesecharacteris-
ticsarisefrequentlyinapplications,andweexemplifyandmotivateLAPusingjoint
reconstructionproblemsinimagingthataimatestimatingimageandmotionpa-
rametersfromanumberofnoisy,indirect,andmeasurements.By
design,LAPismostattractivewhentheoptimizationproblemwithrespecttoone
blockofvariablesiscomparablyeasytosolve.LAPisveryinthesensethatit
LINEARIZEANDPROJECTFORINVERSEPROBLEMS
19
Fig.4.6
.
Thiseshowstherelativeerrorsforboththereconstructedimageandthemotion
parametersfortheMRImotioncorrectionproblemforthreelevelsofnoise.LAPwithhybridreg-
ularizationachievesbetterreconstructionsoftheimageandmotionparametersforfeweriterations
thaneitherVarProandBCD.
5%Noise
Iter.
Rel.Err.
x
Rel.Err.
w
MatVecs.
Time(s)
LAP+
HyBR
76.6
3.55e-3
1.59e-4
3.18e2
5.86e2
LAP+
r
h
76.2
3.74e-3
1.67e-4
3.62e2
4.32e2
VarPro+
I
116.0
7.93e-2
4.32e-2
2.19e4
1.20e4
VarPro+
r
h
115.9
7.92e-2
4.33e-2
2.19e4
1.52e4
BCD+
HyBR
128.6
4.35e-2
2.27e-2
4.03e2
1.01e3
BCD+
r
h
116.6
7.83e-2
4.20e-2
4.27e2
8.59e2
10%Noise
Iter.
Rel.Err.
x
Rel.Err.
w
MatVecs.
Time(s)
LAP+
HyBR
76.8
6.56e-3
3.25e-4
3.11e2
6.52e2
LAP+
r
h
76.1
6.88e-3
4.15e-4
3.55e2
5.19e2
VarPro+
I
116.0
8.08e-2
4.33e-2
2.19e4
1.12e4
VarPro+
r
h
116.0
8.08e-2
4.32e-2
2.19e4
1.27e4
BCD+
HyBR
128.2
4.53e-2
2.21e-2
3.89e2
1.24e3
BCD+
r
h
116.0
8.02e-2
4.21e-2
4.12e2
1.05e3
15%Noise
Iter.
Rel.Err.
x
Rel.Err.
w
MatVecs.
Time(s)
LAP+
HyBR
77.3
9.49e-3
4.69e-2
3.07e2
5.02e2
LAP+
r
h
75.0
1.61e-2
2.92e-2
3.40e2
3.61e2
VarPro+
I
115.8
8.29e-2
4.36e-2
2.18e4
9.98e3
VarPro+
r
h
115.7
8.28e-2
4.35e-2
2.18e4
1.27e4
BCD+
HyBR
127.0
4.80e-2
2.21e-2
3.47e2
1.06e3
BCD+
r
h
129.0
8.20e-2
4.17e-2
3.99e2
8.17e2
Table4.3
ThistableshowstheresultsofLAP,VarPro,andBCDforsolvingtheMRImotioncorrection
exampleformultipleregularizersandvaryinglevelsofaddednoise.Averagedover
10
realizationsof
theproblem,thecolumnsarestoppingiteration,therelativeerrorofthesolutionimage,therelative
errorofthesolutionmotion,numberofmatrix-vectormultipliesduringoptimization,andtimein
secondsusing
tic
and
toc
in
MATLAB
.LAPoutperformstheothermethodsintermsofsolution
quality,computationalcost,andCPUtime.
supportstregularizationstrategies,imposingequalityandinequality
constraintsonbothblocksofvariables,anddoesnotrequire(asinthecaseofVarPro)
thattheforwardproblemdependslinearlyononesetofvariables.Inournumerical
experimentsusingfourseparablenonlinearleast-squaresproblems,weshowedthat
LAPiscompetitiveandoftensuperiortoVarProandBCDwithrespecttoaccuracy
and.
LAPisasgeneralasalternatingminimizationmethodssuchasBlockCoordinate
Descent.HoweverwhileBCDignoresthecouplingbetweenthevariableblockswhen
computingupdates,LAPtakesitintoconsideration.Thus,inourexperimentsLAP
requiresaconsiderablysmallernumberofiterations,matrix-vectormultiplications,
20
J.L.HERRING,J.G.NAGY,ANDL.RUTHOTTO
andCPUtimethanBCDtoachievesimilaraccuracy.
LAPisnotlimitedtoseparablenonlinearleast-squaresproblemsandthusmore
broadlyapplicablethanVarPro.SinceLAPprojectsafterlinearization,itprovides
theopportunitytofreelychoosewhichblockofvariablesgetseliminated.Forexample,
inournumericalexamplesinSec.
4.1
{
4.3
,LAPeliminatestheparametersassociated
withthemotion(whichareofcomparablysmalldimension)whencomputingthe
searchdirectionintheprojectedGauss{Newtonscheme.Duetotherobustnessof
Gauss{Newtonmethods,ittosolvetheimagingproblemiterativelytolow
accuracy.Bycontrast,VarProeliminatestheimagevariablesthatentertheresidual
inalinearway.Whilethisleadstoasmall-dimensionalnonlinearoptimizationprob-
lemforthemotion,eachiterationrequiressolvingtheimagingproblemtorelatively
highaccuracytoobtainreliablegradientinformation;seealsoSec
2.2
.Thiscanbe
problematicforlargeandill-posedimagingproblems,andthusLAPcaninsomecases
reduceruntimesbyanorderofmagnitude;seeTables
4.2
and
4.3
.
ItisworthnotingthatthekeystepofLAP,whichistheprojectionontoone
variableblockwhensolvingtheapproximatedNewtonsystem,isablockelimination,
andthereducedsystemcorrespondstotheSchur-complement.
ToallowforcomparisonwithVarProwefocussedonseparablenonlinearleast-
squaresproblems.Infuturework,wewillstudytheperformanceofLAPtosolve
generalcouplednonlinearoptimizationproblems.
6.ACKNOWLEDGEMENT.
ThisworkissupportedbyEmory'sUniversity
ResearchCommitteeandNationalScienceFoundation(NSF)awardsDMS1522760
andDMS1522599.
REFERENCES
[1]
P.Batchelor,D.Atkinson,P.Irarrazaval,D.Hill,J.Hajnal,andD.Larkman.MatrixDescrip-
tionofGeneralMotionCorrectionAppliedtoMultishotImages.
Magneticresonancein
medicine
,54(5):1273{1280,2005.
[2]
A.Beck.
IntroductiontoNonlinearOptimization:Theory,Algorithms,andApplicationswith
MATLAB
.SIAM,2014.
[3]

A.orck.
Numericalmethodsforleastsquaresproblems
.SIAM,1996.
[4]
J.Chung,E.Haber,andJ.Nagy.Numericalmethodsforcoupledsuper-resolution.
Inverse
Problems
,22(4):1261{1272,2006.
[5]
J.Chung,J.Nagy,andD.O'Leary.AweightedGCVmethodforLanczoshybridregularization.
ElectronicTransactionsonNumericalAnalysis
,2008.
[6]
L.Cordero-Grande,R.Teixeira,E.Hughes,J.Hutter,A.Price,andJ.Hajnal.Sensitivity
encodingforalignedmultishotmagneticresonancereconstruction.
IEEETransactionson
ComputationalImaging
,2(3):266{280,2016.
[7]
E.deSturlerandM.Kilmer.ARegularizedGauss{NewtonTrustRegionApproachtoImaging
inOpticalTomography.
SIAMJournalonScComputing
,33(5):3057{3086,
2011.
[8]
H.W.Engl,M.Hanke,andA.Neubauer.
RegularizationofInverseProblems
.KluwerAcademic
Publishers,Dordrecht,2000.
[9]
R.Estrin,D.Orban,andM.Saunders.LSLQ:anIterativeMethodforLinearLeast-Squares
withanErrorMinimizationProperty.TechnicalReportG-2017-05,LesCahiersdu
GERAD,2017.
[10]
S.Farsiu,D.Robinson,M.Elad,andP.Milanfar.Advancesandchallengesinsuper-resolution.
InternationalJournalofImagingSystemsandTechnology
,14(2):47{57,2004.
[11]
J.Fohring,E.Haber,andL.Ruthotto.GeophysicalImagingofFluidFlowinPorousMedia.
SIAMJournalonComputing
,36(5):S218{S236,2014.
[12]
S.GazzolaandJ.G.Nagy.GeneralizedArnoldi{Tikhonovmethodforsparsereconstruction.
SIAMJ.Sci.Comput.
,36(2):B225{B247,2014.
[13]
S.Gazzola,J.G.Nagy,andP.C.Hansen.IRTools:AMATLABpackageofiterativeregulariza-
LINEARIZEANDPROJECTFORINVERSEPROBLEMS
21
tionmethodsandlarge-scaletestproblems.arXiv:1712.05602,toappearinNumer.Algor.,
2018.
[14]
S.GazzolaandP.Novati.AutomaticparametersettingforArnoldi{Tikhonovmethods.
J.Comp.Appl.Math.
,256(15):180{195,2014.
[15]
S.Gazzola,P.Novati,andM.R.Russo.Embeddedtechniquesforchoosingtheparameterin
Tikhonovregularization.
Num.Lin.Alg.Appl.
,21(6):796{812,2014.
[16]
T.GoldsteinandS.Osher.TheSplitBregmanMethodforL1-RegularizedProblems.
SIAM
JournalonImagingSciences
,2(2):323{343,Jan.2009.
[17]
G.GolubandV.Pereyra.Separablenonlinearleastsquares:thevariableprojectionmethod
anditsapplications.
InverseProblems
,19(2):R1,2003.
[18]
G.H.GolubandV.Pereyra.Thetiationofpseudo-inversesandnonlinearleastsquares
problemswhosevariablesseparate.
SIAMJournalonNumericalAnalysis
,10(2):413{432,
1973.
[19]
E.Haber.
ComputationalMethodsinGeophysicalElectromagnetics
.SocietyforIndustrialand
AppliedMathematics(SIAM),Philadelphia,PA,2014.
[20]
E.HaberandD.Oldenburg.AGCVbasedmethodfornonlinearill-posedproblems.
Compu-
tationalGeosciences
,4:41{63,2000.
[21]
P.C.Hansen.
Randdiscreteill-posedproblems
.SIAMMonographsonMathemat-
icalModelingandComputation.SocietyforIndustrialandAppliedMathematics(SIAM),
Philadelphia,PA,1998.
[22]
P.C.Hansen.
DiscreteInverseProblems:InsightandAlgorithms
.SIAM,Philadelphia,PA,
2010.
[23]
R.Hardie,K.Barnard,andE.Armstrong.Jointmapregistrationandhigh-resolutionim-
ageestimationusingasequenceofundersampledimages.
IEEETransactionsonImage
Processing
,6(12):1621{1633,1997.
[24]
M.HestenesandE.Stiefel.
Methodsofconjugategradientsforsolvinglinearsystems
,vol-
ume49.NBS,1952.
[25]
J.Modersitzki.
FAIR:algorithmsforimageregistration
,volume6of
Fundamentalsof
Algorithms
.SocietyforIndustrialandAppliedMathematics(SIAM),Philadelphia,PA,
2009.
[26]
J.NocedalandS.Wright.Numericaloptimization.
SpringerScience
,35:67{68,1999.
[27]
D.O'LearyandB.Rust.Variableprojectionfornonlinearleastsquaresproblems.
Computa-
tionalOptimizationandApplications.AnInternationalJournal
,54(3):579{593,2013.
[28]
C.C.PaigeandM.A.Saunders.LSQR:AnAlgorithmforSparseLinearEquationsandSparse
LeastSquares.
ACMTransactionsonMathematicalSoftware(TOMS)
,8(1):43{71,1982.
[29]
S.C.Park,M.K.Park,andM.G.Kang.Super-resolutionimagereconstruction:atechnical
overview.
IEEEsignalprocessingmagazine
,20(3):21{36,2003.
[30]
P.RodrandB.Wohlberg.Aniterativelyreweightednormalgorithmfortotalvariation
regularization.In
Proceedingsofthe40thAsilomarConferenceonSignals,Systemsand
Computers(ACSSC)
,2006.
[31]
P.RodrandB.Wohlberg.Antalgorithmforsparserepresentationswith
`
p
data
yterm.In
Proceedingsof4thIEEEAndeanTechnicalConference(ANDESCON)
,
2008.
[32]
L.I.RUDIN,S.Osher,andE.FATEMI.NonlinearTotalVariationBasedNoiseRemoval
Algorithms.
PhysicaD
,60(1-4):259{268,1992.
[33]
Y.Saad.
Iterativemethodsforsparselinearsystems
.SIAM,2003.
[34]
D.SimaandS.V.Separablenonlinearleastsquareswithlinearboundconstraints
anditsapplicationinmagneticresonancespectroscopydataquan
Journalof
computationalandappliedmathematics
,203(1):264{278,2007.
[35]
C.Vogel.
Computationalmethodsforinverseproblems
,volume23.SIAM,2002.
[36]
B.WohlbergandP.RodrAniterativelyreweightednormalgorithmforminimizationof
totalvariationfunctionals.
IEEESignalProcessingLetters
,14:948{951,2007.
"
89,Fruit recognition from images using deep learning,http://arxiv.org/pdf/1712.00580v9.pdf,https://github.com/applecrazy/FruitClassifier,"A
cta
U
niv
.S
apientiae
,I
nformatica
,
10,
1(2018)26Œ42
Fruitrecognitionfromimagesusingdeep
learning
HoreaMures¸an
FacultyofMathematicsandComputer
Science
MihailKog

alniceanu,1
Babes¸-BolyaiUniversity
Romania
email:
horea94@gmail.com
MihaiOltean
FacultyofExactSciencesand
Engineering
Unirii,15-17
ﬂ1Decembrie1918ﬂUniversityofAlba
Iulia
Romania
email:
mihai.oltean@gmail.com
Abstract.
Inthispaperweintroduceanew,high-quality,datasetofimages
containingfruits.Wealsopresenttheresultsofsomenumericalex-
perimentfortraininganeuralnetworktodetectfruits.Wediscussthe
reasonwhywechosetousefruitsinthisprojectbyproposingafew
applicationsthatcouldusesuch.
Keywords:
Deeplearning
,
Objectrecognition
,
Computervision
,
fruits
dataset
,
imageprocessing
1Introduction
Theaimofthispaperistoproposeanewdatasetofimagescontaining
popularfruits.ThedatasetwasnamedFruits-360andcanbedownloaded
fromtheaddressespointedbyreferences[
18
]and[
19
].Currently(asof
2018.12.22)thesetcontains61934imagesof90fruitsanditisconstantly
updatedwithimagesofnewfruitsassoonastheauthorshaveaccessesto
them.Thereaderisencouragedtoaccessthelatestversionofthedataset
fromtheaboveindicatedaddresses.
ComputingSystem1998:
I.2.6
MathematicsSubject2010:
68T45
Keywordsandphrases:
Deeplearning,Objectrecognition,Computervision
1
arXiv:1712.00580v9  [cs.CV]  4 Jan 20192
Havingahigh-qualitydatasetisessentialforobtainingagood.
Mostoftheexistingdatasetswithimages(seeforinstancethepopular
CIFARdataset[
12
])containboththeobjectandthenoisybackground.This
couldleadtocaseswherechangingthebackgroundwillleadtotheincorrect
oftheobject.
Asasecondobjectivewehavetrainedadeepneuralnetworkthatis
capableofidentifyingfruitsfromimages.Thisispartofamorecomplex
projectthathasthetargetofobtainingathatcanidentifyamuch
widerarrayofobjectsfromimages.Thisthecurrenttrendofcompanies
workingintheaugmentedrealityDuringitsannualI
/
Oconference,
Googleannounced[
21
]thatisworkingonanapplicationnamedGoogle
Lenswhichwilltelltheusermanyusefulinformationabouttheobject
towardwhichthephonecameraispointing.Firststepincreatingsuchap-
plicationistocorrectlyidentifytheobjects.Thesoftwarehasbeenreleased
laterin2017asafeatureofGoogleAssistantandGooglePhotosapps.
Currentlytheofobjectsisbasedonadeepneuralnetwork
[
35
].
Suchanetworkwouldhavenumerousapplicationsacrossmultipledo-
mainslikeautonomousnavigation,modelingobjects,controllingprocesses
orhuman-robotinteractions.Theareawearemostinterestediniscreating
anautonomousrobotthatcanperformmorecomplextasksthanaregular
industrialrobot.Anexampleofthisisarobotthatcanperforminspections
ontheaislesofstoresinordertoidentifyoutofplaceitemsorunderstocked
shelves.Furthermore,thisrobotcouldbeenhancedtobeabletointeract
withtheproductssothatitcansolvetheproblemsonitsown.Anotherarea
inwhichthisresearchcanprovideisautonomousfruitharvesting.
Whilethereareseveralpapersonthistopicalready,fromthebestofour
knowledge,theyfocusonfewspeciesoffruitsorvegetables.Inthispaper
weattempttocreateanetworkthatcanclassifyavarietyofspeciesoffruit,
thusmakingitusefulinmanymorescenarios.
Asthestartofthisprojectwechosethetaskofidentifyingfruitsfor
severalreasons.Ononeside,fruitshavecertaincategoriesthatarehardto
di

erentiate,likethecitrusgenus,thatcontainsorangesandgrapefruits.
Thuswewanttoseehowwellcananintelligencecompletethe
taskofclassifyingthem.Anotherreasonisthatfruitsareveryoftenfoundin
stores,sotheyserveasagoodstartingpointforthepreviouslymentioned
project.
Thepaperisstructuredasfollows:inthepartwewillshortlydiscuss
3
afewoutstandingachievementsobtainedusingdeeplearningforfruits
recognition,followedbyapresentationoftheconceptofdeeplearning.In
thesecondpartwedescribetheFruits-360dataset:howitwascreatedand
whatitcontains.Inthethirdpartwewillpresenttheframeworkusedin
thisproject-TensorFlow[
32
]andthereasonswechoseit.Followingthe
frameworkpresentation,wewilldetailthestructureoftheneuralnetwork
thatweused.Wealsodescribethetrainingandtestingdatausedaswell
astheobtainedperformance.Finally,wewillconcludewithafewplans
onhowtoimprovetheresultsofthisproject.Sourcecodeislistedinthe
Appendix.
2Relatedwork
Inthissectionwereviewseveralpreviousattemptstouseneuralnetworks
anddeeplearningforfruitsrecognition.
Amethodforrecognizingandcountingfruitsfromimagesincluttered
greenhousesispresentedin[
28
].Thetargetedplantsarepepperswith
fruitsofcomplexshapesandvaryingcolorssimilartotheplantcanopy.
Theaimoftheapplicationistolocateandcountgreenandredpepper
fruitsonlarge,densepepperplantsgrowinginagreenhouse.Thetraining
andvalidationdatausedinthispaperconsistsof28000imagesofover1000
plantsandtheirfruits.Theusedmethodtolocateandcountthepeppersis
two-step:inthestep,thefruitsarelocatedinasingleimageandina
secondstepmultipleviewsarecombinedtoincreasethedetectionrateof
thefruits.Theapproachtothepepperfruitsinasingleimageisbased
onacombinationof(1)pointsofinterest,(2)applyingacomplex
high-dimensionalfeaturedescriptorofapatcharoundthepointofinterest
and(3)usingaso-calledbag-of-wordsforclassifyingthepatch.
Paper[
25
]presentsanovelapproachfordetectingfruitsfromimages
usingdeepneuralnetworks.ForthispurposetheauthorsadaptaFaster
Region-basedconvolutionalnetwork.Theobjectiveistocreateaneural
networkthatwouldbeusedbyautonomousrobotsthatcanharvestfruits.
ThenetworkistrainedusingRGBandNIR(nearinfrared)images.The
combinationoftheRGBandNIRmodelsisdonein2separatecases:early
andlatefusion.Earlyfusionimpliesthattheinputlayerhas4channels:
3fortheRGBimageandonefortheNIRimage.Latefusionuses2inde-
pendentlytrainedmodelsthataremergedbyobtainingpredictionsfrom
bothmodelsandaveragingtheresults.Theresultisamultimodalnetwork
4
whichobtainsmuchbetterperformancethantheexistingnetworks.
Onthetopicofautonomousrobotsusedforharvesting,paper[
1
]showsa
networktrainedtorecognizefruitsinanorchard.Thisisaparticularlydif-
taskbecauseinordertooptimizeoperations,imagesthatspanmany
fruittreesmustbeused.Insuchimages,theamountoffruitscanbelarge,
inthecaseofalmondsupto1500fruitsperimage.Also,becausetheimages
aretakenoutside,thereisalotofvarianceinluminosity,fruitsize,cluster-
ingandviewpoint.Likeinpaper[
25
],thisprojectmakesuseoftheFaster
Region-basedconvolutionalnetwork,whichispresentedinadetailedview
inpaper[
24
].Relatedtotheautomaticharvestoffruits,article[
22
]presents
amethodofdetectingripestrawberriesandapplesfromorchards.The
paperalsohighlightsexistingmethodsandtheirperformance.
In[
11
]theauthorscompilealistoftheavailablestateoftheartmethods
forharvestingwiththeaidofrobots.Theyalsoanalyzethemethodand
proposewaystoimprovethem.
In[
2
]onecanseeamethodofgeneratingsyntheticimagesthatare
highlysimilartoempiricalimages.,thispaperintroducesa
methodforthegenerationoflarge-scalesemanticsegmentationdatasets
onaplant-partlevelofrealisticagriculturescenes,includingautomated
per-pixelclassanddepthlabeling.Onepurposeofsuchsyntheticdataset
wouldbetobootstraporpre-traincomputervisionmodels,whichare
tunedthereafteronasmallerempiricalimagedataset.Similarly,inpaper
[
23
]wecanseeanetworktrainedonsyntheticimagesthatcancountthe
numberoffruitsinimageswithoutactuallydetectingwheretheyareinthe
image.
Anotherpaper,[
4
],usestwobackpropagationneuralnetworkstrained
onimageswithappleﬂGalaﬂvarietytreesinordertopredicttheyieldfor
theupcomingseason.Forthistask,fourfeatureshavebeenextractedfrom
images:totalcross-sectionalareaoffruits,fruitnumber,totalcross-section
areaofsmallfruits,andcross-sectionalareaoffoliage.
Paper[
10
]presentsananalysisoffruitdetectabilityinrelationtothe
angleofthecamerawhentheimagewastaken.Basedonthisresearch,it
wasconcludedthatthefruitdetectabilitywasthehighestonfrontviews
andlookingwithazenithangleof60

upwards.
Inpapers[
27
,
37
,
15
]wecanseeanapproachtodetectingfruitsbasedon
color,shapeandtexture.Theyhighlightthedi

cultyofcorrectlyclassifying
similarfruitsofdi

erentspecies.Theyproposecombiningexistingmethods
usingthetexture,shapeandcoloroffruitstodetectregionsofinterestfrom
5
images.Similarly,in[
20
]amethodcombiningshape,sizeandcolor,texture
ofthefruitstogetherwithaknearestneighboralgorithmisusedtoincrease
theaccuracyofrecognition.
Oneofthemostrecentworks[
36
]presentsanalgorithmbasedonthe
improvedChanVeselevel-setmodel[
3
]andcombinedwiththelevel-set
ideaandM-Smode[
17
].Theproposedgoalwastoconductnight-timegreen
grapedetection.Combiningtheprincipleoftheminimumcircumscribed
rectangleoffruitandthemethodofHoughstraight-linedetection,the
pickingpointofthefruitstemwascalculated.
3Deeplearning
Intheareaofimagerecognitionandthemostsuccessfulre-
sultswereobtainedusingneuralnetworks[
6
,
30
].Thesenetworks
formthebasisformostdeeplearningmodels.
Deeplearningisaclassofmachinelearningalgorithmsthatusemulti-
plelayersthatcontainnonlinearprocessingunits[
26
].Eachlevellearnsto
transformitsinputdataintoaslightlymoreabstractandcompositerep-
resentation[
6
].Deepneuralnetworkshavemanagedtooutperformother
machinelearningalgorithms.Theyalsoachievedthesuperhumanpat-
ternrecognitionincertaindomains[
5
].Thisisfurtherreinforcedbythefact
thatdeeplearningisconsideredasanimportantsteptowardsobtaining
StrongAI.Secondly,deepneuralnetworks-convolutionalneu-
ralnetworks-havebeenprovedtoobtaingreatresultsintheofimage
recognition.
Intherestofthissectionwewilldescribesomemodelsofdeep
neuralnetworksalongwithsomeresultsforsomerelatedprob-
lems.
3.1Convolutionalneuralnetworks
Convolutionalneuralnetworks(CNN)arepartofthedeeplearningmodels.
Suchanetworkcanbecomposedofconvolutionallayers,poolinglayers,
ReLUlayers,fullyconnectedlayersandlosslayers[
34
].InatypicalCNN
architecture,eachconvolutionallayerisfollowedbyaLinearUnit
(ReLU)layer,thenaPoolinglayerthenoneormoreconvolutionallayerand
oneormorefullyconnectedlayer.Acharacteristicthatsetsapart
theCNNfromaregularneuralnetworkistakingintoaccountthestructure
6
oftheimageswhileprocessingthem.Notethataregularneuralnetwork
convertstheinputinaonedimensionalarraywhichmakesthetrained
lesssensitivetopositionalchanges.
AmongthebestresultsobtainedontheMNIST[
13
]datasetisdoneby
usingmulti-columndeepneuralnetworks.Asdescribedinpaper[
7
],they
usemultiplemapsperlayerwithmanylayersofnon-linearneurons.Even
ifthecomplexityofsuchnetworksmakesthemhardertotrain,byusing
graphicalprocessorsandspecialcodewrittenforthem.Thestructureofthe
networkuseswinner-take-allneuronswithmaxpoolingthatdeterminethe
winnerneurons.
Anotherpaper[
16
]furtherreinforcestheideathatconvolutionalnet-
workshaveobtainedbetteraccuracyinthedomainofcomputervision.In
paper[
29
]anallconvolutionalnetworkthatgainsverygoodperformance
onCIFAR-10[
12
]isdescribedindetail.Thepaperproposesthereplace-
mentofpoolingandfullyconnectedlayerswithequivalentconvolutional
ones.Thismayincreasethenumberofparametersandaddsinter-feature
dependencieshoweveritcanbemitigatedbyusingsmallerconvolutional
layerswithinthenetworkandactsasaformofregularization.
InwhatfollowswewilldescribeeachofthelayersofaCNNnetwork.
3.1.1Convolutionallayers
Convolutionallayersarenamedaftertheconvolutionoperation.Inmathe-
maticsconvolutionisanoperationontwofunctionsthatproducesathird
functionthatisthe(convoluted)versionofoneoftheoriginal
functions.Theresultingfunctiongivesinintegralofthepointwisemulti-
plicationofthetwofunctionsasafunctionoftheamountthatoneofthe
originalfunctionsistranslated[
33
].
Aconvolutionallayerconsistsofgroupsofneuronsthatmakeupkernels.
Thekernelshaveasmallsizebuttheyalwayshavethesamedepthasthe
input.Theneuronsfromakernelareconnectedtoasmallregionofthe
input,calledthereceptivebecauseitishighlyine

cienttolinkall
neuronstoallpreviousoutputsinthecaseofinputsofhighdimensions
suchasimages.Forexample,a100x100imagehas10000pixelsandifthe
layerhas100neurons,itwouldresultin1000000parameters.Instead
ofeachneuronhavingweightsforthefulldimensionoftheinput,aneuron
holdsweightsforthedimensionofthekernelinput.Thekernelsslideacross
thewidthandheightoftheinput,extracthighlevelfeaturesandproduce
a2dimensionalactivationmap.Thestrideatwhichakernelslidesisgiven
7
asaparameter.Theoutputofaconvolutionallayerismadebystackingthe
resultedactivationmapswhichinturnedisusedtotheinputofthe
nextlayer.
Applyingaconvolutionallayeroveranimageofsize32X32resultsin
anactivationmapofsize28X28.Ifweapplymoreconvolutionallayers,
thesizewillbefurtherreduced,and,asaresulttheimagesizeisdrastically
reducedwhichproduceslossofinformationandthevanishinggradient
problem.Tocorrectthis,weusepadding.Paddingincreasesthesizeofa
inputdatabyconstantsaroundinputdata.Inmostofthecases,this
constantiszerosotheoperationisnamedzeropadding.ﬂSameﬂpadding
meansthattheoutputfeaturemaphasthesamespatialdimensionsasthe
inputfeaturemap.Thistriestopadevenlyleftandright,butifthenumber
ofcolumnstobeaddedisodd,itwilladdanextracolumntotheright.
ﬂValidﬂpaddingisequivalenttonopadding.
Thestridescausesakerneltoskipoverpixelsinanimageandnotinclude
themintheoutput.Thestridesdetermineshowaconvolutionoperation
workswithakernelwhenalargerimageandmorecomplexkernelare
used.Asakernelisslidingtheinput,itisusingthestridesparameterto
determinehowmanypositionstoskip.
ReLUlayer,orLinearUnitslayer,appliestheactivationfunction
max(0,x).Itdoesnotreducethesizeofthenetwork,butitincreasesits
nonlinearproperties.
3.1.2Poolinglayers
Poolinglayersareusedononehandtoreducethespatialdimensionsof
therepresentationandtoreducetheamountofcomputationdoneinthe
network.Theotheruseofpoolinglayersistocontrolting.Themost
usedpoolinglayerhasofsize2x2withastride2.Thise

ectively
reducestheinputtoaquarterofitsoriginalsize.
3.1.3Fullyconnectedlayers
Fullyconnectedlayersarelayersfromaregularneuralnetwork.Eachneu-
ronfromafullyconnectedlayerislinkedtoeachoutputoftheprevious
layer.Theoperationsbehindaconvolutionallayerarethesameasinafully
connectedlayer.Thus,itispossibletoconvertbetweenthetwo.
8
3.1.4Losslayers
Losslayersareusedtopenalizethenetworkfordeviatingfromtheex-
pectedoutput.Thisisnormallythelastlayerofthenetwork.Variousloss
functionexist:softmaxisusedforpredictingaclassfrommultipledisjunct
classes,sigmoidcross-entropyisusedforpredictingmultipleindependent
probabilities(fromthe[0,1]interval).
3.2Recurrentneuralnetwork
Anotherdeeplearningalgorithmistherecursiveneuralnetwork[
16
].The
paperproposesanimprovementtothepopularconvolutionalnetworkin
theformofarecurrentconvolutionalnetwork.Inthiskindofarchitecture
thesamesetofweightsisrecursivelyappliedoversomedata.Traditionally,
recurrentnetworkshavebeenusedtoprocesssequentialdata,handwriting
orspeechrecognitionbeingthemostknownexamples.Byusingrecurrent
convolutionallayerswithsomemaxpoollayersinbetweenthemanda
globalmaxpoollayerattheendseveraladvantagesareobtained.Firstly,
withinalayer,everyunittakesintoaccountthestateofunitsinanincreas-
inglylargerareaaroundit.Secondly,byhavingrecurrentlayers,thedepth
ofthenetworkisincreasedwithoutaddingmoreparameters.Recurrent
networkshaveshowngoodresultsinnaturallanguageprocessing.
3.3Deepbeliefnetwork
Yetanothermodelthatispartofthedeeplearningalgorithmsisthedeepbe-
liefnetwork[
14
].Adeepbeliefnetworkisaprobabilisticmodelcomposed
bymultiplelayersofhiddenunits.Theusagesofadeepbeliefnetworkare
thesameastheotherpresentednetworksbutcanalsobeusedtopre-train
adeepneuralnetworkinordertoimprovetheinitialvaluesoftheweights.
Thisprocessisimportantbecauseitcanimprovethequalityofthenetwork
andcanreducetrainingtimes.Deepbeliefnetworkscanbecombinedwith
convolutionalonesinordertoobtainconvolutionaldeepbeliefnetworks
whichexploittheadvantageso

eredbybothtypesofarchitectures.
4Fruits-360dataset
Inthissectionwedescribehowthedatasetwascreatedandwhatitcontains.
9
Theimageswereobtainedbythefruitswhiletheyarerotatedby
amotorandthenextractingframes.
Fruitswereplantedintheshaftofalowspeedmotor(3rpm)andashort
movieof20secondswasrecorded.Behindthefruitsweplacedawhite
sheetofpaperasbackground.
Figure1:Left-side:originalimage.Noticethebackgroundandthemotor
shaft.Right-side:thefruitafterthebackgroundremovalandafteritwas
scaleddownto100x100pixels.
Howeverduetothevariationsinthelightingconditions,thebackground
wasnotuniformandwewroteadedicatedalgorithmwhichextractthe
fruitfromthebackground.Thisalgorithmisoftype:westartfrom
eachedgeoftheimageandwemarkallpixelsthere,thenwemarkall
pixelsfoundintheneighborhoodofthealreadymarkedpixelsforwhich
thedistancebetweencolorsislessthanaprescribedvalue.werepeatthe
previousstepuntilnomorepixelscanbemarked.
Allmarkedpixelsareconsideredasbeingbackground(whichisthen
withwhite)andtherestofpixelsareconsideredasbelongingtothe
object.Themaximumvalueforthedistancebetween2neighborpixelsisa
parameterofthealgorithmandisset(bytrialanderror)foreachmovie.
Fruitswerescaledtoa100x100pixelsimage.Otherdatasets(like
MNIST)use28x28images,butwefeelthatsmallsizeisdetrimentalwhen
10
youhavetoosimilarobjects(aredcherrylooksverysimilartoaredapple
insmallimages).Ourfutureplanistoworkwithevenlargerimages,but
thiswillrequiremuchmorelongertrainingtimes.
Tounderstandthecomplexityofbackground-removalprocesswehave
depictedinFigure
1
afruitwithitsoriginalbackgroundandafterthe
backgroundwasremovedandthefruitwasscaleddownto100x100
pixels.
Theresulteddatasethas61934imagesoffruitsspreadacross90labels.
ThedatasetisavailableonGitHub[
18
]andKaggle[
19
].Thelabelsandthe
numberofimagesfortrainingaregiveninTable
1
.
Table1:Numberofimagesforeachfruit.Therearemultiple
varietiesofappleseachofthembeingconsideredasasep-
arateobject.Wedidnotthe
/
popularnamefor
eachapplesowelabeledwithdigits(e.g.applered1,apple
red2etc).
Label
Numberoftrainingimages
Numberoftestimages
AppleBraeburn
492
164
AppleGolden1
492
164
AppleGolden2
492
164
AppleGolden3
481
161
AppleGrannySmith
492
164
AppleRed1
492
164
AppleRed2
492
164
AppleRed3
429
144
AppleRedDelicious
490
166
AppleRedYellow1
492
164
AppleRedYellow2
672
219
Apricot
492
164
Avocado
427
143
Avocadoripe
491
166
Banana
490
166
BananaLadyFinger
450
152
BananaRed
490
166
Cactusfruit
490
166
Cantaloupe1
492
164
Continuedonnextpage
11
Table1Œcontinuedfrompreviouspage
Label
Numberoftrainingimages
Numberoftestimages
Cantaloupe2
492
164
Carambula
490
166
Cherry1
492
164
Cherry2
738
246
CherryRainier
738
246
CherryWaxBlack
492
164
CherryWaxRed
492
164
CherryWaxYellow
492
164
Chestnut
450
153
Clementine
490
166
Cocos
490
166
Dates
490
166
Granadilla
490
166
GrapeBlue
984
328
GrapePink
492
164
GrapeWhite
490
166
GrapeWhite2
490
166
GrapeWhite3
492
164
GrapeWhite4
471
158
GrapefruitPink
490
166
GrapefruitWhite
492
164
Guava
490
166
Huckleberry
490
166
Kaki
490
166
Kiwi
466
156
Kumquats
490
166
Lemon
492
164
LemonMeyer
490
166
Limes
490
166
Lychee
490
166
Mandarine
490
166
Mango
490
166
Mangostan
300
102
Maracuja
490
166
Continuedonnextpage
12
Table1Œcontinuedfrompreviouspage
Label
Numberoftrainingimages
Numberoftestimages
MelonPieldeSapo
738
246
Mulberry
492
164
Nectarine
492
164
Orange
479
160
Papaya
492
164
PassionFruit
490
166
Peach
492
164
Peach2
738
246
PeachFlat
492
164
Pear
492
164
PearAbate
490
166
PearMonster
490
166
PearWilliams
490
166
Pepino
490
166
Physalis
492
164
PhysaliswithHusk
492
164
Pineapple
490
166
PineappleMini
493
163
PitahayaRed
490
166
Plum
447
151
Pomegranate
492
164
Quince
490
166
Rambutan
492
164
Raspberry
490
166
Redcurrant
492
164
Salak
490
162
Strawberry
492
164
StrawberryWedge
738
246
Tamarillo
490
166
Tangelo
490
166
Tomato1
738
246
Tomato2
672
225
Tomato3
738
246
Tomato4
479
160
Continuedonnextpage
13
Table1Œcontinuedfrompreviouspage
Label
Numberoftrainingimages
Numberoftestimages
TomatoCherryRed
492
164
TomatoMaroon
367
127
Walnut
735
249
14
5TensorFlowlibrary
Forthepurposeofimplementing,trainingandtestingthenetworkde-
scribedinthispaperweusedtheTensorFlowlibrary[
32
].Thisisanopen
sourceframeworkformachinelearningcreatedbyGooglefornumerical
computationusingdatagraphs.Nodesinthegraphrepresentmathe-
maticaloperations,whilethegraphedgesrepresentthemultidimensional
dataarrayscalledtensors.
ThemaincomponentsinaTensorFlowsystemaretheclient,whichuses
theSessioninterfacetocommunicatewiththemaster,andoneormore
workerprocesses,witheachworkerprocessresponsibleforarbitrating
accesstooneormorecomputationaldevices(suchasCPUcoresorGPU
cards)andforexecutinggraphnodesonthosedevicesasinstructedbythe
master.
TensorFlowo

erssomepowerfulfeaturessuchas:itallowscomputation
mappingtomultiplemachines,unlikemostothersimilarframeworks;it
hasbuiltinsupportforautomaticgradientcomputation;itcanpartially
executesubgraphsoftheentiregraphanditcanaddconstraintstodevices,
likeplacingnodesondevicesofacertaintype,ensurethattwoormore
objectsareplacedinthesamespaceetc.
TensorFlowisusedinseveralprojects,suchastheInceptionImageClas-
Model[
31
].Thisprojectintroducedastateoftheartnetworkfor
anddetectionintheImageNetLarge-ScaleVisualRecogni-
tionChallenge2014.Inthisprojecttheusageofthecomputingresources
isimprovedbyadjustingthenetworkwidthanddepthwhilekeepingthe
computationalbudgetconstant[
31
].
AnotherprojectthatemploystheTensorFlowframeworkisDeepSpeech,
developedbyMozilla.ItisanopensourceSpeech-To-Textenginebasedon
Baidu'sDeepSpeecharchitecture[
9
].Thearchitectureisastateoftheart
recognitionsystemdevelopedusingend-to-enddeeplearning.Itissimpler
thatotherarchitecturesanddoesnotneedhanddesignedcomponentsfor
backgroundnoise,reverberationorspeakervariation.
Wewillpresentthemostimportantutilizedmethodsanddatatypesfrom
TensorFlowtogetherwithashortdescriptionforeachofthem.
15
Aconvolutionallayerislikethis:
1
conv2d
(
2
input
,
3
filter
,
4
strides
,
5
padding
,
6
use_cudnn_on_gpu
=
True
,
7
data_format
='NHWC',
8
dilations
=[1,1,1,1],
9
name
=
None
10
)
Computesa2-Dconvolutiongiven4-D
input
and

tensors.Givenan
inputtensorofshape[
batch,in
height,in
width,in
channels
]andakernel
tensorofshape[

height,
width,in
channels,out
channels
],thisop
performsthefollowing:

Flattensthetoa2-Dmatrixwithshape[

height*
width
*in
channels,output
channels
].

Extractsimagepatchesfromtheinputtensortoformavirtualten-
sorofshape[
batch,out
height,out
width,
height*
width*
in
channels
].

Foreachpatch,right-multipliesthematrixandtheimagepatch
vector.
1
tf
.
nn
.
max_pool
(
2
value
,
3
ksize
,
4
strides
,
5
padding
,
6
data_format
='NHWC',
7
name
=
None
8
)
Performsthemaxpoolingoperationontheinput.The
ksize
and
strides
parameterscanbetuplesorlistsoftuplesof4elements.
Ksize
represents
thesizeofthewindowforeachdimensionoftheinputtensorand
strides
representsthestrideoftheslidingwindowforeachdimensionoftheinput
tensor.The
padding
parametercanbe`'VALID'`or`'SAME'`.
16
1
tf
.
nn
.
relu
(
2
features
,
3
name
=
None
4
)
Computestherlinearoperation-max(features,0).
Features
isa
tensor.
1
tf
.
nn
.
dropout
(
2
x
,
3
keep_prob
,
4
noise_shape
=
None
,
5
seed
=
None
,
6
name
=
None
7
)
Appliesdropoutoninput
x
withprobability
keep
prob
.Thismeansthat
foreachvaluein
x
themethodoutputsthevaluescaledby1
/
keep
prob
withprobability
keep
prob
or0.Thescalingisdoneonordertopreserve
thesumoftheelements.The
noise
shape
parameterwhichgroups
ofvaluesarekeptordroppedtogether.Forexample,avalueof[
k,1,1,n
]
forthe
noise
shape
,with
x
havingtheshape[
k,l,m,n
],meansthateachrow
andcolumnwillbekeptordroppedtogether,whilethebatchandchannel
componentswillbekeptordroppedseparately.
6Thestructureoftheneuralnetworkusedinexperi-
ments
Forthisprojectweusedaconvolutionalneuralnetwork.Aspreviously
describedthistypeofnetworkmakesuseofconvolutionallayers,pooling
layers,ReLUlayers,fullyconnectedlayersandlosslayers.InatypicalCNN
architecture,eachconvolutionallayerisfollowedbyaLinearUnit
(ReLU)layer,thenaPoolinglayerthenoneormoreconvolutionallayerand
oneormorefullyconnectedlayer.
NoteagainthatacharacteristicthatsetsaparttheCNNfromaregular
neuralnetworkistakingintoaccountthestructureoftheimageswhile
processingthem.Aregularneuralnetworkconvertstheinputinaone
dimensionalarraywhichmakesthetrainedlesssensitivetoposi-
tionalchanges.
17
TheinputthatweusedconsistsofstandardRGBimagesofsize100x100
pixels.
Theneuralnetworkthatweusedinthisprojecthasthestructuregiven
inTable
2
.
Table2:Thestructureoftheneuralnetworkusedinthis
paper.
Layertype
Dimensions
Output
Convolutional
5x5x4
16
Maxpooling
2x2ŠStride:2
-
Convolutional
5x5x16
32
Maxpooling
2x2ŠStride:2
-
Convolutional
5x5x32
64
Maxpooling
2x2ŠStride:2
-
Convolutional
5x5x64
128
Maxpooling
2x2ŠStride:2
-
Fullyconnected
5x5x128
1024
Fullyconnected
1024
256
Softmax
256
60
AvisualrepresentationoftheneuralnetworkusedisgiveninFigure
2
.

Thelayer(Convolution#1)isaconvolutionallayerwhichapplies
165x5Onthislayerweapplymaxpoolingwithaof
shape2x2withstride2whichthatthepooledregionsdo
notoverlap(Max-Pool#1).Thisalsoreducesthewidthandheightto
50pixelseach.

Thesecondconvolutional(Convolution#2)layerapplies325x5
whichoutputs32activationmaps.Weapplyonthislayerthe
samekindofmaxpooling(Max-Pool#2)asonthelayer,shape2
x2andstride2.

Thethirdconvolutional(Convolution#3)layerapplies645x5
Followingisanothermaxpoollayer(Max-Pool#3)ofshape2x2and
stride2.

Thefourthconvolutional(Convolution#4)layerapplies1285x5
afterwhichweapplyamaxpoollayer(Max-Pool#4).
18
Figure2:Graphicalrepresentationoftheconvolutionalneuralnetworkusedinexperiments.
19

Becauseofthefourmaxpoolinglayers,thedimensionsoftherepre-
sentationhaveeachbeenreducedbyafactorof16,thereforethe
layer,whichisafullyconnectedlayer(FullyConnected#1),has7x7
x16inputs.

Thislayerfeedsintoanotherfullyconnectedlayer(FullyConnected
#2)with1024inputsand256outputs.

Thelastlayerisasoftmaxlosslayer(Softmax)with256inputs.The
numberofoutputsisequaltothenumberofclasses.
Wepresentashortschemecontainingtheofthethetrainingprocess:
1
iterations
=75000
2
3
read_images
(
images
)
4
apply_random_hue_saturation_changes
(
images
)
5
apply_random_vertical_horizontal_flips
(
images
)
6
convert_to_hsv
(
images
)
7
add_grayscale_layer
(
images
)
8
9
define_network_structure
(
images
,
network
,
10
training_operation
)
11
12
for
i
inrange
(1,
iterations
):
13
sess
.
run
(
training_operation
)
7Numericalexperiments
Thedatasetwassplitin2parts:trainingset-whichconsistsof46371images
offruitsandtestingset-whichismadeof15563images.
ThedatawasbundledintoaTFRecordstoTensorFlow).This
isabinarythatcontainsprotocolbu

erswithafeaturemap.Inthismap
itispossibletostoreinformationsuchastheimageheight,width,depth
andeventherawimage.Usingthesewecancreatequeuesinorderto
feedthedatatotheneuralnetwork.
Bycallingthemethod
shuffle
batch
weproviderandomizedinputtothe
network.Thewayweusedthismethodwasprovidingitexampletensors
forimagesandlabelsanditreturnedtensorsofshapebatchsizeximage
20
dimensionsandbatchsizexlabels.Thishelpsgreatlylowerthechanceof
usingthesamebatchmultipletimesfortraining,whichinturnimproves
thequalityofthenetwork.
Weranmultiplescenariosinwhichtheneuralnetworkwastrainedusing
di

erentlevelsofdataaugmentationandpreprocessing:

converttheinputRGBimagestograyscale

keeptheinputimagesintheRGBcolorspace

converttheinputRGBimagestotheHSVcolorspace

converttheinputRGBimagestotheHSVcolorspaceandtograyscale
andmergethem

applyrandomhueandsaturationchangesontheinputRGBimages,
randomlythemhorizontallyandvertically,thenconvertthemto
theHSVcolorspaceandtograyscaleandmergethem
Foreachscenarioweusedthepreviouslydescribedneuralnetwork
whichwastrainedover75000iterationswithbatchesof60imagesselected
atrandomfromthetrainingset.Every50stepswecalculatedtheaccuracy
usingcross-validation.Fortestingweranthetrainednetworkonthetest
set.TheresultsforeachcasearepresentedinTable
3
.
Table3:Resultsoftrainingtheneuralnetworkonthefruits-
360dataset.
Scenario
Accuracyon
trainingset
Accuracyon
testset
Grayscale
99.96%
94.24%
RGB
99.23%
93.47%
HSV
99.98%
97.01%
HSV
+
Grayscale
99.78%
95.71%
HSV
+
Grayscale
+
hue
/
saturation
change
+

99.86%
97.04%
AsrinTable
3
thebestresultswereobtainedbyapplyingdata
augmentationandconvertingtheRGBimagestotheHSVcolorspaceto
whichthegrayscalerepresentationwasadded.Thisisintuitivesincein
thisscenarioweattachthemostamountofinformationtotheinput,thus
21
thenetworkcanlearnmultiplefeaturesinordertoclassifytheimages.
Itisalsoimportanttonoticethattrainingthegrayscaleimagesonly
yieldedthebestresultsonthetrainsetbutveryweakresultsonthetestset.
Weinvestigatedthisproblemandwehavediscoveredthatalotofimages
containingapplesareincorrectlyonthetestset.Inordertofurther
investigatetheissueweranaroundoftrainingandtestingonjusttheapple
classesofimages.Theresultsweresimilar,withhighaccuracyonthetrain
data,butlowaccuracyonthetestdata.Weattributethisto
becausethegrayscaleimageslosetoomanyfeatures,thenetworkdoesnot
learnproperlyhowtoclassifytheimages.
Inordertodeterminethebestnetworkforclassifyingthe
imagesinoutdataset,wetookmultipleusedthetrainsetto
trainthemandthencalculatedtheiraccuracyonthetestandtrainingset.
InTable
4
wepresenttheresults.
Table4:Resultsoftrainingdi

erentnetwork
onthefruits-360dataset.
Nr.

Accuracyon
trainingset
Accuracy
ontestset
1
Convolutional
5x5
16
99.86%
97.04%
Convolutional
5x5
32
Convolutional
5x5
64
Convolutional
5x5
128
Fullyconnected
-
1024
Fullyconnected
-
256
2
Convolutional
5x5
8
99.78%
95.84%
Convolutional
5x5
32
Convolutional
5x5
64
Convolutional
5x5
128
Fullyconnected
-
1024
Fullyconnected
-
256
3
Convolutional
5x5
32
99.95%
95.43%
Convolutional
5x5
32
Convolutional
5x5
64
Convolutional
5x5
128
Fullyconnected
-
1024
Fullyconnected
-
256
22
Table4:Resultsoftrainingdi

erentnetwork
onthefruits-360dataset.
Nr.

Accuracyon
trainingset
Accuracy
ontestset
4
Convolutional
5x5
16
99.86%
96.37%
Convolutional
5x5
16
Convolutional
5x5
64
Convolutional
5x5
128
Fullyconnected
-
1024
Fullyconnected
-
256
5
Convolutional
5x5
16
99.95%
96.60%
Convolutional
5x5
64
Convolutional
5x5
64
Convolutional
5x5
128
Fullyconnected
-
1024
Fullyconnected
-
256
6
Convolutional
5x5
16
99.88%
96.19%
Convolutional
5x5
32
Convolutional
5x5
32
Convolutional
5x5
128
Fullyconnected
-
1024
Fullyconnected
-
256
7
Convolutional
5x5
16
99.84%
95.50%
Convolutional
5x5
32
Convolutional
5x5
128
Convolutional
5x5
128
Fullyconnected
-
1024
Fullyconnected
-
256
8
Convolutional
5x5
16
99.24%
94.53%
Convolutional
5x5
32
Convolutional
5x5
64
Convolutional
5x5
64
Fullyconnected
-
1024
Fullyconnected
-
256
23
Table4:Resultsoftrainingdi

erentnetwork
onthefruits-360dataset.
Nr.

Accuracyon
trainingset
Accuracy
ontestset
9
Convolutional
5x5
16
99.93%
97.00%
Convolutional
5x5
32
Convolutional
5x5
64
Convolutional
5x5
128
Fullyconnected
-
512
Fullyconnected
-
256
10
Convolutional
5x5
16
99.82%
96.37%
Convolutional
5x5
32
Convolutional
5x5
64
Convolutional
5x5
128
Fullyconnected
-
1024
Fullyconnected
-
512
FromTable
4
wecanseethatthebestperformanceonthetestsetwas
obtainedbynr.1,however,thisdidnotobtain
thebestaccuracyonthetrainingset.5and3obtainedthe
bestaccuracyonthetrainset(99.95%),buttheperformanceonthetestset
laggedbehindabit.For3thisphenomenonismuchmore
pronounced,asthedi

erencebetweentrainandtestaccuraciesis4.52%.
Similarly,8hasabigdi

erenceinperformancebetweenthe
trainandtestsets.Thisisaresultofthemodeltothetrain-
ingdataandnotproperlygeneralizingtootherimages.Thesecondbest
performanceonthetrainsetwasobtainedby9,whichalso
obtainedthesecondbestaccuracyonthetestset.
24
TheevolutionofaccuracyduringtrainingisgiveninFigure
3
.Itcanbe
seenthatthetrainingrapidlyimprovesinthe1000iterations(accuracy
becomesgreaterthan90%)andthenitisveryslowlyimprovedinthenext
74000iterations.
SomeoftheincorrectlyimagesaregiveninTable
5
.
Table5:Someoftheimagesthatwereincorrectly.
Onthetopwehavethecorrectclassofthefruitandonthe
bottomwehavetheclass(anditsassociatedprobability)that
wasassignedbythenetwork.
AppleGolden2
AppleGolden3
Braeburn(Apple)
Peach
AppleGolden3
GrannySmith
(Apple)
AppleRed2
AppleRed
Yellow
96.54%
95.22%
97.71%
97.85%
Pomegranate
Peach
Pear
Pomegranate
Nectarine
AppleRed1
AppleGolden2
Braeburn(Apple)
94.64%
97.87%
98.73%
97.21%
8Conclusionsandfurtherwork
Wedescribedanewandcomplexdatabaseofimageswithfruits.Alsowe
madesomenumericalexperimentsbyusingTensorFlowlibraryinorderto
classifytheimagesaccordingtotheircontent.
Fromourpointofviewoneofthemainobjectivesforthefutureisto
improvetheaccuracyoftheneuralnetwork.Thisinvolvesfurtherexperi-
mentingwiththestructureofthenetwork.Varioustweaksandchangesto
25
Figure3:Accuracyevolutionover75000trainingiterations
26
anylayersaswellastheintroductionofnewlayerscanprovidecompletely
di

erentresults.Anotheroptionistoreplacealllayerswithconvolutional
layers.Thishasbeenshowntoprovidesomeimprovementoverthenet-
worksthathavefullyconnectedlayersintheirstructure.Aconsequenceof
replacingalllayerswithconvolutionalonesisthattherewillbeanincrease
inthenumberofparametersforthenetwork[
29
].Anotherpossibilityisto
replacetherlinearunitswithexponentiallinearunits.According
topaper[
8
],thisreducescomputationalcomplexityandadd
bettergeneralizationperformancethanrlinearunitsonnetworks
withmorethat5layers.Wewouldliketotryoutthesepracticesandalso
totrytonewthatprovideinterestingresults.
Inthenearfutureweplantocreateamobileapplicationwhichtakes
picturesoffruitsandlabelsthemaccordingly.
Anotherobjectiveistoexpandthedatasettoincludemorefruits.Thisis
amoretimeconsumingprocesssincewewanttoincludeitemsthatwere
notusedinmostothersrelatedpapers.
Acknowledgments
Apreliminaryversionofthisdatasetwith25fruitswaspresentedduring
theStudentsCommunicationSessionfromBabes¸-BolyaiUniversity,June
2017.
27
Appendix
Inthissectionwepresentthesourcecodeandprojectstructureusedin
thenumericalexperimentdescribedinthispaper.Thesourcecodecanbe
downloadedfromGitHub[
18
].
Thesourcecodeisorganized(onGitHub[
18
])asfollows:
root
directory
fruit
detection
detect
fruits.py
network
fruit
test
net.py
fruit
train
net.py
network
structure
fruit
network.py
utils.py
utils
build
image
data.py
constants.py
freeze
graph.py
labels
Inordertoruntheprojectfromthecommandline,makesurethe
PYTHONPATHsystemvariablecontainsthepathtotheroot
directory.
Ensurethatthe
utils
/
constants.py
containstheproperpaths.
Runthe
utils
/
build
image
data.py
togeneratethetfrecordwith
trainingandtestdata.ThisscriptisprovidedinthetensorFlowlibrary.The
containsseveraldefaultvaluesforthes.Theycanbechangedinthe
codedirectlyordi

erentvaluescanbeprovidedfromthecommandline:
pythonutils
/
build
image
data.py
wherecanbe:

Œtrain
directory
=
pathtothefoldercontainingthetrainimages

Œvalidation
directory
=
pathtothefoldercontainingthevalidation
images

Œoutput
directory
=
pathtowheretooutputthetfrecord

Œlabels

=
pathtothelabel

Œtrain
shards,Œtest
shardsdeterminethenumberoftfrecordfor
traindataandtestdata
28

Œnum
threadsisthenumberofthreadstocreatewhencreatingthe
tfrecord
Afterthetrainandtestdatahasbeenserialized,thetrainandtestscripts
canberun:
pythonnetwork
/
fruit
train
net.py
pythonnetwork
/
fruit
test
net.py
Afterthetraininghascompleted,thepythonutils
/
build
image
data.py
scriptcanberun:
pythonutils
/
build
image
data.pyŒimage
path
=
ﬂpathtoajpeg
Finally,the
utils
/
freeze
graph.py
script,whichisalsoprovidedasautil-
ityscriptintensorFlow,createsasinglewiththetrainedmodeldata.
pythonfreeze
graph
Thesearemandatory:

Œinput
graph
=
pathtothepbtxt

Œinput
checkpoint
=
pathtotheckpt

Œoutput
graph
=
nameoftheoutput

Œoutput
node
names
=
nameofthelastlayerofthenetwork(foundin
thenetwork
structure
/
fruit
network.pyintheconv
netmethod,
inthiscasethenameofthelastlayerisﬂsoftmaxﬂ)
Inthefollowing,wewillprovideexplanationsforthecode.Wewill
beginwiththeofthegeneralparametersandof
theproject.
Thefollowingareinthe
utils
/
constants.py


root
dir
-thetoplevelfolderoftheproject

data
dir
-thefolderwherethe.tfrecordsarepersisted

fruit
models
dir
-thefolderwherethenetworkstructureandparame-
tersaresaved

labels

-thepathtothethatcontainsallthelabelsused

training
images
dir,test
images
dir
-pathstothefolderscontainingthe
trainingandtestimages

number
train
images,number
test
images
-numberoftrainingandtest
images;usedinthetestmethodtocalculateaccuracy
29
Allthesecanbechangedtosuitthesetupofanyoneusing
thecode.
1
utils
/
constants
.
py
2
import
os
3
4
#
needs
to
be
changed
according
to
the
location
of
the
project
5
root_dir
='C:\\root_directory\\'
6
data_dir
=
root_dir
+'\\data\\'
7
fruit_models_dir
=
root_dir
+'\\fruit_models\\'
8
labels_file
=
root_dir
+'\\utils\\labels'
9
10
#
change
this
to
the
path
of
the
folders
that
hold
the
images
11
training_images_dir
='\\Fruit-Images-Dataset\\
Training'
12
test_images_dir
='\\Fruit-Images-Dataset\\Test'
13
14
#
number
of
classes
:
number
of
fruit
classes
+
1
resulted
due
to
the
build_image_data
.
py
script
that
leaves
the
first
class
as
a
background
class
15
#
using
the
labels
file
that
is
also
used
in
the
build_image_data
.
py
16
with
open
(
labels_file
)
as
f
:
17
labels
=
f
.
readlines
()
18
num_classes
=
len
(
labels
)+1
19
number_train_images
=46371
20
number_test_images
=15563
Inthe
network
structure
/
utils.py
wehavehelpermethodsused
acrosstheproject:

conv2d
combinestheTensorFlowmethodofaconvolutional
layer,addingthebiastotheconvolutionallayerandaddingr
linearunits
Œ
aconvolutionallayerconsistsofgroupsofneuronsthatmake
upkernels
Œ
thekernelshaveasmallsizebuttheyalwayshavethesame
depthastheinput
30
Œ
theneuronsfromakernelareconnectedtoasmallregionofthe
input,calledthereceptivebecauseitishighlyine

cientto
linkallneuronstoallpreviousoutputsinthecaseofinputsof
highdimensionssuchasimages

maxpool2d,norml,get
variable,
int64
featureand
bytes
feature
simplify
thecallstothecorrespondingTensorFlowmethods

read

readsmultipletfrecordesandextractstheimagedataand
label.
Herewealsomethodstoperformdataaugmentationontheinput
images.Dataaugmentationisagoodwaytoreduceonmodels.

adjust
image
for
train
appliesthefollowingoperationsonthetrain
images
1.
Altersthehueoftheimage
2.
Altersthesaturationoftheimage
3.
Flipstheimagehorizontally
4.
Flipstheimagevertically
5.
ConvertstheimagetoHSVcolorspace
6.
Createsagrayscaleversionoftheimageandaddsitasafourth
channeltotheHSVimageByalteringthehueandsaturation,
wesimulatehavingalargervarietyoffruitsintheimages.The
valueswithwhichwealterthesepropertiesaresmall,sincein
naturethereisasmallcolorvariancebetweendi

erentfruitsof
thesamespecies.
Flippingtheimagehorizontallyandverticallyhelpspreventtheusethe
orientationofthefruitasafeaturewhentraining.Thisshouldresultin
fruitsbeingcorrectlyregardlessoftheirpositioninanimage.

adjust
image
for
test
convertstheinputimagetoHSVandaddsthe
fourthgrayscalechannel
1
network_structure
/
utils
.
py
2
3
import
tensorflow
as
tf
31
4
5
6
#
perform
data
augmentation
on
images
7
#
add
random
hue
and
saturation
8
#
randomly
flip
the
image
vertically
and
horizontally
9
#
converts
the
image
from
RGB
to
HSV
and
10
#
adds
a
4
th
channel
to
the
HSV
ones
that
contains
the
image
in
gray
scale
11
def
adjust_image_for_train
(
image
):
12
image
=
tf
.
image
.
convert_image_dtype
(
image
,
tf
.
float32
)
13
image
=
tf
.
image
.
random_hue
(
image
,0.02)
14
image
=
tf
.
image
.
random_saturation
(
image
,0.9,
1.2)
15
image
=
tf
.
image
.
random_flip_left_right
(
image
)
16
image
=
tf
.
image
.
random_flip_up_down
(
image
)
17
hsv_image
=
tf
.
image
.
rgb_to_hsv
(
image
)
18
gray_image
=
tf
.
image
.
rgb_to_grayscale
(
image
)
19
rez
=
tf
.
concat
([
hsv_image
,
gray_image
],2)
20
return
rez
21
22
23
#
for
test
just
convert
the
image
to
HSV
and
add
the
gray
scale
channel
24
def
adjust_image_for_test
(
image
):
25
image
=
tf
.
image
.
convert_image_dtype
(
image
,
tf
.
float32
)
26
gray_image
=
tf
.
image
.
rgb_to_grayscale
(
image
)
27
image
=
tf
.
image
.
rgb_to_hsv
(
image
)
28
rez
=
tf
.
concat
([
image
,
gray_image
],2)
29
return
rez
30
31
32
def
_int64_feature
(
value
):
33
ifnotisinstance
(
value
,
list
):
34
value
=[
value
]
35
return
tf
.
train
.
Feature
(
int64_list
=
tf
.
train
.
Int64List
(
value
=
value
))
32
36
37
38
def
_bytes_feature
(
value
):
39
return
tf
.
train
.
Feature
(
bytes_list
=
tf
.
train
.
BytesList
(
value
=[
value
]))
40
41
42
#
read
an
image
tensor
from
tfrecord
files
43
def
read_files
(
filenames
):
44
file_queue
=
tf
.
train
.
string_input_producer
(
filenames
)
45
reader
=
tf
.
TFRecordReader
()
46
_
,
serialized_example
=
reader
.
read
(
file_queue
)
47
features
=
tf
.
parse_single_example
(
48
serialized_example
,
49
features
={
50
'image_raw':
tf
.
FixedLenFeature
([],
tf
.
string
),
51
'label':
tf
.
FixedLenFeature
([],
tf
.
int64
),
52
'height':
tf
.
FixedLenFeature
([],
tf
.
int64
)
,
53
'width':
tf
.
FixedLenFeature
([],
tf
.
int64
)
54
}
55
)
56
image
=
tf
.
image
.
decode_jpeg
(
features
['image_raw'
],
channels
=3)
57
image
=
tf
.
reshape
(
image
,[100,100,3])
58
label
=
tf
.
cast
(
features
['label'],
tf
.
int32
)
59
return
image
,
label
60
61
62
def
get_variable
(
name
,
shape
,
initializer
):
63
return
tf
.
get_variable
(
name
,
shape
,
initializer
=
initializer
,
dtype
=
tf
.
float32
)
64
65
66
def
conv2d
(
op_name
,
x
,
W
,
b
,
strides
=1):
67
x
=
tf
.
nn
.
conv2d
(
x
,
W
,
strides
=[1,
strides
,
33
strides
,1],
padding
='SAME',
name
=
op_name
)
68
x
=
tf
.
nn
.
bias_add
(
x
,
b
)
69
return
tf
.
nn
.
relu
(
x
)
70
71
72
def
maxpool2d
(
op_name
,
x
,
k
=2):
73
return
tf
.
nn
.
max_pool
(
x
,
ksize
=[1,
k
,
k
,1],
strides
=[1,
k
,
k
,1],
padding
='SAME',
name
=
op_name
)
74
75
76
def
norml
(
op_name
,
x
):
77
return
tf
.
nn
.
lrn
(
x
,4,
bias
=1.0,
alpha
=0.001/
9.0,
beta
=0.75,
name
=
op_name
)
Following,inthe
network
structure
/
fruit
network.py
wehavenet-
workparametersandthemethodthatthenetworkstructure.

IMAGE
HEIGHT,IMAGE
WIDTH,IMAGE
CHANNELS
-theimage
height,widthanddepthrespectively;

NETWORK
DEPTH
-thedepthoftheinputforthenetwork(3from
HSVimage
+
1fromgrayscaleimage)

number
of
act
maps
conv
#
-thenumberofactivationmapsforeach
convolutionallayer

number
of
fcl
outputs
#
-thenumberofunitsforeachfullyconnected
layer

num
classes
-thenumberofdi

erentclassesused
Œ
itisdeterminedbycountingthenumberoffoldersintheTraining
folder

batch
size
-thenumberofimagesselectedineachtraining
/
testingstep

dropout
-theprobabilitytokeepanodeineachtrainingstep
Œ
duringtraining,ateachiteration,somenodesareignoredwith
probability1

dropout
34
Œ
thisresultsinareducednetwork,whichisthenusedforafor-
wardorbackwardpass
Œ
dropoutpreventsneuronsfromdevelopingco-dependencyand,
inturn,
Œ
outsideoftraining,thedropoutisignoredandtheentirenetwork
isusedforclassifying

update
learning
rate
dynamicallyadjuststhelearningrateastraining
progresses

weights,biases
-arevariablesthatholdtheforthelayers
used
Œ
Thelayer(Convolution#1)isaconvolutionallayerwhich
applies165x5Onthislayerweapplymaxpooling
withaofshape2x2withstride2whichthatthe
pooledregionsdonotoverlap(Max-Pool#1).Thisalsoreduces
thewidthandheightto50pixelseach.
Œ
Thesecondconvolutional(Convolution#2)layerapplies325x5
whichoutputs32activationmaps.Weapplyonthislayer
thesamekindofmaxpooling(Max-Pool#2)asonthelayer,
shape2x2andstride2.
Œ
Thethirdconvolutional(Convolution#3)layerapplies645x
5Followingisanothermaxpoollayer(Max-Pool#3)of
shape2x2andstride2.
Œ
Thefourthconvolutional(Convolution#4)layerapplies1285x
5afterwhichweapplyamaxpoollayer(Max-Pool
#4).
Œ
Becauseofthefourmaxpoolinglayers,thedimensionsofthe
representationhaveeachbeenreducedbyafactorof16,therefore
thelayer,whichisafullyconnectedlayer(FullyConnected
#1),has7x7x16inputs.
Œ
Thislayerfeedsintoanotherfullyconnectedlayer(FullyCon-
nected#2)with1024inputsand256outputs.
Œ
Thelastlayerisasoftmaxlosslayer(Softmax)with256inputs.
Thenumberofoutputsisequaltothenumberofclasses.
35

new
height,new
width
-theheightandwidthaftertheapplyingallthe
convolutionalandmaxpoollayers;theformulausedworksbecause
allthemaxpoollayershave2x2withstride2;shouldthe
change,theformulausedmustbechanged;
1
network_structure
/
fruit_network
.
py
2
3
import
tensorflow
as
tf
4
import
math
5
from
.
import
utils
6
from
utils
import
constants
7
8
HEIGHT
=100
9
WIDTH
=100
10
#
number
of
channels
for
an
image
-
jpeg
image
has
RGB
channels
11
CHANNELS
=3
12
#
number
of
channels
for
the
input
layer
of
the
network
:
HSV
+
gray
scale
13
NETWORK_DEPTH
=4
14
15
batch_size
=60
16
input_size
=
HEIGHT
*
WIDTH
*
NETWORK_DEPTH
17
#
number
of
max
pool
operations
used
in
the
network
structure
;
18
#
used
when
calculating
the
input
size
for
the
first
fully
connected
layer
19
#
MUST
BE
UPDATED
if
the
number
of
max
pool
operations
changes
or
if
the
stride
of
max
pool
changes
20
number_of_max_pools
=4
21
#
this
works
because
each
max
pool
layer
has
a
2
x
2
filter
and
stride
2
22
#
in
case
this
changes
,
the
formula
will
no
longer
be
accurate
23
new_width
=
math
.
ceil
(
WIDTH
/(1<<
number_of_max_pools
)
)
24
new_height
=
math
.
ceil
(
HEIGHT
/(1<<
number_of_max_pools
))
36
25
#
probability
to
keep
the
values
after
a
training
iteration
26
dropout
=0.8
27
28
#
placeholder
for
input
layer
29
X
=
tf
.
placeholder
(
tf
.
float32
,[
None
,
input_size
],
name
=""X"")
30
#
placeholder
for
actual
labels
31
Y
=
tf
.
placeholder
(
tf
.
int64
,[
batch_size
],
name
=""Y"")
32
33
#
number
of
activation
maps
for
each
convolutional
layer
34
number_of_act_maps_conv1
=16
35
number_of_act_maps_conv2
=32
36
number_of_act_maps_conv3
=64
37
number_of_act_maps_conv4
=128
38
39
#
number
of
outputs
for
each
fully
connected
layer
40
number_of_fcl_outputs1
=1024
41
number_of_fcl_outputs2
=256
42
43
initial_learning_rate
=0.001
44
final_learning_rate
=0.00001
45
learning_rate
=
initial_learning_rate
46
47
48
def
conv_net
(
X
,
weights
,
biases
,
dropout
):
49
X
=
tf
.
reshape
(
X
,
shape
=[-1,
HEIGHT
,
WIDTH
,
NETWORK_DEPTH
])
50
51
conv1
=
utils
.
conv2d
('conv1',
X
,
weights
['
conv_weight1'],
biases
['conv_bias1'])
52
conv1
=
utils
.
maxpool2d
('max_pool1',
conv1
,
k
=2)
53
54
conv2
=
utils
.
conv2d
('conv2',
conv1
,
weights
['
conv_weight2'],
biases
['conv_bias2'])
55
conv2
=
utils
.
maxpool2d
('max_pool2',
conv2
,
k
=2)
56
37
57
conv3
=
utils
.
conv2d
('conv3',
conv2
,
weights
['
conv_weight3'],
biases
['conv_bias3'])
58
conv3
=
utils
.
maxpool2d
('max_pool3',
conv3
,
k
=2)
59
60
conv4
=
utils
.
conv2d
('conv4',
conv3
,
weights
['
conv_weight4'],
biases
['conv_bias4'])
61
conv4
=
utils
.
maxpool2d
('max_pool4',
conv4
,
k
=2)
62
63
fc1
=
tf
.
reshape
(
conv4
,
shape
=[-1,
weights
['
fcl_weight1'].
get_shape
().
as_list
()[0]])
64
fc1
=
tf
.
nn
.
relu
(
tf
.
add
(
tf
.
matmul
(
fc1
,
weights
['
fcl_weight1']),
biases
['fcl_bias1']))
65
fc1
=
tf
.
nn
.
dropout
(
fc1
,
dropout
)
66
67
fc2
=
tf
.
nn
.
relu
(
tf
.
add
(
tf
.
matmul
(
fc1
,
weights
['
fcl_weight2']),
biases
['fcl_bias2']))
68
fc2
=
tf
.
nn
.
dropout
(
fc2
,
dropout
)
69
70
out
=
tf
.
add
(
tf
.
matmul
(
fc2
,
weights
['out_weight'])
,
biases
['out_bias'],
name
='softmax')
71
return
out
72
73
74
def
update_learning_rate
(
acc
,
learn_rate
):
75
returnmax
(
learn_rate
-
acc
*
learn_rate
*0.9,
final_learning_rate
)
76
77
78
weights
={
79
'conv_weight1':
utils
.
get_variable
('conv_weight1',
[5,5,
NETWORK_DEPTH
,
number_of_act_maps_conv1
],
tf
.
truncated_normal_initializer
(
stddev
=5
e
-2,
dtype
=
tf
.
float32
)),
80
81
'conv_weight2':
utils
.
get_variable
('conv_weight2',
[5,5,
number_of_act_maps_conv1
,
number_of_act_maps_conv2
],
tf
.
truncated_normal_initializer
(
stddev
=5
e
-2,
dtype
38
=
tf
.
float32
)),
82
83
'conv_weight3':
utils
.
get_variable
('conv_weight3',
[5,5,
number_of_act_maps_conv2
,
number_of_act_maps_conv3
],
tf
.
truncated_normal_initializer
(
stddev
=5
e
-2,
dtype
=
tf
.
float32
)),
84
85
'conv_weight4':
utils
.
get_variable
('conv_weight4',
[5,5,
number_of_act_maps_conv3
,
number_of_act_maps_conv4
],
tf
.
truncated_normal_initializer
(
stddev
=5
e
-2,
dtype
=
tf
.
float32
)),
86
87
'fcl_weight1':
utils
.
get_variable
('fcl_weight1',[
new_width
*
new_height
*
number_of_act_maps_conv4
,
number_of_fcl_outputs1
],
tf
.
truncated_normal_initializer
(
stddev
=5
e
-2,
dtype
=
tf
.
float32
)),
88
89
'fcl_weight2':
utils
.
get_variable
('fcl_weight2',[
number_of_fcl_outputs1
,
number_of_fcl_outputs2
],
tf
.
truncated_normal_initializer
(
stddev
=5
e
-2,
dtype
=
tf
.
float32
)),
90
'out_weight':
utils
.
get_variable
('out_weight',[
number_of_fcl_outputs2
,
constants
.
num_classes
],
tf
.
truncated_normal_initializer
(
stddev
=5
e
-2,
dtype
=
tf
.
float32
)),
91
}
92
biases
={
93
'conv_bias1':
tf
.
Variable
(
tf
.
zeros
([
number_of_act_maps_conv1
])),
94
'conv_bias2':
tf
.
Variable
(
tf
.
zeros
([
number_of_act_maps_conv2
])),
95
'conv_bias3':
tf
.
Variable
(
tf
.
zeros
([
number_of_act_maps_conv3
])),
96
'conv_bias4':
tf
.
Variable
(
tf
.
zeros
([
39
number_of_act_maps_conv4
])),
97
'fcl_bias1':
tf
.
Variable
(
tf
.
zeros
([
number_of_fcl_outputs1
])),
98
'fcl_bias2':
tf
.
Variable
(
tf
.
zeros
([
number_of_fcl_outputs2
])),
99
'out_bias':
tf
.
Variable
(
tf
.
zeros
([
constants
.
num_classes
]))
100
}
Thefollowing2
network
/
fruit
test
net.py,network
/
fruit
train
net.py
containthelogicfortrainingandtestingthenetworkFirstly,in
network
/
fruit
train
net.py
wehave:

iterations
-thenumberofstepsforwhichthetrainingwillbedone

display
interval
-thenumberofiterationstotrainforbeforedisplaying
thelossandaccuracyofthenetwork

useCkpt
-iftrue,loadapreviouslytrainedmodelandcontinuetrain-
ing,else,trainanewmodelfromscratch

inputs
-readthetfrecordandprepareaqueueofimagessplitinto
shu

edbatches

train
model
-runsthetrainingprocess
1
network
/
fruit_train_net
.
py
2
3
import
tensorflow
as
tf
4
import
numpy
as
np
5
import
time
6
import
os
7
import
re
8
9
from
network_structure
import
fruit_network
as
network
10
from
network_structure
import
utils
11
12
from
utils
import
constants
13
14
#
default
number
of
iterations
to
run
the
training
15
iterations
=75000
40
16
#
default
amount
of
iterations
after
we
display
the
loss
and
accuracy
17
display_interval
=250
18
#
use
the
saved
model
and
continue
training
19
useCkpt
=
False
20
#
placeholder
for
probability
to
keep
the
network
parameters
after
an
iteration
21
keep_prob
=
tf
.
placeholder
(
tf
.
float32
,
name
='keep_prob')
22
23
#
prepare
the
input
tensors
for
the
network
24
def
inputs
(
filenames
,
batch_size
):
25
image
,
label
=
utils
.
read_files
(
filenames
)
26
image
=
utils
.
adjust_image_for_train
(
image
)
27
images
,
labels
=
tf
.
train
.
shuffle_batch
(
28
[
image
,
label
],
29
batch_size
=
batch_size
,
30
capacity
=35000+
batch_size
,
31
min_after_dequeue
=5000,
32
allow_smaller_final_batch
=
True
)
33
return
images
,
labels
34
35
#
build
the
network
36
logits
=
network
.
conv_net
(
network
.
X
,
network
.
weights
,
network
.
biases
,
keep_prob
)
37
#
apply
softmax
on
the
final
layer
38
prediction
=
tf
.
nn
.
softmax
(
logits
)
39
40
#
calculate
the
loss
using
the
predicted
labels
vs
the
expected
labels
41
loss_operation
=
tf
.
reduce_mean
(
42
tf
.
nn
.
sparse_softmax_cross_entropy_with_logits
(
logits
=
logits
,
labels
=
network
.
Y
))
43
44
#
use
adaptive
moment
estimation
optimizer
45
optimizer
=
tf
.
train
.
AdamOptimizer
(
learning_rate
=
network
.
learning_rate
)
46
train_op
=
optimizer
.
minimize
(
loss
=
loss_operation
)
47
41
48
#
calculate
the
accuracy
for
this
training
step
49
correct_prediction
=
tf
.
equal
(
tf
.
argmax
(
prediction
,1)
,
network
.
Y
)
50
accuracy
=
tf
.
reduce_mean
(
tf
.
cast
(
correct_prediction
,
tf
.
float32
))
51
52
init
=
tf
.
global_variables_initializer
()
53
54
55
def
train_model
():
56
time1
=
time
.
time
()
57
for
i
inrange
(1,
iterations
+1):
58
with
tf
.
Graph
().
as_default
():
59
60
batch_x
,
batch_y
=
sess
.
run
([
images
,
labels
])
61
batch_x
=
np
.
reshape
(
batch_x
,[
network
.
batch_size
,
network
.
input_size
])
62
63
sess
.
run
(
train_op
,
feed_dict
={
network
.
X
:
batch_x
,
network
.
Y
:
batch_y
,
keep_prob
:
network
.
dropout
})
64
65
if
i
%
display_interval
==0
or
i
==1:
66
loss
,
acc
=
sess
.
run
([
loss_operation
,
accuracy
],
feed_dict
={
network
.
X
:
batch_x
,
network
.
Y
:
batch_y
,
keep_prob
:1})
67
network
.
learning_rate
=
network
.
update_learning_rate
(
acc
,
learn_rate
=
network
.
initial_learning_rate
)
68
#
save
the
weights
and
the
meta
data
for
the
graph
69
saver
.
save
(
sess
,
constants
.
fruit_models_dir
+'model.ckpt')
70
tf
.
train
.
write_graph
(
sess
.
graph_def
,
constants
.
fruit_models_dir
,'graph.
42
pbtxt')
71
time2
=
time
.
time
()
72
print
(""time:%.4fstep:%dloss:%.4f
accuracy:%.4f""%(
time2
-
time1
,
i
,
loss
,
acc
))
73
time1
=
time
.
time
()
74
75
76
saver
=
tf
.
train
.
Saver
()
77
78
with
tf
.
Session
()
as
sess
:
79
sess
.
run
(
init
)
80
#
input
tfrecord
files
81
tfrecords_files
=[(
constants
.
data_dir
+
f
)
for
f
in
os
.
listdir
(
constants
.
data_dir
)
if
re
.
match
('
train',
f
)]
82
images
,
labels
=
inputs
(
tfrecords_files
,
network
.
batch_size
)
83
coord
=
tf
.
train
.
Coordinator
()
84
threads
=
tf
.
train
.
start_queue_runners
(
sess
=
sess
,
coord
=
coord
)
85
86
#
restore
the
previously
saved
value
if
we
wish
to
continue
the
training
87
if
useCkpt
:
88
ckpt
=
tf
.
train
.
get_checkpoint_state
(
constants
.
fruit_models_dir
)
89
saver
.
restore
(
sess
,
ckpt
.
model_checkpoint_path
)
90
91
train_model
()
92
93
coord
.
request_stop
()
94
coord
.
join
(
threads
)
95
sess
.
close
()
Secondly,in
network
/
fruit
test
net.py
wehave:

checkpoint
dir
-locationofthetrainedmodel
43

useTrain
-iftrue,calculateoverallaccuracyonthetrainset,else,
calculateaccuracyoverthetestset

mislabeled
-mapofpairswherethekeyisastringrepresentingthe
labelandanintegerrepresentinghowmanyimagesfromthatclass
wereincorrectly

inputs
-readthetfrecordandprepareaqueueofimagesbatches;
noneedtoshu

etheimagesfortesting

test
model
-runsthetestingprocess
1
network
/
fruit_test_net
.
py
2
3
import
numpy
4
import
tensorflow
as
tf
5
import
numpy
as
np
6
import
os
7
import
re
8
9
from
network_structure
import
fruit_network
as
network
10
from
network_structure
import
utils
11
from
utils
import
constants
12
13
useTrain
=
False
14
15
checkpoint_dir
=
os
.
getcwd
()+'\\..\\fruit_models\\'
16
keep_prob
=
tf
.
placeholder
(
tf
.
float32
)
17
18
if
useTrain
:
19
images_left_to_process
=
constants
.
number_train_images
20
total_number_of_images
=
constants
.
number_train_images
21
file_name
='train'
22
else
:
23
images_left_to_process
=
constants
.
number_test_images
24
total_number_of_images
=
constants
.
number_test_images
44
25
file_name
='test'
26
27
#
create
a
map
to
add
for
each
label
the
number
of
images
that
were
labeled
incorrectly
28
mislabeled
={}
29
30
#
associate
the
label
number
with
the
actual
human
readable
label
name
31
with
open
(
constants
.
root_dir
+'\\utils\\labels')
as
f
:
32
labels_text
=
f
.
readlines
()
33
labels_text
=[
x
.
strip
()
for
x
in
labels_text
]
34
for
label
in
labels_text
:
35
mislabeled
[
label
]=0
36
37
#
class
0
is
background
class
so
it
'
s
labeled
as
nothing
38
labels_text
=[""nothing""]+
labels_text
39
40
41
def
inputs
(
filename
,
batch_size
):
42
image
,
label
=
utils
.
read_files
(
filename
)
43
image
=
utils
.
adjust_image_for_test
(
image
)
44
images
,
labels
=
tf
.
train
.
batch
(
45
[
image
,
label
],
46
batch_size
=
batch_size
,
47
capacity
=
total_number_of_images
,
48
allow_smaller_final_batch
=
True
)
49
return
images
,
labels
50
51
52
def
test_model
():
53
global
images_left_to_process
54
correct
=0
55
while
images_left_to_process
>0:
56
batch_x
,
batch_y
=
sess
.
run
([
images
,
labels
])
57
batch_x
=
np
.
reshape
(
batch_x
,[
network
.
batch_size
,
network
.
input_size
])
45
58
#
the
results
of
the
classification
is
an
array
of
1
and
0,
1
is
a
correct
classification
59
results
=
sess
.
run
(
correct_pred
,
feed_dict
={
network
.
X
:
batch_x
,
network
.
Y
:
batch_y
,
keep_prob
:1})
60
if
images_left_to_process
<
network
.
batch_size
:
61
length
=
images_left_to_process
62
else
:
63
length
=
network
.
batch_size
64
images_left_to_process
=
images_left_to_process
-
length
65
for
i
inrange
(
length
):
66
ifnot
results
[
i
]:
67
mislabeled
[
labels_text
[
batch_y
[
i
]]]+=
1
68
69
correct
=
correct
+
numpy
.
sum
(
results
[0:
length
])
70
print
(""Predicted%doutof%d;partial
accuracy%.4f""%
71
(
correct
,
total_number_of_images
-
images_left_to_process
,
correct
/(
total_number_of_images
-
images_left_to_process
)))
72
print
(""Finalaccuracyon%sdata:%.8f""%(
file_name
,
correct
/
total_number_of_images
))
73
74
#
build
the
network
75
logits
=
network
.
conv_net
(
network
.
X
,
network
.
weights
,
network
.
biases
,
keep_prob
)
76
#
apply
softmax
on
the
final
layer
77
prediction
=
tf
.
nn
.
softmax
(
logits
)
78
79
correct_pred
=
tf
.
equal
(
tf
.
argmax
(
prediction
,1),
network
.
Y
)
80
accuracy
=
tf
.
reduce_mean
(
tf
.
cast
(
correct_pred
,
tf
.
46
float32
))
81
82
init
=
tf
.
global_variables_initializer
()
83
84
saver
=
tf
.
train
.
Saver
()
85
86
with
tf
.
Session
()
as
sess
:
87
sess
.
run
(
init
)
88
tfrecords_files
=[(
constants
.
data_dir
+
f
)
for
f
in
os
.
listdir
(
constants
.
data_dir
)
if
re
.
match
(
file_name
,
f
)]
89
images
,
labels
=
inputs
(
tfrecords_files
,
network
.
batch_size
)
90
coord
=
tf
.
train
.
Coordinator
()
91
threads
=
tf
.
train
.
start_queue_runners
(
sess
=
sess
,
coord
=
coord
)
92
93
ckpt
=
tf
.
train
.
get_checkpoint_state
(
checkpoint_dir
)
94
saver
.
restore
(
sess
,
ckpt
.
model_checkpoint_path
)
95
test_model
()
96
print
(
mislabeled
)
97
98
coord
.
request_stop
()
99
coord
.
join
(
threads
)
100
sess
.
close
()
Finally,wehavethe
fruit
detection
/
detect
fruits.py
.Thisservesasa
basicexampleonhowtoreadanimagefromaresizeitandfeedit
throughatrainedmodel.Thescriptprintstheclassthathadthehighest
probabilityafterpassingitthroughthemodel.
1
fruit_detection
/
detect_fruits
.
py
2
3
import
tensorflow
as
tf
4
from
utils
import
constants
5
6
with
open
(
constants
.
root_dir
+'\\utils\\labels')
as
f
:
7
labels
=
f
.
readlines
()
47
8
labels
=[
x
.
strip
()
for
x
in
labels
]
9
labels
=[""nothing""]+
labels
10
11
tf
.
app
.
flags
.
DEFINE_string
('image_path','images\\
Lemon2.jpg','Pathtoimage')
12
FLAGS
=
tf
.
app
.
flags
.
FLAGS
13
14
15
#
load
image
16
def
read_image
(
image_path
,
image_reader
):
17
filename_queue
=
tf
.
train
.
string_input_producer
([
image_path
])
18
_
,
image_file
=
image_reader
.
read
(
filename_queue
)
19
local_image
=
tf
.
image
.
decode_jpeg
(
image_file
)
20
local_image
=
tf
.
image
.
convert_image_dtype
(
local_image
,
tf
.
float32
)
21
gray_image
=
tf
.
image
.
rgb_to_grayscale
(
local_image
)
22
local_image
=
tf
.
image
.
rgb_to_hsv
(
local_image
)
23
shape
=
tf
.
shape
(
local_image
)
24
local_height
=
shape
[0]
25
local_width
=
shape
[1]
26
local_depth
=
shape
[2]
27
local_image
=
tf
.
reshape
(
local_image
,[
local_height
,
local_width
,
local_depth
])
28
final_image
=
tf
.
concat
([
local_image
,
gray_image
],
2)
29
return
final_image
,
local_height
,
local_width
,
local_depth
+1
30
31
32
def
predict
(
sess
,
X
,
softmax
,
keep_prob
,
images
):
33
images
=
sess
.
run
(
images
)
34
#
the
result
of
running
this
method
is
an
array
of
probabilities
,
where
each
index
in
the
array
corresponds
to
a
label
35
probability
=
sess
.
run
(
softmax
,
feed_dict
={
X
:
images
,
keep_prob
:1.0})
48
36
#
get
the
highest
probability
from
the
array
and
that
should
be
the
result
37
prediction
=
sess
.
run
(
tf
.
argmax
(
probability
,1))
38
return
prediction
,
probability
[0][
prediction
]
39
40
41
def
process_image
(
sess
,
X
,
softmax
,
keep_prob
,
image
,
image_height
,
image_width
,
image_depth
):
42
image_depth
=
sess
.
run
(
image_depth
)
43
image_height
=
sess
.
run
(
image_height
)
44
image_width
=
sess
.
run
(
image_width
)
45
#
resize
the
image
to
100
x
100
pixels
and
shape
it
to
be
like
an
array
of
one
image
,
since
that
is
the
required
input
for
the
network
46
#
for
smaller
parts
of
an
image
and
feed
those
to
the
network
,
tensorflow
has
a
method
called
""
extract_image_patches
""
47
img
=
tf
.
image
.
resize_images
(
tf
.
reshape
(
image
,
[-1,
image_height
,
image_width
,
image_depth
]),
[100,100])
48
img
=
tf
.
reshape
(
img
,[-1,100*100*4])
49
rez
,
prob
=
predict
(
sess
,
X
,
softmax
,
keep_prob
,
img
)
50
print
('Labelindex:%d-Label:%s-Probability:
%.4f'%(
rez
,
labels
[
rez
[0]],
prob
))
51
52
53
with
tf
.
Session
()
as
sess
:
54
image_path
=
FLAGS
.
image_path
55
image_reader
=
tf
.
WholeFileReader
()
56
57
#
restore
the
trained
model
from
the
saved
checkpoint
;
provide
the
path
to
the
meta
file
58
saver
=
tf
.
train
.
import_meta_graph
(
constants
.
fruit_models_dir
+'model.ckpt.meta')
59
#
provide
the
path
to
the
folder
containing
the
checkpoints
60
saver
.
restore
(
sess
,
tf
.
train
.
latest_checkpoint
(
49
constants
.
fruit_models_dir
))
61
graph
=
tf
.
get_default_graph
()
62
63
#
to
obtain
a
tensor
from
the
saved
model
,
we
must
get
it
by
name
,
which
is
why
we
name
the
tensors
when
we
create
them
64
#
even
if
there
is
only
one
tensor
with
a
name
,
in
the
meta
and
checkpoint
files
it
is
saved
as
an
array
,
so
we
have
to
provide
the
index
of
the
65
#
tensor
that
we
want
to
get
->
thus
we
call
""
get_tensor_by_name
(
tensor_name
:0)
""
66
67
#
obtain
the
input
tensor
by
name
68
X
=
graph
.
get_tensor_by_name
('X:0')
69
#
obtain
the
keep_prob
tensor
70
keep_prob
=
graph
.
get_tensor_by_name
('keep_prob:0'
)
71
#
obtain
the
output
layer
by
name
and
apply
softmax
on
in
in
order
to
obtain
an
output
of
probabilities
72
softmax
=
tf
.
nn
.
softmax
(
graph
.
get_tensor_by_name
('
softmax:0'))
73
74
image
,
height
,
width
,
depth
=
read_image
(
image_path
,
image_reader
)
75
coord
=
tf
.
train
.
Coordinator
()
76
threads
=
tf
.
train
.
start_queue_runners
(
sess
=
sess
,
coord
=
coord
)
77
process_image
(
sess
,
X
,
softmax
,
keep_prob
,
image
,
height
,
width
,
depth
)
78
79
coord
.
request_stop
()
80
coord
.
join
(
threads
)
81
sess
.
close
()
50
References
[1]
B
argoti
,S.,
and
U
nderwood
,J.Deepfruitdetectioninorchards.In
2017IEEEInternationalConferenceonRoboticsandAutomation(ICRA)
(May2017),pp.3626Œ3633.
)
4
[2]
B
arth
,R.,IJ
sselmuiden
,J.,H
emming
,J.,
and
H
enten
,E.V.Datasyn-
thesismethodsforsemanticsegmentationinagriculture:Acapsicum
annuumdataset.
ComputersandElectronicsinAgriculture144
(2018),
284Œ296.
)
4
[3]
C
han
,T.F.,
and
V
ese
,L.A.Activecontourswithoutedges.
IEEE
TransactionsonImageProcessing10
,2(Feb2001),266Œ277.
)
5
[4]
C
heng
,H.,D
amerow
,L.,S
un
,Y.,
and
B
lanke
,M.Earlyyieldprediction
usingimageanalysisofapplefruitandtreecanopyfeatureswith
neuralnetworks.
JournalofImaging3
,1(2017).
)
4
[5]
C
ires
¸
an
,D.C.,G
iusti
,A.,G
ambardella
,L.M.,
and
S
chmidhuber
,
J.Deepneuralnetworkssegmentneuronalmembranesinelectron
microscopyimages.In
Proceedingsofthe25thInternationalConferenceon
NeuralInformationProcessingSystems-Volume2
(USA,2012),NIPS'12,
CurranAssociatesInc.,pp.2843Œ2851.
)
5
[6]
C
ires
¸
an
,D.C.,M
eier
,U.,M
asci
,J.,G
ambardella
,L.M.,
and
S
chmid
-
huber
,J.Flexible,highperformanceconvolutionalneuralnetworksfor
imageIn
ProceedingsoftheTwenty-SecondInternational
JointConferenceonIntelligence-VolumeVolumeTwo
(2011),
IJCAI'11,AAAIPress,pp.1237Œ1242.
)
5
[7]
C
iresan
,D.C.,M
eier
,U.,
and
S
chmidhuber
,J.Multi-columndeep
neuralnetworksforimage
CoRRabs
/
1202.2745
(2012).
)
6
[8]
C
levert
,D.,U
nterthiner
,T.,
and
H
ochreiter
,S.Fastandaccu-
ratedeepnetworklearningbyexponentiallinearunits(elus).
CoRR
abs
/
1511.07289
(2015).
)
26
[9]
H
annun
,A.Y.,C
ase
,C.,C
asper
,J.,C
atanzaro
,B.,D
iamos
,G.,
E
lsen
,E.,P
renger
,R.,S
atheesh
,S.,S
engupta
,S.,C
oates
,A.,
and
N
g
,A.Y.Deepspeech:Scalingupend-to-endspeechrecognition.
CoRRabs
/
1412.5567
(2014).
)
14
[10]
H
emming
,J.,R
uizendaal
,J.,H
ofstee
,J.W.,
andvan
H
enten
,E.J.Fruit
detectabilityanalysisfordi

erentcamerapositionsinsweet-pepper.
Sensors14
,4(2014),6032Œ6044.
)
4
[11]
K
apach
,K.,B
arnea
,E.,M
airon
,R.,E
dan
,Y.,
and
B
en
-S
hahar
,O.
Computervisionforfruitharvestingrobots&#150;stateoftheartand
51
challengesahead.
Int.J.Comput.VisionRobot.3
,1
/
2(Apr.2012),4Œ34.
)
4
[12]
K
rizhevsky
,A.,N
air
,V.,
and
H
inton
,G.Thecifardataset.[Online;
accessed27.10.2018].
)
2
,
6
[13]
L
e
C
un
,Y.,C
ortes
,C.,
and
B
urges
,C.J.Themnistdatabaseofhand-
writtendigits.[Online;accessed27.10.2018].
)
6
[14]
L
ee
,H.,G
rosse
,R.,R
anganath
,R.,
and
N
g
,A.Y.Convolutionaldeep
beliefnetworksforscalableunsupervisedlearningofhierarchicalrep-
resentations.In
Proceedingsofthe26thAnnualInternationalConference
onMachineLearning
(NewYork,NY,USA,2009),ICML'09,ACM,
pp.609Œ616.
)
8
[15]
L
i
,D.,Z
hao
,H.,Z
hao
,X.,G
ao
,Q.,
and
X
u
,L.Cucumberdetec-
tionbasedontextureandcoloringreenhouse.
InternationalJournalof
PatternRecognitionandIntelligence31
(012017).
)
4
[16]
L
iang
,M.,
and
H
u
,X.Recurrentconvolutionalneuralnetworkfor
objectrecognition.In
2015IEEEConferenceonComputerVisionand
PatternRecognition(CVPR)
(June2015),pp.3367Œ3375.
)
6
,
8
[17]
M
umford
,D.,
and
S
hah
,J.Optimalapproximationsbypiecewise
smoothfunctionsandassociatedvariationalproblems.
Communica-
tionsonPureandAppliedMathematics42
,5(1989),577Œ685.
)
5
[18]
M
uresan
,H.,
and
O
ltean
,M.Fruits360datasetongithub.[Online;
accessed27.10.2018].
)
1
,
10
,
27
[19]
M
uresan
,H.,
and
O
ltean
,M.Fruits360datasetonkaggle.[Online;
accessed27.10.2018].
)
1
,
10
[20]
N
inawe
,P.,
and
P
andey
,M.S.Acompletiononfruitrecognition
systemusingk-nearestneighborsalgorithm.In
InternationalJournal
ofAdvancedResearchinComputerEngineering&Technology(IJARCET)
(2014),vol.3.
)
5
[21]
O'B
oyle
,B.,
and
H
all
,C.Whatisgooglelensandhowdoyouuseit?
[Online;accessed05.05.2018].
)
2
[22]
P
uttemans
,S.,V
anbrabant
,Y.,T
its
,L.,
and
G
oedem
,T.Automated
visualfruitdetectionforharvestestimationandroboticharvesting.In
2016SixthInternationalConferenceonImageProcessingTheory,Toolsand
Applications(IPTA)
(Dec2016),pp.1Œ6.
)
4
[23]
R
ahnemoonfar
,M.,
and
S
heppard
,C.Deepcount:Fruitcounting
basedondeepsimulatedlearning.
Sensors17
,4(2017).
)
4
[24]
R
en
,S.,H
e
,K.,G
irshick
,R.B.,
and
S
un
,J.FasterR-CNN:to-
wardsreal-timeobjectdetectionwithregionproposalnetworks.
CoRR
52
abs
/
1506.01497
(2015).
)
4
[25]
S
a
,I.,G
e
,Z.,D
ayoub
,F.,U
pcroft
,B.,P
erez
,T.,
and
M
c
C
ool
,C.Deep-
fruits:Afruitdetectionsystemusingdeepneuralnetworks.
Sensors
16
,8(2016).
)
3
,
4
[26]
S
chmidhuber
,J.Deeplearninginneuralnetworks:Anoverview.
CoRR
abs
/
1404.7828
(2014).
)
5
[27]
S
elvaraj
,A.,S
hebiah
,N.,N
idhyananthan
,S.,
and
G
anesan
,L.Fruit
recognitionusingcolorandtexturefeatures.
JournalofEmergingTrends
inComputingandInformationSciences1
(102010),90Œ94.
)
4
[28]
S
ong
,Y.,G
lasbey
,C.,H
organ
,G.,P
older
,G.,D
ieleman
,J.,
and
vander
H
eijden
,G.Automaticfruitrecognitionandcountingfrom
multipleimages.
BiosystemsEngineering118
(2014),203Œ215.
)
3
[29]
S
pringenberg
,J.T.,D
osovitskiy
,A.,B
rox
,T.,
and
R
iedmiller
,M.A.
Strivingforsimplicity:Theallconvolutionalnet.
CoRRabs
/
1412.6806
(2014).
)
6
,
26
[30]
S
rivastava
,R.K.,G
reff
,K.,
and
S
chmidhuber
,J.Trainingverydeep
networks.
CoRRabs
/
1507.06228
(2015).
)
5
[31]
S
zegedy
,C.,L
iu
,W.,J
ia
,Y.,S
ermanet
,P.,R
eed
,S.E.,A
nguelov
,D.,
E
rhan
,D.,V
anhoucke
,V.,
and
R
abinovich
,A.Goingdeeperwith
convolutions.
CoRRabs
/
1409.4842
(2014).
)
14
[32]
T
ensor
F
low
.T.[Online;accessed05.05.2018].
)
3
,
14
[33]
W
ikipedia
.Convolutioninmathematics.[Online;accessed
05.05.2018].
)
6
[34]
W
ikipedia
.Deeplearningarticleonwikipedia.[Online;accessed
05.05.2018].
)
5
[35]
W
ikipedia
.Googlelensonwikipedia.[Online;accessed05.05.2018].
)
2
[36]
X
iong
,J.,L
iu
,Z.,L
in
,R.,B
u
,R.,H
e
,Z.,Y
ang
,Z.,
and
L
iang
,C.Green
grapedetectionandpicking-pointcalculationinanight-timenatural
environmentusingacharge-coupleddevice(ccd)visionsensorwith
illumination.
Sensors18
,4(2018).
)
5
[37]
Z
awbaa
,H.,A
bbass
,M.,H
azman
,M.,
and
H
assanien
,A.E.Automatic
fruitimagerecognitionsystembasedonshapeandcolorfeatures.
CommunicationsinComputerandInformationScience488
(112014),278Œ
290.
)
4
"
90,Autoregressive Quantile Networks for Generative Modeling,http://arxiv.org/pdf/1806.05575v1.pdf,https://github.com/SSS135/aiqn-vae,"AutoregressiveQuantileNetworksforGenerativeModeling
GeorgOstrovski
*1
WillDabney
*1
R
´
emiMunos
1
Abstract
Weintroduceautoregressiveimplicitquantile
networks(AIQN),afundamentallydifferentap-
proachtogenerativemodelingthanthosecom-
monlyused,thatimplicitlycapturesthedistribu-
tionusingquantileregression.AIQNisableto
achievesuperiorperceptualqualityandimprove-
mentsinevaluationmetrics,withoutincurringa
lossofsamplediversity.Themethodcanbeap-
pliedtomanyexistingmodelsandarchitectures.
InthisworkweextendthePixelCNNmodelwith
AIQNanddemonstrateresultsonCIFAR-10and
ImageNetusingInceptionscore,FID,non-cherry-
pickedsamples,andinpaintingresults.Wecon-
sistentlyobservethatAIQNyieldsahighlystable
algorithmthatimprovesperceptualqualitywhile
maintainingahighlydiversedistribution.
1.Introduction
Therehasbeenastaggeringincreaseinprogressongenera-
tivemodelinginrecentyears,builtlargelyuponfundamental
advancessuchasgenerativeadversarialnetworks(
Goodfel-
lowetal.
,
2014
),variationalinference(
Kingma&Welling
,
2013
),andautoregressivedensityestimation(
vandenOord
etal.
,
2016c
).Thesehaveledtobreakthroughsinstate-of-
the-artgenerationofnaturalimages(
Karrasetal.
,
2017
)and
audio(
vandenOordetal.
,
2016a
),andevenbeenusedfor
unsupervisedlearningofdisentangledrepresentations(
Hig-
ginsetal.
,
2017
;
Chenetal.
,
2016
).Thesedomainsoften
havereal-valueddistributionswithunderlyingmetrics;that
is,thereisanotionofsimilaritybetween
datapoints.Thissimilarityisignoredbythepredominant
work-horseofgenerativemodeling,theKullback-Leibler
(KL)divergence.Progressisnowbeingmadetowardsalgo-
rithmsthatoptimizewithrespecttotheseunderlyingmetrics
(
Arjovskyetal.
,
2017
;
Bousquetetal.
,
2017
).
*
Equalcontribution
1
DeepMind,London,UK.Correspondence
to:GeorgOstrovski
<
ostrovski@google.com
>
,WillDabney
<
wdabney@google.com
>
.
Proceedingsofthe
35
th
InternationalConferenceonMachine
Learning
,Stockholm,Sweden,PMLR80,2018.Copyright2018
bytheauthor(s).
Inthispaper,wepresentanovelapproachtogenerativemod-
eling,that,whilestrikinglydifferentfromexistingmethods,
isgroundedinthewell-understoodstatisticalmethodsof
quantileregression
.Unlikethemajorityofrecentwork,we
approachgenerativemodelingwithouttheuseoftheKLdi-
vergence,andwithoutexplicitlyapproximatingalikelihood
model.LikeGANs,inthiswayweproduceanimplicitly
model,butunlikeGANsouroptimizationprocedure
isinherentlystableandlacksdegeneratesolutionswhich
causelossofdiversityandmodecollapse.
MuchoftherecentresearchonGANshasbeenfocusedon
improvingstability(
Radfordetal.
,
2015
;
Arjovskyetal.
,
2017
;
Daskalakisetal.
,
2017
)andsamplediversity(
Gul-
rajanietal.
,
2017
;
Salimansetal.
,
2016
;
2018
).Bystark
contrast,methodssuchasPixelCNN(
vandenOordetal.
,
2016b
)readilyproducehighdiversity,butduetotheiruse
ofKLdivergenceareunabletomakereasonabletrade-offs
betweenlikelihoodandperceptualsimilarity(
Theisetal.
,
2015
;
Bellemareetal.
,
2017
;
Bousquetetal.
,
2017
).
Ourproposedmethod,
autoregressiveimplicitquantilenet-
works
(AIQN),combinestheofboth:alossfunc-
tionthatrespectstheunderlyingmetricofthedataleading
toimprovedperceptualquality,andastableoptimization
processleadingtohighlydiversesamples.Whiletherehas
beenanincreasingtendencytowardscomplexarchitectures
(
Chenetal.
,
2017
;
Salimansetal.
,
2017
)andmultipleobjec-
tivelossfunctionstoovercomethesechallenges,AIQNis
conceptuallysimpleanddoesnotrelyonanyspecialarchi-
tectureoroptimizationtechniques.Empiricallyitprovesto
berobusttohyperparametervariationsandeasytooptimize.
Ourworkismotivatedbytherecentadvancesachievedby
reframingGANsintermsofoptimaltransport,leadingtothe
WassersteinGANalgorithm(
Arjovskyetal.
,
2017
),aswell
asworktowardsunderstandingtherelationshipbetweenop-
timaltransportandbothGANsandVAEs(
Bousquetetal.
,
2017
).Inagreementwiththeseresults,wefocusonloss
functionsgroundedinperceptuallymeaningfulmetrics.We
builduponrecentworkindistributionalreinforcementlearn-
ing(
Dabneyetal.
,
2018a
),whichhasbeguntobridgethe
gapbetweenapproachesinreinforcementlearningandun-
supervisedlearning.Towardsapracticalalgorithmwebase
ourexperimentalresultsonGatedPixelCNN(
vandenOord
etal.
,
2016b
),andshowthatusingAIQNim-
arXiv:1806.05575v1  [cs.LG]  14 Jun 2018AutoregressiveQuantileNetworksforGenerativeModeling
provesobjectiveperformanceonCIFAR-10andImageNet
32x32intermsofFr
´
echetInceptionDistance(FID)and
Inceptionscore,aswellassubjectiveperceptualqualityin
imagesamplesandinpainting.
2.Background
Webeginbyestablishingsomenotation,beforeturningtoa
reviewofthreeofthemostprevalentmethodsforgenerative
modeling.Calligraphicletters(e.g.
X
)denotesetsorspaces,
capitalletters(e.g.
X
)denoterandomvariables,andlower
caseletters(e.g.
x
)indicatevalues.Aprobabilitydistribu-
tionwithrandomvariable
X
2X
isdenoted
p
X
2
P
(
X
)
,
itscumulativedistributionfunction(c.d.f.)
F
X
,andinverse
c.d.f.orquantilefunction
Q
X
=
F

1
X
.Whenprobabil-
itydistributionsorquantilefunctionsareparameterizedby
some

wewillwrite
p

or
Q

recognizingthatherewedo
notview

asarandomvariable.
Perhapsthesimplestwaytoapproachgenerativemodeling
ofarandomvariable
X
2X
isbysomediscretization
of
X
into
n
separatevalues,say
x
1
;:::;x
n
2X
,andparam-
eterizetheapproximatedistributionwith
p

(
x
i
)
/
exp(

i
)
.
Thistypeofcategoricalparameterizationiswidelyused,
onlyslightlylesscommonlywhen
X
doesnotlenditself
naturallytosuchapartitioning.Typically,theparameters

areoptimizedtominimizetheKullback-Leibler(KL)di-
vergencebetweenobservedvaluesof
X
andthemodel
p

,


=argmin

D
KL
(
p
X
k
p

)
.
However,thisisonlytractablewhen
X
isasmalldiscreteset
oratbestlow-dimensional.Acommonmethodforextend-
ingagenerativemodelordensityestimatortomultivariate
distributionsistofactorthedensityasaproductofscalar-
valuedconditionaldistributions.Let
X
=(
X
1
;:::;X
n
)
,
thenforanypermutationofthedimensions
˙
:
N
n
!
N
n
,
p
X
(
x
)=
n
Y
i
=1
p
X
˙
(
i
)
(
x
˙
(
i
)
j
x
˙
(1)
;:::;x
˙
(
i

1)
)
:
(1)
Whentheconditionaldensityismodeledbyasimple
(e.g.Gaussian)basedistribution,theorderingofthedi-
mensionscanbecrucial(
Papamakariosetal.
,
2017
).How-
ever,itiscommonpracticetochooseanarbitraryordering
andrelyuponamorepowerfulconditionalmodeltoavoid
theseproblems.ThisclassofmodelsincludesPixelRNN
andPixelCNN(
vandenOordetal.
,
2016c
;
b
),MAF(
Papa-
makariosetal.
,
2017
),MADE(
Germainetal.
,
2015
),and
manyothers.Fundamentally,alltheseapproachesusethe
KLdivergenceastheirlossfunction.
Anotherclassofmethods,generallyknownas
latentvari-
ablemethods
,canbypasstheneedforautoregressivemod-
elsusingadifferentmodelingassumption.,
considertheVariationalAutoencoder(VAE)(
Kingma&
Welling
,
2013
;
Rezendeetal.
,
2014
),whichrepresents
p

asthemarginalizationoveralatentrandomvariable
Z
2Z
.TheVAEistrainedtomaximizeanapproximate
lowerboundofthelog-likelihoodoftheobservations:
log
p

(
x
)

D
KL
(
q

(
z
j
x
)
k
p
(
z
))+
E
[log
p

(
x
j
z
)]
:
AlthoughVAEsarestraightforwardtoimplementand
optimize,andeffectiveatcapturingstructureinhigh-
dimensionalspaces,theyoftenmissdetail,re-
sultinginblurryimages.
GenerativeAdversarialNetworks(GANs)(
Goodfellow
etal.
,
2014
)posetheproblemoflearningagenerativemodel
asatwo-playerzero-sumgamebetweenadiscriminator
D
,
attemptingtodistinguishbetween
x
˘
p
X
(realdata)and
x
˘
p

(generateddata),andagenerator
G
,attemptingto
generatedataindistinguishablefromrealdata.Thegenera-
torisanimplicitlatentvariablemodelthatreparameterizes
samples,typicallyfromanisotropicGaussiandistribution,
intovaluesin
X
.TheoriginalformulationofGANs,
argmin
G
sup
D
h
E
X
log(
D
(
X
))+
E
Z
log(1

D
(
G
(
Z
)))
i
;
canbeseenasminimizingalower-boundontheJensen-
Shannondivergence(
Goodfellowetal.
,
2014
;
Bousquet
etal.
,
2017
).Thatis,eveninthecaseofGANsweareoften
minimizingfunctionsoftheKLdivergence
1
.
Manyrecentadvanceshavecomefromprincipledcombina-
tionsofthesethreefundamentalmethods(
Makhzanietal.
,
2015
;
Dumoulinetal.
,
2016
;
Roscaetal.
,
2017
).
2.1.DistanceMetricsandLossFunctions
Acommonperspectiveingenerativemodelingisthatthe
choiceofmodelshouldencodeexistingmetricassump-
tionsaboutthedomain,combinedwithagenericlikelihood-
focusedlosssuchastheKLdivergence.Underthisview,
theKL'sgeneralapplicabilityandrobustoptimizationprop-
ertiesmakeitanaturalchoice,andmostimplementations
ofthemethodswereviewedintheprevioussectionattempt
to,atleastindirectly,minimizeaversionoftheKL.
Ontheotherhand,aseverymodelinevitablymakestrade-
offswhenconstrainedbycapacityorlimitedtraining,itis
desirableforitsoptimizationgoaltoincentivizetrade-offs
prioritizingapproximatelycorrectsolutions,whenthedata
spaceisendowedwithametricsupportingameaningful(al-
beitpotentiallysubjective)notionofapproximation.Ithas
beenargued(
Theisetal.
,
2015
;
Bousquetetal.
,
2017
;
Ar-
jovskyetal.
,
2017
;
Bellemareetal.
,
2017
)thattheKLmay
notalwaysbeappropriatefromthisperspective,bymaking
sub-optimaltrade-offsbetweenlikelihoodandsimilarity.
1
TheJensen-ShannondivergenceisthesumofKLsbetween
distributions
P;Q
andtheiruniformmixture
M
=0
:
5(
P
+
Q
)
:
JSD(
P
jj
Q
)=0
:
5(
D
KL
(
P
jj
M
)+
D
KL
(
Q
jj
M
))
.
AutoregressiveQuantileNetworksforGenerativeModeling
Figure1.
Categorizationofgenerativemodelsbydiscretization
vs.reparameterizationandlossfunctionsbylikelihood-basedvs.
quantile-regression-based.
Indeed,manylimitationsofexistingmodelscanbetraced
backtotheuseofKL,andtheresultingtrade-offsinap-
proximatesolutionsitimplies.Forinstance,itsuseappears
toplayacentralroleinoneoftheprimaryfailuremodes
ofVAEs,thatofblurrysamples.
Zhaoetal.
(
2017
)argue
thattheGaussianposterior
p

(
x
j
z
)
impliesanoverlysimple
model,which,whenunabletoperfectlythedata,isforced
toaverage(thuscreatingblur),andisnotincentivizedby
theKLtowardsanalternativenotionofapproximatesolu-
tion.
Theisetal.
(
2015
)emphasizedthatanimprovement
oflog-likelihooddoesnotnecessarilytranslatetohigher
perceptualquality,andthattheKLlossismorelikelyto
produceatypicalsamplesthansomeothertrainingcriteria.
Weofferanalternativeperspective:agoodmodelshould
encodeassumptionsaboutthedatadistribution,whereas
agoodlossshouldencodethenotionofsimilarity,thatis,
theunderlyingmetriconthedataspace.Fromthispointof
view,theKLcorrespondstoanactualabsenceofexplicit
underlyingmetric,withcompletefocusonprobability.
Theoptimaltransportmetrics
W
c
,forunderlyingmetric
c
(
x;x
0
)
,andinparticularthe
p
-Wassersteindistance,when
c
isan
L
p
metric,havefrequentlybeenproposedasbe-
ingwell-suitedreplacementstoKL(
Bousquetetal.
,
2017
;
Genevayetal.
,
2017
).,theadvantagesare(1)avoid-
anceofmodecollapse(noneedtochoosebetweenspreading
overmodesorcollapsingtoasinglemodeasinKL),and(2)
theabilitytotradeofferrorsandincentivizeapproximations
thatrespecttheunderlyingmetric.
Recently,
Arjovskyetal.
(
2017
)introducedtheWasserstein
GAN,reposingthetwo-playergameastheestimationofthe
gradientofthe
1
-Wassersteindistancebetweenthedataand
generatordistributions.Theyreframethisintermsofthe
dualformofthe
1
-Wasserstein,withthecriticestimatinga
function
f
whichmaximallyseparatesthetwodistributions.
Whilethisisanexcitinglineofwork,itstillfaceslimitations
whenthecriticsolutionisapproximate,i.e.when
f

isnot
foundbeforeeachupdate.Inthiscase,duetoinsufcient
trainingofthecritic(
Bellemareetal.
,
2017
)orlimitationsof
thefunctionapproximator,thegradientdirectionproduced
canbearbitrarilybad(
Bousquetetal.
,
2017
).
Thus,weareleftwiththequestionofhowtominimizea
distributionlossrespectinganunderlyingmetric.Recent
workindistributionalreinforcementlearninghasproposed
theuseof
quantileregression
asamethodforminimizing
the
1
-Wassersteinintheunivariatecasewhenapproximating
usingamixtureofDiracfunctions(
Dabneyetal.
,
2018b
).
2.2.QuantileRegression
Inthissection,wereviewquantileregressionasamethodfor
estimatingthequantilefunctionofadistributionat
points,i.e.itsinversecumulativedistributionfunction.This
leadstorecentworkonapproximatingadistributionby
aneuralnetworkapproximationofitsquantilefunction,
actingasareparameterizationofarandomsamplefromthe
uniformdistribution.
Thequantileregressionloss(
Koenker&Hallock
,
2001
)
foraquantileat
˝
2
[0
;
1]
anderror
u
(positiveforun-
derestimationandnegativeforoverestimation)isgiven
by
ˆ
˝
(
u
)=(
˝

I
f
u

0
g
)
u
.Itisanasymmetric
lossfunctionpenalizingunderestimationbyweight
˝
and
overestimationbyweight
1

˝
.Foragivenscalardis-
tribution
Z
withc.d.f.
F
Z
andaquantile
˝
,theinverse
c.d.f.
q
=
F

1
Z
(
˝
)
minimizestheexpectedquantileregres-
sionloss
E
z
˘
Z
[
ˆ
˝
(
z

q
)]
.
Usingthislossallowsonetotrainaneuralnetworktoap-
proximateascalardistributionrepresentedbyitsinverse
c.d.f.Forthis,thenetworkcanoutputaedgridofquan-
tiles(
Dabneyetal.
,
2018b
),withtherespectivequantilere-
gressionlossesbeingappliedtoeachoutputindependently.
Amoreeffectiveapproachistoprovidethedesiredquantile
˝
asanadditionalinputtothenetwork,andtrainittooutput
thecorrespondingvalueof
F

1
Z
(
˝
)
.The
implicitquantile
network
(IQN)model(
Dabneyetal.
,
2018a
)reparameter-
izesasample
˝
˘U
([0
;
1])
throughadeterministicfunction
toproducesamplesfromtheunderlyingdatadistribution.
Thesetwomethodscanbeseentobelongtothetop-right
andbottom-rightcategoriesinFigure
1
.AnIQN
Q

can
betrainedbystochasticgradientdescentonthequantile
regressionloss,with
u
=
z

Q

(
˝
)
andtrainingsamples
(
z;˝
)
drawnfrom
z
˘
Z
and
˝
˘U
([0
;
1])
.
AutoregressiveQuantileNetworksforGenerativeModeling
Onedrawbacktothequantileregressionlossisthatgradi-
entsdonotscalewiththemagnitudeoftheerror,butinstead
withthesignoftheerrorandthequantileweight
˝
.This
increasesgradientvarianceandcannegativelyimpactthe
model'ssamplequality.Increasingthebatchsize,and
thusaveragingovermorevaluesof
˝
,wouldhavetheeffect
ofloweringthisvariance.Alternatively,wecansmooththe
gradientsasthemodelconvergesbyallowingerrors,under
somethreshold

,tobescaledwiththeirmagnitude,revert-
ingtoan
expectile
loss.ThisresultsintheHuberquantile
loss(
Huber
,
1964
;
Dabneyetal.
,
2018b
):
ˆ

˝
(
u
)=
(
j
˝

I
f
u

0
gj
2

u
2
;
if
j
u
j

j
˝

I
f
u

0
gj
(
j
u
j
1
2

)
;
otherwise
:
(2)
3.AutoregressiveImplicitQuantiles
Let
X
=(
X
1
;:::;X
n
)
2X
1
X
n
=
X
bean
n
-
dimensionalrandomvariable.Webeginbyanalyzingthe
effectoftwonaiveapplicationsofIQNtomodelingthe
distributionof
X
.
First,supposeweusethesamequantiletarget,
˝
2
[0
;
1]
,
foreveryoutputdimension.Theonlyto
IQNwouldbetooutput
n
dimensionsinsteadof
1
,the
lossbeingappliedtoeachoutputdimensionindepen-
dently.Thisisequivalenttoassumingthatthedimen-
sionsof
X
are
comonotonic
.Tworandomvariablesare
comonotonicifandonlyiftheycanbeexpressedasnon-
decreasing(deterministic)functionsofasinglerandom
variable(
Dhaeneetal.
,
2006
).Thusajointquantilefunc-
tionforacomonotonic
X
canbewrittenas
F

1
X
(
˝
)=
(
F

1
X
1
(
˝
)
;F

1
X
2
(
˝
)
;:::;F

1
X
n
(
˝
))
.Whiletherearemanyin-
terestingusesforcomonotonicrandomvariables,webelieve
thisassumptionistoostrongtobeusefulmorebroadly.
Second,onecoulduseaseparatevalue
˝
i
2
[0
;
1]
foreach
X
i
,withtheIQNbeingunchangedfromthecase.This
correspondstomakinganindependenceassumptiononthe
dimensionsof
X
.Againwewouldexpectthistobean
unreasonablyrestrictivemodelingassumptionformanydo-
mains,suchasthecaseofnaturalimages.
Now,weturntoourproposedapproachofextendingIQN
tomultivariatedistributions.Weanorderingofthe
n
dimensions.Ifthedensityfunction
p
X
isexpressedasa
productofconditionallikelihoods,asinEquation
1
,then
thejointc.d.f.canbewrittenas
F
X
(
x
)=P(
X
1

x
1
;:::;X
n

x
n
)
;
=
n
Y
i
=1
F
X
i
j
X
i

1
;:::;X
1
(
x
i
)
:
Furthermore,for
˝
joint
=
Q
n
i
=1
˝
i
,wecanwritethe
joint-
quantilefunction
of
X
as
F

1
X
(
˝
joint
)=(
F

1
X
1
(
˝
1
)
;:::;F

1
X
n
j
X
n

1
;:::
(
˝
n
))
:
Thisapproachhasbeenusedpreviouslyby
Koenker&Xiao
(
2006
),whointroducedaquantileautoregressionmodelfor
quantileregressionontime-series.
WeproposetoextendIQNtoanautoregressivemodelofthe
aboveconditionalformofajoint-quantilefunction.Denot-
ing
X
1:
i
=
X
1
X
i
,let
~
X
:=
S
n
i
=0
X
1:
i
bethespace
of`partial'datapoints.Wecantheautoregressive
IQNasadeterministicfunction
Q

:
~
X
[0
;
1]
n
!
~
X
,map-
pingpartialsamples
~
x
2
~
X
andquantiletargets
˝
i
2
[0
;
1]
toestimatesof
F

1
X
.Wecanthentrain
Q

usingaquantile
regressionloss(Equation
2
).Forgeneration,onecaniterate
x
1:
i
=
Q

(
x
1:
i

1
;˝
i
)
,onasequenceofgrowingpartial
samples
2
x
1:
i

1
andindependentlysampled
˝
i
˘U
([0
;
1])
,
for
i
=1
;:::;n
,toobtainasample
x
=
x
1:
n
.
3.1.QuantileRegressionandtheWasserstein
Aspreviouslymentioned,fortherestrictedmodelclassofa
uniformmixtureofDiracs,quantileregressioncanbeshown
tominimizethe
1
-Wassersteinmetric(
Dabneyetal.
,
2018b
).
Weextendthisanalysisforthecaseofarbitraryapproximate
quantilefunctions,andthatquantileregressionmini-
mizesacloselyrelateddivergencewhichwecall
quantile
divergence
,foranydistributions
P
and
Q
,as
q
(
P;Q
):=
Z
1
0
""
Z
F

1
Q
(
˝
)
F

1
P
(
˝
)
(
F
P
(
x
)

˝
)
dx
#
d˝:
Indeed,theexpectedquantilelossofanyparameterized
quantilefunction

Q

equals,uptoaconstant,thequantile
divergencebetween
P
andthedistribution
Q

implicitly
by

Q

:
E
˝
˘U
([0
;
1])

E
z
˘
P
[
ˆ
˝
(
z


Q

(
˝
))]

=
q
(
P;Q

)+
h
(
P
)
;
where
h
(
P
)
doesnotdependon
Q

.Thusquantileregres-
sionminimizesthequantiledivergence
q
(
P;Q

)
andthe
samplegradient
r

ˆ
˝
(
z


Q

(
˝
))
(for
˝
˘U
([0
;
1])
and
z
˘
P
)isanunbiasedestimateof
r

q
(
P;Q

)
.SeeAp-
pendixforproofs.
3.2.QuantileDensityFunction
AlthoughIQNdoesnotdirectlymodelthelog-likelihood
ofthedatadistribution,observethatwecanstillquerythe
implieddensityatapoint(
Jones
,
1992
):
@
@˝
F

1
X
(
˝
)=
1
p
X
(
F

1
X
(
˝
))
:
Indeed,thisquantity,knownasthe
sparsityfunction
(
Tukey
,
1965
)orthe
quantile-densityfunction
(
Parzen
,
1979
)plays
2
Throughoutweunderstand
x
0
=
x
1:0
2X
1:0
todenote
the`emptytuple',andthefunction
Q

tomapthistoasingle
unconditionalsample
x
1
=
x
1:1
=
Q

(
x
0
;˝
1
)
.
AutoregressiveQuantileNetworksforGenerativeModeling
Figure2.
IllustrationofGatedPixelCNNlayerblockforPixelIQN.
DashedlineshowsboundaryofstandardGatedPixelCNN,with
v
theverticaland
h
thehorizontalstack.Conditioningon
˝
isidenti-
caltothelocation-dependentconditioninginGatedPixelCNN.
acentralroleintheanalysisofquantileregressionmod-
els(
Koenker
,
1994
).Acommonapproachinvolveschoos-
ingabandwidthparameter
h
andestimatingthisquantity
throughferencesaroundthevalueofinterestas
(
F

1
X
(
˝
+
h
)

F

1
X
(
˝

h
))
=
2
h
(
Siddiqui
,
1960
).However,
aswehavethefullquantilefunction,thequantile-density
functioncanbecomputedexactlyusingasinglestepof
back-propagationtocompute
@F

1
(
˝
)
@˝
.Asthisonlyallows
queryingthedensitygiventhevalueof
˝
,applicationto
generallikelihoodswouldrequirethevalueof
˝
thatproducestheclosestapproximationtothequerypoint
x
.Thougharguablytooineffortraining,thiscould
potentiallybeusedtointerrogatethemodel.
4.PixelIQN
Totestourproposedmethod,whichisarchitecturallycom-
patiblewithmanygenerativemodelapproaches,wewanted
tocompareandcontrastIQN,thatisquantileregressionand
quantilereparameterization,withamethodtrainedwithan
explicitparameterizationtominimizeKLdivergence.A
naturalchoiceforthiswasPixelCNN,webuild
upontheGatedPixelCNNof
vandenOordetal.
(
2016b
).
TheGatedPixelCNNtakesasinputanimage
x
˘
X
,sam-
pledfromthetrainingdistributionattrainingtime,andpo-
tentiallyallzerosorpartiallygeneratedatgenerationtime,
aswellasalocation-dependentcontext
s
.Themodelcon-
sistsofanumberofresiduallayerblocks,whosestructure
ischosentoalloweachoutputpixeltobeafunctionofall
precedinginputpixels(inaraster-scanorder).Atitscore,
eachlayerblockcomputestwogatedactivationsoftheform
y
=tanh(
W
k;f

x
+
V
k;f

s
)

˙
(
W
k;g

x
+
V
k;g

s
)
;
with
k
thelayerindex,

denotingconvolution,and
V
k;f
and
V
k;g
being
1

1
convolutionkernels.SeeFigure
2
forafullschematicdepictionofaGatedPixelCNNlayer
block.Afteranumberofsuchlayerblocks,thePixelCNN
producesaoutputlayerwithshape
(
n;n;
3
;
256)
,with
asoftmaxacrossthedimension,correspondingtothe
approximateconditionallikelihoodforthevalueofeach
pixel-channel.Thatis,theconditionallikelihoodisthe
productoftheseindividualautoregressivemodels,
p
(
x
j
s
)=
3
n
2
Y
i
=1
p
(
x
i
j
x
1
;:::;x
i

1
;s
i
)
:
Typicallythelocation-dependentconditioningtermwas
usedtoconditiononclasslabels,buthere,wewilluse
ittoconditiononthesamplepoint
3
˝
2
[0
;
1]
3
n
2
.Thus,in
additiontotheinputimage
x
weinput,inplaceof
s
,the
samplepoints
˝
=(
˝
1
;:::;˝
3
n
2
)
tobereparameterized,
witheach
˝
i
˘U
([0
;
1])
.Finally,ournetworkoutputsonly
thefullsampleimageofshape
(
n;n;
3)
,withouttheneed
foranadditionalsoftmaxlayer.Notethatthenumberof
˝
valuesgeneratedexactlycorrespondstothenumberof
randomdrawsfromsoftmaxdistributionsintheoriginalPix-
elCNN.Wearesimplychangingtheroleoftherandomness,
fromadrawattheoutputtoapartoftheinput.
Architecturally,ourproposedmodel,PixelIQN,isexactly
thenetworkgivenby
vandenOordetal.
(
2016b
),with
theoneexceptionthatweoutputonlyasinglevalueper
pixel-channelanddonotrequirethesoftmaxactivations.
InPixelCNNtrainingisdonebypassingthetrainingim-
agethroughthenetwork,andtrainingeachoutputsoftmax
distributionusingtheKLdivergencebetweenthetraining
imageandtheapproximatedistribution,
X
i
D
KL
(

x
i
;p
(

x
1
;:::;x
i

1
))
:
ForPixelIQN,theinputisthetrainingimage
x
andasample
point
˝
˘U
([0
;
1]
3
n
2
)
.Theoutputvalues
Q
x
(
˝
)
2
R
3
n
2
areinterpretedastheapproximatequantilefunctionat
˝
,
Q
x
(
˝
)
i
=
Q
X
(
˝
i
j
x
i

1
;:::
)
,trainedwithasinglestepof
quantileregressiontowardstheobservedsample
x
:
X
i
ˆ

˝
i
(
x
i

Q
X
(
˝
i
j
x
i

1
;:::
))
:
4.1.CIFAR-10
WebeginbydemonstratingPixelIQNonCIFAR-10
(
Krizhevsky&Hinton
,
2009
).Forcomparison,wetrain
bothabaselineGatedPixelCNNandaPixelIQN.Bothmod-
elscorrespondtothe
15
-layernetworkvariantin(
vanden
Oordetal.
,
2016b
),seeAppendixfordetailedhyperpa-
rametersandtrainingprocedure.Thetwomethodshave
substantiallydifferentlossfunctions,soweperformeda
3
Conditioningonlabelsremainspossible(seeSection
4.2
).
AutoregressiveQuantileNetworksforGenerativeModeling
Figure3.
CIFAR-10:Realexampleimages(left),samplesgeneratedbyPixelCNN(center),andsamplesgeneratedbyPixelIQN(right).
Figure4.
EvaluationsbyInceptionscore(higherisbetter)andFID
(lowerisbetter)onCIFAR-10andImageNet32x32.Dottedlines
correspondtomodelstrainedwithclass-labelconditioning.
CIFAR-10
ImageNet(32x32)
Method
Inception
FID
Inception
FID
WGAN
3.82
-
-
-
WGAN-GP
6.5
36.4
-
-
DC-GAN
6.4
37.11
7.89
-
PixelCNN
4.60
65.93
7.16
40.51
PixelIQN
5.29
49.46
8.68
26.56
PixelIQN(l)
-
-
7.29
37.62
PixelCNN

-
-
8.33
33.27
PixelIQN

-
-
10.18
22.99
Table1.
InceptionscoreandFIDforCIFAR-10andImageNet.
WGANandDC-GANresultstakenfrom(
Arjovskyetal.
,
2017
;
Radfordetal.
,
2015
).PixelIQN(l)isthesmall15-layerversionof
themodel.Modelsmarked

refertoclass-conditionaltraining.
hyperparametersearchusingashorttrainingrun,withthe
samenumber(
500
)ofhyperparameterevalu-
atedforbothmodels.Forallresults,wereportfulltraining
runsusingthebestfoundhyperparametersineachcase.The
evaluationmetricusedforthehyperparametersearchwas
theFr
´
echetInceptionDistance(FID)(
Heuseletal.
,
2017
),
seeAppendixfordetails.InadditiontoFID,wereport
Inceptionscore(
Salimansetal.
,
2016
)forbothmodels.
Figure
4
(left)showsInceptionscoreandFIDforbothmod-
elsevaluatedatseveralpointsthroughouttraining.Thefully
trainedPixelCNNachievesanInceptionscoreandFIDof
4
:
6
and
65
:
9
respectively,whilePixelIQNsubstantiallyout-
performsitwithanInceptionscoreof
5
:
3
andFIDof
49
:
5
.
Thisalsocomparesfavorablywithe.g.WGAN(
Arjovsky
etal.
,
2017
),whichreachesanInceptionscoreof
3
:
8
.For
subjectiveevaluations,wegivesamplesfrombothmodels
inFigure
3
.SamplescomingfromPixelIQNaremuchmore
visuallycoherent.Ofnote,thePixelIQNmodelachieves
aperformancelevelcomparabletothatofthefullytrained
PixelCNNwithonlyaboutonethirdthenumberoftraining
updates(andaboutonethirdofthewall-clocktime).
4.2.ImageNet32x32
Next,weturntothesmallImageNetdataset(
Russakovsky
etal.
,
2015
),rstusedforgenerativemodelinginthePixel-
RNNwork(
vandenOordetal.
,
2016c
).Again,weevaluate
usingFIDandInceptionscore.Forthismuchharderdataset,
webaseourPixelCNNandPixelIQNmodelsonthelarger
20
-layervariantusedin(
vandenOordetal.
,
2016b
).Dueto
substantiallylongertrainingtimeforthismodel,wedidnot
performadditionalhyperparametertuning,andmostlyused
thesamehyperparametervaluesasintheprevioussections
forbothmodels;detailscanbefoundintheAppendix.
Figure
4
showsInceptionscoreandFIDthroughouttrain-
ingofPixelCNNandPixelIQN.Again,PixelIQNsubstan-
AutoregressiveQuantileNetworksforGenerativeModeling
Figure5.
ImageNet32x32:Realexampleimages(left),samplesgeneratedbyPixelCNN(center),andsamplesgeneratedbyPixelIQN
(right).Neitherofthesampledimagesetswerecherry-picked.MoresamplesbyPixelIQNintheAppendix.
tiallyoutperformsthebaselineintermsofperformance
andsamplecomplexity.Forscoresandacompari-
sontostate-of-the-artGANmodels,seeTable
1
.Figure
5
showsrandom(non-cherry-picked)samplesfrombothmod-
els.ComparedtoPixelCNN,PixelIQNsamplesappearto
havesuperiorqualitywithmoreglobalconsistencyandless
`high-frequencynoise'.
InFigure
6
,weshowtheinpaintingperformanceofPix-
elIQN,bythetophalfofavalidationsetimageas
inputandsamplingrepeatedlyfromthemodeltogenerate
differentcompletions.Wenotethatthemodelconsistently
generatesplausiblecompletionswithdiversity
betweendifferentcompletionsamplesforthesameinput
image.Meanwhile,WGAN-GPhasbeenseentoproduce
deterministiccompletions(
Bellemareetal.
,
2017
).
Following(
vandenOordetal.
,
2016b
),wealsotraineda
class-conditionalPixelIQNvariant,providingtothemodel
theone-hotclasslabelcorrespondingtoatrainingimage(in
additiontoa
˝
sample).Samplesfromaclass-conditional
modelcanbeexpectedtohavehighervisualquality,asthe
classlabelprovides
log
2
(1000)
ˇ
10
bitsofinformation,
seeFigure
7
.AsseeninFigure
4
andTable
1
,classcondi-
tioningalsofurtherimprovesInceptionscoreandFID.To
generateeachsampleforthecomputationofthesescores,
wesampleoneof1000classlabelsrandomly,thengenerate
animageconditionedonthislabelviathetrainedmodel.
Finally,motivatedbytheverylongtrainingtimeforthelarge
PixelCNNmodel(approximately1dayper100Ktraining
steps,on16NVIDIATeslaP100GPUs),wealsotrained
smaller
15
-layerversionsofthemodels(sameastheones
usedonCIFAR-10)onthesmallImageNetdataset.For
comparison,thesetakeapproximately12hoursfor100K
trainingstepsonasingleP100GPU,orlessthan3hourson
8P100GPUs.Asexpected,littlePixelCNN,whilesuitable
Figure6.
SmallImageNetinpaintingexamples.Leftimageisthe
inputprovidedtothenetworkatthebeginningofsampling,rightis
theoriginalimage,columnsinbetweenshowdifferentcompletions.
MoreexamplesintheAppendix.
fortheCIFAR-10dataset,failstoachievecompetitivescores
ontheImageNetdataset,achievingInceptionscore
5
:
1
and
FID
66
:
4
.Astonishingly,littlePixelIQNonthisdataset
reachesInceptionscore
7
:
3
andFID
38
:
5
,seeFigure
4
(right).IttherebynotonlyoutperformsthelittlePixelCNN,
butalsothelarger
20
-layerversion!Thisstronglysupports
thehypothesisthatPixelCNN,andpotentiallymanyother
models,areconstrainednotonlybytheirmodelcapacity,
butcruciallyalsobythesub-optimaltrade-offsmadeby
theirlog-likelihoodtrainingcriterion,failingtoalignwith
perceptualorevaluationmetrics.
5.DiscussionandConclusions
Mostexistinggenerativemodelsforimagesbelongtoone
oftwoclasses.Thearelikelihood-basedmodels,
trainedwithanelementwiseKLreconstructionloss,which,
AutoregressiveQuantileNetworksforGenerativeModeling
Figure7.
Class-conditionalsamplesfromPixelIQN.Moresamples
ofeachclassandmoreclassesintheAppendix.
whileperceptuallymeaningless,providesrobustoptimiza-
tionpropertiesandhighsamplediversity.Thesecondare
GANs,trainedbasedonadiscriminatorloss,typicallybetter
alignedwithaperceptualmetricandenablingthegenerator
toproducerealistic,globallyconsistentsamples.Theirad-
vantagescomeatthecostofaharderoptimizationproblem,
highparametersensitivity,andmostimportantly,atendency
tocollapsemodesofthedatadistribution.
AIQNsareanew,fundamentallydifferent,techniquefor
generativemodeling.Byusingaquantileregressionloss
insteadofKLdivergence,theycombinesomeofthebest
propertiesofthetwomodelclasses.Bytheirnature,they
preservemodesofthelearneddistribution,whileproducing
perceptuallyappealinghigh-qualitysamples.Theinevitable
approximationtrade-offsagenerativemodelmakeswhen
constrainedbycapacityorinsuftrainingcanvary
dependingonthelossused.Wearguethatthe
proposedquantileregressionlossalignsmoreeffectively
withagivenmetricandthereforemakessubjectivelymore
advantageoustrade-offs.
Devisingmethodsforquantileregressionovermultidimen-
sionaloutputsisanactiveareaofresearch.Newmethods
arecontinuingtobeinvestigated(
Carlieretal.
,
2016
;
Hallin
&Miroslav
,
2016
),andapromisingdirectionforfuture
workistowaystousethesetoreplaceautoregressive
models.Oneapproachtoreducingthecomputationalburden
ofsuchmodelsistoapplyAIQNtothelatentdimensions
ofaVAE.Similarinspiritto
Roscaetal.
(
2017
),thiswould
usetheVAEtoreducethedimensionalityoftheproblem
andtheAIQNtosamplefromthetruelatentdistribution.
IntheAppendixwegivepreliminaryresultsusingsuchan
technique,onCelebA
64

64
(
Liuetal.
,
2015
).
WehaveshownthatIQN,computationallycheapandtechni-
callysimple,canbereadilyappliedtoexistingarchitectures,
PixelCNNandVAE(Appendix),improvingrobustnessand
samplingqualityoftheunderlyingmodel.Wedemonstrated
thatPixelIQNproducesmorerealistic,globallycoherent
samples,andimprovesInceptionscoreandFID.
Wefurtherpointoutthatmanyrecentadvancesingenera-
tivemodelscouldbeeasilycombinedwithourproposed
method.RecentalgorithmicimprovementstoGANssuch
asmini-batchdiscriminationandprogressivegrowing(
Sal-
imansetal.
,
2016
;
Karrasetal.
,
2017
),whilenotstrictly
necessaryinourwork,couldbeappliedtofurtherimprove
performance.PixelCNN++(
Salimansetal.
,
2017
)isan
architecturalimprovementofPixelCNN,withseveralben-
supportedbyexperimentalevidence.
AlthoughwehavebuiltupontheoriginalGatedPixelCNN
inthiswork,webelieveallofthesetobe
compatiblewithourwork,exceptfortheuseofamixture
oflogisticsinplaceofPixelCNN'ssoftmax.Aswehave
entirelyreplacedthismodelcomponent,thischangedoes
notmapontoourmodel.Ofnote,themotivationbehind
thischangecloselymirrorsourown,inlookingforaloss
thatrespectstheunderlyingmetricbetweenexamples.The
recentPixelSNAILmodel(
Chenetal.
,
2017
)achievesstate-
of-the-artmodelingperformancebyenhancingPixelCNN
withELUnonlinearities,blockstructure,andanat-
tentionmechanism.Again,allofthesearefullycompatible
withourworkandshouldimproveresultsfurther.
Finally,theimplicitquantileformulationliftsanumberofar-
chitecturalrestrictionsofpreviousgenerativemodels.Most
importantly,thereparameterizationasaninversec.d.f.al-
lowstolearndistributionsovercontinuousrangeswithout
boundariesorquantization.Thisenablesmod-
elingcontinuous-valuedvariables,forexampleforgenera-
tionofsound(
vandenOordetal.
,
2016a
),openingmultiple
interestingavenuesforfurtherinvestigation.
Acknowledgements
Wewouldliketoacknowledgetheimportantrolemany
ofourcolleaguesatDeepMindplayedforthiswork.We
especiallythankA
¨
aronvandenOordandSanderDiele-
manforinvaluableadviceonthePixelCNNmodel;Ivo
DanihelkaandDaniloJ.Rezendeforcarefulreadingand
insightfulcommentsonanearlierversionofthepaper;Igor
Babuschkin,AlexandreGalashov,DominikGrewe,Jacob
Menick,andMihaelaRoscafortechnicalhelp.
AutoregressiveQuantileNetworksforGenerativeModeling
References
Arjovsky,M.,Chintala,S.,andBottou,L.Wassersteingen-
erativeadversarialnetworks.In
Proceedingsofthe34th
InternationalConferenceonMachineLearning(ICML)
,
2017.
Bellemare,M.G.,Danihelka,I.,Dabney,W.,Mohamed,S.,
Lakshminarayanan,B.,Hoyer,S.,andMunos,R.The
CramerdistanceasasolutiontobiasedWassersteingra-
dients.
arXivpreprintarXiv:1705.10743
,2017.
Bousquet,O.,Gelly,S.,Tolstikhin,I.,Simon-Gabriel,C.-J.,
andSchoelkopf,B.Fromoptimaltransporttogener-
ativemodeling:thevegancookbook.
arXivpreprint
arXiv:1705.07642
,2017.
Carlier,G.,Chernozhukov,V.,andGalichon,A.Vector
quantileregression:anoptimaltransportapproach.
An-
nalsofStatistics
,44(3):1165Œ1192,062016.
Chen,X.,Duan,Y.,Houthooft,R.,Schulman,J.,Sutskever,
I.,andAbbeel,P.Infogan:Interpretablerepresentation
learningbyinformationmaximizinggenerativeadversar-
ialnets.In
AdvancesinNeuralInformationProcessing
Systems
,pp.2172Œ2180,2016.
Chen,X.,Mishra,N.,Rohaninejad,M.,andAbbeel,P.Pix-
elSNAIL:Animprovedautoregressivegenerativemodel.
arXivpreprintarXiv:1712.09763
,2017.
Dabney,W.,Ostrovski,G.,Silver,D.,andMunos,R.Im-
plicitquantilenetworksfordistributionalreinforcement
learning.
Proceedingsofthe34thInternationalConfer-
enceonMachineLearning(ICML)
,2018a.
Dabney,W.,Rowland,M.,Bellemare,M.G.,andMunos,
R.Distributionalreinforcementlearningwithquantile
regression.In
ProceedingsoftheAAAIConferenceon
Intelligence
,2018b.
Daskalakis,C.,Ilyas,A.,Syrgkanis,V.,andZeng,
H.TrainingGANswithoptimism.
arXivpreprint
arXiv:1711.00141
,2017.
Dhaene,J.,Vanduffel,S.,Goovaerts,M.J.,Kaas,R.,Tang,
Q.,andVyncke,D.Riskmeasuresandcomonotonicity:
areview.
StochasticModels
,22(4):573Œ606,2006.
Dumoulin,V.,Belghazi,I.,Poole,B.,Lamb,A.,Arjovsky,
M.,Mastropietro,O.,andCourville,A.Adversarially
learnedinference.
arXivpreprintarXiv:1606.00704
,
2016.
Genevay,A.,Peyr
´
e,G.,andCuturi,M.GANandVAE
fromanoptimaltransportpointofview.
arXivpreprint
arXiv:1706.01807
,2017.
Germain,M.,Gregor,K.,Murray,I.,andLarochelle,H.
MADE:Maskedautoencoderfordistributionestimation.
In
InternationalConferenceonMachineLearning
,pp.
881Œ889,2015.
Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,
Warde-Farley,D.,Ozair,S.,Courville,A.,andBengio,
Y.Generativeadversarialnets.In
AdvancesinNeural
InformationProcessingSystems
,pp.2672Œ2680,2014.
Gulrajani,I.,Ahmed,F.,Arjovsky,M.,Dumoulin,V.,and
Courville,A.C.ImprovedtrainingofWassersteinGANs.
In
AdvancesinNeuralInformationProcessingSystems
,
pp.5769Œ5779,2017.
Hallin,M.andMiroslav,

S.Multiple-outputquantileregres-
sion.
HandbookofQuantileRegression
,pp.185Œ208,
2016.
Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,and
Hochreiter,S.GANstrainedbyatwotime-scaleupdate
ruleconvergetoalocalnashequilibrium.In
Advancesin
NeuralInformationProcessingSystems
,pp.6629Œ6640,
2017.
Higgins,I.,Matthey,L.,Pal,A.,Burgess,C.,Glorot,X.,
Botvinick,M.,Mohamed,S.,andLerchner,A.Beta-VAE:
Learningbasicvisualconceptswithaconstrainedvari-
ationalframework.In
ProceedingsoftheInternational
ConferenceonLearningRepresentations(ICLR)
,2017.
Huber,P.J.Robustestimationofalocationparameter.
AnnalsofMathematicalStatistics
,35(1):73Œ101,1964.
Jones,M.C.Estimatingdensities,quantiles,quantileden-
sitiesanddensityquantiles.
AnnalsoftheInstituteof
StatisticalMathematics
,44(4):721Œ727,1992.
Karras,T.,Aila,T.,Laine,S.,andLehtinen,J.Progressive
growingofGANsforimprovedquality,stability,and
variation.
arXivpreprintarXiv:1710.10196
,2017.
Kingma,D.P.andWelling,M.Auto-encodingvariational
bayes.
arXivpreprintarXiv:1312.6114
,2013.
Koenker,R.intervalsforregressionquantiles.
In
AsymptoticStatistics
,pp.349Œ359.Springer,1994.
Koenker,R.andHallock,K.Quantileregression:anintro-
duction.
JournalofEconomicPerspectives
,15(4):43Œ56,
2001.
Koenker,R.andXiao,Z.Quantileautoregression.
Journal
oftheAmericanStatisticalAssociation
,101(475):980Œ
990,2006.
Krizhevsky,A.andHinton,G.Learningmultiplelayersof
featuresfromtinyimages.Technicalreport,2009.
AutoregressiveQuantileNetworksforGenerativeModeling
Liu,Z.,Luo,P.,Wang,X.,andTang,X.Deeplearningface
attributesinthewild.In
ProceedingsofInternational
ConferenceonComputerVision(ICCV)
,2015.
Makhzani,A.,Shlens,J.,Jaitly,N.,Goodfellow,I.,and
Frey,B.Adversarialautoencoders.
arXivpreprint
arXiv:1511.05644
,2015.
Papamakarios,G.,Murray,I.,andPavlakou,T.Masked
autoregressiveowfordensityestimation.In
Advancesin
NeuralInformationProcessingSystems
,pp.2335Œ2344,
2017.
Parzen,E.Nonparametricstatisticaldatamodeling.
Journal
oftheAmericanStatisticalAssociation
,74(365):105Œ121,
1979.
Polyak,B.T.andJuditsky,A.B.Accelerationofstochastic
approximationbyaveraging.
SIAMJournalonControl
andOptimization
,30(4):838Œ855,1992.
Radford,A.,Metz,L.,andChintala,S.Unsupervisedrep-
resentationlearningwithdeepconvolutionalgenerative
adversarialnetworks.
arXivpreprintarXiv:1511.06434
,
2015.
Rezende,D.J.,Mohamed,S.,andWierstra,D.Stochastic
backpropagationandapproximateinferenceindeepgen-
erativemodels.In
InternationalConferenceonMachine
Learning
,pp.1278Œ1286,2014.
Rosca,M.,Lakshminarayanan,B.,Warde-Farley,D.,
andMohamed,S.Variationalapproachesforauto-
encodinggenerativeadversarialnetworks.
arXivpreprint
arXiv:1706.04987
,2017.
Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,
Ma,S.,Huang,Z.,Karpathy,A.,Khosla,A.,Bernstein,
M.,etal.Imagenetlargescalevisualrecognitionchal-
lenge.
InternationalJournalofComputerVision
,115(3):
211Œ252,2015.
Salimans,T.,Goodfellow,I.,Zaremba,W.,Cheung,V.,Rad-
ford,A.,andChen,X.Improvedtechniquesfortraining
GANs.In
AdvancesinNeuralInformationProcessing
Systems
,pp.2234Œ2242,2016.
Salimans,T.,Karpathy,A.,Chen,X.,andKingma,D.P.
PixelCNN++:Improvingthepixelcnnwithdiscretized
logisticmixturelikelihoodandother
arXiv
preprintarXiv:1701.05517
,2017.
Salimans,T.,Zhang,H.,Radford,A.,andMetaxas,D.Im-
provinggansusingoptimaltransport.In
Proceedings
oftheInternationalConferenceonLearningRepresenta-
tions(ICLR)
,2018.
Siddiqui,M.M.Distributionofquantilesinsamplesfrom
abivariatepopulation.
J.Res.Nat.Bur.StandardsB
,64:
145Œ150,1960.
Theis,L.,vandenOord,A.,andBethge,M.Anote
ontheevaluationofgenerativemodels.
arXivpreprint
arXiv:1511.01844
,2015.
Tukey,J.W.Whichpartofthesamplecontainstheinforma-
tion?
ProceedingsoftheNationalAcademyofSciences
,
53(1):127Œ134,1965.
vandenOord,A.,Dieleman,S.,Zen,H.,Simonyan,K.,
Vinyals,O.,Graves,A.,Kalchbrenner,N.,Senior,A.,and
Kavukcuoglu,K.Wavenet:Agenerativemodelforraw
audio.
arXivpreprintarXiv:1609.03499
,2016a.
vandenOord,A.,Kalchbrenner,N.,Espeholt,L.,Vinyals,
O.,Graves,A.,andKavukcuoglu,K.Conditionalimage
generationwithPixelCNNdecoders.In
Advancesin
NeuralInformationProcessingSystems
,pp.4790Œ4798,
2016b.
vandenOord,A.,Kalchbrenner,N.,andKavukcuoglu,K.
Pixelrecurrentneuralnetworks.In
Proceedingsofthe
InternationalConferenceonMachineLearning
,2016c.
vandenOord,A.,Vinyals,O.,andKavukcuoglu,K.Neural
discreterepresentationlearning.In
AdvancesinNeural
InformationProcessingSystems
,pp.6309Œ6318,2017.
Zhao,S.,Song,J.,andErmon,S.Towardsdeeperun-
derstandingofvariationalautoencodingmodels.
arXiv
preprintarXiv:1702.08658
,2017.
AutoregressiveQuantileNetworksforGenerativeModeling
Appendix
Quantileregressionminimizesthequantile
divergence
Proposition1.
Foranydistributions
P
and
Q
,the
quantiledivergence
q
(
P;Q
):=
Z
1
0
""
Z
F

1
Q
(
˝
)
F

1
P
(
˝
)
(
F
P
(
x
)

˝
)
dx
#
d˝:
Thentheexpectedquantilelossofaquantilefunction

Q
implicitlythedistribution
Q

E
˝
˘U
([0
;
1])
E
X
˘
P

ˆ
˝
(
X


Q
(
˝
))

=
q
(
P;Q
)+
h
(
P
)
;
where
h
(
P
)
doesnotdependon
Q
.
Proof.
Let
P
beadistributionwithp.d.f.
f
P
andc.d.f.
F
P
.

ˆ
˝
(
u
)=
u
(
˝

I
f
u

0
g
)
;
g
˝
(
q
)=
E
X
˘
P
[
ˆ
˝
(
X

q
)]
:
Wehave,forany
q
and
˝
,
g
˝
(
q
)=
Z
q

(
x

q
)(
˝

1)
f
P
(
x
)
dx
+
Z
1
q
(
x

q
)
˝f
P
(
x
)
dx
=
Z
q

(
q

x
)
f
P
(
x
)
dx
+
Z
1

(
x

q
)
˝f
P
(
x
)
dx
=
qF
P
(
q
)+
Z
q

F
P
(
x
)
dx

[
xF
P
(
x
)]
q

+
˝

E
X
˘
P
[
X
]

q

=
Z
q

F
P
(
x
)
dx
+
˝

E
X
˘
P
[
X
]

q

;
wherethethirdequalityfollowsfromanintegrationby
partsof
R
q

xf
P
(
x
)
dx
.Thusthefunction
q
7!
g
˝
(
q
)
isminimizedfor
q
=
F

1
P
(
˝
)
anditsminimumis
g
˝
(
F

1
P
(
˝
))=
Z
F

1
P
(
˝
)

F
P
(
x
)
dx
+
˝

E
X
˘
P
[
X
]

F

1
P
(
˝
)

:
Wededucethat
g
˝
(
q
)

g
˝
(
F

1
P
(
˝
))
=
Z
q
F

1
P
(
˝
)
F
P
(
x
)
dx
+
˝
(
F

1
P
(
˝
)

q
)
=
Z
q
F

1
P
(
˝
)
(
F
P
(
x
)

˝
)
dx:
Figure8.
Illustrationoftherelationbetweenthe
1
-Wasserstein
metric(red)andthequantiledivergence(blue).
Thusforaquantilefunction

Q
,wehavetheexpectedquan-
tileloss:
E
˝
˘U
([0
;
1])

g
˝
(
Q
(
˝
))

=
q
(
P;Q
)+
E
˝
˘U
([0
;
1])

g
˝
(
F

1
P
(
˝
))

|
{z
}
doesnotdependon
Q
:
Thistheproofoftheproposition.
Weobservethatquantileregressionisnothingelsethan
aprojectionunderthequantiledivergence.Thusfora
parametrizedquantilefunction

Q

withcorrespondingdis-
tribution
Q

,thesample-basedquantileregressiongradient
r

ˆ
˝
(
X


Q

(
˝
))
forasample
˝
˘U
([0
;
1])
and
X
˘
P
isanunbiasedestimateof
r

q
(
P;Q

)
:
E

r

ˆ
˝
(
X


Q

(
˝
))

=
r

E
˝
˘U
([0
;
1])

g
˝
(

Q

(
˝
))

;
=
r

q
(
P;Q

)
:
Weillustratetherelationbetweenthe
1
-Wassersteinmetric
andthequantiledivergenceinFigure
8
.Noticethat,for
each
˝
2
[0
;
1]
,whiletheWassersteinmeasurestheerror
betweenthetwoquantilefunctions,thequantiledivergence
measuresasubsetoftheareaenclosedbetweentheirgraphs.
NetworkandTrainingDetails
AllPixelCNNandPixelIQNmodelsinSection
4
aredirectly
basedonthesmallandlargeconditionalGatedPixelCNN
modelsdevelopedin(
vandenOordetal.
,
2016b
).For
CIFAR-10(Section
4.1
),weareusingthesmallervariant
with
15
layerblocks,convolutionalofsize
5
,
128
featureplanesineachlayerblock,and
1024
featuresplanes
fortheresidualconnectionsfeedingintotheoutputlayerof
thenetwork.ForsmallImageNet(Section
4.2
),weuseboth
thismodel,andalarger
20
layerversionwith
256
feature
planesineachlayerblock.
AutoregressiveQuantileNetworksforGenerativeModeling
ForPixelIQN,werescalethe
˝
2
[0
;
1]
3
n
2
linearlytoliein
[

1
;
1]
3
n
2
,andinputittothenetworkinexactlythesame
wayasthelocation-dependentconditioningin(
vanden
Oordetal.
,
2016b
),thatis,byapplyinga
1

1
convolu-
tionproducingthesamenumberoffeatureplanesasinthe
respectivelayerblock,andaddingittotheoutputofthis
blockpriortothegatingactivation.
AllmodelsonCIFAR-10weretrainedforatotalof
300
K
steps,thoseonImageNetfor
400
Ksteps.Wetrainedthe
smallmodelswithamini-batchsizeof
32
,runningapprox-
imately
200
KupdatesperdayonasingleNVIDIATesla
P100GPU,whilethelargermodelsweretrainedwitha
mini-batchsizeof
128
withsynchronousupdatesfrom
16
P100GPUs,achievingapproximatelyhalfofthissteprate.
HyperparameterTuningandEvaluation
AllquantitativeevaluationsofourPixelCNNandPixelIQN
modelsarebasedontheFr
´
echetInceptionDistance(FID)
(
Heuseletal.
,
2017
),
d
(
x
1
;x
2
)=
k

1


2
k
2
+
Tr

1
+
2


1

2
)
1
=
2
)
;
where
(

1
;

1
)
arethemeanandcovarianceof
10
;
000
sam-
plesfromthemodel(PixelCNNorPixelIQN),and
(

2
;

2
)
arethemeanandcovariancematrixcomputedoverasetof
10
;
000
trainingdatapoints.Weslightlydeviatefromthe
usualpracticeofusingtheentiretrainingsetforFIDcompu-
tation,asthiswouldrequireanequalnumber(
50
;
000
inthe
caseofCIFAR-10)ofsamplestobedrawnfromthemodel,
whichiscomputationallyveryexpensiveforautoregressive
modelslikePixelCNNorPixelIQN.
WeusePolyakaveraging(
Polyak&Juditsky
,
1992
),keep-
inganexponentiallyweightedaverageoverpastparameters
withaweightof
0
:
9999
.Thisaverageisbeingloadedin-
steadofthemodelparametersbeforesamplesaregenerated,
butneverusedfortraining.
TotuneoursmallPixelCNNandPixelIQNmodels,weper-
formedahyperparametersearchover
500
hyperparameter
foreachmodel,eachevaluated
after
100
KtrainingstepsonCIFAR-10,basedonitsFID
scorecomputedonasmallsetof
2500
generatedsamples.
ForPixelCNN,theparametersearchinvolvedchoosingfrom
RMSProp,Adam,andSGDastheoptimizer,andtuningthe
learningrate,involvingbothconstantanddecayinglearning
rateschedules.AsaresultwesettledontheRMSProp
optimizerandasetofthreepossiblelearningrateregimes,
namelyaconstantlearningrateof
10

4
or
3

10

5
,anda
decayinglearningrateregime:
10

4
inthe
120
K,
3

10

5
forthenext
60
K,and
10

5
fortheremainingtraining
steps.WefoundtheofthesetoworkbestonImageNet,
andthedecayingscheduletoworkbestonCIFAR-10,and
onlyreportthebestmodelforeachdataset.
ForPixelIQN,theparametersearchincludedtheabove(but
withconstantlearningratesonly),andadditionallyasweep
overarangeofvaluesfortheHuberlossparameter

(Equa-
tion
2
).Asaresult,weusedAdamwithaconstantlearn-
ingrateof
10

4
forallPixelIQNmodelvariantsonboth
datasets,andset

=0
:
002
.Wefoundthatthemodelis
notsensitivetothishyperparameter,butperformssomewhat
worseiftheregularquantileregressionlossisusedinstead
oftheHubervariant.
AIQN-VAE
OnepotentialdrawbacktoPixelIQNpresentedabove,
sharedbyPixelCNNandmoregenerallyautoregressive
models,isthatduetotheirautoregressivenaturesampling
canbeextremelytime-consuming.Thisisespeciallytrueas
theresolutionofimagesincreases.Althoughitispossible
topartiallyreducethisoverheadwithcleverengineering,
thesemodelsareinherentlymuchslowertosamplefrom
thanmodelssuchasGANsandVAEs.Inthissection,we
demonstratehowPixelIQN,duetothecontinuousnatureof
thequantilefunction,canbeusedtolearndistributionsover
lower-dimensional,latentspaces,suchasthoseproducedby
anautoencoder,variationalorotherwise.,we
useastandardVAE,butsimultaneouslytrainasmallAIQN
tomodelthetrainingdistributionoverlatentcodes.Forsam-
pling,wethengeneratesamplesofthelatentdistribution
usingAIQNinsteadoftheVAEprior.
Thisapproachworkswellfortworeasons.First,evena
thoroughlytrainedVAEdoesnotproduceanencoderthat
fullymatchestheGaussianprior.Generaly,thedatadis-
tributionexistsonanon-Gaussianmanifoldinthelatent
space,despitetheuseofvariationaltraining.Second,un-
likeexistingmethods,AIQNlearnstoapproximatethefull
continuous-valueddistributionwithoutdiscretizingvalues
ormakingpriorassumptionsaboutthevaluerangeorunder-
lyingdistribution.
Wecanseesimilaritiesbetweenthisapproachandtwoother
recentpublications.First,the

-GANproposedby
Rosca
etal.
(
2017
).Inboth,thereisanattempttosamplefromthe
truelatentdistributionofaVAE-likelatentvariablemodel.
Inthecaseof

-GANthissamplingdistributionistrained
usingaGAN,whileweproposetolearnthedistribution
usingquantileregression.Thesimilaritymakessensecon-
sideringAIQNsharessomeoftheofGANs.Unlike
inthisrelatedwork,wehavenotreplacedtheKLpenalty
onthelatentrepresentation.Itwouldbeaninterestingdi-
rectionforfutureresearchtoexploreasimilarformulation.
Generally,thesametrade-offsbetweenGANsandAIQN
shouldbeexpectedtocomeintoplayherejustastheydo
whenlearningimagedistributions.Second,theVQ-VAE
model(
vandenOordetal.
,
2017
),learnsaPixelCNNmodel
ofthe(discretized)latentspace.Here,especiallyinthela-
AutoregressiveQuantileNetworksforGenerativeModeling
Figure9.
CelebA64x64:Realexampleimages(left),samplesgeneratedbyVAE(center),andsamplesgeneratedbyAIQN-VAE(right).
tentspace,distributionlossesrespectingdistancesbetween
individualpointsismoreapplicablethanlikelihood-based
losses.
Let
e
:
R
n
!
R
m
and
d
:
R
m
!
R
n
bethemeanofthe
encoderanddecoderrespectivelyofaVAE,althoughother
formsofautoencodercouldbesubstituted.Then,let
Q
˝
be
anAIQNonthespace
R
m
.Duringtrainingweproposeto
minimize
L
(
x
)=
L
VAE
(
x
)+
E
˝
˘U
([0
;
1]
m
)
ˆ

˝
(
e
(
x
)

Q
˝
)
;
where
L
VAE
isthestandardVAElossfunction.Then,for
generation,wesample
˝
˘U
([0
;
1]
m
)
,andreparameterize
thissamplethroughtheAIQNandthedecodertoproduce
y
=
d
(
Q
˝
)
,asamplefromtheapproximateddistribution.
WecallthissimplecombinationtheAIQN-VAE.
CelebA
WedemonstratetheAIQN-VAEusingtheCelebAdataset
(
Liuetal.
,
2015
),atresolution
64

64
.Wean
opensourceVAEimplementation
4
tosimultaneouslytrain
theAIQNontheoutputoftheVAEencoder,withPolyak
averaging(
Polyak&Juditsky
,
1992
)oftheAIQNweights.
Wereducethelatentdimensionto
32
,asourpurposeisto
investigatetheuseofVAEstolearninlower-dimensional
latentspaces.TheAIQNusedthreefullyconnectedlayers
ofwidth
512
withReLUactivations.FortheAIQN-VAE,
butnottheVAE,weloweredlatentdimensionvarianceto
0
:
1
andtheKL-termweightto
0
:
5
.Ithasbeenobserved
thatinthissettingtheVAEprioralonewillproducepoor
samples,thushigh-qualitysampleswillonlybepossible
bylearningthelatentdistribution.Figure
9
showssamples
frombothaVAEandAIQN-VAEafter
200
K
trainingitera-
tions.Bothmodelsmaybeexpectedtoimprovewithfurther
4
https://github.com/LynnHo/VAE-Tw
training,however,wecanseethattheAIQN-VAEsamples
arefrequentlyclearerandlessblurrythanthosefromthe
VAE.
AutoregressiveQuantileNetworksforGenerativeModeling
Figure10.
SamplesfromPixelIQNtrainedonsmallImageNet.
AutoregressiveQuantileNetworksforGenerativeModeling
Figure11.
Class-conditionalsamplesfromPixelIQNtrainedonsmallImageNet.
AutoregressiveQuantileNetworksforGenerativeModeling
Figure12.
Inpainting.Leftcolumn:Maskedimagegiventothenetwork.Middlecolumns:alternativeimagecompletionsbythePixelIQN
networkfordifferentvaluesof
˝
.Rightcolumn:Originalimage.
"
91,Weakly-Supervised Learning for Tool Localization in Laparoscopic Videos,http://arxiv.org/pdf/1806.05573v2.pdf,https://github.com/CAMMA-public/ai4surgery,"Weakly-SupervisedLearningforTool
LocalizationinLaparoscopicVideos
ArmineVardazaryan
1
,DidierMutter
2
,JacquesMarescaux
2
,and
NicolasPadoy
1
1
ICube,UniversityofStrasbourg,CNRS,IHUStrasbourg,France
f
vardazaryan,npadoy
g
@unistra.fr
2
UniversityHospitalofStrasbourg,IRCAD,IHUStrasbourg,France
Abstract.
Surgicaltoollocalizationisanessentialtaskfortheauto-
maticanalysisofendoscopicvideos.Intheliterature,existingmethods
fortoollocalization,trackingandsegmentationrequiretrainingdatathat
isfullyannotated,therebylimitingthesizeofthedatasetsthatcanbe
usedandthegeneralizationoftheapproaches.Inthiswork,wepropose
tocircumventthelackofannotateddatawithweaksupervision.Wepro-
poseadeeparchitecture,trainedsolelyonimagelevelannotations,that
canbeusedforbothtoolpresencedetectionandlocalizationinsurgical
videos.Ourarchitecturereliesonafullyconvolutionalneuralnetwork,
trainedend-to-end,enablingustolocalizesurgicaltoolswithoutexplicit
spatialannotations.Wedemonstratethebofourapproachona
largepublicdataset,
Cholec80
,whichisfullyannotatedwithbinarytool
presenceinformationandofwhich5videoshavebeenfullyannotated
withboundingboxesandtoolcentersfortheevaluation.
Keywords:
surgicaltoollocalization,endoscopicvideos,weakly-supervised
learning,Cholec80.
1Introduction
Theautomaticanalysisofsurgicalvideosisatthecoreofmanypotentialas-
sistancesystemsfortheoperatingroom.Thelocalizationofsurgicaltools,in
particular,isrequiredinmanyapplications,suchastheanalysisoftool-tissue
interactions,thedevelopmentofnovelhuman-robotassistanceplatformsandthe
automatedannotationofvideodatabases.
Intheliterature,surgicaltoollocalizationhastraditionallybeenapproached
withfullysupervisedmethods[1],withthemostrecentlocalizationandsegmen-
tationmethodsrelyingondeeplearning[4,8,10,11,13].However,trainingfully
supervisedapproachesrequirethedatatobefullyannotatedwithspatialin-
formation,whichistediousandexpensive.Thismayexplainwhythedatasets
usedsofarfortoollocalizationaresmall,namelyintheorderofafewthousand
imagesandwithamaximumof5-6sequences,asdescribedintherecentreview
[1].Thisthenlimitstheapplicabilityandgeneralizabilityoftheapproachesthat
canbedeveloped.
arXiv:1806.05573v2  [cs.CV]  18 Jul 20182Vardazaryanetal.
Recently,ithasbeenshownthatwhenaconvolutionalneuralnetworkis
trainedforthetaskoftheconvolutionallayersofthenetwork
learngeneralnotionsaboutthedetectedobjects.Somerecentworkshaveused
thisfacttosuccessfullylocalizeobjectsinimageswithoutexplicitlytraining
forlocalization[12,15,17].Theproposeddeeplearningapproachesdirectlyout-
putspatialheatmaps,wherethedetectedpositioncorrespondstothestrongest
activations.Thisisachievedbyreplacingallfullyconnectedlayerswithequiv-
alentconvolutionsorremovingthemaltogether.Theresultingarchitecturesare
calledfullyconvolutionalnetworks(FCNs).Othershaveextendedthisapproach
toaddressthechallengingtaskofsemanticsegmentationwithweaksupervision
[2,9,14].Inthemedicalcommunityaswell,weaklysupervisedlearning(WSL)
hasbeenappliedtotaskssuchasdetectionofcancerousregionsinmedicalim-
ages[6,7].Alongwiththerecentreleaseoflargepublicsurgicalvideodatasets,
suchas
Cholec80
[16],whichcontains80completecholecystectomyvideosfully
annotatedwithbinarytoolpresenceinformation(
˘
180Kframesintotal),WSL
techniquescanpotentiallyhelpdeveloptoollocalizationmethodsthatcanscale
uptolargerdatasetscontainingmuchmorevariability.
Inthispaper,weproposeamethodfordetectingandlocalizingsurgicaltools.
Itisbasedonweakly-supervisedlearningusingonlyimage-levellabelsanddoes
notrequireanyspatialannotation.Ourcontributionsaretwofold:(1)wepropose
thesurgicaltoollocalizationapproachbasedonweakly-supervisedlearning;
(2)wedemonstrateourapproachonthelargestpublicendoscopicvideodataset
todate,namely
Cholec80
[16].
2Methodology
Inthiswork,wepresentamethodforthelocalizationofsurgicaltoolsinen-
doscopicvideosthatdoesnotrequirespatialannotations.Thisispossiblewith
aFCNarchitecturethatpreservesthespatialinformationandpermitsusto
observeactivationregionswherethetoolisdetected.Therefore,ourmethodad-
dressestwotasks:binarypresenceandtoollocalization,withthe
latterhingingontheformer.
Ourmodeltakesanimageasinputandreturns
C
localizationheatmaps,
where
C
isthenumberoftoolstobedetected.Forourtaskonthe
Cholec80
dataset,
C
=7.Theheatmapsareusedtondvaluesforeachclass
andperformthebinary
2.1NetworkArchitecture
Asbasisforournetwork(illustratedinFigure1),weuseResNet18[5]because
ithasbeenshowntoperformwellonamultitudeoftasks.Sincewewantto
preserverelativespatialinformationthroughoutournetwork,weremovethefully
connectedlayerandaveragepoolingfromtheendofthenetwork.Additionally,
wechangethestrideinthelasttwobanksofResNetfrom2to1pixeltoobtain
localizationmapswithahigherresolution.Notethatreducingthestridesforall
Weakly-SupervisedLearningforToolLocalizationinLaparoscopicVideos3
Fig.1.
OurFCNfortooldetectionconsistsofamoResnetarchitecture,a1

1
convolutionallayerandaspatialpoolinglayer.
bankswoulddramaticallyincreasethedimensionsofintermediatetensorsduring
training,makingitcomputationallyinfeasible.Thesechangeshavethecollective
ofquadruplingtheresolutionoftheoutput.Usingimagesofsize480

854
asinputtothenetwork,weobtainafeaturemaptensorof60

107

512atthe
outputofResNetandaglobalstrideof8.
Then,weconvertthe512featuremapsintolocalizationmapsbyaddinga
convolutionallayerof1

1kernels.Toobtainonemapperclass,wesetthe
numberofinthislayerto
C
.Finally,withpoolingwetransformthese
mapsintoavectorofclass-wisevalues,whichare,inturn,usedfor
thebinaryofthetools.Insteadofusingconventionalmaxpooling,
weusetheextendedspatialpooling(ESP)
s
c
=max
z
c
+

min
z
c
from[2],which
extractsmoredetailsaboutthedetectionoftheobject.Intheequation,
z
c
2
IR
2
isthelocalizationmapforclass
c
and

is0.6asadvisedby[2].
Duringinference,weusetherawlocalizationmapstothepredicted
positionofthetools.First,thelocalizationmapsareresizedtotheoriginal
sizeoftheinputimagewithbilinearinterpolation.Then,thepositionofthe
maximumactivationisconsideredtobethepredictedlocationofthetool.
2.2Training
Beforetrainingon
Cholec80
,theResNetlayersareinitializedfromImageNet
weights.Duringtraining,dataisrandomlyshandbatched,thendata
augmentationisappliedindependentlytoeachimageinabatch.
DataAugmentation
Duringtraining,allimagesinthebatchareaugmented
beforebeinggiventothenetwork.Augmentationincludeshorizontal
randomrotationby+90/-90degrees,aswellasthemaskingprocedureintro-
ducedin[15].Maskingentailsrandomlyreplacingpatchesintheimagewiththe
meanpixelofthetrainset.Thisimprovesthequalityofpredictedlocalization
maps.
4Vardazaryanetal.
Grasper
Bipolar
Hook
Scissors
Clipper
Irrigator
Spec.Bag
Total
Sp.Annot.
4774
379
4313
327
384
332
375
7175
Cholec80
102588
8876
103106
3254
5986
9814
11462
161397
Table1.
Datasetstatistics.(Row1)Numberofframeswhereeachtoolispresent,for
the5spatiallyannotatedvideos.(Row2)Numberofframeswhereeachtoolispresent
inthecomplete
Cholec80
dataset.
Loss
Themodelsaretrainedformulti-labelclascationwithaweightedcross-
entropyloss
L
presentedinEquation1,where
k
c
and
v
c
arerespectivelythe
groundtruthandpredictedtoolpresenceforclass
c
,
˙
isthesigmoidfunction,
and
W
c
istheweightforclass
c
.Weightsareaddedtocounteractthepolarizing
ofclassimbalance.Theweightforeachclassisinverselyproportionalto
thenumberofoccurrencesoftheclassinthetrainset.
L
=
C
X
c
=1

1
N
[
W
c
k
c
log(
˙
(
v
c
))+(1

k
c
)log(1

˙
(
v
c
))](1)
3ExperimentalResults
3.1Setup
Forourexperiments,weusethe
Cholec80
dataset[16]containing80videosof
cholecystectomyprocedures,fullyannotatedwithimage-levelsurgicaltoollabels
forbinarydetection.Ourtraining,validationandtestsetsconsistof40,10and30
videos,respectively.Additionally,forthepurposeofevaluatingtheperformance
ofourlocalizationmethod,ourteamhasfullyannotated5videosfromthetest
setwithboundingboxesandtoolcenters.Thedetailsoftheseannotationsare
presentedinTable1.Theyarealsoillustratedincolumn1ofFigure3.Aspartof
thepreprocessing,werandomlymaskpatchesof30

30bythesesquares
withtheaveragepixelvalueofthetestset.Foreachpatch,theprobabilityof
maskingis0.5.Wetrainalltheevaluatedmodelsfor120epochswithaninitial
learningrateof0.1,whichdecreasesbyafactorof10at[60,100]epochs.That
learningrateisappliedtothenewconvolutionallayer,whilethelayersofResNet
aretrainedwithalearningratesmallerbyafactorof100.Inourlossfunction,
weuseaweightdecayof10

4
.Themodelsweretrainedwiththemomentum
optimizer(momentum

=0
:
9)andbatchsizeof16.
3.2EvaluatedModels
Weevaluateseveralvariantsofthearchitecturepresentedinsection2.1inorder
tocomparethederencesandsearchforthebestperformingguration.
Themodelswedevisedareasfollows:FCN
ESP(M1),FCN
ESP
Msk
(M2),FCN
ESP
MM(M3),FCN
ESP
MM
Msk(M4),FCN
MSP(M5),
Weakly-SupervisedLearningforToolLocalizationinLaparoscopicVideos5
Grasper
Bipolar
Hook
Scissors
Clipper
Irrigator
Spec.bag
mAP
FCN
ESP
96.5
94.9
99.5
51.4
81.4
93.2
93.7
87.2
FCN
ESP
Msk
96.7
95.5
99.6
50.0
82.3
94.3
93.5
87.4
FCN
ESP
MM
96.6
95.0
99.6
50.2
82.8
94.0
93.5
87.4
FCN
ESP
MM
Msk
96.7
94.8
99.5
44.2
81.9
92.9
93.2
86.1
FCN
MSP
96.6
93.9
99.5
49.6
81.6
92.1
92.4
86.5
FCN
MSP
Msk
96.7
94.1
99.5
49.4
83.2
92.7
93.4
87.0
FCN
MSP
MM
96.7
93.8
99.5
50.4
81.8
91.5
92.7
86.6
FCN
MSP
MM
Msk
96.8
94.2
99.6
49.8
83.0
93.3
94.0
87.2
Table2.
Averageprecision(AP)forbinarytoolpresence
FCN
MSP
Msk(M6),FCN
MSP
MM(M7),FCN
MSP
MM
Msk(M8).The
modelsM1-M4usetheESPmethodseeninsection2.1.Toseewhetherthat
spatialpoolingmethodisbweincludedidenticalmodelsthatusemax
pooling(MSP)instead:M5-M8.Similarly,toevaluatethebofmasking
imagesduringtraining,architecturesM2,M4,M6andM8incorporatemasking,
whileM1,M3,M5andM7donot.Finally,modelsM3,M4,M7andM8use
multi-maps[2],describedbelow.
Multi-maps
Ournetworkarchitecturecontainsaconvolutionallayerof7ker-
nels,eachdedicatedtoonetool.Introducedin[2],thenotionofmulti-maps
isbasedonthefollowingidea:insteadofusingasinglekernelforeachclass,
multiplekernelscanbeusedandbefollowedbyclass-wiseaveragingtoobtain
7localizationmaps.Thishelpsthenetworktoextractmoredetailsaboutthe
objectthanwhenasinglefeaturemapisused.Theauthorsof[2]advisetouse8
kernelsperclass.However,sincetheobjectswedetectaretlysimpler
thantheclassesusedin[2],weuseonly4kernelsperclass(28altogether).
3.3
Asmentionedabove,weusethedataset
Cholec80
totestourmethod.Sp
cally,the30videosofthetestsetareusedfortestingtheperfor-
mance.Toquantifytheresults,weuseaverageprecision(AP),whichisas
theareaundertheprecision-recallcurve.Weillustratethecurveforarchitecture
FCN
ESP
MM
MskinFigure2,whereweseethatresultsforscissorsandclip-
perfallbehindtherestofthetools.AsimilarpatterncanbeobservedinTable
2.AllmodelsdetectmosttoolsquitewellwithAPvaluesabove93%.However,
theresultsforscissors(
˘
50%)andclipper(
˘
82%)aretlyworsethan
thoseoftheothertools.Thismaybeduetothefactthatscissorsandclipper
arepresentonlyin2%and4%ofannotations,respectively.Incontrast,hookis
presentin64%ofallannotations(seeTable1,row2).
6Vardazaryanetal.
Fig.2.
Precision-recallcurveforFCN
ESP
MM
Msk(bestseenincolor).
3.4Localization
Withourmethod,weareabletoobtainlocalizationmapsthatcontaininforma-
tionaboutthepositionsofthetoolsintheframe.Multipleclassesoftoolscan
bedetectedinthesameframe.Note,however,thatourapproachisnotdesigned
todetectmultipleinstancesofthesameclass,becauseallinstanceswouldshare
thesamelocalizationmap.Inthiswork,welimitdetectiontoasingleinstance
ofeachtypeoftool,eventhoughmultipleinstancedetectioncould,forexample,
bepossiblewithpost-processingheuristics.
Weevaluatethequalityofthepredictionsbycomparingthemagainstthe
groundtruthboundingboxesthatwehaveannotatedforthatpurpose.Inthe
caseswheremultipleinstancesofthesametoolarepresentintheframe,wepick
theboundingboxclosesttotheprediction.
LocalizationAP
Toevaluatethequalityoflocalization,wecomputeAPas
describedin[6],whichisbasedonametricin[12].Ifthepredictedlo-
cationliesinagroundtruthboundingboxofthesameclass,withatolerance
of8pixels(theglobalstrideofthenetwork),theexampleisconsideredatrue
positive.Otherwise,itisafalsepositive.Takingthatintoaccount,wecompute
precisionandrecallasdescribedin[3],whererecallisastheproportionof
positivepredictions,andprecisionistheproportionoftruepositivesinpositive
predictions.APisthencomputedastheareaundertheprecision-recallcurve.
Forthisevaluation,weuseonlythepositiveclassesasthenegativeclasscorre-
spondstohavingnotoolintheimageandcannotbeannotatedwithabounding
box.TheresultsofthiscomputationarepresentedinTable3.Thelocalization
APvaluesforallmodelsaresimilar,rangingapproximatelybetween87%and
89%.Ourintuitionisthatallmodelsarealmostequallylikelytopredictatool
centerthatliesintheboundingbox,withoutcapturingthequalityoftheprecise
locationinsidetheboundingbox.Inthenextsection,wequantifytheaccuracy
ofthepredictedtoolcentersrelativetogroundtruth.
Weakly-SupervisedLearningforToolLocalizationinLaparoscopicVideos7
Grasper
Bipolar
Hook
Scissors
Clipper
Irrigator
Spec.bag
mAP
FCN
ESP
97.9
99.3
98.6
63.9
97.5
94.2
69.5
88.7
FCN
ESP
Msk
96.6
99.6
99.0
52.6
98.8
93.3
72.1
87.4
FCN
ESP
MM
97.1
99.7
98.9
64.8
98.3
88.7
72.1
88.5
FCN
ESP
MM
Msk
96.9
99.5
97.9
58.1
97.9
91.8
78.4
88.7
FCN
MSP
97.4
99.6
98.0
57.4
98.2
94.3
70.7
88.0
FCN
MSP
Msk
96.5
99.5
98.7
66.4
98.4
94.3
67.5
88.8
FCN
MSP
MM
97.9
99.7
98.7
46.3
97.7
94.4
73.5
86.9
FCN
MSP
MM
Msk
97.6
99.5
98.9
57.9
97.2
92.9
72.7
88.1
Table3.
Localizationaverageprecision(AP)forthe8evaluatedmodels.
Grasper
Bipolar
Hook
Scissors
Clipper
Irrigator
Spec.bag
mean
FCN
ESP
6.7
5.4
6.0
12.0
8.4
4.4
9.7
7.5
FCN
ESP
Msk
6.7
4.6
4.7
11.3
6.7
4.7
9.1
6.8
FCN
ESP
MM
6.8
5.0
5.5
11.0
8.3
4.5
9.1
7.1
FCN
ESP
MM
Msk
6.9
5.3
5.7
9.6
6.9
4.6
9.1
6.9
FCN
MSP
7.4
4.4
5.9
17.8
8.7
4.0
10.4
8.4
FCN
MSP
Msk
7.1
4.6
5.3
10.1
7.4
4.2
9.4
6.9
FCN
MSP
MM
7.6
4.8
5.8
17.7
9.2
3.9
10.0
8.4
FCN
MSP
MM
Msk
6.7
4.7
5.3
11.6
7.9
4.3
9.2
7.1
Table4.
Meandistancefrompredictedtoolcentertotruecenterinpercents(relative
totheimagediagonal).
DistanceError
LocalizationAPgivesacoarseideaaboutthequalityofob-
tainedpredictions.Togetabettersenseoftheaccuracyofthelocalization,we
computethedistancebetweenthepredictedtoolcenteranditsgroundtruth.
Wenormalizethisvaluebythediagonaloftheimage.Theresultsarepresented
inTable4.Wecanseethat,generally,maskingandESPimprovethequality
ofpredictedtoolcenters.Ontheotherhand,multi-mapsdonotseemto
theoutcometly.Itisalsonoteworthythatspecimenbagislocalized
tlyworsethantheothertools.Thiscanbeexplainedbythevarying
shapeofthebag,aswellastheambiguityofitscenter.
QualitativeResults
Forthesakeofvisualcomparison,wepresentqualitative
resultsfor8evaluatedmodelsinFigure3,whereinputimagesareoverlaidwith
localizationmaps.Justasthequantitativeresultssuggest,theperformancesof
thenetworksareverysimilarandthedetectedtoolcentersareveryclosetoone
anotherinmostcases.However,themodelswithmaskingandESPgenerate
moredetailedmapsthatcoverthetoolsbetterthanothermodelsandprovide
strongROIforthetools.
InFigure4,wepresentadditionalresultsforthearchitecture
FCN
ESP
MM
Msk.Intheweseewhichfeaturesthenetwork
mostdiscriminativeabouteachofthetools.Ideally,weaimtolocalizethe
8Vardazaryanetal.
Fig.3.
Column1:Groundtruthboundingboxandtoolcenter.Columns2-6:input
imagesoverlaidwithcorrespondinglocalizationmaps(aftersigmoid)andpredicted
toolcentersforFCN
ESP,FCN
ESP
Msk,FCN
MSP
Msk,FCN
MSP
MM
Msk,
FCN
ESP
MM
Msk,inthatorder.
workingendofthetoolsonly,astheshaftdoesnotusuallycontaintool-spe
features.InFigure4,wecanseethatforscissorsandirrigator(row4and6
respectively)theshaftsthemselvesareverydistinctiveanddiscriminative.In
thecaseofscissors,thebrightestdetectioncorrespondstotheshaft.Thismay
explainwhythelocalizationAPvaluesforscissorsarethelowestamongall
tools,astheannotatedboundingboxesareovertooltipsonly(see
column1inFigure3).Specimenbag(lastrow)isanexceptionsinceitis
notconnectedtoashaft.Weshouldalsonotethatthesecondtool,bipolar,
isnotfullydetected.Thenetworkdetectstheblueinsulatedsectionofthe
forcepsbutnotthemetaltips.Ourintuitionisthattheylookverysimilarto
thoseofgrasperandhencecannotbeusedtodiscriminateonetoolfromthe
other.Additionalqualitativeresultscanbeseeninthesupplementaryvideo
(https://youtu.be/7VWVY04Z0MA).
4Conclusions
Inthiswork,weshowedthatreliablesurgicaltooldetectionandlocalizationcan
beachievedwithouttheuseofspatialannotationsduringtraining.Ourmethod
reliesonaFCNarchitecturethatpreservesrelativespatialinformationofthe
inputimage.Thisenablesustolocalizethesurgicaltoolswhileusingonlybinary
Weakly-SupervisedLearningforToolLocalizationinLaparoscopicVideos9
Fig.4.
Inputimages(left)andcorrespondinglocalizationmapsaftersigmoidlayer
(right).Eachrowshows3examplesofthesametool.Thetoolsinrows1-7arepresented
inthefollowingorder:grasper,bipolar,hook,scissors,clipper,irrigator,specimenbag.
TheseresultscorrespondtoarchitectureFCN
ESP
MM
Msk.(Bestseenincolor)
presenceannotationsfortraining.Weevaluatedseveralvariantsofournetwork,
obtainingverypromisingAPvaluesofaround87and88forand
localizationonthetestset,respectively.Theseresultsalsosuggestthatthe
proposedapproachcouldbeusedtoeasethegenerationofspatialannotations
withinsurgicalvideolabelingsoftwareandextendedfortoolsegmentation.
Acknowledgements.
ThisworkwassupportedbyFrenchstatefundsmanaged
withintheInvestissementsd'AvenirprogrambyBPIFrance(projectCONDOR)
andbytheANR(referencesANR-11-LABX-0004andANR-10-IAHU-02).The
authorswouldalsoliketoacknowledgethesupportofNVIDIAwiththedonation
ofaGPUusedinthisresearch.
References
1.Bouget,D.,Allan,M.,Stoyanov,D.,Jannin,P.:Vision-basedandmarker-less
surgicaltooldetectionandtracking:areviewoftheliterature.MedicalImage
Analysis
35
,pp.633{654(2017)
2.Durand,T.,Mordan,T.,Thome,N.,Cord,M.:Wildcat:Weaklysupervisedlearn-
ingofdeepconvnetsforimagepointwiselocalizationandsegmenta-
tion.In:IEEEConferenceonComputerVisionandPatternRecognition(CVPR).
pp.5957{5966(2017)
10Vardazaryanetal.
3.Everingham,M.,VanGool,L.,Williams,C.K.I.,Winn,J.,Zisserman,A.:The
pascalvisualobjectclasses(voc)challenge.InternationalJournalofComputer
Vision
88
(2),pp.303{338(2010)
4.Garcia-Peraza-Herrera,L.C.,Li,W.,Fidon,L.,Gruijthuijsen,C.,Devreker,A.,
Attilakos,G.,Deprest,J.,VanderPoorten,E.,Stoyanov,D.,Vercauteren,T.,
etal.:Toolnet:Holistically-nestedreal-timesegmentationofroboticsurgicaltools.
In:ProceedingsoftheIEEE/RSJInternationalConferenceonIntelligentRobots
andSystems(2017)
5.He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.
In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition
(CVPR).pp.770{778(2016)
6.Hwang,S.,Kim,H.E.:Self-transferlearningforweaklysupervisedlesionlocaliza-
tion.In:MedicalImageComputingandComputer-AssistedIntervention(MIC-
CAI).pp.239{246.SpringerInternationalPublishing,Cham(2016)
7.Jia,Z.,Huang,X.,Chang,E.I.C.,Xu,Y.:Constraineddeepweaksupervision
forhistopathologyimagesegmentation.IEEETransactionsonMedicalImaging
36
(11),pp.2376{2388(2017)
8.Jin,A.,Yeung,S.,Jopling,J.,Krause,J.,Azagury,D.,Milstein,A.,Fei-Fei,L.:
Tooldetectionandoperativeskillassessmentinsurgicalvideosusingregion-based
convolutionalneuralnetworks.In:IEEEWinterConferenceonApplicationsof
ComputerVision(WACV).pp.691{699(2018)
9.Kim,D.,Cho,D.,Yoo,D.:Two-phaselearningforweaklysupervisedobjectlo-
calization.In:IEEEInternationalConferenceonComputerVision(ICCV).pp.
3554{3563(2017)
10.Kurmann,T.,Neila,P.M.,Du,X.,Fua,P.,Stoyanov,D.,Wolf,S.,Sznitman,R.:
Simultaneousrecognitionandposeestimationofinstrumentsinminimallyinvasive
surgery.In:InternationalConferenceonMedicalImageComputingandComputer-
AssistedIntervention(MICCAI).pp.505{513.Springer(2017)
11.Laina,I.,Rieke,N.,Rupprecht,C.,VizcaJ.P.,Eslami,A.,Tombari,F.,Navab,
N.:Concurrentsegmentationandlocalizationfortrackingofsurgicalinstruments.
In:MedicalImageComputingandComputer-AssistedIntervention(MICCAI).pp.
664{672.SpringerInternationalPublishing,Cham(2017)
12.Oquab,M.,Bottou,L.,Laptev,I.,Sivic,J.:Isobjectlocalizationforfree?-weakly-
supervisedlearningwithconvolutionalneuralnetworks.In:IEEEConferenceon
ComputerVisionandPatternRecognition(CVPR).pp.685{694(2015)
13.Sahu,M.,Mukhopadhyay,A.,Szengel,A.,Zachow,S.:Addressingmulti-labelim-
balanceproblemofsurgicaltooldetectionusingcnn.InternationalJournalofCom-
puterAssistedRadiologyandSurgery
12
(6),pp.1013{1020(2017)
14.Saleh,F.S.,Aliakbarian,M.S.,Salzmann,M.,Petersson,L.,Alvarez,J.M.,Gould,
S.:Incorporatingnetworkbuilt-inpriorsinweakly-supervisedsemanticsegmenta-
tion.IEEETransactionsonPatternAnalysisandMachineIntelligence
40
(6),pp.
1382{1396(2018)
15.Singh,K.K.,Lee,Y.J.:Hide-and-seek:Forcinganetworktobemeticulousfor
weakly-supervisedobjectandactionlocalization.In:IEEEInternationalConfer-
enceonComputerVision(ICCV)(2017)
16.Twinanda,A.P.,Shehata,S.,Mutter,D.,Marescaux,J.,deMathelin,M.,Padoy,
N.:Endonet:Adeeparchitectureforrecognitiontasksonlaparoscopicvideos.
IEEETransactionsonMedicalImaging
36
(1),pp.86{97(2017)
17.Zhou,B.,Khosla,A.,Lapedriza,

A.,Oliva,A.,Torralba,A.:Objectdetectors
emergeindeepscenecnns.InternationalConferenceonLearningRepresentations
(ICLR)(2015)
"
92,Direct Automated Quantitative Measurement of Spine via Cascade Amplifier Regression Network,http://arxiv.org/pdf/1806.05570v1.pdf,https://github.com/pangshumao/CARN,"arXiv:1806.05570v1  [cs.CV]  14 Jun 2018DirectAutomatedQuantitativeMeasurementof
SpineviaCascadeAmplierRegressionNetwork
ShumaoPang
1
,StephanieLeung
2
,IlanitBenNachum
2
,QianjinFeng
1(
)
,and
ShuoLi
2(
)
1
GuangdongProvincialKeyLaboratoryofMedicalImageProce
ssing,Schoolof
BiomedicalEngineering,SouthernMedicalUniversity,Gua
ngzhou,510515,China
qianjinfeng08@gmail.com
2
DepartmentofMedicalImaging,WesternUniversity,ON,Can
ada
DigitalImagingGroupofLondon,ON,Canada
slishuo@gmail.com
Abstract.
Automatedquantitativemeasurementofthespine(i.e.,mul
-
tipleindicesestimationofheights,widths,areas,andsoo
nforthever-
tebralbodyanddisc)isoftheutmostimportanceinclinical
spinaldis-
easediagnoses,suchasosteoporosis,intervertebraldisc
degeneration,
andlumbardischerniation,yetstillanunprecedentedchal
lengedueto
thevarietyofspinestructureandthehighdimensionalityo
findicesto
beestimated.Inthispaper,weproposeanovelcascadeampli
erre-
gressionnetwork(CARN),whichincludestheCARNarchitect
ureand
localshape-constrainedmanifoldregularization(LSCMR)
lossfunction,
toachieveaccuratedirectautomatedmultipleindicesesti
mation.The
CARNarchitectureiscomposedofacascadeampliernetwork
(CAN)
forexpressivefeatureembeddingandalinearregressionmo
delformul-
tipleindicesestimation.TheCANconsistsofcascadeampli
erunits
(AUs),whichareusedforselectivefeaturereusebystimula
tingeective
featureandsuppressingredundantfeatureduringpropagat
ingfeature
mapbetweenadjacentlayers,thusanexpressivefeatureemb
eddingis
obtained.Duringtraining,theLSCMRisutilizedtoallevia
teovert-
tingandgeneraterealisticestimationbylearningthemult
ipleindices
distribution.ExperimentsonMRimagesof195subjectsshow
thatthe
proposedCARNachievesimpressiveperformancewithmeanab
soluteer-
rorsof1.2496

1.0624mm,1.2887

1.0992mm,and1.2692

1.0811mm
forestimationof15heightsofdiscs,15heightsofvertebra
lbodies,and
totalindicesrespectively.Theproposedmethodhasgreatp
otentialin
clinicalspinaldiseasediagnoses.
1Introduction

Thequantitativemeasurementofthespine(i.e.,multipleindicesestima
tionof
heights,widths,areas,andsoonforthevertebralbodyanddisc
)playsasign-
cantroleinclinicalspinaldiseasediagnoses,suchasosteoporosis,
intervertebral
discdegeneration,andlumbardischerniation.Speccally,thevert
ebralbody
height(VBH)andintervertebraldischeight(IDH)(asshowninFig.
1
)arethe
mostvaluableindicesforthequantitativemeasurementofthespine
.TheVBHs
arecorrelatedwiththebonestrength,whichisofgreatsigncanc
etothever-
tebralfractureriskassessmentfortheosteoporoticpatients
[1
,2
].Furthermore,
theIDHreductionisassociatedwiththeintervertebraldiscdegen
eration[
3
,4
]andlumbardischerniation[
5
].Fig.1.
(a)Illustrationof30indicestobeestimated;(b)Threehei
ghtsforeachdisc
(i.e.,anteriorIDH
h
d

a
,middleIDH
h
d

m
,andposteriorIDH
h
d

p
);(c)Threeheightsfor
eachvertebralbody(i.e.,anteriorVBH
h
v

a
,middleVBH
h
v

m
,andposteriorVBH
h
v

p
),
where
A
d
denotesthediscarea;(d)Ambiguousboundarybetweendisca
ndVBand
implicitcorrelationsofdierentindicesduetospinalabn
ormality.
Automatedquantitativemeasurementofthespineisofsigncantc
linical
importancebecauseitisreliable,time-saving,reproducible,andhas
highercon-
sistencycomparedwithmanualquantitativemeasurement,whichis
usuallyob-
tainedbymanuallydetectinglandmarksoftheintervertebraldisc(
ID)andver-
tebralbody(VB)fromMRimage[
5
,6
].Directautomatedquantitativemeasurementofthespineisanexce
edingly
intractabletaskduetothefollowingchallenges:1)Thehighdimension
alityof
estimatedindices(asshowninFig.
1
(a)),whichleadstodcultyinexpres-
sivefeatureembeddingforsuchcomplexregressionproblem.2)Th
eexcessive
ambiguityoftheboundarybetweenVBandIDforabnormalspine(a
sshown
inFig.
1
(d)),whichincreasesintractabilityofexpressivefeatureembedd
ing.3)
Implicitcorrelationsbetweenderentestimatedindices(asshownin
Fig.
1
(d),
theheightsoftheabnormaldiscandtheheightsofadjacentVBar
ecorrelated
becausediscabnormalityleadstosimultaneouschangesofIDHandt
headjacent
VBH),whichisdculttobecaptured.4)Insucientlabelleddata(as
shown
inFig.
1
(d)),whichpossiblyresultsinovertting.
Inrecentyears,anincreasingnumberofapproachesemergedint
hedi-
rectquantitativemeasurementofotherorgans(e.g.,heart)[
7
,8
].Althoughthese
methodsachievedpromisingperformanceinthequantcationofth
ecardiac
image,theyareincapableofachievingquantitativemeasurementof
thespine
becausetheysuerfromthefollowinglimitations.1)Lackofexpres
sivefeature
representation.Traditionalconvolutionalneuralnetwork(CNN
)[
9
]isincapable
ofgeneratinganexpressivefeatureformultipleindicesestimationb
ecauseCNN
possiblyloseseectivefeatureduetothelackofanexplicitstructu
reforfeature
reuse.2)Incapabilityoflearningtheestimatedindicesdistribution,
whichwill
leadtounreasonableestimationandovertting.
Inthisstudy,weproposeacascadeamperregressionnetwork(
CARN),
whichincludestheCARNarchitectureandlocalshape-constrained
manifoldreg-
ularization(LSCMR)lossfunction,forquantitativemeasuremento
fthespine
fromMRimages.TheCARNarchitectureiscomprisedofacascadeam
per
network(CAN)forexpressivefeatureembeddingandalinearregr
essionmodel
formultipleindicesestimation.InCAN,amperunit(AU)isusedforse
lective
featurereusebetweenadjacentlayers.AsshowninFig.
2
(b),theeectivefea-
tureoftheanteriorlayerisstimulatedwhiletheredundantfeature
issuppressed,
thusgeneratingtheselectedfeature,whichisreusedinposterior
layerbyacon-
catenationoperator.CANreusesmulti-levelfeaturesselectively
forrepresenting
complexspine,thusanexpressivefeatureembeddingisobtained.D
uringtrain-
ing,thehighdimensionalindicescanbeembeddedinalowdimensionalma
nifold
duetothecorrelationsbetweentheseindices.LSCMRisemployedto
restrict
theoutputoftheCARNtothetargetoutputmanifold.Asaresult,
thedistribu-
tionoftheestimatedindicesisclosetotherealdistribution,whichre
ducesthe
impactofoutliersandalleviatesovertting.Combiningtheexpressiv
efeature
embeddingproducedbyCANwithLSCMR,asimplelinearregressionmod
el,
i.e.,fullyconnectednetwork,issucienttoproduceaccurateestim
ationresults.
Themaincontributionsofthestudyarethree-fold.1)Tothebest
ofour
knowledge,itisthersttimetoachieveautomatedquantitativemea
surement
ofthespine,whichwillprovideamorereliablemetricfortheclinicaldiag
nosis
ofspinaldiseases.2)TheproposedCANprovidesanexpressivefe
aturemapfor
automatedquantitativemeasurementofthespine.3)Overtting
isalleviatedby
LSCMR,whichutilizesthelocalshapeofthetargetoutputmanifoldt
orestrict
theestimatedindicestobeingclosetothemanifold,thusarealistices
timation
ofindicesisobtained.

2CascadeAmplierRegressionNetwork

TheCARNemploystheCARNarchitectureandLSCMRlossfunctionto
achieve
accuratequantitativemeasurementofthespine.TheCARNarchit
ectureiscom-
posedoftheCANforexpressivefeatureembeddingandthelinearr
egression
modelformultipleindicesestimation.AsshowninFig.
2
,inCAN,AUisused
forselectivefeaturereusebetweentheadjacentlayersbyagat
e,multiplier,adder
andconcatenateoperator.InAU,theeectivefeaturemapisst
imulatedwhile
theredundantfeaturemapissuppressed.CANprovidesexpress
ivefeatureem-
beddingviareusingmulti-levelfeaturesselectively.Thelinearregre
ssionmodel
inCARNisafullyconnectednetworkwithoutnon-linearactivation.Du
ring
training,overttingisalleviatedbyLSCMR,whichisemployedtoobliget
he
outputofCARNtolieonthetargetoutputmanifoldexpressedbyloc
allinear
representation[
10
],i.e.,asampleonthemanifoldcanbeapproximatelyrepre-
sentedasalinearcombinationofseveralnearestneighborsfromt
hemanifold.
Locallinearrepresentationcapturesthelocalshapeofthemanif
old,therefore,
thedistributionofestimatedindicesisclosetotherealdistributiona
ndthe
indicesestimatedbyCARNarerealistic.

2.1MathematicalFormulation

Automatedquantitativemeasurementofthespineisdescribedasa
multi-output
regressionproblem.Givenatrainingdataset
T
=
f
x
i
;y
i
g
N

i
=1
,weaimtotraina
multi-outputregressionmodel(i.e.,theCARN)tolearnthemapping
f
:x
2
R
h

w!
y
2
R
d
,where
x
i
and
y
i
denotetheMRimageandthecorresponding
multipleindicesrespectively,and
N
isthenumberoftrainingsamples.CARN
shouldlearnaneectivefeatureandareliableregressorsimultaneo
usly.
Fig.2.
(a)OverviewofCARNarchitecture,includingCANforexpres
sivefeatureem-
beddingandalinearregressionmodelformultipleindicese
stimation.(b)AUforselec-
tivefeaturereusebetweenadjacentlayers.(c)LSCMRforob
tainingrealisticestimation
andalleviatingovertting.

2.2CARNArchitecture

TheCARNarchitectureiscomprisedoftheCANforexpressivefeat
ureembed-
dingandthelinearregressionmodelformultipleindicesestimation.
CANforExpressiveFeatureEmbedding
TheCANconsistsofsixAUs,
twoconvolutionallayers,vemaxpoolinglayers,andaglobalavera
gepooling
layerasshowninFig.
2
(a).AUisdesignedforselectivefeaturereusebetween
adjacentlayers.Duringfeatureselection,theselectedfeature
isobtainedbyam-
plifyingtheinputfeatureofAUusinganamper,whoseampcationf
actoris
learnedautomatically(detailsinSectionFeatureSelectionMechanism
).Theef-
fectivelow-levelfeatureisstimulatedandconcatenatedbythehig
h-levelfeature
whiletheredundantlow-levelfeatureissuppressed.Theselective
featurereuse
isachievedbyCANlevelbylevel;thenthemulti-levelselectivereused
feature
generatesanexpressivefeatureembedding.Therstconvolutio
nallayerwith
a7

7kernelsizeandstrideof2reducestheresolutionoffeaturemap
sfrom
512

256to256

128,whilethelastconvolutionallayerwitha1

1kernelsize
andstrideof1linearlycombinesthefeaturemapsforinformationint
egration.
Themaxpoolingwitha2

2kernelsizeandastrideof2isusedtoprovide
translationinvariancetotheinternalrepresentation.Theglobal
averagepooling
layerisutilizedtoreducethedimensionalityoffeaturemaps.
ThemostcrucialcomponentofCANisAU(asdemonstratedinFig.
2
(b)),
whichiscomposedofagateforcontrollinginformationpropagationb
etween
adjacentlayers,aconvolutionallayerwitha3

3kernelsizeandstrideof1
forextractingalinearfeaturemap,whichisusedtocontrolthega
te,abatch
normalizationlayerwithreluactivationforproducingnon-linearfeat
uremap,
amultiplier,anadder,andaconcatenationoperatorwithbatchnor
malization
forcombiningtheselectedfeaturemapandnon-linearfeaturemap
.Theinput
t
ofAUgoesthroughaconvolutionallayerandproducesthelinearfe
aturemap
f
l
(
t
)=
w
l

t
+
b
l
forguidingfeatureselection,where
w
l
and
b
l
aretheconvolu-
tionkernelweightandbiasoftheconvolutionallayerrespectively,
and

isthe
convolutionaloperator.Thenthe
f
l
(
t
)owsintotwopaths.Onepathconsists
ofbatchnormalizationandreluactivation,whichisanalogoustothet
raditional
CNNtogeneratenon-linearfeaturemap
f
n
(
t
)=
relu
(
bn
(
f
l
(
t
))),where
bn
and
relu
denotethebatchnormalizationandreluactivationrespectively.Th
eother
pathisagatecomposedofaconvolutionallayerandtanhactivation
,which
generatesoutput
f
g
(
t
)=
tanh
(
w
g

f
l
(
t
)+
b
g
),where
w
g
and
b
g
arethecon-
volutionkernelweightandbiasinthegaterespectively,forselectin
gfeature
map.Theoutputofthegateowsintoamultiplierfollowedbyanadder
,and
generatestheselectedfeature:
f
s
(
t
)=
t

f
g
(
t
)+
t
=
t

(
f
g
(
t
)+1)
(1)
where

denotestheelement-wisemultiplication.Finally,the
f
n
and
f
s
are
concatenatedalongthechannelaxisandnormalizedbythebatchn
ormalization
layertogenerateaoutputfeaturemap
f
out
(
t
)=
bn
(
f
n
(
t
)

f
s
(
t
)),where

denotestheconcatenationoperator.
FeatureSelectionMechanism
InEq.
1
,thevalueofeachpixelinthe
selectedfeaturemap
f
s
isobtainedbymultiplyinganampcationfactorwith
thecorrespondingvalueintheinputfeaturemap
t
.Theampcationfactor
[f
g
(
t
)+1]rangesfrom0to2;substantially,theselectedfeaturemap
f
s
isequiv-
alenttostimulatingorsuppressingtheinputfeaturemapviaanamp
er.When
theampcationfactorislessthan1,theinputfeaturemapissuppr
essed,vice
versa.Iftheampcationfactoris1,theinputfeaturemapisdirec
tlypropa-
gatedtotheoutput,whichisanalogoustothedenseNet[
11
].LinearRegressionModelforMultipleIndicesEstimation
Thelinear
regressionmodelisafullyconnectedlayer.Theoutputofthelinear
regression
modelis:
f
(
x
i
)=
w
o
h
(
x
i
)+
b
o
,where
h
(
x
i
)istheoutputoftheglobalaverage
pooling(i.e.,thefeatureembedding)asshowninFig.
2
(a),and
w
o
and
b
o
are
theweightsmatrixandbiasofthelinearregressionrespectively.

2.3LocalShape-constrainedManifoldRegularizationLoss
Function
Thelossfunctionisdividedintotwoparts,includingpreliminaryloss
loss
p
and
LSCMRloss
loss
m
.Thepreliminarylossisdesignedtominimizethedistance
betweentheestimationofindicesandthegroundtruth,whiletheLS
CMRloss
isaimedatalleviatingoverttingandgeneratingrealisticresultsbyob
liging
theoutputofCARNtolieonthetargetoutputmanifoldusinglocalline
ar
representation.Thetotallossfunctionisdenedasfollows:
loss
t
(
w
)=
loss
p
(
w
)+

l
loss
l
(
w
)(2)
wherethe

l
isascalingfactorcontrollingtherelativeimportanceoftheLSCMR
loss.Thepreliminarylossfunctionisdenedasfollows:
loss
p
(
w
)=
1
N

d
N
X
i
=1
k
y
i

f
(
x
i
)
k
1
+

p
X
i
k
w
i
k
2
(3)
wherethersttermisthemeanabsoluteerror(MAE)oftheregre
ssionmodel;
thesecondtermisthe
l
2
normregularizationforthetrainableweight
w
i
in
CARN;

p
isahyper-parameter.
Byusingonlythepreliminarylossfunction,unreasonablemultipleindice
s
estimationmaybeobtainedbecausetheestimatedresultispossiblet
obeoutof
theirrealdistribution.Forinstance,asshowninFig.
2
(c),
y
i
,y
j
,and
y
m
arethe
targetoutputsofsamples.Thepoints
a
and
b
aretwopossibleestimationsof
y
i
.Thedistancesbetweenthetwoestimations(thepoints
a
and
b
)andthetarget
output
y
i
arethesame,ie.,theyhaveanidenticalpreliminaryloss.However,t
he
lossofpoint
a
shouldbesmallerthanthepoint
b
as
a
ismuchclosertothelocal
shapeoftheoutputspacethan
b
.Hence,
a
isabetterestimationof
y
i
than
b
.LSCMRisproposedtoachievearealisticandaccurateestimationofm
ultiple
indices.Inspiredby[
12
],y
i
liesonamanifold
M
y
withaninherentdimension
smallerthan
d
astheelementsof
y
i
arecorrelated.Themanifold
M
y
isspanned
by
f
y
i
g
N

i
=1
.Weintroducethelocallinearrepresentation,i.e.,asampleonman-
ifold
M
y
canbeapproximatelyrepresentedasalinearcombinationofsevera
lnearestneighborsfrom
M
y
[10
].Asample
y
i
on
M
y
islocallylinearlyrepre-
sentedas:
y
i
=
k
P
j
=1
y
j

j
+
""
ˇ
k
P
j
=1
y
j

j
=~
y
i
s:t:
k
""
k
<˝;
k
P
j
=1

j
=1
;
j

0
;y
j
2
N
(
y
i
)
(4)
where
""
isthereconstructionerrorand
˝
isasmallnon-negativeconstant.
N
(
y
i
)
denotesthe
k
-nearestneighborsof
y
i
on
M
y
and

j
isthereconstructioncoef-
cient,whichiscalculatedbyLAE[
13
].AsshowninFig.
2
(c),~
y
i
isthelocal
linearrepresentationof
y
i
usingits
k
-nearestneighbors(here
k
isequalto2)
y
j
and
y
m
.Thelocallinearrepresentationof
y
i
reectsthelocalmanifoldshape.If
thepredictedindicesiscloseto~
y
i
,itwillbenearthemanifold
M
y
.Therefore,
theLSCMRlossisdenedas:
loss
l
(
w
)=
1
N

d
N
X
i
=1
k
f
(
x
i
)

~
y
i
k
1
(5)
Usingthe
loss
l
,thepredictionof
y
i
isrestrictedtobeingclosetothemanifold
M
y
,thusamorerealisticresultisobtained(e.g.,themodelgenerateth
epoint
a
astheestimationof
y
i
insteadofpoint
b
inFig.
2
(c)).
3ExperimentalResults

Dataset
Thedatasetconsistsof195midsagittalspineMRimagesfrom195
patients.Thepixelspacingsrangefrom0.4688mm/pixelto0.7813m
m/pixel.
Imagesareresampledto0.4688mm/pixelandthegroundtruthvalu
esareob-
tainedmanuallyinthisspace.Inourexperiments,twolandmarks,i.e.,
theleft-
topcorneroftheL1VBandtheleft-bottomcorneroftheL5VB,a
remanually
markedforeachimagetoprovidereferenceforROIcropping,inwh
ichveVBs,
includingL1,L2,L3,L4andL5,andveIDsunderthemareenclosed
.The
croppedimagesareresizedto512

256.
ExperimentalCongurations
ThenetworkisimplementedbyTensorow.
Fourgroupexperimentsunderderentcongurations,includingC
ARN-
loss
p
,CNN-
loss
p
,CNN-
loss
t
,andCARN-
loss
t
,areusedtovalidatetheeectiveness
ofourproposedmethod.InCNN-
loss
p
andCNN-
loss
t
,AUisreplacedwith
atraditionalconvolutionallayer,inwhichtheoutputfeaturechan
nelsarethe
sameasAU;the-
loss
p
and-
loss
t
denotethelossfunctiondenedinEq.
3
and
Eq.
2
respectivelyusedinthemodel.
OverallPerformance
AsshowninthelastcolumnofTable
1
,theproposed
CARNachieveslowerrorforautomatedquantitativemeasurement
ofthespine,
withMAEof1.2496

1.0624mm,1.2887

1.0992mm,and1.2692

1.0811mm
forIDHs,VBHs,andtotalindicesrespectively.Theseerrorsare
smallreferring
tothemaximumsofIDHs(20.9203mm)andVBHs(36.7140mm)inourda
taset.
CANandLSCMREectiveness
CombiningCANandLSCMR,theper-
formanceimprovedby2.44%,1.16%,and1.80%forIDHs,VBHs,andt
otal
indicesestimationrespectively,whichisclearlydemonstratedbycom
paringthe
thirdandlastcolumnsofTable
1
.UsingCANwithoutLSCMR,theMAEde-
creasedby0.21%,0.49%,and0.36%forIDHs,VBHs,andtotalindice
sestima-
tionrespectively,asshowninthesecondandthirdcolumnsofTable
1
.These
resultsindicatethattheCANimprovestheperformancefortotal
indiceses-
timation,especiallyforVBHs.ThisresultsfromthefactthatCANge
nerates
expressivefeatureembeddingalthoughpathologicalchangesinth
edisccanre-
ducetheintensityoftheadjacentVBandleadtoambiguityinthebou
ndary.
UsingLSCMRwithoutCAN,theperformanceimprovedby2.14%,1.03%
for
IDHs,andtotalindicesestimationrespectively,asshowninthethir
dandfourth
columnsofTable
1
.LSCMRalleviatesoverttingasshowninTable
1
,inwhichCARN-
loss
t
and
CNN-
loss
t
havehightrainingerrors(0.8591mmvs0.5024mm,0.9059mmvs
0.5224mm)butlowtesterrors(1.2692mmvs1.2878mm,1.2791mmvs1
.2924
mm)fortotalindicesestimationcomparedwithCARN-
loss
p
andCNN-
loss
p
.Table1.
PerformanceofCARNintermsofMAEunderdierentcongurat
ionsfor
IDH(mm),VBH(mm),andtotalindices(mm)estimation.MAEis
illustratedineach
cell.Bestresultsareboldedforeachrow.
Method
CARN-
loss
p
CNN-
loss
p
CNN-
loss
t
CARN-
loss
t
IDH
train
0.4633

0.4706
0.4920

0.4574
0.8689

0.7417
0.8265

0.7012
test
1.2782

1.1173
1.2809

1.1172
1.2535

1.0754
1.2496

1.0624
VBH
train
0.5414

0.5846
0.5528

0.5615
0.9429

0.8383
0.8916

0.8004
test
1.2974

1.0922
1.3038

1.1154
1.3047

1.1215
1.2887

1.0992
Total
train
0.5024

0.5321
0.5224

0.5130
0.9059

0.7923
0.8591

0.7531
test
1.2878

1.1049
1.2924

1.1163
1.2791

1.0990
1.2692

1.0811
4Conclusions

Wehaveproposedamulti-outputregressionnetworkCARNforaut
omatedquan-
titativemeasurementofspine.Bytakingadvantageofexpressive
featureex-
tractedfromCAN,andemployingLSCMRforalleviatingovertting,C
ARNis
capableofachievingpromisingaccuracyforallindicesestimation.
Acknowledgements.
ThisworkwassupportedbyChinaScholarshipCoun-
cil(No.201708440350),NationalNaturalScienceFoundationof
China(No.
U1501256),andScienceandTechnologyProjectofGuangdongPr
ovince(No.
2015B010131011).

References
1.McCloskey,E.,Johansson,H.,Oden,A.,Kanis,J.A.:Frac
tureriskassessment.
ClinicalBiochemistry
45
(12)(aug2012)887{893
2.Taton,G.,Rokita,E.,Korkosz,M.,Wrobel,A.:Therati
oofanteriorandposterior
vertebralheightsreinforcestheutilityofDXAinassessme
ntofvertebraestrength.
CalciedTissueInternational
95
(2)(may2014)112{121
3.Jarman,J.P.,Arpinar,V.E.,Baruah,D.,Klein,A.P.,Mai
man,D.J.,Muftuler,
L.T.:Intervertebraldischeightlossdemonstratesthethr
esholdofmajorpatho-
logicalchangesduringdegeneration.EuropeanSpineJourn
al
24
(9)(sep2014)
1944{1950
4.Salamat,S.,Hutchings,J.,Kwong,C.,Magnussen,J.,Han
cock,M.J.:There-
lationshipbetweenquantitativemeasuresofdischeightan
ddiscsignalintensity
withprrmannscoreofdiscdegeneration.SpringerPlus
5
(1)(jun2016)
5.Tunset,A.,Kjaer,P.,Chreiteh,S.S.,Jensen,T.S.:Amet
hodforquantitative
measurementoflumbarintervertebraldiscstructures:ani
ntra-andinter-rater
agreementandreliabilitystudy.Chiropractic&ManualThe
rapies
21
(1)(2013)
26
6.Videman,T.,Battie,M.C.,Gibbons,L.E.,Gill,K.:Agin
gchangesinlumbardiscs
andvertebraeandtheirinteraction:a15-yearfollow-upst
udy.TheSpineJournal
14
(3)(mar2014)469{478
7.Zhen,X.,Zhang,H.,Islam,A.,Bhaduri,M.,Chan,I.,Li,S
.:Directandsimultane-
ousestimationofcardiacfourchambervolumesbymultioutp
utsparseregression.
MedicalImageAnalysis
36
(feb2017)184{196
8.Xue,W.,Lum,A.,Mercado,A.,Landis,M.,Warrington,J.,
Li,S.:Fullquanti-
cationofleftventricleviadeepmultitasklearningnetwor
krespectingintra-and
inter-taskrelatedness.In:LectureNotesinComputerScie
nce.SpringerInterna-
tionalPublishing(2017)276{284
9.Simonyan,K.,Zisserman,A.:Verydeepconvolutionalnet
worksforlarge-scale
imagerecognition.CoRR
abs/1409.1556
(2014)
10.Pang,S.,Jiang,J.,Lu,Z.,Li,X.,Yang,W.,Huang,M.,Zh
ang,Y.,Feng,Y.,
Huang,W.,Feng,Q.:Hippocampussegmentationbasedonloca
llinearmapping.
ScienticReports
7
(apr2017)45501
11.Huang,G.,Liu,Z.,vanderMaaten,L.,Weinberger,K.Q.:
Denselyconnectedcon-
volutionalnetworks.In:2017IEEEConferenceonComputerV
isionandPattern
Recognition(CVPR),IEEE(jul2017)
12.Liu,G.,Lin,Z.,Yu,Y.:Multi-outputregressionontheo
utputmanifold.Pattern
Recognition
42
(11)(nov2009)2737{2743
13.Liu,W.,He,J.,Chang,S.F.:Largegraphconstructionfo
rscalablesemi-supervised
learning.In:Proceedingsofthe27thinternationalconfer
enceonmachinelearning
(ICML-10).(2010)679{686
"
93,"APPLE Picker: Automatic Particle Picking, a Low-Effort Cryo-EM Framework",http://arxiv.org/pdf/1802.00469v2.pdf,https://github.com/PrincetonUniversity/APPLEpicker,"APPLEPicker:AutomaticParticlePicking,aLow-EffortCryo-EMFramework
AyeletHeimowitz
a
,JoakimAnd
´
en
b
,AmitSinger
a,c
a
ThePrograminAppliedandComputationalMathematics,PrincetonUniversity,Princeton,NJ
b
CenterforComputationalBiology,FlatironInstitute,NewYork,NY
c
DepartmentofMathematics,PrincetonUniversity,Princeton,NJ
Abstract
Particlepickingisacrucialstepinthecomputationalpipelineofsingle-particlecryo-electronmicroscopy(cryo-EM).Select-
ingparticlesfromthemicrographsisdifespeciallyforsmallparticleswithlowcontrast.Ashigh-resolutionreconstruction
typicallyrequireshundredsofthousandsofparticles,manuallypickingthatmanyparticlesisoftentootime-consuming.While
semi-automatedparticlepickingiscurrentlyapopularapproach,itmaysufferfromintroducingmanualbiasintotheselec-
tionprocess.Inaddition,semi-automatedparticlepickingisstillsomewhattime-consuming.ThispaperpresentstheAPPLE
(
A
utomatic
P
article
P
ickingwith
L
owuser
E
ffort)picker,asimpleandnovelapproachforfast,accurate,andfullyautomatic
particlepicking.Whileourapproachwasinspiredbytemplatematching,itiscompletelytemplate-free.Thisapproachiseval-
uatedonpubliclyavailabledatasetscontainingmicrographsof

-Galactosidase,T20Sproteasome,70Sribosomeandkeyhole
limpethemocyaninprojections.
Keywords:
cryo-electronmicroscopy,single-particlereconstruction,particlepicking,template-free,cross-correlation,
micrographs,supportvectormachines.
1.Introduction
Single-particlecryo-electronmicroscopy(cryo-EM)aims
todeterminethestructureof3Dspecimens(macromolecules)
frommultiple2Dprojections.Inordertoacquirethese
2Dprojections,asolutioncontainingthemacromoleculesis
frozeninvitreousiceoncarbonthuscreatingasample
grid.Anelectronbeamthenpassesthroughtheiceandthe
macromoleculesfrozenwithin,creating2Dprojections.
Unfortunately,duetoradiationdamageonlyasmallnum-
berofimagingelectronscanbeusedinthecreationofthe
micrograph.Asaresult,micrographshavealowsignal-to-
noiseratio(SNR).Anelaborationonthenoisemodelcanbe
foundin(Sigworth,2004).
SincemicrographstypicallyhavelowSNR,eachmicro-
graphconsistsofregionsofnoiseandregionsofnoisy2D
projectionsofthemacromolecule.Inadditiontothese,mi-
crographsalsocontainregionsofinformation
stemmingfromcontaminantssuchascarbon
Differenttypesofregionshavedifferenttypicalintensity
values.Theregionsofthemicrographthatcontainonlynoise
willtypicallyhavehigherintensityvaluesthanotherregions.
Inaddition,regionscontainingaparticletypicallyhavehigher
variancethanregionscontainingnoisealone(Nicholson&
Glaeser,2001;vanHeel,1982).Duetothis,twocuesthat
canbeusedforprojectionimagearethemean
andvarianceoftheimage.
Inordertodeterminethe3Dstructureathighresolution,
manyprojectionimagesareneeded,ofteninthehundredsof
thousands.Thus,thesteptowards3Dreconstructionof
macromoleculesconsistsofdeterminingregionsofthemicro-
graphthatcontainaparticleasopposedtoregionsthatcontain
noiseorcontaminants.Thisistheparticlepickingstep.
Afullymanualselectionofhundredsofthousandsof2D
projectionsistediousandtime-consuming.Forthisreason,
semi-automaticandautomaticparticlepickingisamuchre-
searchedproblemforwhichnumerousframeworkshavebeen
suggested.Solutionstotheparticlepickingproblemin-
cludeedgedetection(Harauz&Fong-Lochovsky,1989),deep
learning(Ogura&Sato,2004;Wangetal.,2016;Zhuetal.,
2016),supportvectormachine(Aebel
´
aezetal.,
2011),andtemplatematching(Frank&Wagenknecht,1983).
Emailaddresses:
aheimowitz@math.princeton.edu
(AyeletHeimowitz),
janden@flatironinstitute.org
(JoakimAnd
´
en),
amits@math.princeton.edu
(AmitSinger)
PreprintsubmittedtoJournalofStructuralBiology
August7,2018
arXiv:1802.00469v2  [cs.CV]  14 Jun 2018(a)
(b)
(c)
Figure1:Resultofoursuggestedframework.Theleftcolumncontainsmicrographs.Themiddlecolumncontainstheoutputofthe.Therightcolumn
containsthepickedparticles.Toprowcontainsa

-Galactosidasemicrograph.BottomrowcontainsaKLHmicrograph.
Templatematchingisapopularapproachtoparticlepick-
ing.Theinputtotemplatematchingschemesconsistsofami-
crographandimagescontaining2Dtemplatestomatch.These
templatescanbe,forexample,generatedfrommanuallyse-
lectedparticleprojections.Theaimistooutputtheregionsin
themicrographthatcontainthesought-aftertemplates.
Thebasicideabehindthisapproach(Chen&Grigori-
eff,2007;Frank&Wagenknecht,1983;Langloisetal.,
2014;Ludtkeetal.,1999;Scheres,2015)isthatthecross-
correlation
1
betweenatemplateimageandamicrographis
largerinthepresenceofthetemplate.Anissuewiththis
methodisthehighrateoffalsedetection.Thisissuestems
fromthefactthatgivenenoughrandomdata,meaningless
noisecanbeperceivedasapattern.Thisproblemwasex-
in(Henderson,2013;Shatskyetal.,2009),where
animageofEinsteinwasusedasthetemplateandmatchedto
randomnoise.Eventhoughtheimagewasnotpresentinthe
noiseimages,areconstructionfromthebest-matchedimages
yieldedtheoriginalEinsteinimage.
Oneexampleofatemplate-basedframeworkisprovided
inRELION(Scheres,2015,2012a,b).Inthisframework,the
usermanuallyselectsapproximatelyonethousandparticles
fromasmallnumberofmicrographs.Theseparticleimages
arethen2Dtogenerateasmallernumberoftem-
plateimagesthatareusedtoautomaticallyselectparticles
fromallmicrographs.Theseparticleimagesarethenclas-
inordertoidentifynon-particles.Additionalexamples
oftemplate-basedframeworksincludeSIGNATURE(Chen&
Grigorieff,2007)whichemploysapost-processingstepthat
ensuresthelocationsofanytwopickedparticlescannotover-
lap,andgEMpicker(Hoangetal.,2013)whichemployssev-
eralstrategiestospeeduptemplatematching.
Templatematchingcanalsobeperformedwithoutthein-
putoftemplateimages.Forexample,see(Vossetal.,2009)
whichisbasedondifferenceofGaussiansandissuitablefor
identifyingblobsofacertainsizeinthemicrograph.An-
othertemplate-freeparticlepickingframeworkisgautomatch
(Zhang,2017).
Inthispaperweproposeaparticlepickingframeworkthat
isfullyautomaticanddata-adaptiveinthesensethatnoman-
1
Cross-correlationisnottheonlypossiblefunctiontousefortemplatematchingmethods.Forareviewofotherpossibilitiessee(Nicholson&Glaeser,2001).
2
ualselectionisusedandnotemplatesareinvolved.Instead
oftemplatesweuseasetofautomaticallyselectedreference
windows.Thissetincludesbothparticleandnoisewindows.
Weshowthatitispossibletodeterminethepresenceofa
particleinanyqueryimage(
i.e.
,regionofthemicrograph)
throughcross-correlationwitheachwindowofthereference
set.,inthecasewherethequeryimagecontains
noisealone,sincethereisnotemplatetomatch,thecross-
correlationcoefshouldnotindicatethepresenceofa
templateregardlessoftheactualcontentofeachreference
window.Ontheotherhand,inthecasewherethequeryimage
containsaparticle,thecoefwilldependonthecontent
ofeachreferencewindow.
Oncetheircontentisdetermined,thequeryimagesmost
likelytocontainaparticleandthosemostlikelytocontain
noisecanbeusedtotraina.Theoutputofthisclas-
isusedforparticlepicking.
Wetestourframeworkonpubliclyavailabledatasets
of

-Galactosidasedataset(Chenetal.,2013;Scheres,
2015;Scheres&Chen,2012),T20Sproteasome(Danev&
Baumeister,2016),70Sribosome(Fischeretal.,2016)and
keyholelimpethemocyanin(Zhuetal.,2003,2004).Some
sampleresultsarepresentedinFig1.Codeforourframework
ispubliclyavailable.
2
Wenotethatourformulationcanignorethecontrasttrans-
ferfunction(CTF).ThisisbecausetheCTFisroughlythe
samethroughoutthemicrographandourparticleselection
procedureperformsontheindividualmicrographlevel.Thus,
whileCTF-correctionisnotstrictlynecessary,wediscussthe
advantageofapplyingourframeworktoCTF-correctedmi-
crographsinSection2.6
2.MaterialandMethods
InSection2.1wedetailourmethodfordeterminingthe
contentofasinglequeryimage
g
2
R
n

n
,wherethequery
imageisawindowextractedfromthemicrographand
n
is
chosensuchthatthewindowsizeisslightlysmallerthanthe
particlesize(whichweassumeisknown).
3
Thismethodne-
cessitatestheuseofareferenceset
f
f
m
2
R
n

n
g
B
m
=1
se-
lectedfromthemicrographintheautomaticmannerdetailed
inSection2.2.Wegeneralizeourmethodtoparticlepicking
fromthefullmicrographinSection2.3.Section2.4improves
localizationthroughtheuseofafaststep.The
completemethod,knownastheAPPLEpicker,isdescribed
inSection2.5.
2.1.DeterminingtheContentofaQueryImage
Theideabehindtraditionaltemplatematchingmethodsis
thatthecross-correlationscoreoftwosimilarimagesishigh.
,atemplateimageknowntocontainaparticlecan
beusedinordertoidentifysimilarpatternsinthemicrograph
usingcross-correlation.Inthissectionweshowthatthesame
ideacanbeusedtodeterminethecontentofregionsofthemi-
crographevenwhennotemplatesareavailable.Tothisendwe
usethecross-correlationbetweenaqueryimage
g
andasetof
referenceimages
f
f
m
g
B
m
=1
.Thecross-correlationfunctionis
(Nicholson&Glaeser,2001)
c
f
m
;g
(
x;y
)=
X
x
0
X
y
0
f
m
(
x
0
;y
0
)
g
(
x
+
x
0
;y
+
y
0
)
:
(1)
Thisfunctioncanbethoughtofasascoreassociatedwith
f
m
,
g
andanoffset
(
x;y
)
.
Thecross-correlationscoreatacertainoffsetdoesnotin
itselfhavemuchmeaningwithoutthecontextofthescorein
nearbyoffsets.Forthisreasonwethefollowingnor-
malizationonthecross-correlationfunction
^
c
f
m
;g
(
x;y
)=
c
f
m
;g
(
x;y
)

1
n
2
X
x
0
X
y
0
c
f
m
;g
(
x
0
;y
0
)
;
(2)
wherethesecondtermisthemeanof
c
f
m
;g
2
R
n

n
.Wecall
(2)anormalizationsinceitshiftsallcross-correlationstoa
commonbaseline.
Considerthecasewherequeryimage
g
containsaparticle.
Thescore
c
f
m
;g
(
x;y
)
isexpectedtobemaximizedwhen
f
m
containsaparticlewithasimilarview.Inthiscasetherewill
besomeoffset
(
x;y
)
suchthattheimages
f
m
and
g
match
best,and
c
f
m
;g
(
x;y
)
>c
f
m
;g
(
x
0
;y
0
)
forallotheroffsets
(
x
0
;y
0
)
.Thus,
c
f
m
;g
(
x;y
)
>
1
n
2
X
x
0
X
y
0
c
f
m
;g
(
x
0
;y
0
)
:
(3)
Inotherwords,
^
c
f
m
;g
(
x;y
)
isexpectedtobelargeandposi-
tive.Inthiscase,wesay
g
hasastrongresponseto
f
m
.
Next,considerthecasewherequeryimage
g
containsno
particle.Inthiscasethereshouldnotexistanyoffset
(
x;y
)
thatgreatlyincreasesthematchforany
f
m
.Thustypically
^
c
f
m
;g
(
x;y
)
iscomparativelysmallinmagnitude.Inother
words,
g
hasaweakresponseto
f
m
.
Wearesponsesignal
s
g
suchthat
s
g
(
m
)=max
x;y
^
c
f
m
;g
(
x;y
)
;m
=1
;:::;B:
(4)
2
https://www.github.com/PrincetonUniversity/APPLEpicker
3
Thenotation
g
2
R
n

n
simplymeansthatthesizeofaqueryimageis
n

n
anditscontentisreal-valued.
3
Thissignalisassociatedwithasinglequeryimage
g
.Eachen-
try
s
g
(
m
)
containsthemaximalnormalizedcross-correlation
withasinglereferenceimage
f
m
.Thus,theresponsesignal
capturesthestrengthoftheresponseofthequeryimageto
eachofthereferenceimages.
Wesuggestthat
s
g
canbeusedtodeterminethecontentof
g
.Ifthequeryimagecontainsaparticle,
s
g
willshowahigh
responsetoreferenceimagescontainingaparticlewithsim-
ilarviewandacomparativelylowresponsetootherimages.
Asaconsequence,
s
g
willhaveseveralhighpeaks.Onthe
otherhand,ifthequeryimagecontainsnoisealone,
s
g
will
haverelativelyuniformcontent.ThisideaisshowninFigure
2.
Figure2:Responsesignalofaparticleimage(top)andanoiseimage(bot-
tom).Theleftcolumncontainstheresponsesignals.Therightcolumncon-
tainshistogramsoftheresponsesignals.
Theaboveistruedespitethehighrateoffalsepositivesin
cross-correlation-basedmethods.Thisisduetothecompari-
sonofeachqueryimagetomultiplereferencewindows.The
redundancycausesrobustnesstofalsepositives.
2.2.ReferenceSetSelection
Thesetofreferenceimages
f
f
m
g
B
m
=1
couldcontainall
possiblewindowsinthemicrograph.However,thiswould
leadtounnecessarilylongruntimes.Thus,wesuggestto
chooseasubsetof
B
windowsfromthemicrograph,where
eachofthesewindowsiseitherlikelytocontainaparticleor
likelytocontainnoisealone.
Inordertoautomaticallyselectthissubset,wedivide
themicrographinto
B=
4
non-overlapping
containers
.Acon-
tainerissomerectangularportionofthemicrograph.Each
containerholdsmany
n

n
windows.Figure3(a)isanexam-
pleofthedivisionofamicrographintocontainers.
AsmentionedinSection1(fourthparagraph),regionscon-
tainingnoisyprojectionsofparticlestypicallyhavelowerin-
tensityvaluesandhighervariancethanregionscontaining
noisealone.Thuswethatthewindowwiththelowest
meanintensityineachcontainerlikelycontainsaparticleand
thewindowwiththehighestmeanintensitylikelydoesnot.
Weextractthesewindowsfromeachcontainerandinclude
theminthereferenceset.Wedothisalsoforthewindowsthat
havethehighestandlowestvarianceineachcontainer.This
procedureprovidesasetof
B
referencewindows.Figure3(b)
presentsthereferencewindowsextractedfromasinglecon-
tainer.Wesuggestsetting
B
toapproximately
300
.
(a)
(b)
Figure3:(a)Containersofamicrographofthe

-Galactosidasedataset.(b)
Singlecontainerwithfourwindowsofinterest.
Thesetofreferencewindowsmustcontainbothwindows
withnoiseandwindowswithparticles.Itmayseemcounter-
intuitivetoincludenoisewindowsinareferenceset.How-
ever,forroughlysymmetricparticles(
i.e.
,particleswithsim-
ilarprojectionsfromeachangle),anyqueryimagewillhave
asimilarresponsetoeveryreferenceimagewhichcontainsa
particle.Thus,ifnoiseimageswerenotincludedintherefer-
enceset,theresponsesignal
s
g
wouldbeuniformregardless
ofthecontentof
g
.
2.3.GeneralizationtoMicrographs
Weextractasetof
M
queryimagesfromthemicrograph.
Theseimagesshouldhavesomeoverlap.Inaddition,their
unionshouldcovertheentiremicrograph.Forexample,we
canchoosewindowsonagridwithstepsize
n=
2
.Inorderto
determinethecontentofeachqueryimage
g
,weexaminethe
numberofentriesthatareoveracertainthreshold,
i.e.
,
k
(
s
g
)=
jf
i
suchthat
s
g
(
i
)
>t
gj
;
(5)
4
(a)
(b)
(c)
Figure4:Resultofourcross-correlationscheme.(a)Micrographof

-Galactosidase.(b)The
1000
regionscontainedinboxeshavehigh
k
(
s
g
)
andthusshould
containaparticle.(c)Thereare
9000
regionscontainedinboxes.Theseregionshavehighorintermediate
k
(
s
g
)
andthusmaycontainaparticle.Consequently,
theregionsnotcontainedinboxeshavelow
k
(
s
g
)
andthusarelikelytobepurenoise.
wherethethreshold
t
isdeterminedaccordingtothesetof
responsesignalsandisexperimentallysetto
t
=
max
g;i
s
g
(
i
)

min
g;j
s
g
(
j
)
20
+min
g;j
s
g
(
j
)
:
(6)
Anyqueryimage
g
thatpossesseshigh
k
(
s
g
)
isknownto
havehadarelativelystrongresponsetoalargeamountofref-
erencewindowsandisthusexpectedtocontainaparticle.On
theotherhand,aqueryimage
g
thatpossesseslow
k
(
s
g
)
is
expectedtocontainnoise.Inthismannerwemayconsider
k
(
s
g
)
asascorefor
g
.Thehigherthisscore,themore
dentwecanbethat
g
containsaparticle.
Thestrengthoftheresponse,andthusthescoreofaquery
image,isdeterminedbythethreshold
t
.Insteadofchecking
theuniformityoftheresponsesignalforasinglequeryimage
aswasdoneinSection2.1,weusetheresponsesignalsofthe
entiresettodetermineathresholdabovewhichweconsidera
responsetobestrong.
Forvisualizationofoursuggestedframework,weturnto
amicrographof

-Galactosidase(Scheres,2015;Chenetal.,
2013;Scheres&Chen,2012).Weselect
B
=324
refer-
enceimagesinthemannerdetailedinSection2.2,andaimto
classify
21904
queryimages.Thequeryimagesareselected
fromlocationsthroughoutthemicrographinawaythaten-
suressomeoverlapbetweenimages.Foreachqueryimage
wecomputethecorrespondingresponsesignalaccordingto
(4).Thethreshold
t
isthencomputedfromalltheresponse
signalsaccordingto(6).Oncethisisdone,thevalue
k
(
s
g
)
iscomputedforeachqueryimage.WepresentinFigure4
avisualizationoftheresults.Sinceweexpectqueryimages
thatcontainparticlestobeassociatedwithhigh-valued
k
(
s
g
)
,
wepresentthe
1000
queryimageswithhighest
k
(
s
g
)
.Figure
4(b)showsthat,asexpected,theseregionsdocontainpar-
ticles.Inaddition,wepresentthe
9000
queryimageswith
highest
k
(
s
g
)
.Theregionsnotcontainedinanyofthesequery
imagesareassociatedwithlow-valued
k
(
s
g
)
andcanbeseen
ininFigure4(c)tocontainnoparticle.
Wenotethatforthesakeofreducingthecomputational
complexityofoursuggestedframework,thecross-correlation
scoreiscomputedusingfastFouriertransforms.Thisisa
well-establishedmethodofreducingcomplexity(Nicholson
&Glaeser,2001).
2.4.APPLE
Aparticlepickingframeworkshouldproduceasinglewin-
dowcontainingeachpickedparticle.Itispossibletousethe
outputofthecross-correlationschemeintroducedinSections
2.1Œ2.3asthebasisofaparticlepicker.Thisisdoneby
ingthequerysettobethesetofallpossible
n

n
windows
containedinthemicrograph.Thecontentofeachquerywin-
dowisdeterminedaccordingtoitsscore.,ifthe
scoreisaboveathresholdwedeterminethatitcontainsapar-
ticle.Thisdeterminationcanbeappliedtothelocationofthe
centralpixelinthatwindowtoprovideaofeach
pixelinthemicrograph(exceptforboundarypixelsthatare
notinthecenterofanypossible
n

n
window).Unfortu-
nately,thecostofsuchanendeavor,bothinruntimeandin
memoryconsumption,isprohibitive.
Inordertoimproveperformance,theAPPLEpickerdoes
notthesetofqueryimagesasallpossible
n

n
win-
dowsinthemicrograph.Instead,thesetofqueryimages
5
f
h
m
g
C
m
=1
isasthesetofall
n

n
windowsextracted
fromthemicrographat
n=
2
intervals.However,applyinga
determinationtothecenterpixelofeachquerywindowwill
nolongerallowforsuccessfulparticlepicking.Indeed,where
twooverlappingquerywindowsaredeterminedtocontaina
particle,itisunknownwhethertheybothcontainthesamepar-
ticleorwhethereachcontainsadistinctparticle.Itispossible
thattheintervalof
n=
2
betweenthequerywindowscausedus
toskipoverwindowsthatwouldhavebeenasnoise.
Inotherwords,inordertogetagoodlocalizationofthepar-
ticle,thecontentofeachpossiblewindowofthemicrograph
shouldbedetermined.
Toachievethis,theAPPLEpickerdeterminesthecontent
ofallpossiblewindowsinthemicrographviaasupportvec-
tormachine(SVM).Thisisbasedonafew
simpleandeasilycalculatedfeaturesthatareknowntodiffer
betweenparticleregionsandnoiseregions.Inthismannerwe
achievefastandlocalizedparticlepicking.Theis
trainedontheimageswhose(asparticleoras
noise)isgivenwithhighbyourcross-correlation
scheme.
Totrainthe,weneedatrainingset.Thisiscom-
posedofasetofexamplesfortheparticleimages,
S
1
,and
asetofexamplesforthenoiseimages,
S
2
.Thecomplete
trainingsetis
S
1
[
S
2
.Thechoiceof
S
1
and
S
2
dependson
twoparameters,
˝
1
and
˝
2
.Theseparameterscorrespondto
thepercentageoftrainingimagesthatwebelievedocontain
aparticle(
˝
1
)andthepercentageoftrainingimagesthatwe
believemaycontainaparticle(
˝
2
).
Theselectionof
˝
1
and
˝
2
canbemadeaccordingtothe
concentrationoftheparticleprojectionsinthemicrograph.
Thisinformationcanbeestimatedvisuallyatthetimeofdata
collectionfromasetofinitialacquiredmicrographs.
Todemonstratetheselectionof
˝
1
and
˝
2
,weconsiderami-
crographwith
M
=20000
queryimages.Ifitisknownthat
thereisamidtohighconcentrationofprojectedparticles,we
cansafelyassumethat,
e.g.
,
1000
imageswithhighest
k
(
s
h
m
)
containaparticle.Thusweset
˝
1
=5%
.Inaddition,itispos-
siblethatoutof
20000
queryimages
15000
maycontainsome
portionofaparticle.Wecanthereforesafelyassumethatthe
regionsofthemicrographthatarenotcontainedinanyofthe
˝
2
=75%
imageswithhighest
k
(
s
h
m
)
willberegionsof
noise.
Whentheconcentrationofparticleprojectionsisunavail-
able,theselectionof
˝
1
and
˝
2
canbedoneheuristically.For
instance,
˝
1
=5%
and
˝
2
=75%
isoftenagoodselection
for
˝
1
and
˝
2
.Wenotethatwhentheconcentrationofmacro-
moleculesisnothigh,thevalueof
˝
2
islessimportantthan
thatof
˝
1
.
Once
˝
1
isselected,theset
S
1
isdetermined.Duetothe
overlappingnatureofqueryimages,thereisnoneedtouse
all
˝
1
percentofimageswithhighest
k
(
s
h
m
)
fortraining.In-
stead,wenotethattheseimagesformseveralconnectedre-
gionsinthemicrograph(seeFigure4).Theset
S
1
ismadeof
allnon-overlappingwindowsextractedfromtheseregions.
The
˝
2
percentofqueryimageswithhighest
k
(
s
h
m
)
form
theregionsinthemicrographthatmaycontainparticles.An
exampleoftheseregionscanbeseeninFigure4(c).Theset
S
2
ismadeofnon-overlappingwindowsextractedfromthe
complementoftheseregions.Thereasonforthedifference
betweenthedeterminationof
S
1
and
S
2
isthatthequeryim-
agesoverlap,andwedonotwanttotrainthenoisemodelfrom
anysectionofthe
˝
2
percentofqueryimageswithmoderate
tohigh
k
(
s
g
)
.
Thetrainingsetfortheconsistsofvectors
x
1
;:::;
x
j
S
1
[
S
2
j
2
R
2
+
andlabels
y
1
;:::;y
j
S
1
[
S
2
j
2f
0
;
1
g
.
Eachvector
x
i
inthetrainingsetcontainsthemeanandstan-
darddeviationofawindow
h
i
2
S
1
[
S
2
,andisassociated
withalabel
y
i
,where
y
i
=
(
1
;
if
h
i
2
S
1
:
0
;
if
h
i
2
S
2
:
Wenotethatwhiletrainingtheonmeanandvari-
anceworkssufwell,theyarenotnecessarilyoptimal
andotherfeaturescanbeadded.Thisisthesubjectoffuture
work.
Thetrainingsetisusedinordertotrainasupportvector
machine(Sch
¨
olkopf&Smola,2001;Cortes&Vap-
nik,1995).WeproposeusingaGaussianradialbasisfunction
SVM.Oncetheistrained,apredictioncanbeob-
tainedforeachwindowinthemicrograph.This
isattributedtothecentralpixelofthewindow,thusclassi-
fyingeachpixelinthemicrographaseitheraparticleora
noisepixel.Thisprovidesuswithasegmentationofthemi-
crograph.Figure1(b)presentssuchasegmentationforthe
micrographdepictedin1(a).Forconvenience,wesummarize
ourframeworkinFigure5.
2.5.APPLEPicking
Theoutputoftheisabinaryimagewhereeach
pixelislabeledaseitherparticleorasnoise.Eachconnected
region(cluster)ofparticlepixelsmaycontainaparticle.On
theotherhanditmaycontainsomeartifact.Thus,wedisre-
gardclustersthataretoosmallortoobig.Thisisdonethrough
examiningthetotalnumberofpixelsineachcluster,anddis-
cardinganythatareaboveorbelowareasonablenumberof
pixels.Thisnumberisselectedbasedonthetrueparticlesize.
6
Figure5:OverviewofAPPLEpicker.
Alternatively,thiscanbedonethroughuseofmorphologi-
caloperations.Anerosion(Efford,2000)isamorphological
operationpreformedonabinaryimagewhereinpixelsfrom
eachclusterareremoved.Thepixelstoberemovedaredeter-
minedbyproximitytotheclusterboundary.Inthisway,the
erosionoperationshrinkstheclustersofabinaryimage.This
shrinkagecanbeusedtodeterminetheclustersthatcontain
artifacts.Largeartifactswillremainwhenshrinkingbyafac-
torlargerthantheparticlesize.Smallartifactswilldisappear
whenshrinkingbyafactorsmallerthantheparticlesize.We
usethismethodofartifactremovalinSection3.4.Wenote
thatasimilarmethodforcontaminantremovalwasusedin
AutoPicker(Langloisetal.,2014).
Beyondtheseartifacts,itispossiblethattwoparticlesare
frozenveryclosetogether.Thiswilldistortthetrueparti-
cleprojectionandshouldbedisregarded.Forthisreasonit
isgoodpracticetodisregardpairsofclustersofpixelsthat
wereasparticleiftheyaretooclose.Wedothis
bydisregardingclusterswhosecentersarecloserthansome
distance,forexampletheparticlediameter.Wethenoutput
aboxaroundthecenterofeachremainingclusterofpixels
thatwereasparticle.Thesizeoftheboxisdeter-
minedaccordingtotheknownparticlesize.Thepixelcontent
ofeachboxisaparticlepickedbyourframework.SeeFigure
1(c).
Afterallparticlesarepicked,itispossibletocreatetem-
platesoutofthemanduseatemplatematchingschemeto
pickadditionalparticles,asin(Frank&Wagenknecht,1983;
Ludtkeetal.,1999;Scheres,2015).
2.6.CTFcorrection
Intheprocessofacquiringthemicrographeachparticle
projectionisconvolvedwithapointspreadfunction.This
functionistheinverseFouriertransformofafunctioncalled
theContrasttransferfunction(CTF),whichisasfol-
lows(Mindell&Grigorieff,2003)
CTF
(
g
)=

p
1

A
2
sin(
˜
)

A
cos(
˜
)
˜
=
ˇ
2

f

ˇ
2
C
s

3
g
4
;
(7)
where

f
isthedefocus,

isthewavelength,
g
istheradial
frequency,
C
s
isthesphericalaberrationand
A
istheampli-
tudecontrast.
Awell-knowneffectoftheCTFisincreasingthesupport
sizeoftheprojectionimage.Thiseffectmaycausenearby
particleprojectionstobecomediftodistinguish.Another
issueisthattheCTFdecreasesthecontrastoftheprojection
images,whichmakesthemhardertoDuetotheabove,
whilethereisnostrictnecessitytoapplytheAPPLEpickerto
CTF-correctedmicrographs,itisgoodpracticetodoso.The
problemsofCTFestimation(Rohou&Grigorieff,2015)and
CTF-correction(Downing&Glaeser,2008;Turo

nov
´
aetal.,
2017)arewellresearchedproblems.WeuseCTFFIND4(Ro-
hou&Grigorieff,2015)forCTFestimation.
OnemethodofCTF-correctioniswhich
preservesthestatisticsofthenoise,whileeffectivelyprevent-
ingtheCTFfromchangingsign.Whilethismethoddoesnot
correctfortheamplitudeoftheCTF,thephasecorrectional-
readybringsthesupportsizeclosetoitstruevalue.Italso
slightlyincreasestheparticlecontrast.
7
Figure6containsacomparisonbetweenourparticlepick-
ingframeworkwhenappliedtothemicrographswithand
withoutWenotethat,whilemostofthepicked
particlesappearinbothmicrographs,thereareslightdiffer-
encesaroundsomeofthenear-byparticles.Thus,werecom-
mendthismethodwhenapplyingCTFcorrectiontomicro-
graphsbeforeparticlepicking.
3.ExperimentalResults
Wepresentexperimentalresultsfortheframeworkpre-
sentedinthispaper.Weapplyourframeworktodatasetsof

-
Galactosidase,T20Sproteasome,70Sribosomeandkeyhole
limpethemocyanin(KLH)particles.
The

-Galactosidasedatasetweuseispubliclyavail-
ablefromEMPIAR(theElectronMicroscopyPublicImage
Archive)(Iudinetal.,2016)asEMPIAR-10017.
4
Itconsists
of84micrographsof

-Galactosidase.TheT20Sproteasome
datasetispubliclyavailableasEMPIAR-10057
5
(Danev&
Baumeister,2016).Itcontains
158
micrographs.The70Sri-
bosomedatasetisavailableasEMPIAR-10077(Fischeretal.,
2016)andcontainsthousandsofmicrographs.TheKLH
datasetweuse(Zhuetal.,2004,2003)contains82micro-
graphs.
Theexperimentsarerunona
2
:
6GHz
IntelCorei7CPU
withfourcoresand
16GB
ofmemory.Ourmethodhasalso
beenimplementedonaGPU.ItisevaluatedusinganNvidia
TeslaP100GPU.
3.1.

-Galactosidase
Weranthesuggestedframeworkona

-Galactosidase
dataset(Scheres,2015).Wecomparetheperformanceofthe
APPLEpickertothesemi-automatedparticlepickerincluded
inRELION.Forthiscomparison,weinputthelocationsofour
pickedparticlesintoRELIONandobtaina3Dreconstruction.
Wethencomparethistothereconstructionobtainedbythe
fullRELIONpipelinein(Scheres,2015).
The

-Galactosidasemicrographsareobtainedusinga
FALCONIIdetector.Thus,eachmicrographisofsize
4096

4096
pixels.Theoutermostpixelsinthesemicro-
graphsdonotcontainimportantinformation.Inlightofthis,
whenrunningtheAPPLEpickeronthesemicrographs,we
discardthe
100
outermostpixels.Inaddition,forruntimere-
duction,eachdimensionofthemicrographisreducedtohalf
itsoriginalsize,bringingthemicrographintotaltoaquarter
ofitsoriginalsize.Thisisdonebyaveragingadjacentpixels,
alsoknownasbinning.
Eachqueryandreferenceimageextractedfromthereduced
micrographisofsize
26

26
andeachcontainerisofsize
225

225
.Fortrainingwesuggesttouse
˝
1
=3%
and
˝
2
=55%
todeterminethetrainingset.Wesettheband-
widthofthekernelfunctionfortheSVMandits
slackparameterbothto
1
.ExamplesofresultsfortheAPPLE
pickerarepresentedinFigure7.
Forthepurposeofevaluatingourframework,weperform
a3Dreconstructionoftheparticleandcomparetotherecon-
structionof(Scheres,2015)wheretheparticlepickingwas
donebasedon
2555
manuallyselectedparticles.Fromthese
particles,
25
classaverageswerecomputedand
10
wereman-
uallychosen.TheRELIONparticlepickerthenpicked
52495
particles.Ofthese,
4185
particleswerediscardedaccordingto
Z-scores.Aftertheclassaveragingstep
42755
particleswere
selected.Thereportedresolutionin(Scheres,2015)is
4
:
2

A.
Incontrast,weuse
32997
particlesselectedbytheAPPLE
picker.WeenterthemintotheRELIONpipelineandbeginthe
reconstructionfromourparticles.Afterthe2Dclassaverag-
ingstep
15198
particleswereselected.The3Dreconstruction
usingRELION(includingCTFcorrectionusingthewrapper
forCTFFIND4(Rohou&Grigorieff,2015))reachedagold-
standardFSCresolutionof
4
:
5

A.
6
Wepresentacomparisonofsurfaceviewsfromthemodel
reconstructedfromparticlesselectedbytheAPPLE-Picker(in
red)andthereconstructedmodelby(Scheres,2015)inFigure
8.TheserenderingsweredoneinUCSFChimera
7
(Pettersen
etal.,2004).
Runtimeforasinglemicrographisapproximatelytwomin-
uteswhenrunningontheCPU.Thus,theentiredatasetcanbe
processedinunder
3
hours.TheGPUimplementation,onthe
otherhand,takesapproximately
8
seconds.Inotherwords,
theAPPLEpickerprocessesall
84
micrographinunder
15
minutes.Thisisfasterthanmanualpicking.
4
ThisdatasetwasobtainedbytheFALCONIIdirectdetector.Another

-GalactosidasedatasetisEMPIAR-10061(Bartesaghietal.,2015),whichwas
obtainedusingtheK
2
directdetector.WenotetheAPPLEpickeriseffectiveforthisdatasetaswell.ForacomparisonbetweenFALCONIIandK
2
direct
detectorssee(McMullanetal.,2014).
5
ThisdatasetwasobtainedusingtheK
2
directdetector.
6
WerepeatedthisexperimentforCTF-correctedmicrographsandachievedthesameresolution.Anotherexperimentweperformedwas3Dreconstruction
fromthemanuallyselectedparticlesavailablewiththe

-Galactosidasedataset.Whiletheaccuracyofthiswasreportedin(Scheres,2015)tobe
4
:
2

A,we
achieveanimprovementof
0
:
05

Aresolutionoverthe3DreconstructionfromtheAPPLEpickedparticles.
7
http://www.rbvi.ucsf.edu/chimera.
8
(a)
(b)
Figure6:IllustrationofpickingwithandwithoutCTFcorrection.(a)ACTF-correctedmicrograph.CTFestimationwasdoneusingCTFFIND4(Rohou&Grig-
orieff,2015)followedby(b)Particlespickedfromtheoriginalmicrographaresurroundedbyablackbox.Particlespickedfrom
micrographsaresurroundedbyaredcircle.
3.2.T20Sproteasome
TheT20Sproteasome(Danev&Baumeister,2016)dataset
ispubliclyavailableasEMPIAR-10057.Itsmicrographswere
acquiredusingaK
2
directdetector.Thus,theyaresized
3838

3710
pixels.UnlikethedatasetpresentedinSection
3.1,thisdatasetcontainelongatedparticles.
Onceagain,weusebinningtoreducethesizeofthemicro-
graphs.Eachqueryandreferenceimageextractedfromthe
reducedmicrographisofsize
24

24
.Weusethesamecon-
tainersize,
˝
1
,
˝
2
andSVMparametersasreported
inSection3.1.ExamplesofresultsfortheAPPLEpickerare
presentedinFigure10.
Wecorrectedformotionusingunblur(Grant&Grig-
orieff,2015).WeappliedtheAPPLEpickertothemotion-
correctedmicrographsandextracted
21791
particles.These
particleswereenteredintotheRELIONpipeline.Afterthe
classaveragingstep
15252
particleswereselected.The3D
reconstructionofRELIONreachedagold-standardFSCres-
olutionof
3
:
4

A.
Wepresentacomparisonofsurfaceviewsfromthemodel
reconstructedfromparticlesselectedbytheAPPLEpicker
(inred)andthereconstructedmodelby(Danev&Baumeis-
ter,2016)inFigure8.TheserenderingsweredoneinUCSF
Chimera(Pettersenetal.,2004).
Runtimeisapproximately
90
secondspermicrograph
whenrunningonaCPU,or
7
secondspermicrographwhen
runningontheGPU.
3.3.70Sribosome
WeexaminetheEMPIAR-10077(Fischeretal.,2016)
dataset.Themicrographareofsize
4096

4096
andcon-
tainlargeparticles.Eachqueryandreferenceboxisofsize
40

40
pixelsinthereducedmicrograph.Forthisreasonthe
containersizeweuseis
500

500
.Thisreducesthenumberof
containersandthuscausesthenumberofreferencewindows
tobesmaller.
Fortrainingwesuggesttouse
˝
1
=7%
and
˝
2
=7%
todeterminethetrainingset(seeSection4.2fora
discussionaboutthechoiceofparameters.)Wesettheband-
widthofthekernelfunctionfortheSVMandits
slackparameterbothto
1
.ExamplesofresultsfortheAPPLE
pickerarepresentedinFigure11.Runtimeisapproximately
2minutespermicrographontheCPU,orapproximately
14
secondspermicrographontheGPU.
3.4.KLH
ThemicrographsintheKLHdataset(Zhuetal.,2004,
2003)areofsize
2048

2048
.Tolowerruntimeweonce
againperformbinning.Followingthisreductioninsize,we
usequeryandreferenceimagesofsize
30

30
andcontainers
ofsize
115

115
.ThetrainingsetfortheSVMis
determinedusingthethresholds
˝
1
=16%
and
˝
2
=70%
.We
usethesameofthe(bandwidthand
slackparameter)asinthepreviousexperiments.
9
Figure7:Pickedparticlesofsample

-Galactosidasemicrographs.Themicrographsarepresentedintheleftcolumn.resultsarepresentedinthe
center.Thepickedparticlesareontheright.
10
Figure8:ComparisonbetweentheAPPLEpickerandtheRELIONsemi-automaticparticlepicker.Onthetoparesurfaceviewsofthe3Dreconstructioncreated
inRELIONfromtheAPPLEpicksandobtainedinUCSFChimera.Onthebottomaresurfaceviewsofthe3Dviewsdetailedin(Scheres,2015).Weusethe
referencevolumepublishedonEMDB(EMD-2824)andobtaintheviewsinUCSFChimera.
Figure9:ComparisonbetweentheAPPLEpickerandtheparticlespickedin(Danev&Baumeister,2016).Onthetopareviewsofthe3Dreconstructioncreated
inRELIONfromtheAPPLEpicksandobtainedinUCSFChimera.Onthebottomareviewsofthe3Dreconstructiondetailedin(Danev&Baumeister,2016).
WeusethereferencevolumepublishedonEMDB(EMD-3347)andobtaintheviewsinUCSFChimera.
11
Figure10:PickedparticlesofsampleT20Sproteasomemicrographs.Themicrographsarepresentedintheleftcolumn.resultsarepresentedin
thecenter.Thepickedparticlesareontheright.
12
Figure11:Pickedparticlesofsample70Sribosomemicrographs.Themicrographsarepresentedintheleftcolumn.resultsarepresentedinthe
center.Thepickedparticlesareontheright.
13
WepresentinFigures12and13someresultsfortheAP-
PLEpickerontheKLHdataset.Wenotetheseshow
twotypesofisoformsofKLH.Theseisoformsareidenti-
in(Roseman,2004)asKLH1(shortparticles)andKLH2
(longparticles).WeaimtotheKLH1particles.
AsdetailedinSection2.4,weuseonlymeanandvariance
fortraining.Anissuewiththispracticeisexempli-
bythehollowKLHparticles.Awindowcontainingsome
regionsoftheparticleandsomeregionsofnoisethatareinter-
naltothehollowparticleisindistinguishablefromawindow
containingsomeregionsoftheparticleandsomeregionsof
noisethatareexternaltotheparticle.Thisleadsthe
toidentifyaringofpixelsaroundtheparticleasbelongingto
theparticle.Dependingontheconcentrationofparticlesin
themicrograph,particlesmaymergetogetherintheoutputof
the.
Weusemorphologicalerosiontoaddressthisproblem.
Thisprocess,detailedinSection2.5,willdiscardallcon-
nectedcomponentswithmaximumdiametersmallerthan
132
pixelsandlargerthan
184
pixels(wherethediameterofthe
KLHparticlesareapproximately
160
pixels).Inaddition,it
willseparateadjacentparticlesconnectedbyanarrowband
ofpixels.Thispracticeisusefulincaseswhereparticlepro-
jectionsarecloseenoughthattheringsofpixelsaroundeach
particlewillmerge,butdistantenoughthatthemergingisre-
strictedtoanarrowregionbetweentheparticles.
Figure12containsmicrographswheretheparticlesareei-
thercompletelyisolatedordistantenoughthatthemorpho-
logicalerosioncanseparatethepixelsthatwereas
belongingtoeachoftheparticles.Thisisthecaseinwhichthe
APPLEpickerissuccessfuldespitethehollowparticles.Fig-
ure13containsmicrographswheretheparticlesareclustered
closelytogether,causingtheAPPLEpickertotreatmanypar-
ticlesasasingleregionandthusdiscardthem.Itisclearthat
theAPPLEpickerisnotsuitedtopickhollowparticlesthat
appearwithahighconcentration.Weleaveittofuturework
tosolvethisissuethroughadditionofmorediscriminativefea-
turestotheSVM.
AnotherissuewiththeKLHdatasetisthatdifferentmicro-
graphshavevastlydifferentconcentrationsofparticles.This
makesitdiftoselectasinglevalueof
˝
1
thatworkswell
onallthemicrographs.Anexampleofthisisshowninthelast
rowofFigure13.Whenusing
˝
1
=5%
theAPPLEpicker
performswellonthismicrograph.
Oursuggestedframeworkprocessesall
82
micrographsin
under
30
minutesontheCPUandin
4
minutesontheGPU.
4.Discussion
4.1.AComparisonBetweentheAPPLEpickerandExisting
ParticlePickers
Automaticparticlepickerscanbedividedintotwogroups
(Vossetal.,2009).Theofthesegroupsconsistsofmeth-
odsthatassumetemplatesoftheparticleareknownapriori.
Thisknowledgemayexistduetouserprovidedinformation,
projectionsfromsomepredeterminedinitialmodel,
etc
.The
secondgroupimposesmathematicalassumptionsonthepar-
ticle.
Obtaininguser-providedtemplatesishighinusereffort.
RELION(Scheres,2015),forexample,necessitatesauser
tochoose1000Œ2000particleprojectionsfromseveralmicro-
graphs.Thisprocessiscostlyinbotheffortandtime.On
theotherhand,usingsomeinitialmodelmaybiastheparticle
pickingprocess(Henderson,2013).
Imposingmathematicalassumptionsontheparticlecan
producegoodresultssolongastheassumptionsaresatis-
bytheparticle.Incontrast,theAPPLEpickerdoesnot
makeassumptionsontheparticleotherthanusingthewell-
establishedfactthatprojectionimagesandnoiseregionsdiffer
intheirmeanintensityandvariance.
AnotheradvantageoftheAPPLEpickeristhatitsreference
setcontainsredundancy.Thisaddsarobustnesstofalsepos-
itivesthatismissingfromtraditionalcross-correlationmeth-
ods.
Thus,theAPPLEpickerisasimple,robustandfastparti-
clepickerwhichrequireslowusereffortandassumesnoprior
knowledgeoftheparticleotherthanitssize.
Wenotethatthereisanassumptiononthe
artifacts
that
maybeviolated,namelythesizeassumption.Ifthisassump-
tionisviolated,regionscontainingartifactscanbemistaken
forparticleprojections.However,thiscaneasilybecorrected
inthe2Dstep.
4.2.Selectionof
˝
1
and
˝
2
InSection3wepresentseveraldatasetswithdifferentval-
uesof
˝
1
and
˝
2
.Forthe

-GalactosidaseandT20Sprotea-
somedatasetsweusethesamevalues.Inthissectionweex-
plainthedifferenceinvaluesfromthe70SribosomeandKLH
dataset.
Thevalueof
˝
1
determinesthepercentageofqueryimages
thatwebelievecontainaparticle.Whilethisvalueisdif-
ferentbetweenthedatasets,theactualnumberofqueryim-
agesdeterminedby
˝
1
issimilarforalldatasets,andaround
500

800
.Thedifferenceisthatthe70Sribosomedatasetuses
largerqueryimageswhichcauseseachmicrographtocontain
lessofthem.TheKLHmicrographsaremuchsmallerthanthe
14
Figure12:ResultonKLHdataset.Theleftcolumncontainsmicrographs.Themiddlecolumncontainsthewindowsasparticlebyour.The
rightcolumncontainsthepickedparticles.
15
Figure13:ResultonKLHdataset.Theleftcolumncontainsmicrographs.Themiddlecolumncontainsthewindowsasparticlebyour.The
rightcolumncontainsthepickedparticles.
16
micrographsoftheotherdatasetsandthus,onceagain,each
micrographcontainslessqueryimages.
Thevalueof
˝
2
isadifferentmatter.Wherethequeryim-
agesarelarge,theytendtocovermoreofthemicrograph.
8
Thismaynotleavemanyareaslargeenoughtoextracttrain-
ingwindowsofnoise.Thus,wemustusesmallervaluesof
˝
2
.AnexampleofthisispresentedinFigure14whichshows
(inwhite)thelocationsofthe
50%
ofwindowsthatpossess
thehighervaluesof
k
(

)
forthe

-Galactosidaseandforthe
70Sribosomedatasets.
Figure14:Selectionof
˝
2
fora

-Galactosidasesamplemicrograph(left)
anda70Sribosomesamplemicrograph(right).Thewhiteregionscontain
the
50%
ofqueryimageswiththehighest
k
(
s
g
)
.Theblackregionsarethe
regionsfromwhichthetrainingwindowsofnoiseareextracted.Wenotethat
whilethe

-Galactosidasesamplewillhaveplentyoftrainingwindowsfor
noise,the70Sribosomesamplewillnot.
Inconclusion,formicrographsofsize
4
k

4
k
wherepar-
ticlesaresmallandtheirconcentrationissimilartothatof
themicrographswepresentedinSection3,wesuggestus-
ing
˝
2
=50%

55%
.Forlargerparticleswesuggestusing
˝
1
ˇ
˝
2
.Inthefuture,theAPPLEpicker'scodewillauto-
maticallylower
˝
2
untilaminimalamountofnoisetraining
windowsareextracted,inwhichcasethisissuewillnolonger
beaconsiderationfortheuser.
5.Conclusion
InthispaperwehavepresentedtheAPPLEpicker,asim-
pleandfastframework,inspiredbytemplatematching,for
fullyautomatedparticlepicking.TheAPPLEpickerhastwo
mainsteps.Thestepdeterminesthecon-
tentofqueryimagesaccordingtotheirresponsetoasetof
automaticallychosenreferences.Theseresultsareusedto
trainasimple.Wepresentedexperimentalresultson
fourdatasets,andshowedthetypeofparticlesforwhichthis
frameworkiswellsuitedandthereasonourmayen-
counterdif.Weleaveittofutureworktosolvethese
issues.WebelievethattheAPPLEpickerbringsusonestep
closertowardsafullyautomatedcomputationalpipelinefor
highthroughputsingleparticleanalysisusingcryo-EM(Bald-
winetal.,2018).
Acknowledgments
TheauthorswerepartiallysupportedbyAwardNumber
R01GM090200fromtheNIGMS,BSFGrantNo.2014401,
FA9550-17-1-0291fromAFOSR,SimonsInvestigatorAward
andSimonsCollaborationonAlgorithmsandGeometryfrom
SimonsFoundation,andtheMooreFoundationData-Driven
DiscoveryInvestigatorAward.
TheauthorswouldliketothankFredSigworthforhisin-
valuableconversationsandinsightintotheparticlepicking
problemandforhisreviewofthemanuscriptsandsubse-
quentandhelpfulsuggestions.Theauthorsarealsoindebted
toPhilipR.Baldwinforsharinghisexpertiseontheparticle
pickingproblemaswellashisreviewoftheAPPLEpicker
code.
Moleculargraphicsandanalyseswereperformedwiththe
UCSFChimerapackage.ChimeraisdevelopedbytheRe-
sourceforBiocomputing,Visualization,andInformaticsat
theUniversityofCalifornia,SanFrancisco(supportedby
NIGMSP41-GM103311).
Thisresearchwascarriedout,inpart,whiletheauthor
wasavisitingstudentresearchcollaboratorandthesecondau-
thorwasapostdoctoralresearchassociateattheProgramfor
AppliedandComputationalMathematicsatPrincetonUniver-
sity.
References
Aebel
´
aez,P.,Han,B.-G.,Typke,D.,Lim,J.,Glaeser,R.M.,
&Malik,J.(2011).Experimentalevaluationofsupportvec-
tormachine-basedandcorrelation-basedapproachestoau-
tomaticparticleselection.
JournalofStructuralBiology
,
175
,319Œ328.
Baldwin,P.R.,Tan,Y.Z.,Eng,E.T.,Rice,W.J.,Noble,A.J.,
Negro,C.J.,Cianfrocco,M.A.,Potter,C.S.,&Carragher,
B.(2018).BigdataincryoEM:automatedcollection,pro-
cessingandaccessibilityofEMdata.
CurrentOpinionin
Microbiology
,
43
,1Œ8.
8
Especiallysincethereisoverlapbetweenthequeryimages,asdetailedinSection2.4.
17
Bartesaghi,A.,Merk,A.,Banerjee,S.,Matthies,D.,Wu,X.,
Milne,J.L.S.,&Subramanian,S.(2015).
2
:
2

Aresolu-
tioncryo-EMstructureof

-galactosidaseincomplexwith
acell-permeantinhinitor.
Science
,
348
,1147Œ1151.
Chen,J.Z.,&Grigorieff,N.(2007).SIGNATURE:A
single-particleselectionsystemformolecularelectronmi-
croscopy.
JournalofStructuralBiology
,
157
,168Œ173.
Chen,S.,McMullan,G.,Faruqi,A.R.,Murshudov,G.N.,
Short,J.M.,Scheres,S.H.,&Henderson,R.(2013).High-
resolutionnoisesubstitutiontomeasureovandval-
idateresolutionin3Dstructuredeterminationbysinglepar-
ticleelectroncryomicroscopy.
Ultramicroscopy
,
135
,24Œ
35.
Cortes,C.,&Vapnik,V.(1995).Support-vectornetworks.
MachineLearning
,
20
,273Œ297.
Danev,R.,&Baumeister,W.(2016).Cryo-EMsingleparticle
analysiswiththevoltaphaseplate.
eLife
,
5:e13046
.
Downing,K.H.,&Glaeser,R.M.(2008).Restorationof
weakphase-contrastimagesrecordedwithahighdegreeof
defocus:TheﬁtwinimageﬂproblemassociatedwithCTF
correction.
Ultramicroscopy
,
108
,921Œ928.
Efford,N.(2000).
DigitalImageProcessing:APractical
IntroductionUsingJava
.(1sted.).Boston,MA,USA:
Addison-WesleyLongmanPublishingCo.,Inc.
Fischer,N.,Neumann,P.,Bock,L.V.,Maracci,C.,Wang,
Z.,Paleskava,A.,Konevega,A.L.,Schr
¨
oder,G.F.,
Grubm
¨
uller,H.,Ficner,R.,Rodnina,M.,&Stark,H.
(2016).ThepathwaytoGTPaseactivationofelongation
factorselbontheribosome.
Nature
,
540
,80Œ85.
Frank,J.,&Wagenknecht,T.(1983).Automaticselection
ofmolecularimagesfromelectronmicrographs.
Ultrami-
croscopy
,
12
,169Œ175.
Grant,T.,&Grigorieff,N.(2015).Measuringtheoptimal
exposureforsingleparticlecryo-EMusinga
2
:
6

Arecon-
structionofrotavirusVP6.
eLife
,
4:e06980
.
Harauz,G.,&Fong-Lochovsky,A.(1989).Automaticse-
lectionofmacromoleculesfromelectronmicrographsby
componentlabellingandsymbolicprocessing.
Ultrami-
croscopy
,
31
,333Œ344.
Henderson,R.(2013).Avoidingthepitfallsofsingleparti-
clecryo-electronmicroscopy:Einsteinfromnoise.
Pro-
ceedingsoftheNationalAcademyofSciencesoftheUnited
StatesofAmerica
,
110
,18037Œ18041.
Hoang,T.V.,Cavin,X.,Schultz,P.,&Ritchie,D.W.
(2013).gEMpicker:ahighlyparallelGPU-acceleratedpar-
ticlepickingtoolforcryo-electronmicroscopy.
BMCStruc-
turalBiology
,
13
,25.
Iudin,A.,Korir,P.,Salavert-Torres,J.,Kleywegt,G.,&Pat-
wardhan,A.(2016).EMPIAR:Apublicarchiveforraw
electronmicroscopyimagedata.
NatureMethods
,
13
.
Langlois,R.,Pallesen,J.,Ash,J.T.,Ho,D.N.,Rubinstein,
J.L.,&Frank,J.(2014).Automatedparticlepickingfor
low-contrastmacromoleculesincryo-electronmicroscopy.
JournalofStructuralBiology
,
186
,1Œ7.
Ludtke,S.J.,Baldwin,P.R.,&Chiu,W.(1999).EMAN:
Semiautomatedsoftwareforhigh-resolutionsingle-particle
reconstructions.
JournalofStructuralBiology
,
128
,82Œ97.
McMullan,G.,Faruqi,A.R.,Clare,D.,&Henderson,R.
(2014).Comparisonofoptimalperformanceat300keVof
threedirectelectrondetectorsforuseinlowdoseelectron
microscopy.
Ultramicroscopy
,
147
.
Mindell,J.A.,&Grigorieff,N.(2003).Accuratedetermi-
nationoflocaldefocusandspecimentiltinelectronmi-
croscopy.
JournalofStructuralBiology
,
142
,334Œ347.
Nicholson,W.V.,&Glaeser,R.M.(2001).Review:Auto-
maticparticledetectioninelectronmicroscopy.
Journalof
StructuralBiology
,
133
,90Œ101.
Ogura,T.,&Sato,C.(2004).Automaticparticlepickup
methodusinganeuralnetworkhashighaccuracybyap-
plyinganinitialweightderivedfromeigenimages:Anew
referencefreemethodforsingle-particleanalysis.
Journal
ofStructuralBiology
,
145
,63Œ75.
Pettersen,E.,Goddard,T.,Huang,C.,Couch,G.,Greenblatt,
D.,Meng,E.,&Ferrin,T.(2004).UCSFchimeraŒavi-
sualizationsystemforexploratoryresearchandanalysis.
JournalofComputationalChemistry
,
25
,1605Œ1612.
Rohou,A.,&Grigorieff,N.(2015).CTFFIND4:Fastandac-
curatedefocusestimationfromelectronmicrographs.
Jour-
nalofStructuralBiology
,
192
,216Œ221.
Roseman,A.(2004).FindEMŒAfast,efprogramfor
automaticselectionofparticlesfromelectronmicrographs.
JournalofStructuralBiology
,
145
,91Œ99.
Scheres,S.H.(2012a).Abayesianviewoncryo-EMstructure
determination.
JournalofMolecularBiology
,
415
,406Œ
418.
18
Scheres,S.H.(2012b).RELION:Implementationofa
bayesianapproachtocryo-EMstructuredetermination.
JournalofStructuralBiology
,
180
,519Œ530.
Scheres,S.H.(2015).Semi-automatedselectionofcryo-EM
particlesinRELION-1.3.
JournalofStructuralBiology
,
189
,114Œ122.
Scheres,S.H.,&Chen,S.(2012).Preventionofov
incryo-EMstructuredetermination.
NatureMethods
,
9
,
853Œ854.
Sch
¨
olkopf,B.,&Smola,A.J.(2001).
LearningwithKer-
nels:SupportVectorMachines,Regularization,Optimiza-
tion,andBeyond
.Cambridge,MA,USA:MITPress.
Shatsky,M.,Hall,R.J.,Brenner,S.E.,&Glaeser,R.M.
(2009).Amethodforthealignmentofheterogeneous
macromoleculesfromelectronmicroscopy.
Journalof
StructuralBiology
,
166
,67Œ78.
Sigworth,F.J.(2004).Classicaldetectiontheoryandthecryo-
EMparticleselectionproblem.
JournalofStructuralBiol-
ogy
,
145
,111Œ122.
Turo

nov
´
a,B.,Schur,F.K.,Wan,W.,&Briggs,J.A.(2017).
Ef3D-CTFcorrectionforcryo-electrontomography
usingNovaCTFimprovessubtomogramaveragingresolu-
tionto3.4

A.
JournalofStructuralBiology
,
199
,187Œ195.
vanHeel,M.(1982).Detectionofobjectsinquantum-noise-
limitedimages.
Ultramicroscopy
,
7
,331Œ341.
Voss,N.R.,Yoshioka,C.,Radermacher,M.,Potter,C.S.,
&Carragher,B.(2009).DoGpickerandTiltPicker:Soft-
waretoolstofacilitateparticleselectioninsingleparticle
electronmicroscopy.
JournalofStructuralBiology
,
166
,
205Œ213.
Wang,F.,Gong,H.,Liu,G.,Li,M.,Yan,C.,Xia,T.,Li,X.,
&Zeng,J.(2016).DeepPicker:Adeeplearningapproach
forfullyautomatedparticlepickingincryo-EM.
Journalof
StructuralBiology
,
195
,325Œ336.
Zhang,K.(2017).http://www.mrc-lmb.cam.ac.uk/kzhang/.
Zhu,Y.,Carragher,B.,Glaeser,R.M.,Fellmann,D.,Bajaj,
C.,Bern,M.,Mouche,F.,deHaas,F.,Hall,R.J.,Krieg-
man,D.J.,Ludtke,S.J.,Mallick,S.P.,Penczek,P.A.,
Roseman,A.M.,Sigworth,F.J.,Volkmann,N.,&Pot-
ter,C.S.(2004).Automaticparticleselection:Resultsof
acomparativestudy.
JournalofStructuralBiology
,
145
,
3Œ14.
Zhu,Y.,Carragher,B.,Mouche,F.,&Potter,C.S.(2003).
AutomaticparticledetectionthroughefHoughtrans-
forms.
IEEETransactionsonMedicalImaging
,
22
,1053Œ
1062.
Zhu,Y.,Ouyang,Q.,&Mao,Y.(2016).Adeeplearning
approachtosingle-particlerecognitionincryo-electronmi-
croscopy.
CoRR
,
abs/1605.05543
.
19
"
94,Variational Message Passing with Structured Inference Networks,http://arxiv.org/pdf/1803.05589v2.pdf,https://github.com/emtiyaz/vmp-for-svae,"PublishedasaconferencepaperatICLR2018
V
ARIATIONAL
M
ESSAGE
P
ASSINGWITH
S
TRUCTURED
I
NFERENCE
N
ETWORKS
WuLin

,NicolasHubacher

,MohammadEmtiyazKhan

RIKENCenterforAdavancedIntelligeneProject,Tokyo,Japan
wlin2018@cs.ubc.ca,nicolas.hubacher@outlook.com,emtiyaz@gmail.com
A
BSTRACT
Recenteffortsoncombiningdeepmodelswithprobabilisticgraphicalmodelsare
promisinginprovidingxiblemodelsthatarealsoeasytointerpret.Wepropose
avariationalmessage-passingalgorithmforvariationalinferenceinsuchmodels.
Wemakethreecontributions.First,weproposestructuredinferencenetworks
thatincorporatethestructureofthegraphicalmodelintheinferencenetworkof
variationalauto-encoders(VAE).Second,weestablishconditionsunderwhich
suchinferencenetworksenablefastamortizedinferencesimilartoVAE.Finally,
wederiveavariationalmessagepassingalgorithmtoperformefnatural-
gradientinferencewhileretainingtheefyoftheamortizedinference.By
simultaneouslyenablingstructured,amortized,andnatural-gradientinferencefor
deepstructuredmodels,ourmethodandgeneralizesexistingmethods.
1I
NTRODUCTION
Toanalyzereal-worlddata,machinelearningreliesonmodelsthatcanextractusefulpatterns.Deep
NeuralNetworks(DNNs)areapopularchoiceforthispurposebecausetheycanlearnxible
representations.Anotherpopularchoiceareprobabilisticgraphicalmodels(PGMs)whichcan
interpretablestructuresinthedata.Recentworkoncombiningthesetwotypesofmodelshopesto
exploittheircomplimentarystrengthsandprovidepowerfulmodelsthatarealsoeasytointerpret
(Johnsonetal.,2016;Krishnanetal.,2015;Archeretal.,2015;Fraccaroetal.,2016).
Toapplysuchhybridmodelstoreal-worldproblems,weneedefalgorithmsthatcanextract
usefulstructurefromthedata.However,thetwoeldsofdeeplearningandPGMstraditionally
usedifferenttypesofalgorithms.Fordeeplearning,stochastic-gradientmethodsarethemost
popularchoice,e.g.,thosebasedonback-propagation.Thesealgorithmsarenotonlywidelyap-
plicable,butcanalsoemployamortizedinferencetoenablefastinferenceattesttime(Rezende
etal.,2014;Kingma&Welling,2013).Ontheotherhand,mostpopularalgorithmsforPGMs
exploitthemodel'sgraphicalconjugacystructuretogaincomputationalefy,e.g.,variational
messagepassing(VMP)(Winn&Bishop,2005),expectationpropagation(Minka,2001),Kalman
(Ghahramani&Hinton,1996;2000),andmorerecentlynatural-gradientvariationalinfer-
ence(Honkelaetal.,2011)andstochasticvariationalinference(Hoffmanetal.,2013).Inshort,the
twoofdeeplearningandprobabilisticmodellingemployfundamentallydifferentinferential
strategiesandanaturalquestionis,whetherwecandesignalgorithmsthatcombinetheirrespective
strengths.
Therehavebeenseveralattemptstodesignsuchmethodsintherecentyears,e.g.,Krishnanetal.
(2015;2017);Fraccaroetal.(2016);Archeretal.(2015);Johnsonetal.(2016);Chenetal.(2015).
OurworkinthispaperisinspiredbythepreviousworkofJohnsonetal.(2016)thataimstocombine
message-passing,natural-gradient,andamortizedinference.Ourproposedmethodinthispaper
andgeneralizesthemethodofJohnsonetal.(2016).
Todoso,weproposeStructuredInferenceNetworks(SIN)thatincorporatethePGMstructurein
thestandardinferencenetworksusedinvariationalauto-encoders(VAE)(Kingma&Welling,2013;
Rezendeetal.,2014).Wederiveconditionsunderwhichsuchinferencenetworkscanenablefast
amortizedinferencesimilartoVAE.ByusingarecentVMPmethodofKhan&Lin(2017),we

Equalcontributions.WuLinisnowattheUniversityofBritishColumbia,Vancouver,Canada.
1
arXiv:1803.05589v2  [stat.ML]  14 Jun 2018PublishedasaconferencepaperatICLR2018
LatentmixturemodelLatentstate-spacemodel
y
n
x
n
z
n

NN

PGM
N
(a)GenerativeModel
y
n
x
n
z
n

PGM
˚
NN
˚
PGM
N
(b)SIN
y
1
y
2
y
3
y
4
x
1
x
2
x
3
x
4

PGM

NN
(c)GenerativeModel
y
1
y
2
y
3
y
4
x
1
x
2
x
3
x
4

PGM
˚
PGM
˚
NN
(d)SIN
Figure1:Fig.(a)and(c)showtwoexamplesofgenerativemodelsthatcombinedeepmodelswith
PGMs,whileFig.(b)and(d)showourproposedStructuredInferenceNetworks(SIN)forthetwo
models.ThegenerativemodelsarejustlikethedecoderinVAEbuttheyemployastructuredprior,
e.g.,Fig.(a)hasamixture-modelpriorwhileFig.(b)hasadynamicalsystemprior.SINs,justlike
theencoderinVAE,mimicthestructureofthegenerativemodelbyusingparameters
˚
.Onemain
differenceisthatinSINthearrowsbetween
y
n
and
x
n
arereversedcomparedtothemodel,while
restofthearrowshavethesamedirection.
deriveavariationalmessage-passingalgorithmwhosemessagesautomaticallyreducetostochastic-
gradientsforthedeepcomponentsofthemodel,whileperformnatural-gradientupdatesforthePGM
part.Overall,ouralgorithmenablesStructured,Amortized,andNatural-gradient(SAN)updatesand
thereforewecallouralgorithmtheSANalgorithm.Weshowthatouralgorithmgivecomparable
performancetothemethodofJohnsonetal.(2016)whilesimplifyingandgeneralizingit.Thecode
toreproduceourresultsisavailableat
https://github.com/emtiyaz/vmp-for-svae/
.
2T
HE
M
ODELAND
C
HALLENGESWITH
I
TS
I
NFERENCE
Weconsiderthemodellingofdatavectors
y
n
byusinglocallatentvectors
x
n
.Followingprevious
works(Johnsonetal.,2016;Archeretal.,2015;Krishnanetal.,2015),wemodeltheoutput
y
n
given
x
n
usinganeuralnetworkwithparameters

NN
,andcapturethecorrelationsamongdata
vectors
y
:=
f
y
1
;
y
2
;:::;
y
N
g
usingaprobabilisticgraphicalmodel(PGM)overthelatentvectors
x
:=
f
x
1
;
x
2
;:::;
x
N
g
.,weusethefollowingjointdistribution:
p
(
y
;
x
;

):=
""
N
Y
n
=1
p
(
y
n
j
x
n
;

NN
)
#
|
{z
}
DNN
""
p
(
x
j

PGM
)
#
|
{z
}
PGM
""
p
(

PGM
)
#
|
{z
}
Hyperprior
;
(1)
where

NN
and

PGM
areparametersofaDNNandPGMrespectively,and

:=
f

NN
;

PGM
g
.
Thiscombinationofprobabilisticgraphicalmodelandneuralnetworkisreferredtoasstructured
variationalauto-encoder(SVAE)byJohnsonetal.(2016).SVAEemploysastructuredprior
p
(
x
j

PGM
)
toextractusefulstructurefromthedata.SVAEthereforediffersfromVAE(Kingma
&Welling,2013)wherethepriordistributionover
x
issimplyamultivariateGaussiandistribution
p
(
x
)=
N
(
x
j
0
;
I
)
withnospecialstructure.Toillustratethisdifference,wenowgiveanexample.
Example(Mixture-ModelPrior):
Supposewewishtogrouptheoutputs
y
n
into
K
distinct
clusters.Forsuchatask,thestandardGaussianpriorusedinVAEisnotausefulprior.Wecould
insteaduseamixture-modelpriorover
x
n
,assuggestedby(Johnsonetal.,2016),
p
(
x
j

PGM
)=
N
Y
n
=1
p
(
x
n
j

PGM
)=
N
Y
n
=1
""
K
X
k
=1
p
(
x
n
j
z
n
=
k
)
ˇ
k
#
;
(2)
where
z
n
2f
1
;
2
;:::;K
g
isthemixtureindicatorforthe
n
'thdataexample,and
ˇ
k
aremixing
proportionsthatsumto1over
k
.Eachmixturecomponentcanfurtherbemodelled,e.g.,byusing
aGaussiandistribution
p
(
x
n
j
z
n
=
k
):=
N
(
x
n
j

k
;

k
)
givingustheGaussianMixtureModel
(GMM)priorwithPGMhyperparameters

PGM
:=
f

k
;

k
;ˇ
k
g
K
k
=1
.Thegraphicalmodelof
anSVAEwithsuchpriorsisshowninFigure1a.Thistypeofstructured-priorisusefulfor
discoveringclustersinthedata,makingthemeasiertointerpretthanVAE.
2
PublishedasaconferencepaperatICLR2018
Ourmaingoalinthispaperistoapproximatetheposteriordistribution
p
(
x
;

j
y
)
.,
similartoVAE,wewouldliketoapproximatetheposteriorof
x
byusinganinferencenetwork.In
VAE,thisisdonebyusingafunctionparameterizedbyDNN,asshownbelow:
p
(
x
j
y
;

NN
)=
1
p
(
y
j

)
N
Y
n
=1
[
p
(
y
n
j
x
n
;

NN
)
N
(
x
n
j
0
;
I
)]
ˇ
N
Y
n
=1
q
(
x
n
j
f
˚
(
y
n
))
;
(3)
wherethelefthandsideistheposteriordistributionof
x
,andtheequalityisobtainedbyusing
thedistributionofthedecoderintheBayes'rule.Therighthandsideisthedistributionoftheen-
coderwhere
q
istypicallyanexponential-familydistributionwhosenatural-parametersaremodelled
byusingaDNN
f
˚
withparameters
˚
.Thesamefunction
f
˚
(

)
isusedforall
n
whichreducesthe
numberofvariationalparametersandenablessharingofstatisticalstrengthsacross
n
.Thisleadsto
bothfastertrainingandfastertesting(Rezendeetal.,2014).
Unfortunately,forSVAE,suchinferencenetworksmaygiveinaccuratepredictionssincetheyignore
thestructureofthePGMprior
p
(
x
j

PGM
)
.Forexample,suppose
y
n
isatime-seriesandwemodel
x
n
usingadynamicalsystemasdepictedinFig.1c.Inthiscase,theinferencenetworkof(3)is
notanaccurateapproximationsinceitignoresthetime-seriesstructurein
x
.Thismightresultin
inaccuratepredictionsofdistantfutureobservations,e.g.,predictionforanobservation
y
10
giventhe
pastdata
f
y
1
;
y
2
;
y
3
g
wouldbeinaccuratebecausetheinferencenetworkhasnopathconnecting
x
10
to
x
1
;
x
2
;
or
x
3
.Ingeneral,wheneverthepriorstructureisimportantinobtainingaccurate
predictions,wemightwanttoincorporateitintheinferencenetwork.
Asolutiontothisproblemistouseaninferencenetworkwiththesamestructureasthemodelbutto
replaceallitsedgesbyneuralnetworks(Krishnanetal.,2015;Fraccaroetal.,2016).Thissolutionis
reasonablewhenthePGMitselfiscomplex,butmightbetooaggressivewhenthePGMisasimple
model,e.g.,whenthepriorinFig.1cisalineardynamicalsystem.UsingDNNsinsuchcases
woulddramaticallyincreasethenumberofparameterswhichwillleadtoapossibledeteriorationin
bothspeedandperformance.
Johnsonetal.(2016)proposeamethodtoincorporatethestructureofthePGMpartintheinference
network.ForSVAEwithconditionally-conjugatePGMpriors,theyaimtoobtaina
variationalinferencebyoptimizingthefollowingstandardvariationallowerbound
1
:
L
(

x
;

):=
E
q
(
x
)
""
log
(
N
Y
n
=1
p
(
y
n
j
x
n
;

NN
)
p
(
x
j

PGM
)
)

log
q
(
x
j

x
)
#
;
(4)
where
q
(
x
j

x
)
isaminimalexponential-familydistributionwithnaturalparameters

x
.Toincorpo-
rateaninferencenetwork,theyneedtorestricttheparameterof
q
(
x
j

x
)
similartotheVAEencoder
shownin(3),i.e.,

x
mustbeusingaDNNwithparameter
˚
.Forthispurpose,theyuse
atwo-stageiterativeprocedure.Inthestage,theyobtain


x
byoptimizingasurrogatelower
boundwherethedecoderin(4)isreplacedbytheVAEencoderof(3)(highlightedinblue),
^
L
(

x
;

;
˚
):=
E
q
(
x
)
""
log
(
N
Y
n
=1
q
(
x
n
j
f
˚
(
y
n
))
p
(
x
j

PGM
)
)

log
q
(
x
j

x
)
#
:
(5)
Theoptimal


x
isafunctionof

and
˚
andtheydenoteitby


x
(

;
˚
)
.Inthesecondstage,they
substitute


x
into(4)andtakeagradientsteptooptimize
L
(


x
(

;
˚
)
;

)
withrespectto

and
˚
.Thisisiterateduntilconvergence.Thestageensuresthat
q
(
x
j

x
)
isintermsof
˚
similartoVAE,whilethesecondstageimprovesthelowerboundwhilemaintainingthisrestriction.
Theadvantageofthisformulationisthatwhenthefactors
q
(
x
n
j
f
˚
(
y
n
))
arechosentobecon-
jugateto
p
(
x
j

PGM
)
,thestagecanbeperformedefusingVMP.However,theover-
allmethodmightbediftoimplementandtune.Thisisbecausetheprocedureisequiva-
lenttoanimplicitly-constrainedoptimization
2
thatoptimizes(4)withtheconstraint


x
(

;
˚
)=
argmax

x
^
L
(

x
;

;
˚
)
.Suchconstrainedproblemsaretypicallymorediftosolvethantheir
unconstrainedcounterparts,especiallywhentheconstraintsarenonconvex(Heinkenschloss,2008).
Theoretically,theconvergenceofsuchmethodsisdiftoguaranteewhentheconstraintsare
1
Johnsonetal.(2016)consider

tobearandomvariable,butforclarityweassume

tobedeterministic.
2
ThisissimilartoHoffman&Blei(2015)whoalsosolveanimplicitlyconstrainedoptimizationproblem.
3
PublishedasaconferencepaperatICLR2018
violated.Inpractice,thismakestheimplementationdifbecauseineveryiterationtheVMP
updatesneedtorunlongenoughtoreachclosetoalocaloptimumofthesurrogatelowerbound.
AnotherdisadvantageofthemethodofJohnsonetal.(2016)isthatitsefycouldbeensured
onlyunderrestrictiveassumptionsonthePGMprior.Forexample,themethoddoesnotworkfor
PGMsthatcontainnon-conjugatefactorsbecauseinthatcaseVMPcannotbeusedtooptimizethe
surrogatelowerbound.Inaddition,themethodisnotdirectlyapplicablewhen

x
isconstrained
andwhen
p
(
x
j

PGM
)
hasadditionallatentvariables(e.g.,indicatorvariables
z
n
inthemixture-model
example).Insummary,themethodofJohnsonetal.(2016)mightbediftoimplementandtune,
andalsodiftogeneralizetocaseswhenPGMiscomplex.
Inthispaper,weproposeanalgorithmtosimplifyandgeneralizethealgorithmofJohnsonetal.
(2016).Weproposestructuredinferencenetworks(SIN)thatincorporatethestructureofthePGM
partintheVAEinferencenetwork.Evenwhenthegraphicalmodelcontainsanon-conjugatefactor,
SINcanpreservesomestructureofthemodel.WederiveconditionsunderwhichSINcanenable
efamortizedinferencebyusingstochasticgradients.Wediscussmanyexamplestoillustrate
thedesignofSINformanytypesofPGMstructures.Finally,wederiveaVMPalgorithmtoperform
natural-gradientvariationalinferenceonthePGMpartwhileretainingtheefyoftheamortized
inferenceontheDNNpart.
3S
TRUCTURED
I
NFERENCE
N
ETWORKS
WestartwiththedesignofinferencenetworksthatincorporatethePGMstructureintotheinference
networkofVAE.Weproposethefollowing
structured
inferencenetwork(SIN)whichconsistsof
twotypesoffactors,
q
(
x
j
y
;
˚
):=
1
Z
(
˚
)
""
N
Y
n
=1
q
(
x
n
j
f
˚
NN
(
y
n
))
#
|
{z
}
DNNFactor
""
q
(
x
j
˚
PGM
)
#
|
{z
}
PGMFactor
:
(6)
TheDNNfactorhereissimilarto(3)whilethePGMfactorisanexponential-familydistribution
whichhasasimilargraphstructureasthePGMprior
p
(
x
j

PGM
)
.TheroleoftheDNNtermisto
enablexibilitywhiletheroleofthePGMtermistoincorporatethemodel'sPGMstructureintothe
inferencenetwork.Bothfactorshavetheirownparameters.
˚
NN
istheparameterofDNNand
˚
PGM
isthenaturalparameterofthePGMfactor.Theparametersetisdenotedby
˚
:=
f
˚
NN
;
˚
PGM
g
.
Howshouldwechoosethetwofactors?Aswewillshowsoonthat,forfastamortizedinference,
thesefactorsneedtosatisfythefollowingtwoconditions.Theconditionisthatthenormalizing
constant
3
log
Z
(
˚
)
iseasytoevaluateanddifferentiate.Thesecondconditionisthatwecandraw
samplesfromSIN,i.e.,
x

(
˚
)
˘
q
(
x
j
y
;
˚
)
wherewehavedenotedthesampleby
x

(
˚
)
toshowits
dependenceon
˚
.Anadditionaldesirable,althoughnotnecessary,featureistobeabletocompute
thegradientof
x

(
˚
)
byusingthereparameterizationtrick.Now,wewillshowthatgiventhesetwo
conditionswecaneasilyperformamortizedinference.
Weshowthatwhentheabovetwoconditionsaremet,astochasticgradientofthelowerboundcan
becomputedinasimilarwayasinVAE.Fornow,weassumethat

isadeterministicvariable(we
willrelaxthisinthenextsection).Thevariationallowerboundinthiscasecanbewrittenasfollows:
L
SIN
(

;
˚
):=
E
q

log
p
(
y
;
x
j

)
q
(
x
j
y
;
˚
)

=
E
q

log
Q
n
f
p
(
y
n
j
x
n
;

NN
)
g
p
(
x
j

PGM
)
Z
(
˚
)
Q
n
f
q
(
x
n
j
f
˚
NN
(
y
n
))
g
q
(
x
j
˚
PGM
)

(7)
=
N
X
n
=1
E
q

log
p
(
y
n
j
x
n
;

NN
)
q
(
x
n
j
f
˚
NN
(
y
n
))

+
E
q
[log
p
(
x
j

PGM
)]

E
q
[log
q
(
x
j
˚
PGM
)]
+
log
Z
(
˚
)
(8)
ThetermaboveisidenticaltothelowerboundofthestandardVAE,whiletherestofthe
termsaredifferent(showninblue).ThesecondtermdiffersduetothePGMpriorinthegenerative
model.InVAE,
p
(
x
j

PGM
)
isastandardnormal,buthereitisastructuredPGMprior.Thelasttwo
termsariseduetothePGMterminSIN.Ifwecancomputethegradientsofthelastthreetermsand
generatesamples
x

(
˚
)
fromSIN,wecanperformamortizedinferencesimilartoVAE.Fortunately,
3
Notethatboththefactorsarenormalizeddistribution,buttheirproductmaynotbe.
4
PublishedasaconferencepaperatICLR2018
thesecondandthirdtermsareusuallyeasyforPGMs,thereforeweonlyrequirethegradientof
Z
(
˚
)
tobeeasytocompute.Thisthetwoconditionsrequiredforafastamortizedinference.
Theresultingexpressionsforthestochasticgradientsareshownbelowwherewehighlightinblue
theadditionalgradientcomputationsrequiredontopofaVAEimplementation(wealsodropthe
explicitdependenceof
x

(
˚
)
over
˚
fornotationalsimplicity).
@
L
SIN
@
NN
ˇ
N
@
log
p
(
y
n
j
x

n
;

NN
)
@
NN
;
@
L
SIN
@
PGM
ˇ
@
log
p
(
x

j

PGM
)
@
PGM
;
(9)
@
L
SIN
@
˚
ˇ
@
@
x

n

N
log
p
(
y
n
j
x

n
;

NN
)
q
(
x

n
j
f
˚
NN
(
y
n
))

@
x

n
@
˚

N
@
log
q
(
x

n
j
f
˚
NN
(
y
n
))
@
˚
+
@
@
x


log
p
(
x

j

PGM
)
q
(
x

j
˚
PGM
)

@
x

@
˚

@
log
q
(
x

j
˚
PGM
)
@
˚
+
@
log
Z
(
˚
)
@
˚
;
(10)
AderivationisgiveninAppendixD.
Thegradientsof
Z
(
˚
)
and
x

(
˚
)
mightbecheaporcostlydependingonthetypeofPGM.For
example,forLDS,theserequireafullinferencethroughthemodelwhichcosts
O
(
N
)
computation
andisinfeasibleforlarge
N
.However,forGMM,each
x
n
canbeindependentlysampledand
thereforecomputationsareindependentof
N
.Ingeneral,ifthelatentvariablesinPGMarehighly
correlated(e.g.,Gaussianprocessprior),thenBayesianinferenceisnotcomputationallyef
andgradientsarediftocompute.Inthispaper,wedonotconsidersuchdifcasesand
assumethat
Z
(
˚
)
and
x

(
˚
)
canbeevaluatedanddifferentiatedcheaply.
WenowgivemanyexamplesofSINthatmeetthetwoconditionsrequiredforafastamortized
inference.When
p
(
x
j

PGM
)
isaconjugateexponential-familydistribution,choosingthetwofactors
isaveryeasytask.Inthiscase,wecanlet
q
(
x
j
˚
PGM
)=
p
(
x
j
˚
PGM
)
,i.e.,thesecondfactoristhe
samedistributionasthePGMpriorbutwithadifferentsetofparameters
˚
PGM
.Toillustratethis,
wegiveanexamplebelowwhenthePGMpriorisalineardynamicalsystem.
Example(SINforLinearDynamicalSystem(LDS)):
When
y
n
isatimeseries,wecanmodel
thelatent
x
n
usinganLDSas
p
(
x
j

):=
N
(
x
0
j

0
;

0
)
Q
N
n
=1
N
(
x
n
j
Ax
n

1
;
Q
)
,where
A
isthetransitionmatrix,
Q
istheprocess-noisecovariance,and

0
and

0
arethemeanand
covarianceoftheinitialdistribution.Therefore,

PGM
:=
f
A
;
Q
;

0
;

0
g
.Inourinference
network,wechoose
q
(
x
j
˚
PGM
)=
p
(
x
j
˚
PGM
)
asshowbelow,where
˚
PGM
:=
f

A
;

Q
;


0
;


0
g
and,sinceourPGMisaGaussian,wechoosetheDNNfactortobeaGaussianaswell:
q
(
x
j
y
;
˚
):=
1
Z
(
˚
)
""
N
Y
n
=1
N
(
x
n
j
m
n
;
V
n
)
#
|
{z
}
DNNFactor
""
N
(
x
0
j


0
;


0
)
N
Y
n
=1
N
(
x
n
j

Ax
n

1
;

Q
)
#
|
{z
}
LDSFactor
;
(11)
where
m
n
:=
m
˚
NN
(
y
n
)
and
V
n
:=
V
˚
NN
(
y
n
)
aremeanandcovarianceparameterizedbya
DNNwithparameter
˚
NN
.ThegenerativemodelandSINareshowninFig.1cand1d,respec-
tively.TheaboveSINisaconjugatemodelwherethemarginallikelihoodanddistributionscan
becomputedin
O
(
N
)
usingtheforward-backwardalgorithm,a.k.a.Kalmansmoother(Bishop,
2016).Wecanalsocomputethegradientof
Z
(
˚
)
asshowninKokkalaetal.(2015).
WhenthePGMpriorhasadditionallatentvariables,e.g.,theGMMpriorhasclusterindicators
z
n
,
wemightwanttoincorporatetheirstructureinSIN.Thisisillustrateintheexamplebelow.
Example(SINforGMMprior):
Thepriorshownin(2)hasanadditionalsetoflatentvariables
z
n
.TomimicthisstructureinSIN,wechoosethePGMfactorasshownbelowwithparameters
˚
PGM
:=
f


k
;


k
;

ˇ
k
g
K
k
=1
,whilekeepingtheDNNparttobeaGaussiandistributionsimilarto
theLDScase:
q
(
x
j
y
;
˚
):=
1
Z
(
˚
)
N
Y
n
=1
""
N
(
x
n
j
m
n
;
V
n
)
#
|
{z
}
DNNFactor
""
K
X
k
=1
N
(
x
n
j


k
;


k
)
ˇ
k
#
|
{z
}
GMMFactor
;
(12)
5
PublishedasaconferencepaperatICLR2018
ThemodelandSINareshowninFigure1aand1b,respectively.Fortunately,duetoconjugacy
oftheGaussianandmultinomialdistributions,wecanmarginalize
x
n
togetaclosed-formex-
pressionfor
log
Z
(
˚
):=
P
n
log
P
k
N
(
m
n
j


k
;
V
n
+


k
)
ˇ
k
.WecansamplefromSINby
samplingfromthemarginal
q
(
z
n
=
k
j
y
;
˚
)
/N

m
n
j


k
;
V
n
+


k


ˇ
k
.Given
z
n
,we
cansample
x
n
fromthefollowingconditional:
q
(
x
n
j
z
n
=
k;
y
;
˚
)=
N
(
x
n
j
e

n
;
e

n
)
,where
e


1
n
=
V

1
n
+



1
k
and
e

n
=
e

n
(
V

1
n
m
n
+



1
k

k
)
.SeeAppendixBforadetailedderiva-
tion.
Inalloftheaboveexamples,weareabletosatisfythetwoconditionsevenwhenweusethesame
structureasthemodel.Thisisnotpossibleformanyconditionally-conjugateexponentialfamily
distributions.However,wecanstillobtainsamplesfromatractablestructuredapprox-
imationusingVMP.Weillustratethisfortheswitchingstate-spacemodelinAppendixA.Insuch
cases,adrawbackofourmethodisthatweneedtorunVMPlongenoughtogetasample,very
similartothemethodofJohnsonetal.(2016).However,ourgradientsaresimplertocomputethan
theirs.Theirmethodrequiresgradientsof


(

;
˚
)
whichdependsbothon

and
˚
(seeProposition
4.2in(Johnsonetal.,2016)).Inourcase,werequiregradientof
Z
(
˚
)
whichisindependentof

andthereforeissimplertoimplement.
AnadvantageofourmethodoverthemethodofJohnsonetal.(2016)isthatourmethodcanhandle
non-conjugatefactorsinthegenerativemodel.WhenthePGMpriorcontainssomenon-conjugate
factors,wemightreplacethembytheirclosestconjugateapproximationswhilemakingsurethatthe
inferencenetworkcapturestheusefulstructurepresentintheposteriordistribution.Weillustrate
thisonaStudent'st-mixturemodel.
Example(SINforStudent'st-MixtureModel):
Tohandleoutliersinthedata,wemightwantto
usetheStudent'st-mixturecomponentinthemixturemodelshownin(2),i.e.,weset
p
(
x
n
j
z
n
=
k
)=
T
(
x
n
j

k
;

k
;
k
)
withmean

k
,scalematrix

k
anddegreeoffreedom

k
.TheStudent's
t-distributionisnotconjugatetothemultinomialdistribution,therefore,ifweuseitasthePGM
factorinSIN,wewillnotbeabletosatisfybothconditionseasily.Eventhoughourmodel
containsat-distributioncomponents,wecanstillusetheSINshownin(12)thatusesaGMM
factor.Wecanthereforesimplifyinferencebychoosinganinferencenetworkwhichhasasimpler
formthantheoriginalmodel.
Intheory,onecandothisevenwhenallfactorsarenon-conjugate,however,theapproximationerror
mightbequitelargeinsomecasesforthisapproximationtobeuseful.Inourexperiments,we
triedthisfornon-lineardynamicalsystemandfoundthatcapturingnon-linearitywasessentialfor
dynamicalsystemsthatareextremelynon-linear.
4V
ARIATIONAL
M
ESSAGE
P
ASSINGFOR
N
ATURAL
-G
RADIENT
V
ARIATIONAL
I
NFERENCE
Previously,weassumed

PGM
tobedeterministic.Inthissection,werelaxthisconditionandassume

PGM
tofollowanexponential-familyprior
p
(

PGM
j

PGM
)
withnaturalparameter

PGM
.Wederive
aVMPalgorithmtoperformnatural-gradientvariationalinferencefor

PGM
.Ouralgorithmworks
evenwhenthePGMpartcontainsnon-conjugatefactors,anditdoesnotaffecttheefyof
theamortizedinferenceontheDNNpart.Weassumethefollowingapproximation:
q
(
x
;

j
y
):=
q
(
x
j
y
;
˚
)
q
(

PGM
j

PGM
)
wherethersttermisequaltoSINintroducedintheprevious
section,andthesecondtermisanexponential-familydistributionwithnaturalparameter

PGM
.For

NN
and
˚
,wewillcomputepointestimates.
WebuilduponthemethodofKhan&Lin(2017)whichisageneralizationofVMPandstochastic
variationalinference(SVI).Thismethodenablesnatural-gradientupdatesevenwhenPGMcon-
tainsnon-conjugatefactors.Thismethodperformsnatural-gradientvariationalinferencebyusing
amirror-descentupdatewiththeKullback-Leibler(KL)divergence.Toobtainnatural-gradients
withrespecttothenaturalparametersof
q
,themirror-descentneedstobeperformedinthemean
parameterspace.WewillnowderiveaVMPalgorithmusingthismethod.
Westartbyderivingthevariationallowerbound.Thevariationallowerboundcorrespondingtothe
approximationcanbeexpressedintermsof
L
SIN
derivedintheprevioussection.
L
(

PGM
;

NN
;
˚
):=
E
q
(

PGM
j

PGM
)
[
L
SIN
(

;
˚
)]

D
KL
[
q
(

PGM
j

PGM
)
k
p
(

PGM
j

PGM
)]
:
(13)
6
PublishedasaconferencepaperatICLR2018
Algorithm1
Structured,Amortized,andNatural-gradient(SAN)VariationalInference
Require:
Data
y
,Step-sizes

1
;
2
;
3
1:
Initialize

PGM
;

NN
;
˚
.
2:
repeat
3:
Compute
q
(
x
j
y
;
˚
)
forSINshownin(6)eitherbyusinganexactexpressionorusingVMP.
4:
Sample
x

˘
q
(
x
j
y
;
˚
)
,andcompute
r
˚
Z
and
r
˚
x

.
5:
Update

PGM
usingthenatural-gradientstepgivenin(16).
6:
Update

NN
and
˚
usingthegradientsgivenin(9)-(10)with

PGM
˘
q
(

PGM
j

PGM
)
.
7:
until
Convergence
Wewilluseamirror-descentupdatewiththeKLdivergencefor
q
(

PGM
j

PGM
)
becausewewant
natural-gradientupdatesforit.Fortherestoftheparameters,wewillusetheusualEuclidean
distance.Wedenotethemeanparametercorrespondingto

PGM
by

PGM
.Since
q
isaminimal
exponentialfamily,thereisaone-to-onemapbetweenthemeanandnaturalparameters,therefore
wecanreparameterize
q
suchthat
q
(

PGM
j

PGM
)=
q
(

PGM
j

PGM
)
.Denotingthevaluesatiteration
t
withasuperscript
t
andusingEq.19in(Khan&Lin,2017)withthesedivergences,weget:
max

PGM
h

PGM
;
r

PGM
L
t
i
1

1
D
KL
[
q
(

PGM
j

PGM
)
k
q
(

PGM
j

t
PGM
)]
;
(14)
max

NN
h

NN
;
r

NN
L
t
i
1

2
k

NN


t
NN
k
2
2
;
max
˚
h
˚
;
r
˚
L
t
i
1

3
k
˚

˚
t
k
2
2
:
(15)
where

1
to

3
arescalars,
h
;
i
isaninnerproduct,and
rL
t
isthegradientatthevalueiniteration
t
.
AsshownbyKhan&Lin(2017),themaximizationin(14)canbeobtainedinclosed-form:

PGM
 
(1


1
)

PGM
+

1
r

PGM
E
q
(

PGM
j

PGM
)
[log
p
(
x

j

PGM
)]
:
(16)
Whentheprior
p
(

PGM
j

PGM
)
isconjugateto
p
(
x
j

PGM
)
,theabovestepisequaltotheSVIupdate
oftheglobalvariables.Thegradientitselfisequaltothemessagereceivedby

PGM
inaVMPalgo-
rithm,whichisalsothenaturalgradientwithrespectto

PGM
.Whenthepriorisnotconjugate,the
gradientcanbeapproximatedeitherbyusingstochasticgradientsorbyusingthereparameterization
trick(Khan&Lin,2017).Therefore,thisupdateenablesnatural-gradientupdateforPGMsthatmay
containbothconjugateandnon-conjugatefactors.
Theupdateoftherestoftheparameterscanbedonebyusingastochastic-gradientmethod.This
isbecausethesolutionoftheupdate(15)isequaltoastochastic-gradientdescentupdate(onecan
verifythisbysimplifytakingthegradientandsettingittozero).Wecancomputethestochastic-
gradientsbyusingaMonteCarloestimatewithasample


PGM
˘
q
(

PGM
j

PGM
)
asshownbelow:
r
˚
L
(

PGM
;

NN
;
˚
)
ˇr
˚
L
SIN
(


;
˚
)
;
r

NN
L
(

PGM
;

NN
;
˚
)
ˇr

NN
L
SIN
(


;
˚
)
(17)
where


:=
f


PGM
;

NN
g
.Asdiscussedintheprevioussection,thesegradientscanbecomputed
similartoVAE-likebyusingthegradientsgivenin(9)-(10).Therefore,fortheDNNpartwecan
performamortizedinference,anduseanatural-gradientupdateforthePGMpartusingVMP.
ThealgorithmisoutlinedinAlgorithm1.SinceouralgorithmenablesStructured,Amortized,
andNatural-gradient(SAN)updates,wecallittheSANalgorithm.Ourupdatesconvenientlysep-
aratethePGMandDNNcomputations.Step3-6operateonthePGMpart,forwhichwecanuse
existingimplementationforthePGM.Step7operatesontheDNNpart,forwhichwecanreuse
VAEimplementation.Ouralgorithmnotonlygeneralizespreviousworks,butalsothe
implementationbyenablingthereuseoftheexistingsoftware.
5E
XPERIMENTSAND
R
ESULTS
ThemaingoalofourexperimentsistoshowthatourSANalgorithmgivessimilarresultstothe
methodofJohnsonetal.(2016).Forthisreason,weapplyouralgorithmtothetwoexamples
consideredinJohnsonetal.(2016),namelythelatentGMMandlatentLDS(seeFig.1).Inthis
sectionwediscussresultsforlatentGMM.AnadditionalresultforLDSisincludedinAppendixC.
Ourresultsshowthat,similartothemethodofJohnsonetal.(2016)ouralgorithmcanlearncomplex
7
PublishedasaconferencepaperatICLR2018
(a)
(b)
(c)
Figure2:Figure(a)comparesperformancesofGMM,SVAE,andSANonthePinwheelwhere
weseethatSANconvergesfasterthanSVAEandperformsbetterthanGMM.Figure(b)compares
GMM,VAE,andSANontheAutodatasetwhereweseethesametrend.Figure(c)comparesper-
formancesonthePinwheeldatasetwithoutliers.WeseethattheperformanceofSANonStudent's
t-mixturemodel(SAN-TMM)degradesslowerthantheperformanceofmethodsbasedonGMM.
Evenwith
70%
outliers,SAN-TMMperformsbetterthanSAN-GMMwith
10%
outliers.
(a)GMM
(b)SAN
(c)VAE
(d)GMM
(e)SAN
(f)VAE
Figure3:ToprowisforthePinwheeldataset,whilethebottomrowisfortheAutodataset.Point
cloudsinthebackgroundofeachplotshowthesamplesgeneratedfromthelearnedgenerative
model,whereeachmixturecomponentisshownwithadifferentcolorandthecolorintensitiesare
proportionaltotheprobabilityofthemixturecomponent.Thepointsintheforegroundshowdata
sampleswhicharecoloredaccordingtothetruelabels.Weuse
K
=10
mixturecomponentsto
trainallmodels.FortheAutodataset,weshowonlythetwoprinciplecomponents.
representationswithinterpretablestructures.Theadvantageofourmethodisthatitissimplerand
moregeneralthanthemethodofJohnsonetal.(2016).
Wecomparetothreebaselinemethods.Themethodisthevariationalexpectation-maximization
(EM)algorithmappliedtothestandardGaussianmixturemodel.Werefertothismethodas`GMM'.
ThismethodisaclusteringmethodbutdoesnotuseaDNNtodoso.ThesecondmethodistheVAE
approachofKingma&Welling(2013),whichwerefertoas`VAE'.ThismethodusesaDNNbut
doesnotclustertheoutputsorlatentvariables.ThethirdmethodistheSVAEapproachofJohnson
8
PublishedasaconferencepaperatICLR2018
etal.(2016)appliedtolatentGMMshowninFig.1.ThismethodusesbothaDNNandamixture
modeltoclusterthelatentvariables.Werefertothisas`SVAE'.Wecomparethesemethodsto
ourSANalgorithmappliedtolatentGMMmodel.Werefertoourmethodas`SAN'.Allmethods
employaNormal-WishartpriorovertheGMMhyperparameters(seeBishop(2016)fordetails).
Weusetwodatasets.Thedatasetisthesynthetictwo-dimensionalPinwheeldataset(
N
=5000
and
D
=2
)usedin(Johnsonetal.,2016).TheseconddatasetistheAutodataset(
N
=392
and
D
=6
,availableintheUCIrepository)whichcontainsinformationaboutcars.Thedatasetalso
containsave-classlabelwhichindicatesthenumberofcylindersinacar.Weusetheselabelsto
validateourresults.Forbothdatasetsweuse70
%
datafortrainingandtherestfortesting.Forall
methods,wetunethestep-sizes,thenumberofmixturecomponents,andthelatentdimensionality
onavalidationset.WetraintheGMMbaselineusingabatchmethod,and,forVAEandSVAE,we
useminibatchesofsize
64
.DNNsinallmodelsconsistoftwolayerswith
50
hiddenunitsandan
outputlayerofdimensionality
6
and
2
fortheAutoandPinwheeldatasets,respectively.
Figure2aand2bcomparetheperformancesduringtraining.InFigure2a,wecomparetoSVAE
andGMM,whereweseethatSANconvergesfasterthanSVAE.Asexpected,bothSVAEandSAN
achievesimilarperformanceuponconvergenceandperformbetterthanGMM.InFigure2b,we
comparetoVAEandGMM,andobservesimilartrends.TheperformanceofGMMisrepresented
asaconstantbecauseitconvergesafterafewiterationsalready.Wefoundthattheimplementation
providedbyJohnsonetal.(2016)doesnotperformwellontheAutodatasetwhichiswhywehave
notincludeditinthecomparison.Wealsocomparedthetestlog-likelihoodsandimputationerror
whichshowverysimilartrends.Weomittheseresultsduetospaceconstraints.
InthebackgroundofeachplotinFigure3,weshowsamplesgeneratedfromthegenerativemodel.
Intheforeground,weshowthedatawiththetruelabels.Theselabelswerenotusedduringtraining.
Theplots(a)-(c)showresultsforthePinwheeldataset,whileplots(d)-(e)showsresultsfortheAuto
dataset.FortheAutodataset,eachlabelcorrespondstothenumberofcylinderspresentinacar.We
observethatSANcanlearnmeaningfulclustersoftheoutputs.Ontheotherhand,VAEdoesnot
haveanymechanismstoclusterand,eventhoughthegeneratedsamplesmatchthedatadistribution,
theresultsarediftointerpret.Finally,asexpected,bothSANandVAElearnxiblepatterns
whileGMMfailstodoso.Therefore,SANenablesxiblemodelsthatarealsoeasytointerpret.
AnadvantageofourmethodoverthemethodofJohnsonetal.(2016)isthatourmethodapplieseven
whenPGMcontainsnon-conjugatefactors.Now,wediscussaresultforsuchacase.Weconsider
theSINforlatentStudent'st-mixturemodel(TMM)discussedinSection3.Thegenerativemodel
containsthestudent'st-distributionasanon-conjugatefactor,butourSINreplacesitwithaGaussian
factor.Whenthedatacontainsoutliers,weexpecttheSINforlatentTMMtoperformbetterthanthe
SINforlatentGMM.Toshowthis,weaddoutlierstothePinwheeldatasetusingaGaussian
distributionwithalargevariance.WethedegreeoffreedomfortheStudent'st-distributionto
5.WetestonfourdifferentlevelsofnoiseandreportthetestMSEaveragedoverthreerunsfor
eachlevel.Figure2cshowsacomparisonofGMM,SANonlatentGMM,andSANonlatentTMM
whereweseethat,asthenoiselevelisincreased,latentTMM'sperformancedegradesslowerthan
theothermethods(notethatthey-axisisinlog-scale).Evenwith
70%
ofoutliers,thelatentTMM
stillperformsbetterthanthelatentGMMwithonly
10%
ofoutliers.Thisexperimentillustratesthat
aconjugateSINcanbeusedforinferenceonamodelwithanon-conjugatefactor.
6D
ISCUSSIONAND
C
ONCLUSION
WeproposeanalgorithmtosimplifyandgeneralizethealgorithmofJohnsonetal.(2016)formodels
thatcontainbothdeepnetworksandgraphicalmodels.OurproposedVMPalgorithmenablesstruc-
tured,amortized,andnatural-gradientupdatesgiventhatthestructuredinferencenetworkssatisfy
twoconditions.ThetwoconditionsderivedinthispapergenerallyholdforPGMsthatdonotforce
densecorrelationsinthelatentvariables
x
.However,itisnotclearhowtoextendourmethodto
modelswherethisisthecase,e.g.,Gaussianprocessmodels.Itispossibletouseideasfromsparse
Gaussianprocessmodelsandwewillinvestigatethisinthefuture.Anadditionalissueisthatour
resultsarelimitedtosmallscaledata.Wefoundthatitisnon-trivialtoimplementamessage-passing
frameworkthatgoeswellwiththedeeplearningframework.Wearegoingtopursuethisdirection
inthefutureandinvestigategoodplatformstointegratethecapabilitiesofthesetwodifferentvors
ofalgorithms.
9
PublishedasaconferencepaperatICLR2018
Acknowledgement:
WewouldliketothankDidrikNielsen(RIKEN)forhishelpduringthiswork.
WewouldalsoliketothankMatthewJ.Johnson(GoogleBrain)andDavidDuvenaud(Universityof
Toronto)forprovidingtheSVAEcode.Finally,wearethankfulfortheRAIDENcomputingsystem
atRIKENAIPcenter,whichweusedforourexperiments.
R
EFERENCES
EvanArcher,IlMemmingPark,LarsBuesing,JohnCunningham,andLiamPaninski.Blackbox
variationalinferenceforstatespacemodels.
arXivpreprintarXiv:1511.07367
,2015.
ChristopherMBishop.
Patternrecognitionandmachinelearning.
Springer-VerlagNewYork,2016.
Liang-ChiehChen,AlexanderSchwing,AlanYuille,andRaquelUrtasun.Learningdeepstructured
models.In
InternationalConferenceonMachineLearning
,pp.1785Œ1794,2015.
MarcoFraccaro,SørenKaaeSønderby,UlrichPaquet,andOleWinther.Sequentialneuralmodels
withstochasticlayers.In
AdvancesinNeuralInformationProcessingSystems
,pp.2199Œ2207,
2016.
ZoubinGhahramaniandGeoffreyEHinton.Parameterestimationforlineardynamicalsystems.
Technicalreport,TechnicalReportCRG-TR-96-2,UniversityofTotronto,Dept.ofComputer
Science,1996.
ZoubinGhahramaniandGeoffreyEHinton.Variationallearningforswitchingstate-spacemodels.
Neuralcomputation
,12(4):831Œ864,2000.
MatthiasHeinkenschloss.Numericalsolutionofimplicitlyconstrainedoptimizationproblems.
Technicalreport,RiceUniversity,DepartmentofComputationalandAppliedMathematics,2008.
MatthewHoffmanandDavidBlei.Stochasticstructuredvariationalinference.In
Intelli-
genceandStatistics
,pp.361Œ369,2015.
MatthewDHoffman,DavidMBlei,ChongWang,andJohnPaisley.Stochasticvariationalinfer-
ence.
TheJournalofMachineLearningResearch
,14(1):1303Œ1347,2013.
A.Honkela,T.Raiko,M.Kuusela,M.Tornio,andJ.Karhunen.ApproximateRiemannianconjugate
gradientlearningfored-formvariationalBayes.
JournalofMachineLearningResearch
,11:
3235Œ3268,2011.
MatthewJohnson,DavidKDuvenaud,AlexWiltschko,RyanPAdams,andSandeepRDatta.Com-
posinggraphicalmodelswithneuralnetworksforstructuredrepresentationsandfastinference.
In
AdvancesinNeuralInformationProcessingSystems
,pp.2946Œ2954,2016.
MohammadEmtiyazKhanandWuLin.Conjugate-computationvariationalinference:Converting
variationalinferenceinnon-conjugatemodelstoinferencesinconjugatemodels.In
International
conferenceonIntelligenceandStatistics
,2017.
DiederikPKingmaandMaxWelling.Auto-encodingvariationalBayes.
arXivpreprint
arXiv:1312.6114
,2013.
JuhoKokkala,ArnoSolin,andSimoS
¨
arkk
¨
a.Sigma-pointandsmoothingbasedparameter
estimationinnonlineardynamicsystems.
arXivpreprintarXiv:1504.06173
,2015.
RahulGKrishnan,UriShalit,andDavidSontag.DeepKalman
arXivpreprint
arXiv:1511.05121
,2015.
RahulGKrishnan,UriShalit,andDavidSontag.Structuredinferencenetworksfornonlinearstate
spacemodels.In
AAAI
,pp.2101Œ2109,2017.
T.Minka.ExpectationpropagationforapproximateBayesianinference.In
Proceedingsofthe
ConferenceonUncertaintyinIntelligence
,2001.
DaniloJimenezRezende,ShakirMohamed,andDaanWierstra.Stochasticbackpropagationand
approximateinferenceindeepgenerativemodels.
arXivpreprintarXiv:1401.4082
,2014.
JohnWinnandChristopherMBishop.Variationalmessagepassing.
JournalofMachineLearning
Research
,6(Apr):661Œ694,2005.
10
PublishedasaconferencepaperatICLR2018
ASIN
FOR
S
WITCHING
LDS
InSLDS,weintroducediscretevariable
z
n
2f
1
;
2
;:::;K
g
thataresampledusingaMarkovchain:
p
(
z
n
=
i
j
z
n

1
=
j
)=
ˇ
ij
suchthat
ˇ
ij
sumto1overalligiven
j
.ThetransitionforLDS
isconditionedon
z
n
:
p
(
x
n
j
x
n

1
;z
n
=
i;

PGM
):=
N
(
x
n
j
A
i
x
n

1
;
Q
i
)
where
A
i
and
Q
i
areparametersforthe
i
'thindicator.ThesetwodynamicsputtogethertheSLDSprior
p
(
x
;
z
j

PGM
)
.WecanusethefollowingSINwhichusestheSLDSpriorasthePGMfactorbutwith
parameters
˚
PGM
insteadof

PGM
.Theexpressionfor
q
(
x
;
z
j
y
;
˚
)
isshownbelow:
1
Z
(
˚
)
""
N
Y
n
=1
N
(
x
n
j
m
n
;
V
n
)
#
|
{z
}
DNNFactor
""
N
(
x
0
j


0
;z
0
;


0
;z
0
)
N
Y
n
=1
N
(
x
n
j

A
z
n
x
n

1
;

Q
z
n
)
p
(
z
n
j
z
n

1
)
#
|
{z
}
SLDSFactor
;
Eventhoughtheabovemodelisaconditionally-conjugatemodel,thepartitionfunctionisnot
tractableandsamplingisalsonotpossible.However,wecanuseastructuredapproxima-
tion.First,wecancombinetheDNNfactorwiththeGaussianobservationofSLDSfactorandthen
useaapproximation
q
(
x
;
z
j
y
;
˚
)
ˇ
q
(
x
j

x
)
q
(
z
j

z
)
,e.g.,usingthemethodofGhahra-
mani&Hinton(2000).Thiswillgiveusastructuredapproximationwheretheedgesbetween
y
n
and
x
n
aswellasbetween
z
n
and
z
n

1
aremaintainedbut
x
n
and
z
n
independentofeachother.
BSIN
FOR
SVAE
WITH
M
IXTURE
M
ODEL
P
RIOR
InthissectionwegivedetailedderivationsfortheSINshownin(12).Wederivethenormalizing
constant
Z
(
˚
)
andshowhowtogeneratesamplesfromSIN.
WestartbyasimplerearrangementofSINin(12):
q
(
x
j
y
;
˚
)
/
N
Y
n
=1
N
(
x
n
j
m
n
;
V
n
)
""
K
X
k
=1
N
(
x
n
j


k
;


k
)
ˇ
k
#
=
N
Y
n
=1
K
X
k
=1
N
(
x
n
j
m
n
;
V
n
)
N
(
x
n
j


k
;


k
)
ˇ
k
;
(18)
/
N
Y
n
=1
K
X
k
=1
q
(
x
n
;z
n
=
k
j
y
n
;
˚
)
(19)
wherethestepfollowsfromthe(12),thesecondstepfollowsbytakingthesumover
k
outside,andthethirdstepisobtainedbyeachcomponentasajointdistributionover
x
n
andtheindicatorvariable
z
n
.
Wewillexpressthisjointdistributionasamultiplicationofthemarginalof
z
n
andconditionalof
x
n
given
z
n
.Wewillseethatthiswillgiveustheexpressionforthenormalizingconstant,aswell
asawaytosamplefromSIN.
Wecansimplifythejointdistributionfurtherasshownbelow.Thestepfollowsfromthe
tion.Thesecondstepisobtainedbyswapping
m
n
and
x
n
intheterm.Thethirdstepisobtained
bycompletingthesquaresandexpressingthetermasadistributionover
x
n
(thesecondand
thirdtermsareindependentof
x
n
).
q
(
x
n
;z
n
=
k
j
y
n
;
˚
)
/N
(
x
n
j
m
n
;
V
n
)
N
(
x
n
j


k
;


k
)
ˇ
k
(20)
=
N
(
m
n
j
x
n
;
V
n
)
N
(
x
n
j


k
;


k
)
ˇ
k
(21)
=
N
(
x
n
j
e

n
;
e

n
)
N

m
n
j


k
;
V
n
+


k


ˇ
k
;
(22)
where
e


1
n
:=
V

1
n
+



1
k
and
e

n
:=
e

n

V

1
n
m
n
+



1
k

k

.
Usingtheabovewegetthemarginalof
z
n
andconditionalof
x
n
given
z
n
:
q
(
z
n
=
k
j
y
n
;
˚
)
/N

m
n
j


k
;
V
n
+


k


ˇ
k
(23)
q
(
x
n
j
z
n
=
k;
y
n
;
˚
):=
N
(
x
n
j
e

n
;
e

n
)
(24)
11
PublishedasaconferencepaperatICLR2018
Thenormalizingconstantofthemarginalof
z
n
isobtainedbysimplysummingoverall
k
:
Z
n
(
˚
):=
K
X
k
=1
N

m
n
j


k
;
V
n
+


k


ˇ
k
:
(25)
andsince
q
(
x
n
j
z
n
=
k;
y
n
;
˚
)
isalreadyanormalizeddistribution,wecanwritetheexpression
fortheSINasfollows:
q
(
x
j
y
;
˚
)=
N
Y
n
=1
1
Z
n
(
˚
)
K
X
k
=1
q
(
x
n
j
z
n
=
k;
y
n
;
˚
)
q
(
z
n
=
k
j
y
n
;
˚
)
(26)
wherecomponentsarein(23),(24),and(25).Thenormalizingconstantisavailablein
closed-formandwecansample
z
n
andthengenerate
x
n
.Thiscompletesthederivation.
CR
ESULTSFOR
L
ATENT
L
INEAR
D
YNAMICAL
S
YSTEM
Inthisexperiment,weapplyourSANalgorithmtothelatentLDSdiscussedinSection3.For
comparison,wecompareourmethod,StructuredVariationalAuto-Encoder(SVAE)(Johnsonetal.,
2016),andLDSontheDotdatasetusedinJohnsonetal.(2016).Ourresultsshowthatourmethod
achievescomparableperformancetoSVAE.ForLDS,weperformbatchlearningforallmodel
parametersusingtheEMalgorithm.ForSVAEandSAN,weperformmini-batchupdatesforall
modelparameters.WeusethesameneutralnetworkarchitectureasinJohnsonetal.(2016),which
containstwohiddenlayerswithtanhactivationfunction.Werepeatourexperiments10timesand
measuremodelperformanceintermsofthefollowingmeanabsoluteerrorfor
˝
-stepsaheadpredic-
tion.Theerrormeasurestheabsolutedifferencebetweenthegroundtruthandthegenerativeoutputs
byaveragingacrossgeneratedresults.
N
X
n
=1
T

˝
X
t
=1
1
N
(
T

˝
)
d

jj
y

t
+
˝;n

E
p
(
y
t
+
˝;n
j
y
1:
t;n
)

y
t
+
˝;n

jj
1

(27)
where
N
isthenumberoftestingtimeserieswith
T
timesteps,
d
isthedimensionalityofobservation
y
,andobservation
y

t
+
˝;n
denotestheground-truthattimestep
t
+
˝
.
Figure4:Predictionerror,whereshadowsdenotethestandarderrorsacross10runs
FromFigure4,wecanobservethatourmethodperformsasgoodasSVAEandoutperformsLDS.
OurmethodisslightlyrobustthanSVAE.InFigure5,therearegeneratedimagesobtainedfrom
allmethods.FromFigure5,wealsoseethatourmethodperformsasgoodasSAVEandisableto
recovertheground-truthobservation.
12
PublishedasaconferencepaperatICLR2018
(a)SAN
(b)SVAE
(c)LDS
(d)GroundTruth
Figure5:Generatedimages,whereeachcolumnofpixelsrepresentsanobservation,eachrowof
pixelsrepresentsonetimestep,andeachverticalwhitelinedenotesthetimesteptogenerate
images
DD
ERIVATIONOF
E
Q
.10
Wearegoingtoderivethegradientsof
L
SIN
withrespectto
˚
:
L
SIN
(

;
˚
)
(28)
=
N
X
n
=1
E
q

log
p
(
y
n
j
x
n
;

NN
)
q
(
x
n
j
f
˚
NN
(
y
n
))

+
E
q
[log
p
(
x
j

PGM
)]

E
q
[log
q
(
x
j
˚
PGM
)]
+
log
Z
(
˚
)
(29)
Thegradientofthenumeratortermintheterm:
@
@
˚
N
X
n
=1
E
q
[log
p
(
y
n
j
x
n
;

NN
)]
ˇ
N
@
@
˚
log
p
(
y
n
j
x

n
;

NN
)=
N
@
@
x

n
log
p
(
y
n
j
x

n
;

NN
)
@
x

n
@
˚
(30)
wheretheapproximationisaMonte-Carloapproximationwithonesample
x

n
andonedata
example,andthesecondequalityisachainrulesince
x

n
(
˚
)
isafunctionof
˚
duetothereparam-
eterizationtrick.
Thegradientofthedenominatorinthetermisthefollowing:
@
@
˚
N
X
n
=1
E
q
[
q
(
x
n
j
f
˚
NN
(
y
n
))]
ˇ
N
@
@
˚
log
q
(
x

n
j
f
˚
NN
(
y
n
))
(31)
ˇ
N
@
@
x

n
log
q
(
x

n
j
f
˚
NN
(
y
n
))
@
x

n
@
˚
+
N
@
log
q
(
x

n
j
f
˚
NN
(
y
n
))
@
˚
(32)
wheretheapproximationisagainaMonte-Carloapproximationwithonesample
x

n
andone
dataexample,andthesecondequalityisusingthetotal-derivativetocomputederivativeofafunction
h
thatdependson
˚
directlybutalsothroughafunction
x
(
˚
)
:
@h
(
x
(
˚
)
;
˚
)
@
˚
=
@h
(
x
;
˚
)
@
x
@
x
@
˚
+
@h
(
x
;
˚
)
@
˚
(33)
Inourcase,
q
isafunctionof
˚
through
x

n
(
˚
)
andalsothrough
f
˚
NN
.
Thegradientofthesecondtermin(29)isequaltothefollowing:
@
E
q
[log
p
(
x
j

PGM
)]
@
˚
ˇ
@
log
p
(
x

j

PGM
)
@
x

@
x

@
˚
(34)
Thegradientofthethirdtermin(29)againrequiresthetotalderivative:
@
log
q
(
x
j
˚
PGM
)
@
˚
ˇ
@
log
q
(
x

j
˚
PGM
)
@
x

@
x

@
˚
+
@
log
q
(
x

j
˚
PGM
)
@
˚
(35)
Finally,thederivativeofthelasttermisstraightforward.
Thisgivesusthegradientsthatcanberearrangedtoget(10):
@
L
SIN
@
˚
ˇ
N
@
@
x

n
log
p
(
y
n
j
x

n
;

NN
)
@
x

n
@
˚

N
@
@
x

n
log
q
(
x

n
j
f
˚
NN
(
y
n
))
@
x

n
@
˚

N
@
log
q
(
x

n
j
f
˚
NN
(
y
n
))
@
˚
+
@
log
p
(
x

j

PGM
)
@
x

@
x

@
˚

@
log
q
(
x

j
˚
PGM
)
@
x

@
x

@
˚

@
log
q
(
x

j
˚
PGM
)
@
˚
+
@
log
Z
(
˚
)
@
˚
(36)
13
"
95,SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment,http://arxiv.org/pdf/1806.05521v1.pdf,https://github.com/ghdi6758/SemAxis,arXiv:1806.05521v1  [cs.CL]  14 Jun 2018
96,Split-door criterion: Identification of causal effects through auxiliary outcomes,http://arxiv.org/pdf/1611.09414v2.pdf,https://github.com/amit-sharma/splitdoor-causal-criterion,"arXiv:
arXiv:1611.09414
SPLIT-DOORCRITERION:IDENTIFICATIONOFCAUSAL
EFFECTSTHROUGHAUXILIARYOUTCOMES
ByAmitSharma,JakeM.HofmanandDuncanJ.Watts
MicrosoftResearch
Wepresentamethodforestimatingcausalintimeseriesdata
wheninformationabouttheoutcomeofinterestisavailable.
Sp,weexaminewhatwecallthe
split-door
setting,wheretheout-
comevariablecanbesplitintotwoparts:onethatispotentially
bythecausebeingstudiedandanotherthatisindependentofit,with
bothpartssharingthesame(unobserved)confounders.Weshowthatun-
dertheseconditions,theproblemofidenreducestothatoftesting
forindependenceamongobservedvariables,andpresentamethodthatuses
thisapproachtoautomaticallysubsetsofthedatathatarecausally
idenWedemonstratethemethodbyestimatingthecausalimpactof
Amazon'srecommendersystemontoproductpages,thou-
sandsofexampleswithinthedatasetthatsatisfythesplit-doorcriterion.
Unlikepaststudiesbasedonnaturalexperimentsthatwerelimitedtoa
singleproductcategory,ourmethodappliestoalargeandrepresentative
sampleofproductsviewedonthesite.Inlinewithpreviouswork,we
thatthewidely-usedclick-throughrate(CTR)metricoverestimatesthe
causalimpactofrecommendersystems;dependingontheproductcategory,
weestimatethat50-80%oftheattributedtorecommendersystems
wouldhavehappenedevenwithoutanyrecommendations.Weconclude
withguidelinesforusingthesplit-doorcriterionaswellasadiscussionof
othercontextswherethemethodcanbeapplied.
1.Introduction.
Therecentgrowthofdigitalplatformshasgeneratedanavalanche
ofhighlygranularandoftenlongitudinaldataregardingindividualandcollectivebehav-
iorinavarietyofdomainsofinteresttoresearchers,includingine-commerce,health-
care,andsocialmediaconsumption.Becausethevastmajorityofthisdataisgenerated
innon-experimentalsettings,researcherstypicallymustdealwiththepossibilitythat
anycausalofinterestarecomplicatedbyanumberofpotentialconfounds.For
example,evenasconceptuallysimpleasthecausalimpactofrecommendations
oncustomerpurchasesarelikelyconfoundedbyselectione[
Lewis,RaoandReiley
,
2011
],correlateddemand[
Sharma,HofmanandWatts
,
2015
],orothersharedcauses
ofbothexposureandpurchase.Figure
1a
showsthiscanonicalclassofcausalinference
problemsintheformofacausalgraphicalmodel[
Pearl
,
2009
],whereXisthecauseand
Yisitsct.Together
U
and
W
refertoallofthecommoncausesofXandYthat
mayconfoundestimationofthecausalwherecriticallysomeoftheseconfounders
(labeled
W
)maybeobserved,whileothers(
U
)areunobservedorevenunknown.Ideally
onewouldanswersuchquestionsbyrunningrandomizedexperimentsontheseplatforms,
butinpracticesuchtestsarepossibleonlyfortheownersoftheplatforminquestion,
andeventhenareoftenbesetwithimplementationorethicalconcerns[
Fiske
andHauser
,
2014
].Asaresultresearchersareleftwithtwomainstrategiesformaking
causalestimatesfromlarge-scaleobservationaldata,eachwithitsownassumptionsand
limitations:eitherconditioningonobservablesorexploitingnaturalexperiments.
1.1.
Background:Back-doorcriterionandnaturalexperiments.
Theandbyfar
themorecommonapproachistoassumethattheofunobservedconfounders(
U
)
isnegligibleafterconditioningontheobservedvariables(
W
).Undersucha
selectionon
observables
assumption[
ImbensandRubin
,
2015
],oneconditionson
W
toestimatethe

WewouldliketothankDeanEckles,PraneethNetrapalli,JoshuaAngrist,T.TonyKe,andanony-
mousreviewersfortheirvaluablefeedbackonthiswork.
Keywordsandphrases:
causalinference,datamining,causalgraphicalmodel,naturalexperiment,
recommendationsystems
1
arXiv:1611.09414v2  [stat.ME]  14 Jun 20182
SHARMAETAL.
(a)Canonicalcausalin-
ferenceproblem
(b)Estimationwith
back-door
criterion
(c)EstimationwithZasan
instrumentalvariable
Fig1:
Left:
Graphicalmodelforthecanonicalproblemincausalinference.Wewishto
estimatetheof
X
on
Y
.
W
representsobservedcommoncausesof
X
and
Y
;
U
representsotherunobserved(andunknown)commoncausesthatconfoundobservational
estimates.
Middle
:Thecausalmodelunderthe
selectiononobservables
assumption,
wheretherearenoknownunobservedconfounds
U
.
Right:
Thecanonicalcausalmodel
foraninstrumentalvariable
Z
thatsystematicallyshiftsthedistributionofthecause
X
independentlyofconfounds
U
.
of
X
on
Y
whentheseconfoundersareheldconstant.Inthelanguageofgraph-
icalmodels,thisstrategyisreferredtoasthe
back-doorcriterion
[
Pearl
,
2009
]onthe
groundsthatthe\back-doorpathway""fromXtoY(viaW)isblockedbycondition-
ingonW(seeFigure
1b
)andcanbeimplementedbyavarietyofmethods,including
regression,andmatching[
Rubin
,
2006
;
Stuart
,
2010
].Unfortunatelyfor
mostpracticalproblemsitistoestablishthatalloftheimportantconfounders
havebeenobserved.Forexample,considertheproblemofestimatingthecausalimpact
ofarecommendersystemontoe-commercewebsitessuchasAmazon.com,where
X
correspondstothenumberofvisitstoaproduct'swebpage,and
Y
thevisitstoa
recommendedproductshownonthatwebpage.Onecouldcomputetheobservedclick-
throughrateafterconditioningonallavailableuserandproductattributes(e.g.,user
demographics,productcategoriesandpopularities,etc.),assumingthatthesefeatures
constituteaproxyforlatentdemand.Unfortunately,therearealsomanypotentially
unobservedconfounders(e.g.,advertising,mediacoverage,seasonality,etc.)thatimpact
bothaproductanditsrecommendations,whichifexcludedwouldrendertheback-door
criterioninvalid.
Motivatedbythelimitationsoftheback-doorstrategy,asecondmainapproachisto
identifyanexternaleventthatthetreatment
X
inawaythatisarguablyran-
domwithrespecttopotentialconfounds.Thehopeisthatsuchvariation,knownasa
naturalexperiment
[
Dunning
,
2012
],canserveasasubstituteforanactualrandomized
experiment.Continuingwiththeproblemofestimatingthecausalimpactofrecommen-
dations,onemightlookforanaturalexperimentinwhichsomeproductsexperience
largeandsuddenchangesintrc,forinstancewhenabookisfeaturedonOprah's
bookclub[
Carmi,Oestreicher-SingerandSundararajan
,
2012
].Assumingthatthein-
creaseinforthebookisindependentofdemandforitsrecommendations,onecan
estimatethecausaloftherecommenderbymeasuringthechangeinsalestothe
recommendedproductsbeforeandafterthebookwasfeatured,arguingthatthesesales
wouldnothavehappenedintheabsenceoftherecommender.Sucheventsprovide
in-
strumentalvariables
thatidentifytheofinterestbyshiftingthedistributionofthe
cause
X
independentlyofunobservedconfounds
U
[
Angrist,ImbensandRubin
,
1996
].
Figure
1c
depictsthisinagraphicalmodel,wheretheadditionalobservedvariable
Z
denotestheinstrumentalvariable.
Thesetwomainapproachestradecriticalgoalsofidenandgeneralization
incausalinference.Theestimateforback-doorconditioningistypicallyderivedusingall
availabledata,butprovidesnoidenguaranteesinthepresenceofunobserved
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
3
(a)Generalsplit-doormodel:Outcome
Y
issplitinto
Y
R
and
Y
D
(b)Validsplit-doormodel:Datasubsets
where
X
isindependentof
U
Y
Fig2:Panel(a)illustratesthecanonicalcausalinferenceproblemwhenoutcome
Y
can
besplitupintotwocomponents.Forclarity,unobservedconfounders
U
arebrokeninto
U
Y
thatboth
X
and
Y
,and
U
X
thatonly
X
.Thesplit-doorcriterion
subsetsofthedatawherethecause
X
isindependentof
U
Y
bytestingindependenceof
X
and
Y
D
,leadingtotheunconfoundedcausalmodelshowninPanel(b).
confounders.Instrumentalvariables,incontrast,provideidenguaranteeseven
inthepresenceofunobservedconfounders,buttheseguaranteesapplyonlyforlocal
subsetsoftheavailabledata|therelativelyrareinstancesforwhichavalidinstrument
thatexogenouslyvariesthecause
X
isknown(e.g.,lotteries[
Angrist,ImbensandRubin
,
1996
],variationinweather[
PhanandAiroldi
,
2015
],orsudden,largeevents[
Rosenzweig
andWolpin
,
2000
;
Dunning
,
2012
]).
1.2.
The\split-door""criterion.
Inthispaperweintroduceacausaliden
strategythatincorporateselementsofboththeback-doorandnaturalexperimentap-
proaches,butthatappliesinatsetting.Ratherthanconditioningonobservable
confounds
W
orexploitingsourcesofindependentvariationinthecause
X
,weinstead
lookto
auxiliaryoutcomes
[
MealliandPacini
,
2013
]toidentifysubsetsofthedatathat
arecausallyidenSp,ourstrategyapplieswhentheoutcomevariable
Y
can
beely\split""intotwoconstituents:onethatiscausedby
X
andanotherthatis
independentofit.Figure
2a
showsthecorrespondingcausalgraphicalmodel,where
Y
R
denotesthe\referred""outcomeofinterestectedby
X
and
Y
D
indicatesthe\direct""
constituentof
Y
thatdoesnotdirectlydependon
X
.Returningtotherecommendersys-
temexample,
Y
R
correspondstorecommendationclick-throughsonaproductwhereas
Y
D
wouldbeallothertothatproductthatcomesthroughchannelssuchasdirect
searchorbrowsing.Wheneversuchdataon
Y
isavailable,weshowthatitis
possibletoreducecausalidentoanindependencetestbetweenthecause
X
and
theauxiliaryoutcome
Y
D
.Becausethisstrategydependsontheavailabilityofasplitset
ofvariablesfor
Y
,wecallitthe
split-door
criterionforcausalidenbyanalogy
withthemorefamiliarback-doorcriterion.
Althoughwemakenoassumptionsaboutthefunctionalformofrelationshipsbetween
variables,acrucialassumptionunderlyingthesplit-doorcriterionis
connectedness
;i.e.,
thattheauxiliaryoutcome
Y
D
mustbe(possiblyrently)by
all
causesthat
also
Y
R
.AswediscussinmoredetailinSection
5
,thisassumptionisplausiblein
scenariossuchasonlinerecommendersystems,whererecommendedproductsarereach-
ablethroughmultiplechannels(e.g.,searchordirectnavigation)anditisunlikelythat
demandforaproductmanifestsitselfexclusivelythroughonlyoneofthesechannels.
Moregenerally,theconnectednessassumptionisexpectedtoholdinscenarioswhere
directandreferredoutcomesincursimilarcost,whichmakesitunlikelythatsomething
thatcausestheoutcomedoessoonlywhenreferredthrough
X
,butneverdirectly.
Undertheaboveassumption,thesplit-doorcriterionseekstoidentifysubsetsofthe
datawherecausalidenispossible.Inthissense,themethodresemblesanatural
4
SHARMAETAL.
experiment,exceptthatinsteadoflookingforaninstrumentthatcreatesvariationin
X
,
welookforvariationsin
X
directly.Asinanaturalexperiment,however,itisimportant
thatanysuchvariationin
X
isindependentofpotentialconfounds.Forinstanceinthe
exampleabove,itisimportantthatasuddenburstofinterestinaparticularbookis
notcorrelatedwithchangesinlatentdemandforitsrecommendations.Toverifythis
requirement,thesplit-doorcriterionreliesonastatisticaltesttoselectforcaseswhere
therearenoconfounds(observedorotherwise)between
X
and
Y
R
.Sp,weshow
thatgivenasuitableauxiliaryoutcome
Y
D
,andatesttoestablishif
X
and
Y
D
are
independent,thecausalbetween
X
and
Y
R
canbeidenFurthermore,since
thistestinvolvestwoobservedquantities(
X
and
Y
D
),wecansystematicallysearchfor
subsetsofthedatathatsatisfytherequiredcondition,potentiallydiscoveringalarge
numberofcasesinwhichwecanidentifythecausalof
X
on
Y
R
.
Weillustratethismethodwithadetailedexampleinwhichweestimatethecausal
impactofAmazon.com'srecommendationsystemusinghistoricalwebbrowsingdata.
Undertheaboveassumptionsonthedependencebetweenreferredanddirectvisitsto
aproduct'swebpage,weshowhowthecriterionprovidesaprincipledmechanismfor
determiningwhichsubsetsofthedatatoincludeintheanalysis.Thesplit-doorcriterion
identhousandsofsuchinstancesinanine-monthperiod,comparableinmagnitude
toamanuallytunedapproachusingthesamedata[
Sharma,HofmanandWatts
,
2015
],
andanorderofmagnitudemorethantraditionalapproaches[
Carmi,Oestreicher-Singer
andSundararajan
,
2012
].Further,theproductsincludedinouranalysisarerepresenta-
tiveoftheoverallproductdistributionoverproductcategoriesonAmazon.com,thereby
improvingboththeprecisionandgeneralizabilityofestimates.Consistentwithprevious
work[
Sharma,HofmanandWatts
,
2015
],wethatobservationalestimatesofrec-
ommendationclick-throughrates(CTRs)overstatetheactualbyanywherefrom
50%to80%,callingintoquestionthevalidityofpopularCTRmetricsforassessingthe
impactofrecommendationsystems.Forapplicationstootheronlineandscenarios,
weprovideanRpackage
1
thatimplementsthesplit-doorcriterion.
1.3.
Outlineofpaper.
Theremainderofthispaperproceedsasfollows.InSection
2
westartwithaformalofthesplit-doorcriterionandgivepreciseconditionsun-
derwhichthecriterionholds.Forclarityweprovideproofsforcausalidentiationboth
intermsofthecausalgraphicalmodelfromFigure
2a
andalsointermsofstructural
equations.InSection
3
weproposeasimple,scalablealgorithmforidentifyingcausal
usingthesplit-doorcriterion.TheninSection
4
,weexplainmoreformallyhow
thesplit-doorcriterionfromtheinstrumentalvariablesandback-doormethods
mentionedabove.Section
5
presentsdetailsabouttheAmazon.comdataandanappli-
cationofthesplit-doorcriteriontoestimatethecausalimpactofitsrecommendation
system.InSection
6
wethendiscusslimitationsofthesplit-doorcriterionaswellas
othersettingsinwhichthecriterionapplies,arguingthatmanyexistingdatasetsacross
avarietyofdomainshavethestructurethatoutcomesofinterestcanbedecomposedinto
their\direct""and\referred""constituents.Weconcludewithapredictionthatasthesize
andgranularityofavailabledatasets,alongwiththenumberofvariablesinthem,in-
creaseataneverfasterrate,data-drivenapproachestocausalidenwillbecome
commonplace.
2.TheSplit-doorIdenonCriterion.
Thesplit-doorcriterioncanbeused
wheneverobserveddataisgeneratedfromthemodelshowninFigure
2a
.Here
X
repre-
sentsthecauseofinterest,
Y
R
denotesthe\referred""portionoftheoutcomedby
it,and
Y
D
indicatesthe\direct""partoftheoutcomewhichdoesnotdirectlydependon
X
.Wedenotetheoveralloutcomeby
Y
=
Y
R
+
Y
D
.Welet
U
Y
representallunobserved
causesof
Y
,someofwhichmayalsobecommoncausesof
X
,hencethearrowfrom
U
Y
to
X
.Additionallatentfactorsthatonly
X
arecapturedby
U
X
.Both
U
X
and
U
Y
1
URL:
http://www.github.com/amit-sharma/splitdoor-causal-criterion
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
5
canbeacombinationofmanyvariables,someobservedandsomeunobserved.(Forfull
generality,theanalysispresentedhereassumesthatallconfoundsareunobserved.)As
notedearlier,theunobservedvariables
U
Y
create\back-doorpathways""thatconfound
thecausalof
X
on
Y
,resultinginbiasedestimates.Thecentralideabehindthe
split-doorcriterionisthatwecanuseanindependencetestbetweentheauxiliaryout-
come
Y
D
and
X
tosystematicallysearchforsubsetsofthedatathatarefreeofthese
confoundsanddonotcontainback-doorpathwaysbetween
X
and
Y
R
.Inotherwords,
wecanconcludethatsuchsubsetsofthedataweregeneratedfromtheunconfounded
causalmodelshowninFigure
2b
,andthereforethecausalof
X
on
Y
canbeesti-
mateddirectlyfromthesedata.Importantly,idencationofthecausalrestson
theassumptionthatnopartof
U
Y
causesonepartof
Y
andnottheother.
2.1.
Thesplit-doorcriterionthroughagraphicalmodel.
Hereweformalizetheintu-
itionaboveinthecausalgraphicalmodelframework.Toidentifythecausalwe
makethefollowingtwoassumptions.Thepertainstoconnectednessofthecausal
model.
Assumption
1(Connectedness)
.
Anyunobservedconfounder
U
Y
thatcausesboth
X
and
Y
R
alsocauses
Y
D
andthecausalctofsuch
U
Y
on
Y
D
isnon-zero.
NotethatAssumption1requiresonlythatthecausalof
U
Y
on
Y
D
benon-
zero,withoutanyrequirementsonthesizeofthect(s)involved.Thatsaid,itisa
strongrequirementingeneral,asitappliestoallsub-componentsof
U
Y
andthusinvolves
assumptionsaboutpotentiallyhigh-dimensional,unobservedvariables.Whenever
Y
D
and
Y
R
arecomponentsofthesamevariableitisplausiblethattheysharecauses,butonestill
mustestablishthatthisconditionholdstoensurecausalidenItisinstructive
tocomparethisassumptiontothestrictindependenceassumptionsinvolvingunobserved
confoundersrequiredbymethodssuchasinstrumentalvariables[
Angrist,Imbensand
Rubin
,
1996
].
Thesecondassumption,whichrelatesstatisticalandcausalindependencebetween
observedvariables,isstandardformanymethodsofcausaldiscoveryfromobservational
data.
Assumption
2(Independence)
.
If
X
and
Y
D
arestatisticallyindependent,thenthey
arealsocausallyindependentinthegraphicalmodelofFigure
2a
.
Here
causal
independencebetweentwovariablesmeansthattheysharenocommon
causesandnodirectedpathinthecausalgraphicalmodelleadsfromonetoanother.
Moreformally,thetwovariablesare
\d-separated""
[
Pearl
,
2009
]fromeachother.Thus,
Assumption2isavariantoftheFaithfulnessorStabilityassumptionsincausalgraphs
withlatentunobservedvariables[
Spirtes,GlymourandScheines
,
2000
;
Pearl
,
2009
].
InthecausalmodelshowninFigure
2a
,forinstance,thisassumptionrulesoutthe
possibilityofaneventwheretheobservedvariables
X
and
Y
D
arefoundtobestatistically
independent,but
U
Y
stillbothofthemandtheobservedindependenceinthedata
resultsfrom
U
Y
'scancelingoutexactlyoverthepath
X
-
U
Y
-
Y
D
.Inotherwords,this
assumptionservestoruleoutan(unlikely)eventwhereincidentalequalityofparameters
orcertaindatadistributionsrendertwovariablesstatisticallyindependenteventhough
theyarecausallyrelated.
UnderAssumptions1and2,wecanshowthatstatisticalindependenceof
X
and
Y
D
ensuresthat
X
isnotconfoundedby
U
Y
.First,weprovidearesultabouttheresulting
causalgraphstructurewhen
X
??
Y
D
.
Lemma
1
.
Let
X
,
Y
R
and
Y
D
bethreeobservedvariablescorrespondingtothecausal
modelinFigure
2a
,where
U
Y
referstounobservedcausesof
Y
R
.Iftheconnectedness(1)
andindependence(2)assumptionshold,then
X
??
Y
D
impliesthattheedge
U
Y
!
X
doesnotexistorthat
U
Y
isconstant.
6
SHARMAETAL.
Proof(Argument).
TheproofcanbecompleteddirectlyfromFigure
2a
andproperties
ofacausalgraphicalmodel.
X
??
Y
D
impliesthatthecausalof
U
Y
on
Y
D
and
X
somehowcancelsouton
thepath
X
 
U
Y
!
Y
D
.By
Assumption2
,thiscancellationisnotduetoincidental
equalityofparametersoraparticulardatadistribution,butratherapropertyofthe
causalgraphicalmodel.Therefore,thiscanonlyhappenif
(i)
U
Y
isconstant(andthus
blocks
thepath),or
(ii)Oneoftheedgesexiststrivially(doesnothaveacausalUsing
Assumption
1
,
U
Y
hasanon-zerocton
Y
D
.Then,theonlyalternativeisthatthe
X
 
U
Y
edge
doesnotexist,leadingtotheunconfoundedcausalmodelinFigure
2b
.
Proof.
Weprovideaproofbycontradictionusingtheprincipleof
d-separation
[
Pearl
,
2009
]inacausalgraphicalmodel.
Letussuppose
X
??
Y
D
,andthatthe
U
Y
!
X
edgeexistsand
U
Y
isnotconstant.
Usingtherulesof
d-separation
onthecausalmodelinFigure
2a
,thepath
X
-
U
Y
-
Y
D
correspondsto:
(
X
??
Y
D
j
U
Y
)
G
(2.1)
(
X
6
??
Y
D
)
G
(2.2)
wherethenotation(
:
)
G
refersto
d-separation
underacausalmodel
G
.Inourcase,
G
correspondstothecausalmodelinFigure
2a
.
However,usingAssumption2,statisticalindependenceof
X
and
Y
D
impliescausal
independence,andthus,d-separationof
X
and
Y
D
.
(
X
??
Y
D
)
G
(2.3)
Equations2.2and2.3resultinacontradiction.Toresolve,
(i)Either
U
Y
isconstantandthus2.1implies(
X
??
Y
D
)
G
holds,or
(ii)Thepath
X
-
U
Y
-
Y
D
doesnotexist.Using
Assumption1
ofdependenceof
Y
D
on
U
Y
,theonlypossibilityisthatthe
X
 
U
Y
edgedoesnotexist.
WenowshowthatLemma
1
removesconfoundingdueto
U
Y
andthattheobservational
estimate
P
(
Y
R
j
X
=
x
)isalsothecausalestimate.
Theorem
2.1(Split-doorCriterion)
.
UndertheassumptionsofLemma
1
,thecausal
ctof
X
on
Y
R
isnotconfoundedby
U
Y
andisgivenby:
P
(
Y
R
j
do
(
X
=
x
))=
P
(
Y
R
j
X
=
x
)
where
do
(
X
=
x
)
referstoexperimentalmanipulationof
X
and
Y
R
j
X
=
x
referstothe
observedconditionaldistribution.
Proof(Argument).
Lemma
1
leadstotwocases:
(i)Bytheback-doorcriterion[
Pearl
,
2009
],if
U
Y
isconstant,then
X
and
Y
R
areuncon-
founded,becausetheonlyback-doorpathbetween
X
and
Y
R
contains
U
Y
onit.
(ii)Similarly,ifthe
U
Y
!
X
edgedoesnotexist,then
X
and
Y
R
areunconfounded
becauseabsenceofthe
U
Y
!
X
edgeremovestheback-doorpathbetween
X
and
Y
R
.
Inbothcases,unconfoundednessimpliesthattheof
X
on
Y
R
canbeestimated
usingtheobservationaldistribution.
Proof.
Theprooffollowsfromanapplicationofthesecondruleofdo-calculus[
Pearl
,
2009
].
P
(
Yj
do
(
Z
=
z
)
;
W
)=
P
(
YjZ
=
z;
W
)if(
Y
??
ZjW
)
G
Z
(2.4)
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
7
where
G
Z
referstotheunderlyingcausalgraphicalmodelwithalloutgoingedgesfrom
Z
removed.
Substituting
Y
=
Y
R
,
Z
=
X
,
G
X
correspondstothecausalmodelfromFigure
2a
withoutthe
X
!
Y
R
edge.UsingLemma
1
,twocasesexist:
(i)
U
Y
isconstant
Let
W
=
U
Y
.Underthemocausalmodel
G
X
withoutthe
X
!
Y
R
edge,thepath
X
-
U
Y
-
Y
R
istheonlypathconnecting
X
and
Y
R
,whichleadstothefollowing
d-separation
result:
(
Y
R
??
X
j
U
Y
)
G
X
(2.5)
CombiningRule
2.4
andtheabove
d-separation
result,weobtain
P
(
Y
R
j
do
(
X
=
x
)
;U
Y
)=
P
(
Y
R
j
X
=
x;U
Y
)=
P
(
Y
R
j
X
=
x
)
wherethelastequalityholdsbecause
U
Y
isconstantthroughout.
(ii)Theedge
U
Y
!
X
doesnotexist.
Let
W
=
;
.Underthemocausalmodel
G
X
withoutthe
X
!
Y
R
edge,
X
and
Y
R
aretrivially
d-separated
becausenopathconnectsthemwithouttheedge
U
Y
!
X
.
(
Y
R
??
X
)
G
X
(2.6)
FromRule
2.4
andtheabove
d-separation
result,weobtain
P
(
Y
R
j
do
(
X
=
x
))=
P
(
Y
R
j
X
=
x
)
2.2.
Thesplit-doorcriterionthroughstructuralequations.
Althoughwehavealready
analyzedthesplit-doorcriterionintermsofthecausalgraphicalmodelinFigure
2a
,
forexpositionalclaritywenotethatitisalsopossibletodothesameusingstructural
equations.Sp,wecanwritethreestructuralequations:
x
=
g
(
u
x
;u
y
;""
x
)
y
r
=
f
(
x;u
y
;""
yr
)
y
d
=
h
(
u
y
;""
yd
)
;
(2.7)
where
""
x
,
""
yr
,and
""
yd
aremutuallyindependent,zero-meanrandomvariablesthatcap-
turemodelingerrorandstatisticalvariability.Asin
Assumption1
,weassumethat
U
Y
both
Y
D
and
Y
R
.Ingeneral,thecausalamongvariablesmaynotbelinear;
however,forthepurposeofbuildingintuitionwerewritetheaboveequationsinlinear
parametricform:
x
=
u
x
+

1
u
y
+

x
y
r
=
ˆx
+

2
u
y
+

yr
y
d
=

3
u
y
+

yd
;
(2.8)
where
ˆ
isthecausalparameterofinterest,and

x
,

yr

yd
areindependenterrorsinthe
regressionequations.Thesplit-doorcriterionrequiresindependenceof
X
and
Y
D
,which
inturnimpliesthatCov(
X;Y
D
)=0:
0=Cov(
X;Y
D
)=E[
XY
D
]

E[
X
]E[
Y
D
]
=E[(
u
x
+

1
u
y
+

x
)(

3
u
y
+

yd
)]

E[
u
x
+

1
u
y
+

x
]E[

3
u
y
+

yd
]
=

1

3
E[
U
Y
:U
Y
]


1

3
E[
U
Y
]E[
U
Y
]
=

1

3
Var(
U
Y
)
Assumingthat
Y
D
isby
U
Y
(andtherefore

3
isnot0),theabovecanbezero
onlyif

1
=0,orif
U
Y
isconstant(Var[
U
Y
]=0).Inbothcases,
X
becomesindependent
of
U
Y
andthefollowingregressioncanbeusedasanunbiasedestimatorfortheof
X
on
Y
R
:
y
r
=
ˆx
+

0
yr
(2.9)
where

0
yr
denotesanindependenterror.
8
SHARMAETAL.
3.ApplyingtheSplit-doorCriterion.
Theresultsoftheprevioussectionmoti-
vateanalgorithmforapplyingthesplit-doorcriteriontoobservationaldata.Sp,
givenanempiricaltestforindependencebetweenthecause
X
andtheauxiliaryoutcome
Y
D
,wecanselectinstancesinourdatathatpassthistestandsatisfythesplit-door
criterion.Inthissectionwedevelopsuchatestfortimeseriesdata,resultinginasimple,
scalableidencationalgorithm.
Atahighlevel,thealgorithmworksasfollows.First,dividethedataintoequally-
spacedtimeperiods
˝
suchthateachperiodhasenoughdatapointstoreliablyestimate
thejointprobabilitydistribution
P
(
X;Y
D
).Then,foreachtimeperiod
˝
,
1.
Determinewhether
X
and
Y
D
areindependentusinganempiricalindependence
test.
2.
If
X
and
Y
D
aredeterminedtobeindependent,thenthecurrenttimeperiod
˝
correspondstoavalidsplit-door
instance
.Usetheobservedconditionalprobability
P
(
Y
R
j
X
=
x
)toestimatethecausalctinthetimeperiod
˝
.Otherwise,exclude
thecurrenttimeperiodfromtheanalysis.
3.
Averageoveralltimeperiodswhere
X
??
Y
D
toobtainthemeancausalof
X
on
Y
R
.
Implementingthealgorithmrequiresmakingsuitablechoicesforanindependencetest
andalsoitslevel,takingintoaccountmultiplecomparisons.Inthefollow-
ingsections,wediscussthesechoicesindetail,aswellassensitivityofthemethodto
violationsinourassumptions.
3.1.
Choosinganindependencetest.
Each
X
-
Y
D
pairin
Step1
providestwovectors
oflength
˝
withobservedvaluesfor
X
and
Y
D
.Thekeydecisioniswhetherthesevec-
torsareindependentofeachother.Intheoryanyempiricaltestthatreliablyestablishes
independencebetween
X
and
Y
D
isttoidentifyinstanceswherethesplit-door
criterionapplies.Forinstance,assumingwehaveenoughdata,wecouldtestforinde-
pendencebycomparingtheempiricalmutualinformationtozero[
Steueretal.
,
2002
;
PethelandHahs
,
2014
].Inpractice,however,becauseweconsidersubsetsofthedata
overrelativelysmalltimeperiods
˝
,theremaybesubstantiallimitstothestatistical
powerwehaveintestingforindependence.Forexample,itiswellknownthatinsmall
samplesizes,testingforindependenceviamutualinformationestimationcanbeheavily
biased[
Paninski
,
2003
].
Thus,whenworkingwithsmalltimeperiods
˝
werecommendtheuseofexactinde-
pendencetestsandrandomizationinference[
Agresti
,
1992
,
2001
;
Lydersenetal.
,
2007
].
2
Ingeneral,thisapproachinvolvesrepeatedlysamplingrandomizedversionsoftheem-
piricaldatatosimulatethenullhypothesisandthencomparingateststatisticonthe
observeddatatothesameonthenulldistribution.Sp,foreach
X
-
Y
D
pair,
wesimulatethenullhypothesisofindependencebetween
X
and
Y
D
byreplacingthe
observed
X
vectorwitharandomlysampledvectorfromtheoverallempiricaldistribu-
tionof
X
values.Fromthissimulated
X
-
Y
D
instance,wecomputeateststatisticthat
capturesstatisticaldependence,suchasthedistancecorrelation,whichcandetectboth
non-linearandlineardependence[
Szekelyetal.
,
2007
;
deSiqueiraSantosetal.
,
2014
].We
thenrepeatthisproceduremanytimestoobtainanulldistributionfortheteststatistic
ofthis
X
-
Y
D
pair.Finally,wecomputetheprobability
p
ofobtainingateststatistic
asextremeastheobservedstatisticunderthenulldistribution,andselectinstancesin
whichtheprobability
p
isaboveapre-chosenlevel

.
3.2.
Choosingaancelevel.
Incontrasttostandardhypothesistestingwhere
oneislookingtorejectthenullhypothesisthattwovariablesareindependentandthere-
forethresholdsonasmall
p
-value,herewearelookingforindependent
X
-
Y
D
pairsthat
2
When
X
and
Y
D
arediscretevariables,methodssuchasFisher'sexacttestareappropriate.If,
however,
X
and
Y
D
arecontinuous|asisthiscasefortheexamplewestudyinSection
5
|werecommend
theuseofresampling-basedrandomizationinferenceforestablishingindependence.
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
9
(a)Generalmodel:Unobservedvariables
splitinto
U
Y
and
V
Y
(b)Invalidsplit-doormodel:
X
isindepen-
dentof
U
Y
butnot
V
Y
Fig3:Violationoftheconnectednessassumption.Causesfor
X
and
Y
R
consistoftwo
components,
U
Y
and
V
Y
,where
V
Y
doesnot
Y
D
andhenceisundetectableby
thesplit-doorcriterion.InthegeneralcausalmodelshowninPanel(a),
X
!
Y
R
is
confoundedbyboth
U
Y
and
V
Y
.Inthecausalmodelcorrespondingtoasplit-door
instanceinPanel(b),
X
!
Y
R
isstillconfoundedbythecommoncause
V
Y
.
arehighlyprobableunderthenullandthuswantalarge
p
-value.Inotherwords,weare
interestedinalowTypeIIerror(orfalsenegatives),incontrasttostandardnullhypoth-
esistesting,wherethefocusisonTypeIerrors(falsepositives)andhence
levelsaresetlow.Therefore,onewaytochoosealevelwouldbetochoose

ascloseaspossibleto1tominimizeTypeIIerrorswhen
X
and
Y
D
aredependent.Atthe
sametime,weneedtoensurethatthetestyieldsadequatepowerforindependent
X
-
Y
D
pairs.Unlikeaconventionalhypothesistestfordependentpairs,powerforourtest
is1


,theprobabilitythatthetestdeclaresan
X
-
Y
D
pairtobeindependentwhenitis
actuallyindependent.Asweincrease

,typeIIerrorsdecrease,butpoweralsodecreases.
Complicatingmatters,thecombinationoflowpowerandalargenumberofhypothesis
testsraisesconcernsaboutfalselyacceptingpairsthatareactuallydependent.Asanex-
tremeexample,evenwhenall
X
-
Y
D
pairsinagivendatasetaredependent,someofthem
willpasstheindependencetestsimplyduetorandomchance.Therefore,amoreprinci-
pledapproachtoselecting

comesthroughestimatingtheexpectedfractionoferroneous
split-doorinstancesreturnedbytheprocedure,whichwerefertoas
˚
.Asdescribedin
Appendix
A
,weapplytechniquesfromthemultiplecomparisonsliterature[
Storey
,
2002
;
LiangandNettleton
,
2012
;
Farcomeni
,
2008
]toestimatethisfraction
˚
foranygiven
level.
3.3.
Sensitivitytoidentifyingassumptions.
Theabovealgorithmyieldsacausales-
timateonlyiftheidentifyingassumptionsof
connectedness
and
independence
aresatis-
Independenceisbasedonthestandardfaithfulnessassumptionincausaldiscovery
[
Spirtes,GlymourandScheines
,
2000
].Connectedness,ontheotherhand,requiresjus-
basedondomainknowledge.Evenwhentheconnectednessassumptionseems
plausible,werecommendasensitivityanalysistoassesstheectsofpotentialviolations
tothisassumption.
FromAssumption
1
,violationofconnectednessimpliesthatthereexistsomeunob-
servedvariablesthat
X
and
Y
R
butnot
Y
D
.Figure
3a
showsthisscenario,whichis
identicaltothemodelinFigure
2a
withtheadditionofanunobservedvariable
V
Y
that

X
and
Y
R
,butnot
Y
D
.Applyingthesplit-doorcriterioninthissettingensures
thatthereisnoof
U
Y
on
X
,butdoesnotalleviatepossibleconfoundsfrom
V
Y
,as
showninFigure
3b
.Notethatthisisanalogoustothesituationinback-door-basedmeth-
odswhenonefailstoconditiononunobservedvariablesthatboththetreatment
andoutcome.Correspondingly,sensitivityanalysesdesignedforback-door-basedmeth-
ods[
Harding
,
2009
;
Rosenbaum
,
2010
;
VanderWeeleandArah
,
2011
;
Carnegie,Harada
andHill
,
2016
]canbereadilyadaptedtoanalyzingsplit-doorinstances.Inaddition,not-
ingthatsplit-doorestimatesrepresentaveragesoveralldiscoveredsplit-doorinstances,
10
SHARMAETAL.
GraphicalmodelDescriptionUntestableas-
sumptions
LimitationsRecommendations
example
(a)
Back-door
criterion
Conditiononob-
servedconfounders
W
toisolatethe
treatmenteffect.
X
??
U
or
Y
??
U
Unlikelythat
thereareno
unobservedcon-
founders
U
.
Regressclick-
throughsonproduct
attributesanddirect
visitstorecom-
mendedproduct.
(b)
Instrumental
variable
Analyzesubsetof
datathathasinde-
pendentvariation
inthetreatment.
Z
??
U
and
Z
??
Y
j
X;U
Difficulttofinda
sourceofexoge-
nousvariationin
thetreatment.
Measuremarginal
click-throughson
productsthatexpe-
riencelarge,sudden
shocksintraffic.
(c)
Split-door
criterion
Analyzesubsetof
datawherethe
auxiliaryoutcome
Y
D
isindependent
ofthetreatment.
Y
D
6
??
U
Y
Requiresdepen-
dencybetween
anauxiliaryout-
comeandall
confounders.
Measuremarginal
click-throughson
allpairsofproducts
thathaveuncorre-
lateddirecttraffic.
Fig4
Comparisonofmethodsforestimatingthectofatreatment
X
onanoutcome
Y
.
W
and
U
representallobservedandunobservedconfounders,respectively,thatcommonlycauseboth
X
and
Y
.
weintroduceanadditionalsensitivityparameter

thatdenotesthefractionofinstances
forwhichconnectednessisviolated.InAppendix
B
weprovideaderivationshowingthat
sensitivityforthesplit-doorestimatereducestosensitivityforback-doormethodsand
conductthisanalysisfortheapplicationpresentedinSection
5
.
4.Connectionstoothermethods.
Thesplit-doorcriterionisanexampleof
methodsthatuseempiricalindependenceteststoidentifycausalundercertain
assumptions[
Jensenetal.
,
2008
;
Cattaneo,FrandsenandTitiunik
,
2015
;
Sharma,Hof-
manandWatts
,
2015
;
Grosse-Wentrupetal.
,
2016
].Bysearchingforsubsetsofthedata
wheredesiredindependenceholds,italsosharessomepropertieswithnaturalexperiment
methodssuchasinstrumentalvariablesandconditioningmethodssuchasregression.We
discusstheseconnectionsbelow;table
4
providesasummaryforeasycomparison.
4.1.
InstrumentalVariables.
Boththesplit-doorcriterionandinstrumentalvariable
(IV)methodscanbeusedtoexploitnaturallyoccurringvariationinsubsetsofob-
servationaldatatoidentifycausalImportantly,however,theymakedierent
assumptions.InIVmethods,oneusesanauxiliaryvariable
Z
,calledaninstrument,
thatisassumedtobeexogenousandthatsystematicallyshiftsthedistributionofthe
cause
X
.Thevalidityofaninstrumentreliesontwoadditionalassumptions:that
itiselyrandomwithregardtopotentialconfounders(
Z
??
U
),andsecondthat
theinstrumenttheoutcome
Y
onlythroughthecause
X
(
Z
??
Y
j
X;U
).Bothof
theseconditionsinvolveindependenceclaimsbetweenobservedandunobservedvariables,
makingthemimpossibletotestinpractice[
Dunning
,
2012
].
Thesplit-doorcriterionalsoreliesonanauxiliaryvariable,butonethatrelatestothe
outcomeinsteadofthetreatment.Sp,itexploitsanauxiliaryoutcome
Y
D
that
servesasaproxyforunobservedcommoncauses
U
Y
underthreeimportantassumptions.
Theisthatthecause
X
doesnot
Y
D
directly.Thesecondassumptionrequires
thatallunobservedconfounders(betweenthecauseandoutcome)that
Y
R
also

Y
D
.AswithIVmethodsabove,thesetwoassumptionsinvolveknowledgeofan
unobservedvariableand,asaresult,cannotbetested.Thethirdassumptionrequires
independencebetweenthecause
X
andtheauxiliaryoutcome
Y
D
.Sincebothofthese
variablesareobserved,thisassumptioncanbetestedempiricallysolongasweareinthe
standardsettingwherestatisticalindependenceimpliescausalindependence(
Assumption
2
),equivalenttotheassumptionof
faithfulness
[
Spirtes,GlymourandScheines
,
2000
].
Itistocomparethesetwosetsofassumptionsingeneral,butint
scenarios,oneofthesemethodsmaybemoresuitablethantheother.Ifavalidinstrument
isknowntoexist,forinstancethroughchangesinweatherorasaresultofalottery,the
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
11
variationitproducescanandshouldbeexploitedtoidentifycausaltsofinterest.
Thesplit-doorcriterion,incontrast,ismostusefulwhenonesuspectsthereisrandom
variationinthedata,butcannotidentifyitssource
apriori
.Inparticular,itiswell-suited
forlarge-scaledatawherethetwoassumptionsmentionedaboveareplausible,such
asindigitaloronlinesystems.
4.2.
Back-doorcriterion.
Alternatively,thesplit-doorcriterioncanbeinterpretedas
using
Y
D
asaproxyforallconfounders
U
Y
,andestimatingthecausalwhenever
Y
D
(andhence
U
Y
)isindependentof
X
.Viewedthisway,thesplit-doorapproachmay
appeartobenothingmorethanavariantoftheback-doorcriterionwhereoneconditions
on
Y
D
insteadof
U
Y
,howevertherearetwokeysbetweenthetwomethods.
First,substituting
Y
D
for
U
Y
intheback-doorcriterionassumesthat
Y
D
isaperfect
proxyfor
U
Y
.Thisisamuchstrongerassumptionthanrequiringthat
Y
D
besimply
by
U
Y
,becauseany(e.g.,measurementerror)between
Y
D
and
U
Y
caninvalidatetheback-doorcriterion[
Spirtes,GlymourandScheines
,
2000
].Second,the
twomethodsintheirapproachtoidenThesplit-doorcriterion
controls
for
theofunobservedconfoundersbysubsetsofdatawhere
X
isnot
by
U
Y
,whereastheback-doorcriterion
conditions
onaproxyfor
U
Y
tonullifythe
ofunobservedconfounders.Therefore,bydirectlycontrollingatthetimeofdata
selection,thesplit-doorcriterionfocusesonadmittingasubsetofthedataforanalysis
andestimation,whereasmethodsbasedonback-doorcriterionsuchas
regression,matching,andprocessthewholedatasetandextractestimates
viastatisticalmodels[
MorganandWinship
,
2014
].
Toillustratethesewecomparemathematicalformsofthesplit-doorand
back-doorcriteriaintermsofregressionequations.Conditioningon
Y
D
usingregression
willleadtothefollowingequation
y
r
=
ˆ
00
x
+
y
d
+

00
yr
;
appliedtotheentiredataset.Incontrastthesplit-doorcriterionleadstothesimpler
equation(asshownearlierinSection
2.2
)
y
r
=
ˆx
+

0
yr
;
appliedonlytosubsetsofdatawhere
X
and
Y
D
areindependent.
4.3.
Methodsbasedonempiricalindependencetests.
Finally,thesplit-doorcriterion
issimilartorecentworkthatproposesadata-drivenmethodfordeterminingtheap-
propriatewindowsizeinregressiondiscontinuitydesigns[
Cattaneo,FrandsenandTitiu-
nik
,
2015
;
Cattaneo,TitiunikandVazquez-Bare
].Inregressiondiscontinuities,treatment
(e.g.,acceptanceintoaprogram)isassignedbasedonwhetheranobservedvariable(e.g.,
atestscore)isaboveorbelowapre-determinedTheassumptionisthatonecan
compareoutcomesforthosejustaboveandjustbelowthetoestimatecausalef-
fects,butthecentralproblemishowfarfromthethisassumptionholds.The
authorspresentadata-drivenmethodforselectingawindowbytestingforindependence
betweenthetreatmentandpre-determinedcovariatesthatareuncoupledtotheoutcome
ofinterest.Thisapproachresemblesthesplit-doorcriterioninthatbothuseindepen-
denceteststodeterminewhichsubsetsofthedatatoincludewhenmakingacausal
estimate.Asaresult,bothmethodsaresubjecttoconcernsaroundmultiplehypothesis
testing,althoughtheregressiondiscontinuitysettingtypicallyinvolvesmanyfewercom-
parisonsthanthesplit-doorcriterion(dozensinsteadofthethousandsweanalyzehere)
andoccursovernestedwindows.Forthesereasonswetreatmultiplecomparisons
ently,estimatingtheerrorrateinidentifyingindependentinstancesinsteadofadjusting
nominalthresholdstotrytoeliminateerrors.
12
SHARMAETAL.
Fig5:Screenshotofafocalproduct,thebook\Purity"",anditsrecommendationson
Amazon.com.
5.Application:ImpactofaRecommenderSystem.
Wenowapplythesplit-
doorcriteriontotheproblemofestimatingthecausalimpactofAmazon.com'srecom-
mendersystem.Recommendersystemshavebecomeubiquitousinonlinesettings,pro-
vidingsuggestionsforwhattobuy,watch,readordonext[
Ricci,RokachandShapira
,
2011
].Figure
5
showsanexampleofoneofthemillionsofproductpagesonAma-
zon.com,wherethemainitemlistedonthepage,or
focalproduct
,isthebook\Purity""
byJonathanFranzen.Listedalongsidethisitemareafew
recommendedproducts
|two
writtenbyFranzenandonebyanotherauthor|suggestedbyAmazonaspotentiallyof
interesttoauserlookingfor\Purity"".Generatingandmaintainingtheserecommenda-
tionstakesconsiderableresources,andsoanaturalquestiononemightaskishowexactly
exposuretotheserecommendedproductschangesconsumeractivity.
Whilesimpletostate,thisquestionisdulttoanswerbecauseitrequiresanestimate
ofthecounterfactualofwhatwouldhavehappenedhadsomeonevisitedafocalproduct
buthadnotbeenexposedtoanyrecommendations.Sp,wewouldliketoknow
howmuchrecommendersystems
cause
,overandabovewhatwouldhavehappened
intheirabsence.Naivelyonecouldassumethatuserswouldnothaveviewedtheseother
productswithouttherecommendersystem,andasaresultsimplycomputetheobserved
click-throughrateonrecommendations[
Mulpuru
,
2006
;
Grau
,
2009
].Asdiscussedearlier,
however,thisassumptionignorescorrelateddemand:usersmighthavefoundtheirway
tosomeoftheserecommendedproductsanywayviadirectsearchorbrowsing,whichwe
collectivelyrefertoas\directForinstance,someuserswhoareinterestedinthe
book\Purity""mightbefansofFranzeningeneral,andsomighthavedirectlysearched
onAmazon.comforhisotherworkssuchas\Freedom""or\TheCorrections"",evenifthey
hadnotbeenshownrecommendationslinkingtothem.Thekeytoproperlyestimating
thecausalimpactoftherecommender,then,liesinaccountingforthiscorrelateddemand
betweenafocalproductanditsrecommendations.
Inthissectionweshowhowthesplit-doorcriterioncanbeusedtoeliminatethe
issueofcorrelateddemandbyautomaticallyidentifyingandanalyzinginstanceswhere
demandforaproductandone(ormore)ofitsrecommendationsareindependentover
sometimeperiod
˝
.Wedosobyformalizingthisproblemthroughacausalgraphical
modelofrecommendersystemrevealingastructureamenabletothesplit-door
criterion.Thenweapplythecriteriontoalarge-scaledatasetofwebbrowsingactivityon
Amazon.comtodiscoverthousandsofinstancessatisfyingthecriterion.Ourresultsshow
thatanaiveobservationalestimateoftheimpactofthisrecommendersystemoverstates
thecausalimpactontheproductsanalyzedbyafactorofatleasttwo.Weconcludewith
anumberofrobustnesschecksandcommentsonthevalidityandgeneralizabilityofour
results.
5.1.
Buildingthecausalmodel.
Theabovediscussionhighlightsthatunobserved
commondemandforbothafocalproductanditsrecommendationscanintroducebiasin
naiveestimatesofthecausalclick-throughrate(CTR)onrecommendations.Referring
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
13
backtoFigure
2a
,weformalizetheproblemasfollows,withvariablesaggregatedfor
eachday:

X
denotesthenumberofvisitstothefocalproduct
i
'swebpage.

Y
R
denotesrecommendationvisits,thenumberofvisitstotherecommendedprod-
uct
j
throughclicksontherecommendationforproduct
j
onproduct
i
'swebpage.

Y
D
denotesdirectvisits,thenumberofvisitstoproduct
j
thatdidnotoccur
throughclickingonarecommendation.Thesecouldbevisitsto
j
fromAmazon's
searchpageorthroughdirectvisitsto
j
'swebpage.

U
Y
denotesunobserveddemandforproduct
j
,includingbothrecommendation
click-throughsanddirectvisits.

U
X
representsthepartofunobserveddemandforproduct
i
thatisindependentof
U
Y
.
Toapplythesplit-doorcriterion,wemustinvestigatetheplausibilityofthe
con-
nectedness
and
independence
assumptionsfromSection
2.1
.First,theconnectedness
assumptionstatesthatboth
Y
R
and
Y
D
are(possiblytly)bythesame
componentsofdemand
U
Y
fortheproduct
j
.Asmentionedabove,connectednessisespe-
ciallyplausibleinthecontextofonlinerecommendersystemswhereproductsareeasily
reachablethroughmultiplechannels(e.g.,search,directnavigationorrecommendation
click-through)anditisunlikelythatdemandforaproductmanifestsitselfexclusively
throughonlyoneofthesechannels.Spe,itisunlikelythatthereexistsacompo-
nentofdemandforaproductthatmanifestsitselfonlythroughindirectrecommendation
click-throughs,butnotthroughdirectvisits.Putanotherway,forconnectednessnotto
hold,itwouldhavetobethecasethatuserswouldhavedemandforaproduct
only
if
theyarrivedviaarecommendationlink,butnotthroughothermeans.Tothebestof
ourknowledgenopath-spefeatureofthissortexistsonAmazon;thus,weexpectthe
connectednessassumptiontohold.
Second,withrespecttotheindependenceassumption,althoughwecannotruleout
coincidentalcancellationofthatresultin
X
??
Y
D
andviolatetheassumption,
weexpectsucheventstobeunlikelyoveralargenumberofproductpairs.Furthermore,
forcomplementaryproductrecommendations(whicharethefocusofthispaper),we
canlogicallyruleoutviolationoftheindependenceassumptionbecausethedemandfor
twocomplementaryproductsareexpectedtobepositivelycorrelatedwitheachother.
Therefore,itisreasonabletoassumethattheunobserveddemand
U
Y
(andallitssub-
components)ectboth
X
and
Y
D
inthesamedirection.Forinstance,letthe
of
U
Y
beincreasingforboth
X
and
Y
D
.Thentheindependenceassumptionis
becausetheof
U
Y
cannotbecanceledoutonthepath
X
 
U
Y
!
Y
D
ifthe
of
U
Y
(andanyofitssub-components)on
X
and
Y
D
areallpositive.Giventheabove
assumptions,thesamereasoningfromSection
2.1
allowsustoestablishthat
X
??
Y
D
is
atconditionforcausaliden
5.2.
Browsingdata.
EstimatingthecausalimpactofAmazon.com'srecommender
systemrequiresdatadetailingactivityonthesite.Toobtainsuchinforma-
tion,weturntoanonymizedbrowsinglogsfromuserswhoinstalledtheBingToolbar
andconsentedtoprovidetheiranonymizedbrowsingdatathroughit.Theselogscovera
periodofninemonthsfromSeptember2013toMay2014andcontainasessioniden
ananonymoususeridenandatime-stampedsequenceofallnon-secureURLsthat
theuservisitedinthatsession.WerestrictourattentiontobrowsingsessionsonAma-
zon.com,whichleavesuswith23.4millionpagevisitsby2.1millionusersspanning1.3
millionuniqueproducts.Oftheseproducts,weexaminethosethatreceiveaminimum
of10pagevisitsonatleastonedayinthistimeperiod,resultinginroughly22,000focal
productsofinterest.
Amazonshowsmanykindsofrecommendationsonitssite.Welimitouranalysisto
the\Customerswhoboughtthisalsobought""recommendationsdepictedinFigure
5
,as
theserecommendationsarethemostcommonandareshownonproductpagesfromall
14
SHARMAETAL.
productcategories.Toapplythesplit-doorcriterion,weneedtoidentifyfocalproductand
recommendedproductpairsfromthelogdataandseparateoutforrecommended
productsintodirect(
Y
D
)andrecommended(
Y
R
)visits.Fortunatelyithappenstobe
thecasethatAmazonmakesthisidenpossiblebyexplicitlyembeddingthis
informationintheirURLs.Sp,givenaURLforanAmazon.compagevisit,we
canusethe
ref
,orreferrer,parameterintheURLtodetermineifauserarrivedata
pagebyclickingonarecommendationorbyothermeans.Wethenusethesequence
ofpagevisitsinasessiontoidentifyfocalandrecommendedproductpairsbylooking
forfocalproductvisitsthatprecederecommendationvisits.Furtherdetailsaboutthe
toolbardatasetandconstructionoffocalandrecommendedproductpairscanbefound
inpastwork[
Sharma,HofmanandWatts
,
2015
].
5.3.
Applyingthesplit-doorcriterion.
Havingarguedfortheassumptionsunderlying
thesplit-doorcriterionandextractedtherelevantdatafrombrowsinglogs,thestep
inestimatingthecausalofAmazon.com'srecommendationsystemistousethecri-
teriontosearchforinstanceswhereaproductanditsrecommendationhaveuncorrelated
demand.
RecallingSection
3
,weemployarandomizationtesttosearchfor15-daytimeperiods
thatfailtorejectthenullhypothesisthatdirectvisitstoaproductandone(ormore)of
itsrecommendedproductsareindependent.Thechoiceof15daysrepresentsa
betweentworequirements:atimeperiodlargeenoughtoyieldreliableestimates;
andsecond,atimeperiodshortenoughthatAmazon'srecommendationsforanygiven
productareunlikelytohavechangedwithinthatwindow.
Thefullapplicationofthesplit-doorcriterionisasfollows.Foreachfocalproduct
i
andeach
˝
=15daytimeperiod:
1.
Compute
X
(
i
)
,thenumberofvisitstothefocalproductoneachday,and
Y
(
ij
)
R
,
thenumberofclick-throughstoeachrecommendedproduct
j
.Alsorecordthetotal
directvisits
Y
(
j
)
D
toeachrecommendedproduct
j
.
2.
Foreachrecommendedproduct
j
,usetherandomizationtestfromSection
3.1
to
determineif
X
(
i
)
isindependentof
Y
(
j
)
D
atapre-specidlevel.
3

If
X
(
i
)
isfoundtobeindependentof
Y
(
j
)
D
,computetheobservedclick-through
rate(CTR),^
ˆ
ij˝
=(
P
˝
t
=1
Y
(
ij
)
R
)
=
(
P
˝
t
=1
X
(
i
)
),asthecausalestimateofthe
CTR.Otherwiseignorethisproductpair.
3.
AggregatethecausalCTRestimateoverallrecommendedproductstocompute
thetotalcausalCTRperfocalproduct,^
ˆ
i˝
.
Finally,averagethecausalCTRestimateoveralltimeperiodsandfocalproductstoarrive
atthemeancausal^
ˆ
,andcomputetherateoferroneoussplit-doorinstances
˚
to
estimateerrorinthisestimate,asdetailedinAppendices
A
and
C
.
5.4.
Results.
Applyingtheabovealgorithmresultsinover114,000potentialsplit-
doorinstances,whereeachinstanceconsistsofapairoffocalandrecommendedproduct
overa15-daytimeperiod.Atalevelof

=0
:
95,weobtainmorethan7,000
instancesthatsatisfythesplit-doorcriterion.Consistentwithpreviouswork[
Sharma,
HofmanandWatts
,
2015
],thecorrespondingcausalCTRestimate^
ˆ
is2.6%(withthe
errorbarsspanning2.0%to2.7%),roughlyonequarterofthenaiveobservationales-
timateof9
:
6%arrivedatbycomputingtheclick-throughrateacrossallfocalandrec-
ommendedproductpairs.Putanotherway,theseresultsimplythatnearly75%ofpage
visitsgeneratedviarecommendationclick-throughswouldlikelyoccurintheabsenceof
recommendations.
3
Hereweoutanytimeperiodswhere
Y
D
isexactlyconstant(becausethatwillsatisfyempirical
independenceconditionstrivially).
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
15
(a)Acceptedat

=0.95
(b)Rejectedat

=0.95
Fig6:Examplesoftimeseriesforfocalandrecommendedproductsthatare(a)accepted
or(b)rejectedbythesplit-doorcriterionatalevelof

=0
:
95forthe
independencetest.
(a)
(b)
(c)
Fig7:Subplot(a)showsthenumberofvalidsplit-doorinstancesobtainedasthep-value
threshold(

)isincreased.Subplot(b)showstheexpectedfractionoferroneousinstances
(
˚
)returnedbythemethodforthosevaluesof

.Thecorrespondingestimateforcausal
CTRisshowninSubplot(c);errorbarsaccountforbothbiasdueto
˚
andnatural
varianceinthemeanestimate.
Figure
6a
showsexamplesofproductpairsthatareacceptedbythetestat

=0
:
95.
Theexampleontheleftshowsafocalproductthatreceivesalargeandsuddenshockin
pagevisits,whiledirectvisitstoitsrecommendedproductremainsrelativelyThis
isreminiscentoftheexamplesanalyzedin
Carmi,Oestreicher-SingerandSundararajan
[
2012
]and
Sharma,HofmanandWatts
[
2015
].Theexampleontheright,however,shows
moregeneralpatternsthatareacceptedunderthesplit-doorcriterionbutnotconsidered
bythesepreviousapproaches:althoughdirectvisitstoboththefocalandrecommended
productsvarysubstantially,theydosoindependently,andsoarestillusefulinour
estimateoftherecommender'sConversely,twoexampleproductpairsthatare
rejectedbythetestareshowninFigure
6b
.Asisvisuallyapparent,visitpatternsfor
eachofthefocalandrecommendedproductpairsarehighlycorrelated,andtherefore
notusefulinouranalysis.
Changingthenominalp-valuethresholdusedintheindependencetestallowsusto
exploreabetweencoverageacrossproductsinourdatasetandtheprecision
ofourcausalestimate.AsdetailedinAppendix
A
,alowerthresholdresultsinmore
discoveredinstances,butwithahigherlikelihoodoftheseinstancesbeinginvalid.For
instance,Figures
7a
and
7b
showthatdecreasingthethresholdto

=0
:
80resultsin
over20,000split-doorinstancescoveringnearly11,000uniquefocalproducts,butdoesso
attheexpenseofincreasingtheexpectedfractionofinvalidinstancesto0.21,indicating
thatapproximatelyoneineofthereturnedsplit-doorinstancesmaybeinvalid.The
result,summarizedinFigure
7c
,isthattheerrorbarsonourestimateof
ˆ
increaseaswe
decrease

.Theseerrorbars,calculatedusingEquation
C.4
fromAppendix
C
,account
16
SHARMAETAL.
Fig8:ComparisonofthecausalCTRwiththenaiveobservationalCTRforproducts
thatsatisfythesplit-doorcriterion.Categoriesareorderedbythenumberofproducts
foundbythesplit-doorcriterionineachcategory,with
eBooks
containingthemostand
HealthandBeauty
theleast.
forbothbiasduetoerroneoussplit-doorinstancesandthenaturalvarianceinthemean
estimateduetosampling.
4
As

decreases,erroneousinstancesdueto
˚
contributeto
mostofthemagnitudeoftheerrorbarsshowninFigure
7c
.Weobservethat

=0
:
95
agoodcompromise:errorboundsarewithin1percentagepointandweobtain
morethan7,000split-doorinstances.
Furthermore,wecanbreaktheseestimatesdownbythetproductcategories
presentonAmazon.com.Figure
8
showsthevariationof^
ˆ
acrossthemostpopular
categories,atanominallevelof

=0
:
95.Forthesetoffocalproducts
thatsatisfythesplit-doorcriterion,wealsocomputethenaiveobservationalCTR.We
seesubstantialvariationinthenaiveestimate,rangingfrom14%on
e-Books
to5%on
PersonalComputer
.However,whenweusethesplit-doorcriteriontocomputeestimates,
wethatthecausalCTRforallproductcategoriesliesbelow5%.Theseresults
indicatethatnaiveobservationalestimatesoverstatethecausalimpactbyanywhere
fromtwo-toe-foldacrosstproductcategories.
Therearetwoclearadvantagestothesplit-doorcriterioncomparedtopastapproaches
forestimatingthecausalimpactofrecommendersystems.First,weareabletostudya
largerfractionofproductscomparedtoinstrumentalvariableapproachesthatdependon
single-sourcevariations[
Carmi,Oestreicher-SingerandSundararajan
,
2012
]orrestricting
ourattentiontominingonlyshocksinobservationaldata[
Sharma,HofmanandWatts
,
2015
].Onthesamedataset,theshock-basedmethodin
Sharma,HofmanandWatts
[
2015
]
idenvalidinstanceson4,000uniquefocalproducts,whilethesplit-doorcriterion
instancesforover5,000uniquefocalproductsat

=0
:
95,andover11,000at

=0
:
80.Second,thesplit-doorcriterionprovidesaprincipledmethodtoselectvalid
instancesforanalysisbytuning

,thedesiredlevel,whilealsoallowingfor
anestimateofthefractionoffalselyacceptedinstances,
˚
.
5.5.
Threatstovalidity.
Aswithanyobservationalanalysis,ourresultsrelyoncer-
tainassumptionsthatmaybeviolatedinpractice.Furthermore,resultsobtainedona
subsetofdatamaynotberepresentativeofthebroaderdatasetofinterest.Herewecon-
4
Notethattheerrorbarsareasymmetric;weexpecterroneoussplit-doorinstancestodrivethecausal
estimateupfromitstruevalue,undertheassumptionthatdemandforthetwoproductsarepositively
correlatedwitheachother,asarguedinSection
5.1
.
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
17
(a)
(b)
Fig9:Sensitivityanalysisoftheobtainedclick-throughestimate.Panel(a)showsthe
scenariowhereallsplit-doorinstancesmaybeinvalid.Panel(b)assumesthatatmost
halfoftheinstancesareinvalid.Thedeviationofthetrueestimatefromtheobtained
estimateincreasesasthemagnitudeoftheconfoundingfrom
V
Y
to
X
(
c
1
)or
Y
(
c
2
)
increases;howeverthisdeviationislowerinPanel(b).Inbothdottedlineshows
theobtainedCTRestimate.
ductadditionalanalysestoassessboththeinternalandexternalvalidityofourestimate
ofthecausalofAmazon'srecommendations.
5.5.1.
Internalvalidity:Sensitivitytotheconnectednessassumption.
Asdescribedin
Section
3.3
,connectednessisthekeyidentifyingassumptionforthesplit-doorcriterion.
Herewedescribeatestforsensitivityoftheobtainedestimate(^
ˆ
)toviolationsofthe
connectednessassumption.
ReferringtothecausalmodelinFigure
3a
,violationoftheconnectednessassumption
impliesthatthereexistcomponentsofunobserveddemand
V
Y
thatbothfocal
productvisits
X
andrecommendationclick-throughs
Y
R
,butnotdirectvisitstothe
recommendedproduct
Y
D
.Forsimplicity,letusassumethat
V
Y
isunivariatenormal
andboth
X
and
Y
R
linearly.Wecanwritethecorrespondingstructuralequations
forthecausalmodelinFigure
3b
foreachsplit-doorinstanceas
x
=
c
1
v
y
+

1
(5.1)
y
r
=
f
(
x
)+
c
2
v
y
+

2
;
(5.2)
where
f
isanunknownfunction,and

1
and

2
areindependentfromallvariablesmen-
tionedaboveandarealsomutuallyindependent.Notethat

1
includestheof
U
X
and

2
includestheof
U
Y
.Foranysplit-doorinstance,theestimatorfromSec-
tion
5.3
estimatesthecausalassumingthateither
c
1
or
c
2
iszero.
Totestthesensitivityofourestimatetotheconnectednessassumption,wetakeour
actualdataandintroduceanconfound
V
Y
bysimulation,adding
c
1
V
Y
to
X
and
c
2
V
Y
to
Y
R
,respectively,forarangeoft
c
1
and
c
2
values.Wesimulate
V
Y
asastandardnormalandvary
c
1
and
c
2
between[

1
;
1],andcomparethese
confoundedestimatestoouractualestimateof^
ˆ
=2
:
6%for

=0
:
95.Figure
9a
shows
thedeviationbetweenestimatesusingtheactualandsimulateddataas
c
1
and
c
2
vary.
Theismaximizedwhenboth
c
1
and
c
2
arehighinmagnitudeandisnegligible
wheneitherof
c
1
or
c
2
arezero.Thesesimulationresultssuggestabilinearsensitivity
18
SHARMAETAL.
(a)
(b)
Fig10:Thedistributionofproductsandtotalvisitsoverproductcategories.Among
productswithatleast10pagevisitsonatleastoneday,thesubsetoffocalproducts
thatsatisfythesplit-doorcriterionarenearlyidenticaltothesetofallproducts.Fraction
ofpagevisitstothosefocalproductsshowmorevariation,buttheoveralldistributions
aresimilar.
to
c
1
and
c
2
,aresultwetheoreticallyinthecaseofalinearcausalmodelin
Appendix
B
.
Thisanalysisassumesthat
all
split-doorinstancesviolatetheconnectednessassump-
tion.Recognizingthatthisneednotbethecase,andthatonlysomeinstancesmaybe
invalid,weintroduceathirdsensitivityparameter

,whichcorrespondstothefraction
ofsplit-doorinstancesthatviolateconnectedness.Forinstance,wecantestsensitivityof
theestimatewhenatleasthalfofthesplit-doorinstancessatisfyconnectedness,asdone
by
Kangetal.
[
2016
]forinferenceundermultiplepossiblyinvalidinstrumentalvariables.
AsshowninFigure
9b
,when

=0
:
5deviationsfromtheobtainedsplit-doorestimate
arenearlyhalved,resultinginmorerobustestimates.
5.5.2.
Externalvalidity:Generalizability.
Althoughthesplit-doorcriterionyieldsvalid
estimatesofthecausalimpactofrecommendationsforthetimeperiodswhereprod-
uctpairsarefoundtobestatisticallyindependent,itisimportanttoemphasizethat
productsinthesplit-doorsamplemaynotbeselectedatrandom,thusviolatingthe
as-
if-random
[
Angrist,ImbensandRubin
,
1996
]assumptionpoweringgeneralizabilityfor
naturalexperiments.Asaresult,caremustbetakentoextrapolatetheseestimatesto
allproductsonAmazon.com.
Fortunately,asshowninFigure
10
,thedistributionofproductsandpagevisitsinour
samplecloselymatchestheinventoryandactivityonAmazon.com.Productswithatleast
onevalidsplit-doortimeperiodspanmanyproductcategoriesandcovernearlyaquarter
ofallfocalproductsinthedatasetat

=0
:
95.Figure
10a
showsthatthedistributionof
productsanalyzedbythesplit-doorcriterionacrosstproductcategoriesisalmost
identicaltotheoverallsetofproducts.Figure
10b
showsasimilarresultforthenumber
ofpagevisitsoftheseproductsacrosstproductcategories,exceptforeBooks
whichareover-representedinvalidsplit-doorinstances.Forcomparison,weapplythe
samepopularitylterthatweusedforthesplit-doorcriterion|atleast10pagevisits
onatleastoneday|tothedatasetwithallproducts.
Althoughtheseresultsdonotnecessarilyimplythattheas-if-randomassumption
is(indeeditisverylikelynottheydoindicatethatthesplit-door
criterionatleastallowsustoestimatecausaloveradiversesampleofpopular
productcategories,whichisaclearimprovementoverpastwork[
Carmi,Oestreicher-
SingerandSundararajan
,
2012
;
Sharma,HofmanandWatts
,
2015
].
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
19
6.Discussion.
Inthispaperwehavepresentedamethodforcomputingthecausal
ofavariable
X
onanothervariable
Y
R
wheneverwehaveanadditionalvariable
Y
D
whichfollowssometestableconditions,andhaveshownitsapplicationinestimating
thecausalimpactofarecommendersystem.Wenowsuggestguidelinestoensureproper
useofthecriterionanddiscussotherapplicationsforwhichitmightbeused.
6.1.
Guidelinesforusingthecriterion.
Aswithanynon-experimentalmethodfor
causalinference,thesplit-doorcriterionrestsonvariousuntestableassumptionsand
requiresmakingcertainmodelingchoices.Weencourageresearcherstoreasoncarefully
abouttheseassumptions,exploresensitivitytomodelingchoices,andexaminethreats
tothevalidityoftheirresults.
6.1.1.
Reasonaboutassumptions.
Thesplit-doorcriterionreliesontwountestable
assumptions:
independence
(of
X
and
Y
D
),and
connectedness
(i.e.non-zerocausal
of
U
Y
on
Y
D
).Theindependenceassumptionisastandardassumptionforobservational
causalinference.Barringcoincidentalequalityofparameterssuchthattheofunob-
servedconfounderson
X
and
Y
D
cancelout,theindependenceassumptionislikelytobe
Nonethelessweencourageresearcherstothinkcarefullyaboutthisassumption
inapplyingthecriterioninotherdomains.Dependingontheapplicationitmaybepos-
sibletoruleoutsuchcancellations.Forexample,inourrecommendationsystemstudy
weexpectdemandforthefocalandrecommendedproducttobecorrelated.Therefore,
thecausaltofdemandonbothproductsisexpectedtobedirectionallyidentical,
andhencecancellationbecomesimpossible.
Theconnectednessassumptionispotentiallymorerestrictive.Ingeneral,itisplau-
siblewhenevermeasurements
Y
R
and
Y
D
areadditivecomponentsofthesametangible
outcome
Y
thatcanbereachedbysimilarmeans.Thatsaid,connectednessremainsan
untestableassumptionwhere,onceagain,domainknowledgeshouldbeusedtoassess
itsplausibility.Forinstance,evenwhen
Y
R
and
Y
D
areadditivecomponents,insome
isolatedcases,
U
Y
maynotbeconnectedto
Y
D
atall.Inarecommendersystemthis
canhappenwhencustomerswithpre-existinginterestinaproductsomehowvisitit
onlythroughrecommendationclick-throughsfromotherproducts.Insuchascenario,
thesplit-doorcriterionwouldbeinvalid.Wenote,however,thatthissituationcanarise
onlyinthe(unlikely)eventthatnosuchuserfoundtheproductdirectly.Whenthereis
evenasmallnumberofusersthatvisittheproductdirectly,thesplit-doorcriterionwill
againbevalidand,dependingontheprecisionofthestatisticalindependencecondition,
canbeapplied.
6.1.2.
Exploresensitivitytotestparameters.
Akeyadvantageofthesplit-doorcri-
terionisthatoncethesetwoassumptionsaremet,itreducestheproblemofcausal
identothatofimplementingatestforstatisticalindependence.Atthesame
time,thisrequireschoosingasuitablestatisticaltestanddecidingonanyfreeparameters
thetestmayhave.Forinstance,inthecaseoftherandomizationtestusedhere,thereis
alevel

usedtodeterminewhentoacceptorrejectfocalandrecommended
productpairsasstatisticallyindependent.Anysuchparametersshouldbevariedtocheck
thesensitivityofestimatestothesechoices,asinFigures
7b
and
7c
.
6.1.3.
Examinethreatstovalidity.
Afteridentifyingandestimatingtheofinter-
est,oneshouldexamineboththeinternalandexternalvalidityoftheresultingestimate.
Intermsofinternalvalidity,werecommendconductingasensitivityanalysistoassess
howresultschangewhentheassumptionsrequiredforidenareviolated.Inthe
caseoftherecommendersystemexample,wesimulatedviolationsoftheconnectedness
assumptionbyaddingcorrelatednoiseto
X
and
Y
R
(butnot
Y
D
)andre-ran
thesplit-doormethodtolookatvariationinresults,asshowninFigure
9
.
Finally,afterestablishinginternalvalidity,oneneedstoconsiderhowusefulthere-
sultingestimateisforpracticalapplications.Asremarkedearlieranddemonstratedin
20
SHARMAETAL.
ourrecommendersystemapplication,thesplit-doorcriterioniscapableofcapturingthe
localaveragecausalforalargesampleofthedatasetthatsatisestherequired
independenceassumption(
X
??
Y
D
).Theargumenthasbeenmadethatsuchlocales-
timatesareindeedusefulinthemselves[
Imbens
,
2010
].Thatsaid,thesamplemaynot
berepresentativeoftheentirepopulation,andsoonemustalwaysbecarefultoqualify
anextensionofthesplit-doorestimatetothegeneralpopulation.Naturally,themore
instancesdiscoveredbythemethod,themorelikelytheestimateistobeofgeneraluse.
Additionally,werecommendthatresearchersperformcheckssimilartothoseinFigure
10
tocomparethedistributionofanyavailablecovariatestocheckforrencesbetween
thegeneralpopulationandinstancesthatpassthesplit-doorcriterion.
6.2.
Potentialapplicationsofthesplit-doorcriterion.
Thekeyrequirementofthe
split-doorcriterionisthattheoutcomevariablemustcomprisetwodistinctcomponents:
onethatispotentiallybythecause,andanotherthatisnotdirectlybyit.
Inaddition,weshouldhavetreasontobelievethatthetwooutcomecomponents
sharecommoncauses(i.e.theconnectednessassumptionmustbeandthat
oneofoutcomevariablescanbeshowntobeindependentofthecausevariable(i.e.the
independenceassumptionmustbeThesemightseemlikeoverlyrestrictive
assumptionsthatlimitapplicabilityofthecriterion,butinthissectionwearguethat
thereareinfactmanyinterestingcaseswherethesplit-doorcriterioncanbeemployed.
Aswehavealreadynoted,recommendationsystemssuchasAmazon'sareespecially
well-suitedtotheseconditions,inlargepartbecause
Y
D
hasanaturalinterpretation
of\directoranythatisnotcausedbyaparticularrecommendation.
Likewisethecriterioncanbeeasilyappliedtootheronlinesystemsthatautomatically
loguservisits,suchasinestimatingthecausalofadvertisementsonsearchengines
orwebsites.Somewhatmorebroadly,timeseriesdataingeneralmaybeamenableto
thesplit-doorcriterion,inpartbecausetcomponentsoftheoutcomeoccurring
atthesametimearemorelikelytobecorrelatedthancomponentsthatshareother
characteristics,andinpartbecausetimeseriesnaturallygeneratemanyobservationson
theinputandoutputvariables,whichpermitsconvenienttestingforindependence.
Forexample,considertheproblemofestimatingtheofsocialmediaonnews
consumption.Therehasbeenrecentinterest[
Flaxman,GoelandRao
,
2016
]inhowsocial
mediawebsitessuchasFacebookimpactthenewsthatpeopleread,especiallythrough
algorithmicrecommendationssuchasthosefor\Trendingnews"".Giventimeseriesdata
foruseractivityonasocialmediawebsiteandarticlevisitsfromnewswebsitelogs,we
canusethesplit-doorcriteriontoestimatetheofsocialmediaonnewsreading.
Here
Y
R
wouldcorrespondtothevisitsthatarereferredfromsocialmedia,and
Y
D
would
beallotherdirectvisitstothenewsarticle.Mostwebsitesrecordthesourceofeachpage
visit,soobtainingthesetwocomponentsfortheoutcome|visitstoanarticlethrough
socialmediaandthroughothermeans|shouldbestraightforward.Wheneverpeople's
socialmediausageisnotcorrelatedwithdirectvisitstoanewsarticle,wecanidentify
thecausalofsocialmediaonnewsconsumption.Similaranalysiscanbeapplied
toproblemssuchasestimatingtheofonlinepopularityofpoliticiansoncampaign
ortheoftelevisionadvertisementsonpurchases.
Finally,althoughwehavefocusedononlinesettingsforwhichhighlygranulartime
seriesdataisoftencollectedbydefault,wenotethatthereisnothingintrinsictothe
split-doorcriterionthatpreventsitfrombeingappliedForexample,manyretail-
ersroutinelysenddirectmailadvertisementstoexistingcustomerswhomtheyidentify
throughloyaltyprograms.Thesplit-doorcriterioncouldeasilybeusedtoestimatethe
causaloftheseadvertisementsonproductpurchases:
X
wouldbethenumberof
customersthataresentanadvertisement;
Y
R
wouldbethecustomersamongthemwho
purchasedtheproduct;and
Y
D
wouldbethenumberofcustomerswhoboughttheprod-
uctwithoutreceivingthemailer.Moregenerally,thesplit-doorcriterioncouldbeused
inanycontextwheretheoutcomeofinterestcanbetiatedintomorethanone
channel.
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
21
7.Conclusion.
Inclosingwenotethatthesplit-doorcriterionisjustoneexample
ofamoregeneralclassofmethodsthatadoptadata-drivenapproachtocausaldis-
covery[
Jensenetal.
,
2008
;
Sharma,HofmanandWatts
,
2015
;
Cattaneo,Frandsenand
Titiunik
,
2015
;
Grosse-Wentrupetal.
,
2016
].Aswehavediscussed,data-drivenmethods
haveimportantadvantagesovertraditionalmethodsforexploitingnaturalvariation|
allowinginferencetobeperformedonmuchlargerandmorerepresentativesamples|
whilealsobeinglesssusceptibletounobservedconfoundersthanback-dooriden
strategies.Asthevolumeandvarietyofdatacontinuestogrow,weexpect
thesemethodstoincreaseinpopularityandtoraisenumerousquestionsregardingtheir
theoreticalfoundationsandpracticalapplicability.
APPENDIXA:ESTIMATINGTHEFRACTIONOFERRONEOUSSPLIT-DOOR
INSTANCES
Lettheexpectedfractionoferroneous
X
-
Y
D
pairs|split-doorinstances|returned
bythemethodbe
˚
.Intheterminologyofmultipletesting,
˚
referstothe
FalseNon-
DiscoveryRate(FNDR)
[
Delongchampetal.
,
2004
].Thisistfromthemore
commonlyusedFalseDiscoveryRate(FDR)[
Farcomeni
,
2008
],sincewedeviatefrom
standardhypothesistestingbylookingforsplit-doorinstancesthathaveap-valuehigher
thanapre-determinedthreshold.Given
m
hypothesistestsandalevelof

,
weshowthatthefalsenon-discoveryrate
˚
forthesplit-doorcriterioncanbecharacter-
izedas
˚


(1


)
ˇ
dep
m
W

;
(A.1)
where
ˇ
dep
isthefractionofactuallydependent
X
-
Y
D
instancesinthedatasetand
W

istheobservednumberof
X
-
Y
D
instancesreturnedbythemethodatlevel

.
Theaboveestimatecanbederivedusingtheframeworkproposedby
Storey
[
2002
]
undertwoassumptions.Theisthatthethatthedistributionofp-valuesunderthe
nullhypothesisisuniform,andthesecondisthatthedistributionofp-valuesunderthe
alternativehypothesisisstochasticsmallerthantheuniformdistribution.Letthenumber
ofinvalidinstancesfoundusingthesplit-doorcriterionbe
T
.Then,bythe
falsenon-discoveryratecanbewrittenas:
˚

=E

T
W




W>
0

:
Sincethealternativedistributionisstochasticallysmallerthanuniform,wecanarrive
atanupperboundbyreplacing
T
bytheexpectednumberofsplit-doorinstancesifthe
alternativedistributionwereuniform,(1


)

m
dependent
=(1


)

ˇ
dep

m
,giving
˚


(1


)

ˇ
dep
m
W

:
(A.2)
Here
ˇ
dep
isunknown,soitneedstobeestimated.Acommonapproachistoestimate
thefractionofactuallyindependentinstancesornullhypotheses
ˇ
indep
andthenuse
ˇ
dep
=1

ˇ
indep
[
Delongchampetal.
,
2004
].Forrobustness,wesuggestusingmultiple
procedurestoestimate
ˇ
indep
andverifysensitivityofresultstothechoiceof
ˇ
indep
.In
thispaper,weusetwotestimates,derivedfrom
StoreyandTibshirani
[
2003
],
Storey
[
2002
](
Storey's
estimate);and
Nettletonetal.
[
2006
],
LiangandNettleton
[
2012
]
(
Nettleton's
estimate).
Storey'sestimateisas
^
ˇ
indep
=
W

m
(1


)
;
(A.3)
where

2
[0
;
1)isatunableparameter|similarininterpretationto

|and
W

isthe
numberofhypothesistestshavingap-valuehigherthan

.Thechoiceof

involvesa
22
SHARMAETAL.
bias-variancewith

=0
:
5beingacommonchoice,asintheSAMsoftware
developedby
StoreyandTibshirani
[
2003
].
Nettleton'sestimate,ontheotherhand,choosestheevalueof

adaptively,
basedontheobservedp-valuedistribution.First,thep-valuedistributionissummarized
inahistogramcontaining
B
bins.Then,athreshold

ischosenastheindex(
I
)corre-
spondingtotheleft-mostbinwhosecountfailstoexceedtheaveragecountofthebins
toitsright.Thisresultsinthefollowingestimate,where

=(
I

1)
=B
:
^
ˇ
indep
=
W

m
(1


)
=
W

m
(1

I

1
B
)
:
(A.4)
Applyingeachofthesetothe
m
=114
;
469focalandrecommendedproductpairs
analyzedinSection
5
allowsustoestimatethetruenumberofdependent
X
-
Y
D
pairs
inthedataset,
ˇ
dep
.At

=0
:
95,bothmethodsgiveverysimilarresults(
ˇ
dep;Storey
=
0
:
184,
ˇ
dep;Nettleton
=0
:
187);weuse
ˇ
dep
=0
:
187inouranalysis.
APPENDIXB:SENSITIVITYANALYSISFORTHECONNECTEDNESS
ASSUMPTION
Inthissectionweanalyzethesensitivityofanestimateobtainedusingthesplit-door
criteriontoviolationsoftheconnectednessassumption.AsFigure
3a
shows,violation
impliesthatthereexistvariables
V
Y
thatonly
X
and
Y
R
butnot
Y
D
.Weusethe
structuralequationmodelfromSection
2.2
toillustratesensitivityanalysis.
Giventhattheunobservedconfounderscanbebrokendownintotwocomponents
U
Y
and
V
Y
,wecanrewritethelinearstructuralequationsfromEquation
2.8
as:
x
=
u
x
+

1
u
y
+
c
1
v
y
+

x
(B.1)
y
r
=
ˆx
+

2
u
y
+
c
2
v
y
+

yr
(B.2)
y
d
=

3
u
y
+

yd
;
(B.3)
withtwoadditionalparameters
c
1
and
c
2
denotingtheoftheunobservedvariable
V
Y
on
X
and
Y
R
,respectively.Applyingthesplit-doorcriterion
X
??
Y
D
,wewritethe
followingequationsforeachobtainedsplit-doorinstance:
x
=
u
x
+
c
1
v
y
+

0
x
(B.4)
y
r
=
ˆx
+
c
2
v
y
+

0
yr
(B.5)
Here
V
Y
isunobservedandhencethecausalisnotidenUsing(B.5)asanesti-
matingequationwillleadtoabiasedestimateofthecausalctduetotheconfounding
oftheunobservedcommoncause
V
Y
.Notethatthisstructureisidenticaltothe
omittedvariablebiasprobleminback-doorandconditioning-basedmethods[
Harding
,
2009
].Consequently,weobtainasimilarbilineardependenceofthesplit-doorestimate
tosensitivityparameters
c
1
and
c
2
.
Sp,thesplit-doormethodregresses
Y
R
on
X
toobtainanestimate^
ˆ
for
eachobtainedinstance.Whenconnectednessisviolated,thebiasofthisestimatecanbe
characterizedas,
^
ˆ
=(
X
T
X
)

1
X
T
Y
R
=
P
i
x
i
y
r
i
P
j
x
2
j
=
P
i
x
i
(
ˆx
i
+
c
2
v
y
i
+

0
yr
i
)
P
j
x
2
j
=
ˆ
+
c
2
P
i
v
y
i
x
i
P
j
x
2
j
+
P
i

0
yr
i
x
i
P
j
x
2
j
;
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
23
whereweuse(B.5)toexpand
y
r
i
.AsinSection
3
,let
˝
denotethesamplesizeforeach
split-doorinstance.When
X
and
V
Y
arebothstandardizedtohavezeromeanandunit
variance,andtakingexpectationonbothsides,weobtain,
E[^
ˆ
]=
ˆ
+
c
2
E[
1
˝
X
i
v
y
i
x
i
]+E[
1
˝
X
i

0
yr
i
x
i
]
=
ˆ
+
c
2
E[
1
˝
X
i
v
y
i
(
u
x
i
+
c
1
v
y
i
+

0
x
i
)]
=
ˆ
+
c
1
c
2
E[
1
˝
X
i
v
y
i
v
y
i
]+

E[
1
˝
X
i
v
y
i
u
x
i
]+E[
1
˝
X
i
v
y
i

0
x
i
]
E[^
ˆ
]=
ˆ
+
c
1
c
2
(B.6)
whereweusetheindependenceoferrortermsandthat
U
X
??
V
Y
.
Inaddition,notethatthesplit-doormethodaveragestheestimate^
ˆ
obtainedfrom
eachinstance.Notallinstancesmayviolatetheconnectednessassumption,therefore
weintroduceanadditionalsensitivityparameter

thatdenotesthefractionofinvalid
split-doorinstances.Biasinthesplit-doorestimateisthengivenbythefollowing
equationinthethreesensitivityparameters:
E[^
ˆ
]=
ˆ
+

1
c
2
:
(B.7)
Forexpositionalclarity,theaboveanalysisassumedalinearstructuralmodeland
demonstratedsimilaritieswithsensitivityofconditioning-basedmethodstounobserved
commoncauses.However,inpractice,thestructuralmodelmaynotbelinear.Inthe
recommendationexamplediscussedinSection
5
,wedonotassumealinearmodeland
insteaduseanaggregateratioestimator.AsshowninFigure
9
,simulationsshowthat
sensitivityofthisestimatorfollowsasimilarbilineardependenceon
c
1
and
c
2
.
APPENDIXC:CHARACTERIZINGERRORINTHESPLIT-DOORESTIMATE
FORARECOMMENDATIONSYSTEM
InSection
5.3
,thesplit-doorcausalestimateisasthemeanofCTResti-
matesoveralltimeperiodsandfocalproductswithvalidsplit-doorinstances.Herewe
characterizetheerrorinthisestimate.Thekeyideaisthattheerrorcomesfromtwo
components:theduetosomeerroneouslyidentisplit-doorinstances,andthe
secondduetonaturalvarianceinestimatingthemean.Foralevel

ofthe
independencetest,let
W
bethenumberofobtainedsplit-doorinstancesand
N
bethe
numberofaggregatedCTRestimates^
ˆ
i˝
computedfromtheseinstances.Thenthemean
estimatecanbewrittenas:
^
ˆ
=
P
i˝
^
ˆ
i˝
N
;
(C.1)
where
i
referstoafocalproductand
˝
referstoasplit-doortimeperiod.AsinAp-
pendix
A
,let
˚
denotetheexpectedfractionoferroneoussplit-doorinstancesobtained.
Thatis,foranexpectednumberof
˚W
instances,themethodmayhaveerroneously
concludedthatthefocalandrecommendedproductsareindependent.Correspondingly,
anexpected
˚W
=
˚
0
N
numberof^
ˆ
i˝
estimateswillbeinvalid.
5
Theseinvalidestimates
canbeexpandedas:
^
ˆ
i˝
=
ˆ
causal
i˝
+

i˝
;
(C.2)
5
Ingeneral,theexpectednumberofinvalid^
ˆ
i˝
estimatesmaybelessthanorequalto
˚W
,sincea
focalproductmayhavemorethanonerecommendedproductthatcorrespondstoaninvalidsplit-door
instance.
24
SHARMAETAL.
where

referstotheclick-throughrateduetocorrelateddemandbetweenthefocaland
recommendedproducts.Thus,theoverallmeanestimatecanbewrittenas:
^
ˆ
=
P
i˝
2
A
ˆ
causal
i˝
+
P
i˝
2
B
(
ˆ
causal
i˝
+

i˝
)
N
=
P
i˝
ˆ
causal
i˝
N
+
P
i˝
2
B

i˝
N
;
where
A
and
B
referto(
i;˝
)pairswithvalidanderroneoussplit-doorestimatesrespec-
tively(
j
A
j
=(1

˚
0
)
N;
j
B
j
=
˚
0
N
).
Comparingthistothetrue
ˆ
causal
,weobtain
ˆ
causal

^
ˆ
=(
ˆ
causal


ˆ
causal
)

P
i˝
2
B

i˝
N
:
(C.3)
ThetermoftheRHScorrespondstoerrorduetosamplingvariance,andthesecond
termcorrespondstoerrorduetocorrelateddemand(
˚
).Weestimatethesetermsbelow.
Errordueto
˚
.
Basedontheargumentforjustifyingtheindependenceassumption
inSection
5.1
,letusassumethatthetotalof
U
Y
on
Y
R
ispositive(without
stipulatingitforeachindividualinstance).Thismeansthatthetermduetocorrelated
demandispositive,
P
i˝
2
B

i˝

0.Further,themaximumvalueof

i˝
isattainedwhen
alltheobservedclick-throughsareduetocorrelateddemand(

i˝
=^
ˆ
i˝
).Underthis
assumption,
0

P
i˝
2
B

i˝
N

ˆ
maxsum
N
;
where
ˆ
maxsum
correspondstothemaximumsumofanysubsetof
˚
0
N
^
ˆ
i˝
values.An
approximateestimatecanbederivedusing^
ˆ
|theempiricalmeanoverall
N
valuesof
ˆ
i˝
|leadingto
ˆ
maxsum
ˇ
˚
0
N
^
ˆ
.
Errorduetonaturalvariance.
Wecharacterizethiserrorbythe99%interval
forthemeanestimate,givenby2
:
58

^
˙
p
N
,where^
˙
istheempiricalstandarddeviation.
Combiningthesetwo,theresultantintervalforthesplit-doorestimateis
(^
ˆ

ˆ
maxsum
N

2
:
58
^
˙
p
N
;
^
ˆ
+2
:
58
^
˙
p
N
)
:
(C.4)
Theaboveintervaldemonstratesthebias-varianceinchoosinganominal
levelfortheindependencetestandthecorresponding
˚
.Athighnominal
level

,biasdueto
˚
isexpectedtobelowbutvarianceoftheestimatemay
behighduetolow
N
.Conversely,atlowvaluesof

,variancewillbelowerbut
˚
is
expectedtobehigherbecauseweacceptmanymoresplit-doorinstances.
SUPPLEMENTARYMATERIAL
SupplementA:Codeforsplit-doorcriterion
(
http://www.github.com/amit-sharma/splitdoor-causal-criterion
).WeprovideanRpack-
agethatimplementsthesplit-doorcriterion,alongwithcodesamplesforapplyingthe
criteriontonewapplications.
REFERENCES
Agresti,A.
(1992).Asurveyofexactinferenceforcontingencytables.
StatisticalScience
7
131{153.
Agresti,A.
(2001).Exactinferenceforcategoricaldata:recentadvancesandcontinuingcontroversies.
StatisticsinMedicine
20
2709{2722.
Angrist,J.D.
,
Imbens,G.W.
and
Rubin,D.B.
(1996).Idenofcausalusinginstru-
mentalvariables.
JournaloftheAmericanStatisticalAssociation
91
444{455.
SPLIT-DOORCRITERIONFORCAUSALIDENTIFICATION
25
Carmi,E.
,
Oestreicher-Singer,G.
and
Sundararajan,A.
(2012).IsOprahcontagious?Identifying
demandspilloversinonlinenetworks.
NETInstituteWorkingPaper
10-18
.
Carnegie,N.B.
,
Harada,M.
and
Hill,J.L.
(2016).AssessingSensitivitytoUnmeasuredConfounding
UsingaSimulatedPotentialConfounder.
JournalofResearchonEducationalctiveness
9
395-420.
Cattaneo,M.D.
,
Frandsen,B.R.
and
Titiunik,R.
(2015).Randomizationinferenceintheregression
discontinuitydesign:AnapplicationtopartyadvantagesintheUSSenate.
JournalofCausalInference
3
1{24.
Cattaneo,M.D.
,
Titiunik,R.
and
Vazquez-Bare,G.
ComparinginferenceapproachesforRDde-
signs:Areexaminationoftheofheadstartonchildmortality.
JournalofPolicyAnalysisand
Management
36
643-681.
deSiqueiraSantos,S.
,
Takahashi,D.Y.
,
Nakata,A.
and
Fujita,A.
(2014).Acomparativestudy
ofstatisticalmethodsusedtoidentifydependenciesbetweengeneexpressionsignals.
in
Bioinformatics
15
906-918.
Delongchamp,R.R.
,
Bowyer,J.F.
,
Chen,J.J.
and
Kodell,R.L.
(2004).Multiple-testingstrategy
foranalyzingcDNAarraydataongeneexpression.
Biometrics
60
774{782.
Dunning,T.
(2012).
Naturalexperimentsinthesocialsciences:Adesign-basedapproach
.Cambridge
UniversityPress.
Farcomeni,A.
(2008).Areviewofmodernmultiplehypothesistesting,withparticularattentiontothe
falsediscoveryproportion.
StatisticalMethodsinMedicalResearch
17
347{388.
Fiske,S.T.
and
Hauser,R.M.
(2014).Protectinghumanresearchparticipantsintheageofbigdata.
ProceedingsoftheNationalAcademyofSciences
111
13675-13676.
Flaxman,S.
,
Goel,S.
and
Rao,J.M.
(2016).Filterbubbles,echochambers,andonlinenewscon-
sumption.
PublicOpinionQuarterly
80
298{320.
Grau,J.
(2009).Personalizedproductrecommendations:Predictingshoppers'needs.
eMarketer
.
Grosse-Wentrup,M.
,
Janzing,D.
,
Siegel,M.
and
Sch

olkopf,B.
(2016).Idenofcausalrela-
tionsinneuroimagingdatawithlatentconfounders:Aninstrumentalvariableapproach.
NeuroImage
125
825{833.
Harding,D.J.
(2009).Collateralconsequencesofviolenceindisadvantagedneighborhoods.
Social
Forces
88
757{784.
Imbens,G.W.
(2010).BetterLATEthannothing.
JournalofEconomicLiterature
48
.
Imbens,G.W.
and
Rubin,D.B.
(2015).
Causalinferenceinstatistics,social,andbiomedicalsciences
.
CambridgeUniversityPress.
Jensen,D.D.
,
Fast,A.S.
,
Taylor,B.J.
and
Maier,M.E.
(2008).Automaticidenofquasi-
experimentaldesignsfordiscoveringcausalknowledge.In
Proceedingsofthe14thACMInternational
ConferenceonKnowledgeDiscoveryandDataMining
372{380.
Kang,H.
,
Zhang,A.
,
Cai,T.T.
and
Small,D.S.
(2016).Instrumentalvariablesestimationwith
someinvalidinstrumentsanditsapplicationtoMendelianrandomization.
JournaloftheAmerican
StatisticalAssociation
111
132{144.
Lewis,R.A.
,
Rao,J.M.
and
Reiley,D.H.
(2011).Here,there,andeverywhere:Correlatedonlinebe-
haviorscanleadtooverestimatesoftheofadvertising.In
Proceedingsofthe20thInternational
ConferenceonWorldWideWeb
157{166.ACM.
Liang,K.
and
Nettleton,D.
(2012).Adaptiveanddynamicadaptiveproceduresforfalsediscoveryrate
controlandestimation.
JournaloftheRoyalStatisticalSociety.SeriesB(StatisticalMethodology)
74
163-182.
Lydersen,S.
,
Pradhan,V.
,
Senchaudhuri,P.
and
Laake,P.
(2007).Choiceoftestforassociationin
smallsampleunorderedr

ctables.
StatisticsinMedicine
26
4328{4343.
Mealli,F.
and
Pacini,B.
(2013).Usingsecondaryoutcomestosharpeninferenceinrandomizedex-
perimentswithnoncompliance.
JournaloftheAmericanStatisticalAssociation
108
1120{1131.
Morgan,S.L.
and
Winship,C.
(2014).
Counterfactualsandcausalinference
.CambridgeUniversity
Press.
Mulpuru,S.
(2006).Whatyouneedtoknowaboutthird-partyrecommendationengines.
Forrester
Research
.
Nettleton,D.
,
Hwang,J.T.G.
,
Caldo,R.A.
and
Wise,R.P.
(2006).Estimatingthenumberoftrue
nullhypothesesfromahistogramofpvalues.
JournalofAgricultural,Biological,andEnvironmental
Statistics
11
337.
Paninski,L.
(2003).Estimationofentropyandmutualinformation.
NeuralComputation
15
1191{1253.
Pearl,J.
(2009).
Causality
.CambridgeUniversityPress.
Pethel,S.D.
and
Hahs,D.W.
(2014).Exacttestofindependenceusingmutualinformation.
Entropy
16
2839{2849.
Phan,T.Q.
and
Airoldi,E.M.
(2015).Anaturalexperimentofsocialnetworkformationanddynamics.
ProceedingsoftheNationalAcademyofSciences
112
6595{6600.
Ricci,F.
,
Rokach,L.
and
Shapira,B.
(2011).
Introductiontorecommendersystemshandbook
.
Springer.
Rosenbaum,P.R.
(2010).
Designofobservationalstudies
.Springer.
Rosenzweig,M.R.
and
Wolpin,K.I.
(2000).Natural\naturalexperiments""ineconomics.
Journalof
EconomicLiterature
38
827{874.
Rubin,D.B.
(2006).
Matchedsamplingforcausalcts
.CambridgeUniversityPress.
Sharma,A.
,
Hofman,J.M.
and
Watts,D.J.
(2015).Estimatingthecausalimpactofrecommendation
systemsfromobservationaldata.In
Proceedingsofthe16thACMConferenceonEconomicsand
26
SHARMAETAL.
Computation
453{470.
Spirtes,P.
,
Glymour,C.N.
and
Scheines,R.
(2000).
Causation,prediction,andsearch
.MITPress.
Steuer,R.
,
Kurths,J.
,
Daub,C.O.
,
Weise,J.
and
Selbig,J.
(2002).Themutualinformation:
Detectingandevaluatingdependenciesbetweenvariables.
Bioinformatics
18
S231{S240.
Storey,J.D.
(2002).Adirectapproachtofalsediscoveryrates.
JournaloftheRoyalStatisticalSociety:
SeriesB(StatisticalMethodology)
64
479{498.
Storey,J.D.
and
Tibshirani,R.
(2003).
SAMthresholdingandfalsediscoveryratesfordetecting
entialgeneexpressioninDNAmicroarrays
In
TheAnalysisofGeneExpressionData:Methods
andSoftware
272{290.SpringerNewYork,NewYork,NY.
Stuart,E.A.
(2010).Matchingmethodsforcausalinference:Areviewandalookforward.
Statistical
Science:areviewjournaloftheInstituteofMathematicalStatistics
25
1.
Sz

ekely,G.J.
,
Rizzo,M.L.
,
Bakirov,N.K.
etal.(2007).Measuringandtestingdependenceby
correlationofdistances.
TheAnnalsofStatistics
35
2769{2794.
VanderWeele,T.J.
and
Arah,O.A.
(2011).Biasformulasforsensitivityanalysisofunmeasured
confoundingforgeneraloutcomes,treatments,andconfounders.
Epidemiology(Cambridge,Mass.)
22
42{52.
9LavelleRoad
Bangalore,India560008
E-mail:
amshar@microsoft.com
641Ave.oftheAmericas
NewYork,NYUSA10011
E-mail:
jmh@microsoft.com
duncan@microsoft.com
"
97,Translations as Additional Contexts for Sentence Classification,http://arxiv.org/pdf/1806.05516v1.pdf,https://github.com/rktamplayo/MCFA,"TranslationsasAdditionalContextsforSentence
ReinaldKimAmplayo
y
,KyungjaeLee
y
,JinyeongYeo
z
and
Seung-wonHwang
y
y
YonseiUniversity,Seoul,SouthKorea
z
PohangUniversityofScienceandTechnology,Pohang,SouthKorea
f
rktamplayo,lkj0509,seungwonh
g
@yonsei.ac.krjinyeo@postech.edu
Abstract
Insentencetasks,additionalcontexts,
suchastheneighboringsentences,mayimprove
theaccuracyofthe.However,suchcon-
textsare
domain-dependent
andthuscannotbe
usedforanothertaskwithaninappro-
priatedomain.Incontrast,weproposetheuseof
translatedsentencesas
domain-free
contextthatis
alwaysavailableregardlessofthedomain.We
thatnaivefeatureexpansionoftranslationsgains
onlymarginalimprovementsandmaydecreasethe
performanceofthe,duetopossibleinac-
curatetranslationsthusproducingnoisysentence
vectors.Tothisend,wepresentmultiplecontext
attachment(MCFA),aseriesofmodulesat-
tachedtomultiplesentencevectorstothenoise
inthevectorsusingtheothersentencevectorsas
context.Weshowthatourmethodperformscom-
petitivelycomparedtopreviousmodels,achiev-
ingbestperformanceonmultipledata
sets.Wearethetousetranslationsasdomain-
freecontextsforsentence
1Introduction
Oneoftheprimarytasksinnaturallanguageprocessing
(NLP)issentencewheregivenasentence(e.g.
asentenceofareview)asinput,wearetaskedtoclassifyit
intooneofmultipleclasses(e.g.intopositiveornegative).
Thistaskisimportantasitiswidelyusedinalmostallsub-
areasofNLPsuchassentimentforsentiment
analysis
[
PangandLee,2007
]
andquestiontype
forquestionanswering
[
LiandRoth,2002
]
,tonameafew.
Whilepastmethodsrequirefeatureengineering,recentmeth-
odsenjoyneural-basedmethodstoautomaticallyencodethe
sentencesintolow-dimensionaldensevectors
[
Kim,2014;
Joulin
etal.
,2017
]
.Despitethesuccessofthesemethods,the
majorchallengeinthistaskisthatextractingfeaturesfroma
singlesentencelimitstheperformance.
Toovercomethislimitation,recentworksattemptedto
augmentdifferentkindsoffeaturestothesentence,suchas
theneighboringsentences
[
Lin
etal.
,2015
]
andthetopics
ofthesentences
[
Zhao
etal.
,2017
]
.However,thesemeth-
odsused
domain-dependent
contextsthatareonlyeffective
Figure1:PCAvisualizationsofunalteredsentencevectorsonTREC
dataset,whereeachlanguageiseffectiveforaclass,high-
lightedusingayellowcircle.
whenthedomainofthetaskisappropriate.Foronething,
neighboringsentencesmaynotbeavailableinsometasks
suchasquestiontypeMoreover,topicsinferred
usingtopicmodelsmayproducelessusefultopicswhenthe
datasetissuchasmoviereviewsentiment

[
Mimno
etal.
,2011
]
.
Inthispaper,weproposetheusageoftranslationsascom-
pellingandeffective
domain-free
contexts,orcontextsthat
arealwaysavailablenomatterwhatthetaskdomainis.We
observetwoopportunitieswhenusingtranslations.
First,eachlanguagehasitsownlinguisticandcultural
characteristicsthatmaycontaindifferentsignalstoeffec-
tivelyclassifyaclass.Figure1contraststhesentence
vectorsoftheoriginalEnglishsentencesandtheirArabic-
translatedsentencesinthequestiontypetask.A
yellowcircleaclearseparationofaclass.Forexam-
ple,thegreenclass,orthenumericquestiontype,iscircledin
theArabicspaceasitisclearlyseparatedfromotherclasses,
whilesuchseparationcannotbeobservedinEnglish.Mean-
while,locationtypequestions(inorange)arebetter
inEnglish.
Second,theoriginalsentencesmayincludelanguage-
ambiguity,whichmayberesolvedwhenpresented
withitstranslations.ConsidertheexampleEnglishsentence
ﬁ
Themovieisterriblyamazing
ﬂforthesentiment
tiontask.Inthiscase,
terribly
canbeusedinbothpositive
andnegativesense,thusintroducesambiguityinthesentence.
WhentranslatedtoKorean,itbecomesﬁ
%
ò

o
“

H
@/
é
ß
Œ
y˙
¿

Ë
Ò
6
x
½
+
Ë
m˙
ﬂwhichmeansﬁ
Themovieisgreatlyma
ﬂ,
removingtheambiguity.
Theabovetwoobservationsholdonlywhentranslations
aresupportedfor(nearly)arbitrarylanguagepairswithsuf-
highquality.Thankfully,translationservices(e.g.
arXiv:1806.05516v1  [cs.CL]  14 Jun 2018Figure2:PCAvisualizationsofunalteredsentencevectors(left)and
thecorrespondingMCFA-alteredvectors(right)ontheMRdataset.
d
istheMahalanobisdistancebetweenthetwoclassclusters.
GoogleTranslate)Moreover,recentresearchonneuralma-
chinetranslation(NMT)
[
Bahdanau
etal.
,2014
]
improved
theefyandevenenabledzero-shottranslation
[
John-
son
etal.
,2016
]
ofmodelsforlanguageswithnoparallel
data.Thisprovidesanopportunitytoleverageonasmany
languagesaspossibletoanydomain,providingamuchwider
contextcomparedtothelimitedcontextsprovidedbypast
studies.
However,despitethematurityoftranslation,naivelycon-
catenatingtheirvectorstotheoriginalsentencevectormay
introducemorenoisethansignals.Theunalteredtranslation
spaceontheleftofFigure2showsanexamplewheretrans-
lationnoisesmakethetwoclassesindistinguishable.
Inthispaper,weproposeamethodtomitigatethepossible
problemswhenusingtranslatedsentencesascontextbasedon
thefollowingobservations.Supposetherearetwotranslated
sentences
a
and
b
withslighterrors.Wepositthat
a
canbe
usedto
b
when
a
isusedasacontextof
b
,andviceversa
1
.
Revisitingtheexampleabove,tothevectoroftheEnglish
sentenceﬁ
Themovieisterriblyamazing
ﬂ,weusetheKorean
translationtomovethevectortowardsthelocationwherethe
vectorﬁ
Themovieisgreatlyma
ﬂis.
Basedontheseobservations,wepresentaneuralattention-
basedmultiplecontextattachment(MCFA).MCFAisa
seriesofmodulesthatusesallthesentencevectors(e.g.Ara-
bic,English,Korean,etc.)ascontexttoasentencevector
(e.g.Korean).Fixingthevectorsisdonebyselectivelymov-
ingthevectorstoalocationinthesamevectorspacethat
betterseparatestheclass,asshowninFigure2.Noisesfrom
translationmaycauseadverseeffectstothevector
itself
(e.g.
whenanoisyvectorisdirectlyusedforthetask)and
rela-
tively
toothervectors(e.g.whenanoisyvectorisusedto
anothernoisyvector).MCFAcomputestwosentenceus-
abilitymetricstocontrolthenoisewhenvectors:(a)
selfusability
ˆ
i
(
a
)
weighstheofusingsentence
a
insolvingthetask.(b)
relativeusability
ˆ
r
(
a;b
)
weighs
theofusingsentence
a
insentence
b
.
ListedbelowarethethreemainstrengthsoftheMCFA
attachment.(1)MCFAisattachedafterencodingthesen-
tence,whichmakesitwidelyadaptabletoothermodels.(2)
MCFAisextensibleandimprovestheaccuracyasthenum-
beroftranslatedsentencesincreases.(3)MCFAmovesthe
vectorsinsidethesamespace,thuspreservesthemeaningof
vectordimensions.Resultsshowthataconvolutionalneural
network(CNN)attachedwithMCFAyimproves
theperformanceofCNN,achievingstateofthe
1
Hereon,wemeantoastoﬁcorrect,repair,oralter.ﬂ
artperformanceovermultipledatasets.
2Preliminaries
2.1Problem:TranslatedSentencesasContext
Inthispaper,theultimatetaskthatwesolveisthesentence
taskwheregivenasentenceandalistofclasses,
oneistasktoclassifywhichclass(e.g.positiveornegative
sentiment)amongthelistofclassesdoesthesentencebelong.
However,themainchallengethatwetackleisthetaskonhow
toutilizetranslatedsentencesasadditionalcontextinorder
toimprovetheperformanceofthe.,the
problemstates:giventheoriginalsentence
s
,thegoalisto
use
t
1
;t
2
;:::;t
n
,orsentencesinotherlanguageswhichare
translatedfrom
s
,asadditionalcontext.
BaseModel:ConvolutionalNeuralNetwork.
Thebase
modelusedistheconvolutionalneuralnetwork(CNN)for
sentences
[
Kim,2014
]
.Itisasimplevariationoftheorigi-
nalCNNfortexts
[
Collobert
etal.
,2011
]
tobeusedonsen-
tences.Let
x
i
2
R
d
bethe
d
-dimensionalwordvectorof
the
i
-thwordinasentenceoflength
n
.Aconvolutionoper-
ationinvolvesapplyingamatrix
W
2
R
h

d
toawin-
dowof
h
wordsandproducinganewfeaturevector
c
i
using
theequation
c
i
=
f
([
x
i
;
:::
;
x
i
+
h

1
]
>
W
+
b
)
,where
b
isa
biasvectorand
f
(
:
)
isanon-linearfunction.Bydoingthis
onallpossiblewindowsofwordsweproduceafeaturemap
c
=[
c
1
;c
2
;:::
]
.Wethenapplyamax-over-timepoolingop-
eration
[
Collobert
etal.
,2011
]
overthefeaturemapandtake
themaximumvalueasthefeaturevectorofthe.We
dothisonallfeaturevectorsandconcatenateallthefeature
vectorstoobtainthefeaturevector
v
.Wecanthenuse
thisvectorasinputfeaturestotrainasuchaslo-
gisticregression.WeuseCNNtocreatesentencevectorsfor
allsentences
s;t
1
;t
2
;:::;t
n
.Fromhereon,werefertothese
vectorsas
v
s
;
v
t
1
;
v
t
2
;:::;
v
t
n
,respectively.Werefertothem
collectivelyas
V
.
Baseline1:NaiveConcatenation.
Asimplemethodinor-
dertousethetranslatedsentencesasadditionalcontextis
tonaivelyconcatenatetheirvectorswiththevectorofthe
originalsentence.Thatis,wecreateawidevector
^v
=
[
v
s
;
v
t
1
;
:::
;
v
t
n
]
,andusethisastheinputfeaturevectorofthe
sentencetothe.Thismethodworksifthetrans-
latedsentencesaretranslatedproperly.However,sentences
translatedusingmachinetranslationmodelsusuallycontain
incorrecttranslation.Ineffect,thismethodwillhaveadverse
effectsontheoverallperformanceofthe.Thiswill
especiallybeveryevidentifthenumberofadditionalsen-
tencesincreases.
Baseline2:L2Regularization.
Inordertoalleviatethe
problemsabove,wecanuseL2regularizationtoautomat-
icallyselectusefulfeaturesbyweakeningtheappropriate
weights.Themainproblemofthismethodoccurswhenal-
mostalloftheweightscomingfromthevectorsofthetrans-
latedsentenceareweakened.Thisleadstomakingtheaddi-
tionalcontextvectorsuselessandtohavingasimilarperfor-
mancewhentherearenoadditionalcontext.Ultimately,this
methoddoesnotmakeuseofthefullpotentialoftheaddi-
tionalcontext.
(a)Selfandrelativeusabilitymodules
(b)Vectormodule
Figure3:FullarchitectureoftheMCFAattachment.Anarrowmarkedwithavariableisamatrixmultiplicationofthevectorandthevariable.
Anarrowwithoutavariablesimplycarriesthepreviouselementtothenextelement.
3Model
Tosolvetheproblemsofthebaselinesdiscussedabove,we
introduceanattention-basedneuralmultiplecontextat-
tachment(MCFA)
2
,aseriesofmodulesattachedtothesen-
tencevectors
V
.MCFAattachmentisusedtothesentence
vectors,byslightlymodifyingtheper-dimensionvaluesof
thevector,beforeconcatenatingthemintothefeature
vector.Thesentencevectorsarealteredusingothersentence
vectorsascontext(e.g.
v
t
1
isalteredusing
v
s
;
v
t
2
;:::;
v
t
n
).
Thisresultstomovingthevectorsinthesamevectorspace.
ThefullarchitectureisshowninFigure3.
3.1SelfUsabilityModule
Toasourcesentencevector
3
,weusetheothersentence
vectorsasguidetoknowwhichdimensionstoandto
whatextentdoweneedtothem.However,othervectors
mightalsocontainerrorswhichmaytotheofthe
sourcesentencevector.Inordertocopewiththis,weintro-
duceselfusabilitymodules.Aselfusabilitymodulecontains
the
selfusability
ofthevector
ˆ
i
(
a
)
,whichmeasureshow
sentence
a
isforthetaskathand.Forexample,an
ambiguoussentence(e.g.ﬁ
Themovieisterriblyamazing
ﬂ)
mayreceivealowselfusability,whileaclearand
sentence(e.g.ﬁ
Themovieisverygood
ﬂ)mayreceiveahigh
selfusability.
Mathematically,wecalculatetheselfusabilityofthevec-
tor
v
i
ofsentence
i
,denotedas
ˆ
i
(
v
i
)
,usingtheequation
ˆ
i
(
v
i
)=
˙
(
v
>
i
T
i
)
,where
T
i
2
R
d

1
isamatrixtobe
learned.Theproducedvalueisasinglerealnumberfrom
0to1.Wepre-calculatetheselfusabilityofallsentencevec-
tors
v
i
2
V
.Theseareusedinthenextmodule,therelative
usabilitymodule.
3.2RelativeUsabilityModule
Relativeusability
ˆ
r
(
a;b
)
measureshowuseful
a
canbe
when
b
,relativetoothersentences.Therearetwomain
differencesbetween
ˆ
i
(
a
)
and
ˆ
r
(
a;b
)
.First,
ˆ
i
(
a
)
iscal-
culatedbefore
a
knowsabout
b
while
ˆ
r
(
a;b
)
iscalculated
2
Thecodeweuseinthispaperispubliclyshared:https:
//github.com/rktamplayo/MCFA
3
Hereon,wesaythat
v
k
isa
sourcesentencevector
if
v
k
isthecurrentvectortobealtered.
when
a
knowsabout
b
.Second,
ˆ
r
(
a;b
)
canbeloweven
though
ˆ
i
(
a
)
isnot.Thismeansthat
a
isnotabletohelpin
thewronginformationin
b
.Here,weextendtheaddi-
tiveattentionmodule
[
Bahdanau
etal.
,2014
]
anduseitasa
methodtocalculatetherelativeusabilityoftwosentencesof
differentlanguages.Tobettervisualizetheoriginalattention
mechanism,wepresenttheequationsbelow.
e
i
=
u
>
tanh
(
s
>
W
+
t
>
i
U
)
(1)

i
=
exp
(
e
i
)
P
j
2
T
exp
(
e
j
)
(2)
Onemajorchallengeinusingtheattentionmechanisminour
problemisthatthesentencevectorsdonotbelongtothesame
vectorspace.Moreover,onecharacteristicofourproblemis
thatthesentencevectorscanbebothasourceandacontext
vector(e.g.
v
s
canbeboth
s
and
t
i
inEquation1).Because
ofthese,wecannotdirectlyusetheadditiveattentionmodule.
Weextendthemodulesuchthat(1)eachsentencevector
v
k
hasitsownprojectionmatrix
X
k
2
R
d

d
,and(2)eachpro-
jectionmatrix
X
k
canbeusedasprojectionmatrixofboth
thesource(e.g.whensentence
k
isthecurrentsource)and
thecontextvectors.Finally,weincorporatetheselfusability
function
ˆ
i
(
v
k
)
totheselfusabilityofasentence.Ul-
timately,therelativeusabilitydenotedas
ˆ
r
(
v
i
;
v
j
)
iscalcu-
latedusingtheequationsbelow,where

isthemultiplication
ofavectorandascalarthroughbroadcasting.
e
(
v
i
;
v
j
)=
x
>
tanh
(
v
>
i
X
i
+
v
>
j
X
j

ˆ
i
(
v
j
))
(3)
ˆ
r
(
v
i
;
v
j
)=
exp
(
e
(
v
i
;
v
j
))
P
v
k
2
V
exp
(
e
(
v
i
;
v
k
))
(4)
3.3VectorFixingModule
Thevectormoduleappliestheattentionweightstothe
sentencevectorsandcreatesanintegratedcontextvector.We
thenusethisvectoralongsidewiththesourcesentencevector
tocreateaweightedgatevector.Theweightedgatevectoris
usedtodeterminetowhatextentshouldadimensionofthe
sourcesentencevectorbealtered.
Thecommonwaytoapplytheattentionweightstothe
contextvectorsandcreateanintegratedcontextvector
c
i
istodirectlydoweightedsumofallthecontextvectors.
However,thisisnotpossiblebecausethecontextvectors
arenotonthesamespace.Thus,weuseaprojectionma-
trix
U
k
2
R
d

d
tolinearlyprojectthesentencevector
Dataset
c
j
w
j
M
Test
MR
2
20
10662
CV
SUBJ
2
19
10000
CV
CR
2
23
3775
CV
TREC
6
10
5952
500
Table1:Statisticsofthefourdatasetsusedinthispaper.
c
:number
oftargetclasses.
j
w
j
:averagenumberofwords.
M
:numberof
datainstances.
Test
:sizeofthetestdata,ifavailable.Ifnot,weuse
10-foldcrossvalidation(markedasCV)withrandomsplit.
v
k
totransformthesentencevectorsintoacommonvector
space.Theintegratedcontextvector
c
i
isthencalculatedas
c
i
=
P
v
k
2
V
ˆ
r
(
v
i
;
v
k
)
v
>
k
U
k
.
Finally,weconstructaweightedgatevector
w
k
anduse
ittothesourcesentencevectorsusingtheequationsbe-
low,where
V
k
2
R
2
d

d
isatrainableparameterand

isthe
element-wisemultiplicationprocedure.Theweightedgate
vectorisavectorofrealnumbersbetween0and1tomodify
theintensityofper-dimensionvaluesofthesentencevector.
Thiscausesthevectortomoveinthesamevectorspaceto-
wardsthecorrectdirection.
w
k
=
˙
([
v
k
;
c
k
]
>
V
k
)
(5)
^
v
k
=
v
k

w
k
(6)
Analternativeapproachtodovectorcorrectionisusinga
residual-stylecorrection,whereinsteadofmultiplyingagate
vector,aresidualvector
[
He
etal.
,2016
]
isaddedtotheorig-
inalvector.However,thisapproachmakesthecorrectionnot
interpretable;itishardtoexplainwhatdoesaddingavalueto
adimensionmean.OnemajoradvantageofMCFA
isthatthecorrectionsinthevectorsareinterpretable;the
weightsinthegatevectorcorrespondtotheimportanceof
theper-dimensionfeaturesofthevector.Thealteredvectors
^
v
s
;:::;
^
v
t
n
arethenconcatenatedandfeddirectlyasaninput
vectortothelogisticregressionfortraining.
4Experiments
4.1ExperimentalSetting
Wetestourmodelonfourdifferentdatasetsaslistedbelow
andsummarizedinTable1.(a)
MR
4
[
PangandLee,2005
]
:
Moviereviewsdatawherethetaskistoclassifywhetherthe
reviewsentencehaspositiveornegativesentiment.(b)
SUBJ
[
PangandLee,2004
]
:Subjectivitydatawherethetaskisto
classifywhetherthesentenceissubjectiveorobjective.(c)
CR
5
[
HuandLiu,2004
]
:CustomerreviewswhereThetaskis
toclassifywhetherthereviewsentenceispositiveornegative.
(d)
TREC
6
[
LiandRoth,2002
]
:TRECquestiondatasetthe
taskistoclassifythetypeofquestion.
AllourdatasetsareinEnglish.Fortheadditionalcontexts,
weusetenotherlanguages,selectedbasedontheirdiversity
andtheirperformanceonpriorexperiments:Arabic,Finnish,
French,Italian,Korean,Mongolian,Norwegian,Polish,Rus-
sian,andUkranian.WetranslatethedatasetsusingGoogle
4
https://www.cs.cornell.edu/people/pabo/
movie-review-data/
5
http://www.cs.uic.edu/
˘
liub/FBS/sentiment-analysis.
html
6
http://cogcomp.cs.illinois.edu/Data/QA/QC/
Translate.Tokenizationisdoneusingthepolyglotlibrary
7
.
Weexperimentonusingonlyoneadditionalcontext(
N
=1
)
andusingalltenlanguagesatonce(
N
=10
).For
N
=1
,we
onlyshowtheaccuracyofthebestforconciseness.
ForourCNN,weuselinearunitsandthree
terswithdifferentwindowsizes
h
=3
;
4
;
5
with
100
feature
mapseach,following
[
Kim,2014
]
.Forthesentencevec-
tor,weconcatenatethefeaturemapstogeta300-dimension
vector.Weusedropout
[
Srivastava
etal.
,2014
]
onallnon-
linearconnectionswithadropoutrateof0.5.Wealsousean
l
2
constraintof3,following
[
Kim,2014
]
foraccuratecom-
parisons.WeuseFastTextpre-trainedvectors
8
[
Bojanowski
etal.
,2016
]
forallourdatasetsandtheircorrespondingad-
ditionalcontext.Duringtraining,weusemini-batchsizeof
50.Trainingisdoneviastochasticgradientdescentovershuf-
mini-batcheswiththeAdadeltaupdaterule.Weperform
earlystoppingusingarandom
10%
ofthetrainingsetasthe
developmentset.
Wepresentseveralcompetingmodels,listedbelowtocom-
paretheperformanceofourmodel.(A)Asidefromthebase
model(
CNN
)
[
Kim,2014
]
,weuseDependency-basedCNN
(
Dep-CNN
)
[
Ma
etal.
,2015
]
,whichparsesthesentences
anddoesconvolutiononancestorpathsandDependency-
sensitivityCNN(
DSCNN
)
[
Zhang
etal.
,2016
]
,whichuses
LSTMtocapturedependencyinformationwithineachsen-
tence;(B)
AdaSent
[
Zhao
etal.
,2015
]
adoptsahierarchi-
calstructure,whereconsecutivelevelsareconnectedthrough
gatedrecursivecompositionofadjacentsegments,andfeeds
thehierarchyasamulti-scalerepresentationthroughagat-
ingnetwork;(C)Topic-awareConvolutionalNeuralNetwork
(
TopCNN
)
[
Zhao
etal.
,2017
]
usestopicsasadditionalcon-
textsandchangestheCNNarchitecture.TopCNNusestwo
typesoftopics:wtopicand
topic;and(D)
CNN+B1
and
CNN+B2
arethetwobaselines
presentedinthispaper.
WedonotshowresultsfromRNNmodelsbecausethey
wereshowntobelesseffectiveinsentence
inourpriorexperiments.Formodelswithadditionalcon-
text,wefurtheruseanensemblemodelusing
acommonlyusedmethodbyaveragingtheclassprobability
scoresgeneratedbythemultiplevariants(inourmodel'scase,
N
=1
and
N
=10
models),following
[
Zhao
etal.
,2017
]
.
4.2ResultsandDiscussion
Wereporttheaccuracyofthecompetingmod-
elsinTable2.WeshowthatCNN+MCFAachievesstateof
theartperformanceonthreeofthefourdatasetsandperforms
competitivelyononedataset.When
N
=1
,MCFAincreases
theperformanceofanormalCNNfrom
85
:
0
to
87
:
6
,beating
thecurrentstateoftheartontheCRdataset.When
N
=10
,
MCFAadditionallybeatsthestateoftheartontheTRECdata
set.Finally,ourensembleadditionallyoutperforms
allcompetingmodelsontheMRdataset.Weemphasize
thatweonlyusethebasicCNNasoursentenceencoderfor
ourexperiments,yetstillachievestateoftheartperformance
7
https://pypi.python.org/pypi/polyglot
8
https://github.com/facebookresearch/fastText/blob/
master/pretrained-vectors.md
Model
MR
SUBJ
CR
TREC
CNN
81.5
93.4
85.0
93.6
Dep-CNN
81.9
-
-
95.4
DSCNN
82.2
93.2
-
95.6
AdaSent
83.1
95.5
86.3
92.4
C=Topic
wordsentens
wordsentens
wordsentens
wordsentens
TopCNN
81.7
(+0.2)
81.3
(-0.2)
83.0
(+1.5)
93.4
(+0.0)
93.4
(+0.0)
95.0
(+1.6)
84.9
(-0.1)
84.8
(-0.2)
86.4
(+1.4)
92.5
(-1.1)
92.0
(-1.6)
94.0
(+0.4)
C=Trans
N=1N=10ens
N=1N=10ens
N=1N=10ens
N=1N=10ens
CNN+B1
81.9
(+0.4)
81.4
(-0.1)
82.6
(+1.1)
94.6
(+1.2)
93.8
(+0.4)
94.9
(+1.5)
86.2
(+1.2)
85.9
(+0.9)
86.7
(+1.7)
95.4
(+1.8)
95.0
(+1.4)
96.4
(+3.0)
CNN+B2
82.1
(+0.6)
82.1
(+0.6)
82.2
(+0.7)
94.6
(+1.2)
94.0
(+0.6)
94.8
(+1.4)
86.1
(+1.1)
86.3
(+1.3)
86.6
(+1.6)
95.4
(+1.8)
95.2
(+1.6)
96.4
(+3.0)
CNN+MCFA
82.3
(+0.8)
82.7
(+1.2)
83.2
(+1.7)
94.7
(+1.3)
94.8
(+1.4)
95.2
(+1.8)
87.6
(+2.6)
88.6
(+3.6)
89.4
(+4.4)
95.4
(+1.8)
96.0
(+2.4)
96.8
(+3.4)
Table2:accuraciesofcompetingmodels.
C
referstotheadditionalcontext,
N
referstothenumberoftranslations.InTopCNN,
wordreferstousingwtopicwhilesentencereferstousingctopic.Accuraciescolored
red
areaccuraciesthat
performworsethanCNN.Previousstateoftheartresultsandtheresultsofourbestmodelare
bold-faced
.Thewinningresultisunderlined
.
Thenumberinsidetheparenthesisindicatestheincreasefromthebasemodel,CNN.
Model
MR
SUBJ
CR
TREC
CNN
81.5
93.4
85.0
93.6
CNN+B1
81.4
94.2
83.8
93.0
CNN+B2
81.7
94.2
84.0
93.2
CNN+MCFA
81.8
94.4
85.8
94.2
Table3:AccuraciesoftheworstCNN+translationwhen
N
=1
.AccuracieslessthanCNNaccuraciesarehighlightedin
red
.
onmostdatasets.Hence,MCFAissuccessfulineffectively
usingtranslationsasadditionalcontexttoimprovetheperfor-
manceofthe.
Wecompareourmodel(CNN+MCFA)andthebaselines
discussedabove(CNN+B1,CNN+B2).Onallsettings,our
modeloutperformsthebaselines.When
N
=10
,theper-
formanceofourmodelincreasesovertheperformancewhen
N
=1
,howevertheperformanceofCNN+B1decreases
whencomparedtotheperformancewhen
N
=1
.Wealso
showtheaccuraciesoftheworstwhen
N
=1
inTable3.OnalldatasetsexceptSUBJ,theaccuracyof
CNN+B1decreasesfromthebaseCNNaccuracy,whilethe
accuracyofourmodelalwaysimprovesfromthebaseCNN
accuracy.ThisisresolvedbyCNN+B2byapplyingL2regu-
larization,howevertheincreaseinperformanceismarginal.
Wealsocomparetwodifferentkindsofadditionalcon-
text:topics(TopCNN)andtranslations(CNN+B1,CNN+B2,
CNN+MCFA).Overall,weconcludethattranslationsarebet-
teradditionalcontextsthantopics.Whenusingasinglecon-
text(i.e.TopCNN
word
,TopCNN
sent
,andourmodelswhen
N
=1
),translationsalwaysoutperformtopicsevenwhenus-
ingthebaselinemethods.Usingtopicsasadditionalcontext
alsodecreasestheperformanceoftheCNNonmost
datasets,givinganadverseeffecttotheCNN.
5ModelInterpretation
WeprovideexamplesshowninTable4onhowtheself
usabilitymoduledeterminesthescoreofsentences.Inthe
example,itishardtoclassifywhetherthetranslatedsen-
tenceispositiveornegative,thusitisgivenalowselfus-
abilityscore.Inthesecondexample,althoughthesentence
containsmistranslations,theseareminimalandmayactually
helpthebytellingitthat
thirstforviolence
isnota
(a)ExamplewhereEnglishattentionweightislarger
(b)ExamplewhereKoreanattentionweightislarger
Figure4:AttentionweightsofexampleKoreansentencesfromthe
MRdataset.Theredcolorrepresentstheattentionweightsgiven
toeachsentence.Thedarkerthethelargertheattentionweight.
Figure5:PCAvisualizationofunaltered(left)andaltered(right)
vectorsoftheMRdataset.
d
istheMahalanobisdistancebetween
twoclassclusters.
negativephrase.Thus,itisgivenahighselfusabilityscore.
Figure4showstwodatainstanceexampleswhereweshow
theattentionweightsgiventotheothercontextswhen
aKoreansentence.Thelargertheattentionweightis,the
morethecontextisusedtotheKoreansentence.Inthe
Originalsentence:
skipthisturdandpickyournoseinsteadbecauseyou'resuretogetmoreoutof
thelatterexperience.
Koreantranslation:
Êê
_
˚
â
+«>
\†""f˚8
´ú§
ﬁÉr
˚
¯

`

¦
%3
Ü¼˜9
•


s˙
Ö

æ
 
é

s˙
\

¦
|


˛-
8A
ﬁ¦
šï
\

¦Y

J


½
+
Ë
m
.
Humanre-translation:
Inordertogetmorefromthelatterexperience,youneedtoskipthispuddleand
chooseyournose.
SelfUsability:0.3958
(a)Lowselfusabilityexample
Originalsentence:
michaelmoore'slatestdocumentaryaboutamerica'sthirstforviolenceishisbest
yet...
Koreantranslation:
˙
9þt
Áº#Q
(MichaelMoore)
_
þ
j

H
p˙
²DG
©
F
'
p
'˙
ﬁ
;¤
§4
†
©
œ
•


ﬂ
ﬁÉr
Õª
_
þ
j
ﬁ¦
_
%
ò

o

...
Humanre-translation:
MichaelMoore'slatestAmericandocumentaryﬁViolentSceneﬂishisbest
yet...
SelfUsability:1.0000
(b)Highselfusabilityexample
Table4:TwoexamplesofselfusabilityofKoreansentencesfrom
theMRdataset.Textscoloredin
red
aremistranslatedtexts.
Sentence
maytakeitssweettimetogetwhereverit'sgoing,butifyouhave
thepatienceforit,youwon'tfeellikeit'swastedyours.
NN
(Unaltered)
youknowthattenbucksyou'dspendonaticket?justsenditto
cranky.wedon'tgetpaidenoughtositthroughcraplikethis.
NN
(altered)
whatmighthavebeenreadilydismissedasthetiresomerantof
anagingerstillthumbinghisnoseatconventiontakesa
surprising,subtleturnatthemidwaypoint.
Sentence
everynanosecondofthenewguyremindsthatyoucouldbedo-
ingsomethingelsemorepleasurable.likescrubbingthetoilet.
emptyingrattraps.ordoinglastyear'staxeswithyourex-wife.
NN
(Unaltered)
inthenewreleaseofcinemaparadiso,thetalehasturnedfrom
sweettobittersweet,andwhenthetearscomeduringthat
beautifulscene,theyfeelabsolutelyearned.
NN
(altered)
afterscenesofnonsense,you'llbewistfulforthetestosterone-
chargedwizardryofjerrybruckheimerproductions,especially
becausehalfpastdeadisliketherockonwalmartbudget.
Table5:Twoexamplesentences,fromEnglisht)andKorean
(second)vectorspaces,andtheirnearestneighbors(NN)onboth
theunalteredandalteredvectorspaces.Weonlyshowtheoriginal
EnglishsentencesfortheKoreanexampleforconciseness.
example,theKoreansentencecontainstranslationerrors;
especially,thewords
bore
and
climacticsetpiece
werenot
translatedandwereonlyspelledusingtheKoreanalphabet.
Inthisexample,theEnglishattentionweightislargerthanthe
Koreanattentionweight.Inthesecondexample,theKorean
sentencecorrectlytranslatesallpartsoftheEnglishsentence,
exceptforthephrase
asitdoesintrouble
.However,this
phraseisnotnecessarytoclassifythesentencecorrectly,and
mayinducepossiblevaguenessbecauseoftheword
trouble
.
Thus,theKoreanattentionweightislarger.
Figure5showsthePCAvisualizationoftheunalteredand
thealteredvectorsoffourdifferentlanguages.Intheex-
ample,theunalteredsentencevectorsaremostlyinthemid-
dleofthevectorspace,makingithardtodrawaboundary
betweenthetwoexamples.Afterthetheboundaryis
muchclearer.WealsoshowtheEnglishsentencevectorsin
thesecondexample.EvenwithouttheunalteredEn-
glishsentencevectors,itiseasytodistinguishbothclasses.
Afterthethesentencevectorsinthemiddleofthespace
aremoved,makingthedistinctionmoreobviousandclearer.
Wealsoprovidequantitativeevidencebyshowingthatthe
Mahalanobisdistancebetweenthetwoclassesinthealtered
vectorsarefartherthanthatoftheunalteredvec-
tors.
WealsoshowtwoexamplessentencesfromEnglishand
Koreanvectorspacesandtheircorrespondingnearestneigh-
borsonboththeunalteredandalteredvectorspacesinTa-
ble5.Intheexample,theunalteredvectorfocuseson
themeaningof
ﬁwastedyoursﬂ
inthesentence,whichputs
itnearsentencesregardingwastedtimeormoney.After
ing,thesentencevectorfocusesitsmeaningontheslowyet
worth-the-waitpaceofthemovie,thusmovingitclosertothe
correctvectors.Inthesecondexample,allthreesentences
havehighlydescriptivetones,however,thenearestneighbor
onthealteredspaceishyperbolicallynegative,comparingthe
movietoadescriptionunrelatedtothemovieitself.
6RelatedWork
Onewaytoimprovetheperformanceofasentence
istointroducenewcontext.Commonandobviouskindsof
contextaretheneighboringsentencesofthesentence
[
Lin
etal.
,2015
]
,andthedocumentwherethesentencebelongs
[
Huang
etal.
,2012
]
.Topicsofthewordsinthesentence
inducedbyatopicmodelwerealsousedascontexts
[
Zhao
etal.
,2017
]
.Inthispaper,weintroduceyetanothertypeof
additionalcontext,sentencetranslations,whichtothebestof
ourknowledgehavenotbeenusedpreviously.
Sentenceencoderstrainedfromneuralmachinetranslation
(NMT)systemswerealsousedfortransferlearning
[
Hill
et
al.
,2016
]
.
[
Hill
etal.
,2017
]
demonstratedthataltered-length
sentencevectorsfromNMTencodersoutperformsentence
vectorsfrommonolingualencodersonsemanticsimilarity
tasks.Recentworkusedrepresentationofeachwordinthe
sentencetocreateasentencerepresentationsuitableformul-
tipleNLPtasks
[
McCann
etal.
,2017
]
.Ourworksharesthe
commonalityofusingNMTforanothertask,butinsteadof
usingNMTtoencodeoursentences,weuseittotranslatethe
sentencesintonewcontexts.
Increasingthenumberofdatainstancesofthetrainingset
hasalsobeenexploredtoimprovetheperformanceofaclas-
.Recentmethodsincludetheusageofthesaurus
[
Zhang
etal.
,2015
]
,paraphrases
[
Fu
etal.
,2014
]
,amongothers.
Thesesimplevariationtechniquesarepreferredbecausethey
arefoundtobeveryeffectivedespitetheirsimplicity.Our
worksimilarlyaugmentstrainingdata,notbyaddingdatain-
stances(verticalaugmentation),butratherbyaddingmore
context(horizontalaugmentation).Thoughtheparaphrase
of
p
canbealternativelyusedasanaugmentedcontext,this
couldnotleveragetheaddedsemanticscomingfromanother
language,asdiscussedinSection1.
7Conclusion
Thispaperinvestigatestheuseoftranslationsasbetterad-
ditionalcontextsforsentenceToanswerthe
problemonmistranslations,weproposemultiplecontext
ingattachment(MCFA)tothecontextvectorsusingother
contextvectors.Weshowthatourmethodimprovestheclas-
performanceandachievesstate-of-the-artperfor-
manceonmultipledatasets.Inourfuturework,weplan
touseandextendourmodeltoothercomplexNLPtasks.
Acknowledgements
ThisworkwassupportedbyMicrosoftResearchAsiaandthe
ICTR&DprogramofMSIT/IITP.[2017-0-01778,Develop-
mentofExplainableHuman-levelDeepMachineLearning
InferenceFramework]
References
[
Bahdanau
etal.
,2014
]
DzmitryBahdanau,Kyunghyun
Cho,andYoshuaBengio.Neuralmachinetranslation
byjointlylearningtoalignandtranslate.
arXivpreprint
arXiv:1409.0473
,2014.
[
Bojanowski
etal.
,2016
]
PiotrBojanowski,EdouardGrave,
ArmandJoulin,andTomasMikolov.Enrichingword
vectorswithsubwordinformation.
arXivpreprint
arXiv:1607.04606
,2016.
[
Collobert
etal.
,2011
]
RonanCollobert,JasonWeston,
L
´
eonBottou,MichaelKarlen,KorayKavukcuoglu,and
PavelKuksa.Naturallanguageprocessing(almost)
fromscratch.
JournalofMachineLearningResearch
,
12(Aug):2493Œ2537,2011.
[
Fu
etal.
,2014
]
GuohongFu,YuHe,JiayingSong,and
ChaoyueWang.Improvingchinesesentencepolarityclas-
viaopinionparaphrasing.
CLP2014
,page35,
2014.
[
He
etal.
,2016
]
KaimingHe,XiangyuZhang,Shaoqing
Ren,andJianSun.Deepresiduallearningforimagerecog-
nition.In
CVPR
,pages770Œ778,2016.
[
Hill
etal.
,2016
]
FelixHill,KyunghyunCho,andAnnaKo-
rhonen.Learningdistributedrepresentationsofsentences
fromunlabelleddata.
arXivpreprintarXiv:1602.03483
,
2016.
[
Hill
etal.
,2017
]
FelixHill,KyunghyunCho,S
´
ebastien
Jean,andYoshuaBengio.Therepresentationalgeometry
ofwordmeaningsacquiredbyneuralmachinetranslation
models.
MachineTranslation
,pages1Œ16,2017.
[
HuandLiu,2004
]
MinqingHuandBingLiu.Miningand
summarizingcustomerreviews.In
SIGKDD
,pages168Œ
177.ACM,2004.
[
Huang
etal.
,2012
]
EricHHuang,RichardSocher,Christo-
pherDManning,andAndrewYNg.Improvingwordrep-
resentationsviaglobalcontextandmultiplewordproto-
types.In
ACL
,pages873Œ882.AssociationforComputa-
tionalLinguistics,2012.
[
Johnson
etal.
,2016
]
MelvinJohnson,MikeSchuster,
QuocVLe,MaximKrikun,YonghuiWu,ZhifengChen,
NikhilThorat,FernandaVi
´
egas,MartinWattenberg,Greg
Corrado,etal.Google'smultilingualneuralmachine
translationsystem:enablingzero-shottranslation.
arXiv
preprintarXiv:1611.04558
,2016.
[
Joulin
etal.
,2017
]
ArmandJoulin,EdouardGrave,Piotr
Bojanowski,andTomasMikolov.Bagoftricksforef
cienttextIn
EACL
,2017.
[
Kim,2014
]
YoonKim.Convolutionalneuralnetworksfor
sentenceIn
EMNLP
,2014.
[
LiandRoth,2002
]
XinLiandDanRoth.Learningques-
tionIn
COLING
,pages1Œ7.Associationfor
ComputationalLinguistics,2002.
[
Lin
etal.
,2015
]
RuiLin,ShujieLiu,MuyunYang,MuLi,
MingZhou,andShengLi.Hierarchicalrecurrentneural
networkfordocumentmodeling.In
EMNLP
,pages899Œ
907,2015.
[
Ma
etal.
,2015
]
MingboMa,LiangHuang,BingXiang,
andBowenZhou.Dependency-basedconvolutionalneu-
ralnetworksforsentenceembedding.
arXivpreprint
arXiv:1507.01839
,2015.
[
McCann
etal.
,2017
]
BryanMcCann,JamesBradbury,
CaimingXiong,andRichardSocher.Learnedintrans-
lation:Contextualizedwordvectors.
arXivpreprint
arXiv:1708.00107
,2017.
[
Mimno
etal.
,2011
]
DavidMimno,HannaMWallach,Ed-
mundTalley,MiriamLeenders,andAndrewMcCallum.
Optimizingsemanticcoherenceintopicmodels.In
EMNLP
,pages262Œ272.AssociationforComputational
Linguistics,2011.
[
PangandLee,2004
]
BoPangandLillianLee.Asentimen-
taleducation:Sentimentanalysisusingsubjectivitysum-
marizationbasedonminimumcuts.In
ACL
,page271.
AssociationforComputationalLinguistics,2004.
[
PangandLee,2005
]
BoPangandLillianLee.Seeingstars:
Exploitingclassrelationshipsforsentimentcategorization
withrespecttoratingscales.In
ACL
,pages115Œ124.As-
sociationforComputationalLinguistics,2005.
[
PangandLee,2007
]
BoPangandLillianLee.Opinion
miningandsentimentanalysis.
FoundationsandTrends
inInformationRetrieval
,2:1Œ135,2007.
[
Srivastava
etal.
,2014
]
NitishSrivastava,GeoffreyEHin-
ton,AlexKrizhevsky,IlyaSutskever,andRuslan
Salakhutdinov.Dropout:asimplewaytopreventneural
networksfromov
Journalofmachinelearning
research
,15(1):1929Œ1958,2014.
[
Zhang
etal.
,2015
]
XiangZhang,JunboZhao,andYann
LeCun.Character-levelconvolutionalnetworksfortext
In
NIPS
,pages649Œ657,2015.
[
Zhang
etal.
,2016
]
RuiZhang,HonglakLee,andDragomir
Radev.Dependencysensitiveconvolutionalneuralnet-
worksformodelingsentencesanddocuments.
arXiv
preprintarXiv:1611.02361
,2016.
[
Zhao
etal.
,2015
]
HanZhao,ZhengdongLu,andPascal
Poupart.Self-adaptivehierarchicalsentencemodel.In
IJCAI
,pages4069Œ4076,2015.
[
Zhao
etal.
,2017
]
RuiZhao,KezhiMao,RuiZhao,and
KezhiMao.Topic-awaredeepcompositionalmodelsfor
sentence
IEEE/ACMTransactionsonAudio,
SpeechandLanguageProcessing
,25(2):248Œ260,2017.
"
98,Cold-Start Aware User and Product Attention for Sentiment Classification,http://arxiv.org/pdf/1806.05507v1.pdf,https://github.com/rktamplayo/HCSC,"Cold-StartAwareUserandProductAttention
forSentiment
ReinaldKimAmplayo
and
JihyeokKim
and
SuaSung
and
Seung-wonHwang
YonseiUniversity
Seoul,SouthKorea
f
rktamplayo,zizi1532,dormouse,seungwonh
g
@yonsei.ac.kr
Abstract
Theuseofuser/productinformationin
sentimentanalysisisimportant,especially
for
cold-start
users/products,whosenum-
berofreviewsareverylimited.How-
ever,currentmodelsdonotdealwiththe
cold-startproblemwhichistypicalinre-
viewwebsites.Inthispaper,wepresent
HybridContextualizedSentimentClassi-
(HCSC),whichcontainstwomod-
ules:(1)afastwordencoderthatreturns
wordvectorsembeddedwithshortand
longrangedependencyfeatures;and(2)
Cold-StartAwareAttention(CSAA),an
attentionmechanismthatconsiderstheex-
istenceofcold-startproblemwhenatten-
tivelypoolingtheencodedwordvectors.
HCSCintroduces
shared
vectorsthatare
constructedfromsimilarusers/products,
andareusedwhentheoriginal
distinct
vectorsdonothavesufinforma-
tion(i.e.cold-start).Thisisdecided
byafrequency-guidedselectivegatevec-
tor.Ourexperimentsshowthatinterms
ofRMSE,HCSCperforms
betterwhencomparedwithonfamous
datasets,despitehavinglesscomplexity,
andthuscanbetrainedmuchfaster.More
importantly,ourmodelperforms
cantlybetterthanpreviousmodelswhen
thetrainingdataissparseandhascold-
startproblems.
1Introduction
Sentimentisthefundamentaltaskof
sentimentanalysis(
Pangetal.
,
2002
),wherewe
aretoclassifythesentimentofagiventext.Itis
widelyusedononlinereviewwebsitesastheycon-
tainhugeamountsofreviewdatathatcanbeclas-
Figure1:ConceptualschemaofHCSCappliedto
users.Thesameideacanbeappliedtoproducts.
asentiment.Inthesewebsites,asentimentis
usuallyrepresentedasanintensity(e.g.4outof5).
Thereviewsarewrittenbyuserswhohavebought
aproduct.Recently,sentimentanalysisresearch
hasfocusedonpersonalization(
Zhang
,
2015
)to
recommendproducttousers,andviseversa.
Tothisend,manyhaveuseduserandprod-
uctinformationnotonlytodeveloppersonaliza-
tionbutalsotoimprovetheperformanceofthe
model(
Tangetal.
,
2015
).Indeed,
theseinformationareimportantintwoways.First,
someexpressionsareuserforacertain
sentimentintensity.Forexample,thephraseﬁ
very
salty
ﬂmayhavedifferentsentimentsforaperson
wholikessaltyfoodandapersonwholikesoth-
erwise.Thisisalsoapparentintermsofprod-
ucts.Second,theseadditionalcontextshelpmit-
igatedatasparsityandcold-startproblems.Cold-
startisaproblemwhenthemodelcannotdraw
usefulinformationfromusers/productswheredata
isinsufUserandproductinformationcan
helpbyintroducingafrequentuser/productwith
similarattributestothecold-startuser/product.
Thankstothepromisingresultsofdeepneu-
ralnetworkstothesentimenttask
arXiv:1806.05507v1  [cs.CL]  14 Jun 2018(
Glorotetal.
,
2011
;
Tangetal.
,
2014
),morere-
centmodelsincorporateuserandproductinforma-
tiontoconvolutionalneuralnetworks(
Tangetal.
,
2015
)anddeepmemorynetworks(
Dou
,
2017
),
andhaveshownimprovements.The
currentstate-of-the-artmodel,NSC(
Chenetal.
,
2016a
),introducedanattentionmechanismcalled
UPAwhichisbasedonuserandproductinfor-
mationandappliedthistoahierarchicalLSTM.
Themainproblemwithcurrentmodelsisthatthey
useuserandproductinformationnaivelyasan
ordinaryadditionalcontext,notconsideringthe
possibleexistenceofcold-startproblems.This
makesNSCmoreproblematicthanhelpfulinre-
alitysincemajorityoftheusersinreviewwebsites
haveveryfewnumberofreviews.
Tothisend,weproposetheideashowninFig-
ure
1
.Itcanbedescribedasfollows:Ifthe
modeldoesnothaveenoughinformationtocre-
ateauser/productvector,thenweuseavector
computedfromotheruser/productvectorsthatare
similar.WeintroduceanewmodelcalledHy-
bridContextualizedSentiment(HCSC),
whichconsistsoftwomodules.First,webuilda
fastyeteffectivewordencoderthatacceptsword
vectorsandoutputsnewencodedvectorsthatare
contextualizedwithshort-andlong-rangecon-
texts.Second,wecombinethesevectorsintoone
pooledvectorthroughanovelattentionmecha-
nismcalledCold-StartAwareAttention(CSAA).
TheCSAAmechanismhasthreecomponents:
(a)a
distinctvector
derived
fromtheoriginaluser/productinformationofthe
review,(b)a
sharedvec-
tor
derivedfromotherusers/products,and(c)
afrequency-guided
selectivegate
whichdecides
whichvectortouse.Multipleexperimentsare
conductedwiththefollowingresults:Intheorig-
inalnon-sparsedatasets,ourmodelperformssig-
betterthanthepreviousstate-of-the-art,
NSC,intermsofRMSE,despitebeinglesscom-
plex.Inthesparsedatasets,HCSCperformssig-
betterthanpreviouscompetingmodels.
2Relatedwork
Previousstudieshaveshownthatusingadditional
contextsforsentimenthelpsimprove
theperformanceofthe.Wesurveysev-
eralcompetingbaselinemodelsthatuseuserand
productinformationandothermodelsusingother
kindsofadditionalcontext.
Baselines:Modelswithuserandproductinfor-
mation
Userandproductinformationarehelp-
fultoimprovetheperformanceofasentiment
.Thisargumentwasvby
Tang
etal.
(
2015
)throughtheobservationatthecon-
sistencybetweenuser/productinformationand
thesentimentsandexpressionsfoundinthetext.
Listedbelowarethefollowingmodelsthatemploy
userandproductinformation:

JMARS
(
Diaoetal.
,
2014
)jointlymodels
theaspects,ratings,andsentimentsofare-
viewwhileconsideringtheuserandproduct
informationusingcollaborativeand
topicmodelingtechniques.

UPNN
(
Tangetal.
,
2015
)usesaCNN-based
andextendsittoincorporateuser-
andproductextpreferencematrix
inthewordlevelwhichmoditheword
meaning.

TLFM+PRC
(
Songetal.
,
2017
)isatext-
drivenlatentfactormodelthatuser-
andlatentfactormodelsrep-
resentedusingtheconsistencyassumptionby
Tangetal.
(
2015
).

UPDMN
(
Dou
,
2017
)usesanLSTMclas-
asthedocumentencoderandmodi-
theencodedvectorusingadeepmem-
orynetworkwithotherdocumentsofthe
user/productasthememory.

TUPCNN
(
Chenetal.
,
2016b
)extendsthe
CNN-basedbyaddingtemporal
userandproductembeddings,whichareob-
tainedfromasequentialmodelandlearned
throughthetemporalorderofreviews.

NSC
(
Chenetal.
,
2016a
)isthecurrentstate-
of-the-artmodelthatutilizesahierarchical
LSTMmodel(
Yangetal.
,
2016
)andincor-
poratesuserandproductinformationinthe
attentionmechanism.
Modelswithotheradditionalcontexts
Other
additionalcontextsusedpreviouslyarespatial
(
Yangetal.
,
2017
)andtemporal(
Fukuharaetal.
,
2007
)featureswhichhelpcontextualizethesen-
timentbasedonthelocationwhereandthetime
whenthetextiswritten.Inferredcontextswere
alsousedasadditionalcontextsforsentimentclas-
suchaslatenttopics(
LinandHe
,
2009
)
andaspects(
JoandOh
,
2011
)fromatopicmodel,
argumentationfeatures(
Wachsmuthetal.
,
2015
),
andmorerecently,latentreviewclusters(
Am-
playoandHwang
,
2017
).Theseadditionalcon-
Figure2:FullarchitectureofHCSC,whichconsistsoftheHybridContextualizedWordEncoder(middle),and
user(left)and(right)Cold-StartAwareAttention(CSAA).
textswereespeciallyusefulwhendataissparse,
i.e.numberofinstancesissmallorthereexists
cold-startentities.
Ourmodeldiffersfromthebaselinemodels
mainlybecauseweconsiderthepossibleexis-
tenceofthedatasparsityproblem.Throughthis,
weareabletoconstructmoreeffectivemodels
thatarecomparablypowerfulyetmoreef
complexity-wisethanthestate-of-the-art,andare
betterwhenthetrainingdataissparse.Ultimately,
ourgoalistodemonstratethat,similartoother
additionalcontexts,userandproductinformation
canbeusedtoeffectivelymitigatetheproblem
causedbycold-startusersandproducts.
3Ourmodel
Inthissection,wepresentourmodel,
Hybrid
ContextualizedSentiment(HCSC)
1
whichconsistsofafasthybridcontextualized
wordencoderandanattentionmechanismcalled
Cold-StartAwareAttention(CSAA).Theword
encoderreturnswordvectorswithbothlocaland
globalcontextstocoverbothshortandlongrange
dependencyrelationshipbetweenwords.The
CSAAthenincorporatesuserandproductinfor-
mationtothecontextualizedwordsthroughanat-
tentionmechanismthatconsidersthepossibleex-
istenceofcold-startproblems.Thefullarchitec-
tureofthemodelispresentedinFigure
2
.We
1
Thedataandcodeusedinthispaperareavailablehere:
https://github.com/rktamplayo/HCSC
.
describethesubpartsofthemodelbelow.
3.1Hybridcontextualizedwordencoder
Thebasemodelisawordencoderthattransforms
vectorsofwords
f
w
i
g
inthetexttonewwordvec-
tors.Inthispaper,wepresentafastyetveryeffec-
tivewordencoderbasedontwodifferentoff-the-
shelf
ThepartofHCWEisbasedonaCNN
modelwhichiswidelyusedintext
(
Kim
,
2014
).Thisencodercontextualizeswords
basedonlocalcontextwordstocaptureshort
rangerelationshipsbetweenwords.,
wedotheconvolutionoperationusingmatri-
ces
W
f
2
R
h

d
withsize
h
toawindowof
h
words.Wedothisfordifferentsizesof
h
.This
producesnewfeaturevectors
c
i;h
asshownbelow,
where
f
(
:
)
isanon-linearfunction:
c
i;h
=
f
([
w
i

(
h

1)
=
2
;
:::
;
w
i
+(
h

1)
=
2
]
>
W
f
+
b
f
)
Theconvolutionoperationreducesthenumber
ofwordsdifferentlydependingonthesize
h
.Topreventlossofinformationandtopro-
ducethesameamountoffeaturevectors
c
i;h
,we
padthetextsdynamicallysuchthatwhenthe
tersizeis
h
,thenumberofpaddingsoneachside
is
(
h

1)
=
2
.Thisrequiresthesizestobe
oddnumbers.Finally,weconcatenateallfeature
vectorsofdifferent
h
'sforeach
i
asthenewword
vector:
w
cnn
i
=[
c
i;h
1
;
c
i;h
2
;
:::
]
ThesecondpartofHCWEisbasedonanRNN
modelwhichisusedwhentextsarelongerandin-
cludeworddependenciesthatmaynotbecaptured
bytheCNNmodel.,weuseabidi-
rectionalLSTMandconcatenatetheforwardand
backwardhiddenstatevectorsasthenewword
vector,asshownbelow:
!
h
i
=
LSTM
(
w
i
;
!
h
i

1
)
 
h
i
=
LSTM
(
w
i
;
 
h
i
+1
)
w
rnn
i
=[
!
h
i
;
 
h
i
]
Theanswertothequestionwhethertouselo-
calorglobalcontexttoencodewordsforsenti-
mentisstillunclear,andbothCNN
andRNNmodelshavepreviousempiricalevi-
dencethattheyperformbetterthantheother(
Kim
,
2014
;
McCannetal.
,
2017
).Webelievethatboth
shortandlongrangerelationships,capturedby
CNNandRNNrespectively,areusefulforsenti-
mentTherearealreadypreviousat-
temptstointricatelycombinebothCNNandRNN
(
Zhouetal.
,
2016
),resultingtoaslowermodel.
Ontheotherhand,HCWEresortstocombine
thembysimplyconcatenatingthewordvectors
encodedfrombothCNNandRNNencoders,i.e.
w
i
=[
w
cnn
i
;
w
rnn
i
]
.Thisstraightforwardyet
fastalternativeoutputsawordvectorwithseman-
ticscontextualizedfrombothlocalandglobalcon-
texts.Moreover,theyperformaswellascomplex
hierarchicalstructuredmodels(
Yangetal.
,
2016
;
Chenetal.
,
2016a
)whichtrainveryslow.
3.2Cold-startawareattention
Incorporatingtheuserandproductinformation
ofthetextascontextvectors
u
and
p
toatten-
tivelypoolthewordvectors,i.e.
e
(
w
i
;u;p
)=
v
>
tanh
(
W
w
w
i
+
W
u
u
+
W
p
p
+
b
)
,hasbeen
proventoimprovetheperformanceofsentiment
(
Chenetal.
,
2016a
).However,this
methodassumesthattheuserandproductvectors
arealwayspresent.Thisisnotthecaseinreal
worldsettingswhereauser/productmaybenew
andhasjustgotitsreview.Inthiscase,the
vectors
u
and
p
arerendereduselessandmayalso
containnoisysignalsthatdecreasetheoverallper-
formanceofthemodels.
Tothisend,wepresentanattentionmecha-
nismcalledCold-StartAwareAttention(CSAA).
CSAAoperatesontheideathatacold-start
user/productcanusetheinformationofothersim-
ilarusers/productswithsufnumberofre-
views.CSAAseparatestheconstructionofpooled
vectorsforuserandforproduct,unlikeprevi-
ousmethodsthatusebothuser/productinforma-
tiontocreateasinglepooledvector.Construct-
ingapooledvectorconsists
ofthreeparts:thedistinctpooledvectorcreated
usingtheoriginaluser/product,thesharedpooled
vectorcreatedusingsimilarusers/products,and
theselectivegatetoselectbetweenthedistinct
andsharedvectors.Finally,theuser-andproduct-
pooledvectorsarecombinedintoone
pooledvector.
Inthefollowingparagraphs,wediscussthe
step-by-stepprocessonhowtheuser
pooledvectorisconstructed.Asimilarprocessis
donetoconstructthepooledvec-
tor,butisnotpresentedhereforconciseness.
Theuser
distinctpooledvector
v
d
u
is
createdusingamethodsimilartotheadditiveat-
tentionmechanism(
Bahdanauetal.
,
2014
),i.e.
v
d
u
=
att
(
f
w
i
g
;u
)
,wherethecontextvectoristhe
distinctvectorofuser
u
,asshownintheequation
below.Anequivalentmethodisusedtocreatethe
distinctpooledvector
v
d
p
.
e
d
u
(
w
i
;u
)=
v
d
>
tanh
(
W
d
w
w
i
+
W
d
u
u
+
b
d
)
a
d
u
i
=
exp
(
e
d
u
(
w
i
;u
))
P
j
exp
(
e
d
u
(
w
j
;u
))
v
d
u
=
X
i
a
d
u
i

w
i
Theuser
sharedpooledvector
v
s
u
iscre-
atedusingthesamemethodabove,butusinga
sharedcontextvector
u
0
.Thesharedcontextvec-
tor
u
0
isconstructedusingthevectorsofother
usersandweightedbasedonasimilarityweight.
Similarityisashowsimilarthewordus-
agesoftwousersare.Thismeansthatifauser
u
k
useswordssimilarlytothewordusageofthe
originaluser
u
,then
u
k
receivesahighsimilarity
weight.Thesimilarityweight
a
s
u
k
iscalculatedas
thesoftmaxoftheproductof

(
f
w
i
g
)
and
u
k
with
aprojectmatrixinthemiddle,where

(
f
w
i
g
)
is
theaverageofthewordvectors.Thesimilarity
weightsareusedtocreate
u
0
,asshownbelow.
Similarmethodisusedforthesharedproduct-
pooledvector
v
s
p
.
e
s
u
(

(
f
w
i
g
)
;u
k
)=

(
f
w
i
g
)
W
s
u
u
k
a
s
u
k
=
exp
(
e
s
u
(
w
i
;u
k
))
P
j
exp
(
e
s
u
(
w
i
;u
j
))
u
0
=
X
k
a
s
u
k

u
k
v
s
u
=
att
(
f
w
i
g
;u
0
)
Weselectbetweentheuserdistinctand
sharedpooledvector,
v
d
u
and
v
s
u
,intooneuser-
pooledvector
v
u
throughagatevector
g
u
.
Thevector
g
u
shouldputmoreweighttothedis-
tinctvectorwhenuser
u
isnotcold-startandto
thesharedvectorwhen
u
isotherwise.Weusea
frequency-guidedselectivegate
thatutilizesthe
frequency,i.e.thenumberofreviewsuser
u
has
written.Thechallengeisthatwedonotknowhow
manyreviewsshouldbeconsideredcold-startor
not.Thisisautomaticallylearnedthroughatwo-
parameterWeibullcumulativedistributionwhere
giventhereviewfrequencyoftheuser
f
(
u
)
,a
learnedshapevector
k
u
andalearnedscalevector

u
,aprobabilityvectorissampledandisusedas
thegatevector
g
u
tocreate
v
u
,accordingtothe
equationbelow.Wenormalized
f
(
u
)
bydivid-
ingittotheaverageuserreviewfrequency.The
relufunctionensuresthatboth
k
u
and

u
arenon-
negativevectors.Thepooled
vector
v
p
iscreatedinasimilarmanner.
g
u
=1

exp



f
(
u
)
relu
(

u
)

relu
(
k
u
)

v
u
=
g
u

v
d
u
+(1

g
u
)

v
s
u
Finally,wecombineboththeuser-andproduct-
pooledvector,
v
u
and
v
p
,intoonepooled
vector
v
up
.Thisisdonebyusingagatevector
g
up
createdusingasigmoidaltransformationof
theconcatenationof
v
u
and
v
p
,asillustratedin
theequationbelow.
g
up
=
˙
(
W
g
[
v
u
;
v
p
]+
b
g
)
v
up
=
g
up

v
u
+(1

g
up
)

v
p
Wenotethatourattentionmechanismcanbe
appliedtoanywordencoders,includingthebasic
bagofwords(BoW)tomorerecentmodelssuch
asCNNandRNN.Later(inSection
4.2
),weshow
thatCSAAimprovestheperformanceofsimpler
modelsgreatly.
3.3Trainingobjective
Normally,asentimenttransformsthe
nalvector
v
up
,usuallyinalinearfashion,intoa
vectorwithadimensionequivalenttothenum-
berofclasses
C
.Asoftmaxlayeristhenusedto
obtainaprobabilitydistribution
y
0
overthesenti-
mentclasses.Finally,thefullmodelusesacross-
entropyoveralltrainingdocuments
D
asobjec-
tivefunction
L
duringtraining,where
y
isthegold
probabilitydistribution:
y
0
=
softmax
(
Wv
up
+
b
)
L
=

X
d
2
D
X
c
2
C
y
(
d
)
c

log(
y
0
(
d
)
c
)
However,HCSChasanicearchitecture
whichcanbeusedtoimprovethetrain-
ing.Itcontainssevenpooledvectors
V
=
f
v
d
u
;v
d
p
;v
s
u
;v
s
p
;v
u
;v
p
;v
up
g
thatareessentiallyin
thesamevectorspace.Thisisbecausethese
vectorsarecreatedusingweightedsumsofei-
thertheencodedwordvectorsthroughattention
ortheparentpooledvectorsthroughtheselective
gates.Therefore,wecantrainseparate
foreachpooledvectorsusingthesameparame-
ters
W
and
b
.,foreach
v
2
V
,we
calculatetheloss
L
v
usingtheaboveformulas.
Thelossisthenthesumofallthelosses,i.e.
L
=
P
v
2
V
L
v
.
4Experiments
Inthissection,wepresentourexperimentsand
thecorrespondingresults.Weusethemodelsde-
scribedinSection
2
asbaselinemodels:
JMARS
(
Diaoetal.
,
2014
),
UPNN
(
Tangetal.
,
2015
),
TLFM+PRC
(
Songetal.
,
2017
),
UPDMN
(
Dou
,
2017
),
TUPCNN
(
Chenetal.
,
2016b
),and
NSC
(
Chenetal.
,
2016a
),where
NSC
isthemodelwith
state-of-the-artresults.
4.1Experimentalsettings
Implementation
Wesetthesizeoftheword,
user,andproductvectorsto300dimensions.We
usepre-trainedGloVeembeddings
2
(
Pennington
etal.
,
2014
)toinitializeourwordvectors.We
simplysettheparametersforbothBiLSTMsand
CNNtoproduceanoutputwith300dimensions:
FortheBiLSTMs,wesetthestatesizesofthe
LSTMsto75dimensions,foratotalof150dimen-
sions.ForCNN,weset
h
=3
;
5
;
7
,eachwith50
2
https://nlp.stanford.edu/projects/
glove/
Datasets
Classes
Train
Dev
Test
#docs#users#prods
#docs#users#prods
#docs#users#prods
IMDB
10
6742613101635
838113101574
911213101578
Yelp2013
5
6252216311633
777316311559
867116311577
Datasets
Classes
Sparse20
Sparse50
Sparse80
#docs#users#prods
#docs#users#prods
#docs#users#prods
IMDB
10
4426110421323
17963659840
2450250312
Yelp2013
5
3868713011288
16058818823
2406352304
Table1:Datasetstatistics
featuremaps,foratotalof150dimensions.These
twoareconcatenatedtocreatea300-dimension
encodedwordvectors.Weusedropout(
Srivastava
etal.
,
2014
)onallnon-linearconnectionswitha
dropoutrateof0.5.Wesetthebatchsizeto32.
Trainingisdoneviastochasticgradientdescent
overshufmini-batcheswiththeAdadeltaup-
daterule(
Zeiler
,
2012
),with
l
2
constraint(
Hinton
etal.
,
2012
)of3.Weperformearlystoppingusing
asubsetofthegivendevelopmentdataset.Train-
ingandexperimentsarealldoneusingaNVIDIA
GeForceGTX1080Tigraphicscard.
Additionally,wealsoimplementtwoversions
ofourmodelwherethewordencoderisasub-
partofHCSC,i.e.(a)theCNN-basedmodel
(
CNN+CSAA
)and(b)theRNN-basedmodel
(
RNN+CSAA
).FortheCNN-basedmodel,we
use100featuremapsforeachofthesizes
h
=3
;
5
;
7
,foratotalof300dimensions.Forthe
RNN-basedmodel,wesetthestatesizesofthe
LSTMsto150,foratotalof300dimensions.
Datasetsandevaluation
Weevaluateandcom-
pareourmodelswithothercompetingmodels
usingtwowidelyusedsentiment
datasetswithavailableuserandproductinforma-
tion:IMDBandYelp2013.Bothdatasetsare
curatedby
Tangetal.
(
2015
),wheretheyaredi-
videdintotrain,dev,andtestsetsusinga8:1:1ra-
tio,andaretokenizedandsentence-splittedusing
StanfordCoreNLP(
Manningetal.
,
2014
).Inad-
dition,wecreatethreesubsetsofthetraindataset
totesttherobustnessofthemodelsonsparse
datasets.Tocreatethesedatasets,werandomlyre-
moveallthereviewsof
x
%
ofallusersandprod-
ucts,where
x
=20
;
50
;
80
.Thesedatasetsare
notonlymoresparsethantheoriginaldatasets,
butalsohavesmallernumberofusersandprod-
ucts,introducingcold-startusersandproducts.All
datasetsaresummarizedinTable
1
.Evaluationis
doneusingtwometrics:theAccuracywhichmea-
surestheoverallsentimentonperfor-
manceandtheRMSEwhichmeasuresthediver-
Models
IMDB
Yelp2013
Acc.RMSE
Acc.RMSE
JMARS
-1.773

-0.985

UPNN
0.435

1.602

0.596

0.784

TLFM+PRC
-1.352

-0.716

UPDMN
0.465

1.351

0.639

0.662
TUPCNN
0.488

1.451

0.639

0.694

NSC
0.5331.281

0.6500.692

CNN+CSAA
0.522

1.256

0.6540.665
RNN+CSAA
0.527

1.237

0.6540.667
HCSC
0.5421.213
0.6570.660
Table2:AccuracyandRMSEvaluesofcompeting
modelsontheoriginalnon-sparsedatasets.Anaster-
iskindicatesthatHCSCisbetterthanthe
model(
p<
0
:
01
).
gencebetweenpredictedandgroundtruthclasses.
Wenoticeveryminimaldifferencesamongperfor-
mancesofdifferentruns.
4.2Comparisonsonoriginaldatasets
Wereporttheresultsontheoriginaldatasets
inTable
2
.Onbothdatasets,HCSCoutper-
formsallpreviousmodelsbasedonbothaccuracy
andRMSE.Basedonaccuracy,HCSCperforms
betterthanallpreviousmodelsex-
ceptNSC,whereitperformsslightlybetterwith
0.9%and0.7%increaseonIMDBandYelp2013
datasets.BasedonRMSE,HCSCperformssig-
betterthanallpreviousmodels,except
whencomparedwithUPDMNontheYelp2013
datasets,whereitperformsslightlybetter.Wenote
thatRMSEisabettermetricbecauseitmeasures
howclosethewronglypredictedsentimentandthe
groundtruthsentimentare.AlthoughNSCper-
formsaswellasHCSCbasedonaccuracy,itper-
formsworsebasedonRMSE,whichmeansthatits
predictionsdeviatefarfromtheoriginalsentiment.
ItisalsointerestingtonotethatwhenCSAA
isusedasattentivepooling,bothsimpleCNN
andRNNmodelsperformjustaswellasNSC,
despiteNSCbeingverycomplexandmodel-
ingthedocumentswithcompositionality(
Chen
etal.
,
2016a
).Thisisespeciallytruewhencom-
Models
Sparse20Sparse50Sparse80
NSC(LA)
0.4690.4280.309
NSC
0.497
0.408
0.292
CNN+CSAA
0.4970.4440.343
RNN+CSAA
0.505
0.4550.364
HCSC
0.5050.4560.368
(a)IMDBDatasets
Models
Sparse20Sparse50Sparse80
NSC(LA)
0.6240.5900.523
NSC
0.6260.592
0.511
CNN+CSAA
0.6260.6050.522
RNN+CSAA
0.6330.6030.527
HCSC
0.6360.6080.538
(b)Yelp2013Datasets
Table3:Accuracyvaluesofcompetingmodelswhen
thetrainingdatausedissparse.
Bold-faced
valuesare
thebestaccuraciesinthecolumn,while
red
valuesare
accuraciesworsethanNSC(LA).
paredusingRMSE,wherebothCNN+CSAAand
RNN+CSAAperformbetter(
p<
0
:
01
)thanNSC.ThisprovesthatCSAAisanef-
fectiveuseoftheuserandproductinformationfor
sentiment
4.3Comparisonsonsparsedatasets
Table
3
showstheaccuracyofNSC(
Chen
etal.
,
2016a
)andourmodelsCNN+CSAA,
RNN+CSAA,andHCSConthesparsedatasets.
Asshowninthetable,onalldatasetswithdif-
ferentlevelsofsparsity,HCSCperformsthebest
amongthecompetingmodels.Thedifferencebe-
tweentheaccuracyofHCSCandNSCincreasesas
thelevelofsparsityWhiletheHCSC
onlygains0.8%and1.0%overNSContheless
sparseSparse20IMDBandYelp2013datasets,it
improvesoverNSCwith7.6%and
2.7%increaseonthemoresparseSparse80IMDB
andYelp2013datasets,respectively.
WealsorunourexperimentsusingNSCwith-
outuserandproductinformation,i.e.NSC(LA)
whichreducesthemodelintoahierarchicalLSTM
model(
Yangetal.
,
2016
).Resultsshowthat
althoughtheuseofuserandproductinforma-
tioninNSCimprovesthemodelonlesssparse
datasets(asalsoshownintheoriginalpaper(
Chen
etal.
,
2016a
)),itdecreasestheperformanceofthe
modelonmoresparsedatasets:Itperforms2.0%,
1.7%,and1.2%worsethanNSC(LA)onSparse50
IMDB,Sparse80IMDB,andSparse80Yelp2013
datasets.WearguethatthisisbecauseNSCdoes
notconsidertheexistenceofcold-startproblems,
whichmakestheadditionaluserandproductin-
Figure3:Accuracyperuser/productreviewfrequency
onbothdatasets.Thereviewfrequencyvalue
f
repre-
sentsthefrequenciesintherange
[
f;f
+10)
,except
when
f
=100
,whichrepresentsthefrequenciesinthe
range
[
f;
1
)
.
formationmorenoisythanhelpful.
5Analysis
Inthissection,weshowfurtherinterestinganal-
ysesofthepropertiesofHCSC.Weusethe
Sparse50datasetsandthecorrespondingresultsof
severalmodelsastheexperimentaldata.
Performanceperreviewfrequency
Wein-
vestigatetheperformanceofthemodelover
users/productswithdifferentnumberofreviews.
Figure
3
showsplotsofaccuracyofbothNSCand
HCSCover(a)differentuserreviewfrequencyon
IMDBdatasetand(b)differentproductreviewfre-
quencyonYelp2013dataset.Onbothplots,we
observethatwhenthereviewfrequencyissmall,
theperformancegainofHCSCoverNSCisvery
large.However,asthereviewfrequencybecomes
larger,theperformancegainofHCSCoverNSC
decreasestoaverymarginalincrease.Thismeans
thatHCSCitsimprovementsoverNSCfrom
cold-startusersandproducts,inwhichNSCdoes
notconsiderexplicitly.
Howfewiscold-start?
Oneintriguingquestion
iswhendowesaythatauser/productiscold-
startornot.Obviously,users/productswithno
previousreviewsatallshouldbeconsideredcold-
start,howeverthecut-offpointbetweencold-start
andnon-cold-startentitiesisvague.Althoughwe
Figure4:VisualizationofattentionandgatevaluesoftwoexamplesfromtheYelp2013dataset.Example2is
truncated,leavingonlytheimportantparts.Gatevalues
g
'saretheaverageofthevaluesintheoriginalgatevector.
Figure5:GraphoftheWeibull
cumulativedistributiononbothdatasets.
cannotprovideanexactanswertothisquestion,
HCSCisabletoprovideanicevisualizationbyre-
ducingtheshapeandscalevectors,
k
and

,ofthe
frequency-guidedselectivegateintotheiraverages
anddrawaWeibullcumulativedistributiongraph,
asshowninFigure
5
.Theprovidesusthese
observations:First,usershaveamorelenientcold-
startcut-offpointcomparedtoproducts;inthe
IMDBdataset,auseronlyneedsapproximatelyat
leastvereviewstouseatleast80%ofitsownin-
formation(i.e.distinctvector).Ontheotherhand,
productstendtoneedmorereviewstobeconsid-
eredsufandnotcoldstart;intheIMDB
dataset,aproductneedsapproximately40reviews
touseatleast80%ofitsowninformation.This
explainsthemarginalincreaseinperformanceof
previousmodelswhenonlyproductinformationis
usedasadditionalcontext,asreportedbyprevious
papers(
Tangetal.
,
2015
;
Chenetal.
,
2016a
).
Onthedifferentpooledvectors
Wevisualize
theattentionandgatevaluesoftwoexamplere-
sultsfromHCSCinFigure
4
toinvestigateonhow
Models
IMDBYelp2013
NSC
73316569
CNN+CSAA
256(28.6x)146(45.0x)
RNN+CSAA
968(7.6x)561(11.7x)
HCSC
1110(6.6x)615(10.7x)
Table4:Time(inseconds)toprocessthe100
batchesofcompetingmodelsforeachdataset.The
numbersintheparenthesisarethespeedupoftime
whencomparedtoNSC.
user/productvectors,anddistinct/sharedvectors
work.Intheexample,bothuserandprod-
uctarecold-start.Theuserdistinctvectorfocuses
itsattentiontowrongwords,sinceitisnotable
touseanyusefulinformationfromtheuseratall.
Inthiscase,HCSCusestheusersharedvectorby
usingagatevector
g
u
=0
.Theusersharedvec-
torcorrectlyattendstoimportantwordssuchas
fresh
,
baked
,
soft
,and
pretzels
.Inthesecondex-
ample,bothuserandproductarenotcold-start.In
thiscase,thedistinctvectorsareusedalmosten-
tirelybysettingthegatescloseto1.Still,thecor-
respondingsharedvectorsaresimilartothedis-
tinctvectors,provingthatHCSCisabletocreate
usefulcontextfromsimilar
users/products.Finally,welookatthedifferingat-
tentionvaluesofusersandproducts.Weobserve
thatuservectorsfocusonwordsthatdescribethe
productorexpresstheiremotions(e.g.
fresh
and
enjoyed
).Ontheotherhand,productvectorsfocus
moreonwordspertainingtotheproducts/services
(e.g.
pretzels
and
waitress
).
Onthetimecomplexityofmodels
Finally,
wereportthetimeinsecondstorun100
batchesofdataofthemodelsNSC,CNN+CSAA,
RNN+CSAA,andHCSCinFigure
4
.NSCtakes
toolongtotrain,needingatleast6500seconds
toprocess100batchesofdata.Thisisbecauseit
usestwonon-parallelizableLSTMsontopofeach
other.Ourmodels,ontheotherhand,onlyuse
one(ornoneinthecaseofCNN+CSAA)levelof
BiLSTM.Thisresultstoatleast6.6xspeedupon
theIMDBdatasets,andatleast10.7xspeedupon
theYelp2013datasets.ThismeansthatHCSC
doesnotalotoftimecomplexitytoob-
tainbetterresults.
6Conclusion
WeproposeHybridContextualizedSentiment
(HCSC)withafastwordencoderwhich
contextualizeswordstocontainbothshortand
longrangeworddependencyfeatures,andanat-
tentionmechanismcalledCold-startAwareAtten-
tion(CSAA)whichconsiderstheexistenceofthe
cold-startproblemamongusersandproductsby
usingasharedvectorandafrequency-guidedse-
lectivegate,inadditiontotheoriginaldistinctvec-
tor.Ourexperimentalresultsshowthatourmodel
performsbetterthanpreviousmod-
els.Theseimprovementsincreasewhenthelevel
ofsparsityindataincreases,whichthat
HCSCisabletodealwiththecold-startproblem.
Acknowledgements
ThisworkwassupportedbyMicrosoftRe-
searchAsiaandtheICTR&Dprogramof
MSIT/IITP.[2017-0-01778,DevelopmentofEx-
plainableHuman-levelDeepMachineLearning
InferenceFramework]
References
ReinaldKimAmplayoandSeung-wonHwang.2017.
Aspectsentimentmodelformicroreviews.In
2017IEEEInternationalConferenceonDataMin-
ing(ICDM)
.IEEE,pages727Œ732.
DzmitryBahdanau,KyunghyunCho,andYoshua
Bengio.2014.Neuralmachinetranslationby
jointlylearningtoalignandtranslate.
CoRR
abs/1409.0473.
HuiminChen,MaosongSun,CunchaoTu,YankaiLin,
andZhiyuanLiu.2016a.Neuralsentiment
cationwithuserandproductattention.In
Proceed-
ingsofthe2016ConferenceonEmpiricalMethods
inNaturalLanguageProcessing
.pages1650Œ1659.
TaoChen,RuifengXu,YulanHe,YunqingXia,and
XuanWang.2016b.Learninguserandproduct
distributedrepresentationsusingasequencemodel
forsentimentanalysis.
IEEEComputationalIntelli-
genceMagazine
11(3):34Œ44.
QimingDiao,MinghuiQiu,Chao-YuanWu,Alexan-
derJSmola,JingJiang,andChongWang.2014.
Jointlymodelingaspects,ratingsandsentimentsfor
movierecommendation(jmars).In
Proceedingsof
the20thACMSIGKDDinternationalconferenceon
Knowledgediscoveryanddatamining
.ACM,pages
193Œ202.
Zi-YiDou.2017.Capturinguserandproductinforma-
tionfordocumentlevelsentimentanalysiswithdeep
memorynetwork.In
Proceedingsofthe2017Con-
ferenceonEmpiricalMethodsinNaturalLanguage
Processing
.pages521Œ526.
TomohiroFukuhara,HiroshiNakagawa,andToyoaki
Nishida.2007.Understandingsentimentofpeople
fromnewsarticles:Temporalsentimentanalysisof
socialevents.In
ICWSM
.
XavierGlorot,AntoineBordes,andYoshuaBengio.
2011.Domainadaptationforlarge-scalesentiment
Adeeplearningapproach.In
Pro-
ceedingsofthe28thinternationalconferenceonma-
chinelearning(ICML-11)
.pages513Œ520.
GeoffreyE.Hinton,NitishSrivastava,Alex
Krizhevsky,IlyaSutskever,andRuslanSalakhut-
dinov.2012.Improvingneuralnetworksby
preventingco-adaptationoffeaturedetectors.
CoRR
abs/1207.0580.
YohanJoandAliceHOh.2011.Aspectandsentiment
modelforonlinereviewanalysis.In
Pro-
ceedingsofthefourthACMinternationalconference
onWebsearchanddatamining
.ACM,pages815Œ
824.
YoonKim.2014.Convolutionalneuralnetworksfor
sentenceIn
Proceedingsofthe2014
conferenceonempiricalmethodsinnaturallan-
guageprocessing(EMNLP)
.
ChenghuaLinandYulanHe.2009.Jointsenti-
ment/topicmodelforsentimentanalysis.In
Pro-
ceedingsofthe18thACMconferenceonInforma-
tionandknowledgemanagement
.ACM,pages375Œ
384.
ChristopherManning,MihaiSurdeanu,JohnBauer,
JennyFinkel,StevenBethard,andDavidMcClosky.
2014.Thestanfordcorenlpnaturallanguagepro-
cessingtoolkit.In
Proceedingsof52ndannual
meetingoftheassociationforcomputationallin-
guistics:systemdemonstrations
.pages55Œ60.
BryanMcCann,JamesBradbury,CaimingXiong,and
RichardSocher.2017.Learnedintranslation:Con-
textualizedwordvectors.In
AdvancesinNeuralIn-
formationProcessingSystems
.pages6297Œ6308.
BoPang,LillianLee,andShivakumarVaithyanathan.
2002.Thumbsup?:sentimentusing
machinelearningtechniques.In
Proceedingsofthe
ACL-02conferenceonEmpiricalmethodsinnatu-
rallanguageprocessing-Volume10
.Associationfor
ComputationalLinguistics,pages79Œ86.
JeffreyPennington,RichardSocher,andChristopher
Manning.2014.Glove:Globalvectorsforword
representation.In
Proceedingsofthe2014confer-
enceonempiricalmethodsinnaturallanguagepro-
cessing(EMNLP)
.pages1532Œ1543.
KaisongSong,WeiGao,ShiFeng,DalingWang,Kam-
FaiWong,andChengqiZhang.2017.Recommen-
dationvssentimentanalysis:atext-drivenlatentfac-
tormodelforratingpredictionwithcold-startaware-
ness.In
Proceedingsofthe26thInternationalJoint
ConferenceonIntelligence
.AAAIPress,
pages2744Œ2750.
NitishSrivastava,GeoffreyHinton,AlexKrizhevsky,
IlyaSutskever,andRuslanSalakhutdinov.2014.
Dropout:Asimplewaytopreventneuralnetworks
fromov
TheJournalofMachineLearning
Research
15(1):1929Œ1958.
DuyuTang,BingQin,andTingLiu.2015.Learning
semanticrepresentationsofusersandproductsfor
documentlevelsentimentIn
Proceed-
ingsofthe53rdAnnualMeetingoftheAssociation
forComputationalLinguisticsandthe7thInterna-
tionalJointConferenceonNaturalLanguagePro-
cessing(Volume1:LongPapers)
.volume1,pages
1014Œ1023.
DuyuTang,FuruWei,BingQin,TingLiu,andMing
Zhou.2014.Coooolll:Adeeplearningsystemfor
twittersentimentIn
Proceedingsof
the8thInternationalWorkshoponSemanticEvalu-
ation(SemEval2014)
.pages208Œ212.
HenningWachsmuth,JohannesKiesel,andBenno
Stein.2015.Sentimentw-ageneralmodelofweb
reviewargumentation.In
Proceedingsofthe2015
ConferenceonEmpiricalMethodsinNaturalLan-
guageProcessing
.pages601Œ611.
MinYang,JinchengMei,HengJi,ZhouZhao,Xiao-
junChen,etal.2017.Identifyingandtrackingsen-
timentsandtopicsfromsocialmediatextsduring
naturaldisasters.In
Proceedingsofthe2017Con-
ferenceonEmpiricalMethodsinNaturalLanguage
Processing
.pages527Œ533.
ZichaoYang,DiyiYang,ChrisDyer,XiaodongHe,
AlexSmola,andEduardHovy.2016.Hierarchi-
calattentionnetworksfordocument
In
Proceedingsofthe2016ConferenceoftheNorth
AmericanChapteroftheAssociationforComputa-
tionalLinguistics:HumanLanguageTechnologies
.
pages1480Œ1489.
MatthewD.Zeiler.2012.Adadelta:Anadaptivelearn-
ingratemethod.
CoRR
abs/1212.5701.
YongfengZhang.2015.Incorporatingphrase-level
sentimentanalysisontextualreviewsforpersonal-
izedrecommendation.In
Proceedingsoftheeighth
ACMinternationalconferenceonwebsearchand
datamining
.ACM,pages435Œ440.
PengZhou,ZhenyuQi,SuncongZheng,JiamingXu,
HongyunBao,andBoXu.2016.Text
improvedbyintegratingbidirectionallstmwithtwo-
dimensionalmaxpooling.In
ProceedingsofCOL-
ING2016,the26thInternationalConferenceon
ComputationalLinguistics:TechnicalPapers
.pages
3485Œ3495.
"
99,Entity Commonsense Representation for Neural Abstractive Summarization,http://arxiv.org/pdf/1806.05504v1.pdf,https://github.com/rktamplayo/Entity2Topic,"EntityCommonsenseRepresentation
forNeuralAbstractiveSummarization
ReinaldKimAmplayo

and
SeonjaeLim

and
Seung-wonHwang
YonseiUniversity,Seoul,SouthKorea
f
rktamplayo,sun.lim,seungwonh
g
@yonsei.ac.kr
Abstract
Amajorproportionofatextsummaryincludes
importantentitiesfoundintheoriginaltext.
Theseentitiesbuildupthetopicofthesum-
mary.Moreover,theyholdcommonsensein-
formationoncetheyarelinkedtoaknowledge
base.Basedontheseobservations,thispa-
perinvestigatestheusageoflinkedentitiesto
guidethedecoderofaneuraltextsummarizer
togenerateconciseandbettersummaries.To
thisend,weleverageonanoff-the-shelfentity
linkingsystem(ELS)toextractlinkedentities
andpropose
Entity2Topic(E2T)
,amodule
easilyattachabletoasequence-to-sequence
modelthattransformsalistofentitiesintoa
vectorrepresentationofthetopicofthesum-
mary.CurrentavailableELS'sarestillnotsuf-
effective,possiblyintroducingunre-
solvedambiguitiesandirrelevantentities.We
resolvetheimperfectionsoftheELSby(a)en-
codingentitieswithselectivedisambiguation,
and(b)poolingentityvectorsusingatten-
tion.ByapplyingE2Ttoasimplesequence-
to-sequencemodelwithattentionmechanism
asbasemodel,weseeimprove-
mentsoftheperformanceintheGigaword
(sentencetotitle)andCNN(longdocument
tomulti-sentencehighlights)summarization
datasetsbyatleast2ROUGEpoints.
1Introduction
Textsummarizationisatasktogenerateashorter
andconciseversionofatextwhilepreservingthe
meaningoftheoriginaltext.Thetaskcanbedi-
videdintotwosubtaskbasedontheapproach:ex-
tractiveandabstractivesummarization.Extrac-
tivesummarizationisatasktocreatesummaries
bypullingoutsnippetsoftextformtheorigi-
naltextandcombiningthemtoformasummary.
Abstractivesummarizationaskstogeneratesum-
mariesfromscratchwithouttherestrictiontouse

AmplayoandLimareauthorswithequalcon-
tribution.Namesarearrangedalphabetically.
Figure1:Observationsonlinkedentitiesinsummaries.
O1
:Summariesaremainlycomposedofentities.
O2
:
Entitiescanbeusedtorepresentthetopicofthesum-
mary.
O3
:Entitycommonsenselearnedfromalarge
corpuscanbeused.
theavailablewordsfromtheoriginaltext.Due
tothelimitationsofextractivesummarizationon
incoherenttextsandunnaturalmethodology(
Yao
etal.
,
2017
),theresearchtrendhasshiftedtowards
abstractivesummarization.
Sequence-to-sequencemodels(
Sutskeveretal.
,
2014
)withattentionmechanism(
Bahdanauetal.
,
2014
)havefoundgreatsuccessingeneratingab-
stractivesummaries,bothfromasinglesentence
(
Chopraetal.
,
2016
)andfromalongdocument
withmultiplesentences(
Chenetal.
,
2016
).How-
ever,whengeneratingsummaries,itisnecessary
todeterminethemaintopicandtosiftoutunnec-
essaryinformationthatcanbeomitted.Sequence-
to-sequencemodelshavethetendencytoinclude
alltheinformation,relevantornot,thatarefound
intheoriginaltext.Thismayresulttouncon-
cisesummariesthatconcentrateswronglyonir-
relevanttopics.Theproblemisespeciallysevere
whensummarizinglongertexts.
Inthispaper,weproposetouseentitiesfoundin
theoriginaltexttoinferthesummarytopic,miti-
arXiv:1806.05504v1  [cs.CL]  14 Jun 2018gatingtheaforementionedproblem.,
weleverageonlinkedentitiesextractedbyem-
ployingareadilyavailableentitylinkingsystem.
Theimportanceofusinglinkedentitiesinsumma-
rizationisintuitiveandcanbeexplainedbylook-
ingatFigure
1
asanexample.First(
O1
intheFig-
ure),asidefromauxiliarywordstoconstructasen-
tence,asummaryismainlycomposedoflinked
entitiesextractedfromtheoriginaltext.Second
(
O2
),wecandepictthemaintopicofthesum-
maryasaprobabilitydistributionofrelevantenti-
tiesfromthelistofentities.Finally(
O3
),wecan
leverageonentitycommonsenselearnedfroma
separatelargeknowledgebasesuchasWikipedia.
Tothisend,wepresentamethodtoef-
fectivelyapplylinkedentitiesinsequence-to-
sequencemodels,called
Entity2Topic(E2T)
.
E2Tisamodulethatcanbeeasilyattachedto
anysequence-to-sequencebasedsummarization
model.Themoduleencodestheentitiesextracted
fromtheoriginaltextbyanentitylinkingsystem
(ELS),constructsavectorrepresentingthetopic
ofthesummarytobegenerated,andinformsthe
decoderabouttheconstructedtopicvector.Dueto
theimperfectionsofcurrentELS's,theextracted
linkedentitiesmaybetoo
ambiguous
and
coarse
tobeconsideredrelevanttothesummary.We
solvethisissuebyusingentityencoderswith
se-
lectivedisambiguation
andbyconstructingtopic
vectorsusing
attention
.
Weexperimentontwodatasets,Gigawordand
CNN,withvaryinglengths.Weshowthatapply-
ingourmoduletoasequence-to-sequencemodel
withattentionmechanismincreases
itsperformanceonbothdatasets.Moreover,when
comparedwiththestate-of-the-artmodelsforeach
dataset,themodelobtainsacomparableperfor-
manceontheGigaworddatasetwherethetextsare
short,andoutperformsallcompetingmodelson
theCNNdatasetwherethetextsarelonger.Fur-
thermore,weprovideanalysisonhowourmodel
effectivelyusestheextractedlinkedentitiestopro-
duceconciseandbettersummaries.
2Usefulnessoflinkedentitiesin
summarization
Inthenextsubsections,wepresentdetailedar-
gumentswithempiricalandpreviouslyexamined
evidencesontheobservationsandpossibleissues
whenusinglinkedentitiesextractedbyanentity
linkingsystem(ELS)forgeneratingabstractive
summaries.Forthispurpose,weusethedevel-
opmentsetsoftheGigaworddatasetprovidedin
(
Rushetal.
,
2015
)andoftheCNNdatasetpro-
videdin(
Hermannetal.
,
2015
)astheexperi-
mentaldataforquantitativeevidenceandreferthe
readerstoFigure
1
astherunningexample.
2.1Observations
AsdiscussedinSection
1
,wethreeobserva-
tionsthatshowtheusefulnessoflinkedentitiesfor
abstractivesummarization.
First,summariesaremainlycomposedoflinked
entitiesextractedfromtheoriginaltext.Intheex-
ample,itcanbeseenthatthesummarycontains
fourwordsthatrefertodifferententities.Infact,
allnounphrasesinthesummarymentionatleast
onelinkedentity.Inourexperimentaldata,weex-
tractlinkedentitiesfromtheoriginaltextandcom-
parethemtothenounphrasesfoundinthesum-
mary.Wereportthat
77
:
1%
and
75
:
1%
ofthenoun
phrasesontheGigawordandCNNdatasets,re-
spectively,containatleastonelinkedentity,which
ourobservation.
Second,linkedentitiescanbeusedtorepresent
thetopicofthesummary,asamultinomial
distributionoverentities,asgraphicallyshownin
theexample,wheretheprobabilitiesrefertothe
relevanceoftheentities.Entitieshavebeenpre-
viouslyusedtorepresenttopics(
Newmanetal.
,
2006
),astheycanbeutilizedasacontrolledvo-
cabularyofthemaintopicsinadocument(
Hulpus
etal.
,
2013
).Intheexample,weseethattheen-
tityﬁ
JaeSeo
ﬂisthemostrelevantbecauseitisthe
subjectofthesummary,whiletheentityﬁ
South
Korean
ﬂislessrelevantbecauseitislessimpor-
tantwhenconstructingthesummary.
Third,wecanmakeuseoftheentitycommon-
sensethatcanbelearnedasacontinuousvector
representationfromaseparatelargercorpus(
Ni
etal.
,
2016
;
Yamadaetal.
,
2017
).Intheex-
ample,ifweknowthattheentitiesﬁ
LosAnge-
lesDodgers
ﬂandﬁ
NewYorkMets
ﬂareAmerican
baseballteamsandﬁ
JaeSeo
ﬂisabaseballplayer
associatedwiththeteams,thenwecanusethisin-
formationtogeneratemorecoherentsummaries.
Wethat
76
:
0%
oftheextractedlinkedenti-
tiesarecoveredbythepre-trainedvectors
1
inour
experimentaldata,provingourthirdobservation.
1
https://github.com/idio/wiki2vec
2.2Possibleissues
Despiteitsusefulness,linkedentitiesextracted
fromELS'shaveissuesbecauseoflowprecision
rates(
Hasibietal.
,
2016
)anddesignchallengesin
trainingdatasets(
Lingetal.
,
2015
).Theseissues
canbesummarizedintotwoparts:ambiguityand
coarseness.
First,theextractedentitiesmaybeambiguous.
Intheexample,theentityﬁ
SouthKorean
ﬂisam-
biguousbecauseitcanrefertoboththeSouth
KoreanpersonandtheSouthKoreanlanguage,
amongothers
2
.Inourexperimentaldata,weex-
tract(1)thetop100entitiesbasedonfrequency,
and(2)theentitiesextractedfrom100randomly
selectedtexts,andcheckwhethertheyhavedisam-
biguationpagesinWikipediaornot.Wediscover
that
71
:
0%
ofthetop100entitiesand
53
:
6%
of
theentitiespickedatrandomhavedisambiguation
pages,whichshowsthatmostentitiesareproneto
ambiguityproblems.
Second,thelinkedentitiesmayalsobetoocom-
montobeconsideredanentity.Thismayintro-
duce
errors
and
irrelevance
tothesummary.In
theexample,ﬁ
Wednesday
ﬂiserroneousbecauseit
iswronglylinkedtotheentityﬁ
WednesdayNight
Baseball
ﬂ.Also,ﬁ
swap
ﬂisirrelevantbecauseal-
thoughitislinkedcorrectlytotheentityﬁ
Trade
(Sports)
ﬂ,itistoocommonandirrelevantwhen
generatingthesummaries.Inourexperimental
data,werandomlyselect100datainstancesand
tagthecorrectnessandrelevanceofextracteden-
titiesintooneoffourlabels:A:correctandrel-
evant,B:correctandsomewhatrelevant,C:cor-
rectbutirrelevant,andD:incorrect.Resultsshow
that
29
:
4%
,
13
:
7%
,
30
:
0%
,and
26
:
9%
aretagged
withA,B,C,andD,respectively,whichshows
thatthereisalargeamountofincorrectandirrele-
vantentities.
3Ourmodel
Tosolvetheissuesdescribedabove,wepresent
Entity2Topic(E2T)
,amodulethatcanbeeasily
attachedtoanysequence-to-sequencebasedab-
stractivesummarizationmodel.E2Tencodesthe
linkedentitiesextractedfromthetextandtrans-
formsthemintoasingletopicvector.Thisvector
isultimatelyconcatenatedtothedecoderhidden
statevectors.Themodulecontainstwosubmod-
ulesfortheissuespresentedbytheen-
2
https://en.wikipedia.org/wiki/South_
Korean
titylinkingsystems:theentityencodingsubmod-
ulewithselectivedisambiguationandthepooling
submodulewithattention.
Overall,ourfullarchitecturecanbeillustrated
asinFigure
2
,whichconsistsofanentitylink-
ingsystem(ELS),asequence-to-sequencewithat-
tentionmechanismmodel,andtheE2Tmodule.
Wenotethatourproposedmodulecanbeeas-
ilyattachedtomoresophisticatedabstractivesum-
marizationmodels(
Zhouetal.
,
2017
;
Tanetal.
,
2017
)thatarebasedonthetraditionalencoder-
decoderframeworkandconsequentlycanproduce
betterresults.Thecodeofthebasemodelandthe
E2Tareavailableonline
3
.
3.1Basemodel
Asourbasemodel,weemployabasicencoder-
decoderRNNusedinmostneuralmachinetrans-
lation(
Bahdanauetal.
,
2014
)andtextsummariza-
tion(
Nallapatietal.
,
2016
)tasks.Weemploya
two-layerbidirectionalGRU(BiGRU)asthere-
currentunitoftheencoder.TheBiGRUconsists
ofaforwardandbackwardGRU,whichresultsto
sequencesofforwardandbackwardhiddenstates
(
!
h
1
;
!
h
2
;:::;
!
h
n
)
and
(
 
h
1
;
 
h
2
;:::;
 
h
n
)
,respec-
tively:
!
h
i
=
GRU
(
x
i
;
!
h
i

1
)
 
h
i
=
GRU
(
x
i
;
 
h
i
+1
)
Theforwardandbackwardhiddenstatesare
concatenatedtogetthehiddenstatevectorsofthe
tokens(i.e.
h
i
=[
!
h
i
;
 
h
i
]
).Thestatesof
theforwardandbackwardGRUarealsoconcate-
natedtocreatethetextrepresentationvector
oftheencoder
s
=[
!
h
n
;
 
h
1
]
.Thesevaluesare
calculatedperlayer,where
x
t
ofthesecondlayer
is
h
t
ofthelayer.Thetextrepresentation
vectorsareprojectedbyafullyconnectedlayer
andarepassedtothedecoderastheinitialhidden
states
s
0
=
s
.
Forthedecoder,weuseatwo-layeruni-
directionalGRUwithattention.Ateachtimestep
t
,theprevioustoken
y
t

1
,theprevioushidden
state
s
t

1
,andthepreviouscontextvector
c
t

1
are
passedtoaGRUtocalculatethenewhiddenstate
s
t
,asshownintheequationbelow.
s
t
=
GRU
(
w
t

1
;s
t

1
;c
t

1
)
3
https://github.com/rktamplayo/
Entity2Topic
Figure2:Fullarchitectureofourproposedsequence-to-sequencemodelwithEntity2Topic(E2T)module.
Thecontextvector
c
t
iscomputedusingthe
additiveattentionmechanism(
Bahdanauetal.
,
2014
),whichmatchesthecurrentdecoderstate
s
t
andeachencoderstate
h
i
togetanimportance
score.Thescoresarethenpassedtoasoftmaxand
areusedtopooltheencoderstatesusingweighted
sum.Thepooledvectoristhecontextvector,
asshownintheequationsbelow.
g
t;i
=
v
>
a
tanh
(
W
a
s
t

1
+
U
a
h
i
)
a
t;i
=
exp
(
g
t;i
)
P
i
exp
(
g
t;i
)
c
t
=
X
i
a
t;i
h
i
Finally,theprevioustoken
y
t

1
,thecurrent
contextvector
c
t
,andthecurrentdecoderstate
s
t
areusedtogeneratethecurrentword
y
t
with
asoftmaxlayeroverthedecodervocabulary,as
shownbelow.
o
t
=
W
w
w
t

1
+
W
c
c
t
+
W
s
s
t
p
(
y
t
j
y
<t
)=
softmax
(
W
o
o
t
)
3.2Entityencodingsubmodule
Afterperformingentitylinkingtotheinputtextus-
ingtheELS,wereceiveasequentiallistoflinked
entities,arrangedbasedontheirlocationinthe
text.Weembedtheseentitiesto
d
-dimensional
vectors
E
=
f
e
1
;e
2
;:::;e
m
g
where
e
i
2
R
d
.
Sincetheseentitiesmaystillcontainambiguity,
itisnecessarytoresolvethembeforeapplying
themtothebasemodel.Basedontheideathat
anambiguousentitycanbedisambiguatedusing
itsneighboringentities,weintroducetwokindsof
disambiguatingencodersbelow.
Globallydisambiguatingencoder
Onewayto
disambiguateanentityisbyusingalltheother
entities,puttingmoreimportancetoentitiesthat
arenearer.Forthispurpose,weemployanRNN-
basedmodeltogloballydisambiguatetheentities.
,weuseBiGRUandconcatenatethe
forwardandbackwardhiddenstatevectorsasthe
newentityvector:
!
h
i
=
GRU
(
e
i
;
!
h
i

1
)
 
h
i
=
GRU
(
e
i
;
 
h
i
+1
)
e
0
i
=[
!
h
i
;
 
h
i
]
Locallydisambiguatingencoder
Anotherway
todisambiguateanentityisbyusingonlythedi-
rectneighborsoftheentity,puttingnoimportance
valuetoentitiesthatarefar.Todothis,weem-
ployaCNN-basedmodeltolocallydisambiguate
theentities.,wedotheconvolution
operationusingmatrices
W
f
2
R
h

d
with
size
h
toawindowof
h
words.Wedothis
fordifferentsizesof
h
.Thisproducesnewfea-
turevectors
c
i;h
asshownbelow,where
f
(
:
)
isa
non-linearfunction:
c
i;h
=
f
([
e
i

(
h

1)
=
2
;
:::
;
e
i
+
h
(+1)
=
2
]
>
W
f
+
b
f
)
Theconvolutionoperationreducesthenumber
ofentitiesdifferentlydependingonthesize
h
.Topreventlossofinformationandtoproduce
thesameamountoffeaturevectors
c
i;h
,wepad
theentitylistdynamicallysuchthatwhenthe
sizeis
h
,thenumberofpaddingsoneachsideis
(
h

1)
=
2
.Thesize
h
thereforereferstothe
numberofentitiesusedtodisambiguateamiddle
entity.Finally,weconcatenateallfeaturevectors
Figure3:Entityencodingsubmodulewithselective
disambiguationappliedtotheentity
3

.Theleft
urerepresentsthefullsubmodulewhiletheright
representsthetwochoicesofdisambiguatingencoders.
ofdifferent
h
'sforeach
i
asthenewentityvector:
e
0
i
=[
c
i;h
1
;
c
i;h
2
;
:::
]
Thequestiononwhichdisambiguatingencoder
isbetterhasbeenadebate;somearguedthatusing
onlythelocalcontextisappropriate(
Lauetal.
,
2013
)whilesomeclaimedthatadditionallyusing
globalcontextalsohelps(
Wangetal.
,
2015
).The
RNN-basedencoderisgoodasitsmartlymakes
useofallentities,howeveritmayperformbad
whentherearemanyentitiesasitintroducesnoise
whenusingafarentityduringdisambiguation.
TheCNN-basedencoderisgoodasitminimizes
thenoisebytotallyignoringfarentitieswhendis-
ambiguating,howeverdeterminingtheappropri-
atesizes
h
needsengineering.Overall,we
arguethatwhentheinputtextisshort(e.g.asen-
tence),bothencodersperformcomparably,other-
wisewhentheinputtextislong(e.g.adocument),
theCNN-basedencoderperformsbetter.
Selectivedisambiguation
Itisobviousthatnot
allentitiesneedtobedisambiguated.Whena
correctlylinkedandalreadyadequatelydisam-
biguatedentityisdisambiguatedagain,itwould
maketheentityveryconteandmight
notbesuitableforthesummarizationtask.Ouren-
tityencodingsubmodulethereforeusesaselective
mechanismthatdecideswhethertousethedisam-
biguatingencoderornot.Thisisdonebyintro-
ducingaselectivedisambiguationgate
d
.The
entityvector
~
e
i
iscalculatedasthelineartransfor-
mationof
e
i
and
e
0
i
:
e
0
i
=
encoder
(
e
i
)
d
=
˙
(
W
d
e
0
i
+
b
d
)
~
e
i
=
d

f
(
W
x
e
i
+
b
x
)+
(1

d
)

f
(
W
y
e
0
i
+
b
y
)
Thefullentityencodingsubmoduleisillus-
tratedinFigure
3
.Ultimately,thesubmodule
outputsthedisambiguatedentityvectors
~
E
=
f
~
e
1
;
~
e
2
;:::;
~
e
m
g
.
3.3Poolingsubmodule
Theentityvectors
~
E
arepooledtocreateasin-
gletopicvector
t
thatrepresentsthetopicofthe
summary.Onepossiblepoolingtechniqueisto
usesoftattention(
Xuetal.
,
2015
)onthevectors
todeterminetheimportancevalueofeachvector,
whichcanbedonebymatchingeachentityvector
withthetextvector
s
fromthetextencoderasthe
contextvector.Theentityvectorsarethenpooled
usingweightedsum.Oneproblemwithsoftat-
tentionisthatitconsiders
all
entityvectorswhen
constructingthetopicvector.However,notallen-
titiesareimportantandnecessarywhengenerat-
ingsummaries.Moreover,anumberoftheseen-
titiesmaybeerroneousandirrelevant,asreported
inSection
2.2
.Softattentiongivesnon-negligible
importantscorestotheseentities,thusaddsunnec-
essarynoisetotheconstructionofthetopicvector.
Ourpoolingsubmoduleinsteaduses
at-
tention
mechanismtoconsideronlytop
k
entities
whenconstructingthetopicvector.Thisisdonein
adifferentiablewayasfollows:
G
=
v
>
a
tanh
(
W
a
~
E
+
U
a
s
)
K
=
top
k
(
G
)
P
=
sparse
vector
(
K;
0
;

)
g
0
i
=
g
i
+
p
i
a
i
=
exp
(
g
0
i
)
P
i
exp
(
g
0
i
)
t
=
X
i
a
i
~
e
i
wherethefunctions
K
=
top
k
(
G
)
getsthe
indicesofthetop
k
vectorsin
G
and
P
=
sparse
vector
(
K;
0
;

)
createsasparsevector
wherethevaluesof
K
is
0
and

otherwise
4
.
Thesparsevector
P
isaddedtotheoriginalimpor-
tancescorevector
G
tocreateanewimportance
4
Weuse

10
9
torepresent

.
scorevector.Inthisnewvector,importantscores
ofnon-top
k
entitiesare

.Whensoftmaxisap-
plied,thisgivesverysmall,negligible,andclose-
to-zerovaluestonon-top
k
entities.Thevalue
k
dependsonthelengthsoftheinputtextandsum-
mary.Moreover,when
k
increasestowards
ity,attentionbecomessoftattention.Wede-
cide
k
empirically(seeSection
5
).
3.4Extendingfromthebasemodel
Entity2Topicmoduleextendsthebasemodelas
follows.Thetextrepresentationvector
s
is
usedasacontextvectorwhenconstructingthe
topicvector
t
inthepoolingsubmodule.Thetopic
vector
t
isthenconcatenatedtothedecoderhidden
statevectors
s
i
,i.e.
s
0
i
=[
s
i
;
t
]
.Theconcatenated
vectorisusedtocreatetheoutputvector:
o
i
=
W
w
w
i

1
+
W
c
c
i
+
W
s
s
0
i
4Relatedwork
Duetoitsrecentsuccess,neuralnetworkmod-
elshavebeenusedwithcompetitiveresultsonab-
stractivesummarization.Aneuralattentionmodel
wasappliedtothetask,easilyachievingstate-
of-the-artperformanceonmultipledatasets(
Rush
etal.
,
2015
).Themodelhasbeenextendedto
insteaduserecurrentneuralnetworkasdecoder
(
Chopraetal.
,
2016
).Themodelwasfurtherex-
tendedtouseafullRNNencoder-decoderframe-
workandfurtherenhancementsthroughlexical
andstatisticalfeatures(
Nallapatietal.
,
2016
).The
currentstate-of-the-artperformanceisachievedby
selectivelyencodingwordsasaprocessofdistill-
ingsalientinformation(
Zhouetal.
,
2017
).
Neuralabstractivesummarizationmodelshave
alsobeenexploredtosummarizelongerdocu-
ments.Wordextractionmodelshavebeenprevi-
ouslyexplored,performingworsethansentence
extractionmodels(
ChengandLapata
,
2016
).Hi-
erarchicalattention-basedrecurrentneuralnet-
workshavealsobeenappliedtothetask,owingto
theideathattherearemultiplesentencesinadoc-
ument(
Nallapatietal.
,
2016
).Finally,distraction-
basedmodelswereproposedtoenablemodels
totraversethetextcontentandgrasptheoverall
meaning(
Chenetal.
,
2016
).Thecurrentstate-of-
the-artperformanceisachievedbyagraph-based
attentionalneuralmodel,consideringthekeyfac-
torsofdocumentsummarizationsuchassaliency,
yandnovelty(
Tanetal.
,
2017
).
Dataset
GigawordCNN
num(data)
4.0M84K
avg(inputWord)
31.4774.9
avg(outputWord)
8.248.1
min(inputEntity)
11
max(inputEntity)
36743
avg(inputEntity)
4.594.6
Table1:Datasetstatistics.
Previousstudiesonthesummarizationtasks
haveonlyusedentitiesinthepreprocessingstage
toanonymizethedataset(
Nallapatietal.
,
2016
)
andtomitigateout-of-vocabularyproblems(
Tan
etal.
,
2017
).Linkedentitiesforsummarization
arestillnotproperlyexploredandwearethe
touselinkedentitiestoimprovetheperformance
ofthesummarizer.
5Experimentalsettings
Datasets
Weusetwowidelyusedsummariza-
tiondatasetswithdifferenttextlengths.First,we
usetheAnnotatedEnglishGigaworddatasetas
usedin(
Rushetal.
,
2015
).Thisdatasetreceives
thesentenceofanewsarticleasinputand
usetheheadlinetitleasthegoldstandardsum-
mary.Sincethedevelopmentdatasetislarge,we
randomlyselected2000pairsasourdevelopment
dataset.Weusethesameheld-outtestdatasetused
in(
Rushetal.
,
2015
)forcomparison.Second,we
usetheCNNdatasetreleasedin(
Hermannetal.
,
2015
).Thisdatasetreceivesthefullnewsarti-
cleasinputandusethehuman-generatedmultiple
sentencehighlightasthegoldstandardsummary.
Theoriginaldatasethasbeenandpre-
processedforthedocumentsumma-
rizationtask(
Nallapatietal.
,
2016
).Inadditionto
thepreviouslyprovideddatasets,weextractlinked
entitiesusingDexter
5
(
Ceccarellietal.
,
2013
),an
opensourceELSthatlinkstextsnippetsfoundin
agiventexttoentitiescontainedinWikipedia.We
usethedefaultrecommendedparametersstatedin
thewebsite.Wesummarizethestatisticsofboth
datasetsinTable
1
.
Implementation
Forbothdatasets,wefurther
reducethesizeoftheinput,output,andentityvo-
cabulariestoatmost50Kassuggestedin(
See
etal.
,
2017
)andreplacelessfrequentwordsto
5
http://dexter.isti.cnr.it/
ﬁ
<
unk
>
ﬂ.Weuse300DGlove
6
(
Pennington
etal.
,
2014
)and1000Dwiki2vec
7
pre-trainedvec-
torstoinitializeourwordandentityvectors.For
GRUs,wesetthestatesizeto500.ForCNN,we
set
h
=3
;
4
;
5
with
400
;
300
;
300
featuremaps,
respectively.Forattention,
k
istunedbycal-
culatingtheperplexityofthemodelstartingwith
smallervalues(i.e.
k
=1
;
2
;
5
;
10
;
20
;:::
)and
stoppingwhentheperplexityofthemodelbe-
comesworsethanthepreviousmodel.Ourpre-
liminarytuningshowedthat
k
=5
forGigaword
datasetand
k
=10
forCNNdatasetarethebest
choices.Weusedropout(
Srivastavaetal.
,
2014
)
onallnon-linearconnectionswithadropoutrate
of0.5.WesetthebatchsizesofGigawordand
CNNdatasetsto80and10,respectively.Training
isdoneviastochasticgradientdescentovershuf-
mini-batcheswiththeAdadeltaupdaterule,
with
l
2
constraint(
Hintonetal.
,
2012
)of3.We
performearlystoppingusingasubsetofthegiven
developmentdataset.Weusebeamsearchofsize
10
togeneratethesummary.
Baselines
FortheGigaworddataset,wecom-
pareourmodelswiththefollowingabstractive
baselines:
ABS+
(
Rushetal.
,
2015
)isanetuned
versionofABSwhichusesanattentiveCNNen-
coderandanNNLMdecoder,
Feat2s
(
Nallap-
atietal.
,
2016
)isanRNNsequence-to-sequence
modelwithlexicalandstatisticalfeaturesinthe
encoder,
Luong-NMT
(
Luongetal.
,
2015
)isa
two-layerLSTMencoder-decodermodel,
RAS-
Elman
(
Chopraetal.
,
2016
)usesanattentive
CNNencoderandanElmanRNNdecoder,and
SEASS
(
Zhouetal.
,
2017
)usesBiGRUencoders
andGRUdecoderswithselectiveencoding.For
theCNNdataset,wecompareourmodelswith
thefollowingextractiveandabstractivebaselines:
Lead-3
isastrongbaselinethatextractsthe
threesentencesofthedocumentassummary,
LexRank
extractstextsusingLexRank(
Erkan
andRadev
,
2004
),
Bi-GRU
isanon-hierarchical
one-layersequence-to-sequenceabstractivebase-
line,
Distraction-M3
(
Chenetal.
,
2016
)uses
asequence-to-sequenceabstractivemodelwith
distraction-basednetworks,and
GBA
(
Tanetal.
,
2017
)isagraph-basedattentionalneuralabstrac-
tivemodel.Allbaselineresultsusedbeamsearch
andaregatheredfrompreviouspapers.Also,
6
https://nlp.stanford.edu/projects/
glove/
7
https://github.com/idio/wiki2vec
Model
RG-1RG-2RG-L
BASE
:s2s+att
34.1415.4432.47
BASE
+E2T
cnn+sd
37.04
16.66
34.93
BASE
+E2T
rnn+sd
36.89
16.86
34.74
BASE
+E2T
cnn
36.5616.5634.57
BASE
+E2T
rnn
36.5216.2134.32
BASE
+E2T
cnn+soft
36.5616.4434.58
BASE
+E2T
rnn+soft
36.3816.1234.20
ABS+
29.7811.8926.97
Feat2s
32.6715.5930.64
Luong-NMT
33.1014.4530.71
RAS-Elman
33.7815.9731.15
SEASS
36.1517.5433.63
Table2:ResultsontheGigaworddatasetusingthefull-
lengthF1variantsofROUGE.
Model
RG-1RG-2RG-L
BASE
:s2s+att
25.55.820.0
BASE
+E2T
cnn+sd
31.910.123.9
BASE
+E2T
rnn+sd
27.67.921.5
BASE
+E2T
cnn
26.67.320.7
BASE
+E2T
rnn
26.16.920.1
BASE
+E2T
cnn+soft
26.67.020.6
BASE
+E2T
rnn+soft
25.06.719.8
Lead-3
26.19.617.8
LexRank
26.19.617.7
Bi-GRU
19.55.215.0
Distraction-M3
27.18.218.7
GBA
30.39.820.0
Table3:ResultsontheCNNdatasetusingthefull-
lengthF1ROUGEmetric.
wecompareourmodel
BASE
+E2T
withthe
basemodel
BASE
andsomevariantsofourmodel
(withoutselectivedisambiguation,usingsoftat-
tention).
6Results
WereporttheROUGEF1scoresforbothdatasets
ofallthecompetingmodelsusingROUGEF1
scores(
Lin
,
2004
).Wereporttheresultsonthe
GigawordandtheCNNdatasetinTable
2
andTa-
ble
3
,respectively.InGigaworddatasetwherethe
textsareshort,ourbestmodelachievesacompara-
bleperformancewiththecurrentstate-of-the-art.
InCNNdatasetwherethetextsarelonger,ourbest
modeloutperformsallthepreviousmodels.We
emphasizethatE2Tmoduleiseasilyattachable
tobettermodels,andweexpectE2Ttoimprove
Model
1st2nd3rd4th
mean
GOLD
0.270.340.210.18
2.38
BASE
0.140.150.28
0.43
3.00
BASE
+E2T
rnn
0.120.240.390.25
2.77
BASE
+E2T
cnn
0.47
0.270.120.14
1.93
Table4:HumanevaluationsontheGigaworddataset.
Bold-facedvaluesarethebestwhilered-coloredvalues
aretheworstamongthevaluesintheevaluationmetric.
theirperformanceaswell.Overall,E2Tachieves
aimprovementoverthebaselinemodel
BASE
,withatleast2ROUGE-1pointsincrease
intheGigaworddatasetand6ROUGE-1points
increaseintheCNNdataset.Infact,allvariants
ofE2Tgainimprovementsoverthebaseline,im-
plyingthatleveragingonlinkedentitiesimproves
theperformanceofthesummarizer.Amongthe
modelvariants,theCNN-basedencoderwithse-
lectivedisambiguationandattentionperforms
thebest.
AutomaticevaluationontheGigaworddataset
showsthattheCNNandRNNvariantsof
BASE
+E2Thavesimilarperformance.Tobreak
thetiebetweenbothmodels,wealsoconducthu-
manevaluationontheGigaworddataset.Wein-
structtwoannotatorstoreadtheinputsentence
andrankthecompetingsummariesfromto
lastaccordingtotheirrelevanceandy:(a)
theoriginalsummary
GOLD
,andfrommodels(b)
BASE
,(c)
BASE
+E2T
cnn
,and(d)
BASE
+E2T
rnn
.
Wethencompute(i)theproportionofeveryrank-
ingofeachmodeland(ii)themeanrankofeach
model.TheresultsarereportedinTable
4
.The
modelwiththebestmeanrankis
BASE
+E2T
cnn
,
followedby
GOLD
,thenby
BASE
+E2T
rnn
and
BASE
,respectively.WealsoperformANOVAand
post-hocTukeyteststoshowthattheCNNvari-
antis(
p<
0
:
01
)betterthantheRNN
variantandthebasemodel.TheRNNvariantdoes
notperformaswellastheCNNvariant,contrary
totheautomaticROUGEevaluationabove.In-
terestingly,theCNNvariantproducesbetter(but
withnodifference)summariesthanthe
goldsummaries.Wepositthatthisisduetothe
factthatthearticletitledoesnotcorrespondtothe
summaryofthesentence.
Selectivedisambiguationofentities
Weshow
theeffectivenessoftheselectivedisambiguation
gate
d
inselectingwhichentitiestodisambiguate
ornot.Table
6
showsatotaloffourdifferentex-
amplesoftwoentitieswiththehighest/lowest
d
values.Intheexample,sentence
E1.1
con-
tainstheentityﬁ
UnitedStates
ﬂandislinkedwith
thecountryentityofthesamename,however
thecorrectlinkedentityshouldbeﬁ
UnitedStates
DavisCupteam
ﬂ,andthereforeisgivenahigh
d
value.Ontheotherhand,sentence
E1.2
islinked
correctlytothecountryﬁ
UnitedStates
ﬂ,andthus
isgivenalow
d
value..Thesecondexamplepro-
videsasimilarscenario,wheresentence
E2.1
is
linkedtotheentityﬁ
Gold
ﬂbutshouldbelinkedto
theentityﬁ
Goldmedal
ﬂ.Sentence
E2.2
islinked
correctlytothechemicalelement.Hence,thefor-
mercasereceivedahighvalue
d
whilethelatter
casereceivedalow
d
value.
Entitiesassummarytopic
Finally,weprovide
onesampleforeachdatasetinTable
5
forcase
study,comparingourmodelthatuses

attention(
BASE
cnn+sd
),avariantthatuses
soft
attention(
BASE
cnn+soft
),andthe
baseline
model
(
BASE
).Wealsoshowtheattentionweightsofthe

and
soft
models.
IntheGigawordexample,wethreeob-
servations.First,thebasemodelgenerateda
lessinformativesummary,notmentioningﬁ
mex-
icostate
ﬂandﬁ
stedition
ﬂ.Second,thesoft
modelproducedafactuallywrongsummary,say-
ingthatﬁ
guadalajara
ﬂisamexicanstate,while
actuallyitisacity.Third,themodelisable
tosolvetheproblembyfocusingonlyontheve
mostimportantentities,eliminatingpossiblenoise
suchasﬁ
Unk
ﬂandlesscrucialentitiessuchas
ﬁ
Countryclub
ﬂ.Wecanalsoseetheeffective-
nessoftheselectivedisambiguationinthisexam-
ple,wheretheentityﬁ
U.S.state
ﬂiscorrectedto
meantheentityﬁ
Mexicanstate
ﬂwhichbecomes
relevantandisthereforeselected.
IntheCNNexample,wealsothatthebase-
linemodelgeneratedaveryerroneoussummary.
Wearguethatthisisbecausethelengthofthein-
puttextislongandthedecoderisnotguidedasto
whichtopicsitshouldfocuson.Thesoftmodel
generatedamuchbettersummary,howeveritfo-
cusesonthewrongtopics,onﬁ
Iran's
nuclearprogram
ﬂ,makingthesummarylessgen-
eral.Aquickreadoftheoriginalarticletellsus
thatthemaintopicofthearticleisallaboutthetwo
politicalpartiesarguingoverthedealwithIran.
However,theentityﬁ
nuclear
ﬂappearedalotinthe
article,whichmakesthesoftmodelwronglyfocus
ontheﬁ
nuclear
ﬂentity.Themodelproduced
themorerelevantsummary,focusingonthepo-
GigawordDatasetExample
Original
westernmexico@
state
@
jalisco
willhosttheeditionofthe@
UNK
dollar@
lorenaochoa
invitation@
golf
tournamentonnov.##-######,in@
guadalajara
@
countryclub
,the@
lorenaochoa
foundationsaidinastatementonwednesday.
Gold
mexicotohostlorenaochoagolftournamentin####
Baseline
guadalajaratohostochoatournamenttournament
Entities:
U.S.stateJaliscoUnkLorenaOchoaGolfGuadalajaraCountryclubLorenaOchoa
Soft
0.083
0.086
0.124
0.101
0.080
0.161
0.189
0.177
mexicostate
guadalajara
tohost
ochoa
ochoainvitation
Firm
0.173
0.197
0.000
0.213
0.215
0.000
0.00
0.202
mexicanstatetohosteditionofochoainvitation
CNNDatasetExample
Original
URL:
http://edition.cnn.com/2015/04/05/politics/netanyahu-iran-deal/index.html
Gold
netanyahusaysthirdoptionisﬁstandingﬂtogetabetterdeal.
politicalsparringcontinuesinu.s.overthedealwithiran.
Baseline
netanyahusaysheisacountryofﬁUNKcheatingﬂandthatitisacountryofﬁUNKcheatingﬂ
netanyahusaysheisacountryofﬁUNKcheatingﬂandthatﬁisaverybaddealﬂ
hesayshesayshesaystheplanisacountryofﬁUNKcheatingﬂandthatitisacountryofﬁUNKcheatingﬂ
hesaystheu.s.isacountryofﬁUNKcheatingﬂandthatisacountryofﬁUNKcheatingﬂ
Soft
benjaminnetanyahu:ﬁithinkthere'sathirdalternative,andthatisstanding,ﬂnetanyahutellscnn.
hesayshedoesnotrollbackiran'snuclearambitions.
ﬁitdoesnotrollbackiran'snuclearprogram.ﬂ
Firm
new:netanyahu:ﬁithinkthere'sathirdalternative,andthatisstanding,ﬂnetanyahusays.
obama
'scommentscomeasdemocratsandrepublicanssparovertheframeworkannouncedlastweektoliftwesternsanctionsoniran.
Table5:ExamplesfromGigawordandCNNdatasetsandcorrespondingsummariesgeneratedbycompeting
models.Thetaggedpartoftextismarked
bold
andprecededwithatsign(@).Theredcolorrepresentsthe
attentionscoresgiventoeachentity.WeonlyreporttheattentionscoresofentitiesintheGigawordexamplefor
concisenesssincethereare80linkedentitiesintheCNNexample.
Text
d
Linkedentity:
https://en.wikipedia.org/wiki/United_States
E1.1
:andyroddickgotthebetterofdmitrytursunovinstraightsetson
friday,assuringthe@
unitedstates
a#-#leadoverdefendingchampions
russiainthe####daviscup.
0.719
E1.2
:siralexfergusonrevealedfridaythatdavidbeckham'smovetothe
@
unitedstates
hadnotsurprisedhimbecauseheknewthe
wouldnotreturntoenglandifhecouldnotcomebacktomanchester
united.
0.086
Linkedentity:
https://en.wikipedia.org/wiki/Gold
E2.1
:followingisthemedalstandingatthe##tholympicwintergames
-lrb-tabulatedunderteam,@
gold
,silverandbronze-rrb-:UNK
0.862
E2.2
:@
gold
openedlowerhereonmondayat###.##-###.##usdollars
anounce,againstfriday'sclosingrateof###.##-###.##.
0.130
Table6:Exampleswithhighest/lowestdisambiguation
gate
d
valuesoftwoexampleentities(
UnitedStates
and
gold
).Thetaggedpartoftextismarked
bold
and
precededwithatsign(@).
liticalentities(e.g.ﬁ
republicans
ﬂ,ﬁ
democrats
ﬂ).
Thisisduetothefactthatonlythe
k
=10
most
importantelementsareattendedtocreatethesum-
marytopicvector.
7Conclusion
Weproposedtoleverageonlinkedentitiestoim-
provetheperformanceofsequence-to-sequence
modelsonneuralabstractivesummarizationtask.
Linkedentitiesareusedtoguidethedecodingpro-
cessbasedonthesummarytopicandcommon-
senselearnedfromaknowledgebase.Weintro-
ducedEntity2Topic(E2T),amodulethatiseasily
attachabletoanymodelusinganencoder-decoder
framework.E2Tapplieslinkedentitiesintothe
summarizerbyencodingtheentitieswithselec-
tivedisambiguationandpoolingthemintoone
summarytopicvectorwithattentionmecha-
nism.WeshowedthatbyapplyingE2Ttoabasic
sequence-to-sequencemodel,weachieve
cantimprovementsoverthebasemodelandcon-
sequentlyachieveacomparableperformancewith
morecomplexsummarizationmodels.
Acknowledgement
Wewouldliketothankthethreeanonymousre-
viewersfortheirvaluablefeedback.Thiswork
wassupportedbyMicrosoftResearch,andIn-
stituteforInformationcommunicationsTechnol-
ogyPromotion(IITP)grantfundedbytheKorea
government(MSIT)(No.2017-0-01778,Develop-
mentofExplainableHumanlevelDeepMachine
LearningInferenceFramework).S.Hwangisa
correspondingauthor.
References
DzmitryBahdanau,KyunghyunCho,andYoshuaBen-
gio.2014.Neuralmachinetranslationbyjointly
learningtoalignandtranslate.
arXivpreprint
arXiv:1409.0473
.
DiegoCeccarelli,ClaudioLucchese,SalvatoreOr-
lando,RaffaelePerego,andSalvatoreTrani.2013.
Dexter:anopensourceframeworkforentitylinking.
In
Proceedingsofthesixthinternationalworkshop
onExploitingsemanticannotationsininformation
retrieval
.ACM,pages17Œ20.
QianChen,XiaodanZhu,ZhenhuaLing,SiWei,
andHuiJiang.2016.Distraction-basedneuralnet-
worksfordocumentsummarization.
arXivpreprint
arXiv:1610.08462
.
JianpengChengandMirellaLapata.2016.Neural
summarizationbyextractingsentencesandwords.
arXivpreprintarXiv:1603.07252
.
SumitChopra,MichaelAuli,andAlexanderMRush.
2016.Abstractivesentencesummarizationwithat-
tentiverecurrentneuralnetworks.In
Proceedingsof
the2016ConferenceoftheNorthAmericanChap-
teroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies
.pages93Œ98.
G
¨
unesErkanandDragomirRRadev.2004.Lexrank:
Graph-basedlexicalcentralityassalienceintext
summarization.
JournalofIntelligence
Research
22:457Œ479.
FaeghehHasibi,KrisztianBalog,andSveinErikBrats-
berg.2016.Onthereproducibilityofthetagmeen-
titylinkingsystem.In
EuropeanConferenceonIn-
formationRetrieval
.Springer,pages436Œ449.
KarlMoritzHermann,TomasKocisky,Edward
Grefenstette,LasseEspeholt,WillKay,MustafaSu-
leyman,andPhilBlunsom.2015.Teachingma-
chinestoreadandcomprehend.In
AdvancesinNeu-
ralInformationProcessingSystems
.pages1693Œ
1701.
GeoffreyEHinton,NitishSrivastava,AlexKrizhevsky,
IlyaSutskever,andRuslanRSalakhutdinov.2012.
Improvingneuralnetworksbypreventingco-
adaptationoffeaturedetectors.
arXivpreprint
arXiv:1207.0580
.
IoanaHulpus,ConorHayes,MarcelKarnstedt,and
DerekGreene.2013.Unsupervisedgraph-based
topiclabellingusingdbpedia.In
Proceedingsofthe
sixthACMinternationalconferenceonWebsearch
anddatamining
.ACM,pages465Œ474.
JeyHanLau,PaulCook,andTimothyBaldwin.2013.
unimelb:Topicmodelling-basedwordsensein-
ductionforwebsnippetclustering.In
SemEval@
NAACL-HLT
.pages217Œ221.
Chin-YewLin.2004.Rouge:Apackageforauto-
maticevaluationofsummaries.In
Textsummariza-
tionbranchesout:ProceedingsoftheACL-04work-
shop
.Barcelona,Spain,volume8.
XiaoLing,SameerSingh,andDanielSWeld.2015.
Designchallengesforentitylinking.
Transactions
oftheAssociationforComputationalLinguistics
3:315Œ328.
Minh-ThangLuong,HieuPham,andChristopherD
Manning.2015.Effectiveapproachestoattention-
basedneuralmachinetranslation.
arXivpreprint
arXiv:1508.04025
.
RameshNallapati,BowenZhou,CaglarGulcehre,
BingXiang,etal.2016.Abstractivetextsumma-
rizationusingsequence-to-sequencernnsandbe-
yond.
arXivpreprintarXiv:1602.06023
.
DavidNewman,ChaitanyaChemudugunta,Padhraic
Smyth,andMarkSteyvers.2006.Analyzingenti-
tiesandtopicsinnewsarticlesusingstatisticaltopic
models.In
ISI
.Springer,pages93Œ104.
YuanNi,QiongKaiXu,FengCao,YosiMass,Dafna
Sheinwald,HuiJiaZhu,andShaoShengCao.
2016.Semanticdocumentsrelatednessusingcon-
ceptgraphrepresentation.In
Proceedingsofthe
NinthACMInternationalConferenceonWebSearch
andDataMining
.ACM,pages635Œ644.
JeffreyPennington,RichardSocher,andChristopher
Manning.2014.Glove:Globalvectorsforword
representation.In
Proceedingsofthe2014confer-
enceonempiricalmethodsinnaturallanguagepro-
cessing(EMNLP)
.pages1532Œ1543.
AlexanderMRush,SumitChopra,andJasonWe-
ston.2015.Aneuralattentionmodelforab-
stractivesentencesummarization.
arXivpreprint
arXiv:1509.00685
.
AbigailSee,PeterJLiu,andChristopherDMan-
ning.2017.Gettothepoint:Summarization
withpointer-generatornetworks.
arXivpreprint
arXiv:1704.04368
.
NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,
IlyaSutskever,andRuslanSalakhutdinov.2014.
Dropout:asimplewaytopreventneuralnetworks
fromov
Journalofmachinelearningre-
search
15(1):1929Œ1958.
IlyaSutskever,OriolVinyals,andQuocVLe.2014.
Sequencetosequencelearningwithneuralnet-
works.In
Advancesinneuralinformationprocess-
ingsystems
.pages3104Œ3112.
JiweiTan,XiaojunWan,andJianguoXiao.2017.
Abstractivedocumentsummarizationwithagraph-
basedattentionalneuralmodel.In
Proceedingsof
the55thAnnualMeetingoftheAssociationforCom-
putationalLinguistics(Volume1:LongPapers)
.
volume1,pages1171Œ1181.
JingWang,MohitBansal,KevinGimpel,BrianD
Ziebart,andTYuClement.2015.Asense-topic
modelforwordsenseinductionwithunsupervised
dataenrichment.
TransactionsoftheAssociationfor
ComputationalLinguistics
3:59Œ71.
KelvinXu,JimmyBa,RyanKiros,KyunghyunCho,
AaronCourville,RuslanSalakhudinov,RichZemel,
andYoshuaBengio.2015.Show,attendandtell:
Neuralimagecaptiongenerationwithvisualat-
tention.In
InternationalConferenceonMachine
Learning
.pages2048Œ2057.
IkuyaYamada,HiroyukiShindo,HideakiTakeda,and
YoshiyasuTakefuji.2017.Learningdistributedrep-
resentationsoftextsandentitiesfromknowledge
base.
arXivpreprintarXiv:1705.02494
.
Jin-geYao,XiaojunWan,andJianguoXiao.2017.Re-
centadvancesindocumentsummarization.
Knowl-
edgeandInformationSystems
pages1Œ40.
QingyuZhou,NanYang,FuruWei,andMingZhou.
2017.Selectiveencodingforabstractivesentence
summarization.
arXivpreprintarXiv:1704.07073
.
"
