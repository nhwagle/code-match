{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Objective 1 Equation To Code Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhwagle/code-match/blob/main/Objective_1_Equation_To_Code_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This file contains code we utilized with a simple contrastive loss function to attempt to predict when code and equations went together. We find that our models immediately shortcut to the random chance solution, and do not further work to align text embeddings between our latex model and the TransCoder model."
      ],
      "metadata": {
        "id": "aSXUNZ7hbr5w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82i_Cc08zWK3"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing codegen. Their install_env.sh won't work for colab so modifying such that all conda imports are done with pip"
      ],
      "metadata": {
        "id": "Lq8FKDVXwgxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install transformers pylatexenc"
      ],
      "metadata": {
        "id": "YzjVFdEo5rwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from pylatexenc.latex2text import LatexNodes2Text"
      ],
      "metadata": {
        "id": "ZXwX4tnk4xlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUL_IFxeA3L7",
        "outputId": "ca13e266-8c85-4978-a324-3b78630103bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/MyDrive/RSS/Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-dGMK1NA3R8",
        "outputId": "33f8efd2-178e-4581-a643-a290c4d23a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "algorithms.csv\tcode.csv\t  code-small.csv  func-rep-small.csv\n",
            "code-all.csv\tcode-func-rep.pt  equations.csv   func-rep-small.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eqns = pd.read_csv(\"/content/gdrive/MyDrive/RSS/Data/equations.csv\")\n",
        "algs = pd.read_csv(\"/content/gdrive/MyDrive/RSS/Data/algorithms.csv\")\n",
        "code = pd.read_csv(\"/content/gdrive/MyDrive/RSS/Data/code.csv\")"
      ],
      "metadata": {
        "id": "UQe_7U9zmPKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing methods for reading in latex"
      ],
      "metadata": {
        "id": "a6WSS85agxc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "model = DistilBertModel.from_pretrained(\"distilbert-base-cased\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RN3BbnwhWfK",
        "outputId": "708e5168-ba01-480e-cba0-23bd9c46d3e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eqn_index = random.randint(0,10000)\n",
        "#bellman:\n",
        "# eqn_index = 3210\n",
        "\n",
        "\n",
        "print(f\"Example of equation index number {eqn_index}\")\n",
        "\n",
        "latex = eqns.iloc[eqn_index].eqn\n",
        "print(\"\")\n",
        "print(\"======\")\n",
        "print(\"Multistep\")\n",
        "print(\"======\")\n",
        "print(\"Raw latex below:\")\n",
        "print(\"\")\n",
        "print(latex)\n",
        "print(len(latex))\n",
        "print(\"Converted Unicode below:\")\n",
        "uni_rep = LatexNodes2Text().latex_to_text(latex)\n",
        "print(uni_rep)\n",
        "print(\"Tokenization below:\")\n",
        "print(\"\")\n",
        "print(tokenizer(uni_rep)['input_ids'])\n",
        "print(\"\")\n",
        "print(\"Inverted Tokenization\")\n",
        "print(\"\")\n",
        "print(tokenizer.decode(tokenizer(uni_rep)['input_ids']))\n",
        "\n",
        "print(\"======\")\n",
        "print(\"Single step\")\n",
        "print(\"======\")\n",
        "print(\"Inverted Tokenization\")\n",
        "print(\"\")\n",
        "print(tokenizer.decode(tokenizer(latex)['input_ids']))"
      ],
      "metadata": {
        "id": "vGiDynIxAh9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da3b6d7d-5752-48d0-a761-54c2bdb728d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of equation index number 1045\n",
            "\n",
            "======\n",
            "Multistep\n",
            "======\n",
            "Raw latex below:\n",
            "\n",
            " \\label{def-twc}\n",
            "tw\\left(x,s\\right) = \\frac{\\Sigma_{p \\in R\\left(x\\right)} \\mid  \\{c \\mid s \\models c   \n",
            "    \\wedge positiveImpact\\left (p, c \\right) \n",
            "    \\wedge p \\in s \\wedge active\\left(x,p \\right)\\}\\mid \n",
            "%\\quad \\textnormal{and} \\quad\n",
            "%twc^-\\left(x,s\\right) = \\Sigma_{p \\in R\\left(x\\right)} \\mid\\{c \\mid neg\\left(x,p,c,s\\right)\\}\\mid \\quad \\quad\n",
            "}{\n",
            "\\Sigma_{p \\in R\\left(x\\right)} \\mid \\{c \\mid \\left(s \\not\\models c \\vee \\neg positiveImpact\\left (p, c \\right)\\right) \\wedge addBy\\left (c,p \\right) \\wedge p \\in s \\wedge active\\left(x,p \\right) \\}\\mid \\textnormal{+} 1\n",
            "}\n",
            "\n",
            "573\n",
            "Converted Unicode below:\n",
            " \n",
            "tw(x,s) = Σ_p ∈R(x) |{c |s c   \n",
            "    ∧positiveImpact(p, c ) \n",
            "    ∧p ∈s ∧active(x,p )}|\n",
            "\n",
            "/\n",
            "Σ_p ∈R(x) |{c |(s c ∨positiveImpact(p, c )) ∧addBy(c,p ) ∧p ∈s ∧active(x,p ) }|+ 1\n",
            "\n",
            "\n",
            "Tokenization below:\n",
            "\n",
            "[101, 189, 2246, 113, 193, 117, 188, 114, 134, 408, 168, 185, 850, 2069, 113, 193, 114, 197, 196, 172, 197, 188, 172, 856, 5674, 22472, 2240, 8223, 11179, 113, 185, 117, 172, 114, 856, 1643, 850, 1116, 856, 19667, 113, 193, 117, 185, 114, 198, 197, 120, 408, 168, 185, 850, 2069, 113, 193, 114, 197, 196, 172, 197, 113, 188, 172, 857, 5674, 22472, 2240, 8223, 11179, 113, 185, 117, 172, 114, 114, 856, 3556, 1181, 2064, 1183, 113, 172, 117, 185, 114, 856, 1643, 850, 1116, 856, 19667, 113, 193, 117, 185, 114, 198, 197, 116, 122, 102]\n",
            "\n",
            "Inverted Tokenization\n",
            "\n",
            "[CLS] tw ( x, s ) = Σ _ p ∈R ( x ) | { c | s c ∧positiveImpact ( p, c ) ∧p ∈s ∧active ( x, p ) } | / Σ _ p ∈R ( x ) | { c | ( s c ∨positiveImpact ( p, c ) ) ∧addBy ( c, p ) ∧p ∈s ∧active ( x, p ) } | + 1 [SEP]\n",
            "======\n",
            "Single step\n",
            "======\n",
            "Inverted Tokenization\n",
            "\n",
            "[CLS] \\ label { def - twc } tw \\ left ( x, s \\ right ) = \\ frac { \\ Sigma _ { p \\ in R \\ left ( x \\ right ) } \\ mid \\ { c \\ mid s \\ models c \\ wedge positiveImpact \\ left ( p, c \\ right ) \\ wedge p \\ in s \\ wedge active \\ left ( x, p \\ right ) \\ } \\ mid % \\ quad \\ textnormal { and } \\ quad % twc ^ - \\ left ( x, s \\ right ) = \\ Sigma _ { p \\ in R \\ left ( x \\ right ) } \\ mid \\ { c \\ mid neg \\ left ( x, p, c, s \\ right ) \\ } \\ mid \\ quad \\ quad } { \\ Sigma _ { p \\ in R \\ left ( x \\ right ) } \\ mid \\ { c \\ mid \\ left ( s \\ not \\ models c \\ vee \\ neg positiveImpact \\ left ( p, c \\ right ) \\ right ) \\ wedge addBy \\ left ( c, p \\ right ) \\ wedge p \\ in s \\ wedge active \\ left ( x, p \\ right ) \\ } \\ mid \\ textnormal { + } 1 } [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import gensim.downloader\n",
        "print(latex)\n",
        "word_tokenize(latex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpeAXeFm2sin",
        "outputId": "21ab929b-34c6-4445-bc2c-c050388b5162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\\label{NstepOptimBellman}\n",
            " Q^*(s_0, a_0) = \\E_{\\Traj_{:N} \\sim \\pi^* \\mid s_0, a_0} \\left[ \\sum_{t=0}^{N-1} \\gamma^{t}r_t + \\gamma^N \\E_{s_N} \\max_{a^*_N} Q^*(s_N, a^*_N) \\right]   \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\\\label',\n",
              " '{',\n",
              " 'NstepOptimBellman',\n",
              " '}',\n",
              " 'Q^*',\n",
              " '(',\n",
              " 's_0',\n",
              " ',',\n",
              " 'a_0',\n",
              " ')',\n",
              " '=',\n",
              " '\\\\E_',\n",
              " '{',\n",
              " '\\\\Traj_',\n",
              " '{',\n",
              " ':',\n",
              " 'N',\n",
              " '}',\n",
              " '\\\\sim',\n",
              " '\\\\pi^*',\n",
              " '\\\\mid',\n",
              " 's_0',\n",
              " ',',\n",
              " 'a_0',\n",
              " '}',\n",
              " '\\\\left',\n",
              " '[',\n",
              " '\\\\sum_',\n",
              " '{',\n",
              " 't=0',\n",
              " '}',\n",
              " '^',\n",
              " '{',\n",
              " 'N-1',\n",
              " '}',\n",
              " '\\\\gamma^',\n",
              " '{',\n",
              " 't',\n",
              " '}',\n",
              " 'r_t',\n",
              " '+',\n",
              " '\\\\gamma^N',\n",
              " '\\\\E_',\n",
              " '{',\n",
              " 's_N',\n",
              " '}',\n",
              " '\\\\max_',\n",
              " '{',\n",
              " 'a^*_N',\n",
              " '}',\n",
              " 'Q^*',\n",
              " '(',\n",
              " 's_N',\n",
              " ',',\n",
              " 'a^*_N',\n",
              " ')',\n",
              " '\\\\right',\n",
              " ']']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "## Standard Library\n",
        "import random\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "## External Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import gensim.downloader\n",
        "\n",
        "# PyTorch Modules: see http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "print(\"GPU Available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuDWqOgCq_Op",
        "outputId": "292da55a-d1d9-40e5-d81f-c61b4debaa91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "GPU Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appears to be how to work with tokenized latex..."
      ],
      "metadata": {
        "id": "NRWqBHfNsUL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model(**tokenizer(latex,return_tensors='pt',truncation=True))[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JQtdoWxk8JN",
        "outputId": "f23bd828-fe7b-4d96-d684-9be040cf6921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 125, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = tokenizer(latex,return_tensors='pt',truncation=True)\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzfqXM8IoHr9",
        "outputId": "06c76bf5-1e51-4341-d7af-84efce3ea9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,   165,  3107,   196,   151, 13894,  1643,  2346,  6451,  4060,\n",
              "          2064,  3991,  1399,   198,   154,   167,   115,   113,   188,   168,\n",
              "           121,   117,   170,   168,   121,   114,   134,   165,   142,   168,\n",
              "           196,   165,   157, 18663,   168,   196,   131,   151,   198,   165,\n",
              "         27466,  1306,   165,   185,  1182,   167,   115,   165,  2286,   188,\n",
              "           168,   121,   117,   170,   168,   121,   198,   165,  1286,   164,\n",
              "           165,  7584,   168,   196,   189,   134,   121,   198,   167,   196,\n",
              "           151,   118,   122,   198,   165, 21400,   167,   196,   189,   198,\n",
              "           187,   168,   189,   116,   165, 21400,   167,   151,   165,   142,\n",
              "           168,   196,   188,   168,   151,   198,   165, 12477,  1775,   168,\n",
              "           196,   170,   167,   115,   168,   151,   198,   154,   167,   115,\n",
              "           113,   188,   168,   151,   117,   170,   167,   115,   168,   151,\n",
              "           114,   165,  1268,   166,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveModel(nn.Module):\n",
        "  def __init__(self, embedding_model):\n",
        "    super(ContrastiveModel,self).__init__()\n",
        "\n",
        "    self.latex_encoder = embedding_model\n",
        "\n",
        "    self.latex_linear1 = nn.Linear(768,1024)\n",
        "    self.linear_comb1 = nn.Linear(1024,1)\n",
        "    # self.code_linear1 = nn.Linear(1024,900)\n",
        "    # self.code_linear2 = nn.Linear(900,768)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    # self.linear_comb1 = nn.Linear(768,256)\n",
        "    # self.linear_comb2 = nn.Linear(256,64)\n",
        "    # self.linear_comb3 = nn.Linear(64,1)\n",
        "    \n",
        "\n",
        "  def forward(self, embedded_functions, in_ids,atten_mask):\n",
        "      # out1 = self.code_linear1(embedded_functions)\n",
        "      # out2 = self.code_linear2(self.relu(out1))\n",
        "      # embedded_funcs = self.relu(out2)\n",
        "\n",
        "      embedded_latex = self.relu(self.latex_encoder(input_ids=in_ids,attention_mask=atten_mask)[0].mean(dim=1))\n",
        "      upscaled_latex = self.latex_linear1(embedded_latex)\n",
        "\n",
        "      combined_output = self.relu(embedded_functions*upscaled_latex)\n",
        "\n",
        "      return self.linear_comb1(combined_output)\n",
        "      # comb_out1 = self.relu(self.linear_comb1(combined_output))\n",
        "      # comb_out2 = self.relu(self.linear_comb2(comb_out1))\n",
        "      # comb_out3 = self.linear_comb3(comb_out2)\n",
        "      \n",
        "      # return comb_out3#.max(-2)"
      ],
      "metadata": {
        "id": "e_7GNTcLk_2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gdrive/MyDrive/RSS/Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzLHThU1Ci3n",
        "outputId": "a5e284a4-de99-401d-b6af-8dbd7a564ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "algorithms.csv\tcode.csv\t  code-small.csv  func-rep-small.csv\n",
            "code-all.csv\tcode-func-rep.pt  equations.csv   func-rep-small.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "func_rep_df = pd.read_pickle('gdrive/MyDrive/RSS/Data/code-func-rep.pt')"
      ],
      "metadata": {
        "id": "GFgpn_f9C1-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "algs.id.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ncicimvKlGcy",
        "outputId": "23bab9ff-a167-44a5-d392-e301d1c5105b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0          id  \\\n",
              "0              0  2201.02779   \n",
              "1              1  2201.03211   \n",
              "2              2  2201.03211   \n",
              "3              3  2201.03156   \n",
              "4              4  2201.03156   \n",
              "...          ...         ...   \n",
              "6785        1334  1806.03085   \n",
              "6786        1335  1709.04347   \n",
              "6787        1336  1709.04347   \n",
              "6788        1337  1711.01244   \n",
              "6789        1338  1711.01244   \n",
              "\n",
              "                                                    alg  \n",
              "0     [t!]\\n\\n \\SetKwFunction{FMain}{$DGLSegment$}\\n...  \n",
              "1     [ht]\\n\\begin{flushleft}\\n\\caption{Proposed NMC...  \n",
              "2     [tb]\\n\\begin{flushleft}\\n\\caption{Proposed NMC...  \n",
              "3     [H]\\n\\t\\begin{algorithmic}[1]\\n\\t\\t\\caption{Au...  \n",
              "4     [H]\\n\\t\\begin{algorithmic}[1]\\n\\t\\t\\caption{Tr...  \n",
              "...                                                 ...  \n",
              "6785  [H]\\n\\caption{One iteration of the Stein varia...  \n",
              "6786  \\n%\\t\\caption{\\\\Stacked Zoom Out-and-In Net wi...  \n",
              "6787  \\n%\\t\\caption{Zoom Out-and-In Network with Map...  \n",
              "6788  [H] \\n\\t\\caption{MLAP algorithm, meta-training...  \n",
              "6789  [H]\\n\\t\\caption{MLAP algorithm, meta-testing p...  \n",
              "\n",
              "[6790 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22f16ce6-a536-46ff-b802-f9a0f960ce1b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>alg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2201.02779</td>\n",
              "      <td>[t!]\\n\\n \\SetKwFunction{FMain}{$DGLSegment$}\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2201.03211</td>\n",
              "      <td>[ht]\\n\\begin{flushleft}\\n\\caption{Proposed NMC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2201.03211</td>\n",
              "      <td>[tb]\\n\\begin{flushleft}\\n\\caption{Proposed NMC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2201.03156</td>\n",
              "      <td>[H]\\n\\t\\begin{algorithmic}[1]\\n\\t\\t\\caption{Au...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2201.03156</td>\n",
              "      <td>[H]\\n\\t\\begin{algorithmic}[1]\\n\\t\\t\\caption{Tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6785</th>\n",
              "      <td>1334</td>\n",
              "      <td>1806.03085</td>\n",
              "      <td>[H]\\n\\caption{One iteration of the Stein varia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6786</th>\n",
              "      <td>1335</td>\n",
              "      <td>1709.04347</td>\n",
              "      <td>\\n%\\t\\caption{\\\\Stacked Zoom Out-and-In Net wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6787</th>\n",
              "      <td>1336</td>\n",
              "      <td>1709.04347</td>\n",
              "      <td>\\n%\\t\\caption{Zoom Out-and-In Network with Map...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6788</th>\n",
              "      <td>1337</td>\n",
              "      <td>1711.01244</td>\n",
              "      <td>[H] \\n\\t\\caption{MLAP algorithm, meta-training...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6789</th>\n",
              "      <td>1338</td>\n",
              "      <td>1711.01244</td>\n",
              "      <td>[H]\\n\\t\\caption{MLAP algorithm, meta-testing p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6790 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22f16ce6-a536-46ff-b802-f9a0f960ce1b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22f16ce6-a536-46ff-b802-f9a0f960ce1b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22f16ce6-a536-46ff-b802-f9a0f960ce1b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(algs.id.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NSQQ37olMD3",
        "outputId": "a78cbcb8-6d5f-4264-843f-68e52c57c591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3254"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(eqns.id.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC6hi_AplE9w",
        "outputId": "ad0d708c-3af6-44eb-e113-a39f1f7a32ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7472"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "func_rep_df = func_rep_df[func_rep_df.functions.apply(len) != 0]\n",
        "func_rep_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "MmYp0Fo8Jgh6",
        "outputId": "29c48688-8505-46a1-c5f9-e11e243e2ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               id                                          functions  \\\n",
              "0      1811.02182  [def main ( config ) : NEW_LINE INDENT from da...   \n",
              "1      1811.02182  [def check_config_used ( config , target_sourc...   \n",
              "2      1811.02182  [def __init__ ( self , batch_size , paired = F...   \n",
              "3      1811.02182  [def weights_init ( m ) : NEW_LINE INDENT clas...   \n",
              "4      1811.02182  [def weights_init ( m ) : NEW_LINE INDENT clas...   \n",
              "...           ...                                                ...   \n",
              "29193  2112.01073  [def my_lcs ( string , sub ) : NEW_LINE INDENT...   \n",
              "29194  2112.01073  [def __init__ ( self , n = 4 ) : NEW_LINE INDE...   \n",
              "29195  2112.01073  [def precook ( s , n = 4 , out = False ) : NEW...   \n",
              "29197  2112.01073  [def __init__ ( self ) : NEW_LINE INDENT self ...   \n",
              "29199  2112.01073  [def tokenize ( self , captions_for_image ) : ...   \n",
              "\n",
              "                                         representations  \n",
              "0      [[[tensor(0.6232), tensor(-0.2805), tensor(0.3...  \n",
              "1      [[[tensor(0.6108), tensor(-0.2560), tensor(0.1...  \n",
              "2      [[[tensor(0.8430), tensor(0.1034), tensor(0.30...  \n",
              "3      [[[tensor(0.6790), tensor(-0.0221), tensor(0.1...  \n",
              "4      [[[tensor(0.6790), tensor(-0.0221), tensor(0.1...  \n",
              "...                                                  ...  \n",
              "29193  [[[tensor(0.6292), tensor(0.0059), tensor(0.09...  \n",
              "29194  [[[tensor(0.4932), tensor(-0.1197), tensor(-0....  \n",
              "29195  [[[tensor(0.7558), tensor(-0.0280), tensor(0.0...  \n",
              "29197  [[[tensor(0.7251), tensor(-0.0360), tensor(0.0...  \n",
              "29199  [[[tensor(0.8905), tensor(-0.4485), tensor(0.2...  \n",
              "\n",
              "[23814 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74b865fa-1a85-4a68-9173-c0f70168429d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>functions</th>\n",
              "      <th>representations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def main ( config ) : NEW_LINE INDENT from da...</td>\n",
              "      <td>[[[tensor(0.6232), tensor(-0.2805), tensor(0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def check_config_used ( config , target_sourc...</td>\n",
              "      <td>[[[tensor(0.6108), tensor(-0.2560), tensor(0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def __init__ ( self , batch_size , paired = F...</td>\n",
              "      <td>[[[tensor(0.8430), tensor(0.1034), tensor(0.30...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def weights_init ( m ) : NEW_LINE INDENT clas...</td>\n",
              "      <td>[[[tensor(0.6790), tensor(-0.0221), tensor(0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def weights_init ( m ) : NEW_LINE INDENT clas...</td>\n",
              "      <td>[[[tensor(0.6790), tensor(-0.0221), tensor(0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29193</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def my_lcs ( string , sub ) : NEW_LINE INDENT...</td>\n",
              "      <td>[[[tensor(0.6292), tensor(0.0059), tensor(0.09...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29194</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def __init__ ( self , n = 4 ) : NEW_LINE INDE...</td>\n",
              "      <td>[[[tensor(0.4932), tensor(-0.1197), tensor(-0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29195</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def precook ( s , n = 4 , out = False ) : NEW...</td>\n",
              "      <td>[[[tensor(0.7558), tensor(-0.0280), tensor(0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29197</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def __init__ ( self ) : NEW_LINE INDENT self ...</td>\n",
              "      <td>[[[tensor(0.7251), tensor(-0.0360), tensor(0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29199</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def tokenize ( self , captions_for_image ) : ...</td>\n",
              "      <td>[[[tensor(0.8905), tensor(-0.4485), tensor(0.2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23814 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74b865fa-1a85-4a68-9173-c0f70168429d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-74b865fa-1a85-4a68-9173-c0f70168429d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-74b865fa-1a85-4a68-9173-c0f70168429d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eqns['id'] = eqns.id.astype(str)\n",
        "func_rep_df['id'] = func_rep_df['id'].astype(str)\n",
        "collected_func_reps = func_rep_df.groupby(by='id').sum()"
      ],
      "metadata": {
        "id": "ujFld01NTNx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_rep_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "qpjSXqSFkc17",
        "outputId": "f5c6a253-8774-4805-e2c1-79b6b293443a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               id                                          functions  \\\n",
              "0      1811.02182  [def main ( config ) : NEW_LINE INDENT from da...   \n",
              "1      1811.02182  [def check_config_used ( config , target_sourc...   \n",
              "2      1811.02182  [def __init__ ( self , batch_size , paired = F...   \n",
              "3      1811.02182  [def weights_init ( m ) : NEW_LINE INDENT clas...   \n",
              "4      1811.02182  [def weights_init ( m ) : NEW_LINE INDENT clas...   \n",
              "...           ...                                                ...   \n",
              "29193  2112.01073  [def my_lcs ( string , sub ) : NEW_LINE INDENT...   \n",
              "29194  2112.01073  [def __init__ ( self , n = 4 ) : NEW_LINE INDE...   \n",
              "29195  2112.01073  [def precook ( s , n = 4 , out = False ) : NEW...   \n",
              "29197  2112.01073  [def __init__ ( self ) : NEW_LINE INDENT self ...   \n",
              "29199  2112.01073  [def tokenize ( self , captions_for_image ) : ...   \n",
              "\n",
              "                                         representations  \n",
              "0      [[[tensor(0.6232), tensor(-0.2805), tensor(0.3...  \n",
              "1      [[[tensor(0.6108), tensor(-0.2560), tensor(0.1...  \n",
              "2      [[[tensor(0.8430), tensor(0.1034), tensor(0.30...  \n",
              "3      [[[tensor(0.6790), tensor(-0.0221), tensor(0.1...  \n",
              "4      [[[tensor(0.6790), tensor(-0.0221), tensor(0.1...  \n",
              "...                                                  ...  \n",
              "29193  [[[tensor(0.6292), tensor(0.0059), tensor(0.09...  \n",
              "29194  [[[tensor(0.4932), tensor(-0.1197), tensor(-0....  \n",
              "29195  [[[tensor(0.7558), tensor(-0.0280), tensor(0.0...  \n",
              "29197  [[[tensor(0.7251), tensor(-0.0360), tensor(0.0...  \n",
              "29199  [[[tensor(0.8905), tensor(-0.4485), tensor(0.2...  \n",
              "\n",
              "[23814 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6de3914b-180f-41b5-ae94-de0964a63150\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>functions</th>\n",
              "      <th>representations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def main ( config ) : NEW_LINE INDENT from da...</td>\n",
              "      <td>[[[tensor(0.6232), tensor(-0.2805), tensor(0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def check_config_used ( config , target_sourc...</td>\n",
              "      <td>[[[tensor(0.6108), tensor(-0.2560), tensor(0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def __init__ ( self , batch_size , paired = F...</td>\n",
              "      <td>[[[tensor(0.8430), tensor(0.1034), tensor(0.30...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def weights_init ( m ) : NEW_LINE INDENT clas...</td>\n",
              "      <td>[[[tensor(0.6790), tensor(-0.0221), tensor(0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1811.02182</td>\n",
              "      <td>[def weights_init ( m ) : NEW_LINE INDENT clas...</td>\n",
              "      <td>[[[tensor(0.6790), tensor(-0.0221), tensor(0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29193</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def my_lcs ( string , sub ) : NEW_LINE INDENT...</td>\n",
              "      <td>[[[tensor(0.6292), tensor(0.0059), tensor(0.09...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29194</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def __init__ ( self , n = 4 ) : NEW_LINE INDE...</td>\n",
              "      <td>[[[tensor(0.4932), tensor(-0.1197), tensor(-0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29195</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def precook ( s , n = 4 , out = False ) : NEW...</td>\n",
              "      <td>[[[tensor(0.7558), tensor(-0.0280), tensor(0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29197</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def __init__ ( self ) : NEW_LINE INDENT self ...</td>\n",
              "      <td>[[[tensor(0.7251), tensor(-0.0360), tensor(0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29199</th>\n",
              "      <td>2112.01073</td>\n",
              "      <td>[def tokenize ( self , captions_for_image ) : ...</td>\n",
              "      <td>[[[tensor(0.8905), tensor(-0.4485), tensor(0.2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23814 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6de3914b-180f-41b5-ae94-de0964a63150')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6de3914b-180f-41b5-ae94-de0964a63150 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6de3914b-180f-41b5-ae94-de0964a63150');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collected_func_reps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "8m4Xv1Gjltc2",
        "outputId": "cbf820fa-452c-42ca-9ef2-5a0127331a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    functions  \\\n",
              "id                                                              \n",
              "1010.1763   [def robust_nmf ( data , rank , beta , init , ...   \n",
              "1110.2997   [def loglikelihood ( theta ) : NEW_LINE INDENT...   \n",
              "1212.6094   [def main ( ) : NEW_LINE INDENT cleaned_model_...   \n",
              "1604.02071  [def __init__ ( self , products_set_path , rat...   \n",
              "1711.05114  [def exe_time ( func ) : NEW_LINE INDENT def n...   \n",
              "...                                                       ...   \n",
              "2112.01529  [def readme ( ) : NEW_LINE INDENT with open ( ...   \n",
              "2112.0153   [def main ( opt ) : NEW_LINE INDENT uvs = [ jo...   \n",
              "711.0962    [def __init__ ( self , name = None , opts = No...   \n",
              "805.2366    [def deriveNameKey ( surname , first , builder...   \n",
              "809.1092    [def dpssFast ( N , W , K ) : NEW_LINE INDENT ...   \n",
              "\n",
              "                                              representations  \n",
              "id                                                             \n",
              "1010.1763   [[[tensor(0.8621), tensor(-0.1790), tensor(0.4...  \n",
              "1110.2997   [[[tensor(0.8383), tensor(0.0016), tensor(0.13...  \n",
              "1212.6094   [[[tensor(0.5931), tensor(-0.0021), tensor(0.3...  \n",
              "1604.02071  [[[tensor(0.3454), tensor(-0.0555), tensor(-0....  \n",
              "1711.05114  [[[tensor(0.6678), tensor(-0.0102), tensor(0.2...  \n",
              "...                                                       ...  \n",
              "2112.01529  [[[tensor(0.7760), tensor(-0.0030), tensor(0.1...  \n",
              "2112.0153   [[[tensor(0.5144), tensor(-0.2040), tensor(0.2...  \n",
              "711.0962    [[[tensor(0.6273), tensor(-0.0820), tensor(0.2...  \n",
              "805.2366    [[[tensor(0.7622), tensor(-0.3449), tensor(0.4...  \n",
              "809.1092    [[[tensor(0.6523), tensor(0.2216), tensor(-0.0...  \n",
              "\n",
              "[777 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d56b999c-f636-4823-8339-1c55ee4484b1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>functions</th>\n",
              "      <th>representations</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1010.1763</th>\n",
              "      <td>[def robust_nmf ( data , rank , beta , init , ...</td>\n",
              "      <td>[[[tensor(0.8621), tensor(-0.1790), tensor(0.4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1110.2997</th>\n",
              "      <td>[def loglikelihood ( theta ) : NEW_LINE INDENT...</td>\n",
              "      <td>[[[tensor(0.8383), tensor(0.0016), tensor(0.13...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1212.6094</th>\n",
              "      <td>[def main ( ) : NEW_LINE INDENT cleaned_model_...</td>\n",
              "      <td>[[[tensor(0.5931), tensor(-0.0021), tensor(0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1604.02071</th>\n",
              "      <td>[def __init__ ( self , products_set_path , rat...</td>\n",
              "      <td>[[[tensor(0.3454), tensor(-0.0555), tensor(-0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1711.05114</th>\n",
              "      <td>[def exe_time ( func ) : NEW_LINE INDENT def n...</td>\n",
              "      <td>[[[tensor(0.6678), tensor(-0.0102), tensor(0.2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2112.01529</th>\n",
              "      <td>[def readme ( ) : NEW_LINE INDENT with open ( ...</td>\n",
              "      <td>[[[tensor(0.7760), tensor(-0.0030), tensor(0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2112.0153</th>\n",
              "      <td>[def main ( opt ) : NEW_LINE INDENT uvs = [ jo...</td>\n",
              "      <td>[[[tensor(0.5144), tensor(-0.2040), tensor(0.2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>711.0962</th>\n",
              "      <td>[def __init__ ( self , name = None , opts = No...</td>\n",
              "      <td>[[[tensor(0.6273), tensor(-0.0820), tensor(0.2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>805.2366</th>\n",
              "      <td>[def deriveNameKey ( surname , first , builder...</td>\n",
              "      <td>[[[tensor(0.7622), tensor(-0.3449), tensor(0.4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>809.1092</th>\n",
              "      <td>[def dpssFast ( N , W , K ) : NEW_LINE INDENT ...</td>\n",
              "      <td>[[[tensor(0.6523), tensor(0.2216), tensor(-0.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>777 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d56b999c-f636-4823-8339-1c55ee4484b1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d56b999c-f636-4823-8339-1c55ee4484b1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d56b999c-f636-4823-8339-1c55ee4484b1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "23814/777"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bDNYwjmkirf",
        "outputId": "1dff6841-b29a-401b-c296-81f6ebec95da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.64864864864865"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(collected_func_reps.functions.apply(len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbWd5VBeT74V",
        "outputId": "172cdbd7-c357-4c6b-dcc1-23ed69dd5ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "242.06177606177607"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demofuncs = collected_func_reps[collected_func_reps.index=='1811.02182']\n",
        "demofuncs.functions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P73AE7lXZpK",
        "outputId": "97f3d7e5-5a10-44bc-c3bb-e40b8c945887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "1811.02182    [def main ( config ) : NEW_LINE INDENT from da...\n",
              "Name: functions, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demofuncs.functions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woNBKrDPYgGl",
        "outputId": "6989f3d9-1bfe-43fe-eab7-65d41abb0059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"def main ( config ) : NEW_LINE INDENT from data_loader import DataLoader NEW_LINE if ( config . trainer == ' minimize _ DCE ' ) : NEW_LINE INDENT from trainer_DCE import Trainer NEW_LINE paired = True NEW_LINE DEDENT elif ( config . trainer == ' acoustic _ supervision ' ) : NEW_LINE INDENT from trainer_acoustic import Trainer NEW_LINE paired = False NEW_LINE DEDENT elif ( config . trainer == ' AAS ' ) : NEW_LINE INDENT from trainer_AAS import Trainer NEW_LINE paired = False NEW_LINE DEDENT elif ( config . trainer == ' FSEGAN ' ) : NEW_LINE INDENT from trainer_FSEGAN import Trainer NEW_LINE paired = True NEW_LINE DEDENT if config . gpu >= 0 : NEW_LINE INDENT torch . cuda . manual_seed ( config . random_seed ) NEW_LINE torch . cuda . set_device ( config . gpu ) NEW_LINE DEDENT if ( config . DB_name == ' librispeech ' ) : NEW_LINE INDENT if ( paired ) : NEW_LINE INDENT config . tr_ny_manifest = ' data / libri _ tr _ ny _ paired . csv ' NEW_LINE config . trsub_manifest = ' data / libri _ trsub _ ny _ paired . csv ' NEW_LINE config . val_manifest = ' data / libri _ val _ paired . csv ' NEW_LINE DEDENT else : NEW_LINE INDENT config . tr_ny_manifest = ' data / libri _ tr _ ny . csv ' NEW_LINE config . trsub_manifest = ' data / libri _ trsub _ ny . csv ' NEW_LINE config . val_manifest = ' data / libri _ val . csv ' NEW_LINE DEDENT config . tr_cl_manifest = ' data / libri _ tr _ cl . csv ' NEW_LINE DEDENT elif ( config . DB_name == ' chime ' ) : NEW_LINE INDENT if ( paired ) : NEW_LINE INDENT config . tr_ny_manifest = ' data / chime _ ' + config . simul_real + ' _ tr _ ny _ paired . csv ' NEW_LINE config . trsub_manifest = ' data / chime _ ' + config . simul_real + ' _ trsub _ ny _ paired . csv ' NEW_LINE config . val_manifest = ' data / chime _ real _ val _ paired . csv ' NEW_LINE confnig . val2_manifest = ' data / chime _ simul _ val _ paired . csv ' NEW_LINE DEDENT else : NEW_LINE INDENT config . tr_ny_manifest = ' data / chime _ ' + config . simul_real + ' _ tr _ ny . csv ' NEW_LINE config . trsub_manifest = ' data / chime _ ' + config . simul_real + ' _ trsub _ ny . csv ' NEW_LINE config . val_manifest = ' data / chime _ real _ val . csv ' NEW_LINE confnig . val2_manifest = ' data / chime _ simul _ val . csv ' NEW_LINE DEDENT config . tr_cl_manifest = ' data / chime _ tr _ org . csv ' NEW_LINE DEDENT with open ( config . labels_path ) as label_file : NEW_LINE INDENT labels = str ( ' ' . join ( json . load ( label_file ) ) ) NEW_LINE DEDENT data_loader = DataLoader ( batch_size = config . batch_size , paired = paired , tr_cl_manifest = config . tr_cl_manifest , tr_ny_manifest = config . tr_ny_manifest , trsub_manifest = config . trsub_manifest , val_manifest = config . val_manifest , val2_manifest = config . val2_manifest , labels = labels ) NEW_LINE if not os . path . exists ( ' logs / ' + str ( config . expnum ) ) : NEW_LINE INDENT os . makedirs ( ' logs / ' + str ( config . expnum ) ) NEW_LINE DEDENT trainer = Trainer ( config , data_loader ) NEW_LINE torch . manual_seed ( config . random_seed ) NEW_LINE if ( config . mode == ' train ' ) : NEW_LINE INDENT trainer . train ( ) NEW_LINE DEDENT elif ( config . mode == ' test ' ) : NEW_LINE INDENT trainer . test ( ) NEW_LINE DEDENT elif ( config . mode == ' visualize ' ) : NEW_LINE INDENT trainer . visualize ( ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def check_config_used ( config , target_source ) : NEW_LINE INDENT config_count = dict ( vars ( config ) ) NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT config_count [ k ] = 0 NEW_LINE DEDENT for source in target_source : NEW_LINE INDENT fp = open ( source , ' r ' ) NEW_LINE text = fp . read ( ) NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT if ( text . find ( k ) >= 0 ) : NEW_LINE INDENT config_count [ k ] = 1 NEW_LINE DEDENT DEDENT fp . close ( ) NEW_LINE DEDENT config_unused = [ ] NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT if ( config_count [ k ] == 0 ) : NEW_LINE INDENT config_unused . append ( k ) NEW_LINE DEDENT DEDENT print ( ' unused ▁ config ▁ = ▁ ' ) NEW_LINE print ( config_unused ) NEW_LINE assert ( len ( config_unused ) == 0 ) , ' unused ▁ config ▁ exists , ▁ please ▁ properly ▁ use ▁ it ▁ or ▁ comment ▁ it ' NEW_LINE DEDENT\",\n",
              " 'def to_np ( x ) : NEW_LINE INDENT return x . data . cpu ( ) . numpy ( ) NEW_LINE DEDENT',\n",
              " \"def get_weight_statistic ( M ) : NEW_LINE INDENT print ( ' Model ▁ parameter ▁ statistic ' ) NEW_LINE modules = list ( M . modules ( ) ) [ 0 ] . _modules NEW_LINE for k , v in modules . items ( ) : NEW_LINE INDENT if ( len ( v . state_dict ( ) ) > 2 ) : NEW_LINE INDENT for l in range ( len ( v ) ) : NEW_LINE INDENT layer = v [ l ] NEW_LINE if ( hasattr ( layer , ' module ' ) ) : NEW_LINE INDENT layer_m = layer . module NEW_LINE for j in range ( len ( layer_m ) ) : NEW_LINE INDENT sublayer = layer_m [ j ] NEW_LINE if ( hasattr ( sublayer , ' bias ' ) ) : NEW_LINE INDENT if ( sublayer . bias is not None ) : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( sublayer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT else : NEW_LINE INDENT if ( hasattr ( layer , ' weight ' ) ) : NEW_LINE INDENT if ( hasattr ( layer , ' bias ' ) ) : NEW_LINE INDENT if ( hasattr ( layer . bias , ' data ' ) ) : NEW_LINE INDENT print ( str ( layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( layer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT print ( str ( layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT if ( hasattr ( layer , ' rnn ' ) ) : NEW_LINE INDENT rnn_layer = layer . rnn NEW_LINE print ( str ( rnn_layer ) ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0 . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0 . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0 . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 _ reverse ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0_reverse . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ ih _ l0 ▁ = ▁ ' + str ( rnn_layer . weight_ih_l0 . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_ih_l0 . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_ih_l0 . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 _ reverse ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0_reverse . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT if ( hasattr ( layer , ' batch _ norm ' ) ) : NEW_LINE INDENT if ( layer . batch_norm ) : NEW_LINE INDENT bn_layer = layer . batch_norm . module NEW_LINE print ( str ( bn_layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( bn_layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( bn_layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( bn_layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) , ▁ bias ▁ = ▁ ' + str ( bn_layer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( bn_layer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( bn_layer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT DEDENT DEDENT else : NEW_LINE INDENT if ( hasattr ( v , ' weight ' ) ) : NEW_LINE INDENT if ( hasattr ( v , ' bias ' ) ) : NEW_LINE INDENT print ( k + ' ▁ : ▁ weight ▁ = ▁ ' + str ( v . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( v . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( k + ' ▁ : ▁ weight ▁ = ▁ ' + str ( v . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT print ( ' ▁ ' ) NEW_LINE print ( ' ▁ ' ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 and classname . find ( ' ConvResidualBlock ' ) == - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.1 ) NEW_LINE if hasattr ( m , ' bias ' ) : NEW_LINE INDENT if ( hasattr ( m . bias , ' data ' ) ) : NEW_LINE INDENT m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT DEDENT DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def _get_variable ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def _get_variable_volatile ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) , volatile = True ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs , volatile = True ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def _get_variable_nograd ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) , requires_grad = False ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs , requires_grad = False ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def __init__ ( self ) : NEW_LINE INDENT self . reset ( ) NEW_LINE DEDENT',\n",
              " 'def reset ( self ) : NEW_LINE INDENT self . val = 0 NEW_LINE self . avg = 0 NEW_LINE self . sum = 0 NEW_LINE self . count = 0 NEW_LINE DEDENT',\n",
              " 'def update ( self , val , n = 1 ) : NEW_LINE INDENT self . val = val NEW_LINE self . sum += val * n NEW_LINE self . count += n NEW_LINE self . avg = self . sum / self . count NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , batch_size , paired = False , tr_cl_manifest = \" \" , tr_ny_manifest = \" \" , trsub_manifest = \" \" , val_manifest = \" \" , val2_manifest = \" \" , labels = None ) : NEW_LINE INDENT self . batch_size = batch_size NEW_LINE self . labels = labels NEW_LINE if ( paired ) : NEW_LINE INDENT self . Loader = FeatLoader_paired NEW_LINE DEDENT else : NEW_LINE INDENT self . Loader = FeatLoader NEW_LINE DEDENT if ( len ( tr_cl_manifest ) > 0 ) : NEW_LINE INDENT self . tr_cl_ds = FeatDataset ( manifest = tr_cl_manifest , labels = labels ) NEW_LINE self . tr_cl_sp = FeatSampler ( self . tr_cl_ds , batch_size = batch_size ) NEW_LINE self . tr_cl_dl = iter ( self . Loader ( self . tr_cl_ds , num_workers = 1 , batch_sampler = self . tr_cl_sp ) ) NEW_LINE DEDENT if ( len ( tr_ny_manifest ) > 0 ) : NEW_LINE INDENT self . tr_ny_ds = FeatDataset ( manifest = tr_ny_manifest , labels = labels ) NEW_LINE self . tr_ny_sp = FeatSampler ( self . tr_ny_ds , batch_size = batch_size ) NEW_LINE self . tr_ny_dl = iter ( self . Loader ( self . tr_ny_ds , num_workers = 1 , batch_sampler = self . tr_ny_sp ) ) NEW_LINE DEDENT if ( len ( trsub_manifest ) > 0 ) : NEW_LINE INDENT self . trsub_ds = FeatDataset ( manifest = trsub_manifest , labels = labels ) NEW_LINE self . trsub_dl = iter ( self . Loader ( self . trsub_ds , batch_size = batch_size ) ) NEW_LINE DEDENT if ( len ( val_manifest ) > 0 ) : NEW_LINE INDENT self . val_ds = FeatDataset ( manifest = val_manifest , labels = labels ) NEW_LINE self . val_dl = iter ( self . Loader ( self . val_ds , batch_size = batch_size ) ) NEW_LINE DEDENT if ( len ( val2_manifest ) > 0 ) : NEW_LINE INDENT self . val2_ds = FeatDataset ( manifest = val2_manifest , labels = labels ) NEW_LINE self . val2_dl = iter ( self . Loader ( self . val2_ds , batch_size = batch_size ) ) NEW_LINE DEDENT DEDENT',\n",
              " \"def next ( self , cl_ny = ' ' , type = ' ' ) : NEW_LINE INDENT if ( cl_ny == ' ny ' ) : NEW_LINE INDENT if ( type == ' train ' ) : NEW_LINE INDENT loader = self . tr_ny_dl NEW_LINE DEDENT elif ( type == ' trsub ' ) : NEW_LINE INDENT loader = self . trsub_dl NEW_LINE DEDENT elif ( type == ' val ' ) : NEW_LINE INDENT loader = self . val_dl NEW_LINE DEDENT elif ( type == ' val2' ) : NEW_LINE INDENT loader = self . val2_dl NEW_LINE DEDENT DEDENT elif ( cl_ny == ' cl ' ) : NEW_LINE INDENT if ( type == ' train ' ) : NEW_LINE INDENT loader = self . tr_cl_dl NEW_LINE DEDENT DEDENT try : NEW_LINE INDENT data_list = loader . next ( ) NEW_LINE DEDENT except StopIteration : NEW_LINE INDENT if ( cl_ny == ' ny ' ) : NEW_LINE INDENT if ( type == ' train ' ) : NEW_LINE INDENT self . tr_ny_sp . shuffle ( ) NEW_LINE self . tr_ny_dl = iter ( self . Loader ( self . tr_ny_ds , num_workers = 1 , batch_sampler = self . tr_ny_sp ) ) NEW_LINE loader = self . tr_ny_dl NEW_LINE DEDENT elif ( type == ' trsub ' ) : NEW_LINE INDENT self . trsub_dl = iter ( self . Loader ( self . trsub_ds , batch_size = self . batch_size , num_workers = 1 ) ) NEW_LINE loader = self . trsub_dl NEW_LINE DEDENT elif ( type == ' val ' ) : NEW_LINE INDENT self . val_dl = iter ( self . Loader ( self . val_ds , batch_size = self . batch_size , num_workers = 1 ) ) NEW_LINE loader = self . val_dl NEW_LINE DEDENT elif ( type == ' val2' ) : NEW_LINE INDENT self . val2_dl = iter ( self . Loader ( self . val2_ds , batch_size = self . batch_size , num_workers = 1 ) ) NEW_LINE loader = self . val2_dl NEW_LINE loader = self . te_dl NEW_LINE DEDENT DEDENT elif ( cl_ny == ' cl ' ) : NEW_LINE INDENT if ( type == ' train ' ) : NEW_LINE INDENT self . tr_cl_sp . shuffle ( ) NEW_LINE self . tr_cl_dl = iter ( self . Loader ( self . tr_cl_ds , num_workers = 1 , batch_sampler = self . tr_cl_sp ) ) NEW_LINE loader = self . tr_cl_dl NEW_LINE DEDENT DEDENT data_list = loader . next ( ) NEW_LINE DEDENT return data_list NEW_LINE DEDENT\",\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def __init__ ( self , config , data_loader = None ) : NEW_LINE INDENT self . config = config NEW_LINE self . data_loader = data_loader NEW_LINE self . lr = config . lr NEW_LINE self . beta1 = config . beta1 NEW_LINE self . beta2 = config . beta2 NEW_LINE self . optimizer = config . optimizer NEW_LINE self . batch_size = config . batch_size NEW_LINE self . diffLoss = L1Loss_mask ( ) NEW_LINE self . valmin_iter = 0 NEW_LINE self . model_dir = ' logs / ' + str ( config . expnum ) NEW_LINE self . savename_G = ' ' NEW_LINE self . decoder = GreedyDecoder ( data_loader . labels ) NEW_LINE self . kt = 0 NEW_LINE self . lb = 0.001 NEW_LINE self . conv_measure = 0 NEW_LINE self . dce_tr = AverageMeter ( ) NEW_LINE self . dce_val = AverageMeter ( ) NEW_LINE self . wer_tr = AverageMeter ( ) NEW_LINE self . cer_tr = AverageMeter ( ) NEW_LINE self . wer_val = AverageMeter ( ) NEW_LINE self . cer_val = AverageMeter ( ) NEW_LINE self . build_model ( ) NEW_LINE self . G . loss_stop = 100000 NEW_LINE if self . config . gpu >= 0 : NEW_LINE INDENT self . G . cuda ( ) NEW_LINE self . ASR . cuda ( ) NEW_LINE DEDENT if len ( self . config . load_path ) > 0 : NEW_LINE INDENT self . load_model ( ) NEW_LINE DEDENT if config . mode == ' train ' : NEW_LINE INDENT self . logFile = open ( self . model_dir + ' / log . txt ' , ' w ' ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def zero_grad_all ( self ) : NEW_LINE INDENT self . G . zero_grad ( ) NEW_LINE DEDENT',\n",
              " \"def build_model ( self ) : NEW_LINE INDENT print ( ' initialize ▁ enhancement ▁ model ' ) NEW_LINE self . G = stackedBRNN ( I = self . config . nFeat , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE print ( ' load ▁ pre - trained ▁ ASR ▁ model ' ) NEW_LINE package_ASR = torch . load ( self . config . ASR_path , map_location = lambda storage , loc : storage ) NEW_LINE self . ASR = DeepSpeech . load_model_package ( package_ASR ) NEW_LINE DEDENT\",\n",
              " 'def load_model ( self ) : NEW_LINE INDENT print ( \" [ * ] ▁ Load ▁ models ▁ from ▁ { } . . . \" . format ( self . load_path ) ) NEW_LINE postfix = \\' _ valmin \\' NEW_LINE paths = glob ( os . path . join ( self . load_path , \\' G { } * . pth \\' . format ( postfix ) ) ) NEW_LINE paths . sort ( ) NEW_LINE if len ( paths ) == 0 : NEW_LINE INDENT print ( \" [ ! ] ▁ No ▁ checkpoint ▁ found ▁ in ▁ { } . . . \" . format ( self . load_path ) ) NEW_LINE assert ( 0 ) , \\' checkpoint ▁ not ▁ avilable \\' NEW_LINE DEDENT idxes = [ int ( os . path . basename ( path . split ( \\' . \\' ) [ 0 ] . split ( \\' _ \\' ) [ - 1 ] ) ) for path in paths ] NEW_LINE if self . config . start_iter < 0 : NEW_LINE INDENT self . config . start_iter = max ( idxes ) NEW_LINE if ( self . config . start_iter < 0 ) : NEW_LINE INDENT raise Exception ( \" start ▁ iter ▁ is ▁ still ▁ less ▁ than ▁ 0 ▁ - - > ▁ probably ▁ try ▁ to ▁ load ▁ initial ▁ random ▁ model \" ) NEW_LINE DEDENT DEDENT if self . config . gpu < 0 : NEW_LINE INDENT map_location = lambda storage , loc : storage NEW_LINE DEDENT else : NEW_LINE INDENT map_location = None NEW_LINE DEDENT print ( \\' Load ▁ models ▁ from ▁ \\' + self . load_path + \\' , ▁ ITERATION ▁ = ▁ \\' + str ( self . config . start_iter ) ) NEW_LINE self . G . load_state_dict ( torch . load ( \\' { } / G { } _ { } . pth \\' . format ( self . load_path [ : - 1 ] , postfix , self . config . start_iter ) , map_location = map_location ) ) NEW_LINE print ( \" [ * ] ▁ Model ▁ loaded \" ) NEW_LINE DEDENT',\n",
              " 'def train ( self ) : NEW_LINE INDENT optimizer_g = torch . optim . Adam ( self . G . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE for iter in trange ( self . config . start_iter , self . config . max_iter ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' train \\' ) NEW_LINE inputs , cleans , mask = _get_variable_nograd ( data_list [ 0 ] ) , _get_variable_nograd ( data_list [ 1 ] ) , _get_variable_nograd ( data_list [ 2 ] ) NEW_LINE outputs = self . G ( inputs ) NEW_LINE dce , nElement = self . diffLoss ( outputs , cleans , mask ) NEW_LINE self . zero_grad_all ( ) NEW_LINE dce . backward ( ) NEW_LINE optimizer_g . step ( ) NEW_LINE if ( iter + 1 ) % self . config . log_iter == 0 : NEW_LINE INDENT str_loss = \" [ { } / { } ] ▁ ( train ) ▁ DCE : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , dce . data [ 0 ] ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE DEDENT if ( iter + 1 ) % self . config . save_iter == 0 : NEW_LINE INDENT self . G . eval ( ) NEW_LINE self . dce_tr . reset ( ) NEW_LINE self . wer_tr . reset ( ) NEW_LINE self . cer_tr . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . trsub_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' trsub \\' ) NEW_LINE inputs , cleans , mask , targets , input_percentages , target_sizes = _get_variable_volatile ( data_list [ 0 ] ) , _get_variable_volatile ( data_list [ 1 ] ) , _get_variable_volatile ( data_list [ 2 ] ) , data_list [ 3 ] , data_list [ 4 ] , data_list [ 5 ] NEW_LINE outputs = self . G ( inputs ) NEW_LINE dce , nElement = self . diffLoss ( outputs , cleans , mask ) NEW_LINE self . dce_tr . update ( dce . data [ 0 ] , nElement ) NEW_LINE wer , cer , nWord , nChar = self . greedy_decoding ( inputs , targets , input_percentages , target_sizes ) NEW_LINE self . wer_tr . update ( wer , nWord ) NEW_LINE self . cer_tr . update ( cer , nChar ) NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( training ▁ subset ) ▁ DCE : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . dce_tr . avg ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE str_loss = \" [ { } / { } ] ▁ ( training ▁ subset ) ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . wer_tr . avg * 100 , self . cer_tr . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . dce_val . reset ( ) NEW_LINE self . wer_val . reset ( ) NEW_LINE self . cer_val . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . val_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' val \\' ) NEW_LINE inputs , cleans , mask , targets , input_percentages , target_sizes = _get_variable_volatile ( data_list [ 0 ] ) , _get_variable_volatile ( data_list [ 1 ] ) , _get_variable_volatile ( data_list [ 2 ] ) , data_list [ 3 ] , data_list [ 4 ] , data_list [ 5 ] NEW_LINE outputs = self . G ( inputs ) NEW_LINE dce , nElement = self . diffLoss ( outputs , cleans , mask ) NEW_LINE self . dce_val . update ( dce . data [ 0 ] , nElement ) NEW_LINE wer , cer , nWord , nChar = self . greedy_decoding ( inputs , targets , input_percentages , target_sizes ) NEW_LINE self . wer_val . update ( wer , nWord ) NEW_LINE self . cer_val . update ( cer , nChar ) NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( validation ) ▁ DCE : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . dce_val . avg ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE str_loss = \" [ { } / { } ] ▁ ( validation ) ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . wer_val . avg * 100 , self . cer_val . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . G . train ( ) NEW_LINE self . logFile . flush ( ) NEW_LINE if ( len ( self . savename_G ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_G ) : NEW_LINE INDENT os . remove ( self . savename_G ) NEW_LINE DEDENT DEDENT self . savename_G = \\' { } / G _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . G . state_dict ( ) , self . savename_G ) NEW_LINE if ( self . G . loss_stop > self . wer_val . avg ) : NEW_LINE INDENT self . G . loss_stop = self . wer_val . avg NEW_LINE savename_G_valmin_prev = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_G_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_G_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_G_valmin = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_G , savename_G_valmin ) NEW_LINE self . valmin_iter = iter NEW_LINE DEDENT DEDENT DEDENT DEDENT',\n",
              " \"def greedy_decoding ( self , inputs , targets , input_percentages , target_sizes , transcript_prob = 0.001 ) : NEW_LINE INDENT split_targets = [ ] NEW_LINE offset = 0 NEW_LINE for size in target_sizes : NEW_LINE INDENT split_targets . append ( targets [ offset : offset + size ] ) NEW_LINE offset += size NEW_LINE DEDENT enhanced = self . G ( inputs ) NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = input_percentages . mul_ ( int ( T ) ) . int ( ) NEW_LINE decoded_output , _ = self . decoder . decode ( prob . data , sizes ) NEW_LINE target_strings = self . decoder . convert_to_strings ( split_targets ) NEW_LINE we , ce , total_word , total_char = 0 , 0 , 0 , 0 NEW_LINE for x in range ( len ( target_strings ) ) : NEW_LINE INDENT decoding , reference = decoded_output [ x ] [ 0 ] , target_strings [ x ] [ 0 ] NEW_LINE nChar = len ( reference ) NEW_LINE nWord = len ( reference . split ( ) ) NEW_LINE we_i = self . decoder . wer ( decoding , reference ) NEW_LINE ce_i = self . decoder . cer ( decoding , reference ) NEW_LINE we += we_i NEW_LINE ce += ce_i NEW_LINE total_word += nWord NEW_LINE total_char += nChar NEW_LINE if ( random . uniform ( 0 , 1 ) < transcript_prob ) : NEW_LINE INDENT print ( ' reference ▁ = ▁ ' + reference ) NEW_LINE print ( ' decoding ▁ = ▁ ' + decoding ) NEW_LINE print ( ' wer ▁ = ▁ ' + str ( we_i / float ( nWord ) ) + ' , ▁ cer ▁ = ▁ ' + str ( ce_i / float ( nChar ) ) ) NEW_LINE DEDENT DEDENT wer = we / total_word NEW_LINE cer = ce / total_word NEW_LINE return wer , cer , total_word , total_char NEW_LINE DEDENT\",\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def __init__ ( self , config , data_loader = None ) : NEW_LINE INDENT self . config = config NEW_LINE self . data_loader = data_loader NEW_LINE self . lr = config . lr NEW_LINE self . beta1 = config . beta1 NEW_LINE self . beta2 = config . beta2 NEW_LINE self . optimizer = config . optimizer NEW_LINE self . batch_size = config . batch_size NEW_LINE self . diffLoss = L1Loss_mask ( ) NEW_LINE self . valmin_iter = 0 NEW_LINE self . model_dir = ' logs / ' + str ( config . expnum ) NEW_LINE self . savename_G = ' ' NEW_LINE self . savename_D = ' ' NEW_LINE self . savename_ASR = ' ' NEW_LINE self . kt = 0 NEW_LINE self . lb = self . config . lambda_k NEW_LINE self . gamma = self . config . gamma NEW_LINE self . conv_measure = 0 NEW_LINE self . ctc_tr = AverageMeter ( ) NEW_LINE self . ctc_tr_local = AverageMeter ( ) NEW_LINE self . ctc_val = AverageMeter ( ) NEW_LINE self . adv_ny_tr = AverageMeter ( ) NEW_LINE self . adv_ny_val = AverageMeter ( ) NEW_LINE self . wer_tr = AverageMeter ( ) NEW_LINE self . wer_val = AverageMeter ( ) NEW_LINE self . cer_tr = AverageMeter ( ) NEW_LINE self . cer_val = AverageMeter ( ) NEW_LINE self . CTCLoss = CTCLoss ( ) NEW_LINE self . decoder = GreedyDecoder ( data_loader . labels ) NEW_LINE self . build_model ( ) NEW_LINE self . G . loss_stop = 100000 NEW_LINE if self . config . gpu >= 0 : NEW_LINE INDENT self . G . cuda ( ) NEW_LINE self . D . cuda ( ) NEW_LINE self . diffLoss . cuda ( ) NEW_LINE self . ASR . cuda ( ) NEW_LINE DEDENT if len ( self . config . load_path ) > 0 : NEW_LINE INDENT self . load_model ( ) NEW_LINE DEDENT if config . mode == ' train ' : NEW_LINE INDENT self . logFile = open ( self . model_dir + ' / log . txt ' , ' w ' ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def zero_grad_all ( self ) : NEW_LINE INDENT self . G . zero_grad ( ) NEW_LINE self . D . zero_grad ( ) NEW_LINE self . ASR . zero_grad ( ) NEW_LINE DEDENT',\n",
              " \"def build_model ( self ) : NEW_LINE INDENT print ( ' initialize ▁ enhancement ▁ & ▁ discriminator ▁ model ' ) NEW_LINE self . G = stackedBRNN ( I = self . config . nFeat , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE self . D = stackedBRNN ( I = self . config . nFeat , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE print ( ' load ▁ pre - trained ▁ ASR ▁ model ' ) NEW_LINE package_ASR = torch . load ( self . config . ASR_path , map_location = lambda storage , loc : storage ) NEW_LINE self . ASR = DeepSpeech . load_model_package ( package_ASR ) NEW_LINE DEDENT\",\n",
              " 'def load_model ( self ) : NEW_LINE INDENT print ( \" [ * ] ▁ Load ▁ models ▁ from ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE postfix = \\' _ valmin \\' NEW_LINE paths = glob ( os . path . join ( self . config . load_path , \\' G { } * . pth \\' . format ( postfix ) ) ) NEW_LINE paths . sort ( ) NEW_LINE if len ( paths ) == 0 : NEW_LINE INDENT print ( \" [ ! ] ▁ No ▁ checkpoint ▁ found ▁ in ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE assert ( 0 ) , \\' checkpoint ▁ not ▁ avilable \\' NEW_LINE DEDENT idxes = [ int ( os . path . basename ( path . split ( \\' . \\' ) [ 0 ] . split ( \\' _ \\' ) [ - 1 ] ) ) for path in paths ] NEW_LINE if self . config . start_iter <= 0 : NEW_LINE INDENT self . config . start_iter = max ( idxes ) NEW_LINE if ( self . config . start_iter < 0 ) : NEW_LINE INDENT raise Exception ( \" start ▁ iter ▁ is ▁ still ▁ less ▁ than ▁ 0 ▁ - - > ▁ probably ▁ try ▁ to ▁ load ▁ initial ▁ random ▁ model \" ) NEW_LINE DEDENT DEDENT if self . config . gpu < 0 : NEW_LINE INDENT map_location = lambda storage , loc : storage NEW_LINE DEDENT else : NEW_LINE INDENT map_location = None NEW_LINE DEDENT print ( \\' Load ▁ models ▁ from ▁ \\' + self . config . load_path + \\' , ▁ ITERATION ▁ = ▁ \\' + str ( self . config . start_iter ) ) NEW_LINE self . G . load_state_dict ( torch . load ( \\' { } / G { } _ { } . pth \\' . format ( self . config . load_path , postfix , self . config . start_iter ) , map_location = map_location ) ) NEW_LINE print ( \" [ * ] ▁ Model ▁ loaded \" ) NEW_LINE DEDENT',\n",
              " 'def train ( self ) : NEW_LINE INDENT optimizer_g = torch . optim . Adam ( self . G . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE optimizer_asr = torch . optim . Adam ( self . ASR . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE optimizer_d = torch . optim . Adam ( self . D . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE for iter in trange ( self . config . start_iter , self . config . max_iter ) : NEW_LINE INDENT self . zero_grad_all ( ) NEW_LINE data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' train \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes , mask = _get_variable_nograd ( data_list [ 0 ] ) , _get_variable_nograd ( data_list [ 1 ] , cuda = False ) , data_list [ 2 ] , _get_variable_nograd ( data_list [ 3 ] , cuda = False ) , _get_variable_nograd ( data_list [ 4 ] ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE enhanced = self . G ( inputs ) NEW_LINE enhanced_D = enhanced . detach ( ) NEW_LINE ae_ny_G = self . D ( enhanced ) NEW_LINE l_adv_ny_G , _ = self . diffLoss ( ae_ny_G , enhanced , mask ) NEW_LINE l_adv_ny_G = l_adv_ny_G * self . config . w_adversarial NEW_LINE l_adv_ny_G_data = l_adv_ny_G . data [ 0 ] NEW_LINE l_adv_ny_G . backward ( retain_graph = True ) NEW_LINE g_adv = self . get_gradient_norm ( self . G ) NEW_LINE self . D . zero_grad ( ) NEW_LINE del l_adv_ny_G NEW_LINE ae_ny_D = self . D ( enhanced_D ) NEW_LINE l_adv_ny_D , _ = self . diffLoss ( ae_ny_D , enhanced_D , mask ) NEW_LINE l_adv_ny_D = l_adv_ny_D * ( - self . kt ) * self . config . w_adversarial NEW_LINE l_adv_ny_D . backward ( ) NEW_LINE del l_adv_ny_D NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = _get_variable_nograd ( input_percentages . mul_ ( int ( T ) ) . int ( ) , cuda = False ) NEW_LINE l_CTC = self . config . w_acoustic * self . CTCLoss ( prob , targets , sizes , target_sizes ) / N NEW_LINE self . ctc_tr_local . update ( l_CTC . data [ 0 ] , N ) NEW_LINE l_CTC . backward ( ) NEW_LINE g_ctc_adv = self . get_gradient_norm ( self . G ) NEW_LINE del l_CTC NEW_LINE data_list_cl = self . data_loader . next ( cl_ny = \\' cl \\' , type = \\' train \\' ) NEW_LINE inputs , mask = _get_variable_nograd ( data_list_cl [ 0 ] ) , _get_variable_nograd ( data_list_cl [ 4 ] ) NEW_LINE ae_cl = self . D ( inputs ) NEW_LINE l_adv_cl , _ = self . diffLoss ( ae_cl , inputs , mask ) NEW_LINE l_adv_cl = self . config . w_adversarial * l_adv_cl NEW_LINE l_adv_cl . backward ( ) NEW_LINE l_adv_cl_data = l_adv_cl . data [ 0 ] NEW_LINE del l_adv_cl NEW_LINE optimizer_g . step ( ) NEW_LINE optimizer_d . step ( ) NEW_LINE if ( iter > self . config . allow_ASR_update_iter ) : NEW_LINE INDENT optimizer_asr . step ( ) NEW_LINE DEDENT g_d_balance = self . gamma * l_adv_cl_data - l_adv_ny_G_data NEW_LINE self . kt += self . lb * g_d_balance NEW_LINE self . kt = max ( min ( 1 , self . kt ) , 0 ) NEW_LINE conv_measure = l_adv_cl_data + abs ( g_d_balance ) NEW_LINE if ( iter + 1 ) % self . config . log_iter == 0 : NEW_LINE INDENT str_loss = \" [ { } / { } ] ▁ ( train ) ▁ CTC : ▁ { : . 7f } , ▁ ADV _ cl : ▁ { : . 7f } , ▁ ADV _ ny : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_tr_local . avg , l_adv_cl_data , l_adv_ny_G_data ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE str_loss = \" [ { } / { } ] ▁ ( train ) ▁ conv _ measure : ▁ { : . 4f } , ▁ kt : ▁ { : . 4f } ▁ \" . format ( iter , self . config . max_iter , conv_measure , self . kt ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE str_loss = \" [ { } / { } ] ▁ ( train ) ▁ gradient ▁ norm , ▁ adv : ▁ { : . 4f } , ▁ adv ▁ + ▁ ctc ▁ : ▁ { : . 4f } \" . format ( iter , self . config . max_iter , g_adv . data [ 0 ] , g_ctc_adv . data [ 0 ] ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . ctc_tr_local . reset ( ) NEW_LINE DEDENT if ( iter + 1 ) % self . config . save_iter == 0 : NEW_LINE INDENT self . G . eval ( ) NEW_LINE self . ctc_tr . reset ( ) NEW_LINE self . adv_ny_tr . reset ( ) NEW_LINE self . wer_tr . reset ( ) NEW_LINE self . cer_tr . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . trsub_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' trsub \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes , mask = data_list [ 0 ] , data_list [ 1 ] , data_list [ 2 ] , data_list [ 3 ] , _get_variable_volatile ( data_list [ 4 ] ) NEW_LINE ctc , adv_ny , nElement , wer , cer , nWord , nChar = self . greedy_decoding_and_AAS ( inputs , targets , input_percentages , target_sizes , mask ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE self . ctc_tr . update ( ctc . data [ 0 ] , N ) NEW_LINE self . adv_ny_tr . update ( adv_ny . data [ 0 ] , nElement ) NEW_LINE self . wer_tr . update ( wer , nWord ) NEW_LINE self . cer_tr . update ( cer , nChar ) NEW_LINE del ctc , adv_ny NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( training ▁ subset ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_tr . avg , self . wer_tr . avg * 100 , self . cer_tr . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . ctc_val . reset ( ) NEW_LINE self . adv_ny_val . reset ( ) NEW_LINE self . wer_val . reset ( ) NEW_LINE self . cer_val . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . val_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' val \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes , mask = data_list [ 0 ] , data_list [ 1 ] , data_list [ 2 ] , data_list [ 3 ] , _get_variable_volatile ( data_list [ 4 ] ) NEW_LINE ctc , adv_ny , nElement , wer , cer , nWord , nChar = self . greedy_decoding_and_AAS ( inputs , targets , input_percentages , target_sizes , mask ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE self . ctc_val . update ( ctc . data [ 0 ] , N ) NEW_LINE self . adv_ny_val . update ( adv_ny . data [ 0 ] , nElement ) NEW_LINE self . wer_val . update ( wer , nWord ) NEW_LINE self . cer_val . update ( cer , nChar ) NEW_LINE del ctc , adv_ny NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( validation ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_val . avg , self . wer_val . avg * 100 , self . cer_val . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . G . train ( ) NEW_LINE if ( len ( self . savename_G ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_G ) : NEW_LINE INDENT os . remove ( self . savename_G ) NEW_LINE DEDENT DEDENT self . savename_G = \\' { } / G _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . G . state_dict ( ) , self . savename_G ) NEW_LINE if ( len ( self . savename_ASR ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_ASR ) : NEW_LINE INDENT os . remove ( self . savename_ASR ) NEW_LINE DEDENT DEDENT self . savename_ASR = \\' { } / ASR _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . ASR . state_dict ( ) , self . savename_ASR ) NEW_LINE if ( self . G . loss_stop > self . wer_val . avg ) : NEW_LINE INDENT self . G . loss_stop = self . wer_val . avg NEW_LINE savename_G_valmin_prev = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_G_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_G_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_G_valmin = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_G , savename_G_valmin ) NEW_LINE savename_ASR_valmin_prev = \\' { } / ASR _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_ASR_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_ASR_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_ASR_valmin = \\' { } / ASR _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_ASR , savename_ASR_valmin ) NEW_LINE self . valmin_iter = iter NEW_LINE DEDENT DEDENT DEDENT DEDENT',\n",
              " \"def greedy_decoding_and_AAS ( self , inputs , targets , input_percentages , target_sizes , mask , transcript_prob = 0.001 ) : NEW_LINE INDENT inputs = _get_variable_volatile ( inputs ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE split_targets = [ ] NEW_LINE offset = 0 NEW_LINE for size in target_sizes : NEW_LINE INDENT split_targets . append ( targets [ offset : offset + size ] ) NEW_LINE offset += size NEW_LINE DEDENT enhanced = self . G ( inputs ) NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = input_percentages . mul_ ( int ( T ) ) . int ( ) NEW_LINE decoded_output , _ = self . decoder . decode ( prob . data , sizes ) NEW_LINE target_strings = self . decoder . convert_to_strings ( split_targets ) NEW_LINE we , ce , total_word , total_char = 0 , 0 , 0 , 0 NEW_LINE for x in range ( len ( target_strings ) ) : NEW_LINE INDENT decoding , reference = decoded_output [ x ] [ 0 ] , target_strings [ x ] [ 0 ] NEW_LINE nChar = len ( reference ) NEW_LINE nWord = len ( reference . split ( ) ) NEW_LINE we_i = self . decoder . wer ( decoding , reference ) NEW_LINE ce_i = self . decoder . cer ( decoding , reference ) NEW_LINE we += we_i NEW_LINE ce += ce_i NEW_LINE total_word += nWord NEW_LINE total_char += nChar NEW_LINE if ( random . uniform ( 0 , 1 ) < transcript_prob ) : NEW_LINE INDENT print ( ' reference ▁ = ▁ ' + reference ) NEW_LINE print ( ' decoding ▁ = ▁ ' + decoding ) NEW_LINE print ( ' wer ▁ = ▁ ' + str ( we_i / float ( nWord ) ) + ' , ▁ cer ▁ = ▁ ' + str ( ce_i / float ( nChar ) ) ) NEW_LINE DEDENT DEDENT wer = we / total_word NEW_LINE cer = ce / total_word NEW_LINE ae_ny = self . D ( enhanced ) NEW_LINE l_adv_ny , nElement = self . diffLoss ( ae_ny , enhanced , mask ) NEW_LINE l_adv_ny = l_adv_ny * self . config . w_adversarial NEW_LINE targets = _get_variable_volatile ( targets , cuda = False ) NEW_LINE sizes = _get_variable_volatile ( sizes , cuda = False ) NEW_LINE target_sizes = _get_variable_volatile ( target_sizes , cuda = False ) NEW_LINE l_CTC = self . config . w_acoustic * self . CTCLoss ( prob , targets , sizes , target_sizes ) / N NEW_LINE return l_CTC , l_adv_ny , nElement , wer , cer , total_word , total_char NEW_LINE DEDENT\",\n",
              " 'def get_gradient_norm ( self , model ) : NEW_LINE INDENT params = list ( model . parameters ( ) ) NEW_LINE grad_norm = 0 NEW_LINE for param in params : NEW_LINE INDENT grad_norm += torch . pow ( param . grad , 2 ) . sum ( ) NEW_LINE DEDENT grad_norm = torch . pow ( grad_norm , 0.5 ) NEW_LINE return grad_norm NEW_LINE DEDENT',\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def __init__ ( self , config , data_loader = None ) : NEW_LINE INDENT self . config = config NEW_LINE self . data_loader = data_loader NEW_LINE self . lr = config . lr NEW_LINE self . beta1 = config . beta1 NEW_LINE self . beta2 = config . beta2 NEW_LINE self . optimizer = config . optimizer NEW_LINE self . batch_size = config . batch_size NEW_LINE self . valmin_iter = 0 NEW_LINE self . model_dir = ' logs / ' + str ( config . expnum ) NEW_LINE self . savename_G = ' ' NEW_LINE self . savename_ASR = ' ' NEW_LINE self . kt = 0 NEW_LINE self . lb = 0.001 NEW_LINE self . conv_measure = 0 NEW_LINE self . ctc_tr = AverageMeter ( ) NEW_LINE self . ctc_tr_local = AverageMeter ( ) NEW_LINE self . ctc_val = AverageMeter ( ) NEW_LINE self . wer_tr = AverageMeter ( ) NEW_LINE self . wer_val = AverageMeter ( ) NEW_LINE self . cer_tr = AverageMeter ( ) NEW_LINE self . cer_val = AverageMeter ( ) NEW_LINE self . CTCLoss = CTCLoss ( ) NEW_LINE self . decoder = GreedyDecoder ( data_loader . labels ) NEW_LINE self . build_model ( ) NEW_LINE self . G . loss_stop = 100000 NEW_LINE if self . config . gpu >= 0 : NEW_LINE INDENT self . G . cuda ( ) NEW_LINE self . ASR . cuda ( ) NEW_LINE DEDENT if len ( self . config . load_path ) > 0 : NEW_LINE INDENT self . load_model ( ) NEW_LINE DEDENT if config . mode == ' train ' : NEW_LINE INDENT self . logFile = open ( self . model_dir + ' / log . txt ' , ' w ' ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def zero_grad_all ( self ) : NEW_LINE INDENT self . G . zero_grad ( ) NEW_LINE self . ASR . zero_grad ( ) NEW_LINE DEDENT',\n",
              " \"def build_model ( self ) : NEW_LINE INDENT print ( ' initialize ▁ enhancement ▁ model ' ) NEW_LINE self . G = stackedBRNN ( I = self . config . nFeat , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE print ( ' load ▁ pre - trained ▁ ASR ▁ model ' ) NEW_LINE package_ASR = torch . load ( self . config . ASR_path , map_location = lambda storage , loc : storage ) NEW_LINE self . ASR = DeepSpeech . load_model_package ( package_ASR ) NEW_LINE DEDENT\",\n",
              " 'def load_model ( self ) : NEW_LINE INDENT print ( \" [ * ] ▁ Load ▁ models ▁ from ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE postfix = \\' _ valmin \\' NEW_LINE paths = glob ( os . path . join ( self . config . load_path , \\' G { } * . pth \\' . format ( postfix ) ) ) NEW_LINE paths . sort ( ) NEW_LINE if len ( paths ) == 0 : NEW_LINE INDENT print ( \" [ ! ] ▁ No ▁ checkpoint ▁ found ▁ in ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE assert ( 0 ) , \\' checkpoint ▁ not ▁ avilable \\' NEW_LINE DEDENT idxes = [ int ( os . path . basename ( path . split ( \\' . \\' ) [ 0 ] . split ( \\' _ \\' ) [ - 1 ] ) ) for path in paths ] NEW_LINE if self . config . start_iter <= 0 : NEW_LINE INDENT self . config . start_iter = max ( idxes ) NEW_LINE if ( self . config . start_iter < 0 ) : NEW_LINE INDENT raise Exception ( \" start ▁ iter ▁ is ▁ still ▁ less ▁ than ▁ 0 ▁ - - > ▁ probably ▁ try ▁ to ▁ load ▁ initial ▁ random ▁ model \" ) NEW_LINE DEDENT DEDENT if self . config . gpu < 0 : NEW_LINE INDENT map_location = lambda storage , loc : storage NEW_LINE DEDENT else : NEW_LINE INDENT map_location = None NEW_LINE DEDENT print ( \\' Load ▁ models ▁ from ▁ \\' + self . config . load_path + \\' , ▁ ITERATION ▁ = ▁ \\' + str ( self . config . start_iter ) ) NEW_LINE self . G . load_state_dict ( torch . load ( \\' { } / G { } _ { } . pth \\' . format ( self . config . load_path , postfix , self . config . start_iter ) , map_location = map_location ) ) NEW_LINE print ( \" [ * ] ▁ Model ▁ loaded \" ) NEW_LINE DEDENT',\n",
              " 'def train ( self ) : NEW_LINE INDENT optimizer_g = torch . optim . Adam ( self . G . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE optimizer_asr = torch . optim . Adam ( self . ASR . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE for iter in trange ( self . config . start_iter , self . config . max_iter ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' train \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes = _get_variable_nograd ( data_list [ 0 ] ) , _get_variable_nograd ( data_list [ 1 ] , cuda = False ) , data_list [ 2 ] , _get_variable_nograd ( data_list [ 3 ] , cuda = False ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE enhanced = self . G ( inputs ) NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = _get_variable_nograd ( input_percentages . mul_ ( int ( T ) ) . int ( ) , cuda = False ) NEW_LINE loss = self . CTCLoss ( prob , targets , sizes , target_sizes ) NEW_LINE loss = loss / N NEW_LINE self . zero_grad_all ( ) NEW_LINE loss . backward ( ) NEW_LINE optimizer_g . step ( ) NEW_LINE if ( iter > self . config . allow_ASR_update_iter ) : NEW_LINE INDENT optimizer_asr . step ( ) NEW_LINE DEDENT self . ctc_tr_local . update ( loss . data [ 0 ] , N ) NEW_LINE del loss NEW_LINE if ( iter + 1 ) % self . config . log_iter == 0 : NEW_LINE INDENT str_loss = \" [ { } / { } ] ▁ ( train ) ▁ CTC : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_tr_local . avg ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . ctc_tr_local . reset ( ) NEW_LINE DEDENT if ( iter + 1 ) % self . config . save_iter == 0 : NEW_LINE INDENT self . G . eval ( ) NEW_LINE self . ctc_tr . reset ( ) NEW_LINE self . wer_tr . reset ( ) NEW_LINE self . cer_tr . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . trsub_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' trsub \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes = data_list [ 0 ] , data_list [ 1 ] , data_list [ 2 ] , data_list [ 3 ] NEW_LINE ctc , wer , cer , nWord , nChar = self . greedy_decoding_and_CTCLoss ( inputs , targets , input_percentages , target_sizes ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE self . ctc_tr . update ( ctc . data [ 0 ] , N ) NEW_LINE self . wer_tr . update ( wer , nWord ) NEW_LINE self . cer_tr . update ( cer , nChar ) NEW_LINE del ctc NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( training ▁ subset ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_tr . avg , self . wer_tr . avg * 100 , self . cer_tr . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . ctc_val . reset ( ) NEW_LINE self . wer_val . reset ( ) NEW_LINE self . cer_val . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . val_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' val \\' ) NEW_LINE inputs , targets , input_percentages , target_sizes = data_list [ 0 ] , data_list [ 1 ] , data_list [ 2 ] , data_list [ 3 ] NEW_LINE ctc , wer , cer , nWord , nChar = self . greedy_decoding_and_CTCLoss ( inputs , targets , input_percentages , target_sizes ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE self . ctc_val . update ( ctc . data [ 0 ] , N ) NEW_LINE self . wer_val . update ( wer , nWord ) NEW_LINE self . cer_val . update ( cer , nChar ) NEW_LINE del ctc NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( validation ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . ctc_val . avg , self . wer_val . avg * 100 , self . cer_val . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . G . train ( ) NEW_LINE if ( len ( self . savename_G ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_G ) : NEW_LINE INDENT os . remove ( self . savename_G ) NEW_LINE DEDENT DEDENT self . savename_G = \\' { } / G _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . G . state_dict ( ) , self . savename_G ) NEW_LINE if ( len ( self . savename_ASR ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_ASR ) : NEW_LINE INDENT os . remove ( self . savename_ASR ) NEW_LINE DEDENT DEDENT self . savename_ASR = \\' { } / ASR _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . ASR . state_dict ( ) , self . savename_ASR ) NEW_LINE if ( self . G . loss_stop > self . wer_val . avg ) : NEW_LINE INDENT self . G . loss_stop = self . wer_val . avg NEW_LINE savename_G_valmin_prev = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_G_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_G_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_G_valmin = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_G , savename_G_valmin ) NEW_LINE savename_ASR_valmin_prev = \\' { } / ASR _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_ASR_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_ASR_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_ASR_valmin = \\' { } / ASR _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_ASR , savename_ASR_valmin ) NEW_LINE self . valmin_iter = iter NEW_LINE DEDENT DEDENT DEDENT DEDENT',\n",
              " \"def greedy_decoding_and_CTCLoss ( self , inputs , targets , input_percentages , target_sizes , transcript_prob = 0.001 ) : NEW_LINE INDENT inputs = _get_variable_volatile ( inputs ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE split_targets = [ ] NEW_LINE offset = 0 NEW_LINE for size in target_sizes : NEW_LINE INDENT split_targets . append ( targets [ offset : offset + size ] ) NEW_LINE offset += size NEW_LINE DEDENT enhanced = self . G ( inputs ) NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = input_percentages . mul_ ( int ( T ) ) . int ( ) NEW_LINE decoded_output , _ = self . decoder . decode ( prob . data , sizes ) NEW_LINE target_strings = self . decoder . convert_to_strings ( split_targets ) NEW_LINE we , ce , total_word , total_char = 0 , 0 , 0 , 0 NEW_LINE for x in range ( len ( target_strings ) ) : NEW_LINE INDENT decoding , reference = decoded_output [ x ] [ 0 ] , target_strings [ x ] [ 0 ] NEW_LINE nChar = len ( reference ) NEW_LINE nWord = len ( reference . split ( ) ) NEW_LINE we_i = self . decoder . wer ( decoding , reference ) NEW_LINE ce_i = self . decoder . cer ( decoding , reference ) NEW_LINE we += we_i NEW_LINE ce += ce_i NEW_LINE total_word += nWord NEW_LINE total_char += nChar NEW_LINE if ( random . uniform ( 0 , 1 ) < transcript_prob ) : NEW_LINE INDENT print ( ' reference ▁ = ▁ ' + reference ) NEW_LINE print ( ' decoding ▁ = ▁ ' + decoding ) NEW_LINE print ( ' wer ▁ = ▁ ' + str ( we_i / float ( nWord ) ) + ' , ▁ cer ▁ = ▁ ' + str ( ce_i / float ( nChar ) ) ) NEW_LINE DEDENT DEDENT wer = we / total_word NEW_LINE cer = ce / total_word NEW_LINE targets = _get_variable_volatile ( targets , cuda = False ) NEW_LINE sizes = _get_variable_volatile ( sizes , cuda = False ) NEW_LINE target_sizes = _get_variable_volatile ( target_sizes , cuda = False ) NEW_LINE loss = self . CTCLoss ( prob , targets , sizes , target_sizes ) NEW_LINE loss = loss / N NEW_LINE return loss , wer , cer , total_word , total_char NEW_LINE DEDENT\",\n",
              " 'def _collate_fn ( batch ) : NEW_LINE INDENT def func ( p ) : NEW_LINE INDENT return p [ 0 ] . size ( 1 ) NEW_LINE DEDENT batch = sorted ( batch , key = lambda sample : sample [ 0 ] . size ( 1 ) , reverse = True ) NEW_LINE longest_sample = max ( batch , key = func ) [ 0 ] NEW_LINE freq_size = longest_sample . size ( 0 ) NEW_LINE minibatch_size = len ( batch ) NEW_LINE max_seqlength = longest_sample . size ( 1 ) NEW_LINE inputs = torch . zeros ( minibatch_size , freq_size , max_seqlength ) NEW_LINE input_percentages = torch . FloatTensor ( minibatch_size ) NEW_LINE target_sizes = torch . IntTensor ( minibatch_size ) NEW_LINE targets = [ ] NEW_LINE mask = torch . ByteTensor ( minibatch_size , 1 , max_seqlength ) . zero_ ( ) NEW_LINE for x in range ( minibatch_size ) : NEW_LINE INDENT sample = batch [ x ] NEW_LINE tensor = sample [ 0 ] NEW_LINE target = sample [ 1 ] NEW_LINE seq_length = tensor . size ( 1 ) NEW_LINE inputs [ x ] . narrow ( 1 , 0 , seq_length ) . copy_ ( tensor ) NEW_LINE input_percentages [ x ] = seq_length / float ( max_seqlength ) NEW_LINE target_sizes [ x ] = len ( target ) NEW_LINE targets . extend ( target ) NEW_LINE if ( seq_length < max_seqlength ) : NEW_LINE INDENT mask [ x ] [ : , seq_length : ] . fill_ ( 1 ) NEW_LINE DEDENT DEDENT targets = torch . IntTensor ( targets ) NEW_LINE return inputs , targets , input_percentages , target_sizes , mask NEW_LINE DEDENT',\n",
              " 'def _collate_fn_paired ( batch ) : NEW_LINE INDENT def func ( p ) : NEW_LINE INDENT return p [ 0 ] . size ( 1 ) NEW_LINE DEDENT batch = sorted ( batch , key = lambda sample : sample [ 0 ] . size ( 1 ) , reverse = True ) NEW_LINE longest_sample = max ( batch , key = func ) [ 0 ] NEW_LINE freq_size = longest_sample . size ( 0 ) NEW_LINE minibatch_size = len ( batch ) NEW_LINE max_seqlength = longest_sample . size ( 1 ) NEW_LINE inputs = torch . zeros ( minibatch_size , freq_size , max_seqlength ) NEW_LINE outputs = torch . zeros ( minibatch_size , freq_size , max_seqlength ) NEW_LINE mask = torch . ByteTensor ( minibatch_size , 1 , max_seqlength ) . zero_ ( ) NEW_LINE input_percentages = torch . FloatTensor ( minibatch_size ) NEW_LINE target_sizes = torch . IntTensor ( minibatch_size ) NEW_LINE targets = [ ] NEW_LINE for x in range ( minibatch_size ) : NEW_LINE INDENT sample = batch [ x ] NEW_LINE tensor = sample [ 0 ] NEW_LINE txt = sample [ 1 ] NEW_LINE target = sample [ 2 ] NEW_LINE seq_length = tensor . size ( 1 ) NEW_LINE inputs [ x ] . narrow ( 1 , 0 , seq_length ) . copy_ ( tensor ) NEW_LINE outputs [ x ] . narrow ( 1 , 0 , seq_length ) . copy_ ( target ) NEW_LINE if ( seq_length < max_seqlength ) : NEW_LINE INDENT mask [ x ] [ : , seq_length : ] . fill_ ( 1 ) NEW_LINE DEDENT input_percentages [ x ] = seq_length / float ( max_seqlength ) NEW_LINE target_sizes [ x ] = len ( txt ) NEW_LINE targets . extend ( txt ) NEW_LINE DEDENT targets = torch . IntTensor ( targets ) NEW_LINE return inputs , outputs , mask , targets , input_percentages , target_sizes NEW_LINE DEDENT',\n",
              " \"def __init__ ( self , manifest , labels ) : NEW_LINE INDENT with open ( manifest ) as f : NEW_LINE INDENT ids = f . readlines ( ) NEW_LINE DEDENT ids = [ x . strip ( ) . split ( ' , ' ) for x in ids ] NEW_LINE self . ids = ids NEW_LINE self . size = len ( ids ) NEW_LINE self . labels_map = dict ( [ ( labels [ i ] , i ) for i in range ( len ( labels ) ) ] ) NEW_LINE super ( FeatDataset , self ) . __init__ ( ) NEW_LINE DEDENT\",\n",
              " 'def __getitem__ ( self , index ) : NEW_LINE INDENT sample = self . ids [ index ] NEW_LINE if ( len ( sample ) == 2 ) : NEW_LINE INDENT feat_path , transcript_path = sample [ 0 ] , sample [ 1 ] NEW_LINE DEDENT else : NEW_LINE INDENT feat_path , transcript_path , feat_paired_path = sample [ 0 ] , sample [ 1 ] , sample [ 2 ] NEW_LINE DEDENT feat = torch . load ( feat_path ) NEW_LINE transcript = self . parse_transcript ( transcript_path ) NEW_LINE if ( len ( sample ) == 2 ) : NEW_LINE INDENT return feat , transcript NEW_LINE DEDENT else : NEW_LINE INDENT feat_paired = torch . load ( feat_paired_path ) NEW_LINE return feat , transcript , feat_paired NEW_LINE DEDENT DEDENT',\n",
              " \"def parse_transcript ( self , transcript_path ) : NEW_LINE INDENT with open ( transcript_path , ' r ' , encoding = ' utf8' ) as transcript_file : NEW_LINE INDENT transcript = transcript_file . read ( ) . replace ( ' \\\\n ' , ' ' ) NEW_LINE DEDENT transcript = list ( filter ( None , [ self . labels_map . get ( x ) for x in list ( transcript ) ] ) ) NEW_LINE return transcript NEW_LINE DEDENT\",\n",
              " 'def __len__ ( self ) : NEW_LINE INDENT return self . size NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , * args , ** kwargs ) : NEW_LINE INDENT super ( FeatLoader , self ) . __init__ ( * args , ** kwargs ) NEW_LINE self . collate_fn = _collate_fn NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , * args , ** kwargs ) : NEW_LINE INDENT super ( FeatLoader_paired , self ) . __init__ ( * args , ** kwargs ) NEW_LINE self . collate_fn = _collate_fn_paired NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , data_source , batch_size = 1 ) : NEW_LINE INDENT super ( FeatSampler , self ) . __init__ ( data_source ) NEW_LINE self . data_source = data_source NEW_LINE ids = list ( range ( 0 , len ( data_source ) ) ) NEW_LINE self . bins = [ ids [ i : i + batch_size ] for i in range ( 0 , len ( ids ) , batch_size ) ] NEW_LINE DEDENT',\n",
              " 'def __iter__ ( self ) : NEW_LINE INDENT for ids in self . bins : NEW_LINE INDENT np . random . shuffle ( ids ) NEW_LINE yield ids NEW_LINE DEDENT DEDENT',\n",
              " 'def __len__ ( self ) : NEW_LINE INDENT return len ( self . bins ) NEW_LINE DEDENT',\n",
              " 'def shuffle ( self ) : NEW_LINE INDENT np . random . shuffle ( self . bins ) NEW_LINE DEDENT',\n",
              " \"def load_model ( cls , path , gpu = - 1 ) : NEW_LINE INDENT package = torch . load ( path , map_location = lambda storage , loc : storage ) NEW_LINE model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] ) NEW_LINE blacklist = [ ' rnns . 0 . batch _ norm . module . weight ' , ' rnns . 0 . batch _ norm . module . bias ' , ' rnns . 0 . batch _ norm . module . running _ mean ' , ' rnns . 0 . batch _ norm . module . running _ var ' ] NEW_LINE for x in blacklist : NEW_LINE INDENT if x in package [ ' state _ dict ' ] : NEW_LINE INDENT del package [ ' state _ dict ' ] [ x ] NEW_LINE DEDENT DEDENT model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE for x in model . rnns : NEW_LINE INDENT x . flatten_parameters ( ) NEW_LINE DEDENT if gpu >= 0 : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def load_model_package ( cls , package , gpu = - 1 ) : NEW_LINE INDENT model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] , ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE if ( gpu >= 0 ) : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def serialize ( model , optimizer = None , epoch = None , iteration = None , loss_results = None , cer_results = None , wer_results = None , avg_loss = None , meta = None ) : NEW_LINE INDENT package = { ' version ' : model . _version , ' rnn _ size ' : model . rnn_size , ' rnn _ layers ' : model . rnn_layers , ' cnn _ map ' : model . cnn_map , ' cnn _ kernel ' : model . cnn_kernel , ' cnn _ stride ' : model . cnn_stride , ' cnn _ layers ' : model . cnn_layers , ' rnn _ type ' : supported_rnns_inv . get ( model . rnn_type , model . rnn_type . __name__ . lower ( ) ) , ' labels ' : model . _labels , ' state _ dict ' : model . state_dict ( ) } NEW_LINE if optimizer is not None : NEW_LINE INDENT package [ ' optim _ dict ' ] = optimizer . state_dict ( ) NEW_LINE DEDENT if avg_loss is not None : NEW_LINE INDENT package [ ' avg _ loss ' ] = avg_loss NEW_LINE DEDENT if epoch is not None : NEW_LINE INDENT package [ ' epoch ' ] = epoch + 1 NEW_LINE DEDENT if iteration is not None : NEW_LINE INDENT package [ ' iteration ' ] = iteration NEW_LINE DEDENT if loss_results is not None : NEW_LINE INDENT package [ ' loss _ results ' ] = loss_results NEW_LINE package [ ' cer _ results ' ] = cer_results NEW_LINE package [ ' wer _ results ' ] = wer_results NEW_LINE DEDENT if meta is not None : NEW_LINE INDENT package [ ' meta ' ] = meta NEW_LINE DEDENT return package NEW_LINE DEDENT\",\n",
              " 'def get_labels ( model ) : NEW_LINE INDENT return model . _labels NEW_LINE DEDENT',\n",
              " 'def get_param_size ( model ) : NEW_LINE INDENT params = 0 NEW_LINE for p in model . parameters ( ) : NEW_LINE INDENT tmp = 1 NEW_LINE for x in p . size ( ) : NEW_LINE INDENT tmp *= x NEW_LINE DEDENT params += tmp NEW_LINE DEDENT return params NEW_LINE DEDENT',\n",
              " 'def get_audio_conf ( model ) : NEW_LINE INDENT return model . _audio_conf NEW_LINE DEDENT',\n",
              " 'def get_meta ( model ) : NEW_LINE INDENT model_is_cuda = next ( model . parameters ( ) ) . is_cuda NEW_LINE m = model . module if model_is_cuda else model NEW_LINE meta = { \" version \" : m . _version , \" rnn _ size \" : m . rnn_size , \" rnn _ layers \" : m . rnn_layers , \" cnn _ map \" : m . cnn_map , \" cnn _ kernel \" : m . cnn_kernel , \" cnn _ stride \" : m . cnn_stride , \" cnn _ layers \" : m . cnn_layers , \" rnn _ type \" : supported_rnns_inv [ m . rnn_type ] } NEW_LINE return meta NEW_LINE DEDENT',\n",
              " 'def __init__ ( self ) : NEW_LINE INDENT super ( L1Loss_mask , self ) . __init__ ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , input , target , mask ) : NEW_LINE INDENT mask_sum = mask . data . sum ( ) NEW_LINE if ( mask . data [ 0 ] [ 0 ] [ 0 ] == 0 ) : NEW_LINE INDENT nElement = mask . data . nelement ( ) - mask_sum NEW_LINE DEDENT err = torch . abs ( input - target ) NEW_LINE err . masked_fill ( mask , 0 ) NEW_LINE loss = err . sum ( ) / nElement NEW_LINE return loss , nElement NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , module ) : NEW_LINE INDENT super ( SequenceWise , self ) . __init__ ( ) NEW_LINE self . module = module NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT t , n = x . size ( 0 ) , x . size ( 1 ) NEW_LINE x = x . view ( t * n , - 1 ) NEW_LINE x = self . module ( x ) NEW_LINE x = x . view ( t , n , - 1 ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " \"def __repr__ ( self ) : NEW_LINE INDENT tmpstr = self . __class__ . __name__ + ' ▁ ( \\\\n ' NEW_LINE tmpstr += self . module . __repr__ ( ) NEW_LINE tmpstr += ' ) ' NEW_LINE return tmpstr NEW_LINE DEDENT\",\n",
              " 'def forward ( self , input_ ) : NEW_LINE INDENT if not self . training : NEW_LINE INDENT batch_size = input_ . size ( ) [ 0 ] NEW_LINE return torch . stack ( [ F . softmax ( input_ [ i ] , dim = 1 ) for i in range ( batch_size ) ] , 0 ) NEW_LINE DEDENT else : NEW_LINE INDENT return input_ NEW_LINE DEDENT DEDENT',\n",
              " 'def __init__ ( self , input_size , hidden_size , rnn_type = nn . LSTM , bidirectional = False , batch_norm = True ) : NEW_LINE INDENT super ( BatchRNN , self ) . __init__ ( ) NEW_LINE self . input_size = input_size NEW_LINE self . hidden_size = hidden_size NEW_LINE self . bidirectional = bidirectional NEW_LINE self . batch_norm = SequenceWise ( nn . BatchNorm1d ( input_size ) ) if batch_norm else None NEW_LINE self . rnn = rnn_type ( input_size = input_size , hidden_size = hidden_size , bidirectional = bidirectional , bias = False ) NEW_LINE self . num_directions = 2 if bidirectional else 1 NEW_LINE DEDENT',\n",
              " 'def flatten_parameters ( self ) : NEW_LINE INDENT self . rnn . flatten_parameters ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if self . batch_norm is not None : NEW_LINE INDENT x = self . batch_norm ( x ) NEW_LINE DEDENT x , _ = self . rnn ( x ) NEW_LINE if self . bidirectional : NEW_LINE INDENT x = x . view ( x . size ( 0 ) , x . size ( 1 ) , 2 , - 1 ) . sum ( 2 ) . view ( x . size ( 0 ) , x . size ( 1 ) , - 1 ) NEW_LINE DEDENT return x NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , input_size , hidden_size , rnn_type = nn . LSTM , bidirectional = False ) : NEW_LINE INDENT super ( BRNN , self ) . __init__ ( ) NEW_LINE self . input_size = input_size NEW_LINE self . hidden_size = hidden_size NEW_LINE self . bidirectional = bidirectional NEW_LINE self . rnn = rnn_type ( input_size = input_size , hidden_size = hidden_size , bidirectional = bidirectional , bias = False ) NEW_LINE self . num_directions = 2 if bidirectional else 1 NEW_LINE DEDENT',\n",
              " 'def flatten_parameters ( self ) : NEW_LINE INDENT self . rnn . flatten_parameters ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT x , _ = self . rnn ( x ) NEW_LINE if self . bidirectional : NEW_LINE INDENT x = x . view ( x . size ( 0 ) , x . size ( 1 ) , 2 , - 1 ) . sum ( 2 ) . view ( x . size ( 0 ) , x . size ( 1 ) , - 1 ) NEW_LINE DEDENT return x NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , I , O , H , L = 3 , rnn_type = nn . LSTM ) : NEW_LINE INDENT super ( SpeechClassifierRNN , self ) . __init__ ( ) NEW_LINE self . I = I NEW_LINE self . H = H NEW_LINE self . L = L NEW_LINE self . rnn1 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn2 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn3 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE if ( I != H ) : NEW_LINE INDENT self . first_linear = nn . Conv1d ( I , H , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE DEDENT else : NEW_LINE INDENT self . first_linear = None NEW_LINE DEDENT self . final_linear = nn . Linear ( H , O ) NEW_LINE self . criterion = nn . CrossEntropyLoss ( size_average = False ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , input , target ) : NEW_LINE INDENT if ( self . first_linear ) : NEW_LINE INDENT input = self . first_linear ( input ) NEW_LINE DEDENT input = input . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) NEW_LINE h1 = self . rnn1 ( input ) + input NEW_LINE h2 = self . rnn2 ( h1 ) + h1 NEW_LINE h3 = self . rnn3 ( h2 ) + h2 NEW_LINE h3 = h3 . sum ( 0 ) NEW_LINE output = self . final_linear ( h3 ) NEW_LINE loss = self . criterion ( output , target ) NEW_LINE return loss NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , I , H , L , nCH , mel_basis , rnn_type = nn . GRU ) : NEW_LINE INDENT super ( BRNNmultiCH , self ) . __init__ ( ) NEW_LINE self . I = I NEW_LINE self . H = H NEW_LINE self . nCH = nCH NEW_LINE self . rnn_type = rnn_type NEW_LINE self . L = L NEW_LINE self . rnn1 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn2 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE if ( self . L == 3 ) : NEW_LINE INDENT self . rnn3 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE DEDENT self . first_linear = nn . Conv1d ( I , H , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE self . final_linear_real = nn . Conv1d ( H , int ( I / 2 ) , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE self . final_linear_imag = nn . Conv1d ( H , int ( I / 2 ) , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE self . mel_basis = Variable ( torch . unsqueeze ( torch . FloatTensor ( mel_basis ) . repeat ( 1 , self . nCH ) , - 1 ) . cuda ( ) ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , input ) : NEW_LINE INDENT input_linear = self . first_linear ( input ) NEW_LINE input_linear = input_linear . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) NEW_LINE h1 = self . rnn1 ( input_linear ) + input_linear NEW_LINE h2 = self . rnn2 ( h1 ) + h1 NEW_LINE if ( self . L == 3 ) : NEW_LINE INDENT h3 = self . rnn3 ( h2 ) + h2 NEW_LINE h = h3 . transpose ( 0 , 1 ) . transpose ( 1 , 2 ) NEW_LINE DEDENT else : NEW_LINE INDENT h = h2 . transpose ( 0 , 1 ) . transpose ( 1 , 2 ) NEW_LINE DEDENT mask_real = self . final_linear_real ( h ) NEW_LINE mask_imag = self . final_linear_imag ( h ) NEW_LINE stft = input . view ( input . size ( 0 ) , 2 , - 1 , input . size ( - 1 ) ) NEW_LINE stft_real = stft [ : , 0 ] NEW_LINE stft_imag = stft [ : , 1 ] NEW_LINE enh_real = torch . mul ( stft_real , mask_real ) NEW_LINE enh_imag = torch . mul ( stft_imag , mask_imag ) NEW_LINE enh_power = torch . pow ( enh_real , 2 ) + torch . pow ( enh_imag , 2 ) NEW_LINE enh_mel = F . conv1d ( enh_power , self . mel_basis ) NEW_LINE output = torch . log1p ( enh_mel ) NEW_LINE return output NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , I , O , H , L , rnn_type = nn . LSTM ) : NEW_LINE INDENT super ( stackedBRNN , self ) . __init__ ( ) NEW_LINE self . I = I NEW_LINE self . H = H NEW_LINE self . L = L NEW_LINE self . rnn_type = rnn_type NEW_LINE self . rnn1 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn2 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn3 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . rnn4 = BRNN ( input_size = H , hidden_size = H , rnn_type = rnn_type , bidirectional = True ) NEW_LINE self . first_linear = nn . Conv1d ( I , H , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE self . final_linear = nn . Conv1d ( H , O , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , input ) : NEW_LINE INDENT input_linear = self . first_linear ( input ) NEW_LINE input_linear = input_linear . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) NEW_LINE h1 = self . rnn1 ( input_linear ) + input_linear NEW_LINE h2 = self . rnn2 ( h1 ) + h1 NEW_LINE h3 = self . rnn3 ( h2 ) + h2 NEW_LINE h4 = self . rnn4 ( h3 ) + h3 NEW_LINE h4 = h4 . transpose ( 0 , 1 ) . transpose ( 1 , 2 ) NEW_LINE output = self . final_linear ( h4 ) NEW_LINE return output NEW_LINE DEDENT',\n",
              " 'def forward_paired ( self , input , paired ) : NEW_LINE INDENT input = torch . cat ( ( input , paired ) , dim = 1 ) NEW_LINE output = self . forward ( input ) NEW_LINE return output NEW_LINE DEDENT',\n",
              " 'def forward_with_intermediate_output ( self , input ) : NEW_LINE INDENT input_linear = self . first_linear ( input ) NEW_LINE input_linear = input_linear . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) NEW_LINE h1 = self . rnn1 ( input_linear ) + input_linear NEW_LINE h2 = self . rnn2 ( h1 ) + h1 NEW_LINE h3 = self . rnn3 ( h2 ) + h2 NEW_LINE h4 = self . rnn4 ( h3 ) + h3 NEW_LINE h4 = h4 . transpose ( 0 , 1 ) . transpose ( 1 , 2 ) NEW_LINE output = self . final_linear ( h4 ) NEW_LINE return [ output , h4 ] NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , rnn_type = nn . LSTM , labels = \" abc \" , rnn_hidden_size = 512 , rnn_layers = 2 , bidirectional = True , kernel_sz = 11 , stride = 2 , map = 256 , cnn_layers = 2 , nFreq = 40 , nDownsample = 1 , audio_conf = None ) : NEW_LINE INDENT super ( DeepSpeech , self ) . __init__ ( ) NEW_LINE self . nFreq = nFreq NEW_LINE self . _version = \\'0.0.1\\' NEW_LINE self . _audio_conf = audio_conf NEW_LINE self . rnn_size = rnn_hidden_size NEW_LINE self . rnn_layers = rnn_layers NEW_LINE self . rnn_type = rnn_type NEW_LINE self . bidirectional = bidirectional NEW_LINE self . cnn_stride = stride NEW_LINE self . cnn_map = map NEW_LINE self . cnn_kernel = kernel_sz NEW_LINE self . nDownsample = nDownsample NEW_LINE self . cnn_layers = cnn_layers NEW_LINE self . _labels = labels NEW_LINE num_classes = len ( self . _labels ) NEW_LINE conv_list = [ ] NEW_LINE conv_list . append ( nn . Conv1d ( nFreq , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE if ( self . nDownsample == 1 ) : NEW_LINE INDENT stride = 1 NEW_LINE DEDENT for x in range ( self . cnn_layers - 1 ) : NEW_LINE INDENT conv_list . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE DEDENT self . conv = nn . Sequential ( * conv_list ) NEW_LINE rnn_input_size = map NEW_LINE rnns = [ ] NEW_LINE rnn = BatchRNN ( input_size = rnn_input_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional , batch_norm = False ) NEW_LINE rnns . append ( ( \\'0\\' , rnn ) ) NEW_LINE for x in range ( self . rnn_layers - 1 ) : NEW_LINE INDENT rnn = BatchRNN ( input_size = rnn_hidden_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional ) NEW_LINE rnns . append ( ( \\' % d \\' % ( x + 1 ) , rnn ) ) NEW_LINE DEDENT self . rnns = nn . Sequential ( OrderedDict ( rnns ) ) NEW_LINE fully_connected = nn . Sequential ( nn . BatchNorm1d ( rnn_hidden_size ) , nn . Linear ( rnn_hidden_size , num_classes , bias = False ) ) NEW_LINE self . fc = nn . Sequential ( SequenceWise ( fully_connected ) , ) NEW_LINE self . inference_softmax = InferenceBatchSoftmax ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT x = self . conv ( x ) NEW_LINE x = x . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) NEW_LINE x = self . rnns ( x ) NEW_LINE x = self . fc ( x ) NEW_LINE x = x . transpose ( 0 , 1 ) NEW_LINE x = self . inference_softmax ( x ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def __init__ ( self , config , data_loader = None ) : NEW_LINE INDENT self . config = config NEW_LINE self . data_loader = data_loader NEW_LINE self . lr = config . lr NEW_LINE self . beta1 = config . beta1 NEW_LINE self . beta2 = config . beta2 NEW_LINE self . optimizer = config . optimizer NEW_LINE self . batch_size = config . batch_size NEW_LINE self . diffLoss = L1Loss_mask ( ) NEW_LINE self . valmin_iter = 0 NEW_LINE self . model_dir = ' logs / ' + str ( config . expnum ) NEW_LINE self . savename_G = ' ' NEW_LINE self . savename_D = ' ' NEW_LINE self . savename_ASR = ' ' NEW_LINE self . kt = 0 NEW_LINE self . lb = self . config . lambda_k NEW_LINE self . gamma = self . config . gamma NEW_LINE self . conv_measure = 0 NEW_LINE self . dce_tr = AverageMeter ( ) NEW_LINE self . dce_tr_local = AverageMeter ( ) NEW_LINE self . dce_val = AverageMeter ( ) NEW_LINE self . adv_ny_tr = AverageMeter ( ) NEW_LINE self . adv_ny_val = AverageMeter ( ) NEW_LINE self . wer_tr = AverageMeter ( ) NEW_LINE self . wer_val = AverageMeter ( ) NEW_LINE self . cer_tr = AverageMeter ( ) NEW_LINE self . cer_val = AverageMeter ( ) NEW_LINE self . decoder = GreedyDecoder ( data_loader . labels ) NEW_LINE self . build_model ( ) NEW_LINE self . G . loss_stop = 100000 NEW_LINE if self . config . gpu >= 0 : NEW_LINE INDENT self . G . cuda ( ) NEW_LINE self . D . cuda ( ) NEW_LINE self . diffLoss . cuda ( ) NEW_LINE self . ASR . cuda ( ) NEW_LINE DEDENT if len ( self . config . load_path ) > 0 : NEW_LINE INDENT self . load_model ( ) NEW_LINE DEDENT if config . mode == ' train ' : NEW_LINE INDENT self . logFile = open ( self . model_dir + ' / log . txt ' , ' w ' ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def zero_grad_all ( self ) : NEW_LINE INDENT self . G . zero_grad ( ) NEW_LINE self . D . zero_grad ( ) NEW_LINE self . ASR . zero_grad ( ) NEW_LINE DEDENT',\n",
              " \"def build_model ( self ) : NEW_LINE INDENT print ( ' initialize ▁ enhancement ▁ & ▁ discriminator ▁ model ' ) NEW_LINE self . G = stackedBRNN ( I = self . config . nFeat_in , O = self . config . nFeat_out , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE self . D = stackedBRNN ( I = self . config . nFeat_D , O = self . config . nFeat_out , H = self . config . rnn_size , L = self . config . rnn_layers , rnn_type = supported_rnns [ self . config . rnn_type ] ) NEW_LINE print ( ' load ▁ pre - trained ▁ ASR ▁ model ' ) NEW_LINE package_ASR = torch . load ( self . config . ASR_path , map_location = lambda storage , loc : storage ) NEW_LINE self . ASR = DeepSpeech . load_model_package ( package_ASR ) NEW_LINE DEDENT\",\n",
              " 'def load_model ( self ) : NEW_LINE INDENT print ( \" [ * ] ▁ Load ▁ models ▁ from ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE postfix = \\' _ valmin \\' NEW_LINE paths = glob ( os . path . join ( self . config . load_path , \\' G { } * . pth \\' . format ( postfix ) ) ) NEW_LINE paths . sort ( ) NEW_LINE if len ( paths ) == 0 : NEW_LINE INDENT print ( \" [ ! ] ▁ No ▁ checkpoint ▁ found ▁ in ▁ { } . . . \" . format ( self . config . load_path ) ) NEW_LINE assert ( 0 ) , \\' checkpoint ▁ not ▁ avilable \\' NEW_LINE DEDENT idxes = [ int ( os . path . basename ( path . split ( \\' . \\' ) [ 0 ] . split ( \\' _ \\' ) [ - 1 ] ) ) for path in paths ] NEW_LINE if self . config . start_iter <= 0 : NEW_LINE INDENT self . config . start_iter = max ( idxes ) NEW_LINE if ( self . config . start_iter < 0 ) : NEW_LINE INDENT raise Exception ( \" start ▁ iter ▁ is ▁ still ▁ less ▁ than ▁ 0 ▁ - - > ▁ probably ▁ try ▁ to ▁ load ▁ initial ▁ random ▁ model \" ) NEW_LINE DEDENT DEDENT if self . config . gpu < 0 : NEW_LINE INDENT map_location = lambda storage , loc : storage NEW_LINE DEDENT else : NEW_LINE INDENT map_location = None NEW_LINE DEDENT print ( \\' Load ▁ models ▁ from ▁ \\' + self . config . load_path + \\' , ▁ ITERATION ▁ = ▁ \\' + str ( self . config . start_iter ) ) NEW_LINE self . G . load_state_dict ( torch . load ( \\' { } / G { } _ { } . pth \\' . format ( self . config . load_path , postfix , self . config . start_iter ) , map_location = map_location ) ) NEW_LINE print ( \" [ * ] ▁ Model ▁ loaded \" ) NEW_LINE DEDENT',\n",
              " 'def train ( self ) : NEW_LINE INDENT optimizer_g = torch . optim . Adam ( self . G . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE optimizer_d = torch . optim . Adam ( self . D . parameters ( ) , lr = self . config . lr , betas = ( self . beta1 , self . beta2 ) , amsgrad = True ) NEW_LINE for iter in trange ( self . config . start_iter , self . config . max_iter ) : NEW_LINE INDENT self . zero_grad_all ( ) NEW_LINE data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' train \\' ) NEW_LINE mixture , cleans , mask = _get_variable_nograd ( data_list [ 0 ] ) , _get_variable_nograd ( data_list [ 1 ] ) , _get_variable_nograd ( data_list [ 2 ] ) NEW_LINE enhanced = self . G ( mixture ) NEW_LINE enhanced_D = enhanced . detach ( ) NEW_LINE ae_ny_G = self . D . forward_paired ( enhanced , mixture ) NEW_LINE l_adv_ny_G , _ = self . diffLoss ( ae_ny_G , enhanced , mask ) NEW_LINE l_adv_ny_G = l_adv_ny_G * self . config . w_adversarial NEW_LINE l_adv_ny_G_data = l_adv_ny_G . data [ 0 ] NEW_LINE l_adv_ny_G . backward ( retain_graph = True ) NEW_LINE g_adv = self . get_gradient_norm ( self . G ) NEW_LINE self . D . zero_grad ( ) NEW_LINE del l_adv_ny_G NEW_LINE ae_ny_D = self . D . forward_paired ( enhanced_D , mixture ) NEW_LINE l_adv_ny_D , _ = self . diffLoss ( ae_ny_D , enhanced_D , mask ) NEW_LINE l_adv_ny_D = l_adv_ny_D * ( - self . kt ) * self . config . w_adversarial NEW_LINE l_adv_ny_D . backward ( ) NEW_LINE del l_adv_ny_D NEW_LINE dce , nElement = self . diffLoss ( enhanced , cleans , mask ) NEW_LINE dce_loss = dce . data [ 0 ] NEW_LINE dce_tr_local . update ( dce_loss , nElement ) NEW_LINE ae_cl = self . D ( cleans , mixture ) NEW_LINE l_adv_cl , _ = self . diffLoss ( ae_cl , cleans , mask ) NEW_LINE l_adv_cl = self . config . w_adversarial * l_adv_cl NEW_LINE l_adv_cl . backward ( ) NEW_LINE l_adv_cl_data = l_adv_cl . data [ 0 ] NEW_LINE del l_adv_cl NEW_LINE optimizer_g . step ( ) NEW_LINE optimizer_d . step ( ) NEW_LINE g_d_balance = self . gamma * l_adv_cl_data - l_adv_ny_G_data NEW_LINE self . kt += self . lb * g_d_balance NEW_LINE self . kt = max ( min ( 1 , self . kt ) , 0 ) NEW_LINE conv_measure = l_adv_cl_data + abs ( g_d_balance ) NEW_LINE if ( iter + 1 ) % self . config . log_iter == 0 : NEW_LINE INDENT str_loss = \" [ { } / { } ] ▁ ( train ) ▁ DCE : ▁ { : . 7f } , ▁ ADV _ cl : ▁ { : . 7f } , ▁ ADV _ ny : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . dce_tr_local . avg , l_adv_cl_data , l_adv_ny_G_data ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE str_loss = \" [ { } / { } ] ▁ ( train ) ▁ conv _ measure : ▁ { : . 4f } , ▁ kt : ▁ { : . 4f } ▁ \" . format ( iter , self . config . max_iter , conv_measure , self . kt ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . dce_tr_local . reset ( ) NEW_LINE DEDENT if ( iter + 1 ) % self . config . save_iter == 0 : NEW_LINE INDENT self . G . eval ( ) NEW_LINE self . dce_tr . reset ( ) NEW_LINE self . adv_ny_tr . reset ( ) NEW_LINE self . wer_tr . reset ( ) NEW_LINE self . cer_tr . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . trsub_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' trsub \\' ) NEW_LINE mixture , cleans , mask , targets , input_percentages , target_sizes = data_list [ 0 ] , data_list [ 1 ] , _get_variable_volatile ( data_list [ 2 ] ) , data_list [ 3 ] , data_list [ 4 ] , data_list [ 5 ] NEW_LINE dce , adv_ny , nElement , wer , cer , nWord , nChar = self . greedy_decoding_and_FSEGAN ( mixture , cleans , targets , input_percentages , target_sizes , mask ) NEW_LINE self . dce_tr . update ( dce . data [ 0 ] , nElement ) NEW_LINE self . adv_ny_tr . update ( adv_ny . data [ 0 ] , nElement ) NEW_LINE self . wer_tr . update ( wer , nWord ) NEW_LINE self . cer_tr . update ( cer , nChar ) NEW_LINE del dce , adv_ny NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( training ▁ subset ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . dce_tr . avg , self . wer_tr . avg * 100 , self . cer_tr . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . dce_val . reset ( ) NEW_LINE self . adv_ny_val . reset ( ) NEW_LINE self . wer_val . reset ( ) NEW_LINE self . cer_val . reset ( ) NEW_LINE for _ in trange ( 0 , len ( self . data_loader . val_dl ) ) : NEW_LINE INDENT data_list = self . data_loader . next ( cl_ny = \\' ny \\' , type = \\' val \\' ) NEW_LINE mixture , cleans , mask , targets , input_percentages , target_sizes = data_list [ 0 ] , data_list [ 1 ] , _get_variable_volatile ( data_list [ 2 ] ) , data_list [ 3 ] , data_list [ 4 ] , data_list [ 5 ] NEW_LINE dce , adv_ny , nElement , wer , cer , nWord , nChar = self . greedy_decoding_and_FSEGAN ( mixture , cleans , targets , input_percentages , target_sizes , mask ) NEW_LINE self . dce_val . update ( dce . data [ 0 ] , nElement ) NEW_LINE self . adv_ny_val . update ( adv_ny . data [ 0 ] , nElement ) NEW_LINE self . wer_val . update ( wer , nWord ) NEW_LINE self . cer_val . update ( cer , nChar ) NEW_LINE del ctc , adv_ny NEW_LINE DEDENT str_loss = \" [ { } / { } ] ▁ ( validation ) ▁ CTC : ▁ { : . 7f } , ▁ WER : ▁ { : . 7f } , ▁ CER : ▁ { : . 7f } \" . format ( iter , self . config . max_iter , self . dce_val . avg , self . wer_val . avg * 100 , self . cer_val . avg * 100 ) NEW_LINE print ( str_loss ) NEW_LINE self . logFile . write ( str_loss + \\' \\\\n \\' ) NEW_LINE self . logFile . flush ( ) NEW_LINE self . G . train ( ) NEW_LINE if ( len ( self . savename_G ) > 0 ) : NEW_LINE INDENT if os . path . exists ( self . savename_G ) : NEW_LINE INDENT os . remove ( self . savename_G ) NEW_LINE DEDENT DEDENT self . savename_G = \\' { } / G _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE torch . save ( self . G . state_dict ( ) , self . savename_G ) NEW_LINE if ( self . G . loss_stop > self . wer_val . avg ) : NEW_LINE INDENT self . G . loss_stop = self . wer_val . avg NEW_LINE savename_G_valmin_prev = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , self . valmin_iter ) NEW_LINE if os . path . exists ( savename_G_valmin_prev ) : NEW_LINE INDENT os . remove ( savename_G_valmin_prev ) NEW_LINE DEDENT print ( \\' save ▁ model ▁ for ▁ this ▁ checkpoint \\' ) NEW_LINE savename_G_valmin = \\' { } / G _ valmin _ { } . pth \\' . format ( self . model_dir , iter ) NEW_LINE copyfile ( self . savename_G , savename_G_valmin ) NEW_LINE self . valmin_iter = iter NEW_LINE DEDENT DEDENT DEDENT DEDENT',\n",
              " \"def greedy_decoding_and_FSEGAN ( self , mixture , cleans , targets , input_percentages , target_sizes , mask , transcript_prob = 0.001 ) : NEW_LINE INDENT mixture = _get_variable_volatile ( mixture ) NEW_LINE N = inputs . size ( 0 ) NEW_LINE split_targets = [ ] NEW_LINE offset = 0 NEW_LINE for size in target_sizes : NEW_LINE INDENT split_targets . append ( targets [ offset : offset + size ] ) NEW_LINE offset += size NEW_LINE DEDENT enhanced = self . G ( mixture ) NEW_LINE prob = self . ASR ( enhanced ) NEW_LINE prob = prob . transpose ( 0 , 1 ) NEW_LINE T = prob . size ( 0 ) NEW_LINE sizes = input_percentages . mul_ ( int ( T ) ) . int ( ) NEW_LINE decoded_output , _ = self . decoder . decode ( prob . data , sizes ) NEW_LINE target_strings = self . decoder . convert_to_strings ( split_targets ) NEW_LINE we , ce , total_word , total_char = 0 , 0 , 0 , 0 NEW_LINE for x in range ( len ( target_strings ) ) : NEW_LINE INDENT decoding , reference = decoded_output [ x ] [ 0 ] , target_strings [ x ] [ 0 ] NEW_LINE nChar = len ( reference ) NEW_LINE nWord = len ( reference . split ( ) ) NEW_LINE we_i = self . decoder . wer ( decoding , reference ) NEW_LINE ce_i = self . decoder . cer ( decoding , reference ) NEW_LINE we += we_i NEW_LINE ce += ce_i NEW_LINE total_word += nWord NEW_LINE total_char += nChar NEW_LINE if ( random . uniform ( 0 , 1 ) < transcript_prob ) : NEW_LINE INDENT print ( ' reference ▁ = ▁ ' + reference ) NEW_LINE print ( ' decoding ▁ = ▁ ' + decoding ) NEW_LINE print ( ' wer ▁ = ▁ ' + str ( we_i / float ( nWord ) ) + ' , ▁ cer ▁ = ▁ ' + str ( ce_i / float ( nChar ) ) ) NEW_LINE DEDENT DEDENT wer = we / total_word NEW_LINE cer = ce / total_word NEW_LINE ae_ny = self . D . forward_paired ( enhanced , mixture ) NEW_LINE l_adv_ny , nElement = self . diffLoss ( ae_ny , enhanced , mask ) NEW_LINE l_adv_ny = l_adv_ny * self . config . w_adversarial NEW_LINE dce , nElement_ = self . diffLoss ( enhanced , cleans , mask ) NEW_LINE assert ( nElement == nElement_ ) NEW_LINE return dce , l_adv_ny , nElement , wer , cer , total_word , total_char NEW_LINE DEDENT\",\n",
              " \"def str2bool ( v ) : NEW_LINE INDENT return v . lower ( ) in ( ' true ' , '1' ) NEW_LINE DEDENT\",\n",
              " 'def add_argument_group ( name ) : NEW_LINE INDENT arg = parser . add_argument_group ( name ) NEW_LINE arg_lists . append ( arg ) NEW_LINE return arg NEW_LINE DEDENT',\n",
              " \"def get_config ( ) : NEW_LINE INDENT config , unparsed = parser . parse_known_args ( ) NEW_LINE if ( len ( unparsed ) > 0 ) : NEW_LINE INDENT print ( unparsed ) NEW_LINE assert ( len ( unparsed ) == 0 ) , ' length ▁ of ▁ unparsed ▁ option ▁ should ▁ be ▁ 0' NEW_LINE DEDENT return config , unparsed NEW_LINE DEDENT\",\n",
              " 'def random_combination ( iterable , r ) : NEW_LINE INDENT pool = tuple ( iterable ) NEW_LINE n = len ( pool ) NEW_LINE indices = sorted ( random . sample ( range ( n ) , r ) ) NEW_LINE return tuple ( pool [ i ] for i in indices ) NEW_LINE DEDENT',\n",
              " \"def check_config_used ( config , target_source ) : NEW_LINE INDENT config_count = dict ( vars ( config ) ) NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT config_count [ k ] = 0 NEW_LINE DEDENT for source in target_source : NEW_LINE INDENT fp = open ( source , ' r ' ) NEW_LINE text = fp . read ( ) NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT if ( text . find ( k ) >= 0 ) : NEW_LINE INDENT config_count [ k ] = 1 NEW_LINE DEDENT DEDENT fp . close ( ) NEW_LINE DEDENT config_unused = [ ] NEW_LINE for k in config_count . keys ( ) : NEW_LINE INDENT if ( config_count [ k ] == 0 ) : NEW_LINE INDENT config_unused . append ( k ) NEW_LINE DEDENT DEDENT print ( ' unused ▁ config ▁ = ▁ ' ) NEW_LINE print ( config_unused ) NEW_LINE assert ( len ( config_unused ) == 0 ) , ' unused ▁ config ▁ exists , ▁ please ▁ properly ▁ use ▁ it ▁ or ▁ comment ▁ it ' NEW_LINE DEDENT\",\n",
              " 'def to_np ( x ) : NEW_LINE INDENT return x . data . cpu ( ) . numpy ( ) NEW_LINE DEDENT',\n",
              " \"def get_weight_statistic ( M ) : NEW_LINE INDENT print ( ' Model ▁ parameter ▁ statistic ' ) NEW_LINE modules = list ( M . modules ( ) ) [ 0 ] . _modules NEW_LINE for k , v in modules . items ( ) : NEW_LINE INDENT if ( len ( v . state_dict ( ) ) > 2 ) : NEW_LINE INDENT for l in range ( len ( v ) ) : NEW_LINE INDENT layer = v [ l ] NEW_LINE if ( hasattr ( layer , ' module ' ) ) : NEW_LINE INDENT layer_m = layer . module NEW_LINE for j in range ( len ( layer_m ) ) : NEW_LINE INDENT sublayer = layer_m [ j ] NEW_LINE if ( hasattr ( sublayer , ' bias ' ) ) : NEW_LINE INDENT if ( sublayer . bias is not None ) : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( sublayer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT print ( str ( sublayer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( sublayer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( sublayer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( sublayer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT else : NEW_LINE INDENT if ( hasattr ( layer , ' weight ' ) ) : NEW_LINE INDENT if ( hasattr ( layer , ' bias ' ) ) : NEW_LINE INDENT if ( hasattr ( layer . bias , ' data ' ) ) : NEW_LINE INDENT print ( str ( layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( layer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT print ( str ( layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT if ( hasattr ( layer , ' rnn ' ) ) : NEW_LINE INDENT rnn_layer = layer . rnn NEW_LINE print ( str ( rnn_layer ) ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0 . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0 . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0 . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 _ reverse ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0_reverse . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ ih _ l0 ▁ = ▁ ' + str ( rnn_layer . weight_ih_l0 . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_ih_l0 . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_ih_l0 . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE print ( ' \\\\tweight _ hh _ l0 _ reverse ▁ = ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( rnn_layer . weight_hh_l0_reverse . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( rnn_layer . weight_hh_l0_reverse . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT if ( hasattr ( layer , ' batch _ norm ' ) ) : NEW_LINE INDENT if ( layer . batch_norm ) : NEW_LINE INDENT bn_layer = layer . batch_norm . module NEW_LINE print ( str ( bn_layer ) + ' ▁ : ▁ weight ▁ = ▁ ' + str ( bn_layer . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( bn_layer . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( bn_layer . weight . data . mean ( ) ) ) [ : 7 ] + ' ) , ▁ bias ▁ = ▁ ' + str ( bn_layer . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( bn_layer . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( bn_layer . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT DEDENT DEDENT else : NEW_LINE INDENT if ( hasattr ( v , ' weight ' ) ) : NEW_LINE INDENT if ( hasattr ( v , ' bias ' ) ) : NEW_LINE INDENT print ( k + ' ▁ : ▁ weight ▁ = ▁ ' + str ( v . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ' , ▁ bias ▁ = ▁ ' + str ( v . bias . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . bias . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . bias . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( k + ' ▁ : ▁ weight ▁ = ▁ ' + str ( v . weight . data . min ( ) ) [ : 5 ] + ' ▁ ~ ▁ ' + str ( v . weight . data . max ( ) ) [ : 5 ] + ' ▁ ( ' + str ( decimal . Decimal ( v . weight . data . mean ( ) ) ) [ : 7 ] + ' ) ' ) NEW_LINE DEDENT DEDENT DEDENT print ( ' ▁ ' ) NEW_LINE print ( ' ▁ ' ) NEW_LINE DEDENT DEDENT\",\n",
              " \"def weights_init ( m ) : NEW_LINE INDENT classname = m . __class__ . __name__ NEW_LINE if classname . find ( ' Conv ' ) != - 1 and classname . find ( ' ConvResidualBlock ' ) == - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.1 ) NEW_LINE if hasattr ( m , ' bias ' ) : NEW_LINE INDENT if ( hasattr ( m . bias , ' data ' ) ) : NEW_LINE INDENT m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT DEDENT DEDENT elif classname . find ( ' BatchNorm ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 1.0 , 0.01 ) NEW_LINE m . bias . data . fill_ ( 0 ) NEW_LINE DEDENT elif classname . find ( ' Embedding ' ) != - 1 : NEW_LINE INDENT m . weight . data . normal_ ( 0.0 , 0.01 ) NEW_LINE DEDENT DEDENT\",\n",
              " 'def _get_variable ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def _get_variable_volatile ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) , volatile = True ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs , volatile = True ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def _get_variable_nograd ( inputs , cuda = True ) : NEW_LINE INDENT if ( cuda ) : NEW_LINE INDENT out = Variable ( inputs . cuda ( ) , requires_grad = False ) NEW_LINE DEDENT else : NEW_LINE INDENT out = Variable ( inputs , requires_grad = False ) NEW_LINE DEDENT return out NEW_LINE DEDENT',\n",
              " 'def __init__ ( self ) : NEW_LINE INDENT self . reset ( ) NEW_LINE DEDENT',\n",
              " 'def reset ( self ) : NEW_LINE INDENT self . val = 0 NEW_LINE self . avg = 0 NEW_LINE self . sum = 0 NEW_LINE self . count = 0 NEW_LINE DEDENT',\n",
              " 'def update ( self , val , n = 1 ) : NEW_LINE INDENT self . val = val NEW_LINE self . sum += val * n NEW_LINE self . count += n NEW_LINE self . avg = self . sum / self . count NEW_LINE DEDENT',\n",
              " 'def decode_dataset ( logits , test_dataset , batch_size , lm_alpha , lm_beta , mesh_x , mesh_y , labels ) : NEW_LINE INDENT print ( \" Beginning ▁ decode ▁ for ▁ { } , ▁ { } \" . format ( lm_alpha , lm_beta ) ) NEW_LINE test_loader = FeatLoader ( test_dataset , batch_size = batch_size , num_workers = 0 ) NEW_LINE target_decoder = GreedyDecoder ( labels , blank_index = labels . index ( \\' _ \\' ) ) NEW_LINE decoder = BeamCTCDecoder ( labels , beam_width = args . beam_width , cutoff_top_n = args . cutoff_top_n , blank_index = labels . index ( \\' _ \\' ) , lm_path = args . lm_path , alpha = lm_alpha , beta = lm_beta , num_processes = 1 ) NEW_LINE total_cer , total_wer = 0 , 0 NEW_LINE for i , ( data ) in enumerate ( test_loader ) : NEW_LINE INDENT inputs , targets , input_percentages , target_sizes = data NEW_LINE split_targets = [ ] NEW_LINE offset = 0 NEW_LINE for size in target_sizes : NEW_LINE INDENT split_targets . append ( targets [ offset : offset + size ] ) NEW_LINE offset += size NEW_LINE DEDENT out = torch . from_numpy ( logits [ i ] [ 0 ] ) NEW_LINE sizes = torch . from_numpy ( logits [ i ] [ 1 ] ) NEW_LINE decoded_output , _ = decoder . decode ( out , sizes ) NEW_LINE target_strings = target_decoder . convert_to_strings ( split_targets ) NEW_LINE wer , cer = 0 , 0 NEW_LINE for x in range ( len ( target_strings ) ) : NEW_LINE INDENT transcript , reference = decoded_output [ x ] [ 0 ] , target_strings [ x ] [ 0 ] NEW_LINE wer_inst = decoder . wer ( transcript , reference ) / float ( len ( reference . split ( ) ) ) NEW_LINE cer_inst = decoder . cer ( transcript , reference ) / float ( len ( reference ) ) NEW_LINE wer += wer_inst NEW_LINE cer += cer_inst NEW_LINE if ( random . uniform ( 0 , 1 ) < float ( args . detail_log_print_prob ) ) : NEW_LINE INDENT print ( \\' decoding ▁ : ▁ \\' + transcript ) NEW_LINE print ( \\' reference ▁ : ▁ \\' + reference ) NEW_LINE print ( \\' WER ▁ = ▁ \\' + str ( wer_inst ) + \\' , ▁ CER ▁ = ▁ \\' + str ( cer_inst ) ) NEW_LINE print ( \\' ▁ \\' ) NEW_LINE logger . error ( \\' decoding ▁ : ▁ \\' + transcript ) NEW_LINE logger . error ( \\' reference ▁ : ▁ \\' + reference ) NEW_LINE logger . error ( \\' WER ▁ = ▁ \\' + str ( wer_inst ) + \\' , ▁ CER ▁ = ▁ \\' + str ( cer_inst ) ) NEW_LINE logger . error ( \\' ▁ \\' ) NEW_LINE DEDENT DEDENT total_cer += cer NEW_LINE total_wer += wer NEW_LINE DEDENT wer = total_wer / len ( test_loader . dataset ) NEW_LINE cer = total_cer / len ( test_loader . dataset ) NEW_LINE return [ mesh_x , mesh_y , lm_alpha , lm_beta , wer , cer ] NEW_LINE DEDENT',\n",
              " 'def getWER ( item ) : NEW_LINE INDENT return item [ 5 ] NEW_LINE DEDENT',\n",
              " 'def result_callback ( result ) : NEW_LINE INDENT results . append ( result ) NEW_LINE DEDENT',\n",
              " \"def __init__ ( self , labels , blank_index = 0 ) : NEW_LINE INDENT self . labels = labels NEW_LINE self . int_to_char = dict ( [ ( i , c ) for ( i , c ) in enumerate ( labels ) ] ) NEW_LINE self . blank_index = blank_index NEW_LINE space_index = len ( labels ) NEW_LINE if ' ▁ ' in labels : NEW_LINE INDENT space_index = labels . index ( ' ▁ ' ) NEW_LINE DEDENT self . space_index = space_index NEW_LINE DEDENT\",\n",
              " \"def wer ( self , s1 , s2 ) : NEW_LINE INDENT b = set ( s1 . split ( ) + s2 . split ( ) ) NEW_LINE word2char = dict ( zip ( b , range ( len ( b ) ) ) ) NEW_LINE w1 = [ chr ( word2char [ w ] ) for w in s1 . split ( ) ] NEW_LINE w2 = [ chr ( word2char [ w ] ) for w in s2 . split ( ) ] NEW_LINE return Lev . distance ( ' ' . join ( w1 ) , ' ' . join ( w2 ) ) NEW_LINE DEDENT\",\n",
              " \"def cer ( self , s1 , s2 ) : NEW_LINE INDENT s1 , s2 , = s1 . replace ( ' ▁ ' , ' ' ) , s2 . replace ( ' ▁ ' , ' ' ) NEW_LINE return Lev . distance ( s1 , s2 ) NEW_LINE DEDENT\",\n",
              " 'def decode ( self , probs , sizes = None ) : NEW_LINE INDENT raise NotImplementedError NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , labels , lm_path = None , alpha = 0 , beta = 0 , cutoff_top_n = 40 , cutoff_prob = 1.0 , beam_width = 100 , num_processes = 4 , blank_index = 0 ) : NEW_LINE INDENT super ( BeamCTCDecoder , self ) . __init__ ( labels ) NEW_LINE try : NEW_LINE INDENT from ctcdecode import CTCBeamDecoder NEW_LINE DEDENT except ImportError : NEW_LINE INDENT raise ImportError ( \" BeamCTCDecoder ▁ requires ▁ paddledecoder ▁ package . \" ) NEW_LINE DEDENT self . _decoder = CTCBeamDecoder ( labels , lm_path , alpha , beta , cutoff_top_n , cutoff_prob , beam_width , num_processes , blank_index ) NEW_LINE DEDENT',\n",
              " \"def convert_to_strings ( self , out , seq_len ) : NEW_LINE INDENT results = [ ] NEW_LINE for b , batch in enumerate ( out ) : NEW_LINE INDENT utterances = [ ] NEW_LINE for p , utt in enumerate ( batch ) : NEW_LINE INDENT size = seq_len [ b ] [ p ] NEW_LINE if size > 0 : NEW_LINE INDENT transcript = ' ' . join ( map ( lambda x : self . int_to_char [ x ] , utt [ 0 : size ] ) ) NEW_LINE DEDENT else : NEW_LINE INDENT transcript = ' ' NEW_LINE DEDENT utterances . append ( transcript ) NEW_LINE DEDENT results . append ( utterances ) NEW_LINE DEDENT return results NEW_LINE DEDENT\",\n",
              " 'def convert_tensor ( self , offsets , sizes ) : NEW_LINE INDENT results = [ ] NEW_LINE for b , batch in enumerate ( offsets ) : NEW_LINE INDENT utterances = [ ] NEW_LINE for p , utt in enumerate ( batch ) : NEW_LINE INDENT size = sizes [ b ] [ p ] NEW_LINE if sizes [ b ] [ p ] > 0 : NEW_LINE INDENT utterances . append ( utt [ 0 : size ] ) NEW_LINE DEDENT else : NEW_LINE INDENT utterances . append ( torch . IntTensor ( ) ) NEW_LINE DEDENT DEDENT results . append ( utterances ) NEW_LINE DEDENT return results NEW_LINE DEDENT',\n",
              " 'def decode ( self , probs , sizes = None ) : NEW_LINE INDENT probs = probs . cpu ( ) . transpose ( 0 , 1 ) . contiguous ( ) NEW_LINE out , scores , offsets , seq_lens = self . _decoder . decode ( probs ) NEW_LINE strings = self . convert_to_strings ( out , seq_lens ) NEW_LINE offsets = self . convert_tensor ( offsets , seq_lens ) NEW_LINE return strings , offsets NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , labels , blank_index = 0 ) : NEW_LINE INDENT super ( GreedyDecoder , self ) . __init__ ( labels , blank_index ) NEW_LINE DEDENT',\n",
              " \"def process_string ( self , sequence , size , remove_repetitions = False ) : NEW_LINE INDENT string = ' ' NEW_LINE offsets = [ ] NEW_LINE for i in range ( size ) : NEW_LINE INDENT if ( sequence [ i ] != self . blank_index ) : NEW_LINE INDENT char = self . int_to_char [ sequence [ i ] ] NEW_LINE if remove_repetitions and i != 0 and char == self . int_to_char [ sequence [ i - 1 ] ] : NEW_LINE INDENT pass NEW_LINE DEDENT elif char == self . labels [ self . space_index ] : NEW_LINE INDENT string += ' ▁ ' NEW_LINE offsets . append ( i ) NEW_LINE DEDENT else : NEW_LINE INDENT string = string + char NEW_LINE offsets . append ( i ) NEW_LINE DEDENT DEDENT DEDENT return string , torch . IntTensor ( offsets ) NEW_LINE DEDENT\",\n",
              " 'def decode ( self , probs , sizes = None ) : NEW_LINE INDENT _ , max_probs = torch . max ( probs . transpose ( 0 , 1 ) , 2 ) NEW_LINE strings , offsets = self . convert_to_strings ( max_probs . view ( max_probs . size ( 0 ) , max_probs . size ( 1 ) ) , sizes , remove_repetitions = True , return_offsets = True ) NEW_LINE return strings , offsets NEW_LINE DEDENT',\n",
              " \"def str2bool ( v ) : NEW_LINE INDENT return v . lower ( ) in ( ' true ' , '1' ) NEW_LINE DEDENT\",\n",
              " \"def str2bool ( v ) : NEW_LINE INDENT return v . lower ( ) in ( ' true ' , '1' ) NEW_LINE DEDENT\",\n",
              " \"def load_model ( cls , path , gpu = - 1 ) : NEW_LINE INDENT package = torch . load ( path , map_location = lambda storage , loc : storage ) NEW_LINE model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] ) NEW_LINE blacklist = [ ' rnns . 0 . batch _ norm . module . weight ' , ' rnns . 0 . batch _ norm . module . bias ' , ' rnns . 0 . batch _ norm . module . running _ mean ' , ' rnns . 0 . batch _ norm . module . running _ var ' ] NEW_LINE for x in blacklist : NEW_LINE INDENT if x in package [ ' state _ dict ' ] : NEW_LINE INDENT del package [ ' state _ dict ' ] [ x ] NEW_LINE DEDENT DEDENT model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE for x in model . rnns : NEW_LINE INDENT x . flatten_parameters ( ) NEW_LINE DEDENT if gpu >= 0 : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def load_model_package ( cls , package , gpu = - 1 ) : NEW_LINE INDENT model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] , ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE if ( gpu >= 0 ) : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def serialize ( model , optimizer = None , epoch = None , iteration = None , loss_results = None , cer_results = None , wer_results = None , avg_loss = None , meta = None ) : NEW_LINE INDENT package = { ' version ' : model . _version , ' rnn _ size ' : model . rnn_size , ' rnn _ layers ' : model . rnn_layers , ' cnn _ map ' : model . cnn_map , ' cnn _ kernel ' : model . cnn_kernel , ' cnn _ stride ' : model . cnn_stride , ' cnn _ layers ' : model . cnn_layers , ' rnn _ type ' : supported_rnns_inv . get ( model . rnn_type , model . rnn_type . __name__ . lower ( ) ) , ' labels ' : model . _labels , ' state _ dict ' : model . state_dict ( ) } NEW_LINE if optimizer is not None : NEW_LINE INDENT package [ ' optim _ dict ' ] = optimizer . state_dict ( ) NEW_LINE DEDENT if avg_loss is not None : NEW_LINE INDENT package [ ' avg _ loss ' ] = avg_loss NEW_LINE DEDENT if epoch is not None : NEW_LINE INDENT package [ ' epoch ' ] = epoch + 1 NEW_LINE DEDENT if iteration is not None : NEW_LINE INDENT package [ ' iteration ' ] = iteration NEW_LINE DEDENT if loss_results is not None : NEW_LINE INDENT package [ ' loss _ results ' ] = loss_results NEW_LINE package [ ' cer _ results ' ] = cer_results NEW_LINE package [ ' wer _ results ' ] = wer_results NEW_LINE DEDENT if meta is not None : NEW_LINE INDENT package [ ' meta ' ] = meta NEW_LINE DEDENT return package NEW_LINE DEDENT\",\n",
              " 'def get_labels ( model ) : NEW_LINE INDENT return model . _labels NEW_LINE DEDENT',\n",
              " 'def get_param_size ( model ) : NEW_LINE INDENT params = 0 NEW_LINE for p in model . parameters ( ) : NEW_LINE INDENT tmp = 1 NEW_LINE for x in p . size ( ) : NEW_LINE INDENT tmp *= x NEW_LINE DEDENT params += tmp NEW_LINE DEDENT return params NEW_LINE DEDENT',\n",
              " 'def get_audio_conf ( model ) : NEW_LINE INDENT return model . _audio_conf NEW_LINE DEDENT',\n",
              " 'def get_meta ( model ) : NEW_LINE INDENT model_is_cuda = next ( model . parameters ( ) ) . is_cuda NEW_LINE m = model . module if model_is_cuda else model NEW_LINE meta = { \" version \" : m . _version , \" rnn _ size \" : m . rnn_size , \" rnn _ layers \" : m . rnn_layers , \" cnn _ map \" : m . cnn_map , \" cnn _ kernel \" : m . cnn_kernel , \" cnn _ stride \" : m . cnn_stride , \" cnn _ layers \" : m . cnn_layers , \" rnn _ type \" : supported_rnns_inv [ m . rnn_type ] } NEW_LINE return meta NEW_LINE DEDENT',\n",
              " \"def load_model ( cls , path , gpu = - 1 ) : NEW_LINE INDENT package = torch . load ( path , map_location = lambda storage , loc : storage ) NEW_LINE model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] ) NEW_LINE blacklist = [ ' rnns . 0 . batch _ norm . module . weight ' , ' rnns . 0 . batch _ norm . module . bias ' , ' rnns . 0 . batch _ norm . module . running _ mean ' , ' rnns . 0 . batch _ norm . module . running _ var ' ] NEW_LINE for x in blacklist : NEW_LINE INDENT if x in package [ ' state _ dict ' ] : NEW_LINE INDENT del package [ ' state _ dict ' ] [ x ] NEW_LINE DEDENT DEDENT model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE for x in model . rnns : NEW_LINE INDENT x . flatten_parameters ( ) NEW_LINE DEDENT if gpu >= 0 : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def load_model_package ( cls , package , gpu = - 1 ) : NEW_LINE INDENT model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_layers = package [ ' cnn _ layers ' ] , labels = package [ ' labels ' ] , ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE if ( gpu >= 0 ) : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def serialize ( model , optimizer = None , epoch = None , iteration = None , loss_results = None , cer_results = None , wer_results = None , avg_loss = None , param_norm = None , param_max = None , grad_norm = None , grad_max = None , meta = None ) : NEW_LINE INDENT package = { ' version ' : model . _version , ' rnn _ size ' : model . rnn_size , ' rnn _ layers ' : model . rnn_layers , ' cnn _ map ' : model . cnn_map , ' cnn _ kernel ' : model . cnn_kernel , ' cnn _ stride ' : model . cnn_stride , ' cnn _ layers ' : model . cnn_layers , ' rnn _ type ' : supported_rnns_inv . get ( model . rnn_type , model . rnn_type . __name__ . lower ( ) ) , ' labels ' : model . _labels , ' state _ dict ' : model . state_dict ( ) } NEW_LINE if optimizer is not None : NEW_LINE INDENT package [ ' optim _ dict ' ] = optimizer . state_dict ( ) NEW_LINE DEDENT if avg_loss is not None : NEW_LINE INDENT package [ ' avg _ loss ' ] = avg_loss NEW_LINE DEDENT if epoch is not None : NEW_LINE INDENT package [ ' epoch ' ] = epoch + 1 NEW_LINE DEDENT if iteration is not None : NEW_LINE INDENT package [ ' iteration ' ] = iteration NEW_LINE DEDENT if loss_results is not None : NEW_LINE INDENT package [ ' loss _ results ' ] = loss_results NEW_LINE package [ ' cer _ results ' ] = cer_results NEW_LINE package [ ' wer _ results ' ] = wer_results NEW_LINE package [ ' param _ norm ' ] = param_norm NEW_LINE package [ ' param _ max ' ] = param_max NEW_LINE package [ ' grad _ norm ' ] = grad_norm NEW_LINE package [ ' grad _ max ' ] = grad_max NEW_LINE DEDENT if meta is not None : NEW_LINE INDENT package [ ' meta ' ] = meta NEW_LINE DEDENT return package NEW_LINE DEDENT\",\n",
              " 'def get_labels ( model ) : NEW_LINE INDENT return model . _labels NEW_LINE DEDENT',\n",
              " 'def get_param_size ( model ) : NEW_LINE INDENT params = 0 NEW_LINE for p in model . parameters ( ) : NEW_LINE INDENT tmp = 1 NEW_LINE for x in p . size ( ) : NEW_LINE INDENT tmp *= x NEW_LINE DEDENT params += tmp NEW_LINE DEDENT return params NEW_LINE DEDENT',\n",
              " 'def get_audio_conf ( model ) : NEW_LINE INDENT return model . _audio_conf NEW_LINE DEDENT',\n",
              " 'def get_meta ( model ) : NEW_LINE INDENT model_is_cuda = next ( model . parameters ( ) ) . is_cuda NEW_LINE m = model . module if model_is_cuda else model NEW_LINE meta = { \" version \" : m . _version , \" rnn _ size \" : m . rnn_size , \" rnn _ layers \" : m . rnn_layers , \" cnn _ map \" : m . cnn_map , \" cnn _ kernel \" : m . cnn_kernel , \" cnn _ stride \" : m . cnn_stride , \" cnn _ layers \" : m . cnn_layers , \" rnn _ type \" : supported_rnns_inv [ m . rnn_type ] } NEW_LINE return meta NEW_LINE DEDENT',\n",
              " \"def load_model ( cls , path , gpu = - 1 ) : NEW_LINE INDENT package = torch . load ( path , map_location = lambda storage , loc : storage ) NEW_LINE model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_residual_blocks = package [ ' cnn _ residual _ blocks ' ] , labels = package [ ' labels ' ] ) NEW_LINE blacklist = [ ' rnns . 0 . batch _ norm . module . weight ' , ' rnns . 0 . batch _ norm . module . bias ' , ' rnns . 0 . batch _ norm . module . running _ mean ' , ' rnns . 0 . batch _ norm . module . running _ var ' ] NEW_LINE for x in blacklist : NEW_LINE INDENT if x in package [ ' state _ dict ' ] : NEW_LINE INDENT del package [ ' state _ dict ' ] [ x ] NEW_LINE DEDENT DEDENT model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE for x in model . rnns : NEW_LINE INDENT x . flatten_parameters ( ) NEW_LINE DEDENT if gpu >= 0 : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def load_model_package ( cls , package , gpu = - 1 ) : NEW_LINE INDENT model = cls ( rnn_hidden_size = package [ ' rnn _ size ' ] , rnn_layers = package [ ' rnn _ layers ' ] , rnn_type = supported_rnns [ package [ ' rnn _ type ' ] ] , map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , cnn_residual_blocks = package [ ' cnn _ residual _ blocks ' ] , labels = package [ ' labels ' ] , ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE if ( gpu >= 0 ) : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def serialize ( model , optimizer = None , epoch = None , iteration = None , loss_results = None , cer_results = None , wer_results = None , avg_loss = None , meta = None ) : NEW_LINE INDENT package = { ' version ' : model . _version , ' rnn _ size ' : model . rnn_size , ' rnn _ layers ' : model . rnn_layers , ' cnn _ map ' : model . cnn_map , ' cnn _ kernel ' : model . cnn_kernel , ' cnn _ stride ' : model . cnn_stride , ' cnn _ residual _ blocks ' : model . cnn_residual_blocks , ' rnn _ type ' : supported_rnns_inv . get ( model . rnn_type , model . rnn_type . __name__ . lower ( ) ) , ' labels ' : model . _labels , ' state _ dict ' : model . state_dict ( ) } NEW_LINE if optimizer is not None : NEW_LINE INDENT package [ ' optim _ dict ' ] = optimizer . state_dict ( ) NEW_LINE DEDENT if avg_loss is not None : NEW_LINE INDENT package [ ' avg _ loss ' ] = avg_loss NEW_LINE DEDENT if epoch is not None : NEW_LINE INDENT package [ ' epoch ' ] = epoch + 1 NEW_LINE DEDENT if iteration is not None : NEW_LINE INDENT package [ ' iteration ' ] = iteration NEW_LINE DEDENT if loss_results is not None : NEW_LINE INDENT package [ ' loss _ results ' ] = loss_results NEW_LINE package [ ' cer _ results ' ] = cer_results NEW_LINE package [ ' wer _ results ' ] = wer_results NEW_LINE DEDENT if meta is not None : NEW_LINE INDENT package [ ' meta ' ] = meta NEW_LINE DEDENT return package NEW_LINE DEDENT\",\n",
              " 'def get_labels ( model ) : NEW_LINE INDENT return model . _labels NEW_LINE DEDENT',\n",
              " 'def get_param_size ( model ) : NEW_LINE INDENT params = 0 NEW_LINE for p in model . parameters ( ) : NEW_LINE INDENT tmp = 1 NEW_LINE for x in p . size ( ) : NEW_LINE INDENT tmp *= x NEW_LINE DEDENT params += tmp NEW_LINE DEDENT return params NEW_LINE DEDENT',\n",
              " 'def get_audio_conf ( model ) : NEW_LINE INDENT return model . _audio_conf NEW_LINE DEDENT',\n",
              " 'def get_meta ( model ) : NEW_LINE INDENT model_is_cuda = next ( model . parameters ( ) ) . is_cuda NEW_LINE m = model . module if model_is_cuda else model NEW_LINE meta = { \" version \" : m . _version , \" rnn _ size \" : m . rnn_size , \" rnn _ layers \" : m . rnn_layers , \" cnn _ map \" : m . cnn_map , \" cnn _ kernel \" : m . cnn_kernel , \" cnn _ stride \" : m . cnn_stride , \" cnn _ layers \" : m . cnn_layers , \" rnn _ type \" : supported_rnns_inv [ m . rnn_type ] } NEW_LINE return meta NEW_LINE DEDENT',\n",
              " \"def load_model ( cls , path , gpu = - 1 ) : NEW_LINE INDENT package = torch . load ( path , map_location = lambda storage , loc : storage ) NEW_LINE model = cls ( map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , labels = package [ ' labels ' ] ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE for x in model . rnns : NEW_LINE INDENT x . flatten_parameters ( ) NEW_LINE DEDENT if gpu >= 0 : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def load_model_package ( cls , package , gpu = - 1 ) : NEW_LINE INDENT model = cls ( map = package [ ' cnn _ map ' ] , stride = package [ ' cnn _ stride ' ] , kernel_sz = package [ ' cnn _ kernel ' ] , labels = package [ ' labels ' ] , ) NEW_LINE model . load_state_dict ( package [ ' state _ dict ' ] ) NEW_LINE if ( gpu >= 0 ) : NEW_LINE INDENT model = model . cuda ( ) NEW_LINE DEDENT return model NEW_LINE DEDENT\",\n",
              " \"def serialize ( model , optimizer = None , epoch = None , iteration = None , loss_results = None , cer_results = None , wer_results = None , avg_loss = None , meta = None ) : NEW_LINE INDENT package = { ' version ' : model . _version , ' cnn _ map ' : model . cnn_map , ' cnn _ kernel ' : model . cnn_kernel , ' cnn _ stride ' : model . cnn_stride , ' labels ' : model . _labels , ' state _ dict ' : model . state_dict ( ) } NEW_LINE if optimizer is not None : NEW_LINE INDENT package [ ' optim _ dict ' ] = optimizer . state_dict ( ) NEW_LINE DEDENT if avg_loss is not None : NEW_LINE INDENT package [ ' avg _ loss ' ] = avg_loss NEW_LINE DEDENT if epoch is not None : NEW_LINE INDENT package [ ' epoch ' ] = epoch + 1 NEW_LINE DEDENT if iteration is not None : NEW_LINE INDENT package [ ' iteration ' ] = iteration NEW_LINE DEDENT if loss_results is not None : NEW_LINE INDENT package [ ' loss _ results ' ] = loss_results NEW_LINE package [ ' cer _ results ' ] = cer_results NEW_LINE package [ ' wer _ results ' ] = wer_results NEW_LINE DEDENT if meta is not None : NEW_LINE INDENT package [ ' meta ' ] = meta NEW_LINE DEDENT return package NEW_LINE DEDENT\",\n",
              " 'def get_labels ( model ) : NEW_LINE INDENT return model . _labels NEW_LINE DEDENT',\n",
              " 'def get_param_size ( model ) : NEW_LINE INDENT params = 0 NEW_LINE for p in model . parameters ( ) : NEW_LINE INDENT tmp = 1 NEW_LINE for x in p . size ( ) : NEW_LINE INDENT tmp *= x NEW_LINE DEDENT params += tmp NEW_LINE DEDENT return params NEW_LINE DEDENT',\n",
              " 'def get_audio_conf ( model ) : NEW_LINE INDENT return model . _audio_conf NEW_LINE DEDENT',\n",
              " 'def get_meta ( model ) : NEW_LINE INDENT model_is_cuda = next ( model . parameters ( ) ) . is_cuda NEW_LINE m = model . module if model_is_cuda else model NEW_LINE meta = { \" version \" : m . _version , \" cnn _ map \" : m . cnn_map , \" cnn _ kernel \" : m . cnn_kernel , \" cnn _ stride \" : m . cnn_stride , \" cnn _ layers \" : m . cnn_layers , } NEW_LINE return meta NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , module ) : NEW_LINE INDENT super ( SequenceWise , self ) . __init__ ( ) NEW_LINE self . module = module NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT t , n = x . size ( 0 ) , x . size ( 1 ) NEW_LINE x = x . view ( t * n , - 1 ) NEW_LINE x = self . module ( x ) NEW_LINE x = x . view ( t , n , - 1 ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " \"def __repr__ ( self ) : NEW_LINE INDENT tmpstr = self . __class__ . __name__ + ' ▁ ( \\\\n ' NEW_LINE tmpstr += self . module . __repr__ ( ) NEW_LINE tmpstr += ' ) ' NEW_LINE return tmpstr NEW_LINE DEDENT\",\n",
              " 'def forward ( self , input_ ) : NEW_LINE INDENT if not self . training : NEW_LINE INDENT return F . softmax ( input_ , dim = - 1 ) NEW_LINE DEDENT else : NEW_LINE INDENT return input_ NEW_LINE DEDENT DEDENT',\n",
              " 'def __init__ ( self , input_size , hidden_size , rnn_type = nn . LSTM , bidirectional = False , batch_norm = True ) : NEW_LINE INDENT super ( BatchRNN , self ) . __init__ ( ) NEW_LINE self . input_size = input_size NEW_LINE self . hidden_size = hidden_size NEW_LINE self . bidirectional = bidirectional NEW_LINE self . batch_norm = SequenceWise ( nn . BatchNorm1d ( input_size ) ) if batch_norm else None NEW_LINE self . rnn = rnn_type ( input_size = input_size , hidden_size = hidden_size , bidirectional = bidirectional , bias = False ) NEW_LINE self . num_directions = 2 if bidirectional else 1 NEW_LINE DEDENT',\n",
              " 'def flatten_parameters ( self ) : NEW_LINE INDENT self . rnn . flatten_parameters ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if self . batch_norm is not None : NEW_LINE INDENT x = self . batch_norm ( x ) NEW_LINE DEDENT x , _ = self . rnn ( x ) NEW_LINE if self . bidirectional : NEW_LINE INDENT x = x . view ( x . size ( 0 ) , x . size ( 1 ) , 2 , - 1 ) . sum ( 2 ) . view ( x . size ( 0 ) , x . size ( 1 ) , - 1 ) NEW_LINE DEDENT return x NEW_LINE DEDENT',\n",
              " \"def __init__ ( self , n_features , context ) : NEW_LINE INDENT super ( Lookahead , self ) . __init__ ( ) NEW_LINE self . n_features = n_features NEW_LINE self . weight = Parameter ( torch . Tensor ( n_features , context + 1 ) ) NEW_LINE assert context > 0 NEW_LINE self . context = context NEW_LINE self . register_parameter ( ' bias ' , None ) NEW_LINE self . init_parameters ( ) NEW_LINE DEDENT\",\n",
              " 'def init_parameters ( self ) : NEW_LINE INDENT stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) NEW_LINE self . weight . data . uniform_ ( - stdv , stdv ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , input ) : NEW_LINE INDENT seq_len = input . size ( 0 ) NEW_LINE padding = torch . zeros ( self . context , * ( input . size ( ) [ 1 : ] ) ) . type_as ( input . data ) NEW_LINE x = torch . cat ( ( input , Variable ( padding ) ) , 0 ) NEW_LINE x = [ x [ i : i + self . context + 1 ] for i in range ( seq_len ) ] NEW_LINE x = torch . stack ( x ) NEW_LINE x = x . permute ( 0 , 2 , 3 , 1 ) NEW_LINE x = torch . mul ( x , self . weight ) . sum ( dim = 3 ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " \"def __repr__ ( self ) : NEW_LINE INDENT return self . __class__ . __name__ + ' ( ' + ' n _ features = ' + str ( self . n_features ) + ' , ▁ context = ' + str ( self . context ) + ' ) ' NEW_LINE DEDENT\",\n",
              " 'def __init__ ( self , rnn_type = nn . LSTM , labels = \" abc \" , rnn_hidden_size = 512 , rnn_layers = 2 , bidirectional = True , kernel_sz = 11 , stride = 2 , map = 256 , cnn_layers = 2 , nFreq = 40 , nDownsample = 1 , audio_conf = None ) : NEW_LINE INDENT super ( DeepSpeech_ken , self ) . __init__ ( ) NEW_LINE self . nFreq = nFreq NEW_LINE self . _version = \\'0.0.1\\' NEW_LINE self . _audio_conf = audio_conf NEW_LINE self . rnn_size = rnn_hidden_size NEW_LINE self . rnn_layers = rnn_layers NEW_LINE self . rnn_type = rnn_type NEW_LINE self . bidirectional = bidirectional NEW_LINE self . cnn_stride = stride NEW_LINE self . cnn_map = map NEW_LINE self . cnn_kernel = kernel_sz NEW_LINE self . nDownsample = nDownsample NEW_LINE self . cnn_layers = cnn_layers NEW_LINE self . _labels = labels NEW_LINE num_classes = len ( self . _labels ) NEW_LINE conv_list = [ ] NEW_LINE conv_list . append ( nn . Conv1d ( nFreq , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE if ( self . nDownsample == 1 ) : NEW_LINE INDENT stride = 1 NEW_LINE DEDENT for x in range ( self . cnn_layers - 1 ) : NEW_LINE INDENT conv_list . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE DEDENT self . conv = nn . Sequential ( * conv_list ) NEW_LINE rnn_input_size = map NEW_LINE print ( \\' rnn ▁ input ▁ size ▁ = ▁ \\' + str ( rnn_input_size ) ) NEW_LINE rnns = [ ] NEW_LINE rnn = BatchRNN ( input_size = rnn_input_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional , batch_norm = False ) NEW_LINE rnns . append ( ( \\'0\\' , rnn ) ) NEW_LINE for x in range ( self . rnn_layers - 1 ) : NEW_LINE INDENT rnn = BatchRNN ( input_size = rnn_hidden_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional ) NEW_LINE rnns . append ( ( \\' % d \\' % ( x + 1 ) , rnn ) ) NEW_LINE DEDENT self . rnns = nn . Sequential ( OrderedDict ( rnns ) ) NEW_LINE fully_connected = nn . Sequential ( nn . BatchNorm1d ( rnn_hidden_size ) , nn . Linear ( rnn_hidden_size , num_classes , bias = False ) ) NEW_LINE self . fc = nn . Sequential ( SequenceWise ( fully_connected ) , ) NEW_LINE self . inference_softmax = InferenceBatchSoftmax ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if ( x . dim ( ) == 4 ) : NEW_LINE INDENT x = x . squeeze ( ) NEW_LINE DEDENT x = self . conv ( x ) NEW_LINE x = x . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) . contiguous ( ) NEW_LINE x = self . rnns ( x ) NEW_LINE x = self . fc ( x ) NEW_LINE x = x . transpose ( 0 , 1 ) NEW_LINE x = self . inference_softmax ( x ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , rnn_type = nn . LSTM , labels = \" abc \" , rnn_hidden_size = 512 , rnn_layers = 2 , bidirectional = True , kernel_sz = 11 , stride = 2 , map = 256 , cnn_layers = 2 , nFreq = 40 , nDownsample = 1 , audio_conf = None , include_first_BN = True ) : NEW_LINE INDENT super ( DeepSpeech_ken , self ) . __init__ ( ) NEW_LINE self . nFreq = nFreq NEW_LINE self . _version = \\'0.0.1\\' NEW_LINE self . _audio_conf = audio_conf NEW_LINE self . rnn_size = rnn_hidden_size NEW_LINE self . rnn_layers = rnn_layers NEW_LINE self . rnn_type = rnn_type NEW_LINE self . bidirectional = bidirectional NEW_LINE self . cnn_stride = stride NEW_LINE self . cnn_map = map NEW_LINE self . cnn_kernel = kernel_sz NEW_LINE self . nDownsample = nDownsample NEW_LINE self . cnn_layers = cnn_layers NEW_LINE self . _labels = labels NEW_LINE num_classes = len ( self . _labels ) NEW_LINE conv_list = [ ] NEW_LINE conv_list . append ( nn . Conv1d ( nFreq , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE if ( include_first_BN ) : NEW_LINE INDENT conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE DEDENT conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE if ( self . nDownsample == 1 ) : NEW_LINE INDENT stride = 1 NEW_LINE DEDENT for x in range ( self . cnn_layers - 1 ) : NEW_LINE INDENT conv_list . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv_list . append ( nn . BatchNorm1d ( map ) ) NEW_LINE conv_list . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE DEDENT self . conv = nn . Sequential ( * conv_list ) NEW_LINE rnn_input_size = map NEW_LINE print ( \\' rnn ▁ input ▁ size ▁ = ▁ \\' + str ( rnn_input_size ) ) NEW_LINE rnns = [ ] NEW_LINE rnn = BatchRNN ( input_size = rnn_input_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional , batch_norm = False ) NEW_LINE rnns . append ( ( \\'0\\' , rnn ) ) NEW_LINE for x in range ( self . rnn_layers - 1 ) : NEW_LINE INDENT rnn = BatchRNN ( input_size = rnn_hidden_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional ) NEW_LINE rnns . append ( ( \\' % d \\' % ( x + 1 ) , rnn ) ) NEW_LINE DEDENT self . rnns = nn . Sequential ( OrderedDict ( rnns ) ) NEW_LINE fully_connected = nn . Sequential ( nn . BatchNorm1d ( rnn_hidden_size ) , nn . Linear ( rnn_hidden_size , num_classes , bias = False ) ) NEW_LINE self . fc = nn . Sequential ( SequenceWise ( fully_connected ) , ) NEW_LINE self . inference_softmax = InferenceBatchSoftmax ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if ( x . dim ( ) == 4 ) : NEW_LINE INDENT x = x . squeeze ( ) NEW_LINE DEDENT x = self . conv ( x ) NEW_LINE x = x . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) . contiguous ( ) NEW_LINE x = self . rnns ( x ) NEW_LINE x = self . fc ( x ) NEW_LINE x = x . transpose ( 0 , 1 ) NEW_LINE x = self . inference_softmax ( x ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , rnn_type = nn . GRU , labels = \" abc \" , rnn_hidden_size = 512 , rnn_layers = 2 , bidirectional = True , kernel_sz = 11 , stride = 2 , map = 256 , cnn_residual_blocks = 1 , nFreq = 40 , nDownsample = 1 , audio_conf = None ) : NEW_LINE INDENT super ( ResidualDeepSpeech , self ) . __init__ ( ) NEW_LINE self . nFreq = nFreq NEW_LINE self . _version = \\'0.0.1\\' NEW_LINE self . _audio_conf = audio_conf NEW_LINE self . rnn_size = rnn_hidden_size NEW_LINE self . rnn_layers = rnn_layers NEW_LINE self . rnn_type = rnn_type NEW_LINE self . bidirectional = bidirectional NEW_LINE self . cnn_stride = stride NEW_LINE self . cnn_map = map NEW_LINE self . cnn_kernel = kernel_sz NEW_LINE self . nDownsample = nDownsample NEW_LINE self . cnn_residual_blocks = cnn_residual_blocks NEW_LINE self . _labels = labels NEW_LINE num_classes = len ( self . _labels ) NEW_LINE conv1 = [ ] NEW_LINE conv1 . append ( nn . Conv1d ( nFreq , map , kernel_size = kernel_sz , stride = stride ) ) NEW_LINE conv1 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE self . conv1 = nn . Sequential ( * conv1 ) NEW_LINE if ( self . nDownsample == 1 ) : NEW_LINE INDENT stride = 1 NEW_LINE DEDENT padding = int ( ( kernel_sz - 1 ) / 2 ) NEW_LINE assert ( stride == 1 ) , \\' padding ▁ is ▁ only ▁ valid ▁ when ▁ stride = 1\\' NEW_LINE residual2 = [ ] NEW_LINE residual2 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding ) ) NEW_LINE residual2 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE residual2 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE residual2 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding ) ) NEW_LINE residual2 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE self . residual2 = nn . Sequential ( * residual2 ) NEW_LINE self . dim_match_layer = nn . Conv1d ( map , rnn_hidden_size , kernel_size = 1 , stride = 1 , padding = 0 ) NEW_LINE rnn_input_size = rnn_hidden_size NEW_LINE print ( \\' rnn ▁ input ▁ size ▁ = ▁ \\' + str ( rnn_input_size ) ) NEW_LINE rnns = [ ] NEW_LINE rnn = BatchRNN ( input_size = rnn_input_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional , batch_norm = False ) NEW_LINE rnns . append ( ( \\'0\\' , rnn ) ) NEW_LINE for x in range ( self . rnn_layers - 1 ) : NEW_LINE INDENT rnn = BatchRNN ( input_size = rnn_hidden_size , hidden_size = rnn_hidden_size , rnn_type = rnn_type , bidirectional = bidirectional ) NEW_LINE rnns . append ( ( \\' % d \\' % ( x + 1 ) , rnn ) ) NEW_LINE DEDENT self . rnns = nn . Sequential ( OrderedDict ( rnns ) ) NEW_LINE fully_connected = nn . Sequential ( nn . BatchNorm1d ( rnn_hidden_size ) , nn . Linear ( rnn_hidden_size , num_classes , bias = False ) ) NEW_LINE self . fc = nn . Sequential ( SequenceWise ( fully_connected ) , ) NEW_LINE self . inference_softmax = InferenceBatchSoftmax ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if ( x . dim ( ) == 4 ) : NEW_LINE INDENT x = x . squeeze ( ) NEW_LINE DEDENT x = self . conv1 ( x ) NEW_LINE x = self . residual2 ( x ) + x NEW_LINE x = self . dim_match_layer ( x ) NEW_LINE x = x . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) . contiguous ( ) NEW_LINE for l in range ( self . rnn_layers ) : NEW_LINE INDENT x = self . rnns [ l ] ( x ) + x NEW_LINE DEDENT x = self . fc ( x ) NEW_LINE x = x . transpose ( 0 , 1 ) NEW_LINE x = self . inference_softmax ( x ) NEW_LINE return x NEW_LINE DEDENT',\n",
              " 'def __init__ ( self , labels = \" abc \" , kernel_sz = 11 , stride = 2 , map = 512 , nFreq = 40 , nDownsample = 1 , audio_conf = None ) : NEW_LINE INDENT super ( ResidualCNN4block , self ) . __init__ ( ) NEW_LINE self . nFreq = nFreq NEW_LINE self . _version = \\'0.0.1\\' NEW_LINE self . _audio_conf = audio_conf NEW_LINE self . cnn_stride = stride NEW_LINE self . cnn_map = map NEW_LINE self . cnn_kernel = kernel_sz NEW_LINE self . nDownsample = nDownsample NEW_LINE self . _labels = labels NEW_LINE num_classes = len ( self . _labels ) NEW_LINE conv1 = [ ] NEW_LINE conv1 . append ( nn . Conv1d ( nFreq , map , kernel_size = kernel_sz , stride = stride , bias = False ) ) NEW_LINE conv1 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE self . conv1 = nn . Sequential ( * conv1 ) NEW_LINE if ( self . nDownsample == 1 ) : NEW_LINE INDENT stride = 1 NEW_LINE DEDENT padding = int ( ( kernel_sz - 1 ) / 2 ) NEW_LINE assert ( stride == 1 ) , \\' padding ▁ is ▁ only ▁ valid ▁ when ▁ stride = 1\\' NEW_LINE residual2 = [ ] NEW_LINE residual2 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual2 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE residual2 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE residual2 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual2 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE self . residual2 = nn . Sequential ( * residual2 ) NEW_LINE residual3 = [ ] NEW_LINE residual3 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual3 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE residual3 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE residual3 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual3 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE self . residual3 = nn . Sequential ( * residual3 ) NEW_LINE residual4 = [ ] NEW_LINE residual4 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual4 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE residual4 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE residual4 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual4 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE self . residual4 = nn . Sequential ( * residual4 ) NEW_LINE residual5 = [ ] NEW_LINE residual5 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual5 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE residual5 . append ( nn . LeakyReLU ( map , inplace = True ) ) NEW_LINE residual5 . append ( nn . Conv1d ( map , map , kernel_size = kernel_sz , stride = stride , padding = padding , bias = False ) ) NEW_LINE residual5 . append ( nn . BatchNorm1d ( map ) ) NEW_LINE self . residual5 = nn . Sequential ( * residual5 ) NEW_LINE fully_connected = nn . Sequential ( nn . Linear ( map , num_classes ) ) NEW_LINE self . fc = nn . Sequential ( SequenceWise ( fully_connected ) , ) NEW_LINE self . inference_softmax = InferenceBatchSoftmax ( ) NEW_LINE DEDENT',\n",
              " 'def forward ( self , x ) : NEW_LINE INDENT if ( x . dim ( ) == 4 ) : NEW_LINE INDENT x = x . squeeze ( ) NEW_LINE DEDENT x = self . conv1 ( x ) NEW_LINE x = self . residual2 ( x ) + x NEW_LINE x = self . residual3 ( x ) + x NEW_LINE x = self . residual4 ( x ) + x NEW_LINE x = self . residual5 ( x ) + x NEW_LINE x = x . transpose ( 1 , 2 ) . transpose ( 0 , 1 ) . contiguous ( ) NEW_LINE x = self . fc ( x ) NEW_LINE x = x . transpose ( 0 , 1 ) NEW_LINE x = self . inference_softmax ( x ) NEW_LINE return x NEW_LINE DEDENT']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_df = eqns.merge(collected_func_reps , on='id')"
      ],
      "metadata": {
        "id": "SJkjhyPyY6Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "meJyBubNmEHG",
        "outputId": "afbb9a41-dc9d-4470-a372-6f37ab3b852b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0          id  \\\n",
              "0          17744  1811.02182   \n",
              "1          17745  1811.02182   \n",
              "2          17746  1811.02182   \n",
              "3          17747  1811.02182   \n",
              "4          17748  1811.02182   \n",
              "...          ...         ...   \n",
              "5420       25341  2112.01073   \n",
              "5421       25342  2112.01073   \n",
              "5422       25343  2112.01073   \n",
              "5423       25344  2112.01073   \n",
              "5424       25345  2112.01073   \n",
              "\n",
              "                                                    eqn  \\\n",
              "0     \\n\\begin{split}\\n      \\min_{G}\\max_{D} V_{cGA...   \n",
              "1     \\n\\begin{split}\\n      \\min_{G}\\max_{D}V_{upcG...   \n",
              "2     \\n      L_{CTC}(E) =  -\\E_{(\\textbf{m}, \\textb...   \n",
              "3     \\n\\t\\tp(\\textbf{t}|E(\\textbf{m})) =  \\sum_{\\te...   \n",
              "4     \\n\\begin{split}\\n      \\min_{E}\\max_{D}V_{upcB...   \n",
              "...                                                 ...   \n",
              "5420  \\n\\small\\nP(w_t|w_{\\textless t}, V, S;\\theta) ...   \n",
              "5421  \\n\\small\\n\\setlength{\\abovedisplayskip}{5pt}\\n...   \n",
              "5422  \\n\\small\\n\\setlength{\\abovedisplayskip}{5pt}\\n...   \n",
              "5423  \\n\\small\\n\\setlength{\\abovedisplayskip}{3pt}\\n...   \n",
              "5424  \\n\\small\\n\\setlength{\\abovedisplayskip}{3pt}\\n...   \n",
              "\n",
              "                                              functions  \\\n",
              "0     [def main ( config ) : NEW_LINE INDENT from da...   \n",
              "1     [def main ( config ) : NEW_LINE INDENT from da...   \n",
              "2     [def main ( config ) : NEW_LINE INDENT from da...   \n",
              "3     [def main ( config ) : NEW_LINE INDENT from da...   \n",
              "4     [def main ( config ) : NEW_LINE INDENT from da...   \n",
              "...                                                 ...   \n",
              "5420  [def extract_frames ( video , dst ) : NEW_LINE...   \n",
              "5421  [def extract_frames ( video , dst ) : NEW_LINE...   \n",
              "5422  [def extract_frames ( video , dst ) : NEW_LINE...   \n",
              "5423  [def extract_frames ( video , dst ) : NEW_LINE...   \n",
              "5424  [def extract_frames ( video , dst ) : NEW_LINE...   \n",
              "\n",
              "                                        representations  \n",
              "0     [[[tensor(0.6232), tensor(-0.2805), tensor(0.3...  \n",
              "1     [[[tensor(0.6232), tensor(-0.2805), tensor(0.3...  \n",
              "2     [[[tensor(0.6232), tensor(-0.2805), tensor(0.3...  \n",
              "3     [[[tensor(0.6232), tensor(-0.2805), tensor(0.3...  \n",
              "4     [[[tensor(0.6232), tensor(-0.2805), tensor(0.3...  \n",
              "...                                                 ...  \n",
              "5420  [[[tensor(0.7743), tensor(-0.1384), tensor(0.0...  \n",
              "5421  [[[tensor(0.7743), tensor(-0.1384), tensor(0.0...  \n",
              "5422  [[[tensor(0.7743), tensor(-0.1384), tensor(0.0...  \n",
              "5423  [[[tensor(0.7743), tensor(-0.1384), tensor(0.0...  \n",
              "5424  [[[tensor(0.7743), tensor(-0.1384), tensor(0.0...  \n",
              "\n",
              "[5425 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80f4508d-6581-40c0-a4e6-4b00a145d553\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>eqn</th>\n",
              "      <th>functions</th>\n",
              "      <th>representations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17744</td>\n",
              "      <td>1811.02182</td>\n",
              "      <td>\\n\\begin{split}\\n      \\min_{G}\\max_{D} V_{cGA...</td>\n",
              "      <td>[def main ( config ) : NEW_LINE INDENT from da...</td>\n",
              "      <td>[[[tensor(0.6232), tensor(-0.2805), tensor(0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17745</td>\n",
              "      <td>1811.02182</td>\n",
              "      <td>\\n\\begin{split}\\n      \\min_{G}\\max_{D}V_{upcG...</td>\n",
              "      <td>[def main ( config ) : NEW_LINE INDENT from da...</td>\n",
              "      <td>[[[tensor(0.6232), tensor(-0.2805), tensor(0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17746</td>\n",
              "      <td>1811.02182</td>\n",
              "      <td>\\n      L_{CTC}(E) =  -\\E_{(\\textbf{m}, \\textb...</td>\n",
              "      <td>[def main ( config ) : NEW_LINE INDENT from da...</td>\n",
              "      <td>[[[tensor(0.6232), tensor(-0.2805), tensor(0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17747</td>\n",
              "      <td>1811.02182</td>\n",
              "      <td>\\n\\t\\tp(\\textbf{t}|E(\\textbf{m})) =  \\sum_{\\te...</td>\n",
              "      <td>[def main ( config ) : NEW_LINE INDENT from da...</td>\n",
              "      <td>[[[tensor(0.6232), tensor(-0.2805), tensor(0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17748</td>\n",
              "      <td>1811.02182</td>\n",
              "      <td>\\n\\begin{split}\\n      \\min_{E}\\max_{D}V_{upcB...</td>\n",
              "      <td>[def main ( config ) : NEW_LINE INDENT from da...</td>\n",
              "      <td>[[[tensor(0.6232), tensor(-0.2805), tensor(0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5420</th>\n",
              "      <td>25341</td>\n",
              "      <td>2112.01073</td>\n",
              "      <td>\\n\\small\\nP(w_t|w_{\\textless t}, V, S;\\theta) ...</td>\n",
              "      <td>[def extract_frames ( video , dst ) : NEW_LINE...</td>\n",
              "      <td>[[[tensor(0.7743), tensor(-0.1384), tensor(0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5421</th>\n",
              "      <td>25342</td>\n",
              "      <td>2112.01073</td>\n",
              "      <td>\\n\\small\\n\\setlength{\\abovedisplayskip}{5pt}\\n...</td>\n",
              "      <td>[def extract_frames ( video , dst ) : NEW_LINE...</td>\n",
              "      <td>[[[tensor(0.7743), tensor(-0.1384), tensor(0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5422</th>\n",
              "      <td>25343</td>\n",
              "      <td>2112.01073</td>\n",
              "      <td>\\n\\small\\n\\setlength{\\abovedisplayskip}{5pt}\\n...</td>\n",
              "      <td>[def extract_frames ( video , dst ) : NEW_LINE...</td>\n",
              "      <td>[[[tensor(0.7743), tensor(-0.1384), tensor(0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5423</th>\n",
              "      <td>25344</td>\n",
              "      <td>2112.01073</td>\n",
              "      <td>\\n\\small\\n\\setlength{\\abovedisplayskip}{3pt}\\n...</td>\n",
              "      <td>[def extract_frames ( video , dst ) : NEW_LINE...</td>\n",
              "      <td>[[[tensor(0.7743), tensor(-0.1384), tensor(0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5424</th>\n",
              "      <td>25345</td>\n",
              "      <td>2112.01073</td>\n",
              "      <td>\\n\\small\\n\\setlength{\\abovedisplayskip}{3pt}\\n...</td>\n",
              "      <td>[def extract_frames ( video , dst ) : NEW_LINE...</td>\n",
              "      <td>[[[tensor(0.7743), tensor(-0.1384), tensor(0.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5425 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80f4508d-6581-40c0-a4e6-4b00a145d553')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-80f4508d-6581-40c0-a4e6-4b00a145d553 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-80f4508d-6581-40c0-a4e6-4b00a145d553');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for row in full_df.iterrows():\n",
        "  print(tokenizer(row[1].eqn,return_tensors='pt',truncation=True))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rghVH0GYooer",
        "outputId": "0a25b614-90cb-488c-9b07-3a951ec2819e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,   165,  3295,   196,  3325,   198,   165, 11241,   168,   196,\n",
            "           144,   198,   165, 12477,  1775,   168,   196,   141,   198,   159,\n",
            "           168,   196,   172, 10583,  2249,   198,   113,   144,   117,   141,\n",
            "           114,   134,   165,   142,   168,   196,   113,   165,  3087,  1830,\n",
            "          2087,   196,   193,   198,   168,   196,   188,   198,   117,   165,\n",
            "          3087,  1830,  2087,   196,   193,   198,   168,   196,   189,   198,\n",
            "           114,   165, 27466,  1306,   185,   113,   165,  3087,  1830,  2087,\n",
            "           196,   193,   198,   168,   196,   188,   198,   117,   165,  3087,\n",
            "          1830,  2087,   196,   193,   198,   168,   196,   189,   198,   114,\n",
            "           198,   164,   165,  9366,   141,   113,   165,  3087,  1830,  2087,\n",
            "           196,   193,   198,   168,   188,   117,   165,  3087,  1830,  2087,\n",
            "           196,   193,   198,   168,   196,   189,   198,   114,   166,   165,\n",
            "           165,   116,   165,   142,   168,   196,   165,  3087,  1830,  2087,\n",
            "           196,   193,   198,   168,   196,   188,   198,   165, 27466,  1306,\n",
            "           185,   113,   165,  3087,  1830,  2087,   196,   193,   198,   168,\n",
            "           196,   188,   198,   114,   117,   165,  3087,  1830,  2087,   196,\n",
            "           195,   198,   165, 27466,  1306,   151,   113,   121,   117,   146,\n",
            "           114,   198,   164,   165,  9366,   113,   122,   118,   141,   113,\n",
            "           165,  3087,  1830,  2087,   196,   193,   198,   168,   196,   188,\n",
            "           198,   117,   144,   113,   165,  3087,  1830,  2087,   196,   193,\n",
            "           198,   168,   196,   188,   198,   117,   165,  3087,  1830,  2087,\n",
            "           196,   195,   198,   114,   114,   114,   166,   119,   165,  1322,\n",
            "           196,  3325,   198,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveDataset(Dataset):\n",
        "\n",
        "    def __init__(self, combined_df):\n",
        "        \"\"\"Simple init function\"\"\"\n",
        "        self.df = combined_df\n",
        "        tok_eqns = []\n",
        "        for row in self.df.iterrows():\n",
        "          tok_eqns.append(tokenizer(row[1].eqn,return_tensors='pt',truncation=True))\n",
        "        self.tok_eqns = tok_eqns\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Get length of dataset, doubled from our data as we will do 50/50 positive and negative examples\"\"\"\n",
        "        return len(self.df) * 2\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"Contrastive learning and makes half negative samples half positive\"\n",
        "\n",
        "        if idx < len(self.df):\n",
        "            label = 1\n",
        "            entry = self.df.iloc[idx]\n",
        "            eqn_paper_id = entry.id\n",
        "            code_id = eqn_paper_id\n",
        "            code_rep = entry.representations\n",
        "            code_funcs = entry.functions\n",
        "            eqn = entry.eqn\n",
        "            tok_eqn = self.tok_eqns[idx]\n",
        "\n",
        "        else:\n",
        "            eqn_idx = idx - len(self.df)\n",
        "            tok_eqn = self.tok_eqns[eqn_idx]\n",
        "            label = 0\n",
        "\n",
        "            curr_code_idx = random.randint(0,len(self.df)-1)\n",
        "            curr_paper = self.df.iloc[curr_code_idx].id \n",
        "            while (curr_paper == self.df.iloc[eqn_idx].id):\n",
        "                curr_code_idx = random.randint(0,len(self.df)-1)\n",
        "                curr_paper = self.df.iloc[curr_code_idx].id\n",
        "            \n",
        "            code_id = curr_paper\n",
        "            eqn_entry = self.df.iloc[eqn_idx]\n",
        "            code_entry = self.df.iloc[curr_code_idx]\n",
        "\n",
        "            eqn_paper_id = eqn_entry.id \n",
        "            code_rep = code_entry.representations \n",
        "            code_funcs = code_entry.functions \n",
        "            eqn = eqn_entry.eqn\n",
        "\n",
        "        return code_rep,tok_eqn, label, eqn_paper_id,code_id,eqn, code_funcs\n"
      ],
      "metadata": {
        "id": "1jXByPfkDDQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ContrastiveDataset(full_df)"
      ],
      "metadata": {
        "id": "8eGPuAX_eZHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,batch_size=1,shuffle=True)"
      ],
      "metadata": {
        "id": "SNFDNx3menro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DistilBertModel.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "combined_model = ContrastiveModel(model)\n",
        "combined_model.train()\n",
        "combined_model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(combined_model.parameters(), lr=.01)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "epochs = 5\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMBv8zzX05u3",
        "outputId": "1f31aa5d-3a5f-4d5c-f141-936f2e849c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKy8VDLWtVJ2",
        "outputId": "170f8e02-77c3-482f-c0d7-7ab65485b949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 26 15:41:13 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0    42W / 250W |   4649MiB / 16280MiB |      9%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20"
      ],
      "metadata": {
        "id": "5mSV47RXdMhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_avg = 0\n",
        "    for index,input in enumerate(train_loader):\n",
        "\n",
        "      if index % 100 == 0:\n",
        "        print(loss_avg/100)\n",
        "        loss_avg = 0\n",
        "        print(f\"On batch {index}\")\n",
        "        print(correct/(total+1))\n",
        "\n",
        "      code_rep, tok_eqn, label, eqn_paper_id,code_id,eqn, code_funcs = input\n",
        "      orig_label = label\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      curr_rep = torch.cat(code_rep).cuda()\n",
        "      input_ids,a_mask = tok_eqn['input_ids'].flatten(1).cuda(),tok_eqn['attention_mask'].flatten(1).cuda()\n",
        "      label = torch.tensor(label,dtype=float,device='cuda')\n",
        "\n",
        "      logits = combined_model(curr_rep,input_ids,a_mask)\n",
        "      curr_logit = torch.mean(logits)\n",
        "\n",
        "      loss = criterion(curr_logit.reshape((1)),label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_avg += loss.item()\n",
        "      pred = torch.sigmoid(curr_logit).item()\n",
        "      if round(pred) == orig_label:\n",
        "          correct += 1\n",
        "      total += 1\n",
        "      print(pred)\n",
        "    print(\"----\")\n",
        "\n",
        "    print(f\"Epoch {epoch} correct {correct}, total {total}, {loss_avg/50}\")\n",
        "    print(\"----\")\n",
        "        # outputs = net(inputs)\n",
        "        # loss = criterion(outputs, labels)\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        # # print statistics\n",
        "        # running_loss += loss.item()\n",
        "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "        #     print(f'[{epoch + 1}, {i + "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRHGr83gjXt5",
        "outputId": "f18e1159-bbe5-4d27-e6d3-92a1fb78117e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.4907546043395996\n",
            "0.4938477873802185\n",
            "0.4946592152118683\n",
            "0.49264445900917053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.49273890256881714\n",
            "0.48203763365745544\n",
            "0.48144516348838806\n",
            "0.5051873922348022\n",
            "0.49197623133659363\n",
            "0.49131083488464355\n",
            "0.5029921531677246\n",
            "0.4919959604740143\n",
            "0.4892311096191406\n",
            "0.49202948808670044\n",
            "0.4918978810310364\n",
            "0.4891364574432373\n",
            "0.4892519414424896\n",
            "0.48951083421707153\n",
            "0.47943076491355896\n",
            "0.49294668436050415\n",
            "0.4919142723083496\n",
            "0.5051082968711853\n",
            "0.4925523102283478\n",
            "0.492265522480011\n",
            "0.4949178397655487\n",
            "0.4837900698184967\n",
            "0.48476576805114746\n",
            "0.49491164088249207\n",
            "0.5077707171440125\n",
            "0.5085858106613159\n",
            "0.4972054660320282\n",
            "0.5103797912597656\n",
            "0.49911677837371826\n",
            "0.5119848847389221\n",
            "0.501926064491272\n",
            "0.5134442448616028\n",
            "0.5006372928619385\n",
            "0.502032995223999\n",
            "0.5003935098648071\n",
            "0.5007104873657227\n",
            "0.5029435753822327\n",
            "0.5147665739059448\n",
            "0.5011324882507324\n",
            "0.5019500851631165\n",
            "0.5167108178138733\n",
            "0.5175158977508545\n",
            "0.5178390145301819\n",
            "0.5035931468009949\n",
            "0.5062186121940613\n",
            "0.5051178932189941\n",
            "0.5041821599006653\n",
            "0.5045703053474426\n",
            "0.5052367448806763\n",
            "0.5061519145965576\n",
            "0.5092331171035767\n",
            "0.5233333706855774\n",
            "0.5087035894393921\n",
            "0.5235687494277954\n",
            "0.5099558234214783\n",
            "0.5093368887901306\n",
            "0.5068170428276062\n",
            "0.4993889033794403\n",
            "0.5073049664497375\n",
            "0.5039696097373962\n",
            "0.5034021139144897\n",
            "0.5040262937545776\n",
            "0.49631911516189575\n",
            "0.49548104405403137\n",
            "0.5019636154174805\n",
            "0.515272855758667\n",
            "0.49319639801979065\n",
            "0.5147760510444641\n",
            "0.5000710487365723\n",
            "0.5010791420936584\n",
            "0.49779191613197327\n",
            "0.4967293441295624\n",
            "0.5097684860229492\n",
            "0.5085849761962891\n",
            "0.4939778745174408\n",
            "0.5069940090179443\n",
            "0.49255409836769104\n",
            "0.4825226962566376\n",
            "0.482697069644928\n",
            "0.5070651769638062\n",
            "0.49527493119239807\n",
            "0.4927915334701538\n",
            "0.506848156452179\n",
            "0.49329280853271484\n",
            "0.49334385991096497\n",
            "0.49335628747940063\n",
            "0.5067024230957031\n",
            "0.5066673755645752\n",
            "0.49253085255622864\n",
            "0.5070450305938721\n",
            "0.49485933780670166\n",
            "0.5081149339675903\n",
            "0.49535995721817017\n",
            "0.5082139372825623\n",
            "0.4926173686981201\n",
            "0.4919459819793701\n",
            "----\n",
            "Epoch 0 correct 53, total 100, 1.389614211000735\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.5077294707298279\n",
            "0.49311092495918274\n",
            "0.5084355473518372\n",
            "0.48501792550086975\n",
            "0.49623438715934753\n",
            "0.4945254921913147\n",
            "0.48624706268310547\n",
            "0.5100905299186707\n",
            "0.4936094880104065\n",
            "0.4965206980705261\n",
            "0.49502846598625183\n",
            "0.49472013115882874\n",
            "0.48668432235717773\n",
            "0.5123252868652344\n",
            "0.49732521176338196\n",
            "0.4948769211769104\n",
            "0.5128400921821594\n",
            "0.4960545599460602\n",
            "0.49638378620147705\n",
            "0.5147837400436401\n",
            "0.49723905324935913\n",
            "0.5148646235466003\n",
            "0.4967799186706543\n",
            "0.4961601197719574\n",
            "0.4958024024963379\n",
            "0.4953462779521942\n",
            "0.4952419400215149\n",
            "0.49480587244033813\n",
            "0.49593743681907654\n",
            "0.49596235156059265\n",
            "0.5131895542144775\n",
            "0.4968762695789337\n",
            "0.5147017240524292\n",
            "0.49868476390838623\n",
            "0.5020245909690857\n",
            "0.5001678466796875\n",
            "0.50334233045578\n",
            "0.5006716847419739\n",
            "0.5039025545120239\n",
            "0.49470916390419006\n",
            "0.501517117023468\n",
            "0.5021399855613708\n",
            "0.5052016973495483\n",
            "0.5029001832008362\n",
            "0.5036600232124329\n",
            "0.5046916007995605\n",
            "0.5085160732269287\n",
            "0.5255457162857056\n",
            "0.5263218283653259\n",
            "0.5085617303848267\n",
            "0.5264971852302551\n",
            "0.5110230445861816\n",
            "0.50274258852005\n",
            "0.5090230107307434\n",
            "0.5090575814247131\n",
            "0.5253044366836548\n",
            "0.5099531412124634\n",
            "0.5249773859977722\n",
            "0.5097678303718567\n",
            "0.5248870253562927\n",
            "0.5078134536743164\n",
            "0.50771164894104\n",
            "0.5234858989715576\n",
            "0.5016542673110962\n",
            "0.5215158462524414\n",
            "0.5076106786727905\n",
            "0.5201160311698914\n",
            "0.5197621583938599\n",
            "0.5031158924102783\n",
            "0.5028535723686218\n",
            "0.5033105611801147\n",
            "0.49701011180877686\n",
            "0.4964217245578766\n",
            "0.5008141994476318\n",
            "0.5009124279022217\n",
            "0.49930906295776367\n",
            "0.4928649067878723\n",
            "0.5138044357299805\n",
            "0.5130162239074707\n",
            "0.511911928653717\n",
            "0.49707135558128357\n",
            "0.5110270380973816\n",
            "0.498680979013443\n",
            "0.49561047554016113\n",
            "0.4951308071613312\n",
            "0.5099121928215027\n",
            "0.49482372403144836\n",
            "0.5077685713768005\n",
            "0.506340742111206\n",
            "0.4927147626876831\n",
            "0.4917190372943878\n",
            "0.4824107587337494\n",
            "0.4865455627441406\n",
            "0.4893684983253479\n",
            "0.4998377561569214\n",
            "0.4861823618412018\n",
            "0.48739710450172424\n",
            "0.4978107511997223\n",
            "0.4978879392147064\n",
            "0.4983111619949341\n",
            "----\n",
            "Epoch 1 correct 48, total 100, 1.3921954212081618\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.4852845370769501\n",
            "0.48556631803512573\n",
            "0.488768070936203\n",
            "0.49934759736061096\n",
            "0.4988234341144562\n",
            "0.48452362418174744\n",
            "0.4803750514984131\n",
            "0.4796546995639801\n",
            "0.4847698211669922\n",
            "0.4975571036338806\n",
            "0.4860951900482178\n",
            "0.4845854938030243\n",
            "0.4975718855857849\n",
            "0.48281124234199524\n",
            "0.4825165569782257\n",
            "0.4796682894229889\n",
            "0.4829370677471161\n",
            "0.48523932695388794\n",
            "0.49761223793029785\n",
            "0.48617660999298096\n",
            "0.48831334710121155\n",
            "0.48748579621315\n",
            "0.4857008755207062\n",
            "0.5010771155357361\n",
            "0.4897482097148895\n",
            "0.4854021966457367\n",
            "0.490481436252594\n",
            "0.49167490005493164\n",
            "0.48872849345207214\n",
            "0.48943862318992615\n",
            "0.48959892988204956\n",
            "0.4936326742172241\n",
            "0.5041505694389343\n",
            "0.49436548352241516\n",
            "0.49416935443878174\n",
            "0.49216708540916443\n",
            "0.4955291152000427\n",
            "0.4921268820762634\n",
            "0.49146807193756104\n",
            "0.4913236200809479\n",
            "0.5048326849937439\n",
            "0.5048491954803467\n",
            "0.49385544657707214\n",
            "0.4904332756996155\n",
            "0.5041818618774414\n",
            "0.48892730474472046\n",
            "0.4885961711406708\n",
            "0.5041992664337158\n",
            "0.49356168508529663\n",
            "0.48978227376937866\n",
            "0.5051872134208679\n",
            "0.5059067606925964\n",
            "0.49514394998550415\n",
            "0.5081113576889038\n",
            "0.4934544861316681\n",
            "0.4948073625564575\n",
            "0.5128511786460876\n",
            "0.5001369714736938\n",
            "0.5147584080696106\n",
            "0.5016583204269409\n",
            "0.49866995215415955\n",
            "0.5178924798965454\n",
            "0.501226007938385\n",
            "0.5182915329933167\n",
            "0.5034444332122803\n",
            "0.5009780526161194\n",
            "0.5175544023513794\n",
            "0.500143826007843\n",
            "0.5165209770202637\n",
            "0.4995141625404358\n",
            "0.515464723110199\n",
            "0.5147587656974792\n",
            "0.501335084438324\n",
            "0.49861690402030945\n",
            "0.4979930818080902\n",
            "0.500054121017456\n",
            "0.5010866522789001\n",
            "0.49538570642471313\n",
            "0.4976983368396759\n",
            "0.500643253326416\n",
            "0.49691587686538696\n",
            "0.49998998641967773\n",
            "0.49920958280563354\n",
            "0.5111417174339294\n",
            "0.4991868734359741\n",
            "0.49679994583129883\n",
            "0.5113329887390137\n",
            "0.5004827380180359\n",
            "0.5011190176010132\n",
            "0.5134954452514648\n",
            "0.4974454641342163\n",
            "0.5146441459655762\n",
            "0.5004715323448181\n",
            "0.5148186087608337\n",
            "0.5151188373565674\n",
            "0.503753125667572\n",
            "0.5025174617767334\n",
            "0.514011800289154\n",
            "0.5021296739578247\n",
            "0.5119642019271851\n",
            "----\n",
            "Epoch 2 correct 47, total 100, 1.3908049068844412\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.4975097179412842\n",
            "0.5109992623329163\n",
            "0.4960876405239105\n",
            "0.49706557393074036\n",
            "0.49528276920318604\n",
            "0.4989030659198761\n",
            "0.49694350361824036\n",
            "0.49582481384277344\n",
            "0.5065669417381287\n",
            "0.49108731746673584\n",
            "0.4920724928379059\n",
            "0.48951926827430725\n",
            "0.49185577034950256\n",
            "0.48821985721588135\n",
            "0.5018659234046936\n",
            "0.486455500125885\n",
            "0.48690715432167053\n",
            "0.4887802004814148\n",
            "0.4911160469055176\n",
            "0.4924774169921875\n",
            "0.5030037760734558\n",
            "0.5024844408035278\n",
            "0.48867690563201904\n",
            "0.5025429129600525\n",
            "0.4887932538986206\n",
            "0.48841342329978943\n",
            "0.4859832227230072\n",
            "0.5014145374298096\n",
            "0.4880114197731018\n",
            "0.4898645579814911\n",
            "0.4889145493507385\n",
            "0.4867541491985321\n",
            "0.4921639561653137\n",
            "0.4946243166923523\n",
            "0.5059508085250854\n",
            "0.4900749921798706\n",
            "0.5063156485557556\n",
            "0.4950364828109741\n",
            "0.4909694492816925\n",
            "0.4965038299560547\n",
            "0.49864694476127625\n",
            "0.5098379850387573\n",
            "0.5100566744804382\n",
            "0.49853071570396423\n",
            "0.4970376491546631\n",
            "0.4982720911502838\n",
            "0.49951887130737305\n",
            "0.5099698901176453\n",
            "0.5100052952766418\n",
            "0.5096797943115234\n",
            "0.49776265025138855\n",
            "0.4938422441482544\n",
            "0.4983121156692505\n",
            "0.49785536527633667\n",
            "0.4923352003097534\n",
            "0.49733293056488037\n",
            "0.5049661993980408\n",
            "0.4964399039745331\n",
            "0.5040611028671265\n",
            "0.5032099485397339\n",
            "0.49477940797805786\n",
            "0.48872143030166626\n",
            "0.4939655661582947\n",
            "0.492902547121048\n",
            "0.5028645396232605\n",
            "0.5033410787582397\n",
            "0.48963847756385803\n",
            "0.5032639503479004\n",
            "0.48947975039482117\n",
            "0.49443361163139343\n",
            "0.48866358399391174\n",
            "0.4944183826446533\n",
            "0.4922713041305542\n",
            "0.4866692125797272\n",
            "0.4915727376937866\n",
            "0.48592010140419006\n",
            "0.48584794998168945\n",
            "0.49136996269226074\n",
            "0.500072717666626\n",
            "0.4916825592517853\n",
            "0.4917222261428833\n",
            "0.49149230122566223\n",
            "0.48974552750587463\n",
            "0.498750239610672\n",
            "0.48781630396842957\n",
            "0.4894014894962311\n",
            "0.4978629946708679\n",
            "0.48866504430770874\n",
            "0.485734760761261\n",
            "0.48759886622428894\n",
            "0.49575892090797424\n",
            "0.48675844073295593\n",
            "0.49514442682266235\n",
            "0.48384296894073486\n",
            "0.4847477376461029\n",
            "0.4843987226486206\n",
            "0.4844878017902374\n",
            "0.4856327176094055\n",
            "0.4971458315849304\n",
            "0.49810829758644104\n",
            "----\n",
            "Epoch 3 correct 49, total 100, 1.389363592134323\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.4881499409675598\n",
            "0.48240455985069275\n",
            "0.4896867275238037\n",
            "0.5023715496063232\n",
            "0.4933529198169708\n",
            "0.49202701449394226\n",
            "0.49521878361701965\n",
            "0.4946155846118927\n",
            "0.494082510471344\n",
            "0.48939767479896545\n",
            "0.4895773231983185\n",
            "0.5083937644958496\n",
            "0.49594804644584656\n",
            "0.4977216124534607\n",
            "0.4978955388069153\n",
            "0.5103678107261658\n",
            "0.5104990601539612\n",
            "0.4987988770008087\n",
            "0.4981236755847931\n",
            "0.5126420259475708\n",
            "0.5012460947036743\n",
            "0.5024366974830627\n",
            "0.5032318830490112\n",
            "0.502265989780426\n",
            "0.5034453272819519\n",
            "0.5195543766021729\n",
            "0.5059055089950562\n",
            "0.5057719349861145\n",
            "0.5085300207138062\n",
            "0.49975550174713135\n",
            "0.507862389087677\n",
            "0.5239191651344299\n",
            "0.5101406574249268\n",
            "0.5104274153709412\n",
            "0.525415301322937\n",
            "0.5087316632270813\n",
            "0.5243712663650513\n",
            "0.5083133578300476\n",
            "0.5057461261749268\n",
            "0.5063539743423462\n",
            "0.5200825929641724\n",
            "0.5028714537620544\n",
            "0.5025330781936646\n",
            "0.500859260559082\n",
            "0.49945777654647827\n",
            "0.4974006414413452\n",
            "0.4967093765735626\n",
            "0.4867478311061859\n",
            "0.49508002400398254\n",
            "0.5105275511741638\n",
            "0.49266934394836426\n",
            "0.4938628375530243\n",
            "0.4911454916000366\n",
            "0.48319950699806213\n",
            "0.4838801324367523\n",
            "0.4839375913143158\n",
            "0.48345252871513367\n",
            "0.4911801218986511\n",
            "0.49091118574142456\n",
            "0.4890820384025574\n",
            "0.5068825483322144\n",
            "0.5055591464042664\n",
            "0.4888455867767334\n",
            "0.5046684741973877\n",
            "0.4853886067867279\n",
            "0.48528414964675903\n",
            "0.5032160878181458\n",
            "0.4843847453594208\n",
            "0.48245471715927124\n",
            "0.4817890226840973\n",
            "0.49981123208999634\n",
            "0.4813787341117859\n",
            "0.4733636677265167\n",
            "0.48579010367393494\n",
            "0.4740169644355774\n",
            "0.48668816685676575\n",
            "0.501749575138092\n",
            "0.47328588366508484\n",
            "0.5027012228965759\n",
            "0.485808402299881\n",
            "0.5043529868125916\n",
            "0.5053433775901794\n",
            "0.4761006534099579\n",
            "0.48956453800201416\n",
            "0.48746049404144287\n",
            "0.4881913661956787\n",
            "0.4891778826713562\n",
            "0.4903886616230011\n",
            "0.4914429187774658\n",
            "0.5124232769012451\n",
            "0.49764278531074524\n",
            "0.4845060110092163\n",
            "0.4984127879142761\n",
            "0.48723331093788147\n",
            "0.488724946975708\n",
            "0.501127302646637\n",
            "0.5037359595298767\n",
            "0.50191730260849\n",
            "0.5209183692932129\n",
            "0.48984193801879883\n",
            "----\n",
            "Epoch 4 correct 48, total 100, 1.3868350304942578\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.5012633800506592\n",
            "0.5206221342086792\n",
            "0.48836952447891235\n",
            "0.5000292062759399\n",
            "0.486537903547287\n",
            "0.5191830396652222\n",
            "0.4997348487377167\n",
            "0.49663233757019043\n",
            "0.4978351593017578\n",
            "0.5165407657623291\n",
            "0.5160537958145142\n",
            "0.4926937222480774\n",
            "0.49235254526138306\n",
            "0.4908188283443451\n",
            "0.5143170356750488\n",
            "0.4721003770828247\n",
            "0.5121654272079468\n",
            "0.4863469898700714\n",
            "0.5093978047370911\n",
            "0.48315608501434326\n",
            "0.4857982099056244\n",
            "0.48325276374816895\n",
            "0.48357030749320984\n",
            "0.4828483760356903\n",
            "0.48319104313850403\n",
            "0.5096077919006348\n",
            "0.4872138202190399\n",
            "0.4847414493560791\n",
            "0.48495498299598694\n",
            "0.48776546120643616\n",
            "0.4847099184989929\n",
            "0.48462048172950745\n",
            "0.4693911671638489\n",
            "0.48506245017051697\n",
            "0.5107208490371704\n",
            "0.5119372606277466\n",
            "0.5125210285186768\n",
            "0.5125397443771362\n",
            "0.4749479293823242\n",
            "0.49174031615257263\n",
            "0.5129916071891785\n",
            "0.49177077412605286\n",
            "0.48958441615104675\n",
            "0.5147849321365356\n",
            "0.49077603220939636\n",
            "0.4940451383590698\n",
            "0.4927201569080353\n",
            "0.49544528126716614\n",
            "0.4939667880535126\n",
            "0.49658647179603577\n",
            "0.4948159456253052\n",
            "0.4957527220249176\n",
            "0.49692270159721375\n",
            "0.5220413208007812\n",
            "0.5227236151695251\n",
            "0.49963775277137756\n",
            "0.48565781116485596\n",
            "0.522695779800415\n",
            "0.4865400493144989\n",
            "0.5004848837852478\n",
            "0.5024230480194092\n",
            "0.521190345287323\n",
            "0.499526709318161\n",
            "0.4982106685638428\n",
            "0.49753817915916443\n",
            "0.4976431429386139\n",
            "0.4978836178779602\n",
            "0.5177918076515198\n",
            "0.49920162558555603\n",
            "0.5017026662826538\n",
            "0.49962595105171204\n",
            "0.49997901916503906\n",
            "0.48997870087623596\n",
            "0.49099957942962646\n",
            "0.5206009149551392\n",
            "0.521382749080658\n",
            "0.5034324526786804\n",
            "0.5045754909515381\n",
            "0.50529545545578\n",
            "0.5251672863960266\n",
            "0.5082383751869202\n",
            "0.5270994305610657\n",
            "0.510080099105835\n",
            "0.5086782574653625\n",
            "0.5299931168556213\n",
            "0.5092911124229431\n",
            "0.49802929162979126\n",
            "0.49728941917419434\n",
            "0.5078480839729309\n",
            "0.5091878771781921\n",
            "0.5084277987480164\n",
            "0.49540209770202637\n",
            "0.5054572820663452\n",
            "0.5264098644256592\n",
            "0.5053125023841858\n",
            "0.5049153566360474\n",
            "0.49363312125205994\n",
            "0.5047377347946167\n",
            "0.526153028011322\n",
            "0.5262321829795837\n",
            "----\n",
            "Epoch 5 correct 44, total 100, 1.3903425242782397\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.5035765767097473\n",
            "0.4904730021953583\n",
            "0.5024677515029907\n",
            "0.5241790413856506\n",
            "0.5231401920318604\n",
            "0.5003389716148376\n",
            "0.5225222706794739\n",
            "0.500011682510376\n",
            "0.5023238062858582\n",
            "0.4997788965702057\n",
            "0.521926760673523\n",
            "0.500885546207428\n",
            "0.4987257421016693\n",
            "0.49893760681152344\n",
            "0.5011849403381348\n",
            "0.5008678436279297\n",
            "0.49811309576034546\n",
            "0.4976678192615509\n",
            "0.4974100589752197\n",
            "0.48460468649864197\n",
            "0.4959033727645874\n",
            "0.49775493144989014\n",
            "0.5191881060600281\n",
            "0.4936944842338562\n",
            "0.49557769298553467\n",
            "0.4952019453048706\n",
            "0.49454250931739807\n",
            "0.4811861217021942\n",
            "0.48057904839515686\n",
            "0.49023574590682983\n",
            "0.48013412952423096\n",
            "0.4901428818702698\n",
            "0.5157979726791382\n",
            "0.49210599064826965\n",
            "0.49249181151390076\n",
            "0.47920575737953186\n",
            "0.49222368001937866\n",
            "0.4879240095615387\n",
            "0.48919200897216797\n",
            "0.4883570671081543\n",
            "0.517382025718689\n",
            "0.49229803681373596\n",
            "0.4888806939125061\n",
            "0.4922218322753906\n",
            "0.48919886350631714\n",
            "0.4787784814834595\n",
            "0.4914531111717224\n",
            "0.5203824639320374\n",
            "0.5219147801399231\n",
            "0.49577510356903076\n",
            "0.4851621091365814\n",
            "0.5002874732017517\n",
            "0.5272653698921204\n",
            "0.49900728464126587\n",
            "0.5023989677429199\n",
            "0.50334233045578\n",
            "0.5013221502304077\n",
            "0.5022666454315186\n",
            "0.5014107823371887\n",
            "0.501616358757019\n",
            "0.502109944820404\n",
            "0.5315829515457153\n",
            "0.5043699741363525\n",
            "0.5055112838745117\n",
            "0.505370557308197\n",
            "0.5057256817817688\n",
            "0.5071689486503601\n",
            "0.5373350381851196\n",
            "0.5080271363258362\n",
            "0.5081007480621338\n",
            "0.5378966331481934\n",
            "0.5386543869972229\n",
            "0.5387455224990845\n",
            "0.5393317341804504\n",
            "0.5108872652053833\n",
            "0.5112347602844238\n",
            "0.510496973991394\n",
            "0.5410439968109131\n",
            "0.5128408074378967\n",
            "0.5110182166099548\n",
            "0.5131775736808777\n",
            "0.5423067808151245\n",
            "0.5427562594413757\n",
            "0.5103663206100464\n",
            "0.49559035897254944\n",
            "0.541832685470581\n",
            "0.509533703327179\n",
            "0.49556398391723633\n",
            "0.5419057011604309\n",
            "0.5083264112472534\n",
            "0.5068889260292053\n",
            "0.5056369304656982\n",
            "0.5073388814926147\n",
            "0.5365129709243774\n",
            "0.5031455755233765\n",
            "0.4884875416755676\n",
            "0.4872312545776367\n",
            "0.5008841753005981\n",
            "0.5006046295166016\n",
            "0.49960535764694214\n",
            "----\n",
            "Epoch 6 correct 48, total 100, 1.3833530351193621\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.48553547263145447\n",
            "0.4999505281448364\n",
            "0.5026402473449707\n",
            "0.5343150496482849\n",
            "0.48820626735687256\n",
            "0.5363644361495972\n",
            "0.5055107474327087\n",
            "0.5037720799446106\n",
            "0.5077049732208252\n",
            "0.4919975697994232\n",
            "0.5056339502334595\n",
            "0.5095601677894592\n",
            "0.492706835269928\n",
            "0.5104156136512756\n",
            "0.5455215573310852\n",
            "0.5085199475288391\n",
            "0.5476062297821045\n",
            "0.5096027851104736\n",
            "0.5095943808555603\n",
            "0.5104160308837891\n",
            "0.511248767375946\n",
            "0.5113460421562195\n",
            "0.5493412613868713\n",
            "0.5497215986251831\n",
            "0.511598527431488\n",
            "0.5114448666572571\n",
            "0.5531835556030273\n",
            "0.55501389503479\n",
            "0.514906108379364\n",
            "0.5172809958457947\n",
            "0.5150417685508728\n",
            "0.5559963583946228\n",
            "0.5562991499900818\n",
            "0.5557578802108765\n",
            "0.5129153728485107\n",
            "0.5527561902999878\n",
            "0.5123313665390015\n",
            "0.5075740814208984\n",
            "0.5090104937553406\n",
            "0.5059583783149719\n",
            "0.5047827363014221\n",
            "0.5039344429969788\n",
            "0.5419406294822693\n",
            "0.505113422870636\n",
            "0.5074943900108337\n",
            "0.5078982710838318\n",
            "0.5430306196212769\n",
            "0.5073291063308716\n",
            "0.540958821773529\n",
            "0.48795759677886963\n",
            "0.500929057598114\n",
            "0.5354762077331543\n",
            "0.49876660108566284\n",
            "0.5005415081977844\n",
            "0.5312429070472717\n",
            "0.49764177203178406\n",
            "0.49834752082824707\n",
            "0.4843546450138092\n",
            "0.5001455545425415\n",
            "0.5024998188018799\n",
            "0.502148449420929\n",
            "0.49969011545181274\n",
            "0.5018572211265564\n",
            "0.5019126534461975\n",
            "0.4836096465587616\n",
            "0.5005752444267273\n",
            "0.5335376858711243\n",
            "0.4854796528816223\n",
            "0.5353997349739075\n",
            "0.5036123991012573\n",
            "0.5037921667098999\n",
            "0.5383973717689514\n",
            "0.5044429302215576\n",
            "0.5039742588996887\n",
            "0.48894667625427246\n",
            "0.5041205286979675\n",
            "0.5040993690490723\n",
            "0.5371906757354736\n",
            "0.5372940301895142\n",
            "0.5037520527839661\n",
            "0.5043096542358398\n",
            "0.5069534778594971\n",
            "0.5068114995956421\n",
            "0.5040773749351501\n",
            "0.5392341017723083\n",
            "0.5039881467819214\n",
            "0.5070378184318542\n",
            "0.5412673354148865\n",
            "0.5422499775886536\n",
            "0.5055122375488281\n",
            "0.5049107074737549\n",
            "0.5052728056907654\n",
            "0.5423983335494995\n",
            "0.5041468143463135\n",
            "0.5057203769683838\n",
            "0.50234055519104\n",
            "0.48813411593437195\n",
            "0.4993554353713989\n",
            "0.4871900677680969\n",
            "0.5013152360916138\n",
            "----\n",
            "Epoch 7 correct 45, total 100, 1.388958969844971\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.48553603887557983\n",
            "0.49919554591178894\n",
            "0.5313035249710083\n",
            "0.4935862123966217\n",
            "0.4798407256603241\n",
            "0.49133098125457764\n",
            "0.4772265553474426\n",
            "0.4887737035751343\n",
            "0.4911917448043823\n",
            "0.5226584076881409\n",
            "0.49040019512176514\n",
            "0.5212433934211731\n",
            "0.47300469875335693\n",
            "0.473548024892807\n",
            "0.4896281361579895\n",
            "0.49004265666007996\n",
            "0.49141329526901245\n",
            "0.4929809868335724\n",
            "0.4946800470352173\n",
            "0.4965595006942749\n",
            "0.4847364127635956\n",
            "0.5334200859069824\n",
            "0.4869460463523865\n",
            "0.5035941004753113\n",
            "0.5344363451004028\n",
            "0.5024002194404602\n",
            "0.50483238697052\n",
            "0.5034326910972595\n",
            "0.503418505191803\n",
            "0.48767217993736267\n",
            "0.5041952729225159\n",
            "0.5018696784973145\n",
            "0.50177001953125\n",
            "0.5033053755760193\n",
            "0.5028472542762756\n",
            "0.4998929798603058\n",
            "0.5325108170509338\n",
            "0.4996183514595032\n",
            "0.5005945563316345\n",
            "0.5028500556945801\n",
            "0.5009950399398804\n",
            "0.5347304940223694\n",
            "0.48400723934173584\n",
            "0.5016729235649109\n",
            "0.48353081941604614\n",
            "0.48401522636413574\n",
            "0.5372335910797119\n",
            "0.5023658275604248\n",
            "0.5052419900894165\n",
            "0.5042288899421692\n",
            "0.5409966707229614\n",
            "0.5043768286705017\n",
            "0.5061798691749573\n",
            "0.5042290687561035\n",
            "0.5411043167114258\n",
            "0.5415138602256775\n",
            "0.5046446919441223\n",
            "0.5057051777839661\n",
            "0.5046944618225098\n",
            "0.5434231758117676\n",
            "0.484959214925766\n",
            "0.5079228281974792\n",
            "0.48686811327934265\n",
            "0.5479324460029602\n",
            "0.5493932366371155\n",
            "0.5105642080307007\n",
            "0.5508692860603333\n",
            "0.5120754241943359\n",
            "0.5109277367591858\n",
            "0.5550099611282349\n",
            "0.5148532390594482\n",
            "0.5160742402076721\n",
            "0.5155707001686096\n",
            "0.5604288578033447\n",
            "0.5144621133804321\n",
            "0.49291372299194336\n",
            "0.5141847133636475\n",
            "0.4903048574924469\n",
            "0.5568664073944092\n",
            "0.5122660398483276\n",
            "0.5572327375411987\n",
            "0.5118284225463867\n",
            "0.48987987637519836\n",
            "0.48935040831565857\n",
            "0.5586305260658264\n",
            "0.4876851439476013\n",
            "0.5592215657234192\n",
            "0.4851666986942291\n",
            "0.5087800025939941\n",
            "0.5075633525848389\n",
            "0.5073155760765076\n",
            "0.5076930522918701\n",
            "0.5029403567314148\n",
            "0.5516835451126099\n",
            "0.5496413707733154\n",
            "0.5010352730751038\n",
            "0.4688609540462494\n",
            "0.49872711300849915\n",
            "0.4994789958000183\n",
            "0.46713533997535706\n",
            "----\n",
            "Epoch 8 correct 49, total 100, 1.3780214109504596\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.5446813106536865\n",
            "0.49809783697128296\n",
            "0.4959658086299896\n",
            "0.46185919642448425\n",
            "0.4919513165950775\n",
            "0.4924975335597992\n",
            "0.4925158619880676\n",
            "0.49061259627342224\n",
            "0.4869464635848999\n",
            "0.5390831828117371\n",
            "0.49142080545425415\n",
            "0.49030864238739014\n",
            "0.49003103375434875\n",
            "0.4533711075782776\n",
            "0.5384646058082581\n",
            "0.4893118739128113\n",
            "0.49174365401268005\n",
            "0.5370474457740784\n",
            "0.45531773567199707\n",
            "0.49023377895355225\n",
            "0.5352356433868408\n",
            "0.4929552674293518\n",
            "0.5368596315383911\n",
            "0.46185365319252014\n",
            "0.4955308139324188\n",
            "0.5369314551353455\n",
            "0.4670855402946472\n",
            "0.5341514945030212\n",
            "0.4970366656780243\n",
            "0.49534347653388977\n",
            "0.5352325439453125\n",
            "0.4953855872154236\n",
            "0.49854785203933716\n",
            "0.49534276127815247\n",
            "0.4963501989841461\n",
            "0.48108842968940735\n",
            "0.4872227609157562\n",
            "0.4741174876689911\n",
            "0.5032788515090942\n",
            "0.48704761266708374\n",
            "0.5316355228424072\n",
            "0.5361642241477966\n",
            "0.5049439668655396\n",
            "0.5110315680503845\n",
            "0.5115817189216614\n",
            "0.5493221282958984\n",
            "0.5555242896080017\n",
            "0.5276524424552917\n",
            "0.5099906325340271\n",
            "0.5648437142372131\n",
            "0.5591331720352173\n",
            "0.5151405334472656\n",
            "0.5094283819198608\n",
            "0.5109222531318665\n",
            "0.5108153820037842\n",
            "0.5054957270622253\n",
            "0.5075841546058655\n",
            "0.5045889616012573\n",
            "0.5052976012229919\n",
            "0.5053302645683289\n",
            "0.5019005537033081\n",
            "0.5007240176200867\n",
            "0.5003814101219177\n",
            "0.5398461222648621\n",
            "0.5020245909690857\n",
            "0.542378306388855\n",
            "0.5446208119392395\n",
            "0.5482189655303955\n",
            "0.5088018774986267\n",
            "0.5496367812156677\n",
            "0.48788267374038696\n",
            "0.5108038187026978\n",
            "0.5526151061058044\n",
            "0.5103803873062134\n",
            "0.5100592374801636\n",
            "0.4848474860191345\n",
            "0.5480115413665771\n",
            "0.5055028200149536\n",
            "0.5065532922744751\n",
            "0.5445621013641357\n",
            "0.5440617799758911\n",
            "0.5427146553993225\n",
            "0.5423300266265869\n",
            "0.49906179308891296\n",
            "0.4969354271888733\n",
            "0.5395477414131165\n",
            "0.4961986839771271\n",
            "0.4936886727809906\n",
            "0.5351933836936951\n",
            "0.46423861384391785\n",
            "0.4918563961982727\n",
            "0.46250292658805847\n",
            "0.48794132471084595\n",
            "0.5271210670471191\n",
            "0.49029606580734253\n",
            "0.48763808608055115\n",
            "0.4879189431667328\n",
            "0.4857211410999298\n",
            "0.4825263023376465\n",
            "0.4844941794872284\n",
            "----\n",
            "Epoch 9 correct 48, total 100, 1.4003312142007054\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.5193048715591431\n",
            "0.5196142792701721\n",
            "0.4792831242084503\n",
            "0.45627081394195557\n",
            "0.4768425226211548\n",
            "0.4772065579891205\n",
            "0.4768020808696747\n",
            "0.45802876353263855\n",
            "0.480184942483902\n",
            "0.5195195078849792\n",
            "0.4843026399612427\n",
            "0.4840986132621765\n",
            "0.4860004484653473\n",
            "0.4884510636329651\n",
            "0.46831268072128296\n",
            "0.48865440487861633\n",
            "0.4880770742893219\n",
            "0.5253652334213257\n",
            "0.46812158823013306\n",
            "0.4885280430316925\n",
            "0.4877571761608124\n",
            "0.4857354164123535\n",
            "0.4872351586818695\n",
            "0.4847768247127533\n",
            "0.4849240481853485\n",
            "0.48109525442123413\n",
            "0.4808892011642456\n",
            "0.5200169682502747\n",
            "0.47267335653305054\n",
            "0.4551335871219635\n",
            "0.4528217017650604\n",
            "0.46659576892852783\n",
            "0.46414440870285034\n",
            "0.4624461233615875\n",
            "0.4610498249530792\n",
            "0.45528674125671387\n",
            "0.4360162317752838\n",
            "0.4336942434310913\n",
            "0.4295174479484558\n",
            "0.42592737078666687\n",
            "0.4174109697341919\n",
            "0.4701923727989197\n",
            "0.4199710190296173\n",
            "0.4325058162212372\n",
            "0.4814869463443756\n",
            "0.4332881569862366\n",
            "0.42560768127441406\n",
            "0.4326038658618927\n",
            "0.4277811050415039\n",
            "0.4299525320529938\n",
            "0.4783966541290283\n",
            "0.4764353632926941\n",
            "0.4728996455669403\n",
            "0.4258480668067932\n",
            "0.4244919717311859\n",
            "0.41072267293930054\n",
            "0.413276344537735\n",
            "0.41928911209106445\n",
            "0.4081597328186035\n",
            "0.4601331353187561\n",
            "0.40997445583343506\n",
            "0.4157375991344452\n",
            "0.4222088158130646\n",
            "0.476653128862381\n",
            "0.4398952126502991\n",
            "0.48776695132255554\n",
            "0.48971012234687805\n",
            "0.44707635045051575\n",
            "0.492149293422699\n",
            "0.45358625054359436\n",
            "0.4967728555202484\n",
            "0.45663172006607056\n",
            "0.45780548453330994\n",
            "0.49997323751449585\n",
            "0.5025596618652344\n",
            "0.4614352285861969\n",
            "0.4640248715877533\n",
            "0.4687827229499817\n",
            "0.5085458159446716\n",
            "0.4741913378238678\n",
            "0.4756510853767395\n",
            "0.47725293040275574\n",
            "0.5179144144058228\n",
            "0.4815441071987152\n",
            "0.5272645354270935\n",
            "0.4955218434333801\n",
            "0.543107271194458\n",
            "0.5141749978065491\n",
            "0.5252239108085632\n",
            "0.5649463534355164\n",
            "0.5270302295684814\n",
            "0.5734154582023621\n",
            "0.5316342115402222\n",
            "0.5088608264923096\n",
            "0.5403496026992798\n",
            "0.5150231122970581\n",
            "0.5475731492042542\n",
            "0.5599088668823242\n",
            "0.5614311695098877\n",
            "0.5640091300010681\n",
            "----\n",
            "Epoch 10 correct 55, total 100, 1.3826610952056944\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.5679604411125183\n",
            "0.632624089717865\n",
            "0.6461749076843262\n",
            "0.5454015135765076\n",
            "0.5958200097084045\n",
            "0.5986781120300293\n",
            "0.5921745896339417\n",
            "0.6306065917015076\n",
            "0.5754441022872925\n",
            "0.6122530102729797\n",
            "0.5004115700721741\n",
            "0.549818217754364\n",
            "0.549527645111084\n",
            "0.5347577333450317\n",
            "0.5721989274024963\n",
            "0.5274785757064819\n",
            "0.5683258175849915\n",
            "0.5320690274238586\n",
            "0.5278992056846619\n",
            "0.5218741297721863\n",
            "0.5605161190032959\n",
            "0.5232080817222595\n",
            "0.5194605588912964\n",
            "0.5114047527313232\n",
            "0.5088157057762146\n",
            "0.5411747097969055\n",
            "0.5083513855934143\n",
            "0.504630982875824\n",
            "0.4987286627292633\n",
            "0.5346331000328064\n",
            "0.501287579536438\n",
            "0.5325493812561035\n",
            "0.4964500665664673\n",
            "0.4980144500732422\n",
            "0.5263059735298157\n",
            "0.5233700275421143\n",
            "0.4914323687553406\n",
            "0.5202856659889221\n",
            "0.4856688380241394\n",
            "0.486637681722641\n",
            "0.5115116834640503\n",
            "0.44380784034729004\n",
            "0.5110865235328674\n",
            "0.4826827943325043\n",
            "0.4832911491394043\n",
            "0.4847012460231781\n",
            "0.48391374945640564\n",
            "0.48416852951049805\n",
            "0.4823033809661865\n",
            "0.44584184885025024\n",
            "0.4810708165168762\n",
            "0.4828995168209076\n",
            "0.48463717103004456\n",
            "0.4502549469470978\n",
            "0.5071622133255005\n",
            "0.4852250814437866\n",
            "0.4805985391139984\n",
            "0.48134881258010864\n",
            "0.4779413044452667\n",
            "0.47365856170654297\n",
            "0.46854791045188904\n",
            "0.45911744236946106\n",
            "0.480720579624176\n",
            "0.47703179717063904\n",
            "0.4761420488357544\n",
            "0.4777020215988159\n",
            "0.4769902527332306\n",
            "0.4246785640716553\n",
            "0.4399414360523224\n",
            "0.4372463524341583\n",
            "0.4357443153858185\n",
            "0.46274232864379883\n",
            "0.4609419107437134\n",
            "0.42058229446411133\n",
            "0.4214637875556946\n",
            "0.4227234721183777\n",
            "0.4239509403705597\n",
            "0.46197694540023804\n",
            "0.4307674169540405\n",
            "0.4392107427120209\n",
            "0.4282439947128296\n",
            "0.48286673426628113\n",
            "0.4899633824825287\n",
            "0.45957985520362854\n",
            "0.4474309980869293\n",
            "0.5084213018417358\n",
            "0.45457932353019714\n",
            "0.47271791100502014\n",
            "0.4725879430770874\n",
            "0.5173393487930298\n",
            "0.4759257435798645\n",
            "0.4784473478794098\n",
            "0.48366162180900574\n",
            "0.5301856994628906\n",
            "0.5353453159332275\n",
            "0.4946502149105072\n",
            "0.49315035343170166\n",
            "0.4992629587650299\n",
            "0.499162882566452\n",
            "0.48428061604499817\n",
            "----\n",
            "Epoch 11 correct 47, total 100, 1.4205855225631967\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.5061176419258118\n",
            "0.4890928268432617\n",
            "0.5615636110305786\n",
            "0.5240439772605896\n",
            "0.5299113988876343\n",
            "0.5437670350074768\n",
            "0.5956813097000122\n",
            "0.5582796931266785\n",
            "0.616773784160614\n",
            "0.6314986944198608\n",
            "0.5552099347114563\n",
            "0.59095698595047\n",
            "0.59337317943573\n",
            "0.6011708974838257\n",
            "0.6026524901390076\n",
            "0.6482097506523132\n",
            "0.5497755408287048\n",
            "0.6334484219551086\n",
            "0.5792060494422913\n",
            "0.562882661819458\n",
            "0.5981882810592651\n",
            "0.5990822315216064\n",
            "0.5600219368934631\n",
            "0.5545490384101868\n",
            "0.5499901175498962\n",
            "0.5454434156417847\n",
            "0.5397421717643738\n",
            "0.5372401475906372\n",
            "0.5317062735557556\n",
            "0.5311551690101624\n",
            "0.5050088167190552\n",
            "0.504694938659668\n",
            "0.5289876461029053\n",
            "0.5258115530014038\n",
            "0.5247565507888794\n",
            "0.5217399597167969\n",
            "0.5663398504257202\n",
            "0.5667661428451538\n",
            "0.5654945969581604\n",
            "0.5172979235649109\n",
            "0.5167692303657532\n",
            "0.5141810178756714\n",
            "0.5113978385925293\n",
            "0.5078193545341492\n",
            "0.5485777854919434\n",
            "0.4994451105594635\n",
            "0.47490307688713074\n",
            "0.4904199540615082\n",
            "0.4876037538051605\n",
            "0.48142576217651367\n",
            "0.4834060072898865\n",
            "0.47943899035453796\n",
            "0.4767007529735565\n",
            "0.5160230994224548\n",
            "0.45717695355415344\n",
            "0.5220081210136414\n",
            "0.5206020474433899\n",
            "0.45548123121261597\n",
            "0.5237212181091309\n",
            "0.5269829034805298\n",
            "0.4894411861896515\n",
            "0.5299672484397888\n",
            "0.4912232756614685\n",
            "0.4665270447731018\n",
            "0.5327656269073486\n",
            "0.4969905912876129\n",
            "0.49668973684310913\n",
            "0.5423063039779663\n",
            "0.5050227046012878\n",
            "0.5500954985618591\n",
            "0.5152038335800171\n",
            "0.5139970779418945\n",
            "0.5122405290603638\n",
            "0.516814112663269\n",
            "0.5164216160774231\n",
            "0.5168818831443787\n",
            "0.4926113784313202\n",
            "0.5152881145477295\n",
            "0.48886018991470337\n",
            "0.5080366730690002\n",
            "0.5070880651473999\n",
            "0.5101178884506226\n",
            "0.5066162347793579\n",
            "0.5507726073265076\n",
            "0.5080779790878296\n",
            "0.5071091651916504\n",
            "0.5499838590621948\n",
            "0.5489495396614075\n",
            "0.5032101273536682\n",
            "0.544553816318512\n",
            "0.5412854552268982\n",
            "0.49648743867874146\n",
            "0.49587732553482056\n",
            "0.488905131816864\n",
            "0.4867362976074219\n",
            "0.527457058429718\n",
            "0.5239943861961365\n",
            "0.4584866762161255\n",
            "0.48211023211479187\n",
            "0.5224448442459106\n",
            "----\n",
            "Epoch 12 correct 46, total 100, 1.408974515851587\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.4797389507293701\n",
            "0.4834115505218506\n",
            "0.5227611064910889\n",
            "0.4830750524997711\n",
            "0.48532193899154663\n",
            "0.5290846824645996\n",
            "0.5321035385131836\n",
            "0.49243345856666565\n",
            "0.49570026993751526\n",
            "0.5382190346717834\n",
            "0.4798215627670288\n",
            "0.543396532535553\n",
            "0.5027598738670349\n",
            "0.5439168214797974\n",
            "0.5017629265785217\n",
            "0.5424275994300842\n",
            "0.5026171803474426\n",
            "0.4808465242385864\n",
            "0.5032559633255005\n",
            "0.5447430610656738\n",
            "0.48495420813560486\n",
            "0.48566991090774536\n",
            "0.5111821293830872\n",
            "0.5117793083190918\n",
            "0.5130819082260132\n",
            "0.51256263256073\n",
            "0.512043833732605\n",
            "0.5132427215576172\n",
            "0.5518243312835693\n",
            "0.5121687054634094\n",
            "0.5536589622497559\n",
            "0.5557120442390442\n",
            "0.5168479681015015\n",
            "0.497361958026886\n",
            "0.5202785134315491\n",
            "0.5037641525268555\n",
            "0.5076138377189636\n",
            "0.5327366590499878\n",
            "0.5350825190544128\n",
            "0.534663200378418\n",
            "0.5396262407302856\n",
            "0.5396731495857239\n",
            "0.58540940284729\n",
            "0.5321274995803833\n",
            "0.5812076926231384\n",
            "0.5349559187889099\n",
            "0.5354762673377991\n",
            "0.5823961496353149\n",
            "0.520336925983429\n",
            "0.5172832012176514\n",
            "0.5281822085380554\n",
            "0.5720036625862122\n",
            "0.5257475972175598\n",
            "0.52409827709198\n",
            "0.521885097026825\n",
            "0.5011939406394958\n",
            "0.5164015293121338\n",
            "0.49440354108810425\n",
            "0.5555661916732788\n",
            "0.5077962875366211\n",
            "0.5047535300254822\n",
            "0.5474418997764587\n",
            "0.48168477416038513\n",
            "0.5004124641418457\n",
            "0.4783684015274048\n",
            "0.5432742834091187\n",
            "0.49909019470214844\n",
            "0.542707622051239\n",
            "0.4984395503997803\n",
            "0.49793002009391785\n",
            "0.5408517718315125\n",
            "0.4961372911930084\n",
            "0.5420841574668884\n",
            "0.49509531259536743\n",
            "0.5393726229667664\n",
            "0.4952310621738434\n",
            "0.4927683174610138\n",
            "0.5357288122177124\n",
            "0.4927979111671448\n",
            "0.4942082464694977\n",
            "0.4917052090167999\n",
            "0.5345553159713745\n",
            "0.4894757568836212\n",
            "0.487572580575943\n",
            "0.5315713882446289\n",
            "0.4894460439682007\n",
            "0.4873785674571991\n",
            "0.48687535524368286\n",
            "0.48806172609329224\n",
            "0.4690379798412323\n",
            "0.492900013923645\n",
            "0.46972718834877014\n",
            "0.5345838665962219\n",
            "0.4924609363079071\n",
            "0.49195489287376404\n",
            "0.49271470308303833\n",
            "0.48818910121917725\n",
            "0.5302425026893616\n",
            "0.4642362594604492\n",
            "0.5271666049957275\n",
            "----\n",
            "Epoch 13 correct 46, total 100, 1.39328928173054\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.48531705141067505\n",
            "0.4826802909374237\n",
            "0.5225241780281067\n",
            "0.5209672451019287\n",
            "0.480905681848526\n",
            "0.5191752910614014\n",
            "0.45619726181030273\n",
            "0.45524635910987854\n",
            "0.48108819127082825\n",
            "0.47883403301239014\n",
            "0.45460471510887146\n",
            "0.480251669883728\n",
            "0.48041296005249023\n",
            "0.48102396726608276\n",
            "0.5179970264434814\n",
            "0.4799298346042633\n",
            "0.48085489869117737\n",
            "0.48459628224372864\n",
            "0.48454955220222473\n",
            "0.4833817481994629\n",
            "0.4814361035823822\n",
            "0.521520733833313\n",
            "0.4844113886356354\n",
            "0.5239165425300598\n",
            "0.45661941170692444\n",
            "0.4842279553413391\n",
            "0.4859212338924408\n",
            "0.5237364172935486\n",
            "0.4857153296470642\n",
            "0.520988404750824\n",
            "0.4513556957244873\n",
            "0.4498826861381531\n",
            "0.4785892069339752\n",
            "0.5179320573806763\n",
            "0.47879767417907715\n",
            "0.515651285648346\n",
            "0.4748094081878662\n",
            "0.44424504041671753\n",
            "0.44235891103744507\n",
            "0.47397580742836\n",
            "0.4714667499065399\n",
            "0.4370301365852356\n",
            "0.467040091753006\n",
            "0.5045071840286255\n",
            "0.4700949788093567\n",
            "0.46785497665405273\n",
            "0.467343807220459\n",
            "0.46390610933303833\n",
            "0.4643990993499756\n",
            "0.4689098596572876\n",
            "0.469331830739975\n",
            "0.5024471282958984\n",
            "0.4719889163970947\n",
            "0.43202319741249084\n",
            "0.4697020351886749\n",
            "0.47431960701942444\n",
            "0.47401952743530273\n",
            "0.5065087080001831\n",
            "0.47397109866142273\n",
            "0.47495341300964355\n",
            "0.43700024485588074\n",
            "0.47677868604660034\n",
            "0.4775269031524658\n",
            "0.4760776460170746\n",
            "0.44214144349098206\n",
            "0.5138037800788879\n",
            "0.4451630413532257\n",
            "0.4806571900844574\n",
            "0.4781430661678314\n",
            "0.5131174325942993\n",
            "0.4454721212387085\n",
            "0.47933122515678406\n",
            "0.5128528475761414\n",
            "0.5135181546211243\n",
            "0.4772402346134186\n",
            "0.4755563735961914\n",
            "0.4437042474746704\n",
            "0.5115258097648621\n",
            "0.510081946849823\n",
            "0.5093669891357422\n",
            "0.47060635685920715\n",
            "0.4740052819252014\n",
            "0.47388648986816406\n",
            "0.4734310805797577\n",
            "0.5056591629981995\n",
            "0.4723670780658722\n",
            "0.5049030780792236\n",
            "0.47157129645347595\n",
            "0.5047632455825806\n",
            "0.5051464438438416\n",
            "0.5061032176017761\n",
            "0.43350571393966675\n",
            "0.506955623626709\n",
            "0.46748068928718567\n",
            "0.4678571820259094\n",
            "0.5079819560050964\n",
            "0.4726925492286682\n",
            "0.47328808903694153\n",
            "0.4742189645767212\n",
            "0.5104421377182007\n",
            "----\n",
            "Epoch 14 correct 52, total 100, 1.387803483195603\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.4769513010978699\n",
            "0.47870728373527527\n",
            "0.480033814907074\n",
            "0.4829493463039398\n",
            "0.48138028383255005\n",
            "0.45136016607284546\n",
            "0.4858293831348419\n",
            "0.48684826493263245\n",
            "0.4901585578918457\n",
            "0.49358850717544556\n",
            "0.5313538908958435\n",
            "0.4938546419143677\n",
            "0.4908500611782074\n",
            "0.4898231327533722\n",
            "0.4921185076236725\n",
            "0.45558613538742065\n",
            "0.4550516605377197\n",
            "0.489920437335968\n",
            "0.4923197627067566\n",
            "0.4917706549167633\n",
            "0.4914388060569763\n",
            "0.4551618695259094\n",
            "0.4925837218761444\n",
            "0.4910127520561218\n",
            "0.5282577872276306\n",
            "0.49302202463150024\n",
            "0.49007466435432434\n",
            "0.4926767945289612\n",
            "0.52980637550354\n",
            "0.49352556467056274\n",
            "0.49095723032951355\n",
            "0.49022868275642395\n",
            "0.5272259712219238\n",
            "0.49074438214302063\n",
            "0.5268293023109436\n",
            "0.49021217226982117\n",
            "0.49092820286750793\n",
            "0.4912899434566498\n",
            "0.4913458526134491\n",
            "0.493019700050354\n",
            "0.4603973925113678\n",
            "0.4940117597579956\n",
            "0.49260321259498596\n",
            "0.49486038088798523\n",
            "0.4633679687976837\n",
            "0.4924817979335785\n",
            "0.49512675404548645\n",
            "0.5304316878318787\n",
            "0.5296813249588013\n",
            "0.4915061891078949\n",
            "0.4614812731742859\n",
            "0.49177277088165283\n",
            "0.48951950669288635\n",
            "0.4889761507511139\n",
            "0.49174004793167114\n",
            "0.4920085370540619\n",
            "0.46188101172447205\n",
            "0.5238311290740967\n",
            "0.5232981443405151\n",
            "0.5233703851699829\n",
            "0.4927934408187866\n",
            "0.49038419127464294\n",
            "0.52471524477005\n",
            "0.49163103103637695\n",
            "0.4677521884441376\n",
            "0.5278598666191101\n",
            "0.4961986839771271\n",
            "0.47012943029403687\n",
            "0.49637895822525024\n",
            "0.47042229771614075\n",
            "0.499356210231781\n",
            "0.5337487459182739\n",
            "0.534725546836853\n",
            "0.5010998845100403\n",
            "0.4731425940990448\n",
            "0.5375602841377258\n",
            "0.4990388751029968\n",
            "0.5002922415733337\n",
            "0.5363499522209167\n",
            "0.5354644656181335\n",
            "0.4959201216697693\n",
            "0.49568817019462585\n",
            "0.49746546149253845\n",
            "0.49463069438934326\n",
            "0.49610742926597595\n",
            "0.46480250358581543\n",
            "0.5276064276695251\n",
            "0.4616849720478058\n",
            "0.5253921151161194\n",
            "0.4889076054096222\n",
            "0.5239981412887573\n",
            "0.4864172637462616\n",
            "0.5233136415481567\n",
            "0.48858365416526794\n",
            "0.4888351261615753\n",
            "0.4886089265346527\n",
            "0.5240131616592407\n",
            "0.524405837059021\n",
            "0.5254677534103394\n",
            "0.5256456732749939\n",
            "----\n",
            "Epoch 15 correct 55, total 100, 1.3843371534347535\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.48837408423423767\n",
            "0.4882173538208008\n",
            "0.5265973806381226\n",
            "0.48694735765457153\n",
            "0.5280236601829529\n",
            "0.48485296964645386\n",
            "0.48982954025268555\n",
            "0.5324984788894653\n",
            "0.4530649483203888\n",
            "0.48723703622817993\n",
            "0.5322174429893494\n",
            "0.48629724979400635\n",
            "0.4497247636318207\n",
            "0.532471239566803\n",
            "0.4873930811882019\n",
            "0.4841994345188141\n",
            "0.5315594673156738\n",
            "0.486969530582428\n",
            "0.48810553550720215\n",
            "0.45073387026786804\n",
            "0.4906946122646332\n",
            "0.4905473589897156\n",
            "0.4873606860637665\n",
            "0.48795515298843384\n",
            "0.4916086792945862\n",
            "0.4553450047969818\n",
            "0.5362493991851807\n",
            "0.48852282762527466\n",
            "0.5316224694252014\n",
            "0.4831963777542114\n",
            "0.5319075584411621\n",
            "0.48607081174850464\n",
            "0.4824555516242981\n",
            "0.44949933886528015\n",
            "0.4821036458015442\n",
            "0.4826430678367615\n",
            "0.5305418372154236\n",
            "0.48316287994384766\n",
            "0.4869000315666199\n",
            "0.45232129096984863\n",
            "0.5316860675811768\n",
            "0.4513048529624939\n",
            "0.5299405455589294\n",
            "0.5283945202827454\n",
            "0.527634859085083\n",
            "0.48220011591911316\n",
            "0.481795996427536\n",
            "0.5236650109291077\n",
            "0.4799940288066864\n",
            "0.4743283689022064\n",
            "0.4770093262195587\n",
            "0.43673229217529297\n",
            "0.5133729577064514\n",
            "0.4317430555820465\n",
            "0.467742919921875\n",
            "0.4698764383792877\n",
            "0.46699485182762146\n",
            "0.4690435826778412\n",
            "0.46869805455207825\n",
            "0.46716707944869995\n",
            "0.5075052380561829\n",
            "0.4680296778678894\n",
            "0.4656657874584198\n",
            "0.5092844367027283\n",
            "0.464896023273468\n",
            "0.4697379171848297\n",
            "0.47107839584350586\n",
            "0.47373276948928833\n",
            "0.4710097312927246\n",
            "0.43844321370124817\n",
            "0.5206520557403564\n",
            "0.4750022292137146\n",
            "0.47971805930137634\n",
            "0.5243158936500549\n",
            "0.48069772124290466\n",
            "0.48110640048980713\n",
            "0.48581305146217346\n",
            "0.4851444959640503\n",
            "0.4907948076725006\n",
            "0.48917216062545776\n",
            "0.49488386511802673\n",
            "0.5385478138923645\n",
            "0.49313846230506897\n",
            "0.5409358143806458\n",
            "0.46210286021232605\n",
            "0.49791470170021057\n",
            "0.4937536418437958\n",
            "0.46267232298851013\n",
            "0.49422159790992737\n",
            "0.49458250403404236\n",
            "0.5375410914421082\n",
            "0.5354785323143005\n",
            "0.5345680713653564\n",
            "0.48588868975639343\n",
            "0.4578714966773987\n",
            "0.5361209511756897\n",
            "0.49162331223487854\n",
            "0.4579015374183655\n",
            "0.48729607462882996\n",
            "0.4911663234233856\n",
            "----\n",
            "Epoch 16 correct 54, total 100, 1.3874050815030932\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.4570382535457611\n",
            "0.5402203798294067\n",
            "0.5406848192214966\n",
            "0.4942053556442261\n",
            "0.5394461154937744\n",
            "0.5379160642623901\n",
            "0.5372003316879272\n",
            "0.4875155985355377\n",
            "0.48725464940071106\n",
            "0.5349611639976501\n",
            "0.4883803129196167\n",
            "0.492849737405777\n",
            "0.49459031224250793\n",
            "0.5390603542327881\n",
            "0.5409061312675476\n",
            "0.4643409848213196\n",
            "0.5447071194648743\n",
            "0.5467119216918945\n",
            "0.5004497170448303\n",
            "0.5482922196388245\n",
            "0.5042429566383362\n",
            "0.550739049911499\n",
            "0.5512990951538086\n",
            "0.5065893530845642\n",
            "0.5537890195846558\n",
            "0.5082841515541077\n",
            "0.47060248255729675\n",
            "0.5547664761543274\n",
            "0.50928795337677\n",
            "0.5095713138580322\n",
            "0.4735334813594818\n",
            "0.5085756182670593\n",
            "0.5125751495361328\n",
            "0.5146592855453491\n",
            "0.5127987861633301\n",
            "0.5166336297988892\n",
            "0.516042172908783\n",
            "0.5194092392921448\n",
            "0.5168190598487854\n",
            "0.5671395659446716\n",
            "0.5204482674598694\n",
            "0.5177525877952576\n",
            "0.569158673286438\n",
            "0.5175288319587708\n",
            "0.5176305770874023\n",
            "0.5682759881019592\n",
            "0.5194398164749146\n",
            "0.5181534886360168\n",
            "0.5180103182792664\n",
            "0.5631727576255798\n",
            "0.5129172205924988\n",
            "0.5140348672866821\n",
            "0.4808022081851959\n",
            "0.5092095136642456\n",
            "0.5091355443000793\n",
            "0.553968608379364\n",
            "0.5059137940406799\n",
            "0.4762657582759857\n",
            "0.502967894077301\n",
            "0.5014607310295105\n",
            "0.49949386715888977\n",
            "0.5408338904380798\n",
            "0.4983160197734833\n",
            "0.49480491876602173\n",
            "0.5349244475364685\n",
            "0.5342637300491333\n",
            "0.5328945517539978\n",
            "0.4898052215576172\n",
            "0.5313465595245361\n",
            "0.5311185717582703\n",
            "0.5301663875579834\n",
            "0.4618086814880371\n",
            "0.4889342486858368\n",
            "0.483014315366745\n",
            "0.48113030195236206\n",
            "0.5203737020492554\n",
            "0.5196810364723206\n",
            "0.4788360595703125\n",
            "0.4524339735507965\n",
            "0.4804770350456238\n",
            "0.4762261211872101\n",
            "0.4767444133758545\n",
            "0.5164383053779602\n",
            "0.44948455691337585\n",
            "0.4797821044921875\n",
            "0.5141388773918152\n",
            "0.47538724541664124\n",
            "0.4778049886226654\n",
            "0.4734768271446228\n",
            "0.4786924123764038\n",
            "0.4775770902633667\n",
            "0.4781520664691925\n",
            "0.45081815123558044\n",
            "0.5131245851516724\n",
            "0.48136305809020996\n",
            "0.48248568177223206\n",
            "0.5158897638320923\n",
            "0.47949856519699097\n",
            "0.5161601305007935\n",
            "0.48299193382263184\n",
            "----\n",
            "Epoch 17 correct 42, total 100, 1.4070671757729725\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.4818924367427826\n",
            "0.4818224608898163\n",
            "0.4813273549079895\n",
            "0.48190927505493164\n",
            "0.4834318459033966\n",
            "0.48027682304382324\n",
            "0.4841548204421997\n",
            "0.5177215933799744\n",
            "0.45836713910102844\n",
            "0.4854530990123749\n",
            "0.48512378334999084\n",
            "0.5173248648643494\n",
            "0.4815156161785126\n",
            "0.4807640016078949\n",
            "0.514293909072876\n",
            "0.4799606502056122\n",
            "0.5111555457115173\n",
            "0.47641000151634216\n",
            "0.4793992042541504\n",
            "0.5066094398498535\n",
            "0.5060204267501831\n",
            "0.5047093629837036\n",
            "0.47351616621017456\n",
            "0.47317269444465637\n",
            "0.47582802176475525\n",
            "0.4746955931186676\n",
            "0.475705087184906\n",
            "0.47701627016067505\n",
            "0.47685033082962036\n",
            "0.4486919343471527\n",
            "0.4489594101905823\n",
            "0.4726002514362335\n",
            "0.472513884305954\n",
            "0.47198790311813354\n",
            "0.44997018575668335\n",
            "0.5007440447807312\n",
            "0.4710841178894043\n",
            "0.4989233613014221\n",
            "0.4511442184448242\n",
            "0.4512258470058441\n",
            "0.4742465615272522\n",
            "0.5003710985183716\n",
            "0.47223231196403503\n",
            "0.4707459509372711\n",
            "0.4540039002895355\n",
            "0.47189369797706604\n",
            "0.47441887855529785\n",
            "0.4702215790748596\n",
            "0.497090607881546\n",
            "0.49749091267585754\n",
            "0.4699471890926361\n",
            "0.4750975966453552\n",
            "0.471903532743454\n",
            "0.47265365719795227\n",
            "0.47425419092178345\n",
            "0.4752112627029419\n",
            "0.47889000177383423\n",
            "0.47671765089035034\n",
            "0.4594155251979828\n",
            "0.4788588881492615\n",
            "0.4620819389820099\n",
            "0.46290698647499084\n",
            "0.4852428138256073\n",
            "0.48469820618629456\n",
            "0.515813946723938\n",
            "0.48868417739868164\n",
            "0.48853573203086853\n",
            "0.48768356442451477\n",
            "0.4914551079273224\n",
            "0.4910408556461334\n",
            "0.49520188570022583\n",
            "0.4935154616832733\n",
            "0.5262976288795471\n",
            "0.5259327292442322\n",
            "0.5261560678482056\n",
            "0.4959324300289154\n",
            "0.471346378326416\n",
            "0.49312230944633484\n",
            "0.52744060754776\n",
            "0.5282455682754517\n",
            "0.494505375623703\n",
            "0.4983207583427429\n",
            "0.49617040157318115\n",
            "0.4725782573223114\n",
            "0.5332382917404175\n",
            "0.5004643201828003\n",
            "0.5008447766304016\n",
            "0.5362946391105652\n",
            "0.5377910137176514\n",
            "0.5037451982498169\n",
            "0.5017788410186768\n",
            "0.5418533086776733\n",
            "0.5431675314903259\n",
            "0.5028469562530518\n",
            "0.5053478479385376\n",
            "0.5055196285247803\n",
            "0.5441241264343262\n",
            "0.5434365272521973\n",
            "0.5020704865455627\n",
            "0.5429871082305908\n",
            "----\n",
            "Epoch 18 correct 47, total 100, 1.3881234917487018\n",
            "----\n",
            "0.0\n",
            "On batch 0\n",
            "0.0\n",
            "0.5001176595687866\n",
            "0.49951791763305664\n",
            "0.5391961932182312\n",
            "0.5372499227523804\n",
            "0.4971599578857422\n",
            "0.5331891179084778\n",
            "0.49212682247161865\n",
            "0.46862301230430603\n",
            "0.5313912630081177\n",
            "0.5306440591812134\n",
            "0.5304769277572632\n",
            "0.49387064576148987\n",
            "0.46838340163230896\n",
            "0.492887020111084\n",
            "0.4922846853733063\n",
            "0.5269813537597656\n",
            "0.4922318756580353\n",
            "0.49317002296447754\n",
            "0.4908667206764221\n",
            "0.49121785163879395\n",
            "0.4948740005493164\n",
            "0.5302322506904602\n",
            "0.49392634630203247\n",
            "0.5324186682701111\n",
            "0.5326188802719116\n",
            "0.49540069699287415\n",
            "0.5334891676902771\n",
            "0.4762274920940399\n",
            "0.49903151392936707\n",
            "0.49909350275993347\n",
            "0.47633054852485657\n",
            "0.4992724657058716\n",
            "0.5349323749542236\n",
            "0.49770092964172363\n",
            "0.4958646595478058\n",
            "0.4927501380443573\n",
            "0.4909254312515259\n",
            "0.49105125665664673\n",
            "0.4935075640678406\n",
            "0.4910094738006592\n",
            "0.49194368720054626\n",
            "0.49649718403816223\n",
            "0.4967557489871979\n",
            "0.4721946716308594\n",
            "0.4945373833179474\n",
            "0.5335994362831116\n",
            "0.4969850182533264\n",
            "0.49419742822647095\n",
            "0.49428805708885193\n",
            "0.4726773202419281\n",
            "0.4966203570365906\n",
            "0.47145041823387146\n",
            "0.5305004119873047\n",
            "0.5295858383178711\n",
            "0.494886189699173\n",
            "0.49146708846092224\n",
            "0.4914551079273224\n",
            "0.5281141996383667\n",
            "0.49335047602653503\n",
            "0.5264524221420288\n",
            "0.46950003504753113\n",
            "0.492813378572464\n",
            "0.49267709255218506\n",
            "0.5259856581687927\n",
            "0.4916429817676544\n",
            "0.4892512559890747\n",
            "0.48785021901130676\n",
            "0.48978954553604126\n",
            "0.4640028774738312\n",
            "0.51917564868927\n",
            "0.5180929899215698\n",
            "0.4806068539619446\n",
            "0.5140056014060974\n",
            "0.4770909547805786\n",
            "0.4796947240829468\n",
            "0.47755277156829834\n",
            "0.47631195187568665\n",
            "0.5069488286972046\n",
            "0.47275274991989136\n",
            "0.4760937988758087\n",
            "0.47261232137680054\n",
            "0.5103586316108704\n",
            "0.5120747685432434\n",
            "0.48126643896102905\n",
            "0.47774893045425415\n",
            "0.4817318618297577\n",
            "0.48163479566574097\n",
            "0.4829472303390503\n",
            "0.48197615146636963\n",
            "0.4538874924182892\n",
            "0.4825032353401184\n",
            "0.48205259442329407\n",
            "0.4794434905052185\n",
            "0.4567272365093231\n",
            "0.4565792381763458\n",
            "0.48016515374183655\n",
            "0.5213584303855896\n",
            "0.4855346381664276\n",
            "0.487669974565506\n",
            "0.5254331827163696\n",
            "----\n",
            "Epoch 19 correct 53, total 100, 1.3881510338449152\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nBN66IbdbqX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "cdvqAx5Qpbyr",
        "outputId": "4969034f-382f-4b31-ec96-1e996e806c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-158-dc3c7fb06e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedded_latex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatex_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokenized_latex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tok_eqn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQQgeHtnn3Vj",
        "outputId": "8020f337-0284-4e0d-ac3a-f26fa997b332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[[  101,   182,   168,   189,   134,   176,   168,   189,   117,   165,\n",
            "            186, 13284,  1181,   159,   168,   189,   134,   165,   175, 19366,\n",
            "            196,   122,   198,   196,   189,   198,   165, 12523,  9019,   196,\n",
            "           4267,  8517,   198,   165,  1286,   113,   165,  7584,   168,   196,\n",
            "            178,   134,   122,   198,   167,   189,   176,   168,   178,   176,\n",
            "            168,   178,   167,   196,   165,  1499,   198,   165,  1268,   114,\n",
            "            119,   102]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_model.latex_encoder(**tok_eqn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "B6jmhvltnsG1",
        "outputId": "ac9f04fa-5328-4ba8-c799-fe9960281726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-e05a3305cbe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatex_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtok_eqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         )\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             )\n\u001b[1;32m    352\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         )\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    192\u001b[0m             seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n\u001b[1;32m    193\u001b[0m         \"\"\"\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mk_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "giG79D_i_WTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = torch.randn((2,4,1024))"
      ],
      "metadata": {
        "id": "KeVXJGga-zrv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}